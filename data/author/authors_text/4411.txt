Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 369?376,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Exploring the Potential of Intractable Parsers
Mark Hopkins
Dept. of Computational Linguistics
Saarland University
Saarbr?ucken, Germany
mhopkins@coli.uni-sb.de
Jonas Kuhn
Dept. of Computational Linguistics
Saarland University
Saarbr?ucken, Germany
jonask@coli.uni-sb.de
Abstract
We revisit the idea of history-based pars-
ing, and present a history-based parsing
framework that strives to be simple, gen-
eral, and flexible. We also provide a de-
coder for this probability model that is
linear-space, optimal, and anytime. A
parser based on this framework, when
evaluated on Section 23 of the Penn Tree-
bank, compares favorably with other state-
of-the-art approaches, in terms of both ac-
curacy and speed.
1 Introduction
Much of the current research into probabilis-
tic parsing is founded on probabilistic context-
free grammars (PCFGs) (Collins, 1996; Charniak,
1997; Collins, 1999; Charniak, 2000; Charniak,
2001; Klein and Manning, 2003). For instance,
consider the parse tree in Figure 1. One way to de-
compose this parse tree is to view it as a sequence
of applications of CFG rules. For this particular
tree, we could view it as the application of rule
?NP ? NP PP,? followed by rule ?NP ? DT NN,?
followed by rule ?DT ? that,? and so forth. Hence
instead of analyzing P (tree), we deal with the
more modular:
P(NP ? NP PP, NP ? DT NN,
DT ? that, NN ? money, PP ? IN NP,
IN ? in, NP ? DT NN, DT ? the,
NN ? market)
Obviously this joint distribution is just as diffi-
cult to assess and compute with as P (tree). How-
ever there exist cubic-time dynamic programming
algorithms to find the most likely parse if we as-
sume that all CFG rule applications are marginally
NP
NP
DT
that
NN
money
PP
IN
in
NP
DT
the
NN
market
Figure 1: Example parse tree.
independent of one another. The problem, of
course, with this simplification is that although
it is computationally attractive, it is usually too
strong of an independence assumption. To miti-
gate this loss of context, without sacrificing algo-
rithmic tractability, typically researchers annotate
the nodes of the parse tree with contextual infor-
mation. A simple example is the annotation of
nodes with their parent labels (Johnson, 1998).
The choice of which annotations to use is
one of the main features that distinguish parsers
based on this approach. Generally, this approach
has proven quite effective in producing English
phrase-structure grammar parsers that perform
well on the Penn Treebank.
One drawback of this approach is its inflexibil-
ity. Because we are adding probabilistic context
by changing the data itself, we make our data in-
creasingly sparse as we add features. Thus we are
constrained from adding too many features, be-
cause at some point we will not have enough data
to sustain them. We must strike a delicate bal-
ance between how much context we want to in-
clude versus how much we dare to partition our
data set.
369
The major alternative to PCFG-based ap-
proaches are so-called history-based parsers
(Black et al, 1993). These parsers differ from
PCFG parsers in that they incorporate context by
using a more complex probability model, rather
than by modifying the data itself. The tradeoff to
using a more powerful probabilistic model is that
one can no longer employ dynamic programming
to find the most probable parse. Thus one trades
assurances of polynomial running time for greater
modeling flexibility.
There are two canonical parsers that fall into
this category: the decision-tree parser of (Mager-
man, 1995), and the maximum-entropy parser of
(Ratnaparkhi, 1997). Both showed decent results
on parsing the Penn Treebank, but in the decade
since these papers were published, history-based
parsers have been largely ignored by the research
community in favor of PCFG-based approaches.
There are several reasons why this may be. First
is naturally the matter of time efficiency. Mager-
man reports decent parsing times, but for the pur-
poses of efficiency, must restrict his results to sen-
tences of length 40 or less. Furthermore, his two-
phase stack decoder is a bit complicated and is ac-
knowledged to require too much memory to han-
dle certain sentences. Ratnaparkhi is vague about
the running time performance of his parser, stat-
ing that it is ?observed linear-time,? but in any
event, provides only a heuristic, not a complete al-
gorithm.
Next is the matter of flexibility. The main ad-
vantage of abandoning PCFGs is the opportunity
to have a more flexible and adaptable probabilis-
tic parsing model. Unfortunately, both Magerman
and Ratnaparkhi?s models are rather specific and
complicated. Ratnaparkhi?s, for instance, consists
of the interleaved sequence of four different types
of tree construction operations. Furthermore, both
are inextricably tied to the learning procedure that
they employ (decision trees for Magerman, maxi-
mum entropy for Ratnaparkhi).
In this work, our goal is to revisit history-based
parsers, and provide a general-purpose framework
that is (a) simple, (b) fast, (c) space-efficient and
(d) easily adaptable to new domains. As a method
of evaluation, we use this framework with a very
simple set of features to see how well it performs
(both in terms of accuracy and running time) on
the Penn Treebank. The overarching goal is to de-
velop a history-based hierarchical labeling frame-
work that is viable not only for parsing, but for
other application areas that current rely on dy-
namic programming, like phrase-based machine
translation.
2 Preliminaries
For the following discussion, it will be useful to
establish some terminology and notational con-
ventions. Typically we will represent variables
with capital letters (e.g. X , Y ) and sets of vari-
ables with bold-faced capital letters (e.g. X,
Y). The domain of a variable X will be denoted
dom(X), and typically we will use the lower-case
correspondent (in this case, x) to denote a value in
the domain of X . A partial assignment (or simply
assignment) of a set X of variables is a function
w that maps a subset W of the variables of X
to values in their respective domains. We define
dom(w) = W. When W = X, then we say that
w is a full assignment of X. The trivial assign-
ment of X makes no variable assignments.
Let w(X) denote the value that partial assign-
ment w assigns to variable X . For value x ?
dom(X), let w[X = x] denote the assignment
identical to w except that w[X = x](X) = x.
For a set Y of variables, let w|Y denote the re-striction of partial assignment w to the variables
in dom(w) ? Y.
3 The Generative Model
The goal of this section is to develop a probabilis-
tic process that generates labeled trees in a manner
considerably different from PCFGs. We will use
the tree in Figure 2 to motivate our model. In this
example, nodes of the tree are labeled with either
an A or a B. We can represent this tree using two
charts. One chart labels each span with a boolean
value, such that a span is labeled true iff it is a
constituent in the tree. The other chart labels each
span with a label from our labeling scheme (A or
B) or with the value null (to represent that the
span is unlabeled). We show these charts in Fig-
ure 3. Notice that we may want to have more than
one labeling scheme. For instance, in the parse
tree of Figure 1, there are three different types of
labels: word labels, preterminal labels, and nonter-
minal labels. Thus we would use four 5x5 charts
instead of two 3x3 charts to represent that tree.
We will pause here and generalize these con-
cepts. Define a labeling scheme as a set of symbols
including a special symbol null (this will desig-
370
A
B
A B
B
Figure 2: Example labeled tree.
1 2 3
1 true true true
2 - true false
3 - - true
1 2 3
1 A B A
2 - B null
3 - - B
Figure 3: Chart representation of the example tree:
the left chart tells us which spans are tree con-
stituents, and the right chart tells us the labels of
the spans (null means unlabeled).
nate that a given span is unlabeled). For instance,
we can define L1 = {null, A,B} to be a labeling
scheme for the example tree.
Let L = {L1, L2, ...Lm} be a set of labeling
schemes. Define a model variable of L as a sym-
bol of the form Sij or Lkij , for positive integers i,
j, k, such that i ? j and k ? m. Model vari-
ables of the form Sij indicate whether span (i, j)is a tree constituent, hence the domain of Sij is
{true, false}. Such variables correspond to en-
tries in the left chart of Figure 3. Model variables
of the form Lkij indicate which label from scheme
Lk is assigned to span (i, j), hence the domain of
model variable Lkij is Lk. Such variables corre-spond to entries in the right chart of Figure 3. Here
we have only one labeling scheme.
Let VL be the (countably infinite) set of modelvariables of L. Usually we are interested in trees
over a given sentence of finite length n. Let VnLdenote the finite subset of VL that includes pre-cisely the model variables of the form Sij or Lkij ,where j ? n.
Basically then, our model consists of two types
of decisions: (1) whether a span should be labeled,
and (2) if so, what label(s) the span should have.
Let us proceed with our example. To generate the
tree of Figure 2, the first decision we need to make
is how many leaves it will have (or equivalently,
how large our tables will be). We assume that we
have a probability distribution PN over the set ofpositive integers. For our example tree, we draw
the value 3, with probability PN (3).Now that we know our tree will have three
leaves, we can now decide which spans will be
constituents and what labels they will have. In
other words, we assign values to the variables in
V3L. First we need to choose the order in whichwe will make these assignments. For our exam-
ple, we will assign model variables in the follow-
ing order: S11, L111, S22, L122, S33, L133, S12, L112,
S23, L123, S13, L113. A detailed look at this assign-ment process should help clarify the details of the
model.
Assigning S11: The first model variable in ourorder is S11. In other words, we need to decidewhether the span (1, 1) should be a constituent.
We could let this decision be probabilistically de-
termined, but recall that we are trying to gener-
ate a well-formed tree, thus the leaves and the root
should always be considered constituents. To han-
dle situations when we would like to make deter-
ministic variable assignments, we supply an aux-
illiary function A that tells us (given a model vari-
able X and the history of decisions made so far)
whether X should be automatically determined,
and if so, what value it should be assigned. In our
running example, we ask A whether S11 should beautomatically determined, given the previous as-
signments made (so far only the value chosen for
n, which was 3). The so-called auto-assignment
function A responds (since S11 is a leaf span) that
S11 should be automatically assigned the value
true, making span (1, 1) a constituent.
Assigning L111: Next we want to assign a la-bel to the first leaf of our tree. There is no com-
pelling reason to deterministically assign this la-
bel. Therefore, the auto-assignment function A
declines to assign a value to L111, and we pro-ceed to assign its value probabilistically. For this
task, we would like a probability distribution over
the labels of labeling scheme L1 = {null, A,B},
conditioned on the decision history so far. The dif-
ficulty is that it is clearly impractical to learn con-
ditional distributions over every conceivable his-
tory of variable assignments. So first we distill
the important features from an assignment history.
For instance, one such feature (though possibly
not a good one) could be whether an odd or an
even number of nodes have so far been labeled
with an A. Our conditional probability distribu-
tion is conditioned on the values of these features,
instead of the entire assignment history. Consider
specifically model variable L111. We compute itsfeatures (an even number of nodes ? zero ? have
so far been labeled with an A), and then we use
these feature values to access the relevant prob-
371
ability distribution over {null, A,B}. Drawing
from this conditional distribution, we probabilis-
tically assign the value A to variable L111.
Assigning S22, L122, S33, L133: We proceed inthis way to assign values to S22, L122, S33, L133 (the
S-variables deterministically, and the L1-variables
probabilistically).
Assigning S12: Next comes model variable
S12. Here, there is no reason to deterministicallydictate whether span (1, 2) is a constituent or not.
Both should be considered options. Hence we
treat this situation the same as for the L1 variables.
First we extract the relevant features from the as-
signment history. We then use these features to
access the correct probability distribution over the
domain of S12 (namely {true, false}). Drawingfrom this conditional distribution, we probabilis-
tically assign the value true to S12, making span
(1, 2) a constituent in our tree.
Assigning L112: We proceed to probabilisti-cally assign the value B to L112, in the same man-ner as we did with the other L1 model variables.
Assigning S23: Now we must determinewhether span (2, 3) is a constituent. We could
again probabilistically assign a value to S23 as wedid for S12, but this could result in a hierarchi-cal structure in which both spans (1, 2) and (2, 3)
are constituents, which is not a tree. For trees,
we cannot allow two model variables Sij and Sklto both be assigned true if they properly over-
lap, i.e. their spans overlap and one is not a sub-
span of the other. Fortunately we have already es-
tablished auto-assignment function A, and so we
simply need to ensure that it automatically assigns
the value false to model variable Skl if a prop-erly overlapping model variable Sij has previouslybeen assigned the value true.
Assigning L123, S13, L113: In this manner, wecan complete our variable assignments: L123 is au-tomatically determined (since span (2, 3) is not a
constituent, it should not get a label), as is S13 (toensure a rooted tree), while the label of the root is
probabilistically assigned.
We can summarize this generative process as a
general modeling tool. Define a hierarchical la-
beling process (HLP) as a 5-tuple ?L, <,A,F ,P?
where:
? L = {L1, L2, ..., Lm} is a finite set of label-
ing schemes.
? < is a model order, defined as a total ordering
of the model variables VL such that for all
HLPGEN(HLP H = ?L, <,A,F ,P?):
1. Choose a positive integer n from distribution
PN . Let x be the trivial assignment of VL.
2. In the order defined by <, compute step 3 for
each model variable Y of VnL.
3. If A(Y,x, n) = ?true, y? for some y in the
domain of model variable Y , then let x =
x[Y = y]. Otherwise assign a value to Y
from its domain:
(a) If Y = Sij , then let x = x[Sij = sij ],where sij is a value drawn from distri-bution PS(s|FS(x, i, j, n)).
(b) If Y = Lkij , then let x = x[Lkij = lkij ],
where lkij is a value drawn from distribu-
tion Pk(lk|Fk(x, i, j, n)).
4. Return ?n,x?.
Figure 4: Pseudocode for the generative process.
i, j, k: Sij < Lkij (i.e. we decide whethera span is a constituent before attempting to
label it).
? A is an auto-assignment function. Specifi-
cally A takes three arguments: a model vari-
able Y of VL, a partial assignment x of VL,and integer n. The function A maps this 3-
tuple to false if the variable Y should not be
automatically assigned a value based on the
current history, or the pair ?true, y?, where y
is the value in the domain of Y that should be
automatically assigned to Y .
? F = {FS ,F1,F2, ...,Fm} is a set of fea-
ture functions. Specifically, F k (resp., FS)
takes four arguments: a partial assignment
x of VL, and integers i , j , n such that
1 ? i ? j ? n. It maps this 4-tuple to a
full assignment f k (resp., fS) of some finite
set Fk (resp., FS) of feature variables.
? P = {PN , PS , P1, P2, ..., Pm} is a set ofprobability distributions. PN is a marginalprobability distribution over the set of pos-
itive integers, whereas {PS , P1, P2, ..., Pm}are conditional probability distributions.
Specifically, Pk (respectively, PS) is a func-tion that takes as its argument a full assign-
ment fk (resp., fS) of feature set Fk (resp.,
372
A(variable Y , assignment x, int n):
1. If Y = Sij , and there exists a properlyoverlapping model variable Skl such that
x(Skl) = true, then return ?true, false?.
2. If Y = Sii or Y = S1n, then return
?true, true?.
3. If Y = Lkij , and x(Sij) = false, then return
?true, null?.
4. Else return false.
Figure 5: An example auto-assignment function.
FS). It maps this to a probability distribution
over dom(Lk) (resp., {true, false}).
An HLP probabilistically generates an assign-
ment of its model variables using the generative
process shown in Figure 4. Taking an HLP H =
?L, <,A,F ,P? as input, HLPGEN outputs an in-
teger n, and an H-labeling x of length n, defined
as a full assignment of VnL.Given the auto-assignment function in Figure 5,
every H-labeling generated by HLPGEN can be
viewed as a labeled tree using the interpretation:
span (i, j) is a constituent iff Sij = true; span
(i, j) has label lk ? dom(Lk) iff Lkij = lk.
4 Learning
The generative story from the previous section al-
lows us to express the probability of a labeled tree
as P (n,x), where x is an H-labeling of length n.
For model variable X , define V<L (X) as the sub-set of VL appearing before X in model order <.With the help of this terminology, we can decom-
pose P (n,x) into the following product:
P0(n) ?
?
Sij?Y
PS(x(Sij)|fSij)
?
?
Lkij?Y
Pk(x(Lkij)|fkij)
where fSij = FS(x|V<L (Sij), i, j, n) and
fkij = Fk(x|V<L (Lkij), i, j, n) and Y is the sub-set of VnL that was not automatically assigned byHLPGEN.
Usually in parsing, we are interested in comput-
ing the most likely tree given a specific sentence.
In our framework, this generalizes to computing:
argmaxxP (x|n,w), where w is a subassignmentof an H-labeling x of length n. In natural lan-
guage parsing, w could specify the constituency
and word labels of the leaf-level spans. This would
be equivalent to asking: given a sentence, what is
its most likely parse?
Let W = dom(w) and suppose that we choose
a model order < such that for every pair of model
variables W ? W, X ? VL\W, either W < Xor W is always auto-assigned. Then P (x|n,w)
can be expressed as:
?
Sij?Y\W
PS(x(Sij)|fSij)
?
?
Lkij?Y\W
Pk(x(Lkij)|fkij)
Hence the distributions we need to learn
are probability distributions PS(sij|fS) and
Pk(lkij |fk). This is fairly straightforward. Givena data bank consisting of labeled trees (such as
the Penn Treebank), we simply convert each tree
into its H-labeling and use the probabilistically
determined variable assignments to compile our
training instances. In this way, we compile k + 1
sets of training instances that we can use to induce
PS , and the Pk distributions. The choice of whichlearning technique to use is up to the personal
preference of the user. The only requirement
is that it must return a conditional probability
distribution, and not a hard classification. Tech-
niques that allow this include relative frequency,
maximum entropy models, and decision trees.
For our experiments, we used maximum entropy
learning. Specifics are deferred to Section 6.
5 Decoding
For the PCFG parsing model, we can find
argmaxtreeP (tree|sentence) using a cubic-timedynamic programming-based algorithm. By
adopting a more flexible probabilistic model, we
sacrifice polynomial-time guarantees. The central
question driving this paper is whether we can jetti-
son these guarantees and still obtain good perfor-
mance in practice. For the decoding of the prob-
abilistic model of the previous section, we choose
a depth-first branch-and-bound approach, specif-
ically because of two advantages. First, this ap-
proach takes linear space. Second, it is anytime,
373
HLPDECODE(HLP H, int n, assignment w):
1. Initialize stack S with the pair ?x?, 1?, where
x? is the trivial assignment of VL. Let
xbest = x?; let pbest = 0. Until stack S isempty, repeat steps 2 to 4.
2. Pop topmost pair ?x, p? from stack S.
3. If p > pbest and x is an H-labeling of length
n, then: let xbest = x; let pbest = p.
4. If p > pbest and x is not yet a H-labeling oflength n, then:
(a) Let Y be the earliest variable in VnL (ac-cording to model order <) unassigned
by x.
(b) If Y ? dom(w), then push pair ?x[Y =
w(Y )], p? onto stack S.
(c) Else if A(Y,x, n) = ?true, y? for some
value y ? dom(Y ), then push pair
?x[Y = y], p? onto stack S.
(d) Otherwise for every value y ? dom(Y ),
push pair ?x[Y = y], p ?q(y)? onto stack
S in ascending order of the value of
q(y), where:
q(y) =
{PS(y|FS(x, i, j, n)) if Y = Sij
Pk(y|Fk(x, i, j, n)) if Y = Lkij
5. Return xbest.
Figure 6: Pseudocode for the decoder.
i.e. it finds a (typically good) solution early and
improves this solution as the search progresses.
Thus if one does not wish the spend the time to
run the search to completion (and ensure optimal-
ity), one can use this algorithm easily as a heuristic
by halting prematurely and taking the best solution
found thus far.
The search space is simple to define. Given an
HLP H, the search algorithm simply makes as-
signments to the model variables (depth-first) in
the order defined by <.
This search space can clearly grow to be quite
large, however in practice the search speed is
improved drastically by using branch-and-bound
backtracking. Namely, at any choice point in the
search space, we first choose the least cost child
to expand (i.e. we make the most probable assign-
ment). In this way, we quickly obtain a greedy
solution (in linear time). After that point, we can
continue to keep track of the best solution we have
found so far, and if at any point we reach an inter-
nal node of our search tree with partial cost greater
than the total cost of our best solution, we can dis-
card this node and discontinue exploration of that
subtree. This technique can result in a significant
aggregrate savings of computation time, depend-
ing on the nature of the cost function.
Figure 6 shows the pseudocode for the depth-
first branch-and-bound decoder. For an HLP H =
?L, <,A,F ,P?, a positive integer n, and a partial
assignment w of VnL, the call HLPDECODE(H, n,
w) returns the H-labeling x of length n such that
P (x|n,w) is maximized.
6 Experiments
We employed a familiar experimental set-up. For
training, we used sections 2?21 of the WSJ section
of the Penn treebank. As a development set, we
used the first 20 files of section 22, and then saved
section 23 for testing the final model. One uncon-
ventional preprocessing step was taken. Namely,
for the entire treebank, we compressed all unary
chains into a single node, labeled with the label of
the node furthest from the root. We did so in or-
der to simplify our experiments, since the frame-
work outlined in this paper allows only one label
per labeling scheme per span. Thus by avoiding
unary chains, we avoid the need for many label-
ing schemes or more complicated compound la-
bels (labels like ?NP-NN?). Since our goal here
was not to create a parsing tool but rather to ex-
plore the viability of this approach, this seemed a
fair concession. It should be noted that it is indeed
possible to create a fully general parser using our
framework (for instance, by using the above idea
of compound labels for unary chains).
The main difficulty with this compromise is that
it renders the familiar metrics of labeled preci-
sion and labeled recall incomparable with previ-
ous work (i.e. the LP of a set of candidate parses
with respect to the unmodified test set differs from
the LP with respect to the preprocessed test set).
This would be a major problem, were it not for
the existence of other metrics which measure only
the quality of a parser?s recursive decomposition
of a sentence. Fortunately, such metrics do exist,
thus we used cross-bracketing statistics as the ba-
sic measure of quality for our parser. The cross-
bracketing score of a set of candidate parses with
374
word(i+k) = w word(j+k) = w
preterminal(i+k) = p preterminal(j+k) = p
label(i+k) = l label(j+k) = l
category(i+k) = c category(j+k) = c
signature(i,i+k) = s
Figure 7: Basic feature templates used to deter-
mine constituency and labeling of span (i, j). k is
an arbitrary integer.
respect to the unmodified test set is identical to the
cross-bracketing score with respect to the prepro-
cessed test set, hence our preprocessing causes no
comparability problems as viewed by this metric.
For our parsing model, we used an HLP H =
?L, <,A,F ,P? with the following parameters. L
consisted of three labeling schemes: the set Lwd
of word labels, the set Lpt of preterminal labels,
and the set Lnt of nonterminal labels. The or-
der < of the model variables was the unique or-
der such that for all suitable integers i, j, k, l: (1)
Sij < Lwdij < L
pt
ij < Lntij , (2) Lntij < Skl iffspan (i, j) is strictly shorter than span (k, l) or they
have the same length and integer i is less than inte-
ger k. For auto-assignment function A, we essen-
tially used the function in Figure 5, modified so
that it automatically assigned null to model vari-
ables Lwdij and Lptij for i 6= j (i.e. no preterminal orword tagging of internal nodes), and to model vari-
ables Lntii (i.e. no nonterminal tagging of leaves,rendered unnecessary by our preprocessing step).
Rather than incorporate part-of-speech tagging
into the search process, we opted to pretag the sen-
tences of our development and test sets with an
off-the-shelf tagger, namely the Brill tagger (Brill,
1994). Thus the object of our computation was
HLPDECODE(H, n, w), where n was the length
of the sentence, and partial assignment w speci-
fied the word and PT labels of the leaves. Given
this partial assignment, the job of HLPDECODE
was to find the most probable assignment of model
variables Sij and Lntij for 1 ? i < j ? n.
The two probability models, P S and P nt, were
trained in the manner described in Section 4.
Two decisions needed to be made: which fea-
tures to use and which learning technique to em-
ploy. As for the learning technique, we used
maximum entropy models, specifically the imple-
mentation called MegaM provided by Hal Daume
(Daume? III, 2004). For P S , we needed features
? 40 ? 100
CB 0CB CB 0CB
Magerman (1995) 1.26 56.6
Collins (1996) 1.14 59.9
Klein/Manning (2003) 1.10 60.3 1.31 57.2
this paper 1.09 58.2 1.25 55.2
Charniak (1997) 1.00 62.1
Collins (1999) 0.90 67.1
Figure 8: Cross-bracketing results for Section 23
of the Penn Treebank.
that would be relevant to deciding whether a given
span (i, j) should be considered a constituent. The
basic building blocks we used are depicted in Fig-
ure 7. A few words of explanation are in or-
der. By label(k), we mean the highest nonter-
minal label so far assigned that covers word k, or
if such a label does not yet exist, then the preter-
minal label of k (recall that our model order was
bottom-up). By category(k), we mean the cat-
egory of the preterminal label of word k (given
a coarser, hand-made categorization of pretermi-
nal labels that grouped all noun tags into one
category, all verb tags into another, etc.). By
signature(k,m), where k ? m, we mean the
sequence ?label(k), label(k + 1), ..., label(m)?,
from which all consecutive sequences of identi-
cal labels are compressed into a single label. For
instance, ?IN,NP,NP, V P, V P ? would become
?IN,NP, V P ?. Ad-hoc conjunctions of these ba-
sic binary features were used as features for our
probability model P S . In total, approximately
800,000 such conjunctions were used.
For P nt, we needed features that would be rele-
vant to deciding which nonterminal label to give
to a given constituent span. For this somewhat
simpler task, we used a subset of the basic fea-
tures used for P S , shown in bold in Figure 7. Ad-
hoc conjunctions of these boldface binary features
were used as features for our probability model
P nt. In total, approximately 100,000 such con-
junctions were used.
As mentioned earlier, we used cross-bracketing
statistics as our basis of comparision. These re-
sults as shown in Figure 8. CB denotes the av-
erage cross-bracketing, i.e. the overall percent-
age of candidate constituents that properly overlap
with a constituent in the gold parse. 0CB denotes
the percentage of sentences in the test set that ex-
hibit no cross-bracketing. With a simple feature
set, we manage to obtain performance compara-
ble to the unlexicalized PCFG parser of (Klein and
Manning, 2003) on the set of sentences of length
375
40 or less. On the subset of Section 23 consist-
ing of sentences of length 100 or less, our parser
slightly outperforms their results in terms of av-
erage cross-bracketing. Interestingly, our parser
has a lower percentage of sentences exhibiting no
cross bracketing. To reconcile this result with the
superior overall cross-bracketing score, it would
appear that when our parser does make bracketing
errors, the errors tend to be less severe.
The surprise was how quickly the parser per-
formed. Despite its exponential worst-case time
bounds, the search space turned out to be quite
conducive to depth-first branch-and-bound prun-
ing. Using an unoptimized Java implementation
on a 4x Opteron 848 with 16GB of RAM, the
parser required (on average) less than 0.26 sec-
onds per sentence to optimally parse the subset of
Section 23 comprised of sentences of 40 words or
less. It required an average of 0.48 seconds per
sentence to optimally parse the sentences of 100
words or less (an average of less than 3.5 seconds
per sentence for those sentences of length 41-100).
As noted earlier, the parser requires space linear in
the size of the sentence.
7 Discussion
This project began with a question: can we de-
velop a history-based parsing framework that is
simple, general, and effective? We sought to
provide a versatile probabilistic framework that
would be free from the constraints that dynamic
programming places on PCFG-based approaches.
The work presented in this paper gives favorable
evidence that more flexible (and worst-case in-
tractable) probabilistic approaches can indeed per-
form well in practice, both in terms of running
time and parsing quality.
We can extend this research in multiple direc-
tions. First, the set of features we selected were
chosen with simplicity in mind, to see how well a
simple and unadorned set of features would work,
given our probabilistic model. A next step would
be a more carefully considered feature set. For in-
stance, although lexical information was used, it
was employed in only a most basic sense. There
was no attempt to use head information, which has
been so successful in PCFG parsing methods.
Another parameter to experiment with is the
model order, i.e. the order in which the model vari-
ables are assigned. In this work, we explored only
one specific order (the left-to-right, leaves-to-head
assignment) but in principle there are many other
feasible orders. For instance, one could try a top-
down approach, or a bottom-up approach in which
internal nodes are assigned immediately after all
of their descendants? values have been determined.
Throughout this paper, we strove to present the
model in a very general manner. There is no rea-
son why this framework cannot be tried in other
application areas that rely on dynamic program-
ming techniques to perform hierarchical labeling,
such as phrase-based machine translation. Apply-
ing this framework to such application areas, as
well as developing a general-purpose parser based
on HLPs, are the subject of our continuing work.
References
Ezra Black, Fred Jelinek, John Lafferty, David M.Magerman, Robert Mercer, and Salim Roukos.
1993. Towards history-based grammars: usingricher models for probabilistic parsing. In Proc.
ACL.
Eric Brill. 1994. Some advances in rule-based part ofspeech tagging. In Proc. AAAI.
Eugene Charniak. 1997. Statistical parsing with acontext-free grammar and word statistics. In Proc.
AAAI.
Eugene Charniak. 2000. A maximum entropy-inspiredparser. In Proc. NAACL.
Eugene Charniak. 2001. Immediate-head parsing forlanguage models. In Proc. ACL.
Michael Collins. 1996. A new statistical parser based
on bigram lexical dependencies. In Proc. ACL.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-sity of Pennsylvania.
Hal Daume? III. 2004. Notes on CG and LM-BFGS op-
timization of logistic regression. Paper available athttp://www.isi.edu/ hdaume/docs/daume04cg-bfgs.ps, implementation available at
http://www.isi.edu/ hdaume/megam/, August.
Mark Johnson. 1998. Pcfg models of linguistic
tree representations. Computational Linguistics,24:613?632.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proc. ACL.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proc. ACL.
Adwait Ratnaparkhi. 1997. A linear observed time sta-tistical parser based on maximum entropy models.
In Proc. EMNLP.
376
A Framework for Incorporating Alignment Information in Parsing
Mark Hopkins
Dept. of Computational Linguistics
Saarland University
Saarbru?cken, Germany
mhopkins@coli.uni-sb.de
Jonas Kuhn
Dept. of Computational Linguistics
Saarland University
Saarbru?cken, Germany
jonask@coli.uni-sb.de
Abstract
The standard PCFG approach to parsing is
quite successful on certain domains, but is
relatively inflexible in the type of feature
information we can include in its prob-
abilistic model. In this work, we dis-
cuss preliminary work in developing a new
probabilistic parsing model that allows us
to easily incorporate many different types
of features, including crosslingual infor-
mation. We show how this model can
be used to build a successful parser for a
small handmade gold-standard corpus of
188 sentences (in 3 languages) from the
Europarl corpus.
1 Introduction
Much of the current research into probabilis-
tic parsing is founded on probabilistic context-
free grammars (PCFGs) (Collins, 1999; Charniak,
2000; Charniak, 2001). For instance, consider
the parse tree in Figure 1. One way to decom-
pose this parse tree is to view it as a sequence
of applications of CFG rules. For this particular
tree, we could view it as the application of rule
?NP? NP PP,? followed by rule ?NP? DT NN,?
followed by rule ?DT? that,? and so forth. Hence
instead of analyzing P (tree), we deal with the
more modular:
P(NP ? NP PP, NP ? DT NN,
DT? that, NN?money, PP? IN NP,
IN ? in, NP ? DT NN, DT ? the,
NN? market)
Obviously this joint distribution is just as diffi-
cult to assess and compute with as P (tree). How-
ever there exist cubic time algorithms to find the
most likely parse if we assume that all CFG rule
applications are marginally independent of one an-
other. In other words, we need to assume that the
above expression is equivalent to the following:
P(NP ? NP PP) ? P(NP ? DT NN) ?
P(DT ? that) ? P(NN ? money) ?
P(PP ? IN NP) ? P(IN ? in) ?
P(NP ? DT NN) ? P(DT ? the) ?
P(NN? market)
It is straightforward to assess the probability of
the factors of this expression from a corpus us-
ing relative frequency. Then using these learned
probabilities, we can find the most likely parse of
a given sentence using the aforementioned cubic
algorithms.
The problem, of course, with this simplifica-
tion is that although it is computationally attrac-
tive, it is usually too strong of an independence
assumption. To mitigate this loss of context, with-
out sacrificing algorithmic tractability, typically
researchers annotate the nodes of the parse tree
with contextual information. For instance, it has
been found to be useful to annotate nodes with
their parent labels (Johnson, 1998), as shown in
Figure 2. In this case, we would be learning prob-
abilities like: P(PP-NP? IN-PP NP-PP).
The choice of which annotations to use is
one of the main features that distinguish parsers
based on this approach. Generally, this approach
has proven quite effective in producing English
phrase-structure grammar parsers that perform
well on the Penn Treebank.
One drawback of this approach is that it is
somewhat inflexible. Because we are adding prob-
abilistic context by changing the data itself, we
make our data increasingly sparse as we add fea-
tures. Thus we are constrained from adding too
9
NP
NP
DT
that
NN
money
PP
IN
in
NP
DT
the
NN
market
Figure 1: Example parse tree.
NP-TOP
NP-NP
DT-NP
that
NN-NP
money
PP-NP
IN-PP
in
NP-PP
DT-NP
the
NN-NP
market
Figure 2: Example parse tree with parent annota-
tions.
many features, because at some point we will not
have enough data to sustain them. Hence in this
approach, feature selection is not merely a matter
of including good features. Rather, we must strike
a delicate balance between how much context we
want to include versus how much we dare to par-
tition our data set.
This poses a problem when we have spent time
and energy to find a good set of features that work
well for a given parsing task on a given domain.
For a different parsing task or domain, our parser
may work poorly out-of-the-box, and it is no triv-
ial matter to evaluate how we might adapt our fea-
ture set for this new task. Furthermore, if we gain
access to a new source of feature information, then
it is unclear how to incorporate such information
into such a parser.
Namely, in this paper, we are interested in see-
ing how the cross-lingual information contained
by sentence alignments can help the performance
of a parser. We have a small gold-standard cor-
pus of shallow-parsed parallel sentences (in En-
glish, French, and German) from the Europarl cor-
pus. Because of the difficulty of testing new fea-
tures using PCFG-based parsers, we propose a
new probabilistic parsing framework that allows
us to flexibly add features. The closest relative
1 2 3 4 5
1 true true false false true
2 - true false false false
3 - - true false true
4 - - - true true
5 - - - - true
Figure 3: Span chart for example parse tree. Chart
entry (i, j) = true iff span (i, j) is a constituent
in the tree.
of our framework is the maximum-entropy parser
of Ratnaparkhi(Ratnaparkhi, 1997). Both frame-
works are bottom-up, but while Ratnaparkhi?s
views parse trees as the sequence of applications
of four different types of tree construction rules,
our framework strives to be somewhat simpler and
more general.
2 The Probability Model
The example parse tree in Figure 1 can also be de-
composed in the following manner. First, we can
represent the unlabeled tree with a boolean-valued
chart (which we will call the span chart) that as-
signs the value of true to a span if it is a con-
stituent in the tree, and false otherwise. The span
chart for Figure 1 is shown in Figure 3.
To represent the labels, we simply add similar
charts for each labeling scheme present in the tree.
For a parse tree, there are typically three types
of labels: words, preterminal tags, and nontermi-
nals. Thus we need three labeling charts. Labeling
charts for our example parse tree are depicted in
Figure 4. Note that for words and preterminals, it
is not really necessary to have a two-dimensional
chart, but we do so here to motivate the general
model.
The general model is as follows. Define a la-
beling scheme as a set of symbols including a
special symbol null (this will designate that a
given span is unlabeled). For instance, we might
define LNT = {null, NP, PP, IN, DT} to be
a labeling scheme for non-terminals. Let L =
{L1, L2, ...Lm} be a set of labeling schemes. De-
fine a model variable of L as a symbol of the form
Sij or Lkij , for positive integers i, j, k, such that
j ? i and k ? m. The domain of model vari-
able Sij is {true, false} (these variables indicate
whether a given span is a tree constituent). The do-
main of model variable Lkij is Lk (these variables
indicate which label from Lk is assigned to span
10
1 2 3 4 5
1 that null null null null
2 - money null null null
3 - - in null null
4 - - - the null
5 - - - - market
1 2 3 4 5
1 DT null null null null
2 - NN null null null
3 - - IN null null
4 - - - DT null
5 - - - - NN
1 2 3 4 5
1 null NP null null NP
2 - null null null null
3 - - null null PP
4 - - - null NP
5 - - - - null
Figure 4: Labeling charts for example parse tree:
the top chart is for word labels, the middle chart is
for preterminal tag labels, and the bottom chart is
for nonterminal labels. null denotes an unlabeled
span.
i, j). Define a model order of L as a total order-
ing ? of the model variables of L such that for all
i, j, k: ?(Sij) < ?(Lkij) (i.e. we decide whether a
span is a constituent before attempting to label it).
Let ?n denote the finite subset of ? that includes
precisely the model variables of the form Sij or
Lkij , where j ? n.
Given a set L of labeling schemes and a model
order ? of L, a preliminary generative story might
look like the following:
1. Choose a positive integer n.
2. In the order defined by ?n, assign a value
to every model variable of ?n from its do-
main, conditioned on any previous assign-
ments made.
Thus some model order ? for our example
might instruct us to first choose whether span (4,
5) is a constituent, for which we might say ?true,?
then instruct us to choose a label for that con-
stituent, for which we might say ?NP,? and so
forth.
There are a couple of problems with this genera-
tive story. One problem is that it allows us to make
structural decisions that do not result in a well-
formed tree. For instance, we should not be per-
mitted to assign the value true to both variable S
13
and S
24
. Generally, we cannot allow two model
variables Sij and Skl to both be assigned true if
they properly overlap, i.e. their spans overlap and
one is not a subspan of the other. We should also
ensure that the leaves and the root are considered
constituents. Another problem is that it allows us
to make labeling decisions that do not correspond
with our chosen structure. It should not be possi-
ble to label a span which is not a constituent.
With this in mind, we revise our generative
story.
1. Choose a positive integer n from distribution
P
0
.
2. In the order defined by ?n, process model
variable x of ?n:
(a) If x = Sij , then:
i. Automatically assign the value
false if there exists a properly
overlapping model variable Skl such
that Skl has already been assigned
the value true.
ii. Automatically assign the value true
if i = j or if i = 1 and j = n.
iii. Otherwise assign a value sij to Sij
from its domain, drawn from some
probability distribution PS condi-
tioned on all previous variable as-
signments.
(b) If x = Lkij , then:
i. Automatically assign the value null
to Lkij if Sij was assigned the value
false (note that this is well-defined
because of way we defined model
order).
ii. Otherwise assign a value lkij to L
k
ij
from its domain, drawn from some
probability distribution Pk condi-
tioned on all previous variable as-
signments.
Defining ?<n (x) = {y ? ?n|?(y) < ?(x)}
for x ? ?n, we can decompose P (tree) into the
following expression:
11
P
0
(n) ?
?
S
ij
??
n
PS(sij |n,?
<
n (Sij))
?
?
Lk
ij
??
n
Pk(l
k
ij |n,?
<
n (L
k
ij))
where PS and Pk obey the constraints given in
the generative story above (e.g. PS(Sii = true) =
1, etc.)
Obviously it is impractical to learn conditional
distributions over every conceivable history, so in-
stead we choose a small set F of feature variables,
and provide a set of functions Fn that map every
partial history of ?n to some feature vector f ? F
(later we will see examples of such feature func-
tions). Then we make the assumption that:
PS(sij |n,?
<
n (Sij) = PS(sij|f)
where f = Fn(?<n (Sij)) and that
Pk(l
k
ij |n,?
<
n (Sij) = Pk(l
k
ij |f)
where f = Fn(?<n (L
k
ij)).
In this way, our learning task is simplified to
learn functions P
0
(n), PS(sij |f), and Pk(lkij |f).
Given a corpus of labeled trees, it is straightfor-
ward to extract the training instances for these dis-
tributions and then use these instances to learn dis-
tributions using one?s preferred learning method
(e.g., maximum entropy models or decision trees).
For this paper, we are interested in parse trees
which have three labeling schemes. Let L =
{Lword, LPT , LNT }, where Lword is a labeling
scheme for words, LPT is a labeling scheme for
preterminals, and LNT is a labeling scheme for
nonterminals. We will define model order ? such
that:
1. ?(Sij) < ?(Lwordij ) < ?(L
PT
ij ) < ?(L
NT
ij ).
2. ?(LNTij ) < ?(Skl) iff j?i < l?k or (j?i =
l ? k and i < k).
In this work, we are not as much interested in
learning a marginal distribution over parse trees,
but rather a conditional distribution for parse trees,
given a tagged sentence (from which n is also
known). We will assume that Pword is condition-
ally independent of all the other model variables,
given n and the Lwordij variables. We will also as-
sume that Ppt is conditionally independent of the
other model variables, given n, the Lwordij vari-
ables, and the Lptij variables. These assumptions
allow us to express P (tree|n, Lwordij , L
pt
ij ) as the
following:
?
S
ij
??
n
PS(sij |fS) ?
?
Lnt
ij
??
n
Pnt(l
nt
ij |fnt)
where fS = Fn(?<n (Sij)) and fnt =
Fn(?
<
n (L
nt
ij )). Hence our learning task in this pa-
per will be to learn the probability distributions
PS(sij|fS) and Pnt(lntij |fnt), for some choice of
feature functions Fn.
3 Decoding
For the PCFG parsing model, we can find
argmaxtreeP (tree|sentence) using a cubic-time
dynamic programming-based algorithm. By
adopting a more flexible probabilistic model, we
sacrifice polynomial-time guarantees. Neverthe-
less, we can still devise search algorithms that
work efficiently in practice. For the decoding of
the probabilistic model of the previous section, we
choose a depth-first branch-and-bound approach,
specifically because of two advantages. First, this
approach is linear space. Second, it is anytime, i.e.
it finds a (typically good) solution early and im-
proves this solution as the search progresses. Thus
if one does not wish the spend the time to run the
search to completion (and ensure optimality), one
can use this algorithm easily as a heuristic.
The search space is simple to define. Given a
set L of labeling schemes and a model order ? of
L, the search algorithm simply makes assignments
to the model variables (depth-first) in the order de-
fined by ?.
This search space can clearly grow to be quite
large, however in practice the search speed is
improved drastically by using branch-and-bound
backtracking. Namely, at any choice point in the
search space, we first choose the least cost child to
expand. In this way, we quickly obtain a greedy
solution. After that point, we can continue to keep
track of the best solution we have found so far,
and if at any point we reach an internal node of
our search tree with partial cost greater than the
total cost of our best solution, we can discard this
node and discontinue exploration of that subtree.
This technique can result in a significant aggre-
grate savings of computation time, depending on
12
EN: [
1
[
2
On behalf of the European People ?s Party , ] [
3
I] call [
5
for a vote [
6
in favour of that motion ] ] ]
FR: [
1
[
2
Au nom du Parti populaire europe?en ,] [
3
je] demande [
5
l? adoption [
6
de cette re?solution] ] ]
DE: [
1
[
2
Im Namen der Europa?ischen Volkspartei ] rufe [
3
ich] [
4
Sie] auf , [
5
[
6
diesem Entschlie?ungsantrag] zuzustimmen
] ]
ES: [
1
[
2
En nombre del Grupo del Partido Popular Europeo ,] solicito [
5
la aprobacio?n [
6
de la resolucio?n] ] ]
Figure 5: Annotated sentence tuple
the nature of the cost function. For our limited
parsing domain, it appears to perform quite well,
taking fractions of a second to parse each sentence
(which are short, with a maximum of 20 words per
sentence).
4 Experiments
Our parsing domain is based on a ?lean? phrase
correspondence representation for multitexts from
parallel corpora (i.e., tuples of sentences that are
translations of each other). We defined an anno-
tation scheme that focuses on translational corre-
spondence of phrasal units that have a distinct,
language-independent semantic status. It is a hy-
pothesis of our longer-term project that such a se-
mantically motivated, relatively coarse phrase cor-
respondence relation is most suitable for weakly
supervised approaches to parsing of large amounts
of parallel corpus data. Based on this lean phrase
structure format, we intend to explore an alter-
native to the annotation projection approach to
cross-linguistic bootstrapping of parsers by (Hwa
et al, 2005). They depart from a standard treebank
parser for English, ?projecting? its analyses to an-
other language using word alignments over a par-
allel corpus. Our planned bootstrapping approach
will not start out with a given parser for English (or
any other language), but use a small set of manu-
ally annotated seed data following the lean phrase
correspondence scheme, and then bootstrap con-
sensus representations on large amounts of unan-
notated multitext data. At the present stage, we
only present experiments for training an initial
system on a set of seed data.
The annotation scheme underlying in the gold
standard annotation consists of (A) a bracketing
for each language and (B) a correspondence rela-
tion of the constituents across languages. Neither
the constituents nor the embedding or correspon-
dent relations were labelled.
The guiding principle for bracketing (A) is very
simple: all and only the units that clearly play
the role of a semantic argument or modifier in a
larger unit are bracketed. This means that function
words, light verbs, ?bleeched? PPs like in spite
of etc. are included with the content-bearing el-
ements. This leads to a relatively flat bracketing
structure. Referring or quantified expressions that
may include adjectives and possessive NPs or PPs
are also bracketed as single constituents (e.g., [ the
president of France ]), unless the semantic rela-
tions reflected by the internal embedding are part
of the predication of the sentence. A few more
specific annotation rules were specified for cases
like coordination and discontinuous constituents.
The correspondence relation (B) is guided by
semantic correspondence of the bracketed units;
the mapping need not preserve the tree structure.
Neither does a constituent need to have a corre-
spondent in all (or any) of the other languages
(since the content of this constituent may be im-
plicit in other languages, or subsumed by the con-
tent of another constituent). ?Semantic correspon-
dence? is not restricted to truth-conditional equiv-
alence, but is generalized to situations where two
units just serve the same rhetorical function in the
original text and the translation.
Figure 5 is an annotation example. Note that
index 4 (the audience addressed by the speaker)
is realized overtly only in German (Sie ?you?); in
Spanish, index 3 is realized only in the verbal in-
flection (which is not annotated). A more detailed
discussion of the annotation scheme is presented
in (Kuhn and Jellinghaus, to appear).
For the current parsing experiments, only the
bracketing within each of three languages (En-
glish, French, German) is used; the cross-
linguistic phrase correspondences are ignored (al-
though we intend to include them in future ex-
periments). We automatically tagged the train-
ing and test data in English, French, and German
with Schmid?s decision-tree part-of-speech tagger
(Schmid, 1994).
The training data were taken from the sentence-
aligned Europarl corpus and consisted of 188 sen-
tences for each of the three languages, with max-
13
Feature Notation Description
p(language) the preterminal tag of word x ? 1 (null if does not exist)
f(language) the preterminal tag of word x
l(language) the preterminal tag of word y
n(language) the preterminal tag of word y ? 1 (null if does not exist)
lng the length of the span (i.e. y ? x + 1)
Figure 6: Features for span (x, y). E = English, F = French, G = German
English Crosslingual Rec. Prec. F- No
features features score cross
p(E), f(E), l(E) none 40.3 63.6 49.4 (?3.9%) 57.1
p(F), f(F), l(F) 43.1 67.6 52.6 (?4.0%) 61.2
p(G), f(G), l(G) 45.9 66.8 54.4 (?4.0%) 69.4
p(F), f(F), l(F), 44.5 65.5 53.0 (?3.9%) 65.3
p(G), f(G), l(G)
p(E), f(E), l(E), n(E) none 57.2 68.6 62.4 (?4.0%) 65.3
p(F), f(F), l(F), n(F) 56.6 71.9 63.3 (?4.0%) 75.5
p(G), f(G), l(G), n(G) 57.9 67.7 62.5 (?3.9%) 67.3
p(F), f(F), l(F), n(F), 57.9 72.1 64.2 (?4.0%) 77.6
p(G), f(G), l(G), n(G)
p(E), f(E), l(E), n(E), lng none 64.8 71.2 67.9 (?4.0%) 79.6
p(F), f(F), l(F), n(F), lng 62.1 74.4 67.7 (?4.0%) 83.7
p(G), f(G), l(G), n(G), lng 61.4 78.8 69.0 (?4.1%) 83.7
p(F), f(F), l(F), n(F), 63.1 76.9 69.3 (?4.1%) 81.6
p(G), f(G), l(G), n(G), lng
BIKEL 57.9 60.2 59.1 (?3.8%) 57.1
Figure 7: Parsing results for various feature sets, and the Bikel baseline. The F-scores are annotated with
95% confidence intervals.
imal length of 21 words in English (French: 38;
German: 24) and an average length of 14.0 words
in English (French 16.8; German 13.6). The test
data were 50 sentences for each language, picked
arbitrarily with the same length restrictions. The
training and test data were manually aligned fol-
lowing the guidelines.1
For the word alignments used as learning fea-
tures, we used GIZA++, relying on the default pa-
rameters. We trained the alignments on the full
Europarl corpus for both directions of each lan-
guage pair.
As a baseline system we trained Bikel?s reim-
plementation (Bikel, 2004) of Collins? parser
(Collins, 1999) on the gold standard (En-
1A subset of 39 sentences was annotated by two people
independently, leading to an F-Score in bracketing agreement
between 84 and 90 for the three languages. Since finding an
annotation scheme that works well in the bootstrapping set-
up is an issue on our research agenda, we postpone a more
detailed analysis of the annotation process until it becomes
clear that a particular scheme is indeed useful.
glish) training data, applying a simple additional
smoothing procedure for the modifier events in or-
der to counteract some obvious data sparseness is-
sues.2
Since we were attempting to learn unlabeled
trees, in this experiment we only needed to learn
the probabilistic model of Section 3 with no la-
beling schemes. Hence we need only to learn the
probability distribution:
PS(sij|fS)
In other words, we need to learn the probabil-
ity that a given span is a tree constituent, given
some set of features of the words and preterminal
tags of the sentences, as well as the previous span
decisions we have made. The main decision that
2For the nonterminal labels, we defined the left-most lex-
ical daughter in each local subtree of depth 1 to project its
part-of-speech category to the phrase level and introduced
a special nonterminal label for the rare case of nonterminal
nodes dominating no preterminal node.
14
remains, then, is which feature set to use. The fea-
tures we employ are very simple. Namely, for span
(i, j) we consider the preterminal tags of words
i ? 1, i, j, and j + 1, as well as the French and
German preterminal tags of the words to which
these English words align. Finally, we also use
the length of the span as a feature. The features
considered are summarized in Figure 6.
To learn the conditional probability distributu-
tions, we choose to use maximum entropy mod-
els because of their popularity and the availabil-
ity of software packages. Specifically, we use
the MEGAM package (Daume? III, 2004) from
USC/ISI.
We did experiments for a number of different
feature sets, with and without alignment features.
The results (precision, recall, F-score, and the per-
centage of sentences with no cross-bracketing) are
summarized in Figure 7. Note that with a very
simple set of features (the previous, first, last, and
next preterminal tags of the sequence), our parser
performs on par with the Bikel baseline. Adding
the length of the sequence as a feature increases
the quality of the parser to a statistically signif-
icant difference over the baseline. The crosslin-
gual information provided (which is admittedly
naive) does not provide a statistically significant
improvement over the vanilla set of features. The
conclusion to be drawn is not that crosslingual in-
formation does not help (such a conclusion should
not be drawn from the meager set of crosslingual
features we have used here for demonstration pur-
poses). Rather, the take-away point is that such
information can be easily incorporated using this
framework.
5 Discussion
One of the primary concerns about this framework
is speed, since the decoding algorithm for our
probabilistic model is not polynomial-time like the
decoding algorithms for PCFG parsing. Neverthe-
less, in our experiments with shallow parsed 20-
word sentences, time was not a factor. Further-
more, in our ongoing research applying this prob-
abilistic framework to the task of Penn Treebank-
style parsing, this approach appears to also be vi-
able for the 40-word sentences of Sections 22 and
23 of theWSJ treebank. A strong mitigating factor
of the theoretical intractibility is the fact that we
have an anytime decoding algorithm, hence even
in cases when we cannot run the algorithm to com-
pletion (for a guaranteed optimal solution), the al-
gorithm always returns some solution, the quality
of which increases over time. Hence we can tell
the algorithm how much time it has to compute,
and it will return the best solution it can compute
in that time frame.
This work suggests that one can get a good qual-
ity parser for a new parsing domain with relatively
little effort (the features we chose are extremely
simple and certainly could be improved on). The
cross-lingual information that we used (namely,
the foreign preterminal tags of the words to which
our span was aligned by GIZA) did not give a sig-
nificant improvement to our parser. However the
goal of this work was not to make definitive state-
ments about the value of crosslingual features in
parsing, but rather to show a framework in which
such crosslingual information could be easily in-
corporated and exploited. We believe we have pro-
vided the beginnings of one in this work, and work
continues on finding more complex features that
will improve performance well beyond the base-
line.
Acknowledgement
The work reported in this paper was supported by
theDeutsche Forschungsgemeinschaft (DFG; Ger-
man Research Foundation) in the Emmy Noether
project PTOLEMAIOS on grammar learning from
parallel corpora.
References
Daniel M. Bikel. 2004. Intricacies of collins? parsing
model. Computational Linguistics, 30(4):479?511.
Eugene Charniak. 2000. A maximum entropy-inspired
parser. In NAACL.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In ACL.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Hal Daume? III. 2004. Notes on CG and LM-BFGS op-
timization of logistic regression. Paper available at
http://www.isi.edu/ hdaume/docs/daume04cg-
bfgs.ps, implementation available at
http://www.isi.edu/ hdaume/megam/, August.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(3):311?325.
15
Mark Johnson. 1998. PCFG models of linguistic tree
representation. Computational Linguistics, 24:613?
632.
Jonas Kuhn and Michael Jellinghaus. to appear. Mul-
tilingual parallel treebanking: a lean and flexible ap-
proach. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation,
Genoa, Italy.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models.
In EMNLP.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing.
16
 
		ffLexicalized Stochastic Modeling of Constraint-Based Grammars
using Log-Linear Measures and EM Training
Stefan Riezler
IMS, Universit?t Stuttgart
riezler@ims.uni-stuttgart.de
Detlef Prescher
IMS, Universit?t Stuttgart
prescher@ims.uni-stuttgart.de
Jonas Kuhn
IMS, Universit?t Stuttgart
jonas@ims.uni-stuttgart.de
Mark Johnson
Cog. & Ling. Sciences, Brown University
Mark_Johnson@brown.edu
Abstract
We present a new approach to
stochastic modeling of constraint-
based grammars that is based on log-
linear models and uses EM for esti-
mation from unannotated data. The
techniques are applied to an LFG
grammar for German. Evaluation on
an exact match task yields 86% pre-
cision for an ambiguity rate of 5.4,
and 90% precision on a subcat frame
match for an ambiguity rate of 25.
Experimental comparison to train-
ing from a parsebank shows a 10%
gain from EM training. Also, a new
class-based grammar lexicalization is
presented, showing a 10% gain over
unlexicalized models.
1 Introduction
Stochastic parsing models capturing contex-
tual constraints beyond the dependencies of
probabilistic context-free grammars (PCFGs)
are currently the subject of intensive research.
An interesting feature common to most such
models is the incorporation of contextual de-
pendencies on individual head words into rule-
based probability models. Such word-based
lexicalizations of probability models are used
successfully in the statistical parsing mod-
els of, e.g., Collins (1997), Charniak (1997),
or Ratnaparkhi (1997). However, it is still
an open question which kind of lexicaliza-
tion, e.g., statistics on individual words or
statistics based upon word classes, is the best
choice. Secondly, these approaches have in
common the fact that the probability models
are trained on treebanks, i.e., corpora of man-
ually disambiguated sentences, and not from
corpora of unannotated sentences. In all of the
cited approaches, the Penn Wall Street Jour-
nal Treebank (Marcus et al, 1993) is used,
the availability of which obviates the standard
eort required for treebank traininghand-
annotating large corpora of specic domains
of specic languages with specic parse types.
Moreover, common wisdom is that training
from unannotated data via the expectation-
maximization (EM) algorithm (Dempster et
al., 1977) yields poor results unless at
least partial annotation is applied. Experi-
mental results conrming this wisdom have
been presented, e.g., by Elworthy (1994) and
Pereira and Schabes (1992) for EM training
of Hidden Markov Models and PCFGs.
In this paper, we present a new lexicalized
stochastic model for constraint-based gram-
mars that employs a combination of head-
word frequencies and EM-based clustering
for grammar lexicalization. Furthermore, we
make crucial use of EM for estimating the
parameters of the stochastic grammar from
unannotated data. Our usage of EM was ini-
tiated by the current lack of large unication-
based treebanks for German. However, our ex-
perimental results also show an exception to
the common wisdom of the insuciency of EM
for highly accurate statistical modeling.
Our approach to lexicalized stochastic mod-
eling is based on the parametric family of log-
linear probability models, which is used to de-
ne a probability distribution on the parses
of a Lexical-Functional Grammar (LFG) for
German. In previous work on log-linear mod-
els for LFG by Johnson et al (1999), pseudo-
likelihood estimation from annotated corpora
has been introduced and experimented with
on a small scale. However, to our knowledge,
to date no large LFG annotated corpora of
unrestricted German text are available. For-
tunately, algorithms exist for statistical infer-
ence of log-linear models from unannotated
data (Riezler, 1999). We apply this algorithm
to estimate log-linear LFG models from large
corpora of newspaper text. In our largest ex-
periment, we used 250,000 parses which were
produced by parsing 36,000 newspaper sen-
tences with the German LFG. Experimental
evaluation of our models on an exact-match
task (i.e. percentage of exact match of most
probable parse with correct parse) on 550
manually examined examples with on average
5.4 analyses gave 86% precision. Another eval-
uation on a verb frame recognition task (i.e.
percentage of agreement between subcatego-
rization frames of main verb of most proba-
ble parse and correct parse) gave 90% pre-
cision on 375 manually disambiguated exam-
ples with an average ambiguity of 25. Clearly,
a direct comparison of these results to state-
of-the-art statistical parsers cannot be made
because of dierent training and test data and
other evaluation measures. However, we would
like to draw the following conclusions from our
experiments:
 The problem of chaotic convergence be-
haviour of EM estimation can be solved
for log-linear models.
 EM does help constraint-based gram-
mars, e.g. using about 10 times more sen-
tences and about 100 times more parses
for EM training than for training from an
automatically constructed parsebank can
improve precision by about 10%.
 Class-based lexicalization can yield a gain
in precision of about 10%.
In the rest of this paper we intro-
duce incomplete-data estimation for log-linear
models (Sec. 2), and present the actual design
of our models (Sec. 3) and report our experi-
mental results (Sec. 4).
2 Incomplete-Data Estimation for
Log-Linear Models
2.1 Log-Linear Models
A log-linear distribution p

(x) on the set of
analyses X of a constraint-based grammar can
be dened as follows:
p

(x) = Z

 1
e
(x)
p
0
(x)
where Z

=
P
x2X
e
(x)
p
0
(x) is a normal-
izing constant,  = (
1
; : : : ; 
n
) 2 IR
n
is a
vector of log-parameters,  = (
1
; : : : ; 
n
) is
a vector of property-functions 
i
: X ! IR for
i = 1; : : : ; n,   (x) is the vector dot prod-
uct
P
n
i=1

i

i
(x), and p
0
is a xed reference
distribution.
The task of probabilistic modeling with log-
linear distributions is to build salient proper-
ties of the data as property-functions 
i
into
the probability model. For a given vector  of
property-functions, the task of statistical in-
ference is to tune the parameters  to best
reect the empirical distribution of the train-
ing data.
2.2 Incomplete-Data Estimation
Standard numerical methods for statis-
tical inference of log-linear models from
fully annotated dataso-called complete
dataare the iterative scaling meth-
ods of Darroch and Ratcli (1972) and
Della Pietra et al (1997). For data consisting
of unannotated sentencesso-called incom-
plete datathe iterative method of the EM
algorithm (Dempster et al, 1977) has to be
employed. However, since even complete-data
estimation for log-linear models requires
iterative methods, an application of EM to
log-linear models results in an algorithm
which is expensive since it is doubly-iterative.
A singly-iterative algorithm interleaving EM
and iterative scaling into a mathematically
well-dened estimation method for log-linear
models from incomplete data is the IM
algorithm of Riezler (1999). Applying this
algorithm to stochastic constraint-based
grammars, we assume the following to be
given: A training sample of unannotated sen-
tences y from a set Y, observed with empirical
Input Reference model p
0
, property-functions vector  with constant 
#
, parses
X(y) for each y in incomplete-data sample from Y.
Output MLE model p


on X .
Procedure
Until convergence do
Compute p

; k

, based on  = (
1
; : : : ; 
n
),
For i from 1 to n do

i
:=
1

#
ln
P
y2Y
~p(y)
P
x2X(y)
k

(xjy)
i
(x)
P
x2X
p

(x)
i
(x)
,

i
:= 
i
+ 
i
,
Return 

= (
1
; : : : ; 
n
).
Figure 1: Closed-form version of IM algorithm
probability ~p(y), a constraint-based grammar
yielding a set X(y) of parses for each sentence
y, and a log-linear model p

() on the parses
X =
P
y2Yj~p(y)>0
X(y) for the sentences in
the training corpus, with known values of
property-functions  and unknown values
of . The aim of incomplete-data maximum
likelihood estimation (MLE) is to nd a value


that maximizes the incomplete-data log-
likelihood L =
P
y2Y
~p(y) ln
P
x2X(y)
p

(x),
i.e.,


= argmax
2IR
n
L():
Closed-form parameter-updates for this prob-
lem can be computed by the algorithm of Fig.
1, where 
#
(x) =
P
n
i=1

i
(x), and k

(xjy) =
p

(x)=
P
x2X(y)
p

(x) is the conditional prob-
ability of a parse x given the sentence y and
the current parameter value .
The constancy requirement on 
#
can be
enforced by adding a correction property-
function 
l
:
Choose K = max
x2X

#
(x) and

l
(x) = K   
#
(x) for all x 2 X .
Then
P
l
i=1

i
(x) = K for all x 2 X .
Note that because of the restriction of X to
the parses obtainable by a grammar from the
training corpus, we have a log-linear probabil-
ity measure only on those parses and not on
all possible parses of the grammar. We shall
therefore speak of mere log-linear measures in
our application of disambiguation.
2.3 Searching for Order in Chaos
For incomplete-data estimation, a sequence
of likelihood values is guaranteed to converge
to a critical point of the likelihood function
L. This is shown for the IM algorithm in
Riezler (1999). The process of nding likeli-
hood maxima is chaotic in that the nal likeli-
hood value is extremely sensitive to the start-
ing values of , i.e. limit points can be lo-
cal maxima (or saddlepoints), which are not
necessarily also global maxima. A way to
search for order in this chaos is to search for
starting values which are hopefully attracted
by the global maximum of L. This problem
can best be explained in terms of the mini-
mum divergence paradigm (Kullback, 1959),
which is equivalent to the maximum likeli-
hood paradigm by the following theorem. Let
p[f ] =
P
x2X
p(x)f(x) be the expectation of
a function f with respect to a distribution p:
The probability distribution p

that
minimizes the divergence D(pjjp
0
) to
a reference model p
0
subject to the
constraints p[
i
] = q[
i
]; i = 1; : : : ; n
is the model in the parametric fam-
ily of log-linear distributions p

that
maximizes the likelihood L() =
q[ln p

] of the training data
1
.
1
If the training sample consists of complete data
Reasonable starting values for minimum di-
vergence estimation is to set 
i
= 0 for
i = 1; : : : ; n. This yields a distribution which
minimizes the divergence to p
0
, over the
set of models p to which the constraints
p[
i
] = q[
i
]; i = 1; : : : ; n have yet to be ap-
plied. Clearly, this argument applies to both
complete-data and incomplete-data estima-
tion. Note that for a uniformly distributed
reference model p
0
, the minimum divergence
model is a maximum entropy model (Jaynes,
1957). In Sec. 4, we will demonstrate that
a uniform initialization of the IM algorithm
shows a signicant improvement in likelihood
maximization as well as in linguistic perfor-
mance when compared to standard random
initialization.
3 Property Design and
Lexicalization
3.1 Basic Congurational Properties
The basic 190 properties employed in our
models are similar to the properties of
Johnson et al (1999) which incorporate gen-
eral linguistic principles into a log-linear
model. They refer to both the c(onstituent)-
structure and the f(eature)-structure of the
LFG parses. Examples are properties for
 c-structure nodes, corresponding to stan-
dard production properties,
 c-structure subtrees, indicating argument
versus adjunct attachment,
 f-structure attributes, corresponding to
grammatical functions used in LFG,
 atomic attribute-value pairs in f-
structures,
 complexity of the phrase being attached
to, thus indicating both high and low at-
tachment,
 non-right-branching behavior of nonter-
minal nodes,
 non-parallelism of coordinations.
x 2 X , the expectation q[] corresponds to the em-
pirical expectation ~p[]. If we observe incomplete data
y 2 Y, the expectation q[] is replaced by the condi-
tional expectation ~p[k

0
[]] given the observed data y
and the current parameter value 
0
.
3.2 Class-Based Lexicalization
Our approach to grammar lexicalization is
class-based in the sense that we use class-
based estimated frequencies f
c
(v; n) of head-
verbs v and argument head-nouns n in-
stead of pure frequency statistics or class-
based probabilities of head word dependen-
cies. Class-based estimated frequencies are in-
troduced in Prescher et al (2000) as the fre-
quency f(v; n) of a (v; n)-pair in the train-
ing corpus, weighted by the best estimate of
the class-membership probability p(cjv; n) of
an EM-based clustering model on (v; n)-pairs,
i.e., f
c
(v; n) = max
c2C
p(cjv; n)(f(v; n) + 1).
As is shown in Prescher et al (2000) in an
evaluation on lexical ambiguity resolution, a
gain of about 7% can be obtained by using
the class-based estimated frequency f
c
(v; n)
as disambiguation criterion instead of class-
based probabilities p(njv). In order to make
the most direct use possible of this fact, we
incorporated the decisions of the disambigua-
tor directly into 45 additional properties for
the grammatical relations of the subject, di-
rect object, indirect object, innitival object,
oblique and adjunctival dative and accusative
preposition, for active and passive forms of the
rst three verbs in each parse. Let v
r
(x) be the
verbal head of grammatical relation r in parse
x, and n
r
(x) the nominal head of grammatical
relation r in x. Then a lexicalized property 
r
for grammatical relation r is dened as

r
(x) =
8
<
:
1
if f
c
(v
r
(x); n
r
(x)) 
f
c
(v
r
(x
0
); n
r
(x
0
)) 8x
0
2 X(y);
0 otherwise:
The property-function 
r
thus pre-
disambiguates the parses x 2 X(y) of a
sentence y according to f
c
(v; n), and stores
the best parse directly instead of taking the
actual estimated frequencies as its value. In
Sec. 4, we will see that an incorporation of
this pre-disambiguation routine into the mod-
els improves performance in disambiguation
by about 10%.
exact match
evaluation
basic
model
lexicalized
model
selected
+ lexicalized
model
complete-data
estimation
P: 68
E: 59.6
P: 73.9
E: 71.6
P: 74.3
E: 71.8
incomplete-data
estimation
P: 73
E: 65.4
P: 86
E: 85.2
P: 86.1
E: 85.4
Figure 2: Evaluation on exact match task for 550 examples with average ambiguity 5.4
frame match
evaluation
basic
model
lexicalized
model
selected
+ lexicalized
model
complete-data
estimation
P: 80.6
E: 70.4
P: 82.7
E: 76.4
P: 83.4
E: 76
incomplete-data
estimation
P: 84.5
E: 73.1
P: 88.5
E: 84.9
P: 90
E: 86.3
Figure 3: Evaluation on frame match task for 375 examples with average ambiguity 25
4 Experiments
4.1 Incomplete Data and Parsebanks
In our experiments, we used an LFG grammar
for German
2
for parsing unrestricted text.
Since training was faster than parsing, we
parsed in advance and stored the resulting
packed c/f-structures. The low ambiguity rate
of the German LFG grammar allowed us to
restrict the training data to sentences with
at most 20 parses. The resulting training cor-
pus of unannotated, incomplete data consists
of approximately 36,000 sentences of online
available German newspaper text, comprising
approximately 250,000 parses.
In order to compare the contribution of un-
ambiguous and ambiguous sentences to the es-
timation results, we extracted a subcorpus of
4,000 sentences, for which the LFG grammar
produced a unique parse, from the full train-
2
The German LFG grammar is being imple-
mented in the Xerox Linguistic Environment (XLE,
see Maxwell and Kaplan (1996)) as part of the Paral-
lel Grammar (ParGram) project at the IMS Stuttgart.
The coverage of the grammar is about 50% for unre-
stricted newspaper text. For the experiments reported
here, the eective coverage was lower, since the cor-
pus preprocessing we applied was minimal. Note that
for the disambiguation task we were interested in,
the overall grammar coverage was of subordinate rel-
evance.
ing corpus. The average sentence length of
7.5 for this automatically constructed parse-
bank is only slightly smaller than that of
10.5 for the full set of 36,000 training sen-
tences and 250,000 parses. Thus, we conjec-
ture that the parsebank includes a representa-
tive variety of linguistic phenomena. Estima-
tion from this automatically disambiguated
parsebank enjoys the same complete-data es-
timation properties
3
as training from manu-
ally disambiguated treebanks. This makes a
comparison of complete-data estimation from
this parsebank to incomplete-data estimation
from the full set of training data interesting.
4.2 Test Data and Evaluation Tasks
To evaluate our models, we constructed
two dierent test corpora. We rst parsed
with the LFG grammar 550 sentences
which are used for illustrative purposes in
the foreign language learner's grammar of
Helbig and Buscha (1996). In a next step, the
correct parse was indicated by a human dis-
ambiguator, according to the reading intended
in Helbig and Buscha (1996). Thus a precise
3
For example, convergence to the global maximum
of the complete-data log-likelihood function is guar-
anteed, which is a good condition for highly precise
statistical disambiguation.
indication of correct c/f-structure pairs was
possible. However, the average ambiguity of
this corpus is only 5.4 parses per sentence, for
sentences with on average 7.5 words. In order
to evaluate on sentences with higher ambigu-
ity rate, we manually disambiguated further
375 sentences of LFG-parsed newspaper text.
The sentences of this corpus have on average
25 parses and 11.2 words.
We tested our models on two evalua-
tion tasks. The statistical disambiguator was
tested on an exact match task, where ex-
act correspondence of the full c/f-structure
pair of the hand-annotated correct parse and
the most probable parse is checked. Another
evaluation was done on a frame match task,
where exact correspondence only of the sub-
categorization frame of the main verb of the
most probable parse and the correct parse is
checked. Clearly, the latter task involves a
smaller eective ambiguity rate, and is thus
to be interpreted as an evaluation of the com-
bined system of highly-constrained symbolic
parsing and statistical disambiguation.
Performance on these two evaluation tasks
was assessed according to the following evalu-
ation measures:
Precision =
#correct
#correct+#incorrect
,
Eectiveness =
#correct
#correct+#incorrect+#don't know
.
Correct and incorrect species a suc-
cess/failure on the respective evaluation tasks;
don't know cases are cases where the system
is unable to make a decision, i.e. cases with
more than one most probable parse.
4.3 Experimental Results
For each task and each test corpus, we cal-
culated a random baseline by averaging over
several models with randomly chosen pa-
rameter values. This baseline measures the
disambiguation power of the pure symbolic
parser. The results of an exact-match evalu-
ation on the Helbig-Buscha corpus is shown
in Fig. 2. The random baseline was around
33% for this case. The columns list dierent
models according to their property-vectors.
Basic models consist of 190 congurational
properties as described in Sec. 3.1. Lexical-
ized models are extended by 45 lexical pre-
disambiguation properties as described in Sec.
3.2. Selected + lexicalized models result
from a simple property selection procedure
where a cuto on the number of parses with
non-negative value of the property-functions
was set. Estimation of basic models from com-
plete data gave 68% precision (P), whereas
training lexicalized and selected models from
incomplete data gave 86.1% precision, which
is an improvement of 18%. Comparing lex-
icalized models in the estimation method
shows that incomplete-data estimation gives
an improvement of 12% precision over train-
ing from the parsebank. A comparison of mod-
els trained from incomplete data shows that
lexicalization yields a gain of 13% in preci-
sion. Note also the gain in eectiveness (E)
due to the pre-disambigution routine included
in the lexicalized properties. The gain due to
property selection both in precision and eec-
tiveness is minimal. A similar pattern of per-
formance arises in an exact match evaluation
on the newspaper corpus with an ambiguity
rate of 25. The lexicalized and selected model
trained from incomplete data achieved here
60.1% precision and 57.9% eectiveness, for a
random baseline of around 17%.
As shown in Fig. 3, the improvement in per-
formance due to both lexicalization and EM
training is smaller for the easier task of frame
evaluation. Here the random baseline is 70%
for frame evaluation on the newspaper corpus
with an ambiguity rate of 25. An overall gain
of roughly 10% can be achieved by going from
unlexicalized parsebank models (80.6% preci-
sion) to lexicalized EM-trained models (90%
precision). Again, the contribution to this im-
provement is about the same for lexicalization
and incomplete-data training. Applying the
same evaluation to the Helbig-Buscha corpus
shows 97.6% precision and 96.7% eectiveness
for the lexicalized and selected incomplete-
data model, compared to around 80% for the
random baseline.
Optimal iteration numbers were decided by
repeated evaluation of the models at every
fth iteration. Fig. 4 shows the precision of
lexicalized and selected models on the exact
68
70
72
74
76
78
80
82
84
86
88
10 20 30 40 50 60 70 80 90
pre
cis
ion
number of iterations
complete-data estimation
incomplete-data estimation
Figure 4: Precision on exact match task in number of training iterations
match task plotted against the number of it-
erations of the training algorithm. For parse-
bank training, the maximal precision value
is obtained at 35 iterations. Iterating fur-
ther shows a clear overtraining eect. For
incomplete-data estimation more iterations
are necessary to reach a maximal precision
value. A comparison of models with random
or uniform starting values shows an increase
in precision of 10% to 40% for the latter.
In terms of maximization of likelihood, this
corresponds to the fact that uniform starting
values immediately push the likelihood up to
nearly its nal value, whereas random starting
values yield an initial likelihood which has to
be increased by factors of 2 to 20 to an often
lower nal value.
5 Discussion
The most direct points of compar-
ison of our method are the ap-
proaches of Johnson et al (1999) and
Johnson and Riezler (2000). In the rst ap-
proach, log-linear models on LFG grammars
using about 200 congurational properties
were trained on treebanks of about 400
sentences by maximum pseudo-likelihood
estimation. Precision was evaluated on an
exact match task in a 10-way cross valida-
tion paradigm for an ambiguity rate of 10,
and achieved 59% for the rst approach.
Johnson and Riezler (2000) achieved a gain
of 1% over this result by including a class-
based lexicalization. Our best models clearly
outperform these results, both in terms of
precision relative to ambiguity and in terms
of relative gain due to lexicalization. A
comparison of performance is more dicult
for the lexicalized PCFG of Beil et al (1999)
which was trained by EM on 450,000 sen-
tences of German newspaper text. There, a
70.4% precision is reported on a verb frame
recognition task on 584 examples. However,
the gain achieved by Beil et al (1999) due to
grammar lexicalizaton is only 2%, compared
to about 10% in our case. A comparison
is dicult also for most other state-of-the-
art PCFG-based statistical parsers, since
dierent training and test data, and most
importantly, dierent evaluation criteria were
used. A comparison of the performance gain
due to grammar lexicalization shows that our
results are on a par with that reported in
Charniak (1997).
6 Conclusion
We have presented a new approach to stochas-
tic modeling of constraint-based grammars.
Our experimental results show that EM train-
ing can in fact be very helpful for accurate
stochastic modeling in natural language pro-
cessing. We conjecture that this result is due
partly to the fact that the space of parses
produced by a constraint-based grammar is
only mildly incomplete, i.e. the ambiguity
rate can be kept relatively low. Another rea-
son may be that EM is especially useful for
log-linear models, where the search space in
maximization can be kept under control. Fur-
thermore, we have introduced a new class-
based grammar lexicalization, which again
uses EM training and incorporates a pre-
disambiguation routine into log-linear models.
An impressive gain in performance could also
be demonstrated for this method. Clearly, a
central task of future work is a further explo-
ration of the relation between complete-data
and incomplete-data estimation for larger,
manually disambiguated treebanks. An inter-
esting question is whether a systematic vari-
ation of training data size along the lines
of the EM-experiments of Nigam et al (2000)
for text classication will show similar results,
namely a systematic dependence of the rela-
tive gain due to EM training from the relative
sizes of unannotated and annotated data. Fur-
thermore, it is important to show that EM-
based methods can be applied successfully
also to other statistical parsing frameworks.
Acknowledgements
We thank Stefanie Dipper and Bettina
Schrader for help with disambiguation of the
test suites, and the anonymous ACL review-
ers for helpful suggestions. This research was
supported by the ParGram project and the
project B7 of the SFB 340 of the DFG.
References
Franz Beil, Glenn Carroll, Detlef Prescher, Stefan
Riezler, and Mats Rooth. 1999. Inside-outside
estimation of a lexicalized PCFG for German.
In Proceedings of the 37th ACL, College Park,
MD.
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
Proceedings of the 14th AAAI, Menlo Park, CA.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th ACL, Madrid.
J.N. Darroch and D. Ratcli. 1972. General-
ized iterative scaling for log-linear models. The
Annals of Mathematical Statistics, 43(5):1470
1480.
Stephen Della Pietra, Vincent Della Pietra, and
John Laerty. 1997. Inducing features of ran-
dom elds. IEEE PAMI, 19(4):380393.
A. P. Dempster, N. M. Laird, and D. B. Ru-
bin. 1977. Maximum likelihood from incom-
plete data via the EM algorithm. Journal of
the Royal Statistical Society, 39(B):138.
David Elworthy. 1994. Does Baum-Welch re-
estimation help taggers? In Proceedings of the
4th ANLP, Stuttgart.
Gerhard Helbig and Joachim Buscha. 1996.
Deutsche Grammatik. Ein Handbuch f?r den
Ausl?nderunterricht. Langenscheidt, Leipzig.
Edwin T. Jaynes. 1957. Information theory
and statistical mechanics. Physical Review,
106:620630.
Mark Johnson and Stefan Riezler. 2000. Ex-
ploiting auxiliary distributions in stochastic
unication-based grammars. In Proceedings of
the 1st NAACL, Seattle, WA.
Mark Johnson, Stuart Geman, Stephen Canon,
Zhiyi Chi, and Stefan Riezler. 1999. Estimators
for stochastic unication-based grammars. In
Proceedings of the 37th ACL, College Park, MD.
Solomon Kullback. 1959. Information Theory and
Statistics. Wiley, New York.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Build-
ing a large annotated corpus of english: The
Penn treebank. Computational Linguistics,
19(2):313330.
John Maxwell and R. Kaplan. 1996. Unication-
based parsers that automatically take ad-
vantage of context freeness. Unpublished
manuscript, Xerox Palo Alto Research Center.
Kamal Nigam, Andrew McCallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text classi-
cation from labeled and unlabeled documents
using EM. Machine Learning, 39(2/4):103134.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed
corpora. In Proceedings of the 30th ACL,
Newark, Delaware.
Detlef Prescher, Stefan Riezler, and Mats Rooth.
2000. Using a probabilistic class-based lexicon
for lexical ambiguity resolution. In Proceedings
of the 18th COLING, Saarbr?cken.
Adwait Ratnaparkhi. 1997. A linear observed
time statistical parser based on maximum en-
tropy models. In Proceedings of EMNLP-2.
Stefan Riezler. 1999. Probabilistic Constraint
Logic Programming Ph.D. thesis, Seminar
f?r Sprachwissenschaft, Universit?t T?bingen.
AIMS Report, 5(1), IMS, Universit?t Stuttgart.
Compounding and derivational morphology in a finite-state setting
Jonas Kuhn
Department of Linguistics
The University of Texas at Austin
1 University Station, B5100
Austin, TX 78712-11196, USA
jonask@mail.utexas.edu
Abstract
This paper proposes the application of
finite-state approximation techniques on a
unification-based grammar of word for-
mation for a language like German. A
refinement of an RTN-based approxima-
tion algorithm is proposed, which extends
the state space of the automaton by se-
lectively adding distinctions based on the
parsing history at the point of entering a
context-free rule. The selection of history
items exploits the specific linguistic nature
of word formation. As experiments show,
this algorithm avoids an explosion of the
size of the automaton in the approxima-
tion construction.
1 The locus of word formation rules in
grammars for NLP
In English orthography, compounds following pro-
ductive word formation patterns are spelled with
spaces or hyphens separating the components (e.g.,
classic car repair workshop). This is convenient
from an NLP perspective, since most aspects of
word formation can be ignored from the point of
view of the conceptually simpler token-internal pro-
cesses of inflectional morphology, for which stan-
dard finite-state techniques can be applied. (Let
us assume that to a first approximation, spaces and
punctuation are used to identify token boundaries.)
It makes it also very easy to access one or more of
the components of a compound (like classic car in
the example), which is required in many NLP tech-
niques (e.g., in a vector space model).
If an NLP task for English requires detailed in-
formation about the structure of compounds (as
complex multi-token units), it is natural to use the
formalisms of computational syntax for English,
i.e., context-free grammars, or possibly unification-
based grammars. This makes it possible to deal with
the bracketing structure of compounding, which
would be impossible to cover in full generality in
the finite-state setting.
In languages like German, spelling conventions
for compounds do not support such a convenient
split between sub-token processing based on finite-
state technology and multi-token processing based
on context-free grammars or beyond?in German,
even very complex compounds are written without
spaces or hyphens: words like Verkehrswegepla-
nungsbeschleunigungsgesetz (?law for speeding up
the planning of traffic routes?) appear in corpora. So,
for a fully adequate and general account, the token-
level analysis in German has to be done at least with
a context-free grammar:1 For checking the selection
features of derivational affixes, in the general case a
tree or bracketing structure is required. For instance,
the prefix Fehl- combines with nouns (compare (1));
however, it can appear linearly adjacent with a verb,
including its own prefix, and only then do we get the
suffix -ung, which turns the verb into a noun.
(1) N  
N
 
V
 
N

V

V
 
N

Fehl ver arbeit ung
mis work
?misprocessing?
1For a fully general account of derivational morphology in
English, the token-level analysis has to go beyond finite-state
means too: the prefix non- in nonrealizability combines with the
complex derived adjective realizable, not with the verbal stem
realize (and non- could combine with a more complex form).
However, since in English there is much less token-level inter-
action between derivation and compounding, a finite-state ap-
proximation of the relevant facts at token-level is more straight-
forward than in German.
Furthermore, context-free power is required to parse
the internal bracketing structure of complex words
like (2), which occur frequently and productively.
(2) N  
N
 
A
 
A
 
N
 
V
 
N
 
A
 
N

V

V
 
A

N

V
 
N

Gesund heits ver tr?g lich keits pr?f ung
healthy bear examine
?check for health compatibility?
As the results of the DeKo project on deriva-
tional and compositional morphology of German
show (Schmid et al 2001), an adequate account
of the word formation principles has to rely on a
number of dimensions (or features/attributes) of the
morphological units. An affix?s selection of the el-
ement it combines with is based on these dimen-
sions. Besides part-of-speech category, the dimen-
sions include origin of the morpheme (Germanic vs.
classical, i.e., Latinate or Greek2), complexity of
the unit (simplex/derived), and stem type (for many
lemmata, different base stems, derivation stems and
compounding stems are stored; e.g., tr?g in (2) is
a derivational stem for the lemma trag(en) (?bear?);
heits is the compositional stem for the affix heit).
Given these dimensions in the affix feature selec-
tion, we need a unification-based (attribute) gram-
mar to capture the word formation principles explic-
itly in a formal account. A slightly simplified such
grammar is given in (3), presented in a PATR-II-
style notation:3
(3) a. X0   X1 X2
 X1 CAT  = PREFIX
 X0 CAT  =  X1 MOTHER-CAT 
 X0 COMPLEXITY  = PREFIX-DERIVED
 X1 SELECTION  = X2
b. X0   X1 X2
 X2 CAT  = SUFFIX
 X0 CAT  =  X2 MOTHER-CAT 
 X0 COMPLEXITY  = SUFFIX-DERIVED
 X2 SELECTION  = X1
2Of course, not the true ethymology is relevant here; ORIGIN
is a category in the synchronic grammar of speakers, and for
individual morphemes it may or may not be in accordance with
diachronic facts.
3An implementation of the DeKo rules in the unification for-
malism YAP is discussed in (Wurster 2003).
c. X0   X1 X2
 X0 CAT  =  X2 CAT 
 X0 COMPLEXITY  = COMPOUND
(4) Sample lexicon entries
a. X0: intellektual-
 X0 CAT  = A
 X0 ORIGIN  = CLASSICAL
 X0 COMPLEXITY  = SIMPLEX
 X0 STEM-TYPE  = DERIVATIONAL
 X0 LEMMA  = ?intellektuell?
b. X0: -isier-
 X0 CAT  = SUFFIX
 X0 MOTHER-CAT  = V
 X0 SELECTION CAT  = A
 X0 SELECTION ORIGIN  = CLASSICAL
Applying the suffixation rule, we can derive
intellektual.isier- (the stem of ?intellectualize?) from
the two sample lexicon entries in (4). Note how the
selection feature (SELECTION) of prefixes and af-
fixes are unified with the selected category?s features
(triggered by the last feature equation in the prefixa-
tion and suffixation rules (3a,b)).
Context-freeness Since the range of all atomic-
valued features is finite and we can exclude lexicon
entries specifying the SELECTION feature embedded
in their own SELECTION value, the three attribute
grammar rewrite rules can be compiled out into an
equivalent context-free grammar.
2 Arguments for a finite-state word
formation component
While there is linguistic justification for a context-
free (or unification-based) model of word formation,
there are a number of considerations that speak in
favor of a finite-state account. (A basic assumption
made here is that a morphological analyzer is typi-
cally used in a variety of different system contexts,
so broad usability, consistency, simplicity and gen-
erality of the architecture are important criteria.)
First, there are a number of NLP applications
for which a token-based finite-state analysis is stan-
dardly used as the only linguistic analysis. It would
be impractical to move to a context-free technol-
ogy in these areas; at the same time it is desirable
to include an account of word formation in these
tasks. In particular, it is important to be able to break
down complex compounds into the individual com-
ponents, in order to reach an effect similar to the way
compounds are treated in English orthography.
Second, inflectional morphology has mostly been
treated in the finite-state two-level paradigm. Since
any account of word formation has to be combined
with inflectional morphology, using the same tech-
nology for both parts guarantees consistency and re-
usability.4
Third, when a morphological analyzer is used
in a linguistically sophisticated application context,
there will typically be other linguistic components,
most notably a syntactic grammar. In these compo-
nents, more linguistic information will be available
to address derivation/compounding. Since the nec-
essary generative capacity is available in the syntac-
tic grammar anyway, it seems reasonable to leave
more sophisticated aspects of morphological analy-
sis to this component (very much like the syntax-
based account of English compounds we discussed
initially). Given the first two arguments, we will
however nevertheless aim for maximal exactness of
the finite-state word formation component.
3 Previous strategies of addressing
compounding and derivation
Naturally, existing morphological analyzers of lan-
guages like German include a treatment of compo-
sitional morphology (e.g., Schiller 1995). An over-
generation strategy has been applied to ensure cov-
erage of corpus data. Exactness was aspired to for
the inflected head of a word (which is always right-
peripheral in German), but not for the non-head part
of a complex word. The non-head may essentially
be a flat concatenation of lexical elements or even an
arbitrary sequence of symbols. Clearly, an account
making use of morphological principles would be
desirable. While the internal structure of a word
is not relevant for the identification of the part-of-
speech category and morphosyntactic agreement in-
formation, it is certainly important for information
extraction, information retrieval, and higher-level
tasks like machine translation.
4An alternative is to construct an interface component be-
tween a finite-state inflectional morphology and a context-free
word formation component. While this can be conceivably
done, it restricts the applicability of the resulting overall system,
since many higher-level applications presuppose a finite-state
analyzer; this is for instance the case for the Xerox Linguistic
Environment (http://www.parc.com/istl/groups/nltt/xle/), a de-
velopment platform for syntactic Lexical-Functional Grammars
(Butt et al 1999).
An alternative strategy?putting emphasis on a
linguistically satisfactory account of word forma-
tion?is to compile out a higher-level word forma-
tion grammar into a finite-state automaton (FSA),
assuming a bound to the depth of recursive self-
embedding. This strategy was used in a finite-state
implementation of the rules in the DeKo project
(Schmid et al 2001), based on the AT&T Lextools
toolkit by Richard Sproat.5 The toolkit provides
a compilation routine which transforms a certain
class of regular-grammar-equivalent rewrite gram-
mars into finite-state transducers. Full context-free
recursion has to be replaced by an explicit cascading
of special category symbols (e.g., N1, N2, N3, etc.).
Unfortunately, the depth of embedding occur-
ring in real examples is at least four, even if we
assume that derivations like ver.tr?g.lich (?com-
patible?; in (2)) are stored in the lexicon as
complex units: in the initially mentioned com-
pound Verkehrs.wege.planungs.beschleunigungs.ge-
setz (?law for speeding up the planning of traffic
routes?), we might assume that Verkehrs.wege (?traf-
fic routes?) is stored as a unit, but the remainder
of the analysis is rule-based. With this depth of
recursion (and a realistic morphological grammar),
we get an unmanagable explosion of the number of
states in the compiled (intermediate) FSA.
4 Proposed strategy
We propose a refinement of finite-state approxima-
tion techniques for context-free grammars, as they
have been developed for syntax (Pereira and Wright
1997, Grimley-Evans 1997, Johnson 1998, Neder-
hof 2000). Our strategy assumes that we want to
express and develop the morphological grammar at
the linguistically satisfactory level of a (context-
free-equivalent) unification grammar. In process-
ing, a finite-state approximation of this grammar is
used. Exploiting specific facts about morphology,
the number of states for the constructed FSA can be
kept relatively low, while still being in a position to
cover realistic corpus example in an exact way.
The construction is based on the following obser-
vation: Intuitively, context-free expressiveness is not
needed to constrain grammaticality for most of the
5Lextools: a toolkit for finite-state linguistic analysis, AT&T
Labs Research; http://www.research.att.com/sw/tools/lextools/
word formation combinations. This is because in
most cases, either (i) morphological feature selec-
tion is performed between string-adjacent terminal
symbols, or (ii) there are no categorial restrictions
on possible combinations. (i) is always the case
for suffixation, since German morphology is exclu-
sively right-headed.6 So the head of the unit selected
by the suffix is always adjacent to it, no matter how
complex the unit is:
(5) X
Y
. . . Y X

(i) is also the case for prefixes combining with a sim-
ple unit. (ii) is the case for compounding: while
affix-derivation is sensitive to the mentioned dimen-
sions like category and origin, no such grammati-
cal restrictions apply in compounding.7 So the fact
that in compounding, the heads of the two combined
units may not be adjacent (since the right unit may
be complex) does not imply that context-freeness is
required to exclude impossible combinations:
(6) X  
X 
X  X  X  X  
or X  
X  X  
X  X  X  X  
or X  
X  
X  X  X  X  
The only configuration requiring context-freeness
to exclude ungrammatical examples is the combina-
tion of a prefix with a complex morphological unit:
(7)
X
X
X

. . . X
As (1) showed, such examples do occur; so they
should be given an exact treatment. However, the
depth of recursive embeddings of this particular type
(possibly with other embeddings intervening) in re-
alistic text is limited. So a finite-state approximation
6This may appear to be falsified by examples like ver- (V  )
+ Urteil (N, ?judgement?) = verurteilen (V, ?convict?); how-
ever, in this case, a noun-to-verb conversion precedes the prefix
derivation. Note that the inflectional marking is always right-
peripheral.
7Of course, when speakers disambiguate the possible brack-
etings of a complex compound, they can exclude many com-
binations as implausible. But this is a defeasible world
knowledge-based effect, which should not be modeled as strict
selection in a morphological grammar.
keeping track of prefix embeddings in particular, but
leaving the other operations unrestricted seems well
justified. We will show in sec. 6 how such a tech-
nique can be devised, building on the algorithm re-
viewed in sec. 5.
5 RTN-based approximation techniques
A comprehensive overview and experimental com-
parison of finite-state approximation techniques for
context-free grammars is given in (Nederhof 2000).
In Nederhof?s approximation experiments based on
an HPSG grammar, the so-called RTN method
provided the best trade-off between exactness and
the resources required in automaton construction.
(Techniques that involve a heavy explosion of the
number of states are impractical for non-trivial
grammars.) More specifically, a parameterized ver-
sion of the RTN method, in which the FSA keeps
track of possible derivational histories, was consid-
ered most adequate.
The RTN method of finite-state approximation is
inspired by recursive transition networks (RTNs).
RTNs are collections of sub-automata. For each rule

	
in a context-free grammar, a sub-
automaton with  states is constructed:
(8) 
Experiments in Parallel-Text Based Grammar Induction
Jonas Kuhn
Department of Linguistics
The University of Texas at Austin
Austin, TX 78712
jonak@mail.utexas.edu
Abstract
This paper discusses the use of statistical word
alignment over multiple parallel texts for the identi-
fication of string spans that cannot be constituents
in one of the languages. This information is ex-
ploited in monolingual PCFG grammar induction
for that language, within an augmented version of
the inside-outside algorithm. Besides the aligned
corpus, no other resources are required. We discuss
an implemented system and present experimental
results with an evaluation against the Penn Tree-
bank.
1 Introduction
There have been a number of recent studies exploit-
ing parallel corpora in bootstrapping of monolin-
gual analysis tools. In the ?information projection?
approach (e.g., (Yarowsky and Ngai, 2001)), statis-
tical word alignment is applied to a parallel corpus
of English and some other language   for which no
tagger/morphological analyzer/chunker etc. (hence-
forth simply: analysis tool) exists. A high-quality
analysis tool is applied to the English text, and
the statistical word alignment is used to project a
(noisy) target annotation to the   version of the text.
Robust learning techniques are then applied to boot-
strap an analysis tool for
 
, using the annotations
projected with high confidence as the initial train-
ing data. (Confidence of both the English analysis
tool and the statistical word alignment is taken into
account.) The results that have been achieved by
this method are very encouraging.
Will the information projection approach also
work for less shallow analysis tools, in particular
full syntactic parsers? An obvious issue is that
one does not expect the phrase structure representa-
tion of English (as produced by state-of-the-art tree-
bank parsers) to carry over to less configurational
languages. Therefore, (Hwa et al, 2002) extract
a more language-independent dependency structure
from the English parse as the basis for projection
to Chinese. From the resulting (noisy) dependency
treebank, a dependency parser is trained using the
techniques of (Collins, 1999). (Hwa et al, 2002) re-
port that the noise in the projected treebank is still
a major challenge, suggesting that a future research
focus should be on the filtering of (parts of) unre-
liable trees and statistical word alignment models
sensitive to the syntactic projection framework.
Our hypothesis is that the quality of the result-
ing parser/grammar for language   can be signifi-
cantly improved if the training method for the parser
is changed to accomodate for training data which
are in part unreliable. The experiments we report
in this paper focus on a specific part of the prob-
lem: we replace standard treebank training with
an Expectation-Maximization (EM) algorithm for
PCFGs, augmented by weighting factors for the re-
liability of training data, following the approach of
(Nigam et al, 2000), who apply it for EM train-
ing of a text classifier. The factors are only sen-
sitive to the constituent/distituent (C/D) status of
each span of the string in
  (cp. (Klein and Man-
ning, 2002)). The C/D status is derived from an
aligned parallel corpus in a way discussed in sec-
tion 2. We use the Europarl corpus (Koehn, 2002),
and the statistical word alignment was performed
with the GIZA++ toolkit (Al-Onaizan et al, 1999;
Och and Ney, 2003).1
For the current experiments we assume no pre-
existing parser for any of the languages, contrary
to the information projection scenario. While bet-
ter absolute results could be expected using one or
more parsers for the languages involved, we think
that it is important to isolate the usefulness of ex-
ploiting just crosslinguistic word order divergences
in order to obtain partial prior knowledge about the
constituent structure of a language, which is then
exploited in an EM learning approach (section 3).
Not using a parser for some languages also makes
it possible to compare various language pairs at the
same level, and specifically, we can experiment with
grammar induction for English exploiting various
1The software is available at
http://www.isi.edu/?och/GIZA++.html
At   that  moment  the  voting  will  commence  . 
Le   vote  aura  lieu  ?  ce  moment  -la  . 
Figure 1: Alignment example
other languages. Indeed the focus of our initial ex-
periments has been on English (section 4), which
facilitates evaluation against a treebank (section 5).
2 Cross-language order divergences
The English-French example in figure 1 gives a sim-
ple illustration of the partial information about con-
stituency that a word-aligned parallel corpus may
provide. The en bloc reversal of subsequences of
words provides strong evidence that, for instance, [
moment the voting ] or [ aura lieu ? ce ] do not form
constituents.
At first sight it appears as if there is also clear ev-
idence for [ at that moment ] forming a constituent,
since it fully covers a substring that appears in a dif-
ferent position in French. Similarly for [ Le vote
aura lieu ]. However, from the distribution of con-
tiguous substrings alone we cannot distinguish be-
tween two the types of situations sketched in (1) and
(2):
(1) 	


	 	
	 	 	 	 	


   










	 	 	 	 	



	 	
	
(2) 	
	



	
	 	 	 	 	


   










	 	 	 	 	



	
	
	
A string that is contiguous under projection, like

 

 (1) may be a true constituent, but it may also
be a non-constituent part of a larger constituent as
in 
 
in (2).
Word blocks. Let us define the notion of a word
block (as opposed to a phrase or constituent) in-
duced by a word alignment to capture the relevant
property of contiguousness under translation.2 The
alignments induced by GIZA++ (following the IBM
models) are asymmetrical in that several words from
  may be aligned with one word in    , but not vice
versa. So we can view a word alignment as a func-
tion  that maps each word in an    -sentence to
a (possibly empty) subset of words from its trans-
lation in  . For example, in figure 1, Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 17?24,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Parsing Word-Aligned Parallel Corpora in a Grammar Induction Context
Jonas Kuhn
The University of Texas at Austin, Department of Linguistics
jonask@mail.utexas.edu
Abstract
We present an Earley-style dynamic pro-
gramming algorithm for parsing sentence
pairs from a parallel corpus simultane-
ously, building up two phrase structure
trees and a correspondence mapping be-
tween the nodes. The intended use of
the algorithm is in bootstrapping gram-
mars for less studied languages by using
implicit grammatical information in par-
allel corpora. Therefore, we presuppose a
given (statistical) word alignment under-
lying in the synchronous parsing task; this
leads to a significant reduction of the pars-
ing complexity. The theoretical complex-
ity results are corroborated by a quantita-
tive evaluation in which we ran an imple-
mentation of the algorithm on a suite of
test sentences from the Europarl parallel
corpus.
1 Introduction
The technical results presented in this paper1 are
motivated by the following considerations: It is con-
ceivable to use sentence pairs from a parallel corpus
(along with the tentative word correspondences from
a statistical word alignment) as training data for a
grammar induction approach. The goal is to induce
monolingual grammars for the languages under con-
sideration; but the implicit information about syn-
tactic structure gathered from typical patterns in the
alignment goes beyond what can be obtained from
unlabeled monolingual data. Consider for instance
the sentence pair from the Europarl corpus (Koehn,
2002) in fig. 1 (shown with a hand-labeled word
alignment): distributional patterns over this and sim-
ilar sentences may show that in English, the subject
1This work was in part supported by the German Research
Foundation DFG in the context of the author?s Emmy Noether
research group at Saarland University.
(the word block ?the situation?) is in a fixed struc-
tural position, whereas in German, it can appear in
various positions; similarly, the finite verb in Ger-
man (here: stellt) systematically appears in second
position in main clauses. In a way, the translation
of sentences into other natural languages serves as
an approximation of a (much more costly) manual
structural or semantic annotation ? one might speak
of automatic indirect supervision in learning. The
technique will be most useful for low-resource lan-
guages and languages for which there is no funding
for treebanking activities. The only requirement will
be that a parallel corpus exist for the language under
consideration and one or more other languages.2
Induction of grammars from parallel corpora is
rarely viewed as a promising task in its own right;
in work that has addressed the issue directly (Wu,
1997; Melamed, 2003; Melamed, 2004), the syn-
chronous grammar is mainly viewed as instrumental
in the process of improving the translation model in
a noisy channel approach to statistical MT.3 In the
present paper, we provide an important prerequisite
for parallel corpus-based grammar induction work:
an efficient algorithm for synchronous parsing of
sentence pairs, given a word alignment. This work
represents a second pilot study (after (Kuhn, 2004))
for the longer-term PTOLEMAIOS project at Saar-
land University4 with the goal of learning linguis-
tic grammars from parallel corpora (compare (Kuhn,
2005)). The grammars should be robust and assign a
2In the present paper we use examples from English/German
for illustration, but the approach is of course independent of the
language pair under consideration.
3Of course, there is related work (e.g., (Hwa et al, 2002; Lu?
et al, 2002)) using aligned parallel corpora in order to ?project?
bracketings or dependency structures from English to another
language and exploit them for training a parser for the other
language. But note the conceptual difference: the ?parse projec-
tion? approach departs from a given monolingual parser, with a
particular style of analysis, whereas our project will explore to
what extent it may help to design the grammar topology specifi-
cally for the parallel corpus case. This means that the emerging
English parser may be different from all existing ones.
4http://www.coli.uni-saarland.de/?jonask/PTOLEMAIOS/
17
Heute stellt sich die Lage jedoch vo?llig anders dar
The situation now however is radically different
Figure 1: Word-aligned German/English sentence pair from the Europarl corpus
predicate-argument-modifier (or dependency) struc-
ture to sentences, such that they can be applied in
the context of multilingual information extraction or
question answering.
2 Synchronous grammars
For the purpose of grammar induction from parallel
corpora, we assume a fairly straightforward exten-
sion of context-free grammars to the synchronous
grammar case (compare the transduction grammars
of (Lewis II and Stearns, 1968)): Firstly, the termi-
nal and non-terminal categories are pairs of sym-
bols, one for each language; as a special case, one
of the two symbols can be NIL for material realized
in only one of the languages. Secondly, the linear
sequence of daughter categories that is specified in
the rules can differ for the two languages; therefore,
an explicit numerical ranking is used for the linear
precedence in each language. We use a compact
rule notation with a numerical ranking for the lin-
ear precedence in each language. The general form
of a grammar rule for the case of two parallel lan-
guages is N0/M0 ? N1:i1/M1:j1 . . . Nk:ik/Mk:jk,
where Nl,Ml are NIL or a terminal or nonterminal
symbol for language L1 and L2, respectively, and
il, jl are natural numbers for the rank of the phrase
in the sequence for L1 and L2 respectively (for NIL
categories a special rank 0 is assumed).5 Since linear
ordering of daughters in both languages is explic-
itly encoded by the rank indices, the specification
sequence in the rule is irrelevant from a declarative
point of view. To facilitate parsing we assume a nor-
mal form in which the right-hand side is ordered by
the rank in L1, with the exception that the categories
that are NIL in L1 come last. If there are several such
5Note that in the probabilistic variants of these grammars,
we will typically expect that any ordering of the right-hand side
symbols is possible (but that the probability will of course vary
? in a maximum entropy or log-linear model, the probability
will be estimated based on a variety of learning features). This
means that in parsing, the right-hand side categories will be ac-
cepted as they come in, and the relevant probability parameters
are looked up accordingly.
NIL categories in the same rule, they are viewed as
unordered with respect to each other.6
Fig. 2 illustrates our simple synchronous gram-
mar formalism with some rules of a sample grammar
and their application on a German/English sentence
pair. Derivation with a synchronous grammar gives
rise to a multitree, which combines classical phrase
structure trees for the languages involved and also
encodes the phrase level correspondence across the
languages. Note that the two monolingual trees in
fig. 2 for German and English are just two ways of
unfolding the common underlying multitree.
Note that the simple formalism goes along with
the continuity assumption that every complete con-
stituent is continuous in both languages. Various re-
cent studies in the field of syntax-based Statistical
MT have shown that such an assumption is problem-
atic when based on typical treebank-style analyses.
As (Melamed, 2003) discusses for instance, in the
context of binary branching structures even simple
examples like the English/French pair a gift for you
from France ? un cadeau de France pour vouz [a
gift from France for you] lead to discontinuity of a
?synchronous phrase? in one of the two languages.
(Gildea, 2003) and (Galley et al, 2004) discuss dif-
ferent ways of generalizing the tree-level crosslin-
guistic correspondence relation, so it is not confined
to single tree nodes, thereby avoiding a continuity
assumption. We believe that in order to obtain full
coverage on real parallel corpora, some mechanism
along these lines will be required.
However, if the typical rich phrase structure anal-
yses (with fairly detailed fine structure) are replaced
by flat, multiply branching analyses, most of the
highly frequent problematic cases are resolved.7 In
6This detail will be relevant for the parsing inference rule
(5) below.
7Compare the systematic study for English-French align-
ments by (Fox, 2002), who compared (i) treebank-parser style
analyses, (ii) a variant with flattened VPs, and (iii) dependency
structures. The degree of cross-linguistic phrasal cohesion in-
creases from (i) to (iii). With flat clausal trees, we will come
close to dependency structures with respect to cohesion.
18
Synchronous grammar rules:
S/S ? NP:1/NP:2 Vfin:2/Vfin:3 Adv:3/Adv:1
NP:4/PP:5 Vinf:5/Vinf:4
NP/NP ? Pron:1/Pron:1
NP/PP ? Det:1/Det:2 N:2/N:4 NIL:0/P:1 NIL:0/Adj:3
Pron/Pron ? wir:1/we:1
Vfin/Vfin ? mu?ssen:1/must:1
Adv/Adv ? deshalb:1/so:1
NIL/P ? NIL:0/at:1
Det/Det ? die:1/the:1
NIL/Adj ? NIL:0/agricultural:1
N/N ? Agrarpolitik:1/policy:1
Vinf/Vinf ? pru?fen:1/look:1
German tree:
S
NP Vfin Adv NP Vinf
Pron Det N
Wir mu?ssen deshalb die Agrarpolitik pru?fen
we must therefore the agr. policy examine
English tree:
S
Adv NP Vfin Vinf PP
Pron P Det Adj N
So we must look at the agricultural policy
Multitree:
S/S
NP:1/NP:2 Vfin:2/Vfin:3 Adv:3/Adv:1 NP:4/PP:5 Vinf:5/Vinf:4
Pron:1/Pron:1 NIL:0/P:1 Det:1/Det:2 NIL:0/Adj:3 N:2/N:4
Wir/we mu?ssen/must deshalb/so NIL/at die/the NIL/agricultural Agrarpolitik/policy pru?fen/look
Figure 2: Sample rules and analysis for a synchronous grammar
the flat representation that we assume, a clause is
represented in a single subtree of depth 1, with all
verbal elements and the argument/adjunct phrases
(NPs or PPs) as immediate daughters of the clause
node. Similarly, argument/adjunct phrases are flat
internally. Such a flat representation is justified
both from the point of view of linguistic learning
and from the point of view of grammar application:
(i) Language-specific principles of syntactic struc-
ture (e.g., the strong configurationality of English),
which are normally captured linguistically by the
richer phrase structure, are available to be induced
in learning as systematic patterns in the relative or-
dering of the elements of a clause. (ii) The predicate-
argument-modifier structure relevant for application
of the grammars, e.g., in information extraction can
be directly read off the flat clausal representation.
It is a hypothesis of our longer-term project that
a word alignment-based consensus structure which
works with flat representations and under the con-
tinuity assumption is a very effective starting point
for learning the basic language-specific constraints
required for a syntactic grammar. Linguistic phe-
nomena that fall outside what can be captured in this
confined framework (in particular unbounded de-
pendencies spanning more than one clause and dis-
continuous argument phrases) will then be learned
in a later bootstrapping step that provides a richer
set of operations. We are aware of a number of open
practical questions, e.g.: Will the fact that real paral-
lel corpora often contain rather free translations un-
dermine our idea of using the consensus structure
for learning basic syntactic constraints? Statistical
alignments are imperfect ? can the constraints im-
posed by the word alignment be relaxed accordingly
without sacrificing tractability and the effect of indi-
rect supervision?8
3 Alignment-guided synchronous parsing
Our dynamic programming algorithm can be de-
scribed as a variant of standard Earley-style chart
parsing (Earley, 1970) and generation (Shieber,
1988; Kay, 1996). The chart is a data structure
which stores all sub-analyses that cover part of the
input string (in parsing) or meaning representation
(in generation). Memoizing such partial results has
the standard advantage of dynamic programming
techniques ? it helps one to avoid unnecessary re-
computation of partial results. The chart structure
for context-free parsing is also exploited directly in
dynamic programming algorithms for probabilistic
context-free grammars (PCFGs): (i) the inside (or
outside) algorithm for summing over the probabil-
ities for every possible analysis of a given string,
(ii) the Viterbi algorithm for determining the most
likely analysis of a given string, and (iii) the in-
8Ultimately, bootstrapping of not only the grammars, but
also of the word alignment should be applied.
19
side/outside algorithm for re-estimating the param-
eters of the PCFG in an Expectation-Maximization
approach (i.e., for iterative training of a PCFG on
unlabeled data). This aspect is important for the in-
tended later application of our parsing algorithm in
a grammar induction context.
A convenient way of describing Earley-style pars-
ing is by inference rules. For instance, the central
completion step in Earley parsing can be described
by the rule9
(1) ?X ? ? ? Y ?, [i, j]?, ?Y ? ? ?, [j, k]?
?X ? ? Y ? ?, [i, k]?
Synchronous parsing. The input in synchronous
parsing is not a one-dimensional string, but a pair of
sentences, i.e., a two-dimensional array of possible
word pairs (or a multidimensional array if we are
looking at a multilingual corpus), as illustrated in
fig. 3.
policy ?
agricultural
the ?
at
look ?
must ?
we ?
So ?
0 1 2 3 4 5 6
L 2
:
L1: Wir mu?ssen deshalb die Agrar- pru?fen
politik
Figure 3: Synchronous parsing: two-dimensional in-
put (with word alignment marked)
The natural way of generalizing context-free pars-
ing to synchronous grammars is thus to control the
inference rules by string indices in both dimensions.
Graphically speaking, parsing amounts to identify-
ing rectangular crosslinguistic constituents ? by as-
sembling smaller rectangles that will together cover
the full string spans in both dimensions (compare
(Wu, 1997; Melamed, 2003)). For instance in fig. 4,
the NP/NP rectangle [i1, j1, j2, k2] can be combined
with the Vinf/Vinf rectangle [j1, k1, i2, j2] (assum-
ing there is an appropriate rule in the grammar).
9A chart item is specified through a position (?) in a pro-
duction and a string span ([l1, l2]). ?X ? ? ? Y ?, [i, j]?
means that between string position i and j, the beginning of
an X phrase has been found, covering ?, but still missing Y ?.
Chart items for which the dot is at the end of a production (like
?Y ? ??, [j, k]?) are called passive items, the others active.
Vinf/Vinf
NP/NP
i1 j1 k1
k2
j2
i2
her
interview
sie interviewen
Figure 4: Completion in two-dimensional chart:
parsing part of Can I interview her?/Kann ich sie
interviewen?
More generally, we get the inference rules (2) and
(3) (one for the case of parallel sequencing, one for
crossed order across languages).
(2) ?X1/X2 ? ? ? Y1:r1/Y2:r2 ?, [i1, j1, i2, j2]?,
?Y1/Y2 ? ? ?, [j1, k1, j2, k2]?
?X1/X2 ? ? Y1:r1/Y2:r2 ? ?, [i1, k1, i2, k2]?
(3) ?X1/X2 ? ? ? Y1:r1/Y2:r2 ?, [i1, j1, j2, k2]?,
?Y1/Y2 ? ? ?, [j1, k1, i2, j2]?
?X1/X2 ? ? Y1:r1/Y2:r2 ? ?, [i1, k1, i2, k2]?
Since each inference rule contains six free vari-
ables over string positions (i1, j1, k1, i2, j2, k2), we
get a parsing complexity of order O(n6) for unlexi-
calized grammars (where n is the number of words
in the longer of the two strings from language L1 and
L2) (Wu, 1997; Melamed, 2003). For large-scale
learning experiments this may be problematic, es-
pecially when one moves to lexicalized grammars,
which involve an additional factor of n4.10
As a further issue, we observe that the inference
rules are insufficient for multiply branching rules,
in which partial constituents may be discontinuous
in one dimension (only complete constituents need
to be continuous in both dimensions). For instance,
by parsing the first two words of the German string
in fig. 1 (Heute stellt), we should get a partial chart
item for a sentence, but the English correspondents
for the two words (now and is) are discontinuous, so
we couldn?t apply rule (2) or (3).
Correspondence-guided parsing. As an alterna-
tive to the standard ?rectangular indexing? approach
10The assumption here (following (Melamed, 2003)) is that
lexicalization is not considered as just affecting the grammar
constant, but that in parsing, every terminal symbol has to be
considered as the potential head of every phrase of which it is
a part. Melamed demonstrates: If the number of different cat-
egory symbols is taken into consideration as l, we get O(l2n6)
for unlexicalized grammars, and O(l6n10) for lexicalized gram-
mars; however there are some possible optimizations.
20
to synchronous parsing we propose a conceptually
very simple asymmetric approach. As we will show
in sec. 4 and 5, this algorithm is both theoretically
and practically efficient when applied to sentence
pairs for which a word alignment has previously
been determined. The approach is asymmetric in
that one of the languages is viewed as the ?master
language?, i.e., indexing in parsing is mainly based
on this language (the ?primary index? is the string
span in L1 as in monolingual parsing). The other
language contributes a secondary index, which is
mainly used to guide parsing in the master language
? i.e., certain options are eliminated. The choice of
the master language is in principle arbitrary, but for
efficiency considerations it is better to pick the one
that has more words without a correspondent.
A way of visualizing correspondence-guided
parsing is that standard Earley parsing is applied to
L1, with primary indexing by string position; as the
chart items are assembled, the synchronous gram-
mar and the information from the word alignment
is used to check whether the string in L2 could be
generated (essentially using chart-based generation
techniques; cf. (Shieber, 1988; Neumann, 1998)).
The index for chart items consists of two compo-
nents: the string span in L1 and a bit vector for the
words in L2 which are covered. For instance, based
on fig. 3, the noun compound Agrarpolitik corre-
sponding to agricultural policy in English will have
the index ?[4, 5], [0, 0, 0, 0, 0, 0, 1, 1]? (assuming for
illustrative purposes that German is the master lan-
guage in this case).
The completion step in correspondence-guided
parsing can be formulated as the following single in-
ference rule:11
(4) ?X1/X2 ? ? ? Y1:r1/Y2:r2 ?, ?[i, j], v??,
?Y1/Y2 ? ? ?, ?[j, k],w??
?X1/X2 ? ? Y1:r1/Y2:r2 ? ?, ?[i, k], u??
where
(i) j 6= k;
(ii) OR(v,w) = u;
(iii) w is continuous (i.e., it contains maximally
one subsequence of 1?s).
Condition (iii) excludes discontinuity in passive
chart items, i.e., complete constituents; active items
11We use the bold-faced variables v,w,u for bit vectors; the
function OR performs bitwise disjunction on the vectors (e.g.,
OR([0, 1, 1, 0, 0], [0, 0, 1, 0, 1]) = [0, 1, 1, 0, 1]).
(i.e., partial constituents) may well contain discon-
tinuities. The success condition for parsing a string
with N words in L1 is that a chart item with index
?[0, N ],1? has been found for the start category pair
of the grammar.
Words in L2 with no correspondent in L1 (let?s
call them ?L1-NIL?s for short), for example the
words at and agricultural in fig. 3,12 can in princi-
ple appear between any two words of L1. Therefore
they are represented with a ?variable? empty L1-
string span like for instance in ?[i, i], [0, 0, 1, 0, 0]?.
At first blush, such L1-NILs seem to introduce an
extreme amount of non-determinism into the algo-
rithm. Note however that due to the continuity as-
sumption for complete constituents, the distribution
of the L1-NILs is constrained by the other words in
L2. This is exploited by the following inference rule,
which is the only way of integrating L1-NILs into the
chart:
(5) ?X1/X2 ? ? ? NIL:0/Y2:r2 ?, ?[i, j], v??,
?NIL/Y2 ? ? ?, ?[j, j], w??
?X1/X2 ? ? NIL:0/Y2:r2 ? ?, ?[i, j], u??
where
(i) w is adjacent to v (i.e., unioning vectors w
and v does not lead to more 0-separated 1-
sequences than v contains already);
(ii) OR(v,w) = u.
The rule has the effect of finalizing a cross-
linguistic constituent (i.e., rectangle in the two-
dimensional array) after all the parts that have corre-
spondents in both languages have been found. 13
4 Complexity
We assume that the two-dimensional chart is ini-
tialized with the correspondences following from a
word alignment. Hence, for each terminal that is
non-empty in L1, both components of the index are
known. When two items with known secondary in-
dices are combined with rule (4), the new secondary
12It is conceivable that a word alignment would list agricul-
tural as an additional correspondent for Agrarpolitik; but we
use the given alignment for illustrative purposes.
13For instance, the L1-NILs in fig. 3 ? NIL/at and
NIL/agricultural ? have to be added to incomplete NP/PP
constituent in the L1-string span from 3 to 5, consist-
ing of the Det/Det die/the and the N/N Agrarpolitik/policy.
With two applications of rule (5), the two L1-NILs can be
added. Note that the conditions are met, and that as a re-
sult, we will have a continuous NP/PP constituent with index
?[3, 5], [0, 0, 0, 0, 1, 1, 1, 1]?, which can be used as a passive
item Y1/Y2 in rule (4).
21
index can be determined by bitwise disjunction of
the bit vectors. This operation is linear in the length
of the L2-string (which is of the same order as the
length of the L1-string) and has a very small con-
stant factor.14 Since parsing with a simple, non-
lexicalized context-free grammar has a time com-
plexity of O(n3) (due to the three free variables
for string positions in the completion rule), we get
O(n4) for synchronous parsing of sentence pairs
without any L1-NILs. Note that words from L1 with-
out a correspondent in L2 (which we would have to
call L2-NILs) do not add to the complexity, so the
language with more correspondent-less words can
be selected as L1.
For the average complexity of correspondence-
guided parsing of sentence pairs without L1-NILs we
note an advantage over monolingual parsing: cer-
tain hypotheses for complete constituents that would
have to be considered when parsing only L1, are ex-
cluded because the secondary index reveals a dis-
continuity. An example from fig. 3 would be the se-
quence mu?ssen deshalb, which is adjacent in L1, but
doesn?t go through as a continuous rectangle when
L2 is taken into consideration (hence it cannot be
used as a passive item in rule (4)).
The complexity of correspondence-guided pars-
ing is certainly increased by the presence of L1-
NILs, since with them the secondary index can no
longer be uniquely determined. However, with the
adjacency condition ((i) in rule (5)), the number of
possible variants in the secondary index is a func-
tion of the number of L1-NILs. Let us say there are
m L1-NILs, i.e., the bit vectors contain m elements
that we have to flip from 0 to 1 to obtain the final bit
vector. In each application of rule (5) we pick a vec-
tor v, with a variable for the leftmost and rightmost
L1-NIL element (since this is not fully determined
by the primary index). By the adjacency condition,
14Note that the operation does not have to be repeated when
the completion rule is applied on additional pairs of items with
identical indices. This means that the extra time complexity fac-
tor of n doesn?t go along with an additional factor of the gram-
mar constant (which we are otherwise ignoring in the present
considerations). In practical terms this means that changes in
the size of the grammar are much more noticable than moving
from monolingual parsing to alignment-guided parsing.
An additional advantage is that in an Expectation Maximiza-
tion approach to grammar induction (with a fixed word align-
ment), the bit vectors have to be computed only in the first iter-
ation of parsing the training corpus, later iterations are cubic.
either the leftmost or rightmost marks the boundary
for adding the additional L1-NIL element NIL/Y2 ?
hence we need only one new variable for the newly
shifted boundary among the L1-NILs. So, in addition
to the n4 expense of parsing non-nil words, we get
an expense of m3 for parsing the L1-NILs, and we
conclude that for unlexicalized synchronous pars-
ing, guided by an initial word alignment the com-
plexity class is O(n4m3) (where n is the total num-
ber of words appearing in L1, and m is the number
of words appearing in L2, without a correspondent
in L1). Recall that the complexity for standard syn-
chronous parsing is O(n6).
Since typically the number of correspondent-less
words is significantly lower than the total number of
words (at least for one of the two languages), these
results are encouraging for medium-to-large-scale
grammar learning experiments using a synchronous
parsing algorithm.
5 Empirical Evaluation
In order to validate the theoretical complexity results
empirically, we implemented the algorithm and ran
it on sentence pairs from the Europarl parallel cor-
pus. At the present stage, we are interested in quan-
titative results on parsing time, rather than qualita-
tive results of parsing accuracy (for which a more
extensive training of the rule parameters would be
required).
Implementation. We did a prototype implementa-
tion of the correspondence-guided parsing algorithm
in SWI Prolog.15 Chart items are asserted to the
knowledge base and efficiently retrieved using in-
dexing by a hash function. Besides chart construc-
tion, the Viterbi algorithm for selecting the most
probable analysis has been implemented, but for the
current quantitative results only chart construction
was relevant.
Sample grammar extraction. The initial prob-
ablistic grammar for our experiments was ex-
tracted from a small ?multitree bank? of 140 Ger-
man/English sentence pairs (short examples from
the Europarl corpus). The multitree bank was an-
notated using the MMAX2 tool16 and a specially
15http://www.swi-prolog.org ? The advantage of using Pro-
log is that it is very easy to experiment with various conditions
on the inference rules in parsing.
16http://mmax.eml-research.de
22
tailored annotation scheme for flat correspondence
structures as described in sec. 2. A German and En-
glish part-of-speech tagger was used to determine
word categories; they were mapped to a reduced cat-
egory set and projected to the syntactic constituents.
To obtain parameters for a probabilistic grammar,
we used maximum likelihood estimation from the
small corpus, based on a rather simplistic genera-
tive model,17 which for each local subtree decides
(i) what categories will be the two heads, (ii) how
many daughters there will be, and for each non-
head sister (iii) whether it will be a nonterminal or
a terminal (and in that case, what category pair),
and (iv) in which position relative to the head to
place it in both languages. In order to obtain a
realistically-sized grammar, we applied smoothing
to all parameters; so effectively, every sequence of
terminals/nonterminals of arbitrary length was pos-
sible in parsing.
Parsing sentences without NIL words
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
4 5 6 7 8 9 10
number of words (in L1)
pa
rs
in
g 
tim
e 
[se
c] Monolingual parsing L1
CGSP
Figure 5: Comparison of synchronous parsing with
and without exploiting constraints from L2
Results. To validate empirically that the pro-
posed correspondence-guided synchronous parsing
approach (CGSP) can effectively exploit L2 as a
guide, thereby reducing the search space of L1
parses that have to be considered, we first ran a
comparison on sentences without L1-NILs. The re-
sults (average parsing time for Viterbi parsing with
the sample grammar) are shown in fig. 5.18 The
parser we call ?monolingual? cannot exploit any
17For our learning experiments we intend to use a Maximum
Entropy/log-linear model with more features.
18The experiments were run on a 1.4GHz Pentium M proces-
sor.
alignment-induced restrictions from L2.19 Note that
CGSP takes clearly less time.
Comparison wrt. # NIL words
0
0.2
0.4
0.6
0.8
1
1.2
1.4
5 6 7 8 9 10
number of words (in L1)
pa
rs
in
g 
tim
e 
[se
c]
3 L1-NILs,
CGSP
2 L1-NILs,
CGSP
1 L1-NIL,
CGSP
no L1-NILs,
CGSP
monolingual
parsing (L1)
Figure 6: Synchronous parsing with a growing num-
ber of L1-NILs
Fig. 6 shows our comparative results for parsing
performance on sentences that do contain L1-NILs.
Here too, the theoretical results are corroborated that
with a limited number of L1-NILs, the CGSP is still
efficient.
The average chart size (in terms of the number of
entries) for sentences of length 8 (in L1) was 212
for CGSP (and 80 for ?monolingual? parsing). The
following comparison shows the effect of L1-NILs
(note that the values for 4 and more L1-NILs are
based on only one or two cases):
(6) Chart size for sentences of length 8 (in L1)
Number of
L1-NILs
0 1 2 3 4 5 6
Avg. num-
ber of chart
items
77 121 175 256 (330) (435) (849)
We also simulated a synchronous parser which
does not take advantage of a given word alignment
(by providing an alignment link between any pair
of words, plus the option that any word could be a
NULL word). For sentences of length 5, this parser
took an average time of 22.3 seconds (largely inde-
pendent of the presence/absence of L1-NILs).20
19The ?monolingual? parser used in this comparison parses
two identical copies of the same string synchronously, with a
strictly linear alignment.
20While our simulation may be significantly slower than a di-
rect implementation of the algorithm (especially when some of
the optimizations discussed in (Melamed, 2003) are taken into
account), the fact that it is orders of magnitude slower does in-
23
Finally, we also ran an experiment in which the
continuity condition (condition (iii) in rule (4)) was
deactivated, i.e., complete constituents were allowed
to be discontinuous in one of the languages. The re-
sults in (7) underscore the importance of this condi-
tion ? leaving it out leads to a tremendous increase
in parsing time.
(7) Average parsing time in seconds with and with-
out continuity condition
Sentence length (with no L1-
NILs)
4 5 6
Avg. parsing time with CGSP
(incl. continuity condition)
0.005 0.012 0.026
Avg. parsing time without the
continuity condition
0.035 0.178 1.025
6 Conclusion
We proposed a conceptually simple, yet efficient al-
gorithm for synchronous parsing in a context where
a word alignment can be assumed as given ? for in-
stance in a bootstrapping learning scenario. One of
the two languages in synchronous parsing acts as the
master language, providing the primary string span
index, which is used as in classical Earley parsing.
The second language contributes a bit vector as a
secondary index, inspired by work on chart gener-
ation. Continuity assumptions make it possible to
constrain the search space significantly, to the point
that synchronous parsing for sentence pairs with few
?NULL words? (which lack correspondents) may be
faster than standard monolingual parsing. We dis-
cussed the complexity both theoretically and pro-
vided a quantitative evaluation based on a prototype
implementation.
The study we presented is part of the longer-term
PTOLEMAIOS project. The next step is to apply
the synchronous parsing algorithm with probabilis-
tic synchronous grammars in grammar induction ex-
periments on parallel corpora.
References
Jay C. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94?102.
dicate that our correspondence-guided approach is a promising
alternative for an application context in which a word alignment
is available.
Heidi J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP 2002),
pages 304?311.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proceed-
ings of the Human Language Technology Conference of the
North American Chapter of the Association for Computa-
tional Linguistics: HLT-NAACL 2004, pages 273?280.
Daniel Gildea. 2003. Loosely tree-based alignment for ma-
chine translation. In Proceedings of the 41st Annual Meeting
of the Association for Computational Linguistics (ACL?03),
Sapporo, Japan, pages 80?87.
Rebecca Hwa, Philip Resnik, and Amy Weinberg. 2002.
Breaking the resource bottleneck for multilingual parsing.
In Proceedings of LREC.
Martin Kay. 1996. Chart generation. In Proceedings of the
34th Annual Meeting of the Association for Computational
Linguistics, Santa Cruz, CA.
Philipp Koehn. 2002. Europarl: A multilingual corpus for eval-
uation of machine translation. Ms., University of Southern
California.
Jonas Kuhn. 2004. Experiments in parallel-text based grammar
induction. In Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics: ACL 2004,
pages 470?477.
Jonas Kuhn. 2005. An architecture for parallel corpus-based
grammar learning. In Bernhard Fisseni, Hans-Christian
Schmitz, Bernhard Schro?der, and Petra Wagner, editors,
Sprachtechnologie, mobile Kommunikation und linguisti-
sche Ressourcen. Beitra?ge zur GLDV-Tagung 2005 in Bonn,
pages 132?144, Frankfurt am Main. Peter Lang.
Philip M. Lewis II and Richard E. Stearns. 1968. Syntax-
directed transduction. Journal of the Association of Com-
puting Machinery, 15(3):465?488.
Yajuan Lu?, Sheng Li, Tiejun Zhao, and Muyun Yang. 2002.
Learning chinese bracketing knowledge based on a bilingual
language model. In COLING 2002 - Proceedings of the 19th
International Conference on Computational Linguistics.
I. Dan Melamed. 2003. Multitext grammars and synchronous
parsers. In Proceedings of NAACL/HLT.
I. Dan Melamed. 2004. Statistical machine translation by pars-
ing. In Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics: ACL 2004, pages
653?660.
Gu?nter Neumann. 1998. Interleaving natural language parsing
and generation through uniform processing. Artifical Intelli-
gence, 99:121?163.
Stuart Shieber. 1988. A uniform architecture for parsing and
generation. In Proceedings of the 12th International Con-
ference on Computational Linguistics (COLING), Budapest.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
24
OT Syntax: Decidability of Generation-based Optimization  
Jonas Kuhn
Department of Linguistics
Stanford University
jonask@stanford.edu
Abstract
In Optimality-Theoretic Syntax, optimiza-
tion with unrestricted expressive power on
the side of the OT constraints is unde-
cidable. This paper provides a proof for
the decidability of optimization based on
constraints expressed with reference to lo-
cal subtrees (which is in the spirit of OT
theory). The proof builds on Kaplan and
Wedekind?s (2000) construction showing
that LFG generation produces context-
free languages.
1 Introduction
Optimality-Theoretic (OT) grammar systems are an
interesting alternative to classical formal grammars,
as they construe the task of learning from data in
a meaning-based way: a form is defined as gram-
matical if it is optimal (most harmonic) within a set
of generation alternatives for an underlying logical
form. The harmony of a candidate analysis depends
on a language-specific ranking (  ) of violable con-
straints, thus the learning task amounts to adjusting
the ranking over a given set of constraints.
(1) Candidate  is more harmonic than  iff it incurs fewer
violations of the highest-ranking constraint 	
 in
which  and  differ.
The comparison-based setup of OT learning is
closely related to discriminative learning approaches
in probabilistic parsing (Johnson et al, 1999; Rie-
zler et al, 2000; Riezler et al, 2002),1 however the
comparison of generation alternatives ? rather than
parsing alternatives ? adds the possibility of system-
atically learning the basic language-specific gram-
matical principles (which in probabilistic parsing
are typically fixed a priori, using either a treebank-
derived or a manually written grammar for the given

This work was supported by a postdoctoral fellowship of
the German Academic Exchange Service (DAAD).
1This is for instance pointed out by (Johnson, 1998).
language). The ?base grammar? assumed as given
can be highly unrestricted in the OT setup. Using a
linguistically motivated set of constraints, learning
proceeds with a bias for unmarked linguistic struc-
tures (cf. e.g., (Bresnan et al, 2001)).
For computational OT syntax, an interleaving of
candidate generation and constraint checking has
been proposed (Kuhn, 2000). But the decidability
of the optimization task in OT syntax, i.e., the iden-
tification of the optimal candidate(s) in a potentially
infinite candidate set, has not been proven yet.2
2 Undecidability for unrestricted OT
Assume that the candidate set is characterized by
a context-free grammar (cfg)  , plus one addi-
tional candidate ?yes?. There are two constraints
(     ):   is violated if the candidate is neither
?yes? nor a structure generated by a cfg 

;   is vi-
olated only by ?yes?. Now, ?yes? is in the language
defined by this system iff there are no structures in
 that are also in 

. But the emptiness problem
for the intersection of two context-free languages is
known to be undecidable, so the optimization task
for unrestricted OT is undecidable too.3
However, it is not in the spirit of OT to have
extremely powerful individual constraints; the ex-
planatory power should rather arise from interaction
of simple constraints.
3 OT-LFG
Following (Bresnan, 2000; Kuhn, 2000; Kuhn,
2001), we define a restricted OT system based
on Lexical-Functional Grammar (LFG) represen-
tations: c(ategory) structure/f(unctional) structure
2Most computational OT work so far focuses on candidates
and constraints expressible as regular languages/rational rela-
tions, based on (Frank and Satta, 1998) (e.g., (Eisner, 1997;
Karttunen, 1998; Gerdemann and van Noord, 2000)).
3Cf. also (Johnson, 1998) for the sketch of an undecidability
argument and (Kuhn, 2001, 4.2, 6.3) for further constructions.
                  Computational Linguistics (ACL), Philadelphia, July 2002, pp. 48-55.
                         Proceedings of the 40th Annual Meeting of the Association for
pairs
 
like
  (4),(5)  . Each c-structure tree
node is mapped to a node in the f-structure graph
by the function 	 . The mapping is specified by f-
annotations in the grammar rules (below category
symbols, cf. (2)) and lexicon entries (3).4
(2) ROOT 


FP




VP

FP 
 
NP FP


TOPIC 
 


COMP* OBJ 





(NP) F Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 37?40,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Improving data-driven dependency parsing
using large-scale LFG grammars
Lilja ?vrelid, Jonas Kuhn and Kathrin Spreyer
Department of Linguistics
University of Potsdam
{lilja,kuhn,spreyer}@ling.uni-potsdam.de
Abstract
This paper presents experiments which
combine a grammar-driven and a data-
driven parser. We show how the con-
version of LFG output to dependency
representation allows for a technique of
parser stacking, whereby the output of the
grammar-driven parser supplies features
for a data-driven dependency parser. We
evaluate on English and German and show
significant improvements stemming from
the proposed dependency structure as well
as various other, deep linguistic features
derived from the respective grammars.
1 Introduction
The divide between grammar-driven and data-
driven approaches to parsing has become less pro-
nounced in recent years due to extensive work on
robustness and efficiency for the grammar-driven
approaches (Riezler et al, 2002; Cahill et al,
2008b). The linguistic generalizations captured in
such knowledge-based resources are thus increas-
ingly available for use in practical applications.
The NLP-community has in recent years wit-
nessed a surge of interest in dependency-based
approaches to syntactic parsing, spurred by the
CoNLL shared tasks of dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007).
Nivre and McDonald (2008) show how two differ-
ent approaches to dependency parsing, the graph-
based and transition-based approaches, may be
combined and subsequently learn to complement
each other to achieve improved parse results for a
range of different languages.
In this paper, we show how a data-driven depen-
dency parser may straightforwardly be modified to
learn directly from a grammar-driven parser. We
evaluate on English and German and show signifi-
cant improvements for both languages. Like Nivre
and McDonald (2008), we supply a data-driven
dependency parser with features from a different
parser to guide parsing. The additional parser em-
ployed in this work, is not however, a data-driven
parser trained on the same data set, but a grammar-
driven parser outputing a deep LFG analysis. We
furthermore show how a range of other features ?
morphological, structural and semantic ? from the
grammar-driven analysis may be employed dur-
ing data-driven parsing and lead to significant im-
provements.
2 Grammar-driven LFG-parsing
The XLE system (Crouch et al, 2007) performs
unification-based parsing using hand-crafted LFG
grammars. It processes raw text and assigns to it
both a phrase-structural (?c-structure?) and a fea-
ture structural, functional (?f-structure?).
In the work described in this paper, we employ
the XLE platform using the grammars available
for English and German from the ParGram project
(Butt et al, 2002). In order to increase the cover-
age of the grammars, we employ the robustness
techniques of fragment parsing and ?skimming?
available in XLE (Riezler et al, 2002).
3 Dependency conversion and feature
extraction
In extracting information from the output of the
deep grammars we wish to capture as much of the
precise, linguistic generalizations embodied in the
grammars as possible, whilst keeping with the re-
quirements posed by the dependency parser. The
process is illustrated in Figure 1.
3.1 Data
The English data set consists of the Wall Street
Journal sections 2-24 of the Penn treebank (Mar-
cus et al, 1993), converted to dependency format.
The treebank data used for German is the Tiger
37
f1
?
?
?
?
?
?
?
?
?
?
?
?
PRED ?halte?. . .??
VTYPE predicative
SUBJ ?pro?
OBJ
f2
?
?
PRED ?Verhalten?
CASE acc
SPEC f3?das?
ADJUNCT
{
f4?damalige?
}
?
?
XCOMP-PRED
?
?
PRED ?fu?r?. . .??
PTYPE nosem
OBJ
[
PRED ?richtig?
SUBJ
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
SUBJ
converted:
SPEC
XCOMP-PRED
ADJCT
SUBJ-OBJ
OBJ
Ich halte das damalige Verhalten fu?r richtig.
1sg pred. acc nosem
g
SB
old:
NK
OA
NK
MO
NK
Figure 1: Treebank enrichment with LFG output; German example: I consider the past behaviour cor-
rect.
treebank (Brants et al, 2004), where we employ
the version released with the CoNLL-X shared
task on dependency parsing (Buchholz and Marsi,
2006).
3.2 LFG to dependency structure
We start out by converting the XLE output to a
dependency representation. This is quite straight-
forward since the f-structures produced by LFG
parsers can be interpreted as dependency struc-
tures. The conversion is performed by a set of
rewrite rules which are executed by XLE?s built-
in extraction engine. We employ two strategies for
the extraction of dependency structures from out-
put containing multiple heads. We attach the de-
pendent to the closest head and, i) label it with the
corresponding label (Single), ii) label it with the
complex label corresponding to the concatenation
of the labels from the multiple head attachments
(Complex). The converted dependency analysis in
Figure 1 shows the f-structure and the correspond-
ing converted dependency output of a German ex-
ample sentence, where a raised object Verhalten
receives the complex SUBJ-OBJ label. Following
the XLE-parsing of the treebanks and the ensu-
ing dependency conversion, we have a grammar-
based analysis for 95.2% of the English sentence,
45238 sentences altogether, and 96.5% of the Ger-
man sentences, 38189 sentences altogether.
3.3 Deep linguistic features
The LFG grammars capture linguistic generaliza-
tions which may not be reduced to a dependency
representation. For instance, the grammars con-
tain information on morphosyntactic properties
such as case, gender and tense, as well as more se-
mantic properties detailing various types of adver-
bials, specifying semantic conceptual categories
such as human, time and location etc., see Fig-
ure 1. Table 1 presents the features extracted for
use during parsing from the German and English
XLE-parses.
4 Data-driven dependency parsing
MaltParser (Nivre et al, 2006a) is a language-
independent system for data-driven dependency
parsing which is freely available.1 MaltParser is
based on a deterministic parsing strategy in com-
bination with treebank-induced classifiers for pre-
dicting parse transitions. MaltParser constructs
parsing as a set of transitions between parse con-
figurations. A parse configuration is a triple
?S, I,G?, where S represents the parse stack, I is
the queue of remaining input tokens, and G repre-
sents the dependency graph defined thus far.
The feature model in MaltParser defines the rel-
evant attributes of tokens in a parse configuration.
Parse configurations are represented by a set of
features, which focus on attributes of the top of the
stack, the next input token and neighboring tokens
in the stack, input queue and dependency graph
under construction. Table 2 shows an example of
a feature model.2
For the training of baseline parsers we employ
feature models which make use of the word form
(FORM), part-of-speech (POS) and the dependency
relation (DEP) of a given token, exemplified in
Table 2. For the baseline parsers and all subse-
quent parsers we employ the arg-eager algorithm
in combination with SVM learners with a polyno-
mial kernel.3
1http://maltparser.org
2Note that the feature model in Table 2 is an example fea-
ture model and not the actual model employed in the parse
experiments. The details or references for the English and
German models are provided below.
3For training of the baseline parsers we also em-
ploy some language-specific settings. For English we
use learner and parser settings, as well as feature model
from the English pretrained MaltParser-model available from
http://maltparser.org. For German, we use the learner and
parser settings from the parser employed in the CoNLL-X
38
POS XFeats
Verb CLAUSETYPE, GOVPREP, MOOD, PASSIVE, PERF,
TENSE, VTYPE
Noun CASE, COMMON, GOVPREP, LOCATIONTYPE, NUM,
NTYPE, PERS, PROPERTYPE
Pronoun CASE, GOVPREP, NUM, NTYPE, PERS
Prep PSEM, PTYPE
Conj COORD, COORD-FORM, COORD-LEVEL
Adv ADJUNCTTYPE, ADVTYPE
Adj ATYPE, DEGREE
English DEVERBAL, PROG, SUBCAT, GENDSEM, HUMAN,
TIME
German AUXSELECT, AUXFLIP, COHERENT, FUT, DEF, GEND,
GENITIVE, COUNT
Table 1: Features from XLE output, common for
both languages and language-speciffic
FORM POS DEP XFEATS XDEP
S:top + + + + +
I:next + + + +
I:next?1 + +
G:head of top + +
G:leftmost dependent of top + +
InputArc(XHEAD)
Table 2: Example feature model; S: stack, I: input,
G: graph; ?n = n positions to the left(?) or right
(+).
5 Parser stacking
The procedure to enable the data-driven parser to
learn from the grammar-driven parser is quite sim-
ple. We parse a treebank with the XLE platform.
We then convert the LFG output to dependency
structures, so that we have two parallel versions
of the treebank ? one gold standard and one with
LFG-annotation. We extend the gold standard
treebank with additional information from the cor-
responding LFG analysis, as illustrated by Figure
1 and train the data-driven dependency parser on
the enhanced data set.
We extend the feature model of the baseline
parsers in the same way as Nivre and McDon-
ald (2008). The example feature model in Table
2 shows how we add the proposed dependency
relation (XDEP) top and next as features for the
parser. We furthermore add a feature which looks
at whether there is an arc between these two tokens
in the dependency structure (InputArc(XHEAD)),
with three possible values: Left, Right, None. In
order to incorporate further information supplied
by the LFG grammars we extend the feature mod-
els with an additional, static attribute, XFEATS.
This is employed for the range of deep linguistic
features, detailed in section 3.3 above.
5.1 Experimental setup
All parse experiments are performed using 10-fold
cross-validation for training and testing. Overall
parsing accuracy will be reported using the stan-
dard metrics of labeled attachment score (LAS)
and unlabeled attachment score (UAS).Statistical
significance is checked using Dan Bikel?s random-
ized parsing evaluation comparator.4
shared task (Nivre et al, 2006b). For both languages, we em-
ploy so-called ?relaxed? root handling.
4http://www.cis.upenn.edu/?dbikel/software.html
6 Results
We experiment with the addition of two types of
features: i) the dependency structure proposed by
XLE for a given sentence ii) other morphosyntac-
tic, structural or lexical semantic features provided
by the XLE grammar. The results are presented in
Table 3.
For English, we find that the addition of pro-
posed dependency structure from the grammar-
driven parser causes a small, but significant im-
provement of results (p<.0001). In terms of la-
beled accuracy the results improve with 0.15 per-
centage points, from 89.64 to 89.79. The introduc-
tion of complex dependency labels to account for
multiple heads in the LFG output causes a smaller
improvement of results than the single labeling
scheme. The corresponding results for German are
presented in Table 3. We find that the addition of
grammar-driven dependency structures with sin-
gle labels (Single) improves the parse results sig-
nificantly (p<.0001), both in terms of unlabeled
and labeled accuracy. For labeled accuracy we ob-
serve an improvement of 1.45 percentage points,
from 85.97 to 87.42. For the German data, we
find that the addition of dependency structure with
complex labels (Complex) gives a further small,
but significant (p<.03) improvement over the ex-
periment with single labels.
The results following the addition of the
grammar-extracted features in Table 1 (Feats) are
presented in Table 3.5 We observe significant im-
provements of overall parse results for both lan-
guages (p<.0001).
5We experimented with several feature models for the in-
clusion of the additional information, however, found no sig-
nificant differences when performing a forward feature selec-
tion. The simple feature model simply adds the XFEATS of
the top and next tokens of the parse configuration.
39
English German
UAS LAS UAS LAS
Baseline 92.48 89.64 88.68 85.97
Single 92.61 89.79 89.72 87.42
Complex 92.58 89.74 89.76 87.46
Feats 92.55 89.77 89.63 87.30
Single+Feats 92.52 89.69 90.01 87.77
Complex+Feats 92.53 89.70 90.02 87.78
Table 3: Overall results in experiments expressed as unlabeled and labeled attachment scores.
We also investigated combinations of the dif-
ferent sources of information ? dependency struc-
tures and deep features. These results are pre-
sented in the final lines of Table 3. We find
that for the English parser, the combination of
the features do not cause a further improve-
ment of results, compared to the individual ex-
periments. The combined experiments (Sin-
gle+Feats, Complex+Feats) for German, on the
other hand, differ significantly from the base-
line experiment, as well as the individual ex-
periments (Single,Complex,Feats) reported above
(p<.0001). By combination of the grammar-
derived features we improve on the baseline by
1.81 percentage points.
A comparison with the German results obtained
using MaltParser with graph-based dependency
structures supplied by MSTParser (Nivre and Mc-
Donald, 2008) shows that our results using a
grammar-driven parser largely corroborate the ten-
dencies observed there. Our best results for Ger-
man, combining dependency structures and addi-
tional features, slightly improve on those reported
for MaltParser (by 0.11 percentage points).6
7 Conclusions and future work
This paper has presented experiments in the com-
bination of a grammar-driven LFG-parser and a
data-driven dependency parser. We have shown
how the use of converted dependency structures
in the training of a data-driven dependency parser,
MaltParser, causes significant improvements in
overall parse results for English and German. We
have furthermore presented a set of additional,
deep features which may straightforwardly be ex-
tracted from the grammar-based output and cause
individual improvements for both languages and a
combined effect for German.
In terms of future work, a more extensive er-
ror analysis will be performed to locate the pre-
6English was not among the languages investigated in-
Nivre and McDonald (2008).
cise benefits of the parser combination. We will
also investigate the application of the method di-
rectly to raw text and application to a task which
may benefit specifically from the combined anal-
yses, such as semantic role labeling or semantic
verb classification.
It has recently been shown that automatically
acquired LFG grammars may actually outperform
hand-crafted grammars in parsing (Cahill et al,
2008a). These results add further to the relevance
of the results shown in this paper, bypassing the
bottleneck of grammar hand-crafting as a prereq-
uisite for the applicability of our results.
References
Sabine Brants, Stefanie Dipper, Peter Eisenberg, Silvia Hansen-Schirra, Esther
Knig, Wolfgang Lezius, Christian Rohrer, George Smith, and Hans Uszko-
reit. 2004. Tiger: Linguistic interpretation of a German corpus. Research
on Language and Computation, 2:597?620.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilin-
gual dependency parsing. In Proceedings of CoNLL-X).
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hiroshi Masuichi, and
Christian Rohrer. 2002. The Parallel Grammar Project. In Proceedings
of COLING-2002 Workshop on Grammar Engineering and Evaluation.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan Riezler, Josef van Gen-
abith, and Andy Way. 2008a. Wide-coverage deep statistical parsing using
automatic dependency structure annotation. Computational Linguistics.
Aoife Cahill, John T. Maxwell, Paul Meurer, Christian Rohrer, and Victoria
Rosen. 2008b. Speeding up LFG parsing using c-structure pruning. In
Proceedings of the Workshop on Grammar Engineering Across Frame-
works.
D. Crouch, M. Dalrymple, R. Kaplan, T. King, J. Maxwell, and P. Newman,
2007. XLE Documentation. http://www2.parc.com/isl/.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large
annotated corpus for English: The Penn treebank. Computational Linguis-
tics, 19(2):313?330.
Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and
transition-based dependency parsers. In Proceedings of ACL-HLT 2008.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006a. Maltparser: A data-driven
parser-generator for dependency parsing. In Proceedings of LREC.
Joakim Nivre, Jens Nilsson, Johan Hall, Gu?ls?en Eryig?it, and Svetoslav Mari-
nov. 2006b. Labeled pseudo-projective dependency parsing with Support
Vector Machines. In Proceedings of CoNLL.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDonald, Jens Nilsson, Se-
bastian Riedel, and Deniz Yuret. 2007. CoNLL 2007 Shared Task on
Dependency Parsing. In Proceedings of the CoNLL Shared Task Session
of EMNLP-CoNLL 2007, pages 915?932.
Stefan Riezler, Tracy King, Ronald Kaplan, Richard Crouch, John T. Maxwell,
and Mark Johnson. 2002. Parsing the Wall Street journal using a lexical-
functional grammar and discriminative estimation techniques. In Proceed-
ings of ACL.
40
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 41?48,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Machine Translation as Tree Labeling
Mark Hopkins
Department of Linguistics
University of Potsdam, Germany
hopkins@ling.uni-potsdam.de
Jonas Kuhn
Department of Linguistics
University of Potsdam, Germany
kuhn@ling.uni-potsdam.de
Abstract
We present the main ideas behind a new
syntax-based machine translation system,
based on reducing the machine translation
task to a tree-labeling task. This tree la-
beling is further reduced to a sequence of
decisions (of four varieties), which can be
discriminatively trained. The optimal tree
labeling (i.e. translation) is then found
through a simple depth-first branch-and-
bound search. An early system founded
on these ideas has been shown to be
competitive with Pharaoh when both are
trained on a small subsection of the Eu-
roparl corpus.
1 Motivation
Statistical machine translation has, for a while now,
been dominated by the phrase-based translation par-
adigm (Och and Ney, 2003). In this paradigm,
sentences are translated from a source language to
a target language through the repeated substitution
of contiguous word sequences (?phrases?) from the
source language for word sequences in the target
language. Training of the phrase translation model
builds on top of a standard statistical word align-
ment over the training corpus for identifying corre-
sponding word blocks, assuming no further linguis-
tic analysis of the source or target language. In de-
coding, these systems then typically rely on n-gram
language models and simple statistical reordering
models to shuffle the phrases into an order that is
coherent in the target language.
There are limits to what such an approach can ul-
timately achieve. Machine translation based on a
deeper analysis of the syntactic structure of a sen-
tence has long been identified as a desirable objec-
tive in principle (consider (Wu, 1997; Yamada and
Knight, 2001)). However, attempts to retrofit syn-
tactic information into the phrase-based paradigm
have not met with enormous success (Koehn et al,
2003; Och et al, 2003)1, and purely phrase-based
machine translation systems continue to outperform
these syntax/phrase-based hybrids.
In this work, we try to make a fresh start with
syntax-based machine translation, discarding the
phrase-based paradigm and designing a machine
translation system from the ground up, using syntax
as our central guiding star. Evaluation with BLEU
and a detailed manual error analysis of our nascent
system show that this new approach might well have
the potential to finally realize some of the promises
of syntax.
2 Problem Formulation
We want to build a system that can learn to translate
sentences from a source language to a destination
language. As our first step, we will assume that the
system will be learning from a corpus consisting of
triples ?f, e, a?, where: (i) f is a sentence from our
source language, which is parsed (the words of the
sentence and the nodes of the parse tree may or may
not be annotated with auxiliary information), (ii) e is
a gold-standard translation of sentence f (the words
of sentence e may or may not be annotated with aux-
iliary information), and (iii) a is an automatically-
generated word alignment (e.g. via GIZA++) be-
tween source sentence f and destination sentence e.
1(Chiang, 2005) also reports that with his hierarchical gen-
eralization of the phrase-based approach, the addition of parser
information doesn?t lead to any improvements.
41
Figure 1: Example translation object.
Let us refer to these triples as translation objects.
The learning task is: using the training data, pro-
duce a scoring function P that assigns a score to
every translation object ?f, e, a?, such that this scor-
ing function assigns a high score to good transla-
tions, and a low score to poor ones. The decoding
task is: given scoring function P and an arbitrary
sentence f from the source language, find transla-
tion object ?f, e, a? that maximizes P (?f, e, a?).
To facilitate matters, we will map translation ob-
jects to an alternate representation. In (Galley et al,
2003), the authors give a semantics to every trans-
lation object by associating each with an annotated
parse tree (hereafter called a GHKM tree) represent-
ing a specific theory about how the source sentence
was translated into the destination sentence.
In Figure 1, we show an example translation ob-
ject and in Figure 2, we show its associated GHKM
tree. The GHKM tree is simply the parse tree f of
the translation object, annotated with rules (hereafter
referred to as GHKM rules). We will not describe in
depth the mapping process from translation object to
GHKM tree. Suffice it to say that the alignment in-
duces a set of intuitive translation rules. Essentially,
a rule like: ?not 1? ne 1 pas? (see Figure 2) means:
if we see the word ?not? in English, followed by a
phrase already translated into French, then translate
the entire thing as the word ?ne? + the translated
phrase + the word ?pas.? A parse tree node gets la-
beled with one of these rules if, roughly speaking,
its span is still contiguous when projected (via the
alignment) into the target language.
Formally, what is a GHKM tree? Define a rule el-
ement as a string or an indexed variable (e.g. x1,
x4, x32). A GHKM rule of rank k (where k is
a non-negative integer) is a pair ?Rs, Rd?, where
source list Rs and destination list Rd are both lists
of rule elements, such that each variable of Xk ,
{x1, x2, ..., xk} appears exactly once in Rs and ex-
actly once in Rd. Moreover, in Rs, the variables ap-
pear in ascending order. In Figure 2, some of the
tree nodes are annotated with GHKM rules. For
clarity, we use a simplified notation. For instance,
rule ??x1, x2, x3?, ?x3, ?,?, x1, x2?? is represented as
?1 2 3 ? 3 , 1 2?. We have also labeled the nodes
with roman numerals. When we want to refer to a
particular node in later examples, we will refer to it,
e.g., as t(i) or t(vii).
A rule node is a tree node annotated with a
GHKM rule (for instance, nodes t(i) or t(v) of Fig-
ure 2, but not node t(iv)). A tree node t2 is reachable
from tree node t1 iff node t2 is a proper descendant
of node t1 and there is no rule node (not including
nodes t1, t2) on the path from node t1 to node t2.
Define the successor list of a tree node t as the list
of rule nodes and leaves reachable from t (ordered in
left-to-right depth-first search order). For Figure 2,
the successor list of node t(i) is ?t(ii), t(v), t(xiii)?,
and the successor list of node t(v) is ?t(vii), t(viii)?.
The rule node successor list of a tree node is its suc-
cessor list, with all non-rule nodes removed.
Define the signature of a parse tree node t as the
result of taking its successor list, replacing the jth
rule node with variable xj , and replacing every non-
rule node with its word label (observe that all non-
rule nodes in the successor list are parse tree leaves,
and therefore they have word labels). For Figure 2,
the signature of node t(i) is ?x1, x2, x3?, and the sig-
nature of node t(v) is ??am?, x1?.
Notice that the signature of every rule node in Fig-
ure 2 coincides with the source list of its GHKM
rule. This is no accident, but rather a requirement.
Define a GHKM tree node as a parse tree node
whose children are all GHKM tree nodes, and whose
GHKM rule?s source list is equivalent to its signa-
ture (if the node is a rule node).
Given these definitions, we can proceed to define
how a GHKM tree expresses a translation theory.
Suppose we have a list S = ?s1, ..., sk? of strings.
Define the substitution of string list S into rule ele-
42
Figure 2: GHKM tree equivalent of example translation object. The light gray nodes are rule nodes of the
GHKM tree.
ment r as:
r[S] =
 si if r is indexed var xi
r otherwise
Notice that this operation always produces a
string. Define the substitution of string list S into
rule element list R = ?r1, ..., rj? as:
R[S] = concat(r1[S], r2[S], ..., rj [S])
where concat(s1, ..., sk) is the spaced concatenation
of strings s1, ..., sk (e.g., concat( ?hi?, ?there? ) =
?hi there?). This operation also produces a string.
Finally, define the translation of GHKM tree node
t as:
?(t) , Rd[??(t1), ..., ?(tk)?]
where ?t1, ..., tk? is the rule node successor list of
GHKM tree node t.
For Figure 2, the rule node successor list of node
t(viii) is ?t(xi)?. So:
?(t(viii)) = ??ne?, x1, ?pas??[??(t(xi))?]
= ??ne?, x1, ?pas??[??vais??]
= ?ne vais pas?
A similar derivation gives us:
?(t(i)) = ?aujourd?hui , je ne vais pas?
In this way, every GHKM tree encodes a transla-
tion. Given this interpretation of a translation object,
the task of machine translation becomes something
concrete: label the nodes of a parsed source sentence
with a good set of GHKM rules.
3 Probabilistic Approach
To achieve this ?good? labeling of GHKM rules,
we will define a probabilistic generative model P
of GHKM trees, which will serve as our scoring
function. We would like to depart from the stan-
dard probabilistic approach of most phrase-based
translators, which employ very simple probability
models to enable polynomial-time decoding. In-
stead, we will use an alternative probabilistic ap-
proach (an assignment process), which sacrifices
polynomial-time guarantees in favor of a more flexi-
ble and powerful model. This sacrifice of guaranteed
polynomial-time decoding does not entail the sacri-
fice of good running time in practice.
3.1 Assignment Processes
An assignment process builds a sequence of vari-
able assignments (called an assignment history) by
repeatedly iterating the following steps. First, it re-
quests a variable name (say x22) from a so-named
variable generator. It takes this variable name
and the assignment history built so far and com-
presses this information into a set of features (say
{f2, f6, f80}) using a feature function. These fea-
tures are then mapped to a probability distribution by
a function (say p7) requested from a so-named distri-
bution generator. The iteration ends by assigning to
the chosen variable a value (say v4) drawn from this
distribution. In the above running example, the iter-
ation assigns v4 to x22, which was drawn according
to distribution p7({f2, f6, f80}). The process ends
when the variable generator produces the reserved
token STOP instead of a variable name. At this
43
Var Assignment Distribution Features
x23 true p4 {}
x7 ?the? p10 {f12, f102}
x8 blue p2 {f5, f55}
x51 red p2 {f5, f15, f50}
x19 7.29 p5 {f2}
x30 false p4 {f2, f5, f7}
x1 ?man? p10 {f1, f2, f12}
x102 blue p2 {f1, f55, f56}
Figure 3: A example assignment history generated
by an assignment process.
point, the assignment history built so far (like the
example in Figure 3) is returned.
Formally, define a variable signature as a pair
? = ?X, V ?, where X is a set of variable names
and V is a set of values. Define a variable assign-
ment of signature ?X, V ? as a pair ?x, v?, for vari-
able x ? X and value v ? V . Define an assignment
history of signature ? as an ordered list of variable
assignments of ?. The notation H(?) represents the
set of all assignment histories of signature ?.
We define a feature function of signature ? =
?X, V ? as a function f that maps every pair of set
X ?H(?) to a set of assignments (called features)
of an auxiliary variable signature ?f .
We define an assignment process of signature
? = ?X, V ? as a tuple ?f, P, gx, gp?, where: (i) f is
a feature function of ?, (ii) P = {p1, ..., pk} is a fi-
nite set of k functions (called the feature-conditional
distributions) that map each feature set in range(f)
to a probability distribution over V , (iii) gx is a func-
tion (called the variable generator) mapping each
assignment history in the set H(?) to either a vari-
able name in X or the reserved token STOP , and
(iv) gp is a function (called the distribution gener-
ator) mapping each assignment history in the set
H(?) to a positive integer between 1 and k.
An assignment process probabilistically generates
an assignment history of signature ? in the follow-
ing way:
1. h? empty list
2. Do until gx(h) = STOP :
(a) Let x = gx(h) and let j = gp(h).
(b) Draw value v probabilistically from distri-
bution pj(f(x, h)).
(c) Append assignment ?x, v? to history h.
3. Return history h.
3.2 Training
Given all components of an assignment process
of signature ? except for the set P of feature-
conditional distributions, the training task is to learn
P from a training corpus of assignment histories of
signature ?. This can be achieved straightforwardly
by taking the feature vectors generated by a partic-
ular distribution and using them to discriminatively
learn the distribution. For instance, say that our cor-
pus consists of the single history given in Figure ??.
To learn distribution p2, we simply take the three
variable assignments produced by p2 and feed these
feature vectors to a generic discriminative learner.
We prefer learners that produce distributions (rather
than hard classifiers) as output, but this is not re-
quired.
3.3 Decoding
Notice that an assignment process of signature ? in-
duces a probability distribution over the set H(?) of
all assignment histories of ?. The decoding ques-
tion is: given a partial assignment history h, what
is the most probable completion of the history, ac-
cording to this induced distribution? We will use
the natural naive search space for this question. The
nodes of this search space are the assignment his-
tories of H(?). The children of the search node
representing history h are those histories that can be
generated from h in one iteration of the assignment
process. The value of a search node is the proba-
bility of its assignment history (according to the as-
signment process). To decode, we begin at the node
representing history h, and search for the highest-
value descendant that represents a complete assign-
ment history (i.e. an assignment history terminated
by the STOP token).
This is, potentially, a very large and intractible
search space. However, if most assignment deci-
sions can be made with relative confidence, then the
great majority of search nodes have values which
are inferior to those of the best solutions. The
standard search technique of depth-first branch-and-
bound search takes advantage of search spaces with
this particular characteristic by first finding greedy
good-quality solutions and using their values to opti-
mally prune a significant portion of the search space.
44
Figure 4: Partial GHKM tree, after rule nodes have been identified (light gray). Notice that once we identify
the rule node, the rule left-hand sides are already determined.
Depth-first branch-and-bound search has the follow-
ing advantage: it finds a good (suboptimal) solution
in linear time and continually improves on this solu-
tion until it finds the optimal. Thus it can be run ei-
ther as an optimal decoder or as a heuristic decoder,
since we can interrupt its execution at any time to get
the best solution found so far. Additionally, it takes
only linear space to run.
4 Generative Model
We now return to where we left off at the end of Sec-
tion 2, and devise an assignment process that pro-
duces a GHKM tree from an unlabeled parse tree.
This will give us a quality measure that we can use
to produce a ?good? labeling of a given parse tree
with GHKM rules (i.e., the probability of such a la-
beling according to the assignment process).
The simplest assignment process would have a
variable for each node of the parse tree, and these
variables would all be assigned by the same feature-
conditional distribution over the space of all possible
GHKM rules. The problem with such a formulation
is that such a distribution would be inachievably dif-
ficult to learn. We want an assignment process in
which all variables can take only a very small num-
ber of possible values, because it will be much eas-
ier to learn distributions over such variables. This
means we need to break down the process of con-
structing a GHKM rule into simpler steps.
Our assignment process will begin by sequen-
tially assigning a set of boolean variables (which we
will call rule node indicator variables), one for each
node in the parse tree. For parse tree node t, we de-
note its corresponding rule node indicator variable
xrt . Variable xrt is assigned true iff the parse tree
node t will be a rule node in the GHKM tree.
In Figure 3.3, we show a partial GHKM tree af-
ter these assignments are made. The key thing to
observe is that, after this sequence of boolean deci-
sions, the LHS of every rule in the tree is already
determined! To complete the tree, all we need to do
is to fill in their right-hand sides.
Again, we could create variables to do this di-
rectly, i.e. have a variable for each rule whose do-
main is the space of possible right-hand sides for its
established left-hand sides. But this is still a wide-
open decision, so we will break it down further.
For each rule, we will begin by choosing the
template of its RHS, which is a RHS in which
all sequences of variables are replaced with an
empty slot into which variables can later be placed.
For instance, the template of ??ne?, x1, ?pas?? is
??ne?, X, ?pas?? and the template of ?x3, ?,?, x1, x2?
is ?X, ?,?, X?, where X represents the empty slots.
Once the template is chosen, it simply needs to be
filled with the variables from the LHS. To do so, we
process the LHS variables, one by one. By default,
they are placed to the right of the previously placed
variable (the first variable is placed in the first slot).
We repeatedly offer the option to push the variable
to the right until the option is declined or it is no
longer possible to push it further right. If the vari-
able was not pushed right at all, we repeatedly offer
the option to push the variable to the left until the
option is declined or it is no longer possible to push
it further left. Figure 4 shows this generative story
in action for the rule RHS ?x3, ?,?, x1, x2?.
These are all of the decisions we need to make
45
Decision to make Decision RHS so far
RHS template? X , X X , X
default placement of var 1 1 , X
push var 1 right? yes X , 1
default placement of var 2 X , 1 2
push var 2 left? no X , 1 2
default placement of var 3 X , 1 2 3
push var 3 left? yes X , 1 3 2
push var 3 left? yes X , 3 1 2
push var 3 left? yes 3 , 1 2
Figure 5: Trace of the generative story for the right-
hand side of a GHKM rule.
in order to label a parse tree with GHKM rules. No-
tice that, aside from the template decisions, all of the
decisions are binary (i.e. feasible to learn discrimi-
natively). Even the template decisions are not terri-
bly large-domain, if we maintain a separate feature-
conditional distribution for each LHS template. For
instance, if the LHS template is ??not?, X?, then
RHS template ??ne?, X, ?pas?? and a few other se-
lect candidates should bear most of the probability
mass.
5 Evaluation
In this section, we evaluate a preliminary English-
to-German translation system based on the ideas
outlined in this paper. We first present a quantia-
tive comparison with the phrase-based approach, us-
ing the BLEU metric; then we discuss two con-
crete translation examples as a preliminary qualita-
tive evaluation. Finally, we present a detailed man-
ual error analysis.
Our data was a subset of the Europarl corpus con-
sisting of sentences of lengths ranging from 8 to 17
words. Our training corpus contained 50000 sen-
tences and our test corpus contained 300 sentences.
We also had a small number of reserved sentences
for development. The English sentences were parsed
using the Bikel parser (Bikel, 2004), and the sen-
tences were aligned with GIZA++ (Och and Ney,
2000). We used the WEKA machine learning pack-
age (Witten and Frank, 2005) to train the distribu-
tions (specifically, we used model trees).
For comparison, we also trained and evaluated
Pharaoh (Koehn, 2005) on this limited corpus, us-
ing Pharaoh?s default parameters. Pharaoh achieved
a BLEU score of 11.17 on the test set, whereas our
system achieved a BLEU score of 11.52. What is
notable here is not the scores themselves (low due to
the size of the training corpus). However our system
managed to perform comparably with Pharaoh in a
very early stage of its development, with rudimen-
tary features and without the benefit of an n-gram
language model.
Let?s take a closer look at the sentences produced
by our system, to gain some insight as to its current
strengths and weaknesses.
Starting with the English sentence (note that all
data is lowercase):
i agree with the spirit of those amendments .
Our system produces:
ich
I
stimme
vote
die
the.FEM
geist
spirit.MASC
dieser
these
a?nderungsantra?ge
change-proposals
zu
to
.
.
The GHKM tree is depicted in Figure 5. The key
feature of this translation is how the English phrase
?agree with? is translated as the German ?stimme
... zu? construction. Such a feat is difficult to pro-
duce consistently with a purely phrase-based sys-
tem, as phrases of arbitrary length can be placed be-
tween the words ?stimme? and ?zu?, as we can see
happening in this particular example. By contrast,
Pharaoh opts for the following (somewhat less de-
sirable) translation:
ich
I
stimme
vote
mit
with
dem
the.MASC
geist
spirit.MASC
dieser
these
a?nderungsantra?ge
change-proposals
.
.
A weakness in our system is also evident here.
The German noun ?Geist? is masculine, thus our
system uses the wrong article (a problem that
Pharaoh, with its embedded n-gram language model,
does not encounter).
In general, it seems that our system is superior to
Pharaoh at figuring out the proper way to arrange the
words of the output sentence, and inferior to Pharaoh
at finding what the actual translation of those words
should be.
Consider the English sentence:
we shall submit a proposal along these lines before
the end of this year .
46
Figure 6: GHKM tree output for the first test sentence.
Here we have an example of a double verb: ?shall
submit.? In German, the second verb should go at
the end of the sentence, and this is achieved by our
system (translating ?shall? as ?werden?, and ?sub-
mit? as ?vorlegen?).
wir
we
werden
will
eine
a.FEM
vorschlag
proposal.MASC
in
in
dieser
these
haushaltslinien
budget-lines
vor
before
die
the.FEM
ende
end.NEUT
dieser
this.FEM
jahres
year.NEUT
vorlegen
submit
.
.
Pharaoh does not manage this (translating ?sub-
mit? as ?unterbreiten? and placing it mid-sentence).
werden
will
wir
we
unterbreiten
submit
eine
a
vorschlag
proposal
in
in
dieser
these
haushaltslinien
budget-lines
vor
before
ende
end
dieser
this.FEM
jahr
year.NEUT
.
.
It is worth noting that while our system gets the
word order of the output system right, it makes sev-
eral agreement mistakes and (like Pharaoh) doesn?t
get the translation of ?along these lines? right.
To have a more systematic basis for comparison,
we did a manual error analysis for 100 sentences
from the test set. A native speaker of German (in the
present pilot study one of the authors) determined
the editing steps required to transform the system
output into an acceptable translation ? both in terms
of fluency and adequacy of translation. In order to
avoid a bias for our system, we randomized the pre-
sentation of output from one of the two systems.
We defined the following basic types of edits, with
further subdistinctions depending on the word type:
ADD, DELETE, CHANGE and MOVE. A special type
TRANSLATE-untranslated was assumed for untrans-
lated source words in the output. For the CHANGE,
more fine-grained distinctions were made.2 A sin-
gle MOVE operation was assumed to displace an en-
tire phrase; the distance of the movement in terms
of the number of words was calculated. The table in
Figure 7 shows the edits required for correcting the
output of the two systems on 100 sentences.
We again observe that our system, which is at
an early stage of development and contrary to the
Pharaoh system does not include an n-gram lan-
guage model trained on a large corpus, already
yields promising results. The higher proportion
of CHANGE operations, in particular CHANGE-
inflection and CHANGE-function-word edits is pre-
sumably a direct consequence of providing a lan-
guage model or not. An interesting observation is
that our system currently tends to overtranslate, i.e.,
redundantly produce several translations for a word,
which leads to the need of DELETE operations. The
Pharaoh system had a tendency to undertranslate, of-
ten with crucial words missing.
2CHANGE-inflection: keeping the lemma and category the
same, e.g. taken ? takes; CHANGE-part-of-speech: choos-
ing a different derivational form, e.g., judged ? judgement;
CHANGE-function-word: e.g., in ? from; CHANGE-content-
word: e.g., opinion ? consensus.
47
TL-MT Pharaoh
ADD-function-word 40 49
ADD-content-word 17 35
ADD-punctuation 12 13
ADD (total) 69 97
DELETE-function-word 37 18
DELETE-content-word 22 10
DELETE-punctuation 13 15
DELETE-untranslated 2 1
DELETE (total) 74 44
CHANGE-content-word 24 19
CHANGE-function-word 44 26
CHANGE-inflection 101 80
CHANGE-part-of-speech 4 10
CHANGE (total) 173 135
TRANSLATE-untranslated 34 1
MOVE (distance)
1 16 17
2 12 16
3 13 11
4 3 6
? 5 7 5
MOVE (total) 51 55
TOTAL # EDITS 401 332
edits-per-word ratio 0.342 0.295
Figure 7: Edits required for an acceptable system
output, based on 100 test sentences.
6 Discussion
In describing this pilot project, we have attempted
to give a ?big picture? view of the essential ideas
behind our system. To avoid obscuring the presen-
tation, we have avoided many of the implementation
details, in particular our choice of features. There
are exactly four types of decisions that we need to
train: (1) whether a parse tree node should be a rule
node, (2) the RHS template of a rule, (3) whether a
rule variable should be pushed left, and (4) whether
a rule variable should be pushed right. For each of
these decisions, there are a number of possible fea-
tures that suggest themselves. For instance, recall
that in German, typically the second verb of a double
verb (such as ?shall submit? or ?can do?) gets placed
at the end of the sentence or clause. So when the
system is considering whether to push a rule?s noun
phrase to the left, past an existing verb, it would be
useful for it to consider (as a feature) whether that
verb is the first or second verb of its clause.
This system was designed to be very flexible with
the kind of information that it can exploit as fea-
tures. Essentially any aspect of the parse tree, or
of previous decisions that have been taken by the
assignment process, can be used. Furthermore, we
can mark-up the parse tree with any auxiliary infor-
mation that might be beneficial, like noun gender or
verb cases. The current implementation has hardly
begun to explore these possibilities, containing only
features pertaining to aspects of the parse tree.
Even in these early stages of development, the
system shows promise in using syntactic informa-
tion flexibly and effectively for machine translation.
We hope to develop the system into a competitive
alternative to phrase-based approaches.
References
Daniel M. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479?511.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of ACL, pages
263?270.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2003. What?s in a translation rule? In Proc. NAACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings of the Hu-
man Language Technology Conference 2003 (HLT-NAACL
2003), Edmonton, Canada.
Philipp Koehn. 2005. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In Pro-
ceedings of the Sixth Conference of the Association for Ma-
chine Translation in the Americas, pages 115?124.
F. J. Och and H. Ney. 2000. Improved statistical alignment
models. In Proc. ACL, pages 440?447, Hongkong, China,
October.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,
A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, Viren
Jain, Z.Jin, and D. Radev. 2003. Syntax for statistical ma-
chine translation. Technical report, Center for Language and
Speech Processing, Johns Hopkins University, Baltimore.
Summer Workshop Final Report.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical
machine learning tools and techniques. Morgan Kaufmann.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based statis-
tical translation model. In Proceedings of the 39th Annual
Meeting of the Association for Computational Linguistics,
pages 523?530.
48
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 33?40,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Deep Grammars in a Tree Labeling Approach to
Syntax-based Statistical Machine Translation
Mark Hopkins
Department of Linguistics
University of Potsdam, Germany
hopkins@ling.uni-potsdam.de
Jonas Kuhn
Department of Linguistics
University of Potsdam, Germany
kuhn@ling.uni-potsdam.de
Abstract
In this paper, we propose a new syntax-
based machine translation (MT) approach
based on reducing the MT task to a tree-
labeling task, which is further decom-
posed into a sequence of simple decisions
for which discriminative classifiers can be
trained. The approach is very flexible and
we believe that it is particularly well-suited
for exploiting the linguistic knowledge en-
coded in deep grammars whenever possi-
ble, while at the same time taking advantage
of data-based techniques that have proven a
powerful basis for MT, as recent advances in
statistical MT show.
A full system using the Lexical-Functional
Grammar (LFG) parsing system XLE and
the grammars from the Parallel Grammar
development project (ParGram; (Butt et
al., 2002)) has been implemented, and we
present preliminary results on English-to-
German translation with a tree-labeling sys-
tem trained on a small subsection of the Eu-
roparl corpus.
1 Motivation
Machine translation (MT) is probably the oldest ap-
plication of what we call deep linguistic processing
techniques today. But from its inception, there have
been alternative considerations of approaching the
task with data-based statistical techniques (cf. War-
ren Weaver?s well-known memo from 1949). Only
with fairly recent advances in computer technology
have researchers been able to build effective statis-
tical MT prototypes, but in the last few years, the
statistical approach has received enormous research
interest and made significant progress.
The most successful statistical MT paradigm has
been, for a while now, the so-call phrase-based MT
approach (Och and Ney, 2003). In this paradigm,
sentences are translated from a source language to
a target language through the repeated substitution
of contiguous word sequences (?phrases?) from the
source language for word sequences in the target
language. Training of the phrase translation model
builds on top of a standard statistical word alignment
over the training corpus of parallel text (Brown et al,
1993) for identifying corresponding word blocks,
assuming no further linguistic analysis of the source
or target language. In decoding, i.e. the application
of the acquired translation model to unseen source
sentences, these systems then typically rely on n-
gram language models and simple statistical reorder-
ing models to shuffle the phrases into an order that
is coherent in the target language.
An obvious advantage of statistical MT ap-
proaches is that they can adopt (often very id-
iomatic) translations of mid- to high-frequency con-
structions without requiring any language-pair spe-
cific engineering work. At the same time it is clear
that a linguistics-free approach is limited in what
it can ultimately achieve: only linguistically in-
formed systems can detect certain generalizations
from lower-frequency constructions in the data and
successfully apply them in a similar but different lin-
guistic context. Hence, the idea of ?hybrid? MT, ex-
ploiting both linguistic and statistical information is
fairly old. Here we will not consider classical, rule-
based systems with some added data-based resource
acquisition (although they may be among the best
candidates for high-quality special-purpose transla-
tion ? but adaption to new language pairs and sub-
languages is very costly for these systems). The
other form of hybridization ? a statistical MT model
that is based on a deeper analysis of the syntactic
33
structure of a sentence ? has also long been iden-
tified as a desirable objective in principle (consider
(Wu, 1997; Yamada and Knight, 2001)). However,
attempts to retrofit syntactic information into the
phrase-based paradigm have not met with enormous
success (Koehn et al, 2003; Och et al, 2003)1, and
purely phrase-based MT systems continue to outper-
form these syntax/phrase-based hybrids.
In this work, we try to make a fresh start with
syntax-based statistical MT, discarding the phrase-
based paradigm and designing a MT system from
the ground up, using syntax as our central guid-
ing star ? besides the word alignment over a par-
allel corpus. Our approach is compatible with and
can benefit substantially from rich linguistic rep-
resentations obtained from deep grammars like the
ParGram LFGs. Nevertheless, contrary to classi-
cal interlingual or deep transfer-based systems, the
generative stochastic model that drives our system
is grounded only in the cross-language word align-
ment and a surface-based phrase structure tree for
the source language and will thus degrade grace-
fully on input with parsing issues ? which we sus-
pect is an important feature for making the overall
system competitive with the highly general phrase-
based MT approach.
Preliminary evaluation of our nascent system in-
dicates that this new approach might well have the
potential to finally realize some of the promises of
syntax in statistical MT.
2 General Task
We want to build a system that can learn to translate
sentences from a source language to a destination
language. The general set-up is simple.
Firstly, we have a training corpus of paired sen-
tences f and e, where target sentence e is a gold
standard translation of source sentence f . These
sentence pairs are annotated with auxiliary informa-
tion, which can include word alignments and syntac-
tic information. We refer to these annotated sentence
pairs as complete translation objects.
Secondly, we have an evaluation corpus of source
sentences. These sentences are annotated with a sub-
set of the auxiliary information used to annotate the
1(Chiang, 2005) also reports that with his hierarchical gen-
eralization of the phrase-based approach, the addition of parser
information doesn?t lead to any improvements.
Figure 1: Example translation object.
training corpus. We refer to these partially annotated
source sentences as partial translation objects.
The task at hand: use the training corpus to learn
a procedure, through which we can successfully in-
duce a complete translation object from a partial
translation object. This is what we will define as
translation.
3 Specific Task Addressed by this Paper
Before going on to actually describe a translation
procedure (and how to induce it), we need to spec-
ify our prior assumptions about how the translation
objects will be annotated. For this paper, we want to
exploit the syntax information that we can gain from
an LFG-parser, hence we will assume the following
annotations:
(1) In the training and evaluation corpora, the
source sentences will be parsed with the XLE-
parser. The attribute-value information from LFG?s
f-structure is restructured so it is indexed by (c-
structure) tree nodes; thus a tree node can bear mul-
tiple labels for various pieces of morphological, syn-
tactic and semantic information.
(2) In the training corpus, the source and target
sentence of every translation object will be aligned
using GIZA++ (http://www.fjoch.com/).
In other words, our complete translation objects
will be aligned tree-string pairs (for instance, Fig-
ure 1), while our partial translation objects will be
trees (the tree portion of Figure 1). No other annota-
tions will be assumed for this paper.
34
Figure 2: GHKM tree equivalent of example translation object. The light gray nodes are rule nodes of the
GHKM tree.
4 Syntax MT as Tree Labeling
It is not immediately clear how one would learn a
process to map a parsed source sentence into an
aligned tree-string pair. To facilitate matters, we
will map complete translation objects to an alternate
representation. In (Galley et al, 2003), the authors
give a semantics to aligned tree-string pairs by asso-
ciating each with an annotated parse tree (hereafter
called a GHKM tree) representing a specific theory
about how the source sentence was translated into
the destination sentence.
In Figure 1, we show an example translation ob-
ject and in Figure 2, we show its associated GHKM
tree. The GHKM tree is simply the parse tree f of
the translation object, annotated with rules (hereafter
referred to as GHKM rules). We will not describe in
depth the mapping process from translation object to
GHKM tree. Suffice it to say that the alignment in-
duces a set of intuitive translation rules. Essentially,
a rule like: ?not 1 ? ne 1 pas? (see Figure 2) means:
if we see the word ?not? in English, followed by a
phrase already translated into French, then translate
the entire thing as the word ?ne? + the translated
phrase + the word ?pas.? A parse tree node gets la-
beled with one of these rules if, roughly speaking,
its span is still contiguous when projected (via the
alignment) into the target language.
The advantage of using the GHKM interpretation
of a complete translation object is that our transla-
tion task becomes simpler. Now, to induce a com-
plete translation object from a partial translation ob-
ject (parse tree), all we need to do is label the nodes
of the tree with appropriate rules. We have reduced
the vaguely defined task of translation to the con-
crete task of tree labeling.
5 The Generative Process
At the most basic level, we could design a naive gen-
erative process that takes a parse tree and then makes
a series of decisions, one for each node, about what
rule (if any) that node should be assigned. How-
ever it is a bit unreasonable to expect to learn such
a decision without breaking it down somewhat, as
there are an enormous number of rules that could po-
tentially be used to label any given parse tree node.
So let?s break this task down into simpler decisions.
Ideally, we would like to devise a generative process
consisting of decisions between a small number of
possibilities (ideally binary decisions).
We will begin by deciding, for each node, whether
or not it will be annotated with a rule. This is clearly
a binary decision. Once a generative process has
made this decision for each node, we get a conve-
nient byproduct. As seen in Figure 3, the LHS of
each rule is already determined. Hence after this se-
quence of binary decisions, half of our task is al-
ready completed.
The question remains: how do we determine the
RHS of these rules? Again, we could create a gen-
erative process that makes these decisions directly,
but choosing the RHS of a rule is still a rather wide-
open decision, so we will break it down further. For
each rule, we will begin by choosing the template of
its RHS, which is a RHS in which all sequences of
variables are replaced with an empty slot into which
variables can later be placed. For instance, the tem-
35
Figure 3: Partial GHKM tree, after rule nodes have been identified (light gray). Notice that once we identify
the rule node, the rule left-hand sides are already determined.
plate of ??ne?, x1, ?pas?? is ??ne?,X, ?pas?? and the
template of ?x3, ?,?, x1, x2? is ?X, ?,?,X?, where X
represents the empty slots.
Once the template is chosen, it simply needs to be
filled with the variables from the LHS. To do so, we
process the LHS variables, one by one. By default,
they are placed to the right of the previously placed
variable (the first variable is placed in the first slot).
We repeatedly offer the option to push the variable
to the right until the option is declined or it is no
longer possible to push it further right. If the vari-
able was not pushed right at all, we repeatedly offer
the option to push the variable to the left until the
option is declined or it is no longer possible to push
it further left. Figure 4 shows this generative story
in action for the rule RHS ?x3, ?,?, x1, x2?.
These are all of the decisions we need to make
in order to label a parse tree with GHKM rules. A
trace of this generative process for the GHKM tree
of Figure 2 is shown in Figure 5. Notice that, aside
from the template decisions, all of the decisions are
binary (i.e. feasible to learn discriminatively). Even
the template decisions are not terribly large-domain,
if we maintain a separate feature-conditional dis-
tribution for each LHS template. For instance, if
the LHS template is ??not?,X?, then RHS template
??ne?,X, ?pas?? and a few other select candidates
should bear most of the probability mass.
5.1 Training
Having established this generative story, training is
straightforward. As a first step, we can convert each
complete translation object of our training corpus
to the trace of its generative story (as in Figure 5).
Decision to make Decision RHS so far
RHS template? X , X X , X
default placement of var 1 1 , X
push var 1 right? yes X , 1
default placement of var 2 X , 1 2
push var 2 left? no X , 1 2
default placement of var 3 X , 1 2 3
push var 3 left? yes X , 1 3 2
push var 3 left? yes X , 3 1 2
push var 3 left? yes 3 , 1 2
Figure 4: Trace of the generative story for the right-
hand side of a GHKM rule.
These decisions can be annotated with whatever fea-
ture information we might deem helpful. Then we
simply divide up these feature vectors by decision
type (for instance, rule node decisions, template de-
cisions, etc.) and train a separate discriminative clas-
sifier for each decision type from the feature vectors.
This method is quite flexible, in that it allows us to
use any generic off-the-shelf classification software
to train our system. We prefer learners that produce
distributions (rather than hard classifiers) as output,
but this is not required.
5.2 Exploiting deep linguistic information
The use of discriminative classifiers makes our ap-
proach very flexible in terms of the information that
can be exploited in the labeling (or translation) pro-
cess. Any information that can be encoded as fea-
tures relative to GHKM tree nodes can be used. For
the experiments reported in this paper, we parsed
the source language side of a parallel corpus (a
small subsection of the English-German Europarl
corpus; (Koehn, 2002)) with the XLE system, using
36
the ParGram LFG grammar and applying probabilis-
tic disambiguation (Riezler et al, 2002) to obtain
a single analysis (i.e., a c-structure [phrase struc-
ture tree] and an f-structure [an associated attribute-
value matrix with morphosyntactic feature informa-
tion and a shallow semantic interpretation]) for each
sentence. A fall-back mechanism integrated in the
parser/grammar ensures that even for sentences that
do not receive a full parse, substrings are deeply
parsed and can often be treated successfully.
We convert the c-structure/f-structure represen-
tation that is based on XLE?s sophisticated word-
internal analysis into a plain phrase structure tree
representation based on the original tokens in the
source language string. The morphosyntactic fea-
ture information from f-structure is copied as addi-
tional labeling information to the relevant GHKM
tree nodes, and the f-structural dependency relation
among linguistic units is translated into a relation
among corresponding GHKM tree nodes. The rela-
tional information is then used to systematically ex-
tend the learning feature set for the tree-node based
classifiers.
In future experiments, we also plan to exploit lin-
guistic knowledge about the target language by fac-
torizing the generation of target language words into
separate generation of lemmas and the various mor-
phosyntactic features. In decoding, a morphological
generator will be used to generate a string of surface
words.
5.3 Decoding
Because we have purposely refused to make any
Markov assumptions in our model, decoding cannot
be accomplished in polynomial time. Our hypothe-
sis is that it is better to find a suboptimal solution of
a high-quality model than the optimal solution of a
poorer model. We decode through a simple search
through the space of assignments to our generative
process.
This is, potentially, a very large and intractible
search space. However, if most assignment deci-
sions can be made with relative confidence (i.e. the
classifiers we have trained make fairly certain deci-
sions), then the great majority of search nodes have
values which are inferior to those of the best so-
lutions. The standard search technique of depth-
first branch-and-bound search takes advantage of
search spaces with this particular characteristic by
first finding greedy good-quality solutions and using
their values to optimally prune a significant portion
of the search space. Depth-first branch-and-bound
search has the following advantage: it finds a good
(suboptimal) solution in linear time and continually
improves on this solution until it finds the optimal.
Thus it can be run either as an optimal decoder or as
a heuristic decoder, since we can interrupt its execu-
tion at any time to get the best solution found so far.
Additionally, it takes only linear space to run.
6 Preliminary results
In this section, we present some preliminary results
for an English-to-German translation system based
on the ideas outlined in this paper.
Our data was a subset of the Europarl corpus
consisting of sentences of lengths ranging from 8
to 17 words. Our training corpus contained 50000
sentences and our test corpus contained 300 sen-
tences. We also had a small number of reserved
sentences for development. The English sentences
were parsed with XLE, using the English ParGram
LFG grammar, and the sentences were word-aligned
with GIZA++. We used the WEKA machine learn-
ing package (Witten and Frank, 2005) to train the
distributions (specifically, we used model trees).
For comparison, we also trained and evaluated
the phrase-based MT system Pharaoh (Koehn, 2005)
on this limited corpus, using Pharaoh?s default pa-
rameters. In a different set of MT-as-Tree-Labeling
experiments, we used a standard treebank parser
trained on the PennTreebank Wall Street Journal
section. Even with this parser, which produces less
detailed information than XLE, the results are com-
petitive when assessed with quantitative measures:
Pharaoh achieved a BLEU score of 11.17 on the test
set, whereas our system achieved a BLEU score of
11.52. What is notable here is not the scores them-
selves (low due to the size of the training corpus).
However our system managed to perform compara-
bly with Pharaoh in a very early stage of its devel-
opment, with rudimentary features and without the
benefit of an n-gram language model.
For the XLE-based system we cannot include
quantitative results for the same experimental setup
at the present time. As a preliminary qualitative
37
Decision to make Decision Active features
rule node (i)? YES NT=?S?; HEAD = ?am?
rule node (ii)? YES NT=?NP?; HEAD = ?I?
rule node (iv)? NO NT=?VP?; HEAD = ?am?
rule node (v)? YES NT=?VP?; HEAD = ?am?
rule node (vi)? NO NT=?MD?; HEAD = ?am?
rule node (viii)? YES NT=?VP?; HEAD = ?going?
rule node (ix)? NO NT=?RB?; HEAD = ?not?
rule node (xi)? YES NT=?VB?; HEAD = ?going?
rule node (xiii)? YES NT=?ADJP?; HEAD = ?today?
RHS template? (i) X , X NT=?S?
push var 1 right? (i) YES VARNT=?NP?; PUSHPAST= ?,?
push var 2 left? (i) NO VARNT=?VP?; PUSHPAST= ?NP?
push var 3 left? (i) YES VARNT=?ADJP?; PUSHPAST= ?VP?
push var 3 left? (i) YES VARNT=?ADJP?; PUSHPAST= ?NP?
push var 3 left? (i) YES VARNT=?ADJP?; PUSHPAST= ?,?
RHS template? (ii) je NT=?NP?; WD=?I?
RHS template? (v) X NT=?VP?
RHS template? (viii) ne X pas NT=?VP?; WD=?not?
RHS template? (xi) vais NT=?VB?; WD=?going?
RHS template? (xiii) aujourd?hui NT=?ADJP?; WD=?today?
Figure 5: Trace of a top-down generative story for the GHKM tree in Figure 2.
evaluation, let?s take a closer look at the sentences
produced by our system, to gain some insight as to
its current strengths and weaknesses.
Starting with the English sentence (1) (note that
all data is lowercase), our system produces (2).
(1) i agree with the spirit of those amendments .
(2) ich
I
stimme
vote
die
the.FEM
geist
spirit.MASC
dieser
these
a?nderungsantra?ge
change-proposals
zu
to
.
.
The GHKM tree is depicted in Figure 6. The key
feature of this translation is how the English phrase
?agree with? is translated as the German ?stimme
... zu? construction. Such a feat is difficult to pro-
duce consistently with a purely phrase-based sys-
tem, as phrases of arbitrary length can be placed be-
tween the words ?stimme? and ?zu?, as we can see
happening in this particular example. By contrast,
Pharaoh opts for the following (somewhat less de-
sirable) translation:
(3) ich
I
stimme
vote
mit
with
dem
the.MASC
geist
spirit.MASC
dieser
these
a?nderungsantra?ge
change-proposals
.
.
A weakness in our system is also evident here.
The German noun ?Geist? is masculine, thus our
system uses the wrong article (a problem that
Pharaoh, with its embedded n-gram language model,
does not encounter).
In general, it seems that our system is superior to
Pharaoh at figuring out the proper way to arrange the
words of the output sentence, and inferior to Pharaoh
at finding what the actual translation of those words
should be.
Consider the English sentence (4). Here we have
an example of a modal verb with an embedded in-
finitival VP. In German, infinite verbs should go at
the end of the sentence, and this is achieved by our
system (translating ?shall? as ?werden?, and ?sub-
mit? as ?vorlegen?), as is seen in (5).
(4) ) we shall submit a proposal along these lines before the
end of this year .
(5) wir
we
werden
will
eine
a.FEM
vorschlag
proposal.MASC
in
in
dieser
these
haushaltslinien
budget-lines
vor
before
die
the.FEM
ende
end.NEUT
dieser
this.FEM
jahres
year.NEUT
vorlegen
submit
.
.
Pharaoh does not manage this (translating ?sub-
mit? as ?unterbreiten? and placing it mid-sentence).
(6) werden
will
wir
we
unterbreiten
submit
eine
a
vorschlag
proposal
in
in
dieser
these
haushaltslinien
budget-lines
vor
before
ende
end
dieser
this.FEM
jahr
year.NEUT
.
.
It is worth noting that while our system gets the
word order of the output system right, it makes sev-
38
Figure 6: GHKM tree output for a test sentence.
eral agreement mistakes and (like Pharaoh) doesn?t
get the translation of ?along these lines? right.
In Figure 7, we show sample translations by the
three systems under discussion for the first five sen-
tences in our evaluation set. For the LFG-based ap-
proach, we can at this point present only results for
a version trained on 10% of the sentence pairs. This
explains why more source words are left untrans-
lated. But note that despite the small training set,
the word ordering results are clearly superior for this
system: the syntax-driven rules place the untrans-
lated English words in the correct position in terms
of German syntax.
The translations with Pharaoh contain relatively
few agreement mistakes (note that it exploits a lan-
guage model of German trained on a much larger
corpus). The phrase-based approach does however
skip words and make positioning mistakes some of
which are so serious (like in the last sentence) that
they make the result hard to understand.
7 Discussion
In describing this pilot project, we have attempted
to give a ?big picture? view of the essential ideas
behind our system. To avoid obscuring the presen-
tation, we have avoided many of the implementation
details, in particular our choice of features. There
are exactly four types of decisions that we need to
train: (1) whether a parse tree node should be a rule
node, (2) the RHS template of a rule, (3) whether a
rule variable should be pushed left, and (4) whether
a rule variable should be pushed right. For each
of these decisions, there are a number of possible
features that suggest themselves. For instance, re-
call that in German, embedded infinitival verbs get
placed at the end of the sentence or clause. So
when the system is considering whether to push a
rule?s noun phrase to the left, past an existing verb,
it would be useful for it to consider (as a feature)
whether that verb is the first or second verb of its
clause and what the morphological form of the verb
is.
Even in these early stages of development, the
MT-as-Tree-Labeling system shows promise in us-
ing syntactic information flexibly and effectively for
MT. Our preliminary comparison indicates that us-
ing deep syntactic analysis leads to improved trans-
lation behavior. We hope to develop the system
into a competitive alternative to phrase-based ap-
proaches.
References
P.F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mer-
cer. 1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguistics,
19(2):263?311.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hiroshi Ma-
suichi, and Christian Rohrer. 2002. The parallel gram-
39
source we believe that this is a fundamental element .
professional translation wir denken , dass dies ein grundlegender aspekt ist .
PHARAOH (50k) wir halten dies fu?r
:::
eine
::::::::::::
grundlegende element .
TL-WSJ (50k) wir glauben , dass
:::::
diesen ist ein grundlegendes element .
TL-LFG (5k) wir meinen , dass dies
:::
eine
::::::::::::
grundlegende element ist .
source it is true that lisbon is a programme for ten years .
professional translation nun ist lissabon ein programm fu?r zehn jahre .
PHARAOH (50k) es ist richtig , dass lissabon ist
:::
eine programm fu?r zehn
:::::
jahren .
TL-WSJ (50k) es ist richtig , dass lissabon ist
:::
eine programm fu?r zehn
:::::
jahren .
TL-LFG (5k) es ist true , dass lisbon
:::
eine programm fu?r zehn
:::::
jahren ist .
source i completely agree with each of these points .
professional translation ich bin mit jeder einzelnen dieser aussagen voll und ganz einverstanden .
PHARAOH (50k) ich ..... vo?llig einverstanden mit jedem dieser punkte .
TL-WSJ (50k) ich bin vo?llig mit
::::
jedes
:::::
diese fragen einer meinung .
TL-LFG (5k) ich agree completely mit
::::
jeder dieser punkte .
source however , i would like to add one point .
professional translation aber ich mo?chte gern einen punkt hinzufu?gen .
PHARAOH (50k) allerdings mo?chte ich noch eines sagen .
TL-WSJ (50k) ich mo?chte jedoch an noch einen punkt hinzufu?gen .
TL-LFG (5k) allerdings mo?chte ich einen punkt add .
source this is undoubtedly a point which warrants attention .
professional translation ohne jeden zweifel ist dies ein punkt , der aufmerksamkeit verdient .
PHARAOH (50k) das ist sicherlich
:::
eine punkt .... rechtfertigt das aufmerksamkeit .
TL-WSJ (50k) das ist ohne zweifel
::::
eine punkt ,
::
die warrants beachtung .
TL-LFG (5k) das ist undoubtedly .... sache , die attention warrants .
Figure 7: Sample translations by (1) the PHARAOH system, (2) our system with a treebank parser (TL-WSJ),
(3) our system with the XLE parser (TL-LFG). (1) and (2) were trained on 50,000 sentence pairs, (3) just
on (3) sentence pairs. Error coding:
::::::
wrong
:::::::::::::::
morphological
:::::
form, incorrectly positioned word, untranslated
source word, missed word: ...., extra word.
mar project. In Proceedings of COLING-2002 Workshop on
Grammar Engineering and Evaluation, pages 1?7.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of ACL, pages
263?270.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2003. What?s in a translation rule? In Proc. NAACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings of the Hu-
man Language Technology Conference 2003 (HLT-NAACL
2003), Edmonton, Canada.
Philipp Koehn. 2002. Europarl: A multilingual corpus for eval-
uation of machine translation. Ms., University of Southern
California.
Philipp Koehn. 2005. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In Pro-
ceedings of the Sixth Conference of the Association for Ma-
chine Translation in the Americas, pages 115?124.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,
A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, Viren
Jain, Z.Jin, and D. Radev. 2003. Syntax for statistical ma-
chine translation. Technical report, Center for Language and
Speech Processing, Johns Hopkins University, Baltimore.
Summer Workshop Final Report.
Stefan Riezler, Dick Crouch, Ron Kaplan, Tracy King, John
Maxwell, and Mark Johnson. 2002. Parsing the Wall Street
Journal using a Lexical-Functional Grammar and discrim-
inative estimation techniques. In Proceedings of the 40th
Annual Meeting of the Association for Computational Lin-
guistics (ACL?02), Pennsylvania, Philadelphia.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical
machine learning tools and techniques. Morgan Kaufmann.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based statis-
tical translation model. In Proceedings of the 39th Annual
Meeting of the Association for Computational Linguistics,
pages 523?530.
40
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 12?20,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Data-Driven Dependency Parsing of New Languages
Using Incomplete and Noisy Training Data
Kathrin Spreyer and Jonas Kuhn
Department of Linguistics
University of Potsdam, Germany
{spreyer,kuhn}@ling.uni-potsdam.de
Abstract
We present a simple but very effective ap-
proach to identifying high-quality data in
noisy data sets for structured problems like
parsing, by greedily exploiting partial struc-
tures. We analyze our approach in an anno-
tation projection framework for dependency
trees, and show how dependency parsers from
two different paradigms (graph-based and
transition-based) can be trained on the result-
ing tree fragments. We train parsers for Dutch
to evaluate our method and to investigate
to which degree graph-based and transition-
based parsers can benefit from incomplete
training data. We find that partial correspon-
dence projection gives rise to parsers that out-
perform parsers trained on aggressively fil-
tered data sets, and achieve unlabeled attach-
ment scores that are only 5% behind the aver-
age UAS for Dutch in the CoNLL-X Shared
Task on supervised parsing (Buchholz and
Marsi, 2006).
1 Introduction
Many weakly supervised approaches to NLP rely on
heuristics or filtering techniques to deal with noise
in unlabeled or automatically labeled training data,
e.g., in the exploitation of parallel corpora for cross-
lingual projection of morphological, syntactic or se-
mantic information. While heuristic approaches can
implement (linguistic) knowledge that helps to de-
tect noisy data (e.g., Hwa et al (2005)), they are typ-
ically task- and language-specific and thus introduce
a component of indirect supervision. Non-heuristic
filtering techniques, on the other hand, employ re-
liability measures (often unrelated to the task) to
predict high-precision data points (e.g., Yarowsky
et al (2001)). In order to reach a sufficient level
of precision, filtering typically has to be aggressive,
especially for highly structured tasks like parsing.
Such aggressive filtering techniques incur massive
data loss and enforce trade-offs between the quality
and the amount of usable data.
Ideally, a general filtering strategy for weakly su-
pervised training of structured analysis tools should
eliminate noisy subparts in the automatic annota-
tion without discarding its high-precision aspects;
thereby data loss would be kept to a minimum.
In this paper, we propose an extremely simple ap-
proach to noise reduction which greedily exploits
partial correspondences in a parallel corpus, i.e.,
correspondences potentially covering only substruc-
tures of translated sentences. We implemented this
method in an annotation projection framework to
create training data for two dependency parsers rep-
resenting different parsing paradigms: The MST-
Parser (McDonald et al, 2005) as an instance of
graph-based dependency parsing, and the Malt-
Parser (Nivre et al, 2006) to represent transition-
based dependency parsing. In an empirical evalu-
ation, we investigate how they react differently to
incomplete and noisy training data.
Despite its simplicity, the partial correspondence
approach proves very effective and leads to parsers
that achieve unlabeled attachment scores that are
only 5% behind the average UAS for Dutch in the
CoNLL-X Shared Task (Buchholz and Marsi, 2006).
After a summary of related work in Sec. 2, we
discuss dependency tree projection (Sec. 3) and par-
tial correspondence (Sec. 4). In Sec. 5, we give an
overview of graph- and transition-based dependency
parsing and describe how each can be adapted for
training on partial training data in Sec. 6. Experi-
mental results are presented in Sec. 7, followed by
an analysis in Sec. 8. Sec. 9 concludes.
12
a. b. c.
English (L1): I have two questions You are absolutely right You are absolutely right
Dutch (L2): Ik heb twee vragen U heeft volkomen gelijk U heeft volkomen gelijk
1
2 3
Figure 1: Dependency tree projection from English to Dutch. (a) Ideal scenario with bidirectional alignments. (b)
Projection fails due to weak alignments. (c) Constrained fallback projection.
2 Related Work
Annotation projection has been applied to many dif-
ferent NLP tasks. On the word or phrase level, these
include morphological analysis, part-of-speech tag-
ging and NP-bracketing (Yarowsky et al, 2001),
temporal analysis (Spreyer and Frank, 2008), or se-
mantic role labeling (Pado? and Lapata, 2006). In
these tasks, word labels can technically be intro-
duced in isolation, without reference to the rest of
the annotation. This means that an aggressive filter
can be used to discard unreliable data points (words
in a sentence) without necessarily affecting high-
precision data points in the same sentence. By us-
ing only the bidirectional word alignment links, one
can implement a very robust such filter, as the bidi-
rectional links are generally reliable, even though
they have low recall for overall translational cor-
respondences (Koehn et al, 2003). The bidirec-
tional alignment filter is common practice (Pado? and
Lapata, 2006); a similar strategy is to discard en-
tire sentences with low aggregated alignment scores
(Yarowsky et al, 2001).
On the sentence level, Hwa et al (2005) were
the first to project dependency trees from English
to Spanish and Chinese. They identify unreliable
target parses (as a whole) on the basis of the num-
ber of unaligned or over-aligned words. In addition,
they manipulate the trees to accommodate for non-
isomorphic sentences. Systematic non-parallelisms
between source and target language are then ad-
dressed by hand-crafted rules in a post-projection
step. These rules account for an enormous increase
in the unlabeled f-score of the direct projections,
from 33.9 to 65.7 for Spanish and from 26.3 to 52.4
for Chinese. But they need to be designed anew for
every target language, which is time-consuming and
requires knowledge of that language.
Research in the field of unsupervised and weakly
supervised parsing ranges from various forms of EM
training (Pereira and Schabes, 1992; Klein and Man-
ning, 2004; Smith and Eisner, 2004; Smith and Eis-
ner, 2005) over bootstrapping approaches like self-
training (McClosky et al, 2006) to feature-based
enhancements of discriminative reranking models
(Koo et al, 2008) and the application of semi-
supervised SVMs (Wang et al, 2008). The partial
correspondence method we present in this paper is
compatible with such approaches and can be com-
bined with other weakly supervised machine learn-
ing schemes. Our approach is similar to that of
Clark and Curran (2006) who use partial training
data (CCG lexical categories) for domain adaptation;
however, they assume an existing CCG resource for
the language in question to provide this data.
3 Projection of Dependency Trees
Most state-of-the-art parsers for natural languages
are data-driven and depend on the availability of suf-
ficient amounts of labeled training data. However,
manual creation of treebanks is time-consuming and
labour-intensive. One way to avoid the expensive
annotation process is to automatically label the train-
ing data using annotation projection (Yarowsky et
al., 2001): Given a suitable resource (such as a
parser) in language L1, and a word-aligned paral-
lel corpus with languages L1 and L2, label the L1-
portion of the parallel text (with the parser) and copy
the annotations to the corresponding (i.e., aligned)
elements in language L2. This is illustrated in Fig.
1a. The arrows between English and Dutch words
indicate the word alignment. Assuming we have a
parser to produce the dependency tree for the En-
glish sentence, we build the tree for the Dutch sen-
tence by establishing arcs between words wD (e.g.,
Ik) and hD (heb) if there are aligned pairs (wD, wE)
13
#sents w/ avg. sent vocab
projected parse length (lemma)
unfiltered (100,000) 24.92 19,066
bidirectional 2,112 6.39 1,905
fallback 6,426 9.72 4,801
bi+frags?3 7,208 9.44 4,631
Table 1: Data reduction effect of noise filters.
(Ik and I) and (hD, hE) (heb and have) such that hE
is the head of wE in the English tree.
Annotation projection assumes direct correspon-
dence (Hwa et al, 2005) between languages (or
annotations), which?although it is valid in many
cases?does not hold in general: non-parallelism
between corresponding expressions in L1 and L2
causes errors in the target annotations. The word
alignment constitutes a further source for errors if it
is established automatically?which is typically the
case in large parallel corpora.
We have implemented a language-independent
framework for dependency projection and use the
Europarl corpus (Koehn, 2005) as the parallel text.
Europarl consists of the proceedings of the Euro-
pean Parliament, professionally translated in 11 lan-
guages (approx. 30mln words per language). The
data was aligned on the word level with GIZA++
(Och and Ney, 2003).1 In the experiments reported
here, we use the language pair English-Dutch, with
English as the source for projection (L1) and Dutch
as L2. The English portion of the Europarl cor-
pus was lemmatized and POS tagged with the Tree-
Tagger (Schmid, 1994) and then parsed with Malt-
Parser (which is described in Sec. 6), trained on a
dependency-converted version of the WSJ part from
the Penn Treebank (Marcus et al, 1994), but with
the automatic POS tags. The Dutch sentences were
only POS tagged (with TreeTagger).2
3.1 Data Loss Through Filtering
We quantitatively assess the impact of various fil-
tering techniques on a random sample of 100,000
English-Dutch sentence pairs from Europarl (avg.
1Following standard practice, we computed word align-
ments in both directions (L1 ? L2 and L2 ? L1); this gives
rise to two unidirectional alignments. The bidirectional align-
ment is the intersection of the two unidirectional ones.
2The Dutch POS tags are used to train the monolingual
parsers from the projected dependency trees (Sec. 7).
24.9 words/sentence). The English dependency
trees are projected to their Dutch counterparts as ex-
plained above for Fig. 1a.
The first filter we examine is the one that consid-
ers exclusively bidirectional alignments. It admits
dependency arcs to be projected only if the head hE
and the dependent wE are each aligned bidirection-
ally with some word in the Dutch sentence. This is
indicated in Fig. 1b, where the English verb are is
aligned with the Dutch translation heeft only in one
direction. This means that none of the dependencies
involving are are projected, and the projected struc-
ture is not connected. We will discuss in subsequent
sections how less restricted projection methods can
still incorporate such data.
Table 1 shows the quantitative effect of the bidi-
rectional filter in the row labeled ?bidirectional?. The
proportion of usable sentences is reduced to 2.11%.
Consequently, the vocabulary size diminishes by a
factor of 10, and the average sentence length drops
considerably from almost 25 to less than 7 words,
suggesting that most non-trivial examples are lost.
3.2 Constrained Fallback Projection
As an instance of a more relaxed projection of com-
plete structures, we also implemented a fallback to
unidirectional links which projects further depen-
dencies after a partial structure has been built based
on the more reliable bidirectional links. That is, the
dependencies established via unidirectional align-
ments are constrained by the existing subtrees, and
are subject to the wellformedness conditions for de-
pendency trees.3 Fig. 1c shows how the fallback
mechanism, initialized with the unconnected struc-
ture built with the bidirectional filter, recovers a
parse tree for the weakly aligned sentence pair in
Fig. 1b. Starting with the leftmost word in the Dutch
sentence and its English translation (U and You),
there is a unidirectional alignment for the head of
You: are is aligned to heeft, so U is established as
a dependent of heeft via fallback. Likewise, heeft
can now be identified as the root node. Note that the
(incorrect) alignment between heeft and You will not
be pursued because it would lead to heeft being a de-
pendent of itself and thus violating the wellformed-
3I.e., single headedness and acyclicity; we do not require the
trees to be projective, but instead train pseudo-projective models
(Nivre and Nilsson, 2005) on the projected data (cf. fn. 5).
14
#frags 1 2 3 4?15 >15
#words
<4 425 80 12 ? ?
4?9 1,331 1,375 1,567 4,793 ?
10?19 339 859 1,503 27,910 522
20?30 17 45 143 20,756 10,087
>30 0 5 5 4,813 23,362
Table 2: Fragmented parses projected with the alignment
filter. The sentences included in the data set ?bi+frags?3?
are in boldface.
ness conditions. Finally, the subtree rooted in gelijk
is incorporated as the second dependent of heeft.
As expected, the proportion of examples that pass
this filter rises, to 6.42% (Table 1, ?fallback?). How-
ever, we will see in Sec. 7 that parsers trained on
this data do not improve over parsers trained on the
bidirectionally aligned sentences alone. This is pre-
sumably due to the noise that inevitably enters the
training data through fallback.
4 Partial Correspondence Projection
So far, we have only considered complete trees,
i.e., projected structures with exactly one root node.
This is a rather strict requirement, given that even
state-of-the-art parsers sometimes fail to produce
plausible complete analyses for long sentences, and
that non-sentential phrases such as complex noun
phrases still contain valuable, non-trivial informa-
tion. We therefore propose partial correspondence
projection which, in addition to the complete anno-
tations produced by tree-oriented projection, yields
partial structures: It admits fragmented analyses in
case the tree-oriented projection cannot construct a
complete tree. Of course, the nature of those frag-
ments needs to be restricted so as to exclude data
with no (interesting) dependencies. E.g., a sentence
of five words with a parse consisting of five frag-
ments provides virtually no information about de-
pendency structure. Hence, we impose a limit (fixed
at 3 after quick preliminary tests on automatically
labeled development data) on the number of frag-
ments that can make up an analysis. Alternatively,
one could require a minimum fragment size.
As an example, consider again Fig. 1b. This ex-
ample would be discarded in strict tree projection,
but under partial correspondence it is included as a
partial analysis consisting of three fragments:
U heeft volkomen gelijk
Although the amount of information provided in
this analysis is limited, the arc between gelijk and
volkomen, which is strongly supported by the align-
ment, can be established without including poten-
tially noisy data points that are only weakly aligned.
We use partial correspondence in combination
with bidirectional projection.4 As can be seen in
Table 1 (?bi+frags?3?), this combination boosts the
amount of usable data to a range similar to that of
the fallback technique for trees; but unlike the latter,
partial correspondence continues to impose a high-
precision filter (bidirectionality) while improving re-
call through relaxed structural requirements (partial
correspondence). Table 2 shows how fragment size
varies with sentence length.
5 Data-driven Dependency Parsing
Models for data-driven dependency parsing can be
roughly divided into two paradigms: Graph-based
and transition-based models (McDonald and Nivre,
2007).
5.1 Graph-based Models
In the graph-based approach, global optimization
considers all possible arcs to find the tree T? s.t.
T? = argmax
T?D
s(T ) = argmax
T?D
?
(i,j,l)?AT
s(i, j, l)
where D is the set of all well-formed dependency
trees for the sentence, AT is the set of arcs in T , and
s(i, j, l) is the score of an arc between words wi and
wj with label l. The specific graph-based parser we
use in this paper is the MSTParser of McDonald et
al. (2005). The MSTParser learns the scoring func-
tion s using an online learning algorithm (Crammer
and Singer, 2003) which maximizes the margin be-
tween T? and D \ {T?}, based on a loss function that
counts the number of words with incorrect parents
relative to the correct tree.
5.2 Transition-based Models
In contrast to the global optimization employed in
graph-based models, transition-based models con-
struct a parse tree in a stepwise way: At each point,
4Fragments from fallback projection turned out not to be
helpful as training data for dependency parsers.
15
the locally optimal parser action (transition) t? is de-
termined greedily on the basis of the current config-
uration c (previous actions plus local features):
t? = argmax
t?T
s(c, t)
where T is the set of possible transitions. As a rep-
resentative of the transition-based paradigm, we use
the MaltParser (Nivre et al, 2006). It implements in-
cremental, deterministic parsing algorithms and em-
ploys SVMs to learn the transition scores s.
6 Parsing with Fragmented Trees
To make effective use of the fragmented trees pro-
duced by partial correspondence projection, both
parsing approaches need to be adapted for training
on sentences with unconnected substructures. Here
we briefly discuss how we represent these structures,
and then describe how we modified the parsers.
We use the CoNLL-X data format for dependency
trees (Buchholz and Marsi, 2006) to encode partial
structures. Specifically, every fragment root spec-
ifies as its head an artificial root token w0 (distin-
guished from a true root dependency by a special
relation FRAG). Thus, sentences with a fragmented
parse are still represented as a single sentence, in-
cluding all words; the difference from a fully parsed
sentence is that unconnected substructures are at-
tached directly under w0. For instance, the partial
parse in Fig. 1b would be represented as follows (de-
tails omitted):
(1) 1 U pron 0 FRAG
2 heeft verb 0 ROOT
3 volkomen adj 4 mod
4 gelijk noun 0 FRAG
6.1 Graph-based Model: fMST
In the training phase, the MSTParser tries to max-
imize the scoring margin between the correct parse
and all other valid dependency trees for the sentence.
However, in the case of fragmented trees, the train-
ing example is not strictly speaking correct, in the
sense that it does not coincide with the desired parse
tree. In fact, this desired tree is among the other
possible trees that MST assumes to be incorrect, or
at least suboptimal. In order to relax this assump-
tion, we have to ensure that the loss of the desired
tree is zero. While it is impossible to single out this
one tree (since we do not know which one it is), we
can steer the margin in the right direction with a loss
function that assigns zero loss to all trees that are
consistent with the training example, i.e., trees that
differ from the training example at most on those
words that are fragment roots (e.g., gelijk in Fig. 1).
To reflect this notion of loss during optimization, we
also adjust the definition of the score of a tree:
s(T ) = ?
(i,j,l)?AT : l 6=FRAG
s(i, j, l)
We refer to this modified model as f(iltering)MST.
6.2 Transition-based Model: fMalt
In the transition-based paradigm, it is particularly
important to preserve the original context (includ-
ing unattached words) of a partial analysis, because
the parser partly bases its decisions on neighboring
words in the sentence.
Emphasis of the role of isolated FRAG dependents
as context rather than proper nodes in the tree can
be achieved, as with the MSTParser, by eliminat-
ing their effect on the margin learned by the SVMs.
Since MaltParser scores local decisions, this simply
amounts to suppressing the creation of SVM train-
ing instances for such nodes (U and gelijk in (1)).
That is, where the feature model refers to context
information, unattached words provide this infor-
mation (e.g., the feature vector for volkomen in (1)
contains the form and POS of gelijk), but there are
no instances indicating how they should be attached
themselves. This technique of excluding fragment
roots during training will be referred to as fMalt.
7 Experiments
7.1 Setup
We train instances of the graph- and the transition-
based parser on projected dependencies, and occa-
sionally refer to these as ?projected parsers?.5
All results were obtained on the held-out
CoNLL-X test set of 386 sentences (avg. 12.9
5The MaltParsers use the projective Nivre arc-standard pars-
ing algorithm. For SVM training, data are split on the coarse
POS tag, with a threshold of 5,000 instances. MSTParser in-
stances use the projective Eisner parsing algorithm, and first-
order features. The input for both systems is projectivized using
the head+path schema (Nivre and Nilsson, 2005).
16
Malt MST
Alpino 80.05 82.43
EP 75.33 73.09
Alpino + EP 77.47 81.63
baseline 1 (previous) 23.65
baseline 2 (next) 27.63
Table 3: Upper and lower bounds (UAS).
words/sentence) from the Alpino treebank (van der
Beek et al, 2002). The Alpino treebank consists
mostly of newspaper text, which means that we are
evaluating the projected parsers, which are trained
on Europarl, in an out-of-domain setting, in the ab-
sence of manually annotated Europarl test data.
Parsing performance is measured in terms of un-
labeled attachment score (UAS), i.e., the proportion
of tokens that are assigned the correct head, irrespec-
tive of the label.6
To establish upper and lower bounds for our task
of weakly supervised dependency parsing, we pro-
ceed as follows. We train MaltParsers and MST-
Parsers on (i) the CoNLL-X training portion of the
Alpino treebank (195,000 words), (ii) 100,000 Eu-
roparl sentences parsed with the parser obtained
from (i), and (iii) the concatenation of the data
sets (i) and (ii). The first is a supervised upper
bound (80.05/82.43% UAS)7 trained on manually
labeled in-domain data, while the second constitutes
a weaker bound (75.33/73.09%) subject to the same
out-of-domain evaluation as the projected parsers,
and the third (77.47%) is a self-trained version of (i).
We note in passing that the supervised model does
not benefit from self-training. Two simple baselines
provide approximations to a lower bound: Baseline
1 attaches every word to the preceding word, achiev-
ing 23.65%. Analogously, baseline 2 attaches every
word to the following word (27.63%). These sys-
tems are summarized in Table 3.
6The labeled accuracy of our parsers lags behind the UAS,
because the Dutch dependency relations in the projected anno-
tations arise from a coarse heuristic mapping from the original
English labels. We therefore report only UAS.
7The upper bound models are trained with the same param-
eter settings as the projected parsers (see fn. 5), which were ad-
justed for noisy training data. Thus improvements are likely
with other settings: Nivre et al (2006) report 81.35% for a
Dutch MaltParser with optimized parameter settings. McDon-
ald et al (2006) report 83.57% with MST.
words Malt MST
a. trees (bidirectional) 13,500 65.94 67.76
trees (fallback) 62,500 59.28 65.08
bi+frags?3 68,000 55.09 57.14
bi+frags?3 (fMalt/fMST) 68,000 69.15 70.02
b. trees (bidirectional) 100,000 61.86 69.91
trees (fallback) 100,000 60.05 64.84
bi+frags?3 100,000 54.50 55.87
bi+frags?3 (fMalt/fMST) 100,000 68.65 69.86
c. trees (bidirectional) 102,300 63.32 69.85
trees (fallback) 465,500 53.45 64.88
bi+frags?3 523,000 51.48 57.20
bi+frags?3 (fMalt/fMST) 523,000 69.52 70.33
Table 4: UAS of parsers trained on projected dependency
structures for (a) a sample of 100,000 sentences, subject
to filtering, (b) 10 random samples, each with 100,000
words after filtering (average scores given), and (c) the
entire Europarl corpus, subject to filtering.
7.2 Results
Table 4a summarizes the results of training parsers
on the 100,000-sentence sample analyzed above.
Both the graph-based (MST) and the transition-
based (Malt) parsers react similarly to the more or
less aggressive filtering methods, but to different de-
grees. The first two rows of the table show the
parsers trained on complete trees (?trees (bidirec-
tional)? and ?trees (fallback)?). In spite of the ad-
ditional training data gained by the fallback method,
the resulting parsers do not achieve higher accuracy;
on the contrary, there is a drop in UAS, especially
in the transition-based model (?6.66%). The in-
creased level of noise in the fallback data has less
(but significant)8 impact on the graph-based coun-
terpart (?2.68%).
Turning to the parsers trained on partial cor-
respondence data (?bi+frags?3?), we observe even
greater deterioration in both parsing paradigms if the
data is used as is. However, in combination with the
fMalt/fMST systems (?bi+frags?3 (fMalt/fMST)?),
both parsers significantly outperform the tree-
8Significance testing (p<.01) was performed by means of
the t-test on the results of 10 training cycles (Table 4c ?trees
(fb.)? only 2 cycles due to time constraints). For the experiments
in Table 4a and 4c, the cycles differed in terms of the order in
which sentences where passed to the parser. In Table 4b we base
significance on 10 true random samples for training.
17
Recall Precision
dep. length 1 2 3?6 ?7 root 1 2 3?6 ?7 root
a. trees (bi.) 83.41 66.44 52.94 40.64 52.45 82.46 66.06 61.38 34.95 50.97
trees (fb.) 82.20 64.21 54.59 37.95 55.72 82.64 61.41 54.39 31.96 68.55
bi+frags?3 70.18 59.50 46.61 32.14 61.87 83.75 67.22 58.25 32.81 27.01
bi+frags?3 (fMalt) 89.23 75.34 59.18 41.65 59.06 83.46 69.05 65.85 48.21 75.79
Alpino-Malt 92.81 84.94 75.11 65.44 66.15 89.71 81.08 77.56 62.57 84.58
b. trees (bi.) 87.53 73.79 59.57 46.79 71.01 86.43 74.08 64.78 45.17 66.79
trees (fb.) 82.53 69.37 55.77 37.46 70.24 85.31 69.29 59.85 40.14 53.99
bi+frags?3 68.11 57.48 34.30 13.00 90.68 90.28 78.54 66.36 43.70 23.41
bi+frags?3 (fMST) 87.73 72.84 62.55 50.15 67.78 86.94 71.60 66.05 48.48 68.20
Alpino-MST 94.13 86.60 76.91 65.14 71.60 91.76 82.49 76.23 71.96 85.38
Table 5: Performance relative to dependency length. (a) Projected MaltParsers and (b) projected MSTParsers.
oriented models (?trees (bidirectional)?) by 3.21%
(Malt) and 2.26% (MST).
It would be natural to presume that the superior-
ity of the partial correspondence filter is merely due
to the amount of training data, which is larger by
a factor of 5.04. We address this issue by isolat-
ing the effect on the quality of the data, and hence
the success at noise reduction: In Table 4b, we con-
trol for the amount of data that is effectively used
in training, so that each filtered training set consists
of 100,000 words. Considering the Malt models, we
find that the trends suggested in Table 4a are con-
firmed: The pattern of relative performance emerges
even though any quantitative (dis-)advantages have
been eliminated.9 10 Interestingly, the MSTParser
does not appear to gain from the increased variety
(cf. Table 1) in the partial data: it does not differ
significantly from the ?trees (bi.)? model.
Finally, Table 4c provides the results of training
on the entire Europarl, or what remains of the corpus
after the respective filters have applied. The results
corroborate those obtained for the smaller samples.
In summary, the results support our initial hy-
pothesis that partial correspondence for sentences
containing a highly reliable part is preferable to
9The degree of skewedness in the filtered data is not con-
trolled, as it is an important characteristic of the filters.
10Some of the parsers trained on the larger data sets (Table
4b+c) achieve worse results than their smaller counterparts in
Table 4a. We conjecture that it is due to the thresholded POS-
based data split, performed prior to SVM training: Larger train-
ing sets induce decision models with more specialized SVMs,
which are more susceptible to tagging errors. This could be
avoided by increasing the threshold for splitting.
relaxing the reliability citerion, and?in the case
of the transition-based MaltParser?also to aggres-
sively filtering out all but the reliable complete trees.
With UASs around 70%, both systems are only 5%
behind the average 75.07% UAS achieved for Dutch
in the CoNLL-X Shared Task.
8 Analysis
We have seen that the graph- and the transition-
based parser react similarly to the various filtering
methods. However, there are interesting differences
in the magnitude of the performance changes. If
we compare the two tree-oriented filters ?trees (bi.)?
and ?trees (fb.)?, we observe that, although both Malt
and MST suffer from the additional noise that is in-
troduced via the unidirectional alignments, the drop
in accuracy is much less pronounced in the latter,
graph-based model. Recall that in this paradigm,
optimization is performed over the entire tree by
scoring edges independenly; this might explain why
noisy arcs in the training data have only a negligi-
ble impact. Conversely, the transition-based Malt-
Parser, which constructs parse trees in steps of lo-
cally optimal decisions, has an advantage when con-
fronted with partial structures: The individual frag-
ments provide exactly the local context, plus lexical
information about the (unconnected) wider context.
To give a more detailed picture of the differences
between predicted and actual annotations, we show
the performance (of the parsers from Table 4b) sep-
arately for binned arc length (Table 5) and sen-
tence length (Table 6). As expected, the perfor-
mance of both the supervised upper bounds (Alpino-
18
sent. length <4 4?9 10?19 20?30 > 30
a. trees (bi.) 73.87 62.13 65.67 60.81 55.18
trees (fb.) 69.91 57.84 62.29 60.04 55.47
bi+frags?3 74.14 54.40 56.62 54.07 48.95
bi+fr?3 (fMalt) 73.51 65.69 71.70 68.49 63.71
Alpino-Malt 81.98 69.81 81.11 82.82 76.02
b. trees (bi.) 76.67 70.16 73.09 69.56 63.57
trees (fb.) 73.24 64.93 67.79 64.98 57.70
bi+frags?3 77.48 59.65 55.96 55.27 52.74
bi+fr?3 (fMST) 73.24 67.84 73.46 70.04 62.92
Alpino-MST 81.98 72.24 85.10 83.86 78.51
Table 6: UAS relative to sentence length. (a) Projected
MaltParsers and (b) projected MSTParsers.
Malt/MST) and the projected parsers degrades as de-
pendencies get longer, and the difference between
the two grows. Performance across sentence length
remains relatively stable. But note that both tables
again reflect the pattern we saw in Table 4. Impor-
tantly, the relative ranking (in terms of f-score, not
shown, resp. UAS) is still in place even in long dis-
tance dependencies and long sentences. This indi-
cates that the effects we have described are not arti-
facts of a bias towards short dependencies.
In addition, Table 5 sheds some light on the im-
pact of fMalt/fMST in terms of the trade-off between
precision and recall. Without the specific adjust-
ments to handle fragments, partial structures in the
training data lead to an immense drop in recall. By
contrast, when the adapted parsers fMalt/fMST are
applied, they boosts recall back to a level compara-
ble to or even above that of the tree-oriented pro-
jection parsers, while maintaining precision. Again,
this effect can be observed across all arc lengths, ex-
cept arcs to root, which naturally the ?bi+frags? mod-
els are overly eager to predict.
Finally, the learning curves in Fig. 2 illus-
trate how much labeled data would be required to
achieve comparable performance in a supervised
setting. The graph-based upper bound (Alpino-
MST) reaches the performance of fMST (trained
on the entire Europarl) with approx. 25,000 words
of manually labeled treebank data; Alpino-Malt
achieves the performance of fMalt with approx.
35,000 words. The manual annotation of even these
moderate amounts of data involves considerable ef-
forts, including the creation of annotation guidelines
Figure 2: Learning curves for the supervised upper
bounds. They reach the performance of the projected
parsers with ?25,000 (MST) resp. 35,000 (Malt) words.
and tools, the training of annotators etc.
9 Conclusion
In the context of dependency parsing, we have pro-
posed partial correspondence projection as a greedy
method for noise reduction, and illustrated how it
can be integrated with data-driven parsing. Our ex-
perimental results show that partial tree structures
are well suited to train transition-based dependency
parsers. Graph-based models do not benefit as much
from additional partial structures, but instead are
more robust to noisy training data, even when the
training set is very small.
In future work, we will explore how well the tech-
niques presented here for English and Dutch work
for languages that are typologically further apart,
e.g., English-Greek or English-Finnish. Moreover,
we are going to investigate how our approach, which
essentially ignores unknown parts of the annotation,
compares to approaches that marginalize over hid-
den variables. We will also explore ways of combin-
ing graph-based and transition-based parsers along
the lines of Nivre and McDonald (2008).
Acknowledgments
The research reported in this paper has been sup-
ported by the German Research Foundation DFG as
part of SFB 632 ?Information structure? (project D4;
PI: Kuhn).
19
References
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of CoNLL-X, pages 149?164, New York City,
June.
Stephen Clark and James R. Curran. 2006. Partial train-
ing for a lexicalized-grammar parser. In Proceed-
ings of HLT-NAACL 2006, pages 144?151, New York,
June.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Reseach, 3:951?991, Jan-
uary.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(3):311?325.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of ACL
2004, pages 478?485, Barcelona, Spain.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL 2003, pages 127?133.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of the
MT Summit 2005.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-HLT 2008), pages 595?603, Colum-
bus, Ohio, June.
Mitchell Marcus, Grace Kim, Mary Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn tree-
bank: Annotating predicate argument structure. In
ARPA Human Language Technology Workshop.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of HLT-NAACL 2006, pages 152?159, New York,
June.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsing mod-
els. In Proceedings of EMNLP-CoNLL 2007, pages
122?131.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT-EMNLP 2005).
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of CoNLL-
X.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-HLT 2008, pages 950?958,
Columbus, Ohio, June.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of ACL 2005,
pages 99?106.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?ls?en Eryig?it,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector ma-
chines. In Proceedings of CoNLL-X, pages 221?225.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Sebastian Pado? and Mirella Lapata. 2006. Optimal con-
stituent alignment with edge covers for semantic pro-
jection. In Proceedings of COLING/ACL 2006, Syd-
ney, Australia.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed corpora.
In Proceedings of ACL 1992, pages 128?135.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on New Methods in Language Processing, pages 44?
49, Manchester, England.
Noah A. Smith and Jason Eisner. 2004. Annealing
techniques for unsupervised statistical language learn-
ing. In Proceedings of ACL 2004, pages 487?494,
Barcelona, July.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of ACL 2005, pages 354?362, Ann Ar-
bor, MI, June.
Kathrin Spreyer and Anette Frank. 2008. Projection-
based acquisition of a temporal labeller. In Proceed-
ings of IJCNLP 2008, Hyderabad, India, January.
Leonoor van der Beek, Gosse Bouma, Robert Malouf,
and Gertjan van Noord. 2002. The Alpino depen-
dency treebank. In Computational Linguistics in the
Netherlands (CLIN).
Qin Iris Wang, Dale Schuurmans, and Dekang Lin. 2008.
Semi-supervised convex training for dependency pars-
ing. In Proceedings of ACL-HLT 2008, pages 532?
540, Columbus, Ohio, June.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Proceedings
of HLT 2001.
20
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 19?27,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Empirical lower bounds on alignment error rates in syntax-based machine
translation
Anders S?gaard?
Center for Language Technology
University of Copenhagen
soegaard@hum.ku.dk
Jonas Kuhn?
Dpt. of Linguistics
University of Potsdam
kuhn@ling.uni-potsdam.de
Abstract
The empirical adequacy of synchronous
context-free grammars of rank two (2-SCFGs)
(Satta and Peserico, 2005), used in syntax-
based machine translation systems such as
Wu (1997), Zhang et al (2006) and Chi-
ang (2007), in terms of what alignments they
induce, has been discussed in Wu (1997) and
Wellington et al (2006), but with a one-sided
focus on so-called ?inside-out alignments?.
Other alignment configurations that cannot be
induced by 2-SCFGs are identified in this pa-
per, and their frequencies across a wide col-
lection of hand-aligned parallel corpora are
examined. Empirical lower bounds on two
measures of alignment error rate, i.e. the one
introduced in Och and Ney (2000) and one
where only complete translation units are con-
sidered, are derived for 2-SCFGs and related
formalisms.
1 Introduction
Syntax-based approaches to machine translation
typically use synchronous grammars to recognize or
produce translation equivalents. The synchronous
?This work was done while the first author was a Senior
Researcher at the Dpt. of Linguistics, University of Potsdam,
supported by the German Research Foundation in the Emmy
Noether project Ptolemaios on grammar learning from paral-
lel corpora; and while he was a Postdoctoral Researcher at the
ISV Computational Linguistics Group, Copenhagen Business
School, supported by the Danish Research Foundation in the
project Efficient syntax- and semantics-based machine transla-
tion.
?The second author is supported by the German Research
Foundation in the Emmy Noether project Ptolemaios on gram-
mar learning from parallel corpora.
production rules are typically learned from align-
ment structures (Wu, 1997; Zhang and Gildea, 2004;
Chiang, 2007) or from alignment structures and
derivation trees for the source string (Yamada and
Knight, 2001; Zhang and Gildea, 2004). They are
also used for inducing alignments (Wu, 1997; Zhang
and Gildea, 2004).
It is for all three reasons, i.e. translation, in-
duction from alignment structures and induction of
alignment structures, important that the synchronous
grammars are expressive enough to induce all the
alignment structures found in hand-aligned gold
standard parallel corpora (Wellington et al, 2006).
Such alignments are supposed to reflect the structure
of translations, typically contain fewer errors and are
used to evaluate automatically induced alignments.
In this paper it is shown that the synchronous
grammars used in Wu (1997), Zhang et al (2006)
and Chiang (2007) are not expressive enough to do
that. The synchronous grammars used in these sys-
tems are, formally, synchronous context-free gram-
mars of rank two (2-SCFGs), or equivalently (nor-
mal form) inversion transduction grammars (ITGs).1
The notion of rank is defined as the maximum num-
ber of constituents aligned by a production rule,
i.e. the maximum number of distinct indeces. Our
results will be extended to slight extensions of 2-
SCFGs, incl. the extension of ITGs proposed by
Zens and Ney (2003) (xITGs), synchronous tree
substitution grammars of rank two (2-STSGs) (Eis-
ner, 2003; Shieber, 2007), i.e. where tree pairs in-
clude at most two linked pairs of nonterminals, and
synchronous tree-adjoining grammars of rank two
12-SCFGs allow distinct LHS nonterminals, while ITGs do
not; but for any 2-SCFG an equivalent ITG can be constructed
by creating a cross-product of nonterminals from two sides.
19
(2-STAGs) (Shieber and Schabes, 1990; Harbusch
and Poller, 1996; Nesson et al, 2008). The over-
all frequency of alignment structures that cannot
be induced by these approaches is examined across
a wide collection of hand-aligned parallel corpora.
Empirical lower bounds on the coverage of the sys-
tems are derived from our results.
Our notion of an alignment structure is standard.
Words can be aligned to multiple words. Unaligned
nodes are permitted. Maximally connected sub-
graphs are called translation units. There is one
more choice to make in the context of many-to-many
alignments, namely whether the alignment relation
is such that if wi|w?k and wi|w?l, resp., are aligned,
and wj |w?k are aligned too, then wj |w?l are also
aligned. If so, the alignment structure is divided into
complete translation units. Such alignment struc-
tures are therefore called complete; in Goutte et
al. (2004), alignment structures with this property
are said to be closed under transitivity. An align-
ment structure is simply written as a sequence of
alignments, e.g. ?wi|w?k, wi|w?l, wj |w?k, wj |w?k?, or,
alternatively, as sequences of (possibly discontinu-
ous) translation units, e.g. ?wiwj|w?kw?l?.
A translation unit induced by a synchronous
grammar is a set of terminals that are recognized
or generated simultaneously. Consequently, syn-
chronous grammars can only induce complete align-
ment structures (by transitivity of simultaneity).2
Syntax-based approaches to machine translations
are commonly evaluated in terms of their alignment
error rate (AER) on one or more parallel corpora
(Och and Ney, 2000; Zhang and Gildea, 2004). The
AER, in the case where all alignments are sure align-
ments, is
AER = 1? 2|SA ?GA||SA| + |GA|
where GA are the gold standard alignments, and SA
the alignments produced by the system.
AER has been criticized by Fraser and
Marcu (2007). They show that AER does not
penalize unequal precision and recall when a
distinction between sure and possible alignments is
2One of the hand-aligned parallel corpora used in our exper-
iments, the one also used in Pado? and Lapata (2006), includes
incomplete alignment structures.
made. Since no such distinction is assumed below,
the classical definition is used.
We introduce also the notion of translation unit
error rate (TUER), which is defined as
TUER = 1 ? 2|SU ?GU ||SU |+ |GU |
where GU are the translation units in the gold stan-
dard, and SU the translation units produced by the
system. In other words, what is measured is a sys-
tem?s ability to predict translation units relative to
the Gold standard, not just its ability to predict align-
ments. If the system only gets part of a translation
unit right, it is not rewarded.
In the context of many-to-many alignments, this
measure may tell us more about translation quality
than AER. Consider, for instance, the small chil-
dren?s book discourse in Danish:
(1) Mads
Mads
og
CONJ
Mette
Mette
l?gger
put.FIN.PRES
tal
number.PL
sammen.
together
?Mads and Mette add numbers.?
(2) Mads
Mads
og
CONJ
Mette
Mette
l?gger
put.FIN.PRES
tal
number.PL
sammen
together
hver
every
dag.
day
?Mads and Mette add numbers every day.?
(3) Mads
Mads
og
CONJ
Mette
Mette
kan
can.FIN.PRES
godt
good
lide
like.INF
at
to
addere.
add.INF
?Mads and Mette like to add.?
(4) Mette
Mette
sp?rger
ask.FIN.PRES
ofte:
often:
Skal
Shall.FIN.FUT/PRES
vi
PRON.PL.1
addere
add.INF
sammen?
together
?Mette often asks: Do you want to add together??
Say (1-4) and the English translations are a par-
allel corpus on which we would like to evaluate an
aligner or a statistical machine translation system.
Say also that the test corpus has been aligned. Let
the first three sentences be our training data and (4)
our test data.
20
Note that the words l?gger . . . sammen form a dis-
continuous translation unit (?add?). Say our aligner
aligned only sammen and add, but not l?gger and
add. This would mean that the alignments or trans-
lations of add would most likely be associated with
the following probabilities:
.66 (add, sammen)
.33 (add, addere)
which again means that our system is likely to
arrive at the wrong alignment or translation in (4).
Nevertheless these alignments are rewarded in AER.
TUER, on the other hand, reflects the intuition that
unless you get the entire translation unit it?s better to
get nothing at all.
The hand-aligned parallel corpora in our exper-
iments come from the Copenhagen Dependency
Treebank (Buch-Kromann, 2007), for five different
language pairs, the German-English parallel corpus
used in Pado? and Lapata (2006), and the six par-
allel corpora of the first 100 sentences of Europarl
(Koehn, 2005) for different language pairs docu-
mented in Graca et al (2008). Consequently, our
experiments include a total of 12 parallel corpora.
The biggest parallel corpus consists of 4,729 sen-
tence pairs; the smallest of 61 sentence pairs. The
average size is 541 sentence pairs. The six paral-
lel corpora documented in Graca et al (2008) use
sure and possible alignments; in our experiments, as
already mentioned, the two types of alignments are
treated alike.3
3The annotations of the parallel corpora differ in format and
consistency. In fact the empirical lower bounds obtained be-
low are lower bounds in two senses: (i) they are lower bounds
on TUERs because TUERs may be significantly higher than
the empirical lower bounds found here, and (ii) they are lower
bounds in the sense that there may be hidden instances of the
configurations in question in the parallel corpora. Most seri-
ously, our search algorithms only sort alignments, but not their
elements; instead they assume that their elements are listed in
chronological order. Sometimes, but rarely, this is not the case.
Consider, for instance, file 1497, line 12 in the Danish?Spanish
parallel corpus in the Copenhagen Dependency Treebank:
<align out=?a56? type=?? in=?b30+b32+b8? outsign=?af?
insign=?del de de?/>
This is a translation unit. The word in position 56 in the source
string is aligned to the words in positions 8, 30 and 32 in the
target string, but note that the target string words do not appear
in chronological order. In some cases our algorithms take care
of this; they do not, however, in general search all possible com-
binations of words and alignments, but rely on the linear order
Sect. 2 discusses the frequency of inside-out
alignments in our hand-aligned corpora, whereas
Sect. 3 is about complex translation units. Sect. 4
briefly introduces formalisms for syntax-based ma-
chine translation, but some prior knowledge is as-
sumed. Sect. 5 brings the three sections together
and presents lower bounds on the coverage of the
systems discussed in Sect. 4, obtained by inspection
of the results in Sect. 2 and 3. Sect. 6 compares
our results to related work, in particular Zens and
Ney (2003).
2 Inside-out alignments
Wu (1997) identified so-called inside-out align-
ments, two alignment configurations that cannot be
induced by binary synchronous context-free gram-
mars; these alignment configurations, while infre-
quent in language pairs such as English?French
(Cherry and Lin, 2006; Wellington et al, 2006),
have been argued to be frequent in other lan-
guage pairs, incl. English?Chinese (Wellington et
al., 2006) and English?Spanish (Lepage and De-
noual, 2005). While our main focus is on config-
urations that involve discontinuous translation units,
the frequencies of inside-out alignments in our par-
allel corpora are also reported. Recall that inside-out
alignments are of the form (or upside-down):
a b c d
e f g h
or
a b c d
e f g h
Our findings are summarized in Figure 1. Note
that there is some variation across the corpora. The
fact that there are no inside-out alignments in cor-
pora 2?4 may be because annotators of these corpora
have been very conservative, i.e. there are many un-
aligned nodes; the first corpus, which is also part of
the Danish Dependency Treebank, also has very few
inside-out alignments. It is not entirely clear to us if
this has to do with the languages in question or the
annotation guide lines (cf. Danish?Spanish).
In the Danish?Spanish corpus and in the English?
German corpus the number of inside-out alignments
is very high. This, to some extent, has to do with the
number of words that are aligned to multiple words.
of the annotation. This was necessary to do relatively efficient
queries. The effect, however, is that our results are lower than
the actual frequencies in the parallel corpora. They are in this
sense also lower bounds.
21
Snt. TUs IO IO-m IO-m/Snt.
Danish?English: 4,729 110,511 28 4 0.001
Danish?German: 61 1,026 0 0 0
Danish?Italian: 181 2,182 0 0 0
Danish?Russian: 61 618 0 0 0
Danish?Spanish: 710 6,110 2,562 158 0.223
English?German 987 68,760 191,490 1,178 1.194
English?French: 100 937 2,651 80 0.800
English-Portuguese: 100 941 3,856 66 0.660
English?Spanish: 100 950 2,287 67 0.670
Portuguese?French: 100 915 3,643 84 0.840
Portuguese?Spanish: 100 991 1,194 58 0.580
Spanish?French 100 975 1,390 61 0.610
Figure 1: Frequency of inside-out alignments.
Say, in the case of English?German, each inside-out
alignment is made out of eight two-word translation
units. There are 1,178 inside-out alignment modulo
translation units, i.e. when one or more inside-out
alignments over the same eight translation units only
count as one; this means that there would be 28 ?
1, 178 : 301, 568 inside-out alignments in total. The
actual number (191, 491) is smaller, but comparable.
The first example in the English?German corpus,
from sentence 2, illustrates this point. The sentences
are:
(5) Mr Jonckheer, I would like to thank you just as
warmly for your report on the seventh survey
on State aid in the European Union .
(6) Ebenso herzlich mo?chte ich Ihnen, Herr
Jonckheer, fu?r Ihren Bericht u?ber den
siebenten Bericht u?ber staatliche Beihilfen in
der Europa?ischen Union danken (24).
and the alignment structure is (commas count):
1 2 3 4 5 6 7 8 9
?1
3 4 5 7 8 9 24
The aligned translation units are:4
4Note that the alignment 3|5 is probably a mistake made by
the annotator. It should, it seems, be 3|6. Note also that this
alignment is not involved in any of the inside-out alignments.
?Mr|Herr? ?Jonckheer|Jonckheer?
?,|Ihnen . . . ,? ?I|ich?
?would like to|mo?chte? ?thank|danken?
?you|Ihnen?
Note that the following sets of alignments make
up distinct inside-out alignments modulo translation
units:
{?1|7, 4|4, 8|24, 9|5?, ?2|8, 4|4, 8|24, 9|5?,
?3|9, 4|4, 8|24, 9|5?, ?1|7, 5|3, 8|24, 9|5?,
?2|8, 5|3, 8|24, 9|5?, ?3|9, 5|3, 8|24, 9|5?}
The following sets of alignments in addition make
up distinct inside-out alignments, but the new align-
ments 6|3 and 7|3 are from the same translation unit
as 5|3:
{?1|7, 6|3, 8|24, 9|5?, ?2|8, 6|3, 8|24, 9|5?,
?3|9, 6|3, 8|24, 9|5?, ?1|7, 6|3, 8|24, 9|5?,
?2|8, 6|3, 8|24, 9|5?, ?3|9, 6|3, 8|24, 9|5?}
Consequently, the alignment of sentences (5) and
(6) in the English?German parallel corpus contains
12 inside-out alignments, but only six inside-out
alignments modulo translation units.
3 Cross-serial discontinuous translation
units
A discontinuous translation unit (DTU) is a transla-
tion unit where either the substring of source string
words or the substring of target string words that oc-
cur in it, is discontinuous, i.e. there is a gap in it.
Since translation units are induced by simulta-
neous recognition, it is necessary for synchronous
22
grammars to have rules that introduce multiple
source side terminals and/or multiple target side ter-
minals with at least one intervening nonterminal to
induce DTUs. A DTU with multiple gaps in the
same side is called a multigap DTU; it is easy to see
that binary grammars cannot induce multigap DTUs
with more than two gaps.
A sequence of DTUs is said to be cross-serial if it
is of the following form (or upside-down):
ai aj
bk bl bm bn
Call any sequence of cross-serial DTUs a cross-
serial DTU (CDTU). So a CDTU is an alignment
configuration such that the source-side, resp. target-
side, contains four tokens bk, bl, bm, bn such that (i)
bk ? bl ? bm ? bn, (ii) bk and bm belong to the
same translation unit T , and bl and bn belong to the
same translation unit T ?, and (iii) T and T ? are dis-
tinct translation units. The inability of ITGs, xITGs
and 2-STSGs to induce CDTUs follows from the ob-
servation that if bk and bm in the above are gener-
ated or recognized simultatenously in any of these
formalisms, bl and bn cannot be generated or recog-
nized simulaneously. This is a straight-forward con-
sequence of the context-freeness of the component
grammars.
The distinction between CDTUs and CDTUs
modulo translation units (CDTU-ms) is again im-
portant. The number of CDTU-ms is the number of
CDTUs such that all CDTUs differ by at most one
translation unit. The English?German parallel cor-
pus, for example, contains 15,717 CDTUs, but only
2,079 CDTU-ms. Since our evaluation measure is
TUER, we only systematically counted the occur-
rences of CDTU-ms. In a few cases, the number of
CDTUs was extracted too. In general, it was about
eight times higher than the number of CDTU-ms.
Our findings are summarized in Figure 2. There is
again variation, but the average ratio of CDTU-ms is
0.514, i.e. there is a CDTU-m in about every second
aligned sentence pair.
4 Syntax-based machine translation
Syntax-directed translation schemas (SDTSs) were
originally introduced by Culik (1966) and studied
formally by Aho and Ullman (1972), who stressed
the importance of using only binary SDTSs for effi-
ciency reasons,5 and later led to the development of
a number of near-equivalent theories, incl. 2-SCFGs
and (normal form) ITGs. Henceforth, we will refer
to this class of near-equivalent theories as ITGs (see
footnote 1). This also means that production rules
have at most one source-side and one target-side ter-
minal on the RHS (see below).
It is the ability of ITGs to induce alignments that
is our main focus. Related work includes Wu (1997),
Zens and Ney (2003) and Wellington et al (2006).
Our results will also be extended to xITGs, 2-STSGs
and 2-STAGs. O(|G|n6) time recognition algo-
rithms are known for ITGs, xITGs and 2-STSGs. 2-
STAGs (O(|G|n12)) are more complex.
The production rules in ITGs are of the follow-
ing form (Wu, 1997), with a notation similar to what
is typically used for SDTSs and SCFGs in the right
column:
A ? [BC] A ? ?B1C2, B1C2?
A ? ?BC? A ? ?B1C2, C2B1?
A ? e | f A ? ?e, f?
A ? e | ? A ? ?e, ??
A ? ? | f A ? ??, f?
It is important to note that RHSs of production
rules have at most one source-side and one target-
side terminal symbol. This prevents induction of
multiword translation units in any straight-forward
way. xITGs (Zens and Ney, 2003) in part solves this
problem. All production rules in ITGs can be pro-
duction rules in xITGs, but xITG production rules
can also be of the following form:
A ? [e/f1A?/f2] | ?e/f1A?/f2?
Note, however, that these production rules still do
not enable double-sided DTUs, i.e. DTUs that trans-
late into DTUs. Such, however, occur relatively fre-
quently in hand-aligned parallel corpora, e.g. 148
times in the Danish?Spanish corpus.
There is no room for detailed introductions of the
more complex formalisms, but briefly their differ-
ences can be summarized as follows:
The move from ITGs to 2-STSGs is relatively
simple. All production rules in ITGs characterize
5The hierarchy of SDTSs of rank k is non-collapsing, and
the recognition problem without a fixed rank is NP-hard (Aho
and Ullman, 1972; Rambow and Satta, 1994). See Zhang et
al. (2006) for an efficient binarization algorithm.
23
Snt. TUs DTUs DTUs/Snt. CDTU-ms CDTU-ms/Snt.
Danish?English: 4,729 110,511 1,801 0.381 6 0.001
Danish?German: 61 1,026 43 0.705 0 0
Danish?Italian: 181 2,182 63 0.348 1 0.006
Danish?Russian: 61 618 27 0.443 0 0
Danish?Spanish: 710 6,693 779 1.097 121 0.170
English?German 650 68,760 5,062 7.788 2,079 3.199
English?French: 100 937 95 0.950 38 0.380
English-Portuguese: 100 941 100 1.000 85 0.850
English?Spanish: 100 950 90 0.900 50 0.500
Portuguese?French: 100 915 77 0.770 27 0.270
Portuguese?Spanish: 100 991 80 0.800 55 0.550
Spanish?French 100 975 74 0.740 24 0.240
Figure 2: Frequency of cross-serial DTUs.
binary trees of depth 1. It is said that this is the do-
main of locality in ITGs. 2-STSGs extend the do-
main of locality to arbitrarily big trees. 2-STSGs
are collections of ordered pairs of aligned trees with
at most two pairs of linked nonterminals. The leaf
nodes in the trees may be decorated by terminals or
insertion slots where subtrees can be ?plugged in?.
This is exactly what is meant by tree substitution.
It is assumed that all terminals in a tree pair con-
stitute a translation unit. There exists a O(|G|n6)
time parsing algorithm for 2-STSGs. 2-STSGs in-
duce DTUs, double-sided DTUs and DTUs with at
most two gaps, but not inside-out alignments, CD-
TUs and multigap DTUs with more than two gaps.
The substitution operation on elementary trees is
supplied with an adjunction operation in 2-STAGs
(Shieber and Schabes, 1990; Harbusch and Poller,
1996; Nesson et al, 2008). In adjunction, auxil-
iary trees, i.e. elementary trees with a designated leaf
node labeled by a nonterminal identical to the non-
terminal that labels the root node, extend the derived
tree by expanding one of its nodes. If an auxiliary
tree t, with a root node and a leaf node both labeled
A, is adjoined at some node n also labeled A in a
derived tree t?, the subtree s? (of t?) rooted at n is re-
placed by t, and s? is then inserted at the leaf node of
t. In 2-STAGs, paired nodes across the source-side
and target-side trees are simultaneously expanded by
either substitution or adjunction. A O(|G|n12) pars-
ing algorithm can be deviced for 2-STAGs using the
techniques in Seki et al (1991). The following 2-
STAG translates Swiss-style cross-serial dependen-
cies {wambnxcmdny} into {w(ac)mx(bd)ny} and
thus induces cross-serial DTUs whenever m,n ? 1
(superscripts are pairings).
? Sbb""
w Z1 y
, Sbb""
w Z1 y
?? Z1
x
, Z1
x
?
? ZNAZZProceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 23?30,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Exploiting Translational Correspondences for Pattern-Independent MWE
Identification
Sina Zarrie?
Department of Linguistics
University of Potsdam, Germany
sina@ling.uni-potsdam.de
Jonas Kuhn
Department of Linguistics
University of Potsdam, Germany
kuhn@ling.uni-potsdam.de
Abstract
Based on a study of verb translations in
the Europarl corpus, we argue that a wide
range of MWE patterns can be identified in
translations that exhibit a correspondence
between a single lexical item in the source
language and a group of lexical items in
the target language. We show that these
correspondences can be reliably detected
on dependency-parsed, word-aligned sen-
tences. We propose an extraction method
that combines word alignment with syn-
tactic filters and is independent of the
structural pattern of the translation.
1 Introduction
Parallel corpora have proved to be a valuable re-
source not only for statistical machine translation,
but also for crosslingual induction of morphologi-
cal, syntactic and semantic analyses (Yarowsky et
al., 2001; Dyvik, 2004). In this paper, we propose
an approach to the identification of multiword ex-
pressions (MWEs) that exploits translational cor-
respondences in a parallel corpus. We will con-
sider in translations of the following type:
(1) Der
The
Rat
Council
sollte
should
unsere
our
Position
position
beru?cksichtigen.
consider.
(2) The Council should take account of our position.
This sentence pair has been taken from the
German - English section of the Europarl corpus
(Koehn, 2005). It exemplifies a translational cor-
respondence between an English MWE take ac-
count of and a German simplex verb beru?cksichti-
gen. In the following, we refer to such correspon-
dences as one-to-many translations. Based on a
study of verb translations in Europarl, we will ex-
plore to what extent one-to-many translations pro-
vide evidence for MWE realization in the target
language. It will turn out that crosslingual corre-
spondences realize a wide range of different lin-
guistic patterns that are relevant for MWE iden-
tification, but that they pose problems to auto-
matic word alignment. We propose an extraction
method that combines distributional word align-
ment with syntactic filters. We will show that
these correspondences can be reliably detected
on dependency-parsed, wordaligned sentences and
are able to identify various MWE patterns.
In a monolingual setting, the task of MWE ex-
traction is usually conceived of as a lexical as-
sociation problem where distributional measures
model the syntactic and semantic idiosyncracy ex-
hibited by MWEs, e.g. (Pecina, 2008). This ap-
proach generally involves two main steps: 1) the
extraction of a candidate list of potential MWEs,
often constrained by a particular target pattern
of the detection method, like verb particle con-
structions (Baldwin and Villavicencio, 2002) or
verb PP combinations (Villada Moiro?n and Tiede-
mann, 2006), 2) the ranking of this candidate list
by an appropriate assocation measure.
The crosslingual MWE identification we
present in this paper is, a priori, independent
of any specific association measure or syntactic
pattern. The translation scenario allows us to
adopt a completely data-driven definition of what
constitutes an MWE: Given a parallel corpus,
we propose to consider those tokens in a target
language as MWEs which correspond to a single
lexical item in the source language. The intuition
is that if a group of lexical items in one lan-
guage can be realized as a single item in another
language, it can be considered as some kind of
lexically fixed entity. By this means, we will
not approach the MWE identification problem
by asking for a given list of candidates whether
these are MWEs or not. Instead, we will ask for
a given list of lexical items in a source language
whether there exists a one-to-many translation for
this item in a target language (and whether these
23
one-to-many translations correspond to MWEs).
This strategy offers a straightforward solution to
the interpretation problem: As the translation can
be related to the meaning of the source item and
to its other translations in the target language, the
interpretation is independent of the expression?s
transparency. This solution has its limitations
compared to other approaches that need to auto-
matically establish the degree of compositionality
of a given MWE candidate. However, for many
NLP applications, coarse-grained knowledge
about the semantic relation between a wide
range of MWEs and their corresponding atomic
realization is already very useful.
In this work, we therefore focus on a general
method of MWE identification that captures the
various patterns of translational correspondences
that can be found in parallel corpora. Our exper-
iments described in section 3 show that one-to-
many translations should be extracted from syn-
tactic configurations rather than from unstructured
sets of aligned words. This syntax-driven method
is less dependent on frequency distributions in a
given corpus, but is based on the intuition that
monolingual idiosyncracies like MWE realization
of an entity are not likely to be mirrored in another
language (see section 4 for discussion).
Our goal in this paper is twofold: First, we want
to investigate to what extent one-to-many transla-
tional correspondences can serve as an empirical
basis for MWE identification. To this end, Sec-
tion 2 presents a corpus-based study of the rela-
tion between one-to-many translations and MWEs
that we carried out on a translation gold standard.
Second, we investigate methods for the automatic
detection of complex lexical correspondences for
a given parallel corpus. Therefore, Section 3 eval-
uates automatic word alignments against our gold
standard and gives a method for high-precision
one-to-many translation detection that relies on
syntactic filters, in addition to word-alignments.
2 Multiword Translations as MWEs
The idea to exploit one-to-many translations for
the identification of MWE candidates has not re-
ceived much attention in the literature. Thus, it is
not a priori clear what can be expected from trans-
lational correspondences with respect to MWE
identification. To corroborate the intuitions intro-
duced in the last section, we carried out a corpus-
based study that aims to discover linguistic pat-
Verb 1-1 1-n n-1 n-n No
anheben (v1) 53.5 21.2 9.2 16 325
bezwecken (v2) 16.7 51.3 0.6 31.3 150
riskieren (v3) 46.7 35.7 0.5 17 182
verschlimmern (v4) 30.2 21.5 28.6 44.5 275
Table 1: Proportions of types of translational cor-
respondences (token-level) in our gold standard.
terns exhibited by one-to-many translations.
We constructed a gold standard covering all En-
glish translations of four German verb lemmas ex-
tracted from the Europarl Corpus. These verbs
subcategorize for a nominative subject and an ac-
cusative object and are in the middle frequency
layer (around 200 occurrences). We extracted all
sentences in Europarl with occurences of these
lemmas and their automatic word alignments pro-
duced by GIZA++ (Och and Ney, 2003). These
alignments were manually corrected on the basis
of the crosslingual word alignment guidelines de-
velopped by (Grac?a et al, 2008).
For each of the German source lemmas, our
gold standard records four translation categories:
one-to-one, one-to-many, many-to-one, many-to-
many translations. Table 1 shows the distribution
of these categories for each verb. Strikingly, the
four verbs show very different proportions con-
cerning the types of their translational correspon-
dences. Thus, while the German verb anheben
(en. increase) seems to have a frequent parallel
realization, the verbs bezwecken (en. intend to)
or verschlimmern (en. aggravate) tend to be real-
ized by more complex phrasal translations. In any
case, the percentage of one-to-many translations is
relatively high which corroborates our hypothesis
that parallel corpora constitute a very interesting
resource for data-driven MWE discovery.
A closer look at the one-to-many translations re-
veals that these cover a wide spectrum of MWE
phenomena traditionally considered in the liter-
ature, as well as constructions that one would
usually not regard as an MWE. Below, we will
shortly illustrate the different classes of one-to-
many translations we found in our gold standard.
Morphological variations: This type of one-to-
many translations is mainly due to non-parallel re-
alization of tense. It?s rather irrelevant from an
MWE perspective, but easy to discover and filter
automatically.
24
(3) Sie
They
verschlimmern
aggravate
die
the
?Ubel.
misfortunes.
(4) Their action is aggravating the misfortunes.
Verb particle combinations: A typical MWE
pattern, treated for instance in (Baldwin and
Villavicencio, 2002). It further divides into trans-
parent and non-transparent combinations, the lat-
ter is illustrated below.
(5) Der
The
Ausschuss
committe
bezweckt,
intends,
den
the
Institutionen
institutions
ein
a
politisches
political
Instrument
instrument
an
at
die
the
Hand
hand
zu
to
geben.
give.
(6) The committee set out to equip the institutions with a
political instrument.
Verb preposition combinations: While this
class isn?t discussed very often in the MWE lit-
erature, it can nevertheless be considered as an id-
iosyncratic combination of lexical items. Sag et al
(2002) propose an analysis within an MWE frame-
work.
(7) Sie
They
werden
will
den
the
Treibhauseffekt
green house effect
verschlimmern.
aggravate.
(8) They will add to the green house effect.
Light verb constructions (LVCs): This is the
most frequent pattern in our gold standard. It ac-
tually subsumes various subpatterns depending on
whether the light verbs complement is realized as a
noun, adjective or PP. Generally, LVCs are syntac-
tically and semantically more flexible than other
MWE types, such that our gold standard contains
variants of LVCs with similar, potentially mod-
ified adjectives or nouns, as in the example be-
low. However, it can be considered an idiosyn-
cratic combination since the LVCs exhibit specific
lexical restrictions (Sag et al, 2002).
(9) Ich
Ich
werde
will
die
the
Sache
thing
nur
only
noch
just
verschlimmern.
aggravate.
(10) I am just making things more difficult.
Idioms: This MWE type is probably the most
discussed in the literature due to its semantic and
syntactic idiosyncracy. It?s not very frequent in
our gold standard which may be mainly due to its
limited size and the source items we chose.
(11) Sie
They
bezwecken
intend
die
the
Umgestaltung
conversion
in
into
eine
a
zivile
civil
Nation.
nation.
(12) They have in mind the conversion into a civil nation.
v1 v2 v3 v4
Ntype 22 (26) 41 (47) 26 (35) 17 (24)
V Part 22.7 4.9 0.0 0.0
V Prep 36.4 41.5 3.9 5.9
LVC 18.2 29.3 88.5 88.2
Idiom 0.0 2.4 0.0 0.0
Para 36.4 24.3 11.5 23.5
Table 2: Proportions of MWE types per lemma
Paraphrases: From an MWE perspective, para-
phrases are the most problematic and challenging
type of translational correspondence in our gold
standard. While the MWE literature typically dis-
cusses the distinction between collocations and
MWEs, the boarderline between paraphrases and
MWEs is not really clear. On the hand, para-
phrases, as we classified them here, are transparent
combinations of lexical items, like in the exam-
ple below ensure that something increases. How-
ever, semantically, these transparent combinations
can also be rendered by an atomic expression in-
crease. A further problem raised by paraphrases is
that they often involve translational shifts (Cyrus,
2006). These shifts are hard to identify automat-
ically and present a general challenge for seman-
tic processing of parallel corpora. An example is
given below.
(13) Wir
We
brauchen
need
bessere
better
Zusammenarbeit,
cooperation
um
to
die
the
Ru?ckzahlungen
repayments.OBJ
anzuheben .
increase.
(14) We need greater cooperation in this respect to ensure
that repayments increase .
Table 2 displays the proportions of the MWE
categories for the number of types of one-to-many
correspondences in our gold standard. We filtered
the types due to morphological variations only (the
overall number of types is indicated in brackets).
Note that some types in our gold standard fall into
several categories, e.g. they combine a verb prepo-
sition with a verb particle construction. For all
of the verbs, the number of types belonging to
core MWE categories largely outweighs the pro-
portion of paraphrases. As we already observed
in our analysis of general translation categories,
here again, the different verb lemmas show strik-
ing differences with respect to their realization in
English translations. For instance, anheben (en.
increase) or bezwecken (en. intend) are frequently
25
translated with verb particle or preposition combi-
nations, while the other verbs are much more of-
ten translated by means of LVCs. Also, the more
specific LVC patterns differ largely among the
verbs. While verschlimmern (en. aggravate) has
many different adjectival LVC correspondences,
the translations of riskieren (en. risk) are predomi-
nantly nominal LVCs. The fact that we found very
few idioms in our gold standard may be simply
related to our arbitrary choice of German source
verbs that do not have an English idiom realiza-
tion (see our experiment on a random set of verbs
in Section 3.3).
In general, one-to-many translational corre-
spondences seem to provide a very fruitful ground
for the large-scale study of MWE phenomena.
However, their reliable detection in parallel cor-
pora is far from trivial, as we will show in the
next section. Therefore, we will not further in-
vestigate the classification of MWE patterns in
the rest of the paper, but concentrate on the high-
precision detection of one-to-many translations.
Such a pattern-independent identification method
is crucial for the further data-driven study of one-
to-many translations in parallel corpora.
3 Multiword Translation Detection
This section is devoted to the problem of high-
precision detection of one-to-many translations.
Section 3.1 describes an evaluation of automatic
word alignments against our gold standard. In
section 3.2, we describe a method that extracts
loosely aligned syntactic configurations which
yields much more promising results.
3.1 One-to-many Alignments
To illustrate the problem of purely distributional
one-to-many alignment, table 3 presents an eval-
uation of the automatic one-to-many word align-
ments produced by GIZA++ that uses the stan-
dard heuristics for bidirectional word alignment
from phrase-based MT (Och and Ney, 2003). We
evaluate the rate of translational correspondences
on the type-level that the system discovers against
the one-to-many translations in our gold standard.
By type we mean the set of lemmatized English
tokens that makes up the translation of the Ger-
man source lemma. Generally, automatic word
alignment yields a very high FPR if no frequency
threshold is used. Increasing the threshold may
help in some cases, however the frequency of the
verb n > 0 n > 1 n > 3
FPR FNR FPR FNR FPR FNR
v1 0.97 0.93 1.0 1.0 1.0 1.0
v2 0.93 0.9 0.5 0.96 0.0 0.98
v3 0.88 0.83 0.8 0.97 0.67 0.97
v4 0.98 0.92 0.8 0.92 0.34 0.92
Table 3: False positive rate and False negative rate
of GIZA++ one-to-many alignments
translation types is so low, that already at a thresh-
old of 3, almost all types get filtered. This does not
mean that the automatic word alignment does not
discover any correct correspondences at all, but it
means that the detection of the exact set of tokens
that correspond to the source token is rare.
This low precision of one-to-many alignments
isn?t very surprising. Many types of MWEs con-
sist of items that contribute most of the lexical se-
mantic content, while the other items belong to the
class of semantically almost ?empty? items (e.g.
particles, light verbs). These semantically ?light?
items have a distribution that doesn?t necessarily
correlate with the source item. For instance, in
the following sentence pair taken from Europarl,
GIZA++ was not able to capture the correspon-
dence between the German main verb behindern
(en. impede) and the LVC constitute an obstacle
to, but only finds an alignment link between the
verb and the noun obstacle.
(15) Die
The
Korruption
corruption
behindert
impedes
die
the
Entwicklung.
development.
(16) Corruption constitutes an obstacle to development.
Another limitation of the word-alignment mod-
els is that are independent of whether the sen-
tences are largely parallel or rather free transla-
tions. However, parallel corpora like Europarl are
know to contain a very large number of free trans-
lations. In these cases, direct lexical correspon-
dences are much more unlikely to be found.
3.2 Aligning Syntactic Configurations
High-precision extraction of one-to-many trans-
lation detection thus involves two major prob-
lems: 1) How to identify sentences or configura-
tions where reliable lexical correspondences can
be found? 2) How to align target items that have a
low occurrence correlation?
We argue that both of these problems can be
adressed by taking syntactic information into ac-
26
count. As an example, consider the pair of paral-
lel configurations in Figure 1 for the sentence pair
given in (15) and (16). Although there is no strict
one-to-one alignment for the German verb, the ba-
sic predicate-argument structure is parallel: The
verbs arguments directly correspond to each other
and are all dominated by a verbal root node.
Based on these intuitions, we propose a
generate-and-filter strategy for our one-to-many
translation detection which extracts partial, largely
parallel dependency configurations. By admitting
target dependency paths to be aligned to source
single dependency relations, we admit configura-
tions where the source item is translated by more
than one word. For instance, given the configura-
tion in Figure 1, we allow the German verb to be
aligned to the path connecting constitute and the
argument Y2.
Our one-to-many translation detection consists
of the following steps: a) candidate generation
of aligned syntactic configurations, b) filtering the
configurations c) alignment post-editing, i.e. as-
sembling the target tokens corresponding to the
source item. The following paragraphs will briefly
caracterize these steps.
behindert
X 1 Y 1
Y 2
an to
X 2 obstacle
create
Figure 1: Example of a typical syntactic MWE
configuration
Data We word-aligned the German and English
portion of the Europarl corpus by means of the
GIZA++ tool. Both portions where assigned flat
syntactic dependency analyses by means of the
MaltParser (Nivre et al, 2006) such that we ob-
tain a parallel resource of word-aligned depen-
dency parses. Each sentence in our resource can
be represented by the triple (DG, DE , AG,E). DG
is the set of dependency triples (s1, rel, s2) such
that s2 is a dependent of s1 of type rel and s1, s2
are words of the source language. DE is the set
of dependency triples of the target sentence. AG,E
corresponds to the set of pairs (s1, t1) such that
s1, t1 are aligned.
Candidate Generation This step generates a
list of source configurations by searching for oc-
curences of the source lexical verb where it is
linked to some syntactic dependents (e.g. its argu-
ments). An example input would be the configura-
tion ( (verb,SB,%), (verb,OA,%)) for
our German verbs.
Filtering Given our source candidates, a valid
parallel configuration (DG, DE , AG,E) is then de-
fined by the following conditions:
1. The source configuration DG is the set of tu-
ples (s1, rel, sn) where s1 is our source item and
sn some dependent.
2. For each sn ? DG, there is a tuple (sn, tn) ?
AG,E , i.e. every dependent has an alignment.
3. There is a target item t1 ? DE such that
for each tn, there is a p ? DE such that p is
a path (t1, rel, tx), (tx, rel, ty)...(tz, rel, tn) that
connects t1 and tn. Thus, the target dependents
have a common root.
To filter noise due to parsing or alignment er-
rors, we further introduce a filter on the length of
the path that connects the target root and its de-
pendents and w exclude paths cross contain sen-
tence boundaries. Moreover, the above candi-
date filtering doesn?t exclude configurations which
exhibit paraphrases involving head-switching or
complex coordination. Head-switching can be de-
tected with the help of alignment information: if
there is a item in our target configuration that has
an reliable alignment with an item not contained in
our source configuration, our target configuration
is likely to contain such a structural paraphrases
and is excluded from our candidate set. Coordina-
tion can be discarded by imposing the condition on
the configuration not to contain a coordination re-
lation. This Generate-and-Filter strategy now ex-
tracts a set of sentences where we are likely to find
a good one-to-one or one-to-many translation for
the source verb.
Alignment Post-editing In the final alignment
step, one now needs to figure out which lexical
material in the aligned syntactic configurations ac-
tually corresponds to the translation of the source
item. The intuition discussed in 3.2 was that all
27
the items lying on a path between the root item
and the terminals belong to the translation of the
source item. However, these items may have other
syntactic dependents that may also be part of the
one-to-many translation. As an example, consider
the configuration in figure 1 where the article an
which is part of the LVC create an obstacle to has
to be aligned to the German source verb.
Thus, for a set of items ti for which there is a de-
pendency relation (tx, rel, ti) ? DE such that tx is
an element of our target configuration, we need to
decide whether (s1, ti) ? AG,E . This translation
problem now largely parallels collocation trans-
lation problems discussed in the literature, as in
(Smadja and McKeown, 1994). But, crucially, our
syntactic filtering strategy has substantially nar-
rowed down the number of items that are possi-
ble parts of the one-to-many translation. Thus, a
straightforward way to assemble the translational
correspondence is to compute the correlation or
association of the possibly missing items with the
given translation pair as proposed in (Smadja and
McKeown, 1994). Therefore, we propose the fol-
lowing alignment post-editing algorithm:
Given the source item s1 and the set of target items
T , where each ti ? T is an element of our target
configuration,
1. Compute corr(s1, T ), the correlation be-
tween s1 and T .
2. For each ti, tx such that there is
a (ti, rel, tx) ? DE , compute
corr(s1, T + {tx})
3. if corr(s1, T + {tx}) ? corr(s1, T ), add tx
to T .
As the Dice coefficient is often to give the best
results, e.g. in (Smadja and McKeown, 1994), we
also chose Dice as our correlation measure. In fu-
ture work, we will experiment with other associa-
tion measures. Our correlation scores are thus de-
fined by the formula:
corr(s1, T ) =
2(freq(s1 ? T ))
freq(s1) + freq(T )
We define freq(T ) as the number of sentence
pairs whose target sentence contains occurrences
of all ti ? T , and freq(s1) accordingly. The ob-
servation frequency freq(s1?T ) is the number of
sentence pairs that where s1 occurs in the source
sentence, and T in the target sentence.
The output translation can then be rep-
resented as a dependency configuration
of the following kind :((of,PMOD,%),
(risk,NMOD,of),(risk,NMOD,the), (run,OBJ,risk),
(run,SBJ,%)) which is the syntactic representation
for the English MWE run the risk of.
3.3 Evaluation
Our translational approach to MWE extraction
bears the advantage that evaluation is not exclu-
sively bound to the manual judgement of candi-
date lists. Instead, we can first evaluate the system
output against translation gold standards which are
easier to obtain. The linguistic classification of the
candidates according to their compositionality can
then be treated as a separate problem.
We present two experiments in this evaluation
section: We will first evaluate the translation de-
tection on our gold standard to assess the gen-
eral quality of the extraction method. Since this
gold standard is to small to draw conclusions about
the quality of MWE patterns that the system de-
tects, we further evaluate the translational corre-
spondences for a larger set of verbs.
Translation evaluation: In the first experiment,
we extracted all types of translational correspon-
dences for the verbs we annotated in the gold stan-
dard. We converted the output dependency con-
figurations to the lemmatized bag-of-word form
we already applied for the alignment evaluation
and calculated the FPR and FNR of the trans-
lation types. The evaluation is displayed in ta-
ble 4. Nearly all translation types that our sys-
tem detected are correct. This confirms our hy-
pothesis that syntactic filtering yields more reli-
able translations that just coocurrence-based align-
ments. However, the false negative rate is also
very high. This low recall is due to the fact that
our syntactic filters are very restrictive such that a
major part of the occurrences of the source lemma
don?t figure in the prototypical syntactic configu-
ration. Column two and three of the evaluation ta-
ble present the FPR and FNR for experiments with
a relaxed syntactic filter that doesn?t constrain the
syntactic type of the parallel argument relations.
While not decreasing the FNR, the FPR decreases
significantly. This means that the syntactic filters
mainly fire on noisy configurations and don?t de-
crease the recall. A manual error analysis has also
shown that the relatively flat annotation scheme of
our dependency parses significantly narrows down
28
the number of candidate configurations that our al-
gorithm detects. As the dependency parses don?t
provide deep analyses for tense or control phe-
nomena, very often, a verb?s arguments don?t fig-
ure as its syntactic dependents and no configura-
tion is found. Future work will explore the im-
pact of deep syntactic analysis for the detection of
translational correspondences.
MWE evaluation: In a second experiment, we
evaluated the patterns of correspondences found
by our extraction method for use in an MWE con-
text. Therefore, we selected 50 random verbs oc-
curring in the Europarl corpus and extracted their
respective translational correspondences. This set
of 50 verbs yields a set of 1592 one-to-many types
of translational correspondences. We filtered the
types wich display only morphological variation,
such that the set of potential MWE types com-
prises 1302 types. Out of these, we evaluated a
random sample of 300 types by labelling the types
with the MWE categories we established for the
analysis of our gold standard. During the clas-
sification, we encountered a further category of
oneto- many correspondence which cannot be con-
sidered an MWE, the category of alternation. For
instance, we found a translational correspondence
between the active realization of the German verb
begru??en (en. appreciate) and the English passive
be pleased by.
The classification is displayed in table 5. Al-
most 83% of the translational correspondences
that our system extracted are perfect translation
types. Almost 60% of the extracted types can be
considered MWEs that exhibit some kind of se-
mantic idiosyncrasy. The other translations could
be classified as paraphrases or alternations. In our
random sample, the portions of idioms is signifi-
cantly higher than in our gold standard which con-
firms our intuition that the MWE pattern of the
one-to-many translations for a given verb are re-
lated to language-specific, semantic properties of
the verbs and the lexical concepts they realize.
4 Related Work
The problem sketched in this paper has clear con-
ncetions to statistical MT. So-called phrase-based
translation models generally target whole sentence
alignment and do not necessarily recur to linguis-
tically motivated phrase correspondences (Koehn
et al, 2003). Syntax-based translation that speci-
fies formal relations between bilingual parses was
Strict Filter Relaxed Filter
FPR FNR FPR FNR
v1 0.0 0.96 0.5 0.96
v2 0.25 0.88 0.47 0.79
v3 0.25 0.74 0.56 0.63
v4 0.0 0.875 0.56 0.84
Table 4: False positive and false negative rate of
one-to-many translations.
Trans. type Proportion
MWE type Proportion
MWEs 57.5%
V Part 8.2%
V Prep 51.8%
LVC 32.4%
Idiom 10.6%
Paraphrases 24.4%
Alternations 1.0%
Noise 17.1%
Table 5: Classification of 300 types sampled from
the set of one-to-many translations for 50 verbs
established by (Wu, 1997). Our way to use syn-
tactic configurations can be seen as a heuristic to
check relaxed structural parallelism.
Work on MWEs in a crosslingual context has
almost exclusively focussed on MWE translation
(Smadja and McKeown, 1994; Anastasiou, 2008).
In (Villada Moiro?n and Tiedemann, 2006), the au-
thors make use of alignment information in a par-
allel corpus to rank MWE candidates. These ap-
proaches don?t rely on the lexical semantic knowl-
edge about MWEs in form of one-to-many trans-
lations.
By contrast, previous approaches to paraphrase
extraction made more explicit use of crosslingual
semantic information. In (Bannard and Callison-
Burch, 2005), the authors use the target language
as a pivot providing contextual features for iden-
tifying semantically similar expressions. Para-
phrasing is however only partially comparable to
the crosslingual MWE detection we propose in
this paper. Recently, the very pronounced context
dependence of monolingual pairs of semantically
similar expressions has been recognized as a ma-
jor challenge in modelling word meaning (Erk and
Pado, 2009).
The idea that parallel corpora can be used as
a linguistic resource that provides empirical evi-
dence for monolingual idiosyncrasies has already
29
been exploited in, e.g. morphology projection
(Yarowsky et al, 2001) or word sense disambigua-
tion (Dyvik, 2004). While in a monolingual set-
ting, it is quite tricky to come up with theoretical
or empirical definitions of sense discriminations,
the crosslingual scenario offers a theory-neutral,
data-driven solution: Since ambiguity is an id-
iosyncratic property of a lexical item in a given
language, it is not likely to be mirrored in a tar-
get language. Similarly, our approach can also be
seen as a projection idea: we project the semantic
information of simplex realization in a source lan-
guage to an idiosyncratic, multiword realization in
the target language.
5 Conclusion
We have explored the phenomenon of one-to-
many translations in parallel corpora from the
perspective of MWE identification. Our man-
ual study on a translation gold standard as well
as our experiments in automatic translation ex-
traction have shown that one-to-many correspon-
dences provide a rich resource and fruitful basis
of study for data-driven MWE identification. The
crosslingual perspective raises new research ques-
tions about the identification and interpretation of
MWEs. It challenges the distinction between para-
phrases and MWEs, a problem that does not arise
at all in the context of monolingual MWE ex-
traction. It also allows for the study of the rela-
tion between the semantics of lexical concepts and
their MWE realization. Further research in this di-
rection should investigate translational correspon-
dences on a larger scale and further explore these
for monolingual interpretation of MWEs.
Our extraction method that is based on syn-
tactic filters identifies MWE types with a much
higher precision than purely cooccurence-based
word alignment and captures the various patterns
we found in our gold standard. Future work on the
extraction method will have to focus on the gener-
alization of these filters and the generalization to
other items than verbs. The experiments presented
in this paper also suggest that the MWE realiza-
tion of certain lexical items in a target language
is subject to certain linguistic patterns. Moreover,
the method we propose is completely languagein-
dependent such that further research has to study
the impact of the relatedness of the considered
languages on the patterns of one-to-many transla-
tional correspondences.
References
Dimitra Anastasiou. 2008. Identification of idioms by mt?s
hybrid research system vs. three commercial system. In
Proceedings of the EAMT, pp. 12?20.
Timothy Baldwin and Aline Villavicencio. 2002. Extract-
ing the unextractable: a case study on verb-particles. In
Proceedings of the COLING-02, pp. 1?7.
Colin Bannard and Chris Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Proceedings of the
43rd Annual Meeting of the ACL, pp. 597?604 .
Lea Cyrus. 2006. Building a resource for studying transla-
tion shifts. In Proceedings of the 5th LREC, pp. 1240?
1245.
Helge Dyvik. 2004. Translations as semantic mirrors. From
parallel corpus to WordNet. Language and Computers,
1:311 ? 326.
Katrin Erk and Sebastian Pado. 2009. Paraphrase assess-
ment in structured vector space: Exploring parameters and
datasets. In Proc. of the EACL GEMS Workshop, pp. 57?
65.
Joa?o de Almeida Varelas Grac?a, Joana Paulo Pardal, Lu??sa
Coheur, and Diamantino Anto?nio Caseiro. 2008. Multi-
language word alignments annotation guidelines. Techni-
cal report, Tech. Rep. 38 / 2008 INESC-ID Lisboa.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of the
NAACL ?03, pp. 48?54.
Philipp Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT Summit 2005, pp. 79?86.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data driven parser-generator for dependency
parsing. In Proc. of LREC-2006, pp. 2216?2219.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Pavel Pecina. 2008. A machine learning approach to multi-
word expression extraction. In Proceedings of the LREC
MWE 2008 Workshop, pp. 54?57.
Ivan A. Sag, Timothy Baldwin, Francis Bond, and Ann
Copestake. 2002. Multiword expressions: A pain in the
neck for NLP. In Proc. of the CICLing-2002, pp. 1?15.
Frank Smadja and Kathleen McKeown. 1994. Translating
collocations for use in bilingual lexicons. In Proceedings
of the HLT ?94 workshop, pp. 152?156.
Begon?a Villada Moiro?n and Jo?rg Tiedemann. 2006.
Identifying idiomatic expressions using automatic word-
alignment. In Proc. of the EACL MWE 2006 Workshop,
pp. 33?40.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Comput.
Linguist., 23(3):377?403.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via robust
projection across aligned corpora. In Proceedings of HLT
2001, pp. 1?8.
30
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 206?209,
Paris, October 2009. c?2009 Association for Computational Linguistics
Using a maximum entropy-based tagger to improve a very fast vine parser
Anders S?gaard
Center for Language Technology
University of Copenhagen
soegaard@hum.ku.dk
Jonas Kuhn
Dpt. of Linguistics
University of Potsdam
kuhn@ling.uni-potsdam.de
Abstract
In this short paper, an off-the-shelf maxi-
mum entropy-based POS-tagger is used as
a partial parser to improve the accuracy of
an extremely fast linear time dependency
parser that provides state-of-the-art results
in multilingual unlabeled POS sequence
parsing.
1 Introduction
The dependency parsing literature has grown in all
directions the past 10 years or so. Dependency
parsing is used in a wide variety of applications,
and many different parsing techniques have been
proposed.
Two dependency parsers have become more
popular than the rest, namely MSTParser (Mc-
Donald et al, 2005) and MaltParser (Nivre et
al., 2007). MSTParser is slightly more accu-
rate than MaltParser on most languages, especially
when dependencies are long and non-projective,
but MaltParser is theoretically more efficient as it
runs in linear time. Both are relatively slow in
terms of training (hours, sometimes days), and rel-
atively big models are queried in parsing.
MSTParser and MaltParser can be optimized for
speed in various ways,1 but the many applications
of dependency parsers today may turn model size
into a serious problem. MSTParser typically takes
about a minute to parse a small standard test suite,
say 2?300 sentences; the stand-alone version of
MaltParser may take 5?8 minutes. Such parsing
times are problematic in, say, a machine transla-
tion system where for each sentence pair multiple
1Recent work has optimized MaltParser considerably for
speed. Goldberg and Elhadad (2008) speed up the MaltParser
by a factor of 30 by simplifying the decision function for the
classifiers. Parsing is still considerably slower than with our
vine parser, i.e. a test suite is parsed in about 15?20 seconds,
whereas our vine parser parses a test suite in less than two
seconds.
target sentences are parsed (Charniak et al, 2003;
Galley and Manning, 2009). Since training takes
hours or days, researchers are also more reluctant
to experiment with new features, and it is very
likely that the features typically used in parsing
are suboptimal in, say, machine translation.
Conceptually simpler dependency parsers are
also easier to understand, which makes debugging,
cross-domain adaption or cross-language adapta-
tion a lot easier. Finally, state-of-the-art depen-
dency parsers may in fact be outperformed by sim-
pler systems on non-standard test languages with,
say, richer morphology or more flexible word or-
der.
Vine parsing is a parsing strategy that guaran-
tees fast parsing and smaller models, but the ac-
curacy of dependency-based vine parsers has been
non-competitive (Eisner and Smith, 2005; Dreyer
et al, 2006).
This paper shows how the accuracy of
dependency-based vine parsers can be improved
by 1?5% across six very different languages with
a very small cost in training time and practically
no cost in parsing time.
The main idea in our experiments is to use
a maximum entropy-based part-of-speech (POS)
tagger to identify roots and tokens whose heads
are immediately left or right of them. These are
tasks that a tagger can solve. You simply read
off a tagged text from the training, resp. test, sec-
tion of a treebank and replace all tags of roots,
i.e. tokens whose syntactic head is an artificial root
node, with a new tag ROOT. You then train on
the training section and apply your tagger on the
test section. The decisions made by the tagger
are then, subsequently, used as hard constraints by
your parser. When the parser then tries to find root
nodes, for instance, it is forced to use the roots as-
signed by the tagger. This strategy is meaningful
if the tagger has better precision for roots than the
parser. If it has better recall than the parser, the
206
parser may be forced to select roots only from the
set of potential roots assigned by the tagger. In our
experiments, only the first strategy was used (since
the tagger?s precision was typically better than its
recall).
The dependency parser used in our experiments
is very simple. It is based on the Chu-Liu-
Edmonds algorithm (Edmonds, 1967), which is
also used in the MSTParser (McDonald et al,
2005), but it is informed only by a simple MLE
training procedure and omits cycle contraction in
parsing. This means that it produces cyclic graphs.
In the context of poor training, insisting on acyclic
output graphs often compromises accuracy by >
10%. On top of this parser, which is super fast but
often does not even outperform a simple structural
baseline, hard and soft constraints on dependency
length are learned discriminatively. The speed of
the parser allows us to repeatedly parse a tuning
section to optimize these constraints. In particular,
the tuning section (about 7500 tokens) is parsed
a fixed number of times for each POS/CPOS tag
to find the optimal dependency length constraint
when that tag is the tag of the head or dependent
word. In general, this discriminative training pro-
cedure takes about 10 minutes for an average-sized
treebank. The parser only produces unlabeled de-
pendency graphs and is still under development.
While accuracy is below state-of-the-art results,
our improved parser significantly outperforms a
default version of the MaltParser that is restricted
to POS tags only, on 5/6 languages (p ? 0.05),
and it significantly outperforms the baseline vine
parser on all languages.
2 Data
Our languages are chosen from different language
families. Arabic is a Semitic language, Czech is
Slavic, Dutch is Germanic, Italian is Romance,
Japanese is Japonic-Ryukyuan, and Turkish is
Uralic. All treebanks, except Italian, were also
used in the CONLL-X Shared Task (Buchholz and
Marsi, 2006). The Italian treebank is the law
section of the TUT Treebank used in the Evalita
2007 Dependency Parsing Challenge (Bosco et al,
2000).
3 Experiments
The Python/C++ implementation of the maximum
entropy-based part-of-speech (POS) tagger first
described in Ratnaparkhi (1998) that comes with
the maximum entropy library in Zhang (2004) was
used to identify arcs to the root node and to tokens
immediately left or right of the dependent. This
was done by first extracting a tagged text from
each treebank with dependents of the root node as-
signed a special tag ROOT. Similarly, tagged texts
were extracted in which dependents of their im-
mediate left, resp. right neighbors, were assigned a
special tag. Our tagger was trained on the texts ex-
tracted from the training sections of the treebanks
and evaluated on the texts extracted from the test
sections. The number of gold standard, resp. pre-
dicted, ROOT/LEFT/RIGHT tags are presented in
Figure 1. Precision and f-score are also computed.
Note that since our parser uses information from
our tagger as hard constraints, i.e. it disregards
arcs to the root node or immediate neighbors not
predicted by our tagger, precision is really what
is important, not f-score. Or more precisely, preci-
sion indicates if our tagger is of any help to us, and
f-score tells us to what extent it may be of help.
4 Results
The results in Figure 2 show that using a maxi-
mum entropy-based POS tagger to identify roots
(ROOT), tokens with immediate left heads (LEFT)
and tokens with immediate (RIGHT) heads im-
proves the accuracy of a baseline vine parser
across the board for all languages measured in
terms of unlabeled attachment score (ULA), or de-
creases are insignificant (Czech and Turkish). For
all six languages, there is a combination of ROOT,
LEFT and RIGHT that significantly outperforms
the vine parser baseline. In 4/6 cases, absolute im-
provements are ? 2%. The score for Dutch is im-
proved by > 4%. The extended vine parser is also
significantly better than the MaltParser restricted
to POS tags on 5/6 languages. MaltParser is prob-
ably better than the vine parser wrt. Japanese be-
cause average sentence length in this treebank is
very short (8.9); constraints on dependency length
do not really limit the search space.
In spite of the fact that our parser only uses POS
tags (except for the maximum entropy-based tag-
ger which considers both words and tags), scores
are now comparable to more mature dependency
parsers: ULA excl. punctuation for Arabic is
70.74 for Vine+ROOT+LEFT+RIGHT which is
better than six of the systems who participated in
the CONLL-X Shared Task and who had access to
all data in the treebank, i.e. tokens, lemmas, POS
207
Arabic Gold Predicted Precision F-score
ROOT 443 394 89.09 83.87
LEFT 3035 3180 84.28 86.24
RIGHT 313 196 82.14 63.26
Czech Gold Predicted Precision F-score
ROOT 737 649 85.36 79.94
LEFT 1485 1384 85.12 82.12
RIGHT 1288 1177 87.51 83.57
Dutch Gold Predicted Precision F-score
ROOT 522 360 74.44 60.77
LEFT 1734 1595 87.02 83.39
RIGHT 1300 1200 87.00 83.52
Italian Gold Predicted Precision F-score
ROOT 100 58 74.36 65.17
LEFT 1601 1640 90.30 91.39
RIGHT 192 129 84.87 74.14
Japanese Gold Predicted Precision F-score
ROOT 939 984 85.06 87.05
LEFT 1398 1382 97.76 97.19
RIGHT 2838 3016 92.27 95.08
Turkish Gold Predicted Precision F-score
ROOT 694 685 85.55 84.99
LEFT 750 699 91.70 88.47
RIGHT 3433 3416 84.19 83.98
Figure 1: Tag-specific evaluation of our tagger on the extracted texts.
Arabic Czech Dutch Italian Japanese Turkish
MaltParser 66.22 67.78 65.03 75.48 89.13 68.94
Vine 67.99 66.70 65.98 75.50 83.15 68.53
Vine+ROOT 68.68 66.65 66.21 78.06 83.82 68.45
Vine+ROOT+LEFT 69.68 68.14 68.05 77.14 84.64 68.37
Vine+RIGHT 68.50 67.38 68.18 78.55 84.17 69.87
Vine+ROOT+RIGHT 69.20 67.32 68.40 78.29 84.78 69.79
Vine+ROOT+LEFT+RIGHT 70.28 68.70 70.06 77.26 85.45 69.74
Figure 2: Labeled attachment scores (LASs) for MaltParser limited to POS tags, our baseline vine parser
(Vine) and our extensions of Vine. Best scores bold-faced.
208
tags, features and dependency relations; not just
the POS tags as in our case. In particular, our re-
sult is 2.28 better than Dreyer et al (2006) who
also use soft and hard constraints on dependency
lengths. They extend the parsing algorithm in Eis-
ner and Smith (2005) to labeled k-best parsing and
use a reranker to find the best parse according to
predefined global features. ULA excl. punctuation
for Turkish is 67.06 which is better than six of the
shared task participants, incl. Dreyer et al (2006)
(60.45).
The improvements come at an extremely low
cost. The POS tagger simply stores its decisions
in a very small table, typically 5?10 cells per sen-
tence, that is queried in no time in parsing. Pars-
ing a standard small test suite takes less than two
seconds, and the cost of the additional look-up is
too small to be measured. The training time of the
maximum entropy-based tagger is typically a mat-
ter of seconds or half a minute. Even running it on
the 1249k Prague Dependency Treebank (Czech)
is only a matter of minutes.
5 Conclusion and future work
Vine parsers are motivated by efficiency and ro-
bustness (Dreyer et al, 2006), which has become
more and more important over the last few years,
but none of the systems introduced in the liter-
ature provide competitive results in terms of ac-
curacy. Our experiments show how dependency-
based vine parsers can be significantly improved
by using a maximum entropy-based POS tagger
for initial partial parsing with almost no cost in
terms of training and parsing time.
Our choice of parser restricted us in a few re-
spects. Most importantly, our results are below
state-of-the-art results, and it is not clear if the
strategy scales to more accurate parsers. The strat-
egy of using a POS tagger to do partial parsing and
subsequently forward high precision decisions to
a parser only works on graph-based or constraint-
based dependency parsers where previous deci-
sions can be hardwired into candidate weight ma-
trices by setting weights to 0. It would be difficult
if at all possible to implement in history-based de-
pendency parsers such as MaltParser. Experiments
will be performed with the MSTParser soon.
Our parser also restricted us to considering un-
labeled dependency graphs. A POS tagger, how-
ever, can also be used to identify grammatical
functions (subjects, objects, . . . ), for example,
which may be used to hardwire dependency rela-
tions into candidate weight matrices. POS taggers
may also be used to identify other dependency re-
lations or more fine-grained features that can im-
prove the accuracy of dependency parsers.
References
Cristina Bosco, Vincenzo Lombardo, Daniela Vassallo,
and Leonardo Lesmo. 2000. Building a treebank for
Italian. In LREC, pages 99?105, Athens, Greece.
Sabine Buchholz and Erwin Marsi. 2006. CONLL-X
shared task on multilingual dependency parsing. In
CONLL-X, pages 149?164, New York City, NY.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for statistical
machine translation. In MT Summit IX, New Or-
leans, Louisiana.
Markus Dreyer, David A. Smith, and Noah A. Smith.
2006. Vine parsing and minimum risk reranking for
speed and precision. In CONLL-X, pages 201?205,
New York City, NY.
J. Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards,
71:233?240.
Jason Eisner and Noah A. Smith. 2005. Parsing with
soft and hard constraints on dependency length. In
IWPT?05, pages 30?41, Vancouver, Canada.
Michel Galley and Cristopher Manning. 2009.
Quadratic time dependency parsing for machine
translation. In ACL?09, Singapore, Singapore. To
appear.
Yoav Goldberg and Michael Elhadad. 2008.
splitSVM: fast, space-efficient, non-heuristic, poly-
nomial kernel computation for NLP applications. In
ACL?08, Short Papers, pages 237?240, Columbus,
Ohio.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In HLT-EMNLP
2005, pages 523?530, Vancouver, British Columbia.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CONLL 2007 shared task on
dependency parsing. In EMNLP-CONLL?07, pages
915?932, Prague, Czech Republic.
Adwait Ratnaparkhi. 1998. Maximum entropy mod-
els for natural language ambiguity resolution. Ph.D.
thesis, University of Pennsylvania.
Le Zhang. 2004. Maximum entropy mod-
eling toolkit for Python and C++. Uni-
versity of Edinburgh. Available at home-
pages.inf.ed.ac.uk/lzhang10/maxent toolkit.html.
209
Coling 2010: Poster Volume, pages 1122?1130,
Beijing, August 2010
Informed ways of improving data-driven
dependency parsing for German
Wolfgang Seeker
University of Stuttgart
Inst. fu?r Maschinelle Sprachverarbeitung
seeker@ims.uni-stuttgart.de
Bernd Bohnet
University of Stuttgart
Inst. fu?r Maschinelle Sprachverarbeitung
Bernd.Bohnet@ims.uni-stuttgart.de
Lilja ?vrelid
University of Potsdam
Institut fu?r Linguistik
ovrelid@uni-potsdam.de
Jonas Kuhn
University of Stuttgart
Inst. fu?r Maschinelle Sprachverarbeitung
jonas@ims.uni-stuttgart.de
Abstract
We investigate a series of targeted modifi-
cations to a data-driven dependency parser
of German and show that these can be
highly effective even for a relatively well
studied language like German if they are
made on a (linguistically and methodolog-
ically) informed basis and with a parser
implementation that allows for fast and
robust training and application. Mak-
ing relatively small changes to a range
of very different system components, we
were able to increase labeled accuracy on
a standard test set (from the CoNLL 2009
shared task), ignoring gold standard part-
of-speech tags, from 87.64% to 89.40%.
The study was conducted in less than five
weeks and as a secondary project of all
four authors. Effective modifications in-
clude the quality and combination of auto-
assigned morphosyntactic features enter-
ing machine learning, the internal feature
handling as well as the inclusion of global
constraints and a combination of different
parsing strategies.
1 Introduction
The past years have seen an enormous surge of in-
terest in dependency parsing, mainly in the data-
driven paradigm, and with a particular emphasis
on covering a whole set of languages with a single
approach. The reasons for this interest are mani-
fold; the availability of shared task data from var-
ious CoNLL conferences (among others (Buch-
holz and Marsi, 2006; Hajic? et al, 2009)), com-
prising collections of languages based on a sin-
gle representation format, has certainly been in-
strumental. But likewise, the straightforward use-
fulness of dependency representations for a num-
ber of tasks plays an important role. The rela-
tive language independence of the representations
makes dependency parsing particularly attractive
for multilingually oriented work, including ma-
chine translation.
As data-driven approaches to dependency pars-
ing have reached a certain level of maturity, it may
appear as if further improvements of parsing per-
formance have to rely on relatively advanced tun-
ing procedures, such as sophisticated automatic
feature selection procedures or combinations of
different parsing approaches with complementary
strengths. It is indeed still hard to pinpoint the
structural properties of a language (or annotation
scheme) that make the parsing task easier for a
particular approach, so it may seem best to leave
the decision to a higher-level procedure.
This paper starts from the suspicion that
while sophisticated tuning procedures are cer-
tainly helpful, one should not underestimate the
potential of relatively simple modifications of the
experimental set-up, such as a restructuring of as-
pects of the dependency format, a targeted im-
provement of the quality of automatically as-
signed features, or a simplification of the feature
space for machine learning ? the modifications
just have to be made in an informed way. This
1122
presupposes two things: (i) a thorough linguistic
understanding of the issues at hand, and (ii) a rel-
atively powerful and robust experimental machin-
ery which allows for experimentation in various
directions and which should ideally support a fast
turn-around cycle.
We report on a small pilot study exploring the
potential of relatively small, informed modifica-
tions as a way of improving parsing accuracy
even for a language that has received considerable
attention in the parsing literature, including the
dependency parsing literature, namely German.
Within a timeframe of five weeks and spending
only a few hours a day on the project (between a
group of four people), we were able to reach some
surprising improvements in parsing accuracy.
By way of example, we experimented with
modifications in a number of rather different sys-
tem areas, which we will discuss in the course
of this paper after a brief discussion of related
work and the data basis in Section 2. Based on a
second-order maximum spanning tree algorithm,
we used a hash kernel to facilitate the mapping
of the features onto their weights for a very large
number of features (Section 3); we modified the
dependency tree representation for prepositional
phrases, adding hierarchical structure that facili-
tates the picking up of generalizations (Section 4).
We take advantage of a morphological analyzer
to train an improved part-of-speech tagger (Sec-
tion 5), and we use knowledge about the structure
of morphological paradigms and the morphology-
syntax interface in the feature design for machine
learning (Section 6). As is known from other stud-
ies, the combination of different parsing strategies
is advantageous; we include a relatively simple
parser stacking procedure in our pilot study (Sec-
tion 7), and finally, we apply Integer Linear Pro-
gramming in a targeted way to add some global
constraints on possible combinations of arc labels
with a single head (Section 8). Section 9 offers a
brief conclusion.
2 Related Work and Data Basis
We quickly review the situation in data-driven de-
pendency parsing in general and on applying it to
German specifically.
The two main approaches to data-driven de-
pendency parsing are transition based dependency
parsing (Nivre, 2003; Yamada and Matsumoto,
2003; Titov and Henderson, 2007) and maximum
spanning tree based dependency parsing (Eis-
ner, 1996; Eisner, 2000; McDonald and Pereira,
2006). Transition based parsers typically have
a linear or quadratic complexity (Attardi, 2006).
Nivre (2009) introduced a transition based non-
projective parsing algorithm that has a worst case
quadratic complexity and an expected linear pars-
ing time. Titov and Henderson (2007) combined
a transition based parsing algorithm, using beam
search, with a latent variable machine learning
technique.
Maximum spanning tree based dependency
parsers decompose a dependency structure into
factors. The factors of the first order maximum
spanning tree parsing algorithm are edges consist-
ing of the head, the dependent (child) and the edge
label. This algorithm has a quadratic complexity.
The second order parsing algorithm of McDonald
and Pereira (2006) uses a separate algorithm for
edge labeling. In addition to the first order fac-
tors, this algorithm uses the edges to those chil-
dren which are closest to the dependent and has a
complexity of O(n3). The second order algorithm
of Carreras (2007) uses in addition to McDonald
and Pereira (2006) the child of the dependent oc-
curring in the sentence between the head and the
dependent as well as the edge from the dependents
to a grandchild. The edge labeling is an integral
part of the algorithm which requires an additional
loop over the labels. This algorithm therefore has
a complexity of O(n4). Johansson and Nugues
(2008) reduced the required number of loops over
the edge labels by considering only the edges that
existed in the training corpus for a distinct head
and child part-of-speech tag combination.
Predating the surge of interest in data-based
dependency parsing, there is a relatively long
tradition of dependency parsing work on Ger-
man, including for instance Menzel and Schro?der
(1998) and Duchier and Debusmann (2001). Ger-
man was included in the CoNLL shared tasks in
2006 (Multilingual Dependency Parsing, (Buch-
holz and Marsi, 2006)) and in 2009 (Syntactic and
Semantic Dependencies in Multiple Languages,
(Hajic? et al, 2009)) with data based on the TIGER
1123
corpus (Brants et al, 2002) in both cases. Since
the original TIGER treebank is in a hybrid phrase-
structural/dependency format with a relatively flat
hierarchical structure, conversion to a pure depen-
dency format involves some non-trivial steps. The
2008 ACL Workshop on Parsing German included
a specific shared task on dependency parsing of
German (Ku?bler, 2008), based on two sets of data:
again the TIGER corpus ? however with a differ-
ent conversion routine than for the CoNLL tasks ?
and the Tu?Ba-D/Z corpus (Hinrichs et al, 2004).
In the 2006 CoNLL task and in the 2008 ACL
Workshop task, the task was dependency parsing
with given gold standard part-of-speech tags from
the corpus. This is a valid way of isolating the
specific subproblem of parsing, however it is clear
that the task does not reflect the application set-
ting which includes noise from automatic part-of-
speech tagging. In the 2009 CoNLL task, both
gold standard tags and automatically assigned tags
were provided. The auto-tagged version was cre-
ated with the standard model of the TreeTagger
(Schmid, 1995) (i.e., with no domain-specific tag-
ger training).
In our experiments, we used the data set from
the 2009 CoNLL task, for which the broadest
comparison of recent parsing approaches exists.
The highest-scoring system in the shared task was
Bohnet (2009) with a labeled accuracy (LAS) of
87.48%, on auto-tagged data. The highest-scoring
(in fact the only) system in the dependency pars-
ing track of the 2008 ACL Workshop on parsing
German was Hall and Nivre (2008) with an LAS
of 90.80% on gold-tagged data, and with a data
set that is not comparable to the CoNLL data.1
3 Hash Kernel
Our parser is based on a second order maximum
spanning tree algorithm and uses MIRA (Cram-
mer et al, 2006) as learning technique in combi-
nation with a hash kernel. The hash kernel has
a higher accuracy since it can use additional fea-
tures found during the creation of the dependency
1To get an idea of how the data sets compare, we trained
the version of our parser described in Section 3 (i.e., with-
out most of the linguistically informed improvements) on
this data, achieving labeled accuracy of 92.41%, compared
to 88.06% for the 2009 CoNLL task version.
tree in addition to the features extracted from the
training examples. The modification to MIRA is
simple: we replace the feature-index mapping that
maps the features to indices of the weight vector
by a random function. Usually, the feature-index
mapping in the support vector machine has two
tasks: The mapping maps the features to an index
and it filters out features that never occurred in a
dependency tree. In our approach, we do not filter
out these features, but use them as additional fea-
tures. It turns out that this choice improves pars-
ing quality. Instead of the feature-index mapping
we use the following hash function:2
h ? |(l xor(l ? 0xffffffff00000000 >> 32))% size|
The Hash Kernel for structured data uses the hash
function h : J ? {1...n} to index ? where ?
maps the observations X to a feature space. We
define ?(x, y) as the numeric feature representa-
tion indexed by J . The learning problem is to fit
the function F so that the errors of the predicted
parse tree y are as low as possible. The scoring
function of the Hash Kernel is defined as:3
F (x, y) = ??w ? ?(x, y)
For different j, the hash function h(j) might gen-
erate the same value k. This means that the hash
function maps more than one feature to the same
weight which causes weight collisions. This pro-
cedure is similar to randomization of weights (fea-
tures), which aims to save space by sharing val-
ues in the weight vector (Blum, 2006; Rahimi
and Recht, 2008). The Hash Kernel shares values
when collisions occur that can be considered as
an approximation of the kernel function, because
a weight might be adapted due to more than one
feature. The approximation works very well with
a weight vector size of 115 million values.
With the Hash Kernel, we were able to improve
on a baseline parser that already reaches a quite
high LAS of 87.64% which is higher than the top
score for German (87.48%) in the CoNLL Shared
task 2009. The Hash Kernel improved that value
by 0.42 percentage points to 88.06%. In addition
to that, we obtain a large speed up in terms of pars-
ing time. The baseline parser spends an average of
426 milliseconds to parse a sentence of the test
2>> n shifts n bits right, and % is the modulo operation.
3??w is the weight vector and the size of ??w is n.
1124
set and the parser with Hash Kernel only takes
126 milliseconds which is an increase in speed
of 3.4 times. We get the large speed up because
the memory access to a large array causes many
CPU cache misses which we avoid by replacing
the feature-index mapping with a hash function.
As mentioned above, the speedup influences the
experimenters? opportunities for explorative de-
velopment since it reduces the turnaround time for
experimental trials.
4 Restructuring of PPs
In a first step, we applied a treebank transforma-
tion to our data set in order to ease the learning
for the parser. We concentrated on prepositional
phrases (PP) to get an idea how much this kind
of transformation can actually help a parser. PPs
are notoriously flat in the TIGER Treebank anno-
tation (from which our data are derived) and they
do not embed a noun phrase (NP) but rather attach
all parts of the noun phrase directly at PP level.
This annotation was kept in the dependency ver-
sion and it can cause problems for the parser since
there are two different ways of annotating NPs: (i)
for normal NPs where all dependents of the noun
are attached as daughters of the head noun and (ii)
for NPs in PPs where all dependents of the noun
are attached as daughters to the preposition thus
being sisters to their head noun. We changed the
annotation of PPs by identifying the head noun in
the PP and attaching all of its siblings to it. To find
the correct head, we used a heuristic in the style of
Magerman (1995). The head is chosen by taking
the rightmost daughter of the preposition that has
a category label according to the heuristic and is
labeled with NK (noun kernel element).
Table 1 shows the parser performance on the
data after PP-restructuring.4 The explanation for
the benefit of the restructuring is of course that
4Note that we are evaluating against a gold standard here
(and in the rest of the paper) which has been restructured as
well. With a different gold standard one could argue that the
absolute figures we obtain are not fully comparable with the
original CoNLL shared task. However, since we are doing
dependency parsing, the transformation does neither add nor
remove any nodes from the structure nor do we change any
labels. The only thing that is done during the transforma-
tion is the reattachment of some daughters of a PP. This is
only a small modification, and it is certainly linguistically
warranted.
now there is only one type of NP in the whole cor-
pus which eases the parser?s task to correctly learn
and identify them.
dev. set test set
LAS UAS LAS UAS
hash kernel 87.40 89.79 88.06 90.24
+restructured 87.49 89.97 88.30 90.44
Table 1: Parser performance on restructured data
Since restructuring parts of the corpus seems
beneficial, there might be other structures where
more consistent annotation could help the parser,
e. g., coordination or punctuation (like in the 2008
ACL Workshop data set, cp. Footnote 1).
5 Part-of-Speech Tagging
High quality part-of-speech (PoS) tags can greatly
improve parsing quality. Having a verb wrongly
analyzed as a noun and similar mistakes are very
likely to mislead the parser in its decision process.
A lot of the parser?s features include PoS tags and
reducing the amount of errors during PoS tagging
will therefore reduce misleading feature values as
well. Since the quality of the automatically as-
signed PoS tags in the German CoNLL ?09 data
is not state-of-the-art (see Table 2 below), we de-
cided to retag the data with our own tagger which
uses additional information from a symbolic mor-
phological analyzer to direct a statistical classifier.
For the assignment of PoS tags, we apply
a standard maximum entropy classification ap-
proach (see Ratnaparkhi (1996)). The classes of
the classifier are the PoS categories defined in the
Stuttgart-Tu?bingen Tag Set (STTS) (Schiller et al,
1999). We use standard binarized features like
the word itself, its last three letters, whether the
word is capitalized, contains a hyphen, a digit or
whether it consists of digits only. As the only non-
binary feature, word length is recorded. These
standard features are augmented by a number of
binary features that support the classification pro-
cess by providing a preselection of possible PoS
tags. Every word is analyzed by DMOR, a finite
state morphological analyzer, from whose output
analyses all different PoS tags are collected and
added to the feature set. For example, DMOR
assigns the PoS tags NN (common noun) and
ADJD (predicative adjective) to the word gegan-
1125
gen (gone). From these analyses two features are
generated, namely possible-tag:NN and possible-
tag:ADJD, which are strong indicators for the
classifier that one of these classes is very likely
to be the correct one. The main idea here is to
use the morphological analyzer as a sort of lexicon
that preselects the set of possible tags beforehand
and then use the classifier to do the disambigua-
tion (see Jurish (2003) for a more sophisticated
system based on Hidden-Markov models that uses
roughly the same idea). Since the PoS tags are in-
cluded in the feature set, the classifier is still able
to assign every class defined in STTS even if it is
not in the preselection. Where the morphological
analyzer does not know the word in question we
add features for every PoS tag representing a pro-
ductive word class in German, making the reason-
able assumption that the morphology knows about
all closed-class words and word forms. Finally,
we add word form and possible tag features for
the previous and the following word to the feature
set thus simulating a trigram tagger. We used the
method of Kazama and Tsujii (2005) which uses
inequality constraints to do a very efficient feature
selection5 to train the maximum entropy model.
We annotated the entire corpus with versions
of our own tagger, i.e., the training, development
and test data. In order to achieve a realistic be-
havior (including remaining tagging errors, which
the parser may be able to react to if they are sys-
tematic), it was important that each section was
tagged without any knowledge of the gold stan-
dard tags. For the development and test portion,
this is straightforward: we trained a model on the
gold PoS of the training portion of the data and
applied it to retag these two portions. Retagging
the training portion was a bit trickier since we
could not use a model trained on the same data,
but at the same time, we wanted to use a tagger
of similarly high quality ? i.e. one that has seen a
similar amount of training data. The training set
was therefore split into 20 different parts and for
every split, a tagging model was trained on the
other 19 parts which then was used to retag the
remaining 20th part. Table 2 shows the quality
of our tagger evaluated on the German CoNLL
5We used a width factor of 1.0.
?09 data in terms of accuracy and compares it
to the originally annotated PoS tags which have
been assigned by using the TreeTagger (Schmid,
1995) together with the German tagging model
provided from the TreeTagger website. Tagging
accuracy improves consistently by about 2 per-
centage points which equates to an error reduction
of 44.55 % to 49.0 %.
training development test
original 95.69 95.51 95.46
retagged 97.61 97.71 97.52
error red. 44.55% 49.00% 45.37%
Table 2: Tagging accuracy
Table 3 shows the parser performance when
trained on the newly tagged data. The consider-
able improvements in tagging accuracy visibly af-
fect parsing accuracy, raising both the labeled and
the unlabeled attachment score by 0.66 percentage
points (LAS) and 0.51 points (UAS) for the de-
velopment set and by 0.45 points (LAS) and 0.64
points (UAS) for the test set.
dev. set test set
LAS UAS LAS UAS
restructured 87.49 89.97 88.30 90.44
+retagged 88.15 90.48 88.75 91.08
Table 3: Parser performance on retagged data
6 Morphological Information
German, as opposed to English, exhibits a rela-
tively rich morphology. Predicate arguments and
nominal adjuncts are marked with special case
morphology which allows for a less restricted
word order in German. The German case system
comprises four different case values, namely nom-
inative, accusative, dative and genitive case. Sub-
jects and nominal predicates are usually marked
with nominative case, objects receive accusative
or dative case and genitive case is usually used
to mark possessors in possessive constructions.
There are also some temporal and spatial nominal
adjuncts which require certain case values. Since
case is used to mark the function of a noun phrase
in a clause, providing case information to a parser
might improve its performance.
The morphological information in the German
CoNLL ?09 data contains much more information
than case alone and previous models (baseline,
1126
hash kernel, retagged) have used all of it. How-
ever, since we aim to improve a syntactic parser,
we would like to exclude all morphological infor-
mation from the parsing process that is not obvi-
ously relevant to syntax, e. g. mood or tense. By
reducing the morphological annotations to those
that are syntactically relevant, we hope to reduce
the noise that is introduced by irrelevant informa-
tion. (One might expect that machine learning and
feature selection should ?filter out? irrelevant fea-
tures, but given the relative sparsity of unambigu-
ous instances of the linguistically relevant effects,
drawing the line based on just a few thousand sen-
tences of positive evidence would be extremely
hard even for a linguist.)
We annotated every case-bearing word in the
corpus with its case information using DMOR.
With case-bearing words, we mean nouns, proper
nouns, attributive adjectives, determiners and all
kinds of pronouns. Other types of morphologi-
cal information was discarded. We did not use
the manually annotated and disambiguated mor-
phological information already present in the cor-
pus for two reasons: the first one is the same as
with the PoS tagging. Since it is unrealistic to
have gold-standard annotation in a real-world ap-
plication which deals with unseen data, we want
the parser to learn from and hopefully adapt to
imperfectly annotated data. The second reason
is the German-inherent form syncretism in nom-
inal paradigms. The German noun inflection sys-
tem is with over ten different (productive and
non-productive) inflectional patterns quite com-
plicated, and to make matters worse, there are
only five different morphological markers to dis-
tinguish 16 different positions in the pronoun, de-
terminer and adjective paradigms and eight differ-
ent positions in the noun paradigms. Some po-
sitions in the paradigm will therefore always be
marked in the same way and we would like the
parser to learn that some word forms will always
be ambiguous with respect to their case value.
We also conducted experiments where we an-
notated number and gender values in addition to
case. The idea behind this is that number and gen-
der might help to further disambiguate case val-
ues. The downside of this is the increase in fea-
ture values. Combining case and number features
means a multiplication of their values creating
eight new feature values instead of four. Adding
gender annotation raises this number to 24. Be-
side the disambiguation of case, there is also an-
other reason why we might want to add num-
ber and gender: Inside a German noun phrase,
all parts have to agree on their case and number
feature in order to produce a well-formed noun
phrase. Furthermore, the head noun governs the
gender feature of the other parts. Thus, all three
features can be relevant to the construction of a
syntactic structure.6 Table 4 shows the results of
our experiments with morphological features.
dev. set test set
LAS UAS LAS UAS
retagged 88.15 90.48 88.75 91.08
no morph. 87.78 90.18 88.60 90.92
+case 88.04 90.48 88.77 91.13
+c+n 88.21 90.62 88.88 91.13
+c+n+g 87.96 90.33 88.73 90.99
Table 4: Parser performance with morph. infor-
mation (c=case, n=number, g=gender)
The no morph row in Table 4 shows, that
using no morphological information at all de-
creases parser performance. When only case val-
ues are annotated, the parser performance does
not change much in comparison to the retagged
model, so there is no benefit here. Adding num-
ber features on the other hand improves parsing
results significantly. This seems to support our in-
tuition that number helps in disambiguating case
values. However, adding gender information does
not further increase this effect but hurts parser per-
formance even more than case annotation alone.
This leaves us with a puzzle here. Annotating case
and number helps the parser, but case alone or
having case, number and gender together affects
performance negatively. A possible explanation
might be that the effect of the gender information
is masked by the increased number of feature val-
ues (24) which confuses the parsing algorithm.
7 Parser Stacking
Nivre and McDonald (2008) show how two dif-
ferent approaches to data-driven dependency pars-
6Person would be another syntactically relevant informa-
tion. However, since we are dealing with a newspaper cor-
pus, first and second person features appear very rarely.
1127
ing, the graph-based and transition-based ap-
proaches, may be combined and subsequently
learn to complement each other to achieve im-
proved parsing results for different languages.
MaltParser (Nivre et al, 2006) is a language-
independent system for data-driven dependency
parsing which is freely available.7 It is based on a
deterministic parsing strategy in combination with
treebank-induced classifiers for predicting parsing
actions. MaltParser employs a rich feature repre-
sentation in order to guide parsing. For the train-
ing of the Malt parser model that we use in the
stacking experiments, we use learner and parser
settings identical to the ones optimized for Ger-
man in the CoNLL-X shared task (Nivre et al,
2006). Furthermore, we employ the technique
of pseudo-projective parsing described in Nilsson
and Nivre (2005) and a split prediction strategy for
predicting parse transitions and arc labels (Nivre
and Hall, 2008).8 In order to obtain automatic
parses for the whole data set, we perform a 10-
fold split. For the parser stacking, we follow the
approach of Nivre and McDonald (2008), using
MaltParser as a guide for the MST parser with the
hash kernel, i.e., providing the arcs and labels as-
signed by MaltParser as features. Table 5 shows
the scores we obtain by parser stacking. Although
our version of MaltParser does not quite have the
same performance as for instance the version of
Hall and Nivre (2008), its guidance leads to a
small improvement in the overall parsing results.
dev. set test set
LAS UAS LAS UAS
MaltParser 82.47 85.78 83.84 86.8
our parser 88.21 90.62 88.88 91.13
+stacking 88.42 90.77 89.28 91.40
Table 5: Stacked parser performance with guid-
ance by MaltParser
7http://maltparser.org
8The feature models make use of information about the
lexical form (FORM), the predicted PoS (PPOS) and the de-
pendency relation constructed thus far during parsing (DEP).
In addition, we make use of the predicted values for other
morphological features (PFEATS). We employ the arc-eager
algorithm (Nivre, 2003) in combination with SVM learners,
using LIBSVM with a polynomial kernel.
8 Relabeling
In the relabeling step, we pursue the idea that
some erroneous parser decisions concerning the
distribution of certain labels might be detected and
repaired in post-processing. In German and in
most other languages, there are syntactic restric-
tions on the number of subjects and objects that
a verb might select. The parser will learn this be-
havior during training. However, since it is using a
statistical model with a limited context, it can still
happen that two or more of the same grammati-
cal functions are annotated for the same verb. But
having two subjects annotated for a single verb
makes this particular clause uninterpretable for
subsequently applied tasks. Therefore, we would
like to detect those doubly annotated grammatical
functions and correct them in a controlled way.
The detection algorithm is simple: Running
over the words of the output parse, we check for
every word whether it has two or more daughters
annotated with the same grammatical function and
if we find one, we relabel all of its daughters.9 For
the relabeling, we applied a dependency-version
of the function labeler described in Seeker et al
(2010) which uses a maximum entropy classifier
that is restrained by a number of hard constraints
implemented as an Integer Linear Program. These
constraints model the aforementioned selectional
restrictions on the number of certain types of ver-
bal arguments. Since these are hard constraints,
the labeler is not able to annotate more than one
of those grammatical functions per verb. If we
count the number of sentences that contain doubly
annotated grammatical functions in the best pars-
ing results from the previous section, we get 189
for the development set and 153 for the test set.
About two thirds of the doubly annotated func-
tions are subjects and the biggest part of the re-
maining third are accusative objects which are the
most common arguments of German verbs.
Table 6 shows the final results after relabeling
the output of the best performing parser config-
uration from the previous section. The improve-
ments on the overall scores are quite small, which
9The grammatical functions we are looking for are SB
(subject), OA (accusative object), DA (dative), OG (genitive
object), OP (prepositional object), OC (clausal object), PD
(predicate) and OA2 (second accusative object).
1128
dev. set test set
LAS UAS LAS UAS
stacking 88.42 90.77 89.28 91.40
+relabeling 88.48 90.77 89.40 91.40
Table 6: Parse quality after relabeling
is partly due to the fact that the relabeling affects
only a small subset of all labels used in the data.
Furthermore, the relabeling only takes place if a
doubly annotated function is detected; and even
if the relabeling is applied we have no guarantee
that the labeler will assign the labels correctly (al-
though we are guaranteed to not get double func-
tions). Table 7 shows the differences in precision
and recall for the grammatical functions between
the original and the relabeled test set. As one can
see, scores stay mostly the same except for SB,
OA and DA. For OA, scores improve both in recall
and precision. For DA, we trade a small decrease
in precision for a huge improvement in recall and
vice versa for SB, but on a much smaller scale.
Generally spoken, relabeling is a local repair strat-
egy that does not have so much effect on the over-
all score but can help to get some important labels
correct even if the parser made the wrong deci-
sion. Note that the relabeler can only repair incor-
rect label decisions, it cannot help with wrongly
attached words.
original relabeled
rec prec rec prec
DA 64.2 83.2 74.7 79.6
OA 88.9 85.8 90.7 88.2
OA2 0.0 NaN 0.0 NaN
OC 95.2 93.5 95.1 93.7
OG 33.3 66.7 66.7 80.0
OP 54.2 80.8 54.2 79.9
PD 77.1 76.8 77.1 76.8
SB 91.0 90.6 90.7 93.7
Table 7: Improvements on grammatical functions
in the relabeled test set
9 Conclusion
We presented a sequence of modifications to a
data-driven dependency parser of German, depart-
ing from a state-of-the-art set-up in an imple-
mentation that allows for fast and robust train-
ing and application. Our pilot study tested what
can be achieved in a few weeks if the data-driven
technique is combined with a linguistically in-
formed approach, i.e., testing hypotheses of what
should be particularly effective in a very targeted
way. Most modifications were relatively small,
addressing very different dimensions in the sys-
tem, such as the handling of features in the Ma-
chine Learning, the quality and combination of
automatically assigned features and the ability to
take into account global constraints, as well as the
combination of different parsing strategies. Over-
all, labeled accuracy on a standard test set (from
the CoNLL 2009 shared task), ignoring gold stan-
dard part-of-speech tags, increased significantly
from 87.64% (baseline parser without hash ker-
nel) to 89.40%.10 We take this to indicate that a
targeted and informed approach like the one we
tested can have surprising effects even for a lan-
guage that has received relatively intense consid-
eration in the parsing literature.
Acknowledgements
We would like to thank Sandra Ku?bler, Yannick
Versley and Yi Zhang for their support. This work
was partially supported by research grants from
the Deutsche Forschungsgemeinschaft as part of
SFB 632 ?Information Structure? at the Univer-
sity of Potsdam and SFB 732 ?Incremental Speci-
fication in Context? at the University of Stuttgart.
References
Attardi, G. 2006. Experiments with a Multilanguage Non-
Projective Dependency Parser. In Proceedings of CoNLL,
pages 166?170.
Blum, A. 2006. Random Projection, Margins, Kernels, and
Feature-Selection. In LNCS, pages 52?68. Springer.
Bohnet, B. 2009. Efficient Parsing of Syntactic and Se-
mantic Dependency Structures. In Proceedings of CoNLL
2009).
Brants, Sabine, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the Workshop on Treebanks and Lin-
guistic Theories, Sozopol.
Buchholz, Sabine and Erwin Marsi. 2006. CoNLL-X Shared
Task on Multilingual Dependency Parsing. In In Proc. of
CoNLL, pages 149?164.
Carreras, X. 2007. Experiments with a Higher-order Projec-
tive Dependency Parser. In EMNLP/CoNLL.
10?= 0.01, measured with a tool by Dan Bikel from
www.cis.upenn.edu/? dbikel/download/compare.pl
1129
Crammer, K., O. Dekel, S. Shalev-Shwartz, and Y. Singer.
2006. Online Passive-Aggressive Algorithms. Journal of
Machine Learning Research, 7:551?585.
Duchier, Denys and Ralph Debusmann. 2001. Topologi-
cal dependency trees: a constraint-based account of linear
precedence. In Proceedings of ACL 2001, pages 180?
187, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Eisner, J. 1996. Three New Probabilistic Models for Depen-
dency Parsing: An Exploration. In Proceedings of Coling
1996, pages 340?345, Copenhaen.
Eisner, J., 2000. Bilexical Grammars and their Cubic-time
Parsing Algorithms, pages 29?62. Kluwer Academic
Publishers.
Hajic?, J., M. Ciaramita, R. Johansson, D. Kawahara,
M. Anto`nia Mart??, L. Ma`rquez, A. Meyers, J. Nivre,
S. Pado?, J. S?te?pa?nek, P. Stran?a?k, M. Surdeanu, N. Xue,
and Y. Zhang. 2009. The CoNLL-2009 Shared Task:
Syntactic and Semantic Dependencies in Multiple Lan-
guages. In Proceedings of the 13th CoNLL-2009, June
4-5, Boulder, Colorado, USA.
Hall, Johan and Joakim Nivre. 2008. A dependency-driven
parser for German dependency and constituency represen-
tations. In Proceedings of the Workshop on Parsing Ger-
man, pages 47?54, Columbus, Ohio, June. Association for
Computational Linguistics.
Hinrichs, Erhard, Sandra Ku?bler, Karin Naumann, Heike
Telljohann, and Julia Trushkina. 2004. Recent develop-
ments in linguistic annotations of the tu?ba-d/z treebank.
In Proceedings of the Third Workshop on Treebanks and
Linguistic Theories, pages 51?62, Tu?bingen, Germany.
Johansson, R. and P. Nugues. 2008. Dependency-based
Syntactic?Semantic Analysis with PropBank and Nom-
Bank. In Proceedings of the Shared Task Session of
CoNLL-2008, Manchester, UK.
Jurish, Bryan. 2003. A hybrid approach to part-of-speech
tagging. Technical report, Berlin-Brandenburgische
Akademie der Wissenschaften.
Kazama, Jun?Ichi and Jun?Ichi Tsujii. 2005. Maximum en-
tropy models with inequality constraints: A case study on
text categorization. Machine Learning, 60(1):159?194.
Ku?bler, Sandra. 2008. The PaGe 2008 shared task on pars-
ing german. In Proceedings of the Workshop on Parsing
German, pages 55?63, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Magerman, David M. 1995. Statistical decision-tree mod-
els for parsing. In Proceedings of ACL 1995, pages 276?
283, Morristown, NJ, USA. Association for Computa-
tional Linguistics Morristown, NJ, USA.
McDonald, R. and F. Pereira. 2006. Online Learning of Ap-
proximate Dependency Parsing Algorithms. In In Proc.
of EACL, pages 81?88.
Menzel, Wolfgang and Ingo Schro?der. 1998. Decision pro-
cedures for dependency parsing using graded constraints.
In Proceedings of the COLING-ACL ?98 Workshop on
Processing of Dependency-Based Grammars, pages 78?
87.
Nilsson, Jens and Joakim Nivre. 2005. Pseudo-projective
dependency parsing. In Proceedings of ACL 2005, pages
99?106.
Nivre, Joakim and Johan Hall. 2008. A dependency-driven
parser for German dependency and constituency represen-
tations. In Proceedings of the ACL Workshop on Parsing
German.
Nivre, J. and R. McDonald. 2008. Integrating Graph-Based
and Transition-Based Dependency Parsers. In ACL-08,
pages 950?958, Columbus, Ohio.
Nivre, Joakim, Jens Nilsson, Johan Hall, Gu?ls?en Eryig?it, and
Svetoslav Marinov. 2006. Labeled pseudo-projective de-
pendency parsing with Support Vector Machines. In Pro-
ceedings of CoNLL 2006.
Nivre, J. 2003. An Efficient Algorithm for Projective De-
pendency Parsing. In 8th International Workshop on
Parsing Technologies, pages 149?160, Nancy, France.
Nivre, J. 2009. Non-Projective Dependency Parsing in Ex-
pected Linear Time. In Proceedings of the 47th Annual
Meeting of the ACL and the 4th IJCNLP of the AFNLP,
pages 351?359, Suntec, Singapore.
Rahimi, A. and B. Recht. 2008. Random Features for
Large-Scale Kernel Machines. In Platt, J.C., D. Koller,
Y. Singer, and S. Roweis, editors, Advances in Neural
Information Processing Systems, volume 20. MIT Press,
Cambridge, MA.
Ratnaparkhi, Adwait. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of EMNLP 1996,
volume 1, pages 133?142.
Schiller, Anne, Simone Teufel, and Christine Sto?ckert. 1999.
Guidelines fu?r das Tagging deutscher Textcorpora mit
STTS (Kleines und gro?es Tagset). Technical Report Au-
gust, Universita?t Stuttgart.
Schmid, Helmut. 1995. Improvements in part-of-speech tag-
ging with an application to German. In Proceedings of the
ACL SIGDAT-Workshop, volume 11.
Seeker, Wolfgang, Ines Rehbein, Jonas Kuhn, and Josef Van
Genabith. 2010. Hard Constraints for Grammatical Func-
tion Labelling. In Proceedings of ACL 2010, Uppsala.
Titov, I. and J. Henderson. 2007. A Latent Variable Model
for Generative Dependency Parsing. In Proceedings of
IWPT, pages 144?155.
Yamada, H. and Y. Matsumoto. 2003. Statistical Depen-
dency Analysis with Support Vector Machines. In Pro-
ceedings of IWPT, pages 195?206.
1130
Coling 2010: Poster Volume, pages 1426?1434,
Beijing, August 2010
Cross-Lingual Induction for Deep Broad-Coverage Syntax: A Case
Study on German Participles
Sina Zarrie? Aoife Cahill Jonas Kuhn Christian Rohrer
Institut fu?r Maschinelle Sprachverarbeitung (IMS), University of Stuttgart
{zarriesa,cahillae,jonas.kuhn,rohrer}@ims.uni-stuttgart.de
Abstract
This paper is a case study on cross-lingual
induction of lexical resources for deep,
broad-coverage syntactic analysis of Ger-
man. We use a parallel corpus to in-
duce a classifier for German participles
which can predict their syntactic category.
By means of this classifier, we induce a
resource of adverbial participles from a
huge monolingual corpus of German. We
integrate the resource into a German LFG
grammar and show that it improves pars-
ing coverage while maintaining accuracy.
1 Introduction
Parallel corpora are currently exploited in a wide
range of induction scenarios, including projection
of morphologic (Yarowsky et al, 2001), syntactic
(Hwa et al, 2005) and semantic (Pado? and Lap-
ata, 2009) resources. In this paper, we use cross-
lingual data to learn to predict whether a lexi-
cal item belongs to a specific syntactic category
that cannot easily be learned from monolingual re-
sources. In an application test scenario, we show
that this prediction method can be used to obtain
a lexical resource that improves deep, grammar-
based parsing.
The general idea of cross-lingual induction is
that linguistic annotations or structures, which are
not available or explicit in a given language, can
be inferred from another language where these an-
notations or structures are explicit or easy to ob-
tain. Thus, this technique is very attractive for
cheap acquisition of broad-coverage resources, as
is proven by the approaches cited above. More-
over, this induction process can be attractive for
the induction of deep (and perhaps specific) lin-
guistic knowledge that is hard to obtain in a mono-
lingual context. However, this latter perspective
has been less prominent in the NLP community
so far.
This paper investigates a cross-lingual induc-
tion method based on an exemplary problem aris-
ing in the deep syntactic analysis of German. This
showcase is the syntactic flexibility of German
participles, being morphologically ambiguous be-
tween verbal, adjectival and adverbial readings,
and it is instructive for several reasons: first, the
phenomenon is a notorious problem for linguistic
analysis and annotation of German, such that stan-
dard German resources do not represent the under-
lying analysis. Second, in Zarrie? et al (2010),
we showed that integrating the phenomenon of
adverbial participles in a naive way into a broad-
coverage grammar of German leads to significant
parsing problems, due to spurious ambiguities.
Third, it is completely straightforward to detect
adverbial participles in cross-lingual data since in
other languages, e.g. English or French, adverbs
are often morphologically marked.
In this paper, we use instances of adverbially
translated participles in a parallel corpus to boot-
strap a classifier that is able to identify an ad-
verbially used participle based on its monolingual
syntactic context. In contrast to what is commonly
assumed, we show that it is possible to detect ad-
verbial participles using only a relatively narrow
context window. This classifier enables us to iden-
tify an occurence of an adverbial participle inde-
pendently of its translation in a parallel corpus,
going far beyond the induction methodology in
Zarrie? et al (2010). By means of the participle
classifier, we can extract new types of adverbial
participles from a larger corpus of German news-
paper text and substantially augment the size of
the resource extracted only on Europarl data. Fi-
nally, we integrate this new resource into the Ger-
man LFG grammar and show that it improves cov-
erage without negatively affecting performance.
1426
The paper is structured as follows: in Sec-
tion 2, we describe the linguistic and computa-
tional problems related to the parsing of adver-
bial participles in German. Section 3 introduces
the general idea of using the translation data to
find instances of different participle categories. In
Section 4, we illustrate the training of the clas-
sifier, evaluating the impact of the context win-
dow and the quality of the training data obtained
from cross-lingual text. In Section 5, we apply the
classifier to new, monolingual data and describe
the extension of the resource for adverbial partici-
ples. Section 6 evaluates the extended resource by
means of parsing experiments using the German
LFG grammar.
2 The Problem
In German, past perfect participles are ambiguous
with respect to their morphosyntactic category. As
in other languages, they can be used as part of
the verbal complex (Example (1-a)) or as adjec-
tives (Example (1-b)). Since German adjectives
can generally undergo conversion into adverbs,
participles can also be used adverbially (Example
(1-c)). The verbal and adverbial participle forms
are morphologically identical.
(1) a. Sie haben das Experiment wiederholt.
?They have repeated the experiment.?
b. Das wiederholte Experiment war erfolgreich.
?The repeated experiment was succesful.?
c. Sie haben das Experiment wiederholt abge-
brochen.
?They cancelled the experiment repeatedly.?
Moreover, German adjectival modifiers can be
generally used as predicatives that can be either
selected by a verb (Example (2-a)) or that can oc-
cur as free predicatives (Example (2-b)).
(2) a. Er scheint begeistert von dem Experiment.
?He seems enthusiastic about the experiment.?
b. Er hat begeistert experimentiert.
?He has experimented enthusiastic.?
Since predicative adjectives are not inflected,
the surface form of a German participle is ambigu-
ous between a verbal, predicative or adverbial use.
2.1 Participles in the German LFG
In order to account for sentences like (1-c), an in-
tuitive approach would be to generally allow for
adverb conversion of participles in the grammar.
However, in Zarrie? et al (2010), we show that
such a rule can have a strong negative effect on
the overall performance of the parsing system, de-
spite the fact that it produces the desired syntac-
tic and semantic analysis for specific sentences.
This problem was illustrated using a German LFG
grammar (Rohrer and Forst, 2006) constructed as
part of the ParGram project (Butt et al, 2002).
The grammar is implemented in the XLE, a gram-
mar development environment which includes a
very efficient LFG parser and a stochastic dis-
ambiguation component which is based on a log-
linear probability model (Riezler et al, 2002).
In Zarrie? et al (2010), we found that the
naive implementation of adverbial participles in
the German LFG, i.e. in terms of a general gram-
mar rule that allows for participles-adverb conver-
sion, leads to spurious ambiguities that mislead
the disambiguation component of the grammar.
Moreover, the rule increases the number of time-
outs, i.e. sentences that cannot be parsed in a pre-
defined amount of time (20 seconds). Therefore,
we observe a drop in parsing accuracy although
grammar coverage is improved. As a solution, we
induced a lexical resource of adverbial participles
based on their adverbial translations in a paral-
lel corpus. This resource, comprising 46 partici-
ple types, restricts the adverb conversion such that
most of the spurious ambiguities are eliminated.
To assess the impact of specific rules in a broad-
coverage grammar, possibly targeting medium-to-
low frequency phenomena, we have established a
fine-grained evaluation methodology. The chal-
lenge posed by these low-frequent phenomena is
typically two-fold: on the one hand, if one takes
into account the disambiguation component of the
grammar and pursues an evaluation of the most
probable parses on a general test set, the new
grammr rule cannot be expected to show a positive
effect since the phenomenon is not likely to occur
very often in the test set. On the other hand, if one
is interested in a linguistically precise grammar,
it is very unsatisfactory to reduce grammar cov-
erage to statistically frequent phenomena. There-
fore, we combined a coverage-oriented evaluation
on specialised testsuites with a quantitative evalu-
ation including disambiguation, making sure that
1427
the increased coverage does not lead to an overall
drop in accuracy. The evaluation methodolgy will
also be applied to evaluate the impact of the new
participle resource, see Section 6.
2.2 The Standard Flat Analysis of Modifiers
The fact that German adjectival modifiers can gen-
erally undergo conversion into adverbs without
overt morphological marking is a notorious prob-
lem for the syntactic analysis of German: there
are no theoretically established tests to distinguish
predicative adjectives and adverbials, see Geuder
(2004). For this reason, the standard German tag
set assigns a uniform tag (?ADJD?) to modifiers
that are morphologically ambiguous between an
adjectival and adverbial reading. Moreover, in
the German treebank TIGER (Brants et al, 2002)
the resulting syntactic differences between the two
readings are annotated by the same flat structure
that does not disambiguate the sentence.
Despite certain theoretical problems related to
the analysis of German modifiers, their interpre-
tation in real corpus sentences is often unambigu-
ous for native speakers. As an example, consider
example (3) from the TIGER treebank. In the
sentence, the participle unterschrieben (signed)
clearly functions as a predicative modifier of the
sentence?s subject. The other, theoretically possi-
ble reading where the participle would modify the
verb send is semantically not acceptable. How-
ever, in TIGER, the participle is analysed as an
ADJD modifier attached under the VP node which
is the general analysis for adjectival and adverbial
modifiers.
(3) Die
It
sollte
should
unterschrieben
signed
an
to
die
the
Leitung
administration
zuru?ckgesandt
sent back
werden.
be.
?It should be sent back signed to the administation.?
Sentence (4) (also taken from TIGER) illus-
trates the case of an adverbial participle. In this
example, the reading where angemessen (ade-
quately) modifies the main verb is the only one
that is semantically plausible. In the treebank, the
participle is tagged as ADJD and analysed as a
modifier in the VP.
(4) Der
The
menschliche
human
Geist
mind
la??t
lets
sich
itself
rechnerisch
computationally
nicht
not
angemessen
adequately
simulieren.
simulate.
?The human mind cannot be adequately simulated in a
computational way.?
The flat annotation strategy adopted for modi-
fiers in the standard German tag set and in the tree-
bank TIGER entails that instances of adverbs (and
adverbial participles) cannot be extracted from au-
tomatically tagged, or parsed, text. Therefore,
it would be very hard to obtain training mate-
rial from German resources to train a system that
automatically identifies adverbially used partici-
ples. However, the intuition corroborated by the
examples presented in this section is that the struc-
tures can actually be disambiguated in many cor-
pus sentences.
In the following sections, we show how we ex-
ploit parallel text to obtain training material for
learning to predict occurences of adverbial par-
ticiples, without any manual effort. Moreover, by
means of this technique, we can substantially ex-
tend the grammatical resource for adverbial par-
ticiples compared to the resource that can be di-
rectly extracted from the parallel text.
3 Participles in the Parallel Corpus
The intuition of the cross-lingual induction ap-
proach is that adverbial participles can easily be
extracted from parallel corpora since in other lan-
guages (such as English or French) adverbs are
often morphologically marked and easily labelled
by statistical PoS taggers. As an example, con-
sider sentence (5) extracted from Europarl, where
the German participle versta?rkt is translated by an
English adverb (increasingly).
(5) a. Nicht
Not
ohne
without
Grund
reason
sprechen
speak
wir
we
versta?rkt
increasingly
vom
of a
Europa
Europe
der
of the
Regionen.
Regions.
b. It is not without reason that we increasingly speak
in terms of a Europe of the Regions.
The idea is to project specific morphological
information about adverbs which is overt in lan-
guages like English onto German where adverbs
cannot be directly extracted from tagged data.
While this idea might seem intuitively straightfor-
1428
ward, we also know that translation pairs in paral-
lel data are not always lingusitically parallel, and
as a consequence, word-alignment is not always
reliable. To assess the impact of non-parallelism
in adverbial translations of German participles,
we manually annotated a sample of 300 transla-
tions. This data also constitutes the basis for the
experiments reported in Section 4.
3.1 Data
Our experiments are based on the same data as in
(Zarrie? et al, 2010). For convenience, we pro-
vide a short description here.
We limit our investigations to non-lexicalised
participles occuring in the Europarl corpus and
not yet recorded as adverbs in the lexicon of the
German LFG grammar (5054 participle types in
total). Given the participle candidates, we ex-
tract the set of sentences that exhibit a word align-
ment between a German participle and an English,
French or Dutch adverb. The word alignments
have been obtained with GIZA++. The extrac-
tion yields 27784 German-English sentence pairs
considering all alignment links, and 5191 sen-
tence pairs considering only bidirectional align-
ments between a participle and an English adverb.
3.2 Systematic Non-Parallelism
For data exploration and evaluation, we anno-
tated 300 participle alignments out of the 5191
German-English sentences (with a bidirectional
participle-adverb alignment). We distinguish the
following annotation categories: (i) parallel trans-
lation, adverb information can be projected, (ii)
incorrect alignment, (iii) correct alignment, but
translation is a multi-word expression, (iv) correct
alignment, but translation is a paraphrase (possi-
bly involving a translation shift).
Parallel Cases In our annotated sample of En-
glish adverb - German participle pairs, 43%1 of
the translation instances are parallel in the sense
that the overt adverb information from the English
side can be projected onto the German participle.
This means that if we base the induction technique
1The diverging figures we report in Zarrie? et al (2010)
were due to a small bug in the script and it does not affect the
overall interpretation of the data.
on word-alignments alone, its precision would be
relatively low.
Non-Parallel Cases Taking a closer look at the
non-parallel cases in our sample (57% of the
translation pairs), we find that 47% of this set are
due to incorrect word alignments. The remain-
ing 53% thus reflect regular cases of non-parallel
translations. A typical configuration which makes
up 30% of the the non-parallel cases is exempli-
fied in (6) where the German main verb vorlegen
is translated by the English multiword expression
put forward.
(6) a. Wir haben eine Reihe von Vorschla?gen vorgelegt.
b. We have put forward a number of proposals.
An example for the general paraphrase or trans-
lation shift category is given in Sentence (7).
Here, the translational correspondence between
gekommen (arrived) and the adverb now is due
to language-specific, idiomatic realisations of an
identical underlying semantic concept. The para-
phrase translations make up 23% of the non-
parallel cases in the annotated sample.
(7) a. Die
That
Zeit
time
ist
is
noch
yet
nicht
not
gekommen
arrived.
.
b. That time is not now .
Furthermore, it is noticeable that the cross-
lingual approach seems to inherently factor out
the ambiguity between predicative and adverbial
participles. In our annotated sample, there are no
predicative participles that have been translated by
an English adverb.
3.3 Filtering Mechanisms
The data analysis in the previous section, show-
ing only 43% of parallel cases in English adverb
translations for German participles, mainly con-
firms other studies in annotation projection which
find that translational correspondences only allow
for projection of linguistic analyses in a more or
less limited proportion (Yarowsky et al, 2001;
Hwa et al, 2005; Mihalcea et al, 2007).
In previous studies on annotation projection,
quite distinct filtering methods have been pro-
posed: in Yarowsky et al (2001), projection er-
rors are mainly attributed to word alignment er-
rors and filtered based on translation probabilities.
1429
Hwa et al (2005) find that errors in the projec-
tion of syntactic relations are also due to system-
atic grammatical divergences between languages
and propose correcting these errors by means of
specific, manually designed filters. Bouma et al
(2008) make similar observations to Hwa et al
(2005), but try to replace manual correction rules
by filters from additional languages.
In Zarrie? et al (2010), we compared a num-
ber of filtering techniques on our participle data.
The 300 annotated translation instances are used
as a test set for evaluation. In particular, we
have established that a combination of syntactic
dependency-based filters and multilingual filters
can very accurately separate non-parallel transla-
tions from parallel ones where the adverb infor-
mation can be projected. In Section 4, we show
that these filtering techniques are also very useful
for removing noise from the training material that
we use to build a classifier.
4 Bootstrapping a German Participle
Classifier from Crosslingual Data
In the previous section, we have seen that German
adverbial participles can be easily found in cross-
lingual text by looking at their translations in a
language that morphologically marks adverbials.
In previous work, we exploited this observation
by directly extracting types of adverbial partici-
ples based on word alignment links and the filter-
ing mechanisms mentioned in Section 3. How-
ever, this method is very closely tied to data in
the parallel corpus, which only comprises around
5000 participle-adverb translations in total, which
results in 46 types of adverbial participles after fil-
tering. Thus, we have no means of telling whether
we would discover new types of adverbial partici-
ples in other corpora, from different domains to
Europarl. As this corpus is rather small and genre
specific, it even seems very likely that one could
find additional adverbial participles in a bigger
corpus. Moreover, we cannot be sure that certain
adverbial participles have systematically diverg-
ing translations in other languages, due to cross-
lingual lexicalisation differences. Generally, it is
not clear whether we have learned something gen-
eral about the syntactic phenomenon of adverbial
participles in German or whether we have just ex-
tracted a small, corpus-dependent subset of the
class of adverbial participles.
In this section, we use instances of adverbially
translated participles as training material for a
classifier that learns to predict adverbial partici-
ples based on their monolingual syntactic context.
Thus, we exploit the translations in the parallel
corpus as a means of obtaining ?annotated? or dis-
ambiguated training data without any manual ef-
fort. During training, we only consider the mono-
lingual context of the participle, such that the fi-
nal application of the classifier is not dependent
on cross-lingual data anymore.
4.1 Context-based Identification of
Adverbial Participles
Given the general linguistic problems related to
adverbial participles (see Section 2), one could
assume that it is very difficult to identify them
in a given context. To assess the general dif-
ficulty of this syntactic problem, we run a first
experiment comparing a grammar-based identifi-
cation method against a classifier that only con-
siders relatively narrow morpho-syntactic context.
For evaluation, we use the 300 annotated partici-
ple instances described in Section 3. This test
set divides into 172 negative instances, i.e. non-
adverbial participles, and 128 positive instances.
We report accuracy of the identification method,
as well as precision and recall relating to the num-
ber of correctly predicted adverbial participles.
For the grammar-based identification, we use
the German LFG which integrates the lexical
resource for adverbial participles established in
(Zarrie? et al, 2010). We parse the 300 Europarl
sentences and check whether the most probable
parse proposed by the grammar analyses the re-
spective participle as an adverb or not. The gram-
mar obtains a complete parse for 199 sentences
out of the test set and we only consider these in
the evaluation. The results are given in Table 1.
The high precision and accuracy of the
grammar-based identification of adverbial partici-
ples suggests that in a lot of sentences, the adver-
bial analysis is the only possible reading, i.e. the
only analysis that makes the sentence grammati-
cal. But of course, we have substantially restricted
the adverb participle-conversion in the grammar,
1430
Training Data Precision Recall Accuracy
Grammar 97.3 90.12 94.97
Classifier Unigram 87.10 84.38 87.92
Classifier Bigram 88.28 88.28 89.93
Classifier Trigram 89.60 87.5 90.27
Table 1: Evaluation on 300 participle instances
from Europarl
so that it does not propose adverbial analyses for
participles that are very unlikely to function as
modifiers of verbs.
For the classifier-based identification, we use
the adverbially translated participle tokens in our
Europarl data (5191 tokens in total) as training
material. We remove the 300 test instances from
this training set, and then divide it into a set of
positive and negative instances. To do this, we
use the filtering mechanisms already proposed in
Zarrie? et al (2010). These filters apply on the
type level, such that we first identify the positive
types (46 total) and then use all instances of these
types in the 4891 sentences as positive instances
of adverbial participles (1978 instances). The re-
maining sentences are used as negative instances.
For the training of the classifier, we use
maximum-entropy classification, which is also
commonly used for the general task of tagging
(Ratnaparkhi, 1996). In particular, we use the
open source TADM tool for parameter estimation
(Malouf, 2002). The tags of the words surround-
ing the participles are used as features in the clas-
sification task. We explore different sizes of the
context window, where the trigram window is the
most succesful (see Table 1). Beyond the trigram
window, the results of the classifier start decreas-
ing again, probably because of too many mislead-
ing features. Generally, this experiment shows
that the grammar-based identification is more pre-
cise, but that the classifier still performs surpris-
ingly well. Compared to the results from the
grammar-based identification, the high accuracy
of the classifier suggests that even the narrow syn-
tactic contexts of adverbial vs. non-adverbial par-
ticiples are quite distinct.
4.2 Designing Training Data for Participle
Classification
There are several questions related to the design
of the training data that we use to build our clas-
sifier. First, it is not clear how many negative
instances are helpful for learning the adverbial -
non-adverbial distinction. In the above experi-
ment, we simply use the instances that do not pass
the cross-lingual filters. In this section, we exper-
iment with an augmented set of negative instances
that was also obtained by extracting German par-
ticiple that are bi-directionally aligned to an En-
glish participle in Europarl. This is based on the
assumption that these participles are very likely
to be verbal. Second, it is not clear whether we
really need the filtering mechanisms proposed in
Zarrie? et al (2010) and whether we could im-
prove the classifier by training it on a larger set
of positive instances. Therefore, we also experi-
ment with two further sets of positive instances:
one where we used all participles (not necessarily
bidirectionally) aligned to an adverb, one where
we only use the bidirectional alignments. The re-
sults obtained for the different sizes of positive
and negative instance sets are given in Table 2.
The picture that emerges from the results in Ta-
ble 2 is very clear: the stricter the filtering of the
training material (i.e. the positive instances) is,
the better the performance of the classifier. The
fact that we (potentially) loose certain positive in-
stances in the filtering does not negatively impact
on the classifier which substantially benefits from
the fact that noise gets removed. Moreover, we
find that if the training material is appropriately
filtered, adding further negative instances does not
help improving the accuracy. By contrast, if we
train on a noisy set of positive instances, the clas-
sifier benefits from a larger set of negative in-
stances. However, the positive effect that we get
from augmenting the non-filtered training data is
still weaker than the positive effect we get from
the filtering.
5 Induction of Adverbial Participles on
Monolingual Data
Given the classifier from Section 4 that predicts
the syntactic category of a participle instance
1431
Training Data Pos. Instances Neg. Instances Precision Recall Accuracy
Non-Filtered Instances (all alignments) 27.184 10.000 43.10 100 43.10
Non-Filtered Instances (all alignments) 27.184 50.000 74.38 92.97 83.22
Non-Filtered Instances (symm. alignments) 4891 10.000 78.08 89.06 84.56
Non-Filtered Instances (symm. alignments) 4891 50.000 82.31 83.59 85.23
Filtered Instances 1978 10.000 91.60 85.16 90.27
Filtered Instances 1978 50.000 90.83 77.34 86.91
Table 2: Evaluation on 300 participle instances from Europarl
based on its monolingual syntactic context, we
can now detect new instances or types of adver-
bial participles in any PoS-tagged German corpus.
In this section, we investigate whether the classi-
fier can be used to augment the resource of ad-
verbial participles directly induced from Europarl
with new types.
5.1 Data Extraction
We run our extraction experiment on the Huge
German Corpus (HGC), a corpus of 200 million
words of newspaper and other text. This corpus
has been tagged with TreeTagger (Schmid, 1994).
For each of the 5054 participle candidates, we ex-
tract all instances from the HGC which have not
been tagged as finite verbs (at most 2000 tokens
per participle). For each participle token, we also
extract its syntactic context in terms of the 3 pre-
ceding and the 3 following tags. For classification,
we use only those participles that have more than
50 instances in the corpus (2953 types).
In contrast to the cross-lingual filtering mech-
anisms developed in Zarrie? et al (2010) which
operate on the type-level, the classifier makes a
prediction for every token of a given participle
candidate. Thus, for each of the participle can-
didates, we obtain a percentage of instances that
have been classified as adverbs. As we would ex-
pect, the percentage of adverbial instances is very
low for most of the participles in our candidate set:
for 75% of the 2953 types, the percentage is below
5%. This result confirms our initial intuition that
the property of being used as an adverb is strongly
lexically restricted to a certain class of participles.
5.2 Evaluation
Since we know that the classifier has an accu-
racy of 90% on the Europarl data, we only con-
sider participles as candidates for adverbs where
the classifier predicted more than 14% adverbial
instances. This leaves us with a set of 210 partici-
ples, which comprises 13 of the original 46 par-
ticiples extracted from Europarl, meaning we have
discovered 197 new adverbial participle types.
We performed a manual evaluation of 50 ran-
domly selected types out of the set of 197 new
participle types. Therefore, we looked at the in-
stances and their context which the classifier pre-
dicted to be adverbial. If there was at least one ad-
verbial instance among these, the participle type
was evaluated as correctly annotated by the clas-
sifier. By this means, we find that 76% of the par-
ticiples were correctly classified.
This evaluation suggests that the accuracy of
our classifier which we trained and tested on Eu-
roparl data is lower on the HGC data. The rea-
son for this drop in performance will be explained
in the following Section 5.3. However, assuming
an accuracy of 76%, we have discovered 150 new
types of adverbial participles. We argue that this is
a very satisfactory result given that we have not in-
vested any manual effort into the annotation or ex-
traction of adverbial participles. This results also
makes clear that the previous resource we induced
on Europarl data, comprising only 46 participle
types, was a very limited one.
5.3 Error Analysis
Taking a closer look at the 12 participle candi-
dates that the classifier incorrectly labels as adver-
bial, we observe that their adverbially classified
instances are mostly instances of a predicative use.
This means that our Europarl training data does
not contain enough evidence to learn the distinc-
tion between adverbial and predicative participles.
This is not surprising since the set of negative
instances used for training the classifier mainly
comprises verbal instances of participles. More-
over, the syntactic contexts and constructions in
which some predicatives and adverbials are used
1432
Grammar Prec. Rec. F-Sc. Time
in sec
46 Part-Adv 84.12 78.2 81.05 665
243 Part-Adv 84.12 77.67 80.76 665
Table 3: Evaluation on 371 TIGER sentences
are very similar. Thus, in future work, we will
have to include more data on predicatives (which
is more difficult to obtain) and analyse the syntac-
tic contexts in more detail.
6 Assessing the Impact of Resource
Coverage on Grammar-based Parsing
In this section, we evaluate the classifier-based in-
duction of adverbial participles from a grammar-
based perspective. We integrate the entire set of
induced adverbial participles (46 from Europarl
and 197 from the HGC) into the German LFG
grammar. As a consequence, the grammar al-
lows the adverb conversion for 243 lexical par-
ticiple types. We use the evaluation methodolgy
explained in Section 2.
First, we conduct an accuracy-oriented evalua-
tion on the standard TIGER test set. We compare
against the German LFG that only integrates the
small participle resource from Europarl. The re-
sults are given in Table 3. The difference between
the 46 Part-Adv and 243 Part-Adv resource is not
statistically signficant. Thus, the larger participle
resource has no overall negative effect on the pars-
ing performance. As established by an automatic
upperbound evaluation in Zarrie? et al (2010),
we cannot not expect to find a positive effect in
this evaluation because the phenomenon does not
occur in the standard test set.
To show that the augmented resource indeed
improves the coverage of the grammar, we built
a specialised testsuite of 1044 TIGER sentences
that contain an instance of a participle from the
resource. Since this testsuite comprises sen-
tences from the training set, we can only report
a coverage-oriented evaluation here, see Table 4.
The 243 Part-Adv increases the coverage by 8%
on the specialised testsuite.
Moreover, we manually evaluated 20 sentences
covered by the 243-Part-Adv grammar and not
by 46-Part-Adv as to whether they contain a cor-
rectly analysed adverbial participle. In two sen-
Grammar Parsed
Sent.
Starred
Sent.
Time-
outs
Time
in sec
No Part-Adv 665 315 64 3033
46 Part-Adv 710 269 65 3118
243 Part-Adv 767 208 69 3151
Table 4: Performance on the specialised TIGER
test set (1044 sentences)
tences, the grammar obtained an adverbial analy-
sis for clearly predicative modifiers, based on the
enlarged resource. In three different sentences, it
was difficult to decide whether the participle acts
as an adverb or a predicative. In the remaining 15
sentences, the grammar established the the correct
analysis of a clearly adverbially used participle.
7 Conclusion
We have proposed a cross-lingual induction
method to automatically obtain data on adverbial
participles in German. We exploited this cross-
lingual data as training material for a classifier that
learns to predict the syntactic category of a partici-
ple from its monolingual syntactic context. Since
this category is usually not annotated in German
resources and hard to describe in theory, the find-
ing that adverbial participles can be predicted rel-
atively precisely is of general interest for theo-
retic and computational approaches to the syntac-
tic analysis of German.
We showed that, in order to obtain an accurate
participle classifier, the quality of the training ma-
terial induced from the parallel corpus is of crucial
importance. By applying the filtering techniques
from Zarrie? et al (2010), the accuracy of the
classifier increases between 5% and 7%. In future
work, we plan to include more data on predicative
participles to learn a more accurate distinction be-
tween predicative and adverbial participles.
Finally, we used the participle classifier to ex-
tract a lexical resource of adverbial participles for
the German LFG grammar. In comparison to the
relatively small resource of 46 types that can be
directly induced from Europarl, we discovered a
large number of new participle types (197 types
in total). In a parsing experiment, we showed that
this much bigger resource does not negatively im-
pact on parsing performance and improves gram-
mar coverage.
1433
References
Bouma, Gerlof, Jonas Kuhn, Bettina Schrader, and
Kathrin Spreyer. 2008. Parallel LFG Grammars
on Parallel Corpora: A Base for Practical Trian-
gulation. In Butt, Miriam and Tracy Holloway
King, editors, Proceedings of the LFG08 Confer-
ence, pages 169?189, Sydney, Australia. CSLI Pub-
lications, Stanford.
Brants, Sabine, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Butt, Miriam, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
Parallel Grammar Project.
Geuder, Wilhelm. 2004. Depictives and transpar-
ent adverbs. In Austin, J. R., S. Engelbrecht,
and G. Rauh, editors, Adverbials. The Interplay of
Meaning, Context, and Syntactic Structure, pages
131?166. Benjamins.
Hwa, Rebecca, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Nat. Lang. Eng., 11(3):311?325.
Malouf, Robert. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the Sixth Conference on Natural Lan-
guage Learning (CoNLL-2002), pages 49?55.
Mihalcea, Rada, Carmen Banea, and Jan Wiebe.
2007. Learning multilingual subjective language
via cross-lingual projections. In Proceedings of
the Association for Computational Linguistics (ACL
2007), pages 976?983, Prague.
Pado?, Sebastian and Mirella Lapata. 2009. Cross-
lingual annotation projection of semantic roles.
Journal of Artificial Intelligence Research, 36:307?
340.
Ratnaparkhi, Adwait. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP 96, pages 133?142.
Riezler, Stefan, Tracy Holloway King, Ronald M. Ka-
plan, Richard Crouch, John T. Maxwell, and Mark
Johnson. 2002. Parsing the Wall Street Journal
using a Lexical-Functional Grammar and Discrim-
inative Estimation Techniques . In Proceedings of
ACL 2002.
Rohrer, Christian and Martin Forst. 2006. Improving
coverage and parsing quality of a large-scale LFG
for German. In Proceedings of LREC-2006.
Schmid, Helmut. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of In-
ternational Conference on New Methods in Lan-
guage Processing.
Yarowsky, David, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analy-
sis tools via robust projection across aligned cor-
pora. In Proceedings of HLT 2001, First Interna-
tional Conference on Human Language Technology
Research.
Zarrie?, Sina, Aoife Cahill, Jonas Kuhn, and Christian
Rohrer. 2010. A Cross-Lingual Induction Tech-
nique for German Adverbial Participles. In Pro-
ceedings of the 2010 Workshop on NLP and Lin-
guistics: Finding the Common Ground, ACL 2010,
pages 34?42, Uppsala, Sweden.
1434
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 928?939, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Generating Non-Projective Word Order in Statistical Linearization
Bernd Bohnet Anders Bjo?rkelund Jonas Kuhn Wolfgang Seeker Sina Zarrie?
Institut fu?r Maschinelle Sprachverarbeitung
University of Stuttgart
{bohnetbd,anders,jonas,seeker,zarriesa}@ims.uni-stuttgart.de
Abstract
We propose a technique to generate non-
projective word orders in an efficient statisti-
cal linearization system. Our approach pre-
dicts liftings of edges in an unordered syntac-
tic tree by means of a classifier, and uses a
projective algorithm for tree linearization. We
obtain statistically significant improvements
on six typologically different languages: En-
glish, German, Dutch, Danish, Hungarian, and
Czech.
1 Introduction
There is a growing interest in language-independent
data-driven approaches to natural language genera-
tion (NLG). An important subtask of NLG is sur-
face realization, which was recently addressed in the
2011 Shared Task on Surface Realisation (Belz et
al., 2011). Here, the input is a linguistic representa-
tion, such as a syntactic dependency tree lacking all
precedence information, and the task is to determine
a natural, coherent linearization of the words.
The standard data-driven approach is to traverse
the dependency tree deciding locally at each node on
the relative order of the head and its children. The
shared task results have proven this approach to be
both effective and efficient when applied to English.
It is what federal support should try to achieve
SBJ
ROOT OBJ
NMOD SBJ
PRD
VC OPRD IM
Figure 1: A non-projective example from the CoNLL
2009 Shared Task data set for parsing (Hajic? et al 2009).
However, the approach can only generate pro-
jective word orders (which can be drawn with-
out any crossing edges). Figure 1 shows a non-
projective word order: the edge connecting the ex-
tracted wh-pronoun with its head crosses another
edge. Once what has been ordered relative to
achieve, there are no ways of inserting intervening
material. In this case, only ungrammatical lineariza-
tions can be produced from the unordered input tree:
(1) a. *It is federal support should try to what achieve
b. *It is federal support should try to achieve what
c. *It is try to achieve what federal support should
Although rather infrequent in English, non-
projective word orders are quite common in lan-
guages with a less restrictive word order. In these
languages, it is often possible to find a grammati-
cally correct projective linearization for a given in-
put tree, but discourse coherence, information struc-
ture, and stylistic factors will often make speak-
ers prefer some non-projective word order.1 Figure
2 shows an object fronting example from German
where the edge between the subject and the finite
verb crosses the edge between the object and the full
verb. Various other constructions, such as extraposi-
tion of (relative) clauses or scrambling, can lead to
non-projectivity. In languages where word order is
driven to an even larger degree by information struc-
ture, such as Czech and Hungarian, non-projectivity
can likewise result from various ordering decisions.
These phenomena have been studied extensively in
1A categorization of non-projective edges in the Prague
Dependency Treebank (Bo?hmova? et al 2000) is presented in
Hajic?ova? et al(2004).
928
the linguistic literature, and for certain languages,
work on rule-based generation has addressed certain
aspects of the problem.
Das Mandat will er zuru?ckgeben .
the.ACC mandate.ACC want.3SG he.NOM return.INF .
NK
OA#?
SB OC
?
?He wants to return the mandate.?
Figure 2: German object fronting with complex verb in-
troducing a non-projective edge.
In this paper, we aim for a general data-driven ap-
proach that can deal with various causes for non-
projectivity and will work for typologically dif-
ferent languages. Our technique is inspired by
work in data-driven multilingual parsing, where
non-projectivity has received considerable attention.
In pseudo-projective parsing (Kahane et al 1998;
Nivre and Nilsson, 2005), the parsing algorithm is
restricted to projective structures, but the issue is
side-stepped by converting non-projective structures
to projective ones prior to training and application,
and then restoring the original structure afterwards.
Similarly, we split the linearization task in two
stages: initially, the input tree is modified by lifting
certain edges in such a way that new orderings be-
come possible even under a projectivity constraint;
the second stage is the original, projective lineariza-
tion step. In parsing, projectivization is a determin-
istic process that lifts edges based on the linear or-
der of a sentence. Since the linear order is exactly
what we aim to produce, this deterministic conver-
sion cannot be applied before linearization. There-
fore, we use a statistical classifier as our initial lift-
ing component. This classifier has to be trained on
suitable data, and it is an empirical question whether
the projective linearizer can take advantage of this
preceding lifting step.
We present experiments on six languages with
varying degrees of non-projective structures: En-
glish, German, Dutch, Danish, Czech and Hungar-
ian, which exhibit substantially different word order
properties. Our approach achieves significant im-
provements on all six languages. On German, we
also report results of a pilot human evaluation.
2 Related Work
An important concept for tree linearization are word
order domains (Reape, 1989). The domains are bags
of words (constituents) that are not allowed to be dis-
continuous. A straightforward method to obtain the
word order domains from dependency trees and to
order the words in the tree is to use each word and
its children as domain and then to order the domains
and contained words recursively. As outlined in the
introduction, the direct mapping of syntactic trees to
domains does not provide the possibility to obtain
all possible correct word orders.
Linearization systems can be roughly distin-
guished as either rule-based or statistical systems. In
the 2011 Shared Task on Surface Realisation (Belz
et al 2011), the top performing systems were all
statistical dependency realizers (Bohnet et al 2011;
Guo et al 2011; Stent, 2011).
Grammar-based approaches map dependency
structures or phrase structures to a tree that repre-
sents the linear precedence. These approaches are
mostly able to generate non-projective word orders.
Early work was nearly exclusively applied to phrase
structure grammars (e.g. (Kathol and Pollard, 1995;
Rambow and Joshi, 1994; Langkilde and Knight,
1998)). Concerning dependency-based frameworks,
Bro?ker (1998) used the concept of word order do-
mains to separate surface realization from linear
precedence trees. Similarly, Duchier and Debus-
mann (2001) differentiate Immediate Dominance
trees (ID-trees) from Linear Precedence trees (LP-
trees). Gerdes and Kahane (2001) apply a hierarchi-
cal topological model for generating German word
order. Bohnet (2004) employs graph grammars to
map between dependency trees and linear prece-
dence trees represented as hierarchical graphs. In the
frameworks of HPSG, LFG, and CCG, a grammar-
based generator produces word order candidates that
might be non-projective, and a ranker is used to se-
lect the best surface realization (Cahill et al 2007;
White and Rajkumar, 2009).
Statistical methods for linearization have recently
become more popular (Langkilde and Knight, 1998;
Ringger et al 2004; Filippova and Strube, 2009;
Wan et al 2009; He et al 2009; Bohnet et al 2010;
Guo et al 2011). They typically work by travers-
ing the syntactic structure either bottom-up (Filip-
929
pova and Strube, 2007; Bohnet et al 2010) or top-
down (Guo et al 2011; Bohnet et al 2011). These
linearizers are mostly applied to English and do not
deal with non-projective word orders. An excep-
tion is Filippova and Strube (2007), who contribute
a study on the treatment of preverbal and postver-
bal constituents for German focusing on constituent
order at the sentence level. The work most similar
to ours is that of Gamon et al(2002). They use
machine-learning techniques to lift edges in a pre-
processing step to a surface realizer. Their objec-
tive is the same as ours: by lifting, they avoid cross-
ing edges. However, contrary to our work, they use
phrase-structure syntax and focus on a limited num-
ber of cases of crossing branches in German only.
3 Lifting Dependency Edges
In this section, we describe the first of the two stages
in our approach, namely the classifier that lifts edges
in dependency trees. The classifier we aim to train
is meant to predict liftings on a given unordered de-
pendency tree, yielding a tree that, with a perfect lin-
earization, would not have any non-projective edges.
3.1 Preliminaries
The dependency trees we consider are of the form
displayed in Figure 1. More precisely, all words (or
nodes) form a rooted tree, where every node has ex-
actly one parent (or head). Edges point from head
to dependent, denoted in the text by h? d, where h
is the head and d the dependent. All nodes directly
or transitively depend on an artificial root node (de-
picted in Figure 1 as the incoming edge to is).
We say that a node a dominates a node d if a is
an ancestor of d. An edge h ? d is projective iff
h dominates all nodes in the linear span between h
and d. Otherwise it is non-projective. Moreover,
a dependency tree is projective iff all its edges are
projective. Otherwise it is non-projective.
A lifting of an edge h? d (or simply of the node
d) is an operation that replaces h ? d with g ? d,
given that there exists an edge g ? h in the tree, and
undefined otherwise (i.e. the dependent d is reat-
tached to the head of its head).2 When the lifting
2The undefined case occurs only when d depends on the
root, and hence cannot be lifted further; but these edges are by
definition projective, since the root dominates the entire tree.
operation is applied n successive times to the same
node, we say the node was lifted n steps.
3.2 Training
During training we make use of the projectivization
algorithm described by Nivre and Nilsson (2005).
It works by iteratively lifting the shortest non-
projective edges until the tree is projective. Here,
shortest edge refers to the edge spanning over the
fewest number of words. Since finding the shortest
edge relies on the linear order, instead of lifting the
shortest edge, we lift non-projective edges ordered
by depth in the tree, starting with the deepest nested
edge. A lifted version of the tree from Figure 1 is
shown in Figure 3. The edge of what has been lifted
three steps (the original edge is dotted), and the tree
is no longer non-projective.
It is what federal support should try to achieve
SBJ
ROOT OBJ
OBJNMOD SBJ
PRD
VC OPRD IM
Figure 3: The sentence from Figure 1, where what has
been assigned a new head (solid line). The original edge
is dotted.
We model the edge lifting problem as a multi-
class classification problem and consider nodes one
at a time and ask the question ?How far should this
edge be lifted??, where classes correspond to lifting
0, 1, 2, ..., n steps. To create training instances we
use the projectivization algorithm mentioned above.
We traverse the nodes of the tree sorted by depth.
For multiple nodes at the same depth, ties are broken
by linear order, i.e. for multiple nodes at the same
depth, the leftmost is visited first. When a node is
visited, we create a training instance out of it. Its
class is determined by the number of steps it would
be lifted by the projectivization algorithm given the
linear order (in most cases the class corresponds to
no lifting, since most edges are projective). As we
traverse the nodes, we also execute the liftings (if
any) and update the tree on the fly.
The training instances derived are used to train a
logistic regression classifier using the LIBLINEAR
package (Fan et al 2008). The features used for
the lifting classifier are described in Table 1. Since
we use linear classifiers, our feature set al con-
tains conjunctions of atomic features. The features
930
Atomic features
?x ? {w,wp, wgp, wch, ws, wun} morph(x), label(x), lemma(x), PoS(x)
?x ? {wgc, wne, wco} label(x), lemma(x), PoS(x)
Complex features
?x ? {w,wp, wgp} lemma(x)+PoS(x), label(x)+PoS(x), label(x)+lemma(x)
?x ? {wch, ws, wun}, y = w lemma(x)+lemma(y), PoS(y)+lemma(x), PoS(y)+lemma(x)
?x ? {w,wp, wgp}, y = HEAD(x) lemma(x)+lemma(y), lemma(x)+PoS(y), PoS(x)+lemma(y)
?x ? {w,wp, wgp}, y = HEAD(x), z = HEAD(y) PoS(x)+PoS(y)+PoS(z), label(x)+label(y)+label(z)
?x ? {wch, ws, wun}, y = HEAD(x), z = HEAD(y) PoS(x)+PoS(y)+PoS(z), label(x)+label(y)+label(z)
Non-binary features
?x ? {w,wp, wgp} SUBTREESIZE(x), RELSUBTREESIZE(x)
Table 1: Features used for lifting. w refers to the word (dependent) in question. And with respect to w, wp is the
parent; wgp is the grandparent; wch are children; ws are siblings; wun are uncles (i.e. children of the grandparent,
excluding the parent); wgc are grandchildren; wne are nephews (i.e. grandchildren of the parent that are not children
of w); wco are cousins (i.e. grandchildren of the grandparent that are not w or siblings of w). The non-binary feature
functions refer to: SUBTREESIZE ? the absolute number of nodes below x, RELSUBTREESIZE ? the relative size of
the subtree rooted at x with respect to the whole tree.
involve the lemma, dependency edge label, part-of-
speech tag, and morphological features of the node
in question, and of several neighboring nodes in the
dependency tree. We also have a few non-binary fea-
tures that encode the size of the subtree headed by
the node and its ancestors.
We ran preliminary experiments to determine the
optimal architecture. First, other ways of modeling
the liftings are conceivable. To find new reattach-
ment points, Gamon et al(2002) propose two other
ways, both using a binary classifier: applying the
classifier to each node x along the path to the root
asking ?Should d be reattached to x??; or lifting one
step at a time and applying the classifier iteratively
until it says stop. They found that the latter outper-
formed the former. We tried this method, but found
that it was inferior to the multi-class model and more
frequently over- or underlifted.
Second, to avoid data sparseness for infrequent
lifting distances, we introduce a maximum number
of liftings. We found that a maximum of 3 gave the
best performance. In the pseudocode below, we re-
fer to this number as maxsteps.3 This means that we
are able to predict the correct lifting for most (but
not all) of the non-projective edges in our data sets
(cf. Table 3).
Third, as Nivre and Nilsson (2005) do for pars-
3During training, nodes that are lifted further than maxsteps
are assigned to the class corresponding to maxsteps. This ap-
proach worked better than ignoring the training instance or
treating it as a non-lifting (i.e. a lifting of 0 steps).
ing, we experimented with marking edges that were
lifted by indicating this on the edge labels. In the
case of parsing, this step is necessary in order to re-
verse the liftings in the parser output. In our case,
it could potentially be beneficial for both the lifting
classifier, and for the linearizer. However, we found
that marking liftings at best gave similar results as
not marking, so we kept the original labels without
marking.
3.3 Decoding
In the decoding stage, an unordered tree is given and
the goal is to lift edges that would be non-projective
with respect to the gold linear order. Similarly to
how training instances are derived, the decoding al-
gorithm traverses the tree bottom-up and visits every
node once. Ties between nodes at the same depth are
broken in an arbitrary but deterministic way. When
a node is visited, the classifier is applied and the cor-
responding lifting is executed. Pseudocode is given
in Algorithm 1.4
Different orderings of nodes at the same depth
can lead to different lifts. The reason is that lift-
ings are applied immediately and this influences the
features when subsequent nodes are considered. For
instance, consider two sibling nodes ni and nj . If
ni is visited before nj , and ni is lifted, this means
4The MIN function is used to guarantee that the edge is not
lifted beyond the root node of the tree. This does not happen
in practice though, since the feature set of the classifier include
features that implicitly encode the proximity to the root node.
931
that at the time we visit nj , ni is no longer a sibling
of nj , but rather an uncle. An obvious extension of
the decoding algorithm presented above is to apply
beam search. This allows us to consider nj both in
the context where ni has been lifted and when it has
not been lifted.
1 N? NODES(T )
2 SORT-BY-DEPTH-BREAK-TIES-ARBITRARILY(N,T )
3 foreach node ? N do
4 feats? EXTRACT-FEATURES(node, T )
5 steps? CLASSIFY(feats)
6 steps? MIN(steps,ROOT-DIST(node))
7 LIFT(node, T, steps)
8 return T
Algorithm 1: Greedy decoding for lifting.
Pseudocode for the beam search decoder is given
in Algorithm 2. The algorithm keeps an agenda of
trees to explore as each node is visited. For every
node, it clones the current tree and applies every pos-
sible lifting. Every tree also has an associated score,
which is the sum of the scores of each lifting so far.
The score of a lifting is defined to be the log proba-
bility returned from the logistic classifier. After ex-
ploring all trees in the agenda, the k-best new trees
from the beam are extracted and put back into the
agenda. When all nodes have been visited, the best
tree in the agenda is returned. For the experiments
the beam size (k in Algorithm 2) was set to 20.
1 N? NODES(T )
2 SORT-BY-DEPTH-BREAK-TIES-ARBITRARILY(N,T )
3 Tscore ? 0
4 Agenda? {T}
5 foreach node ? N do
6 Beam? ?
7 foreach tree ? Agenda do
8 feats? EXTRACT-FEATURES(node, tree)
9 m? MIN(maxsteps,ROOT-DIST(node))
10 foreach s ? 0 .. maxsteps do
11 t? CLONE(tree)
12 score? GET-LIFT-SCORE(feats, s)
13 tscore = tscore + score
14 LIFT(node, t, s)
15 Beam? Beam ? {t}
16 Agenda? EXTRACTKBEST(Beam, k)
17 return EXTRACTKBEST(Agenda, 1)
Algorithm 2: Beam decoding for lifting.
While beam search allows us to explore the search
space somewhat more thoroughly, a large number of
possibilities remain unaccounted for. Again, con-
sider the sibling nodes ni and nj when ni is visited
before nj . The beam allows us to consider nj both
when ni is lifted and when it is not. However, the
situation where nj is visited before ni is still never
considered. Ideally, all permutations of nodes at the
same depth should be explored before moving on.
Unfortunately this leads to a combinatorial explo-
sion of permutations, and exhaustive search is not
tractable. As an approximation, we create two or-
derings and run the beam search twice. The dif-
ference between the orderings is that in the second
one all ties are reversed. As this bibeam consistently
improved over the beam in Algorithm 2, we only
present these results in Section 5 (there denoted sim-
ply Beam).
4 Linearization
A linearizer searches for the optimal word order
given an unordered dependency tree, where the op-
timal word order is defined as the single reference
order of the dependency tree in the gold standard.
We employ a statistical linearizer that is trained on a
corpus of pairs consisting of unordered dependency
trees and their corresponding sentences. The lin-
earization method consists of the following steps:
Creating word order domains. In the first step,
we build the word order domains dh for all nodes
h ? y of a dependency tree y. A domain is defined
as a node and all of its direct dependents. For ex-
ample, the tree shown in Figure 3 has the following
domains: {it, be, should}, {what, support, should, try},
{federal, support}, {try, to}, {to, achieve}
If an edge was lifted before the linearization, the
lifted node will end up in the word order domain of
its new head rather than in the domain of its original
head. This way, the linearizer can deduce word or-
ders that would result in non-projective structures in
the non-lifted tree.
Ordering the words of the domains. In the sec-
ond step, the linearizer orders the words of each do-
main. The position of a subtree is determined by the
position of the head of the subtree in the enclosing
domain. Algorithm 3 shows the tree linearization
algorithm. In our implementation, the linearizer tra-
verses the tree either top-down or bottom-up.
932
1 // T is the dependency tree with lifted nodes
2 beam-size? 1000
3 for h ? T do
4 domainh? GET-DOMAIN(T ,h)
5 // initialize the beam with a empty word list
6 Agendah? ()
7 foreach w ? domainh do
8 // beam for extending word order lists
9 Beam? ()
10 foreach l ? Agendah do
11 // clone list l and append the word w
12 if w 6? l then
13 l? ? APPEND(l,m)
14 Beam? Beam ? l?
15 score[l?]? COMPUTE-SCORE(l?)
16 if | Beam | > beam-size then
17 SORT-LISTS-DESCENDING-TO-
SCORE(Beam,score)
18 Agendah? SUBLIST(0,beam-size,Beam)
19 else
20 Agendah? Beam
21 foreach l ? Beam do
22 SCOREg[l]? SCORE[l] +
GLOBAL-SCORE(l)
23 Agendah? Beam
24 return Beam
Algorithm 3: Dependency Tree Linearization.
The linearization algorithm initializes the word
order beam (agendah) with an empty order () (line
6). It then iterates over the words of a domain (lines
7-20). In the first iteration, the algorithm clones and
extends the empty word order list () by each word
of the sentence (line 12-15). If the beam (beam)
exceeds a certain size (beam-size), it is sorted by
score and pruned to maximum beam size (beam-
size) (lines 16-20). The following example illus-
trates the extensions of the beam for the top domain
shown in Figure 3.
Iter. agendabe
0: ()
1: ((it) (be) (should))
2: ((it be) (it should) (be it) (be should) ...)
The beam enables us to apply features that encode
information about the first tokens and the last token,
which are important for generating, e.g. the word
order of questions, i. e. if the last token is a question
mark then the sentence should probably be a ques-
tion (cf. feature set shown in Table 2). Furthermore,
the beam enables us to generate alternative lineariza-
tions. For this, the algorithm iterates over the alter-
native word orders of the domains in order to as-
semble different word orders on the sentence level.5
Finally, when traversing the tree bottom-up, the al-
gorithm has to use the different orders of the already
ordered subtrees as context, which also requires a
search over alternative word orders of the domains.
Training of the Linearizer. We use MIRA
(Crammer et al 2006) for the training of the lin-
earizer. The classifier provides a score that we use to
rank the alternative word orders. Algorithm 3 calls
two functions to compute the score: compute-score
(line 15) for features based on pairs of words and tri-
grams and compute-global-score for features based
on word patterns of a domain. Table 2 shows the
feature set for the two functions. In the case that the
linearization of a word order domain is incorrect the
algorithm updates its weight vector w. The follow-
ing equation shows the update function of the weight
vector:
w = w + ?h(?(dh, T, xg)? ?(dh, T, xp))
We update the weight vector w by adding the dif-
ference of the feature vector representation of the
correct linearization xg and the wrongly predicted
linearization xp, multiplied by ? . ? is the passive-
aggressive update factor as defined below. The suf-
fered lossh is ?(dh, T, xp)? ?(dh, T, xg).
? = lossh||?(dh,T,xg)??(dh,T,xp)||2
Creating the word order of a sentence. The lin-
earizer traverses the tree either top-down or bottom-
up and assembles the results in the surface order.
The bottom-up linearization algorithm can take into
account features drawn from the already ordered
subtrees while the top-down algorithm can employ
as context only the unordered nodes. However, the
bottom-up algorithm additionally has to carry out a
search over the alternative linearization of the sub-
domains, as different orders of the subdomain pro-
vide different context features. This leads to a higher
linearization time. We implemented both, but could
only find a rather small accuracy difference. In the
following, we therefore present results only for the
top-down method.
5The beam also makes it possible to employ a generative
language model to rerank alternative linearizations.
933
Atomic features
For nodes w ?
domainh
lemma(w), label(w), PoS(w), num-children(w), num-grandchildren(w), label-children(w),
PoS-children(w)
For domain
domainh
head(w1,w2), head(w1,w2,w3), label(head), PoS(head), PoS(w1), label(wn), label(wn?1),
contains-?(domainh)
Complex features
For bigrams
(w1, w2) ?
domainh
feat2: label(w1)+label(w2), label(w1)+lemma(w2), lemma(w1)+lemma(w2), PoSw1+PoSw2
feat3: label(w1)+num-children(w2)+num-children(w1),PoS-child(w1)+label(w1)+label(w2)
feat4: label(w1)+label(w2)+lemma(w2)+PoS(w1), label(w1)+label(w2)+PoS(head)+head(w1,w2)
feat5: label(w1)+label(w2)+PoS(head)+label(head)+head(w1,w2)
For trigrams
(w1, w2, w3) ?
domainh
feat3: lemma(w1)+lemma(w2)+lemma(w3)
feat4: PoS(w1)+PoS(w2)+PoS(w3)+head(w1,w2,w3)
feat5: label(w1)+label(w2)+label(w3)+PoS(w1)+head(w1,w2,w3)
For sentence s feat6: label(w1)+label(wn?1)+lemma(head)+lemma(w1)+lemma(wn?1)
feat7: PoS(w1)+PoS(w2)+PoS(w3)+PoS(wn?1)+PoS(wn?2)+PoS(wn?3)+contains-?(s)
Table 2: Exemplified features used for scoring linearizations of a word order domain (see Algorithm 3). Atomic
features which represent properties of a node or a domain are conjoined into feature vectors of different lengths.
Linearizations are scored based on bigrams, trigrams, and global sentence-level features.
5 Experiments
We conduct experiments on six European languages
with varying degrees of word order restrictions:
While English word order is very restrictive, Czech
and Hungarian exhibit few word order constraints.
Danish, Dutch, and German (so-called V2, i. e.
verb-second, languages) show a relatively free word
order that is however more restrictive than in Hun-
garian or Czech. The English and the Czech data
are from the CoNLL 2009 Shared Task data sets
(Hajic? et al 2009). The Danish and the Dutch data
are from the CoNLL 2006 Shared Task data sets
(Buchholz and Marsi, 2006). For Hungarian, we use
the Hungarian Dependency Treebank (Vincze et al
2010), and for German, we use a dependency con-
version by Seeker and Kuhn (2012).
# sent?s np sent?s np edges np ? 3 lifts
English 39,279 7.63 % 0.39% 98.39%
German 36,000 28.71% 2.34% 94.98%
Dutch 13,349 36.44% 5.42% 99.80%
Danish 5,190 15.62 % 1.00% 96.72%
Hungarian 61,034 15.81% 1.45% 99.82%
Czech 38,727 22.42% 1.86% 99.84%
Table 3: Size of training sets, percentage of non-
projective (np) sentences and edges, percentage of np
edges covered by 3 lifting steps.
Table 3 shows the sizes of the training corpora
and the percentage of non-projective sentences and
edges in the data. Note that the data sets for Dan-
ish and Dutch are quite small. English has the least
percentage of non-projective edges. Czech, Ger-
man, and Dutch show the highest percentage of non-
projective edges. The last column shows the per-
centage of non-projective edges that can be made
projective by at most 3 lifting steps.
5.1 Setup
In our two-stage approach, we first train the lifting
classifier. The results for this classifier are reported
in Section 5.2.
Second, we train the linearizer on the output of
the lifting classifier. To assess the impact of the
lifting technique on linearization, we built four sys-
tems on each language: (a) a linearizer trained on
the original, non-lifted dependency structures (No-
lift), two trained on the automatically lifted edges
(comparing (b) the beam and (c) greedy decoding),
(d) one trained on the oracle, i. e. gold-lifted struc-
tures, which gives us an upper bound for the lifting
technique. The linearization results are reported in
Section 5.3.
In this two-stage setup, we have the problem that,
if we re-apply the lifting classifier on the data it was
trained on, the input for the linearizer will be better
during training than during testing. To provide real-
istic training data for the linearizer, we make a 10-
fold cross-validation of the lifting classifier on the
training set, and use this as training data for the lin-
earizer. The lifting classifier that is applied to the
test set is trained on the entire training set.
934
5.2 Lifting results
To evaluate the performance of the lifting classifier,
we present precision, recall, and F-measure results
for each language. We also compute the percentage
of sentences that were handled perfectly by the lift-
ing classifier. Precision and recall are defined the
usual way in terms of true positives, false positives,
and false negatives, where true positives are edges
that should be lifted and were lifted correctly; false
positives are edges that should not be lifted but were
and edges that should be lifted and were lifted, but
were reattached in the wrong place; false negatives
are edges that should be lifted but were not.
The performance of both the greedy decoder and
the bibeam decoder are shown in Table 4. The scores
are taken on the cross-validation on the training set,
as this provides more reliable figures. The scores
are micro-averaged, i.e. all folds are concatenated
and compared to the entire training set.
Although the major evaluation of the lifting is
given by the performance of the linearizer, Table 4
gives us some clues about the lifting. We see that
precision is generally much higher than recall. We
believe this is related to the fact that some phenom-
ena encoded by non-projective edges are more sys-
tematic and thus easier to learn than others (e. g. wh-
extraction vs. relative clause extraposition). We also
find that beam search consistently yields modest in-
creases in performance.
Greedy Beam
P R F1 Perfect P R F1 Perfect
Eng 77.31 50.45 61.05 95.76 78.85 50.63 61.66 95.83
Ger 72.33 63.59 67.68 81.91 72.05 64.41 68.02 81.97
Dut 76.66 74.89 75.77 79.28 78.07 76.49 77.27 80.34
Dan 85.90 58.55 69.64 92.76 85.90 58.55 69.64 92.74
Hun 72.60 61.61 66.66 88.46 73.06 64.77 68.67 88.73
Cze 77.79 55.00 64.44 86.28 77.31 55.68 64.74 86.33
Table 4: Precision, recall, F-measure and perfect projec-
tivization results for the lifting classifier.
5.3 Linearization Results and Discussion
We evaluate the linearizer with standard metrics: n-
gram overlap measures (BLEU, NIST), edit distance
(Edit), and the proportion of exactly linearized sen-
tences (Exact). As a means to assess the impact of
lifting more precisely, we propose the word-based
measure Exactlift which only looks at the words
with an incoming lifted edge. The Exactlift score
then corresponds to the percentage of these words
that has been realized in the exact same position as
in the original sentence.
LangLift BLEU NIST Edit Exact Exactlift Nlift
EngNolift 0.911 15.09 0.922 56.40 0.00 0
EngGreedy 0.914 15.10 0.923 57.27 59.87 152
EngBeam 0.916 15.11 0.925 58.48 62.82 156
EngOracle 0.923 15.14 0.928 60.73 70.42 240
GerNolift 0.792 13.76 0.844 40.4 0.00 0
GerGreedy 0.811 13.86 0.864 42.9 55.21 480
GerBeam 0.813 13.86 0.866 43.3 56.47 487
GerOracle 0.843 13.97 0.889 49.95 72.87 634
DutNolift 0.743 11.31 0.796 30.05 0.00 0
DutGreedy 0.784 11.47 0.797 37.56 41.02 256
DutBeam 0.778 11.46 0.8 37.05 47.45 255
DutOracle 0.825 11.63 0.848 44.82 70.55 292
DanNolift 0.836 11.80 0.886 44.41 0.00 0
DanGreedy 0.852 11.88 0.90 45.96 67.65 34
DanBeam 0.858 11.90 0.90 48.76 67.65 34
DanOracle 0.865 11.92 0.90 50.93 74.42 43
HunNolift 0.755 15.70 0.839 30.71 0.00 0
HunGreedy 0.764 15.71 0.844 31.98 41,81 1,538
HunBeam 0.764 15.71 0.844 31.98 41.37 1,581
HunOracle 0.777 15.79 0.849 34.30 57.53 1,933
CzeNolift 0.693 14.32 0.789 25.14 0.00 0
CzeGreedy 0.711 14.45 0.797 26.85 42.04 923
CzeBeam 0.712 14.45 0.795 26.37 41.34 941
CzeOracle 0.729 14.52 0.806 28.79 53.12 1,282
Table 5: Performance of linearizers using different lift-
ings, Exactlift is the exact match for words with an in-
coming lifted edge, Nlift is the total number of lifted
edges.
The results are presented in Table 5. On each
language, the predicted liftings significantly im-
prove on the non-lifted baseline (except the greedy
decoding in English).6 The differences between
the beam and the greedy decoding are not signif-
icant. The scores on the oracle liftings suggest
that the impact of lifting on linearization is heav-
ily language-dependent: It is highest on the V2-
languages, and somewhat smaller on English, Hun-
garian, and Czech. This is not surprising since the
V2-languages (especially German and Dutch) have
the highest proportion of non-projective edges and
sentences (see Table 3). On the other hand, En-
glish has a very small number of non-projective
edges, such that the BLEU score (which captures
the n-gram level) reflects the improvement by only
6We used a t-test, with ? = 0.01.
935
a small increase. However, note that, on the sen-
tence level, the percentage of exactly regenerated
sentences increases by 2 points which suggests that
a non-negligible amount of non-projective sentences
can now be generated more fluently.
50556065707580
Eng
Ger
Dut
Dan
Hun
Cze
langua
ge
accuracy
periph
ery left right
Figure 4: Accuracy for the linearization of the sentences?
left and right periphery, the bars are upper and lower
bounds of the non-lifted and the gold-lifted baseline.
The Exactlift measure refines this picture: The
linearization of the non-projective edges is relatively
exact in English, and much less precise in Hungarian
and Czech where Exactlift is even low on the gold-
lifted edges. The linearization quality is also quite
moderate on Dutch where the lifting leads to con-
siderable improvements. These tendencies point to
some important underlying distinctions in the non-
projective word order phenomena over which we
are generalizing: In certain cases, the linearization
seems to systematically follow from the fact that the
edge has to be lifted, such as wh-extraction in En-
glish (Figure 1). In other cases, the non-projective
linearization is just an alternative to other grammati-
cal, but maybe less appropriate, realizations, such as
the prefield-occupation in German (Figure 2).
Since a lot of non-projective word orders affect
the clause-initial or clause-final position, we evalu-
ate the exact match of the left periphery (first three
words) and the right periphery (last three words) of
the sentence. The accuracies obtained are plotted
in Figure 4, where the lower and upper bars corre-
spond to the lower and upper bound from the non-
lifted and the gold-lifted baseline. It clearly emerges
from this figure that the range of improvements ob-
tainable from lifting is closely tied to the general
linearization quality, and also to word order prop-
erties of the languages. Thus, the range of sentences
affected by the lifting is clearly largest for the V2-
languages. The accuracies are high, but the ranges
are small for English, whereas the accuracies are low
and the ranges quite small for Czech and Hungarian.
System BLEU NIST
(Bohnet et al 2011) (ranked 1st) 0.896 13.93
(Guo et al 2011) (ranked 2nd) 0.862 13.68
Baseline-Non-Lifted + LM 0.896 13.94
Beam-Lifted + LM 0.901 13.96
Table 6: Results on the development set of the 2011
Shared Task on Surface Realisation data, (the test set was
not officially released).
We also evaluated our linearizer on the data of
2011 Shared Task on Surface Realisation, which is
based on the English CoNLL 2009 data (like our
previous evaluations) but excludes information on
morphological realization. For training and evalu-
ation, we used the exact set up of the Shared Task.
For the morphological realization, we used the mor-
phological realizer of Bohnet et al(2010) that pre-
dicts the word form using shortest edit scripts. For
the language model (LM), we use a 5-gram model
with Kneser-Ney (Kneser and Ney, 1995) smoothing
derived from 11 million sentences of the Wikipedia.
In Table 6, we compare our two linearizers (with
and without lifting) to the two top systems of the
2011 Shared Task on Surface Realisation, (Bohnet et
al., 2011) and (Guo et al 2011). Without the lifting,
our system reaches a score comparable to the top-
ranked system in the Shared Task. With the lifting,
we get a small7 but statistically significant improve-
ment in BLEU such that our system reaches a higher
score than the top ranked systems. This shows that
the improvements we obtain from the lifting carry
over to more complex generation tasks which in-
clude morphological realization.
5.4 Human Evaluation
We have carried out a pilot human evaluation on the
German data in order to see whether human judges
prefer word orders obtained from the lifting-based
7Remember that English has the least percentage of non-
projective edges in our data sets, which are however important
to linearize correctly (see Figure 1).
936
linearizer. In particular, we wanted to check whether
the lifting-based linearizer produces more natural
word orders for sentences that had a non-projective
tree in the corpus, and maybe less natural word or-
ders on originally projective sentences. Therefore,
we divided the evaluated items into originally pro-
jective and non-projective sentences.
We asked four annotators to judge 60 sentence
pairs comparing the lifting-based against the non-
lifted linearizer using the toolkit by Kow and Belz
(2012). All annotators are students, two of them
have a background in linguistics. The items were
randomly sampled from the subset of the develop-
ment set containing those sentences where the lin-
earizers produced different surface realizations. The
items are subdivided into 30 originally projective
and 30 originally non-projective sentences.
For each item, we presented the original context
sentence from the corpus and the pair of automat-
ically produced linearizations for the current sen-
tence. The annotators had to decide on two crite-
ria: (i) which sentence do they prefer? (ii) how flu-
ent is that sentence? In both cases, we used con-
tinuous sliders as rating tools, since humans seem
to prefer them (Belz and Kow, 2011). For the first
criterion, the slider positions were mapped to values
from -50 (preference for left sentence) to 50 (pref-
erence for right sentence). If the slider position is
zero, both sentences are equally preferred. For the
second criterion, the slider positions were mapped
to values from 0 (absolutely broken sentence) to 100
(perfectly fluent sentence).
Sentences Scores Equal Lifted Non-lifted
All
% selected 44.58% 35.0% 20.42%
Fluency 56.14 75.77 72.78
Preference 0 34.75 31.06
Non-
Proj.
% selected 29.63% 58.33% 12.04%
Fluency 43.06 76.27 68.85
Preference 0 37.52 24.46
Proj.
% selected 56.82% 15.91% 27.27%
Fluency 61.72 74.29 74.19
Preference 0 26.43 33.44
Table 7: Results from human evaluation.
Table 7 presents the results averaged over all sen-
tences, as well as for the subsets of non-projective
and projective sentences. We report the percentage
of items where the judges selected both, the lifted, or
non-lifted sentence, alongside with the average flu-
ency score (0-100) and preference strength (0-50).
On the entire set of items, the judges selected both
sentences in almost half of the cases. However, on
the subset of non-projective sentences, the lifted ver-
sion is clearly preferred and has a higher average
fluency and preference strength. The percentage of
zero preference items is much higher on the sub-
set of projective sentences. Moreover, the average
fluency of the zero preference items is remarkably
higher on the projective sentences than on the non-
projective subset. We conclude that humans have
a strong preference for lifting-based linearizations
on non-projective sentences. We attribute the low
fluency score on the non-projective zero preference
items to cases where the linearizer did not get a cor-
rect lifting or could not linearize the lifting correctly
such that the lifted and the non-lifted version were
not appropriate. On the other hand, incorrect lift-
ings on projective sentences do not necessarily seem
to result in deprecated linearizations, which leads to
the high percentage of zero preferences with a good
average fluency on this subset.
6 Conclusion
We have presented a novel technique to linearize
sentences for a range of languages that exhibit non-
projective word order. Our approach deals with non-
projectivity by lifting edges in an unordered input
tree which can then be linearized by a standard pro-
jective linearization algorithm.
We obtain significant improvements for the
lifting-based linearization on English, German,
Dutch, Danish, Czech and Hungarian, and show that
lifting has the largest impact on the V2-languages.
In a human evaluation carried out on German we
also show that human judges clearly prefer lifting-
based linearizations on originally non-projective
sentences, and, on the other hand, that incorrect lift-
ings do not necessarily result in bad realizations of
the sentence.
Acknowledgments
This work was funded by the Deutsche Forschungs-
gemeinschaft (DFG) via the SFB 732 ?Incremental
Specification in Context?. We would also like to
thank Anna Ha?tty and our four annotators for their
contribution to the human evaluation.
937
References
A. Belz and E. Kow. 2011. Discrete vs. Continuous Rat-
ing Scales for Language Evaluation in NLP. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 230?235, Portland, Oregon, USA,
June. Association for Computational Linguistics.
A. Belz, M. White, D. Espinosa, D. Hogan, E. Kow, and
A. Stent. 2011. The First Surface Realisation Shared
Task: Overview and Evaluation Results. In ENLG?11.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2000.
The Prague Dependency Treebank: A Three-level an-
notation scenario. In A. Abeille?, editor, Treebanks:
Building and using syntactically annotated corpora.,
chapter 1, pages 103?127. Kluwer Academic Publish-
ers, Amsterdam.
B. Bohnet, L. Wanner, S. Mille, and A. Burga. 2010.
Broad coverage multilingual deep sentence generation
with a stochastic multi-level realizer. In Coling 2010,
pages 98?106.
B. Bohnet, S. Mille, B. Favre, and L. Wanner. 2011.
<stumaba>: From deep representation to surface. In
Proceedings of the Generation Challenges Session at
the 13th European Workshop on NLG, pages 232?235,
Nancy, France.
B. Bohnet. 2004. A Graph Grammar Approach to Map
Between Dependency Trees and Topological Models.
In IJCNLP, pages 636?645.
N. Bro?ker. 1998. Separating Surface Order and Syntactic
Relations in a Dependency Grammar. In COLING-
ACL 98.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Natu-
ral Language Learning, pages 149?164, Morristown,
NJ, USA. Association for Computational Linguistics.
A. Cahill, M. Forst, and C. Rohrer. 2007. Stochastic real-
isation ranking for a free word order language. ENLG
?07, pages 17?24.
K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.
2006. Online Passive-Aggressive Algorithms. Jour-
nal of Machine Learning Research, 7:551?585.
D. Duchier and R. Debusmann. 2001. Topological de-
pendency trees: A constraint-based account of linear
precedence. In Proceedings of the ACL.
R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. 2008.
LIBLINEAR: A library for large linear classification.
Journal of Machine Learning Research, 9:1871?1874.
K. Filippova and M. Strube. 2007. Generating con-
stituent order in german clauses. In ACL, pages 320?
327.
K. Filippova and M. Strube. 2009. Tree linearization in
English: improving language model based approaches.
In NAACL, pages 225?228, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
M. Gamon, E. Ringger, R. Moore, S. Corston-Olivier,
and Z. Zhang. 2002. Extraposition: A case study in
German sentence realization. In Proceedings of Col-
ing 2002. Association for Computational Linguistics.
K. Gerdes and S. Kahane. 2001. Word order in german:
A formal dependency grammar using a topological hi-
erarchy. In Proceedings of the ACL.
Y. Guo, D. Hogan, and J. van Genabith. 2011. Dcu at
generation challenges 2011 surface realisation track.
In Proceedings of the Generation Challenges Session
at the 13th European Workshop on NLG, pages 227?
229.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara, M.-
A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?,
J. Stepa?nek, P. Strana?k, M. Surdeanu, N. Xue, and
Y. Zhang. 2009. The CoNLL-2009 shared task:
Syntactic and Semantic dependencies in multiple lan-
guages. In Proceedings of the 13th CoNLL Shared
Task, pages 1?18, Boulder, Colorado.
E. Hajic?ova?, J. Havelka, P. Sgall, K. Vesela?, and D. Ze-
man. 2004. Issues of projectivity in the prague de-
pendency treebank. Prague Bulletin of Mathematical
Linguistics, 81.
W. He, H. Wang, Y. Guo, and T. Liu. 2009. Dependency
Based Chinese Sentence Realization. In Proceedings
of the ACL and of the IJCNLP, pages 809?816.
S. Kahane, A. Nasr, and O. Rambow. 1998. Pseudo-
projectivity: A polynomially parsable non-projective
dependency grammar. In COLING-ACL, pages 646?
652.
A. Kathol and C. Pollard. 1995. Extraposition via com-
plex domain formation. In Meeting of the Association
for Computational Linguistics, pages 174?180.
R. Kneser and H. Ney. 1995. In In Proceedings of the
IEEE International Conference on Acoustics, Speech
and Signal Processing, pages 181?184.
E. Kow and A. Belz. 2012. LGRT-Eval: A Toolkit for
Creating Online Language Evaluation Experiments.
In Proceedings of the 8th International Conference on
Language Resources and Evaluation (LREC?12).
I. Langkilde and K. Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
COLING-ACL, pages 704?710.
J. Nivre and J. Nilsson. 2005. Pseudo-projective de-
pendency parsing. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL?05), pages 99?106, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
O. Rambow and A. K. Joshi. 1994. A formal look at
dependency grammars and phrase-structure grammars,
with special consideration of word-order phenomena.
938
In Leo Wanner, editor, Current Issues in Meaning-Text
Theory. Pinter, London, UK.
M. Reape. 1989. A logical treatment of semi-free word
order and bounded discontinuous constituency. In
Proceedings of the EACL, EACL ?89, pages 103?110.
E. Ringger, M. Gamon, R. C. Moore, D. Rojas, M. Smets,
and S. Corston-Oliver. 2004. Linguistically informed
statistical models of constituent structure for ordering
in sentence realization. In COLING ?04, pages 673?
679.
W. Seeker and J. Kuhn. 2012. Making Ellipses Explicit
in Dependency Conversion for a German Treebank. In
Proceedings of LREC 2012, Istanbul, Turkey. Euro-
pean Language Resources Association (ELRA).
A. Stent. 2011. Att-0: Submission to generation chal-
lenges 2011 surface realization shared task. In Pro-
ceedings of the Generation Challenges Session at the
13th European Workshop on Natural Language Gener-
ation, pages 230?231, Nancy, France, September. As-
sociation for Computational Linguistics.
V. Vincze, D. Szauter, A. Alma?si, G. Mo?ra, Z. Alexin,
and J. Csirik. 2010. Hungarian Dependency Tree-
bank. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC 2010), pages 1855?1862, Valletta, Malta.
S. Wan, M. Dras, R. Dale, and C. Paris. 2009. Improving
grammaticality in statistical sentence generation: In-
troducing a dependency spanning tree algorithm with
an argument satisfaction model. In EACL, pages 852?
860.
M. White and R. Rajkumar. 2009. Perceptron reranking
for CCG realization. In EMNLP?09, pages 410?419,
Singapore, August.
939
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 333?344,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
The Effects of Syntactic Features in Automatic Prediction of Morphology
Wolfgang Seeker and Jonas Kuhn
Institute for Natural Language Processing
University of Stuttgart
{seeker,jonas}@ims.uni-stuttgart.de
Abstract
Morphology and syntax interact considerably
in many languages and language processing
should pay attention to these interdependen-
cies. We analyze the effect of syntactic fea-
tures when used in automatic morphology pre-
diction on four typologically different lan-
guages. We show that predicting morphology
for languages with highly ambiguous word
forms profits from taking the syntactic context
of words into account and results in state-of-
the-art models.
1 Introduction
In this paper, we investigate the interplay between
syntax and morphology with respect to the task of
assigning morphological descriptions (or tags) to
each token of a sentence. Specifically, we examine
the effect of syntactic information when it is inte-
grated into the feature model of a morphological tag-
ger. We test the effect of syntactic features on four
languages ? Czech, German, Hungarian, and Span-
ish ? and find that syntactic features improve our tag-
ger considerably for Czech and German, but not for
Hungarian and Spanish. Our analysis of construc-
tions that show morpho-syntactic agreement sug-
gests that syntactic features are important if the lan-
guage shows frequent word form syncretisms1 that
can be disambiguated by the syntactic context.
The meaning of a sentence is structurally encoded
1Syncretism describes the situation where a word form is
ambiguous between several different morphological descrip-
tions within its inflection paradigm.
by morphological and syntactic means.2 Different
languages, however, use them to a different extent.
Languages like English encode grammatical infor-
mation (like the subject vs object status of an argu-
ment) via word order, whereas languages like Czech
or Hungarian use different word forms. Automatic
analysis of languages with rich morphology needs
to pay attention to the interaction between morphol-
ogy and syntax in order to arrive at suitable com-
putational models. Linguistic theory (e. g., Bresnan
(2001), Melc?uk (2009)) suggests many interactions
between morphology and syntax. For example, lan-
guages with a case system use different forms of the
same word to mark different syntactic (or seman-
tic) relations (Blake, 2001). In many languages, two
words that participate in a syntactic relation show
covariance in some or all of their morphological fea-
tures (so-called agreement, Corbett (2006)).3
Automatic annotation of morphology assigns
morphological descriptions (e. g., nominative-
singular-masculine) to word forms. It is usually
modeled as a sequence model, often in combination
with part-of-speech tagging and lemmatization
(Collins, 2002; Hajic?, 2004; Smith et al, 2005;
Chrupa?a et al, 2008, and others). Sequence models
achieve high accuracy and coverage but since they
only use linear context they only approximate some
of the underlying hierarchical relationships. As
an example for these hierarchical relationships,
2And also by prosodic means, which we will not discuss
since text-based tools rarely have access to this information.
3For example, in English, the subject of a sentence and the
finite verb agree with respect to their number and person fea-
ture.
333
die wirtschaftlich am weitesten entwickelten , modernen und zum Teil katholisch gepra?gten Regionen
nom/acc.pl.fem nom/acc.pl.fem
the economic - most developed , modern and to part catholic influenced regions
NK
MO
PM MO
NK
CJ
CD
MO
NK MO
CJ
?the regions that are economically most developed, modern, and partly catholic?
Figure 1: Example of a German noun phrase. First and last word agree in number, gender, and case value.
Figure 1 shows a German noun phrase taken from
the German TiGer corpus (Brants et al, 2002).
The two bold-faced words are the determiner and
the head noun of the phrase, and they agree in
their gender, number, and case values. The word
Regionen (regions) is four-way ambiguous for its
case value, which is reduced to a two-way ambi-
guity between nominative and accusative by the
determiner. Further disambiguation would require
information about the syntactic role of the noun
phrase in a sentence. There are 11 tokens between
these two words, which would require a context
window of at least 13 to capture the agreement
relation within a sequence model. Syntactically,
however, as indicated by the dependency tree,
the determiner and the head are linked directly.
The interdependency between morphology and
syntax in the example thus manifests itself in the
morphological disambiguation of a highly syncretic
word form because of its government or agreement
relation to its respective syntactic head/dependents.
Of course, the sequence model is most of the
time a reasonable approximation, because the ma-
jority of noun phrases in the TiGer corpus are not
as long as the example in Figure 1.4 Furthermore,
not all languages show this kind of relationship be-
tween morphological forms and syntactic relation as
demonstrated for German. But taking advantage of
the morphosyntactic dependencies in a language can
give us better models that may even be capable of
handling the more difficult or rare cases. We there-
fore advocate that models for predicting morphology
should be designed with the typological characteris-
tics of a language and its morphosyntactic properties
in mind, and should, where appropriate, integrate
4We find 57,551 noun phrases with less than three tokens
between determiner and noun and 4,670 with three or more.
syntactic information in order to better model the
morphosyntactic interdependencies of the language.
In the remainder of the paper, we show empiri-
cally that taking syntactic information into account
produces state-of-the-art models for languages with
a high interdependency between morphology and
syntax. We use a simple setup, where we combine
a morphological tagger and a dependency parser in
a bootstrapping architecture in order to analyze the
effect of syntactic information on the performance
of the morphological tagger (Section 2). Using syn-
tactic features in morphology prediction requires a
syntactically annotated corpus for training a statisti-
cal parser, which may not be available for languages
with few resources. We show in Section 3 that only
very little syntactically annotated data is required to
achieve the improvements. We furthermore expect
that the improved morphological information also
improves parsing performance and present a prelim-
inary experiment in Section 4.
2 Experiments
In this section, we present a series of experiments
that investigate the effect of syntactic information on
the prediction of morphological features. We start
by describing our data sets and the system that we
used for the experiments.
2.1 Languages and Data Sets
We test our hypotheses on four different languages:
Czech, German, Hungarian, and Spanish.
Spanish, a Romance language, and German, a
Germanic language, constitute inflecting languages
that show verbal and nominal morphology, but not
as sophisticated as Czech and Hungarian. As we
will see in the experiments, it is relatively easy to
334
predict the morphological information annotated in
the Spanish data set.
Czech and Hungarian represent languages with
very rich morphological systems both in verbal and
nominal morphological paradigms. They differ sig-
nificantly in the way in which morphological infor-
mation is encoded in word forms. Czech, a Slavic
language, is an inflecting language, where one suf-
fix may signal several different morphological cate-
gories simultaneously (e. g., number, gender, case).
In contrast, Hungarian, a Finno-Ugric language, is
of the agglutinating type, where each morphological
category is marked by its own morpheme.
Both German and Czech show various form syn-
cretisms in their inflection paradigms. Form syn-
cretisms emerge when the same word form is am-
biguous between several different morphological de-
scriptions, and they are a major challenge to auto-
matic morphological analysis. Spanish shows syn-
cretism in the verbal inflection paradigms. In Hun-
garian, form syncretisms are much less frequent.
The case paradigm of Hungarian only shows one
form syncretism between dative and genitive case
(out of about 18 case suffixes).
All languages show agreement between subject
and verb, and within the noun phrase. The word or-
der in Czech and Hungarian is very variable whereas
it is more restrictive in Spanish and German.
As our data, we use the CoNLL 2009 Shared
Task data sets (Hajic? et al, 2009) for Czech and
Spanish. For German, we use the dependency
conversion of the TiGer treebank by Seeker and
Kuhn (2012), splitting it into 40k/5k/5k sentences
for training/development/test. For Hungarian, we
use the Szeged Dependency Treebank (Vincze et al,
2010), with the split of Farkas et al (2012).
2.2 System Description
To test our hypotheses, we implemented a tagger
that assigns full morphological descriptions to each
token in a sentence. The system was inspired by the
morphological tagger included in mate-tools.5 Like
the tagger provided with mate-tools, it is a classifier
that tags each token using the surrounding tokens in
5A collection of language independent, data-driven analysis
tools for lemmatization, pos-tagging, morphological analysis,
and dependency parsing: http://code.google.com/p/mate-tools
its feature model. Models are trained using passive-
aggressive online training (Crammer et al, 2003).
The system makes two passes over each sentence:
The first pass provides predicted tags that are used
as features during the second pass. We also adopted
the idea of a tag filter, which deterministically as-
signs tags for words that always occur with the same
tag in the training data.
For all matters of syntactic annotation in this pa-
per, we use the graph-based dependency parser by
Bohnet (2010), also included in mate-tools. All data
sets are annotated with gold syntactic information,
which is used to train the parsing models.
For our experiments, we use a bootstrapping ap-
proach: the parser uses the output of the morphology
in its feature set, and the morphological tagger we
want to analyze uses the output of the parser as syn-
tactic features. Since it is best to keep the training
setting as similar as possible to the test setting, we
use 10-fold jackknifing to annotate our training data
with predicted morphology or syntax respectively.
Jackknifing differs from cross-validation only in
its purpose. Cross-validation is used for evaluating
data, jackknifing is used to annotate data. The data
set is split into n parts, and n-1 parts are used to train
a model for annotating the nth part. This is then
rotated n times such that each part is annotated by
the automatic tool without training it on its own test
data. Jackknifing is important for creating a realis-
tic training scenario that provides automatic prepro-
cessing. For annotating development and test sets,
models are trained on the jackknifed training set.
2.3 The Effects of Syntactic Features
In the first experiment, we use the system described
in Section 2.2 to predict morphological information
on all four languages. We start with describing the
general setup and the feature set, and continue with
a discussion of the results.
The experimental setup is as follows: the German
and Spanish data sets are annotated with lemma and
part-of-speech information using 10-fold jackknif-
ing. The annotation is done with mate-tools? lem-
matizer and pos-tagger. For Czech and Hungarian,
we keep the annotation provided with the data sets.
Note that our experimental setup does not include
lemmas or part-of-speech tags as part of the predic-
tion of the morphology but annotates them in a pre-
335
processing step. It is not necessary to separate part-
of-speech and lemma from the prediction of mor-
phology and, in fact, many systems perform these
steps simultaneously (e. g. Spoustova? et al (2009)).
Doing morphology prediction as a separate step al-
lows us to use lemma and part-of-speech informa-
tion in the feature set.6
static features
form form1b form2b
form3b form1a lemma2a
pos1b pos2b pos1a
form+pos pos+s1 pos+s2
pos+s3 pos+s4 lemma+p2
lemma+p3 pos+number form+form1b
pos+pos1a pos+pos1b+pos2b s1+s1 1b
s1+s1 1a s2+s2 1a last-verb-lemma
last-verb-pos next-verb-lemma next-verb-pos
dynamic features
tag1b+tag2b tag2b+tag3b tag1a
tag1a+tag1b tag1a+tag2a tag2a+tag3a
pos1b+case1b last-verb-tag next-verb-tag
pos1b+case1b+pos2b+case2b
Hungarian only features
pos+uppercase
Czech only features
pos+p2
Spanish only features
s5 p1 p4
p5 s2 1a s3 1a
s4 1a
Table 1: Baseline feature set. form means word form,
lemma is lemma, pos is part-of-speech, s1/p1 stand for
suffix and prefix of length 1 (characters), tag is the mor-
phological tag predicted by the system, 1b/1a means 1
token before/after the current token, and + marks feature
conjunctions. number marks if the form contains a digit.
After preprocessing the data, our baseline system
is trained using the feature set shown in Table 1. The
baseline system does not make use of any syntactic
information but predicts morphological information
based solely on tokens and their linear context. The
features are divided into static features, which can be
computed on the input, and dynamic features, which
are computed also on previous output of the system
(cf. two passes in Section 2.2).
6Lemma and part-of-speech prediction may also profit from
syntactic information, see e.g. Prins (2004) or Bohnet and Nivre
(2012).
The feature sets in Table 1 were developed specif-
ically for our experiments and are the result of an
automatic forward/backward feature selection pro-
cess. The purpose of the feature selection was to ar-
rive at a baseline system that performs well without
any syntactic information. With such an optimized
baseline system, we can measure the contribution of
syntactic features more reliably.
The last-verb/next-verb and pos+case features are
variants of the features proposed in Votrubec (2006).
They extract information about the first verb within
the last 10/the next 30 tokens in the sentence. The
case feature extracts the case value from previously
assigned morphological tags. Note that the verb
features are approximating syntactic information by
making the assumption that the closest verbs are
likely to be syntactic heads for many words.
static features
h lemma h s2 h s3 pos+h pos s1+h s1
h dir h dir+h pos
ld s1 ld s2 ld p1 ld p4
dynamic features
h tag ld tag
Table 2: Syntactic features. h and ld mark features from
the head and the left-most daughter, dir is a binary fea-
ture marking the direction of the head with respect to the
current token.
After training the baseline models, we use them to
annotate the whole data set with morphological in-
formation (using 10-fold jackknifing for the training
portions). We then use 10-fold jackknifing again to
annotate the data sets with the dependency parser.
At this point, all our data sets are annotated with
predicted morphology from our baseline system and
with syntactic information from the parser, which
uses the morphological information from our base-
line system in its feature set. We can now retrain our
morphological tagger using features that are derived
from the dependency trees provided by the parser.
Note that this is not a stacking architecture, since
the second system does not use the predicted mor-
phology output from the baseline system. The loop
simply ensures that we get the best possible syntac-
tic features.
We extract two kinds of syntactic features: fea-
tures of the syntactic head of the current token, and
336
dev set test set
all oov all oov
Czech
morfette 90.37 68.66 90.01 67.25
our baseline 92.51 73.12 92.29 72.58
pred syntax *93.18 74.04 *92.82 73.11
gold syntax *93.64 75.20 *93.30 74.96
German
morfette 86.78 66.37 84.58 61.05
our baseline 90.92 72.52 89.11 69.67
pred syntax *92.07 75.06 *90.10 71.18
gold syntax *92.70 *76.29 *90.87 *73.20
Hungarian
morfette *96.19 *85.82 95.99 *85.43
our baseline 96.08 84.49 95.94 83.76
pred syntax 96.18 84.70 96.11 83.85
gold syntax *96.46 85.30 *96.35 84.50
Spanish
morfette 97.83 89.67 97.76 91.00
our baseline 97.83 89.05 97.59 90.88
pred syntax 97.84 89.08 97.67 90.91
gold syntax 98.11 90.34 97.88 91.61
Table 3: The effect of syntactic features when predicting
morphological information. * mark statistically signifi-
cantly better models compared to our baseline (sentence-
based t-test with ? = 0.05).
features of the left-most daughter of the current to-
ken. We also experimented with other types, e. g.
the right-most daughter, but these features did not
improve the model. This is likely due to the way
these languages encode morphological information
and may be different for other languages. From the
head and the left-most daughter, we construct fea-
tures about form, lemma, affixes, and tags. Table 2
lists the syntactic features that we use in the model.
With the syntactic features available due to the
parsing step, we train new models with the full sys-
tem. For each language, we run four experiments.
The first two are baseline experiments, where we
use the off-the-shelf morphological tagger morfette
(Chrupa?a et al, 2008) and our own baseline sys-
tem, both of which do not use any syntactic features.
In the third experiment, we evaluate our full system
using the syntactic features provided by the depen-
dency parser. As an oracle experiment, we also re-
port results on the full system when using the gold
standard syntax from the treebank. Table 3 presents
all results in terms of accuracy on all tokens (all)
dev set test set
all oov all oov
Czech
featurama 94.75 84.12 94.78 84.23
our baseline 93.80 80.47 93.57 80.53
pred syntax *94.40 81.51 *94.24 81.61
gold syntax *94.80 82.45 *94.64 82.80
German
RFTagger 90.63 72.11 89.04 70.80
our baseline 92.59 80.73 91.48 78.83
pred syntax *93.70 82.71 *92.51 80.20
gold syntax *94.28 *84.12 *93.32 *82.35
Hungarian
our baseline 97.27 92.61 97.03 91.28
pred syntax 97.38 92.39 97.19 91.50
gold syntax *97.63 92.79 *97.45 91.92
Spanish
our baseline 98.23 92.46 98.02 93.15
pred syntax 98.24 92.30 98.07 93.03
gold syntax 98.40 92.82 *98.22 93.64
Table 4: The effect of syntactic features when predicting
morphology using lexicons. * mark statistically signifi-
cantly better models compared to our baseline (sentence-
based t-test with ? = 0.05).
and out-of-vocabulary tokens only (oov). Out-of-
vocabulary tokens do not occur in the training data.
We find trends along several axes: Generally, the
syntactic features work well for Czech and Ger-
man, whereas for Hungarian and Spanish, they do
not yield any significant improvement. The im-
provements for German and Czech are between 0.5
(Czech) and 1.0 (German) percentage points abso-
lute in token accuracy, and between 0.2 (Czech test
set) and 2.5 (German dev set) percentage points ab-
solute in accuracy of unknown words. There are no
obvious differences between the development and
the test set in any of the languages.
Compared to the morfette baseline, we find our
systems to be either superior or equal to morfette in
terms of token accuracy. Regarding accuracy on un-
known words, morfette outperforms our systems for
Hungarian, but is outperformed on Czech and Ger-
man. For Spanish, all systems yield similar results.
Looking at the oracle experiment, we see that for
all languages, the system can learn something from
syntax. For Czech and German, this is clearly the
337
case, for Hungarian and Spanish, the differences are
small but visible. There are pronounced differences
between the predicted and the gold syntax experi-
ments in Czech and German. Clearly, the parser
makes mistakes that propagate through to the pre-
diction of the morphology.
2.4 Syntax vs Lexicon
The current state-of-the-art in predicting morpho-
logical features makes use of morphological lexi-
cons (e.g. Hajic? (2000), Hakkani-Tu?r et al (2002),
Hajic? (2004)). Lexicons define the possible morpho-
logical descriptions of a word and a statistical model
selects the most probable one among them. In the
following experiment, we test whether the contribu-
tion of syntactic features is similar or different to the
contribution of morphological lexicons.
Lexicons encode important knowledge that is dif-
ficult to pick up in a purely statistical system, e. g.
the gender of nouns, which often cannot be deduced
from the word form (Corbett, 1991).7
We extend our system from the previous experi-
ment to include information from a morphological
dictionaries. For Czech, we use the morphologi-
cal analyzer distributed with the Prague Dependency
Treebank 2 (Hajic? et al, 2006). For German, we
use DMor (Schiller, 1994). For Hungarian, we use
(Tro?n et al, 2006), and for Spanish, we use the mor-
phological analyzer included in Freeling (Carreras et
al., 2004). The output of the analyzers is given to the
system as features that simply record the presence of
a particular morphological analysis for the current
word. The system can thus use the output of any
tool regardless of its annotation scheme, especially
if the annotation scheme of the treebank is different
from the one of the morphological analyzer.
Table 4 presents the results of experiments where
we add the output of the morphological analyzers
to our system. Again, we run experiments with and
without syntactic features. For Czech, we also show
results from featurama8 with the feature set devel-
oped by Votrubec (2006). For German, we show re-
sults for RFTagger (Schmid and Laws, 2008).
As expected, the information from the morpho-
logical lexicon improves the overall performance
7Lexicons are also often used to speed up processing con-
siderably by restricting the search space of the statistical model.
8http://sourceforge.net/projects/featurama/
considerably compared to the results in Table 3, es-
pecially on unknown tokens. This shows that even
with the considerable amounts of training data avail-
able nowadays, rule-based morphological analyzers
are important resources for morphological descrip-
tion (cf. Hajic? (2000)). The contribution of syn-
tactic features in German and Czech is almost the
same as in the previous experiment, indicating that
the syntactic features contribute information that is
orthogonal to that of the morphological lexicon. The
lexicon provides lexical knowledge about a word
form, while the syntactic features provide the syn-
tactic context that is needed in German and Czech
to decide on the right morphological tag.
2.5 Language Differences
From the previous experiments, we conclude that
syntactic features help in the prediction of morphol-
ogy for Czech and German, but not for Hungarian
and Spanish. To further investigate the difference
between Czech and German on the one hand, and
Hungarian and Spanish on the other, we take a closer
look at the output of the tagger.
We find an interesting difference between the
two pairs of languages, namely the performance
with respect to agreement. Agreement is a phe-
nomenon where morphology and syntax strongly in-
teract. Morphological features co-vary between two
items in the sentence, but the relation between these
items can occur at various linguistic levels (Corbett,
2006). If the syntactic information helps with pre-
dicting morphological information, we expect this
to be particularly helpful with getting agreement
right. All languages show agreement to some ex-
tent. Specifically, all languages show agreement in
number (and person) between the subject and the
verb of a clause. Czech, German, and Spanish show
agreement in number, gender, and case (not Span-
ish) within a noun phrase. Hungarian shows case
agreement within the noun phrase only rarely, e.g.
for attributively used demonstrative pronouns.
In order to test the effect on agreement, we mea-
sure the accuracy on tokens that are in an agreement
relation with their syntactic head. We counted sub-
ject verb agreement as well as agreement with re-
spect to number, gender, and case (where applicable)
between a noun and its dependent adjective and de-
terminer. Table 5 displays the counts from the devel-
338
opment sets of each language. We compare the base-
line system that does not use any syntactic informa-
tion with the output of the morphological tagger that
uses the gold syntax. We use the gold syntax rather
than the predicted one in order to eliminate any in-
fluence from parsing errors. As can be seen from the
results, the level of agreement relations in Czech and
German improves when using syntactic information,
whereas in Spanish and Hungarian, only very tiny
changes occur.
agreement baseline gold syntax
Czech
sbj-verb 3199/4044 = 79.10 3264/4044 = 80.71
NP case 8719/9132 = 95.48 8821/9132 = 96.59
NP num 8933/9132 = 97.82 9016/9132 = 98.73
NP gen 8493/9132 = 93.00 8768/9132 = 96.01
German
sbj-verb 4412/4696 = 93.95 4562/4696 = 97.15
NP case 13340/13951 = 95.62 13510/13951 = 96.84
NP num 13631/13951 = 97.71 13788/13951 = 98.83
NP gen 13253/13951 = 95.00 13528/13951 = 96.97
Hungarian
sbj-verb 8653/10219 = 84.68 8655/10219 = 84.70
NP case 402/891 = 45.12 412/891 = 46.24
Spanish
sbj-verb 1930/2004 = 96.31 1932/2004 = 96.41
NP num 8810/8849 = 99.56 8816/8849 = 99.63
NP gen 8810/8849 = 99.56 8821/8849 = 99.68
Table 5: Agreement counts in morphological annotation
compared between the baseline system and the oracle
system using gold syntax.
For Czech and German, these results sugguest
that syntactic information helps with agreement. We
believe that the reasons why it does not help for
Hungarian and Spanish are the following: for Span-
ish, we see that also the baseline model achieves
very high accuracies (cf. Table 3) and also high rates
of correct agreement. It seems that for Spanish, syn-
tactic context is simply not necessary to make the
correct prediction. For Hungarian, the reason lies
within the inflectional paradigms of the language,
which do not show any form syncretism, mean-
ing that word forms in Hungarian are usually not
ambiguous within one morphological category (e.g.
case). Making a morphological tag prediction, how-
ever, is difficult only if the word form itself is am-
biguous between several morphological tags. In this
case, using the agreement relation between the word
and its syntactic head can help the system making
the proper prediction. This is the situation that we
find in Czech and German, where form syncretism
is pervasive in the inflectional paradigms.
2.6 Syntactic Features in Czech
In Section 2.4 we compared the performance of our
system on Czech to another system, featurama (see
Table 4). Featurama outperforms our baseline sys-
tem by a percentage point in token accuracy (and
even more for unknown tokens). Syntactic informa-
tion closes that gap to a large extent but only using
gold syntax gets our system on a par with featurama.
The question then arises whether the syntactic
features actually contribute something new to the
task, or whether the same effect could also be
achieved with linear context features alone as in fea-
turama. In order to test this we run an additional
experiment, where we add some of the syntax fea-
tures to the feature set of featurama. Specifically,
we add the static features from Table 2 that do not
use lemma or part-of-speech information. Due to the
way featurama works, we cannot use features from
the morphological tags (the dynamic features).
The results in Table 6 show that also featurama
profits from syntactic features, which corroborates
the findings from the previous experiments. We also
note again that better syntax would improve results
even more.
dev set test set
all oov all oov
featurama 94.75 84.12 94.78 84.23
pred syntax 95.18 84.65 95.09 84.52
gold syntax *95.39 84.62 *95.34 85.03
Table 6: Syntactic features for featurama (Czech). * mark
statistically significantly better models compared to feat-
urama (sentence-based t-test with ? = 0.05).
3 How Much Syntax is Needed?
Syntactic features require syntactically annotated
corpora. Without a treebank to train the parser, the
morphology cannot profit from syntactic features.9
This may be problematic for languages for which
there is no treebank, because creating a treebank is
expensive. Fortunately, it turns out that very small
amounts of syntactically annotated data are enough
9Which is of course only a problem for statistical parsers.
339
German Czech
 88
 89
 90
 91
 92
 93
 94
 0  5000  10000  15000  20000  25000  30000  35000  40000
accu
racy
 of m
orph
olog
y
# of sentences in training data of syntactic parser
devtest
 88
 89
 90
 91
 92
 93
 94
 0  5000  10000  15000  20000  25000  30000  35000  40000
accu
racy
 of m
orph
olog
y
# of sentences in training data of syntactic parser
devtest
Figure 2: Dependency between amount of training data for syntactic parser and quality of morphological prediction.
to provide a parsing quality that is sufficient for the
morphological tagger.
In order to test what amount of training data is
needed, we train several parsing models on increas-
ing amounts of syntactically annotated data. For ex-
ample, the first experiment uses the first 1,000 sen-
tences of the treebank. We perform 5-fold jackknif-
ing with the parser on these sentences to annotate
them with syntax. Then we train one parsing model
on these 1,000 sentences and use it to annotate the
rest of the training data as well as the development
and the test set. This gives us the full data set an-
notated with syntax that was learned from the first
1,000 sentences of the treebank. The morphologi-
cal tagger is then trained on the full training set and
applied to development and test set.
Figure 2 shows the dependency between the
amount of training data given to the parser and the
quality of the morphological tagger using syntac-
tic features provided by this parser. The left-most
point corresponds to a model that does not use syn-
tactic information. For both languages, German
and Czech, we find that already 1,000 sentences are
enough training data for the parser to provide useful
syntactic information to the morphological tagger.
After 5,000 sentences, both curves flatten out and
stay on the same level. We conclude that using syn-
tactic features for morphological prediction is viable
even if there is only small amounts of syntactic data
available to train the parser.
As a related experiment, we also test if we can get
the same effect with a very simple and thus much
faster parser. We use the brute-force algorithm de-
scribed in Covington (2001), which selects for each
token in the sentence another token as the head. It
does not have any tree requirements, so it is not even
guaranteed to yield a cycle-free tree structure. In Ta-
ble 7, we compare the simple parser with the mate-
parser, both trained on the first 5,000 sentences of
the treebank. Evaluation is done in terms of labeled
(LAS) and unlabeled attachment score (UAS).10
dev set test set
LAS UAS LAS UAS
Czech
simple parser (5k) 71.57 78.96 69.09 77.23
full parser (5k) 76.77 84.38 74.70 83.00
German
simple parser (5k) 83.06 85.23 78.56 81.18
full parser (5k) 87.56 90.08 83.69 86.58
Table 7: Simple parser vs full parser ? syntactic quality.
Trained on first 5,000 sentences of the training set.
As expected, the simple parser performs much
worse in terms of syntactic quality. Table 8 shows
the performance of the morphological tagger when
using the output of both parsers as syntactic fea-
tures. For Czech, both parsers seem to supply sim-
ilar information to the morphological tagger, while
for German, using the full parser is clearly better.
In both cases, the morphological tagger outperforms
the models that do not use syntactic information (cf.
Table 3). The performance on unknown words is
however much worse for both languages. We con-
clude that even with a simple parser and little train-
ing data, the morphology can make use of syntactic
information to some extent.
10LAS: correct edges with correct labelsall edges , UAS:
correct edges
all edges
340
dev set test set
all oov all oov
Czech
no syntax 92.51 73.12 92.29 72.58
simple syntax 92.96 73.45 92.53 72.66
full syntax 93.08 73.64 92.69 73.39
German
no syntax 90.92 72.52 89.11 69.67
simple syntax 91.52 73.34 89.66 70.52
full syntax 91.92 83.46 89.91 80.50
Table 8: Simple parser vs full parser ? morphological
quality. The parsing models were trained on the first
5,000 sentences of the training data, the morphological
tagger was trained on the full training set.
4 Does Better Morphology lead to Better
Parses?
In the previous sections, we show that syntactic in-
formation improves a model for predicting morphol-
ogy for Czech and German, where syntax and mor-
phology interact considerably. A natural question
then is whether the improvement also occurs in the
other direction, namely whether the improved mor-
phology also leads to better parsing models.
In the previous experiments, we run a 10-fold
jackknifing process to annotate the training data with
morphological information using no syntactic fea-
tures and afterwards use jackknifing with the parser
to annotate syntax. The syntax is subsequently used
as features for our predicted-syntax experiments.
We can apply the same process once more with the
morphology prediction in order to annotate the train-
ing data with morphological information that is pre-
dicted using the syntactic features. A parser trained
on this data will then use the improved morphology
as features. If the improved morphology has an im-
pact on the parser, the quality of the second parsing
model should then be superior to the first parsing
model, which uses the morphology predicted with-
out syntactic information. Note that for the follow-
ing experiments, neither morphology model uses the
morphological lexicon.
Table 9 presents the evaluation of the two pars-
ing models (one using morphology without syntactic
features, the other one using the improved morphol-
ogy). The results show no improvement in parsing
performance when using the improved morphology.
Looking closer at the output, we find differences be-
dev set test set
LAS UAS LAS UAS
Czech
baseline morph 81.73 88.45 81.02 87.77
morph w/ syntax 81.63 88.37 80.83 87.61
German
baseline morph 91.16 92.97 88.06 90.24
morph w/ syntax 91.20 92.97 88.15 90.34
Table 9: Impact of the improved morphology on the qual-
ity of the dependency parser for Czech and German.
tween the two parsing models with respect to gram-
matical functions that are morphologically marked.
For example, in German, performance on subjects
and accusative objects improves while performance
for dative objects and genitives decreases. This sug-
gests different strengths in the two parsing models.
However, the question how to make use of the im-
proved morphology in parsing clearly needs more
research in the future. A promising avenue may be
the approach by Hohensee and Bender (2012).
5 Related Work
Morphological taggers have been developed for
many languages. The most common approach is the
combination of a morphological lexicon with a sta-
tistical disambiguation model (Hakkani-Tu?r et al,
2002; Hajic?, 2004; Smith et al, 2005; Spoustova?
et al, 2009; Zsibrita et al, 2013).
Our work has been inspired by Versley et al
(2010), who annotate a treebank with morphologi-
cal information after the syntax had been annotated
already. The system used a finite-state morphology
to propose a set of candidate tags for each word,
which is then further restricted using hand-crafted
rules over the already available syntax tree.
Lee et al (2011) pursue the idea of jointly predict-
ing syntax and morphology, out of the motivation
that joint models should model the problem more
faithfully. They demonstrate that both sides can use
information from each other. However, their model
is computationally quite demanding and its overall
performance falls far behind the standard pipeline
approach where both tasks are done in sequence.
The problem of modeling the interaction between
morphology and syntax has recently attracted some
attention in the SPMRL workshops (Tsarfaty et al,
341
2010). Modeling morphosyntactic relations explic-
itly has been shown to improve statistical parsing
models (Tsarfaty and Sima?an, 2010; Goldberg and
Elhadad, 2010; Seeker and Kuhn, 2013), but the co-
dependency between morphology and syntax makes
it a difficult problem, and linguistic intuition is often
contradicted by the empirical findings. For example,
Marton et al (2013) show that case information is
the most helpful morphological feature for parsing
Arabic, but only if it is given as gold information,
whereas using case information from an automatic
system may even harm the performance.
Morphologically rich languages pose different
challenges for automatic systems. In this paper, we
work with European languages, where the problem
of predicting morphology can be reduced to a tag-
ging problem. In languages like Arabic, Hebrew,
or Turkish, widespread ambiguity in segmentation
of single words into meaningful morphemes adds an
additional complexity. Given a good segmentation
tool that takes care of this, our approach is appli-
cable to these languages as well. For Hebrew, this
problem has also been addressed by jointly mod-
eling segmentation, morphological prediction, and
syntax (Cohen and Smith, 2007; Goldberg and Tsar-
faty, 2008; Goldberg and Elhadad, 2013).
6 Conclusion
In this paper, we have demonstrated that using syn-
tactic information for predicting morphological in-
formation is helpful if the language shows form syn-
cretism in combination with morphosyntactic phe-
nomena like agreement. A model that uses syntactic
information is superior to a sequence model because
it leverages the syntactic dependencies that may hold
between morphologically dependent words as sug-
gested by linguistic theory. We also showed that
only small amounts of training data for a statistical
parser would be needed to improve the morphologi-
cal tagger. Making use of the improved morphology
in the dependency parser is not straight-forward and
requires more investigation in the future.
Modeling the interaction between morphology
and syntax is important for building successful pars-
ing pipelines for languages with free word order and
rich morphology. Moreover, our experiments show
that paying attention to the individual properties of a
language can help us explain and predict the behav-
ior of automatic tools. Thus, the term ?morpholog-
ically rich language? should be viewed as a broad
term that covers many different languages, whose
differences among each other may be as important as
the difference with languages with a less rich mor-
phology.
Acknowledgments
We would like to thank Jan Hajic? and Jan S?te?pa?nek
for their kind help with the Czech morphology and
featurama. We would also like to thank Thomas
Mu?ller for sharing resources and thoughts with us,
and Anders Bjo?rkelund for commenting on earlier
versions of this paper. This work was funded by
the Deutsche Forschungsgemeinschaft (DFG) via
SFB 732 ?Incremental Specification in Context?,
project D8.
References
Barry J. Blake. 2001. Case. Cambridge University
Press, Cambridge, New York, 2nd edition.
Bernd Bohnet and Joakim Nivre. 2012. A Transition-
Based System for Joint Part-of-Speech Tagging and
Labeled Non-Projective Dependency Parsing. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1455?
1465, Jeju, South Korea. Association for Computa-
tional Linguistics.
Bernd Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings of
the 23rd International Conference on Computational
Linguistics, pages 89?97, Beijing, China. International
Committee on Computational Linguistics.
Sabine Brants, Stefanie Dipper, Silvia Hansen-Shirra,
Wolfgang Lezius, and George Smith. 2002. The
TIGER treebank. In Proceedings of the 1st Workshop
on Treebanks and Linguistic Theories, pages 24?41,
Sozopol, Bulgaria.
Joan Bresnan. 2001. Lexical-Functional Syntax. Black-
well Publishers.
Xavier Carreras, Isaac Chao, Llus Padr, and Muntsa Padr.
2004. Freeling: An open-source suite of language
analyzers. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC?04), pages 239?242. European Language Re-
sources Association (ELRA).
342
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van
Genabith. 2008. Learning morphology with mor-
fette. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation
(LREC?08), pages 2362?2367, Marrakech, Morocco.
European Language Resources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.
Shay B. Cohen and Noah A. Smith. 2007. Joint morpho-
logical and syntactic disambiguation. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 208?217, Prague,
Czech Republic. Association for Computational Lin-
guistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1?8. Association for
Computational Linguistics, July.
Greville G. Corbett. 1991. Gender. Cambridge Text-
books in Linguistics. Cambridge University Press.
Greville G. Corbett. 2006. Agreement. Cambridge Text-
books in Linguistics. Cambridge University Press.
Michael A. Covington. 2001. A fundamental algorithm
for dependency parsing (with corrections). In Pro-
ceedings of the 39th Annual ACM Southeast Confer-
ence, Athens, Gorgia. ACM.
Koby Crammer, Ofer Dekel, Shai Shalev-Shwartz, and
Yoram Singer. 2003. Online passive-aggressive algo-
rithms. In Proceedings of the 16th Annual Conference
on Neural Information Processing Systems, volume 7,
pages 1217?1224, Cambridge, Massachusetts, USA.
MIT Press.
Richa?rd Farkas, Veronika Vincze, and Helmut Schmid.
2012. Dependency parsing of hungarian: Baseline re-
sults and challenges. In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 55?65, Avignon,
France. Association for Computational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. Easy first
dependency parsing of modern Hebrew. In Proceed-
ings of the NAACL HLT 2010 First Workshop on Sta-
tistical Parsing of Morphologically-Rich Languages,
pages 103?107, Los Angeles, California, USA. Asso-
ciation for Computational Linguistics.
Yoav Goldberg and Michael Elhadad. 2013. Word seg-
mentation, unknown-word resolution, and morpholog-
ical agreement in a hebrew parsing system. Computa-
tional Linguistics, 39(1):121?160.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics, pages 371?379, Columbus, Ohio. Association for
Computational Linguistics.
Jan Hajic?. 2000. Morphological Tagging: Data vs. Dic-
tionaries. In Proceedings of the 6th ANLP Conference
/ 1st NAACL Meeting, pages 94?101, Seattle, Wash-
ington. Association for Computational Linguistics.
Jan Hajic?. 2004. Disambiguation of Rich Inflection
(Computational Morphology of Czech). Nakladatel-
stv?? Karolinum, Prague, Czech Republic.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr Sgall,
Petr Pajas, Jan S?te?pa?nek, Ji?? Havelka, and Marie
Mikulova?. 2006. Prague Dependency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan Stepa?nek, Pavel Strana?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and Semantic dependen-
cies in multiple languages. In Proceedings of the
13th Conference on Computational Natural Language
Learning: Shared Task, pages 1?18, Boulder, Col-
orado, USA. Association for Computational Linguis-
tics.
Dilek Z. Hakkani-Tu?r, Kemal Oflazer, and Go?khan Tu?r.
2002. Statistical morphological disambiguation for
agglutinative languages. Computers and the Humani-
ties, 36(4):381?410.
Matt Hohensee and Emily M. Bender. 2012. Getting
more from morphology in multilingual dependency
parsing. In Proceedings of the 2012 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 315?326, Montre?al, Canada. Association
for Computational Linguistics.
John Lee, Jason Naradowsky, and David A. Smith. 2011.
A discriminative model for joint morphological disam-
biguation and dependency parsing. In Proceedings of
the 49th annual meeting of the Association for Compu-
tational Linguistics, pages 885?894, Portland, USA.
Association for Computational Linguistics.
Yuval Marton, Nizar Habash, and Owen Rambow. 2013.
Dependency parsing of modern standard arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1):161?194.
Igor Melc?uk. 2009. Dependency in linguistic descrip-
tion.
Robbert Prins. 2004. Beyond N in N-gram tagging. In
Leonoor Van Der Beek, Dmitriy Genzel, and Daniel
Midgley, editors, Proceedings of the ACL 2004 Student
Research Workshop, pages 61?66, Barcelona, Spain.
Association for Computational Linguistics.
Anne Schiller. 1994. Dmor - user?s guide. Technical
report, University of Stuttgart.
343
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics, pages 777?784, Morristown, NJ,
USA. Association for Computational Linguistics.
Wolfgang Seeker and Jonas Kuhn. 2012. Making El-
lipses Explicit in Dependency Conversion for a Ger-
man Treebank. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation, pages 3132?3139, Istanbul, Turkey. European
Language Resources Association (ELRA).
Wolfgang Seeker and Jonas Kuhn. 2013. Morphologi-
cal and syntactic case in statistical dependency pars-
ing. Computational Linguistics, 39(1):23?55.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 475?482, Vancouver, British Columbia, Canada,
October. Association for Computational Linguistics.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 763?771, Athens, Greece. Association for
Computational Linguistics.
Viktor Tro?n, Pe?ter Hala?csy, Pe?ter Rebrus, Andra?s Rung,
Pe?ter Vajda, and Eszter Simon. 2006. Morphdb.hu:
Hungarian lexical database and morphological gram-
mar. In Proceedings of the 5th International Confer-
ence on Language Resources and Evaluation, pages
1670?1673, Genoa, Italy.
Reut Tsarfaty and Khalil Sima?an. 2010. Modeling mor-
phosyntactic agreement in constituency-based parsing
of Modern Hebrew. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 40?48, Los
Angeles, California, USA. Association for Computa-
tional Linguistics.
Reut Tsarfaty, Djame? Seddah, Yoav Goldberg, Sandra
Ku?bler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing of morphologically rich languages (SPMRL):
what, how and whither. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 1?12, Los
Angeles, California, USA. Association for Computa-
tional Linguistics.
Yannick Versley, Kathrin Beck, Erhard Hinrichs, and
Heike Telljohann. 2010. A syntax-first approach to
high-quality morphological analysis and lemma dis-
ambiguation for the tba-d/z treebank. In 9th Confer-
ence on Treebanks and Linguistic Theories (TLT9),
pages 233?244.
Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgy
Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010. Hungar-
ian Dependency Treebank. In Proceedings of the 7th
Conference on International Language Resources and
Evaluation, pages 1855?1862, Valletta, Malta. Euro-
pean Language Resources Association (ELRA).
Jan Votrubec. 2006. Morphological tagging based on
averaged perceptron. In WDS?06 Proceedings of Con-
tributed Papers, pages 191?195, Praha, Czechia. Mat-
fyzpress, Charles University.
Ja?nos Zsibrita, Veronika Vincze, and Richa?rd Farkas.
2013. magyarlanc 2.0: szintaktikai elemze?s e?s felgy-
ors??tott szo?faji egye?rtelms??te?s. In Attila Tana?cs and
Veronika Vincze, editors, IX. Magyar Sza?m??to?ge?pes
Nyelve?szeti Konferencia, pages 368?374, Szeged,
Hungary.
344
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1892?1897,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Detection of Product Comparisons ? How Far Does an Out-of-the-box
Semantic Role Labeling System Take You?
Wiltrud Kessler and Jonas Kuhn
Institute for Natural Language Processing
University of Stuttgart
wiltrud.kessler@ims.uni-stuttgart.de
Abstract
This short paper presents a pilot study in-
vestigating the training of a standard Seman-
tic Role Labeling (SRL) system on product
reviews for the new task of detecting com-
parisons. An (opinionated) comparison con-
sists of a comparative ?predicate? and up to
three ?arguments?: the entity evaluated posi-
tively, the entity evaluated negatively, and the
aspect under which the comparison is made.
In user-generated product reviews, the ?predi-
cate? and ?arguments? are expressed in highly
heterogeneous ways; but since the elements
are textually annotated in existing datasets,
SRL is technically applicable. We address the
interesting question how well training an out-
of-the-box SRL model works for English data.
We observe that even without any feature en-
gineering or other major adaptions to our task,
the system outperforms a reasonable heuristic
baseline in all steps (predicate identification,
argument identification and argument classifi-
cation) and in three different datasets.
1 Introduction
Sentiment analysis deals with the task of determin-
ing the polarity of an opinionated document or a
sentence, in product reviews typically with regard
to some target product. A common way to express
sentiment about some product is by comparing it to a
different product. In the corpus data we use, around
10% of sentences contain at least one comparison.
Here are some examples of comparison sentences
from our corpus:
(1) a. ?[This camera]E+ . . . its [screen]A is much big-
ger than the [400D].?
b. ?[D70]E+ beats [EOS 300D]E? in almost [ev-
ery category]A, EXCEPT ONE.?
c. ?[Noise suppression]A1A2 was generally
better1 than the [D80]E?1 ?s and much better2
than the [Rebel]E?2 ?s.?
d. ?A striking difference between the [EOS
350D]E? and the new [EOS 400D]E+ concerns
the [image sensor]A.?
Note that our definition of comparisons is broader
than the linguistic category of comparative sen-
tences, which only includes sentences that contain
a comparative adjective or adverb. For our work,
we consider comparisons expressed by any Part of
Speech (POS).
A comparison contains several parts that must be
identified in order to get meaningful information.
We call the word or phrase that is used to express the
comparison (?better?, ?beats?, . . . ) a comparative
predicate. A comparison involves two entities, one
or both of them may be implicit. In our data, most
of the entities are products, e.g., the two cameras
?D70? and ?EOS 300D? in sentence 1b. In graded
comparisons, entity+ (E+) is the entity that is being
evaluated positively, entity- (E-) the entity evaluated
negatively. In many sentences one attribute or part
of a product is being compared, like ?image sensor?
in sentence 1d. We call this the aspect (A).
The task we want to solve for a given compari-
son sentence is to detect the comparative predicate,
the entities that are involved and the aspect that is
being compared. We borrow our methodology from
Semantic Role Labeling (SRL). In SRL, events are
expressed by predicates and participants of these
events are expressed by arguments that fill differ-
ent semantic roles. Adapted to the problem of de-
tecting comparisons, the events we are interested in
are comparative predicates and the arguments are the
two entities and the aspect that is being compared.
Due to the diversity of possible ways of express-
ing comparisons, the ?predicates? and ?arguments?
1892
in this task are more heterogeneous categories than
in standard SRL based on PropBank and Nom-
Bank annotations. Moreoever, the existing labeled
datasets are based on an annotation methodology
which gave the annotators a lot of freedom in de-
ciding on the linguistic anchoring of the ?predicate?
and ?arguments?. This adds to the heterogeneity of
the observed constructions and makes it even more
interesting to ask the question how far an out-of-the-
box SRL model can take you.
In this work, we re-train an existing SRL system
(Bjo?rkelund et al, 2009) on product review data la-
beled with comparative predicates and arguments.
We show that we can get reasonable results with-
out any feature engineering or other major adap-
tions. This is an encouraging result for a linguis-
tically grounded modeling approach to comparison
detection.
2 Related Work
The syntax and semantics of comparative sentences
have been the topic of research in linguistics for a
long time (Moltmann, 1992; Kennedy, 1999). How-
ever, our focus is on computational methods and we
also treat comparisons that are not comparative sen-
tences in a linguistic sense.
In sentiment analysis, some studies have been pre-
sented to identify comparison sentences. Jindal and
Liu (2006a) report good results on English using
class sequential rules based on keywords as features
for a Naive Bayes classifier. A similar approach for
Korean is presented by Yang and Ko (2009; 2011b;
2011a). In our work, we do not address the task of
identifying comparison sentences, we assume that
we are given a set of such sentences.
The step we are concerned with is the detection of
relevant parts of a comparison. To identify entities
and aspect, Jindal and Liu (2006b) use an involved
pattern mining process to mine label sequential rules
from annotated English sentences. A similar ap-
proach is again presented by Yang and Ko (2011a)
for Korean. In contrast to their complicated process-
ing, we simply use an existing SRL system out of
the box. Both approaches consider only nouns and
pronouns for entities and aspects, we use all POS
and allow for multi-word arguments. Jindal and Liu
(2006b) base the recognition of comparative predi-
cates on a list of manually compiled keywords. We
use this as our baseline. Our approach is not de-
pendent on a set of keywords and is therefore more
easily adaptable to a new domain.
All works label the entities according to their po-
sition with respect to the predicate. This requires the
identification of the preferred entity in a non-equal
comparison as an additional step. Ganapathibhotla
and Liu (2008) use hand-crafted rules based on the
polarity of the predicate for this task. As we label
the entities with their roles from the start, we solve
both problems at the same time.
Xu et al (2011) cast the task as a relation extrac-
tion problem. They present an approach that uses
conditional random fields to extract relations (bet-
ter, worse, same and no comparison) between two
entitites, an attribute and a predicate phrase.
The approach of Hou and Li (2008) is most re-
lated to our approach. They use SRL with standard
SRL features to extract comparative relations from
Chinese sentences. We confirm that SRL is a vi-
able method also for English. In their experiments
they report good results on gold parses, but observe
a drop in performance when they use their method
on automatic parses. All our experiments are con-
ducted on automatically obtained parses.
3 Approach
The input to our system is a sentence that we assume
to contain at least one comparison. The result of our
processing are one or more comparative predicates
and for each predicate three arguments: The two en-
tities that are being compared, and the aspect they
are compared in. More formally speaking, for ev-
ery sentence we expect to get one or more 4-tupels
(predicate, entity+, entity-, aspect). Entity+ is the
entity that is being evaluated as better than entity-.
Any of the arguments may be empty. Currently, we
treat only single words as comparative predicates.
Annotated multi-word predicates are mapped to one
word. We allow for multi-word arguments, but an-
notate only the head word of the phrase and treat it
as a one word argument for evaluation. We do not
place any restrictions on possible POS.
We use a standard pipeline approach from SRL.
As a first step, the comparative predicate is iden-
tified. The next step in SRL would be predicate
1893
disambiguation to identify the different frames this
predicate can express. As we do not have such
frame information, predicate disambiguation is not
performed in our pipeline.
After we have identified the predicates, the next
step is to identify their arguments. The identifica-
tion step is a binary classification whether a word in
the sentence is some argument of the identified pred-
icate. As a final classification step, it is determined
for each found argument whether this argument is
entity+, entity- or the aspect.
We use an existing SRL system (Bjo?rkelund et al,
2009)1 and the features developed for SRL, based on
the output of the MATE dependency parser (Bohnet,
2010). Features use attributes of the predicate itself,
its head or its dependents. Additionally, for argu-
ment identification and classification there are fea-
tures that describe the relation of predicate and argu-
ment, the argument itself, its leftmost and rightmost
dependent and left and right sibling.
For the classification tasks of the pipeline, the
SRL system uses regularized linear logistic regres-
sion from the LIBLINEAR package (Fan et al,
2008). We set the SRL system to train separate clas-
sifiers for predicates of different POS. In preliminary
experiments, we have found this to perform slightly
better than training one classifier for all kinds of
predicates, although the difference is not significant.
We do not use the reranker.
4 Experiments
Data. We use the JDPA corpus2 by J. Kessler et al
(2010) for our experiments. It contains blog posts
about cameras and cars. We use the annotation class
?Comparison? that has four annotation slots. We
convert the ?more? slot to entity+, the ?less? slot to
entity- and the ?dimension? slot to the aspect. For
now, we ignore the ?same? slot which indicates if
the two mentions are ranked as equal.
We have also tested our approach on the dataset
used in (Jindal and Liu, 2006b)3. We use all com-
1http://code.google.com/p/mate-tools/
2Available from http://verbs.colorado.edu/
jdpacorpus/ ? we ignore cars batch 009 where no
arguments of comparative predicates are annotated.
3Available from http://www.cs.uic.edu/?liub/
FBS/data.tar.gz ? although the original paper works on
some unknown subset of this data, so our results are not directly
JDPA J&L
cameras cars
all sentences 5230 14003 7986
comparison sentences 505 1094 649
predicates 642 1327 695
distinct predicates 147 252 122
preds. occurring once 87 147 61
Entity+ / 1 517 1091 657
Entity- / 2 511 1068 331
Aspect 623 1107 526
Table 1: Statistics about the datasets
parisons annotated as types 1 to 3 (ignoring type 4,
non-gradable comparisons). In this dataset (J&L),
entities are annotated as entity 1 or entity 2 depend-
ing on their position before or after the predicate.
We keep this annotation and train our system to as-
sign these labels.
We do sentence segmentation and tokenization
with the Stanford Core NLP4. Annotations are
mapped to the extracted tokens. We ignore anno-
tations that do not correspond to complete tokens.
In the JDPA corpus, if an annotated argument is out-
side the current sentence, we follow the coreference
chain to find a coreferent annotation in the same sen-
tence. If this is not successful, the argument is ig-
nored. We extract all sentences where we found at
least one comparative predicate as our dataset.
Table 1 shows some statistics of the data.
Evaluation Setup. We evaluate on each dataset
separately using 5-fold cross-validation. We report
precision (P), recall (R), F1-measure (F1), and for
argument classification macro averaged F1-measure
(F1m) over the three arguments. Bold numbers de-
note the best result in each column and dataset. We
mark a F1-measure result with * if it is significantly
higher than all previous lines.5
Results on Predicates. We have implemented
two baselines based on previous work. The sim-
plest baseline, BL POS classifies all tokens with
a comparative POS (?JJR?, ?JJS?, ?RBR?, ?RBS?)
as predicates. A more sophisticated baseline, BL
Keyphrases, uses a list of about 80 manually com-
comparable to the results reported there.
4http://nlp.stanford.edu/software/
corenlp.shtml
5Statistically significant at p < .05 using the approximate
randomization test (Noreen, 1989) with 10000 iterations.
1894
P R F1
ca
m
s BL POS 66.6 38.2 48.5
BL Keyphrases 53.1 62.8 57.5?
SRL 73.8 58.7 65.4?
ca
rs
BL POS 62.5 34.7 44.6
BL Keyphrases 51.9 56.5 54.1?
SRL 73.2 55.5 63.2?
J&
L
BL POS 74.3 52.9 61.8
BL Keyphrases 61.5 80.0 69.5?
SRL 77.0 68.1 72.3?
Table 2: Results predicate identification
P R F1
ca
m
s BL 49.4 47.1 48.2
SRL 66.5 38.0 48.4
ca
rs BL 50.2 50.1 50.1
SRL 68.7 42.2 52.3?
J&
L BL 38.7 44.6 41.5
SRL 68.5 45.2 54.5?
Table 3: Results argument identification (gold predicates)
Entity+ / 1 Entity- / 2 Aspect F1m
P R F1 P R F1 P R F1
ca
m
s BL 30.1 31.7 30.9 21.2 21.3 21.3 61.8 51.2 56.0 36.1
SRL 38.6 17.4 24.0 43.7 24.5 31.4 69.9 47.7 56.7 37.3
ca
rs BL 31.1 32.7 31.9 23.0 24.0 23.5 49.3 44.5 46.8 34.0
SRL 39.5 22.9 29.0 48.1 31.0 37.7 58.4 36.2 44.7 37.1?
J&
L BL 43.2 39.4 41.2 19.0 31.1 23.6 15.0 17.1 16.0 26.9
SRL 58.3 47.2 52.1 60.8 35.6 45.0 58.8 30.6 40.3 45.8?
Table 4: Results argument classification (gold predicates)
piled comparative keyphrases from (Jindal and Liu,
2006a) in addition to the POS tags.
Table 2 shows the result of our experiments. Our
method significantly outperforms both baselines in
all datasets. The generally low recall values are
mainly a result of the wide variety of predicates that
are used to express comparisons (see Discussion).
Results on Arguments. To get results indepent of
the errors introduced by the relatively low perfor-
mance on predicate identification, we use annotated
predicates (gold predicates) as a starting point for
the argument experiments. All results drop about
10% when system predicates are used.
As a baseline (BL) for argument identification
and classification, we use some heuristics based on
the characteristics of our data. Most entities are
(pro)nouns and most predicates are positive, so we
classify the first noun or pronoun before the predi-
cate as entity+ (entity 1 for J&L) and the first noun
or pronoun after the predicate as a entity- (entity 2).
If the predicate is a comparative adjective, we clas-
sify the predicate itself as aspect, because this type
of annotation is very frequent in the JDPA data. For
other predicates except nouns and verbs, we classify
the direct head of the predicate as aspect.
Table 3 shows the results for argument identifica-
tion, the results for argument classification can be
seen in Table 4. Our system outperforms the base-
line for all datasets. The differences are significant
except for the cameras dataset. In general, the num-
bers are low. We will discuss some reasons for this
in the next section.
5 Discussion
Sparseness. There are many ways to express a
comparison and the size of the available training
data is relatively small. This strongly influences the
recall of our system as many predicates and argu-
ments occur only once. As we can see in Table 1,
60% of the predicates in the cameras dataset occur
only once. In contrast, only 12 predicates occur ten
times or more. The trends are similar in the other
datasets. This particularily affects verbs and nouns,
where many colloquial expressions are used (?ham-
mers?, ?pwns?, ?go head to head with?, ?put X to
the sword?, . . . ).
Argument identification and classification would
benefit from generalizing over the many different
product identifiers like ?EOS 5D? or ?D200?. We
want to try to use a Named Entity Recognition sys-
tem trained on this type of entities for this purpose.
1895
Sentiment Relevance. The following examples
show a problem that is typical for sentiment analysis
and responsible for many false positive predicates:
(2) a. ?Relatively [lower]A noise at higher ISO . . . ?
b. ?. . . but [higher]A then [Sony]E+?
Although ?higher? often expresses a comparison
like in sentence 2b, in sentence 2a it only describes
a camera setting and should not be extracted as a
comparative predicate. There has been considerable
work in the areas of subjectivity classification (Wil-
son and Wiebe, 2003) and the related sentiment rel-
evance (Scheible and Schu?tze, 2013) which we will
try to use to detect such irrelevant, ?descriptive? uses
of comparative words.
Linguistic anchoring. In contrast to SRL, the task
of comparison detection in reviews is a relatively
new task without universally recognized definitions
and annotation schemes. The annotators of the cor-
pora had a lot of freedom in their choice of linguis-
tic anchoring of the predicates and arguments. Con-
sider these examples from the cameras dataset:
(3) a. ?[Lighter]A in weight compared to the
[others]E?.?
b. ?. . . [its]E+ [better]A and faster compared vs
the [SB800 flash]E? as well.?
c. ?. . . this camera?s [screen]E+ is [smaller]A than
the [ones]E? on some competing models . . . ?
Sentences 3a and 3b show a situation where two
words are used to express the same comparison and
it is unclear which one to chose as a predicate. The
decision is left to the individual annotators.
There is some variety of annotations on arguments
as well. In the JDPA data, a comparative adjective
is often annotated as aspect, sometimes even when
there is an alternative, e.g., ?weight? in sentence 3a.
Also, for a phrase like ?its screen?, we find ?screen?
annotated as the aspect (sentence 1a) or an entity
(sentence 3c) ? and both have their merit. We want
to further study how different linguistic anchorings
of comparisons effect classification performance.
Equative comparisons. As we can see from the
confusion matrix of our system, the distinction be-
tween entity+ and entity- is very difficult to learn.
In graded comparisons, the distinction is informa-
tive, but sentiment information would be needed for
the correct assignment. There are also some prob-
lematic cases where the ranking cannot be inferred
without the broader context, e.g., sentence 1d.
A more annotation-related problem concerns
equative comparisons, i.e., both entities are rated as
equal. The difference between entity+ and entity- is
meaningless in this case. In the JDPA corpus, en-
tities still have to be annotated as either entity+ or
entity- and the annotation guidelines allow the anno-
tator to choose freely. As a result, the data is noisy,
for the same predicate sometimes entity- is before
the predicate, sometimes entity+. If we eliminate
this noise by always assigning the entities in order
of surface position, we see a gain in macro averaged
F1-measure for all systems of about 2% (cameras)
to 4% (cars).
6 Conclusions
We presented a pilot experiment on using an SRL-
inspired approach to detect comparisons (compara-
tive predicate, entity+, entity-, aspect) in user gener-
ated content. We re-trained an existing SRL system
on data that is labeled with comparative predicates
and arguments. Even without feature engineering or
major adaptions, our approach outperforms the base-
lines in three datasets in every task. This is an en-
couraging result for a linguistically grounded mod-
eling approach to comparison detection.
For future work, we plan to include features that
have been tailored specifically to the task of detect-
ing product comparisons. To address the inherent di-
versity of expressions typical for user generated con-
tent, we want to employ generalization techniques,
e.g., to detect product names. We also want to fur-
ther study the different possible linguistic anchor-
ings of comparisons and their effect on classification
performance. Studies of this kind may also inform
future data annotation efforts in that certain ways
of anchoring the elements of a comparison linguis-
tically may be more helpful than others. We also
believe that the explicit modeling of different types
(equative, superlative, non-equal gradable) of com-
parisons will have a positive effect on performance.
Acknowledgments
The work reported in this paper was supported by a
Nuance Foundation Grant.
1896
References
Anders Bjo?rkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual Semantic Role Labeling. In Pro-
ceedings of CoNLL ?09 Shared Task, pages 43?48.
Bernd Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings of
COLING ?10, pages 89?97.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. J. Mach. Learn. Res.,
9:1871?1874, June.
Murthy Ganapathibhotla and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings of
COLING ?08, pages 241?248.
Feng Hou and Guo-hui Li. 2008. Mining Chinese com-
parative sentences by semantic role labeling. In Pro-
ceedings of ICMLC ?08, pages 2563?2568.
Nitin Jindal and Bing Liu. 2006a. Identifying compar-
ative sentences in text documents. In Proceedings of
SIGIR ?06, pages 244?251.
Nitin Jindal and Bing Liu. 2006b. Mining comparative
sentences and relations. In Proceedings of AAAI ?06,
pages 1331?1336.
Christopher Kennedy. 1999. Projecting the Adjective:
The Syntax and Semantics of Gradability and Compar-
ison. Outstanding Dissertations in Linguistics. Gar-
land Pub.
Jason S. Kessler, Miriam Eckert, Lyndsay Clark, and
Nicolas Nicolov. 2010. The 2010 ICWSM JDPA Sen-
timent Corpus for the Automotive Domain. In Pro-
ceedings of ICWSM-DWC ?10.
Friederike Moltmann. 1992. Coordination and Compar-
atives. Ph.D. thesis, Massachusetts Institute of Tech-
nology.
Eric W. Noreen. 1989. Computer-intensive methods for
testing hypotheses ? an introduction. Wiley & Sons.
Christian Scheible and Hinrich Schu?tze. 2013. Senti-
ment relevance. In Proceedings of ACL ?13, pages
954?963.
Theresa Wilson and Janyce Wiebe. 2003. Annotating
opinions in the world press. In Proceedings of SIGdial
?03, pages 13?22.
Kaiquan Xu, Stephen Shaoyi Liao, Jiexun Li, and Yuxia
Song. 2011. Mining comparative opinions from cus-
tomer reviews for competitive intelligence. Decis.
Support Syst., 50(4):743?754, March.
Seon Yang and Youngjoong Ko. 2009. Extracting com-
parative sentences from Korean text documents us-
ing comparative lexical patterns and machine learning
techniques. In Proceedings of the ACL-IJCNLP ?09,
pages 153?156.
Seon Yang and Youngjoong Ko. 2011a. Extracting com-
parative entities and predicates from texts using com-
parative type classification. In Proceedings of HLT
?11, pages 1636?1644.
Seon Yang and Youngjoong Ko. 2011b. Finding relevant
features for Korean comparative sentence extraction.
Pattern Recogn. Lett., 32(2):293?296, January.
1897
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 77?87,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
The Best of Both Worlds ? A Graph-based Completion Model for
Transition-based Parsers
Bernd Bohnet and Jonas Kuhn
University of Stuttgart
Institute for Natural Language Processing
{bohnet,jonas}@ims.uni-stuttgart.de
Abstract
Transition-based dependency parsers are
often forced to make attachment deci-
sions at a point when only partial infor-
mation about the relevant graph configu-
ration is available. In this paper, we de-
scribe a model that takes into account com-
plete structures as they become available
to rescore the elements of a beam, com-
bining the advantages of transition-based
and graph-based approaches. We also pro-
pose an efficient implementation that al-
lows for the use of sophisticated features
and show that the completion model leads
to a substantial increase in accuracy. We
apply the new transition-based parser on ty-
pologically different languages such as En-
glish, Chinese, Czech, and German and re-
port competitive labeled and unlabeled at-
tachment scores.
1 Introduction
Background. A considerable amount of recent
research has gone into data-driven dependency
parsing, and interestingly throughout the continu-
ous process of improvements, two classes of pars-
ing algorithms have stayed at the centre of at-
tention, the transition-based (Nivre, 2003) vs. the
graph-based approach (Eisner, 1996; McDonald
et al 2005).1 The two approaches apply funda-
mentally different strategies to solve the task of
finding the optimal labeled dependency tree over
the words of an input sentence (where supervised
machine learning is used to estimate the scoring
parameters on a treebank).
The transition-based approach is based on the
conceptually (and cognitively) compelling idea
1More references will be provided in sec. 2.
that machine learning, i.e., a model of linguis-
tic experience, is used in exactly those situations
when there is an attachment choice in an other-
wise deterministic incremental left-to-right pars-
ing process. As a new word is processed, the
parser has to decide on one out of a small num-
ber of possible transitions (adding a dependency
arc pointing to the left or right and/or pushing or
popping a word on/from a stack representation).
Obviously, the learning can be based on the fea-
ture information available at a particular snapshot
in incremental processing, i.e., only surface in-
formation for the unparsed material to the right,
but full structural information for the parts of the
string already processed. For the completely pro-
cessed parts, there are no principled limitations as
regards the types of structural configurations that
can be checked in feature functions.
The graph-based approach in contrast empha-
sizes the objective of exhaustive search over all
possible trees spanning the input words. Com-
monly, dynamic programming techniques are
used to decide on the optimal tree for each par-
ticular word span, considering all candidate splits
into subspans, successively building longer spans
in a bottom-up fashion (similar to chart-based
constituent parsing). Machine learning drives
the process of deciding among alternative can-
didate splits, i.e., feature information can draw
on full structural information for the entire ma-
terial in the span under consideration. However,
due to the dynamic programming approach, the
features cannot use arbitrarily complex structural
configurations: otherwise the dynamic program-
ming chart would have to be split into exponen-
tially many special states. The typical feature
models are based on combinations of edges (so-
77
called second-order factors) that closely follow
the bottom-up combination of subspans in the
parsing algorithm, i.e., the feature functions de-
pend on the presence of two specific dependency
edges. Configurations not directly supported by
the bottom-up building of larger spans are more
cumbersome to integrate into the model (since the
combination algorithm has to be adjusted), in par-
ticular for third-order factors or higher.
Empirically, i.e., when applied in supervised
machine learning experiments based on existing
treebanks for various languages, both strategies
(and further refinements of them not mentioned
here) turn out roughly equal in their capability
of picking up most of the relevant patterns well;
some subtle strengths and weaknesses are com-
plementary, such that stacking of two parsers rep-
resenting both strategies yields the best results
(Nivre and McDonald, 2008): in training and ap-
plication, one of the parsers is run on each sen-
tence prior to the other, providing additional fea-
ture information for the other parser. Another suc-
cessful technique to combine parsers is voting as
carried out by Sagae and Lavie (2006).
The present paper addresses the question if
and how a more integrated combination of the
strengths of the two strategies can be achieved
and implemented efficiently to warrant competi-
tive results.
The main issue and solution strategy. In or-
der to preserve the conceptual (and complexity)
advantages of the transition-based strategy, the
integrated algorithm we are looking for has to
be transition-based at the top level. The advan-
tages of the graph-based approach ? a more glob-
ally informed basis for the decision among dif-
ferent attachment options ? have to be included
as part of the scoring procedure. As a prerequi-
site, our algorithm will require a memory for stor-
ing alternative analyses among which to choose.
This has been previously introduced in transition-
based approaches in the form of a beam (Johans-
son and Nugues, 2006): rather than representing
only the best-scoring history of transitions, the k
best-scoring alternative histories are kept around.
As we will indicate in the following, the mere
addition of beam search does not help overcome
a representational key issue of transition-based
parsing: in many situations, a transition-based
parser is forced to make an attachment decision
for a given input word at a point where no or only
partial information about the word?s own depen-
dents (and further decendents) is available. Fig-
ure 1 illustrates such a case.
Figure 1: The left set of brackets indicates material
that has been processed or is under consideration; on
the right is the input, still to be processed. Access to in-
formation that is yet unavailable would help the parser
to decide on the correct transition.
Here, the parser has to decide whether to create an
edge between house and with or between bought
and with (which is technically achieved by first
popping house from the stack and then adding the
edge). At this time, no information about the ob-
ject of with is available; with fails to provide what
we call a complete factor for the calculation of the
scores of the alternative transitions under consid-
eration. In other words, the model cannot make
use of any evidence to distinguish between the
two examples in Figure 1, and it is bound to get
one of the two cases wrong.
Figure 2 illustrates the same case from the per-
spective of a graph-based parser.
Figure 2: A second order model as used in graph-based
parsers has access to the crucial information to build
the correct tree. In this case, the parser condsiders the
word friend (as opposed to garden, for instance) as it
introduces the bold-face edge.
Here, the combination of subspans is performed
at a point when their internal structure has been
finalized, i.e., the attachment of with (to bought
or house) is not decided until it is clear that friend
is the object of with; hence, the semantically im-
portant lexicalization of with?s object informs the
higher-level attachment decision through a so-
called second order factor in the feature model.
78
Given a suitable amount of training data, the
model can thus learn to make the correct deci-
sion. The dynamic-programming based graph-
based parser is designed in such a way that any
score calculation is based on complete factors for
the subspans that are combined at this point.
Note that the problem for the transition-based
parser cannot be remedied by beam search alone.
If we were to keep the two options for attach-
ing with around in a beam (say, with a slightly
higher score for attachment to house, but with
bought following narrowly behind), there would
be no point in the further processing of the sen-
tence at which the choice could be corrected: the
transition-based parser still needs to make the de-
cision that friend is attached to with, but this will
not lead the parser to reconsider the decision made
earlier on.
The strategy we describe in this paper applies
in this very type of situation: whenever infor-
mation is added in the transition-based parsing
process, the scores of all the histories stored in
the beam are recalculated based on a scoring
model inspired by the graph-based parsing ap-
proach, i.e., taking complete factors into account
as they become incrementally available. As a con-
sequence the beam is reordered, and hence, the
incorrect preference of an attachment of with to
house (based on incomplete factors) can later be
corrected as friend is processed and the complete
second-order factor becomes available.2
The integrated transition-based parsing strategy
has a number of advantages:
(1) We can integrate and investigate a number of
third order factors, without the need to implement
a more complex parsing model each time anew to
explore the properties of such distinct model.
(2) The parser with completion model main-
tains the favorable complexity of transition-based
parsers.
(3) The completion model compensates for the
lower accuracy of cases when only incomplete in-
formation is available.
(4) The parser combines the two leading pars-
ing paradigms in a single efficient parser with-
out stacking the two approaches. Therefore the
2Since search is not exhaustive, there is of course a slight
danger that the correct history drops out of the beam before
complete information becomes available. But as our experi-
ments show, this does not seem to be a serious issue empiri-
cally.
parser requires only one training phase (without
jackknifing) and it uses only a single transition-
based decoder.
The structure of this paper is as follows. In Sec-
tion 2, we discuss related work. In Section 3, we
introduce our transition-based parser and in Sec-
tion 4 the completion model as well as the im-
plementation of third order models. In Section 5,
we describe experiments and provide evaluation
results on selected data sets.
2 Related Work
Kudo and Matsumoto (2002) and Yamada and
Matsumoto (2003) carried over the idea for de-
terministic parsing by chunks from Abney (1991)
to dependency parsing. Nivre (2003) describes
in a more strict sense the first incremental parser
that tries to find the most appropriate dependency
tree by a sequence of local transitions. In order
to optimize the results towards a more globally
optimal solution, Johansson and Nugues (2006)
first applied beam search, which leads to a sub-
stantial improvment of the results (cf. also (Titov
and Henderson, 2007)). Zhang and Clark (2008)
augment the beam-search algorithm, adapting the
early update strategy of Collins and Roark (2004)
to dependency parsing. In this approach, the
parser stops and updates the model when the or-
acle transition sequence drops out of the beam.
In contrast to most other approaches, the training
procedure of Zhang and Clark (2008) takes the
complete transition sequence into account as it is
calculating the update. Zhang and Clark compare
aspects of transition-based and graph-based pars-
ing, and end up using a transition-based parser
with a combined transition-based/second-order
graph-based scoring model (Zhang and Clark,
2008, 567), which is similar to the approach we
describe in this paper. However, their approach
does not involve beam rescoring as the partial
structures built by the transition-based parser are
subsequently augmented; hence, there are cases in
which our approach is able to differentiate based
on higher-order factors that go unnoticed by the
combined model of (Zhang and Clark, 2008, 567).
One step beyond the use of a beam is a dynamic
programming approach to carry out a full search
in the state space, cf. (Huang and Sagae, 2010;
Kuhlmann et al 2011). However, in this case
one has to restrict the employed features to a set
which fits to the elements composed by the dy-
79
namic programming approach. This is a trade-off
between an exhaustive search and a unrestricted
(rich) feature set and the question which provides
a higher accuracy is still an open research ques-
tion, cf. (Kuhlmann et al 2011).
Parsing of non-projective dependency trees is
an important feature for many languages. At
first most algorithms were restricted to projec-
tive dependency trees and used pseudo-projective
parsing (Kahane et al 1998; Nivre and Nilsson,
2005). Later, additional transitions were intro-
duced to handle non-projectivity (Attardi, 2006;
Nivre, 2009). The most common strategy uses
the swap transition (Nivre, 2009; Nivre et al
2009), an alternative solution uses two planes
and a switch transition to switch between the two
planes (Go?mez-Rodr??guez and Nivre, 2010).
Since we use the scoring model of a graph-
based parser, we briefly review releated work
on graph-based parsing. The most well known
graph-based parser is the MST (maximum span-
ning tree) parser, cf. (McDonald et al 2005; Mc-
Donald and Pereira, 2006). The idea of the MST
parser is to find the highest scoring tree in a graph
that contains all possible edges. Eisner (1996)
introduced a dynamic programming algorithm to
solve this problem efficiently. Carreras (2007) in-
troduced the left-most and right-most grandchild
as factors. We use the factor model of Carreras
(2007) as starting point for our experiments, cf.
Section 4. We extend Carreras (2007) graph-
based model with factors involving three edges
similar to that of Koo and Collins (2010).
3 Transition-based Parser with a Beam
This section specifies the transition-based beam-
search parser underlying the combined approach
more formally. Sec. 4 will discuss the graph-
based scoring model that we are adding.
The input to the parser is a word string x,
the goal is to find the optimal set y of labeled
edges xi?l xj forming a dependency tree over x
?{root}. We characterize the state of a transition-
based parser as pii=??i, ?i, yi, hi?, pii ? ?, the set
of possible states. ?i is a stack of words from x
that are still under consideration; ?i is the input
buffer, the suffix of x yet to be processed; yi the
set of labeled edges already assigned (a partial la-
beled dependency tree); hi is a sequence record-
ing the history of transitions (from the set of op-
erations ? = {shift, left-arcl, right-arcl, reduce,
swap}) taken up to this point.
(1) The initial state pi0 has an empty stack, the
input buffer is the full input string x, and the edge
set is empty. (2) The (partial) transition function
?(pii, t) : ? x ? ? ? maps a state and an opera-
tion t to a new state pii+1. (3) Final states pif are
characterized by an empty input buffer and stack;
no further transitions can be taken.
The transition function is informally defined as
follows: The shift transition removes the first ele-
ment of the input buffer and pushes it to the stack.
The left-arcl transition adds an edge with label l
from the first word in the buffer to the word on
top of the stack, removes the top element from
the stack and pushes the first element of the input
buffer to the stack.
The right-arcl transition adds an edge from word
on top of the stack to the first word in the input
buffer and removes the top element of the input
buffer and pushes that element onto the stack.
The reduce transition pops the top word from the
stack.
The swap changes the order of the two top el-
ements on the stack (possibly generating non-
projkective trees).
When more than one operation is applicable, a
scoring function assigns a numerical value (based
on a feature vector and a weight vector trained
by supervised machine learning) to each possi-
ble continuation. When using a beam search ap-
proach with beam size k, the highest-scoring k al-
ternative states with the same length n of transi-
tion history h are kept in a set ?beamn?.
In the beam-based parsing algorithm (cf. the
pseudo code in Algorithm 1), all candidate states
for the next set ?beamn+1? are determined using
the transition function ? , but based on the scor-
ing function, only the best k are preserved. (Fi-
nal) states to which no more transitions apply are
copied to the next state set. This means that once
all transition paths have reached a final state, the
overall best-scoring states can be read off the fi-
nal ?beamn?. The y of the top-scoring state is the
predicted parse.
Under the plain transition-based scoring
regime scoreT , the score for a state pi is the sum
of the ?local? scores for the transitions ti in the
state?s history sequence:
scoreT (pi) =
?|h|
i=0 w ? f(pii, ti)
80
Algorithm 1: Transition-based parser
// x is the input sentence, k is the beam size
?0 = ?, ?0 = x, y0 = ?, h = ?
pi0 ? ??0, ?0, y0, h0? // initial parts of a state
beam0? {pi0} // create initial state
n? 0 // iteration
repeat
n? n+ 1
for all pij ? beamn?1 do
transitions? possible-applicable-transition (pij)
// if no transition is applicable keep state pij :
if transitions = ? then beamn ? beamn ? {pij}
else for all ti ? transitions do
// apply the transition i to state j
pi ? ?(pij , ti)
beamn ? beamn ? {pi}
// end for
// end for
sort beamn due to the score(pij)
beamn ? sublist (beamn, 0, k)
until beamn?1 = beamn // beam changed?
w is the weight vector. Note that the features
f(pii, ti) can take into account all structural and
labeling information available prior to taking tran-
sition ti, i.e., the graph built so far, the words (and
their part of speech etc.) on the stack and in the
input buffer, etc. But if a larger graph configu-
ration involving the next word evolves only later,
as in Figure 1, this information is not taken into
account in scoring. For instance, if the feature
extraction uses the subcategorization frame of a
word under consideration to compute a score, it is
quite possible that some dependents are still miss-
ing and will only be attached in a future transition.
4 Completion Model
We define an augmented scoring function which
can be used in the same beam-search algorithm in
order to ensure that in the scoring of alternative
transition paths, larger configurations can be ex-
ploited as they are completed in the incremental
process. The feature configurations can be largely
taken from graph-based approaches. Here, spans
from the string are assembled in a bottom-up fash-
ion, and the scoring for an edge can be based on
structurally completed subspans (?factors?).
Our completion model for scoring a state pin
incorporates factors for all configurations (match-
ing the extraction scheme that is applied) that are
present in the partial dependency graph yn built
up to this point, which is continuously augmented.
This means if at a given point n in the transition
path, complete information for a particular config-
uration (e.g., a third-order factor involving a head,
its dependent and its grand-child dependent) is
unavailable, scoring will ignore this factor at time
n, but the configuration will inform the scoring
later on, maybe at point n+ 4, when the complete
information for this factor has entered the partial
graph yn+4.
We present results for a number of different
second-order and third-order feature models.
Second Order Factors. We start with the
model introduced by Carreras (2007). Figure 3
illustrates the factors used.
Figure 3: Model 2a. Second order factors of Carreras
(2007). We omit the right-headed cases, which are
mirror images. The model comprises a factoring into
one first order part and three second order factors (2-
4): 1) The head (h) and the dependent (c); 2) the head,
the dependent and the left-most (or right-most) grand-
child in between (cmi); 3) the head, the dependent and
the right-most (or left-most) grandchild away from the
head (cmo). 4) the head, the dependent and between
those words the right-most (or left-most) sibling (ci).
Figure 4: 2b. The left-most dependent of the head or
the right-most dependent in the right-headed case.
Figure 4 illustrates a new type of factor we use,
which includes the left-most dependent in the left-
headed case and symmetricaly the right-most sib-
ling in the right-head case.
Third Order Factors. In addition to the second
order factors, we investigate combinations of third
order factors. Figure 5 and 6 illustrate the third
order factors, which are similar to the factors of
Koo and Collins (2010). They restrict the factor
to the innermost sibling pair for the tri-siblings
81
and the outermost pair for the grand-siblings. We
use the first two siblings of the dependent from
the left side of the head for the tri-siblings and
the first two dependents of the child for the grand-
siblings. With these factors, we aim to capture
non-projective edges and subcategorization infor-
mation. Figure 7 illustrates a factor of a sequence
of four nodes. All the right headed variants are
symmetrically and left out for brevity.
Figure 5: 3a. The first two children of the head, which
do not include the edge between the head and the de-
pendent.
Figure 6: 3b. The first two children of the dependent.
Figure 7: 3c. The right-most dependent of the right-
most dependent.
Integrated approach. To obtain an integrated
system for the various feature models, the scoring
function of the transition-based parser from Sec-
tion 3 is augmented by a family of scoring func-
tions scoreGm for the completion model, wherem
is from 2a, 2b, 3a etc., x is the input string, and y
is the (partial) dependency tree built so far:
scoreTm(pi) = scoreT (pi) + scoreGm(x, y)
The scoring function of the completion model
depends on the selected factor model Gm. The
model G2a comprises the edge factoring of Fig-
ure 3. With this model, we obtain the following
scoring function.
scoreG2a(x, y) =
?
(h,c)?y w ? ffirst(x,h,c)
+
?
(h,c,ci)?y w ? fsib(x,h,c,ci)
+
?
(h,c,cmo)?y w ? fgra(x,h,c,cmo)
+
?
(h,c,cmi)?y w ? fgra(x,h,c,cmi)
The function f maps the input sentence x, and
a subtree y defined by the indexes to a feature-
vector. Again, w is the corresponding weight vec-
tor. In order to add the factor of Figure 4 to our
model, we have to add the scoring function (2a)
the sum:
(2b) scoreG2b(x, y) = scoreG2a(x, y)
+
?
(h,c,cmi)?y w ? fgra(x,h,c,cmi)
In order to build a scoring function for combi-
nation of the factors shown in Figure 5 to 7, we
have to add to the equation 2b one or more of the
following sums:
(3a)
?
(h,c,ch1,ch2)?y w ? fgra(x,h,c,ch1,ch2)
(3b)
?
(h,c,cm1,cm2)?y w ? fgra(x,h,c,cm1,cm2)
(3c)
?
(h,c,cmo,tmo)?y w ? fgra(x,h,c,cmo,tmo)
Feature Set. The feature set of the transition
model is similar to that of Zhang and Nivre
(2011). In addition, we use the cross product of
morphologic features between the head and the
dependent since we apply also the parser on mor-
phologic rich languages.
The feature sets of the completion model de-
scribed above are mostly based on previous work
(McDonald et al 2005; McDonald and Pereira,
2006; Carreras, 2007; Koo and Collins, 2010).
The models denoted with + use all combinations
of words before and after the head, dependent,
sibling, grandchilrden, etc. These are respectively
three-, and four-grams for the first order and sec-
ond order. The algorithm includes these features
only the words left and right do not overlap with
the factor (e.g. the head, dependent, etc.). We use
feature extraction procedure for second order, and
third order factors. Each feature extracted in this
procedure includes information about the position
of the nodes relative to the other nodes of the part
and a factor identifier.
Training. For the training of our parser, we use
a variant of the perceptron algorithm that uses the
Passive-Aggressive update function, cf. (Freund
and Schapire, 1998; Collins, 2002; Crammer et
al., 2006). The Passive-Aggressive perceptron
uses an aggressive update strategy by modifying
the weight vector by as much as needed to clas-
sify correctly the current example, cf. (Crammer
et al 2006). We apply a random function (hash
function) to retrieve the weights from the weight
vector instead of a table. Bohnet (2010) showed
that the Hash Kernel improves parsing speed and
accuracy since the parser uses additionaly nega-
tive features. Ganchev and Dredze (2008) used
82
this technique for structured prediction in NLP to
reduce the needed space, cf. (Shi et al 2009).
We use as weight vector size 800 million. After
the training, we counted 65 millions non zero
weights for English (penn2malt), 83 for Czech
and 87 millions for German. The feature vectors
are the union of features originating from the
transition sequence of a sentence and the features
of the factors over all edges of a dependency tree
(e.g. G2a, etc.). To prevent over-fitting, we use
averaging to cope with this problem, cf. (Freund
and Schapire, 1998; Collins, 2002). We calculate
the error e as the sum of all attachment errors and
label errors both weighted by 0.5. We use the
following equations to compute the update.
loss: lt = e-(scoreT (x
g
t , y
g
t )-scoreT (xt, yt))
PA-update: ?t =
lt
||fg?fp||2
We train the model to select the transitions and
the completion model together and therefore, we
use one parameter space. In order to compute the
weight vector, we employ standard online learn-
ing with 25 training iterations, and carry out early
updates, cf. Collins and Roark (2004; Zhang and
Clark (2008).
Efficient Implementation. Keeping the scoring
with the completion model tractable with millions
of feature weights and for second- and third-order
factors requires careful bookkeeping and a num-
ber of specialized techniques from recent work on
dependency parsing.
We use two variables to store the scores (a)
for complete factors and (b) for incomplete fac-
tors. The complete factors (first-order factors and
higher-order factors for which further augmenta-
tion is structurally excluded) need to be calculated
only once and can then be stored with the tree fac-
tors. The incomplete factors (higher-order factors
whose node elements may still receive additional
descendants) need to be dynamically recomputed
while the tree is built.
The parsing algorithm only has to compute the
scores of the factored model when the transition-
based parser selects a left-arc or right-arc transi-
tion and the beam has to be sorted. The parser
sorts the beam when it exceeds the maximal beam
size, in order to discard superfluous parses or
when the parsing algorithm terminates in order to
select the best parse tree. The complexity of the
transition-based parser is quadratic due to swap
operation in the worse case, which is rare, and
O(n) in the best case, cf. (Nivre, 2009). The
beam size B is constant. Hence, the complexity
is in the worst case O(n2).
The parsing time is to a large degree deter-
mined by the feature extraction, the score calcu-
lation and the implementation, cf. also (Goldberg
and Elhadad, 2010). The transition-based parser
is able to parse 30 sentences per second. The
parser with completion model processes about 5
sentences per second with a beam size of 80.
Note, we use a rich feature set, a completion
model with third order factors, negative features,
and a large beam. 3
We implemented the following optimizations:
(1) We use a parallel feature extraction for the
beam elements. Each process extracts the fea-
tures, scores the possible transitions and computes
the score of the completion model. After the ex-
tension step, the beam is sorted and the best ele-
ments are selected according to the beam size.
(2) The calculation of each score is optimized (be-
yond the distinction of a static and a dynamic
component): We calculate for each location de-
termined by the last element sl ? ?i and the first
element of b0 ? ?i a numeric feature representa-
tion. This is kept fix and we add only the numeric
value for each of the edge labels plus a value for
the transition left-arc or right-arc. In this way, we
create the features incrementally. This has some
similarity to Goldberg and Elhadad (2010).
(3) We apply edge filtering as it is used in graph-
based dependency parsing, cf. (Johansson and
Nugues, 2008), i.e., we calculate the edge weights
only for the labels that were found for the part-of-
speech combination of the head and dependent in
the training data.
5 Parsing Experiments and Discussion
The results of different parsing systems are of-
ten hard to compare due to differences in phrase
structure to dependency conversions, corpus ver-
sion, and experimental settings. For better com-
parison, we provide results on English for two
commonly used data sets, based on two differ-
ent conversions of the Penn Treebank. The first
uses the Penn2Malt conversion based on the head-
36 core, 3.33 Ghz Intel Nehalem
83
Section Sentences PoS Acc.
Training 2-21 39.832 97.08
Dev 24 1.394 97.18
Test 23 2.416 97.30
Table 1: Overview of the training, development and
test data split converted to dependency graphs with
head-finding rules of (Yamada and Matsumoto, 2003).
The last column shows the accuracy of Part-of-Speech
tags.
finding rules of Yamada and Matsumoto (2003).
Table 1 gives an overview of the properties of the
corpus. The annotation of the corpus does not
contain non-projective links. The training data
was 10-fold jackknifed with our own tagger.4. Ta-
ble 1 shows the tagging accuracy.
Table 2 lists the accuracy of our transition-
based parser with completion model together with
results from related work. All results use pre-
dicted PoS tags. As a baseline, we present in ad-
dition results without the completion model and
a graph-based parser with second order features
(G2a). For the Graph-based parser, we used 10
training iterations. The following rows denoted
with Ta, T2a, T2ab, T2ab3a, T2ab3b, T2ab3bc, and
T2a3abc present the result for the parser with com-
pletion model. The subscript letters denote the
used factors of the completion model as shown
in Figure 3 to 7. The parsers with subscribed plus
(e.g. G2a+) in addition use feature templates that
contain one word left or right of the head, depen-
dent, siblings, and grandchildren. We left those
feature in our previous models out as they may in-
terfere with the second and third order factors. As
in previous work, we exclude punctuation marks
for the English data converted with Penn2Malt in
the evaluation, cf. (McDonald et al 2005; Koo
and Collins, 2010; Zhang and Nivre, 2011).5 We
optimized the feature model of our parser on sec-
tion 24 and used section 23 for evaluation. We use
a beam size of 80 for our transition-based parser
and 25 training iterations.
The second English data set was obtained by
using the LTH conversion schema as used in the
CoNLL Shared Task 2009, cf. (Hajic? et al 2009).
This corpus preserves the non-projectivity of the
phrase structure annotation, it has a rich edge
label set, and provides automatic assigned PoS
4http://code.google.com/p/mate-tools/
5We follow Koo and Collins (2010) and ignore any token
whose POS tag is one of the following tokens ?? ??:,.
Parser UAS LAS
(McDonald et al 2005) 90.9
(McDonald and Pereira, 2006) 91.5
(Huang and Sagae, 2010) 92.1
(Zhang and Nivre, 2011) 92.9
(Koo and Collins, 2010) 93.04
(Martins et al 2010) 93.26
T (baseline) 92.7
G2a (baseline) 92.89
T2a 92.94 91.87
T2ab 93.16 92.08
T2ab3a 93.20 92.10
T2ab3b 93.23 92.15
T2ab3c 93.17 92.10
T2ab3abc+ 93.39 92.38
G2a+ 93.1
(Koo et al 2008) ? 93.16
(Carreras et al 2008) ? 93.5
(Suzuki et al 2009) ? 93.79
Table 2: English Attachment Scores for the
Penn2Malt conversion of the Penn Treebank for the
test set. Punctuation is excluded from the evaluation.
The results marked with ? are not directly comparable
to our work as they depend on additional sources of
information (Brown Clusters).
tags. From the same data set, we selected the
corpora for Czech and German. In all cases, we
used the provided training, development, and test
data split, cf. (Hajic? et al 2009). In contrast
to the evaluation of the Penn2Malt conversion,
we include punctuation marks for these corpora
and follow in that the evaluation schema of the
CoNLL Shared Task 2009. Table 3 presents the
results as obtained for these data set.
The transition-based parser obtains higher ac-
curacy scores for Czech but still lower scores for
English and German. For Czech, the result of T
is 1.59 percentage points higher than the top la-
beled score in the CoNLL shared task 2009. The
reason is that T includes already third order fea-
tures that are needed to determine some edge la-
bels. The transition-based parser with completion
model T2a has even 2.62 percentage points higher
accuracy and it could improve the results of the
parser T by additional 1.03 percentage points.
The results of the parser T are lower for English
and German compared to the results of the graph-
based parser G2a. The completion model T2a can
reach a similar accuracy level for these two lan-
guages. The third order features let the transition-
based parser reach higher scores than the graph-
based parser. The third order features contribute
for each language a relatively small improvement
84
Parser Eng. Czech German
(Gesmundo
et al 2009)? 88.79/- 80.38 87.29
(Bohnet, 2009) 89.88/- 80.11 87.48
T (Baseline) 89.52/92.10 81.97/87.26 87.53/89.86
G2a (Baseline) 90.14/92.36 81.13/87.65 87.79/90.12
T2a 90.20/92.55 83.01/88.12 88.22/90.36
T2ab 90.26/92.56 83.22/88.34 88.31/90.24
T2ab3a 90.20/90.51 83.21.88.30 88.14/90.23
T2ab3b 90.26/92.57 83.22/88.35 88.50/90.59
T2ab3abc 90.31/92.58 83.31/88.30 88.33/90.45
G2a+ 90.39/92.8 81.43/88.0 88.26/90.50
T2ab3ab+ 90.36/92.66 83.48/88.47 88.51/90.62
Table 3: Labeled Attachment Scores of parsers that
use the data sets of the CoNLL shared task 2009. In
line with previous work, punctuation is included. The
parsers marked with ? used a joint model for syntactic
parsing and semantic role labelling. We provide more
parsing results for the languages of CoNLL-X Shared
Task at http://code.google.com/p/mate-tools/.
Parser UAS LAS
(Zhang and Clark, 2008) 84.3
(Huang and Sagae, 2010) 85.2
(Zhang and Nivre, 2011) 86.0 84.4
T2ab3abc+ 87.5 85.9
Table 4: Chinese Attachment Scores for the conver-
sion of CTB 5 with head rules of Zhang and Clark
(2008). We take the standard split of CTB 5 and use
in line with previous work gold segmentation, POS-
tags and exclude punctuation marks for the evaluation.
of the score. Small and statistically significant im-
provements provides the additional second order
factor (2b).6 We tried to determine the best third
order factors or set of factors but we cannot denote
such a factor which is the best for all languages.
For German, we obtained a significant improve-
ment with the factor (3b). We believe that this is
due to the flat annotation of PPs in the German
corpus. If we combine all third order factors we
obtain for the Penn2Malt conversion a small im-
provement of 0.2 percentage points over the re-
sults of (2ab). We think that a more deep feature
selection for third order factors may help to im-
prove the actuary further.
In Table 4, we present results on the Chinese
Treebank. To our knowledge, we obtain the best
published results so far.
6The results of the baseline T compared to T2ab3abc are
statistically significant (p < 0.01).
6 Conclusion and Future Work
The parser introduced in this paper combines
advantageous properties from the two major
paradigms in data-driven dependency parsing,
in particular worst case quadratic complexity of
transition-based parsing with a swap operation
and the consideration of complete second and
third order factors in the scoring of alternatives.
While previous work using third order factors, cf.
Koo and Collins (2010), was restricted to unla-
beled and projective trees, our parser can produce
labeled and non-projective dependency trees.
In contrast to parser stacking, which involves
running two parsers in training and application,
we use only the feature model of a graph-based
parser but not the graph-based parsing algorithm.
This is not only conceptually superior, but makes
training much simpler, since no jackknifing has
to be carried out. Zhang and Clark (2008) pro-
posed a similar combination, without the rescor-
ing procedure. Our implementation allows for the
use of rich feature sets in the combined scoring
functions, and our experimental results show that
the ?graph-based? completion model leads to an
increase of between 0.4 (for English) and about
1 percentage points (for Czech). The scores go
beyond the current state of the art results for ty-
pologically different languages such as Chinese,
Czech, English, and German. For Czech, English
(Penn2Malt) and German, these are to our knowl-
ege the highest reported scores of a dependency
parser that does not use additional sources of in-
formation (such as extra unlabeled training data
for clustering). Note that the efficient techniques
and implementation such as the Hash Kernel, the
incremental calculation of the scores of the com-
pletion model, and the parallel feature extraction
as well as the parallelized transition-based pars-
ing strategy play an important role in carrying out
this idea in practice.
References
S. Abney. 1991. Parsing by chunks. In Principle-
Based Parsing, pages 257?278. Kluwer Academic
Publishers.
G. Attardi. 2006. Experiments with a Multilan-
guage Non-Projective Dependency Parser. In Tenth
Conference on Computational Natural Language
Learning (CoNLL-X).
B. Bohnet. 2009. Efficient Parsing of Syntactic and
85
Semantic Dependency Structures. In Proceedings
of the 13th Conference on Computational Natural
Language Learning (CoNLL-2009).
B. Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational
Linguistics (Coling 2010), pages 89?97, Beijing,
China, August. Coling 2010 Organizing Commit-
tee.
X. Carreras, M. Collins, and T. Koo. 2008. Tag,
dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning, CoNLL ?08, pages 9?16, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
X. Carreras. 2007. Experiments with a Higher-order
Projective Dependency Parser. In EMNLP/CoNLL.
M. Collins and B. Roark. 2004. Incremental parsing
with the perceptron algorithm. In ACL, pages 111?
118.
M. Collins. 2002. Discriminative Training Methods
for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In EMNLP.
K. Crammer, O. Dekel, S. Shalev-Shwartz, and
Y. Singer. 2006. Online Passive-Aggressive Al-
gorithms. Journal of Machine Learning Research,
7:551?585.
J. Eisner. 1996. Three New Probabilistic Models for
Dependency Parsing: An Exploration. In Proceed-
ings of the 16th International Conference on Com-
putational Linguistics (COLING-96), pages 340?
345, Copenhaen.
Y. Freund and R. E. Schapire. 1998. Large margin
classification using the perceptron algorithm. In
11th Annual Conference on Computational Learn-
ing Theory, pages 209?217, New York, NY. ACM
Press.
K. Ganchev and M. Dredze. 2008. Small statisti-
cal models by random feature mixing. In Proceed-
ings of the ACL-2008 Workshop on Mobile Lan-
guage Processing. Association for Computational
Linguistics.
A. Gesmundo, J. Henderson, P. Merlo, and I. Titov.
2009. A Latent Variable Model of Syn-
chronous Syntactic-Semantic Parsing for Multiple
Languages. In Proceedings of the 13th Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), Boulder, Colorado, USA., June 4-5.
Y. Goldberg and M. Elhadad. 2010. An efficient al-
gorithm for easy-first non-directional dependency
parsing. In HLT-NAACL, pages 742?750.
C. Go?mez-Rodr??guez and J. Nivre. 2010. A
Transition-Based Parser for 2-Planar Dependency
Structures. In ACL, pages 1492?1501.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,
M. Anto`nia Mart??, L. Ma`rquez, A. Meyers, J. Nivre,
S. Pado?, J. S?te?pa?nek, P. Stran?a?k, M. Surdeanu,
N. Xue, and Y. Zhang. 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2009): Shared Task, pages
1?18, Boulder, United States, June.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1077?1086, Up-
psala, Sweden, July. Association for Computational
Linguistics.
R. Johansson and P. Nugues. 2006. Investigating
multilingual dependency parsing. In Proceedings
of the Shared Task Session of the Tenth Confer-
ence on Computational Natural Language Learning
(CoNLL-X), pages 206?210, New York City, United
States, June 8-9.
R. Johansson and P. Nugues. 2008. Dependency-
based Syntactic?Semantic Analysis with PropBank
and NomBank. In Proceedings of the Shared Task
Session of CoNLL-2008, Manchester, UK.
S. Kahane, A. Nasr, and O. Rambow. 1998.
Pseudo-projectivity: A polynomially parsable non-
projective dependency grammar. In COLING-ACL,
pages 646?652.
T. Koo and M. Collins. 2010. Efficient third-order
dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 1?11, Uppsala, Sweden,
July. Association for Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
pages 595?603.
T. Kudo and Y. Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In
proceedings of the 6th conference on Natural lan-
guage learning - Volume 20, COLING-02, pages 1?
7, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
M. Kuhlmann, C. Go?mez-Rodr??guez, and G. Satta.
2011. Dynamic programming algorithms for
transition-based dependency parsers. In ACL, pages
673?682.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: De-
pendency parsing by approximate variational infer-
ence. pages 34?44.
R. McDonald and F. Pereira. 2006. Online Learning
of Approximate Dependency Parsing Algorithms.
In In Proc. of EACL, pages 81?88.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line Large-margin Training of Dependency Parsers.
In Proc. ACL, pages 91?98.
J. Nivre and R. McDonald. 2008. Integrating Graph-
Based and Transition-Based Dependency Parsers.
In ACL-08, pages 950?958, Columbus, Ohio.
86
J. Nivre and J. Nilsson. 2005. Pseudo-projective de-
pendency parsing. In ACL.
J. Nivre, M. Kuhlmann, and J. Hall. 2009. An im-
proved oracle for dependency parsing with online
reordering. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies, IWPT
?09, pages 73?76, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
J. Nivre. 2003. An Efficient Algorithm for Pro-
jective Dependency Parsing. In 8th International
Workshop on Parsing Technologies, pages 149?160,
Nancy, France.
J. Nivre. 2009. Non-Projective Dependency Parsing
in Expected Linear Time. In Proceedings of the
47th Annual Meeting of the ACL and the 4th IJC-
NLP of the AFNLP, pages 351?359, Suntec, Singa-
pore.
K. Sagae and A. Lavie. 2006. Parser combina-
tion by reparsing. In NAACL ?06: Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers on XX,
pages 129?132, Morristown, NJ, USA. Association
for Computational Linguistics.
Q. Shi, J. Petterson, G. Dror, J. Langford, A. Smola,
and S.V.N. Vishwanathan. 2009. Hash Kernels for
Structured Data. In Journal of Machine Learning.
J. Suzuki, H. Isozaki, X. Carreras, and M Collins.
2009. An empirical study of semi-supervised struc-
tured conditional models for dependency parsing.
In EMNLP, pages 551?560.
I. Titov and J. Henderson. 2007. A Latent Variable
Model for Generative Dependency Parsing. In Pro-
ceedings of IWPT, pages 144?155.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines.
In Proceedings of IWPT, pages 195?206.
Y. Zhang and S. Clark. 2008. A tale of two
parsers: investigating and combining graph-based
and transition-based dependency parsing using
beam-search. In Proceedings of EMNLP, Hawaii,
USA.
Y. Zhang and J. Nivre. 2011. Transition-based de-
pendency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human
Language Technologies, pages 188?193, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
87
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 767?776,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
To what extent does sentence-internal realisation reflect discourse
context? A study on word order
Sina Zarrie? Jonas Kuhn
Institut fu?r maschinelle Sprachverarbeitung
University of Stuttgart, Germany
zarriesa,jonas@ims.uni-stuttgart.de
Aoife Cahill
Educational Testing Service
Princeton, NJ 08541, USA
acahill@ets.org
Abstract
We compare the impact of sentence-
internal vs. sentence-external features on
word order prediction in two generation
settings: starting out from a discrimina-
tive surface realisation ranking model for
an LFG grammar of German, we enrich
the feature set with lexical chain features
from the discourse context which can be
robustly detected and reflect rough gram-
matical correlates of notions from theoreti-
cal approaches to discourse coherence. In a
more controlled setting, we develop a con-
stituent ordering classifier that is trained
on a German treebank with gold corefer-
ence annotation. Surprisingly, in both set-
tings, the sentence-external features per-
form poorly compared to the sentence-
internal ones, and do not improve over
a baseline model capturing the syntactic
functions of the constituents.
1 Introduction
The task of surface realization, especially in a rel-
atively free word order language like German, is
only partially determined by hard syntactic con-
straints. The space of alternative realizations that
are strictly speaking grammatical is typically con-
siderable. Nevertheless, for any given choice of
lexical items and prior discourse context, only a
few realizations will come across as natural and
will contribute to a coherent text. Hence, any NLP
application involving a non-trivial generation step
is confronted with the issue of soft constraints on
grammatical alternatives in one way or another.
There are countless approaches to modelling
these soft constraints, taking into account their
interaction with various aspects of the discourse
context (givenness or salience of particular refer-
ents, prior mentioning of particular concepts).
Since so many factors are involved and there is
further interaction with subtle semantic and prag-
matic differentiations, lexical choice, stylistics
and presumably processing factors, theoretical ac-
counts making reliable predictions for real cor-
pus examples have for a long time proven elusive.
As for German, only quite recently, a number of
corpus-based studies (Filippova and Strube, 2007;
Speyer, 2005; Dipper and Zinsmeister, 2009) have
made some good progress towards a coherence-
oriented account of at least the left edge of the
German clause structure, the Vorfeld constituent.
What makes the technological application of
theoretical insights even harder is that for most
relevant factors, automatic recognition cannot be
performed with high accuracy (e.g., a coreference
accuracy in the 70?s means there is a good deal
of noise) and for the higher-level notions such
as the information-structural focus, interannotator
agreement on real corpus data tends to be much
lower than for core-grammatical notions (Poesio
and Artstein, 2005; Ritz et al 2008).
On the other hand, many of the relevant dis-
course factors are reflected indirectly in proper-
ties of the sentence-internal material. Most no-
tably, knowing the shape of referring expressions
narrows down many aspects of givenness and
salience of its referent; pronominal realizations
indicate givenness, and in German there are even
two variants of the personal pronoun (er and der)
for distinguishing salience. So, if the genera-
tion task is set in such a way that the actual lex-
ical choice, including functional categories such
as determiners, is fully fixed (which is of course
not always the case), one can take advantage of
767
these reflexes. This explains in part the fairly high
baseline performance of n-gram language mod-
els in the surface realization task. And the effect
can indeed be taken much further: the discrimi-
native training experiments of Cahill and Riester
(2009) show how effective it is to systematically
take advantage of asymmetry patterns in the mor-
phosyntactic reflexes of the discourse notion of
information status (i.e., using a feature set with
well-chosen purely sentence-bound features).
These observations give rise to the question: in
the light of the difficulty in obtaining reliable dis-
course information on the one hand and the effec-
tiveness of exploiting the reflexes of discourse in
the sentence-internal material on the other ? can
we nevertheless expect to gain something from
adding sentence-external feature information?
We propose two scenarios for adressing this
question: first, we choose an approximative ac-
cess to context information and relations between
discourse referents ? lexical reiteration of head
words, combined with information about their
grammatical relation and topological positioning
in prior sentences. We apply these features in a
rich sentence-internal surface realisation ranking
model for German. Secondly, we choose a more
controlled scenario: we train a constituent order-
ing classifier based on a feature model that cap-
tures properties of discourse referents in terms of
manually annotated coreference relations. As we
get the same effect in both setups ? the sentence-
external features do not improve over a baseline
that captures basic morphosyntactic properties of
the constituents ? we conclude that sentence-
internal realisation is actually a relatively accurate
predictor of discourse context, even more accurate
than information that can be obtained from coref-
erence and lexical chain relations.
2 Related Work
In the generation literature, most works on ex-
ploiting sentence-external discourse information
are set in a summarisation or content ordering
framework. Barzilay and Lee (2004) propose an
account for constraints on topic selection based on
probabilistic content models. Barzilay and Lapata
(2008) propose an entity grid model which repre-
sents the distribution of referents in a discourse
for sentence ordering. Karamanis et al(2009)
use Centering-based metrics to assess coherence
in an information ordering system. Clarke and La-
pata (2010) have improved a sentence compres-
sion system by capturing prominence of phrases
or referents in terms of lexical chain information
inspired by Morris and Hirst (1991) and Center-
ing (Grosz et al 1995). In their system, discourse
context is represented in terms of hard constraints
modelling whether a certain constituent can be
deleted or not.
In the linearisation or surface realisation do-
main, there is a considerable body of work ap-
proximating information structure in terms of
sentence-internal realisation (Ringger et al 2004;
Filippova and Strube, 2009; Velldal and Oepen,
2005; Cahill et al 2007). Cahill and Riester
(2009) improve realisation ranking for German ?
which mainly deals with word order variation ? by
representing precedence patterns of constituents
in terms of asymmetries in their morphosyntac-
tic properties. As a simple example, a pattern ex-
ploited by Cahill and Riester (2009) is the ten-
dency of definite elements tend to precede indef-
inites, which, on a discourse level, reflects that
given entities in a sentence tend to precede new
entities.
Other work on German surface realisation has
highlighted the role of the initial position in the
German sentence, the so-called Vorfeld (or ?pre-
field?). Filippova and Strube (2007) show that
once the Vorfeld (i.e. the constituent that precedes
the finite verb) is correctly determined, the pre-
diction of the order in the Mittelfeld (i.e. the con-
stituents that follow the finite verb) is very easy.
Cheung and Penn (2010) extend the approach
of Filippova and Strube (2007) and augment a
sentence-internal constituent ordering model with
sentence-external features inspired from the en-
tity grid model proposed by Barzilay and Lapata
(2008).
3 Motivation
While there would be many ways to construe
or represent discourse context (e.g. in terms of
the global discourse or information structure), we
concentrate on capturing local coherence through
the distribution of discourse referents in a text.
These discourse referents basically correspond to
the constituents that our surface realisation model
has to put in the right order. As the order of refer-
ents or constituents is arguably influenced by the
information structure of a sentence given the pre-
vious text, our main assumption was that infor-
768
(1) a. Kurze Zeit spa?ter erkla?rte ein Anrufer bei Nachrichtenagenturen in Pakistan , die Gruppe Gamaa bekenne sich.
Shortly after, a caller declared at the news agencies in Pakistan, that the group Gamaa avowes itself.
b. Diese Gruppe wird fu?r einen Gro?teil der Gewalttaten verantwortlich gemacht , die seit dreieinhalb Jahren in
A?gypten veru?bt worden sind .
This group is made responsible for most of the violent acts that have been committed in Egypt in the last three and
a half years.
(2) a. Belgien wu?nscht, dass sich WEU und NATO daru?ber einigen.
Belgium wants that WEU and NATO agree on that.
b. Belgien sieht in der NATO die beste milita?rische Struktur in Europa .
Belgium sees the best military structure of Europe in the NATO.
(3) a. Frauen vom Land ka?mpften aktiv darum , ein Staudammprojekt zu verhindern.
Women from the countryside fighted actively to block the dam project.
b. Auch in den Sta?dten fa?nden sich immer mehr Frauen in Selbsthilfeorganisationen zusammen.
Also in the cities, more and more women team up in self-help organisations.
mation about the prior mentioning of a referent
would be helpful for predicting the position of this
referent in a sentence.
The idea that the occurence of discourse refer-
ents in a text is a central aspect of discourse struc-
ture has been systematically pursued by Centering
Theory (Grosz et al 1995). Its most important
notions are related to the realisation of discourse
referents (i.e. described as ?centers?) and the way
the centers are arranged in a sequence of utter-
ances to make this sequence a coherent discourse.
Another important concept is the ?ranking? of dis-
course referents which basically determines the
prominence of a referent in a certain sentence and
is driven by several factors (e.g. their grammati-
cal function). For free word order languages like
German, word order has been proposed as one of
the factors that account for the ranking (Poesio et
al., 2004). In a similar spirit, Morris and Hirst
(1991) have proposed that chains of (related) lex-
ical items in a text are an important indicator of
text structure.
Our main hypothesis was that it is possible to
exploit these intuitions from Centering Theory
and the idea of lexical chains for word order pre-
diction. Thus, we expected that it would be easier
to predict the position of a referent in a sentence
if we have not only given its realisation in the cur-
rent utterance but also its prominence in the previ-
ous discourse. Especially, we expected this intu-
ition to hold for cases where the morpho-syntactic
realisation of a constituent does not provide many
clues. This is illustrated in Examples (1) and (2)
which both exemplify the reiteration of a lexical
item in two subsequent sentences, (reiteration is
one type of lexical chain discussed in Morris and
Hirst (1991)). In Example (1), the second instance
of the noun ?group? is modified by a demonstra-
tive pronoun such that its ?known? and prominent
discourse status is overt in the morpho-syntactic
realisation. In Example (2), both instances of
?Belgium? are realised as bare proper nouns with-
out an overt morphosyntactic clue indicating their
discourse status.
Beyond the simple presence of reitered items in
sequences of sentences, we expected that it would
be useful to look at the position and syntactic
function of the previous mentions of a discourse
referent. In Example (1), the reiterated item is first
introduced in an embedded sentence and realised
in the Vorfeld in the second utterance. In terms
of centering, this transition would correspond to
a topic shift. In Example (2), both instances are
realised in the Vorfeld, such that the topic of the
first sentence is carried over to the next.
In Example (3), we illustrate a further type of
lexical reiteration. In this case, two identical head
nouns are realised in subsequent sentences, even
though they refer to two different discourse refer-
ents. While this type of lexical chain is described
as ?reiteration without identity of referents? by
Morris and Hirst (1991), it would not be captured
in Centering since this is not a case of strict coref-
erence. On the other hand, lexical chains do not
capture types of reiterated discourse referents that
have distinct morpho-syntactic realisations, e.g.
nouns and pronouns.
Originally, we had the hypothesis that strict
corefence information is more useful and accurate
for word order prediction than rather loose lexi-
cal chains which conflate several types of referen-
tial and lexical relations. However, the advantage
of chains, especially chains of reiteration, is that
they can be easily detected in any corpus text and
769
that they might capture ?topics? of sentences be-
yond the identity of referents. Thus, we started
out from the idea of lexical chains and added cor-
responding features in a statistical ranking model
for surface realisation of German (Section 4). As
this strategy did not work out, we wanted to assess
whether an ideal coreference annotation would be
helpful at all for predicting word order. In a sec-
ond experiment, we use a corpus which is manu-
ally annotated for coreference (Section 5).
4 Experiment 1: Realisation Ranking
with Lexical Chains
In this Section, we present an experiment that in-
vestigates sentence-external context in a surface
realisation task. The sentence-external context is
represented in terms of lexical chain features and
compared to sentence-internal models which are
based on morphosyntactic features. The experi-
ment thus targets a generation scenario where no
coreference information is available and aims at
assessing whether relatively naive context infor-
mation is also useful.
4.1 System Description
We carry out our first experiment in a regener-
ation set-up with two components: a) a large-
scale hand-crafted Lexical Functional Grammar
(LFG) for German (Rohrer and Forst, 2006), used
to parse and regenerate a corpus sentence, b)
a stochastic ranker that selects the most appro-
priate regenerated sentence in context according
to an underlying, linguistically motivated feature
model. In contrast to fully statistical linearisation
methods, our system first generates the full set
of sentences that correspond to the grammatically
well-formed realisations of the intermediate syn-
tactic representation.1 This representation is an
f-structure, which underspecifies the order of con-
stituents and, to some extent, their morphological
realisation, such that the output sentences contain
all possible combinations of word order permu-
tations and morphological variants. Depending
on the length and structure of the original corpus
sentence, the set of regenerated sentences can be
huge (see Cahill et al(2007) for details on regen-
erating the German treebank TIGER).
1There are occasional mistakes in the grammar which
sometimes lead to ungrammatical strings being generated,
but this is rare.
The realisation ranking component is an SVM
ranking model implemented with SVMrank,
a Support Vector Machine-based learning tool
(Joachims, 2006). During training, each sentence
is annotated with a rank and a set of features ex-
tracted from the F-structure, its surface string and
external resources (e.g. a language model). If
the sentence matches the original corpus string,
its rank will be highest, the assumption being that
the original sentence corresponds to the optimal
realisation in context. The output of generation,
the top-ranked sentence, is evaluated against the
original corpus sentence.
4.2 The Feature Models
As the aim of this experiment is to better un-
derstand the nature of sentence-internal features
reflecting discourse context and compare them
to sentence-external ones, we build several fea-
ture models which capture different aspects of the
constituents in a given sentence. The sentence-
internal features describe the morphosyntacic re-
alisation of constituents, for instance their func-
tion (?subject?, ?object?), and can be straightfor-
wardly extracted from the f-structure. These fea-
tures are then combined into discriminative prece-
dence features, for instance ?subject-precedes-
object?. We implement the following types of
morphosyntactic features:
? syntactic function (arguments and adjuncts)
? modification (e.g. nouns modified by relative
clauses, genitive etc.)
? syntactic category (e.g. adverbs, proper
nouns, phrasal arguments)
? definiteness for nouns
? number and person for nominal elements
? types of pronouns (e.g. demonstrative, re-
flexive)
? constituent span and number of embedded
nodes in the tree
In addition, we also include language model
scores in our ranking model. In Section 4.4,
we report on results for several subsets of these
features where ?BaseSyn? refers to a model that
only includes the syntactic function features and
?FullMorphSyn? includes all features mentioned
above.
For extracting the lexical chains, we check for
any overlapping nouns in the n sentences previ-
ous to the current one being generated. We check
770
Rank Sentence and Features
% Diese Gruppe wird fu?r einen Gro?teil der Gewalttaten verantwortlich gemacht.
% This group is for a major part of the violent acts responsible made.
1 subject-<-pp-object, demonstrative-<-indefinite, overlap-<-no-overlap, overlap-in-vorfeld, lm:-7.89
% Fu?r einen Gro?teil der Gewalttaten wird diese Gruppe verantwortlich gemacht.
% For a major part of the violent acts is this group responsible made.
3 pp-object-<-subject, indefinite-<-demonstrative, no-overlap-<-overlap, no-overlap-in-vorfeld, lm:-10.33
% Verantwortlich gemacht wird diese Gruppe fu?r einen Gro?teil der Gewalttaten.
% Responsible made is this group for a major part of the violent acts.
3 subject-<-pp-object, demonstrative-<-indefinite, overlap-<-no-overlap, lm:-9.41
Figure 1: Made-up training example for realisation ranking with precedence features
proper and common nouns, considering full and
partial overlaps as shown in Examples (1) and
(2), where the (a) example is the previous sen-
tence in the corpus. For each overlap, we record
the following properties: (i) function in the previ-
ous sentence, (ii) position in the previous sentence
(e.g. Vorfeld), (iii) distance between sentences,
(iv) total number of overlaps.
These overlap features are then also
combined in terms of precedence, e.g.
?has subject overlap:3-precedes-no overlap?,
meaning that in the current sentence a noun
that was previously mentioned in a subject 3
sentences ago precedes a noun that was not
mentioned before.
In Figure 1, we give an example of a set of gen-
eration alternatives and their (partial) feature rep-
resentation for the sentence (1-b). Precedence is
indicated by ?<?.
Basically, our sentence-external feature model
is built on the intuition that lexical chains or over-
laps approximate discourse status in a way which
is similar to sentence-internal morphosyntactic
properties. Thus, we would expect that overlaps
indicate givenness, salience or prominence and
that asymmetries between overlapping and non-
overlapping entities are helpful in the ranking.
4.3 Data
All our models are trained on 7,039 sentences
(subdivided into 1259 texts) from the TIGER
Treebank of German newspaper text (Brants et al
2002). We tune the parameters of our SVM model
on a development set of 55 sentences and report
the final results for our unseen test set of 240 sen-
tences. Table 1 shows how many sentences in our
training, development and test sets have at least
one textually overlapping phrase in the previous
1?10 sentences.
We choose the TIGER treebank, which has no
# Sentences % Sentences with overlap
in context Training Dev Test
1 20.96 23.64 20.42
2 35.42 40.74 35.00
3 45.58 50.00 53.33
4 52.66 53.70 58.75
5 57.45 58.18 64.58
6 61.42 57.41 68.75
7 64.58 61.11 70.83
8 67.05 62.96 72.08
9 69.20 64.81 74.17
10 71.16 70.37 75.83
Table 1: The percentage of sentences that have at least
one overlapping entity in the previous n sentences
coreference annotation, since we already have a
number of resources available to match the syn-
tactic analyses produced by our grammar against
the analyses in the treebank. Thus, in our regen-
eration system, we parse the sentences with the
grammar, and choose the parsed f-structures that
are compatible with the manual annotation in the
TIGER treebank as is done in Cahill et al(2007).
This compatibility check eliminates noise which
would be introduced by generating from incorrect
parses (e.g. incorrect PP-attachments typically re-
sult in unnatural and non-equivalent surface reali-
sations).
For comparing the string chosen by the mod-
els against the original corpus sentence, we use
BLEU, NIST and exact match. Exact match is
a strict measure that only credits the system if it
chooses the exact same string as the original cor-
pus string. BLEU and NIST are more relaxed
measures that compare the strings on the n-gram
level. Finally, we report accuracy scores for the
Vorfeld position (VF) corresponding to the per-
centage of sentences generated with a correct Vor-
feld.
771
Sc BLEU NIST Exact VF
0 0.766 11.885 50.19 64.0
1 0.765 11.756 49.78 64.0
2 0.765 11.886 50.01 64.1
3 0.765 11.885 50.08 63.8
4 0.761 11.723 49.43 63.2
5 0.765 11.884 49.71 64.2
6 0.768 11.892 50.42 64.6
7 0.765 11.885 50.01 64.5
8 0.764 11.884 49.78 64.3
9 0.765 11.888 49.82 63.6
10 0.764 11.889 49.7 63.5
Table 2: Tenfold-crossvalidation for feature model
FullMorphSyn and different context windows (Sc)
Model BLEU VF
Language Model 0.702 51.2
Language Model + Context Sc = 5 0.715 54.3
BaseSyn 0.757 62.0
BaseSyn + Context Sc = 5 0.760 63.0
FullMorphSyn 0.766 64.0
FullMorphSyn + Context Sc = 5 0.763 64.2
Table 3: Evaluation for different feature models; ?Lan-
guage Model?: ranking based on language model
scores, ?BaseSyn?: precedence between constituent
functions, ?FullMorphSyn?: entire set of sentence-
internal features.
4.4 Results
In Table 2, we report the performance of the full
sentence-internal feature model combined with
context windows from zero to ten. The scores
have been obtained from tenfold-crossvalidation.
For none of the context windows, the model out-
performs the baseline with a zero context which
has no sentence-external features. In Table 3,
we compare the performance of several feature
models corresponding to subsets of the features
used so far which are combined with sentence-
external features respectively. We note that the
function precedence features (i.e. the ?BaseSyn?
model) are very powerful, leading to a major im-
provement compared to a language model. The
sentence-external features lead to an improvement
when combined with the language-model based
ranking. However, this improvement is leveled
out in the BaseSyn model.
On the one hand, the fact that the lexical chain
features improve a language-model based ranking
suggests these features are, to some extent, pre-
dictive for certain patterns of German word order.
On the other hand, the fact that they don?t improve
over an informed sentence-internal baseline sug-
gests that these patterns are equally well captured
by morphosyntactic features. However, we cannot
exclude the possibility that the chain features are
too noisy as they conflate several types of lexical
and coreferential relations. This will be adressed
in the following experiment.
5 Experiment 2: Constituent Ordering
with Centering-inspired Features
We now look at a simpler generation setup where
we concentrate on the ordering of constituents in
the German Vorfeld and Mittelfeld. This strat-
egy has also been adopted in previous investiga-
tions of German word order: Filippova and Strube
(2007) show that once the German Vorfeld is cor-
rectly chosen, the prediction accuracy for the Mit-
telfeld (the constituents following the finite verb)
is in the 90s.
In order to eliminate noise introduced from po-
tentially heterogeneous chain features, we look at
coreference features and, again, compare them to
sentence-internal morphosyntactic features. We
target a generation scenario where coreference in-
formation is available. The aim is to establish an
upper bound concerning the quality improvement
for word order prediction by recurring to manual
corefence annotation.
5.1 Data and Setup
We carry out the constituent ordering experiment
on the Tu?ba-D/Z treebank (v5) of German news-
paper articles (Telljohann et al 2006). It com-
prises about 800k tokens in 45k sentences. We
choose this corpus because it is not only annotated
with syntactic analyses but also with coreference
relations (Naumann, 2006). The syntactic annota-
tion format differs from the TIGER treebank used
in the previous experiment, for instance, it ex-
plicitely represents the Vorfeld and Mittelfeld as
phrasal nodes in the tree. This format is very con-
venient for the extraction of constituents in the re-
spective positions.
The Tu?ba-D/Z coreference annotation distin-
guishes several relations between discourse ref-
erents, most importantly ?coreferential relation?
and ?anaphoric relation? where the first denotes
a relation between noun phrases that refer to the
same entity, and the latter refers to a link between
a pronoun and a contextual antecedent, see Nau-
mann (2006) for further detail. We expected the
coreferential relation to be particularly useful, as
772
it cannot always be read off the morphosyntac-
tic realisation of a noun phrase, whereas pronouns
are almost always used in an anaphoric relation.
The constituent ordering model is implemented
as a classifier that is given a set of constituents
and predicts the constituent that is most likely to
be realised in the Vorfeld.
The set of candidate constituents is determined
from the tree of the original corpus sentence. We
will assume that all constituents under a Vorfeld
and Mittelfeld node can be freely reordered. Thus,
we do not check whether the word order variants
we look at are actually grammatical assuming that
most of them are. In this sense, this experiment
is close to fully statistical generation approaches.
As a further simplification, we do not look at mor-
phological generation variants of the constituents
or their head verb.
The classifier is implemented with SVMrank
again. In contrast to the previous experiment
where we learned to rank sentences, the classi-
fier now learns to rank constituents. The con-
stituents have been extracted using the tool de-
scribed in Bouma (2010). The final data set com-
prises 48.513 candidate sets of freely orderable
constituents.
5.2 Centering-inspired Feature Model
To compare the discourse context model against a
sentence-based model, we implemented a number
of sentence-internal features that are very similar
to the features used in the previous experiment.
Since we extract them from the syntactic annota-
tion instead of f-structures, some labels and fea-
ture names will be different, however, the design
of the sentence-internal model is identical to the
previous one in Section 4.
The sentence-external features differ in some
aspects from Section 4, since we extract coref-
erence relations of several types (see (Naumann,
2006) for the anaphoric relations annotated in the
Tueba-D/Z). For each type of coreference link,
we extract the following properties: (i) function
of the antecedent, (ii) position of the antecedent,
(iii) distance between sentences, (iv) type of rela-
tion. We also distinguish coreference links anno-
tated for the whole phrase (?head link?) and links
that are annotated for an element embedded by the
constituent (?contained link?). The two types are
illustrated in Examples (4) and (5). Note that both
cases would not have been captured in the lexical
# VF # MF
Backward Center 3.5% 5.1%
Forward Center 6.8% 6.8%
Coref Link 30.5% 23.4%
Table 4: Backward and forward centers and their posi-
tions
chain model since there is no lexical overlap be-
tween the realisations of the discourse referents.
These types of coreference features implicitly
carry the information that would also be consid-
ered in a Centering formalisation of discourse
context. In addition to these, we designed features
that explicitly describe centers as these might
have a higher weight. In line with Clarke and
Lapata (2010), we compute backward (CB) and
forward centers (CF ) in the following way:
1. Extract all entities from the current sentence
and the previous sentence.
2. Rank the entities of the previous sentence ac-
cording to their function (subject < direct
object < indirect object ...).
3. Find the highest ranked entity in the previous
sentence that has a link to an entity in the
current sentence, this entity is the CB of the
sentence.
In the same way, we mark entities as forward
centers that are ranked highest in the current sen-
tence and have a link to an entity in the following
sentence.2 In Table 4, we report the percentage of
sentences that have backward and forward centers
in the Vorfeld or Mittelfeld. While the percentage
of sentences that realise a backward center is quite
low, the overall proportion of sentences contain-
ing some type of coreference link is in a dimen-
sion such that the learner could definitely pick up
some predictive patterns. Going by the relative
frequencies, coreferential constituents have a bias
towards appearing in the Vorfeld rather than in the
Mittelfeld.
5.3 Results
First, we build three coreference-based con-
stituent classifiers on their entire training set and
compare them to their sentence-internal baseline.
The most simple baseline records the category of
2In Centering, all entities in a given utterance can be seen
as forward centers, however we thought that this implemen-
tation would be more useful.
773
(4) a. Die Rechnung geht an die AWO.
The bill goes to the AWO.
b. [Hintergrund der gegenseitigen Vorwu?rfe in der Arbeiterwohlfahrt] sind offenbar scharfe Konkurrenzen zwischen
Bremern und Bremerhavenern.
Apparently, [the background of the mutual accusations at the labour welfare] are rivalries between people from
Bremen and Bremerhaven.
(5) a. Dies ist die Behauptung, mit der Bremens Ha?fensenator die Skeptiker davon u?berzeugt hat, [...].
This is the claim, which Bremen?s harbour senator used to convince doubters, [...].
b. Fu?r diese Behauptung hat Beckmeyer bisher keinen Nachweis geliefert. So far, Beckmeyer has not given a prove of
this claim.
Model VF
ConstituentLength + HeadPos 47.48%
ConstituentLength + HeadPos + Coref 51.30%
BaseSyn 54.82%
BaseSyn + Coref 56.21%
FullMorphSyn 57.24%
FullMorphSyn + Coref 57.40%
Table 5: Results from Vorfeld classification, training
and evaluation on entire treebank
the constituent head and the number of words that
the constituent spans. Additionally, in parallel to
the experiment in Section 4, we build a ?BaseSyn?
model which has the syntactic function features,
and a ?FullMorphSyn? model which comprises
the entire set of sentence-internal features. To
each of these baseline, we add the coreference
features. The results are reported in Table 5.
In this experiment, we find an effect of
the sentence-external features over the simple
sentence-internal baselines. However, in the fully
spelled-out, sentence-internal model, the effect
is, again, minimal. Moreover, for each base-
line, we obtain higher improvements by adding
further sentence-internal features than by adding
sentence-external ones the accuracy of the sim-
ple baseline (47.48%) improves by 7.34 points
through adding function features (the accuracy
of BaseSyn is 54.82%) and by only 3.48 points
through adding coreference features.
We run a second experiment in order to so see
whether the better performance of the sentence-
internal features is related to their coverage. We
build and evaluate the same set of classifiers on
the subset of sentences that contain at least one
coreference link for one of its constituents (see
Table 4 for the distribution of coreference links
in our data). The results are given in Table 6. In
this experiment, the coreference features improve
over all sentence-internal baselines including the
?FullMorphSyn? model.
Model VF
ConstituentLength + HeadPos 46.61%
ConstituentLength + HeadPos + Coref 52.23%
BaseSyn 54.63%
BaseSyn + Coref 56.67%
FullMorphSyn 55.36%
FullMorphSyn + Coref 57.93%
Table 6: Results from Vorfeld classification, training
and evaluation on sentences that contain a coreference
link
5.4 Discussion
The results presented in this Section consis-
tently complete the picture that emerged from
the experiments in Section 4. Even if we have
high quality information about discourse con-
text in terms of relations between referents, a
non-trivial sentence-internal model for word or-
der prediction can be hardly improved. This
suggests that sentence-internal approximations of
discourse context provide a fairly good way of
dealing with local coherence in a linearisation
task. It is also interesting that the sentence-
external features improve over simple baselines,
but get leveled out in rich sentence-internal fea-
ture models. From this, we conclude that the
sentence-external features we implemented are to
some extent predictive for word order, but that
they can be covered by sentence-internal features
as well.
Our second evaluation concentrating on the
sentences that have coreference information
shows that the better performance of the sentence-
internal features is also related to their cover-
age. These results confirm our initial intuition
that coreference information can add to the pre-
dictive power of the morpho-syntactic features in
certain contexts. This positive effect disappears
when sentences with and without coreferential
constituents are taken together. For future work,
it would be promising to investigate whether the
774
positive impact of coreference features can be
strengthened if the coreference annotation scheme
is more exhaustive, including, e.g., bridging and
event anaphora.
6 Conclusion
We have carried out a number of experiments that
show that sentence-internal models for word order
are hardly improved by features which explicitely
represent the preceding context of a sentence in
terms of lexical and referential relations between
discourse entities. This suggests that sentence-
internal realisation implicitly carries a lot of im-
formation about discourse context. On average,
the morphosyntactic properties of constituents in
a text are better approximates of their discourse
status than actual coreference relations.
This result feeds into a number of research
questions concerning the representation of dis-
course and its application in generation systems.
Although we should certainly not expect a com-
putational model to achieve a perfect accuracy in
the constituent ordering task ? even humans only
agree to a certain extent in rating word order vari-
ants (Belz and Reiter, 2006; Cahill, 2009) ? the
average accuracy in the 60?s for prediction of Vor-
feld occupance is still moderate. An obvious di-
rection would be to further investigate more com-
plex representations of discourse that take into ac-
count the relations between utterances, such as
topic shifts. Moreover, it is not clear whether the
effects we find for linearisation in this paper carry
over to other levels of generation such as tacti-
cal generation where syntactic functions are not
fully specified. In a broader perspective, our re-
sults underline the need for better formalisations
of discourse that can be translated into features for
large-scale applications such as generation.
Acknowledgments
This work was funded by the Collaborative Re-
search Centre (SFB 732) at the University of
Stuttgart.
References
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Com-
putational Linguistics, 34:1?34.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models with applications
to generation and summarization. In Proceedings of
HLT-NAACL 2004, Boston,MA.
Anja Belz and Ehud Reiter. 2006. Comparing auto-
matic and human evaluation of NLG systems. In
Proceedings of EACL 2006, pages 313?320, Trento,
Italy.
Gerlof Bouma. 2010. Syntactic tree queries in prolog.
In Proceedings of the Fourth Linguistic Annotation
Workshop, ACL 2010.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Aoife Cahill and Arndt Riester. 2009. Incorporat-
ing information status into generation ranking. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 817?825, Suntec, Singapore,
August. Association for Computational Linguistics.
Aoife Cahill, Martin Forst, and Christian Rohrer.
2007. Stochastic Realisation Ranking for a Free
Word Order Language. In Proceedings of the
Eleventh European Workshop on Natural Language
Generation, pages 17?24, Saarbru?cken, Germany.
DFKI GmbH.
Aoife Cahill. 2009. Correlating human and automatic
evaluation of a german surface realiser. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, pages 97?100, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
Jackie C.K. Cheung and Gerald Penn. 2010. Entity-
based local coherence modelling using topological
fields. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics
(ACL 2010). Association for Computational Lin-
guistics.
James Clarke and Mirella Lapata. 2010. Discourse
constraints for document compression. Computa-
tional Linguistics, 36(3):411?441.
Stefanie Dipper and Heike Zinsmeister. 2009. The
role of the German Vorfeld for local coherence. In
Christian Chiarcos, Richard Eckart de Castilho, and
Manfred Stede, editors, Von der Form zur Bedeu-
tung: Texte automatisch verarbeiten/From Form to
Meaning: Processing Texts Automatically, pages
69?79. Narr, Tu?bingen.
Katja Filippova and Michael Strube. 2007. The ger-
man vorfeld and local coherence. Journal of Logic,
Language and Information, 16:465?485.
Katja Filippova and Michael Strube. 2009. Tree Lin-
earization in English: Improving Language Model
Based Approaches. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, Companion Volume:
Short Papers, pages 225?228, Boulder, Colorado,
June. Association for Computational Linguistics.
775
Barbara J. Grosz, Aravind Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the
local coherence of discourse. Computational Lin-
guistics, 21(2):203?225.
Thorsten Joachims. 2006. Training linear SVMs in
linear time. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD),
pages 217?226.
Nikiforos Karamanis, Massimo Poesioand Chris Mel-
lish, and Jon Oberlander. 2009. Evaluating center-
ing for information ordering using corpora. Com-
putational Linguistics, 35(1).
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion, the thesaurus, and the structure of text. Com-
putational Linguistics, 17(1):21?225.
Karin Naumann. 2006. Manual for the annotation of
in-document referential relations. Technical report,
Seminar fu?r Sprachwissenschaft, Abt. Computerlin-
guistik, Universita?t Tu?bingen.
Massimo Poesio and Ron Artstein. 2005. The relia-
bility of anaphoric annotation, reconsidered: Taking
ambiguity into account. In Proc. of ACL Workshop
on Frontiers in Corpus Annotation.
Massimo Poesio, Rosemary Stevenson, Barbara di Eu-
genio, and Janet Hitzeman. 2004. Centering: A
parametric theory and its instantiations. Computa-
tional Linguistics, 30(3):309?363.
Eric K. Ringger, Michael Gamon, Robert C. Moore,
David Rojas, Martine Smets, and Simon Corston-
Oliver. 2004. Linguistically Informed Statisti-
cal Models of Constituent Structure for Ordering
in Sentence Realization. In Proceedings of the
2004 International Conference on Computational
Linguistics, Geneva, Switzerland.
Julia Ritz, Stefanie Dipper, and Michael Go?tze. 2008.
Annotation of information structure: An evaluation
across different types of texts. In Proceedings of the
the 6th LREC conference.
Christian Rohrer and Martin Forst. 2006. Improv-
ing Coverage and Parsing Quality of a Large-Scale
LFG for German. In Proceedings of the Fifth In-
ternational Conference on Language Resources and
Evaluation (LREC), Genoa, Italy.
Augustin Speyer. 2005. Competing constraints on
vorfeldbesetzung in german. In Proceedings of the
Constraints in Discourse Workshop, pages 79?87.
Heike Telljohann, Erhard Hinrichs, Sandra Ku?bler,
and Heike Zinsmeister. 2006. Stylebook for the
tu?bingen treebank of written german (tu?ba-d/z).
revised version. Technical report, Seminar fu?r
Sprachwissenschaft, Universita?t Tu?bingen.
Erik Velldal and Stephan Oepen. 2005. Maximum
entropy models for realization ranking. In Proceed-
ings of the 10th Machine Translation Summit, pages
109?116, Thailand.
776
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 57?60,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
A Graphical Interface for Automatic Error Mining in Corpora
Gregor Thiele Wolfgang Seeker Markus G
?
artner Anders Bj
?
orkelund Jonas Kuhn
Institute for Natural Language Processing
University of Stuttgart
{thielegr,seeker,gaertnms,anders,kuhn}@ims.uni-stuttgart.de
Abstract
We present an error mining tool that is de-
signed to help human annotators to find
errors and inconsistencies in their anno-
tation. The output of the underlying al-
gorithm is accessible via a graphical user
interface, which provides two aggregate
views: a list of potential errors in con-
text and a distribution over labels. The
user can always directly access the ac-
tual sentence containing the potential er-
ror, thus enabling annotators to quickly
judge whether the found candidate is in-
deed incorrectly labeled.
1 Introduction
Manually annotated corpora and treebanks are the
primary tools that we have for developing and
evaluating models and theories for natural lan-
guage processing. Given their importance for test-
ing our hypotheses, it is imperative that they are
of the best quality possible. However, manual an-
notation is tedious and error-prone, especially if
many annotators are involved. It is therefore desir-
able to have automatic means for detecting errors
and inconsistencies in the annotation.
Automatic methods for error detection in tree-
banks have been developed in the DECCA
project
1
for several different annotation types, for
example part-of-speech (Dickinson and Meurers,
2003a), constituency syntax (Dickinson and Meur-
ers, 2003b), and dependency syntax (Boyd et al.,
2008). These algorithms work on the assumption
that two data points that appear in identical con-
texts should be labeled in the same way. While
the data points in question, or nuclei, can be single
tokens, spans of tokens, or edges between two to-
kens, context is usually modeled as n-grams over
the surrounding tokens. A nucleus that occurs
1
http://www.decca.osu.edu
multiple times in identical contexts but is labeled
differently shows variation and is considered a po-
tential error.
Natural language is ambiguous and variation
found by an algorithm may be a genuine ambigu-
ity rather than an annotation error. Although we
can support an annotator in finding inconsisten-
cies in a treebank, these inconsistencies still need
to be judged by humans. In this paper, we present
a tool that allows a user to run automatic error de-
tection on a corpus annotated with part-of-speech
or dependency syntax.
2
The tool provides the user
with a graphical interface to browse the variation
nuclei found by the algorithm and inspect their la-
bel distribution. The user can always switch be-
tween high-level aggregate views and the actual
sentences containing the potential error in order to
decide if that particular annotation is incorrect or
not. The interface thus brings together the output
of the error detection algorithm with a direct ac-
cess to the corpus data. This speeds up the pro-
cess of tracking down inconsistencies and errors
in the annotation considerably compared to work-
ing with the raw output of the original DECCA
tools. Several options allow the user to fine-tune
the behavior of the algorithm. The tool is part of
ICARUS (G?artner et al., 2013), a general search
and exploration tool.
3
2 The Error Detection Algorithm
The algorithm, described in Dickinson and Meur-
ers (2003a) for POS tags, works by starting from
individual tokens (the nuclei) by recording their
assigned part-of-speech over an entire treebank.
From there, it iteratively increases the context for
each instance by extending the string to both sides
to include adjacent tokens. It thus successively
builds larger n-grams by adding tokens to the left
2
Generalizing the tool to support any kind of positional
annotation is planned.
3
http://www.ims.uni-stuttgart.de/data/icarus.html
57
Figure 1: The variation n-gram view.
or to the right. Instances are grouped together if
their context is identical, i. e. if their token n-
grams match. Groups where all instances have
the same label do not show variation and are dis-
carded. The algorithm stops when either no vari-
ation nuclei are left or when none of them can be
further extended. All remaining groups that show
variation are considered potential errors. Erro-
neous annotations that do not show variation in the
data cannot be found by the algorithm. This limits
the usefulness of the method for very small data
sets. Also, given the inherent ambiguity of nat-
ural language, the algorithm is not guaranteed to
exclusively output errors, but it achieves very high
precision in experiments on several languages.
The algorithm has been extended to find errors
in constituency and dependency structures (Dick-
inson and Meurers, 2003b; Boyd et al., 2008),
where the definition of a nucleus is changed to
capture phrases and dependency edges. Context
is always modeled using n-grams over surround-
ing tokens, but see, e. g., Boyd et al. (2007) for
extensions.
3 Graphical Error Mining
To start the error mining, a treebank and an error
mining algorithm (part-of-speech or dependency)
must be selected. The algorithm is then executed
on the data to create the variation n-grams. The
user can choose between two views for browsing
the potential errors in the treebank: (1) a view
showing the list of variation n-grams found by the
error detection algorithm and (2) a view showing
label distributions over word forms.
3.1 The Variation N-Gram View
Figure 1 shows a screenshot of the view where the
user is presented with the list of variation n-grams
output by the error detection algorithm. The main
window shows the list of n-grams. When the user
selects one of the n-grams, information about the
nucleus is displayed below the main window. The
user can inspect the distribution over labels (here
part-of-speech tags) with their absolute frequen-
cies. Above the main window, the user can adjust
the length of the presented n-grams, sort them, or
search for specific strings.
For example, Figure 1 shows a part of the vari-
ation n-grams found in the German TiGer corpus
(Brants et al., 2002). The minimum and maximum
length was restricted to four, thus the list contains
only 4-grams. The 4-gram so hoch wie in was se-
lected, which contains wie as its nucleus. In the
lower part, the user can see that wie occurs with
four different part-of-speech tags in the treebank,
namely KOKOM, PWAV, KON, and KOUS. Note
that the combination with KOUS occurs only once
in the entire treebank.
Double clicking on the selected 4-gram in the
list will open up a new tab that displays all sen-
tences that contain this n-gram, with the nucleus
being highlighted. The user can then go through
each of the sentences and decide whether the an-
notated part-of-speech tag is correct. Each time
the user clicks on an n-gram, a new tab will be
created, so that the user can jump back to previous
results without having to recreate them.
A double click on one of the lines in the lower
part of the window will bring up all sentences that
contain that particular combination of word form
58
Figure 2: The label distribution view.
and part-of-speech tag. The fourth line will, for
example, show the one sentence where wie has
been tagged as KOUS, making it easy to quickly
judge whether the tag is correct. In this case, the
annotation is incorrect (it should have been PWAV)
and should thus be marked for correction.
3.2 The Label Distribution View
In addition to the output of the algorithm by Dick-
inson and Meurers (2003a), the tool also provides
a second view, which displays tag distributions of
word forms to the user (see Figure 2). To the left,
a list of unique label combinations is shown. Se-
lecting one of them displays a list of word forms
that occur with exactly these tags in the corpus.
This list is shown below the list of label combina-
tions. To the right, the frequencies of the differ-
ent labels are shown in a bar chart. The leftmost
bar for each label always shows the total frequency
summed over all word forms in the set. Selecting
one or more in the list of word forms adds addi-
tional bars to the chart that show the frequencies
for each selected word form.
As an example, Figure 2 shows the tag combi-
nation [VVINF][VVIZU], which are used to tag in-
finitives with and without incorporated zu in Ger-
man. There are three word forms in the cor-
pus that occur with these two part-of-speech tags:
hinzukommen, aufzul?osen, and anzun?ahern. The
chart on the right shows the frequencies for each
word form and part-of-speech tag, revealing that
hinzukommen is mostly tagged as VVINF but once
as VVIZU, whereas for the other two word forms it
is the other way around. This example is interest-
ing if one is looking for annotation errors in the
TiGer treebank, because the two part-of-speech
tags should have a complementary distribution (a
German verb either incorporates zu or it does not).
Double clicking on the word forms in the list in
the lower left corner will again open up a tab that
shows all sentences containing this word form, re-
gardless of their part-of-speech tag. The user may
then inspect the sentences and decide whether the
annotations are erroneous or not. If the user wants
to see a specific combination, which is more use-
ful if the total number of sentences is large, she
can also click on one of the bars in the chart to get
all sentences matching that combination. In the
example, the one instance of hinzukommen being
tagged as VVIZU is incorrect,
4
and the instances of
the two other verbs tagged as VVINF are as well.
3.3 Dependency Annotation Errors
As mentioned before, the tool also allows the user
to search for errors in dependency structures. The
error mining algorithm for dependency structures
(Boyd et al., 2008) is very similar to the one for
part-of-speech tags, and so is the interface to the
n-gram list or the distribution view. Dependency
edges are therein displayed as triples: the head,
the dependent, and the edge label with the edge?s
direction. As with the part-of-speech tags, the user
can always jump directly to the sentences that con-
tain a particular n-gram or dependency relation.
4
Actually, the word form hinzukommen can belong to two
different verbs, hinzu-kommen and hin-kommen. However,
the latter, which incorporates zu, does not occur in TiGer.
59
4 Error Detection on TiGer
We ran the error mining algorithm for part-of-
speech on the German TiGer Treebank (the de-
pendency version by Seeker and Kuhn (2012)) and
manually evaluated a small sample of n-grams in
order to get an idea of how useful the output is.
We manually checked 115 out of the 207 vari-
ation 6-grams found by the tool, which amounts
to 119 different nuclei. For 99.16% of these nu-
clei, we found erroneous annotations in the asso-
ciated sentences. 95.6% of these are errors where
we are able to decide what the right tag should
be, the remaining ones are more difficult to disam-
biguate because the annotation guidelines do not
cover them.
These results are in line with findings by Dick-
inson and Meurers (2003a) for the Penn Treebank.
They show that even manually annotated corpora
contain errors and an automatic error mining tool
can be a big help in finding them. Furthermore,
it can help annotators to improve their annotation
guidelines by pointing out phenomena that are not
covered by the guidelines, because these phenom-
ena will be more likely to show variation.
5 Related Work
We are aware of only one other graphical tool that
was developed to help with error detection in tree-
banks: Ambati et al. (2010) and Agarwal et al.
(2012) describe a graphical tool that was used in
the annotation of the Hindi Dependency Treebank.
To find errors, it uses a statistical and a rule-based
component. The statistical component is recall-
oriented and learns a MaxEnt model, which is used
to flag dependency edges as errors if their proba-
bility falls below a predefined threshold. In or-
der to increase the precision, the output is post-
processed by the rule-based component, which is
tailored to the treebank?s annotation guidelines.
Errors are presented to the annotators in tables,
also with the option to go to the sentences di-
rectly from there. Unlike the algorithm we im-
plemented, this approach needs annotated training
data for training the classifier and tuning the re-
spective thresholds.
6 Conclusion
High-quality annotations for linguistic corpora are
important for testing hypotheses in NLP and lin-
guistic research. Automatically marking potential
annotation errors and inconsistencies are one way
of supporting annotators in their work. We pre-
sented a tool that provides a graphical interface for
annotators to find and evaluate annotation errors
in treebanks. It implements the error detection al-
gorithms by Dickinson and Meurers (2003a) and
Boyd et al. (2008). The user can view errors from
two perspectives that aggregate error information
found by the algorithm, and it is always easy to
go directly to the actual sentences for manual in-
spection. The tool is currently extended such that
annotators can make changes to the data directly
in the interface when they find an error.
Acknowledgements
We thank Markus Dickinson for his comments.
Funded by BMBF via project No. 01UG1120F,
CLARIN-D, and by DFG via SFB 732, project D8.
References
Rahul Agarwal, Bharat Ram Ambati, and Anil Kumar
Singh. 2012. A GUI to Detect and Correct Errors in
Hindi Dependency Treebank. In LREC 2012, pages
1907?1911.
Bharat Ram Ambati, Mridul Gupta, Samar Husain, and
Dipti Misra Sharma. 2010. A High Recall Error
Identification Tool for Hindi Treebank Validation.
In LREC 2010.
Adriane Boyd, Markus Dickinson, and Detmar Meur-
ers. 2007. Increasing the Recall of Corpus Annota-
tion Error Detection. In TLT 2007, pages 19?30.
Adriane Boyd, Markus Dickinson, and Detmar Meur-
ers. 2008. On Detecting Errors in Dependency
Treebanks. Research on Language and Computa-
tion, 6(2):113?137.
Sabine Brants, Stefanie Dipper, Silvia Hansen-Shirra,
Wolfgang Lezius, and George Smith. 2002. The
TIGER treebank. In TLT 2002, pages 24?41.
Markus Dickinson and W. Detmar Meurers. 2003a.
Detecting Errors in Part-of-Speech Annotation. In
EACL 2003, pages 107?114.
Markus Dickinson and W. Detmar Meurers. 2003b.
Detecting Inconsistencies in Treebanks. In TLT
2003, pages 45?56.
Markus G?artner, Gregor Thiele, Wolfgang Seeker, An-
ders Bj?orkelund, and Jonas Kuhn. 2013. ICARUS
? An Extensible Graphical Search Tool for Depen-
dency Treebanks. In ACL: System Demonstrations,
pages 55?60.
Wolfgang Seeker and Jonas Kuhn. 2012. Making El-
lipses Explicit in Dependency Conversion for a Ger-
man Treebank. In LREC 2012, pages 3132?3139.
60
Morphological and Syntactic Case in
Statistical Dependency Parsing
Wolfgang Seeker?
University of Stuttgart
Jonas Kuhn??
University of Stuttgart
Most morphologically rich languages with free word order use case systems to mark the gram-
matical function of nominal elements, especially for the core argument functions of a verb. The
standard pipeline approach in syntactic dependency parsing assumes a complete disambiguation
of morphological (case) information prior to automatic syntactic analysis. Parsing experiments
on Czech, German, and Hungarian show that this approach is susceptible to propagating
morphological annotation errors when parsing languages displaying syncretism in their mor-
phological case paradigms. We develop a different architecture where we use case as a possibly
underspecified filtering device restricting the options for syntactic analysis. Carefully designed
morpho-syntactic constraints can delimit the search space of a statistical dependency parser and
exclude solutions that would violate the restrictions overtly marked in the morphology of the
words in a given sentence. The constrained system outperforms a state-of-the-art data-driven
pipeline architecture, as we show experimentally, and, in addition, the parser output comes with
guarantees about local and global morpho-syntactic wellformedness, which can be useful for
downstream applications.
1. Introduction
In statistical parsing, many of the first models were developed and optimized for
English. This is not surprising, given that English is the predominant language for
research in both computational linguistics and linguistics proper. By design, the
statistical parsing approach avoids language-specific decisions built into the model
architecture; models should in principle be trainable on any data following the general
treebank representation scheme. At the same time, it is well known from theoretical
and typological work in linguistics that there is a broad multi-dimensional spectrum
of language types, and that English is in a rather ?extreme? area in that it marks
grammatical relations (subject, object, etc.) strictly with phrase-structural configura-
tions. There are only residues of an inflectional morphology left. In other words, one
? Institut fu?r Maschinelle Sprachverarbeitung, Universita?t Stuttgart, Pfaffenwaldring 5b, D-70569 Stuttgart,
Germany. E-mail: seeker@ims.uni-stuttgart.de.
?? Institut fu?r Maschinelle Sprachverarbeitung, Universita?t Stuttgart, Pfaffenwaldring 5b, D-70569 Stuttgart,
Germany. E-mail: jonas@ims.uni-stuttgart.de.
Submission received: 30 September 2011; revised submission received: 20 May 2012; accepted for publication:
3 August 2012.
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 1
cannot exclude that architectural or representational modeling decisions established
as empirically useful on English data may be favoring the specific language type
of English. Indeed, carrying over successful model architectures from English to
typologically different languages mostly leads to a substantial drop in parsing
accuracy. Linguistically aware representational adjustments can help reduce the
problem significantly, as Collins et al (1999) showed in their pivotal study adjusting a
statistical (constituent) parsing model to a highly inflectional language with free word
order, Czech in that case, pushing the results more than seven percentage points up
to a final 80% dependency accuracy (as compared with 91% accuracy for the English
?source? parser on the Wall Street Journal). Even in recent years, however, a clear gap
has remained between the top parsing architecture for English and morphologically
rich(er) languages.1 The relative hardness of the parsing task, compared with English,
cuts across statistical parsing approaches (constituent or dependency parsing) and
across morphological subtypes, such as languages with a moderately sized remaining
inflectional system (like German), highly inflected languages (like Czech), and
languages in which interactions with derivational morphology make the segmentation
question non-trivial (such as Turkish or Arabic, compare, for example, Eryig?it, Nivre,
and Oflazer [2008]).
Still, it remains hard to pinpoint systematic architectural or representational factors
that explain the empirical picture, although there is a collection of ?recipes? one can
try to tune an approach to a ?hard language.? Of course, there are good reasons
for adjusting a well-proven system rather than developing a more general one from
scratch?given that part of the success of statistical parsing in general lies in subtle
ways of exploiting statistical patterns that reflect inaccessible levels of information in an
indirect way.
This article attempts to do justice to the special status of mature data-driven systems
and still contribute to a systematic clarification, by (1) focusing on a clear-cut aspect
of morphological marking relevant to syntactic parsing (namely, case marking of core
arguments); (2) comparing a selection of languages covering part of the typological
spectrum (Czech, German, and Hungarian); (3) using a state-of-the-art data-driven
parser (Bohnet 2009, 2010) to establish how far the technique of representational ad-
justments may take us; and (4) performing a problem-oriented comparison with an
alternative architecture, which allows us to add constraints motivated from linguistic
considerations.
In a first experiment, we vary the morphological information available to the parser
and examine the errors of the parser with respect to the case-related functions. It
turns out that although the parser is indeed able to learn the case-function mapping
for all three languages, it is susceptible to errors that are propagated through the
pipeline model when parsing languages that show syncretism2 in their morphological
paradigms, in our case Czech and German (e. g., for neuter nouns, nominative and
accusative case have the same surface form). In contrast, due to its mostly unambiguous
case system, we find a much smaller effect for Hungarian. Although the parser itself
profits much from morphological information as our experiments with gold standard
morphology show, errors in automatically predicted morphological information fre-
quently cause errors in the syntactic analysis.
1 Compare, for example, the various Shared Tasks on parsing multiple languages, such as the CoNLL
Shared Tasks 2006, 2007, 2009 (Buchholz and Marsi 2006; Nivre et al 2007a; Hajic? et al 2009), or the PaGe
Shared Task on parsing German (Ku?bler 2008).
2 Two or more different feature values are signaled by the same form.
24
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
In order to better handle syncretism in the morphological description, we then
propose a different way of integrating morphology into the parsing process. We develop
an alternative architecture that circumvents the strict separation of morphological and
syntactic analysis in the pipeline model. We adopt the integer linear programming
(henceforth ILP) approach by Martins, Smith, and Xing (2009), which we augment
with a set of linguistically motivated constraints modeling the morpho-syntactic depen-
dencies in the languages. Case is herein interpreted as an underspecified filtering device
that guides a statistical model by restricting the search space of the parser. Due to the
constraints, the output of the ILP parser is guaranteed to obey all syntactic restrictions
that are marked overtly in the morphological form of the words. Although the restric-
tions are implemented as symbolic constraints, they are applied to the parser during
the search for the best tree, which is driven by a statistical model. We show in a second
experiment that restricting the search space in this way improves the performance on
argument functions (indicated by case morphology) considerably on all three languages
while the performance on all other functions stays stable.
We proceed by first discussing the role of case morphology in syntax (Section 2),
followed by a presentation of the parsing architecture of the Bohnet parser with a
discussion of the relevant aspects for our first experiment (Section 3). Next, we compare
the morphological annotation quality of automatic tools with the gold standard across
languages (Section 4). We then turn to the first experiment in this article where we
examine the performance of the parser with respect to core argument functions on
the three languages (Section 5). In the second experiment (Section 6), we apply an
ILP parser to the data sets augmented with a set of linguistic constraints that integrate
morphological information in an underspecified way into the parsing architecture. We
conclude in Section 7.
2. Challenges of Parsing Morphologically Rich Languages
A characteristic property of most languages commonly referred to as morphologically rich
is that they use morphological means at the word level to encode grammatical relations
within the sentence rather than using the phrase-structural configuration. Whereas in
English or Chinese, placement of a word (or phrase) in a particular position relative
to the verbal head marks its function (e. g., as the subject or object), morphologically
rich languages encode grammatical relations largely by changing the morphological
form of the dependent word, the head word, or both. A correlated phenomenon is the
free word order for which many of these languages allow. Because information about
grammatical relations is marked on the words themselves, it stays available regardless
of their relative position, so word order can be used to mark other information such
as topic-focus structure. The richer the morphological system, the freer the word order
tends to be, or, as Bresnan (2001) puts it, morphology competes with syntax. We thus see that
typologically, morphological and syntactic systems are interdependent and influence
each other. Most languages are located somewhere along a continuum between purely
configurational and purely morphological marking.
In principle, data-driven parsing models with word form sensitive features have the
potential to not only pick up configurational patterns for grammatical relation marking,
but also systematic patterns in the observed variation and co-variation of morphological
word forms. It is, however, not only the interaction between syntax and morphol-
ogy that adds challenges?the marking patterns are also non-trivial to pick up from
surface data.
25
Computational Linguistics Volume 39, Number 1
One of the linguistic challenges is that there are different, overlapping regimes for
morphological marking. One can distinguish head-marking and dependent-marking of
a grammatical relation, depending on where the inflection occurs. In addition, Nichols
(1986, page 58) identifies four ways in which inflection markers may play a role in
signaling syntactic dependency:
Example 1
Hebrew, taken from Nichols (1986, page 58)
be?t
house-of
sefer
book
?school?, lit. ?book house?
First, the morphological marker simply registers the presence of a syntactic depen-
dency. In Example (1), the form of the word be?t signals the presence of a dependent,
without specifying the nature of the relation.
Second, the affix marks not only the presence but also the type of the dependency.
A typical example of the dependent-marking kind is nominal case: Accusative case
on a noun marks it not only as a dependent of a verb, but it also marks the type of
relation, namely, direct object. Verb agreement markers in Indo-European languages
are a head-marking kind of example: They indicate that a noun stands specifically
in the subject relation. Third, a morphological marker may, in addition, index certain
lexical or inflectional categories of the dependent on the head (or vice versa). Subject
agreement often indexes the dependent subject?s gender and number properties on
the head verb; attributive adjectives in Czech, for instance, agree with their noun
heads in case, number, and gender. Fourth, for some affixes, there is a paradigm for
indexing internal properties of the head on the head itself (e. g., tense or mood of
a verb) or properties of the dependent on the dependent (e. g., gender marking on
nouns).
An additional linguistic challenge in learning the patterns from data, which we will
discuss in detail in Section 2.2, comes from the fact that the inflectional paradigms may
contain syncretism. This may interfere with the learning of the previously discussed
patterns. Further challenges we do not address in this article include interactions be-
tween syntax and derivational morphology, which for some languages like Turkish and
Arabic can go along with segmentation issues.
The first Workshop on Statistical Parsing of Morphologically Rich Languages has
set the agenda for developing effective systems by identifying three main types of
technical challenges (Tsarfaty et al 2010, page 2), which we rephrase here from our
system perspective:
Architectural challenges. Should data-driven syntactic parsing be split into subtasks,
and how should they interact? Specifically, should morphological analysis (and likewise
tokenization, part-of-speech tagging, etc.) be performed in a separate (data-driven?)
module and how can error propagation through the pipeline be minimized? Can a joint
model be trained on data that captures two-way interactions between several levels
of representation? Should the same system modularization be used in training and
decoding, or can decoding combine locally trained models, taking into account more
global structural and representational constraints?
Representational challenges. At what technical level should morphological distinc-
tions be represented? Should they (or some of them) be included at the part-of-speech
(POS) level, or at a higher level in the structure? Can some type of representation help
26
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
avoid confusions due to syncretisms? What is the most effective set of dependency
labels for capturing morphological marking of grammatical relations?
Lexical challenges. How can lexical probabilities be estimated reliably? The main
problem for morphologically rich languages is the many different forms for one lexeme,
which is amplified by the often limited amount of training data. How can a parser
analyze unseen word forms and use the information profitably?
2.1 Previous Work and Our Approach
The first two types of technical challenges often go hand in hand, as a change in architec-
ture effectively means a change in the interface representations, and vice versa. Collins
et al (1999) reduce the tag set for the Czech treebank, which consists of a combination of
POS tags and a detailed morphological specification, in order to tackle data sparseness.
A combination of POS and case features turns out to be best for their parsing models.
In statistical constituent parsing, many investigations devise treebank transformations
that allow the parsing models to access morphological information higher in the tree
(Schiehlen 2004; Versley 2005; Versley and Rehbein 2009). These transformations apply
category splits by decorating category symbols with morphological information like
case. Whereas these approaches change traditional models to cope with morphological
information, others approach the problem by devising new models tailored to the
special requirements of morphologically rich languages. Tsarfaty and Sima?an (2008,
2010) introduce an additional layer into the parsing process that directly models the sub-
categorization of a non-terminal symbol without taking word order into consideration.
The parser thus separates the functional subcategorization of a word from its surface
realization, which is not a one-to-one relation in morphologically rich languages with
free word order. In statistical dependency parsing, morphological information is mostly
used as features in the statistical classifier that guides the search for the most probable
tree (Bohnet 2009; Goldberg and Elhadad 2010; Marton, Habash, and Rambow 2010).
The standard way established in the CoNLL Shared Tasks (Nivre et al 2007a; Hajic? et al
2009) is a pipeline approach where POS and morphological information is predicted as
a preprocessing step to the actual parsing. Although Goldberg and Elhadad (2010) and
Marton, Habash, and Rambow (2010) find improvements for hand-annotated (gold)
morphological features, automatically predicted morphological information has none
or even negative effects on their parsing models. Goldberg and Elhadad (2010) also
show that linguistically grounded, carefully designed features (here agreement be-
tween adjectives and nouns in Hebrew) can contribute a considerable improvement,
however. Finally, the pipeline approach itself can be questioned. Cohen and Smith
(2007), Goldberg and Tsarfaty (2008), and Lee, Naradowsky, and Smith (2011) present
joint models where the processes of predicting morphological information and syn-
tactic information are performed at the same time. All three approaches acknowl-
edge the fact that syntax and morphology are heavily intertwined and interact with
each other.
Our attempt at tackling the technical and linguistic challenges can be characterized
as follows: In Section 6, we propose a system architecture that at the basic level follows
a pipeline approach, where local data-driven models are used to predict the highest
scoring output in each step. But this pipeline is complemented with a knowledge-
based component modeling grammatical knowledge about inflectional paradigms and
morphological marking of grammatical relations. Both parts are combined using a set
of global constraints that model the language-specific morpho-syntactic dependencies
to which a syntactic structure in that language has to adhere. These constraints are
27
Computational Linguistics Volume 39, Number 1
used to weed out linguistically implausible structures among the candidate outputs
of the parser. Our architecture thus resides between a strict pipeline approach where
no step can influence previous results, and a full joint model, where several subtasks
are predicted simultaneously. Using the global constraints we can precisely define the
parts of the structure where an interaction between the morphology and the syntax is
allowed to take place.
The key design tasks are of a representational nature: What are the linguistic units
for which hard constraints can (and should) be enforced in a language? (For example,
within Czech and German nominal phrases, indexing of case, number, and gender
follows a strict regime?the values have to co-vary.) What underspecified interface
representation is appropriate to negotiate between the potentially ambiguous output
of one local component and the assumed input of another component? How can we
restrict them as much as possible without sacrificing the correct solution? As it turns out,
the explicit enforcement of conservative linguistic constraints over morphological and
syntactic structures in decoding leads to significantly improved parsing performance
on case-bearing dependents, and also to improved overall performance over a state-
of-the-art data-driven pipeline approach.
2.2 Case Between Morphology and Syntax
In this article, we concentrate on the case feature, which resides at the interface be-
tween morphology and syntax. The case feature overtly marks (when unambiguous)
the syntactic function of a nominal element in a language. Languages show different
sophistication in their case systems. Where German has four different case values,
Hungarian uses a complex system of about 20 different values. In all languages with
a case system, it is used to distinguish and mark the function of the different arguments
of verbs (Blake 2001). Correctly recognizing the argument structure of verbs is one of the
most important tasks in automatic syntactic analysis because verbs and their arguments
encode the core meaning of a sentence and are therefore essential to every subsequent
semantic analysis step.
The three languages investigated in this article, German, Czech, and Hungarian,
belong to the broad category of morphologically rich languages. Syntactically, they all
use a case system to mark the function of the arguments of a verb (and a preposi-
tion). The morphological realization of these case systems show important differences,
however, which have a direct influence on syntactic analysis. Czech and German are
both Indo-European languages, Czech from the Slavonic branch and German from
the Germanic branch. Hungarian, on the other hand, is a Finno-Ugric language of
the Ugric branch. Czech and German both are fusional languages, where nominal
inflection suffixes signal gender, number, and case values simultaneously. Hungar-
ian is an agglutinating language, namely, every morphological feature is signaled
by its own morpheme, which is appended to the word. Whereas Hungarian has a
mostly unambiguous case system, Czech and (more so) German show a considerable
amount of syncretism in their nominal inflection. It is this syncretism that makes it so
much harder for a statistical system to learn the morphological marking patterns of a
language.
Table 1 shows examples of declension paradigms for the fusional languages Czech
and German. Note that these are only examples and cannot represent the entire com-
plexity of the systems. We use them here to exemplify the widespread morphological
syncretism in these two languages. In the masculine animate noun of Czech bratr
(?brother?), ACC/GEN SG, DAT/LOC SG, and ACC/INS PL use the same word forms
28
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
Table 1
Examples of nominal declension paradigms in Czech and German. German never distinguishes
gender in plural.
Czech, masc. animate noun brother Czech, neuter noun city
MASC ANI SG PL NEUT SG PL
NOM bratr bratr?i NOM me?sto me?sta
ACC bratra bratry ACC me?sto me?sta
DAT bratrovi/u bratru?m DAT me?stu me?stu?m
GEN bratra bratru? GEN me?sta me?st
VOC bratr?e ? VOC me?sto ?
LOC bratrovi/u bratrech LOC me?ste?/u me?stech
INS bratrem bratry INS me?stem me?sty
German, definite determiner German, masculine noun German, feminine noun
the dog woman
MASC NEUT FEM PL MASC PL FEM PL
NOM der das die die NOM Hund Hunde NOM Frau Frauen
ACC den das die die ACC Hund Hunde ACC Frau Frauen
DAT dem dem der den DAT Hund Hunden DAT Frau Frauen
GEN des des der der GEN Hundes Hunde GEN Frau Frauen
respectively.3 In the neuter noun me?sto (?city?) we find syncretism in NOM/ACC SG and
PL, and DAT/LOC SG. The NOM/ACC syncretism in neuter nouns is a typical property
of Indo-European languages (Blake 2001). Note also that some inflection morphemes
fill different paradigm cells, for instance bratra is ACC SG, me?sta is NOM PL. To resolve
the ambiguity, gender and number features need to be considered.
Unlike Czech, German has determiners, which are also marked for case and agree
with their head noun in the so-called phi-features (gender, number, case). The declen-
sion patterns of determiners and nouns in German have developed in different ways,
leading to highly case-ambiguous forms for nouns. We see in Table 1 two German
nouns, a masculine one and a feminine one. Although the declension paradigm of the
masculine noun has kept some residual formal marking of case in the GEN SG and the
DAT PL, the declension pattern of the feminine noun does not show case distinction at
all. Both nouns, however, mark the number feature overtly. The paradigm of the de-
terminer is much less ambiguous in the case dimension, but shows syncretism between
different number and gender features. Eisenberg (2006) calls the distribution of different
kinds of syncretism over different parts of the German noun phrase Funktionsteilung
(?function sharing?). It makes the morphological agreement between German nouns
and their dependents extremely important because only by agreement can a mutual
disambiguation take place and reduce the morpho-syntactic ambiguity for the noun
phrase. We will show that for the fusional languages Czech and German, automatic
morphological analyzers have problems predicting the correct case, number, and gen-
der values, whereas for the agglutinating language Hungarian, the unambiguous case
paradigm makes case prediction extremely easy.
3 NOM: nominative, GEN: genitive, DAT: dative, ACC: accusative, LOC: locative, INS: instrumental, SG:
singular, PL: plural, M/MASC: masculine, F/FEM: feminine, N/NEUT: neuter.
29
Computational Linguistics Volume 39, Number 1
?
? ?
?
? ?
Jako
?
as
pr?edkapela
nom
support band
se
acc
themselves
pr?edstav??
?
present
kapela
nom
band
Ambivalency
?
?
AuxY
Obj4 Sb
Atr
Atv
The band Ambivalency performs as support band
Figure 1
A dependency tree from the Czech treebank. Sentence no. 3,159 in the CoNLL 2009 data set.
In order to see the influence of morphology on today?s data-driven systems for
syntactic analysis, we investigate the performance of a state-of-the-art dependency
parser (Bohnet 2009, 2010) on the three languages just described paying special attention
to the handling of the core grammatical functions (i. e., the argument functions of verbs).
Dependency syntax (Hudson 1984; Mel?c?uk 1988) models the syntactic structure of a
sentence by directed labeled links between the words (tokens) of a sentence. Figure 1
shows an example tree for a Czech sentence. Every word of the tree is attached to exactly
one other word (its head) by a labeled arc whose label specifies the nature of the relation.
For instance, kapela is labeled as subject (Sb) of the sentence. Morphologically, the subject
is marked with nominative case (nom) whereas the direct object (Obj4) is marked with
accusative case (acc). We see that the object can precede the verb. Syntactically, Czech
allows for all permutations of subject, object, and verb (Janda and Townsend 2000,
page 86). It is thus a free word order language. Another property of free word order
languages is the higher amount of non-projective structures (compared with English).
Non-projective structures are indicated by crossing branches in the tree structure, as
between kapela and pr?edkapela in Figure 1.
3. Parsing Architecture
In this section, we give a brief description of the parser that we use in the first exper-
iment, where we analyze the performance of the parser with respect to morphological
information. The parser is the state-of-the-art data-driven second-order graph-based
dependency parser presented in Bohnet (2010).4 It is an improved version of the parser
described in Bohnet (2009), which ranked first for German and second for Czech for
syntactic labeled attachment score in the CoNLL 2009 Shared Task (Hajic? et al 2009).
The parser follows the standard pipeline approach. Information about lemma,
POS, and morphology is automatically predicted and fully disambiguated prior to
the parsing step. The CoNLL 2009 Shared Task used a tabbed format where every
token in a sentence is represented by a line of tabulator-separated fields holding
gold standard and predicted information about word position, word form, lemma,
POS, morphology, attachment, and function label. Figure 2 gives an example for the
word se in the sentence in Figure 1. Note that for every type of information, the
4 http://code.google.com/p/mate-tools, version: anna-2.
30
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
3 se se se P P SubPOS=7|Num=X|Cas=4 SubPOS=7|Num=X|Cas=4 4 4 Obj4 Obj4
Figure 2
Example of the CoNLL 2009 dependency format for se. Columns are from left to right:
Position, word form, gold lemma, predicted lemma, gold POS, predicted POS, gold
morphology, predicted morphology, gold head position, predicted head position, gold
function label, predicted function label. Semantic information is not displayed. The gold
standard columns are used for evaluation purposes.
human-annotated gold standard and the predicted value by an automatic tool is rep-
resented. The morphology columns contain several morphological features separated
by a vertical bar.
The parser itself consists of two main modules, the decoder and the feature model.
It is a maximum-spanning-tree5 parser (McDonald et al 2005; McDonald and Pereira
2006) that searches for the best-scoring tree using a chart-based dynamic programming
approach similar to the one proposed by Eisner (1997). The substructures are scored by
a statistical feature model that has been trained on treebank data; the best-scoring tree is
the tree with the highest sum over the scores of all substructures in the tree. The actual
implementation is derived from the decoder by Carreras (2007), which was shown to be
efficient even for very rich feature models (Carreras 2007; Johansson and Nugues 2008;
Bohnet 2009).
The features used in the statistical model are combinations of basic features, namely,
word form, lemma, POS, and morphological features. In addition, the distance between
two nodes, the direction of the edge, and the words between head and dependent are
included. Every feature is combined with the function label on the edge. A detailed
description of the feature model is beyond the scope of this article, but the interested
reader can find it in Bohnet (2009, 2010).
Because we are interested in the way the parser handles morphological information,
we will briefly discuss the inclusion of morphological features as described in Bohnet
(2009, page 3). The parser computes morphological features by combining the part-
of-speech tags (pos) of the head and the dependent with the cross-product of their
morphological feature values. For this, the morphological information (see Figure 2:
columns 7 and 8) is split at the vertical bar and every single morphological feature
value is treated as one morphological feature in the statistical model. The cross-product
then pairs the single feature values of dependent and head creating all combinations.
One single feature computed for the edge between an adjective and a noun in Czech
may then look like (A,N,acc,acc), which states the information that both words have
the accusative case. Other features are created as well, however, that might look like
(A,N,sg,masc), which states that the adjective has singular number and the noun has
masculine gender. So the algorithm does not pay attention to category classes. Further-
more, the whole cross-product is computed for every edge in the tree. All features are
additionally combined with the function label between the head and the dependent,
so in the parsing features, a morphological feature like case is directly combined with
the function label with which it appears together in the treebank. Because of this, the
parser should have direct access to the information about which case value signals
a particular grammatical function. Intuitively, the statistical model should learn that
certain dependent head configurations often occur with certain morphological feature
5 Or graph-based as opposed to transition-based (Nivre et al 2007b; Bohnet 2011).
31
Computational Linguistics Volume 39, Number 1
combinations. For example, a subject edge between a noun and a verb should very often
occur together with morphological features involving nominative case, and a dative
object edge should often occur with a dative feature.
The statistical model is a linear multi-class classifier, trained using an on-line learn-
ing procedure (MIRA [Crammer et al 2003] with a hash kernel [Bohnet 2010]). Learning
is an iterative process where the parser repeatedly tries to recreate the training corpus
sentence by sentence. If the parser makes no mistakes, it proceeds to the next training
instance. Otherwise, the feature weights for the tree that would have been correct and
the feature weights for the tree produced by the parser are compared and the weights
in the feature model are adjusted to favor the correct tree and disfavor the incorrect one.
The parser repeatedly parses the treebank, adjusting its feature model to produce trees
that match the trees in the training data. Because the decoder can only derive projective
trees (without crossing edges), the parser reattaches individual edges in the tree in
a post-processing step to allow for non-projective trees (crossing edges, see Figure 1)
using the algorithm in McDonald and Pereira (2006).
4. Data
Before we turn to our first experiment and its analysis, we briefly describe the data
sets that we used in the experiments and discuss the quality of the morphological
annotation. In a pipeline architecture, where morphological features are fully disam-
biguated prior to parsing, low quality in the predicted morphological information will
have considerable impact on the ability of the parser to learn the mapping between
case and grammatical functions that we want it to learn. Furthermore, the errors made
in the morphological preprocessing are the first observable difference between the two
fusional languages and the agglutinating language and directly reflect this typological
difference. We will thus show that whereas the morphological preprocessing for Czech
and German makes mistakes because of the syncretism in the morphological paradigms,
the morphological preprocessing for Hungarian suffers from a different problem.
All the data sets come from the newspaper domain. The Czech data set is the
CoNLL 2009 Shared Task data set consisting of 38,727 sentences from the Prague
Dependency Treebank (Bo?hmova? et al 2000; Hajic? et al 2006). The German data set
(Seeker and Kuhn 2012) is a semi-automatically corrected recreation of the data set that
was used in the CoNLL 2009 Shared Task (36,017 sentences). It uses the exact6 same raw
(surface) data but contains a different syntactic annotation. It was semi-automatically
derived from the original TIGER treebank (Brants et al 2002) and some time was spent
on manually correcting incorrect function labels and POS tags. The Hungarian data
consist of the general newspaper subcorpus (10,188 sentences) of the Szeged Treebank
(Csendes, Csirik, and Gyimo?thy 2004), which was converted from the original con-
stituent structure annotation to dependency annotation and manually checked by four
trained linguists (Vincze et al 2010). For the experiments in the following sections, we
use the training splits for Czech and German, and the whole set for Hungarian.
For the Czech and the Hungarian data, we kept the predicted information for
lemmata, POS, and morphology that was already provided with the data. For both
6 Except for three sentences that for some reason were missing in the 2006 version of the TiGer treebank,
from which this corpus was derived. The original data set in the CoNLL 2009 Shared Task was derived
from the 2005 version, which still contains these three sentences. The 2005 version also contained
spelling errors in the raw data that had been removed in the 2006 version. These errors were manually
reintroduced in order to recreate the data set as exact as possible.
32
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
languages, this information is predicted in a two-step process where a finite-state
analyzer produces a set of possible annotations for a given verb form, which is then
disambiguated by a statistical model trained on gold-standard data (for Czech, see
Spoustova? et al [2009]; for Hungarian, see Zsibrita, Vincze, and Farkas [2010]). The
German data set was cross-annotated by applying statistical tools7 trained on the gold
standard annotation. Contrary to the Czech and Hungarian data sets, lemma, POS,
and morphological information were annotated in three steps, each building upon the
preceding one.
In preparation for the experiments, we made two changes to the annotation in the
Czech and the Hungarian treebanks in order to allow for a more fine-grained analysis.
First, we copied the SubPOS feature value8 over to the respective POS column (gold
to gold, predicted to predicted). This helps us in doing a more fine-grained evaluation,
which is based on certain POS tags, but it also allows us to formulate linguistic con-
straints in the ILP parser more precisely, as we will see in Section 6.1. The German POS
tag set is already rather specific. We also changed the object labels (Obj) in the Czech
data set by combining it with the case value in the gold standard morphology (creating
Obj1-7). This gives us a more fine-grained object distinction for our analysis and it also
separates the case-marked objects from the clausal objects, which do not have a case
feature and therefore keep the original Obj label.9
In order to learn the mapping between case and grammatical functions, the parser
relies on the automatically predicted morphological information in the data sets. When
the parser is trained on predicted morphology, in principle, it has the chance to adapt
to the errors of an automatic morphological analyzer. We will see in Section 5, however,
that this does not seem to happen very often. Therefore, if we want the parser to perform
well, we need to predict morphological information with high quality. Table 2 shows
the prediction quality of the automatic morphological analyzers in the three data sets.
On the left-hand side, precision and recall are shown for the phi-features for the whole
data set; on the right-hand side, only those words were evaluated where the predicted
POS tag matched the gold standard one. We see that Czech and Hungarian achieve
high scores on all three features, with Czech achieving over 95% for each feature, and
Hungarian over 94% recall and almost 98% precision. In contrast, we find a rather
mediocre annotation in the German data set, where only the number feature can be
predicted with comparable quality,10 and gender and case prediction is rather bad. To a
certain extent, the lower performance for German compared to Czech can be explained
by the more informed annotation tool for Czech. The German data set was annotated
by purely statistical tools whereas the Czech annotation tool uses a finite-state lexicon
to support the statistical disambiguator.
Hungarian shows a big gap between precision and recall (97.83% and 94.11% for
case) when evaluating all words, but the performance on the words with the correct
POS tag is almost perfect (99.22% for case!). The reason lies in the POS recognition.
The Hungarian POS tag set uses a category X as a kind of a catch-all category where
annotators would put tokens they could not assign anywhere else. The precision for this
class is below 10%, because the tool is classifying a considerable amount of proper nouns
(Np) as X. The class X, however, does not get a morphological specification so that about
7 Mate-tools by Bernd Bohnet: http://code.google.com/p/mate-tools.
8 The SubPOS feature distinguishes subcategories inside the main POS categories and is part of the
morphological description (see Figure 2).
9 Prepositional objects headed by prepositions (pos: RR, RF, RV) were also excluded.
10 There are only two values to predict though.
33
Computational Linguistics Volume 39, Number 1
Table 2
Annotation quality of the phi-features (case, gender, and number) for all words and for those
words with a correctly predicted POS tag.
all correct POS
precision recall precision recall
Czech case 95.73 95.63 96.06 96.06
gender 97.59 97.45 98.03 98.03
number 98.18 98.08 98.47 98.47
German case 88.69 88.51 89.26 89.06
gender 90.16 89.99 90.95 90.74
number 96.18 95.63 96.92 96.61
Hungarian case 97.83 94.11 99.22 99.22
number 98.64 95.91 99.88 99.88
3,500 out of 12,500 proper nouns do not receive a case and a number value at all. The
reason for the poor morphological annotation in Hungarian is apparently not a problem
of an ambiguous morphology, it is simply a problem of the POS recognition. We already
know that Hungarian is an agglutinating language. The case paradigm of Hungarian,
although comprising about 20 different case values, does not show syncretic forms with
the exception of a regular genitive-dative syncretism. Whereas in Hungarian, getting
the POS correct effectively means getting case and number correct, the results in Table 2
for Czech and German11 are not much better for words with correctly predicted POS
tags than for all words. In Czech and German, this is a problem of the syncretism in the
morphological paradigms.
The low syncretism in the Hungarian case paradigm is due to the agglutinating
nature of its morphological system. Because every feature (e.g., case) is signaled by
its own morpheme, a syncretism in the system would erase the distinction between
the syncretic forms. Because Hungarian uses the same case paradigm for all words,
a regular syncretism would mean that a certain distinction can no longer be made in
the language.12 In fusional languages, an inflection morpheme signals more than one
feature value. Many syncretisms can thus be disambiguated by the other feature values
or by agreement with dependents, as is done in the German noun phrase. We learn
two things from these findings: First, we may need different approaches for handling
morphology in fusional languages like Czech and German than we do for agglutinating
languages like Hungarian. And second, the category morphologically rich encompasses
11 The fact that for German, precision and recall differ is due to the independency of the POS tagger
and the morphological analyzer. In the German data, the morphological analyzer is not bound
to a certain feature template determined by the POS of the word, so that, in principle, it can
assign case to verbs and tense to nouns. This is not the case for the Czech and Hungarian analyzers.
Precision, recall, and F-score measured over all possible values amount to simple accuracy in those
languages.
12 One of the reviewers pointed out to us that Turkish as an agglutinating language also shows much
morphological ambiguity. That is correct and this also holds for Hungarian. The case paradigm itself
seems to have no syncretism in Turkish, however. The ambiguity rather comes from interaction with
vowel harmony and definiteness marking. The syncretism between genitive and dative case in the
Hungarian case system is more of a puzzle. Our best guess is that the distribution of these cases is
so different that the context can disambiguate them relatively easily.
34
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
languages that are not only different from English but also show important differences
among each other that we should take into account when devising parsing technology.
5. Experiment 1
Having examined the quality of the predicted morphological information in the data
sets, we can now investigate how the parser deals with this information. We proceed
as follows: We train three different models for each language, one using gold standard
morphology, one using predicted morphology, and one using no morphological infor-
mation (henceforth GOLD-M, PRED-M, and NO-M). Comparing the performance of these
three models allows us to see the effect that the morphological information has on the
parsing performance. The model using gold morphology serves as an upper bound
where we can observe the behavior of the parser when it is not disturbed by errors
coming from the automatic morphological analyzers. Note that this model is very unre-
alistic in the sense that syncretisms are fully resolved in the morphological information.
The model using predicted morphology serves as a realistic scenario where we can
observe the problems introduced by imperfect preprocessing and propagated errors
in the pipeline (e.g., due to syncretism). And finally, the model using no morphology
shows us how much non-morphological information contributes to the parsing perfor-
mance. In comparison with the other two models, we can then see the contribution of
morphological information13 to the parsing process. All models use the same predicted
lemma and POS information as discussed in the previous section.
5.1 Experimental Set-up
We performed a five-fold cross annotation14 on the training portions of the data sets of
Czech and German, and on the whole subcorpus of Hungarian, varying the morpho-
logical annotation as described. The overall parsing performance is shown in Table 3,
where the German and the Hungarian scores exclude punctuation and the Czech scores
include them.15
Table 3 gives us the usual picture that has been noticed in several shared tasks
on dependency parsing for multiple languages (e.g., CoNLL-ST 2006, 2007, 2009). The
performance on German is pretty high, although not as high as it would be for English,
and the performance on Czech is rather low. Note the extreme divergence between
labeled (LAS) and unlabeled attachment score (UAS) for Czech.16 For Hungarian, the
performance is comparable to Czech in terms of UAS but the LAS for Hungarian is
better. We also see the expected ordering in performance for the models using dif-
ferent kinds of morphological information. The gold models always outperform the
models using predicted morphology, which in turn outperform the models using no
morphological information. Note, however, that whereas the performance on German
does not degrade very much when using no morphological information, it is very
13 It should be noted that by morphological information we always mean the complete annotation available
in the treebanks. Although we concentrate in the analysis on the phi-features (gender, number, case), the
models using morphological information always use the whole set, including also, for example, verbal
morphology.
14 The number of iterations during training was set to 10.
15 Punctuation in the Czech data set is sometimes used as the head in coordination.
16 This is due to the way the Czech data label certain phenomena, which makes it difficult for the parser to
decide on the correct label. See Boyd, Dickinson, and Meurers (2008, pages 8?9) for examples.
35
Computational Linguistics Volume 39, Number 1
Table 3
Overall performance of the Bohnet parser on the five-fold cross annotation for every language
and different kind of morphological annotation. All results in percent. LAS = labeled attachment
score; UAS = unlabeled attachment score. Results for German and Hungarian are without
punctuation. Best score for Czech on the CoNLL 2009 Shared Task was by Gesmundo et al
(2009), best score for German was by Bohnet (2009), best score for Hungarian on the CoNLL
2007 Shared Task was by Nivre et al (2007a). Best CoNLL 09/07 results were obtained on
different data sets.
Czech German Hungarian
LAS UAS LAS UAS LAS UAS
GOLD-M 82.49 88.61 91.26 93.20 86.70 89.70
PRED-M 81.41 88.13 89.61 92.18 84.33 88.02
NO-M 79.00 86.89 89.18 91.97 78.04 86.02
best on CoNLL 09/07 80.38 ? 87.48 ? 80.27 83.55
harmful for Hungarian to do so (78.04% LAS for NO-M in comparison with 84.33%
LAS for PRED-M). The Czech results lie in between. To give a general impression of
the performance of the parser, the last row shows parsing results for the three languages
reported in the literature. The results have been obtained on different data sets, however,
so a direct comparison would be invalid.
5.2 Analysis
Although the scores in Table 3 reflect the quality of the parser on the complete test
data, we would not expect case morphology to influence all of the functions. We will
therefore go into more detail and concentrate on nominal elements (nouns, pronouns,
adjectives, etc.)17 and core grammatical functions (subjects, objects, nominal predicates,
etc.) because in our three languages, nominal elements carry case morphology to mark
their syntactic function. Core grammatical functions are vital to the interpretation of
a sentence because they mark the participants of a situation. We exclude clausal and
prepositional arguments, which can fill the argument slot of a verb but would not
be marked by case morphology. Table 4 shows the encoding of the core grammatical
functions in the three treebanks.
Table 5 shows the performance of the parsing models for each of the three languages
on the core grammatical functions. As described in Section 4, we split the object function
for Czech according to its associated case value. The results are shown for each of the
three models with GOLD-M on the left, PRED-M in the middle, and NO-M on the right.
The results shown for the NO-M models indicate again that morphology plays a
bigger role in Czech and Hungarian for determining the core grammatical functions
than it does for German. The performance on all grammatical functions except the
rather rare genitive object is generally higher for German, showing that to a large
17 We determine a nominal element by its gold standard POS tag:
Czech: AA, AG, AM, AU, C?, Ca, Cd, Ch, Cl, Cn, Cr, Cw, Cy, NN, P1, P4, P5, P6, P7, P8, P9, PD, PE, PH,
PJ, PK, PL, PP, PQ, PS, PW, PZ.
German: ADJA, ART, NE, NN, PDAT, PDS, PIAT, PIS, PPER, PPOSAT, PPOSS, PRELAT, PRELS, PRF,
PWS, PWAT.
Hungarian: Oe, Oi, Md, Py, Oh, Ps, On, Px, Pq, Mf, Pp, Pg, Mo, Pi, Pr, Pd, Mc, Np, Af, Nc.
36
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
Table 4
Core argument functions and their encoding in the different treebanks. The different object
labels for Czech have been introduced by us. The original function is simply Obj.
PDT 2 (Czech) TiGer (German) HunDep (Hungarian)
subject Sb SB SUBJ
nominal predicate Pnom PD PRED
object Obj1-7 OA, OA2, DA, OG OBJ, DAT
Table 5
Precision, recall, and F-score (LAS) for core grammatical functions marked by case. We omit
locative objects in Czech, and second accusative objects in German, due to their low frequency.
GOLD-M PRED-M NO-M
Czech freq prec rec f prec rec f prec rec f
subject 38,742 89.29 91.18 90.22 83.96 87.01 85.46 74.10 78.82 76.39
obj (acc) 21,137 92.50 93.35 92.93 85.25 83.01 84.12 73.42 72.02 72.71
predicate 6,478 89.07 87.14 88.09 88.24 86.00 87.11 82.34 78.19 80.21
obj (dat) 3,896 83.18 85.68 84.41 80.21 78.88 79.54 74.29 48.05 58.35
obj (instr) 1,579 71.38 66.50 68.85 67.74 62.51 65.02 58.93 35.53 44.33
obj (gen) 1,053 86.69 77.30 81.73 80.42 62.39 70.26 74.60 48.81 59.01
obj (nom) 167 57.63 40.72 47.72 56.97 29.34 38.74 48.67 32.93 39.29
GOLD-M PRED-M NO-M
German freq prec rec f prec rec f prec rec f
subject 45,670 95.11 96.05 95.58 89.95 91.23 90.59 88.32 89.86 89.08
obj (acc) 23,830 93.93 94.80 94.36 84.83 84.89 84.86 82.20 83.35 82.77
obj (dat) 3,864 89.56 87.73 88.64 79.17 64.44 71.05 77.09 50.78 61.23
predicate 2,732 78.07 73.35 75.64 75.80 72.91 74.33 76.20 71.01 73.51
obj (gen) 155 80.25 41.93 55.08 60.66 23.87 34.26 52.94 17.42 26.21
GOLD-M PRED-M NO-M
Hungarian freq prec rec f prec rec f prec rec f
subject 11,816 88.34 91.57 89.93 84.96 88.15 86.53 64.58 66.44 65.50
obj (acc) 9,326 93.63 94.22 93.92 92.36 92.70 92.53 66.23 63.86 65.03
obj (dat) 1,254 80.55 76.95 78.71 75.57 71.53 73.49 58.36 30.62 40.17
predicate 941 81.05 75.45 78.15 77.39 72.37 74.79 72.49 71.41 71.95
extent the parser is able to use information from lexicalization and configurational
information (Seeker and Kuhn 2011). Results for Czech and Hungarian are lower in
the NO-M models. They improve by large margins when switching to predicted mor-
phology. Czech accusative objects improve from 72.71% F-score to 84.12% F-score in
the PRED-M model. In Hungarian, the F-scores for dative objects improve by over 33
percentage points to 73.49% F-score when switching to the PRED-M model. In contrast,
although all the scores improve for German, improvements are generally low when
switching from the NO-M to the PRED-M model. The biggest improvement happens
37
Computational Linguistics Volume 39, Number 1
for dative objects, which increase by about 10 percentage points, but for subjects, the
improvement is just over one percentage point. This is in line with the general idea that
German is a borderline case between morphologically poor configurational languages
like English and morphologically rich non-configurational languages like Czech or
Hungarian. We already saw this general trend in Table 3, but the effect is much larger
if we consider those functions that are directly marked by morphological means in
the language.
If we now turn to the GOLD-M models, we see that in general, German and Czech
benefit more from the gold standard morphological annotation than Hungarian. Know-
ing that Hungarian does not have much form syncretism in its inflectional paradigms,
this is not really surprising. There is, however, still a gain of information because
the effect of the wrong POS tags in Hungarian is eliminated in the GOLD-M model.
An effect that comes out very clearly is the improvement for subjects and accusative
objects for Czech and German when moving from predicted to gold morphology,
because the typical syncretism between nominative and accusative in the neuter gen-
der in Indo-European languages (cf. Table 1) is correctly disambiguated: Comparing
the performance on subjects (marked by nominative case) and accusative objects, we
see a considerable improvement between 5 percentage points for Czech subjects and
almost 10 percentage points for German accusative objects when switching to gold
morphology. This improvement does not happen for Hungarian, where there is no such
syncretism. The gold morphology acts as an oracle here and circumvents the ambiguity
problem that a pipeline approach to predicting morphological information prior to
parsing has.
Another interesting observation related to the way the parser works is that for all
languages, predictions are less accurate for the less frequent functions. The general
order for all three languages from most frequent to least frequent is subjects > accusative
objects > predicates/dative objects > instrumental/genitive objects. For all languages, the
parser?s quality of annotation follows this ordering. This effect comes from the statistical
nature of the parsing system, which will in case of doubt resort to the more frequent
function. A clear sign is that for rare objects, the precision is always higher than the
recall. As an example, notice the performance of the parsing models on dative and
genitive objects. The parser annotates genitive objects if it has strong evidence, hence
the high precision, but it frequently fails to find it in the first place, hence the low recall.
Because the NO-M models do not have morphological information, they can only rely
on lexicalization and contextual information to determine the correct grammatical func-
tion. We can see this ranking in all the models regardless of the amount of morphological
information available, although the differences are much smaller for the more informed
models.
Finally, we see that the benefit from morphological information is comparatively
low for nominal predicates. It seems that the non-morphological context already pro-
vides much useful information (e.g., the copular verbs).
5.3 Analysis of Confusion Errors
We now ask ourselves if the parser utilizes the morphological information, in our case
the case morphology, correctly. In principle, there are two possible scenarios: (1) the
feature model of the parser does not integrate the morphological annotation in a useful
way, so that the parser has difficulties learning the association between case values and
the grammatical functions; (2) There is nothing wrong with the feature model, but the
38
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
morphological annotation is not good enough and causes problems because the parser
gets incorrect information in the features.
To answer this question, we examine the confusion errors made by the parser.
If the parser uses morphological information correctly, we expect it to confuse labels
that can all be signaled by the same case value. For example, if the parser learns the
association between nominative and subject/predicate properly, we would still expect
it to make errors in confusing these two functions. Because the mapping between case
and grammatical function is one-to-many, knowing the case value reduces the number
of possible functions but the final decision between these functions must be made by
non-morphological information. The effect should be strongest in the GOLD-M models
because the morphological information is correctly disambiguated. Consequently, we
expect the same results for the PRED-M models blurred by additional errors introduced
by an imperfect morphological prediction. If, however, the parser does not learn the
mapping or has no access to morphological information, we expect confusion errors all
across the case paradigms.
To start with the last hypothesis, we examine the confusion errors with subjects
made by the parser using the NO-M models. Subjects are marked by nominative case in
all three languages, with Czech allowing for dative and genitive subjects under special
circumstances. The NO-M models do not have access to morphological information
and should therefore mix up functions regardless of the case value that would usually
distinguish them. Table 6 shows the top five confusion errors made by the NO-M models
on the subject function. The values are split for correct and incorrect head selection to
tease apart simple label classification errors from errors involving label classification
and attachment.
Table 6
Top five functions with which subjects were confused when parsing with the NO-M models.
M marks a coordinated function in Czech.
Czech German
NO-M correct head wrong head NO-M correct head wrong head
rank label freq label freq rank label freq label freq
1 Obj4 4,996 Atr 2,644 1 OA 2,680 OA 1,498
2 Pnom 1,261 Obj4 981 2 PD 776 NK 906
3 Adv 811 Sb M 948 3 DA 458 DA 431
4 Obj3 752 Adv 273 4 EP 301 AG 313
5 Obj7 380 Obj M 245 5 MO 219 CJ 296
Hungarian
NO-M correct head wrong head
rank label freq label freq
1 OBL 3,029 ATT 1,116
2 OBJ 1,505 Exd 574
3 PRED 250 COORD 313
4 ATT 185 OBL 311
5 DAT 152 OBJ 139
39
Computational Linguistics Volume 39, Number 1
The results in Table 6 confirm the expectation that confusion errors appear regard-
less of the case value involved, which is no surprise given that the models do not have
access to morphological information: For Czech, when the head was chosen correctly,
Obj4, Obj3, and Obj7 (accusative, dative, and instrumental objects, respectively) are all
signaled by a different case value and their confusion rates follow their frequency in
the data. Pnom (nominal predicates) are expected because they are also signaled by
nominative case as are subjects. If the head was chosen incorrectly, the parser assigns
Obj4 and coordinated subjects and objects (Sb M, Obj M). Adverbial (Adv) and attribu-
tive functions (Atr) are expected as they mark adjunct functions that can be filled by
nominal elements. For German, we see confusions with the object functions (accusative
OA and dative objects DA), predicates (PD), and the EP function marking expletive
pronouns in subject position. Both are marked by nominative case. Furthermore, the
parser makes confusion errors with MO, NK, and AG, which are the three adjunct
functions that can be filled by nominal elements (e.g., AG marks genitive adjuncts).
CJ finally marks coordinated elements, which is an expected error if the head was
chosen incorrectly, but, unlike in the Czech treebank, we cannot tell by the coordination
label the particular function the element would have if it were not coordinated. In
Hungarian, we also have errors across the board, with argument functions not marked
by nominative case (accusative objects OBJ, dative objects DAT), the predicate function
PRED, and all types of adjuncts (ATT [attributives] and OBL [obliques]). Obliques are
especially interesting in Hungarian because the language has only a small number of
prepositions. Most oblique adjunct functions are realized by a particular case (hence
the about 20 different case values), which for a parsing model using no morphologi-
cal information makes it rather difficult to distinguish them from the core argument
functions. In summary, we find the expected picture of confusion errors across the case
paradigms.
Turning now to the GOLD-M models, we can test whether the parser is able to
learn the mapping between case and its associated functions. If so, we expect confusion
errors with functions that are all compatible with the case value of the correct function.
Table 7 shows the top five confusion errors that the GOLD-M models made on the subject
function. Here, we see a completely different picture compared with the NO-M model
errors in Table 6. In all three languages, we find?regardless if the head is correct or
not?confusions only with functions that are compatible with the nominative case. In
Czech, subjects are mostly confused with predicates (Pnom) and coordinated subjects
(Sb M). ExD marks suspended nodes moved because of an elliptical constructions. The
label does not tell whether the node would be a subject with regard to the empty
node but it may be, so it is compatible with nominative case. Atr between nominal
elements may mark close appositions like the one in Figure 1, which would be marked
as nominative by default. ObjX marks objects with no annotated case value (mostly for
foreign words). Of all the functions, only Obj4 cannot be signaled by nominative case.
If one checks those 69 cases, only 22 are annotated with accusative case in the gold
standard, the rest consist mostly of various, high-frequent numerals in neuter gender
and quantifiers, most of which are ambiguous between nominative and accusative. In
these cases, lexicalization seems to overrule the case feature. We get the same picture
for German and Hungarian, both models making errors that are compatible with the
nominative case value. Of the 112 errors with accusative objects (OA) in German, only 36
have the correct case value in the gold standard. Unlike in the Czech and the Hungarian
treebank, the morphological annotation in TiGer contains a considerable number of
errors. We then conclude that for subjects, the parser indeed has no problem learning
that subjects are marked by nominative case.
40
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
Table 7
Top five functions with which subjects were confused when parsing with the GOLD-M models.
M marks a coordinated function in Czech.
Czech German
GOLD-M correct head wrong head GOLD-M correct head wrong head
rank label freq label freq rank label freq label freq
1 Pnom 583 Sb M 1,142 1 PD 773 NK 555
2 ObjX 102 Atr 711 2 EP 323 CJ 245
3 Adv 102 ExD M 162 3 MO 117 PNC 139
4 Obj4 69 ExD 145 4 OA 112 PD 129
5 ExD 45 Pnom 65 5 PH 96 APP 127
Hungarian
GOLD-M correct head wrong head
rank label freq label freq
1 PRED 264 ATT 678
2 Exd 102 Exd 494
3 OBL 94 COORD 249
4 ATT 90 NE 32
5 OBJ 50 DET 22
Next, we examine the accusative objects and compare the performance of the
GOLD-M models with their respective PRED-M counterparts to assess the effect of
predicted morphological information. Table 8 shows the confusion errors for the ac-
cusative objects. On the left, the GOLD-M errors are shown; on the right we see the
PRED-M errors. For the GOLD-M models, the picture is basically the same as with the
subjects, with the small exception that all three languages show confusion with subjects
under the top five.18 Although the effect is not strong, it shows that the statistical
model can sometimes overrule the morphological features even for the gold standard
morphology.
The most interesting effect, however, happens when switching to predicted mor-
phological information. The overall number of errors increases, but the biggest in-
crease occurs for subjects in German (SB) and in Czech (Sb), although the same is not
observable in Hungarian (SUBJ). Of the 2,945 confusion errors in Czech, where the
PRED-M model incorrectly predicts an accusative object, 891 have been classified as
accusative despite being nominative in the gold standard and 1,505 have been classified
as nominative although being accusative. If we check the gender of these instances, we
find the overwhelming majority to be neuter, feminine, or masculine inanimate, exactly
those genders whose inflection paradigms show syncretism between nominative and
accusative forms. We find the same effect in the German errors. The syncretism in
the two languages causes the automatic morphological analyzers to confuse these case
18 The AuxT label in the Czech errors is used to mark certain kinds of reflexive pronouns, which can be in
accusative or dative case. The criterion for deciding whether a reflexive pronoun is labeled AuxT or Obj4
(i.e., accusative object) is whether the governing verb denotes a conscious or unconscious action. This is a
very tough criterion to learn for a dependency parser. In any case, however, AuxT is perfectly compatible
with accusative case.
41
Computational Linguistics Volume 39, Number 1
Table 8
Top five functions with which accusative objects were confused when parsing with the gold
(left) and predicted (right) morphology models. M marks a coordinated function in Czech.
Czech
GOLD-M correct head wrong head PRED-M correct head wrong head
rank label freq label freq rank label freq label freq
1 Adv 274 Obj M 750 1 Sb 2,354 Atr 687
2 AuxT 270 Atr 172 2 Adv 262 Obj M 660
3 Sb 69 ExD M 67 3 AuxT 256 Sb 594
4 ExD 34 Adv 65 4 Obj3 137 Sb M 108
5 AuxR 28 Atv 53 5 Obj2 109 ExD M 94
German
GOLD-M correct head wrong head PRED-M correct head wrong head
rank label freq label freq rank label freq label freq
1 MO 283 NK 357 1 SB 2,176 SB 1,329
2 SB 112 CJ 191 2 DA 610 NK 606
3 DA 55 SB 121 3 MO 308 CJ 365
4 CJ 43 APP 97 4 CJ 46 AG 137
5 EP 25 MO 55 5 EP 40 APP 136
Hungarian
GOLD-M correct head wrong head PRED-M correct head wrong head
rank label freq label freq rank label freq label freq
1 OBL 90 COORD 119 1 OBL 119 COORD 140
2 ATT 60 Exd 81 2 SUBJ 86 Exd 111
3 SUBJ 50 ATT 44 3 ATT 65 ATT 78
4 Exd 23 OBL 18 4 Exd 19 ROOT 18
5 MODE 14 ROOT 13 5 MODE 16 OBL 15
values more often, which subsequently leads to errors in the parser due to the pipeline
architecture. That the parser so frequently falls for incorrect annotation is more proof
that it has learned the mapping between case and its associated grammatical functions.
As expected, we do not find this effect for Hungarian. As we discussed in Section 4, there
is almost no syncretism in the Hungarian case paradigm, which therefore does not lead
to this kind of error propagation. The slight increase in errors in Hungarian is instead
related to the POS errors and their influence on missing morphological information than
the quality of the predicted morphology itself.
For reasons of space and because it would not contribute anything new to the
picture, we will not go into detail for the errors for the remaining grammatical functions.
We conclude that learning the morphological dependencies that hold for a language
(cf. the four types by Nichols [1986]) can be facilitated by a statistical model. When
presented with gold standard morphological information, the parser performance im-
proves considerably over the model without morphological information for all three
42
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
languages. The error analysis shows that the parser learns the mapping between case
and grammatical function, which also shows that the feature model of the parser
integrates the information in a useful way. In the more realistic scenario using pre-
dicted morphology, however, the parser starts making more mistakes for Czech and
German that are caused by errors of the automatic morphological predictors, which
are propagated through the pipeline model. This effect does not occur for Hungarian.
The syncretism in the inflectional paradigms in Czech and German makes the task of
learning the morpho-syntactic rules of a language much more difficult for a statistical
parser in a pipeline architecture. With a high amount of syncretism, it is simply not
sensible to fully disambiguate certain morphological properties of a word (e.g., case)
without taking the syntactic context into account.
6. Case as a Filter
From Experiment 1 we learned that one of the problems when parsing morphologi-
cally rich languages like Czech and German is the propagation of annotation errors
in the processing pipeline and the unreliable morphological information. The problem
is that the parser learns a mapping between case values and grammatical functions
but the predicted morphology delivers the wrong case value. As a solution to this
problem, Lee, Naradowsky, and Smith (2011) have proposed a joint architecture where
the morphological information is predicted simultaneously with the syntactic structure,
so that both processes can inform and influence each other. This puts morphological
prediction and syntactic analysis on the same level. We choose a different approach
here: We keep the basic pipeline architecture, because it works very efficiently. We
support the parser, however, with constraints that model the possibly underspecified
morphological restrictions grounded in the surface forms of the words. Especially for
the core argument functions, a morphological feature like case first and foremost serves
as a morpho-syntactic means to support the syntactic analysis by overtly marking
syntactic relations and thus reducing the choice for the parser. For example, if a word
form morphologically cannot be accusative, the parser should not consider grammatical
functions that are signaled by accusative in the language. Case acts here as a filter on
the available functions for the morphologically marked element. Interpreting the role
of case as a filter, we can use the case feature as a formal device to restrict the search
space of the parser. This is different from the joint model, where morphology and syntax
are predicted at the same time, because the parser will not fully disambiguate a token
with respect to its morphology if the syntactic context does not provide the necessary
information. Another thing that we learned from the first experiment is that although
the predicted morphology is not completely reliable, it is still much better than using
none at all, especially for Czech and Hungarian (see difference between PRED-M and
NO-M models in Table 5). In the following, we will therefore still use the predicted
morphology as features in the statistical model in combination with the filter. In this
architecture, the parser gets statistical information from the feature model to prefer a
particular analysis, but the constraints will block this option if it does not comply with
the morphological specification of the words. The parser then needs to choose a different
option.
In order to implement the constrained parser, we use a parsing approach by
Martins, Smith, and Xing (2009) using integer linear programming. It is related to
the Bohnet parser in the sense that it is also a graph-based approach, but it allows
us to elegantly augment the basic decoder with linguistically motivated constraints
43
Computational Linguistics Volume 39, Number 1
(Klenner 2007; Seeker et al 2010). ILP is a mathematical tool for optimizing linear
functions and was first used in dependency parsing by Riedel and Clarke (2006), who
performed experiments on Dutch using linguistically motivated constraints as we will
do. Martins, Smith, and Xing improved the formulation considerably so that the parser
would output well-formed dependency trees without the need for iterative solving. In
our ILP parser, we use the formulation by Martins, Smith, and Xing extended to labeled
dependency parsing. Like the Bohnet parser, the ILP parser consists of a decoder and
a statistical feature model. Whereas the feature model remains basically the same, the
decoder is implemented using ILP. The formulation represents every possible arc that
might appear in the parse tree as a binary variable (arc indicator), where 1 signals
the presence of the arc in the tree, and 0 signals its absence (see also Figure 3). Each
such arc indicator variable is weighted by a score assigned by the statistical model
that is learned from a treebank. During decoding,19 the parser searches for the highest
scoring combination of arcs that also fulfills the global tree constraints as well as any
other global constraints that may be added to the equations to model, for instance,
linguistic knowledge. The tree constraints ensure that every word in the tree has exactly
one head and that there are no cycles in the tree. Martins, Smith, and Xing use the
single commodity flow formulation by Magnanti and Wolsey (1995) to enforce the tree
structure. The idea is that the root node sends N units of flow through the tree (with N
being the number of words in the sentence) and every node in the tree consumes one
unit. If every node consumes exactly one unit of flow and every node can have only one
parent node, then the tree must be connected and acyclic.
max
?
h?H
?
d?N
?
l?L
?ldha
l
dh (1)
?
h?H
?
l?L
aldh = 1 ?d ? N (2)
|N|
?
l?L
aldh ? fdh ?d ? N,?h ? H (3)
?
h?H
fdh ?
?
g?N
fgd = 1 ?d ? N (4)
?
d?N
fdRoot = |N| (5)
a ? {0, 1}, f ? Z (6)
Let N be the set of words in a sentence, H = N ? {Root} is the set of words plus an
artificial root node, and L is the set of function labels. For every sentence, Equations (1)?
(6) constitute the equation system that the constraint solver has to solve in order to find
the highest scoring dependency tree. Equation (1) shows the objective function, which
is simply the sum over all binary arc indicator variables a ? A = N ? H ? L weighted
by their respective score ?. Equation (2) restricts for every dependent d the number
of incoming arcs to exactly one. It thus makes sure that every word will end up with
19 We use the GUROBI constraint solver: www.gurobi.com, version 4.0.
44
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
d\h 0 1 2 3
1
2
3
a1,0
a2,0
a3,0
a1,1
a2,1
a3,1
a1,2
a2,2
a3,2
a1,3
a2,3
a3,3
d\h 0 1 2 3
1
2
3
f1,0
f2,0
f3,0
f1,1
f2,1
f3,1
f1,2
f2,2
f3,2
f1,3
f2,3
f3,3
arc indicators flow variables
Root John loves Mary
0 1 2 3
Head candidates
for John
a1,0 a1,2
a1,3
Equation (3): flow link (for each pair <a,f>)
= 1
Equation (2):
single head
= 1
Equation (4):
flow consumption
Equation (5): root flow
f1,0 + f2,0 + f3,0 = 3
Figure 3
Schematic description of the unlabeled first-order model for the example sentence John loves
Mary. The constraints are shown for the dependent (d) John. There are three head (h) candidates,
from which the decoder needs to choose one because of the single head constraint (Equation (2)).
Equations (3), (4), and (5) show as an example how the flow constraints are applied to ensure a
tree structure. Equation (3) links each arc indicator to one flow variable making sure that only
active arcs (those that are set to 1) carry flow > 0. Equation (5) sends three units of flow from the
root, one for each other token in the tree. Equation (4) finally forces the flow difference between
the incoming arc (horizontal part) of each node (except root) and the flow on all outgoing arcs
(vertical part) to be exactly 1, thus making sure that each node consumes one unit of flow. To find
the optimal tree, the sum over the weights of all arc indicators that are set to one is maximized.
exactly one head. Equations (3)?(5) model the single commodity flow. A set of integer
variables F = N ? H is introduced to represent the flow on each arc. Equation (3) links
every flow variable that represents the flow between two nodes to the set of arc indicator
variables that can connect these two nodes. If there is no arc between the two nodes (all
indicator variables are 0), the flow must be 0 as well. If one arc indicator is 1, then
the flow variable can take any integer value between 0 and |N|. Equation (4) enforces
the consumption of one unit of flow at each node by requiring the difference between
incoming and outgoing arcs to be exactly one. Equation (5) finally sets the amount of
flow that is sent by the artificial root node to the number of words in the sentence. Note
that this does not force the tree structure to be single-rooted, because the artificial root
node can have multiple dependents. It can be done by an additional constraint that sets
the number of dependents for the root node to one. Figure 3 shows an example for the
basic formulation.
Martins, Smith, and Xing (2009) propose several extensions to the basic model; for
example, second-order features, which introduce new variables for each combination
45
Computational Linguistics Volume 39, Number 1
of two arc indicator variables into the ILP model. For our parser, we implemented
the second-order features that they call all grandchildren and all siblings. They also state
that the use of second-order features in the decoder renders exact decoding intractable,
and they propose several techniques to reduce the complexity, which we also apply
to our parser: (1) Before parsing, the trees are pruned by choosing for each token the
ten most probable heads using a linear classifier that is not restricted by structural
requirements, and (2) The integer constraint is dropped, such that the variables can
now take values between 0 and 1 instead of either 0 or 1. The dropping of the integer
constraint can lead to inexact solutions with fractional values. To arrive at a well-formed
dependency tree, we then use the first-order model in Equations (1)?(6) to get the
maximum spanning tree, this time using the fractional values from the actual solution
as arc weights. Two other techniques that we apply are related to the arc labels: (1) We
use an arc filter (Johansson and Nugues 2008) like the Bohnet parser, which blocks
edges that did not appear in the training data based on the POS tags of the dependent
and the head, and the label, and (2) We do not include labels in the second-order
variables.
The feature set of the ILP parser is similar to but not identical to one in the Bohnet
parser. The ILP parser uses loss-augmented MIRA for training (Taskar et al 2005),
which is similar to the MIRA used in the Bohnet parser. We set the number of training
iterations to 10 as well.
6.1 Morpho-Syntax as Constraints
Using case as a filter for the decoder requires an underspecified symbolic representa-
tion of morphological information that we can use to define constraints. This allows
us to have an exact representation of syncretism controlling the search space of the
parser. The case features of a word are represented in the ILP decoder as a set of
binary variables M for which 1 signals the presence of a particular value and 0 signals
its absence. For Hungarian, we only model the different case values, which leads to
one binary variable for each of the values. For Czech and German, we also include
the gender and the number features which then gives, for each case marked word,
a binary variable for every combination of the case, number, and gender values. The
values of the morphological indicator variables are specified by annotating the data
sets with underspecified morphological descriptions that are obtained from finite-state
morphological analyzers.20 If a certain feature value is excluded by the analyzers, the
value of the indicator variable for this feature is fixed at 0, which then means that the
decoder cannot set it to 1. This way, all morphological values that cannot be marked
by the form of the token (according to the morphological analyzer) are blocked and
thereby also all parser solutions that depend on them. Words unknown to the analyzers
are left completely underspecified so that each of the possible values is allowed (none
of the variables are fixed at 0). The symbolic, grammar-based pre-annotations thus set
some of the morphological indicator variables to 0 where the word form gives enough
information while leaving other variables open to be set by the parser, which can use
syntactic context to make a more informed decision.
We now present three types of constraints that model the morpho-syntactic inter-
actions in the three languages. Their purpose is to help the parser during decoding
20 Czech: http://ufal.mff.cuni.cz/pdt/Morphology and Tagging/Morphology/index.html; German:
Schiller (1994); Hungarian: Tro?n et al (2006).
46
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
to find a linguistically plausible solution. They are inspired by the types of morpho-
syntactic interaction that Nichols (1986) describes and guide the parser by enforcing
them globally in the final structure. It is important to emphasize that these constraints
do not interact with or influence the statistical feature model of the parser. They are
applied during decoding when the parser is searching for the highest-scoring tree and
prevent solutions that violate the constraints.
The first type of constraints that we apply explicitly formulates the mapping be-
tween a function label and the case value that it requires. Equation (7) shows an example
of a case licensing constraint for the DAT label in Hungarian. A dependent d cannot be
attached to a head with label DAT if its morphological indicator variable for dative case
(mdatd ) is zero.
?d :
?
h?H
aDATdh ? mdatd (7)
The second type of constraint models the morphological agreement between de-
pendents and their heads in noun phrases (Equations (8)?(9)), for instance, determiners
and adjectives with their head noun in the noun phrases in Czech and German. In the
treebanks, the relation is marked by NK for German and Atr for Czech.21 The constraints
set the morphological indicators for an adjective and a noun in the following relation:
As long as there is no arc (aNKdh is 0) between the adjective (d) and the noun (h), the two
constraints allow for any value in the morphological indicator variables of both words.
If the arc is established (aNKdh is set to 1), the two constraints form an equivalence forcing
all the morphological indicators to agree on their value (i.e., to be both 1 or both 0). We
additionally require every word to have at least one morphological indicator variable
set to 1. Thus, if there is no solution to the equivalence the arc between the adjective and
the noun cannot be established with this function label.
mdat?pl?femh ? m
dat?pl?fem
d + 1 ? a
NK
dh (8)
mdat?pl?femh ? m
dat?pl?fem
d ? 1 + a
NK
dh (9)
For the third type, Equation (10) shows a constraint that was already proposed
by Riedel and Clarke (2006). It models label uniqueness by forcing label l to appear
at most once on all the dependents of a head (h). Due to the design of the decoder
following Carreras (2007), the Bohnet parser has no means of making sure that a
particular function label is annotated at most once per head. Table 9 shows the number
of times a grammatical function occurs more than once per head in the treebank (TRBK)
and how often it was annotated by the models in the previous experiment. Although
doubly annotated argument functions almost never appear in the treebank, the parser
21 In German and mostly also in Czech, if an adjective is attached to a noun by NK (or Atr), they stand in
an agreement relation. This fortunate circumstance allows us to bind the agreement constraint to these
function labels (and to the involved POS tags). In a (very) small number of cases in the Czech treebank,
however, an adjective is attached to a noun by Atr but there is no agreement. This happens, for example,
if the adjective is actually the head of another noun phrase that stands in attributive relation (Atr) to the
noun. The Atr label was not meant to mark agreement relations, it just happens to coincide for most of
the cases. But it might be worth considering whether morpho-syntactic relations like agreement should
be represented explicitly in syntactic treebanks.
47
Computational Linguistics Volume 39, Number 1
frequently annotates them because it has no way of checking whether the function has
already been annotated (see also Khmylko, Foth, and Menzel [2009]).
?h?l :
?
d?N
aldh ? 1 (10)
The global constraint in Equation (10) allows us to restrict the number of argument
functions and thus implements a very conservative version of subcategorization frame
with which we do not risk coverage problems caused by too restrictive verb frames.
For each language, we automatically counted the number of times a function label
occurred on the direct dependents of each node in the treebank. Labels that occurred
more than once per head with a very low frequency were still counted as appearing
at most once if our linguistic intuition would predict that (see, e.g., German subjects
in Table 9). For each function label l in these lists, the constraint in Equation (10) was
applied.
Table 9
Number of times a core grammatical function was annotated more than once in the treebank
(TRBK) by the model using gold morphology (GOLD-M), and by the model using predicted
morphology (PRED-M).
Czech German Hungarian
TRBK GOLD-M PRED-M TRBK GOLD-M PRED-M TRBK GOLD-M PRED-M
subjects 0 772 1,723 44 1,170 2,403 0 586 670
predicates 7 174 190 6 92 108 1 17 19
obj (dat.) 0 28 46 0 33 46 0 9 5
obj (acc.) 22 284 602 2 364 912 0 182 189
Each individual constraint already reduces the choices that the parser has available
for the syntactic structure. They exclude additional incorrect analyses, however, by
interaction. Figure 4 illustrates the interaction between the three constraints for the
German sentence den Ma?dchen helfen Frauen meaning women help the girls. Each individ-
ual word displays a high degree of syncretism. But when the syntactic structure is de-
cided, many options mutually exclude each other. Constraints (8) and (9) disambiguate
den Ma?dchen for dative plural feminine. The case licensing (Constraint (7)) then restricts
the labels for Ma?dchen to dative object (DA), and Constraint (10) ensures uniqueness
by restricting the choice for Frauen. The parser now has to decide whether Frauen is
subject, accusative object, or something else completely. The constraints are applied on-
line during the decoding process. If the statistical model would strongly prefer Ma?dchen
to be accusative object, the parser could label it with OA. In that case, however, it would
not be able to establish the NK label between den and Ma?dchen, because the agreement
constraint would be violated. So, the constraints filter out incorrect solutions but the
decoder is still driven by the statistical model.
6.2 Experiment 2
In the second experiment, we now apply the ILP parser to the same data sets that we
used in the first experiment, again with a five-fold cross-annotation. We trained two
48
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
?
? ?
?
den
the
??????acc-sg-masc
??????dat-pl-masc
dat-pl-fem
?????dat-pl-neut
Ma?dchen
girls
??????nom-pl-fem
?????acc-pl-fem
dat-pl-fem
?????gen-pl-fem
helfen
help
-
-
-
-
Frauen
women
nom-pl-fem
acc-pl-fem
dat-pl-fem
gen-pl-fem
SB/?DA/OA/...?SB/DA/?OA
NK
?Women help the girls?
Figure 4
Constraint interaction for the German sentence den Ma?dchen helfen Frauen meaning women help
the girls.
Table 10
Overall performance of the Bohnet parser and the ILP parser on the five-fold cross annotation
for every language. All results in percent. LAS = labeled attachment score, UAS = unlabeled
attachment score. Results for German and Hungarian are without punctuation.
Czech German Hungarian
model LAS UAS LAS UAS LAS UAS
GOLD-M 82.49 88.61 91.26 93.20 86.70 89.70
PRED-M 81.41 88.13 89.61 92.18 84.33 88.02
NO-M 79.00 86.89 89.18 91.97 78.04 86.02
ILP NO-C 81.69 88.09 89.30 91.98 84.01 87.12
ILP C 81.91 88.18 89.93 92.25 84.35 87.39
models for each language, one using the constraints (c) and one without the constraints
(no-c). In both cases, we used the predicted morphology in the feature set. Table 10
shows the parsing results for the ILP parsing models in terms of LAS and UAS in
comparison to the results of the Bohnet parser (repeated from Table 3). Both ILP models
should be compared to the PRED-M model because they have the most similar feature
sets. As can be seen from the results, the ILP parser without constraints performs overall
slightly worse than the Bohnet parser and the ILP parser using constraints performs
overall slightly better or equal. This shows that both parsers perform on a similar
level. The differences between the Czech and the German models (ILP C vs. PRED-M)
are statistically significant.22 The interesting results, however, occur for the argument
functions.
Table 11 shows the performance of the unconstrained (no-c) and constrained (c) ILP
models and the PRED-M models of the Bohnet parser on the argument functions. Again,
22 According to a two-tailed t-test for related samples with ? = 0.05.
49
Computational Linguistics Volume 39, Number 1
Table 11
Parsing results for the unconstrained (NO-C) and the constrained (C) ILP models, and the
Bohnet parser in terms of F-score (LAS) for core grammatical functions marked by case.
We omit locative objects in Czech, and second accusative objects in German because of their
extremely low frequency. ? Statistically significant when comparing the performance on a
grammatical function for the C model to the PRED-M model (? = 0.05, two-tailed t-test for
related samples).
Czech German Hungarian
NO-C C PRED-M NO-C C PRED-M NO-C C PRED-M
subject 85.41 87.23* 85.46 90.02 92.91* 90.59 85.05 87.67* 86.53
predicate 87.13 90.09* 87.11 72.86 80.70* 74.33 74.16 78.88* 74.79
obj (nom) 47.48 53.19* 38.74 ? ? ? ? ? ?
obj (gen) 70.15 72.54 70.27 31.41 42.98 34.26 ? ? ?
obj (dat) 79.99 80.42 79.54 65.21 77.78* 71.05 75.33 77.92* 73.49
obj (acc) 84.27 86.79* 84.12 83.74 87.96* 84.86 91.96 93.21* 92.53
obj (instr) 67.36 68.76 65.02 ? ? ? ? ? ?
all arg funcs 84.33 86.37* 84.21 86.27 90.11* 87.24 86.87 89.04* 87.78
all other 81.37 81.37 81.05 89.79 89.88 89.98 82.73 82.86 83.43
we only evaluated those tokens that actually carry case morphology, as we did in the
first experiment. For each language, the best results are in boldface. In addition to the
results for the different argument functions, a total score is computed over all argument
functions (all arg funcs) and another is computed over all tokens that are not included
in the first score (all other). The latter illustrates the performance of the parsing models
on the functions that are not marked by case morphology.
For each language, we get the same basic picture: Although the unconstrained ILP
model performs slightly worse than (German, Hungarian) or equally well as (Czech) the
PRED-M model of the Bohnet parser, the constrained ILP model clearly outperforms both
on the argument functions. On each of them, the constrained ILP model improves over
the other two models, raising the score by 1 percentage point for (for example) subjects
in Hungarian up to 7 percentage points on dative objects in German (compared with
the PRED-M model). What we can see is that, in general, the improvements seem to be
higher on the more infrequent arguments like dative objects and predicates than on the
frequent arguments like subject or accusative object. It is not the case, however, that the
performance of one of the infrequent functions suddenly surpasses the performance of
a more frequent function. Those two effects are to be expected because the ILP parser is
still a data-driven parser. The constraints support it by excluding morpho-syntactically
incorrect analyses but they do not resolve ambiguous cases, which are still decided by
the statistical model.
The main work done by the constraints is to establish interactions between parts
of the parse graph that are not represented in the statistical model. Because the graph-
based approach (in both parsers) factors the graph into first- (and some second-) order
arcs, and because both decoders do not use second-order features with more than
one label, a constraint like label uniqueness (Equation (10)), which is not even directly
related to morphology, is impossible to learn for the statistical model. This is because
it never sees two sister dependents and their labels together and thus does not know if
it has already annotated the current function label. Applying the constraints during
the search makes it impossible for the parser to produce an output that does not
50
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
obey label uniqueness even though the statistical model does not have access to this
information.
It should be stressed that the ILP models in their statistical model still use the same
predicted and fully disambiguated morphological information from the pipeline archi-
tecture as the Bohnet parser. As we saw in the first experiment, using no morphological
information in the statistical model is very harmful to the performance on Czech and
Hungarian, though not so much for German.
One advantage of the proposed architecture is the fact that the ILP parser is still
mainly driven by the statistical model. Krivanek and Meurers (2011) compared a data-
driven, transition-based dependency parser (Nivre et al 2007b) and a constraint-based
dependency parser (Foth and Menzel 2006) on learner and newspaper corpora and
found that whereas the former is better on modifier functions (e.g., PP-attachment),
the latter performs better on argument functions. Their explanation is that where
the data-driven parser has access to lots of data and can pick up statistical effects
in the data like semantic or selectional preferences, the constraint-based parser has
access to deep lexical and grammatical information and is thus able to model argu-
ment structure in a better way. In the ILP parser, we can combine both strengths,
letting the statistical model learn preferences but forcing it via constraints to obey hard
grammatical information. The last row in Table 11 shows that compared to the Bohnet
parser, the ILP models perform comparably well on non-argument functions (maybe
with the exception of Hungarian, where the difference is a bit more distinct). At the
same time, they perform clearly better on the argument functions due to the linguistic
constraints.
Foth and Menzel (2006) (see also Khmylko, Foth, and Menzel 2009) are further
relevant to this work in the sense that our architecture mirrors their approach. In
their work, they use a highly sophisticated rule-based parser, which they equip with
statistical components that model various subtasks like pos tagging, supertagging, or
PP-attachment. They demonstrate that a rule-based parser can benefit from statistical
models that model preferences rather than hard constraints. Our approach comes from
the other side: We equip a statistical parser with hard rules that ensure the linguistic
plausibility of the output. Both approaches prove that proper statistical models and
linguistically motivated rules can work well together to produce syntactic structures of
high quality.
One advantage of applying constraints over the argument structure is that we can
give a guarantee that certain ill-formed trees will not be produced by the parser. For
example, the constraints make sure that there will not be any parser output where there
are two subjects annotated for the same verb. Although this does not mean that the
subject will be the correct one, the formal requirement of not having two subjects is met,
which we believe can be helpful for subsequent semantic analysis/interpretation or, for
example, relation extraction. In the same sense, the constraints will also ensure that mor-
phological agreement and case licensing is correct to the degree that the morphological
analyzer was correct. This feature thus implements a tentative notion of grammaticality
for the statistical model.
7. Conclusion
In this article, we investigated the performance of the state-of-the-art statistical de-
pendency parser by Bohnet (2010) on three morphologically rich languages?Czech,
German, and Hungarian. We concentrated on the core grammatical functions (subject,
51
Computational Linguistics Volume 39, Number 1
object, etc.) that are marked by case morphology in each of the three languages. Our first
experiment shows that apart from small frequency effects due to the statistical nature of
the parser, learning the mapping between a case value and the grammatical functions
signaled by it is not a problem for the parser. We also see, however, that the pipeline
approach, where morphological information is fully disambiguated before being used
by the parser as features in the statistical model, is susceptible to error propagation for
languages that show syncretism in their morphological paradigms. Although we can
show that parsing Hungarian, an agglutinating language without major syncretism in
the case paradigm, is not affected by these problems, parsing the fusional languages
Czech and German frequently suffers from propagated errors due to ambiguous case
morphology. Furthermore, although the predicted morphological information does
not help very much in German, it contributes very much when parsing Czech and
Hungarian, even if it is not completely reliable.
Handling syncretism requires changes in the processing architecture and the rep-
resentation of morphological information. We proposed an augmented pipeline where
the parsing model is restricted by possibly underspecified, morpho-syntactic constraints
exploiting grammatical knowledge about the morphological marking regimes and the
inflectional paradigms. Although the statistical parsing model provides scores for local
substructures during decoding, the symbolic constraints are applied globally to the
entire output structure. A morpho-syntactic feature like case is interpreted as a filter
on the parser output. By modeling phenomena like case-function mapping, agreement,
and function uniqueness as constraints in an ILP decoder for dependency parsing,
we showed in a second experiment that supporting a statistical model with these
constraints helps avoiding parsing errors due to incorrect morphological preprocess-
ing. The advantage of this approach is the combination of local statistical models
and globally enforced hard grammatical knowledge. Whereas some key aspects of the
grammatical structure are ensured by the linguistic knowledge (e.g., overtly marked
case morphology) the underlying data-driven model can still exploit statistical effects
to resolve the remaining ambiguity and model semantic preferences, which are difficult
to model with hard rules.
Morphologically rich languages pose various challenges to the standard parsing
approaches because of their different linguistic properties. As one of them, case systems
are a key device in these languages to encode argument structure and reside at the
brink between morphology and syntax. Paying attention to the role of case in statistical
parsing results in more appropriate models. Morphologically rich, however, is a wide cat-
egory and covers a wide range of languages. Taking the idea of linguistically informed
restrictions over data-driven system components may lead to further improvements on
other phenomena and for other languages.
Acknowledgments
The research reported in this article was
supported by the German Research
Foundation (DFG) in project D8 of SFB 732
Incremental Specification in Context. We would
like to thank Richa?rd Farkas and Veronika
Vincze at the University of Szeged for their
help with the Hungarian corpus and language;
Bernd Bohnet for the help with his parser;
and Anders Bjo?rkelund, Anett Diesner, and
Kyle Richardson for their comments on
earlier drafts of this work.
References
Blake, Barry J. 2001. Case. Cambridge
University Press, Cambridge, MA,
2nd edition.
Bo?hmova?, Alena, Jan Hajic?, Eva Hajic?ova?,
and Barbora Hladka?. 2000. The Prague
Dependency Treebank: A three-level
annotation scenario. In A. Abeille?, editor,
Treebanks: Building and Using Syntactically
Annotated Corpora. Kluwer Academic
Publishers, Amsterdam, chapter 1,
pages 103?127.
52
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
Bohnet, Bernd. 2009. Efficient parsing of
syntactic and semantic dependency
structures. In Proceedings of the 13th
Conference on Computational Natural
Language Learning: Shared Task,
volume 2007, pages 67?72, Boulder, CO.
Bohnet, Bernd. 2010. Very high accuracy
and fast dependency parsing is not
a contradiction. In Proceedings of the
23rd International Conference on
Computational Linguistics, pages 89?97,
Beijing.
Bohnet, Bernd. 2011. Comparing advanced
graph-based and transition-based
dependency. In Proceedings of the
International Conference on Dependency
Linguistics, pages 282?289, Barcelona.
Boyd, Adriane, Markus Dickinson, and
W. Detmar Meurers. 2008. On detecting
errors in dependency treebanks. Research
on Language and Computation, 6(2):113?137.
Brants, Sabine, Stefanie Dipper, Silvia
Hansen-Shirra, Wolfgang Lezius, and
George Smith. 2002. The TIGER treebank.
In Proceedings of the 1st Workshop on
Treebanks and Linguistic Theories,
20?21 September 2002, Sozopol,
Bulgaria, pages 24?41.
Bresnan, Joan. 2001. Lexical-Functional Syntax.
Blackwell Publishers, Oxford.
Buchholz, Sabine and Erwin Marsi. 2006.
CoNLL-X shared task on multilingual
dependency parsing. In Proceedings of the
10th Conference on Computational Natural
Language Learning, pages 149?164,
New York, NY.
Carreras, Xavier. 2007. Experiments with a
higher-order projective dependency parser.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning, pages 957?961, Prague.
Cohen, Shay B. and Noah A. Smith. 2007.
Joint morphological and syntactic
disambiguation. In Proceedings of the
2007 Joint Conference on Empirical
Methods in Natural Language Processing
and Computational Natural Language
Learning, pages 208?217, Prague.
Collins, Michael, Jan Hajic?, Lance Ramshaw,
and Christoph Tillmann. 1999. A statistical
parser for Czech. In Proceedings of the
37th Annual Meeting of the Association for
Computational Linguistics, pages 505?512,
College Park, MD.
Crammer, Koby, Ofer Dekel, Shai Shalev-
Shwartz, and Yoram Singer. 2003.
Online passive-aggressive algorithms.
In Proceedings of the 16th Annual
Conference on Neural Information Processing
Systems, volume 7, pages 1217?1224,
Cambridge, MA.
Csendes, Do?ra, Ja?nos Csirik, and Tibor
Gyimo?thy. 2004. The Szeged Corpus:
A POS tagged and syntactically annotated
Hungarian natural language corpus.
In Proceedings of the 5th International
Workshop on Linguistically Interpreted
Corpora, pages 19?23, Geneva.
Eisenberg, Peter. 2006. Grundriss der deutschen
Grammatik: Der Satz. J.B. Metzler, Stuttgart,
3rd edition.
Eisner, Jason. 1997. Bilexical grammars
and a cubic-time probabilistic parser.
In Proceedings of the 5th International
Conference on Parsing Technologies,
pages 54?65, Cambridge, MA.
Eryig?it, Gu?ls?en, Joakim Nivre, and Kemal
Oflazer. 2008. Dependency parsing of
Turkish. Computational Linguistics,
34(3):357?389.
Foth, Kilian A. and Wolfgang Menzel.
2006. Hybrid parsing: Using probabilistic
models as predictors for a symbolic parser.
In Proceedings of the 21st International
Conference on Computational Linguistics
and the 44th annual meeting of the ACL,
pages 321?328, Sidney.
Gesmundo, Andrea, James Henderson,
Paola Merlo, and Ivan Titov. 2009.
A latent variable model of synchronous
syntactic-semantic parsing for multiple
languages. In Proceedings of the 13th
Conference on Computational Natural
Language Learning: Shared Task,
pages 37?42, Boulder, CO.
Goldberg, Yoav and Michael Elhadad. 2010.
Easy first dependency parsing of modern
Hebrew. In Proceedings of the NAACL HLT
2010 First Workshop on Statistical Parsing
of Morphologically-Rich Languages,
pages 103?107, Los Angeles, CA.
Goldberg, Yoav and Reut Tsarfaty. 2008.
A single generative model for joint
morphological segmentation and syntactic
parsing. In Proceedings of the 46th Annual
Meeting of the Association for Computational
Linguistics, pages 371?379, Columbus, OH.
Hajic?, Jan, Massimiliano Ciaramita, Richard
Johansson, Daisuke Kawahara,
Maria Anto`nia Mart??, Llu??s Ma`rquez,
Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan Stepa?nek, Pavel Strana?k, Mihai
Surdeanu, Nianwen Xue, and Yi Zhang.
2009. The CoNLL-2009 shared task:
Syntactic and semantic dependencies in
multiple languages. In Proceedings of the
13th Conference on Computational Natural
53
Computational Linguistics Volume 39, Number 1
Language Learning: Shared Task, pages 1?18,
Boulder, CO.
Hajic?, Jan, Jarmila Panevova?, Eva Hajic?ova?,
Petr Sgall, Petr Pajas, Jan S?te?pa?nek, Jir???
Havelka, and Marie Mikulova?. 2006. Prague
Dependency Treebank 2.0, Linguistic Data
Consortium, Philadelphia, PA.
Hudson, Richard A. 1984. Word Grammar.
Basil Blackwell, Oxford.
Janda, Laura A. and Charles E. Townsend.
2000. Czech. Lincom Europa, Munich.
Johansson, Richard and Pierre Nugues. 2008.
Dependency-based syntactic-semantic
analysis with PropBank and NomBank.
In Proceedings of the 12th Conference on
Computational Natural Language Learning,
pages 183?187, Manchester.
Khmylko, Lidia, Kilian A. Foth, and
Wolfgang Menzel. 2009. Co-parsing with
competitive Models. In Proceedings of the
11th International Conference on Parsing
Technologies, pages 99?107, Paris.
Klenner, Manfred. 2007. Shallow dependency
labeling. In Proceedings of the ACL 2007 Demo
and Poster Sessions, pages 201?204, Prague.
Krivanek, Julia and W. Detmar Meurers.
2011. Comparing rule-based and
datadriven dependency parsing of learner
language. In Proceedings of the International
Conference on Dependency Linguistics,
pages 310?318, Barcelona.
Ku?bler, Sandra. 2008. The PaGe 2008 shared
task on parsing German. In Proceedings
of the Workshop on Parsing German,
pages 55?63, Morristown, NJ.
Lee, John, Jason Naradowsky, and David A.
Smith. 2011. A discriminative model for
joint morphological disambiguation and
dependency parsing. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics, pages 885?894,
Portland, OR.
Magnanti, Thomas and Laurence Wolsey.
1995. Optimal trees. Handbooks in
Operations Research and Management
Science, 7(April):503?615.
Martins, Andre? F. T., Noah A. Smith,
and Eric P. Xing. 2009. Concise integer
linear programming formulations for
dependency parsing. In Proceedings of the
Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint
Conference on Natural Language Processing
of the AFNLP, pages 342?350, Suntec.
Marton, Yuval, Nizar Habash, and
Owen Rambow. 2010. Improving Arabic
dependency parsing with lexical and
inflectional morphological features.
In Proceedings of the NAACL HLT 2010
First Workshop on Statistical Parsing of
Morphologically-Rich Languages,
pages 13?21, Los Angeles, CA.
McDonald, Ryan and Fernando Pereira.
2006. Online learning of approximate
dependency parsing algorithms. In
Proceedings of the 11th Conference of the
European Chapter of the Association for
Computational Linguistics, pages 81?88,
Trento.
McDonald, Ryan, Fernando Pereira,
Kiril Ribarov, and Jan Hajic?. 2005.
Non-projective dependency parsing
using spanning tree algorithms. In
Proceedings of the 2005 Conference on
Human Language Technology and Empirical
Methods in Natural Language Processing,
pages 523?530, Morristown, NJ.
Mel?c?uk, Igor. 1988. Dependency Syntax:
Theory and Practice. SUNY Series in
Linguistics. State University Press of
New York.
Nichols, Joanna. 1986. Head-marking and
dependent-marking grammar. Language,
62(1):56?119.
Nivre, Joakim, Johan Hall, Sandra Ku?bler,
Ryan McDonald, Jens Nilsson, Sebastian
Riedel, and Deniz Yuret. 2007a. The
CoNLL 2007 shared task on dependency
parsing. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural
Language Processing and Computational
Natural Language Learning, pages 915?932,
Prague.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Atanas Chanev, Gu?ls?en Eryig?it,
Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser:
A language-independent system for
data-driven dependency parsing.
Natural Language Engineering,
13(2):95?135.
Riedel, Sebastian and James Clarke. 2006.
Incremental integer linear programming
for non-projective dependency parsing.
In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing, pages 129?137, Sydney.
Schiehlen, Michael. 2004. Annotation
strategies for probabilistic parsing
in German. In Proceedings of the 20th
International Conference on Computational
Linguistics, pages 390?397, Geneva.
Schiller, Anne. 1994. Dmor - user?s guide.
Technical report, University of Stuttgart.
Seeker, Wolfgang and Jonas Kuhn. 2012.
Making ellipses explicit in dependency
conversion for a German treebank.
In Proceedings of the 8th International
54
Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing
Conference on Language Resources and
Evaluation, pages 3132?3139, Istanbul.
Seeker, Wolfgang and Jonas Kuhn. 2011.
On the role of explicit morphological
feature representation in syntactic
dependency parsing for German. In
Proceedings of the 12th International
Conference on Parsing Technologies,
pages 58?62, Dublin.
Seeker, Wolfgang, Ines Rehbein, Jonas Kuhn,
and Josef Van Genabith. 2010. Hard
constraints for grammatical function
labelling. In Proceedings of the 48th Annual
Meeting of the Association for Computational
Linguistics, pages 1087?1097, Uppsala.
Spoustova?, Drahom??ra ?Johanka,? Jan Hajic?,
Jan Raab, and Miroslav Spousta. 2009.
Semi-supervised training for the averaged
perceptron POS tagger. In Proceedings of the
12th Conference of the European Chapter of the
Association for Computational Linguistics,
pages 763?771, Athens.
Taskar, Ben, Vassil Chatalbashev, Daphne
Koller, and Carlos Guestrin. 2005.
Learning structured prediction models:
A large margin approach. In Proceedings
of the 22th Annual International Conference
on Machine Learning, pages 896?903,
Bonn.
Tro?n, Viktor, Pe?ter Hala?csy, Pe?ter Rebrus,
Andra?s Rung, Pe?ter Vajda, and Eszter
Simon. 2006. Morphdb.hu: Hungarian
lexical database and morphological
grammar. In Proceedings of the 5th
International Conference on Language
Resources and Evaluation, pages 1670?1673,
Genoa, Italy.
Tsarfaty, Reut, Djame? Seddah, Yoav
Goldberg, Sandra Ku?bler, Marie Candito,
Jennifer Foster, Yannick Versley, Ines
Rehbein, and Lamia Tounsi. 2010.
Statistical parsing of morphologically
rich languages (SPMRL): What, how and
whither. In Proceedings of the NAACL HLT
2010 First Workshop on Statistical Parsing
of Morphologically-Rich Languages,
pages 1?12, Los Angeles, CA.
Tsarfaty, Reut and Khalil Sima?an. 2008.
Relational-realizational parsing. In
Proceedings of the 22nd International
Conference on Computational Linguistics,
pages 889?896, Manchester.
Tsarfaty, Reut and Khalil Sima?an. 2010.
Modeling morphosyntactic agreement in
constituency-based parsing of Modern
Hebrew. In Proceedings of the NAACL HLT
2010 First Workshop on Statistical Parsing
of Morphologically-Rich Languages,
pages 40?48, Los Angeles, CA.
Versley, Yannick. 2005. Parser evaluation
across text types. In Proceedings of the 4th
Workshop on Treebanks and Linguistic
Theories, pages 209?220, Barcelona.
Versley, Yannick and Ines Rehbein. 2009.
Scalable discriminative parsing for
German. In Proceedings of the 11th
International Conference on Parsing
Technologies, pages 134?137, Paris.
Vincze, Veronika, Do?ra Szauter, Attila
Alma?si, Gyo?rgy Mo?ra, Zolta?n Alexin,
and Ja?nos Csirik. 2010. Hungarian
Dependency Treebank. In Proceedings
of the 7th Conference on International
Language Resources and Evaluation,
pages 1855?1862, Valletta.
Zsibrita, Ja?nos, Veronika Vincze, and
Richa?rd Farkas. 2010. Ismeretlen
kifejeze?sek e?s a szo?faji egye?rtelmu?s??te?s.
In VII. Magyar Sza?m??to?ge?pes Nyelve?szeti
Konferencia, pages 275?283, Szeged.
55

Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1087?1097,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Hard Constraints for Grammatical Function Labelling
Wolfgang Seeker
University of Stuttgart
Institut fu?r Maschinelle Sprachverarbeitung
seeker@ims.uni-stuttgart.de
Ines Rehbein
University of Saarland
Dep. for Comp. Linguistics & Phonetics
rehbein@coli.uni-sb.de
Jonas Kuhn
University of Stuttgart
Institut fu?r Maschinelle Sprachverarbeitung
jonas@ims.uni-stuttgart.de
Josef van Genabith
Dublin City University
CNGL and School of Computing
josef@computing.dcu.ie
Abstract
For languages with (semi-) free word or-
der (such as German), labelling gramma-
tical functions on top of phrase-structural
constituent analyses is crucial for making
them interpretable. Unfortunately, most
statistical classifiers consider only local
information for function labelling and fail
to capture important restrictions on the
distribution of core argument functions
such as subject, object etc., namely that
there is at most one subject (etc.) per
clause. We augment a statistical classifier
with an integer linear program imposing
hard linguistic constraints on the solution
space output by the classifier, capturing
global distributional restrictions. We show
that this improves labelling quality, in par-
ticular for argument grammatical func-
tions, in an intrinsic evaluation, and, im-
portantly, grammar coverage for treebank-
based (Lexical-Functional) grammar ac-
quisition and parsing, in an extrinsic eval-
uation.
1 Introduction
Phrase or constituent structure is often regarded as
an analysis step guiding semantic interpretation,
while grammatical functions (i. e. subject, object,
modifier etc.) provide important information rele-
vant to determining predicate-argument structure.
In languages with restricted word order (e. g.
English), core grammatical functions can often
be recovered from configurational information in
constituent structure analyses. By contrast, sim-
ple constituent structures are not sufficient for less
configurational languages, which tend to encode
grammatical functions by morphological means
(Bresnan, 2001). Case features, for instance, can
be important indicators of grammatical functions.
Unfortunately, many of these languages (including
German) exhibit strong syncretism where morpho-
logical cues can be highly ambiguous with respect
to functional information.
Statistical classifiers have been successfully
used to label constituent structure parser output
with grammatical function information (Blaheta
and Charniak, 2000; Chrupa?a and Van Genabith,
2006). However, as these approaches tend to
use only limited and local context information
for learning and prediction, they often fail to en-
force simple yet important global linguistic con-
straints that exist for most languages, e. g. that
there will be at most one subject (object) per sen-
tence/clause.1
?Hard? linguistic constraints, such as these,
tend to affect mostly the ?core grammatical func-
tions?, i. e. the argument functions (rather than
e. g. adjuncts) of a particular predicate. As these
functions constitute the core meaning of a sen-
tence (as in: who did what to whom), it is impor-
tant to get them right. We present a system that
adds grammatical function labels to constituent
parser output for German in a postprocessing step.
We combine a statistical classifier with an inte-
ger linear program (ILP) to model non-violable
global linguistic constraints, restricting the solu-
tion space of the classifier to those labellings that
comply with our set of global constraints. There
are, of course, many other ways of including func-
tional information into the output of a syntactic
parser. Klein and Manning (2003) show that merg-
ing some linguistically motivated function labels
with specific syntactic categories can improve the
performance of a PCFG model on Penn-II En-
1Coordinate subjects/objects form a constituent that func-
tions as a joint subject/object.
1087
glish data.2 Tsarfaty and Sim?aan (2008) present
a statistical model (Relational-Realizational Pars-
ing) that alternates between functional and config-
urational information for constituency tree pars-
ing and Hebrew data. Dependency parsers like
the MST parser (McDonald and Pereira, 2006) and
Malt parser (Nivre et al, 2007) use function labels
as core part of their underlying formalism. In this
paper, we focus on phrase structure parsing with
function labelling as a post-processing step.
Integer linear programs have already been suc-
cessfully used in related fields including semantic
role labelling (Punyakanok et al, 2004), relation
and entity classification (Roth and Yih, 2004), sen-
tence compression (Clarke and Lapata, 2008) and
dependency parsing (Martins et al, 2009). Early
work on function labelling for German (Brants et
al., 1997) reports 94.2% accuracy on gold data (a
very early version of the TiGer Treebank (Brants
et al, 2002)) using Markov models. Klenner
(2007) uses a system similar to ? but more re-
stricted than ? ours to label syntactic chunks de-
rived from the TiGer Treebank. His research fo-
cusses on the correct selection of predefined sub-
categorisation frames for a verb (see also Klenner
(2005)). By contrast, our research does not involve
subcategorisation frames as an external resource,
instead opting for a less knowledge-intensive ap-
proach. Klenner?s system was evaluated on gold
treebank data and used a small set of 7 dependency
labels. We show that an ILP-based approach can
be scaled to a large and comprehensive set of 42
labels, achieving 97.99% label accuracy on gold
standard trees. Furthermore, we apply the sys-
tem to automatically parsed data using a state-of-
the-art statistical phrase-structure parser with a la-
bel accuracy of 94.10%. In both cases, the ILP-
based approach improves the quality of argument
function labelling when compared with a non-ILP-
approach. Finally, we show that the approach
substantially improves the quality and coverage
(from 93.6% to 98.4%) of treebank-based Lexical-
Functional Grammars for German over previous
work in Rehbein and van Genabith (2009).
The paper is structured as follows: Section 2
presents basic data demonstrating the challenges
presented by German word order and case syn-
cretism for the function labeller. Section 3 de-
2Table 6 shows that for our data a model with merged
category and function labels (but without hard constraints!)
performs slightly worse than the ILP approach developed in
this paper.
scribes the labeller including the feature model of
the classifier and the integer linear program used
to pick the correct labelling. The evaluation part
(Section 4) is split into an intrinsic evaluation mea-
suring the quality of the labelling directly using
the German TiGer Treebank (Brants et al, 2002),
and an extrinsic evaluation where we test the im-
pact of the constraint-based labelling on treebank-
based automatic LFG grammar acquisition.
2 Data
Unlike English, German exhibits a relatively free
word order, i. e. in main clauses, the verb occu-
pies second position (the last position in subor-
dinated clauses) and arguments and adjuncts can
be placed (fairly) freely. The grammatical func-
tion of a noun phrase is marked morphologically
on its constituting parts. Determiners, pronouns,
adjectives and nouns carry case markings and in
order to be well-formed, all parts of a noun phrase
have to agree on their case features. German uses
a nominative?accusative system to mark predicate
arguments. Subjects are marked with nominative
case, direct objects carry accusative case. Further-
more, indirect objects are mostly marked with da-
tive case and sometimes genitive case.
(1) Der Lo?we
NOM
the lion
gibt
gives
dem Wolf
DAT
the wolf
einen Besen.
ACC
a broom
The lion gives a broom to the wolf.
(1) shows a sentence containing the ditransi-
tive verb geben (to give) with its three arguments.
Here, the subject is unambiguously marked with
nominative case (NOM), the indirect object with
dative case (DAT) and the direct object with ac-
cusative case (ACC). (2) shows possible word or-
ders for the arguments in this sentence.3
(2) Der Lo?we gibt einen Besen dem Wolf.
Dem Wolf gibt der Lo?we einen Besen.
Dem Wolf gibt einen Besen der Lo?we.
Einen Besen gibt der Lo?we dem Wolf.
Einen Besen gibt dem Wolf der Lo?we.
Since all permutations of arguments are possi-
ble, there is no chance for a statistical classifier to
decide on the correct function of a noun phrase by
its position alone. Introducing adjuncts to this ex-
ample makes matters even worse.
3Note that although (apart from the position of the finite
verb) there are no syntactic restrictions on the word order,
there are restrictions pertaining to phonological or informa-
tion structure.
1088
Case information for a given noun phrase can
give a classifier some clue about the correct ar-
gument function, since functions are strongly re-
lated to case values. Unfortunately, the German
case system is complex (see Eisenberg (2006) for
a thorough description) and exhibits a high degree
of case syncretism. (3) shows a sentence where
both argument NPs are ambiguous between nom-
inative or accusative case. In such cases, addi-
tional semantic or contextual information is re-
quired for disambiguation. A statistical classifier
(with access to local information only) runs a high
risk of incorrectly classifying both NPs as sub-
jects, or both as direct objects or even as nominal
predicates (which are also required to carry nom-
inative case). This would leave us with uninter-
pretable results. Uninterpretability of this kind can
be avoided if we are able to constrain the number
of subjects and objects globally to one per clause.4
(3) Das Schaf
NOM/ACC
the sheep
sieht
sees
das Ma?dchen.
NOM/ACC
the girl
EITHER The sheep sees the girl
OR The girl sees the sheep.
3 Grammatical Function Labelling
Our function labeller was developed and tested on
the TiGer Treebank (Brants et al, 2002). The
TiGer Treebank is a phrase-structure and gram-
matical function annotated treebank with 50,000
newspaper sentences from the Frankfurter Rund-
schau (Release 2, July 2006). Its overall anno-
tation scheme is quite flat to account for the rel-
atively free word order of German and does not
allow for unary branching. The annotations use
non-projective trees modelling long distance de-
pendencies directly by crossing branches. Words
are lemmatised and part-of-speech tagged with the
Stuttgart-Tu?bingen Tag Set (STTS) (Schiller et al,
1999) and contain morphological annotations (Re-
lease 2). TiGer uses 25 syntactic categories and a
set of 42 function labels to annotate the grammat-
ical function of a phrase.
The function labeller consists of two main com-
ponents, a maximum entropy classifier and an in-
teger linear program. This basic architecture was
introduced by Punyakanok et al (2004) for the
task of semantic role labelling and since then has
been applied to different NLP tasks without signif-
icant changes. In our case, its input is a bare tree
4Although the classifier may, of course, still identify the
wrong phrase as subject or object.
structure (as obtained by a standard phrase struc-
ture parser) and it outputs a tree structure where
every node is labelled with the grammatical rela-
tion it bears to its mother node. For each possi-
ble label and for each node, the classifier assigns
a probability that this node is labelled by this la-
bel. This results in a complete probability distri-
bution over all labels for each node. An integer
linear program then tries to find the optimal over-
all tree labelling by picking for each node the label
with the highest probability without violating any
of its constraints. These constraints implement lin-
guistic rules like the one-subject-per-sentence rule
mentioned above. They can also be used to cap-
ture treebank particulars, such as for example that
punctuation marks never receive a label.
3.1 The Feature Model
Maximum entropy classifiers have been used in a
wide range of applications in NLP for a long time
(Berger et al, 1996; Ratnaparkhi, 1998). They
usually give good results while at the same time
allowing for the inclusion of arbitrarily complex
features. They also have the advantage that they
directly output probability distributions over their
set of labels (unlike e. g. SVMs).
The classifier uses the following features:
? the lemma (if terminal node)
? the category (the POS for terminal nodes)
? the number of left/right sisters
? the category of the two left/right sisters
? the number of daughters
? the number of terminals covered
? the lemma of the left/right corner terminal
? the category of the left/right corner terminal
? the category of the mother node
? the category of the mother?s head node
? the lemma of the mother?s head node
? the category of the grandmother node
? the category of the grandmother?s head node
? the lemma of the grandmother?s head node
? the case features for noun phrases
? the category for PP objects
? the lemma for PP objects (if terminal node)
These features are also computed for the head
of the phrase, determined using a set of head-
finding rules in the style of Magerman (1995)
adapted to TiGer. For lemmatisation, we use Tree-
Tagger (Schmid, 1994) and case features of noun
1089
phrases are obtained from a full German morpho-
logical analyser based on (Schiller, 1994). If a
noun phrase consists of a single word (e. g. pro-
nouns, but also bare common nouns and proper
nouns), all case values output by the analyser are
used to reflect the case syncretism. For multi-word
noun phrases, the case feature is computed by tak-
ing the intersection of all case-bearing words in-
side the noun phrase, i. e. determiners, pronouns,
adjectives, common nouns and proper nouns. If,
for some reason (e.g., due to a bracketing error in
phrase structure parsing), the intersection turns out
to be empty, all four case values are assigned to the
phrase.5
3.2 Constrained Optimisation
In the second step, a binary integer linear pro-
gram is used to select those labels that optimise the
whole tree labelling. A linear program consists of
a linear objective function that is to be maximised
(or minimised) and a set of constraints which im-
pose conditions on the variables of the objective
function (see (Clarke and Lapata, 2008) for a short
but readable introduction). Although solving a lin-
ear program has polynomial complexity, requiring
the variables to be integral or binary makes find-
ing a solution exponentially hard in the worst case.
Fortunately, there are efficient algorithms which
are capable of handling a large number of vari-
ables and constraints in practical applications.6
For the function labeller, we define the set of
binary variables V = N ? L to be the crossprod-
uct of the set of nodes N and the set of labels L.
Setting a variable xn,l to 1 means that node n is
labelled by label l. Every variable is weighted by
the probability wn,l = P (l|f(n)) which the clas-
sifier has assigned to this node-label combination.
The objective function that we seek to optimise is
defined as the sum over all weighted variables:
max
?
n?N
?
l?L
wn,lxn,l (4)
Since we want every node to receive exactly one
5We decided to train the classifier on automatically
assigned and possibly ambiguous morphological informa-
tion instead of on the hand-annotated and manually disam-
biguated morphological information provided by TiGer be-
cause we want the classifier to learn the German case syn-
cretism. This way, the classifier will perform better when pre-
sented with unseen data (e.g. from parser output) for which
no hand-annotated morphological information is available.
6See lpsolve (http://lpsolve.sourceforge.net/) or GLPK
(http://www.gnu.org/software/glpk/glpk.html) for open-
source implementations
label, we add a constraint that for every node n,
exactly one of its variables is set to 1.
?
l?L
xn,l = 1 (5)
Up to now, the whole system is doing exactly
the same as an ordinary classifier that always takes
the most probable label for each node. We will
now add additional global and local linguistic con-
straints.7
The first and most important constraint restricts
the number of each argument function (as opposed
to modifier functions) to at most one per clause.
Let D ? N ? N be the direct dominance rela-
tion between the nodes of the current tree. For ev-
ery node n with category S (sentence) or VP (verb
phrase), at most one of its daughters is allowed
to be labelled SB (subject). The single-subject-
function condition is defined as:
cat(n) ? {S, V P} ??
?
?n,m??D
xm,SB ? 1 (6)
Identical constraints are added for labels OA,
OA2, DA, OG, OP, PD, OC, EP.8
We add further constraints to capture the follow-
ing linguistic restrictions:
? Of all daughters of a phrase, only one is allowed
to be labelled HD (head).
?
?n,m??D
xm,HD ? 1 (7)
? If a noun phrase carries no case feature for nom-
inative case, it cannot be labelled SB, PD or EP.
case(n) 6= nom ??
?
l?{SB,PD,EP}
xn,l = 0
(8)
? If a noun phrase carries no case feature for ac-
cusative case, it cannot be labelled OA or OA2.
? If a noun phrase carries no case feature for da-
tive case, it cannot be labelled DA.
? If a noun phrase carries no case feature for gen-
itive case, it cannot be labelled OG or AG9.
7Note that some of these constraints are language specific
in that they represent linguistic facts about German and do
not necessarily hold for other languages. Furthermore, the
constraints are treebank specific to a certain degree in that
they use a TiGer-specific set of labels and are conditioned on
TiGer-specific configurations and categories.
8SB = subject, OA = accusative object, OA2 = sec-
ond accusative object, DA = dative, OG = genitive object,
OP = prepositional object, PD = predicate, OC = clausal ob-
ject, EP = expletive es
9AG = genitive adjunct
1090
Unlike Klenner (2007), we do not use prede-
fined subcategorization frames, instead letting the
statistical model choose arguments.
In TiGer, sentences whose main verbs are
formed from auxiliary-participle combinations,
are annotated by embedding the participle under
an extra VP node and non-subject arguments are
sisters to the participle. Therefore we add an ex-
tension of the constraint in (6) to the constraint set
in order to also include the daughters of an embed-
ded VP node in such a case.
Because of the particulars of the annotation
scheme of TiGer, we can decide some labels in
advance. As mentioned before, punctuation does
not get a label in TiGer. We set the label for those
nodes to ?? (no label). Other examples are:
? If a node?s category is PTKVZ (separated verb
particle), it is labeled SVP (separable verb par-
ticle).
cat(n) = PTKV Z ?? xn,SV P = 1 (9)
? If a node?s category is APPR, APPRART,
APPO or APZR (prepositions), it is labeled AC
(adpositional case marker).
? All daughters of an MTA node (multi-token
adjective) are labeled ADC (adjective compo-
nent).
These constraints are conditioned on part-of-
speech tags and require high POS-tagging accu-
racy (when dealing with raw text).
Due to the constraints imposed on the classifi-
cation, the function labeller can no longer assign
two subjects to the same S node. Faced with two
nodes whose most probable label is SB, it has to
decide on one of them taking the next best label for
the other. This way, it outputs the optimal solution
with respect to the set of constraints. Note that this
requires the feature model not only to rank the cor-
rect label highest but also to provide a reasonable
ranking of the other labels as well.
4 Evaluation
We conducted a number of experiments using
1,866 sentences of the TiGer Dependency Bank
(Forst et al, 2004) as our test set. The TiGerDB is
a part of the TiGer Treebank semi-automatically
converted into a dependency representation. We
use the manually labelled TiGer trees correspond-
ing to the sentences in the TiGerDB for assessing
the labelling quality in the intrinsic evaluation, and
the dependencies from TiGerDB for assessing the
quality and coverage of the automatically acquired
LFG resources in the extrinsic evaluation.
In order to test on real parser output, the test
set was parsed with the Berkeley Parser (Petrov et
al., 2006) trained on 48k sentences of the TiGer
corpus (Table 1), excluding the test set. Since the
Berkeley Parser assumes projective structures, the
training data and test data were made projective by
raising non-projective nodes in the tree (Ku?bler,
2005).
precision 83.60 recall 82.81
f-score 83.20 tagging acc. 97.97
Table 1: evalb unlabelled parsing scores on test set for Berke-
ley Parser trained on 48,000 sentences (sentence length? 40)
The maximum entropy classifier of the func-
tion labeller was trained on 46,473 sentences of
the TiGer Treebank (excluding the test set) which
yields about 1.2 million nodes as training samples.
For training the Maximum Entropy Model, we
used the BLMVM algorithm (Benson and More,
2001) with a width factor of 1.0 (Kazama and Tsu-
jii, 2005) implemented in an open-source C++ li-
brary from Tsujii Laboratory.10 The integer linear
program was solved with the simplex algorithm in
combination with a branch-and-bound method us-
ing the freely available GLPK.11
4.1 Intrinsic Evaluation
In the intrinsic evaluation, we measured the qual-
ity of the labelling itself. We used the node
span evaluation method of (Blaheta and Char-
niak, 2000) which takes only those nodes into ac-
count which have been recognised correctly by the
parser, i.e. if there are two nodes in the parse and
the reference treebank tree which cover the same
word span. Unlike Blaheta and Charniak (2000)
however, we do not require the two nodes to carry
the same syntactic category label.12
Table 2 shows the results of the node span eval-
uation. The labeller achieves close to 98% label
accuracy on gold treebank trees which shows that
the feature model captures the differences between
the individual labels well. Results on parser output
are about 4 percentage points (absolute) lower as
parsing errors can distort local context features for
the classifier even if the node itself has been parsed
10http://www-tsujii.is.s.u-tokyo.ac.jp/?tsuruoka/maxent/
11http://www.gnu.org/software/glpk/glpk.html
12We also excluded the root node, all punctuation marks
and both nodes in unary branching sub-trees from evaluation.
1091
correctly. The addition of the ILP constraints im-
proves results only slightly since the constraints
affect only (a small number of) argument labels
while the evaluation considers all 40 labels occur-
ring in the test set. Since the constraints restrict the
selection of certain labels, a less probable label has
to be picked by the labeller if the most probable
is not available. If the classifier is ranking labels
sensibly, the correct label should emerge. How-
ever, with an incorrect ranking, the ILP constraints
might also introduce new errors.
label accuracy error red.
without constraints
gold 44689/45691 = 97.81% ?
parser 40578/43140 = 94.06% ?
with constraints
gold 44773/45691 = 97.99%* 8.21%
parser 40593/43140 = 94.10% 0.68%
Table 2: label accuracy and error reduction (all labels) for
node span evaluation, * statistically significant, sign test, ? =
0.01 (Koo and Collins, 2005)
As the main target of the constraint set are argu-
ment functions, we also tested the quality of argu-
ment labels. Table 3 shows the node span evalua-
tion in terms of precision, recall and f-score for ar-
gument functions only, with clear statistically sig-
nificant improvements.
prec. rec. f-score
without constraints
gold standard 92.41 91.86 92.13
parser output 88.14 86.43 87.28
with constraints
gold standard 94.31 92.76 93.53*
parser output 89.51 86.73 88.09*
Table 3: node span results for the test set, argument functions
only (SB, EP, PD, OA, OA2, DA, OG, OP, OC), * statistically
significant, sign test, ? = 0.01 (Koo and Collins, 2005)
For comparison and to establish a highly com-
petitive baseline, we use the best-scoring system
in (Chrupa?a and Van Genabith, 2006), trained and
tested on exactly the same data sets. This purely
statistical labeller achieves accuracy of 96.44%
(gold) and 92.81% (parser) for all labels, and f-
scores of 89.88% (gold) and 84.98% (parser) for
argument labels. Tables 2 and 3 show that our sys-
tem (with and even without ILP constraints) com-
prehensively outperforms all corresponding base-
line scores.
The node span evaluation defines a correct la-
belling by taking only those nodes (in parser out-
put) into account that have a corresponding node
in the reference tree. However, as this restricts at-
tention to correctly parsed nodes, the results are
somewhat over-optimistic. Table 4 provides the
results obtained from an evalb evaluation of the
same data sets.13 The gold standard scores are
high confirming our previous findings about the
performance of the function labeller. However,
the results on parser output are much worse. The
evaluation scores are now taking the parsing qual-
ity into account (Table 1). The considerable drop
in quality between gold trees and parser output
clearly shows that a good parse tree is an impor-
tant prerequisite for reasonable function labelling.
This is in accordance with previous findings by
Punyakanok et al (2008) who emphasise the im-
portance of syntactic parsing for the closely re-
lated task of semantic role labelling.
prec. rec. f-score
without constraints
gold standard 95.94 95.94 95.94
parser output 76.27 75.55 75.91
with constraints
gold standard 96.21 96.21 96.21
parser output 76.36 75.64 76.00
Table 4: evalb results for the test set
4.1.1 Subcategorisation Frames
Early on in the paper we mention that, unlike e. g.
Klenner (2007), we did not include predefined
subcategorisation frames into the constraint set,
but rather let the joint statistical and ILP models
decide on the correct type of arguments assigned
to a verb. The assumption is that if one uses prede-
fined subcategorisation frames which fix the num-
ber and type of arguments for a verb, one runs the
risk of excluding correct labellings due to missing
subcat frames, unless a very comprehensive and
high quality subcat lexicon resource is available.
In order to test this assumption, we run an addi-
tional experiment with about 10,000 verb frames
for 4,508 verbs, which were automatically ex-
tracted from our training section. Following Klen-
ner (2007), for each verb and for each subcat frame
for this verb attested at least once in the training
data, we introduce a new binary variable fn to
the ILP model representing the n-th frame (for the
verb) weighted by its frequency.
We add an ILP constraint requiring exactly one
of the frames to be set to one (each verb has to have
a subcat frame) and replace the ILP constraint in
(6) by:
13Function labels were merged with the category symbols.
1092
??n,m??D
xm,SB ?
?
SB?fi
fi = 0 (10)
This constraint requires the number of subjects
in a phrase to be equal to the number of selected14
verb frames that require a subject. As each verb
is constrained to ?select? exactly one subcat frame
(see additional ILP constraint above), there is at
most one subject per phrase, if the frame in ques-
tion requires a subject. If the selected frame does
not require a subject, then the constraint blocks the
assignment of subjects for the entire phrase. The
same was done for the other argument functions
and as before we included an extension of this con-
straint to cover embedded VPs. For unseen verbs
(i.e. verbs not attested in the training set) we keep
the original constraints as a back-off.
prec. rec. f-score
all labels (cmp. Table 2)
gold standard 97.24 97.24 97.24
parser output 93.43 93.43 93.43
argument functions only (cmp. Table 3)
gold standard 91.36 90.12 90.74
parser output 86.64 84.38 85.49
Table 5: node span results for the test set using constraints
with automatically extracted subcat frames
Table 5 shows the results of the test set node
span evaluation when using the ILP system en-
hanced with subcat frames. Compared to Tables 2
and 3, the results are clearly inferior, and particu-
larly so for argument grammatical functions. This
seems to confirm our assumption that, given our
data, letting the joint statistical and ILP model de-
cide argument functions is superior to an approach
that involves subcat frames. However, and impor-
tantly, our results do not rule out that a more com-
prehensive subcat frame resource may in fact re-
sult in improvements.
4.2 Extrinsic Evaluation
Over the last number of years, treebank-based
deep grammar acquisition has emerged as an
attractive alternative to hand-crafting resources
within the HPSG, CCG and LFG paradigms
(Miyao et al, 2003; Clark and Hockenmaier,
2002; Cahill et al, 2004). While most of the ini-
tial development work focussed on English, more
recently efforts have branched to other languages.
Below we concentrate on LFG.
14The variable representing this frame has been set to 1.
Lexical-Functional Grammar (Bresnan, 2001)
is a constraint-based theory of grammar with min-
imally two levels of representation: c(onstituent)-
structure and f(unctional)-structure. C-structure
(CFG trees) captures language specific surface
configurations such as word order and the hier-
archical grouping of words into phrases, while
f-structure represents more abstract (and some-
what more language independent) grammatical re-
lations (essentially bilexical labelled dependencies
with some morphological and semantic informa-
tion, approximating to basic predicate-argument
structures) in the form of attribute-value struc-
tures. F-structures are defined in terms of equa-
tions annotated to nodes in c-structure trees (gram-
mar rules). Treebank-based LFG acquisition was
originally developed for English (Cahill, 2004;
Cahill et al, 2008) and is based on an f-structure
annotation algorithm that annotates c-structure
trees (from a treebank or parser output) with
f-structure equations, which are read off of the tree
and passed on to a constraint solver producing an
f-structure for the given sentence. The English
annotation algorithm (for Penn-II treebank-style
trees) relies heavily on configurational and catego-
rial information, translating this into grammatical
functional information (subject, object etc.) rep-
resented at f-structure. LFG is ?functional? in the
mathematical sense, in that argument grammatical
functions have to be single valued (there cannot be
two or more subjects etc. in the same clause). In
fact, if two or more values are assigned to a single
argument grammatical function in a local tree, the
LFG constraint solver will produce a clash (i. e.
it will fail to produce an f-structure) and the sen-
tence will be considered ungrammatical (in other
words, the corresponding c-structure tree will be
uninterpretable).
Rehbein (2009) and Rehbein and van Genabith
(2009) develop an f-structure annotation algorithm
for German based on the TiGer treebank resource.
Unlike the English annotation algorithm and be-
cause of the language-particular properties of Ger-
man (see Section 2), the German annotation al-
gorithm cannot rely on c-structure configurational
information, but instead heavily uses TiGer func-
tion labels in the treebank. Learning function la-
bels is therefore crucial to the German LFG an-
notation algorithm, in particular when parsing raw
text. Because of the strong case syncretism in Ger-
man, traditional classification models using local
1093
information only run the risk of predicting mul-
tiple occurences of the same function (subject,
object etc.) at the same level, causing feature
clashes in the constraint solver with no f-structure
being produced. Rehbein (2009) and Rehbein
and van Genabith (2009) identify this as a major
problem resulting in a considerable loss in cov-
erage of the German annotation algorithm com-
pared to English, in particular for parsing raw text,
where TiGer function labels have to be supplied by
a machine-learning-based method and where the
coverage of the LFG annotation algorithm drops
to 93.62% with corresponding drops in recall and
f-scores for the f-structure evaluations (Table 6).
Below we test whether the coverage problems
caused by incorrect multiple assignments of gram-
matical functions can be addressed using the com-
bination of classifier with ILP constraints devel-
oped in this paper. We report experiments where
automatically parsed and labelled data are handed
over to an LFG f-structure computation algorithm.
The f-structures produced are converted into a
dependency triple representation (Crouch et al,
2002) and evaluated against TiGerDB.
cov. prec. rec. f-score
upper bound 99.14 85.63 82.58 84.07
without constraints
gold 95.82 84.71 76.68 80.49
parser 93.41 79.70 70.38 74.75
with constraints
gold 99.30 84.62 82.15 83.37
parser 98.39 79.43 75.60 77.47
Rehbein 2009
parser 93.62 79.20 68.86 73.67
Table 6: f-structure evaluation results for the test set against
TigerDB
Table 6 shows the results of the f-structure
evaluation against TiGerDB, with 84.07% f-score
upper-bound results for the f-structure annotation
algorithm on the original TiGer treebank trees
with hand-annotated function labels. Using the
function labeller without ILP constraints results in
drastic drops in coverage (between 4.5% and 6.5%
points absolute) and hence recall (6% and 12%)
and f-score (3.5% and 9.5%) for both gold trees
and parser output (compared to upper bounds).
By contrast, with ILP constraints, the loss in cov-
erage observed above almost completely disap-
pears and recall and f-scores improve by between
4.4% and 5.5% (recall) and 3% (f-score) abso-
lute (over without ILP constraints). For compar-
ison, we repeated the experiment using the best-
scoring method of Rehbein (2009). Rehbein trains
the Berkeley Parser to learn an extended category
set, merging TiGer function labels with syntactic
categories, where the parser outputs fully-labelled
trees. The results show that this approach suf-
fers from the same drop in coverage as the classi-
fier without ILP constraints, with recall about 7%
and f-score about 4% (absolute) lower than for the
classifier with ILP constraints.
Table 7 shows the dramatic effect of the ILP
constraints on the number of sentences in the test
set that have multiple argument functions of the
same type within the same clause. With ILP con-
straints, the problem disappears and therefore, less
feature-clashes occur during f-structure computa-
tion.
no constraints constraints
gold 185 0
parser 212 0
Table 7: Number of sentences in the test set with doubly an-
notated argument functions
In order to assess whether ILP constraints help
with coverage only or whether they affect the qual-
ity of the f-structures as well, we repeat the experi-
ment in Table 6, however this time evaluating only
on those sentences that receive an f-structure, ig-
noring the rest. Table 8 shows that the impact of
ILP constraints on quality is much less dramatic
than on coverage, with only very small variations
in precison, recall and f-scores across the board,
and small increases over Rehbein (2009).
cov. prec. rec. f-score
no constr. 93.41 79.70 77.89 78.79
constraints 98.39 79.43 77.85 78.64
Rehbein 93.62 79.20 76.43 77.79
Table 8: f-structure evaluation results for parser output ex-
cluding sentences without f-structures
Early work on automatic LFG acquisition and
parsing for German is presented in Cahill et al
(2003) and Cahill (2004), adapting the English
Annotation Algorithm to an earlier and smaller
version of the TiGer treebank (without morpho-
logical information) and training a parser to learn
merged Tiger function-category labels, and report-
ing 95.75% coverage and an f-score of 74.56%
f-structure quality against 2,000 gold treebank
trees automatically converted into f-structures.
Rehbein (2009) uses the larger Release 2 of the
treebank (with morphological information) report-
ing 77.79% f-score and coverage of 93.62% (Ta-
1094
ble 8) against the dependencies in the TiGerDB
test set. The only rule-based approach to German
LFG-parsing we are aware of is the hand-crafted
German grammar in the ParGram Project (Butt
et al, 2002). Forst (2007) reports 83.01% de-
pendency f-score evaluated against a set of 1,497
sentences of the TiGerDB. It is very difficult to
compare results across the board, as individual pa-
pers use (i) different versions of the treebank, (ii)
different (sections of) gold-standards to evaluate
against (gold TiGer trees in TigerDB, the depen-
dency representations provided by TigerDB, auto-
matically generated gold-standards etc.) and (iii)
different label/grammatical function sets. Further-
more, (iv) coverage differs drastically (with the
hand-crafted LFG resources achieving about 80%
full f-structures) and finally, (v) some of the gram-
mars evaluated having been used in the generation
of the gold standards, possibly introducing a bias
towards these resources: the German hand-crafted
LFG was used to produce TiGerDB (Forst et al,
2004). In order to put the results into some per-
spective, Table 9 shows an evaluation of our re-
sources against a set of automatically generated
gold standard f-structures produced by using the
f-structure annotation algorithm on the original
hand-labelled TiGer gold trees in the section cor-
responding to TiGerDB: without ILP constraints
we achieve a dependency f-score of 84.35%, with
ILP constraints 87.23% and 98.89% coverage.
cov. prec. rec. f-score
without constraints
gold 95.24 97.76 90.93 94.22
parser 93.35 88.71 80.40 84.35
with constraints
gold 99.30 97.66 97.33 97.50
parser 98.89 88.37 86.12 87.23
Table 9: f-structure evaluation results for the test set against
automatically generated goldstandard (1,850 sentences)
5 Conclusion
In this paper, we addressed the problem of assign-
ing grammatical functions to constituent struc-
tures. We have proposed an approach to grammat-
ical function labelling that combines the flexibil-
ity of a statistical classifier with linguistic expert
knowledge in the form of hard constraints imple-
mented by an integer linear program. These con-
straints restrict the solution space of the classifier
by blocking those solutions that cannot be correct.
One of the strengths of an integer linear program
is the unlimited context it can take into account
by optimising over the entire structure, providing
an elegant way of supporting classifiers with ex-
plicit linguistic knowledge while at the same time
keeping feature models small and comprehensi-
ble. Most of the constraints are direct formaliza-
tions of linguistic generalizations for German. Our
approach should generalise to other languages for
which linguistic expertise is available.
We evaluated our system on the TiGer corpus
and the TiGerDB and gave results on gold stan-
dard trees and parser output. We also applied
the German f-structure annotation algorithm to
the automatically labelled data and evaluated the
system by measuring the quality of the resulting
f-structures. We found that by using the con-
straint set, the function labeller ensures the inter-
pretability and thus the usefulness of the syntac-
tic structure for a subsequently applied processing
step. In our f-structure evaluation, that means, the
f-structure computation algorithm is able to pro-
duce an f-structure for almost all sentences.
Acknowledgements
The first author would like to thank Gerlof Bouma
for a lot of very helpful discussions. We would
like to thank our anonymous reviewers for de-
tailed and helpful comments. The research was
supported by the Science Foundation Ireland SFI
(Grant 07/CE/I1142) as part of the Centre for
Next Generation Localisation (www.cngl.ie) and
by DFG (German Research Foundation) through
SFB 632 Potsdam-Berlin and SFB 732 Stuttgart.
References
Steven J. Benson and Jorge J. More. 2001. A limited
memory variable metric method in subspaces and
bound constrained optimization problems. Techni-
cal report, Argonne National Laboratory.
Adam L. Berger, Vincent J.D. Pietra, and Stephen A.D.
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational linguis-
tics, 22(1):71.
Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In Proceedings of the
1st North American chapter of the Association for
Computational Linguistics conference, pages 234 ?
240, Seattle, Washington. Morgan Kaufmann Pub-
lishers Inc.
Thorsten Brants, Wojciech Skut, and Brigitte Krenn.
1997. Tagging grammatical functions. In Proceed-
ings of EMNLP, volume 97, pages 64?74.
1095
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories, page 2441.
Joan Bresnan. 2001. Lexical-Functional Syntax.
Blackwell Publishers.
Miriam Butt, Helge Dyvik, Tracy Halloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
parallel grammar project. In COLING-02 on Gram-
mar engineering and evaluation-Volume 15, volume
pages, page 7. Association for Computational Lin-
guistics.
Aoife Cahill, Martin Forst, Mairead McCarthy, Ruth
ODonovan, Christian Rohrer, Josef van Genabith,
and Andy Way. 2003. Treebank-based multilingual
unification-grammar development. In Proceedings
of the Workshop on Ideas and Strategies for Multi-
lingual Grammar Development at the 15th ESSLLI,
page 1724.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef
van Genabith, and Andy Way. 2004. Long-
distance dependency resolution in automatically ac-
quired wide-coverage PCFG-based LFG approxima-
tions. Proceedings of the 42nd Annual Meeting
on Association for Computational Linguistics - ACL
?04, pages 319?es.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan
Riezler, Josef van Genabith, and Andy Way. 2008.
Wide-Coverage Deep Statistical Parsing Using Au-
tomatic Dependency Structure Annotation. Compu-
tational Linguistics, 34(1):81?124, Ma?rz.
Aoife Cahill. 2004. Parsing with Automatically Ac-
quired, Wide-Coverage, Robust, Probabilistic LFG
Approximations. Ph.D. thesis, Dublin City Univer-
sity.
Grzegorz Chrupa?a and Josef Van Genabith. 2006.
Using machine-learning to assign function labels
to parser output for Spanish. In Proceedings of
the COLING/ACL main conference poster session,
page 136143, Sydney. Association for Computa-
tional Linguistics.
Stephen Clark and Judith Hockenmaier. 2002. Evalu-
ating a wide-coverage CCG parser. In Proceedings
of the LREC 2002, pages 60?66.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression an integer linear
programming approach. Journal of Artificial Intelli-
gence Research, 31:399?429.
Richard Crouch, Ronald M. Kaplan, Tracy Halloway
King, and Stefan Riezler. 2002. A comparison of
evaluation metrics for a broad-coverage stochastic
parser. In Proceedings of LREC 2002 Workshop,
pages 67?74, Las Palmas, Canary Islands, Spain.
Peter Eisenberg. 2006. Grundriss der deutschen
Grammatik: Das Wort. J.B. Metzler, Stuttgart, 3
edition.
Martin Forst, Nu?ria Bertomeu, Berthold Crysmann,
Frederik Fouvry, Silvia Hansen-Shirra, and Valia
Kordoni. 2004. Towards a dependency-based gold
standard for German parsers The TiGer Dependency
Bank. In Proceedings of the COLING Workshop
on Linguistically Interpreted Corpora (LINC ?04),
Geneva, Switzerland.
Martin Forst. 2007. Filling Statistics with Linguistics
Property Design for the Disambiguation of German
LFG Parses. In Proceedings of ACL 2007. Associa-
tion for Computational Linguistics.
Jun?Ichi Kazama and Jun?Ichi Tsujii. 2005. Maxi-
mum entropy models with inequality constraints: A
case study on text categorization. Machine Learn-
ing, 60(1):159194.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL
2003, pages 423?430, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Manfred Klenner. 2005. Extracting Predicate Struc-
tures from Parse Trees. In Proceedings of the
RANLP 2005.
Manfred Klenner. 2007. Shallow dependency label-
ing. In Proceedings of the ACL 2007 Demo and
Poster Sessions, page 201204, Prague. Association
for Computational Linguistics.
Terry Koo and Michael Collins. 2005. Hidden-
variable models for discriminative reranking. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing - HLT ?05, pages 507?514, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Sandra Ku?bler. 2005. How Do Treebank Annotation
Schemes Influence Parsing Results? Or How Not to
Compare Apples And Oranges. In Proceedings of
RANLP 2005, Borovets, Bulgaria.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33rd an-
nual meeting on Association for Computational Lin-
guistics, page 276283, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics Morristown,
NJ, USA.
Andre? F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proceedings of
ACL 2009.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL, volume 6.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.
2003. Probabilistic modeling of argument structures
including non-local dependencies. In Proceedings
of the Conference on Recent Advances in Natural
Language Processing RANLP 2003, volume 2.
1096
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135, Januar.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the ACL
- ACL ?06, pages 433?440, Morristown, NJ, USA.
Association for Computational Linguistics.
Vasin Punyakanok, Wen-Tau Yih, Dan Roth, and Dav
Zimak. 2004. Semantic role labeling via integer
linear programming inference. In Proceedings of
the 20th international conference on Computational
Linguistics - COLING ?04, Morristown, NJ, USA.
Association for Computational Linguistics.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The Importance of Syntactic Parsing and Inference
in Semantic Role Labeling. Computational Linguis-
tics, 34(2):257?287, Juni.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution. Ph.D.
thesis, University of Pennsylvania.
Ines Rehbein and Josef van Genabith. 2009. Auto-
matic Acquisition of LFG Resources for German-
As Good as it gets. In Miriam Butt and Tracy Hol-
loway King, editors, Proceedings of LFG Confer-
ence 2009. CSLI Publications.
Ines Rehbein. 2009. Treebank-based grammar acqui-
sition for German. Ph.D. thesis, Dublin City Uni-
versity.
Dan Roth and Wen-Tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of CoNNL 2004.
Anne Schiller, Simone Teufel, and Christine Sto?ckert.
1999. Guidelines fu?r das Tagging deutscher
Textcorpora mit STTS (Kleines und gro?es Tagset).
Technical Report August, Universita?t Stuttgart.
Anne Schiller. 1994. Dmor - user?s guide. Technical
report, University of Stuttgart.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, volume 12. Manchester, UK.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
realizational parsing. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics - COLING ?08, pages 889?896, Morristown, NJ,
USA. Association for Computational Linguistics.
1097
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1007?1017,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Underspecifying and Predicting Voice for Surface Realisation Ranking
Sina Zarrie?, Aoife Cahill and Jonas Kuhn
Institut fu?r maschinelle Sprachverarbeitung
Universita?t Stuttgart, Germany
{sina.zarriess,aoife.cahill,jonas.kuhn}@ims.uni-stuttgart.de
Abstract
This paper addresses a data-driven surface
realisation model based on a large-scale re-
versible grammar of German. We investigate
the relationship between the surface realisa-
tion performance and the character of the in-
put to generation, i.e. its degree of underspec-
ification. We extend a syntactic surface reali-
sation system, which can be trained to choose
among word order variants, such that the can-
didate set includes active and passive variants.
This allows us to study the interaction of voice
and word order alternations in realistic Ger-
man corpus data. We show that with an ap-
propriately underspecified input, a linguisti-
cally informed realisation model trained to re-
generate strings from the underlying semantic
representation achieves 91.5% accuracy (over
a baseline of 82.5%) in the prediction of the
original voice.
1 Introduction
This paper1 presents work on modelling the usage
of voice and word order alternations in a free word
order language. Given a set of meaning-equivalent
candidate sentences, such as in the simplified En-
glish Example (1), our model makes predictions
about which candidate sentence is most appropriate
or natural given the context.
(1) Context: The Parliament started the debate about the state
budget in April.
a. It wasn?t until June that the Parliament approved it.
b. It wasn?t until June that it was approved by the Parliament.
c. It wasn?t until June that it was approved.
We address the problem of predicting the usage of
linguistic alternations in the framework of a surface
1This work has been supported by the Deutsche Forschungs-
gemeinschaft (DFG; German Research Foundation) in SFB 732
Incremental specification in context, project D2 (PIs: Jonas
Kuhn and Christian Rohrer).
realisation ranking system. Such ranking systems
are practically relevant for the real-world applica-
tion of grammar-based generators that usually gen-
erate several grammatical surface sentences from a
given abstract input, e.g. (Velldal and Oepen, 2006).
Moreover, this framework allows for detailed exper-
imental studies of the interaction of specific linguis-
tic features. Thus it has been demonstrated that for
free word order languages like German, word or-
der prediction quality can be improved with care-
fully designed, linguistically informed models cap-
turing information-structural strategies (Filippova
and Strube, 2007; Cahill and Riester, 2009).
This paper is situated in the same framework, us-
ing rich linguistic representations over corpus data
for machine learning of realisation ranking. How-
ever, we go beyond the task of finding the correct or-
dering for an almost fixed set of word forms. Quite
obviously, word order is only one of the means at
a speaker?s disposal for expressing some content in
a contextually appropriate form; we add systematic
alternations like the voice alternation (active vs. pas-
sive) to the picture. As an alternative way of pro-
moting or demoting the prominence of a syntactic
argument, its interaction with word ordering strate-
gies in real corpus data is of high theoretical interest
(Aissen, 1999; Aissen, 2003; Bresnan et al, 2001).
Our main goals are (i) to establish a corpus-based
surface realisation framework for empirically inves-
tigating interactions of voice and word order in Ger-
man, (ii) to design an input representation for gen-
eration capturing voice alternations in a variety of
contexts, (iii) to better understand the relationship
between the performance of a generation ranking
model and the type of realisation candidates avail-
able in its input. In working towards these goals,
this paper addresses the question of evaluation. We
conduct a pilot human evaluation on the voice al-
1007
ternation data and relate our findings to our results
established in the automatic ranking experiments.
Addressing interactions among a range of gram-
matical and discourse phenomena on realistic corpus
data turns out to be a major methodological chal-
lenge for data-driven surface realisation. The set of
candidate realisations available for ranking will in-
fluence the findings, and here, existing surface re-
alisers vary considerably. Belz et al (2010) point
out the differences across approaches in the type of
syntactic and semantic information present and ab-
sent in the input representation; and it is the type of
underspecification that determines the number (and
character) of available candidate realisations and,
hence, the complexity of the realisation task.
We study the effect of varying degrees of under-
specification explicitly, extending a syntactic gen-
eration system by a semantic component capturing
voice alternations. In regeneration studies involving
underspecified underlying representations, corpus-
oriented work reveals an additional methodological
challenge. When using standard semantic represen-
tations, as common in broad-coverage work in se-
mantic parsing (i.e., from the point of view of analy-
sis), alternative variants for sentence realisation will
often receive slightly different representations: In
the context of (1), the continuation (1-c) is presum-
ably more natural than (1-b), but with a standard
sentence-bounded semantic analysis, only (1-a) and
(1-b) would receive equivalent representations.
Rather than waiting for the availability of robust
and reliable techniques for detecting the reference of
implicit arguments in analysis (or for contextually
aware reasoning components), we adopt a relatively
simple heuristic approach (see Section 3.1) that ap-
proximates the desired equivalences by augmented
representations for examples like (1-c). This way
we can overcome an extremely skewed distribution
in the naturally occurring meaning-equivalent active
vs. passive sentences, a factor which we believe jus-
tifies taking the risk of occasional overgeneration.
The paper is structured as follows: Section 2 situ-
ates our methodology with respect to other work on
surface realisation and briefly summarises the rele-
vant theoretical linguistic background. In Section 3,
we present our generation architecture and the de-
sign of the input representation. Section 4 describes
the setup for the experiments in Section 5. In Section
6, we present the results from the human evaluation.
2 Related Work
2.1 Generation Background
The first widely known data-driven approach to
surface realisation, or tactical generation, (Langk-
ilde and Knight, 1998) used language-model n-
gram statistics on a word lattice of candidate re-
alisations to guide a ranker. Subsequent work ex-
plored ways of exploiting linguistically annotated
data for trainable generation models (Ratnaparkhi,
2000; Marciniak and Strube, 2005; Belz, 2005, a.o.).
Work on data-driven approaches has led to insights
into the importance of linguistic features for sen-
tence linearisation decisions (Ringger et al, 2004;
Filippova and Strube, 2009). The availability of dis-
criminative learning techniques for the ranking of
candidate analyses output by broad-coverage gram-
mars with rich linguistic representations, originally
in parsing (Riezler et al, 2000; Riezler et al, 2002),
has also led to a revival of interest in linguistically
sophisticated reversible grammars as the basis for
surface realisation (Velldal and Oepen, 2006; Cahill
et al, 2007). The grammar generates candidate
analyses for an underlying representation and the
ranker?s task is to predict the contextually appropri-
ate realisation.
The work that is most closely related to ours is
Velldal (2008). He uses an MRS representation
derived by an HPSG grammar that can be under-
specified for information status. In his case, the
underspecification is encoded in the grammar and
not directly controlled. In multilingually oriented
linearisation work, Bohnet et al (2010) generate
from semantic corpus annotations included in the
CoNLL?09 shared task data. However, they note that
these annotations are not suitable for full generation
since they are often incomplete. Thus, it is not clear
to which degree these annotations are actually un-
derspecified for certain paraphrases.
2.2 Linguistic Background
In competition-based linguistic theories (Optimal-
ity Theory and related frameworks), the use of
argument alternations is construed as an effect
of markedness hierarchies (Aissen, 1999; Aissen,
2003). Argument functions (subject, object, . . . ) on
1008
the one hand and the various properties that argu-
ment phrases can bear (person, animacy, definite-
ness) on the other are organised in markedness hi-
erarchies. Wherever possible, there is a tendency to
align the hierarchies, i.e., use prominent functions to
realise prominently marked argument phrases. For
instance, Bresnan et al (2001) find that there is a sta-
tistical tendency in English to passivise a verb if the
patient is higher on the person scale than the agent,
but an active is grammatically possible.
Bresnan et al (2007) correlate the use of the En-
glish dative alternation to a number of features such
as givenness, pronominalisation, definiteness, con-
stituent length, animacy of the involved verb argu-
ments. These features are assumed to reflect the dis-
course acessibility of the arguments.
Interestingly, the properties that have been used
to model argument alternations in strict word or-
der languages like English have been identified as
factors that influence word order in free word or-
der languages like German, see Filippova and Strube
(2007) for a number of pointers. Cahill and Riester
(2009) implement a model for German word or-
der variation that approximates the information sta-
tus of constituents through morphological features
like definiteness, pronominalisation etc. We are not
aware of any corpus-based generation studies inves-
tigating how these properties relate to argument al-
ternations in free word order languages.
3 Generation Architecture
Our data-driven methodology for investigating fac-
tors relevant to surface realisation uses a regen-
eration set-up2 with two main components: a) a
grammar-based component used to parse a corpus
sentence and map it to all its meaning-equivalent
surface realisations, b) a statistical ranking compo-
nent used to select the correct, i.e. contextually most
appropriate surface realisation. Two variants of this
set-up that we use are sketched in Figure 1.
We generally use a hand-crafted, broad-coverage
LFG for German (Rohrer and Forst, 2006) to parse
a corpus sentence into a f(unctional) structure3
and generate all surface realisations from a given
2Compare the bidirectional competition set-up in some
Optimality-Theoretic work, e.g., (Kuhn, 2003).
3The choice among alternative f-structures is done with a
discriminative model (Forst, 2007).
Sntx
SVM Ranker
Snta1 Snta2 ... Sntam
LFG grammar
FSa
LFG grammar
Snti
Snty
SVM Ranker
Sntb1 Snta1 Snta2 ... Sntbn
LFG Grammar
FSa FSb
Reverse Sem. Rules
SEM
Sem. Rules
FS1
LFG Grammar
Snti
Figure 1: Generation pipelines
f-structure, following the generation approach of
Cahill et al (2007). F-structures are attribute-
value matrices representing grammatical functions
and morphosyntactic features; their theoretical mo-
tivation lies in the abstraction over details of sur-
face realisation. The grammar is implemented in the
XLE framework (Crouch et al, 2006), which allows
for reversible use of the same declarative grammar
in the parsing and generation direction.
To obtain a more abstract underlying representa-
tion (in the pipeline on the right-hand side of Fig-
ure 1), the present work uses an additional seman-
tic construction component (Crouch and King, 2006;
Zarrie?, 2009) to map LFG f-structures to meaning
representations. For the reverse direction, the mean-
ing representations are mapped to f-structures which
can then be mapped to surface strings by the XLE
generator (Zarrie? and Kuhn, 2010).
For the final realisation ranking step in both
pipelines, we used SVMrank, a Support Vector
Machine-based learning tool (Joachims, 1996). The
ranking step is thus technically independent from the
LFG-based component. However, the grammar is
used to produce the training data, pairs of corpus
sentences and the possible alternations.
The two pipelines allow us to vary the degree to
which the generation input is underspecified. An f-
structure abstracts away from word order, i.e. the
candidate set will contain just word order alterna-
tions. In the semantic input, syntactic function and
voice are underspecified, so a larger set of surface
realisation candidates is generated. Figure 2 illus-
trates the two representation levels for an active and
1009
a passive sentence. The subject of the passive and
the object of the active f-structure are mapped to the
same role (patient) in the meaning representation.
3.1 Issues with ?naive? underspecification
In order to create an underspecified voice represen-
tation that does indeed leave open the realisation op-
tions available to the speaker/writer, it is often not
sufficient to remove just the syntactic function in-
formation. For instance, the subject of the active
sentence (2) is an arbitrary reference pronoun man
?one? which cannot be used as an oblique agent in
a passive, sentence (2-b) is ungrammatical.
(2) a. Man
One
hat
has
den
the
Kanzler
chancellor
gesehen.
seen.
b. *Der
The
Kanzler
chancellor
wurde
was
von
by
man
one
gesehen.
seen.
So, when combined with the grammar, the mean-
ing representation for (2) in Figure 2 contains im-
plicit information about the voice of the original cor-
pus sentence; the candidate set will not include any
passive realisations. However, a passive realisation
without the oblique agent in the by-phrase, as in Ex-
ample (3), is a very natural variant.
(3) Der
The
Kanzler
chancellor
wurde
was
gesehen.
seen.
The reverse situation arises frequently too: pas-
sive sentences where the agent role is not overtly
realised. Given the standard, ?analysis-oriented?
meaning representation for Sentence (4) in Figure
2, the realiser will not generate an active realisation
since the agent role cannot be instantiated by any
phrase in the grammar. However, depending on the
exact context there are typically options for realis-
ing the subject phrase in an active with very little
descriptive content.
Ideally, one would like to account for these phe-
nomena in a meaning representation that under-
specifies the lexicalisation of discourse referents,
and also captures the reference of implicit argu-
ments. Especially the latter task has hardly been
addressed in NLP applications (but see Gerber and
Chai (2010)). In order to work around that problem,
we implemented some simple heuristics which un-
derspecify the realisation of certain verb arguments.
These rules define: 1. a set of pronouns (generic and
neutral pronouns, universal quantifiers) that corre-
spond to ?trivial? agents in active and implicit agents
Active Passive
2-role trans. 71% (82%) 10% (2%)
1-role trans. 11% (0%) 8% (16%)
Table 1: Distribution of voices in SEMh (SEMn)
in passive sentences; 2. a set of prepositional ad-
juncts in passive sentences that correspond to sub-
jects in active sentence (e.g. causative and instru-
mental prepositions like durch ?by means of?); 3.
certain syntactic contexts where special underspec-
ification devices are needed, e.g. coordinations or
embeddings, see Zarrie? and Kuhn (2010) for ex-
amples. In the following, we will distinguish 1-role
transitives where the agent is ?trivial? or implicit
from 2-role transitives with a non-implicit agent.
By means of the extended underspecification rules
for voice, the sentences in (2) and (3) receive an
identical meaning representation. As a result, our
surface realiser can produce an active alternation for
(3) and a passive alternation for (2). In the follow-
ing, we will refer to the extended representations as
SEMh (?heuristic semantics?), and to the original
representations as SEMn (?naive semantics?).
We are aware of the fact that these approximations
introduce some noise into the data and do not always
represent the underlying referents correctly. For in-
stance, the implicit agent in a passive need not be
?trivial? but can correspond to an actual discourse
referent. However, we consider these heuristics as
a first step towards capturing an important discourse
function of the passive alternation, namely the dele-
tion of the agent role. If we did not treat the passives
with an implicit agent on a par with certain actives,
we would have to ignore a major portion of the pas-
sives occurring in corpus data.
Table 1 summarises the distribution of the voices
for the heuristic meaning representation SEMh on
the data-set we will introduce in Section 4, with
the distribution for the naive representation SEMn
in parentheses.
4 Experimental Set-up
Data To obtain a sizable set of realistic corpus ex-
amples for our experiments on voice alternations, we
created our own dataset of input sentences and rep-
resentations, instead of building on treebank exam-
ples as Cahill et al (2007) do. We extracted 19,905
sentences, all containing at least one transitive verb,
1010
f-structure
Example (2)
2
6
6
6
6
4
PRED ?see < (? SUBJ)(? OBJ) >?
SUBJ
?
PRED ?one?
?
OBJ
?
PRED ?chancellor?
?
TOPIC
?
?one?
?
PASS ?
3
7
7
7
7
5
f-structure
Example (3)
2
6
6
4
PRED ?see < NULL (? SUBJ) >?
SUBJ
?
PRED ?chancellor?
?
TOPIC
?
?chancellor?
?
PASS +
3
7
7
5
semantics
Example (2)
HEAD (see)
PAST (see)
ROLE (agent,see,one)
ROLE (patient,see,chancellor)
semantics
Example (3)
HEAD (see)
PAST (see)
ROLE (agent,see,implicit)
ROLE (patient,see,chancellor)
Figure 2: F-structure pair for passive-active alternation
from the HGC, a huge German corpus of newspa-
per text (204.5 million tokens). The sentences are
automatically parsed with the German LFG gram-
mar. The resulting f-structure parses are transferred
to meaning representations and mapped back to f-
structure charts. For our generation experiments,
we only use those f-structure charts that the XLE
generator can map back to a set of surface realisa-
tions. This results in a total of 1236 test sentences
and 8044 sentences in our training set. The data loss
is mostly due to the fact the XLE generator often
fails on incomplete parses, and on very long sen-
tences. Nevertheless, the average sentence length
(17.28) and number of surface realisations (see Ta-
ble 2) are higher than in Cahill et al (2007).
Labelling For the training of our ranking model,
we have to tell the learner how closely each surface
realisation candidate resembles the original corpus
sentence. We distinguish the rank categories: ?1?
identical to the corpus string, ?2? identical to the
corpus string ignoring punctuation, ?3? small edit
distance (< 4) to the corpus string ignoring punc-
tuation, ?4? different from the corpus sentence. In
one of our experiments (Section 5.1), we used the
rank category ?5? to explicitly label the surface real-
isations derived from the alternation f-structure that
does not correspond to the parse of the original cor-
pus sentence. The intermediate rank categories ?2?
and ?3? are useful since the grammar does not al-
ways regenerate the exact corpus string, see Cahill
et al (2007) for explanation.
Features The linguistic theories sketched in Sec-
tion 2.2 correlate morphological, syntactic and se-
mantic properties of constituents (or discourse ref-
erents) with their order and argument realisation. In
our system, this correlation is modelled by a combi-
nation of linguistic properties that can be extracted
from the f-structure or meaning representation and
of the surface order that is read off the sentence
string. Standard n-gram features are also used as
features.4 The feature model is built as follows:
for every lemma in the f-structure, we extract a set
of morphological properties (definiteness, person,
pronominal status etc.), the voice of the verbal head,
its syntactic and semantic role, and a set of infor-
mations status features following Cahill and Riester
(2009). These properties are combined in two ways:
a) Precedence features: relative order of properties
in the surface string, e.g. ?theme < agent in pas-
sive?, ?1st person < 3rd person?; b) ?scale align-
ment? features (ScalAl.): combinations of voice and
role properties with morphological properties, e.g.
?subject is singular?, ?agent is 3rd person in active
voice? (these are surface-independent, identical for
each alternation candidate).
The model for which we present our results is
based on sentence-internal features only; as Cahill
and Riester (2009) showed, these feature carry a
considerable amount of implicit information about
the discourse context (e.g. in the shape of referring
expressions). We also implemented a set of explic-
itly inter-sentential features, inspired by Centering
Theory (Grosz et al, 1995). This model did not im-
prove over the intra-sentential model.
Evaluation Measures In order to assess the gen-
eral quality of our generation ranking models, we
4The language model is trained on the German data release
for the 2009 ACL Workshop on Machine Translation shared
task, 11,991,277 total sentences.
1011
FS SEMn SEMh
Avg. # strings 36.7 68.2 75.8
Random Match 16.98 10.72 7.28
LM
Match 15.45 15.04 11.89
BLEU 0.68 0.68 0.65
NIST 13.01 12.95 12.69
Ling. Model
Match 27.91 27.66 26.38
BLEU 0.764 0.759 0.747
NIST 13.18 13.14 13.01
Table 2: Evaluation of Experiment 1
use several standard measures: a) exact match:
how often does the model select the original cor-
pus sentence, b) BLEU: n-gram overlap between
top-ranked and original sentence, c) NIST: modifi-
cation of BLEU giving more weight to less frequent
n-grams. Second, we are interested in the model?s
performance wrt. specific linguistic criteria. We re-
port the following accuracies: d) Voice: how often
does the model select a sentence realising the correct
voice, e) Precedence: how often does the model gen-
erate the right order of the verb arguments (agent and
patient), and f) Vorfeld: how often does the model
correctly predict the verb arguments to appear in the
sentence initial position before the finite verb, the
so-called Vorfeld. See Sections 5.3 and 6 for a dis-
cussion of these measures.
5 Experiments
5.1 Exp. 1: Effect of Underspecified Input
We investigate the effect of the input?s underspecifi-
cation on a state-of-the-art surface realisation rank-
ing model. This model implements the entire fea-
ture set described in Section 4 (it is further analysed
in the subsequent experiments). We built 3 datasets
from our alternation data: FS - candidates generated
from the f-structure; SEMn - realisations from the
naive meaning representations; SEMh - candidates
from the heuristically underspecified meaning rep-
resentation. Thus, we keep the set of original cor-
pus sentences (=the target realisations) constant, but
train and test the model on different candidate sets.
In Table 2, we compare the performance of the
linguistically informed model described in Section 4
on the candidates sets against a random choice and
a language model (LM) baseline. The differences in
BLEU between the candidate sets and models are
FS SEMn SEMh SEMn?
A
ll
Tr
an
s. Voice Acc. 100 98.06 91.05 97.59
Voice Spec. 100 22.8 0 0
Majority BL 82.4 98.1
2-
ro
le
Tr
an
s. Voice Acc. 100 97.7 91.8 97.59
Voice Spec. 100 8.33 0 0
Majority BL 88.5 98.1
1-
ro
le
Tr
an
s. Voice Acc. 100 100 90.0 -
Voice Spec. 100 100 0 -
Majority BL 53.9 -
Table 3: Accuracy of Voice Prediction by Ling. Model in
Experiment 1
statistically significant.5 In general, the linguistic
model largely outperforms the LM and is less sen-
sitive to the additional confusion introduced by the
SEMh input. Its BLEU score and match accuracy
decrease only slightly (though statistically signifi-
cantly).
In Table 3, we report the performance of the lin-
guistic model on the different candidate sets with re-
spect to voice accuracy. Since the candidate sets dif-
fer in the proportion of items that underspecify the
voice (see ?Voice Spec.? in Table 3), we also report
the accuracy on the SEMn? test set, which is a sub-
set of SEMn excluding the items where the voice is
specified. Table 3 shows that the proportion of active
realisations for the SEMn? input is very high, and
the model does not outperform the majority baseline
(which always selects active). In contrast, the SEMh
model clearly outperforms the majority baseline.
Example (4) is a case from our development set
where the SEMn model incorrectly predicts an ac-
tive (4-a), and the SEMh correctly predicts a passive
(4-b).
(4) a. 26
26
kostspielige
expensive
Studien
studies
erwa?hnten
mentioned
die
the
Finanzierung.
funding.
b. Die
The
Finanzierung
funding
wurde
was
von
by
26
26
kostspieligen
expensive
Studien
studies
erwa?hnt.
mentioned.
This prediction is according to the markedness hier-
archy: the patient is singular and definite, the agent
5According to a bootstrap resampling test, p < 0.05
1012
Features Match BLEU Voice Prec. VF
Prec. 16.3 0.70 88.43 64.1 59.1
ScalAl. 10.4 0.64 90.37 58.9 56.3
Union 26.4 0.75 91.50 80.2 70.9
Table 4: Evaluation of Experiment 2
is plural and indefinite. Counterexamples are possi-
ble, but there is a clear statistical preference ? which
the model was able to pick up.
On the one hand, the rankers can cope surpris-
ingly well with the additional realisations obtained
from the meaning representations. According to the
global sentence overlap measures, their quality is
not seriously impaired. On the other hand, the de-
sign of the representations has a substantial effect
on the prediction of the alternations. The SEMn
does not seem to learn certain preferences because
of the extremely imbalanced distribution in the in-
put data. This confirms the hypothesis sketched in
Section 3.1, according to which the degree of the
input?s underspecification can crucially change the
behaviour of the ranking model.
5.2 Exp. 2: Word Order and Voice
We examine the impact of certain feature types on
the prediction of the variation types in our data. We
are particularly interested in the interaction of voice
and word order (precedence) since linguistic theo-
ries (see Section 2.2) predict similar information-
structural factors guiding their use, but usually do
not consider them in conjunction.
In Table 4, we report the performance of ranking
models trained on the different feature subsets intro-
duced in Section 4. The union of the features corre-
sponds to the model trained on SEMh in Experiment
1. At a very broad level, the results suggest that the
precedence and the scale alignment features interact
both in the prediction of voice and word order.
The most pronounced effect on voice accuracy
can be seen when comparing the precedence model
to the union model. Adding the surface-independent
scale alignment features to the precedence features
leads to a big improvement in the prediction of word
order. This is not a trivial observation since a) the
surface-independent features do not discriminate be-
tween the word orders and b) the precedence fea-
tures are built from the same properties (see Sec-
tion 4). Thus, the SVM learner discovers depen-
dencies between relative precedence preferences and
abstract properties of a verb argument which cannot
be encoded in the precedence alone.
It is worth noting that the precedence features im-
prove the voice prediction. This indicates that wher-
ever the application context allows it, voice should
not be specified at a stage prior to word order. Ex-
ample (5) is taken from our development set, illus-
trating a case where the union model predicted the
correct voice and word order (5-a), and the scale
alignment model top-ranked the incorrect voice and
word order. The active verb arguments in (5-b) are
both case-ambigous and placed in the non-canonical
order (object < subject), so the semantic relation can
be easily misunderstood. The passive in (5-a) is un-
ambiguous since the agent is realised in a PP (and
placed in the Vorfeld).
(5) a. Von
By
den
the
deutschen
German
Medien
media
wurden
were
die
the
Ausla?nder
foreigners
nur
only
erwa?hnt,
mentioned,
wenn
when
es
there
Zoff
trouble
gab.
was.
b. Wenn
When
es
there
Zoff
trouble
gab,
was,
erwa?hnten
mentioned
die
the
Ausla?nder
foreigners
nur
only
die
the
deutschen
German
Medien.
media.
Moreover, our results confirm Filippova and
Strube (2007) who find that it is harder to predict
the correct Vorfeld occupant in a German sentence,
than to predict the relative order of the constituents.
5.3 Exp. 3: Capturing Flexible Variation
The previous experiment has shown that there is a
certain inter-dependence between word order and
voice. This experiment addresses this interaction
by varying the way the training data for the ranker
is labelled. We contrast two ways of labelling the
sentences (see Section 4): a) all sentences that are
not (nearly) identical to the reference sentence have
the rank category ?4?, irrespective of their voice (re-
ferred to as unlabelled model), b) the sentences that
do not realise the correct voice are ranked lower than
sentences with the correct voice (?4? vs. ?5?), re-
ferred to as labelled model. Intuitively, the latter
way of labelling tells the ranker that all sentences
in the incorrect voice are worse than all sentences
in the correct voice, independent of the word order.
Given the first labelling strategy, the ranker can de-
cide in an unsupervised way which combinations of
word order and voice are to be preferred.
1013
Top 1 Top 1 Top 1 Top 2 Top 3
Model Match BLEU NIST Voice Prec. Prec.+Voice Prec.+Voice Prec.+Voice
Labelled, no LM 21.52 0.73 12.93 91.9 76.25 71.01 78.35 82.31
Unlabelled, no LM 26.83 0.75 13.01 91.5 80.19 74.51 84.28 88.59
Unlabeled + LM 27.35 0.75 13.08 91.5 79.6 73.92 79.74 82.89
Table 5: Evaluation of Experiment 3
In Table 5, it can be seen that the unlabelled model
improves over the labelled on all the sentence over-
lap measures. The improvements are statistically
significant. Moreover, we compare the n-best ac-
curacies achieved by the models for the joint pre-
diction of voice and argument order. The unla-
belled model is very flexible with respect to the word
order-voice interaction: the accuracy dramatically
improves when looking at the top 3 sentences. Ta-
ble 5 also reports the performance of an unlabelled
model that additionally integrates LM scores. Sur-
prisingly, these scores have a very small positive ef-
fect on the sentence overlap features and no positive
effect on the voice and precedence accuracy. The
n-best evaluations even suggest that the LM scores
negatively impact the ranker: the accuracy for the
top 3 sentences increases much less as compared to
the model that does not integrate LM scores.6
The n-best performance of a realisation ranker is
practically relevant for re-ranking applications such
as Velldal (2008). We think that it is also concep-
tually interesting. Previous evaluation studies sug-
gest that the original corpus sentence is not always
the only optimal realisation of a given linguistic in-
put (Cahill and Forst, 2010; Belz and Kow, 2010).
Humans seem to have varying preferences for word
order contrasts in certain contexts. The n-best evalu-
ation could reflect the behaviour of a ranking model
with respect to the range of variations encountered
in real discourse. The pilot human evaluation in the
next Section deals with this question.
6 Human Evaluation
Our experiment in Section 5.3 has shown that the ac-
curacy of our linguistically informed ranking model
dramatically increases when we consider the three
6(Nakanishi et al, 2005) also note a negative effect of in-
cluding LM scores in their model, pointing out that the LM was
not trained on enough data. The corpus used for training our
LM might also have been too small or distinct in genre.
best sentences rather than only the top-ranked sen-
tence. This means that the model sometimes predicts
almost equal naturalness for different voice realisa-
tions. Moreover, in the case of word order, we know
from previous evaluation studies, that humans some-
times prefer different realisations than the original
corpus sentences. This Section investigates agree-
ment in human judgements of voice realisation.
Whereas previous studies in generation mainly
used human evaluation to compare different sys-
tems, or to correlate human and automatic evalua-
tions, our primary interest is the agreement or cor-
relation between human rankings. In particular, we
explore the hypothesis that this agreement is higher
in certain contexts than in others. In order to select
these contexts, we use the predictions made by our
ranking model.
The questionnaire for our experiment comprised
24 items falling into 3 classes: a) items where the
3 best sentences predicted by the model have the
same voice as the original sentence (?Correct?), b)
items where the 3 top-ranked sentences realise dif-
ferent voices (?Mixed?), c) items where the model
predicted the incorrect voice in all 3 top sentences
(?False?). Each item is composed of the original
sentence, the 3 top-ranked sentences (if not identical
to the corpus sentence) and 2 further sentences such
that each item contains different voices. For each
item, we presented the previous context sentence.
The experiment was completed by 8 participants,
all native speakers of German, 5 had a linguistic
background. The participants were asked to rank
each sentence on a scale from 1-6 according to its
naturalness and plausibility in the given context. The
participants were explicitly allowed to use the same
rank for sentences they find equally natural. The par-
ticipants made heavy use of this option: out of the
192 annotated items, only 8 are ranked such that no
two sentences have the same rank.
We compare the human judgements by correlat-
1014
ing them with Spearman?s ?. This measure is con-
sidered appropriate for graded annotation tasks in
general (Erk and McCarthy, 2009), and has also
been used for analysing human realisation rankings
(Velldal, 2008; Cahill and Forst, 2010). We nor-
malise the ranks according to the procedure in Vell-
dal (2008). In Table 6, we report the correlations
obtained from averaging over all pairwise correla-
tions between the participants and the correlations
restricted to the item and sentence classes. We used
bootstrap re-sampling on the pairwise correlations to
test that the correlations on the different item classes
significantly differ from each other.
The correlations in Table 6 suggest that the agree-
ment between annotators is highest on the false
items, and lowest on the mixed items. Humans
tended to give the best rank to the original sentence
more often on the false items (91%) than on the oth-
ers. Moreover, the agreement is generally higher on
the sentences realising the correct voice.
These results seem to confirm our hypothesis that
the general level of agreement between humans dif-
fers depending on the context. However, one has to
be careful in relating the effects in our data solely to
voice preferences. Since the sentences were chosen
automatically, some examples contain very unnatu-
ral word orders that probably guided the annotators?
decisions more than the voice. This is illustrated
by Example (6) showing two passive sentences from
our questionnaire which differ only in the position of
the adverb besser ?better?. Sentence (6-a) is com-
pletely implausible for a native speaker of German,
whereas Sentence (6-b) sounds very natural.
(6) a. Durch
By
das
the
neue
new
Gesetz
law
sollen
should
besser
better
Eigenheimbesitzer
house owners
geschu?tzt
protected
werden.
be.
b. Durch
By
das
the
neue
new
Gesetz
law
sollen
should
Eigenheimbesitzer
house owners
besser
better
geschu?tzt
protected
werden.
be.
This observation brings us back to our initial point
that the surface realisation task is especially chal-
lenging due to the interaction of a range of semantic
and discourse phenomena. Obviously, this interac-
tion makes it difficult to single out preferences for a
specific alternation type. Future work will have to
establish how this problem should be dealt with in
Items
All Correct Mixed False
?All? sent. 0.58 0.6 0.54 0.62
?Correct? sent. 0.64 0.63 0.56 0.72
?False? sent. 0.47 0.57 0.48 0.44
Top-ranked
corpus sent.
84% 78% 83% 91%
Table 6: Human Evaluation
the design of human evaluation experiments.
7 Conclusion
We have presented a grammar-based generation ar-
chitecture which implements the surface realisation
of meaning representations abstracting from voice
and word order. In order to be able to study voice
alternations in a variety of contexts, we designed
heuristic underspecification rules which establish,
for instance, the alternation relation between an ac-
tive with a generic agent and a passive that does
not overtly realise the agent. This strategy leads
to a better balanced distribution of the alternations
in the training data, such that our linguistically
informed generation ranking model achieves high
BLEU scores and accurately predicts active and pas-
sive. In future work, we will extend our experiments
to a wider range of alternations and try to capture
inter-sentential context more explicitly. Moreover, it
would be interesting to carry over our methodology
to a purely statistical linearisation system where the
relation between an input representation and a set of
candidate realisations is not so clearly defined as in
a grammar-based system.
Our study also addressed the interaction of dif-
ferent linguistic variation types, i.e. word order
and voice, by looking at different types of linguis-
tic features and exploring different ways of labelling
the training data. However, our SVM-based learn-
ing framework is not well-suited to directly assess
the correlation between a certain feature (or fea-
ture combination) and the occurrence of an alterna-
tion. Therefore, it would be interesting to relate our
work to the techniques used in theoretical papers,
e.g. (Bresnan et al, 2007), where these correlations
are analysed more directly.
1015
References
Judith Aissen. 1999. Markedness and subject choice in
optimality theory. Natural Language and Linguistic
Theory, 17(4):673?711.
Judith Aissen. 2003. Differential Object Marking:
Iconicity vs. Economy. Natural Language and Lin-
guistic Theory, 21:435?483.
Anja Belz and Eric Kow. 2010. Comparing rating
scales and preference judgements in language evalu-
ation. In Proceedings of the 6th International Natural
Language Generation Conference (INLG?10).
Anja Belz, Mike White, Josef van Genabith, Deirdre
Hogan, and Amanda Stent. 2010. Finding common
ground: Towards a surface realisation shared task.
In Proceedings of the 6th International Natural Lan-
guage Generation Conference (INLG?10).
Anja Belz. 2005. Statistical generation: Three meth-
ods compared and evaluated. In Proceedings of Tenth
European Workshop on Natural Language Generation
(ENLG-05), pages 15?23.
Bernd Bohnet, Leo Wanner, Simon Mill, and Alicia
Burga. 2010. Broad coverage multilingual deep sen-
tence generation with a stochastic multi-level realizer.
In Proceedings of the 23rd International Conference
on Computational Linguistics (COLING 2010), Bei-
jing, China.
Joan Bresnan, Shipra Dingare, and Christopher D. Man-
ning. 2001. Soft Constraints Mirror Hard Constraints:
Voice and Person in English and Lummi. In Proceed-
ings of the LFG ?01 Conference.
Joan Bresnan, Anna Cueni, Tatiana Nikitina, and Harald
Baayen. 2007. Predicting the Dative Alternation. In
G. Boume, I. Kraemer, and J. Zwarts, editors, Cogni-
tive Foundations of Interpretation. Amsterdam: Royal
Netherlands Academy of Science.
Aoife Cahill and Martin Forst. 2010. Human Evaluation
of a German Surface Realisation Ranker. In Proceed-
ings of the 12th Conference of the European Chapter
of the ACL (EACL 2009), pages 112 ? 120, Athens,
Greece. Association for Computational Linguistics.
Aoife Cahill and Arndt Riester. 2009. Incorporating
Information Status into Generation Ranking. In Pro-
ceedings of the 47th Annual Meeting of the ACL, pages
817?825, Suntec, Singapore, August. Association for
Computational Linguistics.
Aoife Cahill, Martin Forst, and Christian Rohrer. 2007.
Stochastic realisation ranking for a free word order
language. In Proceedings of the Eleventh European
Workshop on Natural Language Generation, pages
17?24, Saarbru?cken, Germany, June. DFKI GmbH.
Document D-07-01.
Dick Crouch and Tracy Holloway King. 2006. Se-
mantics via F-Structure Rewriting. In Miriam Butt
and Tracy Holloway King, editors, Proceedings of the
LFG06 Conference.
Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy King,
John Maxwell, and Paula Newman. 2006. XLE Docu-
mentation. Technical report, Palo Alto Research Cen-
ter, CA.
Katrin Erk and Diana McCarthy. 2009. Graded Word
Sense Assignment. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 440 ? 449, Singapore.
Katja Filippova and Michael Strube. 2007. Generat-
ing constituent order in German clauses. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics (ACL 07), Prague, Czech
Republic.
Katja Filippova and Michael Strube. 2009. Tree lin-
earization in English: Improving language model
based approaches. In Companion Volume to the Pro-
ceedings of Human Language Technologies Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL-HLT 09,
short)., Boulder, Colorado.
Martin Forst. 2007. Filling Statistics with Linguistics
? Property Design for the Disambiguation of German
LFG Parses. In ACL 2007 Workshop on Deep Linguis-
tic Processing, pages 17?24, Prague, Czech Republic,
June. Association for Computational Linguistics.
Matthew Gerber and Joyce Chai. 2010. Beyond nom-
bank: A study of implicit argumentation for nominal
predicates. In Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD).
Barbara J. Grosz, Aravind Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Thorsten Joachims. 1996. Training linear svms in linear
time. In M. Butt and T. H. King, editors, Proceedings
of the ACM Conference on Knowledge Discovery and
Data Mining (KDD), CSLI Proceedings Online.
Jonas Kuhn. 2003. Optimality-Theoretic Syntax?A
Declarative Approach. CSLI Publications, Stanford,
CA.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of the ACL/COLING-98, pages 704?710,
Montreal, Quebec.
Tomasz Marciniak and Michael Strube. 2005. Using
an annotated corpus as a knowledge source for lan-
guage generation. In Proceedings of Workshop on Us-
ing Corpora for Natural Language Generation, pages
19?24, Birmingham, UK.
Hiroko Nakanishi, Yusuke Miyao, and Junichi Tsujii.
2005. Probabilistic models for disambiguation of an
1016
HPSG-based chart generator. In Proceedings of IWPT
2005.
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proceedings of
NAACL 2000, pages 194?201, Seattle, WA.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized stochastic modeling of
constraint-based grammars using log-linear measures
and EM training. In Proceedings of the 38th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?00), Hong Kong, pages 480?487.
Stefan Riezler, Dick Crouch, Ron Kaplan, Tracy King,
John Maxwell, and Mark Johnson. 2002. Parsing the
Wall Street Journal using a Lexical-Functional Gram-
mar and discriminative estimation techniques. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?02), Pennsyl-
vania, Philadelphia.
Eric K. Ringger, Michael Gamon, Robert C. Moore,
David Rojas, Martine Smets, and Simon Corston-
Oliver. 2004. Linguistically Informed Statistical
Models of Constituent Structure for Ordering in Sen-
tence Realization. In Proceedings of the 2004 In-
ternational Conference on Computational Linguistics,
Geneva, Switzerland.
Christian Rohrer and Martin Forst. 2006. Improving
coverage and parsing quality of a large-scale LFG for
German. In Proceedings of LREC-2006.
Erik Velldal and Stephan Oepen. 2006. Statistical rank-
ing in tactical generation. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, Sydney, Australia.
Erik Velldal. 2008. Empirical Realization Ranking.
Ph.D. thesis, University of Oslo, Department of Infor-
matics.
Sina Zarrie? and Jonas Kuhn. 2010. Reversing F-
structure Rewriting for Generation from Meaning Rep-
resentations. In Proceedings of the LFG10 Confer-
ence, Ottawa.
Sina Zarrie?. 2009. Developing German Semantics on
the basis of Parallel LFG Grammars. In Proceed-
ings of the 2009 Workshop on Grammar Engineering
Across Frameworks (GEAF 2009), pages 10?18, Sun-
tec, Singapore, August. Association for Computational
Linguistics.
1017
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1547?1557,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Combining Referring Expression Generation and Surface Realization:
A Corpus-Based Investigation of Architectures
Sina Zarrie? Jonas Kuhn
Institut fu?r maschinelle Sprachverarbeitung
University of Stuttgart, Germany
sina.zarriess,jonas.kuhn@ims.uni-stuttgart.de
Abstract
We suggest a generation task that inte-
grates discourse-level referring expression
generation and sentence-level surface re-
alization. We present a data set of Ger-
man articles annotated with deep syntax
and referents, including some types of im-
plicit referents. Our experiments compare
several architectures varying the order of a
set of trainable modules. The results sug-
gest that a revision-based pipeline, with in-
termediate linearization, significantly out-
performs standard pipelines or a parallel
architecture.
1 Introduction
Generating well-formed linguistic utterances from
an abstract non-linguistic input involves making
a multitude of conceptual, discourse-level as well
as sentence-level, lexical and syntactic decisions.
Work on rule-based natural language generation
(NLG) has explored a number of ways to com-
bine these decisions in an architecture, ranging
from integrated systems where all decisions hap-
pen jointly (Appelt, 1982) to strictly sequential
pipelines (Reiter and Dale, 1997). While inte-
grated or interactive systems typically face issues
with efficiency and scalability, they can directly
account for interactions between discourse-level
planning and linguistic realization. For instance,
Rubinoff (1992) mentions Example (1) where the
sentence planning component needs to have ac-
cess to the lexical knowledge that ?order? and not
?home? can be realized as a verb in English.
(1) a. *John homed him with an order.
b. John ordered him home.
In recent data-driven generation research, the
focus has somewhat shifted from full data-to-text
systems to approaches that isolate well-defined
subproblems from the NLG pipeline. In particular,
the tasks of surface realization and referring ex-
pression generation (REG) have received increas-
ing attention using a number of available anno-
tated data sets (Belz and Kow, 2010; Belz et al,
2011). While these single-task approaches have
given rise to many insights about algorithms and
corpus-based modelling for specific phenomena,
they can hardly deal with aspects of the architec-
ture and interaction between generation levels.
This paper suggests a middle ground between
full data-to-text and single-task generation, com-
bining two well-studied NLG problems. We in-
tegrate a discourse-level approach to REG with
sentence-level surface realization in a data-driven
framework. We address this integrated task with a
set of components that can be trained on flexible
inputs which allows us to systematically explore
different ways of arranging the components in a
generation architecture. Our main goal is to inves-
tigate how different architectural set-ups account
for interactions between generation decisions at
the level of referring expressions (REs), syntax
and word order.
Our basic set-up is inspired from the Generating
Referring Expressions in Context (GREC) tasks,
where candidate REs have to be assigned to in-
stances of a referent in a Wikipedia article (Belz
and Kow, 2010). We have created a dataset of Ger-
man texts with annotations that extend this stan-
dard in three substantial ways: (i) our domain con-
sists of articles about robbery events that mainly
involve two main referents, a victim and a per-
petrator (perp), (ii) annotations include deep and
shallow syntactic relations similar to the repre-
sentations used in (Belz et al, 2011) (iii) anno-
tations include empty referents, as e.g. in passives
and nominalizations directing attention to the phe-
nomenon of implicit reference, which is largely
understudied in NLG. Figure 1 presents an exam-
ple for a deep syntax tree with underspecified RE
1547
(Tree) be
agent
perp
mod
on
pobj
trial
mod
because
sub
attack
agent
perp
theme
victim
perp italians
two
men
the two
they <empty>
victim man
a young
victim
the
he <empty>
Figure 1: Underspecified tree with RE candidates
slots and lists of candidates REs for each referent.
Applying a strictly sequential pipeline on our
data, we observe incoherent system output that
is related to an interaction of generation levels,
very similar to the interleaving between sentence
planning and lexicalization in Example (1). A
pipeline that first inserts REs into the underspec-
ified tree in Figure 1, then generates syntax and fi-
nally linearizes, produces inappropriate sentences
like (2-a).
(2) a. *[The two men]p are on trial because of an attack
by [two italians]p on [a young man]v .
b. [Two italians]p are on trial because of an attack on
[a young man]v .
Sentence (2-a) is incoherent because the syntac-
tic surface obscurs the intended meaning that ?two
italians? and ?the two men? refer to the same ref-
erent. In order to generate the natural Sentence
(2-b), the RE component needs information about
linear precedence of the two perp instances and the
nominalization of ?attack?. These types of inter-
actions between referential and syntactic realiza-
tion have been thoroughly discussed in theoretical
accounts of textual coherence, as e.g. Centering
Theory (Grosz et al, 1995).
The integrated modelling of REG and surface
realization leads to a considerable expansion of
the choice space. In a sentence with 3 referents
that each have 10 RE candidates and can be freely
ordered, the number of surface realizations in-
creases from 6 to 6?103, assuming that the remain-
ing words can not be syntactically varied. Thus,
even when the generation problem is restricted to
these tasks, a fully integrated architecture faces
scalability issues on realistic corpus data.
In this work, we assume a modular set-up of
the generation system that allows for a flexible
ordering of the single components. Our experi-
ments vary 3 parameters of the generation archi-
tecture: 1) the sequential order of the modules,
2) parallelization of modules, 3) joint vs. sepa-
rate modelling of implicit referents. Our results
suggest that the interactions between RE and syn-
tax can be modelled in sequential generation ar-
chitecture where the RE component has access
to information about syntactic realization and an
approximative, intermediate linearization. Such
a system is reminiscent of earlier work in rule-
based generation that implements an interactive or
revision-based feedback between discourse-level
planning and linguistic realisation (Hovy, 1988;
Robin, 1993).
2 Related Work
Despite the common view of NLG as a pipeline
process, it is a well-known problem that high-
level, conceptual knowledge and low-level lin-
guistic knowledge are tightly interleaved (Danlos,
1984; Mellish et al, 2000). In rule-based, strictly
sequential generators these interactions can lead
to a so-called generation gap, where a down-
stream module cannot realize a text or sentence
plan generated by the preceding modules (Meteer,
1991; Wanner, 1994). For this reason, a num-
ber of other architectures has been proposed, see
De Smedt et al (1996) for an overview. For rea-
sons of tractability and scalability, many prac-
tical NLG systems still have been designed as
sequential pipelines that follow the basic layout
of macroplanning-microplanning-linguistic real-
ization (Reiter, 1994; Cahill et al, 1999; Bateman
and Zock, 2003).
In recent data-driven research on NLG, many
single tasks have been addressed with corpus-
based methods. For surface realization, the stan-
dard set-up is to regenerate from syntactic rep-
resentations that have been produced for realis-
tic corpus sentences. The first widely known sta-
tistical approach by Langkilde and Knight (1998)
used language-model n-gram statistics on a word
lattice of candidate realisations to guide a ranker.
Subsequent work explored ways of exploiting lin-
guistically annotated data for trainable generation
models (Ratnaparkhi, 2000; Belz, 2005). Work on
data-driven approaches has led to insights about
the importance of linguistic features for sentence
1548
linearization decisions (Ringger et al, 2004; Filip-
pova and Strube, 2007; Cahill and Riester, 2009).
(Zarrie? et al, 2012) have recently argued that
the good performance of these linguistically mo-
tivated word order models, which exploit morpho-
syntactic features of noun phrases (i.e. refer-
ents), is related to the fact that these morpho-
syntactic features implicitly encode a lot of knowl-
edge about the underlying discourse or informa-
tion structure.
A considerable body of REG research has been
done in the paradigm established by Dale (1989;
1995). More closely related to our work are ap-
proaches in the line of Siddarthan and Copes-
take (2004) or Belz and Varges (2007) who gener-
ate contextually appropriate REs for instances of a
referent in a text. Belz and Varges (2007)?s GREC
data set includes annotations of implicit subjects
in coordinations. Zarrie? et al (2011) deal with
implicit subjects in passives, proposing a set of
heuristics for adding these agents to the genera-
tion input. Roth and Frank (2012) acquire au-
tomatic annotations of implicit roles for the pur-
pose of studying coherence patterns in texts. Im-
plicit referents have also received attention for the
analysis of semantic roles (Gerber and Chai, 2010;
Ruppenhofer et al, 2010).
Statistical methods for data-to-text generation
have been explored only recently. Belz (2008)
trains a probabilistic CFG to generate weather
forecasts, Chen et al (2010) induce a synchronous
grammar to generate sportcaster text. Both ad-
dress a restricted domain where a direct align-
ment between units in the non-linguistic represen-
tation and the linguistic utterance can be learned.
Marciniak and Strube (2005) propose an ILP
model for global optimization in a generation task
that is decomposed into a set of classifiers. Bohnet
et al (2011) deal with multi-level generation in a
statistical framework and in a less restricted do-
main. They adopt a standard sequential pipeline
approach.
Recent corpus-based generation approaches
faced the problem that existing standard treebank
representations for parsing or other analysis tasks
do not necessarily fit the needs of generation
(Bohnet et al, 2010; Wanner et al, 2012). Zarrie?
et al (2011) discuss the problem of an input rep-
resentation that is appropriately underspecified for
the realistic generation of voice alternations.
3 The Data Set
The data set for our generation experiments con-
sists of 200 newspaper articles about robbery
events. The articles were extracted from a large
German newspaper corpus. A complete example
text with RE annotations is given in Figure 2, Ta-
ble 1 summarizes some data set statistics.
3.1 RE annotation
The RE annotations mark explicit and implicit
mentions of referents involved in the robbery event
described in an article. Explicit mentions are
marked as spans on the surface sentence, labeled
with the referent?s role and an ID. We annotate the
following referential roles: (i) perpetrator (perp),
(ii) victim, (iii) source, according to the core roles
of the Robbery frame in English FrameNet. We
include source since some texts do not mention a
particular victim, but rather the location of the rob-
bery (e.g. a bank, a service station). The ID distin-
guishes referents that have the same role, e.g. ?the
husband? and the ?young family? in Sentences
(3-a) and (3-d) in Figure 2. Each RE is linked to
its syntactic head. This complies with the GREC
data sets, and is also useful for further annotation
of the deep syntax level (see Section 3.2).
The RE implicit mentions of victim, perp, and
source are annotated as attributes of their syntac-
tic heads in the surface sentence. We consider the
following types of implicit referents: (i) agents in
passives (e.g. ?robbed? in (3-a)), (ii) arguments of
nominalizations (e.g. ?resistance? in (3-e)), (iii)
possessives (e.g. ?watch? in (3-f)), (iv) missing
subjects in coordinations. (e.g. ?flee? in (3-f))
The brat tool (Stenetorp et al, 2012) was used
for annotation. We had 2 annotators with a compu-
tational linguistic background, provided with an-
notation guidelines. They were trained on a set of
20 texts. We measure a good agreement on another
set of 15 texts: the simple pairwise agreement for
explicit mentions is 95.14%-96.53% and 78.94%-
76.92% for implicit mentions.1
3.2 Syntax annotation
The syntactic annotation of our data includes two
layers: shallow and deep, labeled dependencies,
similar to the representation used in surface real-
ization shared tasks (Belz et al, 2011). We use
1Standard measures for the ?above chance annotator
agreement? are only defined for task where the set of anno-
tated items is pre-defined.
1549
(3) a.
 Junge Familie v:0 Young family aufon demthe Heimwegposs:vway homeposs:v ausgeraubtag:probbedag:p
b. Die
The
Polizei
police
sucht
looks
nach
for
zwei ungepflegt wirkenden jungen Ma?nnern im Alter von etwa 25 Jahren p:0.
two shabby-looking young men of about 25 years .
c. Sie p:0
They
sollen
are said to
am
on
Montag
Monday
gegen
around
20
20
Uhr
o?clock
 eine junge Familie mit ihrem sieben Monate alten Baby v:0 a young family with their seven month old baby aufondem
the
Heimwegposs:v
way homeposs:v
von
from
einem
a
Einkaufsbummel
shopping tour
u?berfallen
attacked
und
and
ausgeraubt
robbed
haben.
have.
d. Wie
As
die
the
Polizei
police
berichtet,
reports,
drohten
threatened
die zwei Ma?nner p:0
the two men
  dem Ehemann v:1,  the husband
  ihn v:1  him zusammenzuschlagen.beat up.
e.
  Er v:1  He gabgave deshalbtherefore
  seine v:1  his Brieftaschewallet ohnewithout Gegenwehrag:v,the:presistanceag:v,the:p heraus.out.
f. Anschlie?end
Afterwards
nahmen
took
  ihm v:1  him die Ra?uber p:0the robbers nochalso diethe Armbanduhrposs:vwatchposs:v aboff undand flu?chtetenag:p.fleedag:p.
Figure 2: Example text with RE annotations, oval boxes mark victim mentions, square boxes mark perp
mentions, heads of implicit arguments are underlined
the Bohnet (2010) dependency parser to obtain an
automatic annotation of shallow or surface depen-
dencies for the corpus sentences.
The deep syntactic dependencies are derived
from the shallow layer by a set of hand-written
transformation rules. The goal is to link referents
to their main predicate in a uniform way, indepen-
dently of the surface-syntactic realization of the
verb. We address passives, nominalizations and
possessives corresponding to the contexts where
we annotated implicit referents (see above). The
transformations are defined as follows:
1. remove auxiliary nodes, verb morphology and finite-
ness, a tense feature distinguishes past and present, e.g.
?haben:AUX u?berfallen:VVINF? (have attacked) maps
to ?u?berfallen:VV:PAST? (attack:PAST)
2. map subjects in actives and oblique agents in passives
to ?agents?; objects in actives and subjects in passive to
?themes?, e.g. victim/subj was attacked by perp/obl-ag
maps to perp/agent attack victim/theme
3. attach particles to verb lemma, e.g. ?gab? ... ?heraus?
in (3-e) is mapped to ?herausgeben? (give to)
4. map nominalized to verbal lemmas, their prepositional
and genitive arguments to semantic subjects and ob-
jects, e.g. attack on victim is mapped to attack vic-
tim/theme
5. normalize prenominal and genitive postnominal poses-
sives, e.g. ?seine Brieftasche? (his wallet) and ?die
Brieftasche des Opfers? (the wallet of the victim) map
to ?die Brieftasche POSS victim? (the wallet of victim),
only applies if possessive is an annotated RE
Nominalizations are mapped to their verbal
base forms on the basis of lexicalized rules for the
nominalized lemmas observed in the corpus. The
other transformations are defined on the shallow
dependency annotation.
# sentences 2030
# explicit REs 3208
# implicit REs 1778
# passives 383
# nominalizations 393
# possessives 1150
Table 1: Basic annotation statistics
3.3 Multi-level Representation
In the final representation of our data set, we inte-
grate the RE and deep syntax annotation by replac-
ing subtrees corresponding to an RE span. The RE
slot in the tree of the sentence is labeled with its
referential role and its ID. All RE subtrees for a
referent in a text are collected in a candidate list
which is initialized with three default REs: (i) a
pronoun, (ii) a default nominal (e.g. ?the victim?),
(iii) the empty RE. In contrast to the GREC data
sets, our RE candidates are not represented as the
original surface strings, but as non-linearized sub-
trees. The resulting multi-layer representation for
each text is structured as follows:
1. unordered deep trees with RE slots (deepSyn?re)
2. unorderd shallow trees with RE slots
(shallowSyn?re)
3. unordered RE subtrees
4. linearized, fully specified surface trees (linSyn+re)
5. alignments between nodes in 1., 2., 4.
The generation components in Section 4 also
use intermediate layers where REs are inserted
into the deep trees (deepSyn+re) or shallow trees
(shallowSyn+re).
Nodes in unordered trees are deterministically
sorted by their : 1. distance to the root, 2. label,
1550
3. PoS tag, 4. lemma. The generation components
traverse the nodes in this the order.
4 Generation Systems
Our main goal is to investigate different architec-
tures for combined surface realization and refer-
ring expression generation. We assume that this
task is split into three main modules: a syntax gen-
erator, an REG component, and a linearizer. The
components are implemented in a way that they
can be trained and applied on varying inputs, de-
pending on the pipeline. Section 4.1 describes the
basic set-up of our components. Section 4.2 de-
fines the architectures that we will compare in our
experiments (Section 5). Section 4.3 presents the
implementation of the underlying feature models.
4.1 Components
4.1.1 SYN: Deep to Shallow Syntax
For mapping deep to shallow dependency trees,
the syntax generator induces a probabilistic tree
transformation. The transformations are restricted
to verb nodes in the deep tree (possessives are
handled in the RE module) and extracted from
the alignments between the deep and shallow
layer in the training input. As an example, the
deep node ?attack:VV? aligns to ?have:AUX at-
tacked:VVINF?, ?attacks:VVFIN?, ?the:ART at-
tack:NN on:PRP?. The learner is implemented
as a ranking component, trained with SVMrank
(Joachims, 2006). During training, each instance
of a verb node has one optimal shallow depen-
dency alignment and a set of distractor candidates.
During testing, the module has to pick the best
shallow candidate according to its feature model.
In our crossvalidation set-up (see Section 5),
we extract, on average, 374 transformations from
the training sets. This set subdivides into non-
lexicalized and lexicalized transformations. The
mapping rule in (4-a) that simply rewrites the verb
underspecified PoS tag to the finite verb tag in the
shallow tree illustrates the non-lexicalized case.
Most transformation rules (335 out of 374 on aver-
age) are lexicalized for a specific verb lemma and
mostly transform nominalizations as in rule (4-b)
and particles (see Section 3.2).
(4) a. (x,lemma,VV,y)? (x,lemma,VVFIN,y)
b. (x,u?berfallen/attack,VV,y) ? (x,bei/at,PREP,y),
(z,U?berfall/attack,NN,x),(q,der/the,ART,z)
The baseline for the verb transformation com-
ponent is a two-step procedure: 1) pick a lexical-
ized rule if available for that verb lemma, 2) pick
the most frequent transformation.
4.1.2 REG: Realizing Referring Expressions
Similar to the syntax component, the REG mod-
ule is implemented as a ranker that selects surface
RE subtrees for a given referential slot in a deep
or shallow dependency tree. The candidates for
the ranking correspond to the entire set of REs
used for that referential role in the original text
(see Section 3.1). The basic RE module is a joint
model of all RE types, i.e. nominal, pronominal
and empty realizations of the referent. For the ex-
periment in Section 5.4, we use an additional sep-
arate classifier for implicit referents, also trained
with SVMrank. It uses the same feature model
as the full ranking component, but learns a binary
distinction for implicit or explicit mentions of a
referent. The explicit mentions will be passed to
the RE ranking component.
The baseline for the REG component is defined
as follows: if the preceding and the current RE
slot are instances of the same referent, realize a
pronoun, else realize the longest nominal RE can-
didate that has not been used in the preceding text.
4.1.3 LIN: Linearization
For linearization, we use the state-of-the-art
dependency linearizer described in Bohnet et
al. (2012). We train the linearizer on an auto-
matically parsed version of the German TIGER
treebank (Brants et al, 2002). This version
was produced with the dependency parser by
Bohnet (2010), trained on the dependency conver-
sion of TIGER by Seeker and Kuhn (2012).
4.2 Architectures
Depending on the way the generation components
are combined in an architecture, they will have ac-
cess to different layers of the input representation.
The following definitions of architectures recur to
the layers introduced in Section 3.3.
4.2.1 First Pipeline
The first pipeline corresponds most closely to a
standard generation pipeline in the sense of (Reiter
and Dale, 1997). REG is carried out prior to sur-
face realization such that the RE component does
not have access to surface syntax or word order
whereas the SYN component has access to fully
specified RE slots.
? training
1551
1. train REG: (deepSyn?re, deepSyn+re)
2. train SYN: (deepSyn+re, shallowSyn+re)
? prediction
1. apply REG: deepSyn?re ? deepSyn+re
2. apply SYN: deepSyn+re ? shallowSyn+re
3. linearize: shallowSyn+re ? linSyn+re
4.2.2 Second Pipeline
In the second pipeline, the order of the RE and
SYN component is switched. In this case, REG
has access to surface syntax without word order
but the surface realization is trained and applied
on trees with underspecified RE slots.
? training
1. train SYN: (deepSyn?re, shallowSyn?re)
2. train REG: (shallowSyn?re, shallowSyn+re)
? prediction
1. apply SYN: deepSyn?re ? shallowSyn?re
2. apply REG: shallowSyn?re ?
shallowSyn+re
3. linearize: shallowSyn+re ? linSyn+re
4.2.3 Parallel System
A well-known problem with pipeline architectures
is the effect of error propagation. In our parallel
system, the components are trained independently
of each other and applied in parallel on the deep
syntactic input with underspecified REs.
? training
1. train SYN: (deepSyn?re, shallowSyn?re)
2. train REG: (deepSyn?re, deepSyn+re)
? prediction
1. apply REG and SYN:
deepSyn?re ? shallowSyn+re
2. linearize: shallowSyn+re ? linSyn+re
4.2.4 Revision-based System
In the revision-based system, the RE component
has access to surface syntax and a preliminary lin-
earization, called prelinSyn. In this set-up, we ap-
ply the linearizer first on trees with underspeci-
fied RE slots. For this step, we insert the default
REs for the referent into the respective slots. After
REG, the tree is linearized once again.
? training
1. train SYN on gold pairs of
(deepSyn?re, shallowSyn?re)
2. train REG on gold pairs of
(prelinSyn?re, prelinSyn+re)
? prediction
1. apply SYN: deepSyn?re ? shallowSyn?re
2. linearize: shallowSyn?re ? prelinSyn?re
3. apply REG: prelinSyn?re ? prelinSyn+re
4. linearize: prelinSyn+re ? linSyn+re
4.3 Feature Models
The implementation of the feature models is based
on a general set of templates for the SYN and REG
component. The exact form of the models depends
on the input layer of a component in a given ar-
chitecture. For instance, when SYN is trained on
deepSyn?re, the properties of the children nodes
are less specific for verbs that have RE slots as
their dependents. When the SYN component is
trained on deepSyn+re, lemma and POS of the
children nodes are always specified.
The feature templates for SYN combine prop-
erties of the shallow candidate nodes (label, PoS
and lemma for top node and its children) with the
properties of the instance in the tree: (i) lemma,
tense, (ii) sentence is a header, (iii) label, PoS,
lemma of mother node, children and grandchil-
dren nodes (iv) number, lemmas of other verbs in
the sentence.
The feature templates for REG combine proper-
ties of the candidate RE (PoS and lemma for top
node and its children, length) with properties of
the RE slot in the tree: lemma, PoS and labels for
the (i) mother node, (ii) grandmother node, (iii)
uncle and sibling nodes. Additionally, we imple-
ment a small set of global properties of a referent
in a text: (i) identity is known, (ii) plural or sin-
gular referent, (iii) age is known, and a number of
contextual properties capturing the previous refer-
ents and their predicted REs: (i) role and realiza-
tion of the preceding referent, (ii) last mention of
the current referent, (iii) realization of the referent
in the header.
5 Experiments
In this experimental section, we provide a corpus-
based evaluation of the generation components
and architectures introduced in Section 4. In the
following, Section 5.1 presents the details of our
evaluation methodology. In Section 5.2, we dis-
cuss the first experiment that evaluates the pipeline
architectures and the single components on oracle
inputs. Section 5.3 describes an experiment which
compares the parallel and the revision-based ar-
chitecture against the pipeline. In Section 5.4, we
compare two methods for dealing with the implicit
referents in our data. Section 5.5 provides some
general discussion of the results.
1552
Sentence overlap SYN Accuracy RE Accuracy
Input System BLEU NIST BLEUr String Type String Type Impl
deepSyn?re Baseline 42.38 9.9 47.94 35.66 44.81 33.3 36.03 50.43
deepSyn?re 1st pipeline 54.65 11.30 59.95 57.09 68.15 54.61 71.51 84.72
deepSyn?re 2nd pipeline 54.28 11.25 59.62 59.14 68.58 52.24 68.2 82
gold deepSyn+re SYN?LIN 63.9 12.7 62.86 60.83 69.74 100 100 100
gold shallowSyn?re REG?LIN 60.57 11.87 68.06 100 100 60.53 75.86 88.86
gold shallowSyn+re LIN 79.17 13.91 72.7 100 100 100 100 100
Table 2: Evaluating pipeline architectures against the baseline and upper bounds
5.1 Evaluation Measures
We split our data set into 10 splits of 20 articles.
We use one split as the development set, and cross-
validate on the remaining splits. In each case,
the downstream modules of the pipeline will be
trained on the jackknifed training set.
Text normalization: We carry out automatic
evaluation calculated on lemmatized text with-
out punctuation, excluding additional effects that
would be introduced from a morphology genera-
tion component.
Measures: First, we use a number of evalua-
tion measures familiar from previous generation
shared tasks:
1. BLEU, sentence-level geometric mean of 1- to 4-gram
precision, as in (Belz et al, 2011)
2. NIST, sentence-level n-gram overlap weighted in
favour of less frequent n-grams, as in (Belz et al, 2011)
3. RE Accuracy on String, proportion of REs selected by
the system with a string identical to the RE string in the
original corpus, as in (Belz and Kow, 2010)
4. RE Accuracy on Type, proportion of REs selected by
the system with an RE type identical to the RE type in
the original corpus, as in (Belz and Kow, 2010)
Second, we define a number of measures moti-
vated by our specific set-up of the task:
1. BLEUr , sentence-level BLEU computed on post-
processed output where predicted referring expressions
for victim and perp are replaced in the sentences (both
gold and predicted) by their original role label, this
score does not penalize lexical mismatches between
corpus and system REs
2. RE Accuracy on Impl, proportion of REs predicted cor-
rectly as implicit/non-implicit
3. SYN Accuracy on String, proportion of shallow verb
candidates selected by the system with a string identical
to the verb string in the original corpus
4. SYN Accuracy on Type, proportion of shallow verb
candidates selected by the system with a syntactic cat-
egory identical to the category in the original corpus
5.2 Pipelines and Upper Bounds
The first experiment addresses the first and sec-
ond pipeline introduced in Section 4.2.1 and 4.2.2.
The baseline combines the baseline version of
the SYN component (Section 4.1.1) and the REG
component (Section 4.1.2) respectively. As we re-
port in Table 2, both pipelines largely outperform
the baseline. Otherwise, they obtain very similar
scores in all measures with a small, weakly signif-
icant tendency for the first pipeline. The only re-
markable difference is that the accuracy of the in-
dividual components is, in each case, lower when
they are applied as the second step in the pipeline.
Thus, the RE accuracy suffers from mistakes from
the predicted syntax in the same way that the qual-
ity of syntax suffers from predicted REs.
The three bottom rows in Table 2 report the per-
formance of the individual components and lin-
earization when they are applied to inputs with an
REG and SYN oracle, providing upper bounds for
the pipelines applied on deepSyn?re. When REG
and linearization are applied on shallowSyn?re
with gold shallow trees, the BLEU score is
lower (60.57) as compared to the system that ap-
plies syntax and linearization on deepSyn+re,
deep trees with gold REs (BLEU score of 63.9).
However, the BLEUr score, which generalizes
over lexical RE mismatches, is higher for the
REG?LIN components than for SYN?LIN.
Moreover, the BLEUr score for the REG?LIN
system comes close to the upper bound that ap-
plies linearization on linSyn+re, gold shallow
trees with gold REs (BLEUr of 72.4), whereas
the difference in standard BLEU and NIST is
high. This effect indicates that the RE predic-
tion mostly decreases BLEU due to lexical mis-
matches, whereas the syntax prediction is more
likely to have a negative impact on final lineariza-
tion.
The error propagation effects that we find in the
first and second pipeline architecture clearly show
that decisions at the levels of syntax, reference
and word order interact, otherwise their predic-
1553
Input System BLEU NIST BLEUr
deepSyn?re 1st pipeline 54.65 11.30 59.95
deepSyn?re Parallel 54.78 11.30 60.05
deepSyn?re Revision 56.31 11.42 61.30
Table 3: Architecture evaluation
tion would not affect each other. In particular, the
REG module seems to be affected more seriously,
the String Accuracy decreases from 60.53 on gold
shallow trees to 52.24 on predicted shallow trees
whereas the Verb String Accuracy decreases from
60.83 on gold REs to 57.04 on predicted REs.
5.3 Revision or parallelism?
The second experiment compares the first pipeline
against the parallel and the revision-based ar-
chitecture introduced in Section 4.2.3 and 4.2.4.
The evaluation in Table 3 shows that the paral-
lel architecture improves only marginally over the
pipeline. By contrast, we obtain a clearly signifi-
cant improvement for the revision-based architec-
ture on all measures. The fact that this architec-
ture significantly improves the BLEU, NIST and
the BLEUr score of the parallel system indicates
that the REG benefits from the predicted syntax
when it is approximatively linearized. The fact
that also the BLEUr score improves shows that a
higher lexical quality of the REs leads to better fi-
nal linearizations.
Table 4 shows the performance of the REG
module on varying input layers, providing a more
detailed analysis of the interaction between RE,
syntax and word order. In order to produce the
deeplinSyn?re layer, deep syntax trees with ap-
proximative linearizations, we preprocessed the
deep trees by inserting a default surface trans-
formation for the verb nodes. We compare this
input for REG against the prelinSyn?re layer
used in the revision-based architecture, and the
deepSyn?re layer used in the pipeline and the par-
allel architecture. The REG module benefits from
the linearization in the case of deeplinSyn?re
and prelinSyn?re, outperforming the compo-
nent trained applied on the non-linearized deep
syntax trees. However, the REG module ap-
plied on prelinSyn?re, predicted shallow and lin-
earized trees, clearly outperforms the module ap-
plied on deeplinSyn?re. This shows that the
RE prediction can actually benefit from the pre-
dicted shallow syntax, but only when the predicted
trees are approximatively linearized. As an up-
per bound, we report the performance obtained on
RE Accuracy
Input System String Type Impl
deepSyn?re RE 54.61 71.51 84.72
deeplinSyn?re RE 56.78 72.23 84.71
prelinSyn?re RE 58.81 74.34 86.37
gold linSyn?re RE 68.63 83.63 94.74
Table 4: RE generation from different input layers
linSyn?re, gold shallow trees with gold lineariza-
tions. This set-up corresponds to the GREC tasks.
The gold syntax leads to a huge increase in perfor-
mance.
These results strengthen the evidence from the
previous experiment that decisions at the level of
syntax, reference and word order are interleaved.
A parallel architecture that simply ?circumvents?
error propagation effects by making decisions in-
dependent of each other is not optimal. Instead,
the automatic prediction of shallow syntax can
positively impact on RE generation if these shal-
low trees are additionally processed with an ap-
proximative linearization step.
5.4 A joint treatment of implicit referents?
The previous experiments have pursued a joint
approach for modeling implicit referents. The
hypothesis for this experiment is that the SYN
component and the intermediate linearization in
a revision-based architecture could benefit from a
separate treatment of implicit referents since verb
alternations like passive or nominalization often
involve referent deletions.
The evaluation in Table 5 provides contradic-
tory results depending on the evaluation measure.
For the first pipeline, the system with a separate
treatment of implicit referents significantly outper-
forms the joint system in terms of BLEU. How-
ever, the BLEUr score does not improve. In the
revision-based architecture, we do not find a clear
result for or against a joint modelling approach.
The revision-based system with disjoint modelling
of implicits shows a slight, non-significant in-
crease in BLEU score. By contrast, the BLEUr
score is signficantly better for the joint approach.
We experimented with parallelization of syntax
generation and prediction of implicit referents in
a revision-based system. This has a small positive
effect on the BLEUr score and a small negative
effect on the plain BLEU and NIST score. These
contradictory scores might indicate that the auto-
matic evaluation measures cannot capture all as-
pects of text quality, an issue that we discuss in
the following.
1554
(5) Generated by sequential system:
a. Deshalb
Therefore
gab
gave
dem Ta?ter
to the robber
  seine  his Brieftaschewallet ohnewithout da?that
 das Opfer  the victim Widerstandresistance leistetshows heraus.out.
b. Er
He
nahm
takes
anschlie?end
afterwards
 dem Opfer  the victim diethe Armbanduhrwatch aboff undand der Ta?terthe robber flu?chtete.fleed.
(6) Generated by revision-based system:
a.
 Das Opfer  The victim gibtgave deshalbtherefore
  seine  his Brieftaschewallet ohnewithout Widerstandresistance zuto leistenshow heraus.out.
b. Anschlie?end
Afterwards
nahm
took
der Ta?ter
the robber
 dem Opfer  the victim diethe Armbanduhrwatch aboff undand flu?chtete.fleed.
Figure 3: Two automatically generated outputs for the Sentences (3e-f) in Figure 2.
Joint System BLEU NIST BLEUr
+ 1st pipeline 54.65 11.30 59.95
- 1st pipeline 55.38 11.48 59.52
+ Revision 56.31 11.42 61.30
- Revision 56.42 11.54 60.52
- Parallel+Revision 56.29 11.51 60.63
Table 5: Implicit reference and architectures
5.5 Discussion
The results presented in the preceding evaluations
consistenly show the tight connections between
decisions at the level of reference, syntax and word
order. These interactions entail highly interde-
pendent modelling steps: Although there is a di-
rect error propagation effect from predicted verb
transformation on RE accuracy, predicted syntax
still leads to informative intermediate lineariza-
tions that improve the RE prediction. Our optimal
generation architecture thus has a sequential set-
up, where the first linearization step can be seen
as an intermediate feedback that is revised in the
final linearization. This connects to work in, e.g.
(Hovy, 1988; Robin, 1993).
In Figure 3, we compare two system outputs for
the last two sentences of the text in Figure 2. The
output of the sequential system is severely inco-
herent and would probably be rejected by a hu-
man reader: In sentence (5a) the victim subject of
an active verb is deleted, and the relation between
the possessive and the embedded victim RE is not
clear. In sentence (5b) the first conjunct realizes
a pronominal perp RE and the second conjunct a
nominal perp RE. The output of the revision-based
system reads much more natural. This example
shows that the extension of the REG problem to
texts with more than one main referent (as in the
GREC data set) yields interesting inter-sentential
interactions that affect textual coherence.
We are aware of the fact that our automatic eval-
uation might only partially render certain effects,
especially with respect to textual coherence. It
is likely that the BLEU scores do not capture the
magnitude of the differences in text quality illus-
trated by the Examples (5-6). Ultimately, a hu-
man evaluation for this task is highly desirable.
We leave this for future work since our integrated
set-up rises a number of questions with respect to
evaluation design. In a preliminary analysis, we
noticed the problem that human readers find it dif-
ficult to judge discourse-level properties of a text
like coherence or naturalness when the generation
output is not perfectly grammatical or fluent at the
sentence level.
6 Conclusion
We have presented a data-driven approach for in-
vestigating generation architectures that address
discourse-level reference and sentence-level syn-
tax and word order. The data set we created for our
experiments basically integrates standards from
previous research on REG and surface realization
and extends the annotations to further types of im-
plicit referents. Our results show that interactions
between the different generation levels are best
captured in a sequential, revision-based pipeline
where the REG component has access to predic-
tions from the syntax and the linearization mod-
ule. These empirical findings obtained from ex-
periments with generation architectures have clear
connections to theoretical accounts of textual co-
herence.
Acknowledgements
This work was supported by the Deutsche
Forschungsgemeinschaft (German Research
Foundation) in SFB 732 Incremental Specification
in Context, project D2.
1555
References
Douglas Edmund Appelt. 1982. Planning natural lan-
guage utterances to satisfy multiple goals. Ph.D.
thesis, Stanford, CA, USA.
John Bateman and Michael Zock. 2003. Natural
Language Generation. In Ruslan Mitkov, editor,
The Oxford Handbook of Computational Linguis-
tics. Oxford University Press.
Anja Belz and Eric Kow. 2010. The GREC Challenges
2010: overview and evaluation results. In Proc. of
the 6th International Natural Language Generation
Conference, INLG ?10, pages 219?229, Strouds-
burg, PA, USA.
Anja Belz and Sebastian Varges. 2007. Generation of
repeated references to discourse entities. In Proc. of
the 11th European Workshop on Natural Language
Generation, ENLG ?07, pages 9?16, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Anja Belz, Mike White, Dominic Espinosa, Eric Kow,
Deirdre Hogan, and Amanda Stent. 2011. The first
surface realisation shared task: Overview and evalu-
ation results. In Proc. of the Generation Challenges
Session at the 13th European Workshop on Natu-
ral Language Generation, pages 217?226, Nancy,
France, September. Association for Computational
Linguistics.
Anja Belz. 2005. Statistical generation: Three meth-
ods compared and evaluated. In Proc. of the 10th
European Workshop on Natural Language Genera-
tion, pages 15?23.
Anja Belz. 2008. Automatic generation of
weather forecast texts using comprehensive proba-
bilistic generation-space models. Nat. Lang. Eng.,
14(4):431?455, October.
Bernd Bohnet, Leo Wanner, Simon Milles, and Ali-
cia Burga. 2010. Broad coverage multilingual deep
sentence generation with a stochastic multi-level re-
alizer. In Proc. of the 23rd International Conference
on Computational Linguistics, Beijing, China.
Bernd Bohnet, Simon Mille, Beno??t Favre, and Leo
Wanner. 2011. <stumaba >: From deep repre-
sentation to surface. In Proc. of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation, pages 232?235,
Nancy, France, September.
Bernd Bohnet, Anders Bjo?rkelund, Jonas Kuhn, Wolf-
gang Seeker, and Sina Zarriess. 2012. Generating
non-projective word order in statistical linearization.
In Proc. of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 928?
939, Jeju Island, Korea, July.
Bernd Bohnet. 2010. Top accuracy and fast de-
pendency parsing is not a contradiction. In Proc.
of the 23rd International Conference on Computa-
tional Linguistics, pages 89?97, Beijing, China, Au-
gust.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proc. of the Workshop on Treebanks
and Linguistic Theories.
Aoife Cahill and Arndt Riester. 2009. Incorporat-
ing Information Status into Generation Ranking. In
Proc. of the 47th Annual Meeting of the ACL, pages
817?825, Suntec, Singapore, August.
Lynne Cahill, Christy Doran, Roger Evans, Chris Mel-
lish, Daniel Paiva, Mike Reape, Donia Scott, and
Neil Tipper. 1999. In search of a reference architec-
ture for nlg systems. In Proc. of the European Work-
shop on Natural Language Generation (EWNLG),
pages 77?85.
David L. Chen, Joohyun Kim, and Raymond J.
Mooney. 2010. Training a multilingual sportscaster:
Using perceptual context to learn language. Journal
of Artificial Intelligence Research, 37:397?435.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233?263.
Robert Dale. 1989. Cooking up referring expressions.
In Proc. of the 27th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 68?75,
Vancouver, British Columbia, Canada, June.
Laurence Danlos. 1984. Conceptual and linguistic de-
cisions in generation. In Proc. of the 10th Interna-
tional Conference on Computational Linguistics and
22nd Annual Meeting of the Association for Compu-
tational Linguistics, pages 501?504, Stanford, Cali-
fornia, USA, July.
Koenraad De Smedt, Helmut Horacek, and Michael
Zock. 1996. Architectures for natural language gen-
eration: Problems and perspectives. In Trends In
Natural Language Generation: An Artifical Intelli-
gence Perspective, pages 17?46. Springer-Verlag.
Katja Filippova and Michael Strube. 2007. Generating
constituent order in german clauses. In Proc. of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, Prague, Czech Republic.
Matthew Gerber and Joyce Chai. 2010. Beyond nom-
bank: A study of implicit arguments for nominal
predicates. In Proc. of the 48th Annual Meeting
of the Association for Computational Linguistics,
pages 1583?1592, Uppsala, Sweden, July.
Barbara J. Grosz, Aravind Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
1556
Eduard H. Hovy. 1988. Planning coherent multisen-
tential text. In Proc. of the 26th Annual Meeting
of the Association for Computational Linguistics,
pages 163?169, Buffalo, New York, USA, June.
Thorsten Joachims. 2006. Training linear SVMs
in linear time. In Proc. of the ACM Conference
on Knowledge Discovery and Data Mining (KDD),
pages 217?226.
Irene Langkilde and Kevin Knight. 1998. Gener-
ation that exploits corpus-based statistical knowl-
edge. In Proc. of the 36th Annual Meeting of
the Association for Computational Linguistics and
17th International Conference on Computational
Linguistics, Volume 1, pages 704?710, Montreal,
Quebec, Canada, August. Association for Compu-
tational Linguistics.
Tomasz Marciniak and Michael Strube. 2005. Be-
yond the pipeline: discrete optimization in nlp. In
Proc. of the 9th Conference on Computational Nat-
ural Language Learning, CONLL ?05, pages 136?
143, Stroudsburg, PA, USA.
Chris Mellish, Roger Evans, Lynne Cahill, Christy Do-
ran, Daniel Paiva, Mike Reape, Donia Scott, and
Neil Tipper. 2000. A representation for complex
and evolving data dependencies in generation. In
Proc. of the 6th Conference on Applied Natural Lan-
guage Processing, pages 119?126, Seattle, Wash-
ington, USA, April.
Marie Meteer. 1991. Bridging the generation gap be-
tween text planning and linguistic realization. In
Computational Intelligence, volume 7 (4).
Adwait Ratnaparkhi. 2000. Trainable methods for
surface natural language generation. In Proc. of
the 1st North American chapter of the Association
for Computational Linguistics conference, NAACL
2000, pages 194?201, Stroudsburg, PA, USA.
Ehud Reiter and Robert Dale. 1997. Building applied
natural language generation systems. Nat. Lang.
Eng., 3(1):57?87, March.
Ehud Reiter. 1994. Has a Consensus NL Genera-
tion Architecture Appeared, and is it Psycholinguis-
tically Plausible? pages 163?170.
Eric K. Ringger, Michael Gamon, Robert C. Moore,
David Rojas, Martine Smets, and Simon Corston-
Oliver. 2004. Linguistically Informed Statisti-
cal Models of Constituent Structure for Ordering in
Sentence Realization. In Proc. of the 2004 Inter-
national Conference on Computational Linguistics,
Geneva, Switzerland.
Jacques Robin. 1993. A revision-based generation ar-
chitecture for reporting facts in their historical con-
text. In New Concepts in Natural Language Gener-
ation: Planning, Realization and Systems. Frances
Pinter, London and, pages 238?265. Pinter Publish-
ers.
Michael Roth and Anette Frank. 2012. Aligning predi-
cate argument structures in monolingual comparable
texts: A new corpus for a new task. In Proc. of the
1st Joint Conference on Lexical and Computational
Semantics (*SEM), Montreal, Canada.
Robert Rubinoff. 1992. Integrating text planning and
linguistic choice by annotating linguistic structures.
In Robert Dale, Eduard H. Hovy, Dietmar Ro?sner,
and Oliviero Stock, editors, NLG, volume 587 of
Lecture Notes in Computer Science, pages 45?56.
Springer.
Josef Ruppenhofer, Caroline Sporleder, Roser
Morante, Collin Baker, and Martha Palmer. 2010.
Semeval-2010 task 10: Linking events and their
participants in discourse. In Proc. of the 5th
International Workshop on Semantic Evaluation,
pages 45?50, Uppsala, Sweden, July.
Wolfgang Seeker and Jonas Kuhn. 2012. Making El-
lipses Explicit in Dependency Conversion for a Ger-
man Treebank. In Proc. of the 8th conference on
International Language Resources and Evaluation,
Istanbul, Turkey, May.
Advaith Siddharthan and Ann Copestake. 2004. Gen-
erating referring expressions in open domains. In
Proceedings of the 42nd Meeting of the Association
for Computational Linguistics (ACL?04), Main Vol-
ume, pages 407?414, Barcelona, Spain, July.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a web-based tool for nlp-assisted text
annotation. In Proc. of the Demonstrations at the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 102?
107, Avignon, France, April.
Leo Wanner, Simon Mille, and Bernd Bohnet. 2012.
Towards a surface realization-oriented corpus anno-
tation. In Proc. of the 7th International Natural Lan-
guage Generation Conference, pages 22?30, Utica,
IL, May.
Leo Wanner. 1994. Building another bridge over the
generation gap. In Proc. of the 7th International
Workshop on Natural Language Generation, INLG
?94, pages 137?144, Stroudsburg, PA, USA.
Sina Zarrie?, Aoife Cahill, and Jonas Kuhn. 2011. Un-
derspecifying and predicting voice for surface real-
isation ranking. In Proc. of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1007?
1017, Portland, Oregon, USA, June.
Sina Zarrie?, Aoife Cahill, and Jonas Kuhn. 2012.
To what extent does sentence-internal realisation re-
flect discourse context? a study on word order. In
Proc. of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 767?776, Avignon, France, April.
1557
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 55?60,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
ICARUS ? An Extensible Graphical Search Tool
for Dependency Treebanks
Markus Ga?rtner Gregor Thiele Wolfgang Seeker Anders Bjo?rkelund Jonas Kuhn
Institut fu?r Maschinelle Sprachverarbeitung
University of Stuttgart
firstname.lastname@ims.uni-stuttgart.de
Abstract
We present ICARUS, a versatile graphi-
cal search tool to query dependency tree-
banks. Search results can be inspected
both quantitatively and qualitatively by
means of frequency lists, tables, or depen-
dency graphs. ICARUS also ships with
plugins that enable it to interface with tool
chains running either locally or remotely.
1 Introduction
In this paper we present ICARUS1 a search and
visualization tool that primarily targets depen-
dency syntax. The tool has been designed such
that it requires minimal effort to get started with
searching a treebank or system output of an auto-
matic dependency parser, while still allowing for
flexible queries. It enables the user to search de-
pendency treebanks given a variety of constraints,
including searching for particular subtrees. Em-
phasis has been placed on a functionality that
makes it possible for the user to switch back and
forth between a high-level, aggregated view of the
search results and browsing of particular corpus
instances, with an intuitive visualization of the
way in which it matches the query. We believe this
to be an important prerequisite for accessing anno-
tated corpora, especially for non-expert users.
Search queries in ICARUS can be constructed
either in a graphical or a text-based manner. Build-
ing queries graphically removes the overhead of
learning a specialized query language and thus
makes the tool more accessible for a wider audi-
ence. ICARUS provides a very intuitive way of
breaking down the search results in terms of fre-
quency statistics (such as the distribution of part-
of-speech on one child of a particular verb against
the lemma of another child). The dimensions for
1Interactive platform for Corpus Analysis and Research
tools, University of Stuttgart
the frequency break-down are simply specified by
using grouping operators in the query. The fre-
quency tables are filled and updated in real time
as the search proceeds through the corpus ? allow-
ing for a quick detection of misassumptions in the
query.
ICARUS uses a plugin-based architecture that
permits the user to write his own plugins and in-
tegrate them into the system. For example, it
comes with a plugin that interfaces with an exter-
nal parser that can be used to parse a sentence from
within the user interface. The constraints for the
query can then be copy-pasted from the resulting
parse visualization. This facilitates example-based
querying, which is particularly helpful for inexpe-
rienced users ? they do not have to recall details
of the annotation conventions outside of their fo-
cus of interests but can go by what the parser pro-
vides.2
ICARUS is written entirely in Java and runs out
of the box without requiring any installation of
the tool itself or additional libraries. This makes
it platform independent and the only requirement
is that a Java Runtime Environment (JRE) is in-
stalled on the host system. It is open-source and
freely available for download.3
As parsers and other Natural Language Pro-
cessing (NLP) tools are starting to find their way
into other sciences such as (digital) humanities or
social sciences, it gets increasingly important to
provide intuitive visualization tools that integrate
seamlessly with existing NLP tools and are easy
to use also for non-linguists. ICARUS interfaces
readily with NLP tools provided as web services
by CLARIN-D,4 the German incarnation of the
European Infrastructure initiative CLARIN.
2This is of course only practical with rather reliable auto-
matic parsers, but in our experience, the state-of-the-art qual-
ity is sufficient.
3www.ims.uni-stuttgart.de/forschung/
ressourcen/werkzeuge/icarus.en.html
4http://de.clarin.eu
55
The remainder of this paper is structured as fol-
lows: In Section 2 we elaborate on the motivation
for the tool and discuss related work. Section 3
presents a running example of how to build queries
and how results are visualized. In Section 4 we
outline the details of the architecture. Section 5
discusses ongoing work, and Section 6 concludes.
2 Background
Linguistically annotated corpora are among the
most important sources of knowledge for empir-
ical linguistics as well as computational modeling
of natural language. Moreover, for most users the
only way to develop a systematic understanding
of the phenomena in the annotations is through a
process of continuous exploration, which requires
suitable and intuitive tools.
As automatic analysis tools such as syntactic
parsers have reached a high quality standard, ex-
ploration of large collections of auto-parsed cor-
pus material becomes more and more common. Of
course, the querying problem is the same no matter
whether some target annotation was added manu-
ally, as in a treebank, or automatically. Yet, the
strategy changes, as the user will try to make sure
he catches systematic parsing errors and develops
an understanding of how the results he is deal-
ing with come about. While there is no guaran-
teed method for avoiding erroneous matches, we
believe that an easy-to-use transparent querying
mechanism that allows the user to look at the same
or similar results from various angles is the best
possible basis for an informed usage: frequency
tables breaking down the corpus distributions in
different dimensions are a good high-level hint,
and the actual corpus instances should be only one
or two mouse clicks away, presented with a con-
cise visualization of the respective instantiation of
the query constraints.
Syntactic annotations are quite difficult to query
if one is interested in specific constructions that
are not directly encoded in the annotation labels
(which is the case for most interesting phenom-
ena). Several tools have been developed to enable
researchers to do this. However, many of these
tools are designed for constituent trees only.
Dependency syntax has become popular as a
framework for treebanking because it lends itself
naturally to the representation of free word order
phenomena and was thus adopted in the creation of
treebanks for many languages that have less strict
word order, such as the Prague Dependency Tree-
bank for Czech (Hajic? et al, 2000) or SynTagRus
for Russian (Boguslavsky et al, 2000).
A simple tool for visualization of dependency
trees is What?s wrong with my NLP? (Riedel,
2008). Its querying functionality is however lim-
ited to simple string-searching on surface forms. A
somewhat more advanced tool is MaltEval (Nils-
son and Nivre, 2008), which offers a number of
predefined search patterns ranging from part-of-
speech tag to branching degree.
On the other hand, powerful tools such as PML-
TQ (Pajas and S?te?pa?nek, 2009) or INESS (Meurer,
2012) offer expressive query languages and can
facilitate cross-layer queries (e.g., involving both
syntactic and semantic structures). They also
accommodate both constituent and dependency
structures.
In terms of complexity in usage and expressiv-
ity, we believe ICARUS constitutes a middle way
between highly expressive and very simple visu-
alization tools. It is easy to use, requires no in-
stallation, while still having rich query and visual-
ization capabilities. ICARUS is similar to PML-
TQ in that it also allows the user to create queries
graphically. It is also similar to the search tool
GrETEL (Augustinus et al, 2012) as it interfaces
with a parser, allowing the user to create queries
starting from an automatic parse. Thus, queries
can be created without any prior knowledge of the
treebank annotation scheme.
As for searching constituent treebanks, there
is a plethora of existing search tools, such
as TGrep2 (Rohde, 2001), TigerSearch (Lezius,
2002), MonaSearch (Maryns, 2009), and Fangorn
(Ghodke and Bird, 2012), among others. They im-
plement different query languages with varying ef-
ficiency and expressiveness.
3 Introductory Example
Before going into the technical details, we show
an example of what you can do with ICARUS.
Assume that a user is interested in passive con-
structions in English, but does not know exactly
how this is annotated in a treebank. As a first step,
he can use a provided plugin that interfaces with
a tool chain5 to parse a sentence that contains a
passive construction (thus adopting the example-
based querying approach laid out in the introduc-
5using mate-tools by Bohnet (2010); available at
http://code.google.com/p/mate-tools
56
tion). Figure 1 shows the parser interface. In the
lower field, the user entered the sentence. The
other two fields show the output of the parser, once
as a graph and once as a feature value description.
Figure 1: Parsing the sentence ?Mary was kissed
by a boy.? with a predefined tool chain.
In the second step, the user can then mark parts
of the output graph by selecting some nodes and
edges, and have ICARUS construct a query struc-
ture from it, following the drag-and-drop scheme
users are familiar with from typical office soft-
ware. The automatically built query can be man-
ually adjusted by the user (relaxing constraints)
and then be used to search for similar structures
in a treebank. The parsing step can of course be
skipped altogether, and a query can be constructed
by hand right away. Figure 2 shows the query
builder, where the user can define or edit search
graphs graphically in the main window, or enter
them as a query string in the lower window.
Figure 2: Query builder for constructing queries.
For the example, Figure 3 shows the query as it
is automatically constructed by ICARUS from the
partial parse tree (3a), and what it might look like
after the user has changed it (3b). The modified
query matches passive constructions in English, as
annotated in the CoNLL 2008 Shared Task data set
(Surdeanu et al, 2008), which we use here.
(a) automatically extracted (b) manually edited
Figure 3: Search graphs for finding passive con-
structions. (a) was constructed automatically from
the parsed sentence, (b) is a more general version.
The search returns 6,386 matches. Note that
the query (Figure 3b) contains a <*>-expression.
This grouping operator groups the results accord-
ing to the specified dimension, in this case by the
lemma of the passivized verb. Figure 4 shows
the result view. On the left, a list of lemmas is
presented, sorted by frequency. Clicking on the
lemma displays the list of matches containing that
particular lemma on the right side. The match-
ing sentences can then be browsed, with the active
sentence also being shown as a tree. Note that the
instantiation of the query constraints is highlighted
in the tree display.
Figure 4: Passive constructions in the treebank
grouped by lemma and sorted by frequency.
The query could be further refined to restrict it
to passives with an overt logical subject, using a
more complex search graph for the by-phrase and
a second instance of the grouping operator. The
results will then also be grouped by the lemma of
the logical subject, and are therefore presented as
a two-dimensional table. Figure 5 shows the new
query and the resulting view. The user is presented
with a frequency table, where each cell contains
the number of hits for this particular combination
of verb lemma and logical subject. Clicking on
the cell opens up a view similar to the right part of
Figure 4 where the user can then again browse the
actual trees.
57
Figure 5: Search graph and result view for passive
constructions with overt logical subjects, grouped
by lemma of the verb and the lemma of the logical
subject.
Finally, we can add a third grouping operator.
Figure 6 shows a further refined query for passives
with an overt logical subject and an object. In the
results, the user is presented with a list of values
for the first grouping operator to the left. Clicking
on one item in that list opens up a table on the right
presenting the other two dimensions of the query.
Figure 6: Search graph and result view for passive
constructions with an overt logical subject and an
object, grouped by lemma of the verb, the logical
subject, and the object.
This example demonstrates a typical use case
for a user that is interested in certain linguistic
constructions in his corpus. Creating the search
graph and interpreting the results does not re-
quire any specialized knowledge other than fa-
miliarity with the annotation of the corpus being
searched. It especially does not require any pro-
gramming skills, and the possibility to graphically
build a query obviates the need to learn a special-
ized query language.
4 Architecture
This section goes into more details about the in-
ner workings of ICARUS. A main component
is the search engine, which enables the user to
quickly search treebanks for whatever he is inter-
ested in. A second important feature of ICARUS
is the plugin-based architecture, which allows for
the definition of custom extensions. Currently,
ICARUS can read the commonly used CoNLL de-
pendency formats, and it is easy to write exten-
sions in order to add additional formats.
4.1 Search Engine and Query Builder
ICARUS has a tree-based search engine for tree-
banks, and includes a graphical query builder.
Structure and appearance of search graphs are sim-
ilar to the design used for displaying dependency
trees (cf. Figure 1), which is realized with the
open-source library JGraph.6 Queries and/or their
results can be saved to disk and later reloaded for
further processing.
Defining a query graphically basically amounts
to drawing a partial graph structure that defines
the type of structure that the user is interested in.
In practice, this is done by creating nodes in the
query builder and connecting them by edges. The
nodes correspond to words in the dependency trees
of the treebank. Several features like word iden-
tity, lemma, part of speech, etc. can be specified
for each node in the search graph in order to re-
strict the query. Dominance and precedence con-
straints over a set of nodes can be defined by sim-
ply linking nodes with the appropriate edge type.
Edges can be further specified for relation type,
distance, direction, projectivity, and transitivity. A
simple example is shown in Figures 2 and 3. The
search engine supports regular expressions for all
string-properties (form, lemma, part of speech, re-
lation). It also supports negation of (existence of)
nodes and edges, and their properties.
As an alternative to the search graph, the user
can also specify the query in a text-based format
by constructing a comma separated collection of
constraints in the form of key=value pairs for a
single node contained within square brackets. Hi-
erarchical structures are expressed by nesting their
textual representation. Figure 7 shows the text-
based form of the three queries used in the exam-
ples in Section 3.
6http://www.jgraph.com/
58
Query 1: [lemma=be[pos=VBN,lemma=<*>,rel=VC]]
Query 2: [lemma=be[pos=VBN,lemma=<*>,rel=VC[form=by,rel=LGS[lemma=<*>,rel=PMOD]]]]
Query 3: [lemma=be[pos=VBN,lemma=<*>,rel=VC[form=by,rel=LGS[lemma=<*>,rel=PMOD]]
[lemma=<*>,rel=OBJ]]]
Figure 7: Text representation of the three queries used in the example in Section 3.
A central feature of the query language is the
grouping operator (<*>), which will match any
value and cause the search engine to group result
entries by the actual instance of the property de-
clared to be grouped. The results of the search
will then be visualized as a list of instances to-
gether with their respective frequencies. Results
can be sorted alphabetically or by frequency (ab-
solute or relative counts) . Depending on the num-
ber of grouping operators used (up to a maximum
of three) the result is structured as a list of fre-
quencies (cf. Figure 4), a table of frequencies for
pairs of instances (cf. Figure 5), or a list where
each item then opens up a table of frequency re-
sults (cf. Figure 6). In the search graph and the
result view, different colors are used to distinguish
between different grouping operators.
The ICARUS search engine offers three differ-
ent search modes:
Sentence-based. Sentence based search stops at
the first successful hit in a sentence and returns
every sentence on a list of results at most once.
Exhaustive sentence-based. The exhaustive
sentence-based search mode extends the sentence
based search by the possibility of processing mul-
tiple hits within a single sentence. Every sentence
with at least one hit is returned exactly once. In the
result view, the user can then browse the different
hits found in one sentence.
Hit-based. Every successful hit is returned sepa-
rately on the corresponding list of results.
When a query is issued, the search results are
displayed on the fly as the search engine is pro-
cessing the treebank. The sentences can be ren-
dered in one of two ways: either as a tree, where
nodes are arranged vertically by depth in the tree,
or horizontally with all the nodes arranged side-
by-side. If a tree does not fit on the screen, part of
it is automatically collapsed but can be expanded
again by the user.
4.2 Extensibility
ICARUS relies on the Java Plugin Framework,7
which provides a powerful XML-based frame-
7http://jpf.sourceforge.net/
work for defining plugins similarly to the engine
used by the popular Eclipse IDE project. The
plugin-based architecture makes it possible for
anybody to write extensions to ICARUS that are
specialized for a particular task. The parser inte-
gration of mate-tools demonstrated in Section 3 is
an example for such an extension.
The plugin system facilitates custom extensions
that make it possible to intercept certain stages
of an ongoing search process and interact with it.
This makes it possible for external tools to pre-
process search data and apply additional annota-
tions and/or filtering, or even make use of exist-
ing indices by using search constraints to limit the
amount of data passed to the search engine. With
this general setup, it is for example possible to eas-
ily extend ICARUS to work with constituent trees.
ICARUS comes with a dedicated plugin that
enables access to web services provided by
CLARIN-D. The project aims to provide tools and
services for language-centered research in the hu-
manities and social sciences. In contrast to the in-
tegration of, e.g., mate-tools, where the tool chain
is executed locally, the user can define a tool chain
by chaining several web services (e.g., lemmatiz-
ers, part-of-speech taggers etc.) together and ap-
ply them to his own data. To do this, ICARUS
is able to read and write the TCF exchange for-
mat (Heid et al, 2010) that is used by CLARIN-D
web services. The output can then be inspected
and searched using ICARUS. As new NLP tools
are added as CLARIN-D web services they can be
immediately employed by ICARUS.
5 Upcoming Extensions
An upcoming release includes the following ex-
tensions:
? Currently, treebanks are assumed to fit into
the executing computer?s main memory.
The new implementation will support asyn-
chronous loading of data, with notifications
passed to the query engine or a plugin when
required data is available. Treebanks with
millions of entries can then be loaded in less
59
memory consuming chunks, thus keeping the
system responsive when access is requested.
? The search engine is being extended with an
operator that allows disjunctions of queries.
This will enable the user to aggregate fre-
quency output over multiple queries.
6 Conclusion
We have presented ICARUS, a versatile and user-
friendly search and visualization tool for depen-
dency trees. It is aimed not only at (computa-
tional) linguists, but also at people from other dis-
ciplines, e.g., the humanities or social sciences,
who work with language data. It lets the user
create queries graphically and returns results (1)
quantitatively by means of frequency lists and ta-
bles as well as (2) qualitatively by connecting the
statistics to the matching sentences and allowing
the user to browse them graphically. Its plugin-
based architecture enables it to interface for exam-
ple with external processing pipelines, which lets
the user apply processing tools directly from the
user interface.
In the future, specialized plugins are planned
to work with different linguistic annotations, e.g.
cross-sentence annotations as used to annotate
coreference chains. Additionally, a plugin is in-
tended that interfaces the search engine with a
database.
Acknowledgments
This work was funded by the Deutsche
Forschungsgemeinschaft (DFG) via the SFB
732 ?Incremental Specification in Context?,
project D8, and by the Bundesministerium fu?r
Bildung und Forschung (BMBF) via project No.
01UG1120F, CLARIN-D center Stuttgart. The
authors are also indebted to Andre? Blessing and
Heike Zinsmeister for reading an earlier draft of
this paper.
References
Liesbeth Augustinus, Vincent Vandeghinste, and
Frank Van Eynde. 2012. Example-based Treebank
Querying. In Proceedings of the Eight International
Conference on Language Resources and Evaluation
(LREC?12), Istanbul, Turkey. ELRA.
Igor Boguslavsky, Svetlana Grigorieva, Nikolai Grig-
oriev, Leonid Kreidlin, and Nadezhda Frid. 2000.
Dependency Treebank for Russian: Concept, Tools,
Types of Information. In COLING 2000, pages
987?991, Saarbru?cken, Germany.
Bernd Bohnet. 2010. Top Accuracy and Fast Depen-
dency Parsing is not a Contradiction. In COLING
2010, pages 89?97, Beijing, China.
Sumukh Ghodke and Steven Bird. 2012. Fangorn: A
System for Querying very large Treebanks. In COL-
ING 2012: Demonstration Papers, pages 175?182,
Mumbai, India.
Jan Hajic?, Alena Bo?hmova?, Eva Hajic?ova?, and Barbora
Vidova?-Hladka?. 2000. The Prague Dependency
Treebank: A Three-Level Annotation Scenario. In
Treebanks: Building and Using Parsed Corpora,
pages 103?127. Amsterdam:Kluwer.
Ulrich Heid, Helmut Schmid, Kerstin Eckart, and Er-
hard Hinrichs. 2010. A Corpus Representation For-
mat for Linguistic Web Services: The D-SPIN Text
Corpus Format and its Relationship with ISO Stan-
dards. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta. ELRA.
Wolfgang Lezius. 2002. Ein Suchwerkzeug fu?r syn-
taktisch annotierte Textkorpora. Ph.D. thesis, IMS,
University of Stuttgart.
Hendrik Maryns. 2009. MonaSearch ? A Tool for
Querying Linguistic Treebanks. In Proceedings of
TLT 2009, Groningen.
Paul Meurer. 2012. INESS-Search: A Search System
for LFG (and Other) Treebanks. In Miriam Butt and
Tracy Holloway King, editors, Proceedings of the
LFG2012 Conference. CSLI Publications.
Jens Nilsson and Joakim Nivre. 2008. MaltEval: an
Evaluation and Visualization Tool for Dependency
Parsing. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation
(LREC?08), Marrakech, Morocco. ELRA.
Petr Pajas and Jan S?te?pa?nek. 2009. System for Query-
ing Syntactically Annotated Corpora. In Proceed-
ings of the ACL-IJCNLP 2009 Software Demonstra-
tions, pages 33?36, Suntec, Singapore. Association
for Computational Linguistics.
Sebastian Riedel. 2008. What?s Wrong With My
NLP?
http://code.google.com/p/
whatswrong/.
Douglas L.T. Rohde. 2001. TGrep2 the next-
generation search engine for parse trees.
http://tedlab.mit.edu/?dr/Tgrep2/.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL 2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In CoNLL 2008,
pages 159?177, Manchester, England.
60
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 47?57,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning Structured Perceptrons for Coreference Resolution
with Latent Antecedents and Non-local Features
Anders Bj
?
orkelund and Jonas Kuhn
Institute for Natural Language Processing
University of Stuttgart
{anders,jonas}@ims.uni-stuttgart.de
Abstract
We investigate different ways of learning
structured perceptron models for coref-
erence resolution when using non-local
features and beam search. Our experi-
mental results indicate that standard tech-
niques such as early updates or Learning
as Search Optimization (LaSO) perform
worse than a greedy baseline that only uses
local features. By modifying LaSO to de-
lay updates until the end of each instance
we obtain significant improvements over
the baseline. Our model obtains the best
results to date on recent shared task data
for Arabic, Chinese, and English.
1 Introduction
This paper studies and extends previous work us-
ing the structured perceptron (Collins, 2002) for
complex NLP tasks. We show that for the task of
coreference resolution the straightforward combi-
nation of beam search and early update (Collins
and Roark, 2004) falls short of more limited fea-
ture sets that allow for exact search. This contrasts
with previous work on, e.g., syntactic parsing
(Collins and Roark, 2004; Huang, 2008; Zhang
and Clark, 2008) and linearization (Bohnet et
al., 2011), and even simpler structured prediction
problems, where early updates are not even nec-
essary, such as part-of-speech tagging (Collins,
2002) and named entity recognition (Ratinov and
Roth, 2009).
The main reason why early updates underper-
form in our setting is that the task is too difficult
and that the learning algorithm is not able to profit
from all training data. Put another way, early up-
dates happen too early, and the learning algorithm
rarely reaches the end of the instances as it halts,
updates, and moves on to the next instance.
An alternative would be to continue decod-
ing the same instance after the early updates,
which is equivalent to Learning as Search Opti-
mization (LaSO; Daum?e III and Marcu (2005b)).
The learning task we are tackling is however
further complicated since the target structure is
under-determined by the gold standard annotation.
Coreferent mentions in a document are usually an-
notated as sets of mentions, where all mentions in
a set are coreferent. We adopt the recently pop-
ularized approach of inducing a latent structure
within these sets (Fernandes et al, 2012; Chang et
al., 2013; Durrett and Klein, 2013). This approach
provides a powerful boost to the performance of
coreference resolvers, but we find that it does not
combine well with the LaSO learning strategy. We
therefore propose a modification to LaSO, which
delays updates until after each instance. The com-
bination of this modification with non-local fea-
tures leads to further improvements in the cluster-
ing accuracy, as we show in evaluation results on
all languages from the CoNLL 2012 Shared Task ?
Arabic, Chinese, and English. We obtain the best
results to date on these data sets.
1
2 Background
Coreference resolution is the task of grouping re-
ferring expressions (or mentions) in a text into dis-
joint clusters such that all mentions in a cluster
refer to the same entity. An example is given in
Figure 1 below, where mentions from two clusters
are marked with brackets:
[Drug Emporium Inc.]
a
1
said [Gary Wilber]
b
1
was
named CEO of [this drugstore chain]
a
2
. [He]
b
2
suc-
ceeds his father, Philip T. Wilber, who founded [the
company]
a
3
and remains chairman. Robert E. Lyons
III, who headed the [company]
a
4
?s Philadelphia re-
gion, was appointed president and chief operating offi-
cer, succeeding [Gary Wilber]
b
3
.
Figure 1: An excerpt of a document with the men-
tions from two clusters marked.
1
Our system is available at http://www.ims.
uni-stuttgart.de/
?
anders/coref.html
47
In recent years much work on coreference res-
olution has been devoted to increasing the ex-
pressivity of the classical mention-pair model, in
which each coreference classification decision is
limited to information about two mentions that
make up a pair. This shortcoming has been ad-
dressed by entity-mention models, which relate a
candidate mention to the full cluster of mentions
predicted to be coreferent so far (for more discus-
sion on the model types, see, e.g., (Ng, 2010)).
Nevertheless, the two best systems in the lat-
est CoNLL Shared Task on coreference resolu-
tion (Pradhan et al, 2012) were both variants of
the mention-pair model. While the second best
system (Bj?orkelund and Farkas, 2012) followed
the widely used baseline of Soon et al (2001), the
winning system (Fernandes et al, 2012) proposed
the use of a tree representation.
The tree-based model of Fernandes et al (2012)
construes the representation of coreference clus-
ters as a rooted tree. Figure 2 displays an example
tree over the clusters from Figure 1. Every men-
tion corresponds to a node in the tree, and arcs be-
tween mentions indicate that they are coreferent.
The tree additionally has a dummy root node. Ev-
ery subtree under the root node corresponds to a
cluster of coreferent mentions.
Since coreference training data is typically not
annotated with trees, Fernandes et al (2012) pro-
posed the use of latent trees that are induced dur-
ing the training phase of a coreference resolver.
The latent tree provides more meaningful an-
tecedents for training.
2
For instance, the popular
pair-wise instance creation method suggested by
Soon et al (2001) assumes non-branching trees,
where the antecedent of every mention is its lin-
ear predecessor (i.e., he
b
2
is the antecedent of
Gary Wilber
b
3
). Comparing the two alternative
antecedents of Gary Wilber
b
3
, the tree in Fig-
ure 2 provides a more reliable basis for training a
coreference resolver, as the two mentions of Gary
Wilber are both proper names and have an exact
string match.
3 Representation and Learning
LetM = {m
0
,m
1
, ...,m
n
} denote the set of men-
tions in a document, including the artificial root
mention (denoted by m
0
). We assume that the
2
We follow standard practice and overload the terms
anaphor and antecedent to be any type of mention, i.e., names
as well as pronouns. An antecedent is simply the mention to
the left of the anaphor.
Drug Emporium Inc.
a
1
the company
a
3
this drugstore chain
a
2
Gary Wilber
b
1
He
b
2
Gary Wilber
b
3
root
company
a
4
Figure 2: A tree representation of Figure 1.
mentions are ordered ascendingly with respect to
the linear order of the document, where the docu-
ment root precedes all other mentions.
3
For each
mention m
j
, let A
j
denote the set of potential an-
tecedents. That is, the set of all mentions that
precede m
j
according to the linear order includ-
ing the root node, or, A
j
= {m
i
| i < j}. Fi-
nally, let A denote the set of all antecedent sets
{A
0
, A
1
, ..., A
n
}.
In the tree model, each mention corresponds to
a node, and an antecedent-anaphor pair ?a
i
,m
i
?,
where a
i
? A
i
, corresponds to a directed edge (or
arc) pointing from antecedent to anaphor.
The score of an arc ?a
i
,m
i
? is defined as
the scalar product between a weight vector w
and a feature vector ?(?a
i
,m
i
?), where ? is
a feature extraction function over an arc (thus
extracting features from the antecedent and the
anaphor). The score of a coreference tree y =
{?a
1
,m
1
?, ?a
2
,m
2
?, ..., ?a
n
,m
n
?} is defined as
the sum of the scores of all the mention pairs:
score(?a
i
,m
i
?) = w ? ?(?a
i
,m
i
?) (1)
score(y) =
?
?a
i
,m
i
??y
score(?a
i
,m
i
?)
The objective is to find the output y? that maxi-
mizes the scoring function:
y? = arg max
y?Y(A)
score(y) (2)
where Y(A) denotes the set of possible trees given
the antecedent sets A. By treating the mentions as
nodes in a directed graph and assigning scores to
the arcs according to (1), Fernandes et al (2012)
solved the search problem using the Chu-Liu-
Edmonds (CLE) algorithm (Chu and Liu, 1965;
3
We impose a total order on mentions. In case of nested
mentions, the mention that begins first is assumed to precede
the embedded one. If two mentions begin at the same token,
the longer one is taken to precede the shorter one.
48
Edmonds, 1967), which is a maximum spanning
tree algorithm that finds the optimal tree over a
connected directed graph. CLE, however, has the
drawback that the scores of the arcs must remain
fixed and can not change depending on other arcs
and it is not clear how to include non-local features
in a CLE decoder.
3.1 Online learning
We find the weight vector w by online learning us-
ing a variant of the structured perceptron (Collins,
2002). Specifically, we use the passive-aggressive
(PA) algorithm (Crammer et al, 2006), since we
found that this performed slightly better in prelim-
inary experiments.
4
The structured perceptron iterates over train-
ing instances ?x
i
, y
i
?, where x
i
are inputs and y
i
are outputs. For each instance it uses the current
weight vector w to make a prediction y?
i
given the
input x
i
. If the prediction is incorrect, the weight
vector is updated in favor of the correct structure.
Otherwise the weight vector is left untouched. In
our setting inputs x
i
correspond to documents and
outputs y
i
are trees over mentions in a document.
The training data is, however, not annotated with
trees, but only with clusters of mentions. That is,
the y
i
?s are not defined a priori.
3.2 Latent antecedents
In order to have a tree structure to update against,
we use the current weight vector and apply the
decoder to a constrained antecedent set and ob-
tain a latent tree over the mentions in a docu-
ment, where each mention is assigned a single cor-
rect antecedent (Fernandes et al, 2012). We con-
strain the antecedent sets such that only trees that
correspond to the correct clustering can be built.
Specifically, let
?
A
j
denote the set of correct an-
tecedents for a mention m
j
, or
?
A
j
=
{
{m
0
} if m
j
has no correct antecedent
{a
i
| COREF(a
i
,m
j
), a
i
? A
j
} otherwise
that is, if mention m
j
is non-referential or the first
mention of its cluster,
?
A
j
contains only the docu-
ment root. Otherwise it is the set of all mentions
to the left that belong to the same cluster as m
j
.
Analogously to A, let
?
A denote the set of con-
strained antecedent sets. The latent tree y? needed
4
We also implement the feature mapping function ? as
a hash kernel (Bohnet, 2010) and apply averaging (Collins,
2002), though for brevity we omit this from the pseudocode.
for updates is then defined to be the optimal tree
over Y(
?
A), subject to the current weight vector:
y? = arg max
y?Y(
?
A)
score(y)
The intuition behind the latent tree is that during
online learning, the weight vector will start favor-
ing latent trees that are easier to learn (such as the
one in Figure 2).
Algorithm 1 PA algorithm with latent trees
Input: Training data D, number of iterations T
Output: Weight vector w
1: w =
??
0
2: for t ? 1..T do
3: for ?M
i
,A
i
,
?
A
i
? ? D do
4: y?
i
= arg max
Y(A)
score(y) . Predict
5: if ?CORRECT(y?
i
) then
6: y?
i
= arg max
Y(
?
A)
score(y) . Latent tree
7: ? = ?(y?
i
)? ?(y?
i
)
8: ? =
??w+LOSS(y?
i
)
???
2
. PA weight
9: w = w + ?? . PA update
10: return w
Algorithm 1 shows pseudocode for the learn-
ing algorithm, which we will refer to as the base-
line learning algorithm. Instead of looping over
pairs ?x, y? of documents and trees, it loops over
triples ?M,A,
?
A? that comprise the set of men-
tions M and the two sets of antecedent candidates
(line 3). Moreover, rather than checking that the
tree is identical to the latent tree, it only requires
the tree to correctly encode the gold clustering
(line 5). The update that occurs in lines 7-9 is the
passive-aggressive update. A loss function LOSS
that quantifies the error in the prediction is used
to compute a scalar ? that controls how much the
weights are moved in each update. If ? is set to 1,
the update reduces to the standard structured per-
ceptron update. The loss function can be an arbi-
trarily complex function that returns a numerical
value of how bad the prediction is. In the sim-
plest case, Hamming loss can be used, i.e., for
each incorrect arc add 1. We follow Fernandes
et al (2012) and penalize erroneous root attach-
ments, i.e., mentions that erroneously get the root
node as their antecedent, with a loss of 1.5. For all
other arcs we use Hamming loss.
4 Incremental Search
We now show that the search problem in (2) can
equivalently be solved by the more intuitive best-
first decoder (Ng and Cardie, 2002), rather than
using the CLE decoder. The best-first decoder
49
works incrementally by making a left-to-right pass
over the mentions, selecting for each mention the
highest scoring antecedent.
The key aspect that makes the best-first decoder
equivalent to the CLE decoder is that all arcs point
from left to right, both in this paper and in the work
of Fernandes et al (2012). We sketch a proof that
this decoder also returns the highest scoring tree.
First, note that this algorithm indeed returns a
tree. This can be shown by assuming the opposite,
in which case the tree has to have a cycle. Then
there must be a mention that has its antecedent to
the right. Though this is not possible since all arcs
point from left to right.
Second, this tree is the highest scoring tree.
Again, assume the contrary, i.e., that there is a
higher scoring tree in Y(A). This implies that for
some mention there is a higher scoring antecedent
than the one selected by the decoder. This contra-
dicts the fact that the best-first decoder selects the
highest scoring antecedent for each mention.
5
5 Introducing Non-local Features
Since the best-first decoder makes a left-to-right
pass, it is possible to extract features on the partial
structure on the left. Such non-local features are
able to capture information beyond that of a men-
tion and its potential antecedent, e.g., the size of
a partially built cluster, or features extracted from
the antecedent of the antecedent.
When only local features are used, greedy
search (either with CLE or the best-first decoder)
suffices to find the highest scoring tree. That is,
greedy search provides an exact solution to equa-
tion 2. Non-local features, however, render the ex-
act search problem intractable. This is because
with non-local features, locally suboptimal (i.e.,
non-greedy) antecedents for some mentions may
lead to a higher total score over a whole document.
In order to keep some options around during
search, we extend the best-first decoder with beam
search. Beam search works incrementally by
keeping an agenda of state items. At each step,
all items on the agenda are expanded. The subset
of size k (the beam size) of the highest scoring ex-
pansions are retained and put back into the agenda
for the next step. The feature extraction function ?
5
In case there are multiple maximum spanning trees, the
best-first decoder will return one of them. This also holds for
the CLE algorithm. With proper definitions, the proof can be
constructed to show that both search algorithms return trees
belonging to the set of maximum spanning trees over a graph.
is also extended such that it also receives the cur-
rent state s as an argument: ?(?m
i
,m
j
?, s). The
state encodes the previous decisions and enables ?
to extract features from the partial tree on the left.
We now outline three different ways of learning
the weight vector w with non-local features.
5.1 Early updates
The beam search decoder can be plugged into the
training algorithm, replacing the calls to arg max.
Since state items leading to the best tree may
be pruned from the agenda before the decoder
reaches the end of the document, the introduc-
tion of non-local features may cause the decoder
to return a non-optimal tree. This is problem-
atic as it might cause updates although the correct
tree has a higher score than the predicted one. It
has previously been observed (Huang et al, 2012)
that substantial gains can be made by applying an
early update strategy (Collins and Roark, 2004):
if the correct item is pruned before reaching the
end of the document, then stop and update.
While beam search and early updates have been
successfully applied to other NLP applications,
our task differs in two important aspects: First,
coreference resolution is a much more difficult
task, which relies on more (world) knowledge than
what is available in the training data. In other
words, it is unlikely that we can devise a feature
set that is informative enough to allow the weight
vector to converge towards a solution that lets the
learning algorithm see the entire documents dur-
ing training, at least in the situation when no ex-
ternal knowledge sources are used.
Second, our gold structure is not known but
is induced latently, and may vary from iteration
to iteration. With non-local features this is trou-
blesome since the best latent tree of a complete
document may not necessarily coincide with the
best partial tree at some intermediate mentionm
j
,
j < n, i.e., a mention before the last in a docu-
ment. We therefore also apply beam search to find
the latent tree to have a partial gold structure for
every mention in a document.
Algorithm 2 shows pseudocode for the beam
search and early update training procedure. The
algorithm maintains two parallel agendas, one for
gold items and one for predicted items. At ev-
ery mention, both agendas are expanded and thus
cover the same set of mentions. Then the predicted
agenda is checked to see if it contains any correct
50
Algorithm 2 Beam search and early update
Input: Data set D, epochs T , beam size k
Output: weight vector w
1: w =
??
0
2: for t ? 1..T do
3: for ?M
i
,A
i
,
?
A
i
? ? D do
4: Agenda
G
= {}
5: Agenda
P
= {}
6: for j ? 1..n do
7: Agenda
G
= EXPAND(Agenda
G
,
?
A
j
,m
j
, k)
8: Agenda
P
= EXPAND(Agenda
P
, A
j
,m
j
, k)
9: if ?CONTAINSCORRECT(Agenda
P
) then
10: y? = EXTRACTBEST(Agenda
G
)
11: y? = EXTRACTBEST(Agenda
P
)
12: update . PA update
13: GOTO 3 . Skip and move to next instance
14: y? = EXTRACTBEST(Agenda
P
)
15: if ?CORRECT(y?) then
16: y? = EXTRACTBEST(Agenda
G
)
17: update . PA update
item. If there is no correct item in the predicted
agenda, search is halted and an update is made
against the best item from the gold agenda. The
algorithm then moves on to the next document. If
the end of a document is reached, the top scoring
predicted item is checked for correctness. If it is
not, an update is made against the best gold item.
A drawback of early updates is that the remain-
der of the document is skipped when an early up-
date is applied, effectively discarding some train-
ing data.
6
An alternative strategy that makes bet-
ter use of the training data is to apply the max-
violation procedure suggested by Huang et al
(2012). However, since our gold trees change from
iteration to iteration, and even inside of a single
document, it is not entirely clear with respect to
what gold tree the maximum violation should be
computed. Initial experiments with max-violation
updates indicated that they did not improve much
over early updates, and also had a tendency to only
consider a smaller portion of the training data.
5.2 LaSO
To make full use of the training data we imple-
mented Learning as Search Optimization (LaSO;
Daum?e III and Marcu, 2005b). It is very similar
to early updates, but differs in one crucial respect:
When an early update is made, search is continued
rather than aborted. Thus the learning algorithm
always reaches the end of a document, avoiding
the problem that early updates discard parts of the
training data.
6
In fact, after 50 iterations about 70% of the mentions in
the training data are still being ignored due to early updates.
Correct items are computed the same way as
with early updates, where an agenda of gold items
is maintained in parallel. When search is resumed
after an intermediate LaSO update, the prediction
agenda is re-seeded with gold items (i.e., items
that are all correct). This is necessary since the
update influences what the partial gold structure
looks like, and the gold agenda therefore needs to
be recreated from the beginning of the document.
Specifically, after each intermediate LaSO update,
the gold agenda is expanded repeatedly from the
beginning of the document to the point where the
update was made, and is then copied over to seed
the prediction agenda. In terms of pseudocode,
this is accomplished by replacing lines 12 and 13
in Algorithm 2 with the following:
12: update . PA update
13: Agenda
G
= {}
14: for m
i
? {m
1
, ...,m
j
} . Recreate gold agenda
15: Agenda
G
= EXPAND(Agenda
G
,
?
A
i
,m
i
, k)
16: Agenda
P
= COPY(Agenda
G
)
17: GOTO 6 . Continue
5.3 Delayed LaSO updates
When we applied LaSO, we noticed that it per-
formed worse than the baseline learning algorithm
when only using local features. We believe that the
reason is that updates are made in the middle of
documents which means that lexical forms of an-
tecedents are ?fresh in memory? of the weight vec-
tor. This results in fewer mistakes during training
and leads to fewer updates. While this feedback
makes it easier during training, such feedback is
not available during test time, and the LaSO learn-
ing setting therefore mimics the testing setting to
a lesser extent.
We also found that LaSO updates change the
shape of the latent tree and that the average dis-
tance between mentions connected by an arc in-
creased. This problem can also be attributed to
how lexical items are fresh in memory. Such trees
tend to deviate from the intuition that the latent
trees are easier to learn. They also render distance-
based features (which are standard practice and
generally rather useful) less powerful, as distance
in sentences or mentions becomes less of a reliable
indicator for coreference.
To cope with this problem, we devised the
delayed LaSO update, which differs from LaSO
only in the respect that it postpones the actual up-
dates until the end of a document. This is accom-
plished by summing the distance vectors ? at ev-
ery point where LaSO would make an update. At
51
Algorithm 3 Delayed LaSO update
Input: Data set D, iterations T , beam size k
Output: weight vector w
1: w =
??
0
2: for t ? 1..T do
3: for ?M
i
,A
i
,
?
A
i
? ? D do
4: Agenda
G
= {}
5: Agenda
P
= {}
6: ?
acc
=
??
0
7: loss
acc
= 0
8: for j ? 1..n do
9: Agenda
G
= EXPAND(Agenda
G
,
?
A
j
,m
j
, k)
10: Agenda
P
= EXPAND(Agenda
P
, A
j
,m
j
, k)
11: if ?CONTAINSCORRECT(Agenda
P
) then
12: y? = EXTRACTBEST(Agenda
G
)
13: y? = EXTRACTBEST(Agenda
P
)
14: ?
acc
= ?
acc
+ ?(y?)? ?(y?)
15: loss
acc
= loss
acc
+ LOSS(y?)
16: Agenda
P
= Agenda
G
17: y? = EXTRACTBEST(Agenda
P
)
18: if ?CORRECT(y?) then
19: y? = EXTRACTBEST(Agenda
G
)
20: ?
acc
= ?
acc
+ ?(y?)? ?(y?)
21: loss
acc
= loss
acc
+ LOSS(y?)
22: if ?
acc
6=
??
0 then
23: update w.r.t. ?
acc
and loss
acc
the end of a document, an update is made with re-
spect to the sum of all ??s. Similarly, a running
sum of the partial loss is maintained within a doc-
ument. Since the PA update only depends on the
distance vector ? and the loss, it can be applied
with respect to these sums at the end of the doc-
ument. When only local features are used, this
update is equivalent to the updates in the baseline
learning algorithm. This follows because greedy
search finds the optimal tree when only local fea-
tures are used. Similarly, using only local features,
the beam-based best-first decoder will also return
the optimal tree. Algorithm 3 shows the pseu-
docode for the delayed LaSO learning algorithm.
6 Features
In this section we briefly outline the type of fea-
tures we use. The feature sets are customized for
each language. As a baseline we use the features
from Bj?orkelund and Farkas (2012), who ranked
second in the 2012 CoNLL shared task and is pub-
licly available. The exact definitions and feature
sets that we use are available as part of the down-
load package of our system.
6.1 Local features
Basic features that can be extracted on one or
both mentions in a pair include (among oth-
ers): Mention type, which is either root, pro-
noun, name, or common; Distance features, e.g.,
the distance in sentences or mentions; Rule-based
features, e.g., StringMatch or SubStringMatch;
Syntax-based features, e.g., category labels or
paths in the syntax tree; Lexical features, e.g., the
head word of a mention or the last word of a men-
tion.
In order to have a strong local baseline, we ap-
plied greedy forward/backward feature selection
on the training data using a large set of local fea-
ture templates. Specifically, the training set of
each language was split into two parts where 75%
was used for training, and 25% for testing. Feature
templates were incrementally added or removed
in order to optimize the mean of MUC, B
3
, and
CEAF
e
(i.e., the CoNLL average).
6.2 Non-local Features
We experimented with non-local features drawn
from previous work on entity-mention mod-
els (Luo et al, 2004; Rahman and Ng, 2009), how-
ever they did not improve performance in prelimi-
nary experiments. The one exception is the size of
a cluster (Culotta et al, 2007). Additional features
we use are
Shape encodes the linear ?shape? of a cluster in
terms of mention type. For instance, the clusters
representing Gary Wilber and Drug Emporium
Inc. from the example in Figure 1, would be repre-
sented as RNPN and RNCCC, respectively. Where
R, N, P, and C denote the root node, names, pro-
nouns, and common noun phrases, respectively.
Local syntactic context is inspired by the Entity
Grid (Barzilay and Lapata, 2008), where the ba-
sic assumption is that references to an entity fol-
low particular syntactic patterns. For instance, an
entity may be introduced as an object in one sen-
tence, whereas in subsequent sentences it is re-
ferred to in subject position. Grammatical func-
tions are approximated by the path in the syntax
tree from a mention to its closest S node. The par-
tial paths of a mention and its linear predecessor,
given the cluster of the current antecedent, informs
the model about the local syntactic context.
Cluster start distance denotes the distance in
mentions from the beginning of the document
where the cluster of the antecedent in considera-
tion begins.
Additionally, the non-local model also has ac-
cess to the basic properties of other mentions in
the partial tree structure, such as head words. The
52
non-local features were selected with the same
greedy forward strategy as the local features, start-
ing from the optimized local feature sets.
7 Experimental Setup
We apply our model to the CoNLL 2012 Shared
Task data, which includes a training, develop-
ment, and test set split for three languages: Ara-
bic, Chinese and English. We follow the closed
track setting where systems may only be trained
on the provided training data, with the exception
of the English gender and number data compiled
by Bergsma and Lin (2006). We use automatically
extracted mentions using the same mention extrac-
tion procedure as Bj?orkelund and Farkas (2012).
We evaluate our system using the CoNLL 2012
scorer, which computes several coreference met-
rics: MUC (Vilain et al, 1995), B
3
(Bagga and
Baldwin, 1998), and CEAF
e
and CEAF
m
(Luo,
2005). We also report the CoNLL average (also
known as MELA; Denis and Baldridge (2009)),
i.e., the arithmetic mean of MUC, B
3
, and CEAF
e
.
It should be noted that for B
3
and the CEAF met-
rics, multiple ways of handling twinless mentions
7
have been proposed (Rahman and Ng, 2009; Stoy-
anov et al, 2009). We use the most recent ver-
sion of the CoNLL scorer (version 7), which im-
plements the original definitions of these metrics.
8
Our system is evaluated on the version of the
data with automatic preprocessing information
(e.g., predicted parse trees). Unless otherwise
stated we use 25 iterations of perceptron training
and a beam size of 20. We did not attempt to tune
either of these parameters. We experiment with
two feature sets for each language: the optimized
local feature sets (denoted local), and the opti-
mized local feature sets extended with non-local
features (denoted non-local).
8 Results
Learning strategies. We begin by looking at the
different learning strategies. Since early updates
do not always make use of the complete docu-
ments during training, it can be expected that it
will require either a very wide beam or more iter-
ations to get up to par with the baseline learning
algorithm. Figure 3 shows the CoNLL average on
7
i.e., mentions that appear in the prediction but not in
gold, or the other way around
8
Available at http://conll.cemantix.org/
2012/software.html
54
56
58
60
62
64
0 10 20 30 40 50
CoN
LL 
avg
.
Iterations
BaselineEarly (local), k=20Early (local), k=100Early (non-local), k=20Early (non-local), k=100
Figure 3: Comparing early update training with
the baseline training algorithm.
the English development set as a function of num-
ber of training iterations with two different beam
sizes, 20 and 100, over the local and non-local fea-
ture sets. The figure shows that even after 50 itera-
tions, early update falls short of the baseline, even
when the early update system has access to more
informative non-local features.
9
In Figure 4 we compare early update with LaSO
and delayed LaSO on the English development set.
The left half uses the local feature set, and the right
the extended non-local feature set. Recall that with
only local features, delayed LaSO is equivalent to
the baseline learning algorithm. As before, early
update is considerably worse than other learning
strategies. We also see that delayed LaSO out-
performs LaSO, both with and without non-local
features. Note that plain LaSO with non-local fea-
tures only barely outperforms the delayed LaSO
with only local features (i.e., the baseline), which
indicates that only delayed LaSO is able to fully
leverage non-local features. From these results we
conclude that we are better off when the learning
algorithm handles one document at a time, instead
of getting feedback within documents.
Local vs. Non-local feature sets. Table 1 dis-
plays the differences in F-measures and CoNLL
average between the local and non-local systems
when applied to the development sets for each lan-
guage. All metrics improve when more informa-
tive non-local features are added to the local fea-
ture set. Arabic and English show considerable
improvements, and the CoNLL average increases
9
Although the Early systems still seem to show slight in-
creases after 50 iterations, it needs a considerable number of
iterations to catch up with the baseline ? after 100 iterations
the best early system is still more than half a point behind the
baseline.
53
58
59
60
61
62
63
64
65
Local Non-local
CoN
LL 
avg
.
EarlyLaSODelayed LaSO
Figure 4: Comparison of learning algorithms eval-
uated on the English development set.
MUC B
3
CEAF
m
CEAF
e
CoNLL
Arabic
local 47.33 42.51 49.71 46.49 45.44
non-local 49.31 43.52 50.96 47.18 46.67
Chinese
local 65.84 57.94 62.23 57.05 60.27
non-local 66.4 57.99 62.37 57.12 60.5
English
local 69.95 58.7 62.91 56.03 61.56
non-local 70.74 60.03 65.01 56.8 62.52
Table 1: Comparison of local and non-local fea-
ture sets on the development sets.
about one point. For Chinese the gains are gen-
erally not as pronounced, though the MUC metric
goes up by more than half a point.
Final results. In Table 2 we compare the re-
sults of the non-local system (This paper) to the
best results from the CoNLL 2012 Shared Task.
10
Specifically, this includes Fernandes et al?s (2012)
system for Arabic and English (denoted Fernan-
des), and Chen and Ng?s (2012) system for Chi-
nese (denoted C&N). For English we also com-
pare it to the Berkeley system (Durrett and Klein,
2013), which, to our knowledge, is the best pub-
licly available system for English coreference res-
olution (denoted D&K). As a general baseline, we
also include Bj?orkelund and Farkas? (2012) sys-
tem (denoted B&F), which was the second best
system in the shared task. For almost all met-
rics our system is significantly better than the best
competitor. For a few metrics the best competitor
outperforms our results for either precision or re-
call, but in terms of F-measures and the CoNLL
average our system is the best for all languages.
10
Thanks to Sameer Pradhan for providing us with the out-
puts of the other systems for significance testing.
9 Related Work
On the machine learning side Collins and Roark?s
(2004) work on the early update constitutes our
starting point. The LaSO framework was intro-
duced by Daum?e III and Marcu (2005b), but has,
to our knowledge, only been applied to the related
task of entity detection and tracking (Daum?e III
and Marcu, 2005a). The theoretical motivation for
early updates was only recently explained rigor-
ously (Huang et al, 2012). The delayed LaSO
update that we propose decomposes the predic-
tion task of a complex structure into a number of
subproblems, each of which guarantee violation,
using Huang et al?s (2012) terminology. We be-
lieve this is an interesting novelty, as it leverages
the complete structures for every training instance
during every iteration, and expect it to be applica-
ble also to other structured prediction tasks.
Our approach also resembles imitation learning
techniques such as SEARN (Daum?e III et al, 2009)
and DAGGER (Ross et al, 2011), where the search
problem is reduced to a sequence of classification
steps that guide the search algorithm through the
search space. These frameworks, however, rely on
the notion of an expert policy which provides an
optimal decision at each point during search. In
our context that would require antecedents for ev-
ery mention to be given a priori, rather than using
latent antecedents as we do.
Perceptrons for coreference. The perceptron
has previously been used to train coreference re-
solvers either by casting the problem as a binary
classification problem that considers pairs of men-
tions in isolation (Bengtson and Roth, 2008; Stoy-
anov et al, 2009; Chang et al, 2012, inter alia) or
in the structured manner, where a clustering for an
entire document is predicted in one go (Fernandes
et al, 2012). However, none of these works use
non-local features. Stoyanov and Eisner (2012)
train an Easy-First coreference system with the
perceptron to learn a sequence of join operations
between arbitrary mentions in a document and ac-
cesses non-local features through previous merge
operations in later stages. Culotta et al (2007) also
apply online learning in a first-order logic frame-
work that enables non-local features, though using
a greedy search algorithm.
Latent antecedents. The use of latent an-
tecedents goes back to the work of Yu and
Joachims (2009), although the idea of determining
54
MUC B
3
CEAF
m
CEAF
e
CoNLL
Rec Prec F
1
Rec Prec F
1
Rec Prec F
1
Rec Prec F
1
avg.
Arabic
B&F 43.9 52.51 47.82 35.7 49.77 41.58 43.8 50.03 46.71 40.45 41.86 41.15 43.51
Fernandes 43.63 49.69 46.46 38.39 47.7 42.54 47.6 50.85 49.17 48.16 45.03 46.54 45.18
This paper 47.53 53.3 50.25 44.14 49.34 46.6 50.94 55.19 52.98 49.2 49.45 49.33 48.72
Chinese
B&F 58.72 58.49 58.61 49.17 53.2 51.11 56.68 51.86 54.14 55.36 41.8 47.63 52.45
C&N 59.92 64.69 62.21 51.76 60.26 55.69 59.58 60.45 60.02 58.84 51.61 54.99 57.63
This paper 62.57 69.39 65.8 53.87 61.64 57.49 58.75 64.76 61.61 54.65 59.33 56.89 60.06
English
B&F 65.23 70.1 67.58 49.51 60.69 54.47 56.93 59.51 58.19 51.34 49.14 59.21 57.42
Fernandes 65.83 75.91 70.51 51.55 65.19 57.58 57.48 65.93 61.42 50.82 57.28 53.86 60.65
D&K 66.58 74.94 70.51 53.2 64.56 58.33 59.19 66.23 62.51 52.9 58.06 55.36 61.4
This paper 67.46 74.3 70.72 54.96 62.71 58.58 60.33 66.92 63.45 52.27 59.4 55.61 61.63
Table 2: Comparison with other systems on the test sets. Bold numbers indicate significance at the
p < 0.05 level between the best and the second best systems (according to the CoNLL average) using
a Wilcoxon signed rank sum test. We refrain from significance tests on the CoNLL average, as it is an
average over other F-measures.
meaningful antecedents for mentions can be traced
back to Ng and Cardie (2002) who used a rule-
based approach. Latent antecedents have recently
gained popularity and were used by two systems in
the CoNLL 2012 Shared Task, including the win-
ning system (Fernandes et al, 2012; Chang et al,
2012). Durrett and Klein (2013) present a corefer-
ence resolver with latent antecedents that predicts
clusterings over entire documents and fit a log-
linear model with a custom task-specific loss func-
tion using AdaGrad (Duchi et al, 2011). Chang
et al (2013) use a max-margin approach to learn
a pairwise model and rely on stochastic gradient
descent to circumvent the costly operation of de-
coding the entire training set in order to compute
the gradients and the latent antecedents. None of
the aforementioned works use non-local features
in their models, however.
Entity-mention models. Entity-mention mod-
els that compare a single mention to a (partial)
cluster have been studied extensively and several
works have evaluated non-local entity-level fea-
tures (Luo et al, 2004; Yang et al, 2008; Rah-
man and Ng, 2009). Luo et al (2004) also apply
beam search at test time, but use a static assign-
ment of antecedents and learns log-linear model
using batch learning. Moreover, these works al-
ter the basic feature definitions from their pair-
wise models when introducing entity-level fea-
tures. This contrasts with our work, as our
mention-pair model simply constitutes a special
case of the non-local system.
10 Conclusion
We presented experiments with a coreference re-
solver that leverages non-local features to improve
its performance. The application of non-local fea-
tures requires the use of an approximate search al-
gorithm to keep the problem tractable. We eval-
uated standard perceptron learning techniques for
this setting both using early updates and LaSO. We
found that the early update strategy is considerably
worse than a local baseline, as it is unable to ex-
ploit all training data. LaSO resolves this issue by
giving feedback within documents, but still under-
performs compared to the baseline as it distorts the
choice of latent antecedents.
We introduced a modification to LaSO, where
updates are delayed until each document is pro-
cessed. In the special case where only local fea-
tures are used, this method coincides with stan-
dard structured perceptron learning that uses exact
search. Moreover, it is also able to profit from non-
local features resulting in improved performance.
We evaluated our system on all three languages
from the CoNLL 2012 Shared Task and present
the best results to date on these data sets.
Acknowledgments
We are grateful to the anonymous reviewers as
well as Christian Scheible and Wolfgang Seeker
for comments on earlier versions of this paper.
This research has been funded by the DFG via
SFB 732, project D8.
55
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563?566.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1?34.
Eric Bengtson and Dan Roth. 2008. Understand-
ing the value of features for coreference resolution.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
294?303, Honolulu, Hawaii, October. Association
for Computational Linguistics.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 33?40,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Anders Bj?orkelund and Rich?ard Farkas. 2012. Data-
driven multilingual coreference resolution using re-
solver stacking. In Joint Conference on EMNLP and
CoNLL - Shared Task, pages 49?55, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Bernd Bohnet, Simon Mille, Beno??t Favre, and Leo
Wanner. 2011. <stumaba >: From deep represen-
tation to surface. In Proceedings of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation, pages 232?235,
Nancy, France, September. Association for Compu-
tational Linguistics.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 89?97, Bei-
jing, China, August.
Kai-Wei Chang, Rajhans Samdani, Alla Rozovskaya,
Mark Sammons, and Dan Roth. 2012. Illinois-
coref: The ui system in the conll-2012 shared task.
In Joint Conference on EMNLP and CoNLL - Shared
Task, pages 113?117, Jeju Island, Korea, July. Asso-
ciation for Computational Linguistics.
Kai-Wei Chang, Rajhans Samdani, and Dan Roth.
2013. A constrained latent variable model for coref-
erence resolution. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 601?612, Seattle, Washington,
USA, October. Association for Computational Lin-
guistics.
Chen Chen and Vincent Ng. 2012. Combining the
best of two worlds: A hybrid approach to multilin-
gual coreference resolution. In Joint Conference on
EMNLP and CoNLL - Shared Task, pages 56?63,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Yoeng-jin Chu and Tseng-hong Liu. 1965. On the
shortest aborescence of a directed graph. Science
Sinica, 14:1396?1400.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Volume,
pages 111?118, Barcelona, Spain, July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1?8. Associ-
ation for Computational Linguistics, July.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive?aggressive algorithms. Journal of Machine
Learning Reseach, 7:551?585, March.
Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for corefer-
ence resolution. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 81?88,
Rochester, New York, April. Association for Com-
putational Linguistics.
Hal Daum?e III and Daniel Marcu. 2005a. A large-
scale exploration of effective global features for a
joint entity detection and tracking model. In Pro-
ceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Natu-
ral Language Processing, pages 97?104, Vancouver,
British Columbia, Canada, October. Association for
Computational Linguistics.
Hal Daum?e III and Daniel Marcu. 2005b. Learning
as search optimization: approximate large margin
methods for structured prediction. In ICML, pages
169?176.
Hal Daum?e III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Machine
Learning, 75(3):297?325.
Pascal Denis and Jason Baldridge. 2009. Global Joint
Models for Coreference Resolution and Named En-
tity Classification. In Procesamiento del Lenguaje
Natural 42, pages 87?96, Barcelona: SEPLN.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121?2159, July.
Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1971?1982,
56
Seattle, Washington, USA, October. Association for
Computational Linguistics.
Jack Edmonds. 1967. Optimum branchings. Jour-
nal of Research of the National Bureau of Standards,
71(B):233?240.
Eraldo Fernandes, C??cero dos Santos, and Ruy Milidi?u.
2012. Latent structure perceptron with feature in-
duction for unrestricted coreference resolution. In
Joint Conference on EMNLP and CoNLL - Shared
Task, pages 41?48, Jeju Island, Korea, July. Associ-
ation for Computational Linguistics.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142?151, Montr?eal, Canada, June. Association for
Computational Linguistics.
Liang Huang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proceedings
of ACL-08: HLT, pages 586?594, Columbus, Ohio,
June. Association for Computational Linguistics.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the bell tree. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguis-
tics, pages 135?142, Barcelona, Spain, July.
Xiaoqiang Luo. 2005. On coreference resolution
performance metrics. In Proceedings of Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 25?32, Vancouver, British Columbia,
Canada, October. Association for Computational
Linguistics.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of 40th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 104?
111, Philadelphia, Pennsylvania, USA, July. Asso-
ciation for Computational Linguistics.
Vincent Ng. 2010. Supervised noun phrase coref-
erence research: The first fifteen years. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1396?
1411, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-
2012 shared task: Modeling multilingual unre-
stricted coreference in ontonotes. In Joint Confer-
ence on EMNLP and CoNLL - Shared Task, pages
1?40, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968?977, Singapore,
August. Association for Computational Linguistics.
Lev Ratinov and Dan Roth. 2009. Design chal-
lenges and misconceptions in named entity recog-
nition. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 147?155, Boulder, Colorado,
June. Association for Computational Linguistics.
St?ephane Ross, Geoffrey J. Gordon, and J. Andrew
Bagnell. 2011. A reduction of imitation learning
and structured prediction to no-regret online learn-
ing. In AISTATS, pages 627?635.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Veselin Stoyanov and Jason Eisner. 2012. Easy-first
coreference resolution. In Proceedings of COLING
2012, pages 2519?2534, Mumbai, India, December.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 656?664, Suntec,
Singapore, August. Association for Computational
Linguistics.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model the-
oretic coreference scoring scheme. In Proceedings
MUC-6, pages 45?52, Columbia, Maryland.
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan,
Ting Liu, and Sheng Li. 2008. An entity-
mention model for coreference resolution with in-
ductive logic programming. In Proceedings of ACL-
08: HLT, pages 843?851, Columbus, Ohio, June.
Association for Computational Linguistics.
Chun-Nam Yu and T. Joachims. 2009. Learning struc-
tural svms with latent variables. In International
Conference on Machine Learning (ICML).
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 562?
571, Honolulu, Hawaii, October. Association for
Computational Linguistics.
57
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 7?12,
Baltimore, Maryland USA, June 23-24, 2014.
c
?2014 Association for Computational Linguistics
Visualization, Search, and Error Analysis for Coreference Annotations
Markus G
?
artner Anders Bj
?
orkelund Gregor Thiele Wolfgang Seeker Jonas Kuhn
Institute for Natural Language Processing
University of Stuttgart
{thielegr,seeker,gaertnms,anders,kuhn}@ims.uni-stuttgart.de
Abstract
We present the ICARUS Coreference Ex-
plorer, an interactive tool to browse and
search coreference-annotated data. It can
display coreference annotations as a tree,
as an entity grid, or in a standard text-
based display mode, and lets the user
switch freely between the different modes.
The tool can compare two different an-
notations on the same document, allow-
ing system developers to evaluate errors in
automatic system predictions. It features
a flexible search engine, which enables
the user to graphically construct search
queries over sets of documents annotated
with coreference.
1 Introduction
Coreference resolution is the task of automatically
grouping references to the same real-world entity
in a document into a set. It is an active topic in cur-
rent NLP research and has received considerable
attention in recent years, including the 2011 and
2012 CoNLL shared tasks (Pradhan et al., 2011;
Pradhan et al., 2012).
Coreference relations are commonly repre-
sented by sets of mentions, where all mentions
in one set (or coreference cluster) are considered
coreferent. This type of representation does not
support any internal structure within the clusters.
However, many automatic coreference resolvers
establish links between pairs of mentions which
are subsequently transformed to a cluster by tak-
ing the transitive closure over all links, i.e., placing
all mentions that are directly or transitively classi-
fied as coreferent in one cluster. This is particu-
larly the case for several state-of-the-art resolvers
(Fernandes et al., 2012; Durrett and Klein, 2013;
Bj?orkelund and Kuhn, 2014). These pairwise de-
cisions, which give rise to a clustering, can be ex-
ploited for detailed error analysis and more fine-
grained search queries on data automatically an-
notated for coreference.
We present the ICARUS Coreference Explorer
(ICE), an interactive tool to browse and search
coreference-annotated data. In addition to stan-
dard text-based display modes, ICE features two
other display modes: an entity-grid (Barzilay and
Lapata, 2008) and a tree view, which makes use
of the internal pairwise links within the clusters.
ICE builds on ICARUS (G?artner et al., 2013), a
platform for search and exploration of dependency
treebanks.
1
ICE is geared towards two (typically) distinct
users: The NLP developer who designs corefer-
ence resolution systems can inspect the predic-
tions of his system using the three different dis-
play modes. Moreover, ICE can compare the pre-
dictions of a system to a gold standard annotation,
enabling the developer to inspect system errors in-
teractively. The second potential user is the cor-
pus linguist, who might be interested in brows-
ing or searching a document, or a (large) set of
documents for certain coreference relations. The
built-in search engine of ICARUS now also allows
search queries over sets of documents in order to
meet the needs of this type of user.
2 Data Representation
ICE reads the formats used in the 2011 and 2012
CoNLL shared tasks as well as the SemEval 2010
format (Recasens et al., 2010).
2
Since these for-
mats cannot accommodate pairwise links, an aux-
iliary file with standoff annotation can be pro-
vided, which we call allocation. An allocation is a
list of pairwise links between mentions. Multiple
1
ICE is written in Java and is therefore platform indepen-
dent. It is open source (under GNU GPL) and we provide
both sources and binaries for download on http://www.
ims.uni-stuttgart.de/data/icarus.html
2
These two formats are very similar tabular formats, but
differ slightly in the column representations.
7
allocations can be associated with a single docu-
ment and the user can select one of these for dis-
play or search queries. An allocation can also in-
clude properties on mentions and links. The set
of possible properties is not constrained, and the
user can freely specify properties as a list of key-
value pairs. Properties on mentions may include,
e.g., grammatical gender or number, or informa-
tion status labels. Additionally, a special property
that indicates the head word of a mention can be
provided in an allocation. The head property en-
ables the user to access head words of mentions
for display or search queries.
The motivation for keeping the allocation file
separate from the CoNLL or SemEval files is two-
fold: First, it allows ICE to work without hav-
ing to provide an allocation file, thereby making it
easy to use with the established formats for coref-
erence. The user is still able to introduce addi-
tional structure by the use of the allocation file.
Second, multiple allocation files allow the user to
switch between different allocations while explor-
ing a set of documents. Moreover, as we will see
in Section 3.3, ICE can also compare two different
allocations in order to highlight the differences.
In addition to user-specified allocations, ICE
will always by default provide an internal structure
for the clusters, in which the correct antecedent
of every mention is the closest coreferent mention
with respect to the linear order of the document
(this is equivalent to the training instance creation
heuristic proposed by Soon et al. (2001)). There-
fore, the user is not required to define an allocation
on their own.
3 Display Modes
In this section we describe the entity grid and tree
display modes by means of screenshots. ICE addi-
tionally includes a standard text-based view, sim-
ilar to other coreference visualization tools. The
example document is taken from the CoNLL 2012
development set (Pradhan et al., 2012) and we
use two allocations: (1) the predictions output by
Bj?orkelund and Kuhn (2014) system (predicted)
and (2) a gold allocation that was obtained by
running the same system in a restricted setting,
where only links between coreferent mentions are
allowed (gold). The complete document can be
seen in the lower half of Figure 1.
3.1 Entity grid
Barzilay and Lapata (2008) introduce the entity
grid, a tabular view of entities in a document.
Specifically, rows of the grid correspond to sen-
tences, and columns to entities. The cells of the ta-
ble are used to indicate that an entity is mentioned
in the corresponding sentence. Entity grids pro-
vide a compact view on the distribution of men-
tions in a document and allow the user to see how
the description of an entity changes from mention
to mention.
Figure 1 shows ICE?s entity-grid view for the
example document using the predicted allocation.
When clicking on a cell in the entity grid the im-
mediate textual context of the cell is shown in the
lower pane. In Figure 1, the cell with the blue
background has been clicked, which corresponds
to the two mentions firms from Taiwan and they.
These mentions are thus highlighted in the lower
pane. The user can also right-click on a cell and
jump straight to the tree view, centered around the
same mentions.
3.2 Label Patterns
The information that is displayed in the cells of
the entity grid (and also on the nodes in the tree
view, see Section 3.3) can be fully customized by
the user. The customization is achieved by defin-
ing label patterns. A label pattern is a string that
specifies the format according to which a mention
will be displayed. The pattern can extract infor-
mation on a mention according to three axes: (1)
at the token- level for the full mention, extracting,
e.g., the sequence of surface forms or the part-of-
speech tags of a mention; (2) at the mention- level,
extracting an arbitrary property of a mention as de-
fined in an allocation; (3) token-level information
from the head word of a mention.
Label patterns can be defined interactively
while displaying a document and the three axes are
referenced by dedicated operators. For instance,
the label pattern $form$ extracts the full surface
form of a mention, whereas #form# only extracts
the surface form of the head word of a mention.
All properties defined by the user in the allocation
(see Section 2) are accessible via label patterns.
For example, the allocations we use for Fig-
ure 1 include a number of properties on the
mentions, most of which are internally com-
puted by the coreference system: The TYPE of
a mention, which can take any of the values
8
Figure 1: Entity grid over the predicted clustering in the example document.
{Name, Common, Pronoun} and is inferred from
the part- of-speech tags in the CoNLL file; The
grammatical NUMBER of a mention, which is as-
signed based on the number and gender data com-
piled by Bergsma and Lin (2006) and can take
the values {Sin, Plu, Unknown}. The label pat-
tern for displaying the number property associated
with a mention would be %Number%.
The label pattern used in Figure 1 is defined
as ("$form$" - %Type% - %Number%). This pat-
tern accesses the full surface form of the mentions
($form$), as well as the TYPE (%Type%) and gram-
matical NUMBER (%Number%) properties defined
in the allocation file.
Custom properties and label patterns can be
used for example to display the entity grid in the
form proposed by Barzilay and Lapata (2008): In
the allocation, we assign a coarse-grained gram-
matical function property (denoted GF) to every
mention, where each mention is tagged as either
subject, object, or other (denoted S, O, X, respec-
tively).
3
The label pattern %GF% then displays the
grammatical function of each mention in the entity
grid, as shown in Figure 2.
3.3 Tree view
Pairwise links output by an automatic coreference
system can be treated as arcs in a directed graph.
Linking the first mention of each cluster to an ar-
tificial root node creates a tree structure that en-
codes the entire clustering in a document. This
representation has been used in coreference re-
3
The grammatical function was assigned by converting
the phrase-structure trees in the CoNLL file (which lack
grammatical function information) to Stanford dependencies
(de Marneffe and Manning, 2008), and then extracting the
grammatical function from the head word in each mention.
Figure 2: Example entity grid, using the labels by
Barzilay and Lapata (2008).
solvers (Fernandes et al., 2012; Bj?orkelund and
Kuhn, 2014), but ICE uses it to display links be-
tween mentions introduced by an automatic (pair-
wise) resolver.
Figure 3 shows three examples of the tree view
of the same document as before: The gold allo-
cation (3a), the predicted allocation (3b), as well
as the differential view, where the two allocations
are compared (3c). Each mention corresponds to
a node in the trees and all mentions are directly or
transitively dominated by the artificial root node.
Every subtree under the root constitutes its own
cluster and a solid arc between two mentions de-
notes that the two mentions are coreferent accord-
ing to a coreference allocation. The information
displayed in the nodes of the tree can be cus-
tomized using label patterns.
In the differential view (Figure 3c), solid arcs
correspond to the predicted allocation. Dashed
nodes and arcs are present in the gold allocation,
but not in the prediction. Discrepancies between
the predicted and the gold allocations are marked
9
(a) Tree representing the gold allocation. (b) Tree representing the predicted allocation.
(c) Differential view displaying the difference between the gold and predicted allocations.
Figure 3: Tree view over the example document (gold, predicted, differential).
with different colors denoting different types of er-
rors. The example in Figure 3c contains two errors
made by the system:
1. A false negative mention, denoted by the
dashed red node Shangtou. In the gold
standard (Figure 3a) this mention is clus-
tered with other mentions such as Shantou ?s,
Shantou City, etc. The dashed arc between
Shantou ?s and Shangtou is taken from the
gold allocation, and indicates what the sys-
tem prediction should have been like.
4
2. A foreign antecedent, denoted by the solid
orange arc between Shantou ?s new high level
technology development zone and Shantou.
In this case, the coreference system erro-
neously clustered these two mentions. The
correct antecedent is indicated by the dashed
arc that originates from the document root.
4
This error likely stems from the fact that Shantou is
spelled two different ways within the same document which
causes the resolver?s string-matching feature to fail.
This error is particularly interesting since the
system effectively merges the two clusters
corresponding to Shantou and Shantou? s new
high level technology development zone. The
tree view, however, shows that the error stems
from a single link between these two men-
tions, and that the developer needs to address
this.
Since the tree-based view makes pairwise de-
cisions explicit, the differential view shown in
Figure 3c is more informative to NLP develop-
ers when inspecting errors by automatic system
than comparing a gold standard clustering to a pre-
dicted one. The problem with analyzing the error
on clusterings instead of trees is that the clusters
would be merged, i.e., it is not clear where the ac-
tual mistake was made.
Additional error types not illustrated by Fig-
ure 3c include false positive mentions, where
the system invents a mention that is not part
of the gold allocation. When a false positive
mention is assigned as an antecedent of another
10
mention, the corresponding link is marked as an
invented antecedent. Links that erroneously start
a new cluster when it is coreferent with other men-
tions to the left is marked as false new.
4 Searching
The search engine in ICE makes the annotations
in the documents searchable for, e. g., a corpus lin-
guist who is interested in specific coreference phe-
nomena. It allows the user to express queries over
mentions related through the tree. Queries can ac-
cess the different layers of annotation, both from
the allocation file and the underlying document,
using various constructs such as, e.g., transitivity,
regular expressions, and/or disjunctions. The user
can construct queries either textually (through a
query language) or graphically (by creating nodes
and configuring constraints in dialogues). For a
further discussion of the search engine we refer to
the original ICARUS paper (G?artner et al., 2013).
Figure 4 shows a query that matches cataphoric
pronouns, i.e., pronouns that precede their an-
tecedents. The figure shows the query expressed
as a subgraph (on the left) and the corresponding
results (right) obtained on the development set of
the English CoNLL 2012 data using the manual
annotation represented in the gold allocation.
The query matches two mentions that are di-
rectly or transitively connected through the graph.
The first mention (red node) matches mentions of
the type Pronoun that have to be attached to the
document root node. In the tree formalism we
adopt, this implies that it must be the first men-
tion of its cluster. The second mention (green
node) matches any mention that is not of the type
Pronoun.
(a)
(b)
Figure 4: Example search query and correspond-
ing results.
The search results are grouped along two axes:
the surface form of the head word of the first (red)
node, and the type property of the second mention
(green node), indicated by the special grouping
operator <
*
> inside the boxes. The correspond-
ing results are shown in the right half of Figure 4,
where the first group (surface form) runs verti-
cally, and the second group (mention type) runs
horizontally. The number of hits for each configu-
ration is shown in the corresponding cell. For ex-
ample, the case that the first mention of a chain is
the pronoun I and the closest following coreferent
mention that is not a pronoun is of type Common,
occurs 6 times. By clicking on a cell, the user can
jump straight to a list of the matches, and browse
them using any of the three display modes.
5 Related Work
Two popular annotation and visualization tools
for coreference are PAlinkA (Or?asan, 2003) and
MMAX2 (M?uller and Strube, 2006), which fo-
cus on a (customizable) textual visualization with
highlighting of clusters. The TrED (Pajas and
?
St?ep?anek, 2009) project is a very flexible multi-
level annotation tool centered around tree-based
annotations that can be used to annotate and vi-
sualize coreference. It also features a powerful
search engine. Recent annotation tools include the
web-based BRAT (Stenetorp et al., 2012) and its
extension WebAnno (Yimam et al., 2013). A ded-
icated query and exploration tool for multi-level
annotations is ANNIS (Zeldes et al., 2009).
The aforementioned tools are primarily meant
as annotation tools. They have a tendency of lock-
ing the user into one type of visualization (tree- or
text-based), while often lacking advanced search
functionality. In contrast to them, ICE is not meant
to be yet another annotation tool, but was designed
as a dedicated coreference exploration tool, which
enables the user to swiftly switch between differ-
ent views. Moreover, none of the existing tools
provide an entity-grid view.
ICE is also the only tool that can graphically
compare predictions of a system to a gold standard
with a fine-grained distinction on the types of dif-
ferences. Kummerfeld and Klein (2013) present
an algorithm that transforms a predicted corefer-
ence clustering into a gold clustering and records
the necessary transformations, thereby quantify-
ing different types of errors. However, their algo-
rithm only works on clusterings (sets of mentions),
not pairwise links, and is therefore not able to pin-
point some of the mistakes that ICE can (such as
the foreign antecedent described in Section 3).
11
6 Conclusion
We presented ICE, a flexible coreference visual-
ization and search tool. The tool complements
standard text-based display modes with entity-grid
and tree visualizations. It is also able to dis-
play discrepancies between two different corefer-
ence annotations on the same document, allow-
ing NLP developers to debug coreference sys-
tems in a graphical way. The built-in search en-
gine allows corpus linguists to construct complex
search queries and provide aggregate result views
over large sets of documents. Being based on the
ICARUS platform?s plugin-engine, ICE is extensi-
ble and can easily be extended to cover additional
data formats.
Acknowledgments
This work was funded by the German Federal
Ministry of Education and Research (BMBF) via
CLARIN-D, No. 01UG1120F and the German
Research Foundation (DFG) via the SFB 732,
project D8.
References
Regina Barzilay and Mirella Lapata. 2008. Model-
ing Local Coherence: An Entity-Based Approach.
Computational Linguistics, 34(1):1?34.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In COLING-ACL,
pages 33?40, Sydney, Australia, July.
Anders Bj?orkelund and Jonas Kuhn. 2014. Learning
Structured Perceptrons for Coreference Resolution
with Latent Antecedents and Non-local Features. In
ACL, Baltimore, MD, USA, June.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies
representation. In COLING Workshop on Cross-
framework and Cross-domain Parser Evaluation.
Greg Durrett and Dan Klein. 2013. Easy Victo-
ries and Uphill Battles in Coreference Resolution.
In EMNLP, pages 1971?1982, Seattle, Washington,
USA, October.
Eraldo Fernandes, C??cero dos Santos, and Ruy Milidi?u.
2012. Latent Structure Perceptron with Feature In-
duction for Unrestricted Coreference Resolution. In
EMNLP-CoNLL: Shared Task, pages 41?48, Jeju Is-
land, Korea, July.
Markus G?artner, Gregor Thiele, Wolfgang Seeker, An-
ders Bj?orkelund, and Jonas Kuhn. 2013. ICARUS
? An Extensible Graphical Search Tool for Depen-
dency Treebanks. In ACL: System Demonstrations,
pages 55?60, Sofia, Bulgaria, August.
Jonathan K. Kummerfeld and Dan Klein. 2013. Error-
Driven Analysis of Challenges in Coreference Res-
olution. In EMNLP, pages 265?277, Seattle, Wash-
ington, USA, October.
Christoph M?uller and Michael Strube. 2006. Multi-
level annotation of linguistic data with MMAX2. In
Corpus Technology and Language Pedagogy: New
Resources, New Tools, New Methods, pages 197?
214. Peter Lang.
Constantin Or?asan. 2003. PALinkA: A highly cus-
tomisable tool for discourse annotation. In Akira
Kurematsu, Alexander Rudnicky, and Syun Tutiya,
editors, Proceedings of the Fourth SIGdial Work-
shop on Discourse and Dialogue, pages 39?43.
Petr Pajas and Jan
?
St?ep?anek. 2009. System for
Querying Syntactically Annotated Corpora. In ACL-
IJCNLP: Software Demonstrations, pages 33?36,
Suntec, Singapore.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 Shared Task: Modeling
Unrestricted Coreference in OntoNotes. In CoNLL:
Shared Task, pages 1?27, Portland, Oregon, USA,
June.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 Shared Task: Modeling Multilingual Unre-
stricted Coreference in OntoNotes. In EMNLP-
CoNLL: Shared Task, pages 1?40, Jeju Island, Ko-
rea, July.
Marta Recasens, Llu??s M`arquez, Emili Sapena,
M. Ant`onia Mart??, Mariona Taul?e, V?eronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in
multiple languages. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, pages
1?8, Uppsala, Sweden, July.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a Web-based Tool for NLP-Assisted
Text Annotation. In EACL: Demonstrations, pages
102?107, April.
Seid Muhie Yimam, Iryna Gurevych, Richard
Eckart de Castilho, and Chris Biemann. 2013.
WebAnno: A Flexible, Web-based and Visually
Supported System for Distributed Annotations. In
ACL: System Demonstrations, pages 1?6, August.
Amir Zeldes, Julia Ritz, Anke L?udeling, and Christian
Chiarcos. 2009. ANNIS: a search tool for multi-
layer annotated corpora. In Proceedings of Corpus
Linguistics.
12
Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 34?42,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
A Cross-Lingual Induction Technique for German Adverbial Participles
Sina Zarrie? Aoife Cahill Jonas Kuhn Christian Rohrer
Institut fu?r Maschinelle Sprachverarbeitung (IMS)
University of Stuttgart
Stuttgart, Germany
{zarriesa,cahillae,jonas.kuhn,rohrer}@ims.uni-stuttgart.de
Abstract
We provide a detailed comparison of
strategies for implementing medium-to-
low frequency phenomena such as Ger-
man adverbial participles in a broad-
coverage, rule-based parsing system. We
show that allowing for general adverb con-
version of participles in the German LFG
grammar seriously affects its overall per-
formance, due to increased spurious am-
biguity. As a solution, we present a
corpus-based cross-lingual induction tech-
nique that detects adverbially used par-
ticiples in parallel text. In a grammar-
based evaluation, we show that the auto-
matically induced resource appropriately
restricts the adverb conversion to a limited
class of participles, and improves parsing
quantitatively as well as qualitatively.
1 Introduction
In German, past perfect participles are ambigu-
ous with respect to their morphosyntactic cate-
gory. As in other languages, they can be used
as part of the verbal complex (example (1-a)) or
as adjectives (example (1-b)). Since German ad-
jectives can generally undergo conversion into ad-
verbs, participles can also be used adverbially (ex-
ample (1-c)). All three participle forms in (1) are
morphologically identical.
(1) a. Das Experiment hat ihn begeistert.
?The experiment has enthused him.?
b. Er scheint von dem Experiment begeistert.
?He seems enthusiastic about the experiment.?
c. Er hat begeistert experimentiert.
?He has experimented in an enthusiastic way? or:
?He was enthusiastic when he experimented.?
This paper adresses the question of how to deal
with medium-to-low frequency phenomena such
as adverbial participles in a broad-coverage, rule-
based parsing system. In order to account for sen-
tences like (1-c), an intuitive approach would be to
generally allow for adverb conversion of partici-
ples in the grammar. However, on the basis of the
German LFG grammar (Rohrer and Forst, 2006),
we show that such a rule can have a strong negative
on the overall performance of the parsing system,
despite the fact that it produces the desired syntac-
tic and semantic analysis for specific sentences.
This trade-off between large-scale, statistical
and theoretically precise coverage is often en-
countered in engineering broad-coverage and, at
the same time, linguistically motivated parsing
systems: adding the analysis for a specific phe-
nomenon does not necessarily improve the overall
quality of the system since the rule might overgen-
erate and interact with completely different phe-
nomena in unpredicted ways.
In principle, there are two ways of dealing with
such an overgeneration problem in a grammar-
based framework: First, one could hand-craft
word lists or other linguistic constraints that re-
strict the adverb conversion to a certain set of par-
ticiples. Second, one could try to mine corpora for
this particular type of adverbs and integrate this
automatically induced knowledge into the gram-
mar (i.e. by means of pre-tagged input, word lists,
etc.). In the case of adverbial participles, both
ways are prone with difficulties. To our knowl-
edge, there has not been much theoretical work on
the linguistic properties of the participle adverb
conversion. Moreover, since the distinction be-
tween (predicative) adjectives and adverbs is the-
oretically hard to establish, the standard tag set
for German and, in consequence, annotated cor-
pora for German do not explicitly capture this phe-
nomenon. Thus, available statistical taggers and
parsers for German usually conflate the syntactic
structures underlying (1-b) and (1-c).
In this paper, we present a corpus-based ap-
proach to restricting the overgenerating adverb
conversion for participles in German, exploiting
34
parallel corpora and cross-lingual NLP induc-
tion techniques. Since adverbs are often overtly
marked in other languages (i.e. the ly-suffix in
English), adverbial participles can be straightfor-
wadly detected on word-aligned parallel text. We
describe the ingretation of the automatically in-
duced resource of adverbial participles into the
German LFG, and provide a detailed evaluation of
its effect on the grammar, see Section 5.
While the use of parallel resources is rather
familiar in a wide range of NLP domains, such
as statistical machine translation (Koehn, 2005)
or annotation projection (Yarowsky et al, 2001),
our work shows that they can be exploited for
very specific problems that arise in deep linguis-
tic analysis (see Section 4). In this way, high-
precision, data-oriented induction techniques can
clearly improve rule-based system development
through combining the benefits of high empirical
accuracy and little manual effort.
2 A Broad-Coverage LFG for German
Lexical Functional Grammar (LFG) (Bresnan,
2000) is a constraint-based theory of grammar. It
posits two levels of representation, c(onstituent)-
structure and f(unctional)- structure. C-structure
is represented by contextfree phrase-structure
trees, and captures surface grammatical configu-
rations. F-structures approximate basic predicate-
argument and adjunct structures.
The experiments reported in this paper use the
German LFG grammar constructed as part of the
ParGram project (Butt et al, 2002). The grammar
is implemented in the XLE, a grammar develop-
ment environment which includes a very efficient
LFG parser. Within the spectrum of appraoches
to natural language parsing, XLE can be consid-
ered a hybrid system combining a hand-crafted
grammar with a number of automatic ambiguity
management techniques: (i) c-structure pruning
where, based on information from statstically ob-
tained parses, some trees are ruled out before f-
structure unification (Cahill et al, 2007), (ii) an
Optimaly Theory-style constraint mechanism for
filtering and ranking competing analyses (Frank
et al, 2001), and (iii) a stochastic disambiguation
component which is based on a log-linear proba-
bility model (Riezler et al, 2002) and works on
the packed representations.
The German LFG grammar integrates a mor-
phological component which is a variant of
DMOR1 (Becker, 2001). This means that the (in-
ternal) lexicon does not comprise entries for sur-
face word forms, but entries for specific morpho-
logical tags, see (Dipper, 2003).
3 Participles in the German LFG
3.1 Analysis
The morphosyntactic ambiguity of German par-
ticiples presents a notorious difficulty for theoreti-
cal and computational analysis. The reason is that
adjectives (i.e. adjectival participles) do not only
occur as attributive modifiers (shown in (1-a)), but
can also be used as predicatives (see (2-b)). These
predicatives have exactly the same form as ver-
bal or adverbial participles (compare the three sen-
tences in (2)). Predicatives do appear either as ar-
guments of verbs like seem or as free adjuncts such
that they are not even syntactically distinguishable
from adverbs. The sentence in (2-c) is thus am-
biguous as to whether the participle is an adverb
modifying the main verb, or a predicative which
modifies the subject. Especially in the case of
modifiers refering to a psychological state, the two
underlying readings are hard to tell apart (Geuder,
2004). It is due to the lack of reliable semantic
tests that the standard German tag set (Schiller et
al., 1995) assigns the tag ?ADJD? to predicative
adjectives as well as adverbs.
(2) a. Das Experiment hat ihn begeistert.
?The experiment has enthused him.?
b. Er scheint von dem Experiment begeistert.
?He seems enthusiastic about the experiment.?
c. Er hat begeistert experimentiert.
?He has experimented in an enthusiastic way? or:
?He was enthusiastic when he experimented.?
For performance reasons, the German LFG does
not cover free predicatives at the moment. In the
context of our crosslingual induction approach,
the distinction between predicatives and adverbs
is rather straigtforward since we base our experi-
ments on languages that have morphologically dis-
tinct forms for these categories. In the follow-
ing, we will thus limit the discussion to adverbial
participles and ignore the complexities related to
predicative participles.
In the German LFG, the treatment of a given
participle form is closely tight to the morphologi-
cal analysis encoded in DMOR. In particular, ad-
verbial participles can have different degrees of
lexicalisation. For bestimmt (probably) in (3-a),
which is completely lexicalised, the morphology
35
proposes two analyses: (i) a participle tag of the
verbal lemma bestimmen (determine) and (ii) an
adverb tag for the lemma bestimmt. In this case,
the LFG parsing algorithm will figure out which
morphological analysis yields a syntactically well-
formed analysis. For gezielt (purposeful) in (3-b),
DMOR outputs, besides the participle analysis, an
adjective tag for the lemma. However, the gram-
mar can turn it into an adverb by a general ad-
verb conversion rule for adjectives. The difficult
case for the German LFG grammar is illustrated in
(3-c) by means of the adverbial participle wieder-
holt (repeatedly). This participle is neither lexi-
calised as an adverb nor as an adjective, but it still
can be used as an adverb.
(3) a. Bestimmt
Probably
ist
is
dieser
the
Mann
man
sehr
very
traurig.
sad.
b. Der
The
Mann
man
hat
has
gezielt
acted
gehandelt.
purposefully.
c. Der
The
Mann
man
hat
has
wiederholt
repeatedly
geweint.
cried.
To cover sentences like (3-c), the grammar
needs to include a rule that allows adverb conver-
sion for participles. Unfortunately, this rule is very
costly in terms of the overall performance of the
grammar, as is shown in the following section.
3.2 Assessing the Effect of Participle
Ambiguity on the German LFG
In this section, we want to illustrate the effect of
one specific grammar rule, i.e. the rule that gener-
ally allows for conversion of participles into ad-
verbs. We perform a contrastive evaluation of
two versions of the grammar: (i) the No-Part-Adv
version which does not allow for adverb conver-
sion (except for the lexicalised participles from
DMOR), (ii) the All-Part-Adv version which al-
lows every participle to be analysed as adverb.
Otherwise, the two versions of the grammar are
completely identical.
The comparison between the All-Part-Adv and
No-Part-Adv grammar version pursues two major
goals: On the one hand, we want to assess their
overall quantitative performance on representative
gold standard data, as it is common practice for
statistical parsing systems. On the other hand, we
are interested in getting a detailed picture of the
quality of the grammar for parsing adverbial par-
ticiples. These two goals do not necessarily go to-
gether since we know that the phenomenon is not
very frequent in the data which we use for evalu-
ation. Therefore, we do not only report accuracy
on gold standard data in the following, but also fo-
cus on error analysis and describe ways of qualti-
tatively assessing the grammar performance.
For evaluation, we use the TIGER treebank
(Brants et al, 2002). We report grammar per-
formance on the development set which consists
of the first 5000 TIGER sentences, and statistical
accuracy on the standard heldout set which com-
prises 371 sentences.
Quantitative Evaluation We first want to assess
the quantitative impact of the phenomenon of ad-
verbial participles in our evaluation data. We parse
the heldout set storing all possible analyses ob-
tained by both grammars, in order to compare the
upperbound score that the both versions can op-
timally achieve (i.e. independently of the disam-
biguation quality). Then, we run the XLE eval-
uation in the ?oracle? mode which means that the
disambiguation compares all system analyses for a
given sentence to its gold analysis, and chooses the
best system analysis for computing accuracy. The
upperbound f-score for both grammar versions is
almost identical (at about 83.6%). This suggests
that the phenomenon of adverbial participles does
not occur in the heldout set.
If we run the grammar versions on a larger
set of sentences, the difference in coverage be-
comes more obvious. In Table 1, we report the
absolute number of parsed sentences, starred sen-
tences (only receiving a partial or fragment parse),
and the timeouts 1 on our standard TIGER devel-
opment set. Not very surprisingly, the coverage
of the All-Part-Adv version seems to be broader.
However, this does not necessarily mean that the
40 additionally covered sentences all exhibit ad-
verbial participles (see below). Moreover, Table 2
gives a first indication of the fact that the extended
coverage comes at a price: the All-Part-Adv ver-
sion massively increases the number of ambigui-
ties per sentence. Related to this, in the All-Part-
Adv version, the number of timeouts increases by
16% and parsing speed goes down by 6% com-
pared to the No-Part-Adv version.
To assess the effect of the massively increased
ambiguity rate and the bigger proportion of time-
outs in All-Part-Adv, we perform a statistical eval-
uation of the two versions of the grammar against
the heldout set, i.e. we compute f-score based
1Sentences whose parsing can not be finished in prede-
fined amount of time, the maximally allowed parse time is
set to 20 seconds.
36
Grammar Parsed
Sent.
Starred
Sent.
Time-
outs
Time
in sec
No-Part-Adv 4301 608 90 6853
All-Part-Adv 4339 555 105 7265
Table 1: Coverage-based evaluation on the TIGER
development set (sentences 1-5000), 4999 sen-
tences total
Sent. Av. ambiguities per sent. Av.
length No-Part-Adv All-Part-Adv Incr.
1-10 2.95 3.3 11%
11-20 24.99 36.09 44%
21-30 250.4 343.76 37%
31-40 1929.06 2972.847 54%
41-50 173970.0 663310.4 429%
Table 2: Average number of ambiguities per sen-
tence
on the parses that the XLE disambiguation selects
as the most probable parse. Both versions use
the same disambiguation model which results in
a slightly biased comparison but still reflects the
effect of increased ambiguity on the disambigua-
tion component. In Table 3, we can see that the
All-Part-Adv version performs significantly worse
than the grammar version which does not cap-
ture adverbial participles. The spurious ambigu-
ities and timeouts produced in All-Part-Adv have
such a strong negative impact on the disambigua-
tion component that it can not be outweighed by
the extended coverage of the grammar.
Qualitative Evaluation The fact that the All-
Part-Adv version generally increases parse ambi-
guity suggests that it produces a lot of undesired
analyses for constructions not related to adverbial
participles. To assess this assumption, we drew a
random sample of 20 sentences out of the addi-
tionally covered 41 sentences and checked manu-
ally whether these contained an adverbial partici-
ple: Only 40% of these sentences are actually cor-
rectly analysed. In all other cases, the grammar
lacks an analysis for a completely different phe-
Grammar Prec. Rec. F-Sc. Time
in sec
All-Part-Adv 83.80 76.71 80.1 666.55
No-Part-Adv 84.25 78.3 81.17 632.21
Table 3: Evaluation on the TIGER heldout set, 371
sentences total
nomenon (mostly related to coordination), but ob-
tains an (incorrect) analysis on the basis of the ad-
verb conversion rule.
As an example, Figure 1 presents two c-
structure analyses for the sentence in (4) in the
All-Part-Adv grammar. In the second c-structure
(CS2), the participle kritisiert (criticised) is anal-
ysed as adverb modifing the main verb haben
(have). This results in a very strange underlying f-
structure, meaning something like the Greens pos-
sess the SPD in a criticising manner.
(4) Die
The
Gru?nen
Greens
haben
have
die
the
SPD
SPD
kritisiert.
criticised.
?The Greens have criticised the SPD?
3.3 Interim Conclusion
This section has illustrated an exemplary dilemma
for parsing systems that aim broad-coverage and
linguisitically motivated analyses at the same time.
Since these systems need to explicitly address and
represent ambiguities that purely statistical sys-
tems are able to conflate or ignore, their perfor-
mance is not automatically improved by adding
a specific rule for a specific phenomenon. Inter-
estingly, the negative consequences affecting the
quantitative (statistical) as well as the qualitative
(linguistic) dimension of the grammar seem to be
closely related: The overgenerating adverb con-
version rule empirically leads to linguistically un-
motivated analyses which causes problems for the
disambiguation component. In the rest of the pa-
per, we show how the adverbial analysis of partici-
ples can be reasonably constrained on the basis of
a lexical resource induced from a parallel corpus.
4 Cross-Lingual Induction of Adverbial
Participles
The intuition of the cross-lingual induction ap-
proach is that adverbial participles can be easily
extracted from parallel corpora since in other lan-
guages (such as English or French) adverbs are
often morphologically marked and easily labelled
by statistical PoS taggers. As an example, con-
sider the sentence in (5), extracted from Europarl,
where the German participle versta?rkt is translated
by unambiguous adverbs in English and French
(increasingly and davantage).
(5) a. Nach der Osterweiterung stehen die Zeichen
versta?rkt auf Liberalisierung.
b. Following enlargement towards the east, the emphasis
is increasingly on liberalisation.
37
CS 1: ROOT:2543
CProot[std]:2536
DP[std]:984
DPx[std]:981
D[std]:616
die:34
NP:773
N[comm]:717
NAdj:714
Gr?nen:85
Cbar:2506
Vaux[haben,fin]:1054
haben:159
VP[v,part]:2080
DP[std]:1856
DPx[std]:2321
D[std]:1180
die:204
NP:1720
N[comm]:284
SPD:257
VC[v,part]:2009
V[v,part]:1593
Vx[v,part]:1590
kritisiert:348
PERIOD:418
.:410
CS 2: ROOT:2543
CProot[std]:2536
DP[std]:984
DPx[std]:981
D[std]:616
die:34
NP:773
N[comm]:717
NAdj:714
Gr?nen:85
Cbar:2506
V[v,fin]:2494
Vx[v,fin]:2491
haben:159
DP[std]:1856
DPx[std]:2321
D[std]:1180
die:204
NP:1720
N[comm]:284
SPD:257
ADVP[std]:1493
V[v,-infl]:1491
Vx[v,-infl]:1488
kritisiert:348
PERIOD:418
.:410
Figure 1: Two c-structures for sentence (4), obtained by the grammar All-Part-Adv - CS1 is correct, CS2
is semantically very strange
c. Apre`s l? e?largissement a` l? Est, la tendance sera da-
vantage a` la libe?ralisation.
In the following, we describe experiments on
Europarl where we automatically extract and fil-
ter adverbially translated German participles.
4.1 Data
We base our experiments on the German, En-
glish, French and Dutch part of the Europarl cor-
pus. We automatically word-aligned the German
part to each of the others with the GIZA++ tool
(Och and Ney, 2003). Note that, due to diver-
gences in sentence alignment and tokenisation,
the three word-alignments are not completely syn-
chronised. Moreover, each of the 4 languages has
been automatically PoS tagged using the TreeTag-
ger (Schmid, 1994). In addition, the German and
English parts have been parsed with MaltParser
(Nivre et al, 2006).
Since we want to limit our investigation to those
participles that are not already recorded as lexi-
calised adjective or adverb in the DMOR morphol-
ogy, we first have to generate the set of participle
candidates from the tagged Europarl data. We ex-
tract all distinct words (types) from the German
part that have been either tagged as ADJD (pred-
icative or adverbial modifier), 6089 types in total,
or as VVPP (past perfect participle), 5469 types
in total. We intersect this set of potential partici-
ples with the set of DMOR participles that only
have a verbal lemma. The resulting intersection
(5054 types in total) constitutes the set of all Ger-
man participles in Europarl that are not recorded
as lexicalised in the DMOR morphology .
Given the participle candidates, we now ex-
tract the set of sentences that exhibit a word
alignment between a German participle and an
English, French or Dutch adverb. The extrac-
tion yields 5191 German-English sentence pairs,
2570 German-French, and 4129 German-Dutch
sentence pairs. The German-English pairs com-
prise 1070 types of potentially adverbial partici-
ples. The types found in the German-French and
German-Dutch part form a proper subset of the
types extracted from the German-English pairs.
Thus, the additional languages will not increase
the recall of the induction. However, we will show
that they are extremely useful for filtering incor-
rect or uninteresting participle alignments.
For data exploration and evaluation, we anno-
tated 300 participle alignments out of the 5191
German-English sentences as to whether the En-
glish adverbial really points to an adverbial par-
ticiple on the German side (and/or the word-
alignment was correct). Throughout the entire set
of annotated sentences, this ratio between the par-
allel cases (where an English adverbial correctly
indicates a German adverbial) and all adverbially
translated participles is at about 30%. This means
that if we base the induction on word-alignments
alone, its precision would be relatively low.
The remaining 60% translation pairs do not only
reflect word alignment errors, but also cases where
we find a proper participle in the German sentence
that has a correct adverbial translation for other
reasons. A typical configuration is exemplified in
(6) where the German main verb vorlegen is trans-
lated as the verb-adverb combination put forward.
(6) a. Wir haben eine Reihe von Vorschla?gen vorgelegt.
b. We have put forward a number of proposals.
These sentence pairs are cases of free or para-
38
Figure 2: Type/token ratio for adverbial participles
phrasing translations. Ideally, we want our induc-
tion method to filter such type of configurations.
The 300 annotated sentences comprise 121 to-
ken instances of German adverbially used partici-
ples that have an adverbial translation in English.
However, these 121 tokens reduce to 24 partici-
ple types. The graph in Figure 2 displays the
type/token-ratio for an increasing number of in-
stances in our gold standard. The curve exponen-
tially decays from about 10 tokens onward and
suggests that from about 30 tokens onward, the
number of unseen types is relatively low. This can
be interpreted as evidence in favour of the hypoth-
esis that the number of adverbially used participles
is actually fairly limited and can be integrated into
the grammar in terms of a hard-coded resource.
4.2 Filtering
The data analysis in the previous section has
shown that approximately one third of the English
adverb alignments actually point to an adverbial
participle on the German side. This means that we
have to rigorously filter the data that we extract on
the basis of word-alignments in order to obtain a
high quality resource for our grammar. In this sec-
tion, we will investigate several filtering methods
and evaluate them on our annotated sentence pairs.
Frequency-based filtering As a first attempt,
we filtered the non-parallel cases in our set of
participle-adverb translations by means of the rel-
ative frequency of the adverb translations. For
each participle candidate, we counted the number
of tokens that exhibit an adverbial alignment on
the English side, and divided this number by its
total number of occurrences in the German Eu-
roparl. The best f-score of the ADV-FREQ filter
(see Table 4) is achieved by the 0.05 threshold, but
generally, the precision of the frequency filters is
too low for high-quality resource induction. The
reason for the poor performance of the frequency-
based filters seems to be that some German verbs
are systematically translated as verb - adverb com-
binations as in (6). For these participles, the rel-
ative frequency of adverbial alignments is not a
good indicator for their adverbial use in German.
Multilingual Filtering Similar to filters used
in annotation projection where noisy word-
alignments are ?cleaned? with the help of addi-
tional languages (Bouma et al, 2008), we have
implemented a filter that only selects those par-
ticiples as adverbials which also exhibit a certain
amount of adverbial translations in the French and
Dutch Europarl. We count the total number of
adverbial translations of a given participle on the
French side and divide it by the number of English
adverbial translations. For French, the best f-score
is achieved at a threshold of >0.1 (filter FR). For
Dutch, the best f-score is achieved at a threshold
of >0.05 (filter NL). The exact precision and re-
call values are given in Table 4.
Syntax-based Filtering The intuition behind
the filters presented in this section is that adver-
bial translations which are due to cross-lingual di-
vergences can be identified on the basis of their
syntactic contexts. Information about these con-
texts can be extracted from the dependency anal-
yses produced by MaltParser for the German and
English data. On the German side, we want to ex-
clude those participle instances for which the Ger-
man parser has found an auxiliary head, since this
configuration points to a normal partciple context
in German. The filter is called G-HEAD in Table
4. It filters all types which have an auxiliary head
in more than 40% of their adverbial translation
configurations. On the English side, we exclude
all translations where the adverb has a verbal head
which is also aligned to the German partciple. The
filter is called E-HEAD in Table 4. It excludes all
participle types which exhibit the E-HEAD con-
figuration in more than 50% of the cases.
39
filter prec. rec. f-sc.
ADV-FREQ 0.38 0.75 0.51
FR 0.48 0.76 0.58
NL 0.33 0.73 0.45
G-HEAD 0.65 0.8 0.71
E-HEAD 0.4 0.8 0.53
COMBINED-1 0.61 0.8 0.69
COMBINED-2 0.86 0.76 0.81
Table 4: Performance of filters on the set of gold
adverbial participle types
Combined Token-level Filtering So far, we
have shown that multilingual and syntactic in-
formation is useful to filter non-parallel partici-
ple translations. We have found that the pre-
cision of the syntactic filters can still be in-
creased by combining it with the multilingual fil-
ters. COMBINED-1 in Table 4 refers to the filter
which only includes those participle types which
have at least one adverbial translation on the En-
glish target side such that (i) the adverbial trans-
lation is paralleled on the French or Dutch target
side for the same German participle token and (ii)
the German participle token does not have an aux-
iliary head. If we combine this token-level filter-
ing with the syntactic type-level filtering G-HEAD
and E-HEAD (the filter called COMBINED-2 in
Table 4), the precision increases by about 25%
with little loss in recall.
4.3 Analysis
Based on the filtering techniques described in the
previous section, we can finally induce a list of 46
German adverbial participles from Europarl. The
fact that this participle class seems fairly delimited
in our data raises the theoretical question whether
the adverb conversion is licensed by any linguistic,
i.e. lexical-semantic, properties of these partici-
ples. However, we observe that the automatically
induced list comprises very diverse types of ad-
verbs, as well as very distinct types of underlying
verbs. Thus, besides adverbs that clearly modify
events (see sentence (5)), we also found adverbs
that are more likely to modify adjectives (sentence
(7-a)), or propositions (sentence (7-b)).
(7) a. Es ist eine verdammt gefa?hrliche Situation.
?It is a damned dangerous situation.?
b. Wir machen einen Bericht u?ber den Bericht des Rech-
nungshofes , zugegeben.
?We are drafting a report about the report of the Court
of Auditors , admittedly.?
A more fine-grained classification and analysis
of adverbial participles is left for future research.
5 Grammar-based Evaluation
The resource of participles licensing adverbial use,
whose induction was described in the previous
section, can be straightforwardly integrated into
the German LFG. By explicitly enumerating the
participles in the adverb lexicon, the grammar can
apply the standard adverb macros to them. To as-
sess the effect of the filtering, we built two new
versions of the grammar: (i) Euro-Part-Adv, its ad-
verb lexicon comprises all adverbially translated
participles found in Europarl (1091 types) and (ii)
Filt-Part-Adv, its adverb lexicon comprises only
the syntactically and multilingually filtered par-
ticiples found in Europarl (46 types).
Although we have seen in section 3.2 that adver-
bial participles do not seem to occur in the TIGER
heldout set, we also know that it is important to
assess the effect of ambiguity rate on the overall
grammar performance. Therefore, we computed
the accuracy of the most probable parses produced
by the Euro-Part-Adv and Filt-Part-Adv on the
heldout set. As is shown in Table 5, the Euro-Part-
Adv performs significantly worse than Filt-Part-
Adv. This suggests that the non-filtered participle
resource is not constrained enough and still pro-
duces a lot of spurious ambiguites that mislead the
disambiguation component. The coverage values
in Table 6 further corroborate the observation that
the unfiltered participle resource behaves similar
to the unrestricted adverb conversion in All-Part-
Adv (see Section 3.2). The coverage of the filtered
vs. the unfiltered version on the development set is
identical, however the timeouts in Euro-Part-Adv
increase by 17% and parsing time by 8%.
By contrast, there is no significant difference
in f-score between the No-Part-Adv version pre-
sented in Section 3.2 and the Filt-Part-Adv ver-
sion. Thus, we can, at least, assume that the fil-
tered participles resources has restricted the mas-
sive overgeneration caused by the general adverb
conversion rule such that the overall performance
of the original grammar is not negatively affected.
To evaluate the participle resource as to whether
it could have a positive qualtitative effect on pars-
ing TIGER at all, we built a specialised test-
suite which comprises only sentences containing
a non-lexicalised participle, which has an adver-
bial translation in Europarl and is tagged as ADJD
40
Grammar Prec. Rec. F-Sc. Time
in sec
Euro-Part-Adv 82.32 75.78 78.91 701
Filt-Part-Adv 84.12 78.2 81.05 665
Table 5: Evaluation on the TIGER heldout set, 371
sentences total
Grammar Parsed
Sent.
Starred
Sent.
Time-
outs
Time
in sec
Euro-Part-Adv 4304 588 107 7359
Filt-Part-Adv 4304 604 91 6791
Table 6: Performance on the TIGER development
set (sentences 1-5000), 4999 sentences total
in TIGER. The sentences were extracted from the
whole TIGER corpus yielding a set of 139 sen-
tences. In this quality-oriented evaluation, we
only contrast the No-Part-Adv version with the
filtered Filt-Part-Adv version since the unfiltered
version leads to worse overall performance. As
can be seen in Table 7, the No-Part-Adv can only
completely cover 36% of the specialised testsuite
which is much lower than its average complete
coverage on the development set (86%). This sug-
gests that a substantial number of the extracted
ADJD participles are actually used as adverbial in
the specialised testsuite.
Similar to the qualitative evaluation procedure
in 3.2, we manually evaluated a random sample of
20 sentences covered by Filt-Part-Adv and not by
No-Part-Adv as to whether they contain an adver-
bial participle that has been correctly recognised.
This was the case for 90% of the sentences, the
remaining 2 sentences were cases of secondary
predications. An example of a relatively simple
TIGER sentence that the grammar could not cover
in the No-Part-Adv version is given in (8).
(8) Die Anti-Baby-Pillen stehen im Verdacht , vermehrt
Thrombosen auszulo?sen.
?The birth control pill is suspected to increasingly cause
thromboses.?
We also manually checked a random sample of
Grammar Parsed
Sent.
Starred
Sent.
Time-
outs
Time
in sec
No-Part-Adv 50 77 12 427
Filt-Part-Adv 92 39 8 366
Table 7: Performance on the specialised TIGER
test set, 139 sentences total
20 sentences that the Filt-Part-Adv grammar could
not cover, in order to see whether the grammar sys-
tematically misses certain cases of adverbial par-
ticiples. In this second random sample, the per-
centage of sentences containing a true adverbial
participle was again 90%. The grammar could
not correctly analyse these because of their spe-
cial syntax that is not covered by the general ad-
verb macro (or, of course, because of difficult con-
structions not related to adverbial participles). An
example for such a case is given in (9).
(9) Transitreisen junger Ma?nner vom Gaza-Streifen ins
Westjordanland und umgekehrt sind nicht gestattet.
?Transit travels from the Gaza Strip to the West Bank and
vice versa are not allowed for young men.?
The high proportion of true adverbial participle
instances in our specific testsuite suggests that the
data we induced from Europarl largely carries over
to TIGER (despite genre differences, for instance)
and constitutes a generally useful resource. Thus,
we can not only say that the filtered participle re-
source has no negative effect on the overall per-
formance of the German LFG, but also extends its
coverage for a less frequent phenomenon in a lin-
guistically precise way.
6 Conclusion
We have proposed an empirical account for detect-
ing adverbial participles in German. Since this
category is usually not annotated in German re-
sources and hard to describe in theory, we based
our method on multilingual parallel data. This
data suggests that only a fairly limited class of par-
ticiples actually undergo the conversion to adverbs
in free text. We have described a set of linguisti-
cally motivated filters which are necessary to in-
duce a high-precision resource for adverbial par-
ticiples from parallel data. This resource has been
integrated into the German LFG grammar. In con-
trast to the version of the grammar which does not
restrict the participle - adverb conversion, the re-
stricted version produces less spurious ambigui-
ties which leads to better f-score on gold standard
data. Moreover, by manually evaluating a spe-
cialised data set, we have established that the re-
stricted version also extends the coverage and pro-
duces the correct analyses which can be used for
further linguistic study.
41
References
Tanja Becker. 2001. DMOR: Handbuch. Technical
report, IMS, University of Stuttgart.
Gerlof Bouma, Jonas Kuhn, Bettina Schrader, and
Kathrin Spreyer. 2008. Parallel LFG Grammars
on Parallel Corpora: A Base for Practical Trian-
gulation. In Miriam Butt and Tracy Holloway
King, editors, Proceedings of the LFG08 Confer-
ence, pages 169?189, Sydney, Australia. CSLI Pub-
lications, Stanford.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The tiger
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Joan Bresnan. 2000. Lexical-Functional Syntax.
Blackwell, Oxford.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
Parallel Grammar Project.
Aoife Cahill, John T. Maxwell III, Paul Meurer, Chris-
tian Rohrer, and Victoria Rose?n. 2007. Speeding
up LFG Parsing using C-Structure Pruning . In Col-
ing 2008: Proceedings of the workshop on Grammar
Engineering Across Frameworks, pages 33 ? 40.
Stefanie Dipper. 2003. Implementing and Document-
ing Large-Scale Grammars ? German LFG. Ph.D.
thesis, Universita?t Stuttgart, IMS.
Anette Frank, Tracy Holloway King, Jonas Kuhn, and
John T. Maxwell. 2001. Optimality Theory Style
Constraint Ranking in Large-Scale LFG Grammars
. In Peter Sells, editor, Formal and Empirical Issues
in Optimality Theoretic Syntax, page 367?397. CSLI
Publications.
Wilhelm Geuder. 2004. Depictives and transparent ad-
verbs. In J. R. Austin, S. Engelbrecht, and G. Rauh,
editors, Adverbials. The Interplay of Meaning, Con-
text, and Syntactic Structure, pages 131?166. Ben-
jamins.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit 2005.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data driven parser-generator for de-
pendency parsing. In Proc. of LREC-2006.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Stefan Riezler, Tracy Holloway King, Ronald M. Ka-
plan, Richard Crouch, John T. Maxwell, and Mark
Johnson. 2002. Parsing the Wall Street Journal us-
ing a Lexical-Functional Grammar and Discrimina-
tive Estimation Techniques . In Proceedings of ACL
2002.
Christian Rohrer and Martin Forst. 2006. Improving
coverage and parsing quality of a large-scale LFG
for German. In Proceedings of LREC-2006.
Anne Schiller, Simone Teufel, and Christine Thielen.
1995. Guidelines fuer das Tagging deutscher Tex-
tkorpora mit STTS. Technical report, IMS, Univer-
sity of Stuttgart.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora. In
Proceedings of HLT 2001, First International Con-
ference on Human Language Technology Research.
42
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 55?64,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Towards a Tool for Interactive Concept Building for Large Scale Analysis
in the Humanities
Andre Blessing1 Jonathan Sonntag2 Fritz Kliche3
Ulrich Heid3 Jonas Kuhn1 Manfred Stede2
1Institute for Natural Language Processing
Universitaet Stuttgart, Germany
2Institute for Applied Computational Linguistics
University of Potsdam, Germany
3Institute for Information Science and Natural Language Processing
University of Hildesheim, Germany
Abstract
We develop a pipeline consisting of var-
ious text processing tools which is de-
signed to assist political scientists in find-
ing specific, complex concepts within
large amounts of text. Our main focus is
the interaction between the political scien-
tists and the natural language processing
groups to ensure a beneficial assistance for
the political scientists and new application
challenges for NLP. It is of particular im-
portance to find a ?common language? be-
tween the different disciplines. Therefore,
we use an interactive web-interface which
is easily usable by non-experts. It inter-
faces an active learning algorithm which
is complemented by the NLP pipeline to
provide a rich feature selection. Political
scientists are thus enabled to use their own
intuitions to find custom concepts.
1 Introduction
In this paper, we give examples of how NLP meth-
ods and tools can be used to provide support for
complex tasks in political sciences. Many con-
cepts of political science are complex and faceted;
they tend to come in different linguistic realiza-
tions, often in complex ones; many concepts are
not directly identifiable by means of (a small set
of) individual lexical items, but require some in-
terpretation.
Many researchers in political sciences either
work qualitatively on small amounts of data which
they interpret instance-wise, or, if they are in-
terested in quantitative trends, they use compara-
tively simple tools, such as keyword-based search
in corpora or text classification on the basis of
terms only; this latter approach may lead to im-
precise results due to a rather unspecific search as
well as semantically invalid or ambigious search
words. On the other hand, large amounts of e.g.
news texts are available, also over longer periods
of time, such that e.g. tendencies over time can
be derived. The corpora we are currently working
on contain ca. 700,000 articles from British, Irish,
German and Austrian newspapers, as well as (yet
unexplored) material in French.
Figure 1 depicts a simple example of a quantita-
tive analysis.1 The example shows how often two
terms, Friedensmission(?peace operation?), and
Auslandseinsatz(?foreign intervention?) are used
in the last two decades in newspaper texts about
interventions and wars. The long-term goal of the
project is to provide similar analysis for complex
concepts. An example of a complex concept is
the evocation of collective identities in political
contexts, as indirect in the news. Examples for
such collective identities are: the Europeans, the
French, the Catholics.
The objective of the work we are going to dis-
cuss in this paper is to provide NLP methods and
tools for assisting political scientists in the ex-
ploration of large data sets, with a view to both,
a detailed qualitative analysis of text instances,
and a quantitative overview of trends over time,
at the level of corpora. The examples discussed
here have to do with (possibly multiple) collective
identities. Typical context of such identities tend
to report communication, as direct or as indirect
speech. Examples of such contexts are given in 1.
(1) Die
The
Europa?er
Europeans
wu?rden
would
die
the
Lu?cke
gap
fu?llen,
fill,
1The figure shows a screenshot of our web-based
prototype.
55
Figure 1: The screenshot of our web-based system shows a simple quantitative analysis of the frequency
of two terms in news articles over time. While in the 90s the term Friedensmission (peace operation) was
predominant a reverse tendency can be observed since 2001 with Auslandseinsatz (foreign intervention)
being now frequently used.
sagte
said
Ru?he.
Ru?he.
,,The Europeans would fill the gap, Ru?he said.?
The tool support is meant to be semi-automatic,
as the automatic tools propose candidates that
need to be validated or refused by the political sci-
entists.
We combine a chain of corpus processing tools
with classifier-based tools, e.g. for topic clas-
sifiers, commentary/report classifiers, etc., make
the tools interoperable to ensure flexible data ex-
change and multiple usage scenarios, and we em-
bed the tool collection under a web (service) -
based user interface.
The remainder of this paper is structured as fol-
lows. In section 2, we present an outline of the ar-
chitecture of our tool collection, and we motivate
the architecture. Section 3 presents examples of
implemented modules, both from corpus process-
ing and search and retrieval of instances of com-
plex concepts. We also show how our tools are re-
lated to the infrastructural standards in use in the
CLARIN community. In section 4, we exemplify
the intended use of the methods with case studies
about steps necessary for identifying evocation:
being able to separate reports from comments, and
strategies for identifying indirect speech. Section
6 is devoted to a conclusion and to the discussion
of future work.
2 Project Goals
A collaboration between political scientists and
computational linguists necessarily involves find-
ing a common language in order to agree on
the precise objectives of a project. For exam-
ple, social scientists use the term codebook for
manual annotations of text, similar to annotation
schemes or guidelines in NLP. Both disciplines
share methodologies of interactive text analysis
which combine term based search, manual an-
notation and learning-based annotation of large
amounts of data. In this section, we give a brief
56
summary of the goals from the perspective of each
of the two disciplines, and then describe the text
corpus that is used in the project. Section 3 will
describe our approach to devising a system archi-
tecture that serves to realize the goals.
2.1 Social Science Research Issue
Given the complexity of the underlying research
issues (cf. Section 1) and the methodological tra-
dition of manual text coding by very well-trained
annotators in the social science and particular in
political science, our project does not aim at any
fully-automatic solution for empirical issues in po-
litical science. Instead, the goal is to provide as
much assistance to the human text analyst as possi-
ble, by means of a workbench that integrates many
tasks that otherwise would have to be carried out
with different software tools (e.g., corpus prepro-
cessing, KWIC searches, statistics). In our project,
the human analyst is concerned specifically with
manifestations of collective identities in newspa-
per texts on issues of war and military interven-
tions: who are the actors in political crisis man-
agement or conflict? How is this perspective of
responsible actors characterized in different news-
papers (with different political orientation; in dif-
ferent countries)? The analyst wants to find doc-
uments that contain facets of such constellations,
which requires search techniques involving con-
cepts on different levels of abstraction, ranging
from specific words or named entities (which may
appear with different names in different texts) to
event types (which may be realized with different
verb-argument configurations). Thus the text cor-
pus should be enriched with information relevant
to such queries, and the workbench shall provide
a comfortable interface for building such queries.
Moreover, various types and (possibly concurrent)
layers of human annotations have to complement
the automatic analysis, and the manual annota-
tion would benefit from automatic control of code-
book2 compliance and the convergence of coding
decisions.
2.2 Natural Language Processing Research
Issue
Large collections of text provide an excellent op-
portunity for computational linguists to scale their
methods. In the scenario of a project like ours, this
becomes especially challenging, because standard
2or, in NLP terms: annotation scheme.
automatic analysis components have to be com-
bined with manual annotation or interactive inter-
vention of the human analyst.
In addition to this principled challenge, there
may be more mundane issues resulting from pro-
cessing corpora whose origin stretches over many
years. In our case, the data collection phase coin-
cided with a spelling reform in German-speaking
countries. Many aspects of spelling changed twice
(in 1996 and in 2006), and thus it is the responsi-
bility of the NLP branch of the project to provide
an abstraction over such changes and to enable to-
day?s users to run a homogeneous search over the
texts using only the current spelling. While this
might be less important for generic web search ap-
plications, it is of great importance for our project,
where the overall objective is a combination of
quantitative and qualitative text analysis.
In our processing chain, we first need to harmo-
nize the data formats so that the processing tools
operate on a common format. Rather than defin-
ing these from scratch, we aim at compatibility
with the standardization efforts of CLARIN3 and
DARIAH4, two large language technology infras-
tructure projects in Europe that in particular target
eHumanities applications. One of the objectives
is to provide advanced tools to discover, explore,
exploit, annotate, analyse or combine textual re-
sources. In the next section we give more details
about how we interact which the CLARIN-D in-
frastructure (Boehlke et al, 2013).
3 Architecture
The main goal is to provide a web-based user-
interface to the social scientist to avoid any soft-
ware installation. Figure 2 presents the workflow
of the different processing steps in this project.
The first part considers format issues that occur
if documents from different sources are used. The
main challenge is to recognize metadata correctly.
Date and source name are two types of metadata
which are required for analyses in the social sci-
ences. But also the separation of document con-
tent (text) and metadata is important to ensure that
only real content is processed with the NLP meth-
ods. The results are stored in a repository which
uses a relational database as a back-end. All fur-
ther modules are used to add more annotations to
the textual data. First a complex linguistic pro-
3http://www.clarin.eu/
4http://www.dariah.eu/
57
cessing chain is used to provide state-of-the-art
corpus linguistic annotations (see Section 3.2 for
details). Then, to ensure that statistics over oc-
currence counts of words, word combinations and
constructions are valid and not blurred by the mul-
tiple presence of texts or text passages in the cor-
pus, we filter duplicates. Duplicates can occur
if our document set contains the same document
twice or if two documents are very similar, e.g.
they differ in only one sentence.
Raw documents
Repository:MetadataStructural dataTextual data Topic filter
Duplicate filter
Linguistic analysisSentence splitter Tokenizer
Web-basedUserinterface
Tagger ParserCoref NER
ImportExploration Workbench
Concept detection
Complex Concept Builder
Figure 2: Overview of the complete processing
chain.
We split the workflow for the user into two
parts: The first part is only used if the user im-
ports new data into the repository. For that he
can use the exploration workbench (Section 3.1).
Secondly, all steps for analyzing the data are done
with the Complex Concept Builder (Section 3.2).
3.1 Exploration Workbench
Formal corpus inhomogeneity (e.g. various data
formats and inconsistent data structures) are a ma-
jor issue for researchers working on text corpora.
The web-based ?Exploration Workbench? allows
for the creation of a consistent corpus from vari-
ous types of data and prepares data for further pro-
cessing with computational linguistic tools. The
workbench can interact with to existing computa-
tional linguistic infrastructure (e.g. CLARIN) and
provides input for the repository also used by the
Complex Concept Builder.
The workbench converts several input formats
(TXT, RTF, HTML) to a consistent XML repre-
sentation. The conversion tools account for differ-
ent file encodings and convert input files to Uni-
code (UTF-8). We currently work on newspa-
per articles wrapped with metadata. Text mining
components read out those metadata and identify
text content in the documents. Metadata appear
at varying positions and in diverse notations, e.g.
for dates, indications of authors or newspaper sec-
tions. The components account for these varia-
tions and convert them to a consistent machine
readable format. The extracted metadata are ap-
pended to the XML representation. The result-
ing XML is the starting point for further compu-
tational linguistic processing of the source docu-
ments.
The workbench contains a tool to identify text
duplicates and semi-duplicates via similarity mea-
sures of pairs of articles (Kantner et al, 2011).
The method is based on a comparison of 5-grams,
weighted by significance (tf-idf measure (Salton
and Buckley, 1988)). For a pair of documents it
yields a value on a ?similarity scale? ranging from
0 to 1. Values at medium range (0.4 to 0.8) are
considered semi-duplicates.
Data cleaning is important for the data-driven
studies. Not only duplicate articles have a nega-
tive impact, also articles which are not of interest
for the given topic have to be filtered out. There
are different approaches to classify articles into a
range of predefined topics. In the last years LDA
(Blei et al, 2003; Niekler and Ja?hnichen, 2012)
is one of the most successful methods to find top-
ics in articles. But for social scientists the cate-
gories typically used in LDA are not sufficient. We
follow the idea of Dualist (Settles, 2011; Settles
and Zhu, 2012) which is an interactive method for
classification. The architecture of Dualist is based
on MALLET (McCallum, 2002) which is easily
integrable into our architecture. Our goal is to
design the correct feature to find relevant articles
for a given topic. Word features are not sufficient
since we have to model more complex features (cf.
Section 2.1).
The workbench is not exclusively geared to the
data of the current project. We chose a modular
set-up of the tools of the workbench and provide
user-modifiable templates for the extraction of var-
ious kinds of metadata, in order to keep the work-
bench adaptable to new data and to develop tools
suitable for data beyond the scope of the current
corpus.
58
3.2 Complex Concept Builder
A central problem for political scientists who in-
tend to work on large corpora is the linguistic va-
riety in the expression of technical terms and com-
plex concepts. An editorial or a politician cited
in a news item can mobilize a collective identity
which can be construed from e.g. regional or so-
cial affiliation, nationality or religion. A reason-
able goal in the context of the search for collec-
tive identity evocation contexts is therefore to find
all texts which (possibly) contain collective iden-
tities. Moreover, while we are training our inter-
active tools on a corpus on wars and military in-
terventions the same collective identities might be
expressed in different ways in a corpus i.e. on the
Eurocrisis.
From a computational point of view, many dif-
ferent tools need to be joined to detect interest-
ing texts. An example application could be a case
where a political scientist intends to extract news-
paper articles that cite a politician who tries to
rally support for his political party. In order to
detect such text, we need a system to identify di-
rect and indirect speech and a sentiment system to
determine the orientation of the statement. These
systems in turn need various kinds of preprocess-
ing starting from tokenization over syntactic pars-
ing up to coreference resolution. The Complex
Concept Builder is the collection of all these sys-
tems with the goal to assist the political scientists.
So far, the Complex Concept Builder imple-
ments tokenization (Schmid, 2009), lemmatisation
(Schmid, 1995), part-of-speech tagging (Schmid
and Laws, 2008), named entity detection (Faruqui
and Pado?, 2010), syntactical parsing (Bohnet,
2010), coreference analysis for German (Lappin
and Leass, 1994; Stuckardt, 2001), relation extrac-
tion (Blessing et al, 2012) and sentiment analysis
for English (Taboada et al, 2011).
It is important for a researcher of the humanities
to be able to adapt existing classification systems
according to his own needs. A common procedure
in both, NLP and political sciences, is to annotate
data. Therefore, one major goal of the project and
the Complex Concept Builder is to provide ma-
chine learning systems with a wide range of pos-
sible features ? including high level information
like sentiment, text type, relations to other texts,
etc. ? that can be used by non-experts for semi-
automatic annotation and text selection. Active
learning is used to provide immediate results that
can then be improved continuously. This aspect
of the Complex Concept Builder is especially im-
portant because new or adapted concepts that may
be looked for can be found without further help of
natural language processing experts.
3.3 Implementation
We decided to use a web-based platform for our
system since the social scientist needs no software
installation and we are independent of the used
operating system. Only a state-of-the-art web-
browser is needed. On the server side, we use a
tomcat installation that interacts with our UIMA
pipeline (Ferrucci and Lally, 2004). A HTML-
rendering component designed in the project (and
parametrizable) allows for a flexible presentation
of the data. A major issue of our work is interac-
tion. To solve this, we use JQuery and AJAX to
dynamically interact between client- and server-
side.
4 Case Study
In this section we explore the interaction between
various sub-systems and how they collaborate to
find complex political concepts. The following
Section 4.1 describes the detection of direct and
indirect speech and its evaluation follows in Sec-
tion 4.2. Section 4.3 is a general exploration of a
few selected sub-systems which require, or benefit
from direct and indirect speech. Finally, Section
4.4 discusses a specific usage scenario for indirect
speech.
4.1 Identifying Indirect Speech
The Complex Concept Builder provides analy-
ses on different linguistic levels (currently mor-
phosyntax, dependency syntax, named entities) of
annotation. We exploit this knowledge to identify
indirect speech along with a mentioned speaker.
Our indirect speech recognizer is based on three
conditions: i) Consider all sentences that contain
at least one word which is tagged as subjunctive
(i.e. ?*.SUBJ?) by the RFTagger. ii) This verb
has to be a direct successor of another verb in the
dependency tree. iii) This verb needs to have a
subject.
Figure 3 depicts the dependency parse tree of
sentence 2.
(2) Der Einsatz werde wegen der Risiken fu?r die
unbewaffneten Beobachter ausgesetzt, teilte
59
Einsatzmission
theDer
,,
ausgesetztstopped
werde
wegenbecause of
Risikorisks
teilteinformed
will be Missionschefhead of mission
MoodMood
RobertRobert
mitam
SaturdaySamstag
on
..
SBOC
VFIN.Aux.3.Sg.Pres.Subj
VFIN.Full.3.Sg.Past.IndRFTags
Figure 3: Dependency parse of a sentence that
contains indirect speech (see Sentence 2).
Missionschef Robert Mood am Samstag mit.
The mission will be stopped because of the risks to the
unarmed observers, informed Head of Mission Robert
Mood on Saturday.
The speaker of the indirect speech in Sentence
2 is correctly identified as Missionschef (Head of
Mission) and the corresponding verb is teilte mit
(from mitteilen) (to inform).
The parsing-based analysis helps to identify the
speaker of the citation which is a necessary in-
formation for the later interpretation of the cita-
tion. As a further advantage, such an approach
helps to minimize the need of lexical knowledge
for the identification of indirect speech. Our er-
ror analysis below will show that in some cases
a lexicon can help to avoid false positives. A lexi-
con of verbs of communication can easily be boot-
strapped by using our approach to identify candi-
dates for the list of verbs which then restrict the
classifier in order to achieve a higher precision.
4.2 Indirect Speech Evaluation
For a first impression, we present a list of sen-
tences which were automatically annotated as pos-
itive instances by our indirect speech detector.
The sentences were rated by political scientists.
Additionally, for each sentence we extracted the
speaker and the used verb of speech. We man-
ually evaluated 200 extracted triples (sentence,
speaker, verb of speech): The precision of our
system is: 92.5%
Examples 2, 3 and 4 present good candidates
which are helpful for further investigations on col-
lective identities. In example 3 Cardinal Lehmann
is a representative speaker of the Catholic commu-
nity which is a collective identity. Our extracted
sentences accelerate the search for such candidates
which amounts to looking manually for needles in
a haystack.
example speaker verb of speech
(2) Robert Mood teilte (told)
(3) Kardinal Karl Lehmann sagte (said)
(4) Sergej Ordzhonikidse sagte (said)
(5) Bild (picture) tru?ben (tarnish)
(6) sein (be) sein (be)
Examples 5 and 6 show problems of our first
approach. In this case, the speaker is not a person
or an organisation, and the verb is not a verb of
speech.
(3) Ein Angriffskrieg jeder Art sei ? sit-
tlich verwerflich ?, sagte der Vorsitzende
der Bischoffskonferenz, Kardinal Karl
Lehmann.
Any kind of war of aggression is ?morally reprehen-
sible,? said the chairman of the Bishops? Conference,
Cardinal Karl Lehmann.
(4) Derartige Erkla?rungen eines Staatschefs
seien im Rahmen der internationalen
Beziehungen inakzeptabel, sagte der UN-
Generaldirektor Sergej Ordzhonikidse
gestern in Genf.
Such statements of heads of states are unacceptable in
the context of international relations, said UN General
Director Sergei Ordzhonikidse in Geneva yesterday.
(5) Wu?rden die Wahlen verschoben, tru?bte sich
das gescho?nte Bild.
Would the elections be postponed, the embellished im-
age would tarnish.
(6) Dies sei alles andere als einfach, ist aus Of-
fizierskreisen zu ho?ren.
This is anything but simple, is to hear from military
circles.
60
EinsatzEimosheDrhsatzs,uDigao
psgasatzEdidsowshgbcdsatzhsu hdo
pgddsgDsatzEcihsoEbchsgwsatzfhgdso
ciwsatzciRsofshksatzfgDDo
wsd asatzspuciEglsoshlrcDsatzdsDDo
MSMMy.SMMy BMSMMyB.SMMyOMSMMyO.SMMyCMSMMyC.SMMyVMSMMy
p EdtFEsktEussbctRshwE
Figure 4: 10 most used verbs (lemma) in indirect
speech.
4.3 Using Indirect Speech
Other modules benefit from the identification of
indirect speech, as can be seen from Sentence 7.
The sentiment system assigns a negative polarity
of ?2.15 to the sentence. The nested sentiment
sources, as described by (Wiebe et al, 2005), of
this sentence require a) a direct speech with the
speaker ?Mazower? and b) an indirect speech with
the speaker ?no one? to be found.5
(7) ?There were serious arguments about what
should happen to the Slavs and Poles in east-
ern Europe,? says Mazower, ?and how many
of them should be sent to the camps and what
proportion could be Germanised . . . No one
ever came out and directly said Hitler had got
it wrong, but there was plenty of implied crit-
icism through comparisons with the Roman
empire. [...]?6
A collective identity evoked in Sentence 7 is
?the Germans?? although the term is not explic-
itly mentioned. This collective identity is de-
scribed as non-homogeneous in the citation and
can be further explored manually by the political
scientists.
The following are further applications of the
identified indirect speeches a) using the frequency
of speeches per text as a feature for classifica-
tion; e.g. a classification system for news re-
ports/commentaries as described in Section 4.4 b)
a project-goal is to find texts in which collective
5The reported sentiment value for the whole sentence is
applicable only to the direct speech. The indirect speech (i.e.
?Hitler had got it wrong?) needs a more fine-grained polarity
score. Since our Complex Concept Builder is very flexible, it
is trivial to score each component separately.
6http://www.guardian.co.uk/education/2008/jul
/01/academicexperts.highereducationprofile
identities are mobilised by entities of political de-
bate (i.e. persons, organisations, etc.); the detec-
tion of indirect speech is mandatory for any such
analysis.
4.4 Commentary/Report Classification
A useful distinction for political scientists dealing
with newspaper articles is the distinction between
articles that report objectively on events or back-
grounds and editorials or press commentaries.
We first extracted opinionated and objective
texts from DeReKo corpus (Stede, 2004; Kupietz
et al, 2010). Some texts were removed in order to
balance the corpus. The balanced corpus contains
2848 documents and has been split into a develop-
ment and a training and test set. 570 documents
were used for the manual creation of features. The
remaining 2278 documents were used to train and
evaluate classifiers using 10-fold cross-validation
with the WEKA machine learning toolkit (Hall et
al., 2009) and various classifiers (cf. Table 1).
The challenge is that the newspaper articles
from the training and evaluation corpus come from
different newspapers and, of course, from differ-
ent authors. Commentaries in the yellow press
tend to have a very different style and vocabulary
than commentaries from broadsheet press. There-
fore, special attention needs to be paid to the in-
dependence of the classifier from different authors
and different newspapers. For this reason, we use
hand-crafted features tailored to this problem. In
return, this means omitting surface-form features
(i.e. words themselves).
The support vector machine used the SMO al-
gorithm (Platt and others, 1998) with a polynomial
kernel K(x, y) =< x, y > e with e = 2. All other
algorithms were used with default settings.
precision recall f-score
SVM 0.819 0.814 0.813
Naive Bayes 0.79 0.768 0.764
Multilayer Percep-
tron
0.796 0.795 0.794
Table 1: Results of a 10-fold cross-validation for
various machine learning algorithms.
A qualitative evaluation shows that direct and
indirect speech is a problem for the classifier.
Opinions voiced via indirect speech should not
lead to a classification as ?Commentary?, but
should be ignored. Additionally, the number of
61
uses of direct and indirect speech by the author can
provide insight into the intention of the author. A
common way to voice one?s own opinion, without
having to do so explicitly, is to use indirect speech
that the author agrees with. Therefore, the number
of direct and indirect speech uses will be added
to the classifier. First experiments indicate that the
inclusion of direct and indirect speech increase the
performance of the classifier.
5 Related Work
Many approaches exist to assist social scientists in
dealing with large scale data. We discuss some
well-known ones and highlight differences to the
approach described above.
The Europe Media Monitor (EMM) (Stein-
berger et al, 2009) analyses large amounts of
newspaper articles and assists anyone interested in
news. It allows its users to search for specific top-
ics and automatically clusters articles from differ-
ent sources. This is a key concept of the EMM,
because it collects about 100, 000 articles in ap-
proximately 50 languages per day and it is impos-
sible to scan through these by hand. EMM users
are EU institutions, national institutions of the EU
member states, international organisations and the
public (Steinberger et al, 2009).
The topic clusters provide insight into ?hot?
topics by simply counting the amount of articles
per cluster or by measuring the amount of news on
a specific topic with regards to its normal amount
of news. Articles are also data-mined for geo-
graphical information, e.g. to update in which
geographical region the article was written and
where the topic is located. Social network infor-
mation is gathered and visualised as well.
Major differences between the EMM and our
approach are the user group and the domain of
the corpus. The complex concepts political sci-
entists are interested in are much more nuanced
than the concepts relevant for topic detection and
the construction of social networks. Additionally,
the EMM does not allow its users to look for their
own concepts and issues, while this interactivity
is a central contribution of our approach (cf. Sec-
tions 1, 2.1 and 3.2).
The CLARIN-D project also provides a web-
based platform to create NLP-chains. It is called
WebLicht (Hinrichs et al, 2010), but in its cur-
rent form, the tool is not immediately usable for
social scientists as the separation of metadata and
textual data and the encoding of the data is hard
for non-experts. Furthermore, WebLicht does not
yet support the combination of manual and au-
tomatic annotation needed for text exploration in
the social science. Our approach is based on the
webservices used by WebLicht. But in contrast to
WebLicht, we provide two additional components
that simplify the integration (exploration work-
bench) and the interpretation (complex concept
builder) of the research data. The former is in-
tended, in the medium term, to be made available
in the CLARIN framework.
6 Conclusion and Outlook
We developed and implemented a pipeline of var-
ious text processing tools which is designed to as-
sist political scientists in finding specific, complex
concepts within large amounts of text. Our case
studies showed that our approach can provide ben-
eficial assistance for the research of political sci-
entists as well as researcher from other social sci-
ences and the humanities. A future aspect will be
to find metrics to evaluate our pipeline. In recently
started annotation experiments on topic classifica-
tion Cohen?s kappa coefficient (Carletta, 1996) is
mediocre. It may very well be possible that the
complex concepts, like multiple collective identi-
ties, are intrinsically hard to detect, and the anno-
tations cannot be improved substantially.
The extension of the NLP pipeline will be an-
other major working area in the future. Examples
are sentiment analysis for German, adding world
knowledge about named entities (e.g. persons and
events), identification of relations between enti-
ties.
Finally, all these systems need to be evaluated
not only in terms of f-score, precision and recall,
but also in terms of usability for the political scien-
tists. This also includes a detailed investigation of
various political science concepts and if they can
be detected automatically or if natural language
processing can help the political scientists to de-
tect their concepts semi-automatically. The defini-
tion of such evaluation is an open research topic in
itself.
Acknowledgements
The research leading to these results has been
done in the project eIdentity which is funded from
the Federal Ministry of Education and Research
(BMBF) under grant agreement 01UG1234.
62
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Andre Blessing, Jens Stegmann, and Jonas Kuhn.
2012. SOA meets relation extraction: Less may be
more in interaction. In Proceedings of the Work-
shop on Service-oriented Architectures (SOAs) for
the Humanities: Solutions and Impacts, Digital Hu-
manities, pages 6?11.
Volker Boehlke, Gerhard Heyer, and Peter Wittenburg.
2013. IT-based research infrastructures for the hu-
manities and social sciences - developments, exam-
ples, standards, and technology. it - Information
Technology, 55(1):26?33, February.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional, pages 89?97.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The kappa statistic. Computational
Linguistics, 22(2):249?254.
Manaal Faruqui and Sebastian Pado?. 2010. Train-
ing and evaluating a german named entity recog-
nizer with semantic generalization. In Proceedings
of KONVENS 2010, Saarbru?cken, Germany.
D. Ferrucci and A. Lally. 2004. UIMA: an architec-
tural approach to unstructured information process-
ing in the corporate research environment. Natural
Language Engineering, 10(3-4):327?348.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an update.
ACM SIGKDD Explorations Newsletter, 11(1):10?
18.
Erhard W. Hinrichs, Marie Hinrichs, and Thomas Za-
strow. 2010. WebLicht: Web-Based LRT Services
for German. In Proceedings of the ACL 2010 System
Demonstrations, pages 25?29.
Cathleen Kantner, Amelie Kutter, Andreas Hilde-
brandt, and Mark Puettcher. 2011. How to get rid
of the noise in the corpus: Cleaning large samples
of digital newspaper texts. International Relations
Online Working Paper, 2, July.
Marc Kupietz, Cyril Belica, Holger Keibel, and An-
dreas Witt. 2010. The german reference corpus
dereko: a primordial sample for linguistic research.
In Proceedings of the 7th conference on interna-
tional language resources and evaluation (LREC
2010), pages 1848?1854.
Shalom Lappin and Herbert J Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Compu-
tational linguistics, 20(4):535?561.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
Andreas Niekler and Patrick Ja?hnichen. 2012. Match-
ing results of latent dirichlet alocation for text.
In Proceedings of ICCM 2012, 11th International
Conference on Cognitive Modeling, pages 317?322.
Universita?tsverlag der TU Berlin.
John Platt et al 1998. Sequential minimal optimiza-
tion: A fast algorithm for training support vector
machines. technical report msr-tr-98-14, Microsoft
Research.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation processing & management, 24(5):513?
523.
Helmut Schmid and Florian Laws. 2008. Estima-
tion of conditional probabilities with decision trees
and an application to fine-grained POS tagging. In
Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
777?784, Manchester, UK, August.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to german. In
Proceedings of the ACL SIGDAT-Workshop, pages
47?50.
Helmut Schmid, 2009. Corpus Linguistics: An In-
ternational Handbook, chapter Tokenizing and Part-
of-Speech Tagging. Handbooks of Linguistics and
Communication Science. Walter de Gruyter, Berlin.
Burr Settles and Xiaojin Zhu. 2012. Behavioral fac-
tors in interactive training of text classifiers. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
563?567. Association for Computational Linguis-
tics.
Burr Settles. 2011. Closing the loop: Fast, inter-
active semi-supervised annotation with queries on
features and instances. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 1467?1478. Association for Com-
putational Linguistics.
Manfred Stede. 2004. The potsdam commentary
corpus. In Proceedings of the 2004 ACL Work-
shop on Discourse Annotation, DiscAnnotation ?04,
pages 96?102, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ralf Steinberger, Bruno Pouliquen, and Erik Van
Der Goot. 2009. An introduction to the europe me-
dia monitor family of applications. In Proceedings
of the Information Access in a Multilingual World-
Proceedings of the SIGIR 2009 Workshop, pages 1?
8.
63
Roland Stuckardt. 2001. Design and enhanced evalua-
tion of a robust anaphor resolution algorithm. Com-
putational Linguistics, 27(4):479?506.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165?210.
64
