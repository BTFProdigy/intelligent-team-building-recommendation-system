Message  C lass i f i ca t ion  in the  Ca l l  Center  
Stephan Busemann, Seen Schmeier~ Roman G. Arens 
DFKI GmbH 
Stuhlsatzenhausweg 3, D-66123 Saarbriicken, Germany 
e-mail: {busemann, schmeier, arens}@dfki.de 
Abstract 
Customer care in technical domains is increasingly 
based on e-mail communication, allowing for the re- 
production of approved solutions. Identifying the 
customer's problem is often time-consuming, as the 
problem space changes if new products are launched. 
This paper describes a new approach to the classifi- 
cation of e-mail requests based on shallow text pro- 
cessing and machine learning techniques. It is im- 
plemented within an assistance system for call center 
agents that is used in a commercial setting. 
1 I n t roduct ion  
Customer care in technical domains is increasingly 
based on e-mail communication, allowing for the re- 
production of approved solutions. For a call cen- 
ter agent, identifying the customer's problem is of- 
ten time-consuming, as the problem space changes 
if new products are launched or existing regulations 
are modified. The typical task of a call center agent 
processing e-mail requests consists of the following 
steps: 
Recogn ize  the  prob lem(s) :  read and understand 
the e-mail request; 
Search  a solut ion:  identify and select predefined 
text blocks; 
P rov ide  the  solut ion:  if necessary, customize 
text blocks to meet the current request, and 
send the text. 
This task can partly be automated by a system 
suggesting relevant solutions for an incoming e-mail. 
This would cover the first two steps. The last step 
can be delicate, as its primary goal is to keep the 
customer satisfied. Thus human intervention seems 
mandatory to allow for individual, customized an- 
swers. Such a system will 
? reduce the training effort required since agents 
don't have to know every possible solution for 
every possible problem; 
? increase the agents' performance since agents 
can more quickly select a solution among several 
offered than searching one; 
? improve the quality of responses ince agents 
will behave more homogeneously - both as a 
group and over time - and commit fewer errors. 
Given that free text about arbitrary topics must 
be processed, in-depth approaches to language un- 
derstanding are not feasible. Given further that the 
topics may change over time, a top-down approach 
to knowledge modeling is out of the question. Rather 
a combination of shallow text processing (STP) with 
statistics-based machine learning techniques (SML) 
is called for. STP gathers partial information about 
text such as part of speech, word stems, negations, 
or sentence type. These types of information can be 
used to identify the linguistic properties of a large 
training set of categorized e-mails. SML techniques 
are used to build a classifier that is used for new, 
incoming messages. Obviously, the change of topics 
can be accommodated by adding new categories and 
e-mails and producing a new classifier on the basis 
of old and new data. We call this replacement of a 
classifier "relearning". 
This paper describes a new approach to the clas- 
sification of e-mail requests along these lines. It is 
implemented within the ICe-MAIL system, which 
is an assistance system for call center agents that 
is currently used in a commercial setting. Section 2 
describes important properties of the input data, i.e. 
the e-mail texts on the one hand, and the categories 
on the other. These properties influenced the system 
architecture, which is presented in Section 3. Vari- 
ous publicly available SML systems have been tested 
with different methods of STP-based preprocessing. 
Section 4 describes the results. The implementation 
and usage of the system including the graphical user 
interface is presented in Section 5. We conclude by 
giving an outlook to further expected improvements 
(Section 6). 
2 Data  Character ist ics  
A closer look at the data the ICe-MAIL system is 
processing will clarify the task further. We carried 
out experiments with unmodified e-mail data accu- 
mulated over a period of three months in the call 
center database. The total amount was 4777 e-mails. 
158
We used 47 categories, which contained at least 30 
documents. This minimum amount of documents 
turned out to render the category sufficiently dis- 
tinguishable for the SML tools. The database con- 
tained 74 categories with at least 10 documents, but 
the selected ones covered 94% of all e-malls, i.e. 4490 
documents. 
It has not yet generally been investigated how the 
type of data influences the learning result (Yang, 
1999), or under which circumstances which kind of 
preprocessing and which learning algorithm is most 
appropriate. Several aspects must be considered: 
Length of the documents, morphological and syn- 
tactic well-formedness, the degree to which a docu- 
ment can be uniquely classified, and, of course, the 
language of the documents. 
In our application domain the documents differ 
very much from documents generally used in bench- 
mark tests, for example the Reuters corpus 1. First 
of all, we have to deal with German, whereas the 
Reuters data are in English. The average length of 
our e-mails is 60 words, whereas for documents of 
Reuters-21578 it is 129 words. The number of cat- 
egories we used compares to the top 47 categories 
of the Reuters TOPICS category set. While we 
have 5008 documents, TOPICS consists of 13321 in- 
stances 2. The Reuters documents usually are mor- 
phologically and syntactically well-formed. As e- 
mails are a more spontaneously created and infor- 
mal type of document, they require us to cope with 
a large amount of jargon, misspellings and gram- 
matical inaccuracy. A drastic example is shown in 
Figure 2. The bad conformance to linguistic stan- 
dards was a major argument in favor of STP instead 
of in-depth syntactic and semantic analysis. 
The degree to which a document can be uniquely 
classified is hard to verify and can only be inferred 
from the results in general terms. 3 It is, however, 
dependent on the ability to uniquely distinguish the 
classes. In our application we encounter overlapping 
and non-exhaustive categories as the category sys- 
tem develops over time. 
3 Integrating Language Technology 
With  Machine Learning 
STP and SML correspond to two different 
paradigms. STP tools used for classification tasks 
promise very high recall/precision or accuracy val- 
ues. Usually human experts define one or several 
template structures to be filled automatically by ex- 
tracting information from the documents (cf. e.g. 
(Ciravegna et al, 1999)). Afterwards, the partially 
lhttp ://~wv. research, a~t. com/'le~is/reuters21578. 
html 
2We took only uniquely classified ocuments into account.  
3Documents containing multiple requests can at present 
only be treated manually, as described in Section 5. 
filled templates are classified by hand-made rules. 
The whole process brings about high costs in analyz- 
ing and modeling the application domain, especially 
if it is to take into account he problem of changing 
categories in the present application. 
SML promises low costs both in analyzing and 
modeling the application at the expense of a lower 
accuracy. It is independent of the domain on the 
one hand, but does not consider any domain specific 
knowledge on the other. 
By combining both methodologies in ICe -MAIL ,  
we achieve high accuracy and can still preserve a use- 
ful degree of domain-independence. STP may use 
both general inguistic knowledge and linguistic al- 
gorithms or heuristics adapted to the application in 
order to extract information from texts that is rele- 
vant for classification. The input to the SML tool is 
enriched with that information. The tool builds one 
or several categorizers 4 that will classify new texts. 
In general, SML tools work with a vector epresen- 
tation of data. First, a relevancy vector of relevant 
features for each class is computed (Yang and Ped- 
ersen, 1997). In our case the relevant features con- 
sist of the user-defined output of the linguistic pre- 
processor. Then each single document is translated 
into a vector of numbers isomorphic to the defining 
vector. Each entry represents the occurrence of the 
corresponding feature. More details will be given in 
Section 4 
The ICe-MAIL architecture is shown in Figure 1. 
The workflow of the system consists of a learning 
step carried out off-line (the light gray box) and an 
online categorization step (the dark gray box). In 
the off-line part, categorizers are built by processing 
classified data first by an STP and then by an SML 
tool. In this way, categorizers can be replaced by the 
system administrator as she wants to include new 
or remove expired categories. The categorizers are 
used on-line in order to classify new documents after 
they have passed the linguistic preprocessing. The 
resulting category is in our application associated 
with a standard text that the call center agent uses 
in her answer. The on-line step provides new clas- 
sified data that is stored in a dedicated ICe-MAIL 
database (not shown in Figure 1). The relearning 
step is based on data from this database. 
3.1 Shal low Text  Processing 
Linguistic preprocessing of text documents is car- 
ried out by re-using sines, an information extrac- 
tion core system for real-world German text pro- 
cessing (Neumann et al, 1997). The fundamental 
design criterion of sines is to provide a set of basic, 
powerful, robust, and efficient STP components and 
4Almost all tools we examined build a single multi- 
categorizer except for SVM-Light, which builds multiple bi- 
nary classifiers. 
1 I ;Q  159
Cate gorY) 
J 
Figure 1: Architecture of the ICC-MAIL System. 
generic linguistic knowledge sources that can eas- 
ily be customized to deal with different asks in a 
flexible manner, sines includes a text tokenizer, a 
lexical processor and a chunk parser. The chunk 
parser itself is subdivided into three components. In 
the first step, phrasal fragments like general nominal 
expressions and verb groups are recognized. Next, 
the dependency-based structure of the fragments of 
each sentence is computed using a set of specific sen- 
tence patterns. Third, the grammatical functions 
are determined for each dependency-based structure 
on the basis of a large subcategorization lexicon. 
The present application benefits from the high mod- 
ularity of the usage of the components. Thus, it is 
possible to run only a subset of the components and 
to tailor their output. The experiments described in 
Section 4 make use of this feature. 
3.2 Statistics-Based Machine Learning 
Several SML tools representing different learning 
paradigms have been selected and evaluated in dif- 
ferent settings of our domain: 
Lazy Learning: Lazy Learners are also known 
as memory-based, instance-based, exemplar- 
based, case-based, experience-based, or k- 
nearest neighbor algorithms. They store all 
documents as vectors during the learning phase. 
In the categorization phase, the new document 
vector is compared to the stored ones and is 
categorized to same class as the k-nearest neigh- 
bors. The distance is measured by computing 
e.g. the Euclidean distance between the vectors. 
By changing the number of neighbors k or the 
kind of distance measure, the amount of gener- 
alization can be controlled. 
We used IB (Aha, 1992), which is part of 
the MLC++ library (Kohavi and Sommerfield, 
1996). 
Symbolic Eager Learning: This type of learners 
constructs a representation for document vec- 
tors belonging to a certain class during the 
learning phase, e.g. decision trees, decision rules 
or probability weightings. During the catego- 
rization phase, the representation is used to as- 
sign the appropriate class to a new document 
vector. Several pruning or specialization heuris- 
tics can be used to control the amount of gen- 
eralization. 
We used ID3 (Quinlan, 1986), C4.5 (Quinlan, 
1992) and C5.0, R IPPER (Cohen, 1995), and 
the Naive Bayes inducer (Good, 1965) con- 
tained in the MLCq-q- library. ID3, C4.5 and 
C5.0 produce decision trees, R IPPER i sa  rule- 
based learner and the Naive Bayes algorithm 
computes conditional probabilities of the classes 
from the instances. 
Support Vector Machines (SVMs) :  SVMs are 
described in (Vapnik, 1995). SVMs are binary 
learners in that they distinguish positive and 
negative examples for each class. Like eager 
learners, they construct a representation dur- 
ing the learning phase, namely a hyper plane 
supported by vectors of positive and negative 
examples. For each class, a categorizer is built 
by computing such a hyper plane. During the 
categorization phase, each categorizer is applied 
to the new document vector, yielding the prob- 
abilities of the document belonging to a class. 
The probability increases with the distance of 
thevector from the hyper plane. A document 
is said to belong to the class with the highest 
probability. 
We chose SVM_Light (Joachims, 1998). 
Neura l  Networks :  Neural Networks are a special 
kind of "non-symbolic" eager learning algo- 
1 60 
rithm. The neural network links the vector el- 
ements to the document categories The learn- 
ing phase defines thresholds for the activation 
of neurons. In the categorization phase, a new 
document vector leads to the activation of a sin- 
gle category. For details we refer to (Wiener et 
al., 1995). 
In our application, we tried out the Learning 
Vector Quantization (LVQ) (Kohonen et al, 
1996). LVQ has been used in its default config- 
uration only. No adaptation to the application 
domain has been made. 
4 Exper iments  and  Resu l ts  
We describe the experiments and results we achieved 
with different linguistic preprocessing and learning 
algorithms and provide some interpretations. 
We start out from the corpus of categorized e- 
mails described in Section 2. In order to normalize 
the vectors representing the preprocessing results of 
texts of different length, and to concentrate on rel- 
evant material (cf. (Yang and Pedersen, 1997)), we 
define the relevancy vector as follows. First, all doc- 
uments are preprocessed, yielding a list of results 
for each category. From each of these lists, the 100 
most frequent results - according to a TF / IDF  mea- 
sure - are selected. The relevancy vector consists of 
all selected results, where doubles are eliminated. 
Its length was about 2500 for the 47 categories; it 
slightly varied with the kind of preprocessing used. 
During the learning phase, each document is pre- 
processed. The result is mapped onto a vector of 
the same length as the relevancy vector. For ev- 
ery position in the relevancy vector, it is determined 
whether the corresponding result has been found. In 
that case, the value of the result vector element is 1, 
otherwise it is 0. 
In the categorization phase, the new document is 
preprocessed, and a result vector is built as described 
above and handed over to the categorizer (cf. Fig- 
ure 1). 
While we tried various kinds of linguistic prepro- 
cessing, systematic experiments have been carried 
out with morphological nalysis (MorphAna), shal- 
low parsing heuristics (STP-Heuristics), and a com- 
bination of both (Combined). 
MorphAna:  Morphological Analysis provided by 
sines yields the word stems of nouns, verbs and 
adjectives, as well as the full forms of unknown 
words. We are using a lexicon of approx. 100000 
word stems of German (Neumann et al, 1997). 
STP-Heur i s t i cs :  Shallow parsing techniques are 
used to heuristically identify sentences contain- 
ing relevant information. The e-mails usually 
contain questions and/or descriptions of prob- 
lems. The manual analysis of a sample of 
the data suggested some linguistic constructions 
frequently used to express the problem. We ex- 
pected that content words in these construc- 
tions should be particularly influential to the 
categorization. Words in these constructions 
are extracted and processed as in MorphAna, 
and all other words are ignored. 5 The heuris- 
tics were implemented in ICC-MAIL  using sines. 
The constructions of interest include negations 
at the sentence and the phrasal level, yes-no 
and wh-questions, and declaratives immediately 
preceding questions. Negations were found to 
describe a state to be changed or to refer to 
missing objects, as in I cannot read my email 
or There is no correct date. We identified them 
through negation particles. 8 Questions most of- 
ten refer to the problem in hand, either directly, 
e.g. How can I start my email program. ~ or in- 
directly, e.g. Why is this the case?. The lat- 
ter most likely refers to the preceding sentence, 
e.g. My system drops my e-mails. Questions are 
identified by their word order, i.e. yes-no ques- 
tions start with a verb and wh-questions with a 
wh-particle. 
Combined:  In order to emphasize words found 
relevant by the STP heuristics without losing 
other information retrieved by MorphAna, the 
previous two techniques are combined. Empha- 
sis is represented here by doubling the number 
of occurrences of the tokens in the normaliza- 
tion phase, thus increasing their TF / IDF  value. 
Call center agents judge the performance of ICC-  
MAIL  most easily in terms of accuracy: In what per- 
centage of cases does the classifier suggest the correct 
text block? In Table 1, detailed information about 
the accuracy achieved is presented. All experiments 
were carried out using 10-fold cross-validation  the 
data described in Section 2. 
In all experiments he SVM_Light system outper- 
formed other learning algorithms, which confirms 
Yang's (Yang and Liu, 1999) results for SVMs fed 
with Reuters data. The k-nearest neighbor algo- 
rithm IB performed surprisingly badly although dif- 
ferent values ofk were used. For IB, ID3, C4.5, C5.0, 
Naive Bayes, R IPPER and SVM_Light, linguis- 
tic preprocessing increased the overall performance. 
In fact, the method performing best, SVM_Light, 
gained 3.5% by including the task-oriented heuris- 
tics. However, the boosted R IPPER and LVQ scored 
a decreased accuracy value there. For LVQ the de- 
crease may be due to the fact that no adaptations to 
5If no results were found this way, MorphAna was applied 
instead. 
6We certainly would have benefited from lexical semantic 
information, e.g. The correct date is missing would not be 
captured by our approach. 
161 
Neural Nets 
Lazy Learner 
Symbolic Eager 
Learners 
Support Vectors \[\] 
SML algorithm 
LVQ 
IB 
Naive Bayes 
ID3 
R IPPER 
Boosted Ripper 
C4.5 
C5.0 
SVM_L ight  
MorphAna 
Best Best5 
35.66 
33.81 
33.83 
38.53 
47.08 
52.73 
52.00 
52.60 
53.85 74.91 
STP-Heuristics 
Best Best5 
22.29 
33.01 
33.76 
38.11 
49.38 
49.96 
52.90 
53.20 
54.84 78.05 
Combined 
Best Best5 
25.97 
35.14 
34.01 
40.02 
50.54 
50.78 
53.40 
54.20 
56.23 78.17 
Table 1: Results of Experiments. Most SML tools deliver the best result only. SVM_Light produces ranked 
results, allowing to measure the accuracy of the top five alternatives (Best5). 
the domain were made, such as adapting the number 
of codebook vectors, the initial learning parameters 
or the number of iterations during training (cf. (Ko- 
honen et al, 1996)). Neural networks are rather sen- 
sitive to misconfigurations. The boosting for RIP- 
PER seems to run into problems of overfitting. We 
noted that in six trials the accuracy could be im- 
proved in Combined compared to MorphAna, but in 
four trials, boosting led to deterioration. This effect 
is also mentioned in (Quinlan, 1996). 
These figures are slightly lower than the ones re- 
ported by (Neumann and Schmeier, 1999) that were 
obtained from a different data set. Moreover, these 
data did not contain multiple queries in one e-mall. 
It would be desirable to provide explanations for 
the behavior of the SML algorithms on our data. As 
we have emphasized in Section 2, general methods 
of explanation do not exist yet. In the application 
in hand, we found it difficult to account for the ef- 
fects of e.g. ungrammatical text or redundant cate- 
gories. For the time being, we can only offer some 
speculative and inconclusive assumptions: Some of 
the tools performing badly - IB, ID3, and the Naive 
Bayes inducer of the MLC++ library - have no or 
little pruning ability. With rarely occurring data, 
this leads to very low generalization rates, which 
again is a problem of overfitting. This suggests that 
a more canonical representation for the many ways 
of expressing a technical problem should be sought 
for. Would more extensive linguistic preprocessing 
help? 
Other tests not reported in Table 1 looked at im- 
provements hrough more general and sophisticated 
STP such as chunk parsing. The results were very 
discouraging, leading to a significant decrease com- 
pared to MorphAna. We explain this with the bad 
compliance of e-mall texts to grammatical standards 
(cf. the example in Figure 2). 
However, the practical usefulness of chunk parsing 
or even deeper language understanding such as se- 
mantic analysis may be questioned in general: In a 
moving domain, the coverage of linguistic knowledge 
will always be incomplete, as it would be too expen- 
sive for a call center to have language technology 
experts keep pace with the occurrence of new to~ 
ics. Thus the preprocessing results will often differ 
for e-mails expressing the same problem and hence 
not be useful for SML. 
As a result of the tests in our application domain, 
we identified a favorite statistical tool and found that 
task-specific linguistic preprocessing is encouraging, 
while general STP is not. 
5 Imp lementat ion  and  Use  
In this section we describe the integration of the 
ICC-MAIL system into the workflow of the call cen- 
ter of AOL Bertelsmann Online GmbH & Co. KG, 
which answers requests about the German version 
of AOL software. A client/server solution was built 
that allows the call center agents to connect as 
clients to the ICe-MAIL server, which implements 
the system described in Section 3. For this purpose, 
it was necessary to 
? connect the server module to AOL's own Sybase 
database that delivers the incoming mail and 
dispatches the outgoing answers, and to I ce -  
MAIL'S own database that stores the classified 
e-mall texts; 
? design the GUI of the client module in a self- 
explanatory and easy to use way (cf. Figure 2). 
The agent reads in an e-mall and starts ICe-MAIL 
using GUI buttons. She verifies the correctness of 
the suggested answer, displaying and perhaps se- 
lecting alternative solutions. If the agent finds the 
appropriate answer within these proposals, the asso- 
ciated text is filled in at the correct position of the 
answer e-mall. If, on the other hand, no proposed 
solution is found to be adequate, the ICe-MAIL tool 
can still be used to manually select any text block 
162 
0" ~ GPF 
~) ~ In~allatice, 
~AOL. 
\[~CD, $o'el~alt, Ha~du~te 
~) FAO - (fmlln 
r~ i$ON 
\[~ Me4em 
Before deinstalling the AOL-Soltware please check your folders for 
-downloaded data 
-saved passwords 
and copy them into a backup folder. 
Then remove the AOL-Software using the Windows Control Panel and 
reinstall it from your CD. 
Alter reinstallation please copy the data from the bac~p folder into 
the dght destinations. 
Figure 2: The GUI of the ICe-MAIL Client. All labels and texts were translated by the authors. The English 
input is based on the following original text, which is similarly awkward though understandable: Wie mache 
ich zurn mein Programm total deinstalieren, und wieder neu instalierem, mit, wen Sic mir senden Version 
4.0 ??????????????. The suggested answer text is associated with the category named "Delete & Reinstall 
AOL 4.0". Four alternative answers can be selected using the tabs. The left-hand side window displays the 
active category in context. 
from the database. The ICe-MAIL client had to pro- 
vide the functionality of the tool already in use since 
an additional tool was not acceptable to the agents, 
who are working under time pressure. 
In the answer e-mail window, the original e-mail 
is automatically added as a quote. If an e-mail con- 
tains several questions, the classification process can 
be repeated by marking each question and iteratively 
applying the process to the marked part. The agent 
can edit the suggested texts before sending them off. 
In each case, the classified text together with the se- 
lected category is stored in the ICe-MAIL database 
for use in future learning steps. 
Other features of the ICe-MAIL client module in- 
clude a spell checker and a history view. The latter 
displays not only the previous e-mails of the same 
author but also the solutions that have been pro- 
posed and the elapsed time before an answer was 
sent. 
The assumed average time for an agent to an- 
swer an e-mail is a bit more than two minutes with 
AOL's own mail processing system. ~With the ICC- 
MAIL system the complete cycle of fetching the mail, 
checking the proposed solutions, choosing the ap- 
propriate solutions, inserting additional text frag- 
ments and sending the answer back can probably 
be achieved in half the time. Systematic tests sup- 
~This system does not include automatic analysis of mails. 
porting this claim are not completed yet, s but the 
following preliminary results are encouraging: 
? A test under real-time conditions at the call- 
center envisaged the use of the ICe -MAIL  sys- 
tem as a mail tool only, i.e. without taking ad- 
vantage of the system's intelligence. It showed 
that the surface and the look-and-feel is ac- 
cepted and the functionality corresponds to the 
real-time needs of the call center agents, as users 
were slightly faster than within their usual en- 
vironment. 
? A preliminary test of the throughput achieved 
by using the STP and SML technology in I ce -  
MAIL showed that experienced users take about 
50-70 seconds on average for one cycle, as de- 
scribed above. This figure was gained through 
experiments with three users over a duration of 
about one hour each. 
Using the system with a constant set of categories 
will improve its accuracy after repeating the off-line 
learning step. If a new category is introduced, the 
accuracy will slightly decline until 30 documents are 
manually classified and the category is automatically 
included into a new classifier. Relearning may take 
place at regular intervals. The definition of new cat- 
egories must be fed into ICe-MAIL by a "knowledge 
8As of end of February 2000. 
163
engineer", who maintains the system. The effects of 
new categories and new data have not been tested 
yet. 
The optimum performance of ICe-MAIL can be 
achieved only with a well-maintained category sys- 
tem. For a call center, this may be a difficult task 
to achieve, espescially under severe time pressure, 
but it will pay off. In particular, all new categories 
should be added, outdated ones should be removed, 
and redundant ones merged. Agents should only use 
these categories and no others. The organizational 
structure of the team should reflect this by defin- 
ing the tasks of the "knowledge ngineer" and her 
interactions with the agents. 
6 Conc lus ions  and  Future  Work  
We have presented new combinations of STP and 
SML methods to classify unrestricted e-mail text ac- 
cording to a changing set of categories. The current 
accuracy of the ICC-MAIL system is 78% (correct so- 
lution among the top five proposals), corresponding 
to an overall performance of 73% since ICC-MAIL 
processes only 94% of the incoming e-mails. The 
accuracy improves with usage, since each relearning 
step will yield better classifiers. The accuracy is ex- 
pected to approximate that of the agents, but not 
improve on it. With ICe-MAIL, the performance of
an experienced agent can approximately be doubled. 
The system is currently undergoing extensive tests 
at the call center of AOL Bertelsmann Online. De- 
tails about the development of the performance de- 
pending on the throughput and change of categories 
are expected to be available by mid 2000. 
Technically, we expect improvements from the fol- 
lowing areas of future work. 
? Further task-specific heuristics aiming at gen- 
eral structural inguistic properties hould be 
defined. This includes heuristics for the identi- 
fication of multiple requests in a single e-mail 
that could be based on key words and key 
phrases as well as on the analysis of the doc- 
ument structure. 
? Our initial experiments with the integration 
of GermaNet (Hamp and Feldweg, 1997), the 
evolving German version of WordNet, seem to 
confirm the positive results described for Word- 
Net (de Buenaga Rodriguez et al, 1997) and 
will thus be extended. 
? A reorganization f the existing three-level cate- 
gory system into a semantically consistent tree 
structure would allow us to explore the non- 
terminal nodes of the tree for multi-layered 
SML. This places additional requirements on 
the knowledge ngineering task and thus needs 
to be thoroughly investigated for pay-off. 
? Where system-generated answers are acceptable 
to customers, a straightforward extension of 
ICe-MAIL can provide this functionality. For 
the application in hand, this was not the case. 
The potential of the technology presented extends 
beyond call center applications. We intend to ex- 
plore its use within an information broking assis- 
tant in document classification. In a further indus- 
trial project with German Telekom, the ICC-MAIL 
technology will be extended to process multi-lingual 
press releases. The nature of these documents will 
allow us to explore the application of more sophis- 
ticated language technologies during linguistic pre- 
processing. 
Acknowledgments  
We are grateful to our colleagues Giinter Neumann, 
Matthias Fischmann, Volker Morbach, and Matthias 
Rinck for fruitful discussions and for support with 
sines modules. This work was partially supported by 
a grant of the Minister of Economy and Commerce 
of the Saarland, Germany, to the project ICC. 
References  
David W. Aha. 1992. Tolerating noisy, irrelevant 
and novel attributes in instance based learning al- 
gorithms. International Journal of Man-Machine 
Studies, 36(1), pages 267-287. 
Fabio Ciravegna, Alberto Lavelli, Nadia Mana, Jo- 
hannes Matiasek, Luca Gilardoni, Silvia Mazza, 
Massimo Ferraro, William J.Black, Fabio RJ- 
naldi, and David Mowatt. 1999. Facile: Classi- 
fying texts integrating pattern matching and in- 
formation extraction. In Proceedings of IJCAI'99, 
Stockholm, pages 890-895. 
William W. Cohen. 1995. Fast effective rule induc- 
tion. In Proceedings of the Twelfth International 
Conference on Machine Learning, Lake Tahoe, 
California. 
Manuel de Buenaga Rodriguez, Jose Maria Gomez- 
Hidalgo, and Belen Diaz-Agudo. 1997. Using 
WordNet to complement training information in 
text categorization. In Proceedings of the Second 
International Conference on Recent Advances in 
Natural Language Processing, Montreal, Canada. 
I.J. Good. 1965. The Estimation of Probabilities. 
An Essay on Modern Bayesian Methods. MIT- 
Press. 
Birgit Hamp and Helmut Feldweg. 1997. GermaNet 
- a lexical-semantic net for German. In Proceed- 
ings of A CL workshop Automatic Information Ex- 
traction and Building of Lexical Semantic Re- 
sources for NLP Applications, Madrid, Spain 
Thorsten Joachims. 1998. Text categorization with 
support vector machines - learning with meany 
relevant features. In Proceedings of the Euro- 
164 
pean Conference on Machine Learning (ECML), 
Chemnitz, Germany, pages 137-142. 
Ronny Kohavi and Dan Sommerfield, 1996. 
MLC++ Machine Learning library in C++. 
http://www.sgi.com/Technology/mlc. 
Teuvo Kohonen, Jussi Hynninen, Jari Kangas, 
Jorma Laaksonen, and Kari Torkkola. 1996. 
LVQ-PAK the learning vector quantization pro- 
gram package. Technical Report A30, Helsinki 
University of Technology. 
G/inter Neumann, Rolf Backofen, Judith Baur, 
Markus Becket, and Christian Braun. 1997. An 
information extraction core system for real world 
German text processing. In Proceedings of 5th 
ANLP, Washington, pages 209-216. 
G/inter Neumann and Sven Schmeier. 1999. Com- 
bining shallow text processing and macine learn- 
ing in real world applications. In Proceedings of 
IJCAI workshop on Machine Learning for Infor- 
mation Filtering, Stockholm, pages 55-60. 
J.R. Quinlan. 1986. Induction of Decision Trees. 
Reprinted in Shavlik, Jude W. and Dietterich, 
Thomas G, Readings in machine learning. Ma- 
chine learning series. Morgan Kaufmann (1990) 
J.R. Quinlan. 1992. C4.5: Programs for Machine 
Learning. Morgan Kaufmann, San Mateo, Cali- 
fornia. 
J.R. Quinlan. 1996. Bagging, Boosting and C4.5. In 
Proceedings of AAAI'96, Portland, pages 725-730. 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer. 
E.D. Wiener, J. Pedersen, and A.S. Weigend. 1995. 
A neural network approach to topic spotting. In 
Proceedings of the SDAIR. 
Y. Yang and Xin Liu. 1999. A re-examination f 
text categorization methods. In Proceedings of 
A CMSIGIR Conference on Research and Devel- 
opment in Information Retrieval, Berkley, Calfor- 
nia. 
Y. Yang and J.P. Pedersen. 1997. A comparative 
study on feature selection. In Proceedings of the 
Fourteenth International Conference on Machine 
Learning (ICML '97). 
Y. Yang. 1999. An evaluation of statistical ap- 
proaches to text categorization. Information Re- 
trieval Journal (May 1999). 
165 
Integrating Information Extraction and Automatic Hyperlinking 
 
Stephan Busemann, Witold'UR G \ VNL Hans-Ulrich Krieger,  
Jakub Piskorski, Ulrich Sch?fer, Hans Uszkoreit, Feiyu Xu 
German Research Center for Artificial Intelligence (DFKI GmbH) 
Stuhlsatzenhausweg 3, D-66123 Saarbr?cken, Germany 
sprout@dfki.de 
 
 
Abstract 
This paper presents a novel information sys-
tem integrating advanced information extrac-
tion technology and automatic hyper-linking. 
Extracted entities are mapped into a domain 
ontology that relates concepts to a selection of 
hyperlinks. For information extraction, we use 
SProUT, a generic platform for the develop-
ment and use of multilingual text processing 
components. By combining finite-state and 
unification-based formalisms, the grammar 
formalism used in SProUT offers both pro-
cessing efficiency and a high degree of decal-
rativeness. The ExtraLink demo system show-
cases the extraction of relevant concepts from 
German texts in the tourism domain, offering 
the direct connection to associated web docu-
ments on demand.   
1 Introduction 
The utilization of language technology for the 
creation of hyperlinks has a long history (e.g., 
Allen et al, 1993). Information extraction (IE) is a 
technology that can be applied to identifying both 
sources and targets of new hyperlinks. IE systems 
are becoming commercially viable in supporting 
diverse information discovery and management 
tasks. Similarly, automatic hyperlinking is a matu-
ring technology designed to interrelate pieces of 
information, using ontologies to define the rela-
tionships. With ExtraLink, we present a novel 
information system that integrates both technolo-
gies in order to reach at an improved level of 
informativeness and comfort. Extraction and link 
generation occur completely in the background. 
Entities identified by the IE system are mapped 
into a domain ontology that relates concepts to a 
structured selection of predefined hyperlinks, 
which can be directly visualized on demand using 
a standard web browser. This way, the user can, 
while reading a text, immediately link up textual 
information to the Internet or to any other docu-
ment base without accessing a search engine.  
The quality of the link targets is much higher 
than with standard search engines since, first of all, 
only domain-specific interpretations are sought, 
and second, the ontology provides additional 
structure, including related information. 
ExtraLink uses as its IE system SProUT, a gene-
ric multilingual shallow analysis platform, which 
currently provides linguistic processing resources 
for English, German, Italian, French, Spanish, 
Czech, Polish, Japanese, and Chinese (Becker et 
al., 2002). SProUT is used for tokenization, mor-
phological analysis, and named entity recognition 
in free texts. In Section 2 to 4, we describe innova-
tive features of SProUT. Section 5 gives details 
about the ExtraLink demonstrator. 
2 Integrating Typed Feature Structures 
and Finite State Machines 
The main motivation for developing SProUT 
comes from the need to have a system that (i) 
allows a flexible integration of different processing 
modules and (ii) to find a good trade-off between 
processing efficiency and linguistic expressive-
ness. On the one hand, very efficient finite state 
devices have been successfully applied to real-
world applications. On the other hand, unification-
based grammars (UBGs) are designed to capture 
fine-grained syntactic and semantic constraints, 
resulting in better descriptions of natural language 
phenomena. In contrast to finite state devices, 
unification-based grammars are also assumed to be 
more transparent and more easily modifiable. 
SProUT?s mission is to take the best from these 
two worlds, having a finite state machine that 
operates on typed feature structures (TFSs). I.e., 
transduction rules in SProUT do not rely on simple 
atomic symbols, but instead on TFSs, where the 
left-hand side of a rule is a regular expression over 
TFSs, representing the recognition pattern, and the 
right-hand side is a sequence of TFSs, specifying 
the output structure. Consequently, equality of 
atomic symbols is replaced by unifiability of TFSs 
and the output is constructed using TFS unification 
w.r.t. a type hierarchy. Such rules not only recog-
nize and classify patterns, but also extract frag-
ments embedded in the patterns and fill output 
templates with them. 
Standard finite state techniques such as minimi-
zation and determinization are no longer applicable 
here, due to the fact that edges in our automata are 
annotated by TFSs, instead of atomic symbols. 
However, not every outgoing edge in such an 
automaton must be analyzed, since TFS annota-
tions can be arranged under subsumption, and the 
failure of a general edge automatically causes the 
failure of several, more specialized edges, without 
applying the unifiability test. Such information can 
in fact be precompiled. This and other optimization 
techniques are described in (Krieger and Piskorski, 
2003). 
When compared to symbol-based finite state 
approaches, our method leads to smaller grammars 
and automata, which usually better approximate a 
given language.  
3 XTDL ? The Formalism in SProUT 
XTDL combines two well-known frameworks, 
viz., typed feature structures and regular ex-
pressions. XTDL is defined on top of TDL, a defi-
nition language for TFSs (Krieger and Sch?fer, 
1994) that is used as a descriptive device in several 
grammar systems (LKB, PAGE, PET).  
Apart from the integration into the rule 
definitions, we also employ TDL in SProUT for 
the establishment of a type hierarchy of linguistic 
entities. In the example definition below, the 
morph type inherits from sign and introduces three 
more morphologically motivated attributes with 
the corresponding typed values: 
morph := sign & [ POS  atom, STEM atom, INFL infl ]. 
A rule in XTDL is straightforwardly defined as 
a recognition pattern on the left-hand side, written 
as a regular expression, and an output description 
on the right-hand side. A named label serves as a 
handle to the rule. Regular expressions over TFSs 
describe sequential successions of linguistic signs. 
We provide a couple of standard operators. Con-
catenation is expressed by consecutive items. Dis-
junction, Kleene star, Kleene plus, and optionality 
are represented by the operators |, *, +, and ?, resp. 
{n} after an expression denotes an n-fold repetition. 
{m,n} repeats at least m times and at most n times. 
The XTDL grammar rule below may illustrate 
the syntax. It describes a sequence of morphologi-
cally analyzed tokens (of type morph). The first 
TFS matches one or zero items (?) with part-of-
speech Determiner. Then, zero or more Adjective 
items are matched (*). Finally, one or two Noun 
items ({1,2}) are consumed. The use of a variable 
(e.g., #1) in different places establishes a 
coreference between features. This example enfor-
ces agreement in case, number, and gender for the 
matched items. Eventually, the description on the 
RHS creates a feature structure of type phrase, 
where the category is coreferent with the category 
Noun of the right-most token(s), and the agreement 
features corefer to features of the morph tokens. 
 np :> 
   (morph & [ POS  Determiner, 
 INFL  [CASE #1, NUM #2, GEN #3 ]] )?  
   (morph & [ POS  Adjective, 
 INFL  [CASE #1, NUM #2, GEN #3 ]] )*  
   (morph & [ POS  Noun & #4, 
 INFL  [CASE #1, NUM #2, GEN #3 ]] ){1,2} 
 -> phrase & [CAT #4, 
 AGR agr & [CASE #1, NUM #2, GEN #3 ]]. 
 
The choice of TDL has a couple of advantages. 
TFSs as such provide a rich descriptive language 
over linguistic structures and allow for a fine-
grained inspection of input items. They represent a 
generalization over pure atomic symbols. Unifia-
bility as a test criterion in a transition is a generali-
zation over symbol equality. Coreferences in 
feature structures express structural identity. Their 
properties are exploited in two ways. They provide 
a stronger expressiveness, since they create 
dynamic value assignments on the automaton 
transitions and thus exceed the strict locality of 
constraints in an atomic symbol approach. Further-
more, coreferences serve as a means of information 
transport into the output description on the RHS of 
the rule. Finally, the choice of feature structures as 
primary citizens of the information domain makes 
composition of modules very simple, since input 
and output are all of the same abstract data type.  
Functional (in contrast to regular) operators are 
a door to the outside world of SProUT.  They 
either serve as predicates, helping to locate 
complex tests that might cancel a rule application, 
or they construct new material, involving pieces of 
information from the LHS of a rule.  The sketch of 
a rule below transfers numerals into their 
corresponding digits using the functional operator 
normalize() that is defined externally. For instance, 
"one" is mapped onto "1", "two" onto "2", etc. 
 
  ?  numeral & [ SURFACE #surf, ... ] .?  -> 
  digit & [ ID #id, ... ],  where #id = normalize(#surf). 
4 The SProUT System  
The core of SProUT comprises of the following 
components: (i) a finite-state machine toolkit for 
building, combining, and optimizing finite-state 
devices; (ii) a flexible XML-based regular com-
piler for converting regular patterns into their cor-
responding compressed finite-state representation 
(Piskorski et al, 2002); (iii) a JTFS package which 
provides standard operations for constructing and 
manipulating TFSs; and (iv) an XTDL grammar 
interpreter. 
Currently, SProUT offers three online compo-
nents: a tokenizer, a gazetteer, and a morphological 
analyzer. The tokenizer maps character sequences 
to tokens and performs fine-grained token classifi-
cation. The gazetteer recognizes named entities 
based on static named entity lexica.  
The morphology unit provides lexical resources 
for English, German (equipped with online shallow 
compound recognition), French, Italian, and 
Spanish, which were compiled from the full form 
lexica of MMorph (Petitpierre and Russell, 1995). 
Considering Slavic languages, a component for 
Czech presented in (Haji?, 2001), and Morfeusz 
(Przepi?rkowski and Wolinski, 2003) for Polish. 
For Asian languages, we integrated Chasen 
(Asahara and Matsumoto, 2000) for Japanese and 
Shanxi (Liu, 2000) for Chinese.  
The XTDL-based grammar engineering plat-
form has been used to define grammars for 
English, German, French, Spanish, Chinese and 
Japanese allowing for named entity recognition 
and extraction. To guarantee a comparable 
coverage, and to ease evaluation, an extension of 
the MUC-7 standard for entities has been adopted.   
 
ne-person := enamex & [ TITLE list-of-strings, 
                          GIVEN_NAME list-of-strings, 
                          SURNAME list-of-strings, 
                                  P-POSITION list-of-strings, 
                          NAME-SUFFIX string, 
                                    DESCRIPTOR string ]. 
 
Given the expressiveness of XTDL expressions, 
MUC-7/MET-2 named entity types can be 
enhanced with more complex internal structures. 
For instance, a person name ne-person is defined 
as a subtype of enamex with the above structure. 
The named entity grammars can handle types 
such as person, location, organization, time point, 
time span (instead of date and time defined by 
MUC), percentage, and currency.  
The core system together with the grammars 
forms a basis for developing applications. SProUT 
is being used by several sites in both research and 
industrial contexts. 
A component for resolving coreferent named 
entities disambiguates and classifies incomplete 
named entities via dynamic lexicon search, e.g., 
Microsoft is coreferent with Microsoft corporation 
and is thus correctly classified as an organization. 
5 ExtraLink: Integrating Information 
Extraction and Automatic Hyperlinking  
A methodology for automatically enriching web 
documents with typed hyperlinks has been develo-
ped and applied to several domains, among them 
the domain of tourism information. A core compo-
nent is a domain ontology describing tourist sites 
in terms of sights, accommodations, restaurants, 
cultural events, etc. The ontology was specialized 
for major European tourism sites and regions (see 
Figure 1). It is associated with a large selection of  
 
 
 
Figure 1: Link Target Page (excerpt). The instance the 
web document is associated to (Isle of Capri) is shown 
on the left, together with neighboring concepts in the 
ontology, which the user can navigate through. 
 
link targets gathered, intellectually selected and 
continuously verified. Although language techno-
logy could also be employed to prime target 
selection, for most applications quality require-
ments demand the expertise of a domain specialist. 
In the case of the tourism domain, the selection 
was performed by a travel business professional. 
The system is equipped with an XML interface and 
accessible as a server. 
The ExtraLink GUI marks the relevant entities 
(usually locations) identified by SProUT (see 
second window on the left in Figure 2). Clicking 
on a marked expression causes a query related to 
the entity being shipped to the server. Coreferent 
concepts are handled as expanded queries. The 
server returns a set of links structured according to 
the ontology, which is presented in the ExtraLink 
GUI (Figure 2). The user can choose to visualize 
any link target in a new browser window that also 
shows the respective subsection of the ontology in 
an indented tree notation (see Figure 1).  
 
 
 
Figure 2: ExtraLink GUI. The links in the right-hand 
window are generated after clicking on the marked 
named entity for Lisbon (marked in dark). The bottom 
left window shows the SProUT result for ?Lissabon?. 
 
The ExtraLink demonstrator has been imple-
mented in Java and C++, and runs under both MS 
Windows and Linux. It is operational for German, 
but it can easily be extended to other languages 
covered by SProUT. This involves the adaptation 
of the mapping into the ontology and a multi-
lingual presentation of the ontology in the link 
target page. 
Acknowledgements 
Work on ExtraLink has been partially funded 
through grants by the German Ministry for 
Education, Science, Research and Technology 
(BMBF) to the project Whiteboard (contract 01 IW 
002), by the EC to the project Airforce (contract 
IST-12179), and by the state of the Saarland to the 
project SATOURN. We are indebted to Tim vor 
der Br?ck, Thierry Declerck, Adrian Raschip, and 
Christian Woldsen for their contributions to 
developing ExtraLink. 
References 
J. Allen, J. Davis, D. Krafft, D. Rus, and D. Subrama-
nian. Information agents for building hyperlinks. J. 
Mayfield and C. Nicholas: Proceedings of the Work-
shop on Intelligent Hypertext, 1993. 
M. Asahara and Y. Matsumoto. Extended models and 
tools for high-performance part-of-speech tagger. 
Proceedings of  COLING, 21-27, 2000. 
0 %HFNHU : 'UR G \ VNL +-U. Krieger, J. 
Piskorski, U. Sch?fer, F. Xu. SProUT?Shallow Pro-
cessing with Typed Feature Structures and Unifica-
tion. In Proceedings of  ICON, 2002. 
J. +DML? Disambiguation of rich inflection?compu-
tational morphology of Czech. Prague Karolinum, 
Charles University Press, 2001. 
H.-U. Krieger and U. Sch?fer. TDL?A Type Description 
Language for Constraint-Based Grammars. Procee-
dings of COLING, 893-899, 1994. 
H.-U. Krieger and J. Piskorski. Speed-up methods for 
complex annotated finite state grammars. DFKI 
Report, 2003. 
K. Liu. Research of automatic Chinese word segmen-
tation. Proceedings of ILT&CIP, 2001. 
D. Petitpierre and G. Russell. MMORPH?the Multext 
morphology program. Multext deliverable report 
2.3.1. ISSCO, University of Geneva, 1995. 
J. PiskRUVNL:'UR G \ VNL );X DQG2 6FKHUIA 
flexible XML-based regular compiler for creation 
and converting linguistic resources. Proceedings of 
LREC 2002, Las Palmas, Spain, 2002. 
A. Przepi?rkowski and M. Wolinski. The Unbearable 
Lightness of Tagging: A Case Study in Morphosyn-
tactic Tagging of Polish. Proceedings of the Work-
shop on Linguistically Interpreted Corpora, 2003. 
 
Ten Years After: An Update on TG/2 (and Friends)
Stephan Busemann
DFKI GmbH
Stuhlsatzenhausweg 3
D-66123 Saarbru?cken
busemann@dfki.de
Abstract
Since its first implementation in 1995, the shallow
NLG system TG/2 has been used as a component in
many NLG applications that range from very shal-
low template systems to in-depth realization en-
gines. TG/2 has continuously been refined, the Java
brother implementation XtraGen has become avail-
able, and the grammar development environment
eGram today allows for designing grammars on a
more abstract level. Besides a better understanding
of the usability of shallow systems like TG/2 has
emerged. Time has come to summarize the devel-
opments and look forward to new borders.
1 Introduction
Shallow NLG is known as ?quick and dirty? on the one hand,
and as a practical approach to implementing real-world ap-
plications on the other. Its legitimization stems from practi-
cal success rather than from theoretical advantages. As with
shallow analysis, methods have become acceptable that had
been rejected twentyfive years ago as linguistically unjusti-
fied. For instance, template-based NLG systems were known
to be unscalable and unflexible. Besides they were quite triv-
ial and did not contribute to solving any research questions
in the field. However, it became evident that many practical
applications involving NLG required limited linguistic cov-
erage, used canned text and/or templates, and badly needed
improvements to make the NLG systems more flexible. A
revival of template-based systems followed, and subsequent
scientific discussions clarified the relation to more advanced
NLG research themes. The papers in [Becker and Buse-
mann, 1999] nicely show a continuum between template- and
?plan?-based systems.
Since its first implementation in 1995, the shallow NLG
system TG/2 [Busemann, 1996] has been used as a compo-
nent in several diverse applications involving NLG. Imple-
mented in Common Lisp, TG/2 has continuously been re-
fined over the years; a Java brother implementation, called
XtraGen, has eventually become available, and the grammar
development environment eGram eventually allows the gram-
mar writer to design large-scale grammars.
Among the attractive properties of TG/2 is the quick de-
velopment of new NLG applications with limited require-
ments on linguistic expressiveness. Numerous implementa-
tions show that TG/2 is well suited for simple dialogues, re-
port generation (from database content), and even as a real-
izer for complex surface-semantic sentence representations.
Besides a better understanding of the pros and cons of TG/2
has emerged.
Time has come to summarize these developments and,
more generally, reassess the value of TG/2 as a framework
to specify generation systems.
In the following section, TG/2 is localized on the NLG
map, clarifying a few common misconceptions on what it can
be used for. In Section 3 we sketch major use cases involving
TG/2 that exhibit different degrees of ?shallowness?. Sec-
tion 4 summarizes the major extensions and refinements that
have been implemented over the last decade, taking into ac-
count some critical comments from the literature. We then
describe in Section 5 the need for, and the benefits of, the ded-
icated grammar development environment eGram that sup-
ports the fast developments of large rule sets. The paper con-
cludes with an outlook to upcoming work.
2 What TG/2 is and What it isn?t
TG/2 has been described originally in [Busemann, 1996;
Busemann and Horacek, 1998] as a template-based gener-
ator. To remind the reader of the main points, TG/2 is a
flexible production system [Davis and King, 1977] that pro-
vides a generic interpreter to a separate set of user-defined
condition-action rules representing the generation grammar.
The generic task is to map a content representation, which
must be encoded as a feature structure1, onto a chain of ter-
minal elements as defined by the rule set. The rules have a
context-free categorial backbone used for standard top-down
derivation guided by the input representation. The rules spec-
ify conditions on the input ? the so-called test predicates ?
that determine their applicability. Due to the context-free
backbone each subtree of depth 1 in a derivation tree cor-
responds to the application of one rule. TG/2 is equipped
with a constraint propagation mechanism that supports the es-
tablishment of agreement relations across the derivation tree.
Figure 1 shows a sample rule.
1A feature structure is either an atomic value, or a pair
[feature-name feature-value], where feature-name
is a string and feature-value a feature structure.
Figure 2: TG/2-based NLG applications Arranged on a Scale
From Shallow to In-Depth Generation.
TG/2 production rules has a simple interpretation proce-
dure that corresponds to the classical three-step evaluation
cycle in production systems (matching, conflict resolution,
firing) [Davis and King, 1977]. The algorithm starts from
a (piece of the) input structure and a category.
1. Matching: Select all rules carrying the current category.
Execute the tests for each of these rules on the input
structure and add those passing their test to the conflict
set.
2. Conflict resolution: Select an element from the conflict
set, e.g. on the basis of some conflict resolution mecha-
nism.
3. Firing: Evaluate its constraints (if any). For each right-
hand side element, read the category, determine the sub-
structure of the input, and goto step 1.
The processing strategy is top-down and depth-first. The set
of actions is fired from left to right. Failure of executing some
action causes the rule to be backtracked.
The right-hand side of a rule can consist of any mixture of
terminal elements (canned text) and non-terminal categories,
as in Figure 1. The presence of canned text is useful if the
input does not express explicitly everything that should be
generated. The grammar thus adds text to the output that does
not have an explicit semantic basis in the input. With very
detailed input and hence less ?implicit? semantics, only little
canned text will be needed in the grammar, and the terminal
elements of the grammar usually are word stems.
Canned parts of the grammar are ?invented?. This gives
rise to the notion of ?shallow generation?, as opposed to shal-
low analysis, where parts of the input text are ignored. TG/2
leaves complete freedom to using canned text, mixing it with
context free rules, or sticking to the more traditional distinc-
tion between (context-free) rules and the lexicon. [Busemann
and Horacek, 1998] refer to the former kind as shallow and to
the latter as in-depth generation. One may thus identify TG/2
applications on a scale ranging from more shallow to more
in-depth systems. Figure 2 attempts to compare some TG/2-
based NLG applications along this dimension. They will be
discussed in Section 3.
As mentioned above, there is no strict borderline between
template-based and plan-based generation systems. While
this insight resulted from comparing different systems, TG/2
implements this claim by forming a single framework that
may host any approach ranging from pure canned text to com-
pletely lexicon-based. As Section 3 demonstrates, TG/2 can
implement template-based systems and full-fledged realizers.
In an attempt to relate existing NLG systems to the RAGS
framework [Mellish et al, 2000], TG/2 was among the sys-
tems to look at. It turned out that TG/2 differs from the prin-
ciples underlying RAGS in that it does not support any of the
levels of conceptual, semantic, rhetoric, document or syntac-
tic representation, which were abstractly defined to capture
many (most) NLG approaches. Rather TG/2 entails a single
mapping from input to output, and any tasks generally as-
cribed to components delivering the above intermediate rep-
resentations must be encoded by one or several production
rules. There is no pipeline of modules with intermediate rep-
resentations, as ideally assumed in RAGS. Rather all tasks
need to be encoded within the production rules. During this
experiment it actually became evident that TG/2 isn?t a clas-
sical generation system at all.
In non-trivial NLG applications, TG/2 is complemented by
other components. On the output side it can be hooked up
to morphological inflection components using a shared rep-
resentation of word stems and morpho-syntactic features. On
the input side TG/2 has been combined with a text structuring
component in the TEMSIS application, with a context man-
agement system in COMET, and with a lexical choice com-
ponent in the MUSI system.
3 Major Use Cases
3.1 Template generation in the appointment
scheduling domain
Software agents communicated with human agents in order
to schedule appointments on behalf of their owners. Com-
municative goals to be verbalized as dialogue steps in the
COSMA system [Busemann et al, 1994] include just a few
speech acts for proposing, accepting, modifying, rejecting, or
confirming dates or date intervals. See [Busemann, 1996] for
a discussion and examples.
The event-oriented input is created by a non-linguistic
component, the scheduling agent system, and converted into a
surface-semantic representation, referred to as GIL in [Buse-
mann, 1996], which is verbalized by TG/2.
GIL was defined since the necessary distinctions at the lin-
guistic level are often based on information distributed all
over the input structure. For instance, the choice of prepo-
sitions depends on the choice of verbs, which is based on the
speech act. TG/2 can only access part of the input at a given
moment. Yet generating directly from event-oriented input
would have been possible at the cost of complex tests or con-
straints ? thereby affecting the transparency of the grammar
? or of more backtracking.
The necessary restructuring was implemented by simply
exploiting the expressive power of the directed acyclic graphs
used for feature structure input representation in TG/2. Using
co-references, relations between the event-based IN and the
language-oriented OUT feature of a set of semantic templates
covering all possible inputs were defined. An input is unified
with the IN feature, and TG/2 generates from the associated
OUT value.
An additional, practical reason for adopting an ?internal?
OUT representation is to encapsulate the generation gram-
mar, rendering it independent of external changes of the input
(defproduction "s2 top-subj.1"
(:PRECOND (:CAT DECL
:TEST ((sbp ?s2) (top-deep-subj ?y) (vc-voice ?active)))
:ACTIONS (:TEMPLATE (X1 :RULE ARG ?deep-subj)
(X2 :RULE FIN ?vc)
(X3 :RULE ARG ?deep-obj)
(X4 :OPTRULE INF ?verb-complex)
:CONSTRAINTS ( X1.CASE := ?nom
X3.CASE := ?acc
X1.NUMBER = X2.NUMBER = X4.NUMBER
X1.PERSON = X2.PERSON = X4.PERSON ))))
Figure 1: A rule for the German transitive main clause in the MUSI grammar in the format processed by TG/2. Tests specify
that this rule is applicable if the input suggests a certain syntactic structure called ?s2?, the subject should be in topic position,
and active voice is called for. The context-free rule underlying the rule is DECL? ARG FIN ARG {INF}. Path expressions
following the category such as deep-subj refer to substructures of the input, cf. Figure 3. Feature constraints assign nomina-
tive case to the first ARG and accusative case to the second; number and person are set to be equal on the first ARG and the verb
complex, thus establishing subject verb agreement. Notation: Constraint variables refer to right-hand side elements by virtue
of the indices Xi. The reserved index for the left-hand side is X0.
language, which are accommodated by the feature structure
mappings.
As was to be expected it turned out that GIL, as it stood,
was never reused. Instead other internal encodings were re-
quired, which could, however, be implemented straightfor-
wardly using the technique mentioned above.
3.2 Shallow multilingual generation from
non-linguistic input
Later projects required the verbalization of non-linguistic
domain-specific representations in multiple languages. In
Mietta, database content is verbalized in German, Finnish,
Italian and English as part of cross-language information re-
trieval [Xu et al, 2000]. A useful input representation is cre-
ated by applying a similar mechanism as in COSMA.
In COMET, TG/2 is used to generate personalized activity
recommendations in a conference scenario that differ with the
context consisting of interest and focus values, which form
part of the input [Geldof, 1999]. This application uses the
possibility of creating side-effects from applying a rule to
update a discourse memory whenever a discourse referent
is mentioned. It is indeed possible to create arbitrary side-
effects by function calls, but care has to be taken that these
functions can be called a second time during backtracking to
undo the side effects. As side-effects are rarely required, its
backtrack functionality is currently not supported and thus re-
quires explicit Lisp (and Java) programming.
In TEMSIS, air quality reports reports are generated from
a database containing measurement data [Busemann and Ho-
racek, 1998]. The communicative goal is interactively spec-
ified by the user: the type of report (time series, threshold
passing, etc.), the measuring station, the pollutant, a time in-
terval of interest, and some further options relating to con-
tent selection. The generated texts can include confirmations
about the user?s choices as well as canned paragraphs inform-
ing about the measuring station or the pollutant in question.
Corresponding views on previous periods are generated for
comparison. If the report is composed of multiple elements,
it is concluded by a summary that answers the key question
again.
A separate, language-independent component for text
structuring, accessing the database and calculating further
values (e.g., average values) was implemented. It produces
the actual inputs for TG/2, each corresponding to one para-
graph. All language-specific issues remained within the TG/2
grammar.
The TEMSIS project was designed to be used in a border
region of Germany and France. Thus the application initially
generated German and French texts. The grammars com-
prised about 120 and 100 rules, respectively. In order to find
out more details about the speed of grammar development
for such an application, and in particular the time it takes to
transport the application to a new language, native speakers
of English, Chinese, Japanese and Portuguese were asked to
produce a TG/2 grammar with the same coverage. Depending
on programming skills, it took between two and four person-
weeks to get acquainted with the system and to complete the
final tests set up. While this result was very encouraging, the
grammar writers stated that larger grammars would be less
easily developed and maintained.2
As [Busemann and Horacek, 1998] show, the input struc-
tures are non-linguistic, i.e., they do not uniquely deter-
mine the content-bearing linguistic elements and the sen-
tential structure to be used. These matters were defined in
co-operation with the users (cf. [Reiter et al, 1997]). The
agreed-upon pieces of text were entered as canned parts into
the grammar.
The grammars of Mietta, COMRIS and TEMSIS contain
much more canned parts than the COSMA grammar. This is
in direct correspondence to the nature of the respective input
representations (see Figure 2).
3.3 In-depth realization of surface-semantic
sentence representations
A much more in-depth use case for TG/2 is the generation
of German sentences that form part of cross-lingual sum-
2A demonstrator of the resulting multilingual system is online at
http://www.dfki.de/service/nlg-demo.
maries of scientific medical papers written in Italian or En-
glish (MUSI project, [Lenci et al, 2002]). The sentences ex-
hibit quite a complicated structure and much medical termi-
nology. Their average length in a sample corpus is 22 words.
The input structures (cf. Figure 3 for an example) are the re-
sults of a lexical and syntactic choice component [Busemann,
2002] that feeds TG/2. The structures contain specific refer-
ences to syntactic ?plans? (features SBP and NR). Test pred-
icates in the rules check for these features, thus realizing the
corresponding structure (cf. Figure 1). The morpho-syntacic
features for inflecting the lexical stems are collected through
the constraint mechanism and made available to the separate
word inflection components MORPHIX-3 [Finkler and Neu-
mann, 1988].
The input is a rather typical for linguistic realization, a task
initially not deemed suitable for systems like TG/2. Previous
applications show that TG/2 grammars are domain-dependent
and must be replaced when a new task is at at stake. [Buse-
mann and Horacek, 1998] consider this lack of reuse a dis-
advantage, but state that it is nevertheless acceptable since
new grammars can be developed very quickly. For realiza-
tion, however, a linguistically justified, domain-independent
grammar is needed that is expensive to develop but can be
reused across applications.
The parts of a grammar rule depending on input elements
can be isolated and treated as an interface between the gram-
mar and any input language. If an input language changes, the
test predicates and the access to input substructures need to be
recoded.3 This interface allows us to develop generic gram-
mar knowledge that abstracts from specific semantics of test
predicates and access details. We call such a generic grammar
a protogrammar, as it is supposed to form the reusable basis
for different instances geared towards different applications.
Technically, a protogrammar can be instantiated by defining
the test predicates and access methods needed for the input
language in question.
The protogrammar developed covers the main types of sen-
tential structures, as specified by the Duden grammar [Du-
denredaktion, 1998]. The NP syntax comprises prenomi-
nal APs (on the basis of adjective subcategorization frames),
generic possessive constructions, a temporal, a locative and
an adverbial modifier and a relative clause. In addition, nouns
and adjectives can subcategorize for specific arguments.
How could a protogrammar be developed independently
of a particular input language, as it needs testing? An in-
tuitive, minimal input representation would need to deter-
mine the depth of the nesting of constituents (as to avoid
endless recursion), specify morpho-syntactic features such as
case, number, tense etc., indicate the prepositions and distin-
guish the syntactic adjuncts at the sentence and NP level. Af-
ter defining a corresponding language, grammar development
could proceed in a way independent of the MUSI application.
When the other parts of the MUSI system became stable and
well-defined, the necessary adaptation of the input language
and the grammar were made. It goes without saying that the
3Changes may include restructuring and recoding of information.
Obviously if the input language encodes different kinds of informa-
tion, the grammar possibly cannot be reused.
(defproduction "parser-grammar"
(:PRECOND (:CAT ANALYSIS
:TEST ((always-true))
:ACTIONS (
:TEMPLATE (X1 :RULE PARSER ?self)
(X2 :RULE GRAMMAR ?self)
:CONSTRAINTS ( X0.LANG = X2.LANG
X1.API = X2.API ))))
Figure 4: A rule linking a parser and a grammar with com-
patible interfaces. The parser is language-independent, the
grammar is not. The LANG feature is specified in the input,
whereas the API feature is specified in the rules representing
the individual grammars.
grammar had to be extended to cover linguistic structures not
foreseen explicitly in [Dudenredaktion, 1998], but the addi-
tional effort was surprisingly small.4
The MUSI grammar comprises about 950 rules with 135
categories, and 14 features for constraints. A sample rule
is shown in Figure 1. During the development of this large
grammar the use of standard text editors became a nuisance.
A grammar development environment was designed and im-
plemented that supports multi-format development of large
grammars (see Section 5). With this system, all practical
needs arising from using TG/2 as a syntactic realizer could
be fulfilled.
3.4 Other usage
TG/2 is general enough to be usable for other tasks than NLG.
A sample grammar for software configuration has been writ-
ten using the constraint mechanism to define API properties
of software components (the ?lexicon?) and matching con-
ditions for components to be integrated into a larger piece
of software (the ?grammar?). The input is a specification of
the desired system (e.g., machine translation from Spanish to
English), and the system would enumerate the possible spec-
ifications it can derive. A sample rule is shown in Figure 4.
4 Modifications and Extensions
The experience gained from the various applications sug-
gested some modifications and extensions to the system. Also
a closer look at comparable systems, most importantly YAG
[McRoy et al, 2003], revealed opportunities for improve-
ment. YAG differs from TG/2 in that it is deterministic and
thus does not search. Every next rule to be chosen is depicted
in the input or identified by a preceding table lookup. There-
fore YAG is probably faster than TG/2 since TG/2 lets the
interpreter select the next rule. On the other hand, as we will
see in Section 4.2, this gives TG/2 some flexibility YAG does
not exhibit: TG/2 output can vary according to non-linguistic
parameters.
The need for backtracking associated with search can be
kept small in practice. Moreover the costs are small since
TG/2 reuses previously generated substrings during back-
tracking, as described in [Busemann, 1996]. In practice an-
4The author?s guess is that adaptation work required about 20%
of the overall effort; unfortunately no reliable figures are available.
[(SENTENCE DECL)
(VC [(SBP S2) ;;name of sentence plan
(G AKTIV) ;;active voice
(STEM "verursach")])
(DEEP-OBJ [(DET DEMONST) (STEM "wirkung")])
(DEEP-SUBJ [(TOP Y) ;;this constituent to the fore-field
(DET INDEF) ;;indefinite article
(NR V2) ;;name of nominal plan
(STEM "antagonismus")
(PP-ATR [(MODALITY
[(PP-OBJ
[(TERM [(DET DEF) ;;definite article
(STEM "bindungsstelle")
(ADJ [(STEM "muskarinisch") (DEG POS)])
(TERM [(DET DEMONST1) ;;demonstrative
(STEM "substanz")])])
(STEM "Niveau") (DET DEF)
(PREP AUF-DAT)])]) ;;P governs dative NP here
(STEM "acetylcholin")
(DET WITHOUT) ;;no article
(PREP ZU)]) ;;this P always governs dative NP
(ADJ [(STEM "kompetitiv") (DEG POS)])])]
Figure 3: A TG/2 MUSI input for ?Ein kompetitiver Antagonismus zu Acetylcholin auf dem Niveau der muskarinischen
Bindungsstellen dieser Substanzen verursacht diese Wirkungen.? [?These effects are caused by a competitive antagonism with
acetylcholine on the level of the muscarinic sights of these substances.?]. Comments are separated by semicolons. The structure
is simplified by omitting gender, number, mood and phrase type information.
other cost factor turned out to be sensible, namely the number
of rules to be checked in each cycle. In experiments with au-
tomatic rule generation using meta-rules in eGram, [Rinck,
2003] showed a linear increase of TG/2 runtime with the
number of rules per left-hand side category.
With large grammars such as in MUSI, which has about
950 rules, it is important to reduce the number of alterna-
tive rules. This can be achieved by using optional right-hand
side elements, thus covering many possible verbalizations by
a single rule. The semantics of optional right-hand side ele-
ments has been refined to capture this idea fully; they must be
verbalized if and only if there is input for them. A right-hand
side element failing on non-empty input causes the parent rule
to fail.
4.1 Reducing the need for programming
In comparing YAG and TG/2, [McRoy et al, 2003, p. 417]
observe that ?YAGs template language is also more declara-
tive, yielding higher maintainability and comprehensibility?.
While they do not point out details, it is true that defining
TG/2 rules requires some Lisp programming. The test predi-
cates have to be defined and the defined ones have to be called
properly, and, in the version reviewed, the access functions to
relevant parts of the input have to be specified. Calling an
access function such as (theme) should return some part
of the input structure that is accessible at the current state of
processing. The function would encapsulate the way this is
achieved. Access functions may fail, in which case the parent
rule may fail.
A number of frequently used general test predicates such
as testing the presence of a feature at a certain location in the
input structure, equality of some feature value with a given
object, or a list element being the but last one in a list, can
be held on offer for grammar developers. Usually most tests
can be carried out using one of these, but new demands need
programming. Since the structure of the Boolean test predi-
cates is simple, such tasks are not difficult to solve. eGram
offers support as for Java all the embedding code such as ex-
ception handling is provided and only the core condition must
be written.
Access functions were met with some disgust by grammar
writers as new ones are required with the change of the input
language, i.e. with any new generation task. It turned out that
the advantage of having the access to input material encapsu-
lated did not pay off. The implied possibility of reorganizing
or renaming input was never used, as other ways to do this
were preferred (cf. [Busemann, 1999]). Instead just relative
path descriptions were implemented. The need to provide ac-
cess functions in both Lisp and Java eventually gave rise to
a uniform solution: now a single format for relative path de-
scriptions is used in eGram as a source code that is compiled
into Lisp and Java expressions to serve the runtime systems.
Hence the rule in Figure 1 now has a feature path on each
right-hand side element instead of function calls, as in [Buse-
mann, 1996].
4.2 Generating Personalized Text
Given a certain input, different outputs may well desirable for
different users. Some examples:
? A user may be an expert or a novice in the topic at stake.
Expert users will read terminology whereas novices
need explanations or circumscriptions.
? Depending on whether a user is interested in receiving
background information, relevant hyperlinks may be in-
serted into the text.
? When text is generated for display on a hand-held de-
vice, it must be organized and presented differently.
Sometimes the components producing input for the generator
are not capable of accounting for the respective linguistic dif-
ferences since they don?t have a model of the grammar at their
disposal. A mechanism is needed to feed the system with pa-
rameters corresponding to such distinctions and to translate
the parameter settings into appropriate decisions in the gen-
eration process. For this purpose the approach to parameteri-
zation introduced in [Busemann, 1996] has been refined.
First and foremost, all variations that could be generated
for a given input must be covered by the grammar. Then the
system would produce the complete set, one by one. The
grammar writer defines, in cooperation with the application
developer, parameters such as expertise, background, and de-
vice with appropriate values. She tags all rules that exhibit
properties of some parameter value. Then the system can se-
lect a rule according to a single parameter. But parameters
may be in conflict as well. TG/2 offers a linear preference
scheme for the defined parameters implemented in step 2 of
the basic algorithm. The grammar writer defines, in coopera-
tion with the application developer, a partial order describing
the relative importance of the parameters. With this scheme
the system can produce a text that conforms best to the user?s
preferences.
The scheme is best explained using an example of two
parameters. Let us assume that the user chooses ?non-
expert? text with background information. Assume that
a conflict set contains the following tagged rules: {R1-
[expertise: expert, background: -], R2-
[expertise: non-expert, background: -],
R3-[expertise: expert, background: +]}.
None of the tags matches exactly the specifications. If the
parameter expertise is defined to be more important than
background, R2 will be selected. If, however, background is
preferred over expertise, R3 is applied. [Busemann, 1998]
has a more detailed description of this idea.
While subsequent experiments seem to show the viability
of this simple approach to let other components (or the user,
via a task interface) influence the system behavior5, a real
test will probably consist in its envisaged usage for answer
presentation in Semantic Web contexts.
5 Grammar Development
The development of small grammars with 100 to 200 rules
such as the ones underlying COSMA, TEMSIS, Mietta or
COMET could safely be developed with standard text edi-
tors using the syntax exemplified in Figure 1. However even
in this work, the difficulty of maintenance and a considerable
error-proneness were observed. With the MUSI grammar, a
dimension was reached that made a dedicated grammar devel-
opment environment necessary. While some abstraction from
5Parameters should not depend on each other to guarantee that
the best version is generated first, cf. the discussion in[Busemann,
1996, Section 5].
the Lisp-like rule format was desirable, the Java implemen-
tation XtraGen [Stenzhorn, 2002] required a different format
anyway, as it is consistently using XML to encode all objects.
eGram [Busemann, 2004] was hence designed to develop-
ing grammars without bothering about their syntax or size or
interpreting NLG system. Major benefits of eGram include
? a developer-friendly grammar format,
? syntactic and semantic checks of grammar knowledge,
? the option to derive additional grammar rules by meta-
rules, and
? integration with grammar testing in generation systems.
A major difficulty in the course of developing the MUSI
grammar was to maintain consistency. Features used are
sometimes not defined, values are not sufficiently restricted,
or certain categories do not occur in any other rule. When
such grammars are interpreted, errors occur that can be dif-
ficult and time-consuming to trace. eGram verifies that ev-
ery new piece of grammar knowledge is fully consistent with
what already exists, thus eliminating many obvious sources
of mistake.
eGram allows the definition of complex objects only after
all their elements are defined. Before a rule may be entered,
the categories, test predicates, access paths and constraints
used must be defined. The eGram GUI offers dynamically
generated menus for more complex elements in addition to
textual input windows, where these remain necessary. For the
definition of e.g. a constraint, a menu would offer all defined
features, and for the selected feature, all defined values.
Different working styles are supported: either the grammar
writer pro-actively plans her work by first defining all low-
level elements and then proceeding to higher-level ones, or
she prefers to add missing elements ?on the fly?, i.e. when
eGram complains.
eGram?s main pane contains a set of tabs corresponding to
the different elements. Clicking on a tab opens a new screen
with all the tabs remaining available at any moment (see Fig-
ure 5). A set of tabs opens separate sub-panes allowing for
the definition of the tests, RHS elements, and constraints of
rules.
In MUSI the major disadvantage of context-free grammars
posed a problem. The rules cannot easily express certain lin-
guistic phenomena, such as word order variation, pronom-
inalization, voice, the relation between sentential structures
and relative clauses, or verb positions. To cover these phe-
nomena, several hundreds, if not thousands, of different rules
must be defined. Every-day practice involved copy-and-paste
approaches that are error-prone. Moreover such phenomena
are often captured only partially, leaving unknown gaps in the
coverage of the grammar.
eGram is equipped with a meta-rule mechanism that is
technically similar to that of Generalized Phrase Structure
Grammars [Gazdar et al, 1985]. Meta-rule expansion starts
with a set of base rules and then applies to the set of base
rules and derived rules. Meta-rules serve as an abbreviation
technique and do not affect the expressive power of the sys-
tem. The basic meta-rule mechanisms and their integration
into eGram are described in detail in [Rinck, 2003]. A re-
design of the MUSI grammar led to a reduction to 452 base
Figure 5: A Screenshot of eGram with the Rule Pane Active. It displays a simple NP rule for German. The feature constraints
express various agreement relations. The rule window can be dragged to some other location on the screen, allowing to view
multiple objects at the same time. The rules names shown on the left-hand side can be filtered by the elements contained in the
rules. For instance by selecting category NP, only the rules with NP as their LHS category are shown.
rules. Applying to these base rules 19 meta-rules modeling
the above phenomena resulted in 2.488 derived rules, demon-
strating that the original grammar did in fact not systemati-
cally cover all the phenomena represented by the meta-rules.
Integrating grammar development and grammar testing is
crucial to verify the effects of modifying a grammar. eGram
is implemented in Java and integrated with TG/2 via a client-
server interface. The integration with XtraGen is achieved via
a Java API. eGram provides suitable export formats for both.
Calls to the generators can be issued from within eGram. A
call to a running generation system consists of an input struc-
ture that can be defined within eGram, and the modifications
of the grammar since the last call. The generator either re-
turns the generated string or an error message.
6 Conclusions and Outlook
TG/2 has been used continuously for more than ten years.
From its first appearance as a ?template generator? it has
evolved into a framework that accommodates both shallow
template-based generation and in-depth realization tasks. In
combination with the grammar development environment
eGram, large grammars can be developed and maintained.
They can be used by both TG/2 in Lisp and XtraGen in Java.
To take up the comparison with YAG again, the most im-
portant difference is perhaps the way the rules are defined.
YAG uses complex nested conditionals covering alternative
verbalizations, whereas TG/2 sticks to production rules based
on a context-free backbone that license local trees in a deriva-
tion.
The RAGS experiment showed that comparing TG/2 with
in-depth NLG systems proves difficult. TG/2 remains shal-
low in that it does not support complex interrelated NLG tasks
such as lexical choice, aggregation, or the generation of refer-
ring expressions.
Future applications of TG/2 are geared towards presenting
personalized summaries about multilingual results of ques-
tion answering, generating meaningful and consistent anno-
tations of objects in the process of modeling software, and
producing user manuals for technical devices in multiple lan-
guages.
Though (or because) it has matured for more than a decade,
TG/2 is alive and kicking.6
Acknowledgments
This work was partially supported by a research grant from
the German Bundesministerium fu?r Bildung und Forschung to
the project COLLATE-II (contract 01 IN C02). I am indebted
to many people who have contributed to the different projects.
Space limitations permit only to list those who implemented
and/or documented major parts of the TG/2 and eGram sys-
tems, or of the grammars: Ana ?Agua, Tim vor der Bru?ck,
Matthias Gro?klo?, Eelco Mossel, Matthias Rinck, Joachim
6TG/2 has been licensed to more than 30 sites for commercial,
research and educational purposes. TG/2 and eGram are available
from DFKI GmbH. The licensed software includes a user manual
for TG/2 and a guidebook for writing grammars in eGram. XtraGen
can be licensed from XtraMind GmbH.
Sauer, Holger Stenzhorn and Michael Wein. Special thanks
go to Sabine Geldof, who was a patient and inspiring user
of TG/2 during the COMRIS project. Her feedback helped
making TG/2 usable.
References
[Becker and Busemann, 1999] Tilman Becker and Stephan
Busemann, editors. May I Speak Freely? Between Tem-
plates and Free Choice in Natural Language Generation.
Workshop at the 23rd German Annual Conference for Arti-
ficial Intelligence (KI ?99). Proceedings, Document D-99-
01, 1999.
[Busemann and Horacek, 1998] Stephan Busemann and
Helmut Horacek. A flexible shallow approach to text
generation. In Eduard Hovy, editor, Nineth International
Natural Language Generation Workshop. Proceedings,
pages 238?247, Niagara-on-the-Lake, Canada, 1998.
[Busemann et al, 1994] Stephan Busemann, Stephan
Oepen, Elizabeth Hinkelman, Gu?nter Neumann, and Hans
Uszkoreit. COSMA?multi-participant NL interaction for
appointment scheduling. Technical Report RR-94-34,
DFKI, Saarbru?cken, 1994.
[Busemann, 1996] Stephan Busemann. Best-first surface re-
alization. In Donia Scott, editor, Eighth International
Natural Language Generation Workshop. Proceedings,
pages 101?110, Herstmonceux, Univ. of Brighton, Eng-
land, 1996.
[Busemann, 1998] Stephan Busemann. A shallow formal-
ism for defining personalized text. In Proceedings
of Workshop Professionelle Erstellung von Papier- und
Online-Dokumenten: Perspektiven fu?r die automatische
Textgenerierung. 22nd Annual German Conference on Ar-
tificial Intelligence (KI ?98), Bremen, Germany, 1998.
[Busemann, 1999] Stephan Busemann. Constraint-based
techniques for interfacing software modules. In Chris Mel-
lish and Donia Scott, editors, Proc. of the AISB?99 Work-
shop on Reference Architectures and Data Standards for
NLP, pages 48?54, University of Edinburgh, Scotland,
April 1999. The Society for the Study of Artificial Intel-
ligence and Simulation of Behgaviour.
[Busemann, 2002] Stephan Busemann. Language genera-
tion for cross-lingual document summarisation. In Huanye
Sheng, editor, International Workshop on Innovative Lan-
guage Technology and Chinese Information Processing
(ILT&CIP-2001), April 6-7, 2001, Shanghai, China, Bei-
jing, China, 2002. Science Press, Chinese Academy of Sci-
ences.
[Busemann, 2004] Stephan Busemann. eGram ? a grammar
development environment and its usage for language gen-
eration. In Proc. Fourth International Conference on Lan-
guage Resources and Evaluation (LREC), Lisbon, Portu-
gal, May 2004.
[Davis and King, 1977] Randall Davis and Jonathan King.
An overview of production systems. In E. W. Elcock and
D. Michie, editors, Machine Intelligence 8, pages 300?
332. Ellis Horwood, Chichester, 1977.
[Dudenredaktion, 1998] Die Dudenredaktion. Duden.
Die Grammatik. Grammatik der deutschen Gegen-
wartssprache, volume 4 of Duden - Das Standardwerk
zur deutschen Sprache. Dudenverlag, Mannheim - Wien
- Zu?rich, 6. edition, 1998.
[Finkler and Neumann, 1988] Wolfgang Finkler and Gu?nter
Neumann. Morphix: A fast realization of a classification?
based approach to morphology. In H. Trost, editor, Pro-
ceedings der 4. ?Osterreichischen Artificial?Intelligence
Tagung, Wiener Workshop Wissensbasierte Sprachverar-
beitung, pages 11?19, Berlin, August 1988. Springer.
[Gazdar et al, 1985] Gerald Gazdar, Ewan Klein, Geoffrey
Pullum, and Ivan Sag. Generalized Phrase Structure
Grammar. Basil Blackwell, London, 1985.
[Geldof, 1999] Sabine Geldof. Templates for wearables in
context. In Becker and Busemann [1999], pages 48?51.
[Lenci et al, 2002] Alessandro Lenci, Ana ?Agua, Roberto
Bartolini, Stephan Busemann, Nicoletta Calzolari, Em-
manuel Cartier, Karine Chevreau, and Jose? Coch. Multi-
lingual summarization by integrating linguistic resources
in the MLIS-MUSI project. In Proc. Third Interna-
tional Conference on Language Resources and Evaluation
(LREC), pages 1464?1471, Las Palmas, Canary Islands,
Spain, May 2002.
[McRoy et al, 2003] Susan W. McRoy, Songsak Chan-
narukul, and Syed S. Ali. An augmented template-based
approach to text realization. Natural Language Engineer-
ing, 9(4):381?420, 2003.
[Mellish et al, 2000] Chris Mellish, Roger Evans, Lynne
Cahill, Christy Doran, Daniel Paiva, Mike Reape, Donia
Scott, and Neil Tipper. A representation for complex and
evolving data dependencies in generation. In Proceedings
of the Sixth Applied Natural Language Processing Confer-
ence (ANLP-NAACL), pages 119?126, Seattle, Washing-
ton, USA, 2000.
[Reiter et al, 1997] Ehud Reiter, Alison Cawsey, Liesl Os-
man, and Yvonne Roff. Knowledge acquisition for content
selection. In Proceedings of the 6th European Workshop
on Natural Language Generation (ENLGWS-97), pages
117?126, Duisburg, 1997.
[Rinck, 2003] Matthias Rinck. Ein Metaregelformalismus
fu?r TG/2. Master?s thesis, Department for Computational
Linguistics, University of the Saarland, 2003.
[Stenzhorn, 2002] Holger Stenzhorn. XtraGen. A natural
language generation system using Java and XML tech-
nologies. In Proceedings of the Second Workshop on NLP
and XML, Taipeh, Taiwan, 2002.
[Xu et al, 2000] Feiyu Xu, Klaus Netter, and Holger Sten-
zhorn. MIETTA - a framework for uniform and multilin-
gual access to structured database and web information. In
Proceedings of the 5th International Workshop on Infor-
mation Retrieval with Asian Languages (IRAL?00), Hong
Kong, 2000.
Annotating text using the Linguistic Description Scheme of MPEG-7: 
The DIRECT-INFO Scenario 
 
 
Thierry Declerck, Stephan Busemann 
Language Technology Lab  
DFKI GmbH  
Saarbr?cken, Germany 
{declerck|busemann}@dfki.de 
Herwig Rehatschek, Gert Kienast 
Institute for Information Systems & 
Information Management,  
JRS GmbH  
Graz, Austria 
{rehatschek|kienast}@joanneum.at 
 
 
Abstract 
We describe the way we adapted a text 
analysis tool for annotating with the Lin-
guistic Description Scheme of MPEG-7 
text related to and extracted from multi-
media content. Practically applied in the 
DIRECT-INFO EC R&D project we 
show how such linguistic annotation con-
tributes to semantic annotation of multi-
modal analysis systems, demonstrating 
also the use of the XML schema of 
MPEG-7 for supporting cross-media se-
mantic content annotation. 
1 Introduction 
In the R&D project DIRECT-INFO the concrete 
business case of sponsorship tracking was tar-
geted. The scenario investigated within the pro-
ject was that sponsors want to know how often 
their brands are mentioned in connection with 
the sponsored company. The visual detection of a 
brand (e.g. in videos) is not sufficient to meet the 
requirements of this business case. Multimodal 
analysis and fusion ? as implemented within DI-
RECT-INFO ? is needed in order to fulfill these 
requirements (Rehatschek, 2004).  
Within this context text analysis has been ap-
plied to documents reporting on entities, like 
football teams, that have close relations to large 
sponsoring companies. In the text analysis com-
ponent of the system we had to detect if an entity 
was mentioned positively, negatively or neu-
trally. Besides all the processing and annotation 
issues to positive or negative mentions, we had 
to make our results available to a global MPEG-7 
document, which is encoding the annotation re-
sults of various analysis of the modalities in-
volved (logo detection, speech recognition, text 
analysis etc.). This global MPEG-7 document 
was the input for a fusion component. 
In the next sections we describe the Text 
Analysis (TA) component of DIRECT-INFO. 
We then briefly describe the linguistic descrip-
tion scheme (LDS) of MPEG-7 and show the 
annotation generated by the TA. Finally we 
briefly discuss the role the LDS, and generally 
speaking MPEG-7, can play in supporting an 
interoperable cross-media annotation strategy. It 
seems to us, that LDS is offering a good mean 
for adding semantic metadata to image/video, but 
not for a real semantic integration of text and 
media content annotation, which in the case of 
DIRECT-INFO was performed by an additional 
fusion component. 
2 The detection of positive/negative 
mentioning 
Our work in DIRECT-INFO has been dedicated 
in enhancing an already existing tool for linguis-
tic annotation. This tool, called SCHUG (Shal-
low and CHunk-based Unification Grammar 
tool), is annotating texts considering both lin-
guistic constituency and dependency structures 
(T. Declerck, M. Vela 2005). 
A first development step was dedicated in cre-
ating specialized lexicons for various types of 
lexical categories (like nouns, adjectives and 
verbs) that can bear the property of being intrin-
sically positive or negative in a specific domain, 
as can be seen just below in the case of soccer: 
 
command => {POS => Noun, INT => "positive"} 
dominate => {POS => Verb, INT => "positive"} 
weak => {POS => Adj, INT => "negative"} 
 
Considering a sentence like ?ManU takes the 
command in the game against the weak Spanish 
53
team?, the head-noun of the direct object (lin-
guistically speaking) ?the command? gets from 
the access to the specialized DIRECT-INFO 
lexicon a tag ?INTERPRETATION? with value 
?positive?. Whereas the adjective ?weak? in the 
PP-adjunct ?in the game against the weak Span-
ish team? gets an ?INTERPRETATION? tag 
with value ?negative?.  
Once the words in the sentence have been 
lexically tagged with respect to their interpreta-
tion, the computing of the pos./neg. interpreta-
tion at the level of linguistic fragments and then 
at the level of the sentences can start. For this we 
have defined heuristics along the lines of the de-
pendency structures delivered by the linguistic 
analysis. So in the case of the NP ?the weak 
Spanish team?, the head noun ?team?, as such a 
neutral expression, is getting the ?INTERPRE-
TATION? tag with the value ?negative?, since it 
is modified by a ?negative? adjective. In case the 
reference resolution algorithm of the linguistic 
tools has been able to specify that the ?Spanish 
team? is in fact ?Real Madrid? this entity gets a 
negative ?INTERPRETATION? tag. 
The head noun of the NP realizing the subject 
of the sentence, ?ManU? gets a positive mention 
tag, since it is the subject of a positive verb and 
direct object combination (the NP ?the com-
mand? having a positive reading, whereas the 
verb ?takes? has a neutral reading). 
A last aspect to be mentioned here concerns 
the treatment of the so-called polarity items. 
Specific words in natural language intrinsically 
carry a negation or position force (or scope). So 
the words not, none or no have an intrinsic nega-
tion force and negate the words and fragments in 
the context in which those specific words are 
occurring. The context that is negated by such 
words can be also called the ?scope? (or the 
range) of the negation. Consider for example the 
sentence: ?I would definitely pay ?15 million to 
get Owen, not even a decent striker, instead?? 
Our tools are able to detect that the NP ?decent 
striker? is negated, and therefore the positive 
reading of ?decent striker? is being ruled out. 
3 Metadata Description 
The different content analysis modules of the 
DIRECT-INFO system extract different types of 
metadata, ranging from low-level audiovisual 
feature descriptions to semantic metadata. The 
global metadata description must be rich and has 
to clearly interrelate the various analysis results, 
as it is the input of the fusion component. 
4.1 Using MPEG-7 for Detailed Description of 
Audiovisual Content 
In DIRECT-INFO the MPEG-7 standard is used 
for metadata description. It is an excellent choice 
for describing audiovisual content because of its 
comprehensiveness and flexibility. The compre-
hensiveness results from the fact that the stan-
dard has been designed for a broad range of ap-
plications and thus employs very general and 
widely applicable concepts. The standard con-
tains a large set of tools for diverse types of an-
notations on different semantic levels. The flexi-
bility of MPEG-7, which is provided by a high 
level of generality, makes it usable for a broad 
application area without imposing strict con-
straints on the metadata models of these applica-
tions. The flexibility is very much based on the 
structuring tools and allows the description to be 
modular and on different levels of abstraction. 
MPEG-7 supports fine grained description, and it 
is possible to attach descriptors to arbitrary seg-
ments on any level of detail of the description.  
Among the descriptive tools developed within 
the MPEG-7 framework, one is concerned with 
the use of natural language for adding metadata 
to the content description of image and video: the 
so-called Linguistic Description Scheme (LDS). 
4.2 MPEG-7: The Linguistic Description 
Scheme (LDS) 
MPEG-7 foresees four kinds of textual annota-
tion that can be attached as metadata to some 
audio-video content. The natural language ex-
pression used here is ?Spain scores a goal against 
Sweden. The scoring player is Morientes?. 
Free Text Annotation: Here only tags are put 
around the text: 
<TextAnnotation> 
   <FreeTextAnnotation xml:lang="en"> 
   Spain scores a goal against Sweden. 
   The scoring player is Morientes. 
   </FreeTextAnnotation> 
</TextAnnotation> 
 
Key Word Annotation: Key Words are ex-
tracted from text and correspondingly annotated: 
<TextAnnotation> 
   <KeywordAnnotation> 
     <Keyword>score</Keyword> 
     <Keyword>Sweden</Keyword> 
     <Keyword>Spain</Keyword> 
     <Keyword>Morientes</Keyword> 
   </KeywordAnnotation> 
</TextAnnotation> 
 
 
 
54
Structured Annotation: Question/Answering 
like semantics is associated to the text: 
<TextAnnotation> 
  <StructuredAnnotation> 
    <Who><Name>Spain</Name></Who> 
    <WhatAction><Name>score      
goal</Name></WhatAction> 
    <Where><Name>A Coru?a, 
Spain</Name></Where> 
    <When><Name>March 25, 
1998<Name></When> 
  </StructuredAnnotation> 
</TextAnnotation> 
 
Dependency Structure: Here the full linguis-
tic apparatus is used for annotating the text: 
<TextAnnotation> 
 <DependencyStructure> 
  <Sentence> 
   <Phrase operator="subject"> 
    <Head type="noun">Spain</Head> 
   </Phrase> 
   <Head type="verb" base-
Form="score">scored</Head> 
   <Phrase operator="object"> 
    <Head type="article noun">a 
goal</Head> 
   </Phrase> 
   <Phrase> 
    <Head 
type="preposition">against</Head> 
   <Phrase> 
    <Head>Sweden</Head></Phrase> 
   </Phrase> 
  </Sentence> 
 </DependencyStructure> 
</TextAnnotation>1 
4 MPEG-7 Format of the Text Analysis 
component in DIRECT-INFO 
On the base of the linguistic analysis of our de-
pendency  parser, we generate the ?structured 
annotation? of the MPEG-7 Linguistic Descrip-
tion Scheme. We think that this kind of annota-
tion is the most practical of LDS for adding se-
mantics to multimedia content, since it is proba-
bly more intuitive for the media expert as the 
underlying linguistic dependency structure. At 
the same  time it seems also straightforward to 
go first for a (internal) dependency analysis, 
since it is then relatively easy to map automati-
cally dependency units to the ?Who?, ?WhatAc-
tion? and other tags of LDS. 
The MPEG-7 output of the TA module of DI-
RECT-INFO looks like: 
 
<MediaInformation> 
 <MediaProfile> 
  <MediaFormat> 
   <Content href="http://www.direct-
info.net/mpeg7/cs/ContentCS.2004.xml/di.
content.writtenText"> 
    <Name>Written text</Name> 
                                                 
1
 These examples are taken from a former and excellent 
online tutorial on MPEG-7 by Philippe Salembier.  
   </Content> 
  </MediaFormat> 
  <MediaInstance> 
   <InstanceIdentifier/> 
   <MediaLocator> 
    <!-- essence id--> 
    <MediaUri>5543</MediaUri> 
   </MediaLocator> 
  </MediaInstance> 
 </MediaProfile> 
</MediaInformation> 
<StructuralUnit href="http://www.direct-
info.net/mpeg7/cs/StructuralUnitCS.2004.
xml/di.vis.pdf"> 
 <Name>PDF</Name> 
</StructuralUnit> 
 <!-- more than one page can be stored  
within a file --> 
<SpatialDecomposition criteria="Page"> 
 <StillRegion id="TA_PAGE1"> 
  <StructuralUnit 
href="http://www.direct- 
info.net/mpeg7/cs/StructuralUnitCS.2004.
xml/di.vis.page"> 
   <Name>Page</Name> 
  </StructuralUnit> 
 <SpatialDecomposition   crite-
ria="TextAnalysis" gap="true" over-
lap="false"> 
  <StillRegion> 
   <StructuralUnit 
href="http://www.direct-
info.net/mpeg7/cs/StructuralUnitCS.2004.
xml/di.vis.textAnal  ysisAnnotation"> 
    <Name>Text analysis annota-
tion</Name> 
   </StructuralUnit> 
   <TextAnnotation> 
    <StructuredAnnotation> 
     <WhatObject 
href="http://www.direct-
info.net/mpeg7/cs/LogoCS.2004.xml/di.ta.
object.juventus"> 
      <Name 
xml:lang="it">Juventus</Name> 
     </WhatObject 
     <WhatAction 
href="http://www.direct-
info.net/mpeg7/cs/TextAnalysisCS.2004.xm
l/di.ta.action.teamMentioned"> 
<Name xml:lang="it">mentioning of 
team</Name> 
     </WhatAction> 
     <Why> 
      <Name xml:lang="it"> 
295 771120 Con DVD Auto da Sogno Porsche 
e 10, con calendario ufficiale 2006 Ju-
ventus o Milan" o Inter o Palermo o 
Fiorentina o Totti" o Wrestling" e 6, 9 
Euro 1, Poste Italiane Sped . in A.P 
      </Name> 
     </Why> 
     <How href="http://www.direct-
info.net/mpeg7/cs/TextAnalysisCS.2004.xm
l/di.ta.mentioning.neut"> 
     <Name xml:lang="it">neut</Name> 
   </How> 
  </StructuredAnnotation> 
 </TextAnnotation> 
</StillRegion> 
 
Without going into too much detail here, it is 
enough to stress that in the first part of the anno-
tation, the link to the general multimedia and 
multimodal repository is ensured. We have to 
55
deal with a PDF document that should be proc-
essed by a Text Analysis tool. The ?essence? ID 
is giving information about the location where 
the application relevant data is stored and where 
the results of the Text Analysis should be stored. 
All this metadata is ensuring the combination of 
the results of the analysis of various modalities 
dealing with one application relevant dataset (for 
example the combination of the logo detection of 
a brand and the related positive or negative men-
tioning of a team sponsored by this brand).  For 
reason of place, we can not show and comment 
here the complete (and multimodal) MPEG-7 
annotation, but details are given in (G. Kienast, 
2005). 
The second part of the annotation gives the re-
sults of the combined linguistic and ?structured? 
analysis we are dealing with. As mentioned 
above, in the case of DIRECT-INFO, results of 
text analysis are accessed via the structured an-
notation of the Linguistic Description Schema of 
MPEG-7. 
5 Conclusions and future Work 
In the DIRECT-INFO project we managed to 
include results of text analysis in an automated 
fashion into a MPEG-7 description, which was 
dealing with the XML representation of the 
analysis of various modalities. Using correspond-
ing metadata, it was possible to ensure the en-
coding/annotation of the related results in one 
file and to facilitate the access to the separated 
annotation using XPath. As such the DIRECT-
INFO MPEG-7 annotation schema is offering a 
practicable multi-dimensional annotation 
scheme, if we consider a ?dimensions? as being 
the output of the analysis of various modalities. 
MPEG-7 proved to be generic and flexible 
enough for combining, saving and accessing 
various types of annotation.  
Limitations of MPEG-7 were encountered 
when the task was about fusion or merging of 
information encoded in the various descriptors 
(or features), and this task was addressed in a 
posterior step, whereas the encoding scheme of 
MPEG-7 was not longer helpful, in defining for 
example relations between the annotation result-
ing from the different modules or for defining 
constraints between those annotation. There 
seems to be a need for a higher level of represen-
tation for annotation resulting from the analysis 
of distinct media, being low-level features for 
images or high-level semantic features for texts.  
The need of  an ?ontologization? of multime-
dia features has been already recognized and pro-
jects are already dealing with this, like AceMe-
dia. Initial work in relating multimodal annota-
tion in DIRECT-INFO will be further developed 
in K-Space, a new Network of Excellence, which 
goal is to provide for support in semantic infer-
ence for both automatic and semi-automatic an-
notation and retrieval of multimedia content. K-
Space aims at closing the ?semantic gap? be-
tween the low-level content descriptions and the 
richness and subjectivity of semantics in high-
level human interpretations of audiovisual media. 
6 Acknowledgements 
The R&D work presented in this paper was par-
tially conducted within the DIRECT-INFO pro-
ject, funded under the 6th Framework Programme 
of the European Community within the strategic 
objective "Semantic-based knowledge manage-
ment systems" (IST FP6-506898). Actual work 
on interoperability of media, language and se-
mantic annotation is being funded by the Net-
work of Excellence K-Space (IST FP6-027026). 
References  
T. Declerck, J. Kuper, H. Saggion, A. Samiotou, P. 
Wittenburg, J. Contreras. Contribution of NLP to 
the Content Indexing of Multimedia Documents. In 
Lecture Notes in Computer Science Volume 3115 / 2004 
Pages 610-618,Springer-Verlag Heidelberg, 6 2004. 
T. Declerck, M. Vela, ?Linguistic Dependencies as a 
Basis for the Extraction of Semantic Relations?, in 
Proceedings of the ECCB'05 Workshop on Bio-
medical Ontologies and Text Processing, Madrid 
(2005) 
G. Kienast, A. Horti, Andr?s, H. Rehatschek, S.  
Busemann, T.    Declerck, V. Hahn and R. Cavet. 
?DIRECT INFO: A Media Monitoring System for 
Sponsorship Tracking.? In Proceedings of the 
ACM SIGIR Workshop on Multimedia Information 
Retrieval. 2005. 
H. Rehatschek: "DIRECT-INFO: Media monitoring 
and multimodal analysis for time critical deci-
sions". Proceedings of the 5th International Work-
shop on Image Analysis for Multimedia Interactive 
Services (WIAMIS), ISBN-972-98115-7-1, Lis-
bon, April 2004. 
AceMedia project: http://www.acemedia.org/aceMedia 
DIRECT-INFO project: http://www.direct-info.net/ 
K-Space project: http://kspace.qmul.net/ 
MPEG-7: http://www.chiariglione.org/mpeg/ 
 
56
