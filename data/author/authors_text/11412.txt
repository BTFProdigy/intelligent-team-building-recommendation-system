Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1056?1065,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
A Compact Forest for Scalable Inference
over Entailment and Paraphrase Rules
Roy Bar-Haim
?
, Jonathan Berant
?
, Ido Dagan
?
?
Computer Science Department, Bar-Ilan University, Ramat Gan 52900, Israel
{barhair,dagan}@cs.biu.ac.il
?
The Blavatnik School of Computer Science, Tel-Aviv University, Tel-Aviv 69978, Israel
jonatha6@post.tau.ac.il
Abstract
A large body of recent research has been
investigating the acquisition and applica-
tion of applied inference knowledge. Such
knowledge may be typically captured as
entailment rules, applied over syntactic
representations. Efficient inference with
such knowledge then becomes a funda-
mental problem. Starting out from a for-
malism for entailment-rule application we
present a novel packed data-structure and
a corresponding algorithm for its scalable
implementation. We proved the validity of
the new algorithm and established its effi-
ciency analytically and empirically.
1 Introduction
Applied semantic inference is concerned with de-
riving target meanings from texts. In the textual
entailment framework, this is reduced to infer-
ring a textual statement (the hypothesis h) from
a source text (t). Traditional formal semantics
approaches perform such inferences over logi-
cal forms derived from the text. By contrast,
most practical NLP applications operate over shal-
lower representations such as parse trees, possibly
supplemented with limited semantic information
about named entities, semantic roles etc.
Most commonly, inference over such represen-
tations is made by applying some kind of transfor-
mations or substitutions to the tree or graph rep-
resenting the text. Such transformations may be
generally viewed as entailment (inference) rules,
which capture semantic knowledge about para-
phrases, lexical relations such as synonyms and
hyponyms, syntactic variations etc. Such knowl-
edge is either composed manually, e.g. WordNet
(Fellbaum, 1998), or learned automatically.
A large body of work has been dedicated to
learning paraphrases and entailment rules, e.g.
(Lin and Pantel, 2001; Shinyama et al, 2002;
Szpektor et al, 2004; Bhagat and Ravichandran,
2008), identifying appropriate contexts for their
application (Pantel et al, 2007) and utilizing them
for inference (de Salvo Braz et al, 2005; Bar-
Haim et al, 2007). Although current avail-
able rule bases are still quite noisy and incom-
plete, the progress made in recent years suggests
that they may become increasingly valuable for
text understanding applications. Overall, applied
knowledge-based inference is a prominent line of
research gaining much interest, with recent exam-
ples including the series of workshops on Knowl-
edge and Reasoning for Answering Questions
(KRAQ)
1
and the planned evaluation of knowledge
resources in the forthcoming 5
th
Recognizing Tex-
tual Entailment challenge (RTE-5)
2
.
While many applied systems utilize semantic
knowledge via such inference rules, their use is
typically limited, application-specific, and quite
heuristic. Formalizing these practices seems im-
portant for applied semantic inference research,
analogously to the role of well-formalized mod-
els in parsing and machine translation. Bar-Haim
et al (2007) made a step in this direction by in-
troducing a generic formalism for semantic infer-
ence over parse trees. Their formalism uses entail-
ment rules as a unifying representation for various
types of inference knowledge, allowing unified in-
ference as well. In this formalism, rule application
has a clear, intuitive interpretation as generating a
new sentence parse (a consequent), semantically
entailed by the source sentence. The inferred con-
sequent may be subject to further rule applications
1
http://www.irit.fr/recherches/ILPL/kraq09.html
2
http://www.nist.gov/tac/2009/RTE/
1056
and so on. In their implementation, each conse-
quent was generated explicitly as a separate tree.
Following this line of work, our long-term re-
search goal is to investigate effective utilization
of entailment rules for inference. While the for-
malism of Bar-Haim et al provides a princi-
pled framework for modeling such inferences,
its implementation using explicit generation of
consequents raises severe efficiency issues, since
the number of consequents may grow exponen-
tially in the number of rule applications. Con-
sider, for example, the sentence ?Children are
fond of candies.?, and the following entailment
rules: ?children?kids?, ?candies?sweets?, and ?X
is fond of Y?X likes Y?. The number of derivable
sentences (including the source sentence) would
be 2
3
as each rule can either be applied or not, in-
dependently. Indeed, we found that this exponen-
tial explosion leads to poor scalability in practice.
Intuitively, we would like that each rule applica-
tion would add just the entailed part (e.g. kids) to a
packed sentence representation. Yet, we still want
the resulting structure to represent a set of entailed
sentences, rather than some mixture of sentence
fragments whose semantics is unclear.
As discussed in section 5, previous work pro-
posed only partial solutions to this problem. In this
paper we present a novel data structure, termed
compact forest, and a corresponding inference al-
gorithm, which efficiently generate and represent
all consequents while preserving the identity of
each individual one (section 3). Our work is
inspired by previous work on packed represen-
tations in other fields, such as parsing, genera-
tion and machine translation (section 5). As
we follow a well-defined formalism, we could
prove that all inference operations of Bar-Haim
et al are equivalently applied over the compact
forest. We compare inference cost over compact
forests to explicit consequent generation both the-
oretically (section 3.4), illustrating an exponential-
to-linear complexity ratio, and empirically (sec-
tion 4), showing improvement by orders of magni-
tude. These results suggest that our data-structure
and algorithm are both valid and scalable, open-
ing up the possibility to investigate large-scale en-
tailment rule application within a well-formalized
framework.
2 Inference Framework
This section briefly presents a (simplified) descrip-
tion of the tree transformations inference formal-
ism of Bar-Haim et al (2007). Given a source text,
syntactically parsed, and a set of entailment rules
representing tree transformations, the formalism
defines the set of consequents derivable from the
text using the rules. Each consequent is obtained
through a sequence of rule applications, each gen-
erates an intermediate parse tree, similar to a proof
process in logic.
More specifically, sentences are represented as
dependency trees, where nodes are annotated with
lemma and part-of-speech, and edges are anno-
tated with dependency relation. A rule ?L ? R?
is primarily composed of two templates, termed
left-hand-side (L), and right-hand-side (R). Tem-
plates are dependency subtrees which may con-
tain POS-tagged variables, matching any lemma.
Figure 1(a) shows passive-to-active transforma-
tion rule, and (b) illustrates its application.
A rule application generates a derived tree d
from a source tree s through the following steps:
L matching: First, a match of L in the source
tree s is sought. In our example, the variable V is
matched in the verb see, N1 is matched in Mary
and N2 is matched in John.
R instantiation: Next, a copy of R is generated
and its variables are instantiated according to their
matching node in L. In addition, a rule may spec-
ify alignments, defined as a partial function from
L nodes to R nodes. An alignment indicates that
for each modifier m of the source node that is not
part of the rule structure, the subtree rooted at m
should also be copied as a modifier of the target
node. In addition to defining alignments explic-
itly, each variable in L is implicitly aligned to its
counterpart in R. In our example, the alignment
between the V nodes implies that yesterday (mod-
ifying see) should be copied to the generated sen-
tence, and similarly beautiful (modifying Mary) is
copied for N1.
Derived tree generation: Let r be the instanti-
ated R, along with its descendants copied from L
through alignment, and l be the subtree matched
by L. The formalism has two methods for gen-
erating the derived tree d: substitution and intro-
duction, as specified by the rule type. Substitution
rules specify modification of a subtree of s, leav-
ing the rest of s unchanged. Thus, d is formed by
copying s while replacing l (and the descendants
1057
LV VERB
obj
ssf
f
f
f
f
f
f
f
f
f
be

by
++
X
X
X
X
X
X
X
X
X
X
R
V VERB
subj
ssf
f
f
f
f
f
f
f
f
f
obj
++
X
X
X
X
X
X
X
X
X
X
N1 NOUN be VERB by PREP
pcomp?n

N2 NOUN N1 NOUN
N2 NOUN
(a) Passive to active transformation (substitution rule)
ROOT
i

see VERB
obj
qqc
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
be
ssf
f
f
f
f
f
f
f
f
f
by

mod
++
X
X
X
X
X
X
X
X
X
X
Mary NOUN
mod

be VERB by PREP
pcomp?n

yesterday NOUN
beautiful ADJ John NOUN
ROOT
i

see VERB
subj
rre
e
e
e
e
e
e
e
e
e
e
obj

mod
,,
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
John NOUN Mary NOUN
mod

yesterday NOUN
beautiful ADJ
Source: Beautiful Mary was seen by John yesterday. Derived: John saw beautiful Mary yesterday.
(b) Application of passive to active transformation
Figure 1: An inference rule application. POS and relation labels are based on Minipar (Lin, 1998)
of l?s nodes) with r. This is the case for the pas-
sive rule, as well as for lexical rules such as ?buy
? purchase?. By contrast, introduction rules are
used to make inferences from a subtree of s, while
the other parts of s are ignored and do not affect d.
A typical example is inferring a proposition em-
bedded as a relative clause in s. In this case, the
derived tree d is simply taken to be r.
In addition to inference rules, the formalism in-
cludes annotation rules which add features to ex-
isting parse tree nodes. These rules have been used
for identifying contexts that affect the polarity of
predicates.
As shown by Bar-Haim et al, this concise, well
defined formalism allows unified representation of
diverse types of knowledge which are commonly
used for applied semantic inference.
3 Efficient Inference over Compact Parse
Forests
As shown in the introduction, explicit genera-
tion of consequents (henceforth explicit inference)
leads to an exponential explosion of the number
of generated trees. In this section we present our
efficient implementation for this formalism. Our
implementation is based on a novel data structure,
termed compact forest (Section 3.1), which com-
pactly represents a large set of trees. Each rule
application generates explicitly only the nodes of
the rule right-hand-side while the rest of the con-
sequent tree is shared with the source, which also
reduces the number of redundant rule applications.
As we shall see, this novel representation is based
primarily on disjunction edges, an extension of
dependency edges that specify a set of alterna-
tive edges of multiple trees. Section 3.2 presents
an efficient algorithm for inference over compact
forests, followed by a discussion of its correctness
and complexity (sections 3.3 and 3.4).
3.1 The Compact Forest Data Structure
A compact forestF represents a set of dependency
trees. Figure 2 shows an example of a compact
forest, containing both the source and derived sen-
tences of Figure 1. We first define a more general
data structure for directed graphs, and then narrow
the definition to the case of trees.
A Compact Directed Graph (cDG) is a pair
G = (V, E) where V is a set of nodes and E is a
set of disjunction edges (d-edges). Let D be a
set of dependency relations. A d-edge d is a triple
(S
d
, rel
d
, T
d
), where S
d
and T
d
are disjoint sets
of source nodes and target nodes; rel
d
: S
d
? D
is a function specifying the dependency relation
corresponding to each source node. Graphically,
d-edges are shown as point nodes, with incoming
edges from source nodes and outgoing edges to
target nodes. For instance, let d be the bottom-
most d-edge in Figure 3. Then S
d
= {of, like},
T
d
= {candy, sweet}, rel(of ) = pcomp-n, and
rel(like) = obj.
A d-edge represents, for each s
i
? S
d
, a set of
1058
ROOT
i
John
see
by objbe mod
by
pcomp-n
beautiful
Mary
mod
be yesterday
see
subj
objmod
Figure 2: A compact forest containing both the
source and derived sentences of Figure 1. Parts
of speech are omitted.
alternative directed edges {(s
i
, t
j
) : t
j
? T
d
}, all
of which are labeled with the same relation given
by rel
d
(s
i
). Each of these edges, termed embed-
ded edge (e-edge), would correspond to a differ-
ent graph represented in G. In the previous exam-
ple, the e-edges are like
obj
???candy, like
obj
???sweet,
of
pcomp?n
???????candy and of
pcomp?n
???????sweet (notice
that the definition implies that all source nodes in
S
d
have the same set of alternative target nodes
T
d
). d is called an outgoing d-edge of a node v if
v ? S
d
and an incoming d-edge of v if v ? T
d
.
A Compact Directed Acyclic Graph (cDAG) is a
cDG that contains no cycles of e-edges.
A DAG G rooted in a node v ? V of a cDAG
G is embedded in G if it can be derived as follows:
we initialize G with v alone; then, we expand v
by choosing exactly one target node t ? T
d
from
each outgoing d-edge d of v, and adding t and the
corresponding e-edge (v, t) to G. This expansion
process is repeated recursively for each new node
added to G.
Each such set of choices results in a different
DAG with v as its only root. In Figure 2, we may
choose to connect the root either to the left see,
resulting in the source passive sentence, or to the
right see, resulting in the derived active sentence.
A Compact Forest F is a cDAG with a single
root r (i.e. r has no incoming d-edges) where all
the embedded DAGs rooted in r are trees. This set
of trees, termed embedded trees, comprise the set
of trees represented by F .
Figure 3 shows another example for a compact
ROOT
i
child
be
pred
fond
subjmod
of
pcomp-n
candy
like
subj
obj
kid
sweet
Figure 3: A compact forest representing the 2
3
sentences derivable from the sentence ?children
are fond of candies? using the following three
rules: ?children?kids?, ?candies?sweets?, and ?X
is fond of Y?X likes Y?.
forest efficiently representing the 2
3
sentences re-
sulting from three independently-applied rules.
3.2 The Inference Process
Next, we describe the algorithm implementing the
inference process of Section 2 over the compact
forest (henceforth, compact inference), illustrating
it through Figures 1 and 2.
Forest initialization F is initialized with the
set of dependency trees representing the text sen-
tences, with their roots connected under the forest
root as the target nodes of a single d-edge. Depen-
dency edges are transformed trivially to d-edges
with a single source and target. Annotation rules
are applied at this stage to the initial F . The black
part of Figure 2 corresponds to the initial forest.
Rule application comprises the following steps:
L matching: L is matched in F if there exists
an embedded tree t in F such that L is matched
in t, as in Section 2. We denote by l the subtree
of t in which L was matched. As in section 2, the
match in our example is (V,N1, N2)=(see, Mary,
John). Notice that this definition does not allow l
to be scattered over multiple embedded trees.
As the target nodes of a d-edge specify alterna-
tives for the same position in the tree, their parts-
of-speech are expected to be of substitutable types.
1059
In this paper we further assume that all target
nodes of the same d-edge have the same part-of-
speech
3
. Consequently, variables that are leaves in
L and may match a certain target node of a d-edge
d are mapped to the whole set of target nodes T
d
rather than to a single node. This yields a compact
representation of multiple matches, and prevents
redundant rule applications. For instance, given
a compact representation of ?{Children/kids} are
fond of {candies/sweeets}? (cf. Figure 3), the rule
?X is fond of Y?X likes Y? will be matched and
applied only once, rather than four times (for each
combination of matching X and Y ).
Derived tree generation: A template r consist-
ing of R while excluding variables that are leaves
of both L and R (termed dual leaf-variables)
4
is
generated and inserted into F . In case of a substi-
tution rule (as in our example), r is set as an alter-
native to l by adding r?s root to T
d
, where d is the
incoming d-edge of l?s root. In case of an intro-
duction rule, it is set as an alternative to the other
trees in the forest by adding r?s root to the target
node set of the forest root?s outgoing d-edge. In
our example, r is the gray node (still labeled with
the variable V ) , and it becomes an additional tar-
get node of the d-edge entering the original (left)
see.
Variable instantiation: Each variable in r (i.e.
a non-dual leaf) is instantiated according to its
match in L (as in Section 2), e.g. V is instantiated
with see. As specified above, if the variable is a
leaf inL is not a dual leaf then it is matched in a set
of nodes, and hence each of them should be instan-
tiated in r. This is decomposed into a sequence
of simpler operations: first, r is instantiated with a
representative from the set, and then we apply (ad-
hoc) lexical substitution rules for creating a new
node for each other node in the set
5
.
Alignment sharing: Modifiers of aligned nodes
are shared (rather than copied) as follows. Given
a node n
L
in l aligned to a node n
R
in r, and an
outgoing d-edge d of n
L
which is not part of l, we
share d between n
L
and n
R
by adding n
R
to S
d
3
This is the case in our current implementation, which is
based on the coarse tag-set of Minipar (Lin, 1998).
4
With the following exceptions: variables that are the
only node in R (and hence are both the root and a leaf), and
variables with additional alignments (other than the implicit
alignment between their occurrences in L and R) are not con-
sidered dual-leaves.
5
Notice that these nodes, in addition to the usual align-
ment with their source nodes in l, share the same daughters
in r.
and setting rel
d
(n
R
) = rel
d
(n
L
). This is illus-
trated by the sharing of yesterday in Figure 2. We
also copy annotation features from n
L
to n
R
.
We note at this point that the instantiation of
variables that are not dual leaves (e.g. V in our
example) cannot be shared because they typically
have different modifiers at the two sides of the
rule. Yet, their modifiers which are not part of
the rule are shared through the alignment opera-
tion (recall that common variables are always con-
sidered aligned). Dual leaf variables, on the other
hand, might be shared, as described next, since the
rule doesn?t specify any modifiers for them.
Dual leaf variable sharing: This final step is
performed analogously to alignment sharing. Sup-
pose that a dual leaf variable X is matched in a
node v in l whose incoming d-edge is d. Then
we simply add the parent p of X in r to S
d
and
set rel
d
(p) to the relation between p and X (in
R). Since v itself is shared, its modifiers become
shared as well, implicitly implementing the align-
ment operation. The subtrees beautiful Mary and
John are shared this way for variablesN1 andN2.
Applying the rule in our example added only
a single node and linked it to four d-edges, com-
pared to duplicating the whole tree in explicit in-
ference.
3.3 Correctness
In this section we present two theorems, which
prove that the inference algorithm is a valid imple-
mentation of the inference formalism of Section 2.
Due to space limitations, the proofs themselves
are omitted, and instead we outline their general
scheme.
We first argue that performing any sequence of
rule applications over the set of initial trees results
in a compact forest:
Theorem 1: The compact inference process
generates a compact forest.
Proof scheme: We prove by induction on the
number of rule applications. Initialization gen-
erates a single-rooted cDAG, whose embedded
DAGs are all trees, as required. We then prove that
if applying a rule on a compact forest creates a cy-
cle or an embedded DAG that is not a tree, then
such a cycle or a non-tree DAG already existed
prior to rule application, in contradiction with the
inductive assumption. A crucial observation for
this proof is that for any path from a node u to a
node v that passes through r, where u and v are
1060
outside r, there is also an analogous path from u
to v that passes through l instead, QED.
Next, we argue that the inference process over a
compact forest is complete and sound, i.e., it gen-
erates the set of consequents derivable from a text
according to the inference formalism.
Theorem 2: Given a rule base R and a set of
initial trees T , a tree t is embedded in a compact
forest derivable from T by the compact inference
process? t is a consequent of T according to the
inference formalism.
Proof scheme: We first show completeness by
induction on the number of explicit rule applica-
tions. Let t
n+1
be a tree derived from a tree t
n
using the rule r
n
according to the inference for-
malism. The inductive assumption asserts that t
n
is embedded in some derivable compact forest F .
It is easy to verify that applying r
n
to F will yield
a compact forest F
?
in which t
n+1
is embedded.
Next, we show soundness by induction on the
number of rule applications over the compact for-
est. Let t
n+1
be a tree represented in some derived
compact forest F
n+1
. F
n+1
was derived from the
compact forest F
n
, using the rule r
n
. It can be
shown that F
n
represents a tree t
n
, such that ap-
plying r
n
on t
n
will yield t
n+1
according to the
formalism. The inductive assumption asserts that
t
n
is a consequent in the inference formalism and
therefore t
n+1
is a consequent as well, QED.
These two theorems guarantee that the compact
inference process is valid - i.e., it yields a compact
forest that represents the set of consequents deriv-
able from a given text by a given rule set.
3.4 Complexity
In this section we explain why compact inference
exponentially reduces the time and space com-
plexity in typical scenarios.
We consider a set of rule matches in a tree T
independent if their matched left-hand-sides (ex-
cluding dual-leaf variables) do not overlap in T ,
and their application over T can be chained in any
order. For example, the three rule matches pre-
sented in Figure 3 are independent.
Let us consider explicit inference first. Assume
we start with a single tree T with k independent
rules matched. Applying k rules will yield 2
k
trees, since any subset of the rules might be ap-
plied to T . Therefore, the time and space com-
plexity of applying k independent rule matches is
?(2
k
). Applying more rules on the newly derived
Compact Explicit Ratio
Time (msec) 61 24,184 396
Rule applications 12 123 10
Node count 69 5,901 86
Edge endpoints 141 11,552 82
Table 1: Compact vs. explicit inference, us-
ing generic rules. Results are averaged per text-
hypothesis pair.
consequents behaves in a similar manner.
Next, we examine compact inference. Apply-
ing a rule using compact inference adds the right-
hand-side of the rule and shares with it existing
d-edges. Since that the size of the right-hand-side
and the number of outgoing d-edges per node are
practically bounded by low constants, applying k
rules on a tree T yields a linear increase in the size
of the forest. Thus, the resulting size isO(|T |+k),
as we can see from Figure 3.
The time complexity of rule application is com-
posed of matching the rule in the forest and apply-
ing the matched rule. Applying a matched rule is
linear in its size. Matching a rule of size r in a
forest F takes O(|F|
r
) time even when perform-
ing an exhaustive search for matches in the forest.
Since r tends to be quite small and can be bounded
by a low constant, this already gives polynomial
time complexity. In practice, indexing the forest
nodes, as well as the typical low connectivity of
the forest, result in a very fast matching procedure,
as illustrated in the empirical evaluation, described
next.
4 Empirical Evaluation
This section reports empirical evaluation of the ef-
ficiency of compact inference, tested in the recog-
nizing textual entailment setting using the RTE-3
and RTE-4 datasets (Giampiccolo et al, 2007; Gi-
ampiccolo et al, 2009). These datasets consist of
(text, hypothesis) pairs, which need to be classi-
fied as entailing/non entailing. Our first experi-
ment shows, using a small rule set, that compact
inference outperforms explicit inference by orders
of magnitude (Section 4.1). The second experi-
ment shows that compact inference scales well to
a full-blown RTE setting with several large-scale
rule bases, where up to hundreds of rules are ap-
plied for a text (Section 4.2).
1061
4.1 Compact vs. Explicit Inference
To compare explicit and compact inference we
randomly sampled 100 pairs from the RTE-3 de-
velopment set, and parsed the text in each pair
using Minipar (Lin, 1998). We used a set of
manually-composed entailment rules for inference
over generic linguistic phenomena such as pas-
sive, conjunction, relative clause, apposition, pos-
sessives, and determiners, which contains a few
dozens of rules. To make a fair comparison, we
aimed to make the explicit inference implementa-
tion reasonably efficient, e.g. by preventing gen-
eration of the same tree by different permutations
of the same rule applications. Both configurations
perform rule application iteratively, until no new
matches are found. In each iteration we first find
all rule matches and then apply all matching rules.
We compare run time, number of rule applications,
and the overall generated size of nodes and edges,
where edge size is represented by the sum of its
endpoints.
The results are summarized in Table 1. As ex-
pected, the results show that compact inference is
by orders of magnitude more efficient than explicit
inference. To avoid memory overflow, inference
was terminated after reaching 100,000 nodes. 3
out of the 100 pairs reached that limit with explicit
inference, while the maximal node count for com-
pact inference was only 268. The number of rule
applications is reduced thanks to the sharing of
common subtrees in the compact forest, by which
a single rule application operates simultaneously
over a large number of embedded trees. The re-
sults suggest that scaling to larger rule bases and
longer inference chains would be feasible for com-
pact inference, but prohibitive for explicit infer-
ence.
4.2 Application to an RTE System
Experimental setting The goal of the second
experiment was to assess that compact inference
scales well for broad entailment rule bases. In
this experiment we used the Bar-Ilan RTE system
(Bar-Haim et al, 2009). The system operates in
two primary stages: Inference, in which entail-
ment rules are applied to the initial compact forest
F , aiming to bring it closer to the hypothesis H,
and Classification, in which a set of features is ex-
tracted from the resulting F and from H and fed
into an SVM classifier, which determines entail-
ment.
The classification setting and its features are
quite typical for the RTE literature. They include
lexical and structural measures for the coverage of
H by F , where high coverage is assumed to cor-
relate with entailment, as well as features aiming
to detect inconsistencies between F and H such
as incompatible arguments for the same predicate
or incompatible verb polarity (see below). For a
complete feature description, see (Bar-Haim et al,
2009).
Rule Bases In addition to the generic rules de-
scribed in Section 4.1, the following large-scale
sources for entailment rules were used: Wikipeda:
We used the lexical rulebase of Shnarch et al
(2009), who extracted rules such as ?Janis Joplin
? singer? from Wikipedia based on both its meta-
data (e.g. links and redirects) and text defini-
tions, using patterns such as ?X is a Y?. Word-
Net: We extracted from WordNet (Fellbaum,
1998) lexical rules based on synonyms, hyper-
nyms and derivation relations. DIRT: The DIRT
algorithm (Lin and Pantel, 2001) learns from a
corpus entailment rules between binary predicates,
e.g. ?X is fond of Y?X likes Y?. We used the
version described in (Szpektor and Dagan, 2007),
which learns canonical rule forms. Argument-
MappedWordNet (AmWN):A resource for entail-
ment rules between verbal and nominal predicates
(Szpektor and Dagan, 2009), including their argu-
ment mapping, based on WordNet and NomLex-
plus (Meyers et al, 2004), verified statistically
through intersection with the unary-DIRT algo-
rithm (Szpektor and Dagan, 2008). In total, these
rule bases represent millions of rules. Polarity An-
notation Rules: We compiled a small set of anno-
tation rules for marking the polarity of predicates
as negative or unknown due to verbal negation,
modal verbs, conditionals etc. (Bar-Haim et al,
2009).
Search In this work we focus on efficient rep-
resentation of the search space, leaving for future
work the complementary problem of devising ef-
fective search heuristics over our representation.
In the current experiment we implemented a sim-
ple search strategy, in the spirit of (de Salvo Braz
et al, 2005): first, we applied three exhaustive iter-
ations of generic rules. Since these rules have low
fan-out (few possible right-hand-sides for a given
left-hand-side) it is affordable to apply and chain
them more freely. We then perform a single itera-
tion of all other lexical and lexical-syntactic rules,
1062
applying them only if their L part was matched in
F and their R part was matched inH.
The system was trained over the RTE-3 devel-
opment set, and tested on both RTE-3 test set and
RTE-4 (which includes only a test set).
Results Table 2 provides statistics on rule appli-
cation using all rule bases, over the RTE-3 devel-
opment set and the RTE-4 dataset
6
. Overall, the
primary result is that the compact forest indeed ac-
commodates well extensive rule application from
large-scale rule bases. The resulting forest size is
kept small, even in the maximal cases which were
causing memory overflow for explicit inference.
The accuracies obtained in this experiment and
the overall contribution of rule-based inference are
shown in Table 3. The results on RTE-3 are quite
competitive: compared to our 66.4%, only 3 teams
out of the 26 who participated in RTE-3 scored
higher than 67%, and three more systems scored
between 66% and 67%. The results for RTE4 rank
9-10 out of 26, with only 6 teams scoring higher by
more than 1%. Overall, these results validate that
the setting of our experiment represents a state-of-
the-art system.
Inference over the rule bases utilized in our
experiment improved the accuracy on both test
sets. The contribution was more prominent for
the RTE-4 dataset. These results illustrate a typ-
ical contribution of current knowledge sources for
current RTE systems. This contribution is likely
to increase with current and near future research,
on topics such as extending and improving knowl-
edge resources, applying them only in seman-
tically suitable contexts, improved classification
features and broader search strategies. As for our
current experiment, we may conclude that the goal
of assessing the compact forest scalability in a
state-of-the-art setting was achieved
7
.
Finally, Tables 4 and 5 illustrate the usage and
contribution of individual rule bases. Table 4
shows the distribution of rule applications over the
various rule bases. Table 5 presents ablation study
showing the marginal accuracy gain for each rule
base. These results show that each of the rule
bases is applicable for a large portion of the pairs,
and contributes to the overall accuracy.
6
Running time is omitted since most of it was dedicated
to rule fetching, which was rather slow for our available im-
plementation of some resources. The elapsed time was a few
seconds per (t, h) pair.
7
We note that common RTE research issues, such as im-
proving accuracy, fall out of the scope of the current paper.
RTE3-Dev RTE4
Avg. Max. Avg. Max.
Rule applications 14 275 15 110
Node count 71 606 80 357
Edge endpoints 155 1,741 173 1,062
Table 2: Application of compact inference to the
RTE-3 Dev. and RTE-4 datasets, using all rule
types.
Accuracy
Test set No inference Inference ?
RTE3 64.6% 66.4% 1.8%
RTE4 57.5% 60.6% *3.1%
Table 3: Inference contribution to RTE perfor-
mance. The system was trained on the RTE-3 de-
velopment set. * indicates statistically significant
difference (at level p < 0.02, using McNemar?s
test).
Rule base RTE3-Dev RTE4
Rules App Rules App
WordNet 0.6 1.2 0.6 1.1
AmWN 0.3 0.4 0.3 0.4
Wikipedia 0.6 1.7 0.6 1.3
DIRT 0.5 0.7 0.5 1.0
Generic 4.7 10.4 5.4 11.5
Polarity 0.2 0.2 0.2 0.2
Table 4: Average number of rule applications per
(t, h) pair, for each rule base. App counts each rule
application, while Rules ignores multiple matches
of the same rule in the same iteration.
Rule base ?Accuracy (RTE4)
WordNet 0.8%
AmWN 0.7%
Wikipedia 1.0%
DIRT 0.9%
Generic 0.4%
Polarity 0.9%
Table 5: Contribution of various rule bases. Re-
sults show accuracy loss on RTE-4, obtained for
removing each rule base (ablation tests).
5 Related Work
This section discusses related work on applying
knowledge-based transformations within RTE sys-
tems, as well as on using packed representations in
other NLP tasks.
RTE Systems Previous RTE systems usually re-
stricted both the type of allowed transformations
and the search space. Systems based on lexical
(word-based or phrase-based) matching of h in t
typically applied only lexical rules (without vari-
1063
ables), where both sides of the rule are matched
directly in t and h (Haghighi et al, 2005; Mac-
Cartney et al, 2008). The inference formalism
we use is more expressive, allowing also syntac-
tic and lexical-syntactic transformations as well as
rule chaining.
Hickl (2008) derived from a given (t, h) pair
a small set of discourse commitments, which are
quite similar to the kind of consequents we derive
by our syntactic and lexical-syntactic rules. The
commitments were generated by several different
tools and techniques, compared to our generic uni-
fied inference process, and commitment genera-
tion efficiency was not discussed.
Braz et al (2005) presented a semantic infer-
ence framework which ?augments? the text repre-
sentation with only the right-hand-side of an ap-
plied rule, and in this respect is similar to ours.
However, in their work, both rule application and
the semantics of the resulting ?augmented? struc-
ture were not fully specified. In particular, the dis-
tinction between individual consequents was lost
in the augmented graph. By contrast, our com-
pact inference is fully formalized and is provably
equivalent to an expressive, well-defined formal-
ism operating over individual trees, and each in-
ferred consequent can be recovered from the com-
pact forest.
Packed representations Packed representations
in various NLP tasks share common principles,
which also underly our compact forest: factor-
ing out common substructures and representing
choice as local disjunctions. Applying this gen-
eral scheme to individual problems typically re-
quires specific representations and algorithms, de-
pending on the type of alternatives that should be
represented and the specified operations for creat-
ing them. In our work, alternatives are created by
rule application, where a newly derived subtree is
set as an alternative to existing subtrees. Alterna-
tives are specified locally using d-edges.
Packed chart representations for parse forests
were introduced in classical parsing algorithms
such as CYK and Earley (Jurafsky and Martin,
2008), and have been extended in later work
for various purposes (Maxwell III and Kaplan,
1991; Kay, 1996). Alternatives in the parse chart
stem from syntactic ambiguities, and are speci-
fied locally as the possible decompositions of each
phrase into its sub-phrases.
Packed representations have been utilized also
in transfer-based machine translation. Emele and
Dorna (1998) translated packed source language
representation to packed target language represen-
tation while avoiding unnecessary unpacking dur-
ing transfer. Unlike our rule application, in their
work transfer rules preserve ambiguity stemming
from source language, rather than generating new
alternatives. Mi et al(2008) applied statistical ma-
chine translation to a source language parse forest,
rather than to the 1-best parse. Their transfer rules
are tree-to-string, contrary to our tree-to-tree rules,
and chaining is not attempted (rules are applied in
a single top-down pass over the source forest), and
thus their representation and algorithms are quite
different from ours.
6 Conclusion
This work addresses the efficiency of entailment
and paraphrase rule application. We presented a
novel compact data structure and a rule application
algorithm for it, which are provably a valid imple-
mentation of a generic inference formalism. We il-
lustrated inference efficiency both analytically and
empirically. Beyond entailment inference, we sug-
gest that the compact forest would also be use-
ful in generation tasks (e.g. paraphrasing). Our
efficient representation of the consequent search
space opens the way to future investigation of the
benefit of larger-scale rule chaining, and to the de-
velopment of efficient search strategies required to
support such inferences.
Acknowledgments
This work was partially supported by the
PASCAL-2 Network of Excellence of the Eu-
ropean Community FP7-ICT-2007-1-216886, the
FBK-irst/Bar-Ilan University collaboration and
the Israel Science Foundation grant 1112/08. The
second author is grateful to the Azrieli Foundation
for the award of an Azrieli Fellowship. The au-
thors would like to thank Yonatan Aumann, Marco
Pennacchiotti and Marc Dymetman for their valu-
able feedback on this work.
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI.
Roy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo
Greental, Shachar Mirkin, Eyal Shnarch, and Idan
1064
Szpektor. 2009. Efficient semantic deduction and
approximate matching over compact parse forests.
In Proceedings of the First Text Analysis Conference
(TAC 2008).
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-08: HLT.
Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-
yakanok, Dan Roth, and Mark Sammons. 2005. An
inference model for semantic entailment in natural
language. In Proceedings of AAAI.
Martin C. Emele and Michael Dorna. 1998. Ambi-
guity preserving machine translation using packed
representations. In Proceedings of Coling-ACL.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Language, Speech and
Communication. MIT Press.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The Third PASCAL Recog-
nizing Textual Entailment Challenge. In Proceed-
ings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, and Bill Dolan. 2009. The
Fourth PASCAL Recognizing Textual Entailment
Challenge. In Proceedings of the First Text Analy-
sis Conference (TAC 2008).
Aria D. Haghighi, Andrew Y. Ng, and Christopher D.
Manning. 2005. Robust textual inference via graph
matching. In Proceedings of EMNLP.
Andrew Hickl. 2008. Using discourse commitments
to recognize textual entailment. In Proceedings of
COLING.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics and Speech Recognition. Prentice Hall, second
edition.
Martin Kay. 1996. Chart generation. In Proceedings
of ACL.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalu-
ation of Parsing Systems at LREC.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of
EMNLP.
John T. Maxwell III and Ronald M. Kaplan. 1991.
A method for disjunctive constraint satisfaction. In
Masaru Tomita, editor, Current Issues in Parsing
Technology. Kluwer Academic Publishers.
A. Meyers, R. Reeves, Catherine Macleod, Rachel
Szekeley, Veronkia Zielinska, and Brian Young.
2004. The cross-breeding of dictionaries. In Pro-
ceedings of LREC.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Pro-
ceedings of NAACL-HLT.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase ac-
quisition from news articles. In Proceedings of Hu-
man Language Technology Conference (HLT 2002),
San Diego, USA.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting lexical reference rules from Wikipedia. In
Proceedings of ACL-IJCNLP.
Idan Szpektor and Ido Dagan. 2007. Learning canon-
ical forms of entailment rules. In Proceedings of
RANLP.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Idan Szpektor and Ido Dagan. 2009. Augmenting
WordNet-based inference with argument mapping.
In Proceedings of ACL-IJCNLP Workshop on Ap-
plied Textual Inference (TextInfer).
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
ture Coppola. 2004. Scaling web based acquisition
of entailment patterns. In Proceedings of EMNLP.
1065
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 770?778,
Beijing, August 2010
Recognising Entailment within Discourse
Shachar Mirkin?, Jonathan Berant?, Ido Dagan?, Eyal Shnarch?
? Computer Science Department, Bar-Ilan University
? The Blavatnik School of Computer Science, Tel-Aviv University
Abstract
Texts are commonly interpreted based on
the entire discourse in which they are sit-
uated. Discourse processing has been
shown useful for inference-based applica-
tion; yet, most systems for textual entail-
ment ? a generic paradigm for applied in-
ference ? have only addressed discourse
considerations via off-the-shelf corefer-
ence resolvers. In this paper we explore
various discourse aspects in entailment in-
ference, suggest initial solutions for them
and investigate their impact on entailment
performance. Our experiments suggest
that discourse provides useful informa-
tion, which significantly improves entail-
ment inference, and should be better ad-
dressed by future entailment systems.
1 Introduction
This paper investigates the problem of recognising
textual entailment within discourse. Textual En-
tailment (TE) is a generic framework for applied
semantic inference (Dagan et al, 2009). Under
TE, the relationship between a text (T) and a tex-
tual assertion (hypothesis, H) is defined such that
T entails H if humans reading T would infer that
H is most likely true (Dagan et al, 2006).
TE has been successfully applied to a variety of
natural language processing applications, includ-
ing information extraction (Romano et al, 2006)
and question answering (Harabagiu and Hickl,
2006). Yet, most entailment systems have thus
far paid little attention to discourse aspects of in-
ference. In part, this is the result of the unavail-
ability of adept tools for handling the kind of dis-
course processing required for inference. In addi-
tion in the main TE benchmarks, the Recognising
Textual Entailment (RTE) challenges, discourse
played little role. This state of affairs has started
to change with the recent introduction of the RTE
Pilot ?Search? task (Bentivogli et al, 2009b), in
which assessed texts are situated within complete
documents. In this setting, texts need to be inter-
preted based on their entire discourse (Bentivogli
et al, 2009a), hence attending to discourse issues
becomes essential. Consider the following exam-
ple from the task?s dataset:
(T) The seven men on board were said to have
as little as 24 hours of air.
For the interpretation of T, e.g. the identity and
whereabouts of the seven men, one must consider
T?s discourse. The preceding sentence T?, for in-
stance, provides useful information to that aim:
(T?) The Russian navy worked desperately to
save a small military submarine.
This example demonstrates a common situation in
texts, and is also applicable to the RTE Search
task?s setting. Still, little was done by the task?s
participants to consider discourse, and sentences
were mostly processed independently.
Analyzing the Search task?s development set,
we identified several key discourse aspects that af-
fect entailment in a discourse-dependent setting.
First, we observed that the coverage of available
coreference resolution tools is considerably lim-
ited. To partly address this problem, we extend the
set of coreference relations to phrase pairs with
a certain degree of lexical overlap, as long as no
semantic incompatibility is found between them.
Second, many bridging relations (Clark, 1975) are
realized in the form of ?global information? per-
ceived as known for entire documents. As bridg-
ing falls completely out of the scope of available
resolvers, we address this phenomenon by iden-
tifying and weighting prominent document terms
and allowing their incorporation in inference even
770
when they are not explicitly mentioned in a sen-
tence. Finally, we observed a coherence-related
discourse phenomenon, namely inter-relations be-
tween entailing sentences in the discourse, such
as the tendency of entailing sentences to be ad-
jacent to one another. To that end, we apply a
two-phase classification scheme, where a second-
phase meta-classifier is applied, extracting dis-
course and document-level features based on the
classification of each sentence on its own.
Our results show that, even when simple so-
lutions are employed, the reliance on discourse-
based information is helpful and achieves a sig-
nificant improvement of results. We analyze the
contribution of each component and suggest some
future work to better attend to discourse in entail-
ment systems. To our knowledge, this is the most
extensive effort thus far to empirically explore the
effect of discourse on entailment systems.
2 Background
Discourse plays a key role in text understanding
applications such as question answering or infor-
mation extraction. Yet, such applications typically
only handle a narrow aspect of discourse, address-
ing coreference by term substitution (Dali et al,
2009; Li et al, 2009). The limited coverage and
scope of existing tools for coreference resolution
and the unavailability of tools for addressing other
discourse aspects also contribute to this situation.
For instance, VP anaphora and bridging relations
are usually not handled at all by such resolvers. A
similar situation is seen in the TE research field.
The prominent benchmark for entailment sys-
tems evaluation is the series of RTE challenges.
The main task in these challenges has tradition-
ally been to determine, given a text-hypothesis
pair (T,H), whether T entails H. Discourse played
no role in the first two RTE challenges as
T?s were constructed of short simplified texts.
In RTE-3 (Giampiccolo et al, 2007), where
some paragraph-long texts were included, inter-
sentential relations became relevant for correct in-
ference. Yet the texts in the task were manually
modified to ensure they are self-contained. Con-
sequently, little effort was invested by the chal-
lenges? participants to address discourse issues
beyond the standard substitution of coreferring
nominal phrases, using publicly available tools
such as JavaRap (Qiu et al, 2004) or OpenNLP1,
e.g. (Bar-Haim et al, 2008).
A major step in the RTE challenges towards a
more practical setting of text processing applica-
tions occurred with the introduction of the Search
task in the Fifth RTE challenge (RTE-5). In this
task entailing sentences are situated within doc-
uments and depend on other sentences for their
correct interpretation. Thus, discourse becomes
a substantial factor impacting inference. Surpris-
ingly, discourse hardly received any treatment in
this task beyond the standard use of coreference
resolution (Castillo, 2009; Litkowski, 2009), and
an attempt to address globally-known information
by removing from H words that appear in docu-
ment headlines (Clark and Harrison, 2009).
3 The RTE Search Task
The RTE-5 Search task was derived from the
TAC Summarization task2. The dataset consists
of several corpora, each comprised of news arti-
cles concerning a specific topic, such as the im-
pact of global warming on the Arctic or the Lon-
don terrorist attacks in 2005. Hypotheses were
manually generated based on Summary Content
Units (Nenkova et al, 2007), clause-long state-
ments taken from manual summaries of the cor-
pora. Texts are unmodified sentences in the arti-
cles. Given a topic and a hypothesis, entailment
systems are required to identify all sentences in
the topic?s corpus that entail the hypothesis.
Each sentence-hypothesis pair in both the de-
velopment and test sets was annotated, judging
whether the sentence entails the hypothesis. Out
of 20,104 annotations in the development set, only
810 were judged as positive. This small ratio (4%)
of positive examples, in comparison to 50% in tra-
ditional RTE tasks, better corresponds to the natu-
ral distribution of entailing texts in a corpus, thus
better simulates practical settings.
The task may seem as a variant of information
retrieval (IR), as it requires finding specific texts
in a corpus. Yet, it is fundamentally different from
IR for two reasons. First, the target output is a set
1http://opennlp.sourceforge.net
2http://www.nist.gov/tac/2009/Summarization/
771
of sentences, each evaluated independently, rather
than a set of documents. Second, the decision cri-
terion is entailment rather than relevance.
Despite the above, apparently, IR techniques
provided hard-to-beat baselines for the RTE
Search task (MacKinlay and Baldwin, 2009), out-
performing every other system that relied on in-
ference without IR-based pre-filtering. At the cur-
rent state of performance of entailment systems, it
seems that lexical coverage largely overshadows
any other approach in this task. Still, most (6 out
of 8) participants in the challenge applied their en-
tailment systems to the entire dataset without a
prior retrieval of candidate sentences. F1 scores
for such systems vary between 10% and 33%, in
comparison to over 40% of the IR-based methods.
4 The Baseline RTE System
In this work we used BIUTEE, Bar-Ilan Univer-
sity Textual Entailment Engine (Bar-Haim et al,
2008; Bar-Haim et al, 2009), a state of the art
RTE system, as a baseline and as a basis for our
discourse-based enhancements. This section de-
scribes this system?s architecture; the methods by
which it was augmented to address discourse are
presented in Section 5.
To determine entailment, BIUTEE performs the
following main steps:
Preprocessing First, all documents are parsed
and processed with standard tools for named en-
tity recognition (Finkel et al, 2005) and corefer-
ence resolution. For the latter purpose, we use
OpenNLP and enable the substitution of corefer-
ring terms. This is the only way by which BIUTEE
addresses discourse, representing the state of the
art in entailment systems.
Entailment-based transformations Given a
T-H pair (both represented as dependency
parse trees), the system applies a sequence of
knowledge-based entailment transformations over
T, generating a set of texts which are entailed by
it. The goal is to obtain consequent texts which
are more similar to H. Based on preliminary re-
sults on the development set, in our experiments
(Section 6) we use WordNet (Fellbaum, 1998) as
the system?s only knowledge resource, using its
synonymy, hyponymy and derivation relations.
Classification A supervised classifier, trained
on the development set, is applied to determine
entailment of each pair based on a set of syntactic
and lexical syntactic features assessing the degree
by which T and its consequents cover H.
5 Addressing Discourse
In the following subsections we describe the
prominent discourse phenomena that affect infer-
ence, which we have identified in an analysis of
the development set and addressed in our imple-
mentation. As mentioned, these phenomena are
poorly addressed by available reference resolvers
or fall completely out of their scope.
5.1 Augmented coreference set
A large number of coreference relations are com-
prised of terms which share lexical elements, (e.g.
?airliners?s first flight? and ?Airbus A380?s first
flight?). Although common in coreference rela-
tions, standard resolvers miss many of these cases.
For the purpose of identifying additional corefer-
ring terms, we consider two noun phrases in the
same document as coreferring if: (i) their heads
are identical and (ii) no semantic incompatibil-
ity is found between their modifiers. The types
of incompatibility we handle are: (a) mismatch-
ing numbers, (b) antonymy and (c) co-hyponymy
(coordinate terms), as specified by WordNet. For
example, two nodes of the noun distance would
be considered incompatible if one is modified by
short and the second by its antonym long. Simi-
larly, two modifier co-hyponyms of distance, such
as walking and running would also result such
an incompatibility. Adding more incompatibility
types (e.g. first vs. second flight) may further im-
prove the precision of this method.
5.2 Global information
Key terms or prominent pieces of information that
appear in the document, typically at the title or the
first few sentences, are many times perceived as
?globally? known throughout the document. For
example, the geographic location of the document
theme, mentioned at the beginning of the docu-
ment, is assumed to be known from that point on,
and will often not be mentioned explicitly in fur-
ther sentences. This is a bridging phenomenon
772
that is typically not addressed by available dis-
course processing tools. To compensate for that,
we identify key terms for each document based
on tf-idf scores and consider them as global in-
formation for that document. For example, global
terms for the topic discussing the ice melting in
the Arctic, typically contain a location such as
Arctic or Antarctica and terms referring to ice, like
permafrost or iceshelf.
We use a variant of tf-idf, where term frequency
is computed as follows: tf(ti,j) = ni,j+~?> ? ~fi,j .
Here, ni,j is the frequency of term i in document j
(ti,j), which is incremented by additional positive
weights (~?) for a set of features ( ~fi,j) of the term.
Based on our analysis, we defined the following
features, which correlated mostly with global in-
formation: (i) does the term appear in the title?
(ii) is it a proper name? (iii) is it a location? The
weights for these features are set empirically.
The document?s top-n global terms are added
to each of its sentences. As a result, a global term
that occurs in the hypothesis is matched in each
sentence of the document, regardless of whether
the term explicitly appears in the sentence.
Considering the previous sentence Another
method for addressing missing coreference and
bridging relations is based on the assumption that
adjacent sentences often refer to the same entities
and events. Thus, when extracting classification
features for a given sentence, in addition to the
features extracted from the parse tree of the sen-
tence itself, we extract the same set of features
from the current and previous sentences together.
Recall the example presented in Section 1. T is
annotated as entailing the hypothesis ?The AS-28
mini-submarine was trapped underwater?, but the
word submarine, e.g., appears only in its preced-
ing sentence T?. Thus, considering both sentences
together when classifying T increases its coverage
of the hypothesis. Indeed, a bridging reference re-
lates on board in T with submarine in T?, justify-
ing our assumption in this case.
5.3 Document-level classification
Beyond discourse references addressed above,
further information concerning discourse and doc-
ument structure is available in the Search setting
and may contribute to entailment classification.
We observed that entailing sentences tend to come
in bulks. This reflects a common coherence as-
pect, where the discussion of a specific topic is
typically continuous rather than scattered across
the entire document. This locality phenomenon
may be useful for entailment classification since
knowing that a sentence entails the hypothesis in-
creases the probability that adjacent sentences en-
tail the hypothesis as well.
To capture this phenomenon, we use a two-
phase meta-classification scheme, in which a
meta-classifier utilizes entailment classifications
of the first classification phase to extract meta-
features and determine the final classification de-
cision. This scheme also provides a convenient
way to combine scores from multiple classifiers
used in the first classification phase. We refer
to these as base-classifiers. This scheme and the
meta-features we used are detailed hereunder.
Let us write (s, h) for a sentence-hypothesis
pair. We denote the set of pairs in the development
(training) set asD and in the test set as T . We split
D into two halves, D1 and D2. We make use of n
base-classifiers, C1, . . . , Cn, among which C? is
a designated classifier with additional roles in the
process, as described below. Classifiers may dif-
fer, for example, in their classification algorithm.
An additional meta-classifier is denoted CM . The
classification scheme is shown as Algorithm 1.
Algorithm 1 Meta-classification
Training
1: Extract features for every (s, h) in D
2: Train C1, . . . , Cn on D1
3: Classify D2, using C1, . . . , Cn
4: Extract meta-features for D2 using the
classification of C1, . . . , Cn
5: Train CM on D2
Classification
6: Extract features for every (s, h) in T
7: Classify T using C1, . . . , Cn
8: Extract meta-features for T
9: Classify T using CM
At Step 1, features are extracted for every (s, h)
pair in the training set, as in the baseline system.
773
In Steps 2 and 3 we split the training set into two
halves (taking half of each topic), train n different
classifiers on the first half and then classify the
second half using each of the n classifiers. Given
the classification scores of the n base-classifiers
to the (s, h) pairs in the second half of the train-
ing set, D2, we add in Step 4 the meta-features
described in Section 5.3.1.
After adding the meta-features, we train
(Step 5) a meta-classifier on this new set of fea-
tures. Test sentences then go through the same
process: features are extracted for them and they
are classified by the already trained n classifiers
(Steps 6 and 7), meta-features are extracted in
Step 8, and a final classification decision is made
by the meta-classifier in Step 9.
A retrieval step may precede the actual en-
tailment classification, allowing the processing of
fewer and potentially ?better? candidates.
5.3.1 Meta-features
The following features are extracted in our
meta-classification scheme:
Classification scores The classification score of
each of the n base-classifiers.
Title entailment In many texts, and in news ar-
ticles in particular, the title and the first few sen-
tences often represent the entire document?s con-
tent. Thus, knowing whether these sentences en-
tail the hypothesis may be an indicator to the gen-
eral potential of the document to include entailing
sentences. Two binary features are added accord-
ing to the classification of C? indicating whether
the title entails the hypothesis and whether the first
sentence entails it.
Second-closest entailment Considering the lo-
cality phenomenon described above, we add a fea-
ture assigning higher scores to sentences in the
vicinity of an entailment environment. This fea-
ture is computed as the distance to the second-
closest entailing sentence in the document (count-
ing the sentence itself as well), according to the
classification ofC?. Formally, let i be the index of
the current sentence and J be the set of indices of
entailing sentences in the document according to
C?. For each j ? J we compute di,j = |i?j|, and
choose the second smallest di,j as di. The idea is
Ent?
# 1110987654321
NO NOYESYESYESYESNONONONONO
d
2nd cl
oses
t
8887 or 96 or 8777777
2 3111123456
d
Close
st
8887 or 96 or 8766666
1 2111112345
Figure 1: Comparison of the closest and second-closest
schemes when applied to a bulk of entailing sentences (in
white) situated within a non-entailing environment (in gray).
Unlike the closest one, the second-closest scheme assigns
larger distance values to non-entailing sentences located on
the ?edge? of the bulk (5 and 10) than to entailing ones.
that if entailing sentences indeed always come in
bulks, then di = 1 for all entailing sentences, but
di > 1 for all non-entailing ones. Figure 1 illus-
trates such a case, comparing the second-closest
distance with the distance to the closest entailing
sentence. In the closest scheme we do not count
the sentence as closest to itself since it would dis-
regard the environment of the sentence altogether,
eliminating the desired effect. We scale the dis-
tance and add the feature score: ? log(di).
Smoothed entailment This feature addressed
the locality phenomenon by smoothing the
classification score of sentence i with the scores
of adjacent sentences, weighted by their distance
from the current sentence i. Let s(i) be the
score assigned by C? to sentence i. We add the
Smoothed Entailment feature score:
SE(i) =
?
w(b|w|?s(i+w))?
w(b|w|)
where 0 < b < 1 is the decay parameter and w is
an integer bounded between?N and N , denoting
the distance from sentence i.
1st sentence entailing title Bensley and Hickl
(2008) showed that the first sentence in a news ar-
ticle typically entails the article?s title. We there-
fore assume that in each document, s1 ? s0,
where s1 and s0 are the document?s first sentence
and title respectively. Hence, under entailment
transitivity, if s0 ? h then s1 ? h. The cor-
responding binary feature states whether the sen-
tence being classified is the document?s first sen-
tence and the title entails h according to C?.
774
P (%) R (%) F1 (%)
BIU-BL 14.53 55.25 23.00
BIU-DISC 20.82 57.25 30.53
BIU-BL3 14.86 59.00 23.74
BIU-DISCno?loc 22.35 57.12 32.13All-yes baseline 4.6 100.0 8.9
Table 1: Micro-average results.
Note that the above locality-based features rely
on high accuracy of the base classifier C?. Oth-
erwise, it will provide misleading information to
the features computation. We analyze the effect
of this accuracy in Section 6.
6 Results and Analysis
Using the RTE-5 Search data, we compare
BIUTEE in its baseline configuration (cf. Sec-
tion 4), denoted BIU-BL, with its discourse-aware
enhancement (BIU-DISC) which uses all the com-
ponents described in Section 5. To alleviate the
strong IR effect described in Section 3, both sys-
tems are applied to the complete datasets (both
training and test), without candidates pre-filtering.
BIU-DISC uses three base-classifiers (n = 3):
SVMperf (Joachims, 2006), and Na??ve Bayes and
Logistic Regression from the WEKA package
(Witten and Frank, 2005). The first among these
is set as our designated classifier C?, which is
used for the computation of the document-level
features. SVMperf is also used for the meta-
classifier. For the smoothed entailment score (cf.
Section 5.3), we used b = 0.9 and N = 3. Global
information is added by enriching each sentence
with the highest-ranking term in the document, ac-
cording to tf-idf scores (cf. Section 5.2), where
document frequencies were computed based on
about half a million documents from the TIP-
STER corpus (Harman, 1992). The set of weights
~? equals {2, 1, 4} for title terms, proper names and
locations, respectively. All parameters were tuned
based on a 10-fold cross-validation on the devel-
opment set, optimizing the micro-averaged F1.
The results are presented in Table 1. As can be
seen in the table, BIU-DISC outperforms BIU-BL in
every measure, showing the impact of addressing
discourse in this setting. To rule out the option that
the improvement is simply due to the fact that we
use three classifiers for BIU-DISC and a single one
P (%) R (%) F1 (%)
By Topic
BIU-BL 16.54 55.62 25.50
BIU-DISC 22.69 57.96 32.62
All-yes baseline 4.85 100.00 9.25
By Hypothesis
BIU-BL 22.87 59.62 33.06
BIU-DISC 27.81 61.97 38.39
All-yes baseline 4.96 100.00 9.46
Table 2: Macro-average results.
for BIU-BL, we show (BIU-BL3) the results when
the baseline system is applied in the same meta-
classification configuration as BIU-DISC, with the
same three classifiers. Apparently, without the
discourse information this configuration?s contri-
bution is limited.
As mentioned in Section 5.3, the benefit from
the locality features rely directly on the perfor-
mance of the base classifiers. Hence, considering
the low precision scores obtained here, we applied
BIU-DISC to the data in the meta-classification
scheme, but with locality features removed. The
results, shown as BIU-DISCno?loc in the Table, in-
dicate that indeed performance increases without
these features. The last line of the table shows the
results obtained by a na??ve baseline where all test-
set pairs are considered entailing.
For completeness, Table 2 shows the macro-
averaged results, when averaged over the topics or
over the hypotheses. Although we tuned our sys-
tem to maximize micro-averaged F1, these figures
comply with the ones shown in Table 1.
Analysis of locality As discussed in Section 5,
determining whether a sentence entails a hypothe-
sis should take into account whether adjacent sen-
tences also entail the hypothesis. In the above ex-
periment we were unable to show the contribution
of our system?s component that attempts to cap-
ture this information; on the contrary, the results
show it had a negative impact on performance.
Still, we claim that this information can be use-
ful when used within a more accurate system. We
try to validate this conjecture by understanding
how performance of the locality features varies as
the systems becomes more accurate. We do so via
the following simulation.
When classifying a certain sentence, the classi-
775
2025303540 0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
p
F1
Figure 2: F1 performance of BIU-DISC as a function of
the accuracy in classifying adjacent sentences.
fications of its adjacent sentences are given by an
oracle classifier that provides the correct answer
with probability p. The system is applied using
two locality features: the 1st sentence entailing
title feature and a close variant of the smoothed
entailment feature, which calculates the weighted
average of adjacent sentences, but disregards the
score of the currently evaluated sentence.3 Thus
we supply information about adjacent sentences
and test whether overall performance increases
with the accuracy of this information.
We performed this experiment for p in a range
of [0.5-1.0]. Figure 2 shows the results of this sim-
ulation, based on the average F1 of five runs for
each p. Since performance, from a certain point,
increases with the accuracy of the oracle classi-
fier, we can conclude that indeed precise infor-
mation about adjacent sentences improves perfor-
mance on the current sentence, and that locality is
a true phenomenon in the data. We note, however,
that performance improves only when accuracy is
very high, suggesting the currently limited prac-
tical potential of this information, at least in the
way locality was represented in this work.
Ablation tests Table 3 presents the results of the
ablation tests performed to evaluate the contribu-
tion of each component. Based on the result re-
ported in Table 1 and the above discussion, the
tests were performed relative to BIU-DISCno?loc,
the optimal configuration. As seen in the table,
the removal of each component causes a drop
in results. For global information we see a mi-
3The second-closest entailment feature was not used as it
considers the oracle?s decision for the current sentence, while
we wish to use only information about adjacent sentences.
Component removed F1 (%) ?F1 (%)
Previous sent. features 28.55 3.58
Augmented coref. 26.73 5.40
Global information 31.76 0.37
Table 3: Results of ablation tests relative to
BIU-DISCno?loc. The columns specify the compo-nent removed, the micro-averaged F1 score achieved without
it, and the marginal contribution of the component.
nor difference, which is not surprising considering
the conservative approach we took, using a sin-
gle global term for each sentence. Possibly, this
information is also included in the other compo-
nents, thus proving no marginal contribution rel-
ative to them. Under the conditions of an over-
whelming majority of negative examples, this is
a risky method to use, and should be considered
when the ratio of positive examples is higher. For
future work, we intend to use this information via
classification features (e.g. the coverage obtained
with and without global information), rather than
the crude addition of the term to the sentence.
Analysis of augmented coreferences We an-
alyzed the performance of the component for
augmenting coreference relations relative to the
OpenNLP resolver. Recall that our component
works on top of the resolver?s output and can add
or remove coreference relations. As a complete
annotation of coreference chains in the dataset is
unavailable, we performed the following evalua-
tion. Recall is computed based on the number
of identified pairs from a sample of 100 intra-
document coreference and bridging relations from
the annotated dataset described in (Mirkin et al,
2010). Precision is computed based on 50 pairs
sampled from the output of each method, equally
distributed over topics. The results, shown in Ta-
ble 4, indicate the much higher recall obtained
by our component at some cost in precision. Al-
though rather simple, the ablation test of this com-
ponent shows its usefulness. Still, both methods
achieve low absolute recall, suggesting the need
for more robust tools for this task.
P (%) R (%) F1 (%)
OpenNLP 74 16 26.3
Augmented coref. 60 28 38.2
Table 4: Performance of coreference methods.
776
05101520253035404550 0
10
20
30
40
50
60
70
80
90
100
k
F1
BIU-B
L
BIU-D
ISC
Lucen
e
Figure 3: F1 performance as a function of the number of
retrieved candidates.
Candidate retrieval setting As mentioned in
Section 3, best performance of RTE systems in the
task was obtained when applying a first step of IR-
based candidate filtering. We therefore compare
the performance of BIU-DISC with that of BIU-BL
under this setting as well.4 For candidate retrieval
we used Lucene, a state of the art search engine5,
in a range of top-k retrieved candidates. The re-
sults are shown in Figure 3. For reference, the fig-
ure also shows the performance along this range
of Lucene as-is, when no further inference is ap-
plied to the retrieved candidates.
While BIU-DISC does not outperform BIU-BL at
every point, the area under the curve is clearly
larger for BIU-DISC. The figure also indicates that
BIU-DISC is far more robust, maintaining a stable
F1 and enabling a stable tradeoff between recall
and precision along the whole range (recall ranges
between 42% and 55% for k ? [15 ? 100], with
corresponding precision range of 51% to 33%).
Finally, Table 5 shows the results of the best
systems as determined in our first experiment.
We performed a single experiment to compare
BIU-DISCno?loc and BIU-BL3 under a candidate re-
trieval setting, using k = 20, where both systems
highly perform. We compare these results to the
highest score obtained by Lucene, as well as to the
two best submissions to the RTE-5 Search task6.
BIU-DISCno?loc outperforms all other methods and
its result is significantly better than BIU-BL3 with
p < 0.01 according to McNemar?s test.
4This time, for global information, the document?s three
highest ranking terms were added to each sentence.
5http://lucene.apache.org
6The best one is an earlier version of this work (Mirkin et
al., 2009); the second is MacKinlay and Baldwin?s (2009).
P (%) R (%) F1 (%)
BIU-DISCno?loc 50.77 45.12 47.78
BIU-BL3 51.68 40.38 45.33
Lucene, top-15 35.93 52.50 42.66
RTE-5 best 40.98 51.38 45.59
RTE-5 second-best 42.94 38.00 40.32
Table 5: Performance of best configurations.
7 Conclusions
While it is generally assumed that discourse inter-
acts with semantic entailment inference, the con-
crete impacts of discourse on such inference have
been hardly explored. This paper presented a first
empirical investigation of discourse processing
aspects related to entailment. We argue that avail-
able discourse processing tools should be substan-
tially improved towards this end, both in terms of
the phenomena they address today, namely nom-
inal coreference, and with respect to the cover-
ing of additional phenomena, such as bridging
anaphora. Our experiments show that even rather
simple methods for addressing discourse can have
a substantial positive impact on the performance
of entailment inference. Concerning the local-
ity phenomenon stemming from discourse coher-
ence, we learned that it does carry potentially use-
ful information, which might become beneficial
in the future when better-performing entailment
systems become available. Until then, integrating
this information with entailment confidence may
be useful. Overall, we suggest that entailment sys-
tems should extensively incorporate discourse in-
formation, while developing sound algorithms for
addressing various discourse phenomena, includ-
ing the ones described in this paper.
Acknowledgements
The authors are thankful to Asher Stern and Ilya
Kogan for their help in implementing and evalu-
ating the augmented coreference component, and
to Roy Bar-Haim for useful advice concerning
this paper. This work was partially supported
by the Israel Science Foundation grant 1112/08
and the PASCAL-2 Network of Excellence of the
European Community FP7-ICT-2007-1-216886.
Jonathan Berant is grateful to the Azrieli Foun-
dation for the award of an Azrieli Fellowship.
777
References
Bar-Haim, Roy, Jonathan Berant, Ido Dagan, Iddo
Greental, Shachar Mirkin, Eyal Shnarch, and Idan
Szpektor. 2008. Efficient semantic deduction and
approximate matching over compact parse forests.
In Proc. of Text Analysis Conference (TAC).
Bar-Haim, Roy, Jonathan Berant, and Ido Dagan.
2009. A compact forest for scalable inference
over entailment and paraphrase rules. In Proc. of
EMNLP.
Bensley, Jeremy and Andrew Hickl. 2008. Unsuper-
vised resource creation for textual inference appli-
cations. In Proc. of LREC.
Bentivogli, Luisa, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, Medea Lo Leggio, and Bernardo
Magnini. 2009a. Considering discourse refer-
ences in textual entailment annotation. In Proc. of
the 5th International Conference on Generative Ap-
proaches to the Lexicon (GL2009).
Bentivogli, Luisa, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009b. The
fifth PASCAL recognizing textual entailment chal-
lenge. In Proc. of TAC.
Castillo, Julio J. 2009. Sagan in TAC2009: Using
support vector machines in recognizing textual en-
tailment and TE search pilot task. In Proc. of TAC.
Clark, Peter and Phil Harrison. 2009. An inference-
based approach to recognizing entailment. In Proc.
of TAC.
Clark, Herbert H. 1975. Bridging. In Schank, R. C.
and B. L. Nash-Webber, editors, Theoretical issues
in natural language processing, pages 169?174. As-
sociation of Computing Machinery.
Dagan, Ido, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Machine Learning Challenges, vol-
ume 3944 of Lecture Notes in Computer Science,
pages 177?190. Springer.
Dagan, Ido, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Natural Language
Engineering, pages 15(4):1?17.
Dali, Lorand, Delia Rusu, Blaz Fortuna, Dunja
Mladenic, and Marko Grobelnik. 2009. Question
answering based on semantic graphs. In Proc. of the
Workshop on Semantic Search (SemSearch 2009).
Fellbaum, Christiane, editor. 1998. WordNet: An
Electronic Lexical Database (Language, Speech,
and Communication). The MIT Press.
Finkel, Jenny Rose, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proc. of ACL.
Giampiccolo, Danilo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third PASCAL recog-
nizing textual entailment challenge. In Proc. of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing.
Harabagiu, Sanda and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain ques-
tion answering. In Proc. of ACL.
Harman, Donna. 1992. The DARPA TIPSTER
project. SIGIR Forum, 26(2):26?28.
Joachims, Thorsten. 2006. Training linear SVMs in
linear time. In Proc. of the ACM Conference on
Knowledge Discovery and Data Mining (KDD).
Li, Fangtao, Yang Tang, Minlie Huang, and Xiaoyan
Zhu. 2009. Answering opinion questions with ran-
dom walks on graphs. In Proc. of ACL-IJCNLP.
Litkowski, Ken. 2009. Overlap analysis in textual en-
tailment recognition. In Proc. of TAC.
MacKinlay, Andrew and Timothy Baldwin. 2009. A
baseline approach to the RTE5 search pilot. In Proc.
of TAC.
Mirkin, Shachar, Roy Bar-Haim, Jonathan Berant, Ido
Dagan Eyal Shnarch, Asher Stern, and Idan Szpek-
tor. 2009. Addressing discourse and document
structure in the RTE search task. In Proc. of TAC.
Mirkin, Shachar, Ido Dagan, and Sebastian Pado?.
2010. Assessing the role of discourse references in
entailment inference. In Proc. of ACL.
Nenkova, Ani, Rebecca Passonneau, and Kathleen
Mckeown. 2007. The pyramid method: incorpo-
rating human content selection variation in summa-
rization evaluation. In ACM Transactions on Speech
and Language Processing.
Qiu, Long, Min-Yen Kan, and Tat-Seng Chua. 2004.
A public reference implementation of the RAP
anaphora resolution algorithm. In Proc. of LREC.
Romano, Lorenza, Milen Kouylekov, Idan Szpektor,
and Ido Dagan. 2006. Investigating a generic
paraphrase-based approach for relation extraction.
In Proc. of EACL.
Witten, Ian H. and Eibe Frank. 2005. Data Mining:
Practical machine learning tools and techniques,
2nd Edition. Morgan Kaufmann, San Francisco.
778
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 194?204, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Learning Verb Inference Rules from Linguistically-Motivated Evidence
Hila Weisman?, Jonathan Berant?, Idan Szpektor?, Ido Dagan?
? Computer Science Department, Bar-Ilan University
? The Blavatnik School of Computer Science, Tel Aviv University
? Yahoo! Research Israel
{weismah1,dagan}@cs.biu.ac.il
{jonatha6}@post.tau.ac.il
{idan}@yahoo-inc.com
Abstract
Learning inference relations between verbs is
at the heart of many semantic applications.
However, most prior work on learning such
rules focused on a rather narrow set of in-
formation sources: mainly distributional sim-
ilarity, and to a lesser extent manually con-
structed verb co-occurrence patterns. In this
paper, we claim that it is imperative to uti-
lize information from various textual scopes:
verb co-occurrence within a sentence, verb co-
occurrence within a document, as well as over-
all corpus statistics. To this end, we propose
a much richer novel set of linguistically mo-
tivated cues for detecting entailment between
verbs and combine them as features in a su-
pervised classification framework. We empir-
ically demonstrate that our model significantly
outperforms previous methods and that infor-
mation from each textual scope contributes to
the verb entailment learning task.
1 Introduction
Inference rules are an important building block of
many semantic applications, such as Question An-
swering (Ravichandran and Hovy, 2002) and In-
formation Extraction (Shinyama and Sekine, 2006).
For example, given the sentence ?Churros are
coated with sugar?, one can use the rule ?coat ?
cover? to answer the question ?What are Churros
covered with??. Inference rules specify a directional
inference relation between two text fragments, and
we follow the Textual Entailment modeling of infer-
ence (Dagan et al2006), which refers to such rules
as entailment rules. In this work we focus on one
of the most important rule types, namely, lexical en-
tailment rules between verbs (verb entailment), e.g.,
?whisper ? talk?, ?win ? play? and ?buy ? own?.
The significance of such rules has led to active re-
search in automatic learning of entailment rules be-
tween verbs or verb-like structures (Zanzotto et al
2006; Abe et al2008; Schoenmackers et al2010).
Most prior efforts to learn verb entailment rules
from large corpora employed distributional similar-
ity methods, assuming that verbs are semantically
similar if they occur in similar contexts (Lin, 1998;
Berant et al2012). This led to the automatic ac-
quisition of large scale knowledge bases, but with
limited precision. Fewer works, such as VerbOcean
(Chklovski and Pantel, 2004), focused on identi-
fying verb entailment through verb instantiation of
manually constructed patterns. For example, the
sentence ?he scared and even startled me? implies
that ?startle? scare?. This led to more precise rule
extraction, but with poor coverage since contrary
to nouns, in which patterns are common (Hearst,
1992), verbs do not co-occur often within rigid pat-
terns. However, verbs do tend to co-occur in the
same document, and also in different clauses of the
same sentence.
In this paper, we claim that on top of standard
pattern-based and distributional similarity methods,
corpus-based learning of verb entailment can greatly
benefit from exploiting additional linguistically-
motivated cues that are specific to verbs. For in-
stance, when verbs co-occur in different clauses of
the same sentence, the syntactic relation between the
clauses can be viewed as a proxy for the semantic re-
lation between the verbs. Moreover, we claim that to
194
improve performance it is crucial to combine infor-
mation sources from different textual scopes: verb
co-occurrence within a sentence and within a docu-
ment, distributional similarity over the entire corpus,
etc.
Our contribution in this paper is two-fold. First,
we suggest a novel set of entailment indicators that
help to detect the likelihood of verb entailment.
Our novel indicators are specific to verbs and are
linguistically-motivated. Second, we encode our
novel indicators as features within a supervised clas-
sification framework and integrate them with other
standard features adapted from prior work. This re-
sults in a supervised corpus-based learning method
that combines verb entailment information at the
sentence, document and corpus levels.
We test our model on a manually labeled data
set, and show that it outperforms the best perform-
ing previous work by 24%. In addition, we ex-
amine the effectiveness of indicators that operate at
the sentence-level, document-level and corpus-level.
This analysis reveals that using a rich and diverse
set of indicators that capture sentence-level interac-
tions between verbs substantially improves verb en-
tailment detection.
2 Background
The main approach for learning entailment rules be-
tween verbs and verb-like structures has employed
the distributional hypothesis, which assumes that
words with similar meanings appear in similar con-
texts. For example, we expect the words ?buy? and
?purchase? to occur with similar subjects and objects
in a large corpus. This observation has led to ample
work on developing both symmetric and directional
similarity measures that attempt to capture semantic
relations between lexical items by comparing their
neighborhood context (Lin, 1998; Weeds and Weir,
2003; Geffet and Dagan, 2005; Szpektor and Dagan,
2008; Kotlerman et al2010).
A far less explored direction for learning verb en-
tailment involves exploiting verb co-occurrence in
a sentence or a document. One prominent work
is Chklovsky and Pantel?s VerbOcean (2004). In
VerbOcean, the authors manually constructed 33
patterns and divided them into five pattern groups,
where each group signals one of the following five
semantic relations: similarity, strength, antonymy,
enablement and happens-before. For example, the
pattern ?Xed and later Yed? signals the happens-
before relation between the verbs ?X? and ?Y?. Start-
ing with candidate verb pairs based on a distribu-
tional similarity measure, the patterns are used to
choose a semantic relation per verb pair based on
the different patterns this pair instantiates. This
method is more precise than distributional similarity
approaches, but it is highly susceptible to sparseness
issues, since verbs do not typically co-occur within
rigid patterns. Utilizing verb co-occurrence at the
document level, Chambers and Jurafsky (2008) es-
timate whether a pair of verbs is narratively related
by counting the number of times the verbs share an
argument in the same document. In a similar man-
ner, Pekar (2008) detects entailment rules between
templates from shared arguments within discourse-
related clauses in the same document.
Recently, supervised classification has become
standard in performing various semantic tasks.
Mirkin et al2006) introduced a system for learn-
ing entailment rules between nouns (e.g., ?novel?
book?) that combines distributional similarity and
Hearst patterns as features in a supervised clas-
sifier. Pennacchiotti and Pantel (2009) augment
Mirkin et alfeatures with web-based features for
the task of entity extraction. Hagiwara et al2009)
perform synonym identification based on both dis-
tributional and contextual features. Tremper (2010)
extract ?loose? sentence-level features in order to
identify the presupposition relation (e.g., , the verb
?win? presupposes the verb ?play?). Last, Be-
rant et al2012) utilized various distributional
similarity features to identify entailment between
lexical-syntactic predicates.
In this paper, we follow the supervised approach
for semantic relation detection in order to identify
verb entailment. While we utilize and adapt useful
features from prior work, we introduce a diverse set
of novel features for the task, effectively combining
verb co-occurrence information at the sentence, doc-
ument, and corpus levels.
3 Linguistically-Motivated Indicators
As mentioned in Section 1, verbs behave quite dif-
ferently from nouns in corpora. In this section, we
195
introduce linguistically motivated indicators that are
specific to verbs and may signal the semantic re-
lation between verb pairs. Then, in Section 4 we
describe how these indicators are exactly encoded
as features within a supervised classification frame-
work.
Verb co-occurrence When (non-auxiliary) verbs
co-occur in a sentence, they are often the main verbs
of different clauses. We thus aim to use information
about the relation between clauses to learn about
the relation between the clauses? main verbs. Dis-
course markers (Hobbs, 1979; Schiffrin, 1988) are
lexical terms such as ?because? and ?however? that
indicate a semantic relation between discourse frag-
ments (i.e., propositions or speech acts). We suggest
that these markers can indicate semantic relations
between the main verbs of the connected clauses.
For example, in the sentence ?He always snores
while he sleeps?, the marker ?while? indicates a tem-
poral relation between the clauses, indicating that
?snoring? occurs while ?sleeping? (and so ?snore?
sleep?).
Often the relation between clauses is not ex-
pressed explicitly with an overt discourse marker,
but is still implied by the syntactic structure of
the sentence. For example, in dependency parsing
the relation can be captured by labeled dependency
edges expressing that one clause is an adverbial ad-
junct of the other, or that two clauses are coordi-
nated. This can indicate the existence (or lack) of
entailment between verbs. For instance, in the sen-
tence ?When I walked into the room, he was working
out?, the verb ?walk? is an adverbial adjunct of the
verb ?work out?. Such co-occurrence structure does
not indicate a deep semantic relation, such as entail-
ment, between the two verbs.
Verb classes Verb classes are sets of semantically-
related verbs sharing some linguistic properties
(Levin, 1993). One of the most general verb classes
are stative vs. event verbs (Jackendoff, 1983). Sta-
tive verb, such as ?love? and ?think?, usually describe
a state that lasts some time. On the other hand, event
verbs, such as ?run? and ?kiss?, describe an action.
We hypothesize that verb classes are relevant for de-
termining entailment, for example, that stative verbs
are not likely to entail event verbs.
Verb generality Verb-particle constructions are
multi-word expressions consisting of a head verb
and a particle, e.g., switch off (Baldwin and Villav-
icencio, 2002). We conjecture that the more gen-
eral a verb is, the more likely it is to appear with
many different particles. Detecting verb generality
can help us tackle an infamous property of distribu-
tional similarity methods, namely, the difficulty in
detecting the direction of entailment (Berant et al
2012). For example, the verb ?cover? appears with
many different particles such as ?up? and ?for?, while
the verb ?coat? does not. Thus, assuming we have
evidence for an entailment relation between the two
verbs, this indicator can help us discern the direction
of entailment and determine that ?coat? cover?.
Typed Distributional Similarity As discussed in
section 2, distributional similarity is the most com-
mon source of information for learning semantic re-
lations between verbs. Yet, we suggest that on top
of standard distributional similarity measures, which
take several verbal arguments into account (such as
subject, object, etc.) simultaneously, we should also
focus on each type of argument independently. In
particular, we apply this approach to compute simi-
larity between verbs based on the set of adverbs that
modify them. Our hypothesis is that adverbs may
contain relevant information for capturing the direc-
tion of entailment. If a verb appears with a small set
of adverbs, it is more likely to be a specific verb that
already conveys a specific action or state, making an
additional adverb redundant. For example, the verb
?whisper? conveys a specific manner of talking and
will probably not appear with the adverb ?loudly?,
while the verb ?talk? is more likely to appear with
such an adverb. Thus, measuring similarity based
solely on adverb modifiers could reveal this phe-
nomenon.
4 Supervised Entailment Detection
In the previous section, we discussed linguistic ob-
servations regarding novel indicators that may help
in detecting entailment relations between verbs. We
next describe how to incorporate these indicators as
features within a supervised framework for learning
lexical entailment rules between verbs. We follow
prior work on supervised lexical semantics (Mirkin
et al2006; Hagiwara et al2009; Tremper, 2010)
196
and address the rule learning task as a classification
task. Specifically, given an ordered verb pair (v1, v2)
as input, we learn a classifier that detects whether the
entailment relation ?v1? v2? holds for this pair.
We next detail how our novel indicators, as well
as other diverse sources of information found useful
in prior work, are encoded as features. Then, we
describe the learning model and our feature analysis
procedure.
4.1 Entailment features
Most of our features are based on information ex-
tracted from the target verb pair co-occurring within
varying textual scopes (sentence, document, cor-
pus). Hence, we group the features according to
their related scope. Naturally, when the scope is
small, i.e., at a sentence level, the semantic rela-
tion between the verbs is easier to discern but the
information may be sparse. Conversely, when co-
occurrence is loose the relation is harder to discern
but coverage is increased.
4.1.1 Sentence-level co-occurrence
We next detail features that address co-occurrence
of the target verb pair within a sentence. These in-
clude our novel linguistically-motivated indicators,
as well as features that were adapted from prior
work.
Discourse markers As discussed in Section 3,
discourse markers may signal relations between the
main verbs of adjacent clauses. The literature is
abundant with taxonomies that classify markers to
various discourse relations (Mann and Thompson,
1988; Hovy and Maier, 1993; Knott and Sanders,
1998). Inspired by Marcu and Echihabi (2002), we
employ markers that are mapped to four discourse
relations ?Contrast?, ?Cause?, ?Condition? and ?Tem-
poral?, as specified in Table 1. This definition
can be viewed as a relaxed version of VerbOcean?s
(Chklovski and Pantel, 2004) patterns, although the
underlying intuition is different (see Section 3).
For a target verb pair (v1, v2) and each discourse
relation r, we count the number of times that v1 is
the main verb in the main clause, v2 is the main verb
in the subordinate clause, and the clauses are con-
nected via a marker mapped to r. For example, given
the sentence ?You must enroll in the competition be-
fore you can participate in it?, the verb pair (?en-
roll?,?participate?) appears in the ?Temporal? rela-
tion, indicated by the marker ?before?, where ?enroll?
is in the main clause. Each count is then normalized
by the total number of times (v1, v2) appear with any
marker. The same procedure is done when v1 is in
the subordinate clause and v2 in the main clause. We
term the features by the relevant discourse relation,
e.g., ?v1-contrast-v2? refers to v1 being in the main
clause and connected to the subordinate clause via a
contrast marker.
Dependency relations between clauses As noted
in Section 3, the syntactic structure of verb co-
occurrence can indicate the existence or lack of en-
tailment. In dependency parsing this may be ex-
pressed via the label of the dependency relation con-
necting the main and subordinate clauses. In our ex-
periments we used the ukWaC corpus1 (Baroni et al
2009) which was parsed by the MALT parser (Nivre
et al2006). Hence, we identified three MALT de-
pendency relations that connect a main clause with
its subordinate clause. The first relation is the object
complement relation ?obj?. In this case the subor-
dinate clause is an object complement of the main
clause. For example, in ?it surprised me that the
lizard could talk? the verb pair (?surprise?,?talk?) is
connected by the ?obj? relation. The second rela-
tion is the adverbial adjunct relation ?adv?, in which
the subordinate clause is adverbial and describes the
time, place, manner, etc. of the main clause, e.g., ?he
gave his consent without thinking about the reper-
cussions?. The last relation is the coordination rela-
tion ?coord?, e.g., ?every night my dog Lucky sleeps
on the bed and my cat Flippers naps in the bathtub?.
Similar to discourse markers, we compute for
each verb pair (v1,v2) and each dependency label d
the proportion of times that v1 is the main verb of the
main clause, v2 is the main verb of the subordinate
clause, and the clauses are connected by dependency
relation d, out of all the times they are connected by
any dependency relation. We term the features by
the dependency label, e.g., ?v1-adv-v2? refers to v1
being in the main clause and connected to the subor-
dinate clause via an adverbial adjunct.
1http://wacky.sslmit.unibo.it/doku.php?
id=corpora
197
Discourse Rel. Discourse Markers
Contrast although , despite , but , whereas , notwithstanding , though
Cause because , therefore , thus
Condition if , unless
Temporal whenever , after , before , until , when , finally , during , afterwards , meanwhile
Table 1: Discourse relations and their mapped markers.
Pattern-based We follow Chklovski and Pan-
tel (2004) and extract occurrences of VerbOcean pat-
terns that are instantiated by the target verb pair. As
mentioned in Section 2, VerbOcean patterns were
originally grouped into five semantic classes. Based
on a preliminary study we conducted, we decided
to utilize only four strength-class patterns as posi-
tive indicators for entailment, e.g., ?he scared and
even startled me?, and three antonym-class patterns
as negative indicators for entailment, e.g., ?you can
either open or close the door?. We note that these
patterns are also commonly used by RTE systems2.
Since the corpus pattern counts were very sparse,
we defined for a target verb pair (v1, v2) two bi-
nary features: the first denotes whether the verb
pair instantiates at least one positive pattern, and
the second denotes whether the verb pair instanti-
ates at least one negative pattern. For example, given
the aforementioned sentences, the value of the pos-
itive feature for the verb pair (?startle?,?scare?) is
?1?. Patterns are directional, and so the value of
(?scare?,?startle?) is ?0?.
Polarity We compute the proportion of times that
the two verbs appear in different polarity. For exam-
ple, in ?he didn?t say why he left?, the verb ?say? ap-
pears in negative polarity and the verb ?leave? in pos-
itive polarity. Such change in polarity is usually an
indicator of non-entailment between the two verbs.
Tense ordering The temporal relation between
verbs may provide information about their seman-
tic relation. For each verb pair co-occurrence, we
extract the verbs? tenses and order them as follows:
past < present < future. We then add the fea-
tures ?tense-v1<tense-v2?, ?tense-v1=tense-v2?, and
?tense-v1>tense-v2?, corresponding to the propor-
2http://aclweb.org/aclwiki/index.php?
title=RTE_Knowledge_Resources#Ablation_
Tests
tion of times the tense of v1 is smaller, equal to,
or bigger than the tense of v2. This indicates the
prevalent temporal relation between the verbs in the
corpus and may assist in detecting the direction of
entailment. e.g., if tense-v1>tense-v2, the verb pair
is less likely to entail.
Co-reference Following Tremper (2010), in every
co-occurrence of (v1,v2) we extract for each verb
the set of arguments at either the subject or object
positions, denoted A1 and A2 (for v1 and v2, re-
spectively). We then compute the proportion of co-
occurrences in which v1 and v2 share an argument,
i.e., A1 ? A2 6= ?, out of all the co-occurrences in
which bothA1 andA2 are non-empty. The intuition,
which is similar to distributional similarity, is that
semantically related verbs tend to share arguments.
Syntactic and lexical distance Following Trem-
per (2010) again, we compute the average distance
d in dependency edges between the co-occurring
verbs. We compute three features corresponding to
three bins indicating if d < 3, 3 ? d ? 7, or
d > 7. Similar features are computed for the dis-
tance in words (bins are 0 < d < 5, 5 ? d ? 10,
d > 10). This feature provides insight into the syn-
tactic relatedness of the verbs.
Sentence-level pmi Pointwise mutual information
(pmi) between v1 and v2 is computed, where the co-
occurrence scope is a sentence. Higher pmi should
hint at semantically related verbs.
4.1.2 Document-level co-occurrence
This group of features addresses co-occurrence of
a target verb pair within the same document. These
features are less sparse, but tend to capture coarser
semantic relations between the target verbs.
Narrative score Chambers and Jurafsky (2008)
suggested a method for learning sequences of ac-
tions or events (expressed by verbs) in which a sin-
198
gle entity is involved. They proposed a pmi-like nar-
rative score (see Eq. (1) in their paper) that esti-
mates whether a pair consisting of a verb and one
of its dependency relations (v1, r1) is narratively-
related to another such pair (v2, r2). Their estima-
tion is based on quantifying the likelihood that two
verbs will share an argument that instantiates both
the dependency position (v1, r1) and (v2, r2) within
documents in which the two verbs co-occur. For ex-
ample, given the document ?Lindsay was prosecuted
for DUI. Lindsay was convicted of DUI.? the pairs
(?prosecute?,?subj?) and (?convict?,?subj?) share the
argument ?Lindsay? and are part of a narrative chain.
Such narrative relations may provide cues to the se-
mantic relatedness of the verb pair.
We compute for every target verb pair nine fea-
tures using their narrative score. In four features,
r1 = r2 and the common dependency is either a sub-
ject, an object, a preposition complement (e.g., ?we
meet at the station.), or an adverb (termed chamb-
subj, chamb-obj, and so on). In the next three fea-
tures, r1 6= r2 and r1, r2 denote either a subject,
object, or preposition complement3 (termed chamb-
subj-obj and so on). Last, we add as features the
average of the four features where r1 = r2 (termed
chamb-same), and the average of the three features
where r1 6= r2 (termed chamb-diff ).
Document-level pmi Similar to sentence-level
pmi, we compute the pmi between v1 and v2, but
this time the co-occurrence scope is a document.
4.1.3 Corpus-level statistics
The final group of features ignores sentence or
document boundaries and is based on overall corpus
statistics.
Distributional similarity Following our hypoth-
esis regarding typed distributional similarity (Sec-
tion 3), we first compute for each verb and each
argument (subject, object, preposition complement
and adverb) a separate vector that counts the num-
ber of times each word in the corpus instantiates
the argument of that verb. In addition, we also
compute a vector that is the concatenation of the
previous separate vectors, which captures the stan-
dard distributional similarity statistics. We then
3adverbs never instantiate the subject, object or preposition
complement positions.
apply three state-of-the-art distributional similarity
measures, Lin (Lin, 1998), Weeds precision (Weeds
and Weir, 2003) and BInc (Szpektor and Dagan,
2008), to compute for every verb pair a similarity
score between each of the five count vectors4. We
term each feature by the method and argument, e.g.,
weeds-prep and lin-all represent the Weeds measure
over prepositional complements and the Lin mea-
sure over all arguments.
Verb classes Following our discussion in Sec-
tion 3, we first measure for each target verb v a ?sta-
tive? feature f by computing the proportion of times
it appears in progressive tense, since stative verbs
usually do not appear in the progressive tense (e.g.,
?knowing?). Then, given a verb pair (v1,v2) and their
corresponding stative features f1 and f2, we add two
features f1 ? f2 and
f1
f2
, which capture the interaction
between the verb classes of the two verbs.
Verb generality For each verb, we add as a feature
the number of different particles it appears with in
the corpus, following the hypothesis that this is a
cue to its generality. Then, given a verb pair (v1,v2)
and their corresponding features f1 and f2, we add
the feature f1f2 . We expect that when
f1
f2
is high, v1 is
more general than v2, which is a negative entailment
indicator.
4.2 Learning model and feature analysis
The total number of features in our model as de-
scribed above is 63. We combine the features in
a supervised classification framework with a linear
SVM. Since our model contains many novel fea-
tures, it is important to investigate their utility for
detecting verb entailment. To that end, we employ
feature ranking methods as suggested by Guyon et
al. (2003). In feature ranking methods, features are
ranked by some score computed for each feature in-
dependently. In this paper we use Pearson correla-
tion between the feature values and the correspond-
ing labels as the ranking criterion.
4We employ the common practice of using the pmi between
a verb and an argument rather than the argument count as the
argument?s weight.
199
5 Evaluation and Analysis
5.1 Experimental Setting
To evaluate our proposed supervised model, we con-
structed a dataset containing labeled verb pairs. We
started by randomly sampling 50 verbs out of the
common verbs in the RCV1 corpus5, which we de-
note here as seed verbs. Next, we extracted the 20
most similar verbs to each seed verb according to
the Lin similarity measure (Lin, 1998), which was
computed on the RCV1 corpus. Then, for each seed
verb vs and one of its extracted similar verbs vis we
generated the two directed pairs (vs, vis) and (v
i
s, vs),
which represent the candidate rules ?vs ? vis? and
?vis ? vs? respectively. To reduce noise, we filtered
out verb pairs where one of the verbs is an auxiliary
or a light verb such as ?do?, ?get? and ?have?. This
step resulted in 812 verb pairs as our dataset6, which
were manually annotated by the authors as repre-
senting a valid entailment rule or not. To annotate
these pairs, we generally followed the rule-based ap-
proach for entailment rule annotation, where a rule
?v1 ? v2? is considered as correct if the annotator
could think of reasonable contexts under which the
rule holds (Dekang and Pantel, 2001; Szpektor et
al., 2004). In total 225 verb pairs were labeled as
entailing (the rule ?v1 ? v2? was judged as correct)
and 587 verb pairs were labeled as non-entailing (the
rule ?v1 ? v2? was judged as incorrect). The Inter-
Annotator Agreement (IAA) for a random sample of
100 pairs was moderate (0.47), as expected from the
rule-based approach (Szpektor et al2007).
For each verb pair, all 63 features within our
model (Section 4) were computed using the ukWaC
corpus (Baroni et al2009), which contains 2 billion
words. For classification, we utilized SVM-perf?s
(Joachims, 2005) linear SVM implementation with
default parameters, and evaluated our model by per-
forming 10-fold cross validation (CV) over the la-
beled dataset.
5http://trec.nist.gov/data/reuters/
reuters.html
6The data set is available at http://www.cs.biu.ac.
il/?nlp/downloads/verb-pair-annotation.
html
5.2 Feature selection and analysis
As discussed in Section 4.2, we followed the feature
ranking method proposed by Guyon et al2003) to
investigate the utility of our proposed features. Ta-
ble 2 depicts the 10 most positively and negatively
correlated features with entailment according to the
Pearson correlation measure
From Table 2, it is clear that distributional simi-
larity features are amongst the most positively cor-
related with entailment, which is in line with prior
work (Geffet and Dagan, 2005; Kotlerman et al
2010). Looking more closely, our suggestion for
typed distributional similarity proved to be useful,
and indeed most of the highly correlated distribu-
tional similarity features are typed measures. Stand-
ing out are the adverb-typed measures, with two fea-
tures in the top 10, including the highest, ?Weeds-
adverb?, and ?BInc-adverb?. We also note that the
highly correlated distributional similarity measures
are directional, Weeds and BInc.
The table also indicates that document-level co-
occurrence contributes positively to entailment de-
tection. This includes both the Chambers narrative
measure, with the typed feature Chambers-obj, and
document-level PMI, which captures a more loose
co-occurrence relationship between verbs. Again,
we point at the significant correlation of our novel
typed measures with verb entailment, in this case the
typed narrative measure.
Last, our feature analysis shows that many of our
novel co-occurrence features at the sentence level
contribute useful negative information. For exam-
ple, verbs connected via an adverbial adjunct (?v2-
adverb-v1?) or an object complement (?v1-obj-v2?)
are negatively correlated with entailment. In addi-
tion, the novel ?verb generality? feature as well as
the tense difference feature (?tense-v1 > tense-v2?)
are also strong negative indicators. On the other
hand, ?v2-coord-v1? is positively correlated with en-
tailment. This shows that encoding various aspects
of verb co-occurrence at the sentence level can lead
to better prediction of verb entailment. Finally, we
note that PMI at the sentence level is highly corre-
lated with entailment even more than at the docu-
ment level, since the local textual scope is more in-
dicative, though sparser.
To conclude, our feature analysis shows that fea-
200
Rank Top Positive Top Negative
1 Weeds-adverb tense-v1 > tense-v2
2 Sentence-level PMI v2-adverb-v1 co-occurrence
3 Weeds-subj v2-obj-v1 co-occurrence
4 Weeds-prep v1-obj-v2 co-occurrence
5 Weeds-all v1-adverb-v2 co-occurrence
6 Chambers-obj verb generality f1f2
7 v2-coord-v1 co-occurrence v1-contrast-v2
8 BInc-adverb tense-v1 < tense-v2
9 Document-level PMI lexical-distance 0-5
10 Chambers-same Lin-subj
Table 2: Top 10 positive and negative features according to the Pearson correlation score.
tures at all levels: sentence, document and corpus,
contain useful information for entailment detection,
both positive and negative, and should be combined
together. Moreover, many of our novel features are
among the highly correlated features, showing that
devising a rich set of verb-specific and linguistically-
motivated features provides better discriminative ev-
idence for entailment detection.
5.3 Results and Analysis
We compared our method to the following baselines
which were mostly taken from or inspired by prior
work:
Random: A simple decision rule: for any
pair (v1, v2), randomly classify as ?yes? with a
probability equal to the number of entailing verb
pairs out of all verb pairs in the labeled dataset (i.e.,
225
812 = 0.277).
VO-KB: A simple unsupervised rule: for any
pair (v1, v2), classify as ?yes? if the pair appears in
the strength relation (corresponding to entailment)
in the VerbOcean knowledge-base, which was com-
puted over Web counts.
VO-ukWaC: A simple unsupervised rule: for any
pair (v1, v2), classify as ?yes? if the value of the
positive VerbOcean feature is ?1? (Section 4.1, com-
puted over ukWaC).
TDS: Include only the 15 distributional similarity
features in our supervised model. This baseline ex-
tends Berant et al2012), who trained an entailment
Method P% R% AUC F1
All 40.2 71.0 0.65 0.51
TDS+VO 36.8 53.2 0.58 0.41
TDS 34.6 44.8 0.56 0.37
Random 27.9 28.8 0.51 0.28
VO-KB 33.1 14.8 0.53 0.2
VO-ukWaC 23.3 4.7 0.29 0.08
Table 3: Average precision, recall, AUC and F1 for our
method and the baselines.
classifier over several distributional similarity fea-
tures, and provides an evaluation of the discrimina-
tive power of distributional similarity alone, without
co-occurrence features.
TDS+VO: Include only the 15 typed distribu-
tional similarity features and the two VerbOcean
features in our supervised model. This baseline
is inspired by Mirkin et al2006), who combined
distributional similarity features and Hearst pat-
terns (Hearst, 1992) for learning entailment between
nouns.
All: Our full-blown model, including all features
described in Section 4.1.
For all tested methods, we performed 10-fold
cross validation and averaged Precision, Recall,
Area under the ROC curve (AUC) and F1 over the 10
folds. Table 3 presents the results of our full-blown
model as well as the baselines.
First, we note that, as expected, the VerbOcean
baselines VO-KB and VO-ukWaC provide low recall,
201
Method P% R% AUC F1
All 40.2 71.0 0.65 0.51
Sent+Corpus-level 39.7 70.4 0.64 0.50
Sent+Doc-level 39.0 70.0 0.63 0.50
Doc+Corpus-level 37.7 64.0 0.62 0.47
Sent-level 35.8 63.8 0.59 0.46
Doc-level 30.0 45.4 0.52 0.35
Corpus-level 35.4 58.1 0.58 0.44
Table 4: Average precision, recall, AUC and F1 for each
subset of the feature groups.
due to the sparseness of rigid pattern instantiation
for verbs both in the ukWaC corpus and on the web.
Yet, VerbOcean positive and negative patterns do
add some discriminative power over only distribu-
tional similarity measures, as seen by the improve-
ment of TDS+VO over TDS in all criteria. But, it is
the combination of all types of information sources
that yields the best performance. Our complete
model, employing the full set of features, outper-
forms all other models in terms of both precision and
recall. Its improvement in terms of F1 over the sec-
ond best model (TDS+VO), which includes all distri-
butional similarity features as well as pattern-based
features, is by 24%. This result shows the benefits
of integrating linguistically motivated co-occurrence
features with traditional pattern-based and distribu-
tional similarity information.
To further investigate the contribution of fea-
tures at various co-occurrence levels, we trained
and tested our model with all possible combina-
tions of feature groups corresponding to a certain
co-occurrence scope (sentence, document and cor-
pus). Table 4 presents the results of these tests.
The most notable result of this analysis is that
sentence-level features play an important role within
our model. Indeed, removing either the document-
level features (Sent+Corpus-level) or the corpus-
level features (Sent+Doc-level) results in only a
slight decline in performance. Yet, removing the
sentence-level features (Doc+Corpus-level), ends in
a more substantial decline of 8.5% in F1. In addi-
tion, sentence-level features alone (Sent-level) pro-
vide the best discriminative power for verb entail-
ment, compared to document and corpus levels,
which include distributional similarity features. Yet,
we note that sentence-level features alone do not
capture all the information within our model, and
they should be combined with one of the other fea-
ture groups to reach performance close to the com-
plete model. This shows again the importance of
combining co-occurrence indicators at different lev-
els.
As an additional insight from Table 4, we point
out that document-level features are not good en-
tailment indicators by themselves (Doc-level in Ta-
ble 4), and they perform worse than the distribu-
tional similarity baseline (TDS at Table 3). Still, they
do complement each of the other feature groups. In
particular, since the Sent+Doc-level model performs
almost as good as the full model, this subset may
be a good substitute to the full model, since its fea-
tures are easier to extract from large corpora, as they
may be extracted in an on-line fashion, processing
one document at a time (contrary to corpus-level fea-
tures).
As a final analysis, we randomly sampled cor-
rect entailment rules learned by our model but
missed by the typed distributional similarity classi-
fier (TDS). Our overall impression is that employ-
ing co-occurrence information helps to better cap-
ture entailment relations other than synonymy and
troponymy. For example, our model learns that ac-
quire? own, corresponding to the cause-effect en-
tailment relation, and that patent ? invent, corre-
sponding to the presupposition entailment relation.
6 Conclusions and Future Work
We presented a supervised classification model for
detecting lexical entailment between verbs. At the
heart of our model stand novel linguistically moti-
vated indicators that capture positive and negative
entailment information. These indicators encom-
pass co-occurrence relationships between verbs at
the sentence, document and corpus level, as well
as more fine-grained typed distributional similarity
measures. Our model incorporates these novel indi-
cators together with useful features from prior work,
combining co-occurrence and distributional similar-
ity information about verb pairs.
Our experiment over a manually labeled dataset
showed that our model significantly outperforms
several state-of-the-art models both in terms of Pre-
202
cision and Recall. Further feature analysis indicated
that our novel indicators contribute greatly to the
performance of the model, and that co-occurrence
at multiple levels, combined with distributional sim-
ilarity features, is necessary to achieve the model?s
best performance.
In future work we?d like to investigate which in-
dicators may contribute to learning different fine-
grained types of entailment, such as presupposition
and cause-effect, and attempt to perform a more
fine-grained classification to subtypes of entailment.
Acknowledgments
This work was partially supported by the Israel
Science Foundation grant 1112/08, the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Community?s Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT).
References
Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008.
Acquiring event relation knowledge by learning cooc-
currence patterns and fertilizing cooccurrence samples
with verbal nouns. In Proceedings of IJCNLP.
Timothy Baldwin and Aline Villavicencio. 2002. Ex-
tracting the unextractable: a case study on verb-
particles. In proceedings of COLING.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2012. Learning entailment relations by global graph
structure optimization. Computational Linguistics,
38(1):73?111.
Nathanael Chambers and Dan Jurafsky. 2008. Unsuper-
vised learning of narrative event chains. In Proceed-
ings of ACL.
Timothy Chklovski and Patrick Pantel. 2004. Verb
ocean: Mining the web for fine-grained semantic verb
relations. In Proceedings of EMNLP.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment chal-
lenge. In Machine Learning Challenges, volume 3944
of Lecture Notes in Computer Science, pages 177?190.
Springer.
Lin Dekang and Patrick Pantel. 2001. Dirt - discovery
of inference rules from text. In In Proceedings of the
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining.
Maayan Geffet and Ido Dagan. 2005. The distributional
inclusion hypotheses and lexical entailment. In Pro-
ceedings of ACL.
Isabelle Guyon and Andre Elisseeff. 2003. An intro-
duction to variable and feature selection. Journal of
Machine Learning Research, 3:1157?1182.
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko
Toyama. 2009. Supervised synonym acquisition us-
ing distributional features and syntactic patterns. In
Journal of Natural Language Processing.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING.
Jerry Hobbs. 1979. Coherence and coreference. Cogni-
tive Science, 3:67?90.
Eduard Hovy and Elisabeth Maier. 1993. Organizing
Discourse Structure Relations using Metafunctions.
Pinter Publishing.
Ray Jackendoff. 1983. Semantics and Cognition. The
MIT Press.
T. Joachims. 2005. A support vector method for mul-
tivariate performance measures. In Proceedings of
ICML.
Alistair Knott and Ted Sanders. 1998. The classification
of coherence relations and their linguistic markers: An
exploration of two languages. In Journal of Pragmat-
ics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language En-
gineering, 16(4):359?389.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University Of
Chicago Press.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of ICML.
William Mann and Sandra Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse rela-
tions. In Proceedings of ACL.
Shachar Mirkin, Ido Dagan, and Maayan Geffet. 2006.
Integrating pattern-based and distributional similarity
methods for lexical entailment acquisition. In Pro-
ceedings of the COLING/ACL.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of LREC.
203
Viktor Pekar. 2008. Discovery of event entailment
knowledge from text corpora. Comput. Speech Lang.,
22(1):1?16.
Marco Pennacchiotti and Patrick Pantel. 2009. Entity
extraction via ensemble semantics. In Proceedings of
EMNLP.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Deborah Schiffrin. 1988. Discourse Markers. Cam-
bridge University Press.
Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and
Daniel S. Weld. 2010. Learning first-order horn
clauses from web text. In Proceedings of EMNLP.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In Proceedings of NAACL-HLT.
Idan Szpektor and Ido Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of COLING.
Idan Szpektor, Hristo Tanev, and Ido Dagan. 2004. Scal-
ing web-based acquisition of entailment relations. In
In Proceedings of EMNLP.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007. In-
stance based evaluation of entailment rule acquisition.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics.
Galina Tremper. 2010. Weakly supervised learning of
presupposition relations between verbs. In Proceed-
ings of ACL student workshop.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
EMNLP.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using se-
lectional preferences. In Proceedings of the COL-
ING/ACL.
204
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533?1544,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Semantic Parsing on Freebase from Question-Answer Pairs
Jonathan Berant Andrew Chou Roy Frostig Percy Liang
Computer Science Department, Stanford University
{joberant,akchou}@stanford.edu {rf,pliang}@cs.stanford.edu
Abstract
In this paper, we train a semantic parser that
scales up to Freebase. Instead of relying on
annotated logical forms, which is especially
expensive to obtain at large scale, we learn
from question-answer pairs. The main chal-
lenge in this setting is narrowing down the
huge number of possible logical predicates for
a given question. We tackle this problem in
two ways: First, we build a coarse mapping
from phrases to predicates using a knowledge
base and a large text corpus. Second, we
use a bridging operation to generate additional
predicates based on neighboring predicates.
On the dataset of Cai and Yates (2013), despite
not having annotated logical forms, our sys-
tem outperforms their state-of-the-art parser.
Additionally, we collected a more realistic and
challenging dataset of question-answer pairs
and improves over a natural baseline.
1 Introduction
We focus on the problem of semantic parsing nat-
ural language utterances into logical forms that can
be executed to produce denotations. Traditional se-
mantic parsers (Zelle and Mooney, 1996; Zettle-
moyer and Collins, 2005; Wong and Mooney, 2007;
Kwiatkowski et al, 2010) have two limitations: (i)
they require annotated logical forms as supervision,
and (ii) they operate in limited domains with a small
number of logical predicates. Recent developments
aim to lift these limitations, either by reducing the
amount of supervision (Clarke et al, 2010; Liang et
al., 2011; Goldwasser et al, 2011; Artzi and Zettle-
moyer, 2011) or by increasing the number of logical
Occidental College, Columbia University
Execute on Database
Type.University u Education.BarackObama
Type.University
Education
BarackObama
Which college did Obama go to ?
alignment
alignment
bridging
Figure 1: Our task is to map questions to answers via la-
tent logical forms. To narrow down the space of logical
predicates, we use a (i) coarse alignment based on Free-
base and a text corpus and (ii) a bridging operation that
generates predicates compatible with neighboring predi-
cates.
predicates (Cai and Yates, 2013). The goal of this
paper is to do both: learn a semantic parser with-
out annotated logical forms that scales to the large
number of predicates on Freebase.
At the lexical level, a major challenge in semantic
parsing is mapping natural language phrases (e.g.,
?attend?) to logical predicates (e.g., Education).
While limited-domain semantic parsers are able
to learn the lexicon from per-example supervision
(Kwiatkowski et al, 2011; Liang et al, 2011), at
large scale they have inadequate coverage (Cai and
Yates, 2013). Previous work on semantic parsing on
Freebase uses a combination of manual rules (Yahya
et al, 2012; Unger et al, 2012), distant supervision
(Krishnamurthy and Mitchell, 2012), and schema
1533
matching (Cai and Yates, 2013). We use a large
amount of web text and a knowledge base to build a
coarse alignment between phrases and predicates?
an approach similar in spirit to Cai and Yates (2013).
However, this alignment only allows us to gen-
erate a subset of the desired predicates. Aligning
light verbs (e.g., ?go?) and prepositions is not very
informative due to polysemy, and rare predicates
(e.g., ?cover price?) are difficult to cover even given
a large corpus. To improve coverage, we propose
a new bridging operation that generates predicates
based on adjacent predicates rather than on words.
At the compositional level, a semantic parser must
combine the predicates into a coherent logical form.
Previous work based on CCG requires manually
specifying combination rules (Krishnamurthy and
Mitchell, 2012) or inducing the rules from anno-
tated logical forms (Kwiatkowski et al, 2010; Cai
and Yates, 2013). We instead define a few simple
composition rules which over-generate and then use
model features to simulate soft rules and categories.
In particular, we use POS tag features and features
on the denotations of the predicted logical forms.
We experimented with two question answering
datasets on Freebase. First, on the dataset of Cai
and Yates (2013), we showed that our system out-
performs their state-of-the-art system 62% to 59%,
despite using no annotated logical forms. Second,
we collected a new realistic dataset of questions by
performing a breadth-first search using the Google
Suggest API; these questions are then answered by
Amazon Mechanical Turk workers. Although this
dataset is much more challenging and noisy, we are
still able to achieve 31.4% accuracy, a 4.5% ab-
solute improvement over a natural baseline. Both
datasets as well as the source code for SEMPRE, our
semantic parser, are publicly released and can be
downloaded from http://nlp.stanford.edu/
software/sempre/.
2 Setup
Problem Statement Our task is as follows: Given
(i) a knowledge base K, and (ii) a training set of
question-answer pairs {(xi, yi)}ni=1, output a se-
mantic parser that maps new questions x to answers
y via latent logical forms z and the knowledge base
K.
2.1 Knowledge base
Let E denote a set of entities (e.g., BarackObama),
and let P denote a set of properties (e.g.,
PlaceOfBirth). A knowledge base K is a set
of assertions (e1, p, e2) ? E ? P ? E (e.g.,
(BarackObama, PlaceOfBirth, Honolulu)).
We use the Freebase knowledge base (Google,
2013), which has 41M non-numeric entities, 19K
properties, and 596M assertions.1
2.2 Logical forms
To query the knowledge base, we use a logical lan-
guage called Lambda Dependency-Based Compo-
sitional Semantics (?-DCS)?see Liang (2013) for
details. For the purposes of this paper, we use a re-
stricted subset called simple ?-DCS, which we will
define below for the sake of completeness.
The chief motivation of ?-DCS is to produce
logical forms that are simpler than lambda cal-
culus forms. For example, ?x.?a.p1(x, a) ?
?b.p2(a, b) ? p3(b, e) is expressed compactly in
?-DCS as p1.p2.p3.e. Like DCS (Liang et al,
2011), ?-DCS makes existential quantification im-
plicit, thereby reducing the number of variables.
Variables are only used for anaphora and building
composite binary predicates; these do not appear in
simple ?-DCS.
Each logical form in simple ?-DCS is either a
unary (which denotes a subset of E) or a binary
(which denotes a subset of E ? E). The basic ?-
DCS logical forms z and their denotations JzKK are
defined recursively as follows:
? Unary base case: If e ? E is an entity (e.g.,
Seattle), then e is a unary logical form with
JzKK = {e}.
? Binary base case: If p ? P is a property (e.g.,
PlaceOfBirth), then p is a binary logical form
with JpKK = {(e1, e2) : (e1, p, e2) ? K}.
2
? Join: If b is a binary and u is a unary, then b.u
(e.g., PlaceOfBirth.Seattle) is a unary de-
noting a join and project: Jb.uKK = {e1 ? E :
?e2.(e1, e2) ? JbKK ? e2 ? JuKK}.
1In this paper, we condense Freebase names for readability
(/people/person becomes Person).
2Binaries can be also built out of lambda abstractions (e.g.,
?x.Performance.Actor.x), but as these constructions are
not central to this paper, we defer to (Liang, 2013).
1534
? Intersection: If u1 and u2 are both unaries,
then u1 u u2 (e.g., Profession.Scientist u
PlaceOfBirth.Seattle) denotes set intersec-
tion: Ju1 u u2KK = Ju1KK ? Ju2KK.
? Aggregation: If u is a unary, then count(u)
denotes the cardinality: Jcount(u)KK =
{|JuKK|}.
As a final example, ?number of dramas star-
ring Tom Cruise? in lambda calculus would
be represented as count(?x.Genre(x, Drama) ?
?y.Performance(x, y) ? Actor(y, TomCruise));
in ?-DCS, it is simply count(Genre.Drama u
Performance.Actor.TomCruise).
It is useful to think of the knowledge base K as
a directed graph in which entities are nodes and
properties are labels on the edges. Then simple ?-
DCS unary logical forms are tree-like graph patterns
which pick out a subset of the nodes.
2.3 Framework
Given an utterance x, our semantic parser constructs
a distribution over possible derivations D(x). Each
derivation d ? D(x) is a tree specifying the appli-
cation of a set of combination rules that culminates
in the logical form d.z at the root of the tree?see
Figure 2 for an example.
Composition Derivations are constructed recur-
sively based on (i) a lexicon mapping natural lan-
guage phrases to knowledge base predicates, and (ii)
a small set of composition rules.
More specifically, we build a set of derivations for
each span of the utterance. We first use the lexicon to
generate single-predicate derivations for any match-
ing span (e.g., ?born? maps to PeopleBornHere).
Then, given any logical form z1 that has been con-
structed over the span [i1 : j1] and z2 over a non-
overlapping span [i2 : j2], we generate the following
logical forms over the enclosing span [min(i1, i2) :
max(j1, j2)]: intersection z1 u z2, join z1.z2, ag-
gregation z1(z2) (e.g., if z1 = count), or bridging
z1 u p.z2 for any property p ? P (explained more in
Section 3.2).3
Note that the construction of derivations D(x)
allows us to skip any words, and in general heav-
3We also discard logical forms are incompatible according
to the Freebase types (e.g., Profession.Politician u
Type.City would be rejected).
Type.Locationu PeopleBornHere.BarackObama
Type.Location
where
was PeopleBornHere.BarackObama
BarackObama
Obama
PeopleBornHere
born
?join
intersection
lexicon
lexicon lexicon
Figure 2: An example of a derivation d of the utterance
?Where was Obama born?? and its sub-derivations, each
labeled with composition rule (in blue) and logical form
(in red). The derivation d skips the words ?was? and ???.
ily over-generates. We instead rely on features and
learning to guide us away from the bad derivations.
Modeling Following Zettlemoyer and Collins
(2005) and Liang et al (2011), we define a
discriminative log-linear model over derivations
d ? D(x) given utterances x: p?(d | x) =
exp{?(x,d)>?}
?
d??D(x) exp{?(x,d
?)>?} , where ?(x, d) is a feature
vector extracted from the utterance and the deriva-
tion, and ? ? Rb is the vector of parameters to
be learned. As our training data consists only of
question-answer pairs (xi, yi), we maximize the log-
likelihood of the correct answer (Jd.zKK = yi), sum-
ming over the latent derivation d. Formally, our
training objective is
O(?) =
n?
i=1
log
?
d?D(x):Jd.zKK=yi
p?(d | xi). (1)
Section 4 describes an approximation of this ob-
jective that we maximize to choose parameters ?.
3 Approach
Our knowledge base has more than 19,000 proper-
ties, so a major challenge is generating a manage-
able set of predicates for an utterance. We propose
two strategies for doing this. First (Section 3.1),
we construct a lexicon that maps natural language
phrases to logical predicates by aligning a large text
corpus to Freebase, reminiscent of Cai and Yates
(2013). Second, we generate logical predicates com-
patible with neighboring predicates using the bridg-
ing operation (Section 3.2). Bridging is crucial when
aligning phrases is difficult or even impossible. The
derivations produced by combining these predicates
1535
grew up in[Person,Location]born in[Person,Date]married in[Person,Date]born in[Person,Location]
DateOfBirth
PlaceOfBirth
Marriage.StartDate
PlacesLived.Location
(BarackObama,Honolulu)
(MichelleObama,Chicago)
(BarackObama,Chicago)(RandomPerson,Seattle)
F(r1) F(r2)
C(r1, r2)
Alignment featureslog-phrase-count:log(15765)log-predicate-count: log(9182)log-intersection-count: log(6048)KB-best-match: 0
Figure 3: We construct a bipartite graph over phrasesR1
and predicates R2. Each edge (r1, r2) is associated with
alignment features.
are scored using features that capture lexical, syn-
tactic and semantic regularities (Section 3.3).
3.1 Alignment
We now discuss the construction of a lexicon L,
which is a mapping from natural language phrases
to logical predicates accompanied by a set of fea-
tures. Specifically, for a phrase w (e.g., ?born in?),
L(w) is a set of entries (z, s), where z is a predicate
and s is the set of features. A lexicon is constructed
by alignment of a large text corpus to the knowledge
base (KB). Intuitively, a phrase and a predicate align
if they co-occur with many of the same entities.
Here is a summary of our alignment proce-
dure: We construct a set of typed4 phrases
R1 (e.g., ?born in?[Person,Location]) and pred-
icates R2 (e.g., PlaceOfBirth). For each
r ? R1 ? R2, we create its extension
F(r), which is a set of co-occurring entity-
pairs (e.g., F(?born in?[Person,Location]) =
{(BarackObama, Honolulu), . . . }. The lexicon is
generated based on the overlap F(r1) ? F(r2), for
r1 ? R1 and r2 ? R2.
Typed phrases 15 million triples (e1, r, e2) (e.g.,
(?Obama?, ?was also born in?, ?August 1961?))
4Freebase associates each entity with a set of types using the
Type property.
were extracted from ClueWeb09 using the ReVerb
open IE system (Fader et al, 2011). Lin et al (2012)
released a subset of these triples5 where they were
able to substitute the subject arguments with KB en-
tities. We downloaded their dataset and heuristically
replaced object arguments with KB entities by walk-
ing on the Freebase graph from subject KB entities
and performing simple string matching. In addition,
we normalized dates with SUTime (Chang and Man-
ning, 2012).
We lemmatize and normalize each text phrase
r ? R1 and augment it with a type signature
[t1, t2] to deal with polysemy (?born in? could ei-
ther map to PlaceOfBirth or DateOfBirth). We
add an entity pair (e1, e2) to the extension of
F(r[t1, t2]) if the (Freebase) type of e1 (e2) is t1
(t2). For example, (BarackObama, 1961) is added
to F(?born in?[Person, Date]). We perform a simi-
lar procedure that uses a Hearst-like pattern (Hearst,
1992) to map phrases to unary predicates. If a
text phrase r ? R1 matches the pattern ?(is|was
a|the) x IN?, where IN is a preposition, then we
add e1 to F(x). For (Honolulu, ?is a city in?,
Hawaii), we extract x = ?city ?? and add Honolulu
to F(?city?). From the initial 15M triples, we ex-
tracted 55,081 typed binary phrases (9,456 untyped)
and 6,299 unary phrases.
Logical predicates Binary logical predicates con-
tain (i) all KB properties6 and (ii) concatenations of
two properties p1.p2 if the intermediate type repre-
sents an event (e.g., the married to relation is rep-
resented by Marriage.Spouse). For unary pred-
icates, we consider all logical forms Type.t and
Profession.t for all (abstract) entities t ? E (e.g.
Type.Book and Profession.Author). The types
of logical predicates considered during alignment is
restricted in this paper, but automatic induction of
more compositional logical predicates is an interest-
ing direction. Finally, we define the extension of a
logical predicate r ? R2 to be its denotation, that is,
the corresponding set of entities or entity pairs.
Lexicon construction Given typed phrases R1,
logical predicates R2, and their extensions F , we
now generate the lexicon. It is useful to think of a
5http://knowitall.cs.washington.edu/
linked_extractions/
6We filter properties from the domains user and base.
1536
Category Description
Alignment Log of # entity pairs that occur with the
phrase r1 (|F(r1)|)
Log of # entity pairs that occur with the
logical predicate r2 (|F(r2)|)
Log of # entity pairs that occur with both
r1 and r2 (|F(r1) ? F(r2)|)
Whether r2 is the best match for r1 (r2 =
argmaxr |F(r1) ? F(r)|)
Lexicalized Conjunction of phrase w and predicate z
Text similarity Phrase r1 is equal/prefix/suffix of s2
Phrase overlap of r1 and s2
Bridging Log of # entity pairs that occur with bridg-
ing predicate b (|F(b)|)
Kind of bridging (# unaries involved)
The binary b injected
Composition # of intersect/join/bridging operations
POS tags in join/bridging and skipped
words
Size of denotation of logical form
Table 1: Full set of features. For the alignment and text sim-
ilarity, r1 is a phrase, r2 is a predicate with Freebase name s2,
and b is a binary predicate with type signature (t1, t2).
bipartite graph with left nodes R1 and right nodes
R2 (Figure 3). We add an edge (r1, r2) if (i) the
type signatures of r1 and r2 match7 and (ii) their ex-
tensions have non-empty overlap (F(r1)?F(r2) 6=
?). Our final graph contains 109K edges for binary
predicates and 294K edges for unary predicates.
Naturally, non-zero overlap by no means guaran-
tees that r1 should map to r2. In our noisy data,
even ?born in? and Marriage.EndDate co-occur 4
times. Rather than thresholding based on some cri-
terion, we compute a set of features, which are used
by the model downstream in conjunction with other
sources of information.
We compute three types of features (Table 1).
Alignment features are unlexicalized and measure
association based on argument overlap. Lexicalized
features are standard conjunctions of the phrase w
and the logical form z. Text similarity features com-
pare the (untyped) phrase (e.g., ?born?) to the Free-
base name of the logical predicate (e.g., ?People
born here?): Given the phrase r1 and the Freebase
name s2 of the predicate r2, we compute string sim-
ilarity features such as whether r1 and s2 are equal,
7Each Freebase property has a designated type signa-
ture, which can be extended to composite predicates, e.g.,
sig(Marriage.StartDate) = (Person,Date).
as well as some other measures of token overlap.
3.2 Bridging
While alignment can cover many predicates, it is un-
reliable for cases where the predicates are expressed
weakly or implicitly. For example, in ?What govern-
ment does Chile have??, the predicate is expressed
by the light verb have, in ?What actors are in Top
Gun??, it is expressed by a highly ambiguous prepo-
sition, and in ?What is Italy money?? [sic], it is
omitted altogether. Since natural language doesn?t
offer much help here, let us turn elsewhere for guid-
ance. Recall that at this point our main goal is to
generate a manageable set of candidate logical forms
to be scored by the log-linear model.
In the first example, suppose the phrases ?Chile?
and ?government? are parsed as Chile and
Type.FormOfGovernment, respectively, and we hy-
pothesize a connecting binary. The two predicates
impose strong type constraints on that binary, so we
can afford to generate all the binary predicates that
type check (see Table 2). More formally, given two
unaries z1 and z2 with types t1 and t2, we generate a
logical form z1 u b.z2 for each binary b whose type
signature is (t1, t2). Figure 1 visualizes bridging of
the unaries Type.University and Obama.
Now consider the example ?What is the
cover price of X-men?? Here, the binary
ComicBookCoverPrice is expressed explicitly, but
is not in our lexicon since the language use is rare.
To handle this, we allow bridging to generate a bi-
nary based on a single unary; in this case, based on
the unary X-Men (Table 2), we generate several bina-
ries including ComicBookCoverPrice. Generically,
given a unary z with type t, we construct a logical
form b.z for any predicate b with type (?, t).
Finally, consider the question ?Who did
Tom Cruise marry in 2006??. Suppose we
parse the phrase ?Tom Cruise marry? into
Marriage.Spouse.TomCruise, or more explicitly,
?x.?e.Marriage(x, e) ? Spouse(e, TomCruise).
Here, the neo-Davidsonian event variable e is an
intermediate quantity, but needs to be further mod-
ified (in this case, by the temporal modifier 2006).
To handle this, we apply bridging to a unary and the
intermediate event (see Table 2). Generically, given
a logical form p1.p2.z? where p2 has type (t1, ?),
and a unary z with type t, bridging injects z and
1537
# Form 1 Form 2 Bridging
1 Type.FormOfGovernment Chile Type.FormOfGovernmentu GovernmentTypeOf.Chile
2 X-Men ComicBookCoverPriceOf.X-Men
3 Marriage.Spouse.TomCruise 2006 Marriage.(Spouse.TomCruise u StartDate.2006)
Table 2: Three examples of the bridging operation. The bridging binary predicate b is in boldface.
constructs a logical form p1.(p2.z? u b.z) for each
logical predicate b with type (t1, t).
In each of the three examples, bridging gener-
ates a binary predicate based on neighboring logi-
cal predicates rather than on explicit lexical material.
In a way, our bridging operation shares with bridg-
ing anaphora (Clark, 1975) the idea of establishing
a novel relation between distinct parts of a sentence.
Naturally, we need features to distinguish between
the generated predicates, or decide whether bridging
is even appropriate at all. Given a binary b, features
include the log of the predicate count log |F(b)|, in-
dicators for the kind of bridging, an indicator on the
binary b for injections (Table 1). In addition, we add
all text similarity features by comparing the Free-
base name of b with content words in the question.
3.3 Composition
So far, we have mainly focused on the generation of
predicates. We now discuss three classes of features
pertaining to their composition.
Rule features Each derivation d is the result of ap-
plying some number of intersection, join, and bridg-
ing operations. To control this number, we define
indicator features on each of these counts. This is in
contrast to the norm of having a single feature whose
value is equal to the count, which can only repre-
sent one-sided preferences for having more or fewer
of a given operation. Indicator features stabilize the
model, preferring derivations with a well-balanced
inventory of operations.
Part-of-speech tag features To guide the compo-
sition of predicates, we use POS tags in two ways.
First, we introduce features indicating when a word
of a given POS tag is skipped, which could capture
the fact that skipping auxiliaries is generally accept-
able, while skipping proper nouns is not. Second,
we introduce features on the POS tags involved in a
composition, inspired by dependency parsing (Mc-
Donald et al, 2005). Specifically, when we combine
logical forms z1 and z2 via a join or bridging, we
include a feature on the POS tag of (the first word
spanned by) z1 conjoined with the POS tag corre-
sponding to z2. Rather than using head-modifier in-
formation from dependency trees (Branavan et al,
2012; Krishnamurthy and Mitchell, 2012; Cai and
Yates, 2013; Poon, 2013), we can learn the appro-
priate relationships tailored for downstream accu-
racy. For example, the phrase ?located? is aligned
to the predicate ContainedBy. POS features can de-
tect that if ?located? precedes a noun phrase (?What
is located in Beijing??), then the noun phrase is the
object of the predicate, and if it follows the noun
phrase (?Where is Beijing located??), then it is in
subject position.
Note that our three operations (intersection, join,
and bridging) are quite permissive, and we rely on
features, which encode soft, overlapping rules. In
contrast, CCG-based methods (Kwiatkowski et al,
2010; Kwiatkowski et al, 2011) encode the com-
bination preferences structurally in non-overlapping
rules; these could be emulated with features with
weights clamped to ??.
Denotation features While it is clear that learning
from denotations rather than logical forms is a draw-
back since it provides less information, it is less ob-
vious that working with denotations actually gives
us additional information. Specifically, we include
four features indicating whether the denotation of
the predicted logical form has size 0, 1, 2, or at least
3. This feature encodes presupposition constraints
in a soft way: when people ask a question, usually
there is an answer and it is often unique. This allows
us to favor logical forms with this property.
4 Experiments
We now evaluate our semantic parser empirically.
In Section 4.1, we compare our approach to Cai
and Yates (2013) on their recently released dataset
(henceforth, FREE917) and present results on a new
1538
dataset that we collected (henceforth, WEBQUES-
TIONS). In Section 4.2, we provide detailed experi-
ments to provide additional insight on our system.
Setup We implemented a standard beam-based
bottom-up parser which stores the k-best derivations
for each span. We use k = 500 for all our experi-
ments on FREE917 and k = 200 on WEBQUES-
TIONS. The root beam yields the candidate set D?(x)
and is used to approximate the sum in the objective
functionO(?) in (1). In experiments on WEBQUES-
TIONS, D?(x) contained 197 derivations on average.
We write the approximate objective as O(?; ??) =
?
i log
?
d?D?(xi;??):Jd.zKK=yi
p(d | xi; ?) to explic-
itly show dependence on the parameters ?? used for
beam search. We optimize the objective by initial-
izing ?0 to 0 and applying AdaGrad (stochastic gra-
dient ascent with per-feature adaptive step size con-
trol) (Duchi et al, 2010), so that ?t+1 is set based on
taking a stochastic approximation of ?O(?;?t)??
?
?
?=?t
.
We make six passes over the training examples.
We used POS tagging and named-entity recogni-
tion to restrict what phrases in the utterance could
be mapped by the lexicon. Entities must be named
entities, proper nouns or a sequence of at least two
tokens. Unaries must be a sequence of nouns, and
binaries must be either a content word, or a verb fol-
lowed by either a noun phrase or a particle. In addi-
tion, we used 17 hand-written rules to map question
words such as ?where? and ?how many? to logical
forms such as Type.Location and Count.
To compute denotations, we convert a logical
form z into a SPARQL query and execute it on our
copy of Freebase using the Virtuoso engine. On
WEBQUESTIONS, a full run over the training exam-
ples involves approximately 600,000 queries. For
evaluation, we predict the answer from the deriva-
tion with highest probability.
4.1 Main results
4.1.1 FREE917
Cai and Yates (2013) created a dataset consist-
ing of 917 questions involving 635 Freebase rela-
tions, annotated with lambda calculus forms. We
converted all 917 questions into simple ?-DCS, ex-
ecuted them on Freebase and used the resulting an-
swers to train and evaluate. To map phrases to Free-
base entities we used the manually-created entity
lexicon used by Cai and Yates (2013), which con-
tains 1,100 entries. Because entity disambiguation
is a challenging problem in semantic parsing, the en-
tity lexicon simplifies the problem.
Following Cai and Yates (2013), we held out 30%
of the examples for the final test, and performed all
development on the remaining 70%. During devel-
opment, we split the data and used 512 examples
(80%) for training and the remaining 129 (20%) for
validation. All reported development numbers are
averaged across 3 random splits. We evaluated us-
ing accuracy, the fraction of examples where the pre-
dicted answer exactly matched the correct answer.
Our main empirical result is that our system,
which was trained only on question-answer pairs,
obtained 62% accuracy on the test set, outperform-
ing the 59% accuracy reported by Cai and Yates
(2013), who trained on full logical forms.
4.1.2 WEBQUESTIONS
Dataset collection Because FREE917 requires
logical forms, it is difficult to scale up due to the
required expertise of annotating logical forms. We
therefore created a new dataset, WEBQUESTIONS,
of question-answer pairs obtained from non-experts.
To collect this dataset, we used the Google Sug-
gest API to obtain questions that begin with a wh-
word and contain exactly one entity. We started with
the question ?Where was Barack Obama born??
and performed a breadth-first search over questions
(nodes), using the Google Suggest API supplying
the edges of the graph. Specifically, we queried the
question excluding the entity, the phrase before the
entity, or the phrase after it; each query generates 5
candidate questions, which are added to the queue.
We iterated until 1M questions were visited; a ran-
dom 100K were submitted to Amazon Mechanical
Turk (AMT).
The AMT task requested that workers answer the
question using only the Freebase page of the ques-
tions? entity, or otherwise mark it as unanswerable
by Freebase. The answer was restricted to be one of
the possible entities, values, or list of entities on the
page. As this list was long, we allowed the user to
filter the list by typing. We paid the workers $0.03
per question. Out of 100K questions, 6,642 were
annotated identically by at least two AMT workers.
We again held out a 35% random subset of the
1539
Dataset # examples # word types
GeoQuery 880 279
ATIS 5,418 936
FREE917 917 2,036
WEBQUESTIONS 5,810 4,525
Table 3: Statistics on various semantic parsing datasets. Our
new dataset, WEBQUESTIONS, is much larger than FREE917
and much more lexically diverse than ATIS.
questions for the final test, and performed all devel-
opment on the remaining 65%, which was further
divided into an 80%?20% split for training and val-
idation. To map entities, we built a Lucene index
over the 41M Freebase entities.
Table 3 provides some statistics about the new
questions. One major difference in the datasets is
the distribution of questions: FREE917 starts from
Freebase properties and solicits questions about
these properties; these questions tend to be tai-
lored to the properties. WEBQUESTIONS starts from
questions completely independent of Freebase, and
therefore the questions tend to be more natural and
varied. For example, for the Freebase property
ComicGenre, FREE917 contains the question ?What
genre is Doonesbury??, while WEBQUESTIONS for
the property MusicGenre contains ?What music did
Beethoven compose??.
The number of word types in WEBQUESTIONS is
larger than in datasets such as ATIS and GeoQuery
(Table 3), making lexical mapping much more chal-
lenging. On the other hand, in terms of structural
complexity WEBQUESTIONS is simpler and many
questions contain a unary, a binary and an entity.
In some questions, the answer provided by AMT
workers is only roughly accurate, because workers
are restricted to selecting answers from the Freebase
page. For example, the answer given by workers to
the question ?What is James Madison most famous
for?? is ?President of the United States? rather than
?Authoring the Bill of Rights?.
Results AMT workers sometimes provide partial
answers, e.g., the answer to ?What movies does Tay-
lor Lautner play in?? is a set of 17 entities, out
of which only 10 appear on the Freebase page. We
therefore allow partial credit and score an answer us-
ing the F1 measure, comparing the predicted set of
entities to the annotated set of entities.
System FREE917 WebQ.
ALIGNMENT 38.0 30.6
BRIDGING 66.9 21.2
ALIGNMENT+BRIDGING 71.3 32.9
Table 4: Accuracies on the development set under different
schemes of binary predicate generation. In ALIGNMENT, bi-
naries are generated only via the alignment lexicon. In BRIDG-
ING, binaries are generated through the bridging operation only.
ALIGNMENT+BRIDGING corresponds to the full system.
As a baseline, we omit from our system the main
contributions presented in this paper?that is, we
disallow bridging, and remove denotation and align-
ment features. The accuracy on the test set of this
system is 26.9%, whereas our full system obtains
31.4%, a significant improvement.
Note that the number of possible derivations for
questions in WEBQUESTIONS is quite large. In the
question ?What kind of system of government does
the United States have?? the phrase ?United States?
maps to 231 entities in our lexicon, the verb ?have?
maps to 203 binaries, and the phrases ?kind?, ?sys-
tem?, and ?government? all map to many different
unary and binary predicates. Parsing correctly in-
volves skipping some words, mapping other words
to predicates, while resolving many ambiguities in
the way that the various predicates can combine.
4.2 Detailed analysis
We now delve deeper to explore the contributions of
the various components of our system. All ablation
results reported next were run on the development
set (over 3 random splits).
Generation of binary predicates Recall that our
system has two mechanisms for suggesting binaries:
from the alignment lexicon or via the bridging op-
eration. Table 4 shows accuracies when only one or
both is used. Interestingly, alignment alone is better
than bridging alone on WEBQUESTIONS, whereas
for FREE917, it is the opposite. The reason for this
is that FREE917 contains questions on rare pred-
icates. These are often missing from the lexicon,
but tend to have distinctive types and hence can be
predicted from neighboring predicates. In contrast,
WEBQUESTIONS contains questions that are com-
monly searched for and focuses on popular predi-
cates, therefore exhibiting larger lexical variation.
1540
System FREE917 WebQ.
FULL 71.3 32.9
-POS 70.5 28.9
-DENOTATION 58.6 28.0
Table 5: Accuracies on the development set with features re-
moved. POS and DENOTATION refer to the POS tag and deno-
tation features from Section 3.3.
System FREE917 WebQ.
ALIGNMENT 71.3 32.9
LEXICALIZED 68.5 34.2
LEXICALIZED+ALIGNMENT 69.0 36.4
Table 6: Accuracies on the development set using either
unlexicalized alignment features (ALIGNMENT) or lexicalized
features (LEXICALIZED).
For instance, when training without an align-
ment lexicon, the system errs on ?When did Nathan
Smith die??. Bridging suggests binaries that are
compatible with the common types Person and
Datetime, and the binary PlaceOfBirth is cho-
sen. On the other hand, without bridging, the sys-
tem errs on ?In which comic book issue did Kitty
Pryde first appear??, which refers to the rare pred-
icate ComicBookFirstAppearance. With bridging,
the parser can identify the correct binary, by linking
the types ComicBook and ComicBookCharacter. On
both datasets, best performance is achieved by com-
bining the two sources of information.
Overall, running on WEBQUESTIONS, the parser
constructs derivations that contain about 12,000 dis-
tinct binary predicates.
Feature variations Table 5 shows the results of
feature ablation studies. Accuracy drops when POS
tag features are omitted, e.g., in the question ?What
number is Kevin Youkilis on the Boston Red Sox? the
parser happily skips the NNPs ?Kevin Youkilis? and
returns the numbers of all players on the Boston Red
Sox. A significant loss is incurred without denota-
tion features, largely due to the parser returning log-
ical forms with empty denotations. For instance, the
question ?How many people were at the 2006 FIFA
world cup final?? is answered with a logical form
containing the property PeopleInvolved rather than
SoccerMatchAttendance, resulting in an empty de-
notation.
Next we study the impact of lexicalized versus
0 iterations 1 iterations 2 iterations
Figure 4: Beam of candidate derivations D?(x) for 50
WEBQUESTIONS examples. In each matrix, columns
correspond to examples and rows correspond to beam po-
sition (ranked by decreasing model score). Green cells
mark the positions of derivations with correct denota-
tions. Note that both the number of good derivations and
their positions improve as ? is optimized.
 0 0.2 0.4 0.6 0.8 1
 0  100  200  300  400  500oracleaccuracy
(a) FREE917
 0 0.2 0.4 0.6 0.8 1
 0  100  200oracleaccuracy
(b) WEBQUESTIONS
Figure 5: Accuracy and oracle as beam size k increases.
unlexicalized features (Table 6). In the large WE-
BQUESTIONS dataset, lexicalized features helped,
and so we added those features to our model when
running on the test set. In FREE917 lexicalized fea-
tures result in overfitting due to the small number of
training examples. Thus, we ran our final parser on
the test set without lexicalized features.
Effect of beam size An intrinsic challenge in se-
mantic parsing is to handle the exponentially large
set of possible derivations. We rely heavily on the
k-best beam approximation in the parser keeping
good derivations that lead to the correct answer. Re-
call that the set of candidate derivations D?(x) de-
pends on the parameters ?. In the initial stages of
learning, ? is far from optimal, so good derivations
are likely to fall below the k-best cutoff of inter-
nal parser beams. As a result, D?(x) contains few
derivations with the correct answer. Still, placing
these few derivations on the beam allows the train-
ing procedure to bootstrap ? into a good solution.
Figure 4 illustrates this improvement in D?(x) across
early training iterations.
Smaller choices of k yield a coarser approxima-
1541
tion in beam search. As we increase k (Figure 5), we
see a tapering improvement in accuracy. We also see
a widening gap between accuracy and oracle score,8
as including a good derivation in D?(x) is made eas-
ier but the learning problem is made more difficult.
Error analysis The accuracy on WEBQUES-
TIONS is much lower than on FREE917. We an-
alyzed WEBQUESTIONS examples and found sev-
eral main causes of error: (i) Disambiguating en-
tities in WEBQUESTIONS is much harder because
the entity lexicon has 41M entities. For example,
given ?Where did the battle of New Orleans start??
the system identifies ?New Orleans? as the target
entity rather than its surrounding noun phrase. Re-
call that all FREE917 experiments used a carefully
chosen entity lexicon. (ii) Bridging can often fail
when the question?s entity is compatible with many
binaries. For example, in ?What did Charles Bab-
bage make??, the system chooses a wrong binary
compatible with the type Person. (iii) The system
sometimes incorrectly draws verbs from subordinate
clauses. For example, in ?Where did Walt Disney
live before he died?? it returns the place of death of
Walt Disney, ignoring the matrix verb live.
5 Discussion
Our work intersects with two strands of work.
The first involves learning models of semantics
guided by denotations or interactions with the world.
Besides semantic parsing for querying databases
(Popescu et al, 2003; Clarke et al, 2010; Liang
et al, 2011), previous work has looked at inter-
preting natural language for performing program-
ming tasks (Kushman and Barzilay, 2013; Lei et
al., 2013), playing computer games (Branavan et al,
2010; Branavan et al, 2011), following navigational
instructions (Chen, 2012; Artzi and Zettlemoyer,
2013), and interacting in the real world via percep-
tion (Matuszek et al, 2012; Tellex et al, 2011; Kr-
ishnamurthy and Kollar, 2013). Our system uses
denotations rather than logical forms as a training
signal, but also benefits from denotation features,
which becomes possible in the grounded setting.
The second body of work involves connecting
natural language and open-domain databases. Sev-
8Oracle score is the fraction of examples for which D?(x)
contains any derivation with the correct denotation.
eral works perform relation extraction using dis-
tant supervision from a knowledge base (Riedel et
al., 2010; Carlson et al, 2010; Hoffmann et al,
2011; Surdeanu et al, 2012). While similar in spirit
to our alignment procedure for building the lexi-
con, one difference is that relation extraction cares
about facts, aggregating over phrases, whereas a
lexicon concerns specific phrases, thus aggregating
over facts. On the question answering side, recent
methods have made progress in building semantic
parsers for the open domain, but still require a fair
amount of manual effort (Yahya et al, 2012; Unger
et al, 2012; Cai and Yates, 2013). Our system re-
duces the amount of supervision and has a more ex-
tensive evaluation on a new dataset.
Finally, although Freebase has thousands of prop-
erties, open information extraction (Banko et al,
2007; Fader et al, 2011; Masaum et al, 2012)
and associated question answering systems (Fader
et al, 2013) work over an even larger open-ended
set of properties. The drawback of this regime is
that the noise and the difficulty in canonicaliza-
tion make it hard to perform reliable composition,
thereby nullifying one of the key benefits of se-
mantic parsing. An interesting midpoint involves
keeping the structured knowledge base but aug-
menting the predicates, for example using random
walks (Lao et al, 2011) or Markov logic (Zhang
et al, 2012). This would allow us to map atomic
words (e.g., ?wife?) to composite predicates (e.g.,
?x.Marriage.Spouse.(Gender.Femaleux)). Learn-
ing these composite predicates would drastically in-
crease the possible space of logical forms, but we
believe that the methods proposed in this paper?
alignment via distant supervision and bridging?can
provide some traction on this problem.
Acknowledgments
We would like to thank Thomas Lin, Mausam and
Oren Etzioni for providing us with open IE triples
that are partially-linked to Freebase, and also Arun
Chaganty for helpful comments. The authors grate-
fully acknowledge the support of the Defense Ad-
vanced Research Projects Agency (DARPA) Deep
Exploration and Filtering of Text (DEFT) Program
under Air Force Research Laboratory (AFRL) prime
contract no. FA8750-13-2-0040.
1542
References
Y. Artzi and L. Zettlemoyer. 2011. Bootstrapping
semantic parsers from conversations. In Empirical
Methods in Natural Language Processing (EMNLP),
pages 421?432.
Y. Artzi and L. Zettlemoyer. 2013. Weakly supervised
learning of semantic parsers for mapping instructions
to actions. Transactions of the Association for Com-
putational Linguistics (TACL), 1:49?62.
M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. In International Joint Conference on
Artificial Intelligence (IJCAI), pages 2670?2676.
S. Branavan, L. Zettlemoyer, and R. Barzilay. 2010.
Reading between the lines: Learning to map high-level
instructions to commands. In Association for Compu-
tational Linguistics (ACL), pages 1268?1277.
S. Branavan, D. Silver, and R. Barzilay. 2011. Learning
to win by reading manuals in a Monte-Carlo frame-
work. In Association for Computational Linguistics
(ACL), pages 268?277.
S. Branavan, N. Kushman, T. Lei, and R. Barzilay. 2012.
Learning high-level planning from text. In Association
for Computational Linguistics (ACL), pages 126?135.
Q. Cai and A. Yates. 2013. Large-scale semantic parsing
via schema matching and lexicon extension. In Asso-
ciation for Computational Linguistics (ACL).
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. H.
Jr, and T. M. Mitchell. 2010. Toward an architecture
for never-ending language learning. In Association for
the Advancement of Artificial Intelligence (AAAI).
A. X. Chang and C. Manning. 2012. SUTime: A library
for recognizing and normalizing time expressions. In
Language Resources and Evaluation (LREC), pages
3735?3740.
D. Chen. 2012. Fast online lexicon learning for grounded
language acquisition. In Association for Computa-
tional Linguistics (ACL).
H. H. Clark. 1975. Bridging. In Workshop on theoretical
issues in natural language processing, pages 169?174.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world?s re-
sponse. In Computational Natural Language Learn-
ing (CoNLL), pages 18?27.
J. Duchi, E. Hazan, and Y. Singer. 2010. Adaptive
subgradient methods for online learning and stochas-
tic optimization. In Conference on Learning Theory
(COLT).
A. Fader, S. Soderland, and O. Etzioni. 2011. Identifying
relations for open information extraction. In Empirical
Methods in Natural Language Processing (EMNLP).
A. Fader, L. Zettlemoyer, and O. Etzioni. 2013.
Paraphrase-driven learning for open question answer-
ing. In Association for Computational Linguistics
(ACL).
D. Goldwasser, R. Reichart, J. Clarke, and D. Roth.
2011. Confidence driven unsupervised semantic pars-
ing. In Association for Computational Linguistics
(ACL), pages 1486?1495.
Google. 2013. Freebase data dumps (2013-06-
09). https://developers.google.com/
freebase/data.
M. A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Interational Conference on
Computational linguistics, pages 539?545.
R. Hoffmann, C. Zhang, X. Ling, L. S. Zettlemoyer, and
D. S. Weld. 2011. Knowledge-based weak super-
vision for information extraction of overlapping rela-
tions. In Association for Computational Linguistics
(ACL), pages 541?550.
J. Krishnamurthy and T. Kollar. 2013. Jointly learning
to parse and perceive: Connecting natural language to
the physical world. Transactions of the Association for
Computational Linguistics (TACL), 1:193?206.
J. Krishnamurthy and T. Mitchell. 2012. Weakly super-
vised training of semantic parsers. In Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP/CoNLL),
pages 754?765.
N. Kushman and R. Barzilay. 2013. Using semantic uni-
fication to generate regular expressions from natural
language. In Human Language Technology and North
American Association for Computational Linguistics
(HLT/NAACL), pages 826?836.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2010. Inducing probabilistic CCG
grammars from logical form with higher-order unifi-
cation. In Empirical Methods in Natural Language
Processing (EMNLP), pages 1223?1233.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2011. Lexical generalization in CCG
grammar induction for semantic parsing. In Empirical
Methods in Natural Language Processing (EMNLP),
pages 1512?1523.
N. Lao, T. Mitchell, and W. W. Cohen. 2011. Random
walk inference and learning in a large scale knowledge
base. In Empirical Methods in Natural Language Pro-
cessing (EMNLP).
T. Lei, F. Long, R. Barzilay, and M. Rinard. 2013.
From natural language specifications to program input
parsers. In Association for Computational Linguistics
(ACL).
P. Liang, M. I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. In As-
1543
sociation for Computational Linguistics (ACL), pages
590?599.
P. Liang. 2013. Lambda dependency-based composi-
tional semantics. Technical report, ArXiv.
T. Lin, Mausam, and O. Etzioni. 2012. Entity link-
ing at web scale. In Knowledge Extraction Workshop
(AKBC-WEKEX).
Masaum, M. Schmitz, R. Bart, S. Soderland, and O. Et-
zioni. 2012. Open language learning for informa-
tion extraction. In Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP/CoNLL), pages 523?534.
C. Matuszek, N. FitzGerald, L. Zettlemoyer, L. Bo, and
D. Fox. 2012. A joint model of language and percep-
tion for grounded attribute learning. In International
Conference on Machine Learning (ICML).
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In As-
sociation for Computational Linguistics (ACL), pages
91?98.
H. Poon. 2013. Grounded unsupervised semantic pars-
ing. In Association for Computational Linguistics
(ACL).
A. Popescu, O. Etzioni, and H. Kautz. 2003. Towards
a theory of natural language interfaces to databases.
In International Conference on Intelligent User Inter-
faces (IUI), pages 149?157.
S. Riedel, L. Yao, and A. McCallum. 2010. Model-
ing relations and their mentions without labeled text.
In Machine Learning and Knowledge Discovery in
Databases (ECML PKDD), pages 148?163.
M. Surdeanu, J. Tibshirani, R. Nallapati, and C. D. Man-
ning. 2012. Multi-instance multi-label learning for
relation extraction. In Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning (EMNLP/CoNLL), pages 455?
465.
S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G.
Banerjee, S. J. Teller, and N. Roy. 2011. Understand-
ing natural language commands for robotic navigation
and mobile manipulation. In Association for the Ad-
vancement of Artificial Intelligence (AAAI).
C. Unger, L. Bhmann, J. Lehmann, A. Ngonga, D. Ger-
ber, and P. Cimiano. 2012. Template-based ques-
tion answering over RDF data. In World Wide Web
(WWW), pages 639?648.
Y. W. Wong and R. J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Association for Computational Linguis-
tics (ACL), pages 960?967.
M. Yahya, K. Berberich, S. Elbassuoni, M. Ramanath,
V. Tresp, and G. Weikum. 2012. Natural language
questions for the web of data. In Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP/CoNLL), pages
379?390.
M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming. In
Association for the Advancement of Artificial Intelli-
gence (AAAI), pages 1050?1055.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Uncer-
tainty in Artificial Intelligence (UAI), pages 658?666.
C. Zhang, R. Hoffmann, and D. S. Weld. 2012. Onto-
logical smoothing for relation extraction with minimal
supervision. In Association for the Advancement of
Artificial Intelligence (AAAI).
1544
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1710?1720,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Learning Biological Processes with Global Constraints
Aju Thalappillil Scaria?, Jonathan Berant?, Mengqiu Wang and Christopher D. Manning
Stanford University, Stanford
Justin Lewis and Brittany Harding
University of Washington, Seattle
Peter Clark
Allen Institute for Artificial Intelligence, Seattle
Abstract
Biological processes are complex phenom-
ena involving a series of events that are re-
lated to one another through various relation-
ships. Systems that can understand and rea-
son over biological processes would dramat-
ically improve the performance of semantic
applications involving inference such as ques-
tion answering (QA) ? specifically ?How??
and ?Why?? questions. In this paper, we
present the task of process extraction, in
which events within a process and the rela-
tions between the events are automatically ex-
tracted from text. We represent processes by
graphs whose edges describe a set of temporal,
causal and co-reference event-event relations,
and characterize the structural properties of
these graphs (e.g., the graphs are connected).
Then, we present a method for extracting rela-
tions between the events, which exploits these
structural properties by performing joint in-
ference over the set of extracted relations.
On a novel dataset containing 148 descrip-
tions of biological processes (released with
this paper), we show significant improvement
comparing to baselines that disregard process
structure.
1 Introduction
A process is defined as a series of inter-related
events that involve multiple entities and lead to an
end result. Product manufacturing, economical de-
velopments, and various phenomena in life and so-
cial sciences can all be viewed as types of processes.
Processes are complicated objects; consider for ex-
ample the biological process of ATP synthesis de-
scribed in Figure 1. This process involves 12 en-
tities and 8 events. Additionally, it describes rela-
tions between events and entities, and the relation-
ship between events (e.g., the second occurrence of
the event ?enter?, causes the event ?changing?).
?Both authors equally contributed to the paper
Automatically extracting the structure of pro-
cesses from text is crucial for applications that re-
quire reasoning, such as non-factoid QA. For in-
stance, answering a question on ATP synthesis, such
as ?How do H+ ions contribute to the production
of ATP?? requires a structure that links H+ ions
(Figure 1, sentence 1) to ATP (Figure 1, sentence
4) through a sequence of intermediate events. Such
?How?? questions are common on FAQ websites
(Surdeanu et al, 2011), which further supports the
importance of process extraction.
Process extraction is related to two recent lines
of work in Information Extraction ? event extrac-
tion and timeline construction. Traditional event ex-
traction focuses on identifying a closed set of events
within a single sentence. For example, the BioNLP
2009 and 2011 shared tasks (Kim et al, 2009; Kim
et al, 2011) consider nine events types related to
proteins. In practice, events are currently almost al-
ways extracted from a single sentence. Process ex-
traction, on the other hand, is centered around dis-
covering relations between events that span multiple
sentences. The set of possible event types in process
extraction is also much larger.
Timeline construction involves identifying tem-
poral relations between events (Do et al, 2012; Mc-
Closky and Manning, 2012; D?Souza and Ng, 2013),
and is thus related to process extraction as both fo-
cus on event-event relations spanning multiple sen-
tences. However, events in processes are tightly cou-
pled in ways that go beyond simple temporal order-
ing, and these dependencies are central for the pro-
cess extraction task. Hence, capturing process struc-
ture requires modeling a larger set of relations that
includes temporal, causal and co-reference relations.
In this paper, we formally define the task of
process extraction and present automatic extraction
methods. Our approach handles an open set of event
types and works over multiple sentences, extract-
ing a rich set of event-event relations. Furthermore,
1710
7/5/13 7:01 PMbrat
Page 1 of 1http://127.0.0.1:8001/index.xhtml#/examples/emnlp2013/p66
H+ ions flowing down their gradient enter a half channel in a stator, which is anchored in the membrane.
H+ ions enter binding sites within a rotor, changing the shape of each subunit so that the rotor spins within the membrane.
Spinning of the rotor causes an internal rod to spin as well.
Turning of the rod activates catalytic sites in the knob that can produce ATP from ADP and P_i.
Entity Event Entity Event Entity Entity
cotemp
prev
Entity Event Entity Entity Event Entity Entity Event Entity
same
causes causes
prev
Event Entity Entity Event
same
same causes
Event Entity Event Entity Event Entity Entity
resultsame
raw-materialcauses causes
1
2
3
4
Figure 1: Partial annotation of the ATP synthesis process. Most of the semantic roles have been removed for simplicity.
we characterize a set of global properties of process
structure that can be utilized during process extrac-
tion. For example, all events in a process are some-
how connected to one another. Also, processes usu-
ally exhibit a ?chain-like? structure reflecting pro-
cess progression over time. We show that incor-
porating such global properties into our model and
performing joint inference over the extracted rela-
tions significantly improves the quality of process
structures predicted. We conduct experiments on a
novel dataset of process descriptions from the text-
book ?Biology? (Campbell and Reece, 2005) that
were annotated by trained biologists. Our method
does not require any domain-specific knowledge and
can be easily adapted to non-biology domains.
The main contributions of this paper are:
1. We define process extraction and characterize
processes? structural properties.
2. We model global structural properties in pro-
cesses and demonstrate significant improve-
ment in extraction accuracy.
3. We publicly release a novel data set of 148
fully annotated biological process descrip-
tions along with the source code for our sys-
tem. The dataset and code can be down-
loaded from http://nlp.stanford.edu/
software/bioprocess/.
2 Process Definition and Dataset
We define a process description as a paragraph or
sequence of tokens x = {x1, . . . x|x|} that describes
a series of events related by temporal and/or causal
relations. For example, in ATP synthesis (Figure 1),
the event of rotor spinning causes the event where
an internal rod spins.
We model the events within a process and their
relations by a directed graph P = (V,E), where
the nodes V = {1, . . . , |V |} represent event men-
tions and labeled edges E correspond to event-event
relations. An event mention v ? V is defined by a
trigger tv, which is a span of words xi, xi+1, . . . , xj ;
and by a set of argument mentions Av, where each
argument mention av ? Av is also a span of words
labeled by a semantic role l taken from a set L. For
example, in the last event mention of ATP synthesis,
tv = produce, and one of the argument mentions is
av = (ATP, RESULT). A labeled edge (u, v, r) in the
graph describes a relation r ? R between the event
mentions u and v. The task of process extraction is
to extract the graph P from the text x.1
A natural way to break down process extraction
into sub-parts is to first perform semantic role label-
ing (SRL), that is, identify triggers and predict ar-
gument mentions with their semantic role, and then
extract event-event relations between pairs of event
mentions. In this paper, we focus on the second
step, where given a set of event triggers T , we find
all event-event relations, where a trigger represents
the entire event. For completeness, we now describe
the semantic roles L used in our dataset, and then
1Argument mentions are also related by coreference rela-
tions, but we neglect that since it is not central in this paper.
1711
present the set of event-event relationsR.
The setL contains standard semantic roles such as
AGENT, THEME, ORIGIN, DESTINATION and LO-
CATION. Two additional semantic roles were em-
ployed that are relevant for biological text: RESULT
corresponds to an entity that is the result of an event,
and RAW-MATERIAL describes an entity that is used
or consumed during an event. For example, the last
event ?produce? in Figure 1, has ?ATP? as the RE-
SULT, and ?ADP? as the RAW-MATERIAL.
The event-event relation set R contains the fol-
lowing (assuming a labeled edge (u, v, r)):
1. PREV denotes that u is an event immediately
before v. Thus, the edges (u, v, PREV) and
(v, w, PREV), preclude the edge (u,w, PREV).
For example, in ?When a photon strikes
. . . energy is passed . . . until it reaches . . . ?,
there is no edge (strikes, reaches, PREV) due
to the intervening event ?passed?.
2. COTEMP denotes that events u and v overlap in
time (e.g., the first two event mentions flowing
and enter in Figure 1).
3. SUPER denotes that event u includes event
v. For instance, in ?During DNA replica-
tion, DNA polymerases proofread each nu-
cleotide. . . ? there is an edge (DNA replication,
proofread, SUPER).
4. CAUSES denotes that event u causes event v
(e.g., the relation between changing and spins
in sentence 2 of Figure 1).
5. ENABLES denotes that event u creates precon-
ditions that allow event v to take place. For
example, the description ?. . . cause cancer cells
to lose attachments to neighboring cells. . . , al-
lowing them to spread into nearby tissues? has
the edge (lose, spread, ENABLES). An in-
tuitive way to think about the difference be-
tween Causes and Enables is the following: if
u causes v this means that if u happens, then
v happens. If u enables v, then if u does not
happen, then v does not happen.
6. SAME denotes that u and v both refer to the
same event (spins and Spinning in Figure 1).
Early work on temporal logic (Allen, 1983) con-
tained more temporal relations than are used in our
Avg Min Max
# of sentences 3.80 1 15
# of tokens 89.98 19 319
# of events 6.20 2 15
# of non-NONE relations 5.64 1 24
Table 1: Process statistics over 148 process descriptions.
NONE is used to indicate no relation.
relation set R. We chose a relation set R that cap-
tures the essential aspects of temporal relations be-
tween events in a process, while keeping the annota-
tion as simple as possible. For instance, we include
the SUPER relation that appears in temporal anno-
tations such as the Timebank corpus (Pustejovsky
et al, 2003) and Allen?s work, but in practice was
not considered by many temporal ordering systems
(Chambers and Jurafsky, 2008; Yoshikawa et al,
2009; Do et al, 2012). Importantly, our relation set
also includes the relations CAUSES and ENABLES,
which are fundamental to modeling processes and
go beyond simple temporal ordering.
We also added event coreference (SAME) to R.
Do et al (2012) used event coreference information
in a temporal ordering task to modify probabilities
provided by pairwise classifiers prior to joint infer-
ence. In this paper, we simply treat SAME as an-
other event-event relation, which allows us to easily
perform joint inference and employ structural con-
straints that combine both coreference and temporal
relations simultaneously. For example, if u and v are
the same event, then there can exist no w, such that
u is before w, but v is after w (see Section 3.3)
We annotated 148 process descriptions based on
the aforementioned definitions. Further details on
annotation and data set statistics are provided in Sec-
tion 4 and Table 1.
Structural properties of processes Coherent pro-
cesses exhibit many structural properties. For ex-
ample, two argument mentions related to the same
event cannot overlap ? a constraint that has been
used in the past in SRL (Toutanova et al, 2008). In
this paper we focus on three main structural prop-
erties of the graph P . First, in a coherent pro-
cess, all events mentioned are related to one another,
and hence the graph P must be connected. Sec-
ond, processes tend to have a ?chain-like? structure
where one event follows another, and thus we expect
1712
Deg. Gold Local Global
0 0 29 0
1 219 274 224
2 369 337 408
3 46 14 17
? 4 22 2 7
Table 2: Node degree distribution for event mentions on
the training set. Predictions for the Local and Global
models were obtained using 10-fold cross validation.
nodes? degree to generally be ? 2. Indeed, 90% of
event mentions have degree ? 2, as demonstrated
by the Gold column of Table 2. Last, if we consider
relations between all possible triples of events in a
process, clearly some configurations are impossible,
while others are common (illustrated in Figure 2).
In Section 3.3, we show that modeling these proper-
ties using a joint inference framework improves the
quality of process extraction significantly.
3 Joint Model for Process Extraction
Given a paragraph x and a trigger set T , we wish
to extract all event-event relations E. Similar to Do
et al (2012), our model consists of a local pairwise
classifier and global constraints. We first introduce
a classifier that is based on features from previous
work. Next, we describe novel features specific for
process extraction. Last, we incorporate global con-
straints into our model using an ILP formulation.
3.1 Local pairwise classifier
The local pairwise classifier predicts relations be-
tween all event mention pairs. In order to model
the direction of relations, we expand the set R to
include the reverse of four directed relations: PREV-
NEXT, SUPER- SUB, CAUSES-CAUSED, ENABLES-
ENABLED. After adding NONE to indicate no rela-
tion, and including the undirected relations COTEMP
and SAME,R contains 11 relations. The classifier is
hence a function f : T ? T ? R. As an example,
f(ti, tj) = PREV iff f(tj , ti) = NEXT. Let n be the
number of triggers in a process, and ti be the i-th
trigger in its description. Since f(ti, tj) completely
determines f(tj , ti), it suffices to consider only pairs
with i < j. Note that the process graph P is undi-
rected under the new definition ofR.
Table 3 describes features from previous
Feature Description
POS Pair of POS tags
Lemma Pair of lemmas
Prep? Preposition lexeme, if in a prepositional phrase
Sent. count Quantized number of sentences between triggers
Word count Quantized number of words between triggers
LCA Least common ancestor on constituency tree, if exists
Dominates? Whether one trigger dominates other
Share Whether triggers share a child on dependency tree
Adjacency Whether two triggers are adjacent
Words btw. For adjacent triggers, content words between triggers
Temp. btw. For adjacent triggers, temporal connectives (from a
small list) between triggers
Table 3: Features extracted for a trigger pair (ti, tj). As-
teriks (*) indicate features that are duplicated, once for
each trigger.
work (Chambers and Jurafsky, 2008; Do et al,
2012) extracted for a trigger pair (ti, tj). Some
features were omitted since they did not yield
improvement in performance on a development set
(e.g., lemmas and part-of-speech tags of context
words surrounding ti and tj), or they require gold
annotations provided in TimeBank, which we do
not have (e.g., tense and aspect of triggers). To
reduce sparseness, we convert nominalizations into
their verbal forms when computing word lemmas,
using WordNet?s (Fellbaum, 1998) derivation links.
3.2 Classifier extensions
A central source of information to extract event-
event relations from text are connectives such as af-
ter, during, etc. However, there is variability in the
occurrence of these connectives as demonstrated by
the following two sentences (connectives in bold-
face, triggers in italics):
1. Because alleles are exchanged during gene flow, ge-
netic differences are reduced.
2. During gene flow, alleles are exchanged, and genetic
differences are hence reduced.
Even though both sentences express the same re-
lation (exchanged, reduced,CAUSES), the connec-
tives used and their linear position with respect to the
triggers are different. Also, in sentence 1, gene flow
intervenes between exchanged and reduced. Since
our dataset is small, we wish to identify the trig-
gers related to each connective, and share features
between such sentences. We do this using the syn-
tactic structure and by clustering the connectives.
1713
tj
ti tk
(a) SAME transitivity
SAMESAME
SAME
tj
ti tk
(b) CAUSE-COTEMP
CAUSES
CAUSES COTEMP
tj
ti tk
(c) COTEMP transitivity
COTEMPCOTEMP
COTEMP / SAME
tj
ti tk
(d) SAME contradiction
PREVPREV
SAME
tj
ti tk
(e) PREV contradiction
PREVPREV
PREV
Figure 2: Relation triangles (a)-(c) are common in the gold standard while (d)-(e) are impossible.
Sentence 1 presents a typical case where by walk-
ing up the dependency tree from the marker because,
we can find the triggers related by this marker:
because
mark
???? exchanged
advcl
???? reduced. When-
ever a trigger is the head of an adverbial clause and
marked by a mark dependency label, we walk on the
dependency tree and look for a trigger in the main
clause that is closest to the root (or the root itself
in this example). By utilizing the syntactic struc-
ture, we can correctly spot that the trigger gene flow
is not related to the trigger exchanged through the
connective because, even though they are linearly
closer. In order to reduce sparseness of connectives,
we created a hand-made clustering of 30 connectives
that maps words into clusters2 (e.g., because, since
and hence to a ?causality? cluster). After locating
the relevant pair of triggers, we use these clusters
to fire the same feature for connectives belonging to
the same cluster. We perform a similar procedure
whenever a trigger is part of a prepositional phrase
(imagine sentence 1 starting with ?due to allele ex-
change during gene flow . . . ?) by walking up the
constituency tree, but details are omitted for brevity.
In sentence 2, the connective hence is an adverbial
modifier of the trigger reduced. We look up the clus-
ter for the connective hence and fire the same feature
for the adjacent triggers exchanged and reduced.
We further extend our features to handle the rich
relation set necessary for process extraction. The
first event of a process is often expressed as a nom-
inalization and includes subsequent events (SUPER
relation), e.g., ?The Calvin cycle begins by incor-
porating...?. To capture this, we add a feature that
fires when the first event of the process description
is a noun. We also add two features targeted at the
2The full set of connectives and their clustering are provided
as part of our publicly released package.
SAME relation: one indicating if the lemmas of ti
and tj are the same, and another specifying the de-
terminer of tj , if it exists. Certain determiners in-
dicate that an event trigger has already been men-
tioned, e.g., the determiner this hints a SAME rela-
tion in ?The next steps decompose citrate back to
oxaloacetate. This regeneration makes . . . ?. Last,
we add as a feature the dependency path between ti
and tj , if it exists, e.g., in ?meiosis produces cells
that divide . . . ?, the feature
dobj
???
rcmod
???? is fired for
the trigger pair produces and divide. In Section 4.1
we empirically show that our extensions to the local
classifier substantially improve performance.
For our pairwise classifier, we train a maximum
entropy classifier that computes a probability pijr
for every trigger pair (ti, tj) and relation r. Hence,
f(ti, tj) = arg maxr pijr.
3.3 Global Constraints
Naturally, pairwise classifiers are local models that
can violate global properties in the process structure.
Figure 3 (left) presents an example for predictions
made by the pairwise classifier, which result in two
triggers (deleted and dupcliated) that are isolated
from the rest of the triggers. In this section, we dis-
cuss how we incorporate constraints into our model
to generate coherent global process structures.
Let ?ijr be the score for a relation r between the
trigger pair (ti, tj) (e.g., ?ijr = log pijr), and yijr be
the corresponding indicator variable. Our goal is to
find an assignment for the indicators y = {yijr | 1 ?
i < j ? n, r ? R}. With no global constraints this
can be formulated as the following ILP:
1714
arg max
y
?
ijr
?ijryijr (1)
s.t.?i,j
?
r
yijr = 1
where the constraint ensures exactly one relation be-
tween each event pair. We now describe constraints
that result in a coherent global process structure.
Connectivity Our ILP formulation for enforcing
connectivity is a minor variation of the one sug-
gested by Martins et al (2009) for dependency pars-
ing. In our setup, we want P to be a connected undi-
rected graph, and not a directed tree. However, an
undirected graph P is connected iff there exists a
directed tree that is a subgraph of P when edge di-
rections are ignored. Thus the resulting formulation
is almost identical and is based on flow constraints
which ensure that there is a path from a designated
root in the graph to all other nodes.
Let R? be the set R \ NONE. An edge (ti, tj) is
in E iff there is some non-NONE relation between
ti and tj , i.e. iff yij :=
?
r?R? yijr is equal to 1.
For each variable yij we define two auxiliary binary
variables zij and zji that correspond to edges of the
directed tree that is a subgraph of P . We ensure that
the edges in the tree exist also in P by tying each
auxiliary variable to its corresponding ILP variable:
?i<j zij ? yij , zji ? yij (2)
Next, we add constraints that ensure that the graph
structure induced by the auxiliary variables is a tree
rooted in an arbitrary node 1 (The choice of root
does not affect connectivity). We add for every i 6= j
a flow variable ?ij which specifies the amount of
flow on the directed edge zij .
?
i
zi1 = 0, ?j 6=1
?
i
zij = 1 (3)
?
i
?1i = n? 1 (4)
?j 6=1
?
i
?ij ?
?
k
?jk = 1 (5)
?i 6=j ?ij ? n ? zij (6)
Equation 3 says that all nodes in the graph have
exactly one parent, except for the root that has no
parents. Equation 4 ensures that the outgoing flow
from the root is n?1, and Equation 5 states that each
of the other n ? 1 nodes consume exactly one unit
of flow. Last, Equation 6 ties the auxiliary variables
to the flow variables, making sure that flow occurs
only on edges. The combination of these constraints
guarantees that the graph induced by the variables
zij is a directed tree and consequently the graph in-
duced by the objective variables y is connected.
Chain structure A chain is a connected graph
where the degree of all nodes is ? 2. Table 2
presents nodes? degree and demonstrates that indeed
process graphs are close to being chains. The fol-
lowing constraint bounds nodes? degree by 2:
?j(
?
i<j
yij +
?
j<k
yjk ? 2) (7)
Since graph structures are not always chains, we
add this as a soft constraint, that is, we penalize the
objective for each node with degree > 2. The chain
structure is one of the several soft constraints we
enforce. Thus, our modified objective function is
?
ijr ?ijryijr +
?
k?K ?kCk, where K is the set of
soft constraints, ?k is the penalty (or reward for de-
sirable structures), and Ck indicates whether a con-
straint is violated (or satisfied). Note that under this
formulation our model is simply a constrained con-
ditional model (wei Chang et al, 2012). The param-
eters ?k are tuned on a development set (see Sec-
tion 4).
Relation triads A relation triad (or a re-
lation triangle) for any three triggers ti, tj
and tk in a process is a 3-tuple of relations
(f(ti, tj), f(tj , tk), f(ti, tk)). Clearly, some triads
are impossible while others are quite common. To
find triads that could improve process extraction, the
frequency of all possible triads in both the training
set and the output of the pairwise classifier were
found, and we focused on those for which the clas-
sifier and the gold standard disagree. We are inter-
ested in triads that never occur in training data but
are predicted by the classifier, and vice versa. Fig-
ure 2 illustrates some of the triads found and Equa-
1715
tions 8-12 provide the corresponding ILP formula-
tions. Equations 8-10 were formulated as soft con-
straints (expanding the setK) and were incorporated
by defining a reward ?k for each triad type.3 On
the other hand, Equations 11-12 were formulated as
hard constraints to prevent certain structures.
1. SAME transitivity (Figure 2a, Eqn. 8): Co-
reference transitivity has been used in past
work (Finkel and Manning, 2008) and we in-
corporate it by a constraint that encourages tri-
ads that respect transitivity.
2. CAUSE-COTEMP (Figure 2b, Eqn. 9): If ti
causes both tj and tk, then often tj and tk are
co-temporal. E.g, in ?genetic drift has led to
a loss of genetic variation and an increase in
the frequency of . . .?, a single event causes two
subsequent events that occur simultaneously.
3. COTEMP transitivity (Figure 2c, Eqn. 10): If
ti is co-temporal with tj and tj is co-temporal
with tk, then usually ti and tk are either co-
temporal or denote the same event.
4. SAME contradiction (Figure 2d, Eqn. 11): If
ti is the same event as tk, then their tempo-
ral ordering with respect to a third trigger tj
may result in a contradiction, e.g., if tj is af-
ter ti, but before tk. We define 5 temporal
categories that generate
(5
2
)
possible contradic-
tions, but for brevity present just one represen-
tative hard constraint. This constraint depends
on prediction of temporal and co-reference re-
lations jointly.
5. PREV contradiction (Figure 2e, Eqn. 12): As
mentioned (Section 3.3), if ti is immediately
before tj , and tj is immediately before tk, then
ti cannot be immediately before tk.
yijSAME + yjkSAME + yikSAME ? 3 (8)
yijCAUSES + yikCAUSES + yjkCOTEMP ? 3 (9)
yijCOTEMP + yjkCOTEMP + yikCOTEMP+
yikSAME ? 3 (10)
yijPREV + yjkPREV + yikSAME ? 2 (11)
yijPREV + yjkPREV ? yikNONE ? 1 (12)
3We experimented with a reward for certain triads or a
penalty for others and empirically found that using rewards re-
sults in better performance on the development set.
We used the Gurobi optimization package4 to
find an exact solution for our ILP, which contains
O(n2|R|) variables and O(n3) constraints. We also
developed an equivalent formulation amenable to
dual decomposition (Sontag et al, 2011), which is a
faster approximation method. But practically, solv-
ing the ILP exactly with Gurobi was quite fast (av-
erage/median time per process: 0.294 sec/0.152 sec
on a standard laptop).
4 Experimental Evaluation
We extracted 148 process descriptions by going
through chapters from the textbook ?Biology? and
marking any contiguous sequence of sentences that
describes a process, i.e., a series of events that lead
towards some objective. Then, each process descrip-
tion was annotated by a biologist. The annotator was
first presented with annotation guidelines and anno-
tated 20 descriptions. The annotations were then
discussed with the authors, after which all process
descriptions were annotated. After training a sec-
ond biologist, we measured inter-annotator agree-
ment ? = 0.69, on 30 random process descriptions.
Process descriptions were parsed with Stanford
constituency and dependency parsers (Klein and
Manning, 2003; de Marneffe et al, 2006), and 35
process descriptions were set aside as a test set
(number of training set trigger pairs: 1932, number
of test set trigger pairs: 906). We performed 10-
fold cross validation over the training set for feature
selection and tuning of constraint parameters. For
each constraint type (connectivity, chain-structure,
and five triad constraints) we introduced a param-
eter and tuned the seven parameters by coordinate-
wise ascent, where for hard constraints a binary pa-
rameter controls whether the constraint is used, and
for soft constraints we attempted 10 different re-
ward/penalty values. For our global model we de-
fined ?ijr = log pijr, where pijr is the probability at
edge (ti, tj) for label r, given by the pairwise clas-
sifier.
We test the following systems: (a) All-Prev: Since
the most common process structure was chain-like,
we simply predict PREV for every two adjacent trig-
gers in text. (b) Localbase: A pairwise classifier with
features from previous work (Section 3.1) (c) Local:
4www.gurobi.com
1716
Temporal Full
P R F1 P R F1
All-Prev 58.4 54.8 56.6 34.1 32.0 33.0
Localbase 61.5 51.8 56.2 52.1 43.9 47.6
Local 63.2 55.7? 59.2 54.7 48.3? 51.3
Chain 64.5 60.5?? 62.4? 56.1 52.6?? 54.3?
Global 63.9 61.4?? 62.6?? 56.2 54.0?? 55.0??
Table 4: Test set results on all experiments. Best number
in each column is bolded. ? and ? denote statistical signif-
icance (p < 0.01) against Localbase and Local baselines,
respectively.
A pairwise classifier with all features (Section 3.2)
(d) Chain: For every two adjacent triggers, choose
the non-NONE relation with highest probability ac-
cording to Local. This baseline heuristically com-
bines our structural assumptions with the pairwise
classifier. We deterministically choose a connected
chain structure, and then use the classifier to label
the edges. (e) Global: Our full model that uses ILP
inference.
To evaluate system performance we compare the
set of predictions on all trigger pairs to the gold stan-
dard annotations and compute micro-averaged pre-
cision, recall and F1. We perform two types of eval-
uations: (a) Full: evaluation on our full set of 11
relations (b) Temporal: Evaluation on temporal re-
lations only, by collapsing PREV, CAUSES, and EN-
ABLES to a single category and similarly for NEXT,
CAUSED, and ENABLED (inter-annotator agreement
? = 0.75). We computed statistical significance
of our results with the paired bootstrap resampling
method of 2000 iterations (Efron and Tibshirani,
1993), where the units resampled are trigger-trigger-
relation triples.
4.1 Results
Table 4 presents performance of all systems. We see
that using global constraints improves performance
almost invariably on all measures in both full and
temporal evaluations. Particularly, in the full eval-
uation Global improves recall by 12% and overall
F1 improves significantly by 3.7 points against Lo-
cal (p < 0.01). Recall improvement suggests that
modeling connectivity allowed Global to add cor-
rect relations in cases where some events were not
connected to one another.
The Local classifier substantially outperforms
Localbase. This indicates that our novel features
(Section 3.2) are important for discriminating be-
tween process relations. Specifically, in the full eval-
uation Local improves precision more than in the
temporal evaluation, suggesting that designing syn-
tactic and semantic features for connectives is useful
for distinguishing PREV, CAUSES, and ENABLES
when the amount of training data is small.
The Chain baseline performs only slightly worse
than our global model. This demonstrates the strong
tendency of processes to proceed linearly from one
event to the other, which is a known property of dis-
course structure (Schegloff and Sacks, 1973). How-
ever, since the structure is deterministically fixed,
Chain is highly inflexible and does not allow any
extensions or incorporation of other structural con-
straints or domain knowledge. Thus, it can be used
as a simple and efficient approximation but is not a
good candidate for a real system. Further support
for the linear nature of process structure is provided
by the All-Prev baseline, which performs poorly in
the full evaluation, but in temporal evaluation works
reasonably well.
Table 2 presents the degree distribution of Local
and Global on the development set comparing to the
gold standard. The degree distribution of Global is
more similar to the gold standard than Local. In par-
ticular, the connectivity constraint ensures that there
are no isolated nodes and shifts mass from nodes
with degree 0 and 1 to nodes with degree 2.
Table 5 presents the order in which constraints
were introduced into the global model using coor-
dinate ascent on the development set. Connectivity
is the first constraint to be introduced, and improves
performance considerably. The chain constraint, on
the other hand, is included third and the improve-
ment in F1 score is relatively smaller. This can be
explained by the distribution of degrees in Table 2
which shows that the predictions of Local does not
have many nodes with degree > 2. As for triad con-
straints, we see that four constraints are important
and are included in the model, but one is discarded.
Last, we examined the results of Global when
macro-averaging over processes, i.e., assigning each
process the same weight by computing recall, pre-
cision and F1 for each process and averaging those
scores. We found that results are quite similar
(with a slight improvement): in the full evalua-
1717
Order Parameter name Value (?) F1 score
? Local model ? 49.9
1 Connectivity constraint ? 51.2
2 SAME transitivity 0.5 52.9
3 Chain constraint -0.5 53.3
4 CAUSE-COTEMP 1.0 53.7
6 PREV contradiction ? 53.8
7 SAME contradiction ? 53.9
Table 5: Order by which constraint parameters were set
using coordinate ascent on the development set. For each
parameter, the value chosen and F1 score after including
the constraint are provided. Negative values correspond
to penalties, positive values to rewards, and a value of?
indicates a hard constraint.
tion Global obtains R/P/F1 of 56.4/55.0/55.7, and
in the temporal evaluation Global obtains R/P/F1 of
63.8/62.3/63.1.
4.2 Qualitative Analysis
Figure 3 shows two examples where global con-
straints corrected the predictions of Local. In Fig-
ure 3, left, Local failed to predict the causal rela-
tions skipped-deleted and used-duplicated, possibly
because they are not in the same sentence and are not
adjacent to one another. By enforcing the connectiv-
ity constraint, Global correctly adds the correct re-
lations and connects deleted and duplicated to the
other triggers in the process.
In Figure 3, right, Local predicts a structure that
results in a ?SAME contradiction? structure. The
triggers bind and binds cannot denote the same event
if a third trigger secrete is temporally between them.
However, Local predicts they are the same event, as
they share a lemma. Global prohibits this structure
and correctly predicts the relation as NONE.
To better understand the performance of Local,
we analyzed the confusion matrix generated based
on its predictions. Although this is a challenging
11-class classification task, most of the mass is con-
centrated on the matrix diagonal, as desired. Error
analysis reveals that 17.5% of all errors are con-
fusions between NONE and PREV, 11.1% between
PREV and CAUSES, and 8.6% between PREV and
COTEMP. This demonstrates that distinguishing the
classes PREV, CAUSES and COTEMP is challenging
for Local. Our current global constraints do not ad-
dress this type of error, and thus an important direc-
tion for future work is to improve the local model.
The global model depends on the predictions of
the local classifier, and so enforcing global con-
straints does not guarantee improvement in perfor-
mance. For instance, if Local produces a graph that
is disconnected (e.g., deleted in Figure 3, left), then
Global will add an edge. However, the label of the
edge is determined by scores computed based on
the local classifier, and if this prediction is wrong,
we will now be penalized for both the false nega-
tive of the correct class (just as before), and also for
the false positive of the predicted class. Despite that
we see that Global improves overall performance by
3.7 F1 points on the test set.
5 Related Work
A related line of work is biomedical event extrac-
tion in recent BioNLP shared tasks (Kim et al,
2009; Kim et al, 2011). Earlier work employed a
pipeline architecture where first events are found,
and then their arguments are identified (Miwa et al,
2010; Bjo?rne et al, 2011). Subsequent methods pre-
dicted events and arguments jointly using Markov
logic (Poon and Vanderwende, 2010) and depen-
dency parsing algorithms (McClosky et al, 2011).
Riedel and McCallum (2011) further improved per-
formance by capturing correlations between events
and enforcing consistency across arguments.
Temporal event-event relations have been ex-
tensively studied (Chambers and Jurafsky, 2008;
Yoshikawa et al, 2009; Denis and Muller, 2011;
Do et al, 2012; McClosky and Manning, 2012;
D?Souza and Ng, 2013), and we leverage such
techniques in our work (Section 3.1). However,
we extend beyond temporal relations alone, and
strongly rely on dependencies between process
events. Chambers and Jurafsky (2011) learned event
templates (or frames), where events that are related
to one another and their semantic roles are extracted.
Recently, Cheung et al (2013) proposed an unsuper-
vised generative model for inducing such templates.
A major difference in our work is that we do not
learn typical event relations from a large and redun-
dant corpus, but are given a paragraph and have a
?one-shot? chance to extract the process structure.
We showed in this paper that global structural
properties lead to significant improvements in ex-
traction accuracy, and ILP is an effective framework
1718
shifts
skippedCAUSESCAUSES
usedCAUSESCAUSES
deletedCAUSESCAUSES
duplicatedCAUSESCAUSES bind
secreteCOTEMPPREV bindsSAMENONE
PREVENABLES
Figure 3: Process graph fragments. Black edges (dotted) are predictions of Local, green (solid) are predictions of
Global, and gold (dashed) are gold standard edges. To reduce clutter, we present the predictions of Global only when
it disagrees with Local. In all other cases, the predictions of Global and Local are identical. Original text, Left: ?... the
template shifts . . . , and a part of the template strand is either skipped by the replication machinery or used twice as a
template. As a result, a segment of DNA is deleted or duplicated.? Right: ?Cells of mating type A secrete a signaling
molecule, which can bind to specific receptor proteins on nearby cells. At the same time, cells secrete factor, which
binds to receptors on A cells.?
for modeling global constraints. Similar observa-
tions and techniques have been proposed in other
information extraction tasks. Reichart and Barzi-
lay (2012) tied information from multiple sequence
models that describe the same event by using global
higher-order potentials. Berant et al (2011) pro-
posed a global inference algorithm to identify entail-
ment relations. There is an abundance of examples
of enforcing global constraints in other NLP tasks,
such as in coreference resolution (Finkel and Man-
ning, 2008), parsing (Rush et al, 2012) and named
entity recognition (Wang et al, 2013).
6 Conclusion
Developing systems that understand process de-
scriptions is an important step towards building ap-
plications that require deeper reasoning, such as bi-
ological process models from text, intelligent tutor-
ing systems, and non-factoid QA systems. In this
paper we have presented the task of process extrac-
tion, and developed methods for extracting relations
between process events. Processes contain events
that are tightly coupled through strong dependen-
cies. We have shown that exploiting these structural
dependencies and performing joint inference over all
event mentions can significantly improve accuracy
over several baselines. We have also released a new
dataset containing 148 fully annotated descriptions
of biological processes. Though the models we built
were trained on biological processes, they do not en-
code domain specific information, and hence should
be extensible to other domains.
In this paper we assumed that event triggers are
given as input. In future work, we want to perform
trigger identification jointly with extraction of event-
event relations. As explained in Section 4.2, the
performance of our system is confined by the per-
formance of the local classifier, which is trained on
relatively small amounts of data. Since data annota-
tion is expensive, it is important to improve the lo-
cal classifier without increasing the annotation bur-
den. For example, one can use unsupervised meth-
ods that learn narrative chains (Chambers and Ju-
rafsky, 2011) to provide some prior on the typical
order of events. Alternatively, we can search on the
web for redundant descriptions of the same process
and use this redundancy to improve classification.
Last, we would like to integrate our method into QA
systems and allow non-factoid questions that require
deeper reasoning to be answered by matching the
questions against the learned process structures.
Acknowledgments
The authors would like to thank Roi Reichart for
fruitful discussion and the anonymous reviewers for
their constructive feedback. This work was partially
funded by Vulcan Inc. The second author was spon-
sored by a Rothschild fellowship.
References
James F. Allen. 1983. Maintaining knowledge about
temporal intervals. Commun. ACM, 26(11):832?843.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Learning entailment relations by global graph
structure optimization. Journal of Computational Lin-
guistics, 38(1).
1719
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2011. Extract-
ing contextualized complex biological events with rich
graph-based feature sets. Computational Intelligence,
27(4):541?557.
Neil Campbell and Jane Reece. 2005. Biology. Ben-
jamin Cummings.
Nathanael Chambers and Daniel Jurafsky. 2008. Jointly
combining implicit constraints improves temporal or-
dering. In Proceedings of EMNLP.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
ACL, pages 976?986.
Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction. In
Proceedings of NAACL-HLT.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC.
Pascal Denis and Philippe Muller. 2011. Predicting
globally-coherent temporal structures from texts via
endpoint inference and graph decomposition. In Pro-
ceedings of IJCAI.
Quang Do, Wei Lu, and Dan Roth. 2012. Joint infer-
ence for event timeline construction. In Proceedings
of EMNLP-CoNLL.
Jennifer D?Souza and Vincent Ng. 2013. Classifying
temporal relations with rich linguistic knowledge. In
Proceedings of NAACL-HLT.
Bradley Efron and Robert Tibshirani. 1993. An introduc-
tion to the bootstrap, volume 57. CRC press.
Christiane Fellbaum, editor. 1998. WordNet: An elec-
tronic lexical database. MIT Press.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
Proceedings of ACL.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Junichi Tsujii. 2009. Overview of
BioNLP 09 shared task on event extraction. In Pro-
ceedings of BioNLP.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Junichi Tsujii. 2011. Overview of BioNLP
shared task 2011. In Proceedings of BioNLP.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL.
Andre? L. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proceedings of
ACL/IJCNLP.
David McClosky and Christopher D. Manning. 2012.
Learning constraints for consistent timeline extraction.
In Proceedings of EMNLP-CoNLL, pages 873?882.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011. Event extraction as dependency pars-
ing. In Proceedings of ACL, pages 1626?1635.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and Jun?ichi
Tsujii. 2010. Event extraction with complex event
classification using rich features. J. Bioinformatics
and Computational Biology, 8(1).
Hoifung Poon and Lucy Vanderwende. 2010. Joint in-
ference for knowledge extraction from biomedical lit-
erature. In Proceedings of HLT-NAACL.
James Pustejovsky, Jose? M. Castan?o, Robert Ingria,
Roser Sauri, Robert J. Gaizauskas, Andrea Setzer,
Graham Katz, and Dragomir R. Radev. 2003.
TimeML: Robust specification of event and temporal
expressions in text. In New Directions in Question An-
swering.
Roi Reichart and Regina Barzilay. 2012. Multi-event ex-
traction guided by global constraints. In Proceedings
of HLT-NAACL.
Sebastian Riedel and Andrew McCallum. 2011. Fast and
robust joint models for biomedical event extraction. In
Proceedings of EMNLP.
Alexander M. Rush, Roi Reichert, Michael Collins, and
Amir Globerson. 2012. Improved parsing and POS
tagging using inter-sentence consistency constraints.
In Proceedings of EMNLP.
Emanuel A Schegloff and Harvey Sacks. 1973. Opening
up closings. Semiotica, 8(4):289?327.
David Sontag, Amir Globerson, and Tommi Jaakkola.
2011. Introduction to dual decomposition for in-
ference. In Suvrit Sra, Sebastian Nowozin, and
Stephen J. Wright, editors, Optimization for Machine
Learning. MIT Press.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-
factoid questions from web collections. Computa-
tional Linguistics, 37(2).
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34(2):161?
191.
Mengqiu Wang, Wanxiang Che, and Christopher D. Man-
ning. 2013. Effective bilingual constraints for semi-
supervised learning of named entity recognizers. In
Proceedings of AAAI.
Ming wei Chang, Lev Ratinov, and Dan Roth. 2012.
Structured learning with constrained conditional mod-
els. Machine Learning, 88(3):399?431, 6.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identifying
temporal relations with Markov logic. In Proceedings
of ACL/IJCNLP.
1720
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1499?1510,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Modeling Biological Processes for Reading Comprehension
Jonathan Berant
?
, Vivek Srikumar
?
, Pei-Chun Chen, Brad Huang and Christopher D. Manning
Stanford University, Stanford
Abby Vander Linden and Brittany Harding
University of Washington, Seattle
Abstract
Machine reading calls for programs that
read and understand text, but most current
work only attempts to extract facts from
redundant web-scale corpora. In this pa-
per, we focus on a new reading compre-
hension task that requires complex reason-
ing over a single document. The input is
a paragraph describing a biological pro-
cess, and the goal is to answer questions
that require an understanding of the re-
lations between entities and events in the
process. To answer the questions, we first
predict a rich structure representing the
process in the paragraph. Then, we map
the question to a formal query, which is
executed against the predicted structure.
We demonstrate that answering questions
via predicted structures substantially im-
proves accuracy over baselines that use
shallower representations.
1 Introduction
The goal of machine reading is to develop pro-
grams that read text to learn about the world
and make decisions based on accumulated knowl-
edge. Work in this field has focused mostly on
macro-reading, i.e., processing large text collec-
tions and extracting knowledge bases of facts (Et-
zioni et al., 2006; Carlson et al., 2010; Fader et al.,
2011). Such methods rely on redundancy, and are
thus suitable for answering common factoid ques-
tions which have ample evidence in text (Fader et
al., 2013). However, reading a single document
(micro-reading) to answer comprehension ques-
tions that require deep reasoning is currently be-
yond the scope of state-of-the-art systems.
In this paper, we introduce a task where given
a paragraph describing a process, the goal is to
?
Both authors equally contributed to the paper.
answer reading comprehension questions that test
understanding of the underlying structure. In par-
ticular, we consider processes in biology text-
books such as this excerpt and the question that
follows:
?. . . Water is split, providing a source of elec-
trons and protons (hydrogen ions, H
+
) and giv-
ing off O
2
as a by-product. Light absorbed by
chlorophyll drives a transfer of the electrons
and hydrogen ions from water to an acceptor
called NADP
+
. . . ?
Q What can the splitting of water lead to?
a Light absorption
b Transfer of ions
This excerpt describes a process in which a com-
plex set of events and entities are related to one
another. A system trying to answer this ques-
tion must extract a rich structure spanning multi-
ple sentences and reason that water splitting com-
bined with light absorption leads to transfer of
ions. Note that shallow methods, which rely on
lexical overlap or text proximity, will fail. Indeed,
both answers are covered by the paragraph and the
wrong answer is closer in the text to the question.
We propose a novel method that tackles this
challenging problem (see Figure 1). First, we train
a supervised structure predictor that learns to ex-
tract entities, events and their relations describing
the biological process. This is a difficult prob-
lem because events have complex interactions that
span multiple sentences. Then, treating this struc-
ture as a small knowledge-base, we map ques-
tions to formal queries that are executed against
the structure to provide the answer.
Micro-reading is an important aspect of natural
language understanding (Richardson et al., 2013;
Kushman et al., 2014). In this work, we focus
specifically on modeling processes, where events
and entities relate to one another through com-
plex interactions. While we work in the biology
1499
?. . . Water is split, providing a source of elec-
trons and protons (hydrogen ions, H
+
) and
giving off O
2
as a by-product. Light ab-
sorbed by chlorophyll drives a transfer of
the electrons and hydrogen ions from water
to an acceptor called NADP+ . . . ?
Q What can the splitting of water lead to?
a Light absorption
b Transfer of ions
water
split
THEME
absorb
light
THEME
transfer ions
THEME
ENABLE
CAUSE
water
split
absorb
light
THEME
(CAUSE|ENABLE)
+
THEME
water
split
transfer ions
THEME
(CAUSE|ENABLE)
+
THEME
Step 1
Step 2
Step 3: Answer = b
Figure 1: An overview of our reading comprehension system. First, we predict a structure from the input paragraph (the
top right portion shows a partial structure skipping some arguments for brevity). Circles denote events, squares denote argu-
ments, solid arrows represent event-event relations, and dashed arrows represent event-argument relations. Second, we map
the question paired with each answer into a query that will be answered using the structure. The bottom right shows the query
representation. Last, the two queries are executed against the structure, and a final answer is returned.
domain, processes are abundant in domains such
as chemistry, economics, manufacturing, and even
everyday events like shopping or cooking, and our
model can be applied to these domains as well.
The contributions of this paper are:
1. We propose a reading comprehension task
which requires deep reasoning over struc-
tures that represent complex relations be-
tween multiple events and entities.
2. We present PROCESSBANK, a new dataset
consisting of descriptions of biological pro-
cesses, fully-annotated with rich process
structures, and accompanied by multiple-
choice questions.
3. We present a novel method for answer-
ing questions, by predicting process struc-
tures and mapping questions to queries. We
demonstrate that by predicting structures we
can improve reading comprehension accu-
racy over baselines that do not exploit the un-
derlying structure.
The data and code for this paper are avail-
able at http://www-nlp.stanford.edu/
software/bioprocess.
2 Task Definition and Setup
This section describes the reading comprehension
task we address and the accompanying dataset.
We will use the example in Figure 1 as our run-
ning example throughout the paper.
Our goal is to tackle a complex reading com-
prehension setting that centers on understanding
the underlying meaning of a process description.
We target a multiple-choice setting in which each
input consists of a paragraph of text describing a
biological process, a question, and two possible
answers. The goal is to identify the correct answer
using the text (Figure 1, left). We used the 148
paragraphs from the textbook Biology (Campbell
and Reece, 2005) that were manually identified by
Scaria et al. (2013). We extended this set to 200
paragraphs by including additional paragraphs that
describe biological processes. Each paragraph in
the collection represents a single biological pro-
cess and describes a set of events, their partici-
pants and their interactions.
Because we target understanding of paragraph
meaning, we use the following desiderata for
building the corpus of questions and answers:
1. The questions should focus on the events and
entities participating in the process described
in the paragraph, and answering the questions
should require reasoning about the relations
between those events and entities.
2. Both answers should have similar lexical
overlap with the paragraph. Moreover, names
of entities and events in the question and an-
swers should appear as in the paragraph and
not using synonyms. This is to ensure that the
task revolves around reading comprehension
rather than lexical variability.
1
A biologist created the question-answer part of
1
Lexical variability is an important problem in NLP, but
is not the focus of this task.
1500
the corpus comprising of 585 questions spread
over the 200 paragraphs. A second annotator val-
idated 326 randomly chosen questions and agreed
on the correct answer with the first annotator in
98.1% of cases. We provide the annotation guide-
lines in the supplementary material.
Figure 1 (left) shows an excerpt of a paragraph
describing a process and an example of a ques-
tion based on it. In general, questions test an un-
derstanding of the interactions between multiple
events (such as causality, inhibition, temporal or-
dering), or between events and entities (i.e., roles
of entities in events), and require complex reason-
ing about chains of event-event and event-entity
relations.
3 The Structure of Processes
A natural first step for answering reading compre-
hension questions is to identify a structured rep-
resentation of the text. In this section, we define
this structure. We broadly follow the definition of
Scaria et al. (2013), but modify important aspects,
highlighted at the end of this section.
A paragraph describing a process is a sequence
of tokens that describes events, entities and their
relations (see Figure 1, top right). A process is
a directed graph (T ,A, E
tt
, E
ta
), where the nodes
T are labeled event triggers, the nodes A are ar-
guments, E
tt
are labeled edges describing event-
event relations, and E
ta
are labeled edges from
triggers to arguments denoting semantic roles (see
Figure 1 top right for a partial structure of the run-
ning example). The goal of process extraction is
to generate the process graph given the input para-
graph.
Triggers and arguments A trigger is a token
span denoting the occurrence of an event. In Fig-
ure 1, split, absorbed and transfer are event trig-
gers. In rare cases, a trigger denotes the non-
occurrence of an event. For example, in ?sym-
patric speciation can occur when gene flow is
blocked?, sympatric speciation occurs if gene flow
does not happen. Thus, nodes in T are labeled as
either a T-YES or T-NO to distinguish triggers of
events that occur from triggers of events that do
not occur. Arguments are token spans denoting
entities that participate in the process (such as wa-
ter, light and ions in Figure 1).
Semantic roles The edges E
ta
from triggers
to arguments are labeled by the semantic roles
AGENT, THEME, SOURCE, DESTINATION, LO-
CATION, RESULT, and OTHER for all other roles.
Our running example shows three THEME seman-
tic roles for the three triggers. For brevity, the fig-
ure does not show the RESULT of the event split,
namely, both source of electrons and protons (hy-
drogen ions, H
+
) and O
2
.
Event-event relations The directed edges E
tt
between triggers are labeled by one of eight pos-
sible event-event relations. These relations are
central to answering reading comprehension ques-
tions, which test understanding of the depen-
dencies and causal relations between the process
events. We first define three relations that express
a dependency between two event triggers u and v.
1. CAUSE denotes that u starts before v, and if
u happens then v happens (Figure 1).
2. ENABLE denotes that u creates conditions
necessary for the occurrence of v. This
means that u starts before v and v can only
happen if u happens (Figure 1).
2
3. PREVENT denotes that u starts before v and
if u happens, then v does not happen.
In processes, events sometimes depend on more
than one other event. For example, in Figure 1
(right top) transfer of ions depends on both water
splitting as well as light absorption. Conversely,
in Figure 2, the shifting event results in either one
of two events but not both. To express both con-
junctions and disjunctions of related events we
add the relations CAUSE-OR, ENABLE-OR and
PREVENT-OR, which express disjunctions, while
the default CAUSE, ENABLE, and PREVENT ex-
press conjunction (Compare the CAUSE-OR rela-
tions in Figure 2 with the relations in Figure 1).
We define the SUPER relation to denote that
event u is part of event v. (In Figure 2, slip-
page is a sub-event of replication.) Last, we use
the event coreference relation SAME to denote two
event mentions referring to the same event.
Notice that the assignments of relation labels in-
teract across different pairs of events. As an ex-
ample, if event u causes event v, then v can not
cause u. Our inference algorithm uses such struc-
tural constraints when predicting process structure
(Section 4).
2
In this work, we do not distinguish causation from facil-
itation, where u can help v but is not absolutely required. We
instructed the annotators to ignore the inherent uncertainty in
these cases and use CAUSE.
1501
Figure 2: Partial example of a process, as annotated in our dataset.
Avg Min Max
# of triggers 7.0 2 18
# of arguments 11.3 1 36
# of relation 7.9 1 37
Table 1: Statistics of triggers, arguments and rela-
tions over the 200 annotated paragraphs.
Three biologists annotated the same 200 para-
graphs described in Section 2 using the brat anno-
tation tool (Stenetorp et al., 2012). For each para-
graph, one annotator annotated the process, and
a second validated its correctness. Importantly,
the questions and answers were authored sepa-
rately by a different annotator, thus ensuring that
the questions and answers are independent from
the annotated structures. Table 1 gives statistics
over the dataset. The annotation guidelines are in-
cluded in the supplementary material.
Relation to Scaria et al. (2013) Scaria et al.
(2013) also defined processes as graphs where
nodes are events and edges describe event-event
relations. Our definition differs in a few important
aspects.
First, the set of event-event relations in that
work included temporal relations in addition to
causal ones. In this work, we posit that because
events in a process are inter-related, causal depen-
dencies are sufficient to capture the relevant tem-
poral ordering between them. Figure 1 illustrates
this phenomenon, where the temporal ordering be-
tween the events of water splitting and light ab-
sorption is unspecified. It does not matter whether
one happens before, during, or after the other. Fur-
thermore, the incoming causal links to transfer im-
ply that the event should happen after splitting and
absorption.
A second difference is that Scaria et al. (2013)
do not include disjunctions and conjunctions of
events in their formulation. Last, Scaria et al.
(2013) predict only relations given input triggers,
while we predict a full process structure.
4 Predicting Process Structures
We now describe the first step of our algorithm.
Given an input paragraph we predict events, their
arguments and event-event relations (Figure 1,
top). We decompose this into three sub-problems:
1. Labeling trigger candidates using a multi-
class classifier (Section 4.1).
2. For each trigger, identifying an over-
complete set of possible arguments, using a
classifier tuned for high recall (Section 4.2).
3. Jointly assigning argument labels and rela-
tion labels for all trigger pairs (Section 4.3).
The event-event relations CAUSE, ENABLE,
CAUSE-OR and ENABLE-OR, form a semantic
cluster: If (u, v) is labeled by one of these, then
the occurrence of v depends on the occurrence of
u. Since our dataset is small, we share statistics by
collapsing all four labels to a single ENABLE la-
bel. Similarly, we collapse the PREVENT and
PREVENT-OR labels, overall reducing the number
of relations to four.
For brevity, in what follows we only provide
a flavor of the features we extract, and refer the
reader to the supplementary material for details.
4.1 Predicting Event Triggers
The first step is to identify the events in the pro-
cess. We model the trigger detector as a multi-
class classifier that labels all content words in
the paragraph as one of T-YES, T-NO or NOT-
TRIGGER (Recall that a word can trigger an event
that occurred, an event that did not occur, or not
be a trigger at all). For simplicity, we model trig-
gers as single words, but in the gold annotation
about 14% are phrases (such as gene flow). Thus,
we evaluate trigger prediction by taking heads of
gold phrases. To train the classifier, we extract
1502
the lemma and POS tag of the word and adja-
cent words, dependency path to the root, POS
tag of children and parent in the dependency tree,
and clustering features from WordNet (Fellbaum,
1998), Nomlex (Macleod et al., 1998), Levin verb
classes (Levin, 1993), and a list of biological pro-
cesses compiled from Wikipedia.
4.2 Filtering Argument Candidates
Labeling trigger-argument edges is similar to se-
mantic role labeling. Following the standard ap-
proach (Punyakanok et al., 2008), for each trigger
we collect all constituents in the same sentence to
build an over-complete set of plausible candidate
arguments. This set is pruned with a binary classi-
fier that is tuned for high recall (akin to the argu-
ment identifier in SRL systems). On the develop-
ment set we filter more than half of the argument
candidates, while achieving more than 99% recall.
This classifier is trained using argument identifica-
tion features from Punyakanok et al. (2008).
At the end of this step, each trigger has a set of
candidate arguments which will be labeled during
joint inference. In further discussion, the argument
candidates for trigger t are denoted by A
t
.
4.3 Predicting Arguments and Relations
Given the output of the trigger classifier, our goal
is to jointly predict event-argument and event-
event relations. We model this as an integer linear
program (ILP) instance described below. We first
describe the inference setup assuming a model that
scores inference decisions and defer description of
learning to Section 4.4. The ILP has two types of
decision variables: arguments and relations.
Argument variables These variables capture
the decision that a candidate argument a, belong-
ing to the set A
t
of argument candidates, takes a
label A (from Section 3). We denote the Boolean
variables by y
t,a,A
, which are assigned a score
b
t,a,A
by the model. We include an additional label
NULL-ARG, indicating that the candidate is not an
argument for the trigger.
Event-event relation variables These variables
capture the decision that a pair of triggers t
1
and
t
2
are connected by a directed edge (t
1
, t
2
) labeled
by the relation R. We denote these variables by
z
t
1
,t
2
,R
, which are associated with a score c
t
1
,t
2
,R
.
Again, we introduce a label NULL-REL to indicate
triggers that are not connected by an edge.
Name Description
Unique labels Every argument candidate and trigger pair has ex-
actly one label.
Argument overlap Two arguments of the same trigger cannot overlap.
Relation symmetry The SAME relation is symmetric. All other rela-
tions are anti-symmetric, i.e., for any relation la-
bel other than SAME, at most one of (t
i
, t
j
) or
(t
j
, t
i
) can take that label and the other is assigned
the label NULL-REL.
Max arguments per
trigger
Every trigger can have no more than two arguments
with the same label.
Max triggers per ar-
gument
The same span of text can not be an argument for
more than two triggers.
Connectivity The triggers must form a connected graph, framed
as flow constraints as in Magnanti and Wolsey
(1995) and Martins et al. (2009).
Shared arguments If the same span of text is an argument of two trig-
gers, then the triggers must be connected by a rela-
tion that is not NULL-REL. This ensures that trig-
gers that share arguments are related.
Unique parent For any trigger, at most one outgoing edge can be
labeled SUPER.
Table 2: Constraints for joint inference.
Formulation Given the two sets of variables,
the objective of inference is to find a global as-
signment that maximizes the score. That is, the
objective can be stated as follows:
max
y,z
?
t,a?A
t
,A
b
t,a,A
? y
t,a,A
+
?
t
1
,t
2
,R
c
t
1
,t
2
,R
? z
t
1
,t
2
,R
Here, y and z refer to all the argument and rela-
tion variables respectively.
Clearly, all possible assignments to the infer-
ence variables are not feasible and there are both
structural as well as prior knowledge constraints
over the output space. Table 2 states the con-
straints we include, which are expressed as linear
inequalities over output variables using standard
techniques (e.g., (Roth and Yih, 2004)).
4.4 Learning in the Joint Model
We train both the trigger classifier and the argu-
ment identifier using L
2
-regularized logistic re-
gression. For the joint model, we use a linear
model for the scoring functions, and train jointly
using the structured averaged perceptron algo-
rithm (Collins, 2002).
Since argument labeling is similar to semantic
role labeling (SRL), we extract standard SRL fea-
tures given the trigger and argument from the syn-
tactic tree for the corresponding sentence. In ad-
dition, we add features extracted from an off-the-
shelf SRL system. We also include all feature con-
junctions. For event relations, we include the fea-
tures described in Scaria et al. (2013), as well as
context features for both triggers, and the depen-
dency path between them, if one exists.
1503
5 Question Answering via Structures
This section describes our question answering sys-
tem that, given a process structure, a question and
two answers, chooses the correct answer (steps 2
and 3 in Figure 1).
Our strategy is to treat the process structure as
a small knowledge-base. We map each answer
along with the question into a structured query that
we compare against the structure. The query can
prove either the correctness or incorrectness of the
answer being considered. That is, either we get a
valid match for an answer (proving that the cor-
responding answer is correct), or we get a refu-
tation in the form of a contradicted causal chain
(thus proving that the other answer is correct).
This is similar to theorem proving approaches sug-
gested in the past for factoid question answering
(Moldovan et al., 2003).
The rest of this section is divided into three
parts: Section 5.1 defines the queries we use, Sec-
tion 5.2 describes a rule-based algorithm for con-
verting a question and an answer into a query and
finally, 5.3 describes the overall algorithm.
5.1 Queries over Processes
We model a query as a directed graph path with
regular expressions over edge labels. The bot-
tom right portion of Figure 1 shows examples of
queries for our running example. In general, given
a question and one of the answer candidates, one
end of the path is populated by a trigger/argument
found in the question and the other is populated
with a trigger/ argument from the answer.
We define a query to consist of three parts:
1. A regular expression over relation labels, de-
scribing permissible paths,
2. A source trigger/argument node, and
3. A target trigger/argument node.
For example, the bottom query in Figure 1 looks
for paths labeled with CAUSE or ENABLE edges
from the event split to the event transfer.
Note that the representation of questions as di-
rected paths is a modeling choice and did not influ-
ence the authoring of the questions. Indeed, while
most questions do fit this model, there are rare
cases that require a more complex query structure.
5.2 Query Generation
Mapping a question and an answer into a query
involves identifying the components of the query
listed above. We do this in two phases: (1) In the
alignment phase, we align triggers and arguments
in the question and answer to the process structure
to give us candidate source and target nodes. (2)
In the query construction phase, we identify the
regular expression and the direction of the query
using the question, the answer and the alignment.
We identify three broad categories of QA pairs
(see Table 3) that can be identified using simple
lexical rules: (a) Dependency questions ask which
event or argument depends on another event or ar-
gument, (b) Temporal questions ask about tempo-
ral ordering of events, and (c) True-false questions
ask whether some fact is true. Below, we describe
the two phases of query generation primarily in the
context of dependency questions with a brief dis-
cussion about temporal and true-false questions at
the end of the section.
Alignment Phase We align triggers in the struc-
ture to the question and the answer by matching
lemmas or nominalizations. In case of multiple
matches, we use the context to disambiguate and
resolve ties using the highest matching candidate
in the syntactic dependency tree.
We align arguments in the question and the an-
swer in a similar manner. Since arguments are
typically several words long, we prefer maximal
spans. Additionally, if a question (or an answer)
contains an aligned trigger, we prefer to align
words to its arguments.
Query Construction Phase We construct a
query using the aligned question and answer trig-
gers/arguments. We will explain query construc-
tion using our running example (reproduced as the
dependency question in Table 3).
First, we identify the source and the target of
the query. We select either the source or the tar-
get to be a question node and populate the other
end of the query path with an answer node. To
make the choice between source or target for the
question node, we use the main verb in the ques-
tion, its voice and relative position of the question
word with respect to the main verb. In our exam-
ple, the main verb lead to is in active voice and the
question word what is not in subject position. This
places the trigger from the question as the source
of the query path (see both queries in the bottom
right portion of the running example). In contrast,
had the verb been require, the trigger would be the
target of the query. We construct two verb clusters
that indicate query direction using a small seed set
1504
Type Example # (%)
Dependency Q: What can the splitting of water lead to? 407 (69.57%)
a: Light absorption
b: Transfer of ions
Temporal Q: What is the correct order of events? 57 (9.74%)
a: PDGF binds to tyrosine kinases, then cells divide, then wound healing
b: Cells divide, then PDGF binds to tyrosine kinases, then wound healing
True-False Q: Cdk associates with MPF to become cyclin 121 (20.68%)
a: True
b: False
Table 3: Examples and statistics for each of the three coarse types of questions.
Is main verb trigger?
Condition Regular Exp.
Wh- word subjective? AGENT
Wh- word object? THEME
Condition Regular Exp.
default (ENABLE|SUPER)
+
DIRECT (ENABLE|SUPER)
PREVENT (ENABLE|SUPER)
?
PREVENT(ENABLE|SUPER)
?
Yes No
Figure 3: Rules for determining the regular expressions for queries concerning two triggers. In each table, the condition
column decides the regular expression to be chosen. In the left table, we make the choice based on the path from the root to
the Wh- word in the question. In the right table, if the word directly modifies the main trigger, the DIRECT regular expression
is chosen. If the main verb in the question is in the synset of prevent, inhibit, stop or prohibit, we select the PREVENT regular
expression. Otherwise, the default one is chosen. We omit the relation label SAME from the expressions, but allow going
through any number of edges labeled by SAME when matching expressions to the structure.
that we expand using WordNet.
The final step in constructing the query is to
identify the regular expression for the path con-
necting the source and the target. Due to paucity
of data, we do not map a question and an answer
to arbitrary regular expressions. Instead, we con-
struct a small set of regular expressions, and build
a rule-based system that selects one. We used the
training set to construct the regular expressions
and we found that they answer most questions (see
Section 6.4). We determine the regular expression
based on whether the main verb in the sentence is
a trigger and whether the source and target of the
path are triggers or arguments. Figure 3 shows the
possible regular expressions and the procedure for
choosing one when both the source and target are
triggers. If either of them are argument nodes, we
append the appropriate semantic role to the regu-
lar expression, based on whether the argument is
the source or the target of the path (or both).
True-false questions are treated similarly, ex-
cept that both source and target are chosen from
the question. For temporal questions, we seek to
identify the ordering of events in the answers. We
use the keywords first, then, or simultaneously to
identify the implied order in the answer. We use
the regular expression SUPER
+
for questions ask-
ing about simultaneous events and ENABLE
+
for
those asking about sequential events.
5.3 Answering Questions
We match the query of an answer to the process
structure to identify the answer. In case of a match,
the corresponding answer is chosen. The matching
path can be thought of as a proof for the answer.
If neither query matches the graph (or both do),
we check if either answer contradicts the struc-
ture. To do so, we find an undirected path from
the source to the target. In the event of a match, if
the matching path traverses any ENABLE edge in
the incorrect direction, we treat this as a refutation
for the corresponding answer and select the other
one. In our running example, in addition to the
valid path for the second query, for the first query
we see that there is an undirected path from split
to absorb through transfer that matches the first
query. This tells us that light absorption cannot
be the answer because it is not along a causal path
from split.
Finally, if none of the queries results in a match,
we look for any unlabeled path between the source
and the target, before backing off to a dependency-
based proximity baseline described in Section 6.
When there are multiple aligning nodes in the
question and answer, we look for any proof or
refutation before backing off to the baselines.
1505
6 Empirical Evaluation
In this section we aim to empirically evaluate
whether we can improve reading comprehension
accuracy by predicting process structures. We first
provide details of the experimental setup.
6.1 Experimental setup
We used 150 processes (435 questions) for train-
ing and 50 processes (150 questions) as the test
set. For development, we randomly split the train-
ing set 10 times (80%/20%), and tuned hyper-
parameters by maximizing average accuracy on
question answering. We preprocessed the para-
graphs with the Stanford CoreNLP pipeline ver-
sion 3.4 (Manning et al., 2014) and Illinois SRL
(Punyakanok et al., 2008; Clarke et al., 2012). We
used the Gurobi optimization package
3
for infer-
ence.
We compare our system PROREAD to baselines
that do not have access to the process structure:
1. BOW: For each answer, we compute the
proportion of content word lemmas covered
by the paragraph and choose the one with
higher coverage. For true-false questions, we
compute the coverage of the question state-
ment, and answer ?True? if it is higher than a
threshold tuned on the development set.
2. TEXTPROX: For dependency questions, we
align content word lemmas in both the ques-
tion and answer against the text and select the
answer whose aligned tokens are closer to the
aligned tokens of the question. For tempo-
ral questions, we return the answer for which
the order of events is identical to their order
in the paragraph. For true-false questions, we
return ?True? if the number of bigrams from
the question covered in the text is higher than
a threshold tuned on the development set.
3. SYNTPROX: For dependency questions, we
use proximity as in TEXTPROX, except that
distance is measured using dependency tree
edges. To support multiple sentences we con-
nect roots of adjacent sentences with bidi-
rectional edges. For temporal questions this
baseline is identical to TEXTPROX. For true-
false questions, we compute the number of
dependency tree edges in the question state-
ment covered by edges in the paragraph (an
edge has a source lemma, relation, and target
lemma), and answer ?True? if the coverage is
3
http://www.gurobi.com/
Method Depen. Temp. True-
false
All
PROREAD 68.1 80.0 55.6 66.7
SYNTPROX 61.9 70.0 48.1 60.0
TEXTPROX 58.4 70.0 33.3 54.7
BOW 47.8 40.0 44.4 46.7
GOLD 77.9 80.0 70.4 76.7
Table 4: Reading comprehension test set accuracy. The All
column shows overall accuracy across all questions. The first
three columns show accuracy for each coarse type.
higher than a threshold tuned on the training
set.
To separate the contribution of process struc-
tures from the performance of our structure pre-
dictor, we also run our QA system given manually
annotated gold standard structures (GOLD).
4
6.2 Reading Comprehension Task
We evaluate our system using accuracy, i.e., the
proportion of questions answered correctly. Ta-
ble 4 presents test set results, where we break
down questions by their coarse-type.
PROREAD improves accuracy compared to the
best baseline by 6.7 absolute points (last column).
Most of the gain is due to improvement on de-
pendency questions, which are the most common
question type. The performance of BOW indicates
that lexical coverage alone does not distinguish the
correct answer from the wrong answer. In fact,
guessing the answer with higher lexical overlap
results in performance that is slightly lower than
random. Text proximity and syntactic proximity
provide a stronger cue, but exploiting predicted
process structures substantially outperforms these
baselines.
Examining results using gold information high-
lights the importance of process structures inde-
pendently of the structure predictor. Results of
GOLD demonstrate that given gold structures we
can obtain a dramatic improvement of almost 17
points compared to the baselines, using our sim-
ple deterministic QA system.
Results on true-false questions are low for
PROREAD and all the baselines. True-false ques-
tions are harder for two main reasons. First, in
dependency and temporal questions, we create a
query for both answers, and can find a proof or
a refutation for either one of them. In true-false
4
We also ran an experiment where gold triggers are
given and arguments and relations are predicted. We found
that this results in slightly higher performance compared to
PROREAD.
1506
Precision Recall F
1
Triggers 75.4 73.9 74.6
Arguments 43.4 34.4 38.3
Relations 27.0 22.5 24.6
Table 5: Structured prediction test set results.
questions we must determine given a single state-
ment whether it holds. Second, an analysis of true-
false questions reveals that they focus less on re-
lations between events and entities in the process,
and require modeling lexical variability.
5
6.3 Structure Prediction Task
Our evaluation demonstrates that gold structures
improve accuracy substantially more than pre-
dicted structures. To examine this, we now di-
rectly evaluate the structure predictor by com-
paring micro-average precision, recall and F
1
be-
tween predicted and gold structures (Table 5).
While performance for trigger identification is
reasonable, performance on argument and relation
prediction is low. This explains the higher perfor-
mance obtained in reading comprehension given
gold structures. Note that errors in trigger predic-
tion propagate to argument and relation prediction
? a relation cannot be predicted correctly if either
one of the related triggers is not previously identi-
fied. One reason for low performance is the small
size of the dataset. Thus, training process predic-
tors with less supervision is an important direction
for future work. Furthermore, the task of process
prediction is inherently difficult, because often re-
lations are expressed only indirectly in text. For
example, in Figure 1 the relation between water
splitting and transfer of ions is only recoverable
by understanding that water provides the ions that
need to be transferred.
Nevertheless, we find that questions can often
be answered correctly even if the structure con-
tains some errors. For example, the gold structure
for the sentence ?Some . . . radioisotopes have
long half-lives, allowing . . . ?, contains the trigger
long half-lives, while we predict have as a trigger
and long half-lives as an argument. This is good
enough to answer questions related to this part of
the structure correctly, and overall, to improve per-
formance using predicted structures.
5
The low performance of TEXTPROX and SYNTPROX on
true-false questions can also be attributed to the fact that we
tuned a threshold parameter on the training set, and this did
not generalize well to the test set.
Reason GOLD PROREAD
Alignment 35% 15%
Missing from annotation 25% 10%
Entity coreference 20% 10%
Missing regular expression 10%
Lexical variability 5% 10%
Error in predicted structure 55%
Other 5%
Table 6: Error analysis results. An explanation of the vari-
ous categories are in the body of the paper.
6.4 Error Analysis
This section presents the results of an analysis of
20 sampled errors of GOLD (gold structures), and
20 errors of PROREAD (predicted structures). We
have categorized the primary reason for error in
Table 6.
As expected, the main problem when using pre-
dicted structures, is structure errors which account
for more than half of the errors.
Errors in GOLD are distributed across various
categories, which we briefly describe. Alignment
errors occur due to multiple words aligning to mul-
tiple triggers and arguments. For example, in the
question ?What is the result of gases being pro-
duced in the lysosome??, the answer ?engulfed
pathogens are poisoned? is incorrectly aligned to
the trigger engulfed rather than to poisoned.
Another reason for errors are cases where ques-
tions are asked about parts of the paragraph that
are missing from annotation. This is possible since
questions were authored independently of struc-
ture annotation. Two other causes for errors are
entity coreference errors, where a referent for an
entity is missing from the structure, and lexical
variability, where the author of questions uses
names for triggers or arguments that are missing
from the paragraph, and so alignment fails.
Last, in 10% of the cases in GOLD we found
that the answer could not be retrieved using the set
of regular expressions that are currently used by
our QA system.
7 Discussion
This work touches on several strands of work in
NLP including information extraction, semantic
role labeling, semantic parsing and reading com-
prehension.
Event and relation extraction have been studied
via the ACE data (Doddington et al., 2004) and
related work. The BioNLP shared tasks (Kim et
al., 2009; Kim et al., 2011; Riedel and McCal-
1507
lum, 2011) focused on biomedical data to extract
events and their arguments. Event-event relations
have been mostly studied from the perspective of
temporal ordering; e.g., (Chambers and Jurafsky,
2008; Yoshikawa et al., 2009; Do et al., 2012; Mc-
Closky and Manning, 2012). The process struc-
ture predicted in this work differs from these lines
of work in two important ways: First, we predict
events, arguments and their interactions from mul-
tiple sentences, while most earlier work focused
on one or two of these components. Second, we
model processes, and thus target causal relations
between events, rather than temporal order only.
Our semantic role annotation is similar to ex-
isting SRL schemes such as PropBank (Palmer et
al., 2005), FrameNet (Ruppenhofer et al., 2006)
and BioProp (Chou et al., 2006). However, in con-
trast to PropBank and FrameNet, we do not allow
all verbs to trigger events and instead let the an-
notators decide on biologically important triggers,
which are not restricted to verbs (unlike BioProp,
where 30 pre-specified verbs were selected for an-
notation). Like PropBank and BioProp, the argu-
ment labels are not trigger specific.
Mapping questions to queries is effectively a se-
mantic parsing task. In recent years, several lines
of work addressed semantic parsing using vari-
ous formalisms and levels of supervision (Zettle-
moyer and Collins, 2005; Wong and Mooney,
2006; Clarke et al., 2010; Berant et al., 2013).
In particular, Krishnamurthy and Kollar (2013)
learned to map natural language utterances to ref-
erents in an image by constructing a KB from the
image and then mapping the utterance to a query
over the KB. This is analogous to our process of
constructing a process structure and performing
QA by querying that structure. In our work, we
parse questions into graph-based queries, suitable
for modeling processes, using a rule-based heuris-
tic. Training a statistical semantic parser that will
replace the QA system is an interesting direction
for future research.
Multiple choice reading comprehension tests
are a natural choice for evaluating machine read-
ing. Hirschman et al. (1999) presented a bag-of-
words approach to retrieving sentences for read-
ing comprehension. Richardson et al. (2013) re-
cently released the MCTest reading comprehen-
sion dataset that examines understanding of fic-
tional stories. Their work shares our goal of ad-
vancing micro-reading, but they do not focus on
process understanding.
Developing programs that perform deep reason-
ing over complex descriptions of processes is an
important step on the road to fulfilling the higher
goals of machine reading. In this paper, we present
an end-to-end system for reading comprehen-
sion of paragraphs which describe biological pro-
cesses. This is, to the best of our knowledge, the
first system to both predict a rich structured rep-
resentation that includes entities, events and their
relations, and utilize this structure for answering
reading comprehension questions. We also created
a new dataset, PROCESSBANK, which contains
200 paragraphs that are both fully-annotated with
process structure, as well as accompanied by ques-
tions. We empirically demonstrated that model-
ing biological processes can substantially improve
reading comprehension accuracy in this domain.
Acknowledgments
The authors would like to thank Luke
Amuchastegui for authoring the multiple-choice
questions, and also the anonymous reviewers for
their constructive feedback. We thank the Allen
Institute for Artificial Intelligence for assistance
in funding this work.
References
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of EMNLP.
Neil Campbell and Jane Reece. 2005. Biology. Ben-
jamin Cummings.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of AAAI.
Nathanael Chambers and Daniel Jurafsky. 2008.
Jointly combining implicit constraints improves
temporal ordering. In Proceedings of EMNLP.
Wen-Chi Chou, Richard Tzong-Han Tsai, Ying-Shan
Su, Wei Ku, Ting-Yi Sung, and Wen-Lian Hsu.
2006. A semi-automatic method for annotating a
biomedical proposition bank. In Proceedings of the
Workshop on Frontiers in Linguistically Annotated
Corpora 2006, July.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world?s response. In Proceedings of CoNLL.
James Clarke, Vivek Srikumar, Mark Sammons, and
Dan Roth. 2012. An NLP Curator (or: How I
1508
Learned to Stop Worrying and Love NLP Pipelines).
In Proceedings of LREC.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of ACL.
Quang Do, Wei Lu, and Dan Roth. 2012. Joint infer-
ence for event timeline construction. In Proceedings
of EMNLP-CoNLL.
George R. Doddington, Alexis Mitchell, Mark A. Przy-
bocki, Lance A. Ramshaw, Stephanie Strassel, and
Ralph M. Weischedel. 2004. The Automatic Con-
tent Extraction (ACE) Program-Tasks, Data, and
Evaluation. In Proceedings of LREC.
Oren Etzioni, Michele Banko, and Michael J. Cafarella.
2006. Machine reading. In Proceedings of AAAI.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-Driven Learning for Open Ques-
tion Answering. In Proceedings of ACL.
Christiane Fellbaum, editor. 1998. WordNet: An elec-
tronic lexical database. MIT Press.
Lynette Hirschman, Marc Light, Eric Breck, and
John D. Burger. 1999. Deep read: A reading com-
prehension system. In Proceedings of ACL.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Junichi Tsujii. 2009. Overview of
BioNLP 09 shared task on event extraction. In Pro-
ceedings of BioNLP.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Junichi Tsujii. 2011. Overview of
BioNLP shared task 2011. In Proceedings of
BioNLP.
Jayant Krishnamurthy and Thomas Kollar. 2013.
Jointly learning to parse and perceive: Connect-
ing natural language to the physical world. TACL,
1:193?206.
Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and
Regina Barzilay. 2014. Learning to automatically
solve algebra word problems. In Proceedings of
ACL.
Beth Levin. 1993. English verb classes and alter-
nations: A preliminary investigation. University of
Chicago Press.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. Nomlex: A
lexicon of nominalizations. In Proceedings of EU-
RALEX.
Thomas L. Magnanti and Laurence A. Wolsey. 1995.
Optimal trees. Handbooks in operations research
and management science, 7:503?615.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of ACL:
System Demonstrations.
Andr?e L. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proceedings of
ACL/IJCNLP.
David McClosky and Christopher D. Manning. 2012.
Learning constraints for consistent timeline extrac-
tion. In Proceedings of EMNLP-CoNLL.
Dan Moldovan, Christine Clark, Sanda Harabagiu, and
Steve Maiorano. 2003. Cogex: A logic prover
for question answering. In Proceedings of NAACL-
HLT.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2).
Matthew Richardson, Christopher JC Burges, and Erin
Renshaw. 2013. MCTest: A challenge dataset for
the open-domain machine comprehension of text. In
Proceedings of EMNLP.
Sebastian Riedel and Andrew McCallum. 2011. Fast
and robust joint models for biomedical event extrac-
tion. In Proceedings of EMNLP.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of CoNLL.
Josef Ruppenhofer, Michael Ellsworth, Miriam RL
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2006. FrameNet II: Extended theory and
practice. Berkeley FrameNet Release, 1.
Aju Thalappillil Scaria, Jonathan Berant, Mengqiu
Wang, Peter Clark, Justin Lewis, Brittany Harding,
and Christopher D. Manning. 2013. Learning bi-
ological processes with global constraints. In Pro-
ceedings of EMNLP.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. Brat: a web-based tool for NLP-assisted
text annotation. In Proceedings of the demonstra-
tions at EACL.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proceedings of HLT-NAACL.
1509
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki
Asahara, and Yuji Matsumoto. 2009. Jointly identi-
fying temporal relations with Markov logic. In Pro-
ceedings of ACL/IJCNLP.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of UAI.
1510
Learning Entailment Relations by Global
Graph Structure Optimization
Jonathan Berant?
Tel Aviv University
Ido Dagan??
Bar-Ilan University
Jacob Goldberger?
Bar-Ilan University
Identifying entailment relations between predicates is an important part of applied semantic
inference. In this article we propose a global inference algorithm that learns such entailment
rules. First, we define a graph structure over predicates that represents entailment relations as
directed edges. Then, we use a global transitivity constraint on the graph to learn the optimal set
of edges, formulating the optimization problem as an Integer Linear Program. The algorithm is
applied in a setting where, given a target concept, the algorithm learns on the fly all entailment
rules between predicates that co-occur with this concept. Results show that our global algorithm
improves performance over baseline algorithms by more than 10%.
1. Introduction
The Textual Entailment (TE) paradigm is a generic framework for applied semantic
inference. The objective of TE is to recognize whether a target textual meaning can
be inferred from another given text. For example, a question answering system has
to recognize that alcohol affects blood pressure is inferred from the text alcohol reduces
blood pressure to answer the question What affects blood pressure? In the TE framework,
entailment is defined as a directional relationship between pairs of text expressions,
denoted by T, the entailing text, and H, the entailed hypothesis. The text T is said to
entail the hypothesis H if, typically, a human reading T would infer that H is most likely
true (Dagan et al 2009).
TE systems require extensive knowledge of entailment patterns, often captured as
entailment rules?rules that specify a directional inference relation between two text
fragments (when the rule is bidirectional this is known as paraphrasing). A common
type of text fragment is a proposition, which is a simple natural language expression
that contains a predicate and arguments (such as alcohol affects blood pressure), where
the predicate denotes some semantic relation between the concepts that are expressed
? Tel-Aviv University, P.O. Box 39040, Tel-Aviv, 69978, Israel. E-mail: jonatha6@post.tau.ac.il.
?? Bar-Ilan University, Ramat-Gan, 52900, Israel. E-mail: dagan@cs.biu.ac.il.
? Bar-Ilan University, Ramat-Gan, 52900, Israel. E-mail: goldbej@eng.biu.ac.il.
Submission received: 28 September 2010; revised submission received: 5 May 2011; accepted for publication:
5 July 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 1
by the arguments. One important type of entailment rule specifies entailment between
propositional templates, that is, propositions where the arguments are possibly re-
placed by variables. A rule corresponding to the aforementioned example may be X
reduce blood pressure ? X affect blood pressure. Because facts and knowledge are mostly
expressed by propositions, such entailment rules are central to the TE task. This has
led to active research on broad-scale acquisition of entailment rules for predicates
(Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009;
Schoenmackers et al 2010).
Previous work has focused on learning each entailment rule in isolation. It is clear,
however, that there are interactions between rules. A prominent phenomenon is that
entailment is inherently a transitive relation, and thus the rules X ? Y and Y ? Z imply
the rule X ? Z.1 In this article we take advantage of these global interactions to improve
entailment rule learning.
After reviewing relevant background (Section 2), we describe a structure termed
an entailment graph that models entailment relations between propositional templates
(Section 3). Next, we motivate and discuss a specific type of entailment graph, termed a
focused entailment graph, where a target concept instantiates one of the arguments of
all propositional templates. For example, a focused entailment graph about the target
concept nausea might specify the entailment relations between propositional templates
like X induce nausea, X prevent nausea, and nausea is a symptom of X.
In the core section of the article, we present an algorithm that uses a global approach
to learn the entailment relations, which comprise the edges of focused entailment
graphs (Section 4). We define a global objective function and look for the graph that
maximizes that function given scores provided by a local entailment classifier and a
global transitivity constraint. The optimization problem is formulated as an Integer
Linear Program (ILP) and is solved with an ILP solver, which leads to an optimal
solution with respect to the global function. In Section 5 we demonstrate that this
algorithm outperforms by 12?13% methods that utilize only local information as well
as methods that employ a greedy optimization algorithm (Snow, Jurafsky, and Ng 2006)
rather than an ILP solver.
The article also includes a comprehensive investigation of the algorithm and its
components. First, we perform manual comparison between our algorithm and the
baselines and analyze the reasons for the improvement in performance (Sections 5.3.1
and 5.3.2). Then, we analyze the errors made by the algorithm against manually pre-
pared gold-standard graphs and compare them to the baselines (Section 5.4). Last, we
perform a series of experiments in which we investigate the local entailment classifier
and specifically experiment with various sets of features (Section 6). We conclude and
suggest future research directions in Section 7.
This article is based on previous work (Berant, Dagan, and Goldberger 2010), while
substantially expanding upon it. From a theoretical point of view, we reformulate the
two ILPs previously introduced by incorporating a prior. We show a theoretical relation
between the two ILPs and prove that the optimization problem tackled is NP-hard.
From an empirical point of view, we conduct many new experiments that examine
both the local entailment classifier as well as the global algorithm. Last, a rigorous
analysis of the algorithm is performed and an extensive survey of previous work is
provided.
1 Assuming that Y has the same sense in both X ? Y and Y ? Z, as we discuss later in Section 3.
74
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
2. Background
In this section we survey methods proposed in past literature for learning entailment
rules between predicates. First, we discuss local methods that assess entailment given a
pair of predicates, and then global methods that perform inference over a larger set of
predicates.
2.1 Local Learning
Three types of information have primarily been utilized in the past to learn entailment
rules between predicates: lexicographic methods, distributional similarity methods, and
pattern-based methods.
Lexicographic methods use manually prepared knowledge bases that contain in-
formation about semantic relations between lexical items. WordNet (Fellbaum 1998b),
by far the most widely used resource, specifies relations such as hyponymy, synonymy,
derivation, and entailment that can be used for semantic inference (Budanitsky and
Hirst 2006). For example, if WordNet specifies that reduce is a hyponym of affect, then
one can infer that X reduces Y ? X affects Y. WordNet has also been exploited to
automatically generate a training set for a hyponym classifier (Snow, Jurafsky, and Ng
2004), and we make a similar use of WordNet in Section 4.1.
A drawback of WordNet is that it specifies semantic relations for words and terms
but not for more complex expressions. For example, WordNet does not cover a complex
predicate such as X causes a reduction in Y. Another drawback of WordNet is that it only
supplies semantic relations between lexical items, but does not provide any information
on how to map arguments of predicates. For example, WordNet specifies that there is
an entailment relation between the predicates pay and buy, but does not describe the
way in which arguments are mapped: if X pays Y for Z then X buys Z from Y. Thus,
using WordNet directly to derive entailment rules between predicates is possible only
for semantic relations such as hyponymy and synonymy, where arguments typically
preserve their syntactic positions on both sides of the rule.
Some knowledge bases try to overcome this difficulty: Nomlex (Macleod et al
1998) is a dictionary that provides the mapping of arguments between verbs and their
nominalizations and has been utilized to derive predicative entailment rules (Meyers
et al 2004; Szpektor and Dagan 2009). FrameNet (Baker, Fillmore, and Lowe 1998) is
a lexicographic resource that is arranged around ?frames?: Each frame corresponds to
an event and includes information on the predicates and arguments relevant for that
specific event supplemented with annotated examples that specify argument positions.
Consequently, FrameNet was also used to derive entailment rules between predicates
(Coyne and Rambow 2009; Ben Aharon, Szpektor, and Dagan 2010). Additional man-
ually constructed resources for predicates include PropBank (Kingsbury, Palmer, and
Marcus 2002) and VerbNet (Kipper, Dang, and Palmer 2000).
Distributional similarity methods are used to learn broad-scale resources, because
lexicographic resources tend to have limited coverage. Distributional similarity algo-
rithms employ ?the distributional hypothesis? (Harris 1954) and predict a semantic
relation between two predicates by comparing the arguments with which they occur.
Quite a few methods have been suggested (Lin and Pantel 2001; Szpektor et al 2004;
Bhagat, Pantel, and Hovy 2007; Szpektor and Dagan 2008; Yates and Etzioni 2009;
Schoenmackers et al 2010), which differ in terms of the specifics of the ways in which
predicates are represented, the features that are extracted, and the function used to com-
pute feature vector similarity. Next, we elaborate on some of the prominent methods.
75
Computational Linguistics Volume 38, Number 1
Lin and Pantel (2001) proposed an algorithm that is based on a mutual information
criterion. A predicate is represented by a binary template, which is a dependency path
between two arguments of a predicate where the arguments are replaced by variables.
Note that in a dependency tree, a path between two arguments must pass through their
common predicate. Also note that if a predicate has more than two arguments, then it
is represented by more than one binary template, where each template corresponds to
a different aspect of the predicate. For example, the proposition I bought a gift for her
contains a predicate and three arguments, and therefore is represented by the following
three binary templates: X
subj
??? buys
obj
?? Y, X
obj
?? buys
prep
??? for
pcomp?n
?????? Y and X
subj
??? buys
prep
??? for
pcomp?n
?????? Y.
For each binary template Lin and Pantel compute two sets of features Fx and Fy,
which are the words that instantiate the arguments X and Y, respectively, in a large
corpus. Given a template t and its feature set for the X variable Ftx, every fx ? F
t
x is
weighted by the pointwise mutual information between the template and the feature:
wtx( fx) = log
Pr( fx|t)
Pr( fx )
, where the probabilities are computed using maximum likelihood
over the corpus. Given two templates u and v, the Lin measure (Lin 1998a) is computed
for the variable X in the following manner:
Linx(u, v) =
?
f?Fux?Fvx
[wux ( f ) + w
v
x( f )]
?
f?Fux
wux ( f ) +
?
f?Fvx
wvx( f )
(1)
The measure is computed analogously for the variable Y and the final distributional
similarity score, termed DIRT, is the geometric average of the scores for the two
variables:
DIRT(u, v) =
?
Linx(u, v) ? Liny(u, v) (2)
If DIRT(u, v) is high, this means that the templates u and v share many ?informative?
arguments and so it is possible that u ? v. Note, however, that the DIRT similarity
measure computes a symmetric score, which is appropriate for modeling synonymy
but not entailment, an inherently directional relation.
To remedy that, Szpektor and Dagan (2008) suggested a directional distributional
similarity measure. In their work, Szpektor and Dagan chose to represent predicates
with unary templates, which are identical to binary templates, only they contain a pred-
icate and a single argument, such as: X
subj
??? buys. Szpektor and Dagan explain that unary
templates are more expressive than binary templates, and that some predicates can only
be encoded using unary templates. They propose that if for two unary templates u ? v,
then relatively many of the features of u should be covered by the features of v. This
is captured by the asymmetric Cover measure suggested by Weeds and Weir (2003) (we
omit the subscript x from Fux and F
v
x because in their setting there is only one argument):
Cover(u, v) =
?
f?Fu?Fv w
u( f )
?
f?Fu w
u( f )
(3)
The final directional score, termed BInc (Balanced Inclusion), is the geometric average
of the Lin measure and the Cover measure:
BInc(u, v) =
?
Lin(u, v) ? Cover(u, v) (4)
76
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Both Lin and Pantel as well as Szpektor and Dagan compute a similarity score for
each argument separately, effectively decoupling the arguments from one another. It is
clear, however, that although this alleviates sparsity problems, it disregards an impor-
tant piece of information, namely, the co-occurrence of arguments. For example, if one
looks at the following propositions: coffee increases blood pressure, coffee decreases fatigue,
wine decreases blood pressure, wine increases fatigue, one can notice that the predicates occur
with similar arguments and might mistakenly infer that decrease ? increase. However,
looking at pairs of arguments reveals that the predicates do not share a single pair of
arguments.
Yates and Etzioni (2009) address this issue and propose a generative model that
estimates the probability that two predicates are synonymous (synonymy is simply
bidirectional entailment) by comparing pairs of arguments. They represent predicates
and arguments as strings and compute for every predicate a feature vector that counts
that number of times it occurs with any ordered pair of words as arguments. Their main
modeling decision is to assume that two predicates are synonymous if the number of
pairs of arguments they share is maximal. An earlier work by Szpektor et al (2004)
also tried to learn entailment rules between predicates by using pairs of arguments as
features. They utilized an algorithm that learns new rules by searching for distributional
similarity information on the Web for candidate predicates.
Pattern-based methods. Although distributional similarity measures excel at iden-
tifying the existence of semantic similarity between predicates, they are often unable
to discern the exact type of semantic similarity and specifically determine whether it is
entailment. Pattern-based methods are used to automatically extract pairs of predicates
for a specific semantic relation. Pattern-based methods identify a semantic relation
between two predicates by observing that they co-occur in specific patterns in sentences.
For example, from the single proposition He scared and even startled me one might infer
that startle is semantically stronger than scare and thus startle ? scare. Chklovski and
Pantel (2004) manually constructed a few dozen patterns and learned semantic relations
between predicates by looking for these patterns on the Web. For example, the pattern
X and even Y implies that Y is stronger than X, and the pattern to X and then Y indicates
that Y follows X. The main disadvantage of pattern-based methods is that they are based
on the co-occurrence of two predicates in a single sentence in a specific pattern. These
events are quite rare and require working on a very large corpus, or preferably, the Web.
Pattern-based methods were mainly utilized so far to extract semantic relations
between nouns, and there has been some work on automatically learning patterns for
nouns (Snow, Jurafsky, and Ng 2004). Although these methods can be expanded for
predicates, we are unaware of any attempt to automatically learn patterns that describe
semantic relations between predicates (as opposed to the manually constructed patterns
suggested by Chklovski and Pantel [2004]).
2.2 Global Learning
It is natural to describe entailment relations between predicates (or language expres-
sions in general) by a graph. Nodes represent predicates, and edges represent entail-
ment between nodes. Nevertheless, using a graph for global learning of all entailment
relations within a set of predicates, rather then between pairs of predicates, has attracted
little attention. Recently, Szpektor and Dagan (2009) presented the resource Argument-
mapped WordNet, providing entailment relations for predicates in WordNet. This re-
source was built on top of WordNet and augments it with mapping of arguments for
predicates using NomLex (Macleod et al 1998) and a corpus-based resource (Szpektor
77
Computational Linguistics Volume 38, Number 1
and Dagan 2008). Their resource makes simple use of WordNet?s global graph structure:
New rules are suggested by transitively chaining graph edges, and then verified using
distributional similarity measures. Effectively, this is equivalent to using the intersection
of the set of rules derived by this transitive chaining and the set of rules in a distribu-
tional similarity knowledge base.
The most similar work to ours is Snow, Jurafsky, and Ng?s (2006) algorithm for
taxonomy induction, although it involves learning the hyponymy relation between
nouns, which is a special case of entailment, rather than learning entailment between
predicates. We provide here a brief review of a simplified form of this algorithm.
Snow, Jurafsky, and Ng define a taxonomy T to be a set of pairs of words, expressing
the hyponymy relation between them. The notation Huv ? T means that the noun u is a
hyponym of the noun v in T. They define D to be the set of observed data over all pairs of
words, and define Duv ? D to be the observed evidence we have in the data for the event
Huv ? T. Snow, Jurafsky, and Ng assume a model exists for inferring P(Huv ? T|Duv):
the posterior probability of the event Huv ? T, given the data. Their goal is to find the
taxonomy that maximizes the likelihood of the data, that is, to find
T? = argmax
T
P(D|T) (5)
Using some independence assumptions and Bayes rule, the likelihood P(D|T) is
expressed:
P(D|T) =
?
Huv?T
P(Huv ? T|Duv)P(Duv)
P(Huv ? T)
?
?
Huv/?T
P(Huv /? T|Duv)P(Duv)
P(Huv /? T)
(6)
Crucially, they demand that the taxonomy learned respects the constraint that hy-
ponymy is a transitive relation. To ensure that, they propose the following greedy
algorithm: At each step they go over all pairs of words (u, v) that are not in the taxonomy,
and try to add the single hyponymy relation Huv. Then, they calculate the set of relations
Suv that Huv will add to the taxonomy due to the transitivity constraint (all of the
relations Huw, where w is a hypernym of v in the taxonomy). Last, they choose to
add that set of relations Suv that maximizes P(D|T) out of all the possible candidates.
This iterative process stops when P(D|T) starts dropping. Their implementation of the
algorithm uses a hyponym classifier presented in an earlier work (Snow, Jurafsky, and
Ng 2004) as a model for P(Huv ? T|Duv) and a single sparsity parameter k =
P(Huv/?T)
P(Huv?T)
. In
this article we tackle a similar problem of learning a transitive relation, but we use linear
programming (Vanderbei 2008) to solve the optimization problem.
2.3 Linear Programming
A Linear Program (LP) is an optimization problem where a linear objective function is
minimized (or maximized) under linear constraints.
min
x?Rd
cx (7)
such that Ax ? b
where c ? Rd is a coefficient vector, and A ? Rn ? Rd and b ? Rn specify the constraints.
In short, we wish to find the optimal assignment for the d variables in the vector x, such
78
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
that all n linear constraints specified by the matrix A and the vector b are satisfied by this
assignment. If the variables are forced to be integers, the problem is termed an Integer
Linear Program (ILP). ILP has attracted considerable attention recently in several
fields of NLP, such as semantic role labeling, summarization, and parsing (Althaus,
Karamanis, and Koller 2004; Roth and Yih 2004; Riedel and Clarke 2006; Clarke and
Lapata 2008; Finkel and Manning 2008; Martins, Smith, and Xing 2009). In this article we
formulate the entailment graph learning problem as an ILP, which leads to an optimal
solution with respect to the objective function (vs. a greedy optimization algorithm
suggested by Snow, Jurafsky, and Ng [2006]). Recently, Do and Roth (2010) used ILP
in a related task of learning taxonomic relations between nouns, utilizing constraints
between sibling nodes and ancestor?child nodes in small graphs of three nodes.
3. Entailment Graph
In this section we define a structure termed the entailment graph that describes the
entailment relations between propositional templates (Section 3.1), and a specific type
of entailment graph, termed the focused entailment graph, that concentrates on entail-
ment relations that are relevant for some pre-defined target concept (Section 3.2).
3.1 Entailment Graph: Definition and Properties
The nodes of an entailment graph are propositional templates. A propositional tem-
plate is a binary template2 where at least one of the two arguments is a variable whereas
the second may be instantiated. In addition, the sense of the predicate is specified (ac-
cording to some sense inventory, such as WordNet) and so each sense of a polysemous
predicate corresponds to a separate template (and a separate graph node). For example,
X
subj
??? treats#1
obj
?? Y and X
subj
??? treats#2
obj
?? nausea are propositional templates for the
first and second sense of the predicate treat, respectively. An edge (u, v) represents the
fact that template u entails template v. Note that the entailment relation transcends
hyponymy/troponomy. For example, the template X is diagnosed with asthma entails the
template X suffers from asthma, although one is not a hyponym of the other. An example
of an entailment graph is given in Figure 1.
Because entailment is a transitive relation, an entailment graph is transitive, that is,
if the edges (u, v) and (v, w) are in the graph, so is the edge (u, w). Note that the property
of transitivity does not hold when the senses of the predicates are not specified. For
example, X buys Y ? X acquires Y and X acquires Y ? X learns Y, but X buys Y X learns
Y. This violation occurs because the predicate acquire has two distinct senses in the two
templates, but this distinction is lost when senses are not specified.
Transitivity implies that in each strongly connected component3 of the graph all
nodes entail each other. For example, in Figure 1 the nodes X-related-to-nausea and X-
associated-with-nausea form a strongly connected component. Moreover, if we merge
every strongly connected component to a single node, the graph becomes a Directed
Acyclic Graph (DAG), and a hierarchy of predicates can be obtained.
2 We restrict our discussion to templates with two arguments, but generalization is straightforward.
3 A strongly connected component is a subset of nodes in the graph where there is a path from any
node to any other node.
79
Computational Linguistics Volume 38, Number 1
Figure 1
A focused entailment graph. For clarity, edges that can be inferred by transitivity are omitted.
The single strongly connected component is surrounded by a dashed line.
3.2 Focused Entailment Graphs
In this article we concentrate on learning a type of entailment graph, termed the focused
entailment graph. Given a target concept, such as nausea, a focused entailment graph
describes the entailment relations between propositional templates for which the target
concept is one of the arguments (see Figure 1). Learning such entailment rules in real
time for a target concept is useful in scenarios such as information retrieval and question
answering, where a user specifies a query about the target concept. The need for such
rules has been also motivated by Clark et al (2007), who investigated what types
of knowledge are needed to identify entailment in the context of the RTE challenge,
and found that often rules that are specific to a certain concept are required. Another
example for a semantic inference algorithm that is utilized in real time is provided by
Do and Roth (2010), who recently described a system that, given two terms, determines
the taxonomic relation between them on the fly. Last, we have recently suggested an
application that uses focused entailment graphs to present information about a target
concept according to a hierarchy of entailment (Berant, Dagan, and Goldberger 2010).
The benefit of learning focused entailment graphs is three-fold. First, the target
concept that instantiates the propositional template usually disambiguates the predicate
and hence the problem of predicate ambiguity is greatly reduced. Thus, we do not
employ any form of disambiguation in this article, but assume that every node in a
focused entailment graph has a single sense (we further discuss this assumption when
describing the experimental setting in Section 5.1), which allows us to utilize transitivity
constraints.
An additional (albeit rare) reason that might also cause violations of transitivity
constraints is the notion of probabilistic entailment. Whereas troponomy rules
(Fellbaum 1998a) such as X walks ? X moves can be perceived as being almost always
correct, rules such as X coughs ? X is sick might only be true with some probability.
Consequently, chaining a few probabilistic rules such as A ? B, B ? C, and C ? D
might not guarantee the correctness of A ? D. Because in focused entailment graphs
the number of nodes and diameter4 are quite small (for example, in the data set we
4 The distance between two nodes in a graph is the number of edges in a shortest path connecting them.
The diameter of a graph is the maximal distance between any two nodes in the graph.
80
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
present in Section 5 the maximal number of nodes is 26, the average number of nodes
is 22.04, the maximal diameter is 5, and the average diameter is 2.44), we do not find
this to be a problem in our experiments in practice.
Last, the optimization problem that we formulate is NP-hard (as we show in Sec-
tion 4.2). Because the number of nodes in focused entailment graphs is rather small, a
standard ILP solver is able to quickly reach the optimal solution.
To conclude, the algorithm we suggest next is applied in our experiments on
focused entailment graphs. However, we believe that it is suitable for any entailment
graph whose properties are similar to those of focused entailment graphs. For brevity,
from now on the term entailment graph will stand for focused entailment graph.
4. Learning Entailment Graph Edges
In this section we present an algorithm that, given the set of propositional templates
constituting the nodes of an entailment graph, learns its edges (i.e., the entailment
relations between all pairs of nodes). The algorithm comprises two steps (described in
Sections 4.1 and 4.2): In the first step we use a large corpus and a lexicographic resource
(WordNet) to train a generic entailment classifier that given any pair of propositional
templates estimates the likelihood that one template entails the other. This generic step
is performed only once, and is independent of the specific nodes of the target entailment
graph whose edges we want to learn. In the second step we learn on the fly the edges of
a specific target graph: Given the graph nodes, we use a global optimization approach
that determines the set of edges that maximizes the probability (or score) of the entire
graph. The global graph decision is determined by the given edge probabilities (or
scores) supplied by the entailment classifier and by the graph constraints (transitivity
and others).
4.1 Training an Entailment Classifier
We describe a procedure for learning a generic entailment classifier, which can be used
to estimate the entailment likelihood for any given pair of templates. The classifier
is constructed based on a corpus and a lexicographic resource (WordNet) using the
following four steps:
(1) Extract a large set of propositional templates from the corpus.
(2) Use WordNet to automatically generate a training set of pairs of
templates?both positive and negative examples.
(3) Represent each training set example with a feature vector of various
distributional similarity scores.
(4) Train a classifier over the training set.
(1) Template extraction. We parse the corpus with the Minipar dependency parser
(Lin 1998b) and use the Minipar representation to extract all binary templates from
every parse tree, employing the procedure described by Lin and Pantel (2001), which
considers all dependency paths between every pair of nouns in the parse tree. We
also apply over the extracted paths the syntactic normalization procedure described
by Szpektor and Dagan (2007), which includes transforming passive forms into active
forms and removal of conjunctions, appositions, and abbreviations. In addition, we use
81
Computational Linguistics Volume 38, Number 1
Table 1
Positive and negative examples for entailment in the training set. The direction of entailment is
from the left template to the right template.
Positive examples Negative examples
(X
subj
??? desires
obj
?? Y, X
subj
??? wants
obj
?? Y) (X
subj
??? pushes
obj
?? Y,X
subj
??? blows
obj
?? Y)
(X
subj
??? causes vrel?? Y, X
subj
??? creates vrel?? Y) (X
subj
??? issues vrel?? Y,X
subj
??? signs vrel?? Y)
a simple heuristic to filter out templates that probably do not include a predicate: We
omit ?uni-directional? templates where the root of template has a single child, such as
therapy
prep
???in
p?comp
?????patient nn??cancer, unless one of the edges is labeled with a passive
relation, such as in the template nausea
vrel???characterized
subj
???poisoning, which contains
the Minipar passive label vrel.5 Last, the arguments are replaced by variables, resulting
in propositional templates such as X
subj
??? affect
obj
?? Y. The lexical items that remain in
the template after replacing the arguments by variables are termed predicate words.
(2) Training set generation. WordNet is used to automatically generate a training
set of positive (entailing) and negative (non-entailing) template pairs. Let T be the set
of propositional templates extracted from the corpus. For each ti ? T with two variables
and a single predicate word w, we extract from WordNet the set H of direct hypernyms
(distance of one in WordNet) and synonyms of w. For every h ? H, we generate a new
template tj from ti by replacing w with h. If tj ? T, we consider (ti, tj) to be a positive
example. Negative examples are generated analogously, only considering direct co-
hyponyms of w, which are direct hyponyms of direct hypernyms of w that are not
synonymous to w. It has been shown in past work that in most cases co-hyponym terms
do not entail one another (Mirkin, Dagan, and Gefet 2006). A few examples for positive
and negative training examples are given in Table 1.
This generation method is similar to the ?distant supervision? method proposed by
Snow, Jurafsky, and Ng (2004) for training a noun hypernym classifier. It differs in some
important aspects, however: First, Snow, Jurafsky, and Ng consider a positive example
to be any Wordnet hypernym, irrespective of the distance, whereas we look only at
direct hypernyms. This is because predicates are mainly verbs and precision drops
quickly when looking at verb hypernyms in WordNet at a longer distance. Second,
Snow, Jurafsky, and Ng generate negative examples by looking at any two nouns where
one is not the hypernym of the other. In the spirit of ?contrastive estimation? (Smith and
Eisner 2005), we prefer to generate negative examples that are ?hard,? that is, negative
examples that, although not entailing, are still semantically similar to positive examples
and thus focus the classifier?s attention on determining the boundary of the entailment
class. Last, we use a balanced number of positive and negative examples, because
classifiers tend to perform poorly on the minority class when trained on imbalanced
data (Van Hulse, Khoshgoftaar, and Napolitano 2007; Nikulin 2008).
(3) Distributional similarity representation. We aim to train a classifier that for an
input template pair (t1, t2) determines whether t1 entails t2. Our approach is to represent
a template pair by a feature vector where each coordinate is a different distributional
similarity score for the pair of templates. The different distributional similarity scores
5 This passive construction is not handled by the normalization scheme employed by Szpektor and Dagan
(2007).
82
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
are obtained by utilizing various distributional similarity algorithms that differ in one
or more of their characteristics. In this way we hope to combine the various methods
proposed in the past for measuring distributional similarity. The distributional similar-
ity algorithms we employ vary in one or more of the following dimensions: the way the
predicate is represented, the way the features are represented, and the function used to
measure similarity between the feature representations of the two templates.
Predicate representation. As mentioned, we represent predicates over dependency
tree structures. However, some distributional similarity algorithms measure similarity
between binary templates directly (Lin and Pantel 2001; Szpektor et al 2004; Bhagat,
Pantel, and Hovy 2007; Yates and Etzioni 2009), whereas others decompose binary
templates into two unary templates, estimate similarity between two pairs of unary
templates, and combine the two scores into a single score (Szpektor and Dagan 2008).
Feature representation. The features of a template are some function of the terms that
instantiated the argument variables in a corpus. Two representations that are used in
our experiments are derived from an ontology that maps natural language phrases to
semantic identifiers (see Section 5). Another variant occurs when using binary tem-
plates: a template may be represented by a pair of feature vectors, one for each variable
as in the DIRT algorithm (Lin and Pantel 2001), or by a single vector, where features
represent pairs of instantiations (Szpektor et al 2004; Yates and Etzioni 2009). The
former variant reduces sparsity problems, whereas Yates and Etzioni showed the latter
is more informative and performs favorably on their data.
Similarity function. We consider two similarity functions: The symmetric Lin (Lin
and Pantel 2001) similarity measure, and the directional BInc (Szpektor and Dagan
2008) similarity measure, reviewed in Section 2. Thus, information about the direction
of entailment is provided by the BInc measure.
We compute for any pair of templates (t1, t2) 12 distributional similarity scores using
all possible combinations of the aforementioned dimensions. These scores are then used
as 12 features representing the pair (t1, t2). (A full description of the features is given in
Section 5.) This is reminiscent of Connor and Roth (2007), who used the output of unsu-
pervised classifiers as features for a supervised classifier in a verb disambiguation task.
(4) Training a classifier Two types of classifiers may be trained in our scheme over
the training set: margin classifiers (such as SVM) and probabilistic classifiers. Given a
pair of templates (u, v) and their feature vector Fuv, we denote by an indicator variable
Iuv the event that u entails v. A margin classifier estimates a score Suv for the event
Iuv = 1, which indicates the positive or negative distance of the feature vector Fuv from
the separating hyperplane. A probabilistic classifier provides the posterior probability
Puv = P(Iuv = 1|Fuv).
4.2 Global Learning of Edges
In this step we get a set of propositional templates as input, and we would like to learn
all of the entailment relations between these propositional templates. For every pair of
templates we can compute the distributional similarity features and get a score from
the trained entailment classifier. Once all the scores are calculated we try to find the
optimal graph?that is, the best set of edges over the propositional templates. Thus, in
this scenario the input is the nodes of the graph and the output are the edges.
To learn edges we consider global constraints, which allow only certain graph
topologies. Because we seek a global solution under transitivity and other constraints,
ILP is a natural choice, enabling the use of state-of-the-art ILP optimization packages.
Given a set of nodes V and a weighting function f : V ? V ? R (derived from the
83
Computational Linguistics Volume 38, Number 1
entailment classifier in our case), we want to learn the directed graph G = (V, E), where
E = {(u, v)| Iuv = 1}, by solving the following ILP over the variables Iuv:
G? = argmax
G
?
u=v
f (u, v) ? Iuv (8)
s.t. ?u,v,w?V Iuv + Ivw ? Iuw ? 1 (9)
?u,v?Ayes Iuv = 1 (10)
?u,v?Ano Iuv = 0 (11)
?u=v Iuv ? {0, 1} (12)
The objective function in Equation (8) is simply a sum over the weights of the graph
edges. The global constraint is given in Equation (9) and states that the graph must
respect transitivity. This constraint is equivalent to the one suggested by Finkel and
Manning (2008) in a coreference resolution task, except that the edges of our graph
are directed. The constraints in Equations (10) and (11) state that for a few node pairs,
defined by the sets Ayes and Ano, respectively, we have prior knowledge that one node
does or does not entail the other node. Note that if (u, v) ? Ano, then due to transitivity
there must be no path in the graph from u to v, which rules out additional edge combi-
nations. We elaborate on how the sets Ayes and Ano are computed in our experiments in
Section 5. Altogether, this Integer Linear Program contains O(|V|2) variables and O(|V|3)
constraints, and can be solved using state-of-the-art optimization packages.
A theoretical aspect of this optimization problem is that it is NP-hard. We can phrase
it as a decision problem in the following manner: Given V, f , and a threshold k, we
wish to know if there is a set of edges E that respects transitivity and
?
(u,v)?E
f (u, v) ? k.
Yannakakis (1978) has shown that the simpler problem of finding in a graph G? =
(V?, E?) a subset of edges A ? E? that respects transitivity and |A| ? k is NP-hard. Thus,
we can conclude that our optimization problem is also NP-hard by the trivial poly-
nomial reduction defining the function f that assigns the score 0 for node pairs (u, v) /? E?
and the score 1 for node pairs (u, v) ? E?. Because the decision problem is NP-hard, it is
clear that the corresponding maximization problem is also NP-hard. Thus, obtaining a
solution using ILP is quite reasonable and in our experiments also proves to be efficient
(Section 5).
Next, we describe two ways of obtaining the weighting function f , depending on
the type of entailment classifier we prefer to train.
4.2.1 Score-Based Weighting Function. In this case, we assume that we choose to train a
margin entailment classifier estimating the score Suv (a positive score if the classifier
predicts entailment, and a negative score otherwise) and define f score(u, v) = Suv ? ?.
This gives rise to the following objective function:
G?score = argmax
G
?
u=v
(Suv ? ?) ? Iuv = argmax
G
?
?
?
u=v
Suv ? Iuv
?
?? ? ? |E| (13)
The term ? ? |E| is a regularization term reflecting the fact that edges are sparse. Intu-
itively, this means that we would like to insert into the graph only edges with a score
84
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Suv > ?, or in other words to ?push? the separating hyperplane towards the positive
half space by ?. Note that the constant ? is a parameter that needs to be estimated and
we discuss ways of estimating it in Section 5.2.
4.2.2 Probabilistic Weighting Function. In this case, we assume that we choose to train
a probabilistic entailment classifier. Recall that Iuv is an indicator variable denoting
whether u entails v, that Fuv is the feature vector for the pair of templates u and v, and de-
fine F to be the set of feature vectors for all pairs of templates in the graph. The classifier
estimates the posterior probability of an edge given its features: Puv = P(Iuv = 1|Fuv),
and we would like to look for the graph G that maximizes the posterior probability
P(G|F). In Appendix A we specify some simplifying independence assumptions under
which this graph maximizes the following linear objective function:
G?prob = argmax
G
?
u=v
(log
Puv
1 ? Puv
+ log?) ? Iuv = argmax
G
?
u=v
log
Puv
1 ? Puv
? Iuv + log? ? |E|
(14)
where ? = P(Iuv=1)P(Iuv=0) is the prior odds ratio for an edge in the graph, which needs to be
estimated in some manner. Thus, the weighting function is defined by fprob(u, v) =
log Puv1?Puv + log?.
Both the score-based and the probabilistic objective functions obtained are quite
similar: Both contain a weighted sum over the edges and a regularization component
reflecting the sparsity of the graph. Next, we show that we can provide a probabilistic
interpretation for our score-based function (under certain conditions), which will allow
us to use a margin classifier and interpret its output probabilistically.
4.2.3 Probabilistic Interpretation of Score-Based Weighting Function. We would like to use
the score Suv, which is bounded in (?,??), and derive from it a probability Puv. To
that end we project Suv onto (0, 1) using the sigmoid function, and define Puv in the
following manner:
Puv =
1
1 + exp(?Suv)
(15)
Note that under this definition the log probability ratio is equal to the inverse of the
sigmoid function:
log
Puv
1 ? Puv
= log
1
1+exp(?Suv )
exp(?Suv )
1+exp(?Suv )
= log 1
exp(?Suv)
= Suv (16)
Therefore, when we derive Puv from Suv with the sigmoid function, we can rewrite
G?prob as:
G?prob = argmax
G
?
u=v
Suv ? Iuv + log? ? |E| = G?score (17)
where we see that in this scenario the two objective functions are identical and the
regularization term ? is related to the edge prior odds ratio by: ? = ? log?.
85
Computational Linguistics Volume 38, Number 1
Moreover, assume that the score Suv is computed as a linear combination over n
features (such as a linear-kernel SVM), that is, Suv =
?n
i=1 S
i
uv ? ?i, where S
i
uv denotes
feature values and ?i denotes feature weights. In this case, the projected probability
acquires the standard form of a logistic classifier:
Puv =
1
1 + exp(?
n
?
i=1
Siuv ? ?i)
(18)
Hence, we can train the weights ?i using a margin classifier and interpret the output
of the classifier probabilistically, as we do with a logistic classifier. In our experiments
in Section 5 we indeed use a linear-kernel SVM to train the weights ?i and then we
can interchangeably interpret the resulting ILP as either score-based or probabilistic
optimization.
4.2.4 Comparison to Snow, Jurafsky, and Ng (2006). Our work resembles Snow, Jurafsky,
and Ng?s work in that both try to learn graph edges given a transitivity constraint. There
are two key differences in the model and in the optimization algorithm, however. First,
they employ a greedy optimization algorithm that incrementally adds hyponyms to a
large taxonomy (WordNet), whereas we simultaneously learn all edges using a global
optimization method, which is more sound and powerful theoretically, and leads to
the optimal solution. Second, Snow, Jurafsky, and Ng?s model attempts to determine
the graph that maximizes the likelihood P(F|G) and not the posterior P(G|F). If we cast
their objective function as an ILP we get a formulation that is almost identical to ours,
only containing the inverse prior odds ratio log 1? = ? log? rather than the prior odds
ratio as the regularization term (cf. Section 2):
G?Snow = argmax
G
?
u=v
log
Puv
(1 ? Puv)
? Iuv ? log? ? |E| (19)
This difference is insignificant when ? ? 1, or when ? is tuned empirically for optimal
performance on a development set. If, however, ? is statistically estimated, this might
cause unwarranted results: Their model will favor dense graphs when the prior odds
ratio is low (? < 1 or P(Iuv = 1) < 0.5), and sparse graphs when the prior odds ratio is
high (? > 1 or P(Iuv = 1) > 0.5), which is counterintuitive. Our model does not suffer
from this shortcoming because it optimizes the posterior rather than the likelihood. In
Section 5 we show that our algorithm significantly outperforms the algorithm presented
by Snow, Jurafsky, and Ng.
5. Experimental Evaluation
This section presents an evaluation and analysis of our algorithm.
5.1 Experimental Setting
A health-care corpus of 632MB was harvested from the Web and parsed using the Mini-
par parser (Lin 1998b). The corpus contains 2,307,585 sentences and almost 50 million
86
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Table 2
The similarity score features used to represent pairs of templates. The columns specify the
corpus over which the similarity score was computed, the template representation, the
similarity measure employed, and the feature representation (as described in Section 4.1).
# Corpus Template Similarity measure Feature representation
1 health-care binary BInc pair of CUI tuples
2 health-care binary BInc pair of CUIs
3 health-care binary BInc CUI tuple
4 health-care binary BInc CUI
5 health-care binary Lin pair of CUI tuples
6 health-care binary Lin pair of CUIs
7 health-care binary Lin CUI tuple
8 health-care binary Lin CUI
9 health-care unary BInc CUI tuple
10 health-care unary BInc CUI
11 health-care unary Lin CUI tuple
12 health-care unary Lin CUI
13 RCV1 binary Lin lexical items
14 RCV1 unary Lin lexical items
15 RCV1 unary BInc lexical items
16 Lin & Pantel binary Lin lexical items
word tokens. We used the Unified Medical Language System (UMLS)6 to annotate
medical concepts in the corpus. The UMLS is a database that maps natural language
phrases to over one million concept identifiers in the health-care domain (termed
CUIs). We annotated all nouns and noun phrases that are in the UMLS with their
(possibly multiple) CUIs. We now provide the details of training an entailment classifier
as explained in Section 4.1.
We extracted all templates from the corpus where both argument instantiations are
medical concepts, that is, annotated with a CUI (?50,000 templates). This was done to
increase the likelihood that the extracted templates are related to the health-care domain
and reduce problems of ambiguity.
As explained in Section 4.1, a pair of templates constitutes an input example for
the entailment classifier, and should be represented by a set of features. The features
we used were different distributional similarity scores for the pair of templates, as
summarized in Table 2. Twelve distributional similarity measures were computed over
the health-care corpus using the aforementioned variations (Section 4.1), where two
feature representations were considered: in the UMLS each natural language phrase
may be mapped not to a single CUI, but to a tuple of CUIs. Therefore, in the first
representation, each feature vector coordinate counts the number of times a tuple of
CUIs was mapped to the term instantiating the template argument, and in the second
representation it counts the number of times each single CUI was one of the CUIs
mapped to the term instantiating the template argument. In addition, we obtained the
original template similarity lists learned by Lin and Pantel (2001), and had available
three distributional similarity measures learned by Szpektor and Dagan (2008), over the
RCV1 corpus,7 as detailed in Table 2. Thus, each pair of templates is represented by a
total of 16 distributional similarity scores.
6 http://www.nlm.nih.gov/research/umls.
7 http://trec.nist.gov/data/reuters/reuters.html.
87
Computational Linguistics Volume 38, Number 1
We automatically generated a balanced training set of 20,144 examples using Word-
Net and the procedure described in Section 4.1, and trained the entailment classifier
with SVMperf (Joachims 2005). We use the trained classifier to obtain estimates for Puv
and Suv, given that the score-based and probabilistic scoring functions are equivalent
(cf. Section 4.2.3).
To evaluate the performance of our algorithm, we manually constructed gold-
standard entailment graphs. First, 23 medical target concepts, representing typical top-
ics of interest in the medical domain, were manually selected from a (longer) list of
the most frequent concepts in the health-care corpus. The 23 target concepts are: alcohol,
asthma, biopsy, brain, cancer, CDC, chemotherapy, chest, cough, diarrhea, FDA, headache, HIV,
HPV, lungs, mouth, muscle, nausea, OSHA, salmonella, seizure, smoking, and x-ray. For each
concept, we wish to learn a focused entailment graph (cf. Figure 1). Thus, the nodes of
each graph were defined by extracting all propositional templates in which the corre-
sponding target concept instantiated an argument at least K(= 3) times in the health-
care corpus (average number of graph nodes = 22.04, std = 3.66, max = 26, min = 13).
Ten medical students were given the nodes of each graph (propositional templates)
and constructed the gold standard of graph edges using a Web interface. We gave an
oral explanation of the annotation process to each student, and the first two graphs
annotated by every student were considered part of the annotator training phase and
were discarded. The annotators were able to select every propositional template and
observe all of the instantiations of that template in our health-care corpus. For example,
selecting the template X helps with nausea might show the propositions relaxation helps
with nausea, acupuncture helps with nausea, and Nabilone helps with nausea. The concept
of entailment was explained under the framework of TE (Dagan et al 2009), that is, the
template t1 entails the template t2 if given that the instantiation of t1 with some concept
is true then the instantiation of t2 with the same concept is most likely true.
As explained in Section 3.2, we did not perform any disambiguation because a
target concept disambiguates the propositional templates in focused entailment graphs.
In practice, cases of ambiguity were very rare, except for a single scenario where in
templates such as X treats asthma, annotators were unclear whether X is a type of doctor
or a type of drug. The annotators were instructed in such cases to select the template,
read the instantiations of the template in the corpus, and choose the sense that is most
prevalent in the corpus. This instruction was applicable to all cases of ambiguity.
Each concept graph was annotated by two students. Following the current recog-
nizing TE (RTE) practice (Bentivogli et al 2009), after initial annotation the two students
met for a reconciliation phase. They worked to reach an agreement on differences and
corrected their graphs. Inter-annotator agreement was calculated using the kappa statis-
tic (Siegel and Castellan 1988) both before (? = 0.59) and after (? = 0.9) reconciliation.
Each learned graph was evaluated against the two reconciliated graphs.
Summing the number of possible edges over all 23 concept graphs we get 10,364
possible edges, of which 882 on average were included by the annotators (averaging
over the two gold-standard annotations for each graph). The concept graphs were
randomly split into a development set (11 concepts) and a test set (12 concepts).
We used the lpsolve8 package to learn the edges of the graphs. This package ef-
ficiently solves the model without imposing integer restrictions9 and then uses the
branch-and-bound method to find an optimal integer solution. We note that in the
8 http://lpsolve.sourceforge.net/5.5/.
9 While ILP is an NP-hard problem, LP is a polynomial problem and can be solved efficiently.
88
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
experiments reported in this article the optimal solution without integer restrictions
was already integer. Thus, although in general our optimization problem is NP-hard,
in our experiments we were able to reach an optimal solution for the input graphs
very efficiently (we note that in some scenarios not reported in this article the optimal
solution was not integer and so an integer solution is not guaranteed a priori).
As mentioned in Section 4.2, we added a few constraints in cases where there was
strong evidence that edges are not in the graph. This is done in the following scenarios
(examples given in Table 3): (1) When two templates u and v are identical except for
a pair of words wu and wv, and wu is an antonym of wv, or a hypernym of wv at
distance ? 2 in WordNet. (2) When two nodes u and v are transitive ?opposites,? that
is, if u = X
subj
??? w
obj
?? Y and v = X
obj
?? w
subj
??? Y, for any word w. We note that there are
some transitive verbs that express a reciprocal activity, such as X marries Y, but usually
reciprocal events are not expressed using a transitive verb structure.
In addition, in some cases we have strong evidence that edges do exist in the graph.
This is done in a single scenario (see Table 3), which is specific to the output of Minipar:
when two templates differ by a single edge and the first is of the type X
obj
?? Y and
the other is of the type X
vrel??? Y, which expresses a passive verb modifier of nouns.
Altogether, these initializations took place in less than 1% of the node pairs in the
graphs. We note that we tried to use WordNet relations such as hypernym and synonym
as ?positive? hard constraints (using the constraint Iuv = 1), but this resulted in reduced
performance because the precision of WordNet was not high enough.
The graphs learned by our algorithm were evaluated by two measures. The first
measure evaluates the graph edges directly, and the second measure is motivated by
semantic inference applications that utilize the rules in the graph. The first measure is
simply the F1 of the set of learned edges compared to the set of gold-standard edges.
In the second measure we take the set of learned rules and infer new propositions by
applying the rules over all propositions extracted from the health-care corpus. We apply
the rules iteratively over all propositions until no new propositions are inferred. For
example, given the corpus proposition relaxation reduces nausea and the edges X reduces
nausea ? X helps with nausea and X helps with nausea ? X related to nausea, we eval-
uate the set {relaxation reduces nausea, relaxation helps with nausea, relaxation related to
nausea}. For each graph we measure the F1 of the set of propositions inferred by the
learned graphs when compared to the set of propositions inferred by the gold-standard
graphs. For both measures the final score of an algorithm is a macro-average F1 over
the 24 gold-standard test-set graphs (two gold-standard graphs for each of the 12 test
concepts).
Table 3
Scenarios in which we added hard constraints to the ILP.
Scenario Example Initialization
antonym (X
subj
??? decrease
obj
?? Y,X
subj
??? increase
obj
?? Y) Iuv = 0
hypernym ? 2 (X
subj
??? affect
obj
?? Y,X
subj
??? irritate
obj
?? Y) Iuv = 0
transitive opposite (X
subj
??? cause
obj
?? Y,Y
subj
??? cause
obj
?? X) Iuv = 0
syntactic variation (X
subj
??? follow
obj
?? Y,Y
subj
??? follow vrel?? X) Iuv = 1
89
Computational Linguistics Volume 38, Number 1
Learning the edges of a graph given an input concept takes about 1?2 seconds on a
standard desktop.
5.2 Evaluated Algorithms
First, we describe some baselines that do not utilize the entailment classifier or the
ILP solver. For each of the 16 distributional similarity measures (Table 2) and for each
template t, we computed a list of templates most similar to t (or entailing t for directional
measures). Then, for each measure we learned graphs by inserting an edge (u, v), when
u is in the top K templates most similar to v. The parameter K can be optimized either on
the automatically generated training set (from WordNet) or on the manually annotated
development set. We also learned graphs using WordNet: We inserted an edge (u, v)
when u and v differ by a single word wu and wv, respectively, and wu is a direct hyponym
or synonym of wv. Next, we describe algorithms that utilize the entailment classifier.
Our algorithm, named ILP-Global, utilizes global information and an ILP formula-
tion to find maximum a posteriori graphs. Therefore, we compare it to the following
three variants: (1) ILP-Local: An algorithm that uses only local information. This is
done by omitting the global transitivity constraints, and results in an algorithm that
inserts an edge (u, v) if and only if (Suv ? ?) > 0. (2) Greedy-Global: An algorithm that
looks for the maximum a posteriori graphs but only employs the greedy optimization
procedure as described by Snow, Jurafsky, and Ng (2006). (3) ILP-Global-Likelihood:
An ILP formulation where we look for the maximum likelihood graphs, as described by
Snow, Jurafsky, and Ng (cf. Section 4.2).
We evaluate these algorithms in three settings which differ in the method by which
the edge prior odds ratio, ? (or ?), is estimated: (1) ? = 1 (? = 0), which means that
no prior is used. (2) Tuning ? and using the value that maximizes performance over the
development set. (3) Estimating ? using maximum likelihood over the development set,
which results in ? ? 0.1 (? ? 2.3), corresponding to the edge density P(Iuv = 1) ? 0.09.
For all local algorithms whose output does not respect transitivity constraints, we
added all edges inferred by transitivity. This was done because we assume that the rules
learned are to be used in the context of an inference or entailment system. Because such
systems usually perform chaining of entailment rules (Raina, Ng, and Manning 2005;
Bar-Haim et al 2007; Harmeling 2009), we conduct this chaining as well. Nevertheless,
we also measured performance when edges inferred by transitivity are not added: We
once again chose the edge prior value that maximizes F1 over the development set
and obtained macro-average recall/precision/F1 of 51.5/34.9/38.3. This performance is
comparable to the macro-average recall/precision/F1 of 44.5/45.3/38.1 we report next
in Table 4.
5.3 Experimental Results and Analysis
In this section we present experimental results and analysis that show that the
ILP-Global algorithm improves performance over baselines, specifically in terms of
precision.
Tables 4?7 and Figure 2 summarize the performance of the algorithms. Table 4
shows our main result when the parameters ? and K are optimized to maximize per-
formance over the development set. Notice that the algorithm ILP-Global-Likelihood
is omitted, because when optimizing ? over the development set it conflates with
ILP-Global. The rows Local1 and Local2 present the best algorithms that use a single
distributional similarity resource. Local1 and Local2 correspond to the configurations
90
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Table 4
Results when tuning for performance over the development set.
Edges Propositions
Recall Precision F1 Recall Precision F1
ILP-Global (? = 0.45) 46.0 50.1 43.8 67.3 69.6 66.2
Greedy-Global (? = 0.3) 45.7 37.1 36.6 64.2 57.2 56.3
ILP-Local (? = 1.5) 44.5 45.3 38.1 65.2 61.0 58.6
Local1 (K = 10) 53.5 34.9 37.5 73.5 50.6 56.1
Local2 (K = 55) 52.5 31.6 37.7 69.8 50.0 57.1
Table 5
Results when the development set is not used to estimate ? and K.
Edges Propositions
Recall Precision F1 Recall Precision F1
ILP-Global 58.0 28.5 35.9 76.0 46.0 54.6
Greedy-Global 60.8 25.6 33.5 77.8 41.3 50.9
ILP-Local 69.3 19.7 26.8 82.7 33.3 42.6
Local1 (K = 100) 92.6 11.3 20.0 95.3 18.9 31.1
Local2 (K = 100) 63.1 25.5 34.0 77.7 39.9 50.9
WordNet 10.8 44.1 13.2 39.9 72.4 47.3
Table 6
Results with prior estimated on the development set, that is ? = 0.1, which is equivalent to
? = 2.3.
Edges Propositions
Recall Precision F1 Recall Precision F1
ILP-Global 16.8 67.1 24.4 43.9 86.8 56.3
ILP-Global-Likelihood 91.8 9.8 17.5 94.0 16.7 28.0
Greedy-Global 14.7 62.9 21.2 43.5 86.6 56.2
Greedy-Global-Likelihood 100.0 9.3 16.8 100.0 15.5 26.5
described in Table 2 by features no. 5 and no. 1, respectively (see also Table 8). ILP-
Global improves performance by at least 13%, and significantly outperforms all local
methods, as well as the greedy optimization algorithm both on the edges F1 measure
(p < 0.05) and on the propositions F1 measure (p < 0.01).
10
Table 5 describes the results when the development set is not used to estimate the
parameters ? and K: A uniform prior (Puv = 0.5) is assumed for algorithms that use
the entailment classifier, and the automatically generated training set is employed to
estimate K. Again ILP-Global-Likelihood is omitted in the absence of a prior. ILP-Global
outperforms all other methods in this scenario as well, although by a smaller margin
for a few of the baselines. Comparing Table 4 to Table 5 reveals that excluding the
10 We tested significance using the two-sided Wilcoxon rank test (Wilcoxon 1945).
91
Computational Linguistics Volume 38, Number 1
Table 7
Results per concept for the ILP-Global.
Concept R P F1
Smoking 58.1 81.8 67.9
Seizure 64.7 51.2 57.1
Headache 60.9 50.0 54.9
Lungs 50.0 56.5 53.1
Diarrhea 42.1 60.0 49.5
Chemotherapy 44.7 52.5 48.3
HPV 35.2 76.0 48.1
Salmonella 27.3 80.0 40.7
X-ray 75.0 23.1 35.3
Asthma 23.1 30.6 26.3
Mouth 17.7 35.5 23.7
FDA 53.3 15.1 23.5
sparse prior indeed increases recall at a price of a sharp decrease in precision. Note,
however, that local algorithms are more vulnerable to this phenomenon. This makes
sense because in local algorithms eliminating the prior adds edges that in turn add more
edges due to the constraint of transitivity and so recall dramatically rises at the expense
of precision. Global algorithms are not as prone to this effect because they refrain from
adding edges that eventually lead to the addition of many unwarranted edges.
Table 5 also shows that WordNet, a manually constructed resource, has notably
the highest precision and lowest recall. The low recall exemplifies how the entailment
relations given by the gold-standard annotators transcend much beyond simple lexical
relations that appear in WordNet: Many of the gold-standard entailment relations are
missing from WordNet or involve multi-word phrases that do not appear in WordNet
at all.
Note that although the precision of WordNet is the highest in Table 5, its absolute
value (44.1%) is far from perfect. This illustrates that hierarchies of predicates are quite
Figure 2
Recall-precision curve comparing ILP-Global with Greedy-Global and ILP-Local.
92
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Table 8
Results of all distributional similarity measures when tuning K over the development set.
We encode the description of the measures presented in Table 2 in the following manner?
h = health-care corpus; R = RCV1 corpus; b = binary templates; u = unary templates; L = Lin similarity
measure; B = BInc similarity measure; pCt = pair of CUI tuples representation; pC = pair of CUIs
representation; Ct = CUI tuple representation; C = CUI representation; Lin & Pantel = similarity
lists learned by Lin and Pantel.
Edges Propositions
Dist. sim. measure Recall Precision F1 Recall Precision F1
h-b-B-pCt 52.5 31.6 37.7 69.8 50.0 57.1
h-b-B-pC 50.5 26.5 30.7 67.1 43.5 50.1
h-b-B-Ct 10.4 44.5 15.4 39.1 78.9 51.6
h-b-B-C 7.6 42.9 11.1 37.9 79.8 50.7
h-b-L-pCt 53.4 34.9 37.5 73.5 50.6 56.1
h-b-L-pC 47.2 35.2 35.6 68.6 52.9 56.2
h-b-L-Ct 47.0 26.6 30.2 64.9 47.4 49.6
h-b-L-C 34.6 22.9 22.5 57.2 52.6 47.6
h-u-B-Ct 5.1 37.4 8.5 35.1 91.0 49.7
h-u-B-C 7.2 42.4 11.5 36.1 90.3 50.1
h-u-L-Ct 22.8 22.0 18.3 49.7 49.2 44.5
h-u-L-C 16.7 26.3 17.8 47.0 56.8 48.1
R-b-L-l 49.4 21.8 25.2 72.4 39.0 45.5
R-u-L-l 24.1 30.0 16.8 47.1 55.2 42.1
R-u-B-l 9.5 57.1 14.1 37.2 84.0 49.5
Lin & Pantel 37.1 32.2 25.1 58.9 54.6 48.6
ambiguous and thus using WordNet directly yields relatively low precision. WordNet
is vulnerable to such ambiguity because it is a generic domain-independent resource,
whereas our algorithm learns from a domain-specific corpus. For example, the words
have and cause are synonyms according to one of the senses in WordNet and so the
erroneous rule X have asthma ? X cause asthma is learned using WordNet. Another
example is the rule X follows chemotherapy ? X takes chemotherapy, which is incorrectly
inferred because follow is a hyponym of take according to one of WordNet?s senses (she
followed the feminist movement). Due to these mistakes made by WordNet, the precision
achieved by our automatically trained ILP-Global algorithm when tuning parameters
on the development set (Table 4) is higher than that of WordNet.
Table 6 shows the results when the prior ? is estimated using maximum likelihood
over the development set (by computing the edge density over all the development
set graphs), and not tuned empirically with grid search. This allows for a comparison
between our algorithm that maximizes the a posteriori probability and Snow, Jurafsky,
and Ng?s (2006) algorithm that maximizes the likelihood. The gold-standard graphs are
quite sparse (? ? 0.1); therefore, as explained in Section 4.2.4, the effect of the prior is
substantial. ILP-Global and Greedy-Global learn sparse graphs with high precision and
low recall, whereas ILP-Global-Likelihood and Greedy-Global-Likelihood learn dense
graphs with high recall but very low precision. Overall, optimizing the a posteriori
probability is substantially better than optimizing likelihood, but still leads to a large
degradation in performance. This can be explained because our algorithm is not purely
probabilistic: The learned graphs are the product of mixing a probabilistic objective
function with non-probabilistic constraints. Thus, plugging the estimated prior into this
model results in performance that is far from optimal. In future work, we will examine
93
Computational Linguistics Volume 38, Number 1
a purely probabilistic approach that will allow us to reach good performance when
estimating ? directly. Nevertheless, currently optimal results are achieved when the
prior ? is tuned empirically.
Figure 2 shows a recall?precision curve for ILP-Global, Greedy-Global, and ILP-
Local, obtained by varying the prior parameter, ?. The figure clearly demonstrates the
advantage of using global information and ILP. ILP-Global is better than Greedy-Global
and ILP-Local in almost every point of the recall?precision curve, regardless of the exact
value of the prior parameter. Last, we present for completeness in Table 7 the results of
ILP-Global for all concepts in the test set.
In Table 8 we present the results obtained for all 16 distributional similarity mea-
sures. The main conclusion we can derive from this table is that the best distributional
similarity measures are those that represent templates using pairs of argument instan-
tiations rather than each argument separately. A similar result was found by Yates and
Etzioni (2009), who described the RESOLVER paraphrase learning system and have
shown that it outperforms DIRT. In their analysis, they attribute this result to their
representation that utilizes pairs of arguments comparing to DIRT, which computes a
separate score for each argument.
In the next two sections we perform a more thorough qualitative and quantitative
comparison trying to analyze the importance of using global information in graph
learning (Section 5.3.1), as well as the contribution of using ILP rather than a greedy
optimization procedure (Section 5.3.2). We note that the analysis presented in both sec-
tions is for the results obtained when optimizing parameters over the development set.
5.3.1 Global vs. Local Information. We looked at all edges in the test-set graphs where
ILP-Global and ILP-Local disagree and checked which algorithm was correct. Table 9
presents the result. The main advantage of using ILP-Global is that it avoids inserting
wrong edges into the graph. This is because ILP-Local adds any edge (u, v) such that
Puv crosses a certain threshold, disregarding edges that will be consequently added due
to transitivity (recall that for local algorithms we add edges inferred by transitivity, cf.
Section 5.2). ILP-Global will avoid such edges of high probability if it results in inserting
many low probability edges. This results in an improvement in precision, as exhibited
by Table 4.
Figures 3 and 4 show fragments of the graphs learned by ILP-Global and ILP-
Local (prior to adding transitive edges) for the test-set concepts diarrhea and seizure,
and illustrate qualitatively how global considerations improve precision. In Figure 3,
we witness that the single erroneous edge X results in diarrhea ? X prevents diarrhea
inserted by the local algorithm because Puv is high, effectively bridges two strongly
connected components and induces a total of 12 wrong edges (all edges from the
upper component to the lower component), whereas ILP-Global refrains from inserting
this edge. Figure 4 depicts an even more complex scenario. First, ILP-Local induces
a strongly connected component of five nodes for the predicates control, treat, stop,
Table 9
Comparing disagreements between ILP-Global and ILP-Local against the gold-standard graphs.
Global=True/Local=False Global=False/Local=True
Gold standard=true 48 42
Gold standard=false 78 494
94
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Figure 3
A comparison between ILP-Global and ILP-local for two fragments of the test-set concept
diarrhea.
reduce, and prevent, whereas ILP-Global splits this strongly connected component into
two, which although not perfect, is more compatible with the gold-standard graphs.
In addition, ILP-Local inserts four erroneous edges that connect two components of
size 4 and 5, which results in adding eventually 30 wrong edges. On the other hand,
Figure 4
A comparison between ILP-Global and ILP-Local for two fragments of the test-set concept
seizure.
95
Computational Linguistics Volume 38, Number 1
ILP-Global is aware of the consequences of adding these four seemingly good edges,
and prefers to omit them from the learned graph, leading to much higher precision.
Although the main contribution of ILP-Global, in terms of F1, is in an increase in
precision, we also notice an increase in recall in Table 4. This is because the optimal
prior is ? = 0.45 in ILP-Global but ? = 1.5 in ILP-Local. Thus, any edge (u, v) such that
0.45 < Suv < 1.5 will have positive weight in ILP-Global and might be inserted into the
graph, but will have negative weight in ILP-Local and will be rejected. The reason is that
in a local setting, reducing false positives is handled only by applying a large penalty
for every wrong edge, whereas in a global setting wrong edges can be rejected because
they induce more ?bad? edges. Overall, this leads to an improved recall in ILP-Global.
This also explains why ILP-Local is severely harmed when no prior is used at all, as
shown in Table 5.
Last, we note that across the 12 test-set graphs, ILP-Global achieves better F1 over
the edges in 7 graphs with an average advantage of 11.7 points, ILP-Local achieves
better F1 over the edges in 4 graphs with an average advantage of 3.0 points, and one
performance is equal.
5.3.2 Greedy vs. Non-Greedy Optimization. We would like to understand how using an
ILP solver improves performance compared with a greedy optimization procedure.
Table 4 demonstrates that ILP-Global and Greedy-Global reach a similar level of re-
call, although ILP-Global achieves far better precision. Again, we investigated edges
for which the two algorithms disagree and checked which one was correct. Table 10
demonstrates that the higher precision is because ILP-Global avoids inserting wrong
edges into the graph.
Figure 5 illustrates some of the reasons ILP-Global performs better than Greedy-
Global. Parts A1?A3 show the progression of Greedy-Global, which is an incremental
algorithm, for a fragment of the headache graph. In part A1 the learning algorithm still
separates the nodes X prevents headache and X reduces headache from the nodes X causes
headache and X results in headache (nodes surrounded by a bold oval shape constitute
a strongly connected component). After two iterations, however, the four nodes are
joined into a single strongly connected component, which is an error in principle
but at this point seems to be the best decision to increase the posterior probability
of the graph. This greedy decision has two negative ramifications. First, the strongly
connected component can no longer be untied. Thus, in A3 we observe that in future
iterations the strongly connected component expands further and many more wrong
edges are inserted into the graph. On the other hand, in B we see that ILP-Global takes
into consideration the global interaction between the four nodes and other nodes of the
graph, and decides to split this strongly connected component in two, which improves
the precision of ILP-Global. Second, note that in A3 the nodes Associate X with headache
and Associate headache with X are erroneously isolated. This is because connecting them
to the strongly connected component that contains six nodes will add many edges with
Table 10
Comparing disagreements between ILP-Global and Greedy-Global against the gold-standard
graphs.
ILP=True/Greedy=False ILP=False/Greedy=True
Gold standard=true 66 56
Gold standard=false 44 480
96
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Figure 5
A comparison between ILP-Global and Greedy-Global. Parts A1?A3 depict the incremental
progress of Greedy Global for a fragment of the headache graph. Part B depicts the corresponding
fragment in ILP-Global. Nodes surrounded by a bold oval shape are strongly connected
components.
low probability and so this is avoided by Greedy-Global. Because in ILP-Global the
strongly connected component was split in two, it is possible to connect these two nodes
to some of the other nodes and raise the recall of ILP-Global. Thus, we see that greedy
optimization might get stuck in local maxima and consequently suffer in terms of both
precision and recall.
Last, we note that across the 12 test-set graphs, ILP-Global achieves better F1 over
the edges in 9 graphs with an average advantage of 10.0 points, Greedy-Global achieves
better F1 over the edges in 2 graphs with an average advantage of 1.5 points, and in one
case performance is equal.
5.4 Error Analysis
In this section, we compare the results of ILP-Global with the gold-standard graphs
and perform error analysis. Error analysis was performed by comparing the 12 graphs
learned by ILP-Global to the corresponding 12 gold-standard graphs (randomly sam-
pling from the two available gold-standard graphs), and manually examining all edges
for which the two disagree. We found that the number of false positives and false
negatives is almost equal: 282 edges were learned by ILP-Global but are not in the gold-
standard graphs (false positive) and 287 edges were in the gold-standard graphs but
were not learned by ILP-Global (false negatives).
97
Computational Linguistics Volume 38, Number 1
Table 11
Error analysis for false positives and false negatives.
False positives False negatives
Total count 282 Total count 287
Classifier error 84.8% Classifier error 73.5%
Co-hyponym error 18.0% Long-predicate error 36.2%
Direction error 15.1% Generality error 26.8%
String overlap error 20.9%
Table 11 presents the results of our manual error analysis. Most evident is the fact
that the majority of mistakes are misclassifications of the entailment classifier. For 73.5%
of the false negatives the classifier?s probability was Puv < 0.5 and for 84.8% of the false
positives the classifier?s probability was Puv > 0.5. This shows that our current classifier
struggles to distinguish between positive and negative examples. Figure 6 illustrates
some of this difficulty by showing the distribution of the classifier?s probability, Puv,
over all node pairs in the 12 test-set graphs. Close to 80% of the scores are in the range
0.45?0.5, most of which are simply node pairs for which all distributional similarity
features are zero. Although in the great majority of such node pairs (t1, t2) t1 indeed
does not entail t2, there are also some cases where t1 does entail t2. This implies that the
current feature representation is not rich enough, and in the next section we explore a
larger feature set.
Table 11 also shows some other reasons found for false positives. Many false posi-
tives are pairs of predicates that are semantically related, that is, 18% of false positives
are templates that are hyponyms of a common predicate (co-hyponym error), and 15.1%
of false positives are pairs where we err in the direction of entailment (direction error).
For example ILP-Global learns that place X in mouth ? remove X from mouth, which is a
Figure 6
Distribution of probabilities given by the classifier over all node pairs of the test-set graphs.
98
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
co-hyponym error, and also that X affects lungs ? X damages lungs, which is a direction
error because entailment holds in the other direction. This illustrates the infamous
difficulty of distributional similarity features to discern the type of semantic relation
between two predicates.
Table 11 also shows additional reasons for false negatives. We found that in 36.2% of
false negatives one of the two templates contained a ?long? predicate, that is a predicate
composed of more than one content word, such as Ingestion of X causes injury to Y. This
might indicate that the size of the health-care corpus is too small to collect sufficient
statistics for complex predicates. In addition, 26.8% of false negatives were manually
analyzed as ?generality errors.? An example is the edge HPV strain causes X ? associate
HPV with X that is in the gold-standard graph but was missed by ILP-Global. Indeed,
this edge falls within the definition of textual entailment and is correct: For example,
if some HPV strain causes cervical cancer then cervical cancer is associated with HPV.
Because the entailed template is much more general than the entailing template, how-
ever, they are not instantiated by similar arguments in the corpus and distributional
similarity features fail to capture their semantic similarity. Last, we note that in 20.9%
of the false negatives, there was some string overlap between the entailing and entailed
templates, for example in X controls asthma symptoms ? X controls asthma. In the next
section we experiment with a feature that is based on string similarity.
Tables 8 and 9 show that there are cases where ILP-Global makes a mistake, whereas
ILP-Local or Greedy-Global are correct. An illustrating example for such a case is
shown in Figure 7. Looking at ILP-Local we see that the entailment classifier correctly
classifies the edges X triggers asthma ? X causes asthma and X causes asthma ? Associate
X with asthma, but misclassifies X triggers asthma ? Associate X with asthma. Because this
configuration violates a transitivity constraint, ILP-Global must make a global decision
whether to add the edge X triggers asthma ? Associate X with asthma or to omit one of
Figure 7
A scenario where ILP-Global makes a mistake, but ILP-Local is correct.
99
Computational Linguistics Volume 38, Number 1
the correct edges. The optimal global decision in this case causes a mistake with respect
to the gold standard. More generally, a common phenomenon of ILP-Global is that it
splits components that are connected in ILP-Local, for example, in Figures 3 and 4. ILP-
Global splits the components in a way that is optimal according to the scores of the local
entailment classifier, but these are not always accurate according to the gold standard.
Figure 5 exemplifies a scenario where ILP-Global errs, but Greedy-Global is (partly)
correct. ILP-Global mistakenly learns entailment rules from the templates Associate
X with headache and Associate headache with X to the templates X causes headache and
X results in headache, whereas Greedy-Global isolates the templates Associate X with
headache and Associate headache with X in a separate component. This happens because
of the greedy nature of Greedy-Global. Notice that in step A2 the templates X causes
headache and X results in headache are already included (erroneously) in a connected
component with the templates X prevents headache and X reduces headache. Thus, adding
the rules from Associate X with headache and Associate headache with X to X causes headache
and X results in headache would also add the rules to X reduces headache and X prevents
headache and the Greedy-Global avoids that. ILP-Global does not have that problem: It
simply chooses the optimal choice according to the entailment classifier, which splits the
connected component presented in A2. Thus, once again we see that mistakes made by
ILP-Global are often due to the inaccuracies of the scores given by the local entailment
classifier.
6. Local Classifier Extensions
The error analysis in Section 5.4 exemplified that most errors are the result of misclassi-
fications made by the local entailment classifier. In this section, we investigate the local
entailment classifier component, focusing on the set of features used for classification.
We first present an experimental setting in which we consider a wider set of features,
then we present the results of the experiment, and last we perform feature analysis and
draw conclusions.
6.1 Feature Set and Experimental Setting
In previous sections we employed a distant supervision framework: We generated
training examples automatically with WordNet, and represented each example with
distributional similarity features. Distant supervision comes with a price, however?it
prevents us from utilizing all sources of information. For example, looking at the pair of
gold-standard templates X manages asthma and X improves asthma management, one can
exploit the fact that management is a derivation of manage to improve the estimation of
entailment. The automatically generated training set was generated by looking at Word-
Net?s hypernym, synonym, and co-hyponyms relations, however, and hence no such
examples appear in the training set, rendering this type of feature useless. Moreover,
one cannot use WordNet?s hypernym, synonym, and co-hyponym relations as features
because the generated training set is highly biased?all positive training examples are
either hypernyms or synonyms and all negative examples are co-hyponyms.
In this section we would like to examine the utility of various features, while avoid-
ing the biases that occur due to distant supervision. Therefore, we use the 23 manually
annotated gold-standard graphs for both training and testing, in a cross-validation
setting. Although this reduces the size of the training set it allows us to estimate the
utility of various features in a setting where the training set and test set are sampled
from the same underlying distribution, without the aforementioned biases.
100
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
We would like to extract features that express information that is diverse and
orthogonal to the one given by distributional similarity. Therefore, we turn to existing
knowledge resources that were created using both manual and automatic methods,
expressing various types of linguistic and statistical information that is relevant for
entailment prediction:
1. WordNet: contains manually annotated relations such as hypernymy,
synonymy, antonymy, derivation, and entailment.
2. VerbOcean11 (Chklovski and Pantel 2004): contains verb relations such as
stronger-than and similar that were learned with pattern-based methods.
3. CATVAR12 (Habash and Dorr 2003): contains word derivations such as
develop?development.
4. FRED13 (Ben Aharon, Szpektor, and Dagan 2010): contains entailment
rules between templates learned automatically from FrameNet.
5. NomLex14 (Macleod et al 1998): contains English nominalizations
including their argument mapping to the corresponding verbal form.
6. BAP15 (Kotlerman et al 2010): contains directional distributional
similarity scores between lexical terms (rather than propositional
templates) calculated with the BAP similarity scoring function.
Table 12 describes the 16 new features that were generated for each of the gold-
standard examples (resulting in a total of 32 features). The first 15 features were gen-
erated by the aforementioned knowledge bases. The last feature measures the edit
distance between templates: Given a pair of templates (t1, t2), we concatenate the words
in each template and derive a pair of strings (s1, s2). Then we compute the Levenshtein
string edit-distance (Cohen, Ravikumar, and Fienberg 2003) between s1 and s2 and
divide the score by |s1|+ |s2| for normalization.
Table 12 also describes for each feature the number and percentage of examples for
which the feature value is non-zero (out of the examples generated from the 23 gold-
standard graphs). A salient property of many of the new features is that they are sparse:
The four antonymy features as well as the Derivation, Entailment, Nomlex, and FRED
features occur in very few examples in our data set, which might make training with
these features difficult.
After generating the new features we employ a leave-one-graph-out strategy to
maximally exploit the manually annotated gold standard for training. For each of the
test-set graphs, we train over all development and test-set graphs except for the one
that is left out,16 after tuning the algorithm?s parameters and test. Parameter tuning is
done by cross-validation over the development set, tuning to maximize the F1 of the set
11 http://demo.patrickpantel.com/demos/verbocean/.
12 http://clipdemos.umiacs.umd.edu/catvar/.
13 http://u.cs.biu.ac.il/?nlp/downloads/FRED.html.
14 http://nlp.cs.nyu.edu/nomlex/index.html.
15 http://u.cs.biu.ac.il/?nlp/downloads/DIRECT.html.
16 As described in Section 5, we train with a balanced number of positive and negative examples. Because
the number of positive examples in the gold standard is smaller than the number of negative examples,
we use all positives and randomly sample the same number of negatives, resulting in ? 1, 500 training
examples.
101
Computational Linguistics Volume 38, Number 1
Table 12
The set of new features. The last two columns denote the number and percentage of examples
for which the value of the feature is non-zero in examples generated from the 23 gold-standard
graphs.
Name Type Description # %
Hyper. boolean Whether (t1, t2) are identical except for a pair of words
(w1, w2) such that w2 is a hypernym (distance ? 2) of w1
in WordNet.
120 1.1
Syno. boolean Whether (t1, t2) are identical except for a pair of words
(w1, w2) such that w2 is a synonym of w1 in WordNet.
94 0.9
Co-hypo. boolean Whether (t1, t2) are identical except for a pair of words
(w1, w2) such that w2 is a co-hyponym of w1 in WordNet.
302 2.8
WN Ant. boolean Whether (t1, t2) are identical except for a pair of words
(w1, w2) such that w2 is an antonym of w1 in WordNet.
6 0.06
VO Ant. boolean Whether (t1, t2) are identical except for a pair of words
(w1, w2) such that w2 is an antonym of w1 in VerbOcean.
25 0.2
WN Ant. 2 boolean Whether there exists in (t1, t2) a pair of words (w1, w2)
such that w2 is an antonym of w1 in WordNet.
22 0.2
VO Ant. 2 boolean Whether there exists in (t1, t2) a pair of words (w1, w2)
such that w2 is an antonym of w1 in VerbOcean.
73 0.7
Derivation boolean Whether there exists in (t1, t2) a pair of words (w1, w2)
such that w2 is a derivation of w1 in WordNet or CATVAR.
78 0.7
Entailment boolean Whether there exists in (t1, t2) a pair of words (w1, w2)
such that w2 is entailed by w1 in WordNet.
20 0.2
FRED boolean Whether t1 entails t2 in FRED. 9 0.08
Nomlex boolean Whether t1 entails t2 in Nomlex. 8 0.07
VO strong boolean Whether (t1, t2) are identical except for a pair of words
(w1, w2) such that w2 is stronger than w1 in VerbOcean.
104 1
VO simil. boolean Whether (t1, t2) are identical except for a pair of words
(w1, w2) such that w2 is similar to w1 in VerbOcean.
191 1.8
Positive boolean Disjunction of the features Hypernym, Synonym, Nom-
lex, and VO stronger.
289 2.7
BAP real maxw1?t1,w2?t2BAP(w1, w2). 506 4.7
Edit real Normalized edit-distance. 100
of learned edges (the development and test set are described in Section 5). Graphs are
always learned with the LP-Global algorithm.
Our main goal is to check whether the added features improve performance, and
therefore we run the experiment both with and without the new features. In addi-
tion, we would like to test whether using different classifiers affects performance.
Therefore, we run the experiments with a linear-kernel SVM, a square-kernel SVM,
a Gaussian-kernel SVM, logistic regression, and naive Bayes. We use the SVMPerf
package (Joachims 2005) to train the SVM classifiers and the Weka package (Hall et al
2009) for logistic regression and naive Bayes.
6.2 Experiment Results
Table 13 describes the macro-average recall, precision, and F1 of all classifiers both with
and without the new features on the development set and test set. Using all features is
denoted by Xall, and using the original features is denoted by Xold.
102
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Table 13
Macro-average recall, precision, and F1 on the development set and test set using the parameters
that maximize F1 of the learned edges over the development set.
Development set Test set
Algorithm Recall Precision F1 Recall Precision F1
Linearall 48.1 31.9 36.3 51.7 37.7 40.3
Linearold 40.3 33.3 34.8 47.2 42.2 41.1
Gaussianall 41.8 32.4 35.1 48.0 41.1 40.7
Gaussianold 41.1 31.2 33.9 50.3 39.7 40.5
Squareall 39.9 32.0 34.1 43.7 39.8 38.9
Squareold 38.0 31.6 32.9 50.2 41.0 41.3
Logisticall 34.4 27.6 29.1 39.8 41.7 37.8
Logisticold 39.3 31.2 33.5 45.4 40.9 39.9
Bayesall 20.8 33.2 24.5 27.4 46.0 31.7
Bayesold 20.3 34.9 24.6 26.4 45.4 30.9
Examining the results it does not appear that the new features improve perfor-
mance. Whereas on the development set the new features add 1.2?1.5 F1 points for all
SVM classifiers, on the test set using the new features decreases performance for the
linear and square classifiers. This shows that even if there is some slight increase in
performance when using SVM on the development set, it is masked by the variance
added in the process of parameter tuning. In general, including the new features does
not yield substantial differences in performance.
Secondly, the SVM classifiers perform better than the logistic and naive Bayes clas-
sifiers. Using the more complex square and Gaussian kernels does not seem justified,
however, as the differences between the various kernels are negligible. Therefore, in our
analysis we will use a linear kernel SVM classifier.
Last, we note that although we use supervised learning rather than distant super-
vision, the results we get are slightly lower than those presented in Section 5. This is
probably due to the fact that our manually annotated data set is rather small. Nev-
ertheless, this shows that the quality of the distant supervision training set generated
automatically from WordNet is reasonable.
Next, we perform analysis of the different features of the classifier to better under-
stand the reasons for the negative result obtained.
6.3 Feature Analysis
We saw that the new features slightly improved performance for SVM classifiers on
the development set, although no clear improvement was witnessed on the test set.
To further check whether the new features carry useful information we measured the
training set accuracy for each of the 12 training sets (leaving out each time one test-
set graph). Using the new features improved the average training set accuracy from
71.6 to 72.3. More importantly, it improved performance consistently in all 12 training
sets by 0.4?1.2 points. This strengthens our belief that the new features do carry a
certain amount of information, but this information is too sparse to affect the overall
103
Computational Linguistics Volume 38, Number 1
performance of the algorithm. In addition, notice that the absolute accuracy on the
training set is low?72.3. This shows that separating entailment from non-entailment
using the current set of features is challenging.
Next, we would like to perform analysis on each of the features. First, we perform
an ablation test over the features by omitting each one of them and re-training the
classifier Linearall. In Table 14, the columns ablation F1 and ? show the F1 obtained and
the difference in performance from the Linearall classifier, which scored 40.3 F1 points.
Results show that there is no ?bad? feature that deteriorates performance. For almost all
features ablation causes a decrease in performance, although this decrease is relatively
small. There are only four features for which ablation decreases performance by more
than one point: three distributional similarity features, but also the new hypernym
feature.
The next three columns in the table describe the precision and recall of the new
boolean features. The column Feature type indicates whether we expect a feature to
indicate entailment or non-entailment and the columns Prec. and Recall specify the
Table 14
Results of feature analysis. The second column denotes the proportion of manually annotated
examples for which the feature value is non-zero. A detailed explanation of the other columns is
provided in the body of the article.
Feature name % Ablation F1 ? Feature type Prec. Recall Classification F1
h-b-B-pCt 8.2 39.3 ?1 14.9
h-b-B-pC 6.9 39.5 ?0.8 33.2
h-b-B-Ct 1.6 40.3 0 15.4
h-b-B-C 1.6 40.5 0.2 11.2
h-b-L-pCt 23.6 38.3 ?2.0 37.0
h-b-L-pC 21.4 39.4 ?0.9 35.2
h-b-L-Ct 9.7 40.1 ?0.2 27.3
h-b-L-C 8.1 39.7 ?0.6 14.1
h-u-B-Ct 1.0 39.4 ?0.9 10.9
h-u-B-C 1.1 39.8 ?0.5 12.6
h-u-L-Ct 6.1 39.8 ?0.5 18.5
h-u-L-C 6.3 39.2 ?1.1 19.3
R-b-L-l 22.5 40.1 ?0.2 26.7
R-u-L-l 8.3 39.4 ?0.9 23.2
R-u-B-l 1.9 39.8 ?0.5 16.7
Lin & Pantel 8.8 38.7 ?1.6 23.0
Hyper. 1.1 38.7 ?1.6 + 37.1 4.9 9.7
Syno. 0.9 40.3 0 + 43.1 4.5 15.8
Co-hypo. 2.8 40.1 ?0.2 ? 82.0 2.5 17.9
WN ant. 0.06 39.8 ?0.5 ? 75.0 0.05 1.2
VO ant. 0.2 40.1 ?0.2 ? 96.0 0.2 2.2
WN ant. 2. 0.2 39.4 ?0.9 ? 59.1 0.1 2.7
VO ant. 2 0.7 40.2 ?0.1 ? 98.6 0.7 2.2
Derivation 0.7 39.5 ?0.8 + 47.4 4.1 10.2
Entailment 0.2 39.7 ?0.6 + 15.0 0.3 1.2
FRED 0.08 39.7 ?0.6 + 77.8 0.8 3.2
NomLex 0.07 39.8 ?0.5 + 75.0 0.7 3.3
VO strong. 1 39.4 ?0.9 + 34.6 4 6.9
VO simil. 1.8 39.4 ?0.9 + 28.8 6.1 12.5
Positive 2.7 39.8 ?0.5 + 36.7 11.8
BAP 4.7 40.1 ?0.2 13.3
Edit 100 39.9 ?0.4 15.5
104
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
precision and recall of that feature. For example, the feature FRED is a positive feature
that we expect to support entailment, and indeed 77.8% of the gold-standard examples
for which it is turned on are positive examples. It is turned on only in 0.8% of the
positive examples, however. Similarly, VO ant. is a negative feature that we expect to
support non-entailment, and indeed 96% of the gold-standard examples for which it is
on are negative examples, but it is turned on in only 0.2% of the negative examples.
The precision results are quite reasonable: For most positive features the precision is
well over the proportion of positive examples in the gold standard, which is about
10% (except for the Entailment feature whose precision is only 15%). For the negative
features it seems that the precision of VerbOcean features is very high (though they are
sparse), and the precision of WordNet antonyms and co-hyponyms is lower. Looking
at the recall we can see that the coverage of the boolean features is low.
The last column in the table describes results of training the classifier with a single
feature. For each feature we train a linear kernel SVM, tune the sparsity parameter on
the development set, and measure F1 over the test set. Naturally, classifiers that are
trained on sparse features yield low performance.
This column allows us once again (cf. Table 8) to examine the original distributional
similarity features. There are three distributional similarity features that achieve F1 of
more than 30 points, and all three represent features using pairs of argument instan-
tiations rather than treat each argument separately, as we have already witnessed in
Section 5.
Note also that the feature h-b-L-pCt, which uses binary templates, the Lin similarity
measure, and features that are pairs of CUI tuples, is the best feature both in terms of the
ablation test and when it is used as a single feature for the classifier. The result obtained
by this feature is only 3.3 points lower than that obtained when using the entire feature
set. We believe this is for two reasons: First, the 16 distributional similarity features are
correlated with one another and thus using all of them does not boost performance
substantially. For example, the Pearson correlation coefficients between the features
h-b-B-pCt, h-b-B-Ct, h-b-L-pCt, h-b-L-Ct, h-u-B-Ct, and h-u-L-Ct (all utilize CUI tuples) and
h-b-B-pC, h-b-B-C, h-b-L-pC, h-b-L-C, h-u-B-C, and h-u-L-C (all use CUIs), respectively, are
over 0.9. The second reason for gaining only 3.3 points by the remaining features is that,
as discussed, the new set of features is relatively sparse.
To sum up, we suggest several hypotheses that explain our results and analysis:
 The new features are too sparse to substantially improve the performance
of the local entailment classifier in our data set. This perhaps can be
attributed to the nature of our domain-specific health-care corpus. In the
future, we would like to examine the sparsity of these features in a general
domain.
 Looking at the training set accuracy, ablations, and precision of the new
features, it seems that the behavior of most of them is reasonable. Thus,
it is possible that in a different learning scheme that does not use the
resources as features the information they provide may become beneficial.
For example, in a simple ?back-off? approach one can use rules from
precise resources to determine entailment, and apply a classifier only
when no precise resource contains a relevant rule.
 In our corpus representing distributional similarity features with
pairs of argument instantiations is better than treating each argument
independently.
105
Computational Linguistics Volume 38, Number 1
 Given the current training set accuracy and the sparsity of the new
features, it is important to develop methods that gather large-scale
information that is orthogonal to distributional similarity. In our opinion,
the most promising direction for acquiring such rich information is by
methods that look at co-occurrence of predicates or templates on the Web
(Chklovski and Pantel 2004; Pekar 2008).
7. Conclusions and Future Work
This article presented a global optimization algorithm for learning entailment rules
between predicates, represented as propositional templates. Most previous work on
learning entailment rules between predicates focused on local learning methods, which
consider each pair of predicates in isolation. To the best of our knowledge, this is the
most comprehensive attempt to date to exploit global interactions between predicates
for improving the set of learned entailment rules.
We modeled the problem as a graph learning problem, and searched for the best
graph under a global transitivity constraint. Two objective functions were defined for
the optimization procedure, one score-based and the other probabilistic, and we have
shown that under certain conditions (specified in Appendix A) the score-based function
can be interpreted probabilistically. This allowed us to use both margin as well as
probabilistic classifiers for the underlying entailment classifier. We solved the optimiza-
tion problem using Integer Linear Programming, which provides an optimal solution
(compared to the greedy algorithm suggested by Snow, Jurafsky, and Ng [2006]), and
demonstrated empirically that this method outperforms local algorithms as well as
a state-of-the-art greedy optimization algorithm on the graph learning task. We also
analyzed quantitatively and qualitatively the reasons for the improved performance of
our global algorithm and performed detailed error analysis. Last, we experimented with
various entailment classifiers that utilize different sets of features from many knowledge
bases.
The experiments and analysis performed indicate that the current performance of
the local entailment classifier needs to be improved. We believe that the most promising
direction for improving the local classifier is to use methods that look for co-occurrence
of predicates in sentences or documents on the Web, because these methods excel at
identifying specific semantic relations. It is also possible to use other sources of infor-
mation such as lexicographic resources, although this probably will require a learning
scheme that is robust to the relatively low coverage of these resources. Increasing the
size of the training corpus is also an important direction for improving the entailment
classifier.
Another important direction for future work is to apply our algorithm to graphs
that are larger by a few orders of magnitude than the focused entailment graphs dealt
with in this article. This will introduce a challenge to our current optimization algorithm
due to complexity issues, as our ILP contains O(|V|3) constraints. In addition, this will
require careful handling of predicate ambiguity, which interferes with the transitivity
of entailment and will become a pertinent issue in large graphs. Some first steps in this
direction have already been carried out (Berant, Dagan, and Goldberger 2011).
In addition, our graphs currently contain a single type of edge, namely, the entail-
ment relation. We would like to model more types of edges in the graph, representing
additional semantic relations such as co-hyponymy, and to explicitly describe the inter-
actions between the various types of edges, aiming to further improve the quality of the
learned entailment rules.
106
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Figure 8
A hierarchical summary of propositions involving nausea as an argument, such as headache is
related to nausea, acupuncture helps with nausea, and Lorazepam treats nausea.
Last, in Section 3.1 we mentioned that by merging strongly connected components
in entailment graphs, hierarchies of predicates can be generated (recall Figure 1). As
proposed by Berant, Dagan, and Goldberger (2010), we believe that these hierarchies can
be useful not only in the context of semantic inference applications, but also in the field
of faceted search and hierarchical text exploration (Stoica, Hearst, and Richardson 2007).
Figure 8 exemplifies how a set of propositions can be presented to a user according to
the hierarchy of predicates shown in Figure 1. In the field of faceted search, information
is presented using a number of hierarchies, corresponding to different facets or dimen-
sions of the data. One can easily use the hierarchy of predicates learned by our algorithm
as an additional facet in the context of a text-exploration application. In future work,
we intend to implement this application and perform user experiments to test whether
adding this hierarchy facilitates exploration of textual information.
Appendix A: Derivation of the Probabilistic Objective Function
In this section we provide a full derivation for the probabilistic objective function
given in Section 4.2.2. Given two nodes u and v from a set of nodes V, we denote by
Iuv = 1 the event that u entails v, by Fuv the feature vector representing the ordered
pair (u, v), and by F the set of feature vectors over all ordered pairs of nodes, that is,
F = ?u=vFuv. We wish to learn a set of edges E, such that the posterior probability P(G|F)
is maximized, where G = (V, E). We assume that we have a ?local? model estimating the
edge posterior probability Puv = P(Iuv = 1|Fuv). Because this model was trained over a
balanced training set, the prior for the event that u entails v under the model is uniform:
P(Iuv = 1) = P(Iuv = 0) =
1
2 . Using Bayes?s rule we get:
P(Iuv = 1|Fuv) =
P(Iuv = 1)
P(Fuv)
? P(Fuv|Iuv = 1) = a ? P(Fuv|Iuv = 1) (A.1)
P(Iuv = 0|Fuv) =
P(Iuv = 0)
P(Fuv)
? P(Fuv|Iuv = 0) = a ? P(Fuv|Iuv = 0) (A.2)
107
Computational Linguistics Volume 38, Number 1
where a = 12?P(Fuv ) is a constant with respect to any graph. Thus, we conclude that
P(Iuv|Fuv) = a ? P(Fuv|Iuv). Next, we make three independence assumptions (the first two
are following Snow, Jurafsky, and Ng [2006]):
P(F|G) =
?
u=v
P(Fuv|G) (A.3)
P(Fuv|G) = P(Fuv|Iuv) (A.4)
P(G) =
?
u=v
P(Iuv) (A.5)
Assumption A.3 states that each feature vector is independent from other feature
vectors given the graph. Assumption A.4 states that the features Fuv for the pair (u, v)
are generated by a distribution depending only on whether entailment holds for (u, v).
Last, Assumption A.5 states that edges are independent and the prior probability of a
graph is a product of the prior probabilities of the edges. Using these assumptions and
equations A.1 and A.2, we can now express the posterior P(G|F):
P(G|F) ? P(G) ? P(F|G) (A.6)
=
?
u=v
[P(Iuv) ? P(Fuv|Iuv)] (A.7)
=
?
u=v
P(Iuv) ?
P(Iuv|Fuv)
a (A.8)
?
?
u=v
P(Iuv) ? Puv (A.9)
=
?
(u,v)?E
P(Iuv = 1) ? Puv ?
?
(u,v)/?E
P(Iuv = 0) ? (1 ? Puv) (A.10)
Note that under the ?local model? the prior for an edge in the graph was uniform,
because the model was trained over a balanced training set. Generally, however, this is
not the case, and thus we introduce an edge prior into the model when formulating the
global objective function. Now, we can formulate P(G|F) as a linear function:
G? = argmax
G
?
(u,v)?E
P(Iuv = 1) ? Puv ?
?
(u,v)/?E
P(Iuv = 0) ? (1 ? Puv) (A.11)
= argmax
G
?
(u,v)?E
log(Puv ? P(Iuv = 1)) +
?
(u,v)/?E
log[(1 ? Puv) ? P(Iuv = 0)] (A.12)
= argmax
G
?
u=v
(
Iuv ? log(Puv ? P(Iuv = 1)) + (1 ? Iuv) ? log[(1 ? Puv) ? P(Iuv = 0)]
)
(A.13)
= argmax
G
?
u=v
(
log
Puv ? P(Iuv = 1)
(1 ? Puv) ? P(Iuv = 0)
? Iuv + (1 ? Puv) ? P(Iuv = 0)
)
(A.14)
108
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
= argmax
G
?
u=v
log
Puv
(1 ? Puv)
? Iuv + log? ? |E| (A.15)
In the last transition we omit
?
u=v(1 ? Puv) ? P(Iuv = 0), which is a constant with
respect to the graph and denote the prior odds ratio by ? = P(Iuv=1)P(Iuv=0) . This leads to the
final formulation described in Section 4.2.2.
Acknowledgments
We would like to thank Roy Bar-Haim, David
Carmel, and the anonymous reviewers for
their useful comments. We also thank Dafna
Berant and the nine students who prepared
the gold-standard data set. This work was
developed under the collaboration of
FBK-irst/University of Haifa and was
partially supported by the Israel Science
Foundation grant 1112/08. The first author is
grateful to the Azrieli Foundation for the
award of an Azrieli Fellowship, and has
carried out this research in partial fulfilment
of the requirements for the Ph.D. degree.
References
Althaus, Ernst, Nikiforos Karamanis, and
Alexander Koller. 2004. Computing locally
coherent discourses. In Proceedings of the
ACL, pages 399?406, Barcelona.
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley framenet
project. In Proceedings of COLING-ACL,
pages 86?90, Montreal.
Bar-Haim, Roy, Ido Dagan, Iddo Greental,
and Eyal Shnarch. 2007. Semantic inference
at the lexical-syntactic level. In Proceedings
of AAAI, pages 871?876, Vancouver.
Ben Aharon, Roni, Idan Szpektor, and Ido
Dagan. 2010. Generating entailment rules
from framenet. In Proceedings of ACL,
pages 241?246, Uppsala.
Bentivogli, Luisa, Ido Dagan, Hoa Trang
Dang, Danilo Giampiccolo, and Bernarde
Magnini. 2009. The fifth Pascal recognizing
textual entailment challenge. In Proceedings
of TAC-09, pages 14?24, Gaithersburg, MD.
Berant, Jonathan, Ido Dagan, and Jacob
Goldberger. 2010. Global learning of
focused entailment graphs. In Proceedings
of ACL, pages 1220?1229, Uppsala.
Berant, Jonathan, Ido Dagan, and Jacob
Goldberger. 2011. Global learning of typed
entailment rules. In Proceedings of ACL,
pages 610?619, Portland, OR.
Bhagat, Rahul, Patrick Pantel, and Eduard
Hovy. 2007. LEDIR: An unsupervised
algorithm for learning directionality of
inference rules. In Proceedings of
EMNLP-CoNLL, pages 161?170, Prague.
Budanitsky, Alexander and Graeme Hirst.
2006. Evaluating Wordnet-based measures
of lexical semantic relatedness.
Computational Linguistics, 32(1):13?47.
Chklovski, Timothy and Patrick Pantel.
2004. VerbOcean: Mining the Web for
fine-grained semantic verb relations.
In Proceedings of EMNLP, pages 33?40,
Barcelona.
Clark, Peter, William Murray, John
Thompson, Phil Harrison, Jerry Hobbs,
and Christiane Fellbaum. 2007. On the role
of lexical and world knowledge in RTE3.
In Proceedings of the Workshop on Textual
Entailment and Paraphrasing, pages 54?59,
Prague.
Clarke, James and Mirella Lapata. 2008.
Global inference for sentence compression:
An integer linear programming approach.
Journal of Artificial Intelligence Research,
31:273?381.
Cohen, William, Pradeep Ravikumar,
and Stephen E. Fienberg. 2003. A
comparison of string distance metrics for
name-matching tasks. In Proceedings of
IIWeb, pages 73?78, Acapulco.
Connor, Michael and Dan Roth. 2007.
Context sensitive paraphrasing with
a single unsupervised classifier.
In Proceedings of ECML, pages 104?115,
Warsaw.
Coyne, Bob and Owen Rambow. 2009.
Lexpar: A freely available English
paraphrase lexicon automatically extracted
from Framenet. In Proceedings of the IEEE
International Conference on Semantic
Computing, pages 53?58, Berkeley, CA.
Dagan, Ido, Bill Dolan, Bernardo Magnini,
and Dan Roth. 2009. Recognizing textual
entailment: Rational, evaluation and
approaches. Natural Language Engineering,
15(4):1?17.
Do, Quang and Dan Roth. 2010. Constraints
based taxonomic relation classification. In
Proceedings of EMNLP, pages 1099?1109,
Cambridge, MA.
Fellbaum, Christiane. 1998a. A semantic
network of English: The mother of all
109
Computational Linguistics Volume 38, Number 1
wordNets. Natural Language Engineering,
32:209?220.
Fellbaum, Christiane, editor. 1998b. WordNet:
An Electronic Lexical Database (Language,
Speech, and Communication). The MIT Press,
Cambridge, MA.
Finkel, Jenny R. and Christopher D.
Manning. 2008. Enforcing transitivity in
coreference resolution. In Proceedings of
ACL-08: HLT, Short Papers, pages 45?48,
Columbus, OH.
Habash, Nizar and Bonnie Dorr. 2003.
A categorial variation database for
English. In Proceedings of the NAACL,
pages 17?23, Edmonton.
Hall, Mark, Eibe Frank, Geoffrey Holmes,
Bernhard Pfahringer, Peter Reutemann,
and Ian H. Witten. 2009. The WEKA data
mining software: An update. SIGKDD
Explorations, 11(1):10?18.
Harmeling, Stefan. 2009. Inferring textual
entailment with a probabilistically sound
calculus. Natural Language Engineering,
15(4):459?477.
Harris, Zellig. 1954. Distributional structure.
Word, 10(23):146?162.
Joachims, Thorsten. 2005. A support vector
method for multivariate performance
measures. In Proceedings of ICML,
pages 377?384, Bonn.
Kingsbury, Paul, Martha Palmer, and
Mitch Marcus. 2002. Adding semantic
annotation to the Penn TreeBank.
In Proceedings of HLT, pages 252?256,
San Diego, CA.
Kipper, Karin, Hoa T. Dang, and Martha
Palmer. 2000. Class-based construction
of a verb lexicon. In Proceedings of AAAI,
pages 691?696, Austin, TX.
Kotlerman, Lili, Ido Dagan, Idan Szpektor,
and Maayan Zhitomirsky-Geffet. 2010.
Directional distributional similarity for
lexical inference. Natural Language
Engineering, 16:359?389.
Lin, Dekang. 1998a. Automatic retrieval
and clustering of similar words.
In Proceedings of COLING-ACL,
pages 768?774, Montreal.
Lin, Dekang. 1998b. Dependency-based
evaluation of Minipar. In Proceedings of the
Workshop on Evaluation of Parsing Systems at
LREC, pages 317?329, Granada.
Lin, Dekang and Patrick Pantel. 2001.
Discovery of inference rules for question
answering. Natural Language Engineering,
7(4):343?360.
Macleod, Catherine, Ralph Grishman,
Adam Meyers, Leslie Barrett, and
Ruth Reeves. 1998. Nomlex: A lexicon of
nominalizations. In Proceedings of Euralex,
pages 187?193, Lieg`e.
Martins, Andre, Noah Smith, and Eric Xing.
2009. Concise integer linear programming
formulations for dependency parsing.
In Proceedings of ACL, pages 342?350,
Singapore.
Meyers, Adam, Ruth Reeves, Catherine
Macleod, Rachel Szekeley, Veronika
Zielinska, and Brian Young. 2004.
The cross-breeding of dictionaries. In
Proceedings of LREC, pages 1095?1098,
Lisbon.
Mirkin, Shachar, Ido Dagan, and Maayan
Gefet. 2006. Integrating pattern-based
and distributional similarity methods
for lexical entailment acquisition.
In Proceedings of COLING-ACL,
pages 579?586, Sydney.
Nikulin, Vladimir. 2008. Classification of
imbalanced data with random sets and
mean-variance filtering. International
Journal of Data Warehousing and Mining,
4(2):63?78.
Pekar, Viktor. 2008. Discovery of event
entailment knowledge from text corpora.
Computer Speech & Language, 22(1):1?16.
Raina, Rajat, Andrew Ng, and Christopher
Manning. 2005. Robust textual inference
via learning and abductive reasoning.
In Proceedings of AAAI, pages 1099?1105,
Pittsburgh, PA.
Riedel, Sebastian and James Clarke. 2006.
Incremental integer linear programming
for non-projective dependency parsing.
In Proceedings of EMNLP, pages 129?137,
Sydney.
Roth, Dan and Wen-tau Yih. 2004. A linear
programming formulation for global
inference in natural language tasks.
In Proceedings of CoNLL, pages 1?8,
Boston, MA.
Schoenmackers, Stefan, Jesse Davis,
Oren Etzioni, and Daniel S. Weld.
2010. Learning first-order horn
clauses from Web text. In Proceedings
of EMNLP, pages 1088?1098,
Cambridge, MA.
Sekine, Satoshi. 2005. Automatic paraphrase
discovery based on context and keywords
between NE pairs. In Proceedings of IWP,
pages 80?87, Jeju Island.
Siegel, Sidney and N. John Castellan. 1988.
Non-parametric Statistics for the Behavioral
Sciences. McGraw-Hill, New-York.
Smith, Noah and Jason Eisner. 2005.
Contrastive estimation: Training log-linear
models on unlabeled data. In Proceedings
of ACL, pages 354?362, Ann Arbor, MI.
110
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Snow, Rion, Daniel Jurafsky, and Andrew Y.
Ng. 2004. Learning syntactic patterns for
automatic hypernym discovery. In
Proceedings of NIPS, pages 1297?1304,
Vancouver.
Snow, Rion, Daniel Jurafsky, and Andrew Y.
Ng. 2006. Semantic taxonomy induction
from heterogenous evidence. In
Proceedings of ACL, pages 801?808, Prague.
Stoica, Emilia, Marti Hearst, and Megan
Richardson. 2007. Automating creation of
hierarchical faceted metadata structures. In
Proceedings of NAACL-HLT, pages 244?251,
Rochester, NY.
Szpektor, Idan and Ido Dagan. 2007.
Learning canonical forms of entailment
rules. In Proceedings of RANLP, pages 1?8,
Borovetz.
Szpektor, Idan and Ido Dagan. 2008.
Learning entailment rules for unary
templates. In Proceedings of COLING,
pages 849?856, Manchester.
Szpektor, Idan and Ido Dagan. 2009.
Augmenting Wordnet-based inference
with argument mapping. In Proceedings
of TextInfer, pages 27?35, Singapore.
Szpektor, Idan, Hristo Tanev, Ido Dagan,
and Bonaventura Coppola. 2004. Scaling
Web-based acquisition of entailment
relations. In Proceedings of EMNLP,
pages 41?48, Barcelona.
Van Hulse, Jason, Taghi Khoshgoftaar, and
Amri Napolitano. 2007. Experimental
perspectives on learning from imbalanced
data. In Proceedings of ICML,
pages 935?942, Corvallis, OR.
Vanderbei, Robert. 2008. Linear Programming:
Foundations and Extensions. Springer,
New-York.
Weeds, Julie and David Weir. 2003. A general
framework for distributional similarity.
In Proceedings of EMNLP, pages 81?88,
Sapporo.
Wilcoxon, Frank. 1945. Individual
comparisons by ranking methods.
Biometrics Bulletin, 1:80?83.
Yannakakis, Mihalis. 1978. Node-and
edge-deletion NP-complete problems. In
STOC ?78: Proceedings of the Tenth Annual
ACM Symposium on Theory of Computing,
pages 253?264, New York, NY.
Yates, Alexander and Oren Etzioni. 2009.
Unsupervised methods for determining
object and relation synonyms on the web.
Journal of Artificial Intelligence Research,
34:255?296.
111
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1220?1229,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Global Learning of Focused Entailment Graphs
Jonathan Berant
Tel-Aviv University
Tel-Aviv, Israel
jonatha6@post.tau.ac.il
Ido Dagan
Bar-Ilan University
Ramat-Gan, Israel
dagan@cs.biu.ac.il
Jacob Goldberger
Bar-Ilan University
Ramat-Gan, Israel
goldbej@eng.biu.ac.il
Abstract
We propose a global algorithm for learn-
ing entailment relations between predi-
cates. We define a graph structure over
predicates that represents entailment rela-
tions as directed edges, and use a global
transitivity constraint on the graph to learn
the optimal set of edges, by formulating
the optimization problem as an Integer
Linear Program. We motivate this graph
with an application that provides a hierar-
chical summary for a set of propositions
that focus on a target concept, and show
that our global algorithm improves perfor-
mance by more than 10% over baseline al-
gorithms.
1 Introduction
The Textual Entailment (TE) paradigm (Dagan et
al., 2009) is a generic framework for applied se-
mantic inference. The objective of TE is to recog-
nize whether a target meaning can be inferred from
a given text. For example, a Question Answer-
ing system has to recognize that ?alcohol affects
blood pressure? is inferred from ?alcohol reduces
blood pressure? to answer the question ?What af-
fects blood pressure??
TE systems require extensive knowledge of en-
tailment patterns, often captured as entailment
rules: rules that specify a directional inference re-
lation between two text fragments (when the rule
is bidirectional this is known as paraphrasing). An
important type of entailment rule refers to propo-
sitional templates, i.e., propositions comprising
a predicate and arguments, possibly replaced by
variables. The rule required for the previous ex-
ample would be ?X reduce Y ? X affect Y?. Be-
cause facts and knowledge are mostly expressed
by propositions, such entailment rules are central
to the TE task. This has led to active research
on broad-scale acquisition of entailment rules for
predicates, e.g. (Lin and Pantel, 2001; Sekine,
2005; Szpektor and Dagan, 2008).
Previous work has focused on learning each en-
tailment rule in isolation. However, it is clear that
there are interactions between rules. A prominent
example is that entailment is a transitive relation,
and thus the rules ?X ? Y ? and ?Y ? Z? imply
the rule ?X ? Z?. In this paper we take advantage
of these global interactions to improve entailment
rule learning.
First, we describe a structure termed an entail-
ment graph that models entailment relations be-
tween propositional templates (Section 3). Next,
we show that we can present propositions accord-
ing to an entailment hierarchy derived from the
graph, and suggest a novel hierarchical presenta-
tion scheme for corpus propositions referring to a
target concept. As in this application each graph
focuses on a single concept, we term those focused
entailment graphs (Section 4).
In the core section of the paper, we present an
algorithm that uses a global approach to learn the
entailment relations of focused entailment graphs
(Section 5). We define a global function and look
for the graph that maximizes that function under
a transitivity constraint. The optimization prob-
lem is formulated as an Integer Linear Program
(ILP) and solved with an ILP solver. We show that
this leads to an optimal solution with respect to
the global function, and demonstrate that the algo-
rithm outperforms methods that utilize only local
information by more than 10%, as well as meth-
ods that employ a greedy optimization algorithm
rather than an ILP solver (Section 6).
2 Background
Entailment learning Two information types have
primarily been utilized to learn entailment rules
between predicates: lexicographic resources and
distributional similarity resources. Lexicographic
1220
resources are manually-prepared knowledge bases
containing information about semantic relations
between lexical items. WordNet (Fellbaum,
1998), by far the most widely used resource, spec-
ifies relations such as hyponymy, derivation, and
entailment that can be used for semantic inference
(Budanitsky and Hirst, 2006). WordNet has also
been exploited to automatically generate a training
set for a hyponym classifier (Snow et al, 2005),
and we make a similar use of WordNet in Section
5.1.
Lexicographic resources are accurate but tend
to have low coverage. Therefore, distributional
similarity is used to learn broad-scale resources.
Distributional similarity algorithms predict a se-
mantic relation between two predicates by com-
paring the arguments with which they occur. Quite
a few methods have been suggested (Lin and Pan-
tel, 2001; Bhagat et al, 2007; Yates and Etzioni,
2009), which differ in terms of the specifics of the
ways in which predicates are represented, the fea-
tures that are extracted, and the function used to
compute feature vector similarity. Details on such
methods are given in Section 5.1.
Global learning It is natural to describe en-
tailment relations between predicates by a graph.
Nodes represent predicates, and edges represent
entailment between nodes. Nevertheless, using a
graph for global learning of entailment between
predicates has attracted little attention. Recently,
Szpektor and Dagan (2009) presented the resource
Argument-mapped WordNet, providing entailment
relations for predicates in WordNet. Their re-
source was built on top of WordNet, and makes
simple use of WordNet?s global graph structure:
new rules are suggested by transitively chaining
graph edges, and verified against corpus statistics.
The most similar work to ours is Snow et al?s al-
gorithm for taxonomy induction (2006). Snow et
al.?s algorithm learns the hyponymy relation, un-
der the constraint that it is a transitive relation.
Their algorithm incrementally adds hyponyms to
an existing taxonomy (WordNet), using a greedy
search algorithm that adds at each step the set of
hyponyms that maximize the probability of the ev-
idence while respecting the transitivity constraint.
In this paper we tackle a similar problem of
learning a transitive relation, but we use linear pro-
gramming. A Linear Program (LP) is an optimiza-
tion problem, where a linear function is minimized
(or maximized) under linear constraints. If the
variables are integers, the problem is termed an In-
teger Linear Program (ILP). Linear programming
has attracted attention recently in several fields of
NLP, such as semantic role labeling, summariza-
tion and parsing (Roth and tau Yih, 2005; Clarke
and Lapata, 2008; Martins et al, 2009). In this
paper we formulate the entailment graph learning
problem as an Integer Linear Program, and find
that this leads to an optimal solution with respect
to the target function in our experiment.
3 Entailment Graph
This section presents an entailment graph struc-
ture, which resembles the graph in (Szpektor and
Dagan, 2009).
The nodes of an entailment graph are propo-
sitional templates. A propositional template is a
path in a dependency tree between two arguments
of a common predicate1 (Lin and Pantel, 2001;
Szpektor and Dagan, 2008). Note that in a de-
pendency parse, such a path passes through the
predicate. We require that a variable appears in at
least one of the argument positions, and that each
sense of a polysemous predicate corresponds to a
separate template (and a separate graph node): X
subj
??? treat#1
obj
??? Y and X
subj
??? treat#1
obj
??? nau-
sea are propositional templates for the first sense
of the predicate treat. An edge (u, v) represents
the fact that template u entails template v. Note
that the entailment relation transcends beyond hy-
ponymy. For example, the template X is diagnosed
with asthma entails the template X suffers from
asthma, although one is not a hyponoym of the
other. An example of an entailment graph is given
in Figure 1, left.
Since entailment is a transitive relation, an en-
tailment graph is transitive, i.e., if the edges (u, v)
and (v, w) are in the graph, so is the edge (u,w).
This is why we require that nodes be sense-
specified, as otherwise transitivity does not hold:
Possibly a ? b for one sense of b, b ? c for an-
other sense of b, but a9 c.
Because graph nodes represent propositions,
which generally have a clear truth value, we can
assume that transitivity is indeed maintained along
paths of any length in an entailment graph, as en-
tailment between each pair of nodes either occurs
or doesn?t occur with very high probability. We
support this further in section 4.1, where we show
1We restrict our discussion to templates with two argu-
ments, but generalization is straightforward.
1221
X-related-to-nausea X-associated-with-nauseaX-prevent-nausea X-help-with-nauseaX-reduce-nausea X-treat-nausea
related to nauseaheadacheOxicontine
help with nausea
prevent nausea
acupuncture
ginger
reduce nausearelaxationtreat nauseadrugsNabiloneLorazepam
Figure 1: Left: An entailment graph. For clarity, edges that can be inferred by transitivity are omitted. Right: A hierarchical
summary of propositions involving nausea as an argument, such as headache is related to nausea, acupuncture helps with
nausea, and Lorazepam treats nausea.
that in our experimental setting the length of paths
in the entailment graph is relatively small.
Transitivity implies that in each strong connec-
tivity component2 of the graph, all nodes are syn-
onymous. Moreover, if we merge every strong
connectivity component to a single node, the
graph becomes a Directed Acyclic Graph (DAG),
and the graph nodes can be sorted and presented
hierarchically. Next, we show an application that
leverages this property.
4 Motivating Application
In this section we propose an application that pro-
vides a hierarchical view of propositions extracted
from a corpus, based on an entailment graph.
Organizing information in large collections has
been found to be useful for effective information
access (Kaki, 2005; Stoica et al, 2007). It allows
for easier data exploration, and provides a compact
view of the underlying content. A simple form of
structural presentation is by a single hierarchy, e.g.
(Hofmann, 1999). A more complex approach is
hierarchical faceted metadata, where a number of
concept hierarchies are created, corresponding to
different facets or dimensions (Stoica et al, 2007).
Hierarchical faceted metadata categorizes con-
cepts of a domain in several dimensions, but does
not specify the relations between them. For ex-
ample, in the health-care domain we might have
facets for categories such as diseases and symp-
toms. Thus, when querying about nausea, one
might find it is related to vomitting and chicken
pox, but not that chicken pox is a cause of nausea,
2A strong connectivity component is a subset of nodes in
the graph where there is a path from any node to any other
node.
while nausea is often accompanied by vomitting.
We suggest that the prominent information
in a text lies in the propositions it contains,
which specify particular relations between the
concepts. Propositions have been mostly pre-
sented through unstructured textual summaries or
manually-constructed ontologies, which are ex-
pensive to build. We propose using the entail-
ment graph structure, which describes entailment
relations between predicates, to naturally present
propositions hierarchically. That is, the entailment
hierarchy can be used as an additional facet, which
can improve navigation and provide a compact hi-
erarchical summary of the propositions.
Figure 1 illustrates a scenario, on which we
evaluate later our learning algorithm. Assume a
user would like to retrieve information about a tar-
get concept such as nausea. We can extract the set
of propositions where nausea is an argument auto-
matically from a corprus, and learn an entailment
graph over propositional templates derived from
the extracted propositions, as illustrated in Figure
1, left. Then, we follow the steps in the process
described in Section 3: merge synonymous nodes
that are in the same strong connectivity compo-
nent, and turn the resulting DAG into a predicate
hierarchy, which we can then use to present the
propositions (Figure 1, right). Note that in all
propositional templates one argument is the tar-
get concept (nausea), and the other is a variable
whose corpus instantiations can be presented ac-
cording to another hierarchy (e.g. Nabilone and
Lorazepam are types of drugs).
Moreover, new propositions are inferred from
the graph by transitivity. For example, from the
proposition ?relaxation reduces nausea? we can in-
1222
fer the proposition ?relaxation helps with nausea?.
4.1 Focused entailment graphs
The application presented above generates entail-
ment graphs of a specific form: (1) Propositional
templates have exactly one argument instantiated
by the same entity (e.g. nausea). (2) The predicate
sense is unspecified, but due to the rather small
number of nodes and the instantiating argument,
each predicate corresponds to a unique sense.
Generalizing this notion, we define a focused
entailment graph to be an entailment graph where
the number of nodes is relatively small (and con-
sequently paths in the graph are short), and predi-
cates have a single sense (so transitivity is main-
tained without sense specification). Section 5
presents an algorithm that given the set of nodes
of a focused entailment graph learns its edges, i.e.,
the entailment relations between all pairs of nodes.
The algorithm is evaluated in Section 6 using our
proposed application. For brevity, from now on
the term entailment graph will stand for focused
entailment graph.
5 Learning Entailment Graph Edges
In this section we present an algorithm for learn-
ing the edges of an entailment graph given its set
of nodes. The first step is preprocessing: We use
a large corpus and WordNet to train an entail-
ment classifier that estimates the likelihood that
one propositional template entails another. Next,
we can learn on the fly for any input graph: given
the graph nodes, we employ a global optimiza-
tion approach that determines the set of edges that
maximizes the probability (or score) of the entire
graph, given the edge probabilities (or scores) sup-
plied by the entailment classifier and the graph
constraints (transitivity and others).
5.1 Training an entailment classifier
We describe a procedure for learning an entail-
ment classifier, given a corpus and a lexicographic
resource (WordNet). First, we extract a large set of
propositional templates from the corpus. Next, we
represent each pair of propositional templates with
a feature vector of various distributional similar-
ity scores. Last, we use WordNet to automatically
generate a training set and train a classifier.
Template extraction We parse the corpus with
a dependency parser and extract all propositional
templates from every parse tree, employing the
procedure used by Lin and Pantel (2001). How-
ever, we only consider templates containing a
predicate term and arguments3. The arguments are
replaced with variables, resulting in propositional
templates such as X
subj
??? affect
obj
??? Y.
Distributional similarity representation We
aim to train a classifier that for an input template
pair (t1, t2) determines whether t1 entails t2. A
template pair is represented by a feature vector
where each coordinate is a different distributional
similarity score. There are a myriad of distribu-
tional similarity algorithms. We briefly describe
those used in this paper, obtained through varia-
tions along the following dimensions:
Predicate representation Most algorithms mea-
sure the similarity between templates with two
variables (binary templates) such as X
subj
??? af-
fect
obj
??? Y (Lin and Pantel, 2001; Bhagat et al,
2007; Yates and Etzioni, 2009). Szpketor and Da-
gan (2008) suggested learning over templates with
one variable (unary templates) such as X
subj
??? af-
fect, and using them to estimate a score for binary
templates.
Feature representation The features of a tem-
plate are some representation of the terms that in-
stantiated the argument variables in a corpus. Two
representations are used in our experiment (see
Section 6). Another variant occurs when using bi-
nary templates: a template may be represented by
a pair of feature vectors, one for each variable (Lin
and Pantel, 2001), or by a single vector, where fea-
tures represent pairs of instantiations (Szpektor et
al., 2004; Yates and Etzioni, 2009). The former
variant reduces sparsity problems, while Yates and
Etzioni showed the latter is more informative and
performs favorably on their data.
Similarity function We consider two similarity
functions: The Lin (2001) similarity measure, and
the Balanced Inclusion (BInc) similarity measure
(Szpektor and Dagan, 2008). The former is a
symmetric measure and the latter is asymmetric.
Therefore, information about the direction of en-
tailment is provided by the BInc measure.
We then generate for any (t1, t2) features that
are the 12 distributional similarity scores using all
combinations of the dimensions. This is reminis-
cent of Connor and Roth (2007), who used the out-
put of unsupervised classifiers as features for a su-
pervised classifier in a verb disambiguation task.
3Via a simple heuristic, omitted due to space limitations
1223
Training set generation Following the spirit of
Snow et al (2005), WordNet is used to automati-
cally generate a training set of positive (entailing)
and negative (non-entailing) template pairs. Let
T be the set of propositional templates extracted
from the corpus. For each ti ? T with two vari-
ables and a single predicate word w, we extract
from WordNet the set H of direct hypernyms and
synonyms of w. For every h ? H , we generate a
new template tj from ti by replacing w with h. If
tj ? T , we consider (ti, tj) to be a positive exam-
ple. Negative examples are generated analogously,
by looking at direct co-hyponyms of w instead of
hypernyms and synonyms. This follows the no-
tion of ?contrastive estimation? (Smith and Eisner,
2005), since we generate negative examples that
are semantically similar to positive examples and
thus focus the classifier?s attention on identifying
the boundary between the classes. Last, we filter
training examples for which all features are zero,
and sample an equal number of positive and neg-
ative examples (for which we compute similarity
features), since classifiers tend to perform poorly
on the minority class when trained on imbalanced
data (Van Hulse et al, 2007; Nikulin, 2008).
5.2 Global learning of edges
Once the entailment classifier is trained we learn
the graph edges given its nodes. This is equiv-
alent to learning all entailment relations between
all propositional template pairs for that graph.
To learn edges we consider global constraints,
which allow only certain graph topologies. Since
we seek a global solution under transitivity and
other constraints, linear programming is a natural
choice, enabling the use of state of the art opti-
mization packages. We describe two formulations
of integer linear programs that learn the edges: one
maximizing a global score function, and another
maximizing a global probability function.
Let Iuv be an indicator denoting the event that
node u entails node v. Our goal is to learn the
edges E over a set of nodes V . We start by formu-
lating the constraints and then the target functions.
The first constraint is that the graph must re-
spect transitivity. Our formulation is equivalent to
the one suggested by Finkel and Manning (2008)
in a coreference resolution task:
?u,v,w?V Iuv + Ivw ? Iuw ? 1
In addition, for a few pairs of nodes we have
strong evidence that one does not entail the other
and so we add the constraint Iuv = 0. Combined
with the constraint of transitivity this implies that
there must be no path from u to v. This is done in
the following two scenarios: (1) When two nodes
u and v are identical except for a pair of words wu
and wv, and wu is an antonym of wv, or a hyper-
nym of wv at distance ? 2. (2) When two nodes
u and v are transitive opposites, that is, if u =
X
subj
??? w
obj
??? Y and v = X
obj
??? w
subj
??? Y ,
for any word w4.
Score-based target function We assume an en-
tailment classifier estimating a positive score Suv
if it believes Iuv = 1 and a negative score other-
wise (for example, an SVM classifier). We look
for a graph G that maximizes the sum of scores
over the edges:
G? = argmax
G
S(G)
= argmax
G
?
?
?
u6=v
SuvIuv
?
?? ?|E|
where ?|E| is a regularization term reflecting
the fact that edges are sparse. Note that this con-
stant needs to be optimized on a development set.
Probabilistic target function Let Fuv be the
features for the pair of nodes (u, v) and F =
?u6=vFuv. We assume an entailment classifier es-
timating the probability of an edge given its fea-
tures: Puv = P (Iuv = 1|Fuv). We look for the
graph G that maximizes the posterior probability
P (G|F ):
G? = argmax
G
P (G|F )
Following Snow et al, we make two inde-
pendence assumptions: First, we assume each
set of features Fuv is independent of other sets
of features given the graph G, i.e., P (F |G) =
?
u6=v P (Fuv|G). Second, we assume the features
for the pair (u, v) are generated by a distribution
depending only on whether entailment holds for
(u, v). Thus, P (Fuv|G) = P (Fuv|Iuv). Last,
for simplicity we assume edges are independent
and the prior probability of a graph is a product
of the prior probabilities of the edge indicators:
4We note that in some rare cases transitive verbs are in-
deed reciprocal, as in ?X marry Y?, but in the grand ma-
jority of cases reciprocal activities are not expressed using
a transitive-verb structure.
1224
P (G) =
?
u6=v P (Iuv). Note that although we
assume edges are independent, dependency is still
expressed using the transitivity constraint. We ex-
press P (G|F ) using the assumptions above and
Bayes rule:
P (G|F ) ? P (G)P (F |G)
=
?
u6=v
[P (Iuv)P (Fuv|Iuv)]
=
?
u6=v
P (Iuv)
P (Iuv|Fuv)P (Fuv)
P (Iuv)
?
?
u6=v
P (Iuv|Fuv)
=
?
(u,v)?E
Puv ?
?
(u,v)/?E
(1? Puv)
Note that the prior P (Fuv) is constant with re-
spect to the graph. Now we look for the graph that
maximizes logP (G|F ):
G? = argmax
G
?
(u,v)?E
logPuv +
?
(u,v)/?E
log(1? Puv)
= argmax
G
?
u6=v
[Iuv ? logPuv
+ (1? Iuv) ? log(1? Puv)]
= argmax
G
?
u6=v
log
Puv
1? Puv
? Iuv
(in the last transition we omit the constant
?
u6=v log(1?Puv)). Importantly, while the score-
based formulation contains a parameter ? that re-
quires optimization, this probabilistic formulation
is parameter free and does not utilize a develop-
ment set at all.
Since the variables are binary, both formula-
tions are integer linear programs with O(|V |2)
variables and O(|V |3) transitivity constraints that
can be solved using standard ILP packages.
Our work resembles Snow et al?s in that both
try to learn graph edges given a transitivity con-
straint. However, there are two key differences
in the model and in the optimization algorithm.
First, Snow et al?s model attempts to determine
the graph that maximizes the likelihood P (F |G)
and not the posterior P (G|F ). Therefore, their
model contains an edge prior P (Iuv) that has to
be estimated, whereas in our model it cancels out.
Second, they incrementally add hyponyms to a
large taxonomy (WordNet) and therefore utilize a
greedy algorithm, while we simultaneously learn
all edges of a rather small graph and employ in-
teger linear programming, which is more sound
theoretically, and as shown in Section 6, leads to
an optimal solution. Nevertheless, Snow et al?s
model can also be formulated as a linear program
with the following target function:
argmax
G
?
u6=v
log
Puv ? P (Iuv = 0)
(1? Puv) ? P (Iuv = 1)
Iuv
Note that if the prior inverse odds k =
P (Iuv=0)
P (Iuv=1)
= 1, i.e., P (Iuv = 1) = 0.5, then
this is equivalent to our probabilistic formulation.
We implemented Snow et als model and optimiza-
tion algorithm and in Section 6.3 we compare our
model and optimization algorithm to theirs.
6 Experimental Evaluation
This section presents our evaluation, which is
geared for the application proposed in Section 4.
6.1 Experimental setting
A health-care corpus of 632MB was harvested
from the web and parsed with the Minipar parser
(Lin, 1998). The corpus contains 2,307,585
sentences and almost 50 million word tokens.
We used the Unified Medical Language System
(UMLS)5 to annotate medical concepts in the cor-
pus. The UMLS is a database that maps nat-
ural language phrases to over one million con-
cept identifiers in the health-care domain (termed
CUIs). We annotated all nouns and noun phrases
that are in the UMLS with their possibly multi-
ple CUIs. We extracted all propositional templates
from the corpus, where both argument instantia-
tions are medical concepts, i.e., annotated with a
CUI (?50,000 templates). When computing dis-
tributional similarity scores, a template is repre-
sented as a feature vector of the CUIs that instan-
tiate its arguments.
To evaluate the performance of our algo-
rithm, we constructed 23 gold standard entailment
graphs. First, 23 medical concepts, representing
typical topics of interest in the medical domain,
were manually selected from a list of the most fre-
quent concepts in the corpus. For each concept,
nodes were defined by extracting all propositional
5http://www.nlm.nih.gov/research/umls
1225
Using a development set Not using a development set
Edges Propositions Edges Propositions
R P F1 R P F1 R P F1 R P F1
LP 46.0 50.1 43.8 67.3 69.6 66.2 48.7 41.9 41.2 67.9 62.0 62.3
Greedy 45.7 37.1 36.6 64.2 57.2 56.3 48.2 41.7 41.0 67.8 62.0 62.4
Local-LP 44.5 45.3 38.1 65.2 61.0 58.6 69.3 19.7 26.8 82.7 33.3 42.6
Local1 53.5 34.9 37.5 73.5 50.6 56.1 92.9 11.1 19.7 95.4 18.6 30.6
Local2 52.5 31.6 37.7 69.8 50.0 57.1 63.2 24.9 33.6 77.7 39.3 50.5
Local?1 53.5 38.0 39.8 73.5 54.6 59.1 92.6 11.3 20.0 95.3 18.9 31.1
Local?2 52.5 32.1 38.1 69.8 50.6 57.4 63.1 25.5 34.0 77.7 39.9 50.9
WordNet - - - - - - 10.8 44.1 13.2 39.9 72.4 47.3
Table 1: Results for all experiments
templates for which the target concept instanti-
ated an argument at least K(= 3) times (average
number of graph nodes=22.04, std=3.66, max=26,
min=13).
Ten medical students constructed the gold stan-
dard of graph edges. Each concept graph was
annotated by two students. Following RTE-5
practice (Bentivogli et al, 2009), after initial an-
notation the two students met for a reconcili-
ation phase. They worked to reach an agree-
ment on differences and corrected their graphs.
Inter-annotator agreement was calculated using
the Kappa statistic (Siegel and Castellan, 1988)
both before (? = 0.59) and after (? = 0.9) rec-
onciliation. 882 edges were included in the 23
graphs out of a possible 10,364, providing a suf-
ficiently large data set. The graphs were randomly
split into a development set (11 graphs) and a test
set (12 graphs)6. The entailment graph fragment
in Figure 1 is from the gold standard.
The graphs learned by our algorithm were eval-
uated by two measures, one evaluating the graph
directly, and the other motivated by our applica-
tion: (1) F1 of the learned edges compared to the
gold standard edges (2) Our application provides
a summary of propositions extracted from the cor-
pus. Note that we infer new propositions by prop-
agating inference transitively through the graph.
Thus, we compute F1 for the set of propositions
inferred from the learned graph, compared to the
set inferred based on the gold standard graph. For
example, given the proposition from the corpus
?relaxation reduces nausea? and the edge ?X re-
duce nausea? X help with nausea?, we evaluate
the set {?relaxation reduces nausea?, ?relaxation
helps with nausea?}. The final score for an algo-
rithm is a macro-average over the 12 graphs of the
6Test set concepts were: asthma, chemotherapy, diarrhea,
FDA, headache, HPV, lungs, mouth, salmonella, seizure,
smoking and X-ray.
test set.
6.2 Evaluated algorithms
Local algorithms We described 12 distributional
similarity measures computed over our corpus
(Section 5.1). For each measure we computed for
each template t a list of templates most similar to
t (or entailing t for directional measures). In ad-
dition, we obtained similarity lists learned by Lin
and Pantel (2001), and replicated 3 similarity mea-
sures learned by Szpektor and Dagan (2008), over
the RCV1 corpus7. For each distributional similar-
ity measure (altogether 16 measures), we learned a
graph by inserting any edge (u, v), when u is in the
top K templates most similar to v. We also omit-
ted edges for which there was strong evidence that
they do not exist, as specified by the constraints
in Section 5.2. Another local resource was Word-
Net where we inserted an edge (u, v) when v was
a direct hypernym or synonym of u. For all algo-
rithms, we added all edges inferred by transitivity.
Global algorithms We experimented with all
6 combinations of the following two dimensions:
(1) Target functions: score-based, probabilistic
and Snow et al?s (2) Optimization algorithms:
Snow et al?s greedy algorithm and a standard ILP
solver. A training set of 20,144 examples was au-
tomatically generated, each example represented
by 16 features using the distributional similarity
measures mentioned above. SVMperf (Joachims,
2005) was used to train an SVM classifier yield-
ing Suv, and the SMO classifier from WEKA (Hall
et al, 2009) estimated Puv. We used the lpsolve8
package to solve the linear programs. In all re-
sults, the relaxation ?u,v0 ? Iuv ? 1 was used,
which guarantees an optimal output solution. In
7http://trec.nist.gov/data/reuters/reuters.html. The simi-
larity lists were computed using: (1) Unary templates and
the Lin function (2) Unary templates and the BInc function
(3) Binary templates and the Lin function
8http://lpsolve.sourceforge.net/5.5/
1226
Global=T/Local=F Global=F/Local=T
GS= T 50 143
GS= F 140 1087
Table 2: Comparing disagreements between the best local
and global algorithms against the gold standard
all experiments the output solution was integer,
and therefore it is optimal. Constructing graph
nodes and learning its edges given an input con-
cept took 2-3 seconds on a standard desktop.
6.3 Results and analysis
Table 1 summarizes the results of the algorithms.
The left half depicts methods where the develop-
ment set was needed to tune parameters, and the
right half depicts methods that do not require a
(manually created) development set at all. Hence,
our score-based LP (tuned-LP), where the param-
eter ? is tuned, is on the left, and the probabilis-
tic LP (untuned-LP) is on the right. The row
Greedy is achieved by using the greedy algorithm
instead of lpsolve. The row Local-LP is achieved
by omitting global transitivity constraints, making
the algorithm completely local. We omit Snow et
al.?s formulation, since the optimal prior inverse
odds k was almost exactly 1, which conflates with
untuned-LP.
The rows Local1 and Local2 present the best
distributional similarity resources. Local1 is
achieved using binary templates, the Lin function,
and a single vector with feature pairs. Local2 is
identical but employs the BInc function. Local?1
and Local?2 also exploit the local constraints men-
tioned above. Results on the left were achieved
by optimizing the top-K parameter on the devel-
opment set, and on the right by optimizing on the
training set automatically generated from Word-
Net.
The global methods clearly outperform local
methods: Tuned-LP outperforms significantly all
local methods that require a development set both
on the edges F1 measure (p<.05) and on the
propositions F1 measure (p<.01)9. The untuned-
LP algorithm also significantly outperforms all lo-
cal methods that do not require a development
set on the edges F1 measure (p<.05) and on
the propositions F1 measure (p<.01). Omitting
the global transitivity constraints decreases perfor-
mance, as shown by Local-LP. Last, local meth-
9We tested significance using the two-sided Wilcoxon
rank test (Wilcoxon, 1945)
Global
X-treat-headache
X-prevent-headache
X-reduce-headache
X-report-headache
X-suffer-from-headache
X-experience-headache
Figure 2: Subgraph of tuned-LP output for ?headache?
Global
X-treat-headache
X-prevent-headache
X-reduce-headache
X-report-headache
X-suffer-from-headache
X-experience-headache
Figure 3: Subgraph of Local?1 output for?headache?
ods are sensitive to parameter tuning and in the
absence of a development set their performance
dramatically deteriorates.
To further establish the merits of global algo-
rithms, we compare (Table 2) tuned-LP, the best
global algorithm, with Local?1, the best local al-
gorithm. The table considers all edges where the
two algorithms disagree, and counts how many
are in the gold standard and how many are not.
Clearly, tuned-LP is superior at avoiding wrong
edges (false positives). This is because tuned-
LP refrains from adding edges that subsequently
induce many undesirable edges through transitiv-
ity. Figures 2 and 3 illustrate this by compar-
ing tuned-LP and Local?1 on a subgraph of the
Headache concept, before adding missing edges
to satisfy transitivity to Local?1 . Note that Local
?
1
inserts a single wrong edge X-report-headache?
X-prevent-headache, which leads to adding 8 more
wrong edges. This is the type of global considera-
tion that is addressed in an ILP formulation, but is
ignored in a local approach and often overlooked
when employing a greedy algorithm. Figure 2 also
illustrates the utility of a local entailment graph for
information presentation. Presenting information
according to this subgraph distinguishes between
propositions dealing with headache treatments and
1227
propositions dealing with headache risk groups.
Comparing our use of an ILP algorithm to
the greedy one reveals that tuned-LP significantly
outperforms its greedy counterpart on both mea-
sures (p<.01). However, untuned-LP is practically
equivalent to its greedy counterpart. This indicates
that in this experiment the greedy algorithm pro-
vides a good approximation for the optimal solu-
tion achieved by our LP formulation.
Last, when comparing WordNet to local distri-
butional similarity methods, we observe low recall
and high precision, as expected. However, global
methods achieve much higher recall than WordNet
while maintaining comparable precision.
The results clearly demonstrate that a global ap-
proach improves performance on the entailment
graph learning task, and the overall advantage of
employing an ILP solver rather than a greedy al-
gorithm.
7 Conclusion
This paper presented a global optimization algo-
rithm for learning entailment relations between
predicates represented as propositional templates.
We modeled the problem as a graph learning prob-
lem, and searched for the best graph under a global
transitivity constraint. We used Integer Linear
Programming to solve the optimization problem,
which is theoretically sound, and demonstrated
empirically that this method outperforms local al-
gorithms as well as a greedy optimization algo-
rithm on the graph learning task.
Currently, we are investigating a generalization
of our probabilistic formulation that includes a
prior on the edges, and the relation of this prior
to the regularization term introduced in our score-
based formulation. In future work, we would like
to learn general entailment graphs over a large
number of nodes. This will introduce a challenge
to our current optimization algorithm due to com-
plexity issues, and will require careful handling of
predicate ambiguity. Additionally, we will inves-
tigate novel features for the entailment classifier.
This paper used distributional similarity, but other
sources of information are likely to improve per-
formance further.
Acknowledgments
We would like to thank Roy Bar-Haim, David
Carmel and the anonymous reviewers for their
useful comments. We also thank Dafna Berant
and the nine students who prepared the gold stan-
dard data set. This work was developed under
the collaboration of FBK-irst/University of Haifa
and was partially supported by the Israel Science
Foundation grant 1112/08. The first author is
grateful to the Azrieli Foundation for the award of
an Azrieli Fellowship, and has carried out this re-
search in partial fulllment of the requirements for
the Ph.D. degree.
References
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernarde Magnini. 2009. The
fifth Pascal recognizing textual entailment chal-
lenge. In Proceedings of TAC-09.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
LEDIR: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of
EMNLP-CoNLL.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. Journal of Artificial Intelli-
gence Research, 31:273?381.
Michael Connor and Dan Roth. 2007. Context sensi-
tive paraphrasing with a single unsupervised classi-
fier. In Proceedings of ECML.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Natural Language
Engineering, 15(4):1?17.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
Proceedings of ACL-08: HLT, Short Papers.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explorations, 11(1).
Thomas Hofmann. 1999. The cluster-abstraction
model: Unsupervised learning of topic hierarchies
from text data. In Proceedings of IJCAI.
Thorsten Joachims. 2005. A support vector method for
multivariate performance measures. In Proceedings
of ICML.
1228
Mika Kaki. 2005. Findex: Search results categories
help users when document ranking fails. In Pro-
ceedings of CHI.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of
Minipar. In Proceedings of the Workshop on Evalu-
ation of Parsing Systems at LREC.
Andre Martins, Noah Smith, and Eric Xing. 2009.
Concise integer linear programming formulations
for dependency parsing. In Proceedings of ACL.
Vladimir Nikulin. 2008. Classification of imbalanced
data with random sets and mean-variance filtering.
IJDWM, 4(2):63?78.
Dan Roth and Wen tau Yih. 2005. Integer linear pro-
gramming inference for conditional random fields.
In Proceedings of ICML, pages 737?744.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Sideny Siegel and N. John Castellan. 1988. Non-
parametric Statistics for the Behavioral Sciences.
McGraw-Hill, New-York.
Noah Smith and Jason Eisner. 2005. Contrastive es-
timation: Training log-linear models on unlabeled
data. In Proceedings of ACL.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Proceedings of NIPS.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of ACL.
Emilia Stoica, Marti Hearst, and Megan Richardson.
2007. Automating creation of hierarchical faceted
metadata structures. In Proceedings of NAACL-
HLT.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Idan Szpektor and Ido Dagan. 2009. Augmenting
wordnet-based inference with argument mapping.
In Proceedings of TextInfer-2009.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. In Proceedings of EMNLP.
Jason Van Hulse, Taghi Khoshgoftaar, and Amri
Napolitano. 2007. Experimental perspectives on
learning from imbalanced data. In Proceedings of
ICML.
Frank Wilcoxon. 1945. Individual comparisons by
ranking methods. Biometrics Bulletin, 1:80?83.
Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised methods for determining object and relation
synonyms on the web. Journal of Artificial Intelli-
gence Research, 34:255?296.
1229
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 610?619,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Global Learning of Typed Entailment Rules
Jonathan Berant
Tel Aviv University
Tel Aviv, Israel
jonatha6@post.tau.ac.il
Ido Dagan
Bar-Ilan University
Ramat-Gan, Israel
dagan@cs.biu.ac.il
Jacob Goldberger
Bar-Ilan University
Ramat-Gan, Israel
goldbej@eng.biu.ac.il
Abstract
Extensive knowledge bases of entailment rules
between predicates are crucial for applied se-
mantic inference. In this paper we propose an
algorithm that utilizes transitivity constraints
to learn a globally-optimal set of entailment
rules for typed predicates. We model the task
as a graph learning problem and suggest meth-
ods that scale the algorithm to larger graphs.
We apply the algorithm over a large data set
of extracted predicate instances, from which a
resource of typed entailment rules has been re-
cently released (Schoenmackers et al, 2010).
Our results show that using global transitiv-
ity information substantially improves perfor-
mance over this resource and several base-
lines, and that our scaling methods allow us
to increase the scope of global learning of
entailment-rule graphs.
1 Introduction
Generic approaches for applied semantic infer-
ence from text gained growing attention in recent
years, particularly under the Textual Entailment
(TE) framework (Dagan et al, 2009). TE is a
generic paradigm for semantic inference, where the
objective is to recognize whether a target meaning
can be inferred from a given text. A crucial com-
ponent of inference systems is extensive resources
of entailment rules, also known as inference rules,
i.e., rules that specify a directional inference rela-
tion between fragments of text. One important type
of rule is rules that specify entailment relations be-
tween predicates and their arguments. For example,
the rule ?X annex Y? X control Y? helps recognize
that the text ?Japan annexed Okinawa? answers the
question ?Which country controls Okinawa??. Thus,
acquisition of such knowledge received considerable
attention in the last decade (Lin and Pantel, 2001;
Sekine, 2005; Szpektor and Dagan, 2009; Schoen-
mackers et al, 2010).
Most past work took a ?local learning? approach,
learning each entailment rule independently of oth-
ers. It is clear though, that there are global inter-
actions between predicates. Notably, entailment is
a transitive relation and so the rules A ? B and
B ? C imply A? C.
Recently, Berant et al (2010) proposed a global
graph optimization procedure that uses Integer Lin-
ear Programming (ILP) to find the best set of entail-
ment rules under a transitivity constraint. Imposing
this constraint raised two challenges. The first of
ambiguity: transitivity does not always hold when
predicates are ambiguous, e.g., X buy Y? X acquire
Y and X acquire Y ? X learn Y, but X buy Y 9 X
learn Y since these two rules correspond to two dif-
ferent senses of acquire. The second challenge is
scalability: ILP solvers do not scale well since ILP
is an NP-complete problem. Berant et al circum-
vented these issues by learning rules where one of
the predicate?s arguments is instantiated (e.g., ?X re-
duce nausea? X affect nausea?), which is useful for
learning small graphs on-the-fly, given a target con-
cept such as nausea. While rules may be effectively
learned when needed, their scope is narrow and they
are not useful as a generic knowledge resource.
This paper aims to take global rule learning one
step further. To this end, we adopt the represen-
tation suggested by Schoenmackers et al (2010),
who learned inference rules between typed predi-
cates, i.e., predicates where the argument types (e.g.,
city or drug) are specified. Schoenmackers et al uti-
610
lized typed predicates since they were dealing with
noisy and ambiguous web text. Typing predicates
helps disambiguation and filtering of noise, while
still maintaining rules of wide-applicability. Their
method employs a local learning approach, while the
number of predicates in their data is too large to be
handled directly by an ILP solver.
In this paper we suggest applying global opti-
mization learning to open domain typed entailment
rules. To that end, we show how to construct a
structure termed typed entailment graph, where the
nodes are typed predicates and the edges represent
entailment rules. We suggest scaling techniques that
allow to optimally learn such graphs over a large
set of typed predicates by first decomposing nodes
into components and then applying incremental ILP
(Riedel and Clarke, 2006). Using these techniques,
the obtained algorithm is guaranteed to return an op-
timal solution. We ran our algorithm over the data
set of Schoenmackers et al and release a resource
of 30,000 rules1 that achieves substantially higher
recall without harming precision. To the best of our
knowledge, this is the first resource of that scale
to use global optimization for learning predicative
entailment rules. Our evaluation shows that global
transitivity improves the F1 score of rule learning by
27% over several baselines and that our scaling tech-
niques allow dealing with larger graphs, resulting in
improved coverage.
2 Background
Most work on learning entailment rules between
predicates considered each rule independently of
others, using two sources of information: lexico-
graphic resources and distributional similarity.
Lexicographic resources are manually-prepared
knowledge bases containing semantic information
on predicates. A widely-used resource is WordNet
(Fellbaum, 1998), where relations such as synonymy
and hyponymy can be used to generate rules. Other
resources include NomLex (Macleod et al, 1998;
Szpektor and Dagan, 2009) and FrameNet (Baker
and Lowe, 1998; Ben Aharon et al, 2010).
Lexicographic resources are accurate but have
1The resource can be downloaded from
http://www.cs.tau.ac.il/j?onatha6/homepage files/resources
/ACL2011Resource.zip
low coverage. Distributional similarity algorithms
use large corpora to learn broader resources by as-
suming that semantically similar predicates appear
with similar arguments. These algorithms usually
represent a predicate with one or more vectors and
use some function to compute argument similarity.
Distributional similarity algorithms differ in their
feature representation: Some use a binary repre-
sentation: each predicate is represented by one fea-
ture vector where each feature is a pair of argu-
ments (Szpektor et al, 2004; Yates and Etzioni,
2009). This representation performs well, but suf-
fers when data is sparse. The binary-DIRT repre-
sentation deals with sparsity by representing a pred-
icate with a pair of vectors, one for each argument
(Lin and Pantel, 2001). Last, a richer form of repre-
sentation, termed unary, has been suggested where
a different predicate is defined for each argument
(Szpektor and Dagan, 2008). Different algorithms
also differ in their similarity function. Some employ
symmetric functions, geared towards paraphrasing
(bi-directional entailment), while others choose di-
rectional measures more suited for entailment (Bha-
gat et al, 2007). In this paper, We employ several
such functions, such as Lin (Lin and Pantel, 2001),
and BInc (Szpektor and Dagan, 2008).
Schoenmackers et al (2010) recently used dis-
tributional similarity to learn rules between typed
predicates, where the left-hand-side of the rule may
contain more than a single predicate (horn clauses).
In their work, they used Hearst-patterns (Hearst,
1992) to extract a set of 29 million (argument, type)
pairs from a large web crawl. Then, they employed
several filtering methods to clean this set and au-
tomatically produced a mapping of 1.1 million ar-
guments into 156 types. Examples for (argument,
type) pairs are (EXODUS, book), (CHINA, coun-
try) and (ASTHMA, disease). Schoenmackers et
al. then utilized the types, the mapped arguments
and tuples from TextRunner (Banko et al, 2007)
to generate 10,672 typed predicates (such as con-
quer(country,city) and common in(disease,place)),
and learn 30,000 rules between these predicates2. In
this paper we will learn entailment rules over the
same data set, which was generously provided by
2The rules and the mapping of arguments into types can
be downloaded from http://www.cs.washington.edu/research/
sherlock-hornclauses/
611
Schoenmackers et al
As mentioned above, Berant et al (2010) used
global transitivity information to learn small entail-
ment graphs. Transitivity was also used as an in-
formation source in other fields of NLP: Taxonomy
Induction (Snow et al, 2006), Co-reference Reso-
lution (Finkel and Manning, 2008), Temporal Infor-
mation Extraction (Ling and Weld, 2010), and Un-
supervised Ontology Induction (Poon and Domin-
gos, 2010). Our proposed algorithm applies to any
sparse transitive relation, and so might be applicable
in these fields as well.
Last, we formulate our optimization problem as
an Integer Linear Program (ILP). ILP is an optimiza-
tion problem where a linear objective function over
a set of integer variables is maximized under a set of
linear constraints. Scaling ILP is challenging since
it is an NP-complete problem. ILP has been exten-
sively used in NLP lately (Clarke and Lapata, 2008;
Martins et al, 2009; Do and Roth, 2010).
3 Typed Entailment Graphs
Given a set of typed predicates, entailment rules can
only exist between predicates that share the same
(unordered) pair of types (such as place and coun-
try)3. Hence, every pair of types defines a graph
that describes the entailment relations between pred-
icates sharing those types (Figure 1). Next, we show
how to represent entailment rules between typed
predicates in a structure termed typed entailment
graph, which will be the learning goal of our algo-
rithm.
A typed entailment graph is a directed graph
where the nodes are typed predicates. A typed pred-
icate is a triple p(t1, t2) representing a predicate in
natural language. p is the lexical realization of the
predicate and the types t1, t2 are variables repre-
senting argument types. These are taken from a
set of types T , where each type t ? T is a bag
of natural language words or phrases. Examples
for typed predicates are: conquer(country,city) and
contain(product,material). An instance of a typed
predicate is a triple p(a1, a2), where a1 ? t1 and
a2 ? t2 are termed arguments. For example, be
common in(ASTHMA,AUSTRALIA) is an instance of
be common in(disease,place). For brevity, we refer
3Otherwise, the rule would contain unbound variables.
to typed entailment graphs and typed predicates as
entailment graphs and predicates respectively.
Edges in typed entailment graphs represent en-
tailment rules: an edge (u, v) means that predicate
u entails predicate v. If the type t1 is different
from the type t2, mapping of arguments is straight-
forward, as in the rule ?be find in(material,product)
? contain(product,material)?. We term this a two-
types entailment graph. When t1 and t2 are equal,
mapping of arguments is ambiguous: we distin-
guish direct-mapping edges where the first argu-
ment on the left-hand-side (LHS) is mapped to
the first argument on the right-hand-side (RHS),
as in ?beat(team,team)
d
?? defeat(team,team)?, and
reversed-mapping edges where the LHS first argu-
ment is mapped to the RHS second argument, as
in ?beat(team,team)
r
?? lose to(team,team)?. We
term this a single-type entailment graph. Note
that in single-type entailment graphs reversed-
mapping loops are possible as in ?play(team,team)
r
?? play(team,team)?: if team A plays team B, then
team B plays team A.
Since entailment is a transitive relation, typed-
entailment graphs are transitive: if the edges (u, v)
and (v, w) are in the graph so is the edge (u,w).
Note that in single-type entailment graphs one needs
to consider whether mapping of edges is direct or re-
versed: if mapping of both (u, v) and (v, w) is either
direct or reversed, mapping of (u,w) is direct, oth-
erwise it is reversed.
Typing plays an important role in rule transitiv-
ity: if predicates are ambiguous, transitivity does not
necessarily hold. However, typing predicates helps
disambiguate them and so the problem of ambiguity
is greatly reduced.
4 Learning Typed Entailment Graphs
Our learning algorithm is composed of two steps:
(1) Given a set of typed predicates and their in-
stances extracted from a corpus, we train a (local)
entailment classifier that estimates for every pair of
predicates whether one entails the other. (2) Using
the classifier scores we perform global optimization,
i.e., learn the set of edges over the nodes that maxi-
mizes the global score of the graph under transitivity
and background-knowledge constraints.
Section 4.1 describes the local classifier training
612
province of(place,country)
be part of(place,country)
annex(country,place)
invade(country,place)
be relate to(drug,drug)be derive from(drug,drug)
be process from(drug,drug)
be convert into(drug,drug)
Figure 1: Top: A fragment of a two-types entailment
graph. bottom: A fragment of a single-type entailment
graph. Mapping of solid edges is direct and of dashed
edges is reversed.
procedure. Section 4.2 gives an ILP formulation for
the optimization problem. Sections 4.3 and 4.4 pro-
pose scaling techniques that exploit graph sparsity
to optimally solve larger graphs.
4.1 Training an entailment classifier
Similar to the work of Berant et al (2010), we
use ?distant supervision?. Given a lexicographic re-
source (WordNet) and a set of predicates with their
instances, we perform the following three steps (see
Table 1):
1) Training set generation We use WordNet to
generate positive and negative examples, where each
example is a pair of predicates. Let P be the
set of input typed predicates. For every predicate
p(t1, t2) ? P such that p is a single word, we extract
from WordNet the set S of synonyms and direct hy-
pernyms of p. For every p? ? S, if p?(t1, t2) ? P
then p(t1, t2) ? p?(t1, t2) is taken as a positive ex-
ample.
Negative examples are generated in a similar
manner, with direct co-hyponyms of p (sister nodes
in WordNet) and hyponyms at distance 2 instead of
synonyms and direct hypernyms. We also generate
negative examples by randomly sampling pairs of
typed predicates that share the same types.
2) Feature representation Each example pair of
predicates (p1, p2) is represented by a feature vec-
tor, where each feature is a specific distributional
Type example
hyper. beat(team,team)? play(team,team)
syno. reach(team,game)? arrive at(team,game)
cohypo. invade(country,city) 9 bomb(country,city)
hypo. defeat(city,city) 9 eliminate(city,city)
random hold(place,event) 9 win(place,event)
Table 1: Automatically generated training set examples.
similarity score estimating whether p1 entails p2.
We compute 11 distributional similarity scores for
each pair of predicates based on the arguments ap-
pearing in the extracted arguments. The first 6
scores are computed by trying all combinations of
the similarity functions Lin and BInc with the fea-
ture representations unary, binary-DIRT and binary
(see Section 2). The other 5 scores were provided
by Schoenmackers et al (2010) and include SR
(Schoenmackers et al, 2010), LIME (McCreath and
Sharma, 1997), M-estimate (Dzeroski and Brakto,
1992), the standard G-test and a simple implementa-
tion of Cover (Weeds and Weir, 2003). Overall, the
rationale behind this representation is that combin-
ing various scores will yield a better classifier than
each single measure.
3) Training We train over an equal number of
positive and negative examples, as classifiers tend to
perform poorly on the minority class when trained
on imbalanced data (Van Hulse et al, 2007; Nikulin,
2008).
4.2 ILP formulation
Once the classifier is trained, we would like to learn
all edges (entailment rules) of each typed entailment
graph. Given a set of predicates V and an entail-
ment score function f : V ? V ? R derived from
the classifier, we want to find a graph G = (V,E)
that respects transitivity and maximizes the sum of
edge weights
?
(u,v)?E f(u, v). This problem is
NP-hard by a reduction from the NP-hard Transitive
Subgraph problem (Yannakakis, 1978). Thus, em-
ploying ILP is an appealing approach for obtaining
an optimal solution.
For two-types entailment graphs the formulation
is simple: The ILP variables are indicators Xuv de-
noting whether an edge (u, v) is in the graph, with
the following ILP:
613
G? = argmax
?
u6=v
f(u, v) ?Xuv (1)
s.t. ?u,v,w?V Xuv +Xvw ?Xuw ? 1 (2)
?u,v?Ayes Xuv = 1 (3)
?u,v?Ano Xuv = 0 (4)
?u6=v Xuv ? {0, 1} (5)
The objective in Eq. 1 is a sum over the weights
of the eventual edges. The constraint in Eq. 2 states
that edges must respect transitivity. The constraints
in Eq. 3 and 4 state that for known node pairs, de-
fined by Ayes and Ano, we have background knowl-
edge indicating whether entailment holds or not. We
elaborate on how Ayes and Ano were constructed in
Section 5. For a graph with n nodes we get n(n?1)
variables and n(n?1)(n?2) transitivity constraints.
The simplest way to expand this formulation for
single-type graphs is to duplicate each predicate
node, with one node for each order of the types, and
then the ILP is unchanged. However, this is inef-
ficient as it results in an ILP with 2n(2n ? 1) vari-
ables and 2n(2n?1)(2n?2) transitivity constraints.
Since our main goal is to scale the use of ILP, we
modify it a little. We denote a direct-mapping edge
(u, v) by the indicator Xuv and a reversed-mapping
edge (u, v) by Yuv. The functions fd and fr provide
scores for direct and reversed mappings respectively.
The objective in Eq. 1 and the constraint in Eq. 2 are
replaced by (Eq. 3, 4 and 5 still exist and are carried
over in a trivial manner):
argmax
?
u6=v
fd(u, v)Xuv +
?
u,v
fr(u, v)Yuv (6)
s.t. ?u,v,w?V Xuv +Xvw ?Xuw ? 1
?u,v,w?V Xuv + Yvw ? Yuw ? 1
?u,v,w?V Yuv +Xvw ? Yuw ? 1
?u,v,w?V Yuv + Yvw ?Xuw ? 1
The modified constraints capture the transitivity
behavior of direct-mapping and reversed-mapping
edges, as described in Section 3. This results in
2n2 ? n variables and about 4n3 transitivity con-
straints, cutting the ILP size in half.
Next, we specify how to derive the function f
from the trained classifier using a probabilistic for-
mulation4. Following Snow et al (2006) and Be-
rant et al (2010), we utilize a probabilistic entail-
ment classifier that computes the posterior Puv =
P (Xuv = 1|Fuv). We want to use Puv to derive the
posterior P (G|F ), where F = ?u6=vFuv and Fuv is
the feature vector for a node pair (u, v).
Since the classifier was trained on a balanced
training set, the prior over the two entailment
classes is uniform and so by Bayes rule Puv ?
P (Fuv|Xuv = 1). Using that and the exact same
three independence assumptions described by Snow
et al (2006) and Berant et al (2010) we can show
that (for brevity, we omit the full derivation):
G? = argmaxG logP (G|F ) = (7)
argmax
?
u6=v
(log
Puv ? P (Xuv = 1)
(1? Puv)P (Xuv = 0)
)Xuv
= argmax
?
u6=v
(log
Puv
1? Puv
)Xuv + log ? ? |E|
where ? = P (Xuv=1)P (Xuv=0) is the prior odds ratio for
an edge in the graph. Comparing Eq. 1 and 7 we
see that f(u, v) = log Puv ?P (Xuv=1)(1?Puv)P (Xuv=0) . Note that f
is composed of a likelihood component and an edge
prior expressed by P (Xuv = 1), which we assume
to be some constant. This constant is a parameter
that affects graph sparsity and controls the trade-off
between recall and precision.
Next, we show how sparsity is exploited to scale
the use of ILP solvers. We discuss two-types entail-
ment graphs, but generalization is simple.
4.3 Graph decomposition
Though ILP solvers provide an optimal solution,
they substantially restrict the size of graphs we can
work with. The number of constraints is O(n3),
and solving graphs of size > 50 is often not feasi-
ble. To overcome this, we take advantage of graph
sparsity: most predicates in language do not entail
one another. Thus, it might be possible to decom-
pose graphs into small components and solve each
4We describe two-types graphs but extending to single-type
graphs is straightforward.
614
Algorithm 1 Decomposed-ILP
Input: A set V and a function f : V ? V ? R
Output: An optimal set of directed edges E?
1: E? = {(u, v) : f(u, v) > 0 ? f(v, u) > 0}
2: V1, V2, ..., Vk ? connected components of
G? = (V,E?)
3: for i = 1 to k do
4: Ei ? ApplyILPSolve(Vi,f)
5: end for
6: E? ?
?k
i=1Ei
component separately. This is formalized in the next
proposition.
Proposition 1. If we can partition a set of nodes
V into disjoint sets U,W such that for any cross-
ing edge (u,w) between them (in either direction),
f(u,w) < 0, then the optimal set of edgesEopt does
not contain any crossing edge.
Proof Assume by contradiction that Eopt con-
tains a set of crossing edges Ecross. We can
construct Enew = Eopt \ Ecross. Clearly?
(u,v)?Enew f(u, v) >
?
(u,v)?Eopt f(u, v), as
f(u, v) < 0 for any crossing edge.
Next, we show that Enew does not violate tran-
sitivity constraints. Assume it does, then the viola-
tion is caused by omitting the edges in Ecross. Thus,
there must be a node u ? U and w ? W (w.l.o.g)
such that for some node v, (u, v) and (v, w) are in
Enew, but (u,w) is not. However, this means either
(u, v) or (v, w) is a crossing edge, which is impossi-
ble since we omitted all crossing edges. Thus, Enew
is a better solution than Eopt, contradiction.
This proposition suggests a simple algorithm (see
Algorithm 1): Add to the graph an undirected edge
for any node pair with a positive score, then find the
connected components, and apply an ILP solver over
the nodes in each component. The edges returned
by the solver provide an optimal (not approximate)
solution to the optimization problem.
The algorithm?s complexity is dominated by the
ILP solver, as finding connected components takes
O(V 2) time. Thus, efficiency depends on whether
the graph is sparse enough to be decomposed into
small components. Note that the edge prior plays an
important role: low values make the graph sparser
and easier to solve. In Section 5 we empirically test
Algorithm 2 Incremental-ILP
Input: A set V and a function f : V ? V ? R
Output: An optimal set of directed edges E?
1: ACT,VIO? ?
2: repeat
3: E? ? ApplyILPSolve(V,f,ACT)
4: VIO? violated(V,E?)
5: ACT? ACT ? VIO
6: until |VIO| = 0
how typed entailment graphs benefit from decompo-
sition given different prior values.
From a more general perspective, this algo-
rithm can be applied to any problem of learning
a sparse transitive binary relation. Such problems
include Co-reference Resolution (Finkel and Man-
ning, 2008) and Temporal Information Extraction
(Ling and Weld, 2010). Last, the algorithm can be
easily parallelized by solving each component on a
different core.
4.4 Incremental ILP
Another solution for scaling ILP is to employ in-
cremental ILP, which has been used in dependency
parsing (Riedel and Clarke, 2006). The idea is
that even if we omit the transitivity constraints, we
still expect most transitivity constraints to be satis-
fied, given a good local entailment classifier. Thus,
it makes sense to avoid specifying the constraints
ahead of time, but rather add them when they are
violated. This is formalized in Algorithm 2.
Line 1 initializes an active set of constraints and a
violated set of constraints (ACT;VIO). Line 3 applies
the ILP solver with the active constraints. Lines 4
and 5 find the violated constraints and add them to
the active constraints. The algorithm halts when no
constraints are violated. The solution is clearly op-
timal since we obtain a maximal solution for a less-
constrained problem.
A pre-condition for using incremental ILP is that
computing the violated constraints (Line 4) is effi-
cient, as it occurs in every iteration. We do that in
a straightforward manner: For every node v, and
edges (u, v) and (v, w), if (u,w) /? E? we add
(u, v, w) to the violated constraints. This is cubic
in worst-case but assuming the degree of nodes is
bounded by a constant it is linear, and performs very
615
fast in practice.
Combining Incremental-ILP and Decomposed-
ILP is easy: We decompose any large graph into
its components and apply Incremental ILP on each
component. We applied this algorithm on our evalu-
ation data set (Section 5) and found that it converges
in at most 6 iterations and that the maximal num-
ber of active constraints in large graphs drops from
? 106 to ? 103 ? 104.
5 Experimental Evaluation
In this section we empirically answer the follow-
ing questions: (1) Does transitivity improve rule
learning over typed predicates? (Section 5.1) (2)
Do Decomposed-ILP and Incremental-ILP improve
scalability? (Section 5.2)
5.1 Experiment 1
A data set of 1 million TextRunner tuples (Banko
et al, 2007), mapped to 10,672 distinct typed predi-
cates over 156 types was provided by Schoenmack-
ers et al (2010). Readers are referred to their pa-
per for details on mapping of tuples to typed predi-
cates. Since entailment only occurs between pred-
icates that share the same types, we decomposed
predicates by their types (e.g., all predicates with the
types place and disease) into 2,303 typed entailment
graphs. The largest graph contains 118 nodes and
the total number of potential rules is 263,756.
We generated a training set by applying the proce-
dure described in Section 4.1, yielding 2,644 exam-
ples. We used SVMperf (Joachims, 2005) to train a
Gaussian kernel classifier and computed Puv by pro-
jecting the classifier output score, Suv, with the sig-
moid function: Puv = 11+exp(?Suv) . We tuned two
SVM parameters using 5-fold cross validation and a
development set of two typed entailment graphs.
Next, we used our algorithm to learn rules. As
mentioned in Section 4.2, we integrate background
knowledge using the sets Ayes and Ano that contain
predicate pairs for which we know whether entail-
ment holds. Ayes was constructed with syntactic
rules: We normalized each predicate by omitting the
first word if it is a modal and turning passives to ac-
tives. If two normalized predicates are equal they are
synonymous and inserted into Ayes. Ano was con-
structed from 3 sources (1) Predicates differing by a
single pair of words that are WordNet antonyms (2)
Predicates differing by a single word of negation (3)
Predicates p(t1, t2) and p(t2, t1) where p is a transi-
tive verb (e.g., beat) in VerbNet (Kipper-Schuler et
al., 2000).
We compared our algorithm (termed ILPscale) to
the following baselines. First, to 10,000 rules re-
leased by Schoenmackers et al (2010) (Sherlock),
where the LHS contains a single predicate (Schoen-
mackers et al released 30,000 rules but 20,000 of
those have more than one predicate on the LHS,
see Section 2), as we learn rules over the same data
set. Second, to distributional similarity algorithms:
(a) SR: the score used by Schoenmackers et al as
part of the Sherlock system. (b) DIRT: (Lin and
Pantel, 2001) a widely-used rule learning algorithm.
(c) BInc: (Szpektor and Dagan, 2008) a directional
rule learning algorithm. Third, we compared to the
entailment classifier with no transitivity constraints
(clsf ) to see if combining distributional similarity
scores improves performance over single measures.
Last, we added to all baselines background knowl-
edge with Ayes and Ano (adding the subscript Xk to
their name).
To evaluate performance we manually annotated
all edges in 10 typed entailment graphs - 7 two-
types entailment graphs containing 14, 22, 30, 53,
62, 86 and 118 nodes, and 3 single-type entailment
graphs containing 7, 38 and 59 nodes. This annota-
tion yielded 3,427 edges and 35,585 non-edges, re-
sulting in an empirical edge density of 9%. We eval-
uate the algorithms by comparing the set of edges
learned by the algorithms to the gold standard edges.
Figure 2 presents the precision-recall curve of the
algorithms. The curve is formed by varying a score
threshold in the baselines and varying the edge prior
in ILPscale5. For figure clarity, we omit DIRT and
SR, since BInc outperforms them.
Table 2 shows micro-recall, precision and F1 at
the point of maximal F1, and the Area Under the
Curve (AUC) for recall in the range of 0-0.45 for all
algorithms, given background knowledge (knowl-
edge consistently improves performance by a few
points for all algorithms). The table also shows re-
sults for the rules from Sherlockk.
5we stop raising the prior when run time over the graphs
exceeds 2 hours. Often when the solver does not terminate in 2
hours, it also does not terminate after 24 hours or more.
616
00 . 2
0 . 4
0 . 6
0 . 8
1
0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9
prec
isio
n
recall
BInc
clsf
BInc_k
clsf_k
ILP_scale
Figure 2: Precision-recall curve for the algorithms.
micro-average
R (%) P (%) F1 (%) AUC
ILPscale 43.4 42.2 42.8 0.22
clsfk 30.8 37.5 33.8 0.17
Sherlockk 20.6 43.3 27.9 N/A
BInck 31.8 34.1 32.9 0.17
SRk 38.4 23.2 28.9 0.14
DIRTk 25.7 31.0 28.1 0.13
Table 2: micro-average F1 and AUC for the algorithms.
Results show that using global transitivity
information substantially improves performance.
ILPscale is better than all other algorithms by a large
margin starting from recall .2, and improves AUC
by 29% and the maximal F1 by 27%. Moreover,
ILPscale doubles recall comparing to the rules from
the Sherlock resource, while maintaining compara-
ble precision.
5.2 Experiment 2
We want to test whether using our scaling tech-
niques, Decomposed-ILP and Incremental-ILP, al-
lows us to reach the optimal solution in graphs that
otherwise we could not solve, and consequently in-
crease the number of learned rules and the overall
recall. To check that, we run ILPscale, with and with-
out these scaling techniques (termed ILP?).
We used the same data set as in Experiment 1
and learned edges for all 2,303 entailment graphs
in the data set. If the ILP solver was unable to
hold the ILP in memory or took more than 2 hours
log ? # unlearned # rules 4 Red.
-1.75 9/0 6,242 / 7,466 20% 75%
-1 9/1 16,790 / 19,396 16% 29%
-0.6 9/3 26,330 / 29,732 13% 14%
Table 3: Impact of scaling techinques (ILP?/ILPscale).
for some graph, we did not attempt to learn its
edges. We ran ILPscale and ILP? in three den-
sity modes to examine the behavior of the algo-
rithms for different graph densities: (a) log ? =
?0.6: the configuration that achieved the best
recall/precision/F1 of 43.4/42.2/42.8. (b) log ? =
?1 with recall/precision/F1 of 31.8/55.3/40.4. (c)
log ? = ?1.75: A high precision configuration with
recall/precision/F1 of 0.15/0.75/0.23 6.
In each run we counted the number of graphs that
could not be learned and the number of rules learned
by each algorithm. In addition, we looked at the
20 largest graphs in our data (49-118 nodes) and
measured the ratio r between the size of the largest
component after applying Decomposed-ILP and the
original size of the graph. We then computed the av-
erage 1?r over the 20 graphs to examine how graph
size drops due to decomposition.
Table 3 shows the results. Column # unlearned
and # rules describe the number of unlearned graphs
and the number of learned rules. Column 4 shows
relative increase in the number of rules learned and
column Red. shows the average 1? r.
ILPscale increases the number of graphs that we
are able to learn: in our best configuration (log ? =
?0.6) only 3 graphs could not be handled com-
paring to 9 graphs when omitting our scaling tech-
niques. Since the unlearned graphs are among the
largest in the data set, this adds 3,500 additional
rules. We compared the precision of rules learned
only by ILPscale with that of the rules learned by
both, by randomly sampling 100 rules from each and
found precision to be comparable. Thus, the addi-
tional rules learned translate into a 13% increase in
relative recall without harming precision.
Also note that as density increases, the number of
rules learned grows and the effectiveness of decom-
position decreases. This shows how Decomposed-
ILP is especially useful for sparse graphs. We re-
6Experiment was run on an Intel i5 CPU with 4GB RAM.
617
lease the 29,732 rules learned by the configuration
log ? = ?0.6 as a resource.
To sum up, our scaling techniques allow us to
learn rules from graphs that standard ILP can not
handle and thus considerably increase recall without
harming precision.
6 Conclusions and Future Work
This paper proposes two contributions over two re-
cent works: In the first, Berant et al (2010) pre-
sented a global optimization procedure to learn en-
tailment rules between predicates using transitivity,
and applied this algorithm over small graphs where
all predicates have one argument instantiated by a
target concept. Consequently, the rules they learn
are of limited applicability. In the second, Schoen-
mackers et al learned rules of wider applicability by
using typed predicates, but utilized a local approach.
In this paper we developed an algorithm that uses
global optimization to learn widely-applicable en-
tailment rules between typed predicates (where both
arguments are variables). This was achieved by
appropriately defining entailment graphs for typed
predicates, formulating an ILP representation for
them, and introducing scaling techniques that in-
clude graph decomposition and incremental ILP.
Our algorithm is guaranteed to provide an optimal
solution and we have shown empirically that it sub-
stantially improves performance over Schoenmack-
ers et al?s recent resource and over several baselines.
In future work, we aim to scale the algorithm
further and learn entailment rules between untyped
predicates. This would require explicit modeling of
predicate ambiguity and using approximation tech-
niques when an optimal solution cannot be attained.
Acknowledgments
This work was performed with financial support
from the Turing Center at The University of Wash-
ington during a visit of the first author (NSF grant
IIS-0803481). We deeply thank Oren Etzioni and
Stefan Schoenmackers for providing us with the data
sets for this paper and for numerous helpful discus-
sions. We would also like to thank the anonymous
reviewers for their useful comments. This work
was developed under the collaboration of FBK-
irst/University of Haifa and was partially supported
by the Israel Science Foundation grant 1112/08. The
first author is grateful to IBM for the award of an
IBM Fellowship, and has carried out this research
in partial fulllment of the requirements for the Ph.D.
degree.
References
J. Fillmore Baker, C. F. and J. B. Lowe. 1998. The
Berkeley framenet project. In Proc. of COLING-ACL.
Michele Banko, Michael Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI.
Roni Ben Aharon, Idan Szpektor, and Ido Dagan. 2010.
Generating entailment rules from framenet. In Pro-
ceedings of ACL.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2010. Global learning of focused entailment graphs.
In Proceedings of ACL.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
LEDIR: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of
EMNLP-CoNLL.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:273?381.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rational, eval-
uation and approaches. Natural Language Engineer-
ing, 15(4):1?17.
Quang Do and Dan Roth. 2010. Constraints based
taxonomic relation classification. In Proceedings of
EMNLP.
Saso Dzeroski and Ivan Brakto. 1992. Handling noise
in inductive logic programming. In Proceedings of the
International Workshop on Inductive Logic Program-
ming.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and Com-
munication). The MIT Press.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
Proceedings of ACL-08: HLT, Short Papers.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING.
Thorsten Joachims. 2005. A support vector method for
multivariate performance measures. In Proceedings of
ICML.
Karin Kipper-Schuler, Hoa Trand Dang, and Martha
Palmer. 2000. Class-based construction of verb lex-
icon. In Proceedings of AAAI/IAAI.
618
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Xiao Ling and Daniel S. Weld. 2010. Temporal informa-
tion extraction. In Proceedings of AAAI.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. NOMLEX:
A lexicon of nominalizations. In Proceedings of COL-
ING.
Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of ACL.
Eric McCreath and Arun Sharma. 1997. ILP with noise
and fixed example size: a bayesian approach. In Pro-
ceedings of the Fifteenth international joint conference
on artificial intelligence - Volume 2.
Vladimir Nikulin. 2008. Classification of imbalanced
data with random sets and mean-variance filtering.
IJDWM, 4(2):63?78.
Hoifung Poon and Pedro Domingos. 2010. Unsuper-
vised ontology induction from text. In Proceedings of
ACL.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In Proceedings of EMNLP.
Stefan Schoenmackers, Oren Etzioni Jesse Davis, and
Daniel S. Weld. 2010. Learning first-order horn
clauses from web text. In Proceedings of EMNLP.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of ACL.
Idan Szpektor and Ido Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of COLING.
Idan Szpektor and Ido Dagan. 2009. Augmenting
wordnet-based inference with argument mapping. In
Proceedings of TextInfer.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP.
Jason Van Hulse, Taghi Khoshgoftaar, and Amri Napoli-
tano. 2007. Experimental perspectives on learning
from imbalanced data. In Proceedings of ICML.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
EMNLP.
Mihalis Yannakakis. 1978. Node-and edge-deletion NP-
complete problems. In STOC ?78: Proceedings of the
tenth annual ACM symposium on Theory of comput-
ing, pages 253?264, New York, NY, USA. ACM.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
619
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 117?125,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Efficient Tree-based Approximation for Entailment Graph Learning
Jonathan Berant?, Ido Dagan?, Meni Adler?, Jacob Goldberger?
? The Blavatnik School of Computer Science, Tel Aviv University
? Department of Computer Science, Bar-Ilan University
? Faculty of Engineering, Bar-Ilan University
jonatha6@post.tau.ac.il
{dagan,goldbej}@{cs,eng}.biu.ac.il
adlerm@cs.bgu.ac.il
Abstract
Learning entailment rules is fundamental in
many semantic-inference applications and has
been an active field of research in recent years.
In this paper we address the problem of learn-
ing transitive graphs that describe entailment
rules between predicates (termed entailment
graphs). We first identify that entailment
graphs exhibit a ?tree-like? property and are
very similar to a novel type of graph termed
forest-reducible graph. We utilize this prop-
erty to develop an iterative efficient approxi-
mation algorithm for learning the graph edges,
where each iteration takes linear time. We
compare our approximation algorithm to a
recently-proposed state-of-the-art exact algo-
rithm and show that it is more efficient and
scalable both theoretically and empirically,
while its output quality is close to that given
by the optimal solution of the exact algorithm.
1 Introduction
Performing textual inference is in the heart of many
semantic inference applications such as Question
Answering (QA) and Information Extraction (IE). A
prominent generic paradigm for textual inference is
Textual Entailment (TUE) (Dagan et al, 2009). In
TUE, the goal is to recognize, given two text frag-
ments termed text and hypothesis, whether the hy-
pothesis can be inferred from the text. For example,
the text ?Cyprus was invaded by the Ottoman Em-
pire in 1571? implies the hypothesis ?The Ottomans
attacked Cyprus?.
Semantic inference applications such as QA and
IE crucially rely on entailment rules (Ravichandran
and Hovy, 2002; Shinyama and Sekine, 2006) or
equivalently inference rules, that is, rules that de-
scribe a directional inference relation between two
fragments of text. An important type of entailment
rule specifies the entailment relation between natu-
ral language predicates, e.g., the entailment rule ?X
invade Y ? X attack Y? can be helpful in inferring
the aforementioned hypothesis. Consequently, sub-
stantial effort has been made to learn such rules (Lin
and Pantel, 2001; Sekine, 2005; Szpektor and Da-
gan, 2008; Schoenmackers et al, 2010).
Textual entailment is inherently a transitive rela-
tion , that is, the rules ?x ? y? and ?y ? z? imply
the rule ?x ? z?. Accordingly, Berant et al (2010)
formulated the problem of learning entailment rules
as a graph optimization problem, where nodes are
predicates and edges represent entailment rules that
respect transitivity. Since finding the optimal set of
edges respecting transitivity is NP-hard, they em-
ployed Integer Linear Programming (ILP) to find the
exact solution. Indeed, they showed that applying
global transitivity constraints improves rule learning
comparing to methods that ignore graph structure.
More recently, Berant et al (Berant et al, 2011) in-
troduced a more efficient exact algorithm, which de-
composes the graph into connected components and
then applies an ILP solver over each component.
Despite this progress, finding the exact solution
remains NP-hard ? the authors themselves report
they were unable to solve some graphs of rather
moderate size and that the coverage of their method
is limited. Thus, scaling their algorithm to data sets
with tens of thousands of predicates (e.g., the extrac-
tions of Fader et al (2011)) is unlikely.
117
In this paper we present a novel method for learn-
ing the edges of entailment graphs. Our method
computes much more efficiently an approximate so-
lution that is empirically almost as good as the exact
solution. To that end, we first (Section 3) conjecture
and empirically show that entailment graphs exhibit
a ?tree-like? property, i.e., that they can be reduced
into a structure similar to a directed forest.
Then, we present in Section 4 our iterative ap-
proximation algorithm, where in each iteration a
node is removed and re-attached back to the graph in
a locally-optimal way. Combining this scheme with
our conjecture about the graph structure enables a
linear algorithm for node re-attachment. Section 5
shows empirically that this algorithm is by orders of
magnitude faster than the state-of-the-art exact al-
gorithm, and that though an optimal solution is not
guaranteed, the area under the precision-recall curve
drops by merely a point.
To conclude, the contribution of this paper is two-
fold: First, we define a novel modeling assumption
about the tree-like structure of entailment graphs and
demonstrate its validity. Second, we exploit this as-
sumption to develop a polynomial approximation al-
gorithm for learning entailment graphs that can scale
to much larger graphs than in the past. Finally, we
note that learning entailment graphs bears strong
similarities to related tasks such as Taxonomy In-
duction (Snow et al, 2006) and Ontology induction
(Poon and Domingos, 2010), and thus our approach
may improve scalability in these fields as well.
2 Background
Until recently, work on learning entailment rules be-
tween predicates considered each rule independently
of others and did not exploit global dependencies.
Most methods utilized the distributional similarity
hypothesis that states that semantically similar pred-
icates occur with similar arguments (Lin and Pan-
tel, 2001; Szpektor et al, 2004; Yates and Etzioni,
2009; Schoenmackers et al, 2010). Some meth-
ods extracted rules from lexicographic resources
such as WordNet (Szpektor and Dagan, 2009) or
FrameNet (Bob and Rambow, 2009; Ben Aharon et
al., 2010), and others assumed that semantic rela-
tions between predicates can be deduced from their
co-occurrence in a corpus via manually-constructed
patterns (Chklovski and Pantel, 2004).
Recently, Berant et al (2010; 2011) formulated
the problem as the problem of learning global entail-
ment graphs. In entailment graphs, nodes are predi-
cates (e.g., ?X attack Y?) and edges represent entail-
ment rules between them (?X invade Y ? X attack
Y?). For every pair of predicates i, j, an entailment
score wij was learned by training a classifier over
distributional similarity features. A positive wij in-
dicated that the classifier believes i? j and a nega-
tive wij indicated that the classifier believes i 9 j.
Given the graph nodes V (corresponding to the pred-
icates) and the weighting function w : V ? V ? R,
they aim to find the edges of a graph G = (V,E)
that maximize the objective
?
(i,j)?E wij under the
constraint that the graph is transitive (i.e., for every
node triplet (i, j, k), if (i, j) ? E and (j, k) ? E,
then (i, k) ? E).
Berant et al proved that this optimization prob-
lem, which we term Max-Trans-Graph, is NP-hard,
and so described it as an Integer Linear Program
(ILP). Let xij be a binary variable indicating the ex-
istence of an edge i ? j in E. Then, X = {xij :
i 6= j} are the variables of the following ILP for
Max-Trans-Graph:
argmax
X
?
i 6=j
wij ? xij (1)
s.t. ?i,j,k?V xij + xjk ? xik ? 1
?i,j?V xij ? {0, 1}
The objective function is the sum of weights over the
edges of G and the constraint xij + xjk ? xik ? 1
on the binary variables enforces that whenever xij=
xjk=1, then also xik = 1 (transitivity).
Since ILP is NP-hard, applying an ILP solver di-
rectly does not scale well because the number of
variables isO(|V |2) and the number of constraints is
O(|V |3). Thus, even a graph with?80 nodes (predi-
cates) has more than half a million constraints. Con-
sequently, in (Berant et al, 2011), they proposed a
method that efficiently decomposes the graph into
smaller components and applies an ILP solver on
each component separately using a cutting-plane
procedure (Riedel and Clarke, 2006). Although this
method is exact and improves scalability, it does
not guarantee an efficient solution. When the graph
does not decompose into sufficiently small compo-
nents, and the weights generate many violations of
118
transitivity, solving Max-Trans-Graph becomes in-
tractable. To address this problem, we present in
this paper a method for approximating the optimal
set of edges within each component and show that
it is much more efficient and scalable both theoreti-
cally and empirically.
Do and Roth (2010) suggested a method for a re-
lated task of learning taxonomic relations between
terms. Given a pair of terms, a small graph is con-
structed and constraints are imposed on the graph
structure. Their work, however, is geared towards
scenarios where relations are determined on-the-fly
for a given pair of terms and no global knowledge
base is explicitly constructed. Thus, their method
easily produces solutions where global constraints,
such as transitivity, are violated.
Another approximation method that violates tran-
sitivity constraints is LP relaxation (Martins et al,
2009). In LP relaxation, the constraint xij ? {0, 1}
is replaced by 0 ? xij ? 1, transforming the prob-
lem from an ILP to a Linear Program (LP), which
is polynomial. An LP solver is then applied on the
problem, and variables xij that are assigned a frac-
tional value are rounded to their nearest integer and
so many violations of transitivity easily occur. The
solution when applying LP relaxation is not a transi-
tive graph, but nevertheless we show for comparison
in Section 5 that our method is much faster.
Last, we note that transitive relations have been
explored in adjacent fields such as Temporal Infor-
mation Extraction (Ling and Weld, 2010), Ontol-
ogy Induction (Poon and Domingos, 2010), and Co-
reference Resolution (Finkel and Manning, 2008).
3 Forest-reducible Graphs
The entailment relation, described by entailment
graphs, is typically from a ?semantically-specific?
predicate to a more ?general? one. Thus, intuitively,
the topology of an entailment graph is expected to be
?tree-like?. In this section we first formalize this in-
tuition and then empirically analyze its validity. This
property of entailment graphs is an interesting topo-
logical observation on its own, but also enables the
efficient approximation algorithm of Section 4.
For a directed edge i ? j in a directed acyclic
graphs (DAG), we term the node i a child of node
j, and j a parent of i. A directed forest is a DAG
Xdisease be 
epidemic in 
 Ycountry 
Xdisease 
common in 
 Ycountry 
Xdisease 
occur in 
 Ycountry 
Xdisease 
frequent in 
 Ycountry 
Xdisease 
begin in 
 Ycountry 
be epidemic in 
common in 
frequent in 
occur in 
begin in 
be epidemic in 
common in 
 frequent in 
occur in 
begin in 
(a) 
(b) 
(c) 
Figure 1: A fragment of an entailment graph (a), its SCC
graph (b) and its reduced graph (c). Nodes are predicates
with typed variables (see Section 5), which are omitted in
(b) and (c) for compactness.
where all nodes have no more than one parent.
The entailment graph in Figure 1a (subgraph from
the data set described in Section 5) is clearly not a
directed forest ? it contains a cycle of size two com-
prising the nodes ?X common in Y? and ?X frequent in
Y?, and in addition the node ?X be epidemic in Y? has
3 parents. However, we can convert it to a directed
forest by applying the following operations. Any
directed graph G can be converted into a Strongly-
Connected-Component (SCC) graph in the follow-
ing way: every strongly connected component (a set
of semantically-equivalent predicates, in our graphs)
is contracted into a single node, and an edge is added
from SCC S1 to SCC S2 if there is an edge in G from
some node in S1 to some node in S2. The SCC graph
is always a DAG (Cormen et al, 2002), and if G is
transitive then the SCC graph is also transitive. The
graph in Figure 1b is the SCC graph of the one in
119
Xcountry annex  Yplace 
Xcountry invade  Yplace Yplace be part of Xcountry  
Figure 2: A fragment of an entailment graph that is not
an FRG.
Figure 1a, but is still not a directed forest since the
node ?X be epidemic in Y? has two parents.
The transitive closure of a directed graph G is
obtained by adding an edge from node i to node j
if there is a path in G from i to j. The transitive
reduction of G is obtained by removing all edges
whose absence does not affect its transitive closure.
In DAGs, the result of transitive reduction is unique
(Aho et al, 1972). We thus define the reduced graph
Gred = (Vred, Ered) of a directed graph G as the
transitive reduction of its SCC graph. The graph in
Figure 1c is the reduced graph of the one in Fig-
ure 1a and is a directed forest. We say a graph is a
forest-reducible graph (FRG) if all nodes in its re-
duced form have no more than one parent.
We now hypothesize that entailment graphs are
FRGs. The intuition behind this assumption is
that the predicate on the left-hand-side of a uni-
directional entailment rule has a more specific mean-
ing than the one on the right-hand-side. For instance,
in Figure 1a ?X be epidemic in Y? (where ?X? is a type
of disease and ?Y? is a country) is more specific than
?X common in Y? and ?X frequent in Y?, which are
equivalent, while ?X occur in Y? is even more gen-
eral. Accordingly, the reduced graph in Figure 1c
is an FRG. We note that this is not always the case:
for example, the entailment graph in Figure 2 is not
an FRG, because ?X annex Y? entails both ?Y be part
of X? and ?X invade Y?, while the latter two do not
entail one another. However, we hypothesize that
this scenario is rather uncommon. Consequently, a
natural variant of the Max-Trans-Graph problem is
to restrict the required output graph of the optimiza-
tion problem (1) to an FRG. We term this problem
Max-Trans-Forest.
To test whether our hypothesis holds empirically
we performed the following analysis. We sampled
7 gold standard entailment graphs from the data set
described in Section 5, manually transformed them
into FRGs by deleting a minimal number of edges,
and measured recall over the set of edges in each
graph (precision is naturally 1.0, as we only delete
gold standard edges). The lowest recall value ob-
tained was 0.95, illustrating that deleting a very
small proportion of edges converts an entailment
graph into an FRG. Further support for the prac-
tical validity of this hypothesis is obtained from
our experiments in Section 5. In these experiments
we show that exactly solving Max-Trans-Graph and
Max-Trans-Forest (with an ILP solver) results in
nearly identical performance.
An ILP formulation for Max-Trans-Forest is sim-
ple ? a transitive graph is an FRG if all nodes in
its reduced graph have no more than one parent. It
can be verified that this is equivalent to the following
statement: for every triplet of nodes i, j, k, if i ? j
and i ? k, then either j ? k or k ? j (or both).
Therefore, the ILP is formulated by adding this lin-
ear constraint to ILP (1):
?i,j,k?V xij+xik+(1? xjk)+(1? xkj) ? 3 (2)
We note that despite the restriction to FRGs, Max-
Trans-Forest is an NP-hard problem by a reduction
from the X3C problem (Garey and Johnson, 1979).
We omit the reduction details for brevity.
4 Sequential Approximation Algorithms
In this section we present Tree-Node-Fix, an efficient
approximation algorithm for Max-Trans-Forest, as
well as Graph-Node-Fix, an approximation for Max-
Trans-Graph.
4.1 Tree-Node-Fix
The scheme of Tree-Node-Fix (TNF) is the follow-
ing. First, an initial FRG is constructed, using some
initialization procedure. Then, at each iteration a
single node v is re-attached (see below) to the FRG
in a way that improves the objective function. This
is repeated until the value of the objective function
cannot be improved anymore by re-attaching a node.
Re-attaching a node v is performed by removing
v from the graph and connecting it back with a better
set of edges, while maintaining the constraint that it
is an FRG. This is done by considering all possible
edges from/to the other graph nodes and choosing
120
(a) 
d 
c 
v ? c v 
c 
d1 ? d2 
v 
? ? ? 
r1 r2 
v (b) (b?) (c) 
r3 
? 
Figure 3: (a) Inserting v into a component c ? Vred. (b)
Inserting v as a child of c and a parent of a subset of c?s
children in Gred. (b?) A node d that is a descendant but
not a child of c can not choose v as a parent, as v becomes
its second parent. (c) Inserting v as a new root.
the optimal subset, while the rest of the graph re-
mains fixed. Formally, let Sv?in =
?
i 6=v wiv ? xiv
be the sum of scores over v?s incoming edges and
Sv?out =
?
k 6=v wvk ? xvk be the sum of scores over
v?s outgoing edges. Re-attachment amounts to opti-
mizing a linear objective:
argmax
Xv
(Sv-in + Sv-out) (3)
where the variables Xv ? X are indicators for all
pairs of nodes involving v. We approximate a solu-
tion for (1) by iteratively optimizing the simpler ob-
jective (3). Clearly, at each re-attachment the value
of the objective function cannot decrease, since the
optimization algorithm considers the previous graph
as one of its candidate solutions.
We now show that re-attaching a node v is lin-
ear. To analyze v?s re-attachment, we consider the
structure of the directed forest Gred just before v is
re-inserted, and examine the possibilities for v?s in-
sertion relative to that structure. We start by defin-
ing some helpful notations. Every node c ? Vred
is a connected component in G. Let vc ? c be an
arbitrary representative node in c. We denote by
Sv-in(c) the sum of weights from all nodes in c and
their descendants to v, and by Sv-out(c) the sum of
weights from v to all nodes in c and their ancestors:
Sv-in(c) =
?
i?c
wiv +
?
k /?c
wkvxkvc
Sv-out(c) =
?
i?c
wvi +
?
k /?c
wvkxvck
Note that {xvck, xkvc} are edge indicators in G
and not Gred. There are two possibilities for re-
attaching v ? either it is inserted into an existing
component c ? Vred (Figure 3a), or it forms a new
component. In the latter, there are also two cases:
either v is inserted as a child of a component c (Fig-
ure 3b), or not and then it becomes a root in Gred
(Figure 3c). We describe the details of these 3 cases:
Case 1: Inserting v into a component c ? Vred.
In this case we add in G edges from all nodes in c
and their descendants to v and from v to all nodes in
c and their ancestors. The score (3) in this case is
s1(c) , Sv-in(c) + Sv-out(c) (4)
Case 2: Inserting v as a child of some c ? Vred.
Once c is chosen as the parent of v, choosing v?s
children in Gred is substantially constrained. A node
that is not a descendant of c can not become a child
of v, since this would create a new path from that
node to c and would require by transitivity to add a
corresponding directed edge to c (but all graph edges
not connecting v are fixed). Moreover, only a direct
child of c can choose v as a parent instead of c (Fig-
ure 3b), since for any other descendant of c, v would
become a second parent, and Gred will no longer be
a directed forest (Figure 3b?). Thus, this case re-
quires adding in G edges from v to all nodes in c and
their ancestors, and also for each new child of v, de-
noted by d ? Vred, we add edges from all nodes in
d and their descendants to v. Crucially, although the
number of possible subsets of c?s children in Gred is
exponential, the fact that they are independent trees
in Gred allows us to go over them one by one, and
decide for each one whether it will be a child of v
or not, depending on whether Sv-in(d) is positive.
Therefore, the score (3) in this case is:
s2(c) , Sv-out(c)+
?
d?child(c)
max(0, Sv-in(d)) (5)
where child(c) are the children of c.
Case 3: Inserting v as a new root in Gred. Similar
to case 2, only roots of Gred can become children of
v. In this case for each chosen root r we add in G
edges from the nodes in r and their descendants to
v. Again, each root can be examined independently.
Therefore, the score (3) of re-attaching v is:
s3 ,
?
r
max(0, Sv-in(r)) (6)
where the summation is over the roots of Gred.
It can be easily verified that Sv-in(c) and
Sv-out(c) satisfy the recursive definitions:
121
Algorithm 1 Computing optimal re-attachment
Input: FRG G = (V,E), function w, node v ? V
Output: optimal re-attachment of v
1: remove v and compute Gred = (Vred, Ered).
2: for all c ? Vred in post-order compute Sv-in(c) (Eq.
7)
3: for all c ? Vred in pre-order compute Sv-out(c) (Eq.
8)
4: case 1: s1 = maxc?Vred s1(c) (Eq. 4)
5: case 2: s2 = maxc?Vred s2(c) (Eq. 5)
6: case 3: compute s3 (Eq. 6)
7: re-attach v according to max(s1, s2, s3).
Sv-in(c) =
?
i?c
wiv +
?
d?child(c)
Sv-in(d), c ? Vred (7)
Sv-out(c) =
?
i?c
wvi + Sv-out(p), c ? Vred (8)
where p is the parent of c in Gred. These recursive
definitions allow to compute in linear time Sv-in(c)
and Sv-out(c) for all c (given Gred) using dynamic
programming, before going over the cases for re-
attaching v. Sv-in(c) is computed going over Vred
leaves-to-root (post-order), and Sv-out(c) is com-
puted going over Vred root-to-leaves (pre-order).
Re-attachment is summarized in Algorithm 1.
Computing an SCC graph is linear (Cormen et al,
2002) and it is easy to verify that transitive reduction
in FRGs is also linear (Line 1). Computing Sv-in(c)
and Sv-out(c) (Lines 2-3) is also linear, as explained.
Cases 1 and 3 are trivially linear and in case 2 we go
over the children of all nodes in Vred. As the reduced
graph is a forest, this simply means going over all
nodes of Vred, and so the entire algorithm is linear.
Since re-attachment is linear, re-attaching all
nodes is quadratic. Thus if we bound the number
of iterations over all nodes, the overall complexity is
quadratic. This is dramatically more efficient and
scalable than applying an ILP solver. In Section
5 we ran TNF until convergence and the maximal
number of iterations over graph nodes was 8.
4.2 Graph-node-fix
Next, we show Graph-Node-Fix (GNF), a similar
approximation that employs the same re-attachment
strategy but does not assume the graph is an FRG.
Thus, re-attachment of a node v is done with an
ILP solver. Nevertheless, the ILP in GNF is sim-
pler than (1), since we consider only candidate edges
v  
i  k  
v  
i  k  
v
i k
v  
i  k  
Figure 4: Three types of transitivity constraint violations.
involving v. Figure 4 illustrates the three types of
possible transitivity constraint violations when re-
attaching v. The left side depicts a violation when
(i, k) /? E, expressed by the constraint in (9) below,
and the middle and right depict two violations when
the edge (i, k) ? E, expressed by the constraints
in (10). Thus, the ILP is formulated by adding the
following constraints to the objective function (3):
?i,k?V \{v} if (i, k) /? E, xiv + xvk ? 1 (9)
if (i, k) ? E, xvi ? xvk, xkv ? xiv (10)
xiv, xvk ? {0, 1} (11)
Complexity is exponential due to the ILP solver;
however, the ILP size is reduced by an order of mag-
nitude to O(|V |) variables and O(|V |2) constraints.
4.3 Adding local constraints
For some pairs of predicates i, j we sometimes have
prior knowledge whether i entails j or not. We term
such pairs local constraints, and incorporate them
into the aforementioned algorithms in the following
way. In all algorithms that apply an ILP solver, we
add a constraint xij = 1 if i entails j or xij = 0 if i
does not entail j. Similarly, in TNF we incorporate
local constraints by settingwij =? orwij = ??.
5 Experiments and Results
In this section we empirically demonstrate that TNF
is more efficient than other baselines and its output
quality is close to that given by the optimal solution.
5.1 Experimental setting
In our experiments we utilize the data set released
by Berant et al (2011). The data set contains 10 en-
tailment graphs, where graph nodes are typed pred-
icates. A typed predicate (e.g., ?Xdisease occur in
Ycountry?) includes a predicate and two typed vari-
ables that specify the semantic type of the argu-
ments. For instance, the typed variable Xdisease can
be instantiated by arguments such as ?flu? or ?dia-
betes?. The data set contains 39,012 potential edges,
122
of which 3,427 are annotated as edges (valid entail-
ment rules) and 35,585 are annotated as non-edges.
The data set alo contains, for every pair of pred-
icates i, j in every graph, a local score sij , which is
the output of a classifier trained over distributional
similarity features. A positive sij indicates that the
classifier believes i? j. The weighting function for
the graph edges w is defined as wij = sij??, where
? is a single parameter controlling graph sparseness:
as ? increases, wij decreases and becomes nega-
tive for more pairs of predicates, rendering the graph
more sparse. In addition, the data set contains a set
of local constraints (see Section 4.3).
We implemented the following algorithms for
learning graph edges, where in all of them the graph
is first decomposed into components according to
Berant et als method, as explained in Section 2.
No-trans Local scores are used without transitiv-
ity constraints ? an edge (i, j) is inserted iffwij > 0.
Exact-graph Berant et al?s exact method (2011)
for Max-Trans-Graph, which utilizes an ILP solver1.
Exact-forest Solving Max-Trans-Forest exactly
by applying an ILP solver (see Eq. 2).
LP-relax Solving Max-Trans-Graph approxi-
mately by applying LP-relaxation (see Section 2)
on each graph component. We apply the LP solver
within the same cutting-plane procedure as Exact-
graph to allow for a direct comparison. This also
keeps memory consumption manageable, as other-
wise all |V |3 constraints must be explicitly encoded
into the LP. As mentioned, our goal is to present
a method for learning transitive graphs, while LP-
relax produces solutions that violate transitivity.
However, we run it on our data set to obtain empiri-
cal results, and to compare run-times against TNF.
Graph-Node-Fix (GNF) Initialization of each
component is performed in the following way: if the
graph is very sparse, i.e. ? ? C for some constantC
(set to 1 in our experiments), then solving the graph
exactly is not an issue and we use Exact-graph. Oth-
erwise, we initialize by applying Exact-graph in a
sparse configuration, i.e., ? = C.
Tree-Node-Fix (TNF) Initialization is done as in
GNF, except that if it generates a graph that is not an
FRG, it is corrected by a simple heuristic: for every
node in the reduced graph Gred that has more than
1We use the Gurobi optimization package in all experiments.
l
l
l
l
l
l
l
?0.8 ?0.6 ?0.4 ?0.2 0.0
10
50
100
500
500
0
500
00
?lambda
sec
l Exact?graphLP?relaxGNFTNF
Figure 5: Run-time in seconds for various ?? values.
one parent, we choose from its current parents the
single one whose SCC is composed of the largest
number of nodes in G.
We evaluate algorithms by comparing the set of
gold standard edges with the set of edges learned by
each algorithm. We measure recall, precision and
F1 for various values of the sparseness parameter
?, and compute the area under the precision-recall
Curve (AUC) generated. Efficiency is evaluated by
comparing run-times.
5.2 Results
We first focus on run-times and show that TNF is
efficient and has potential to scale to large data sets.
Figure 5 compares run-times2 of Exact-graph,
GNF, TNF, and LP-relax as ?? increases and the
graph becomes denser. Note that the y-axis is in
logarithmic scale. Clearly, Exact-graph is extremely
slow and run-time increases quickly. For ? = 0.3
run-time was already 12 hours and we were unable
to obtain results for ? < 0.3, while in TNF we easily
got a solution for any ?. When ? = 0.6, where both
Exact-graph and TNF achieve best F1, TNF is 10
times faster than Exact-graph. When ? = 0.5, TNF
is 50 times faster than Exact-graph and so on. Most
importantly, run-time for GNF and TNF increases
much more slowly than for Exact-graph.
2Run on a multi-core 2.5GHz server with 32GB of RAM.
123
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0.0
0.2
0.4
0.6
0.8
1.0
recall
prec
isio
n
l
l
ll l
l
l
l l
l
l
l l l
l
l l l
l
l
l
l Exact?graphTNFNo?trans
Figure 6: Precision (y-axis) vs. recall (x-axis) curve.
Maximal F1 on the curve is .43 for Exact-graph, .41 for
TNF, and .34 for No-trans. AUC in the recall range 0-0.5
is .32 for Exact-graph, .31 for TNF, and .26 for No-trans.
Run-time of LP-relax is also bad compared to
TNF and GNF. Run-time increases more slowly than
Exact-graph, but still very fast comparing to TNF.
When ? = 0.6, LP-relax is almost 10 times slower
than TNF, and when ? = ?0.1, LP-relax is 200
times slower than TNF. This points to the difficulty
of scaling LP-relax to large graphs.
As for the quality of learned graphs, Figure 6 pro-
vides a precision-recall curve for Exact-graph, TNF
and No-trans (GNF and LP-relax are omitted from
the figure and described below to improve readabil-
ity). We observe that both Exact-graph and TNF
substantially outperform No-trans and that TNF?s
graph quality is only slightly lower than Exact-graph
(which is extremely slow). Following Berant et al,
we report in the caption the maximal F1 on the curve
and AUC in the recall range 0-0.5 (the widest range
for which we have results for all algorithms). Note
that compared to Exact-graph, TNF reduces AUC by
a point and the maximal F1 score by 2 points only.
GNF results are almost identical to those of TNF
(maximal F1=0.41, AUC: 0.31), and in fact for all
? configurations TNF outperforms GNF by no more
than one F1 point. As for LP-relax, results are just
slightly lower than Exact-graph (maximal F1: 0.43,
AUC: 0.32), but its output is not a transitive graph,
and as shown above run-time is quite slow. Last, we
note that the results of Exact-forest are almost iden-
tical to Exact-graph (maximal F1: 0.43), illustrating
that assuming that entailment graphs are FRGs (Sec-
tion 3) is reasonable in this data set.
To conclude, TNF learns transitive entailment
graphs of good quality much faster than Exact-
graph. Our experiment utilized an available data
set of moderate size; However, we expect TNF to
scale to large data sets (that are currently unavail-
able), where other baselines would be impractical.
6 Conclusion
Learning large and accurate resources of entailment
rules is essential in many semantic inference appli-
cations. Employing transitivity has been shown to
improve rule learning, but raises issues of efficiency
and scalability.
The first contribution of this paper is a novel mod-
eling assumption that entailment graphs are very
similar to FRGs, which is analyzed and validated
empirically. The main contribution of the paper is
an efficient polynomial approximation algorithm for
learning entailment rules, which is based on this
assumption. We demonstrate empirically that our
method is by orders of magnitude faster than the
state-of-the-art exact algorithm, but still produces an
output that is almost as good as the optimal solution.
We suggest our method as an important step to-
wards scalable acquisition of precise entailment re-
sources. In future work, we aim to evaluate TNF on
large graphs that are automatically generated from
huge corpora. This of course requires substantial ef-
forts of pre-processing and test-set annotation. We
also plan to examine the benefit of TNF in learning
similar structures, e.g., taxonomies or ontologies.
Acknowledgments
This work was partially supported by the Israel
Science Foundation grant 1112/08, the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Community?s Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT). The first author has carried out
this research in partial fulfilment of the requirements
for the Ph.D. degree.
124
References
Alfred V. Aho, Michael R. Garey, and Jeffrey D. Ullman.
1972. The transitive reduction of a directed graph.
SIAM Journal on Computing, 1(2):131?137.
Roni Ben Aharon, Idan Szpektor, and Ido Dagan. 2010.
Generating entailment rules from framenet. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2010. Global learning of focused entailment graphs.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics.
Coyne Bob and Owen Rambow. 2009. Lexpar: A freely
available english paraphrase lexicon automatically ex-
tracted from framenet. In Proceedings of IEEE Inter-
national Conference on Semantic Computing.
Timothy Chklovski and Patrick Pantel. 2004. Verb
ocean: Mining the web for fine-grained semantic verb
relations. In Proceedings of Empirical Methods in
Natural Language Processing.
Thomas H. Cormen, Charles E. leiserson, Ronald L.
Rivest, and Clifford Stein. 2002. Introduction to Al-
gorithms. The MIT Press.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rational, eval-
uation and approaches. Natural Language Engineer-
ing, 15(4):1?17.
Quang Do and Dan Roth. 2010. Constraints based tax-
onomic relation classification. In Proceedings of Em-
pirical Methods in Natural Language Processing.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of Empirical Methods in Nat-
ural Language Processing.
J. R. Finkel and C. D. Manning. 2008. Enforcing transi-
tivity in coreference resolution. In Proceedings of the
46th Annual Meeting of the Association for Computa-
tional Linguistics.
Michael R. Garey and David S. Johnson. 1979. Comput-
ers and Intractability: A Guide to the Theory of NP-
Completeness. W. H. Freeman.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Xiao Ling and Dan S. Weld. 2010. Temporal informa-
tion extraction. In Proceedings of the 24th AAAI Con-
ference on Artificial Intelligence.
Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of the 47th Annual
Meeting of the Association for Computational Linguis-
tics.
Hoifung Poon and Pedro Domingos. 2010. Unsuper-
vised ontology induction from text. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In Proceedings of Empirical Methods
in Natural Language Processing.
Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and
Daniel S. Weld. 2010. Learning first-order horn
clauses from web text. In Proceedings of Empirical
Methods in Natural Language Processing.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
information extraction using unrestricted relation dis-
covery. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Main Conference.
Rion Snow, Dan Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous ev-
idence. In Proceedings of the 44th Annual Meeting of
the Association for Computational Linguistics.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of the
22nd International Conference on Computational Lin-
guistics.
Idan Szpektor and Ido Dagan. 2009. Augmenting
wordnet-based inference with argument mapping. In
Proceedings of TextInfer.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. In Proceedings of Empirical
Methods in Natural Language Processing.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
125
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156?160,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Crowdsourcing Inference-Rule Evaluation
Naomi Zeichner
Bar-Ilan University
Ramat-Gan, Israel
zeichner.naomi@gmail.com
Jonathan Berant
Tel-Aviv University
Tel-Aviv, Israel
jonatha6@post.tau.ac.il
Ido Dagan
Bar-Ilan University
Ramat-Gan, Israel
dagan@cs.biu.ac.il
Abstract
The importance of inference rules to semantic
applications has long been recognized and ex-
tensive work has been carried out to automat-
ically acquire inference-rule resources. How-
ever, evaluating such resources has turned out
to be a non-trivial task, slowing progress in the
field. In this paper, we suggest a framework
for evaluating inference-rule resources. Our
framework simplifies a previously proposed
?instance-based evaluation? method that in-
volved substantial annotator training, making
it suitable for crowdsourcing. We show that
our method produces a large amount of an-
notations with high inter-annotator agreement
for a low cost at a short period of time, without
requiring training expert annotators.
1 Introduction
Inference rules are an important component in se-
mantic applications, such as Question Answering
(QA) (Ravichandran and Hovy, 2002) and Informa-
tion Extraction (IE) (Shinyama and Sekine, 2006),
describing a directional inference relation between
two text patterns with variables. For example, to an-
swer the question ?Where was Reagan raised?? a
QA system can use the rule ?X brought up in Y?X
raised in Y? to extract the answer from ?Reagan was
brought up in Dixon?. Similarly, an IE system can
use the rule ?X work as Y?X hired as Y? to ex-
tract the PERSON and ROLE entities in the ?hiring?
event from ?Bob worked as an analyst for Dell?.
The significance of inference rules has led to sub-
stantial effort into developing algorithms that au-
tomatically learn inference rules (Lin and Pantel,
2001; Sekine, 2005; Schoenmackers et al, 2010),
and generate knowledge resources for inference sys-
tems. However, despite their potential, utilization of
inference rule resources is currently somewhat lim-
ited. This is largely due to the fact that these al-
gorithms often produce invalid rules. Thus, evalu-
ation is necessary both for resource developers as
well as for inference system developers who want to
asses the quality of each resource. Unfortunately, as
evaluating inference rules is hard and costly, there is
no clear evaluation standard, and this has become a
slowing factor for progress in the field.
One option for evaluating inference rule resources
is to measure their impact on an end task, as that is
what ultimately interests an inference system devel-
oper. However, this is often problematic since infer-
ence systems have many components that address
multiple phenomena, and thus it is hard to assess the
effect of a single resource. An example is the Recog-
nizing Textual Entailment (RTE) framework (Dagan
et al, 2009), in which given a text T and a textual
hypothesis H, a system determines whether H can
be inferred from T. This type of evaluation was es-
tablished in RTE challenges by ablation tests (see
RTE ablation tests in ACLWiki) and showed that re-
sources? impact can vary considerably from one sys-
tem to another. These issues have also been noted
by Sammons et al (2010) and LoBue and Yates
(2011). A complementary application-independent
evaluation method is hence necessary.
Some attempts were made to let annotators judge
rule correctness directly, that is by asking them to
judge the correctness of a given rule (Shinyama et
al., 2002; Sekine, 2005). However, Szpektor et al
(2007) observed that directly judging rules out of
context often results in low inter-annotator agree-
ment. To remedy that, Szpektor et al (2007) and
156
Bhagat et al (2007) proposed ?instance-based eval-
uation?, in which annotators are presented with an
application of a rule in a particular context and
need to judge whether it results in a valid inference.
This simulates the utility of rules in an application
and yields high inter-annotator agreement. Unfortu-
nately, their method requires lengthy guidelines and
substantial annotator training effort, which are time
consuming and costly. Thus, a simple, robust and
replicable evaluation method is needed.
Recently, crowdsourcing services such as Ama-
zon Mechanical Turk (AMT) and CrowdFlower
(CF)1 have been employed for semantic inference
annotation (Snow et al, 2008; Wang and Callison-
Burch, 2010; Mehdad et al, 2010; Negri et al,
2011). These works focused on generating and an-
notating RTE text-hypothesis pairs, but did not ad-
dress annotation and evaluation of inference rules.
In this paper, we propose a novel instance-based
evaluation framework for inference rules that takes
advantage of crowdsourcing. Our method substan-
tially simplifies annotation of rule applications and
avoids annotator training completely. The nov-
elty in our framework is two-fold: (1) We simplify
instance-based evaluation from a complex decision
scenario to two independent binary decisions. (2)
We apply methodological principles that efficiently
communicate the definition of the ?inference? rela-
tion to untrained crowdsourcing workers (Turkers).
As a case study, we applied our method to evalu-
ate algorithms for learning inference rules between
predicates. We show that we can produce many an-
notations cheaply, quickly, at good quality, while
achieving high inter-annotator agreement.
2 Evaluating Rule Applications
As mentioned, in instance-based evaluation individ-
ual rule applications are judged rather than rules in
isolation, and the quality of a rule-resource is then
evaluated by the validity of a sample of applications
of its rules. Rule application is performed by finding
an instantiation of the rule left-hand-side in a cor-
pus (termed LHS extraction) and then applying the
rule on the extraction to produce an instantiation of
the rule right-hand-side (termed RHS instantiation).
For example, the rule ?X observe Y?X celebrate Y?
1https://www.mturk.com and http://crowdflower.com
can be applied on the LHS extraction ?they observe
holidays? to produce the RHS instantiation ?they cel-
ebrate holidays?.
The target of evaluation is to judge whether each
rule application is valid or not. Following the stan-
dard RTE task definition, a rule application is con-
sidered valid if a human reading the LHS extrac-
tion is highly likely to infer that the RHS instanti-
ation is true (Dagan et al, 2009). In the aforemen-
tioned example, the annotator is expected to judge
that ?they observe holidays? entails ?they celebrate
holidays?. In addition to this straightforward case,
two more subtle situations may arise. The first is
that the LHS extraction is meaningless. We regard
a proposition as meaningful if a human can easily
understand its meaning (despite some simple gram-
matical errors). A meaningless LHS extraction usu-
ally occurs due to a faulty extraction process (e.g.,
Table 1, Example 2) and was relatively rare in our
case study (4% of output, see Section 4). Such rule
applications can either be extracted from the sam-
ple so that the rule-base is not penalized (since the
problem is in the extraction procedure), or can be
used as examples of non-entailment, if we are in-
terested in overall performance. A second situation
is a meaningless RHS instantiation, usually caused
by rule application in a wrong context. This case is
tagged as non-entailment (for example, applying the
rule ?X observe Y?X celebrate Y? in the context of
the extraction ?companies observe dress code?).
Each rule application therefore requires an answer
to the following three questions: 1) Is the LHS ex-
traction meaningful? 2) Is the RHS instantiation
meaningful? 3) If both are meaningful, does the
LHS extraction entail the RHS instantiation?
3 Crowdsourcing
Previous works using crowdsourcing noted some
principles to help get the most out of the ser-
vice(Wang et al, 2012). In keeping with these find-
ings we employ the following principles: (a) Simple
tasks. The global task is split into simple sub-tasks,
each dealing with a single aspect of the problem. (b)
Do not assume linguistic knowledge by annota-
tors. Task descriptions avoid linguistic terms such
as ?tense?, which confuse workers. (c) Gold stan-
dard validation. Using CF?s built-in methodology,
157
Phrase Meaningful Comments
1) Doctors be treat Mary Yes Annotators are instructed to ignore simple inflectional errors
2) A player deposit an No Bad extraction for the rule LHS ?X deposit Y?
3) humans bring in bed No Wrong context, result of applying ?X turn in Y?X bring in Y? on ?humans turn in bed?
Table 1: Examples of phrase ?meaningfulness? (Note that the comments are not presented to Turkers).
gold standard (GS) examples are combined with ac-
tual annotations to continuously validate annotator
reliability.
We split the annotation process into two tasks,
the first to judge phrase meaningfulness (Questions
1 and 2 above) and the second to judge entailment
(Question 3 above). In Task 1, the LHS extrac-
tions and RHS instantiations of all rule applications
are separated and presented to different Turkers in-
dependently of one another. This task is simple,
quick and cheap and allows Turkers to focus on
the single aspect of judging phrase meaningfulness.
Rule applications for which both the LHS extrac-
tion and RHS instantiation are judged as meaningful
are passed to Task 2, where Turkers need to decide
whether a given rule application is valid. If not for
Task 1, Turkers would need to distinguish in Task 2
between non-entailment due to (1) an incorrect rule
(2) a meaningless RHS instantiation (3) a meaning-
less LHS extraction. Thanks to Task 1, Turkers are
presented in Task 2 with two meaningful phrases and
need to decide only whether one entails the other.
To ensure high quality output, each example is
evaluated by three Turkers. Similarly to Mehdad et
al. (2010) we only use results for which the confi-
dence value provided by CF is greater than 70%.
We now describe the details of both tasks. Our
simplification contrasts with Szpektor et al (2007),
whose judgments for each rule application are simi-
lar to ours, but had to be performed simultaneously
by annotators, which required substantial training.
Task 1: Is the phrase meaningful?
In keeping with the second principle above, the task
description is made up of a short verbal explana-
tion followed by positive and negative examples.
The definition of ?meaningfulness? is conveyed via
examples pointing to properties of the automatic
phrase extraction process, as seen in Table 1.
Task 2: Judge if one phrase is true given another.
As mentioned, rule applications for which both sides
were judged as meaningful are evaluated for entail-
ment. The challenge is to communicate the defini-
tion of ?entailment? to Turkers. To that end the task
description begins with a short explanation followed
by ?easy? and ?hard? examples with explanations,
covering a variety of positive and negative entail-
ment ?types? (Table 2).
Defining ?entailment? is quite difficult when deal-
ing with expert annotators and still more with non-
experts, as was noted by Negri et al (2011). We
therefore employ several additional mechanisms to
get the definition of entailment across to Turkers
and increase agreement with the GS. We run an
initial small test run and use its output to improve
annotation in two ways: First, we take examples
that were ?confusing? for Turkers and add them to
the GS with explanatory feedback presented when
a Turker answers incorrectly. (E.g., the pair (?The
owner be happy to help drivers?, ?The owner assist
drivers?) was judged as entailing in the test run but
only achieved a confidence value of 0.53). Second,
we add examples that were annotated unanimously
by Turkers to the GS to increase its size, allowing
CF to better estimate Turker?s reliability (following
CF recommendations, we aim to have around 10%
GS examples in every run). In Section 4 we show
that these mechanisms improved annotation quality.
4 Case Study
As a case study, we used our evaluation methodol-
ogy to compare four methods for learning entailment
rules between predicates: DIRT (Lin and Pantel,
2001), Cover (Weeds and Weir, 2003), BInc (Szpek-
tor and Dagan, 2008) and Berant et al (2010). To
that end, we applied the methods on a set of one
billion extractions (generously provided by Fader
et al (2011)) automatically extracted from the
ClueWeb09 web crawl2, where each extraction com-
prises a predicate and two arguments. This resulted
in four learned inference rule resources.
2http://lemurproject.org/clueweb09.php/
158
Example Entailed Explanation given to Turkers
LHS: The lawyer sign the contract Yes There is a chance the lawyer has not read the contract, but
most likely that as he signed it, he must have read it.RHS: The lawyer read the contract
LHS: John be related to Jerry No The LHS can be understood from the RHS, but not the
other way around as the LHS is more general.RHS: John be a close relative of Jerry
LHS: Women be at increased risk of cancer No Although the RHS is correct, it cannot be understood from
the LHS.RHS: Women die of cancer
Table 2: Examples given in the description of Task 2.
We randomly sampled 5,000 extractions, and for
each one sampled four rules whose LHS matches the
extraction from the union of the learned resources.
We then applied the rules, which resulted in 20,000
rule applications. We annotated rule applications
using our methodology and evaluated each learn-
ing method by comparing the rules learned by each
method with the annotation generated by CF.
In Task 1, 281 rule applications were annotated as
meaningless LHS extraction, and 1,012 were anno-
tated as meaningful LHS extraction but meaningless
RHS instantiation and so automatically annotated as
non-entailment. 8,264 rule applications were passed
on to Task 2, as both sides were judged meaning-
ful (the remaining 10,443 discarded due to low CF
confidence). In Task 2, 5,555 rule applications were
judged with a high confidence and supplied as out-
put, 2,447 of them as positive entailment and 3,108
as negative. Overall, 6,567 rule applications (dataset
of this paper) were annotated for a total cost of
$1000. The annotation process took about one week.
In tests run during development we experimented
with Task 2 wording and GS examples, seeking to
make the definition of entailment as clear as pos-
sible. To do so we randomly sampled and manu-
ally annotated 200 rule applications (from the initial
20,000), and had Turkers judge them. In our initial
test, Turkers tended to answer ?yes? comparing to
our own annotation, with 0.79 agreement between
their annotation and ours, corresponding to a kappa
score of 0.54. After applying the mechanisms de-
scribed in Section 3, false-positive rate was reduced
from 18% to 6% while false-negative rate only in-
creased from 4% to 5%, corresponding to a high
agreement of 0.9 and kappa of 0.79.
In our test, 63% of the 200 rule applications were
annotated unanimously by the Turkers. Importantly,
all these examples were in perfect agreement with
our own annotation, reflecting their high reliability.
For the purpose of evaluating the resources learned
by the algorithms we used annotations with CF con-
fidence ? 0.7 for which kappa is 0.99.
Lastly, we computed the area under the recall-
precision curve (AUC) for DIRT, Cover, BInc and
Berant et al?s method, resulting in an AUC of 0.4,
0.43, 0.44, and 0.52 respectively. We used the AUC
curve, with number of recall-precision points in the
order of thousands, to avoid tuning a threshold pa-
rameter. Overall, we demonstrated that our evalua-
tion framework allowed us to compare four different
learning methods in low costs and within one week.
5 Discussion
In this paper we have suggested a crowdsourcing
framework for evaluating inference rules. We have
shown that by simplifying the previously-proposed
instance-based evaluation framework we are able to
take advantage of crowdsourcing services to replace
trained expert annotators, resulting in good quality
large scale annotations, for reasonable time and cost.
We have presented the methodological principles we
developed to get the entailment decision across to
Turkers, achieving very high agreement both with
our annotations and between the annotators them-
selves. Using the CrowdFlower forms we provide
with this paper, the proposed methodology can be
beneficial for both resource developers evaluating
their output as well as inference system developers
wanting to assess the quality of existing resources.
Acknowledgments
This work was partially supported by the Israel
Science Foundation grant 1112/08, the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Communitys Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT).
159
References
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2010. Global learning of focused entailment graphs.
In Proceedings of the annual meeting of the Associa-
tion for Computational Linguistics (ACL).
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
LEDIR: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rational, eval-
uation and approaches. Natural Language Engineer-
ing, 15(Special Issue 04):i?xvii.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information
extraction. In Proceedings of the Conference of
Empirical Methods in Natural Language Processing
(EMNLP ?11).
Dekang Lin and Patrick Pantel. 2001. DIRT - discov-
ery of inference rules from text. In Proceedings of the
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining.
Peter LoBue and Alexander Yates. 2011. Types of
common-sense knowledge needed for recognizing tex-
tual entailment. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards cross-lingual textual entailment. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (ACL).
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and conquer: Crowdsourcing the creation of
cross-lingual textual entailment corpora. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP ?11).
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the annual meeting of the Associa-
tion for Computational Linguistics (ACL).
Mark Sammons, V. G. Vinod Vydiswaran, and Dan Roth.
2010. ?ask not what textual entailment can do for
you...?. In Proceedings of the annual meeting of the
Association for Computational Linguistics (ACL).
Stefan Schoenmackers, Oren Etzioni Jesse Davis, and
Daniel S. Weld. 2010. Learning first-order horn
clauses from web text. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP ?10).
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of the Third International Workshop on
Paraphrasing (IWP2005).
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics (HLT-NAACL ?06).
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news
articles. In Proceedings of the second international
conference on Human Language Technology Research
(HLT ?02).
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP
?08).
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (Coling 2008).
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acquisi-
tion. In Proceedings of the annual meeting of the As-
sociation for Computational Linguistics (ACL).
Rui Wang and Chris Callison-Burch. 2010. Cheap facts
and counter-facts. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk.
Aobo Wang, Cong Duy Vu Hoang, and Min-Yen Kan.
2012. Perspectives on crowdsourcing annotations for
natural language processing. Journal of Language Re-
sources and Evaluation).
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2003).
160
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 79?84,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Entailment-based Text Exploration
with Application to the Health-care Domain
Meni Adler
Bar Ilan University
Ramat Gan, Israel
adlerm@cs.bgu.ac.il
Jonathan Berant
Tel Aviv University
Tel Aviv, Israel
jonatha6@post.tau.ac.il
Ido Dagan
Bar Ilan University
Ramat Gan, Israel
dagan@cs.biu.ac.il
Abstract
We present a novel text exploration model,
which extends the scope of state-of-the-art
technologies by moving from standard con-
cept-based exploration to statement-based ex-
ploration. The proposed scheme utilizes the
textual entailment relation between statements
as the basis of the exploration process. A user
of our system can explore the result space of
a query by drilling down/up from one state-
ment to another, according to entailment re-
lations specified by an entailment graph and
an optional concept taxonomy. As a promi-
nent use case, we apply our exploration sys-
tem and illustrate its benefit on the health-care
domain. To the best of our knowledge this is
the first implementation of an exploration sys-
tem at the statement level that is based on the
textual entailment relation.
1 Introduction
Finding information in a large body of text is be-
coming increasingly more difficult. Standard search
engines output a set of documents for a given query,
but do not allow any exploration of the thematic
structure in the retrieved information. Thus, the need
for tools that allow to effectively sift through a target
set of documents is becoming ever more important.
Faceted search (Stoica and Hearst, 2007; Ka?ki,
2005) supports a better understanding of a target do-
main, by allowing exploration of data according to
multiple views or facets. For example, given a set of
documents on Nobel Prize laureates we might have
different facets corresponding to the laureate?s na-
tionality, the year when the prize was awarded, the
field in which it was awarded, etc. However, this
type of exploration is still severely limited insofar
that it only allows exploration by topic rather than
content. Put differently, we can only explore accord-
ing to what a document is about rather than what
a document actually says. For instance, the facets
for the query ?asthma? in the faceted search engine
Yippy include the concepts allergy and children, but
do not specify what are the exact relations between
these concepts and the query (e.g., allergy causes
asthma, and children suffer from asthma).
Berant et al (2010) proposed an exploration
scheme that focuses on relations between concepts,
which are derived from a graph describing textual
entailment relations between propositions. In their
setting a proposition consists of a predicate with two
arguments that are possibly replaced by variables,
such as ?X control asthma?. A graph that specifies
an entailment relation ?X control asthma ? X af-
fect asthma? can help a user, who is browsing doc-
uments dealing with substances that affect asthma,
drill down and explore only substances that control
asthma. This type of exploration can be viewed as
an extension of faceted search, where the new facet
concentrates on the actual statements expressed in
the texts.
In this paper we follow Berant et al?s proposal,
and present a novel entailment-based text explo-
ration system, which we applied to the health-care
domain. A user of this system can explore the re-
sult space of her query, by drilling down/up from
one proposition to another, according to a set of en-
tailment relations described by an entailment graph.
In Figure 1, for example, the user looks for ?things?
79
Figure 1: Exploring asthma results.
that affect asthma. She invokes an ?asthma? query
and starts drilling down the entailment graph to ?X
control asthma? (left column). In order to exam-
ine the arguments of a selected proposition, the user
may drill down/up a concept taxonomy that classi-
fies terms that occur as arguments. The user in Fig-
ure 1, for instance, drills down the concept taxon-
omy (middle column), in order to focus on Hor-
mones that control asthma, such as ?prednisone?
(right column). Each drill down/up induces a subset
of the documents that correspond to the aforemen-
tioned selections. The retrieved document in Fig-
ure 1 (bottom) is highlighted by the relevant propo-
sition, which clearly states that prednisone is often
given to treat asthma (and indeed in the entailment
graph ?X treat asthma? entails ?X control asthma?).
Our system is built over a corpus of documents,
a set of propositions extracted from the documents,
an entailment graph describing entailment relations
between propositions, and, optionally, a concept hi-
erarchy. The system implementation for the health-
care domain, for instance, is based on a web-crawled
health-care corpus, the propositions automatically
extracted from the corpus, entailment graphs bor-
rowed from Berant et al (2010), and the UMLS1
taxonomy. To the best of our knowledge this is the
first implementation of an exploration system, at the
proposition level, based on the textual entailment re-
lation.
2 Background
2.1 Exploratory Search
Exploratory search addresses the need of users to
quickly identify the important pieces of information
in a target set of documents. In exploratory search,
users are presented with a result set and a set of ex-
ploratory facets, which are proposals for refinements
of the query that can lead to more focused sets of
documents. Each facet corresponds to a clustering
of the current result set, focused on a more specific
topic than the current query. The user proceeds in
the exploration of the document set by selecting spe-
cific documents (to read them) or by selecting spe-
cific facets, to refine the result set.
1http://www.nlm.nih.gov/research/umls/
80
Early exploration technologies were based on a
single hierarchical conceptual clustering of infor-
mation (Hofmann, 1999), enabling the user to drill
up and down the concept hierarchies. Hierarchi-
cal faceted meta-data (Stoica and Hearst, 2007), or
faceted search, proposed more sophisticated explo-
ration possibilities by providing multiple facets and
a hierarchy per facet or dimension of the domain.
These types of exploration techniques were found to
be useful for effective access of information (Ka?ki,
2005).
In this work, we suggest proposition-based ex-
ploration as an extension to concept-based explo-
ration. Our intuition is that text exploration can
profit greatly from representing information not only
at the level of individual concepts, but also at the
propositional level, where the relations that link con-
cepts to one another are represented effectively in a
hierarchical entailment graph.
2.2 Entailment Graph
Recognizing Textual Entailment (RTE) is the task
of deciding, given two text fragments, whether the
meaning of one text can be inferred from another
(Dagan et al, 2009). For example, ?Levalbuterol
is used to control various kinds of asthma? entails
?Levalbuterol affects asthma?. In this paper, we use
the notion of proposition to denote a specific type
of text fragments, composed of a predicate with two
arguments (e.g., Levalbuterol control asthma).
Textual entailment systems are often based on en-
tailment rules which specify a directional inference
relation between two fragments. In this work, we
focus on leveraging a common type of entailment
rules, in which the left-hand-side of the rule (LHS)
and the right-hand-side of the rule (RHS) are propo-
sitional templates - a proposition, where one or both
of the arguments are replaced by a variable, e.g., ?X
control asthma? X affect asthma?.
The entailment relation between propositional
templates of a given corpus can be represented by an
entailment graph (Berant et al, 2010) (see Figure 2,
top). The nodes of an entailment graph correspond
to propositional templates, and its edges correspond
to entailment relations (rules) between them. Entail-
ment graph representation is somewhat analogous to
the formation of ontological relations between con-
cepts of a given domain, where in our case the nodes
correspond to propositional templates rather than to
concepts.
3 Exploration Model
In this section we extend the scope of state-of-the-
art exploration technologies by moving from stan-
dard concept-based exploration to proposition-based
exploration, or equivalently, statement-based explo-
ration. In our model, it is the entailment relation
between propositional templates which determines
the granularity of the viewed information space. We
first describe the inputs to the system and then detail
our proposed exploration scheme.
3.1 System Inputs
Corpus A collection of documents, which form
the search space of the system.
Extracted Propositions A set of propositions, ex-
tracted from the corpus document. The propositions
are usually produced by an extraction method, such
as TextRunner (Banko et al, 2007) or ReVerb (Fader
et al, 2011). In order to support the exploration
process, the documents are indexed by the proposi-
tional templates and argument terms of the extracted
propositions.
Entailment graph for predicates The nodes of
the entailment graph are propositional templates,
where edges indicate entailment relations between
templates (Section 2.2). In order to avoid circular-
ity in the exploration process, the graph is trans-
formed into a DAG, by merging ?equivalent? nodes
that are in the same strong connectivity component
(as suggested by Berant et al (2010)). In addition,
for clarity and simplicity, edges that can be inferred
by transitivity are omitted from the DAG. Figure 2
illustrates the result of applying this procedure to a
fragment of the entailment graph for ?asthma? (i.e.,
for propositional templates with ?asthma? as one of
the arguments).
Taxonomy for arguments The optional concept
taxonomy maps terms to one or more pre-defined
concepts, arranged in a hierarchical structure. These
terms may appear in the corpus as arguments of
predicates. Figure 3, for instance, illustrates a sim-
ple medical taxonomy, composed of three concepts
(medical, diseases, drugs) and four terms (cancer,
asthma, aspirin, flexeril).
81
Figure 2: Fragment of the entailment graph for ?asthma?
(top), and its conversion to a DAG (bottom).
3.2 Exploration Scheme
The objective of the exploration scheme is to support
querying and offer facets for result exploration, in
a visual manner. The following components cover
the various aspects of this objective, given the above
system inputs:
Querying The user enters a search term as a query,
e.g., ?asthma?. The given term induces a subgraph of
the entailment graph that contains all propositional
templates (graph nodes) with which this term ap-
pears as an argument in the extracted propositions
(see Figure 2). This subgraph is represented as a
DAG, as explained in Section 3.1, where all nodes
that have no parent are defined as the roots of the
DAG. As a starting point, only the roots of the DAG
are displayed to the user. Figure 4 shows the five
roots for the ?asthma? query.
Exploration process The user selects one of the
entailment graph nodes (e.g., ?associate X with
asthma?). At each exploration step, the user can
drill down to a more specific template or drill up to a
Figure 3: Partial medical taxonomy. Ellipses denote con-
cepts, while rectangles denote terms.
Figure 4: The roots of the entailment graph for the
?asthma? query.
more general template, by moving along the entail-
ment hierarchy. For example, the user in Figure 5,
expands the root ?associate X with asthma?, in order
to drill down through ?X affect asthma? to ?X control
Asthma?.
Selecting a propositional template (Figure 1, left
column) displays a concept taxonomy for the argu-
ments that correspond to the variable in the selected
template (Figure 1, middle column). The user can
explore these argument concepts by drilling up and
down the concept taxonomy. For example, in Fig-
ure 1 the user, who selected ?X control Asthma?,
explores the arguments of this template by drilling
down the taxonomy to the concept ?Hormone?.
Selecting a concept opens a third column, which
lists the terms mapped to this concept that occurred
as arguments of the selected template. For example,
in Figure 1, the user is examining the list of argu-
ments for the template ?X control Asthma?, which
are mapped to the concept ?Hormone?, focusing on
the argument ?prednisone?.
82
Figure 5: Part of the entailment graph for the ?asthma?
query, after two exploration steps. This corresponds to
the left column in Figure 1.
Document retrieval At any stage, the list of docu-
ments induced by the current selected template, con-
cept and argument is presented to the user, where
in each document snippet the relevant proposition
components are highlighted. Figure 1 (bottom)
shows such a retrieved document. The highlighted
extraction in the snippet, ?prednisone treat asthma?,
entails the proposition selected during exploration,
?prednisone control asthma?.
4 System Architecture
In this section we briefly describe system compo-
nents, as illustrated in the block diagram (Figure 6).
The search service implements full-text and
faceted search, and document indexing. The data
service handles data (e.g., documents) replication
for clients. The entailment service handles the logic
of the entailment relations (for both the entailment
graph and the taxonomy).
The index server applies periodic indexing of new
texts, and the exploration server serves the explo-
ration application on querying, exploration, and data
Figure 6: Block diagram of the exploration system.
access. The exploration application is the front-end
user application for the whole exploration process
described above (Section 3.2).
5 Application to the Health-care Domain
As a prominent use case, we applied our exploration
system to the health-care domain. With the advent
of the internet and social media, patients now have
access to new sources of medical information: con-
sumer health articles, forums, and social networks
(Boulos and Wheeler, 2007). A typical non-expert
health information searcher is uncertain about her
exact questions and is unfamiliar with medical ter-
minology (Trivedi, 2009). Exploring relevant infor-
mation about a given medical issue can be essential
and time-critical.
System implementation For the search service,
we used SolR servlet, where the data service is
built over FTP. The exploration application is im-
plemented as a web application.
Input resources We collected a health-care cor-
pus from the web, which contains more than 2M
sentences and about 50M word tokens. The texts
deal with various aspects of the health care domain:
answers to questions, surveys on diseases, articles
on life-style, etc. We extracted propositions from
the health-care corpus, by applying the method de-
scribed by Berant et al (2010). The corpus was
parsed, and propositions were extracted from depen-
dency trees according to the method suggested by
Lin and Pantel (2001), where propositions are de-
pendency paths between two arguments of a predi-
83
cate. We filtered out any proposition where one of
the arguments is not a term mapped to a medical
concept in the UMLS taxonomy.
For the entailment graph we used the 23 entail-
ment graphs published by Berant et al2. For the ar-
gument taxonomy we employed UMLS ? a database
that maps natural language phrases to over one mil-
lion unique concept identifiers (CUIs) in the health-
care domain. The CUIs are also mapped in UMLS
to a concept taxonomy for the health-care domain.
The web application of our system is
available at: http://132.70.6.148:
8080/exploration
6 Conclusion and Future Work
We presented a novel exploration model, which ex-
tends the scope of state-of-the-art exploration tech-
nologies by moving from standard concept-based
exploration to proposition-based exploration. Our
model combines the textual entailment paradigm
within the exploration process, with application to
the health-care domain. According to our model, it
is the entailment relation between propositions, en-
coded by the entailment graph and the taxonomy,
which leads the user between more specific and
more general statements throughout the search re-
sult space. We believe that employing the entail-
ment relation between propositions, which focuses
on the statements expressed in the documents, can
contribute to the exploration field and improve in-
formation access.
Our current application to the health-care domain
relies on a small set of entailment graphs for 23
medical concepts. Our ongoing research focuses on
the challenging task of learning a larger entailment
graph for the health-care domain. We are also in-
vestigating methods for evaluating the exploration
process (Borlund and Ingwersen, 1997). As noted
by Qu and Furnas (2008), the success of an ex-
ploratory search system does not depend simply on
how many relevant documents will be retrieved for a
given query, but more broadly on how well the sys-
tem helps the user with the exploratory process.
2http://www.cs.tau.ac.il/?jonatha6/
homepage_files/resources/HealthcareGraphs.
rar
Acknowledgments
This work was partially supported by the Israel
Ministry of Science and Technology, the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Communitys Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT).
References
Michele Banko, Michael J Cafarella, Stephen Soderl,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI, pages 2670?2676.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2010. Global learning of focused entailment graphs.
In Proceedings of ACL, Uppsala, Sweden.
Pia Borlund and Peter Ingwersen. 1997. The develop-
ment of a method for the evaluation of interactive in-
formation retrieval systems. Journal of Documenta-
tion, 53:225?250.
Maged N. Kamel Boulos and Steve Wheeler. 2007. The
emerging web 2.0 social software: an enabling suite of
sociable technologies in health and health care educa-
tion. Health Information & Libraries, 24:2?23.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rational, eval-
uation and approaches. Natural Language Engineer-
ing, 15(Special Issue 04):i?xvii.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In EMNLP, pages 1535?1545. ACL.
Thomas Hofmann. 1999. The cluster-abstraction model:
Unsupervised learning of topic hierarchies from text
data. In Proceedings of IJCAI, pages 682?687.
Mika Ka?ki. 2005. Findex: search result categories help
users when document ranking fails. In Proceedings
of SIGCHI, CHI ?05, pages 131?140, New York, NY,
USA. ACM.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7:343?360.
Yan Qu and George W. Furnas. 2008. Model-driven for-
mative evaluation of exploratory search: A study un-
der a sensemaking framework. Inf. Process. Manage.,
44:534?555.
Emilia Stoica and Marti A. Hearst. 2007. Automating
creation of hierarchical faceted metadata structures. In
Proceedings of NAACL HLT.
Mayank Trivedi. 2009. A study of search engines for
health sciences. International Journal of Library and
Information Science, 1(5):69?73.
84
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1331?1340,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Two Level Model for Context Sensitive Inference Rules
Oren Melamud?, Jonathan Berant?, Ido Dagan?, Jacob Goldberger?, Idan Szpektor?
? Computer Science Department, Bar-Ilan University
? Computer Science Department, Stanford University
? Faculty of Engineering, Bar-Ilan University
? Yahoo! Research Israel
{melamuo,dagan,goldbej}@{cs,cs,eng}.biu.ac.il
joberant@stanford.edu
idan@yahoo-inc.com
Abstract
Automatic acquisition of inference rules
for predicates has been commonly ad-
dressed by computing distributional simi-
larity between vectors of argument words,
operating at the word space level. A re-
cent line of work, which addresses context
sensitivity of rules, represented contexts in
a latent topic space and computed similar-
ity over topic vectors. We propose a novel
two-level model, which computes simi-
larities between word-level vectors that
are biased by topic-level context repre-
sentations. Evaluations on a naturally-
distributed dataset show that our model
significantly outperforms prior word-level
and topic-level models. We also release a
first context-sensitive inference rule set.
1 Introduction
Inference rules for predicates have been identi-
fied as an important component in semantic ap-
plications, such as Question Answering (QA)
(Ravichandran and Hovy, 2002) and Information
Extraction (IE) (Shinyama and Sekine, 2006). For
example, the inference rule ?X treat Y ? X relieve
Y? can be useful to extract pairs of drugs and the
illnesses which they relieve, or to answer a ques-
tion like ?Which drugs relieve headache??. Along
this vein, such inference rules constitute a crucial
component in generic modeling of textual infer-
ence, under the Textual Entailment paradigm (Da-
gan et al, 2006; Dinu and Wang, 2009).
Motivated by these needs, substantial research
was devoted to automatic learning of inference
rules from corpora, mostly in an unsupervised dis-
tributional setting. This research line was mainly
initiated by the highly-cited DIRT algorithm (Lin
and Pantel, 2001), which learns inference for bi-
nary predicates with two argument slots (like the
rule in the example above). DIRT represents a
predicate by two vectors, one for each of the ar-
gument slots, where the vector entries correspond
to the argument words that occurred with the pred-
icate in the corpus. Inference rules between pairs
of predicates are then identified by measuring the
similarity between their corresponding argument
vectors. This general scheme was further en-
hanced in several directions, e.g. directional sim-
ilarity (Bhagat et al, 2007; Szpektor and Dagan,
2008) and meta-classification over similarity val-
ues (Berant et al, 2011). Consequently, several
knowledge resources of inference rules were re-
leased, containing the top scoring rules for each
predicate (Schoenmackers et al, 2010; Berant et
al., 2011; Nakashole et al, 2012).
The above mentioned methods provide a sin-
gle confidence score for each rule, which is based
on the obtained degree of argument-vector sim-
ilarities. Thus, a system that applies an infer-
ence rule to a text may estimate the validity of
the rule application based on the pre-specified rule
score. However, the validity of an inference rule
may depend on the context in which it is applied,
such as the context specified by the given predi-
cate?s arguments. For example, ?AT&T acquire T-
Mobile ? AT&T purchase T-Mobile?, is a valid
application of the rule ?X acquire Y ? X pur-
chase Y?, while ?Children acquire skills ? Chil-
dren purchase skills? is not. To address this issue, a
line of works emerged which computes a context-
sensitive reliability score for each rule application,
based on the given context.
The major trend in context-sensitive inference
models utilizes latent or class-based methods for
context modeling (Pantel et al, 2007; Szpektor et
al., 2008; Ritter et al, 2010; Dinu and Lapata,
2010b). In particular, the more recent methods
(Ritter et al, 2010; Dinu and Lapata, 2010b) mod-
eled predicates in context as a probability distribu-
tion over topics learned by a Latent Dirichlet Allo-
1331
cation (LDA) model. Then, similarity is measured
between the two topic distribution vectors corre-
sponding to the two sides of the rule in the given
context, yielding a context-sensitive score for each
particular rule application.
We notice at this point that while context-
insensitive methods represent predicates by ar-
gument vectors in the original fine-grained word
space, context-sensitive methods represent them
as vectors at the level of latent topics. This raises
the question of whether such coarse-grained topic
vectors might be less informative in determining
the semantic similarity between the two predi-
cates.
To address this hypothesized caveat of prior
context-sensitive rule scoring methods, we pro-
pose a novel generic scheme that integrates word-
level and topic-level representations. Our scheme
can be applied on top of any context-insensitive
?base? similarity measure for rule learning, which
operates at the word level, such as Cosine or
Lin (Lin, 1998). Rather than computing a single
context-insensitive rule score, we compute a dis-
tinct word-level similarity score for each topic in
an LDA model. Then, when applying a rule in a
given context, these different scores are weighed
together based on the specific topic distribution
under the given context. This way, we calculate
similarity over vectors in the original word space,
while biasing them towards the given context via
a topic model.
In order to promote replicability and equal-term
comparison with our results, we based our experi-
ments on publicly available datasets, both for un-
supervised learning of the evaluated models and
for testing them over a random sample of rule ap-
plications. We apply our two-level scheme over
three state-of-the-art context-insensitive similar-
ity measures. The evaluation compares perfor-
mances both with the original context-insensitive
measures and with recent LDA-based context-
sensitive methods, showing consistent and robust
advantages of our scheme. Finally, we release
a context-sensitive rule resource comprising over
2,000 frequent verbs and one million rules.
2 Background and Model Setting
This section presents components of prior work
which are included in our model and experiments,
setting the technical preliminaries for the rest of
the paper. We first present context-insensitive rule
learning, based on distributional similarity at the
word level, and then context-sensitive scoring for
rule applications, based on topic-level similarity.
Some further discussion of related work appears
in Section 6.
2.1 Context-insensitive Rule Learning
A predicate inference rule ?LHS ? RHS?, such
as ?X acquire Y ? X purchase Y?, specifies a
directional inference relation between two predi-
cates. Each rule side consists of a lexical pred-
icate and (two) variable slots for its arguments.1
Different representations have been used to spec-
ify predicates and their argument slots, such as
word lemma sequences, regular expressions and
dependency parse fragments. A rule can be ap-
plied when its LHS matches a predicate with a
pair of arguments in a text, allowing us to infer its
RHS, with the corresponding instantiations for the
argument variables. For example, given the text
?AT&T acquires T-Mobile?, the above rule infers
?AT&T purchases T-Mobile?.
The DIRT algorithm (Lin and Pantel, 2001)
follows the distributional similarity paradigm to
learn predicate inference rules. For each predi-
cate, DIRT represents each of its argument slots
by an argument vector. We denote the two vectors
of the X and Y slots of a predicate pred by vxpred
and vypred, respectively. Each entry of a vector vcorresponds to a particular word (or term) w that
instantiated the argument slot in a learning corpus,
with a value v(w) = PMI(pred, w) (with PMI
standing for point-wise mutual information).
To learn inference rules, DIRT considers (in
principle) each pair of binary predicates that
occurred in the corpus for a candidate rule,
?LHS ? RHS?. Then, DIRT computes a reliabil-
ity score for the rule by combining the measured
similarities between the corresponding argument
vectors of the two rule sides. Concretely, denot-
ing by l and r the predicates appearing in the two
rule sides, DIRT?s reliability score is defined as
follows:
(1)scoreDIRT(LHS ? RHS)
=
?
sim(vxl , vxr ) ? sim(v
y
l , v
y
r )
where sim(v, v?) is a vector similarity measure.
Specifically, DIRT employs the Lin similarity
1We follow most of the inference-rule learning literature,
which focused on binary predicates. However, our context-
sensitive scheme can be applied to any arity.
1332
measure from (Lin, 1998), defined as follows:
(2)Lin(v, v?) =
?
w?v?v? [v(w) + v?(w)]?
w?v?v? [v(w) + v?(w)]
We note that the general DIRT scheme may be
used while employing other ?base? vector similar-
ity measures. For example, the Lin measure is
symmetric, and thus using it would yield the same
reliability score when swapping the two sides of
a rule. This issue has been addressed in a sepa-
rate line of research which introduced directional
similarity measures suitable for inference rela-
tions (Bhagat et al, 2007; Szpektor and Dagan,
2008; Kotlerman et al, 2010). In our experiments
we apply our proposed context-sensitive similarity
scheme over three different base similarity mea-
sures.
DIRT and similar context-insensitive inference
methods provide a single reliability score for a
learned inference rule, which aims to predict the
validity of the rule?s applications. However, as
exemplified in the Introduction, an inference rule
may be valid in some contexts but invalid in oth-
ers (e.g. acquiring entails purchasing for goods,
but not for skills). Since vector similarity in DIRT
is computed over the single aggregate argument
vector, the obtained reliability score tends to be
biased towards the dominant contexts of the in-
volved predicates. For example, we may expect
a higher score for ?acquire ? purchase? than for
?acquire ? learn?, since the former matches a
more frequent sense of acquire in a typical corpus.
Following this observation, it is desired to obtain
a context-sensitive reliability score for each rule
application in a given context, as described next.
2.2 Context-sensitive Rule Applications
To assess the reliability of applying an inference
rule in a given context we need some model for
context representation, that should affect the rule
reliability score. A major trend in past work is
to represent contexts in a reduced-dimensionality
latent or class-based model. A couple of earlier
works utilized a cluster-based model (Pantel et al,
2007) and an LSA-based model (Szpektor et al,
2008), in a selectional-preferences style approach.
Several more recent works utilize a Latent Dirich-
let Allocation (LDA) (Blei et al, 2003) frame-
work. We now present an underlying unified view
of the topic-level models in (Ritter et al, 2010;
Dinu and Lapata, 2010b), which we follow in our
own model and in comparative model evaluations.
We note that a similar LDA model construction
was employed also in (Se?aghdha, 2010), for esti-
mating predicate-argument likelihood.
First, an LDA model is constructed, as follows.
Similar to the construction of argument vectors
in the distributional model (described above in
subsection 2.1), all arguments instantiating each
predicate slot are extracted from a large learning
corpus. Then, for each slot of each predicate, a
pseudo-document is constructed containing the set
of all argument words that instantiated this slot in
the corpus. We denote the two documents con-
structed for the X and Y slots of a predicate pred
by dxpred and dypred, respectively. In comparison tothe distributional model, these two documents cor-
respond to the analogous argument vectors vxpred
and vypred, both containing exactly the same set ofwords.
Next, an LDA model is learned from the set
of all pseudo-documents, extracted for all predi-
cates.2 The learning process results in the con-
struction of K latent topics, where each topic t
specifies a distribution over all words, denoted by
p(w|t), and a topic distribution for each pseudo-
document d, denoted by p(t|d).
Within the LDA model we can derive the
a-posteriori topic distribution conditioned on a
particular word within a document, denoted by
p(t|d,w) ? p(w|t) ? p(t|d). In the topic-level
model, d corresponds to a predicate slot and w to
a particular argument word instantiating this slot.
Hence, p(t|d,w) is viewed as specifying the rele-
vance (or likelihood) of the topic t for the predi-
cate slot in the context of the given argument in-
stantiation. For example, for the predicate slot ?ac-
quire Y? in the context of the argument ?IBM?, we
expect high relevance for a topic about companies,
while in the context of the argument ?knowledge?
we expect high relevance for a topic about abstract
concepts. Accordingly, the distribution p(t|d,w)
over all topics provides a topic-level representa-
tion for a predicate slot in the context of a particu-
lar argument w. This representation is used by the
topic-level model to compute a context-sensitive
score for inference rule applications, as follows.
2We note that there are variants in the type of LDA model
and the way the pseudo-documents are constructed in the
referenced prior work. In order to focus on the inference
methods rather than on the underlying LDA model, we use
the LDA framework described in this paper for all compared
methods.
1333
Consider the application of an inference rule
?LHS ? RHS? in the context of a particular pair
of arguments for the X and Y slots, denoted by
wx and wy, respectively. Denoting by l and r the
predicates appearing in the two rule sides, the reli-
ability score of the topic-level model is defined as
follows (we present a geometric mean formulation
for consistency with DIRT):
(3)scoreTopic(LHS ? RHS, wx, wy)
=
?
sim(dxl , dxr , wx) ? sim(d
y
l , d
y
r , wy)
where sim(d, d?, w) is a topic-distribution similar-
ity measure conditioned on a given context word.
Specifically, Ritter et al (2010) utilized the dot
product form for their similarity measure:
(4)simDC(d, d?, w) = ?t[p(t|d,w) ? p(t|d?, w)]
(the subscript DC stands for double-conditioning,
as both distributions are conditioned on the argu-
ment word, unlike the measure below).
Dinu and Lapata (2010b) presented a slightly
different similarity measure for topic distributions
that performed better in their setting as well as in a
related later paper on context-sensitive scoring of
lexical similarity (Dinu and Lapata, 2010a). In this
measure, the topic distribution for the right hand
side of the rule is not conditioned on w:
(5)simSC(d, d?, w) = ?t[p(t|d,w) ? p(t|d?)]
(the subscript SC stands for single-conditioning,
as only the left distribution is conditioned on the
argument word). They also experimented with a
few variants for the structure of the similarity mea-
sure and assessed that best results are obtained
with the dot product form. In our experiments,
we employ these two similarity measures for topic
distributions as baselines representing topic-level
models.
Comparing the context-insensitive and context-
sensitive models, we see that both of them mea-
sure similarity between vector representations of
corresponding predicate slots. However, while
DIRT computes sim(v, v?) over vectors in the
original word-level space, topic-level models com-
pute sim(d, d?, w) by measuring similarity of vec-
tors in a reduced-dimensionality latent space. As
conjectured in the introduction, such coarse-grain
representation might lead to loss of information.
Hence, in the next section we propose a com-
bined two-level model, which represents predicate
slots in the original word-level space while biasing
the similarity measure through topic-level context
models.
3 Two-level Context-sensitive Inference
Our model follows the general DIRT scheme
while extending it to handle context-sensitive scor-
ing of rule applications, addressing the scenario
dealt by the context-sensitive topic models. In
particular, we define the context-sensitive score
scoreWT, where WT stands for the combination
of the Word/Topic levels:
(6)scoreWT(LHS ? RHS, wx, wy)
=
?
sim(vxl , vxr , wx) ? sim(v
y
l , v
y
r , wy)
Thus, our model computes similarity over word-
level (rather than topic-level) argument vectors,
while biasing it according to the specific argu-
ment words in the given rule application con-
text. The core of our contribution is thus defining
the context-sensitive word-level vector similarity
measure sim(v, v?, w), as described in the remain-
der of this section.
Following the methods in Section 2, for each
predicate pred we construct, from the learning
corpus, its argument vectors vxpred and vypred aswell as its argument pseudo-documents dxpred and
dypred. For convenience, when referring to an ar-gument vector v, we will denote the correspond-
ing pseudo-document by dv. Based on all pseudo-
documents we learn an LDA model and obtain its
associated probability distributions.
The calculation of sim(v, v?, w) is composed of
two steps. At learning time, we compute for each
candidate rule a separate, topic-biased, similarity
score per each of the topics in the LDA model.
Then, at rule application time, we compute an
overall reliability score for the rule by combining
the per-topic similarity scores, while biasing the
score combination according to the given context
of w. These two steps are described in the follow-
ing two subsections.
3.1 Topic-biased Word-vector Similarities
Given a pair of word vectors v and v?, and
any desired ?base? vector similarity measure sim
(e.g. simLin), we compute a topic-biased sim-
ilarity score for each LDA topic t, denoted by
simt(v, v?). simt(v, v?) is computed by applying
1334
the original similarity measure over topic-biased
versions of v and v?, denoted by vt and v?t:
simt(v, v?) = sim(vt, v?t)
where
vt(w) = v(w) ? p(t|dv, w)
That is, each value in the biased vector, vt(w),
is obtained by weighing the original value v(w)
by the relevance of the topic t to the argument
word w within dv. This way, rather than replac-
ing altogether the word-level values v(w) by the
topic probabilities p(t|dv, w), as done in the topic-
level models, we use the latter to only bias the for-
mer while preserving fine-grained word-level rep-
resentations. The notation Lint denotes the simt
measure when applied using Lin as the base simi-
larity measure sim.
This learning process results in K different
topic-biased similarity scores for each candidate
rule, where K is the number of LDA topics. Ta-
ble 1 illustrates topic-biased similarities for the Y
slot of two rules involving the predicate ?acquire?.
As can be seen, the topic-biased score Lint for ?ac-
quire? learn? for t2 is higher than the Lin score,
since this topic is characterized by arguments that
commonly appear with both predicates of the rule.
Consequently, the two predicates are found to be
distributionally similar when biased for this topic.
On the other hand, the topic-biased similarity for
t1 is substantially lower, since prominent words
in this topic are likely to occur with ?acquire? but
not with ?learn?, yielding low distributional simi-
larity. Opposite behavior is exhibited for the rule
?acquire? purchase?.
3.2 Context-sensitive Similarity
When applying an inference rule, we compute
for each slot its context-sensitive similarity score
simWT(v, v?, w), where v and v? are the slot?s ar-
gument vectors for the two rule sides and w is the
word instantiating the slot in the given rule appli-
cation. This score is computed as a weighted aver-
age of the rule?s K topic-biased similarity scores
simt. In this average, each topic is weighed by
its ?relevance? for the context in which the rule is
applied, which consists of the left-hand-side pred-
icate v and the argument w. This relevance is cap-
Topic t1 t2
Top 5
words
calbiochem rights
corel syndrome
networks majority
viacom knowledge
financially skill
acquire ? learn
Lint(v, v?) 0.040 0.334
Lin(v, v?) 0.165
acquire ? purchase
Lint(v, v?) 0.427 0.241
Lin(v, v?) 0.267
Table 1: Two characteristic topics for the Y slot of
?acquire?, along with their topic-biased Lin sim-
ilarities scores Lint, compared with the original
Lin similarity, for two rules. The relevance of each
topic to different arguments of ?acquire? is illus-
trated by showing the top 5 words in the argument
vector vyacquire for which the illustrated topic is the
most likely one.
tured by p(t|dv, w):
simWT(v, v?, w) =
?
t
[p(t|dv, w) ? simt(v, v?)]
(7)
This way, a rule application would obtain a high
score only if the current context fits those topics
for which the rule is indeed likely to be valid, as
captured by a high topic-biased similarity. The no-
tation LinWT denotes the simWT measure, when
using Lint as the topic-biased similarity measure.
Table 2 illustrates the calculation of context-
sensitive similarity scores in four rule applica-
tions, involving the Y slot of the predicate ?ac-
quire?. We observe that relative to the fixed
context-insensitive Lin score, the score of ?ac-
quire ? learn? is substantially promoted for
the argument ?skill? while being demoted for
?Skype?. The opposite behavior is observed for
?acquire ? purchase?, altogether demonstrating
how our model successfully biases the similarity
score according to rule validity in context.
4 Experimental Settings
To evaluate our model, we compare it both to
context-insensitive similarity measures as well as
to prior context-sensitive methods. Furthermore,
to better understand its applicability in typical
NLP tasks, we focus on an evaluation setting that
corresponds to a natural distribution of examples
from a large corpus.
1335
Topic t1 t2
Top 5
words
calbiochem rights
corel syndrome
networks majority
viacom knowledge
financially skill
?acquire Skype ? learn Skype?
p(t|dv, w) 0.974 0.000
Lint(v, v?) 0.040 0.334
LinWT(v, v?, w) 0.039
Lin(v, v?) 0.165
?acquire Skype ? purchase Skype?
p(t|dv, w) 0.974 0.000
Lint(v, v?) 0.427 0.241
LinWT(v, v?, w) 0.417
Lin(v, v?) 0.267
?acquire skill ? learn skill?
p(t|dv, w) 0.000 0.380
Lint(v, v?) 0.040 0.334
LinWT(v, v?, w) 0.251
Lin(v, v?) 0.165
?acquire skill ? purchase skill?
p(t|dv, w) 0.000 0.380
Lint(v, v?) 0.427 0.241
LinWT(v, v?, w) 0.181
Lin(v, v?) 0.267
Table 2: Context-sensitive similarity scores (in
bold) for the Y slots of four rule applications. The
components of the score calculation are shown for
the topics of Table 1. For each rule application,
the table shows a couple of the topic-biased scores
Lint of the rule (as in Table 1), along with the topic
relevance for the given context p(t|dv, w), which
weighs the topic-biased scores in the LinWT cal-
culation. The context-insensitive Lin score is
shown for comparison.
4.1 Evaluated Rule Application Methods
We evaluated the following rule application meth-
ods: the original context-insensitive word model,
following DIRT (Lin and Pantel, 2001), as de-
scribed in Equation 1, denoted by CI; our own
topic-word context-sensitive model, as described
in Equation 6, denoted by WT. In addition, we
evaluated two variants of the topic-level context-
sensitive model, denoted DC and SC. DC follows
the double conditioned contextualized similarity
measure according to Equation 4, as implemented
by (Ritter et al, 2010), while SC follows the sin-
gle conditioned one at Equation 5, as implemented
by (Dinu and Lapata, 2010b; Dinu and Lapata,
2010a).
Since our model can contextualize various dis-
tributional similarity measures, we evaluated the
performance of all the above methods on several
base similarity measures and their learned rule-
sets, namely Lin (Lin, 1998), BInc (Szpektor and
Dagan, 2008) and vector Cosine similarity. The
Lin similarity measure is described in Equation 2.
Binc (Szpektor and Dagan, 2008) is a directional
similarity measure between word vectors, which
outperformed Lin for predicate inference (Szpek-
tor and Dagan, 2008).
To build the rule-sets and models for the tested
approaches we utilized the ReVerb corpus (Fader
et al, 2011), a large scale publicly available web-
based open extractions data set, containing about
15 million unique template extractions.3 ReVerb
template extractions/instantiations are in the form
of a tuple (x, pred, y), containing pred, a verb
predicate, x, the argument instantiation of the tem-
plate?s slot X , and y, the instantiation of the tem-
plate?s slot Y .
ReVerb includes over 600,000 different tem-
plates that comprise a verb but may also include
other words, for example ?X can accommodate up
to Y?. Yet, many of these templates share a similar
meaning, e.g. ?X accommodate up to Y?, ?X can
accommodate up to Y?, ?X will accommodate up
to Y?, etc. Following Sekine (2005), we clustered
templates that share their main verb predicate in
order to scale down the number of different pred-
icates in the corpus and collect richer word co-
occurrence statistics per predicate.
Next, we applied some clean-up preprocessing
to the ReVerb extractions. This includes discard-
ing stop words, rare words and non-alphabetical
words instantiating either the X or the Y argu-
ments. In addition, we discarded all predicates
that co-occur with less than 100 unique argument
words in each slot. The remaining corpus consists
of 7 million unique extractions and 2,155 verb
predicates.
Finally, we trained an LDA model, as described
in Section 2, using Mallet (McCallum, 2002).
Then, for each original context-insensitive simi-
larity measure, we learned from ReVerb a rule-set
comprised of the top 500 rules for every identi-
fied predicate. To complete the learning, we cal-
culated the topic-biased similarity score for each
learned rule under each LDA topic, as specified
in our context-sensitive model. We release a rule
set comprising the top 500 context-sensitive rules
that we learned for each of the verb predicates in
our learning corpus, along with our trained LDA
3ReVerb is available at http://reverb.cs.
washington.edu/
1336
Method Lin BInc Cosine
Valid 266 254 272
Invalid 545 523 539
Total 811 777 811
Table 3: Sizes of rule application test set for each
learned rule-set.
model.4
4.2 Evaluation Task
To evaluate the performance of the different meth-
ods we chose the dataset constructed by Zeich-
ner et al (2012). 5 This publicly available dataset
contains about 6,500 manually annotated predi-
cate template rule applications, each one labeled
as correct or incorrect. For example, ?Jack agree
with Jill 9 Jack feel sorry for Jill? is a rule ap-
plication in this dataset, labeled as incorrect, and
?Registration open this month? Registration be-
gin this month? is another rule application, labeled
as correct. Rule applications were generated by
randomly sampling extractions from ReVerb, such
as (?Jack?,?agree with?,?Jill?) and then sampling
possible rules for each, such as ?agree with? feel
sorry for?. Hence, this dataset provides naturally
distributed rule inferences with respect to ReVerb.
Whenever we evaluated a distributional similar-
ity measure (namely Lin, BInc, or Cosine), we
discarded instances from Zeichner et al?s dataset
in which the assessed rule is not in the context-
insensitive rule-set learned for this measure or the
argument instantiation of the rule is not in the LDA
lexicon. We refer to the remaining instances as the
test set per measure, e.g. Lin?s test set. Table 3
details the size of each such test set in our experi-
ment.
Finally, the task under which we assessed the
tested models is to rank all rule applications in
each test set, aiming to rank the valid rule appli-
cations above the invalid ones.
5 Results
We evaluated the performance of each tested
method by measuring Mean Average Precision
(MAP) (Manning et al, 2008) of the rule appli-
cation ranking computed by this method. In order
4Our resource is available at: http://www.cs.biu.
ac.il/? nlp/downloads/wt-rules.html
5The dataset is available at: http://
www.cs.biu.ac.il/?nlp/downloads/
annotation-rule-application.htm
Method Lin BInc Cosine
CI 0.503 0.513 0.513
DC 0.451 (1200) 0.455 (1200) 0.455 (1200)
SC 0.443 (1200) 0.458 (1200) 0.452 (1200)
WT 0.562 (100) 0.584 (50) 0.565 (25)
Table 4: MAP values on corresponding test set ob-
tained by each method. Figures in parentheses in-
dicate optimal number of LDA topics.
to compute MAP values and corresponding statis-
tical significance, we randomly split each test set
into 30 subsets. For each method we computed
Average Precision on every subset and then took
the average over all subsets as the MAP value.
Since all tested context-sensitive approaches are
based on LDA topics, we varied for each method
the number of LDA topics K that optimizes its
performance, ranging from 25 to 1600 topics. We
used LDA hyperparameters ? = 0.01 and ? = 0.1
for K < 600 and ? = 50K for K >= 600.
Table 4 presents the optimal MAP performance
of each tested measure. Our main result is that
our model outperforms all other methods, both
context-insensitive and context-sensitive, by a rel-
ative increase of more than 10% for all three sim-
ilarity measures that we tested. This improvement
is statistically significant at p < 0.01 for BInc and
Lin, and p < 0.015 for Cosine, using paired t-
test. This shows that our model indeed success-
fully leverages contextual information beyond the
basic context-agnostic rule scores and is robust
across measures.
Surprisingly, both baseline topic-level context-
sensitive methods, namely DC and SC, underper-
formed compared to their context-insensitive base-
lines. While Dinu and Lapata (Dinu and Lap-
ata, 2010b) did show improvement over context-
insensitive DIRT, this result was obtained on the
verbs of the Lexical Substitution Task in SemEval
(McCarthy and Navigli, 2007), which was manu-
ally created with a bias for context-sensitive sub-
stitutions. However, our result suggests that topic-
level models might not be robust enough when ap-
plied to a random sample of inferences.
An interesting indication of the differences be-
tween our word-topic model, WT, and topic-only
models, DC and SC, lies in the optimal number of
LDA topics required for each method. The num-
ber of topics in the range 25-100 performed almost
equally well under the WT model for all base mea-
sures, with a moderate decline for higher numbers.
1337
The need for this rather small number of topics is
due to the nature of utilization of topics in WT.
Specifically, topics are leveraged for high-level
domain disambiguation, while fine grained word-
level distributional similarity is computed for each
rule under each such domain. This works best for
a relatively low number of topics. However, in
higher numbers, topics relate to narrower domains
and then topic biased word level similarity may
become less effective due to potential sparseness.
On the other hand, DC and SC rely on topics as
a surrogate to predicate-argument co-occurrence
features, and thus require a relatively large num-
ber of them to be effective.
Delving deeper into our test-set, Zeichner et al
provided a more detailed annotation for each in-
valid rule application. Specifically, they annotated
whether the context under which the rule is ap-
plied is valid. For example, in ?John bought my
car 9 John sold my car? the inference is invalid
due to an inherently incorrect rule, but the con-
text is valid. On the other hand in ?my boss raised
my salary 9 my boss constructed my salary? the
context {?my boss?, ?my salary?} for applying
?raise? construct? is invalid. Following, we split
the test-set for the base Lin measure into two test-
sets: (a) test-setvc, which includes all correct rule
applications and incorrect ones only under valid
contexts, and (b) test-setivc, which includes again
all correct rule applications but incorrect ones only
under invalid contexts.
Table 5 presents the performance of each com-
pared method on the two test sets. On test-
setivc, where context mismatches are abundant,
our model outperformed all other baselines (sta-
tistically significant at p < 0.01). In addition,
this time DC slightly outperformed CI. This re-
sult more explicitly shows the advantages of in-
tegrating word-level and context-sensitive topic-
level similarities for differentiating valid and in-
valid contexts for rule applications. Yet, many in-
valid rule applications occur under valid contexts
due to inherently incorrect rules, and we want to
make sure that also in this scenario our model
does not fall behind the context-insensitive mea-
sure. Indeed, on test-setvc, in which context mis-
matches are rare, our algorithm is still better than
the original measure, indicating that WT can be
safely applied to distributional similarity measures
without concerns of reduced performance in dif-
ferent context scenarios.
test-setivc test-setvc
Size
(valid:invalid)
432
(266:166)
645
(266:379)
CI 0.780 0.587
DC 0.796 0.498
SC 0.779 0.512
WT 0.854 0.621
Table 5: MAP results for the two split Lin test-
sets.
6 Discussion and Future Work
This paper addressed the problem of computing
context-sensitive reliability scores for predicate in-
ference rules. In particular, we proposed a novel
scheme that applies over any base distributional
similarity measure which operates at the word
level, and computes a single context-insensitive
score for a rule. Based on such a measure, our
scheme constructs a context-sensitive similarity
measure that computes a reliability score for pred-
icate inference rules applications in the context of
given arguments.
The contextualization of the base similarity
score was obtained using a topic-level LDA
model, which was used in a novel way. First,
it provides a topic bias for learning separate per-
topic word-level similarity scores between predi-
cates. Then, given a specific candidate rule ap-
plication, the LDA model is used to infer the
topic distribution relevant to the context speci-
fied by the given arguments. Finally, the context-
sensitive rule application score is computed as a
weighted average of the per-topic word-level sim-
ilarity scores, which are weighed according to the
inferred topic distribution.
While most works on context-insensitive pred-
icate inference rules, such as DIRT (Lin and Pan-
tel, 2001), are based on word-level similarity mea-
sures, almost all prior models addressing context-
sensitive predicate inference rules are based on
topic models (except for (Pantel et al, 2007),
which was outperformed by later models). We
therefore focused on comparing the performance
of our two-level scheme with state-of-the-art prior
topic-level and word-level models of distributional
similarity, over a random sample of inference rule
applications. Under this natural setting, the two-
level scheme consistently outperformed both types
of models when tested with three different base
similarity measures. Notably, our model shows
stable performance over a large subset of the data
1338
where context sensitivity is rare, while topic-level
models tend to underperform in such cases com-
pared to the base context-insensitive methods.
Our work is closely related to another research
line that addresses lexical similarity and substi-
tution scenarios in context. While we focus on
lexical-syntactic predicate templates and instanti-
ations of their argument slots as context, lexical
similarity methods consider various lexical units
that are not necessarily predicates, with their con-
text typically being the collection of words in a
window around them.
Various approaches have been proposed to ad-
dress lexical similarity. A number of works are
based on a compositional semantics approach,
where a prior representation of a target lexical unit
is composed with the representations of words in
its given context (Mitchell and Lapata, 2008; Erk
and Pado?, 2008; Thater et al, 2010). Other works
(Erk and Pado?, 2010; Reisinger and Mooney,
2010) use a rather large word window around tar-
get words and compute similarities between clus-
ters comprising instances of word windows. In ad-
dition, (Dinu and Lapata, 2010a) adapted the pred-
icate inference topic model from (Dinu and Lap-
ata, 2010b) to compute lexical similarity in con-
text.
A natural extension of our work would be to ex-
tend our two level model to accommodate context-
sensitive lexical similarity. For this purpose we
will need to redefine the scope of context in our
model, and adapt our method to compute context-
biased lexical similarities accordingly. Then we
will also be able to evaluate our model on the
Lexical Substitution Task (McCarthy and Navigli,
2007), which has been commonly used in recent
years as a benchmark for context-sensitive lexical
similarity models.
In a different NLP task, Eidelman et al (2012)
utilize a similar approach to ours for improving
the performance of statistical machine translation
(SMT). They learn an LDA model on the source
language side of the training corpus with the pur-
pose of identifying implicit sub-domains. Then
they utilize the distribution over topics inferred for
each document in their corpus to compute sepa-
rate per-topic translation probability tables. Fi-
nally, they train a classifier to translate a given
target word based on these tables and the inferred
topic distribution of the given document in which
the target word appears. A notable difference be-
tween our approach and theirs is that we use predi-
cate pseudo-documents consisting of argument in-
stantiations to learn our LDA model, while Eidel-
man et al use the real documents in a corpus.
We believe that combining these two approaches
may improve performance for both textual infer-
ence and SMT and plan to experiment with this
direction in future work.
Acknowledgments
This work was partially supported by the Israeli
Ministry of Science and Technology grant 3-8705,
the Israel Science Foundation grant 880/12, and
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
References
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
ACL.
Rahul Bhagat, Patrick Pantel, Eduard Hovy, and Ma-
rina Rey. 2007. Ledir: An unsupervised algorithm
for learning directionality of inference rules. In Pro-
ceedings of EMNLP-CoNLL.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. the Journal of ma-
chine Learning research, 3:993?1022.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Lecture Notes in Computer Science,
volume 3944, pages 177?190.
Georgiana Dinu and Mirella Lapata. 2010a. Measur-
ing distributional similarity in context. In Proceed-
ings of EMNLP.
Georgiana Dinu and Mirella Lapata. 2010b. Topic
models for meaning similarity in context. In Pro-
ceedings of COLING: Posters.
Georgiana Dinu and Rui Wang. 2009. Inference rules
and their application to recognizing textual entail-
ment. In Proceedings EACL.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic transla-
tion model adaptation. In Proceedings of the ACL
conference short papers.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP.
Katrin Erk and Sebastian Pado?. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of the ACL conference short papers.
1339
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Dekang Lin and Patrick Pantel. 2001. DIRT ? discov-
ery of inference rules from text. In Proceedings of
ACM SIGKDDConference on Knowledge Discovery
and Data Mining 2001.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In
Proceedings of SemEval.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: A taxonomy of relational
patterns with semantic types. EMNLP12.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Hu-
man Language Technologies 2007: The Conference
of the North American Chapter of the Association
for Computational Linguistics.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In Proceedings of ACL.
Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter
of the Association for Computational Linguistics.
Alan Ritter, Oren Etzioni, et al 2010. A latent dirich-
let alocation method for selectional preferences. In
Proceedings of ACL.
Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and
Daniel Weld. 2010. Learning first-order horn
clauses from web text. In Proceedings of EMNLP.
Diarmuid O Se?aghdha. 2010. Latent variable models
of selectional preference. In Proceedings of ACL.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of the Third International Workshop on
Paraphrasing (IWP2005).
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main
Conference.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In Pro-
ceedings of ACL-08: HLT.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of ACL.
Naomi Zeichner, Jonathan Berant, and Ido Dagan.
2012. Crowdsourcing inference-rule evaluation. In
Proceedings of ACL (short papers).
1340
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1415?1425,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Semantic Parsing via Paraphrasing
Jonathan Berant
Stanford University
joberant@stanford.edu
Percy Liang
Stanford University
pliang@cs.stanford.edu
Abstract
A central challenge in semantic parsing is
handling the myriad ways in which knowl-
edge base predicates can be expressed.
Traditionally, semantic parsers are trained
primarily from text paired with knowledge
base information. Our goal is to exploit
the much larger amounts of raw text not
tied to any knowledge base. In this pa-
per, we turn semantic parsing on its head.
Given an input utterance, we first use a
simple method to deterministically gener-
ate a set of candidate logical forms with
a canonical realization in natural language
for each. Then, we use a paraphrase model
to choose the realization that best para-
phrases the input, and output the corre-
sponding logical form. We present two
simple paraphrase models, an association
model and a vector space model, and train
them jointly from question-answer pairs.
Our system PARASEMPRE improves state-
of-the-art accuracies on two recently re-
leased question-answering datasets.
1 Introduction
We consider the semantic parsing problem of map-
ping natural language utterances into logical forms
to be executed on a knowledge base (KB) (Zelle
and Mooney, 1996; Zettlemoyer and Collins,
2005; Wong and Mooney, 2007; Kwiatkowski
et al, 2010). Scaling semantic parsers to large
knowledge bases has attracted substantial atten-
tion recently (Cai and Yates, 2013; Berant et al,
2013; Kwiatkowski et al, 2013), since it drives
applications such as question answering (QA) and
information extraction (IE).
Semantic parsers need to somehow associate
natural language phrases with logical predicates,
e.g., they must learn that the constructions ?What
What party did Clay establish?
paraphrase model
What political party founded by Henry Clay? ... What event involved the people Henry Clay?
Type.PoliticalParty u Founder.HenryClay ... Type.Event u Involved.HenryClay
Whig Party
Figure 1: Semantic parsing via paraphrasing: For each
candidate logical form (in red), we generate canonical utter-
ances (in purple). The model is trained to paraphrase the in-
put utterance (in green) into the canonical utterances associ-
ated with the correct denotation (in blue).
does X do for a living??, ?What is X?s profes-
sion??, and ?Who is X??, should all map to the
logical predicate Profession. To learn these map-
pings, traditional semantic parsers use data which
pairs natural language with the KB. However, this
leaves untapped a vast amount of text not related
to the KB. For instance, the utterances ?Where is
ACL in 2014?? and ?What is the location of ACL
2014?? cannot be used in traditional semantic
parsing methods, since the KB does not contain
an entity ACL2014, but this pair clearly contains
valuable linguistic information. As another refer-
ence point, out of 500,000 relations extracted by
the ReVerb Open IE system (Fader et al, 2011),
only about 10,000 can be aligned to Freebase (Be-
rant et al, 2013).
In this paper, we present a novel approach for
semantic parsing based on paraphrasing that can
exploit large amounts of text not covered by the
KB (Figure 1). Our approach targets factoid ques-
tions with a modest amount of compositionality.
Given an input utterance, we first use a simple de-
terministic procedure to construct a manageable
set of candidate logical forms (ideally, we would
generate canonical utterances for all possible logi-
cal forms, but this is intractable). Next, we heuris-
1415
utterance
underspecifiedlogicalform
canonicalutterance
logicalform
ontologymatching
paraphrase
direct(traditional)
(Kwiatkowski et al 2013)
(this work)
Figure 2: The main challenge in semantic parsing is cop-
ing with the mismatch between language and the KB. (a)
Traditionally, semantic parsing maps utterances directly to
logical forms. (b) Kwiatkowski et al (2013) map the utter-
ance to an underspecified logical form, and perform ontology
matching to handle the mismatch. (c) We approach the prob-
lem in the other direction, generating canonical utterances for
logical forms, and use paraphrase models to handle the mis-
match.
tically generate canonical utterances for each log-
ical form based on the text descriptions of predi-
cates from the KB. Finally, we choose the canoni-
cal utterance that best paraphrases the input utter-
ance, and thereby the logical form that generated
it. We use two complementary paraphrase mod-
els: an association model based on aligned phrase
pairs extracted from a monolingual parallel cor-
pus, and a vector space model, which represents
each utterance as a vector and learns a similarity
score between them. The entire system is trained
jointly from question-answer pairs only.
Our work relates to recent lines of research
in semantic parsing and question answering.
Kwiatkowski et al (2013) first maps utterances to
a domain-independent intermediate logical form,
and then performs ontology matching to produce
the final logical form. In some sense, we ap-
proach the problem from the opposite end, using
an intermediate utterance, which allows us to em-
ploy paraphrasing methods (Figure 2). Fader et
al. (2013) presented a QA system that maps ques-
tions onto simple queries against Open IE extrac-
tions, by learning paraphrases from a large mono-
lingual parallel corpus, and performing a single
paraphrasing step. We adopt the idea of using
paraphrasing for QA, but suggest a more general
paraphrase model and work against a formal KB
(Freebase).
We apply our semantic parser on two datasets:
WEBQUESTIONS (Berant et al, 2013), which
contains 5,810 question-answer pairs with
common questions asked by web users; and
FREE917 (Cai and Yates, 2013), which has
917 questions manually authored by annota-
tors. On WEBQUESTIONS, we obtain a relative
improvement of 12% in accuracy over the
state-of-the-art, and on FREE917 we match the
current best performing system. The source
code of our system PARASEMPRE is released
at http://www-nlp.stanford.edu/
software/sempre/.
2 Setup
Our task is as follows: Given (i) a knowledge
base K, and (ii) a training set of question-answer
pairs {(x
i
, y
i
)}
n
i=1
, output a semantic parser that
maps new questions x to answers y via latent log-
ical forms z. Let E denote a set of entities (e.g.,
BillGates), and let P denote a set of properties
(e.g., PlaceOfBirth). A knowledge base K is a
set of assertions (e
1
, p, e
2
) ? E ? P ? E (e.g.,
(BillGates, PlaceOfBirth, Seattle)). We use
the Freebase KB (Google, 2013), which has 41M
entities, 19K properties, and 596M assertions.
To query the KB, we use a logical language
called simple ?-DCS. In simple ?-DCS, an
entity (e.g., Seattle) is a unary predicate
(i.e., a subset of E) denoting a singleton set
containing that entity. A property (which is a
binary predicate) can be joined with a unary
predicate; e.g., Founded.Microsoft denotes
the entities that are Microsoft founders. In
PlaceOfBirth.Seattle u Founded.Microsoft,
an intersection operator allows us to denote
the set of Seattle-born Microsoft founders.
A reverse operator reverses the order of ar-
guments: R[PlaceOfBirth].BillGates
denotes Bill Gates?s birthplace (in con-
trast to PlaceOfBirth.Seattle). Lastly,
count(Founded.Microsoft) denotes set cardinal-
ity, in this case, the number of Microsoft founders.
The denotation of a logical form z with respect to
a KB K is given by JzK
K
. For a formal description
of simple ?-DCS, see Liang (2013) and Berant et
al. (2013).
3 Model overview
We now present the general framework for seman-
tic parsing via paraphrasing, including the model
and the learning algorithm. In Sections 4 and 5,
we provide the details of our implementation.
Canonical utterance construction Given an ut-
terance x and the KB, we construct a set of candi-
1416
date logical forms Z
x
, and then for each z ? Z
x
generate a small set of canonical natural language
utterances C
z
. Our goal at this point is only to gen-
erate a manageable set of logical forms containing
the correct one, and then generate an appropriate
canonical utterance from it. This strategy is feasi-
ble in factoid QA where compositionality is low,
and so the size of Z
x
is limited (Section 4).
Paraphrasing We score the canonical utter-
ances in C
z
with respect to the input utterance x
using a paraphrase model, which offers two ad-
vantages. First, the paraphrase model is decoupled
from the KB, so we can train it from large text cor-
pora. Second, natural language utterances often do
not express predicates explicitly, e.g., the question
?What is Italy?s money?? expresses the binary
predicate CurrencyOf with a possessive construc-
tion. Paraphrasing methods are well-suited for
handling such text-to-text gaps. Our framework
accommodates any paraphrasing method, and in
this paper we propose an association model that
learns to associate natural language phrases that
co-occur frequently in a monolingual parallel cor-
pus, combined with a vector space model, which
learns to score the similarity between vector rep-
resentations of natural language utterances (Sec-
tion 5).
Model We define a discriminative log-linear
model that places a probability distribution over
pairs of logical forms and canonical utterances
(c, z), given an utterance x:
p
?
(c, z | x) =
exp{?(x, c, z)
>
?}
?
z
?
?Z
x
,c
?
?C
z
exp{?(x, c
?
, z
?
)
>
?}
,
where ? ? R
b
is the vector of parameters to be
learned, and ?(x, c, z) is a feature vector extracted
from the input utterance x, the canonical utterance
c, and the logical form z. Note that the candidate
set of logical forms Z
x
and canonical utterances
C
x
are constructed during the canonical utterance
construction phase.
The model score decomposes into two terms:
?(x, c, z)
>
? = ?
pr
(x, c)
>
?
pr
+ ?
lf
(x, z)
>
?
lf
,
where the parameters ?
pr
define the paraphrase
model (Section 5), which is based on features ex-
tracted from text only (the input and canonical ut-
terance). The parameters ?
lf
correspond to seman-
tic parsing features based on the logical form and
input utterance, and are briefly described in this
section.
Many existing paraphrase models introduce la-
tent variables to describe the derivation of c from
x, e.g., with transformations (Heilman and Smith,
2010; Stern and Dagan, 2011) or alignments
(Haghighi et al, 2005; Das and Smith, 2009;
Chang et al, 2010). However, we opt for a sim-
pler paraphrase model without latent variables in
the interest of efficiency.
Logical form features The parameters ?
lf
corre-
spond to the following features adopted from Be-
rant et al (2013). For a logical form z, we extract
the size of its denotation JzK
K
. We also add all bi-
nary predicates in z as features. Moreover, we ex-
tract a popularity feature for predicates based on
the number of instances they have in K. For Free-
base entities, we extract a popularity feature based
on the entity frequency in an entity linked subset
of Reverb (Lin et al, 2012). Lastly, Freebase for-
mulas have types (see Section 4), and we conjoin
the type of z with the first word of x, to capture the
correlation between a word (e.g., ?where?) with
the Freebase type (e.g., Location).
Learning As our training data consists of
question-answer pairs (x
i
, y
i
), we maximize the
log-likelihood of the correct answer. The proba-
bility of an answer y is obtained by marginaliz-
ing over canonical utterances c and logical forms
z whose denotation is y. Formally, our objective
function O(?) is as follows:
O(?) =
n
?
i=1
log p
?
(y
i
| x
i
)? ????
1
,
p
?
(y | x) =
?
z?Z
x
:y=JzK
K
?
c?C
z
p
?
(c, z | x).
The strength ? of the L
1
regularizer is set based
on cross-validation. We optimize the objective by
initializing the parameters ? to zero and running
AdaGrad (Duchi et al, 2010). We approximate
the set of pairs of logical forms and canonical ut-
terances with a beam of size 2,000.
4 Canonical utterance construction
We construct canonical utterances in two steps.
Given an input utterance x, we first construct a
set of logical forms Z
x
, and then generate canon-
ical utterances from each z ? Z
x
. Both steps are
performed with a small and simple set of deter-
ministic rules, which suffices for our datasets, as
1417
they consist of factoid questions with a modest
amount of compositional structure. We describe
these rules below for completeness. Due to its so-
porific effect though, we advise the reader to skim
it quickly.
Candidate logical forms We consider logical
forms defined by a set of templates, summarized
in Table 1. The basic template is a join of a bi-
nary and an entity, where a binary can either be
one property p.e (#1 in the table) or two proper-
ties p
1
.p
2
.e (#2). To handle cases of events in-
volving multiple arguments (e.g., ?Who did Brad
Pitt play in Troy??), we introduce the template
p.(p
1
.e
1
u p
2
.e
2
) (#3), where the main event is
modified by more than one entity. Logical forms
can be further modified by a unary ?filter?, e.g.,
the answer to ?What composers spoke French??
is a set of composers, i.e., a subset of all people
(#4). Lastly, we handle aggregation formulas for
utterances such as ?How many teams are in the
NCAA?? (#5).
To construct candidate logical forms Z
x
for a
given utterance x, our strategy is to find an en-
tity in x and grow the logical form from that en-
tity. As we show later, this procedure actually pro-
duces a set with better coverage than construct-
ing logical forms recursively from spans of x, as
is done in traditional semantic parsing. Specifi-
cally, for every span of x, we take at most 10 en-
tities whose Freebase descriptions approximately
match the span. Then, we join each entity e with
all type-compatible
1
binaries b, and add these log-
ical forms to Z
x
(#1 and #2).
To construct logical forms with multiple en-
tities (#3) we do the following: For any logical
form z = p.p
1
.e
1
, where p
1
has type signa-
ture (t
1
, ?), we look for other entities e
2
that
were matched in x. Then, we add the logical
form p.(p
1
.e
1
u p
2
.e
2
), if there exists a binary
p
2
with a compatible type signature (t
1
, t
2
),
where t
2
is one of e
2
?s types. For example, for
the logical form Character.Actor.BradPitt,
if we match the entity Troy in x, we obtain
Character.(Actor.BradPitt u Film.Troy).
We further modify logical forms by intersecting
with a unary filter (#4): given a formula z with
some Freebase type (e.g., People), we look at
all Freebase sub-types t (e.g., Composer), and
1
Entities in Freebase are associated with a set of types,
and properties have a type signature (t
1
, t
2
) We use these
types to compute an expected type t for any logical form z.
check whether one of their Freebase descriptions
(e.g., ?composer?) appears in x. If so, we
add the formula Type.t u z to Z
x
. Finally, we
check whether x is an aggregation formula by
identifying whether it starts with phrases such as
?how many? or ?number of? (#5).
On WEBQUESTIONS, this results in 645 for-
mulas per utterance on average. Clearly, we can
increase the expressivity of this step by expand-
ing the template set. For example, we could han-
dle superlative utterances (?What NBA player is
tallest??) by adding a template with an argmax
operator.
Utterance generation While mapping general
language utterances to logical forms is hard, we
observe that it is much easier to generate a canoni-
cal natural language utterances of our choice given
a logical form. Table 2 summarizes the rules used
to generate canonical utterances from the template
p.e. Questions begin with a question word, are fol-
lowed by the Freebase description of the expected
answer type (d(t)), and followed by Freebase de-
scriptions of the entity (d(e)) and binary (d(p)).
To fill in auxiliary verbs, determiners, and prepo-
sitions, we parse the description d(p) into one of
NP, VP, PP, or NP VP. This determines the gen-
eration rule to be used.
Each Freebase property p has an explicit prop-
erty p
?
equivalent to the reverse R[p] (e.g.,
ContainedBy and R[Contains]). For each logical
form z, we also generate using equivalent logical
forms where p is replaced with R[p
?
]. Reversed
formulas have different generation rules, since en-
tities in these formulas are in the subject position
rather than object position.
We generate the description d(t) from the Free-
base description of the type of z (this handles #4).
For the template p
1
.p
2
.e (#2), we have a similar
set of rules, which depends on the syntax of d(p
1
)
and d(p
2
) and is omitted for brevity. The tem-
plate p.(p
1
.e
1
u p
2
.e
2
) (#3) is generated by ap-
pending the prepositional phrase in d(e
2
), e.g,
?What character is the character of Brad Pitt in
Troy??. Lastly, we choose the question phrase
?How many? for aggregation formulas (#5), and
?What? for all other formulas.
We also generate canonical utterances using
an alignment lexicon, released by Berant et al
(2013), which maps text phrases to Freebase bi-
nary predicates. For a binary predicate b mapped
from text phrase d(b), we generate the utterance
1418
# Template Example Question
1 p.e Directed.TopGun Who directed Top Gun?
2 p
1
.p
2
.e Employment.EmployerOf.SteveBalmer Where does Steve Balmer work?
3 p.(p
1
.e
1
u p
2
.e
2
) Character.(Actor.BradPitt u Film.Troy) Who did Brad Pitt play in Troy?
4 Type.t u z Type.Composer u SpeakerOf.French What composers spoke French?
5 count(z) count(BoatDesigner.NatHerreshoff) How many ships were designed by
Nat Herreshoff?
Table 1: Logical form templates, where p, p
1
, p
2
are Freebase properties, e, e
1
, e
2
are Freebase entities, t is a Freebase type,
and z is a logical form.
d(p) Categ. Rule Example
p.e NP WH d(t) has d(e) as NP ? What election contest has George Bush as winner?
VP WH d(t) (AUX) VP d(e) ? What radio station serves area New-York?
PP WH d(t) PP d(e) ? What beer from region Argentina?
NP VP WH d(t) VP the NP d(e) ? What mass transportation system served the area Berlin?
R(p).e NP WH d(t) is the NP of d(e) ? What location is the place of birth of Elvis Presley?
VP WH d(t) AUX d(e) VP ? What film is Brazil featured in?
PP WH d(t) d(e) PP ? What destination Spanish steps near travel destination?
NP VP WH NP is VP by d(e) ? What structure is designed by Herod?
Table 2: Generation rules for templates of the form p.e and R[p].e based on the syntactic category of the property description.
Freebase descriptions for the type, entity, and property are denoted by d(t), d(e) and d(p) respectively. The surface form of the
auxiliary AUX is determined by the POS tag of the verb inside the VP tree.
WH d(t) d(b) d(e) ?. On the WEBQUESTIONS
dataset, we generate an average of 1,423 canonical
utterances c per input utterance x. In Section 6,
we show that an even simpler method of gener-
ating canonical utterances by concatenating Free-
base descriptions hurts accuracy by only a modest
amount.
5 Paraphrasing
Once the candidate set of logical forms paired with
canonical utterances is constructed, our problem
is reduced to scoring pairs (c, z) based on a para-
phrase model. The NLP paraphrase literature is
vast and ranges from simple methods employing
surface features (Wan et al, 2006), through vec-
tor space models (Socher et al, 2011), to latent
variable models (Das and Smith, 2009; Wang and
Manning, 2010; Stern and Dagan, 2011).
In this paper, we focus on two paraphrase mod-
els that emphasize simplicity and efficiency. This
is important since for each question-answer pair,
we consider thousands of canonical utterances as
potential paraphrases. In contrast, traditional para-
phrase detection (Dolan et al, 2004) and Recog-
nizing Textual Entailment (RTE) tasks (Dagan et
al., 2013) consider examples consisting of only a
single pair of candidate paraphrases.
Our paraphrase model decomposes into an as-
sociation model and a vector space model:
?
pr
(x, c)
>
?
pr
= ?
as
(x, c)
>
?
as
+ ?
vs
(x, c)
>
?
vs
.
x : What type of music did Richard Wagner play
c : What is the musical genres of Richard Wagner
Figure 3: Token associations extracted for a paraphrase
pair. Blue and dashed (red and solid) indicate positive (neg-
ative) score. Line width is proportional to the absolute value
of the score.
5.1 Association model
The goal of the association model is to deter-
mine whether x and c contain phrases that are
likely to be paraphrases. Given an utterance x =
?x
0
, x
1
, .., x
n?1
?, we denote by x
i:j
the span from
token i to token j. For each pair of utterances
(x, c), we go through all spans of x and c and
identify a set of pairs of potential paraphrases
(x
i:j
, c
i
?
:j
?
), which we call associations. (We will
describe how associations are identified shortly.)
We then define features on each association; the
weighted combination of these features yields a
score. In this light, associations can be viewed
as soft paraphrase rules. Figure 3 presents exam-
ples of associations extracted from a paraphrase
pair and visualizes the learned scores. We can see
that our model learns a positive score for associ-
ating ?type? with ?genres?, and a negative score
for associating ?is? with ?play?.
We define associations in x and c primarily by
looking up phrase pairs in a phrase table con-
structed using the PARALEX corpus (Fader et al,
2013). PARALEX is a large monolingual parallel
1419
Category Description
Assoc. lemma(x
i:j
) ? lemma(c
i
?
:j
?
)
pos(x
i:j
) ? pos(c
i
?
:j
?
)
lemma(x
i:j
) = lemma(c
i
?
:j
?
)?
pos(x
i:j
) = pos(c
i
?
:j
?
)?
lemma(x
i:j
) and lemma(c
i
?
:j
?
) are synonyms?
lemma(x
i:j
) and lemma(c
i
?
:j
?
) are derivations?
Deletions Deleted lemma and POS tag
Table 3: Full feature set in the association model. x
i:j
and
c
i
?
:j
?
denote spans from x and c. pos(x
i:j
) and lemma(x
i:j
)
denote the POS tag and lemma sequence of x
i:j
.
corpora, containing 18 million pairs of question
paraphrases from wikianswers.com, which
were tagged as having the same meaning by users.
PARALEX is suitable for our needs since it fo-
cuses on question paraphrases. For example, the
phrase ?do for a living? occurs mostly in ques-
tions, and we can extract associations for this
phrase from PARALEX. Paraphrase pairs in PAR-
ALEX are word-aligned using standard machine
translation methods. We use the word alignments
to construct a phrase table by applying the con-
sistent phrase pair heuristic (Och and Ney, 2004)
to all 5-grams. This results in a phrase table with
approximately 1.3 million phrase pairs. We let A
denote this set of mined candidate associations.
For a pair (x, c), we also consider as candidate
associations the set B (represented implicitly),
which contains token pairs (x
i
, c
i
?
) such that x
i
and c
i
?
share the same lemma, the same POS tag,
or are linked through a derivation link on WordNet
(Fellbaum, 1998). This allows us to learn para-
phrases for words that appear in our datasets but
are not covered by the phrase table, and to han-
dle nominalizations for phrase pairs such as ?Who
designed the game of life?? and ?What game de-
signer is the designer of the game of life??.
Our model goes over all possible spans of x
and c and constructs all possible associations from
A and B. This results in many poor associations
(e.g., ?play? and ?the?), but as illustrated in Fig-
ure 3, we learn weights that discriminate good
from bad associations. Table 3 specifies the full
set of features. Note that unlike standard para-
phrase detection and RTE systems, we use lexi-
calized features, firing approximately 400,000 fea-
tures on WEBQUESTIONS. By extracting POS
features, we obtain soft syntactic rules, e.g., the
feature ?JJ N ? N? indicates that omitting ad-
jectives before nouns is possible. Once associa-
tions are constructed, we mark tokens in x and c
that were not part of any association, and extract
deletion features for their lemmas and POS tags.
Thus, we learn that deleting pronouns is accept-
able, while deleting nouns is not.
To summarize, the association model links
phrases of two utterances in multiple overlapping
ways. During training, the model learns which
associations are characteristic of paraphrases and
which are not.
5.2 Vector space model
The association model relies on having a good set
of candidate associations, but mining associations
suffers from coverage issues. We now introduce
a vector space (VS) model, which assigns a vec-
tor representation for each utterance, and learns a
scoring function that ranks paraphrase candidates.
We start by constructing vector representations
of words. We run the WORD2VEC tool (Mikolov et
al., 2013) on lower-cased Wikipedia text (1.59 bil-
lion tokens), using the CBOW model with a win-
dow of 5 and hierarchical softmax. We also ex-
periment with publicly released word embeddings
(Huang et al, 2012), which were trained using
both local and global context. Both result in k-
dimensional vectors (k = 50). Next, we construct
a vector v
x
? R
k
for each utterance x by simply
averaging the vectors of all content words (nouns,
verbs, and adjectives) in x.
We can now estimate a paraphrase score for two
utterances x and c via a weighted combination of
the components of the vector representations:
v
>
x
Wv
c
=
k
?
i,j=1
w
ij
v
x,i
v
c,j
where W ? R
k?k
is a parameter matrix. In terms
of our earlier notation, we have ?
vs
= vec(W ) and
?
vs
(x, c) = vec(v
x
v
>
c
), where vec(?) unrolls a ma-
trix into a vector. In Section 6, we experiment with
W equal to the identity matrix, constraining W to
be diagonal, and learning a full W matrix.
The VS model can identify correct paraphrases
in cases where it is hard to directly associate
phrases from x and c. For example, the answer
to ?Where is made Kia car?? (from WEBQUES-
TIONS), is given by the canonical utterance ?What
city is Kia motors a headquarters of??. The as-
sociation model does not associate ?made? and
?headquarters?, but the VS model is able to de-
termine that these utterances are semantically re-
lated. In other cases, the VS model cannot distin-
guish correct paraphrases from incorrect ones. For
1420
Dataset # examples # word types
FREE917 917 2,036
WEBQUESTIONS 5,810 4,525
Table 4: Statistics on WEBQUESTIONS and FREE917.
example, the association model identifies that the
paraphrase for ?What type of music did Richard
Wagner Play?? is ?What is the musical genres
of Richard Wagner??, by relating phrases such as
?type of music? and ?musical genres?. The VS
model ranks the canonical utterance ?What com-
position has Richard Wagner as lyricist?? higher,
as this utterance is also in the music domain. Thus,
we combine the two models to benefit from their
complementary nature.
In summary, while the association model aligns
particular phrases to one another, the vector space
model provides a soft vector-based representation
for utterances.
6 Empirical evaluation
In this section, we evaluate our system on WE-
BQUESTIONS and FREE917. After describing the
setup (Section 6.1), we present our main empirical
results and analyze the components of the system
(Section 6.2).
6.1 Setup
We use the WEBQUESTIONS dataset (Berant et
al., 2013), which contains 5,810 question-answer
pairs. This dataset was created by crawling
questions through the Google Suggest API, and
then obtaining answers using Amazon Mechani-
cal Turk. We use the original train-test split, and
divide the training set into 3 random 80%?20%
splits for development. This dataset is character-
ized by questions that are commonly asked on the
web (and are not necessarily grammatical), such
as ?What character did Natalie Portman play in
Star Wars?? and ?What kind of money to take to
Bahamas??.
The FREE917 dataset contains 917 questions,
authored by two annotators and annotated with
logical forms. This dataset contains questions on
rarer topics (for example, ?What is the engine
in a 2010 Ferrari California?? and ?What was
the cover price of the X-men Issue 1??), but the
phrasing of questions tends to be more rigid com-
pared to WEBQUESTIONS. Table 4 provides some
statistics on the two datasets. Following Cai and
Yates (2013), we hold out 30% of the data for the
final test, and perform 3 random 80%-20% splits
of the training set for development. Since we train
from question-answer pairs, we collect answers by
executing the gold logical forms against Freebase.
We execute ?-DCS queries by converting them
into SPARQL and executing them against a copy
of Freebase using the Virtuoso database engine.
We evaluate our system with accuracy, that is, the
proportion of questions we answer correctly. We
run all questions through the Stanford CoreNLP
pipeline (Toutanova and Manning, 2003; Finkel et
al., 2005; Klein and Manning, 2003).
We tuned the L
1
regularization strength, devel-
oped features, and ran analysis experiments on the
development set (averaging across random splits).
On WEBQUESTIONS, without L
1
regularization,
the number of non-zero features was 360K; L
1
regularization brings it down to 17K.
6.2 Results
We compare our system to Cai and Yates (2013)
(CY13), Berant et al (2013) (BCFL13), and
Kwiatkowski et al (2013) (KCAZ13). For
BCFL13, we obtained results using the SEMPRE
package
2
and running Berant et al (2013)?s sys-
tem on the datasets.
Table 5 presents results on the test set. We
achieve a substantial relative improvement of 12%
in accuracy on WEBQUESTIONS, and match the
best results on FREE917. Interestingly, our system
gets an oracle accuracy of 63% on WEBQUES-
TIONS compared to 48% obtained by BCFL13,
where the oracle accuracy is the fraction of ques-
tions for which at least one logical form in the
candidate set produced by the system is correct.
This demonstrates that our method for construct-
ing candidate logical forms is reasonable. To fur-
ther examine this, we ran BCFL13 on the devel-
opment set, allowing it to use only predicates from
logical forms suggested by our logical form con-
struction step. This improved oracle accuracy on
the development set to 64.5%, but accuracy was
32.2%. This shows that the improvement in accu-
racy should not be attributed only to better logical
form generation, but also to the paraphrase model.
We now perform more extensive analysis of our
system?s components and compare it to various
baselines.
Component ablation We ablate the association
model, the VS model, and the entire paraphrase
2
http://www-nlp.stanford.edu/software/sempre/
1421
FREE917 WEBQUESTIONS
CY13 59.0 ?
BCFL13 62.0 35.7
KCAZ13 68.0 ?
This work 68.5 39.9
Table 5: Results on the test set.
FREE917 WEBQUESTIONS
Our system 73.9 41.2
?VSM 71.0 40.5
?ASSOCIATION 52.7 35.3
?PARAPHRASE 31.8 21.3
SIMPLEGEN 73.4 40.4
Full matrix 52.7 35.3
Diagonal 50.4 30.6
Identity 50.7 30.4
JACCARD 69.7 31.3
EDIT 40.8 24.8
WDDC06 71.0 29.8
Table 6: Results for ablations and baselines on develop-
ment set.
model (using only logical form features). Table 5
shows that our full system obtains highest accu-
racy, and that removing the association model re-
sults in a much larger degradation compared to re-
moving the VS model.
Utterance generation Our system generates
relatively natural utterances from logical forms us-
ing simple rules based on Freebase descriptions
(Section 4). We now consider simply concate-
nating Freebase descriptions. For example, the
logical form R[PlaceOfBirth].ElvisPresley
would generate the utterance ?What location Elvis
Presley place of birth??. Row SIMPLEGEN in Ta-
ble 6 demonstrates that we still get good results in
this setup. This is expected given that our para-
phrase models are not sensitive to the syntactic
structure of the generated utterance.
VS model Our system learns parameters for a
full W matrix. We now examine results when
learning parameters for a full matrix W , a diago-
nal matrix W , and when setting W to be the iden-
tity matrix. Table 6 (third section) illustrates that
learning a full matrix substantially improves accu-
racy. Figure 4 gives an example for a correct para-
phrase pair, where the full matrix model boosts
the overall model score. Note that the full ma-
trix assigns a high score for the phrases ?official
language? and ?speak? compared to the simpler
models, but other pairs are less interpretable.
Baselines We also compared our system to the
following implemented baselines:
Full do people czech republic speakoffical 0.7 8.09 15.34 21.62 24.44
language 3.86 -3.13 7.81 2.58 14.74
czech 0.67 16.55 2.76
republic -8.71 12.47 -10.75
Diagonal do people czech republic speakoffical 2.31 -0.72 1.88 0.27 -0.49
language 0.27 4.72 11.51 12.33 11
czech 1.4 8.13 5.21
republic -0.16 6.72 9.69
Identity do people czech republic speakoffical 2.26 -1.41 0.89 0.07 -0.58
language 0.62 4.19 11.91 10.78 12.7
czech 2.88 7.31 5.42
republic -1.82 4.34 9.44
Figure 4: Values of the paraphrase score v
>
x
i
Wv
c
i
?
for all
content word tokens x
i
and c
i
?
, where W is an arbitrary full
matrix, a diagonal matrix, or the identity matrix. We omit
scores for the words ?czech? and ?republic? since they ap-
pear in all canonical utterances for this example.
? JACCARD: We compute the Jaccard score
between the tokens of x and c and define
?
pr
(x, c) to be this single feature.
? EDIT: We compute the token edit distance
between x and c and define ?
pr
(x, c) to be
this single feature.
? WDDC06: We re-implement 13 features
from Wan et al (2006), who obtained close to
state-of-the-art performance on the Microsoft
Research paraphrase corpus.
3
Table 6 demonstrates that we improve perfor-
mance over all baselines. Interestingly, JACCARD
and WDDC06 obtain reasonable performance
on FREE917 but perform much worse on WE-
BQUESTIONS. We surmise this is because ques-
tions in FREE917 were generated by annotators
prompted by Freebase facts, whereas questions
in WEBQUESTIONS originated independently of
Freebase. Thus, word choice in FREE917 is of-
ten close to the generated Freebase descriptions,
allowing simple baselines to perform well.
Error analysis We sampled examples from the
development set to examine the main reasons
PARASEMPRE makes errors. We notice that in
many cases the paraphrase model can be further
improved. For example, PARASEMPRE suggests
3
We implement all features that do not require depen-
dency parsing.
1422
that the best paraphrase for ?What company did
Henry Ford work for?? is ?What written work
novel by Henry Ford?? rather than ?The em-
ployer of Henry Ford?, due to the exact match
of the word ?work?. Another example is the
question ?Where is the Nascar hall of fame??,
where PARASEMPRE suggests that ?What hall of
fame discipline has Nascar hall of fame as halls
of fame?? is the best canonical utterance. This
is because our simple model allows to associate
?hall of fame? with the canonical utterance three
times. Entity recognition also accounts for many
errors, e.g., the entity chosen in ?where was the
gallipoli campaign waged?? is Galipoli and not
GalipoliCampaign. Last, PARASEMPRE does not
handle temporal information, which causes errors
in questions like ?Where did Harriet Tubman live
after the civil war??
7 Discussion
In this work, we approach the problem of seman-
tic parsing from a paraphrasing viewpoint. A
fundamental motivation and long standing goal
of the paraphrasing and RTE communities has
been to cast various semantic applications as para-
phrasing/textual entailment (Dagan et al, 2013).
While it has been shown that paraphrasing meth-
ods are useful for question answering (Harabagiu
and Hickl, 2006) and relation extraction (Romano
et al, 2006), this is, to the best of our knowledge,
the first paper to perform semantic parsing through
paraphrasing. Our paraphrase model emphasizes
simplicity and efficiency, but the framework is ag-
nostic to the internals of the paraphrase method.
On the semantic parsing side, our work is most
related to Kwiatkowski et al (2013). The main
challenge in semantic parsing is coping with the
mismatch between language and the KB. In both
Kwiatkowski et al (2013) and this work, an inter-
mediate representation is employed to handle the
mismatch, but while they use a logical represen-
tation, we opt for a text-based one. Our choice
allows us to benefit from the parallel monolingual
corpus PARALEX and from word vectors trained
on Wikipedia. We believe that our approach is
particularly suitable for scenarios such as factoid
question answering, where the space of logical
forms is somewhat constrained and a few gener-
ation rules suffice to reduce the problem to para-
phrasing.
Our work is also related to Fader et al (2013),
who presented a paraphrase-driven question an-
swering system. One can view this work as a
generalization of Fader et al along three dimen-
sions. First, Fader et al use a KB over natu-
ral language extractions rather than a formal KB
and so querying the KB does not require a gener-
ation step ? they paraphrase questions to KB en-
tries directly. Second, they suggest a particular
paraphrasing method that maps a test question to a
question for which the answer is already known in
a single step. We propose a general paraphrasing
framework and instantiate it with two paraphrase
models. Lastly, Fader et al handle queries with
only one property and entity whereas we general-
ize to more types of logical forms.
Since our generated questions are passed to
a paraphrase model, we took a very simple ap-
proach, mostly ensuring that we preserved the se-
mantics of the utterance without striving for the
most fluent realization. Research on generation
(Dale et al, 2003; Reiter et al, 2005; Turner et
al., 2009; Piwek and Boyer, 2012) typically fo-
cuses on generating natural utterances for human
consumption, where fluency is important.
In conclusion, the main contribution of this pa-
per is a novel approach for semantic parsing based
on a simple generation procedure and a paraphrase
model. We achieve state-of-the-art results on two
recently released datasets. We believe that our ap-
proach opens a window of opportunity for learn-
ing semantic parsers from raw text not necessarily
related to the target KB. With more sophisticated
generation and paraphrase, we hope to tackle com-
positionally richer utterances.
Acknowledgments
We thank Kai Sheng Tai for performing the er-
ror analysis. Stanford University gratefully ac-
knowledges the support of the Defense Advanced
Research Projects Agency (DARPA) Deep Ex-
ploration and Filtering of Text (DEFT) Program
under Air Force Research Laboratory (AFRL)
contract no. FA8750-13-2-0040. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the view of the DARPA,
AFRL, or the US government. The second author
is supported by a Google Faculty Research Award.
1423
References
J. Berant, A. Chou, R. Frostig, and P. Liang. 2013.
Semantic parsing on Freebase from question-answer
pairs. In Empirical Methods in Natural Language
Processing (EMNLP).
Q. Cai and A. Yates. 2013. Large-scale semantic pars-
ing via schema matching and lexicon extension. In
Association for Computational Linguistics (ACL).
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Discriminative learning over constrained la-
tent representations. In North American Association
for Computational Linguistics (NAACL).
I. Dagan, D. Roth, M. Sammons, and F. M. Zanzotto.
2013. Recognizing Textual Entailment: Models and
Applications. Morgan and Claypool Publishers.
R. Dale, S. Geldof, and J. Prost. 2003. Coral: using
natural language generation for navigational assis-
tance. In Australasian computer science conference,
pages 35?44.
D. Das and N. A. Smith. 2009. Paraphrase identifica-
tion as probabilistic quasi-synchronous recognition.
In Association for Computational Linguistics (ACL),
pages 468?476.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Ex-
ploiting massively parallel news sources. In Inter-
national Conference on Computational Linguistics
(COLING).
J. Duchi, E. Hazan, and Y. Singer. 2010. Adaptive sub-
gradient methods for online learning and stochastic
optimization. In Conference on Learning Theory
(COLT).
A. Fader, S. Soderland, and O. Etzioni. 2011. Identi-
fying relations for open information extraction. In
Empirical Methods in Natural Language Processing
(EMNLP).
A. Fader, L. Zettlemoyer, and O. Etzioni. 2013.
Paraphrase-driven learning for open question an-
swering. In Association for Computational Linguis-
tics (ACL).
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by Gibbs sampling. In Associ-
ation for Computational Linguistics (ACL), pages
363?370.
Google. 2013. Freebase data dumps (2013-06-
09). https://developers.google.com/
freebase/data.
A. Haghighi, A. Y. Ng, and C. D. Manning. 2005.
Robust textual inference via graph matching. In
Empirical Methods in Natural Language Processing
(EMNLP).
S. Harabagiu and A. Hickl. 2006. Methods for using
textual entailment in open-domain question answer-
ing. In Association for Computational Linguistics
(ACL).
M. Heilman and N. A. Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In Human Lan-
guage Technology and North American Association
for Computational Linguistics (HLT/NAACL), pages
1011?1019.
E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng.
2012. Improving word representations via global
context and multiple word prototypes. In Associa-
tion for Computational Linguistics (ACL).
D. Klein and C. Manning. 2003. Accurate unlexical-
ized parsing. In Association for Computational Lin-
guistics (ACL), pages 423?430.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2010. Inducing probabilistic CCG
grammars from logical form with higher-order uni-
fication. In Empirical Methods in Natural Language
Processing (EMNLP), pages 1223?1233.
T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer.
2013. Scaling semantic parsers with on-the-fly on-
tology matching. In Empirical Methods in Natural
Language Processing (EMNLP).
P. Liang. 2013. Lambda dependency-based composi-
tional semantics. Technical report, ArXiv.
T. Lin, Mausam, and O. Etzioni. 2012. Entity linking
at web scale. In Knowledge Extraction Workshop
(AKBC-WEKEX).
T. Mikolov, K. Chen, G. Corrado, and Jeffrey. 2013.
Efficient estimation of word representations in vec-
tor space. Technical report, ArXiv.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30:417?449.
P. Piwek and K. E. Boyer. 2012. Varieties of question
generation: Introduction to this special issue. Dia-
logue and Discourse, 3:1?9.
E. Reiter, S. Sripada, J. Hunter, J. Yu, and I. Davy.
2005. Choosing words in computer-generated
weather forecasts. Artificial Intelligence, 167:137?
169.
L. Romano, M. kouylekov, I. Szpektor, I. Dagan,
and A. Lavelli. 2006. Investigating a generic
paraphrase-based approach for relation extraction.
In Proceedings of ECAL.
R. Socher, E. H. Huang, J. Pennin, C. D. Manning, and
A. Ng. 2011. Dynamic pooling and unfolding re-
cursive autoencoders for paraphrase detection. In
Advances in Neural Information Processing Systems
(NIPS), pages 801?809.
1424
A. Stern and I. Dagan. 2011. A confidence model
for syntactically-motivated entailment proofs. In
Recent Advances in Natural Language Processing,
pages 455?462.
K. Toutanova and C. D. Manning. 2003. Feature-
rich part-of-speech tagging with a cyclic depen-
dency network. In Human Language Technology
and North American Association for Computational
Linguistics (HLT/NAACL).
R. Turner, Y. Sripada, and E. Reiter. 2009. Generating
approximate geographic descriptions. In European
Workshop on Natural Language Generation, pages
42?49.
S. Wan, M. Dras, R. Dale, and C. Paris. 2006. Using
dependency-based features to take the ?para-farce?
out of paraphrase. In Australasian Language Tech-
nology Workshop.
M. Wang and C. D. Manning. 2010. Probabilistic tree-
edit models with structured latent variables for tex-
tual entailment and question answering. In The In-
ternational Conference on Computational Linguis-
tics, pages 1164?1172.
Y. W. Wong and R. J. Mooney. 2007. Learning
synchronous grammars for semantic parsing with
lambda calculus. In Association for Computational
Linguistics (ACL), pages 960?967.
M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming.
In Association for the Advancement of Artificial In-
telligence (AAAI), pages 1050?1055.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Un-
certainty in Artificial Intelligence (UAI), pages 658?
666.
1425
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 82?86,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
Freebase QA: Information Extraction or Semantic Parsing?
Xuchen Yao
1
Jonathan Berant
3
Benjamin Van Durme
1,2
1
Center for Language and Speech Processing
2
Human Language Technology Center of Excellence
Johns Hopkins University
3
Computer Science Department
Stanford University
Abstract
We contrast two seemingly distinct ap-
proaches to the task of question answering
(QA) using Freebase: one based on infor-
mation extraction techniques, the other on
semantic parsing. Results over the same
test-set were collected from two state-of-
the-art, open-source systems, then ana-
lyzed in consultation with those systems?
creators. We conclude that the differ-
ences between these technologies, both
in task performance, and in how they
get there, is not significant. This sug-
gests that the semantic parsing commu-
nity should target answering more com-
positional open-domain questions that are
beyond the reach of more direct informa-
tion extraction methods.
1 Introduction
Question Answering (QA) from structured data,
such as DBPedia (Auer et al., 2007), Freebase
(Bollacker et al., 2008) and Yago2 (Hoffart et
al., 2011), has drawn significant interest from
both knowledge base (KB) and semantic pars-
ing (SP) researchers. The majority of such work
treats the KB as a database, to which standard
database queries (SPARQL, MySQL, etc.) are is-
sued to retrieve answers. Language understand-
ing is modeled as the task of converting natu-
ral language questions into queries through inter-
mediate logical forms, with the popular two ap-
proaches including: CCG parsing (Zettlemoyer
and Collins, 2005; Zettlemoyer and Collins, 2007;
Zettlemoyer and Collins, 2009; Kwiatkowski et
al., 2010; Kwiatkowski et al., 2011; Krishna-
murthy and Mitchell, 2012; Kwiatkowski et al.,
2013; Cai and Yates, 2013a), and dependency-
based compositional semantics (Liang et al., 2011;
Berant et al., 2013; Berant and Liang, 2014).
We characterize semantic parsing as the task
of deriving a representation of meaning from lan-
guage, sufficient for a given task. Traditional
information extraction (IE) from text may be
coarsely characterized as representing a certain
level of semantic parsing, where the goal is to
derive enough meaning in order to populate a
database with factoids of a form matching a given
schema.
1
Given the ease with which reasonably
accurate, deep syntactic structure can be automat-
ically derived over (English) text, it is not surpris-
ing that IE researchers would start including such
?features? in their models.
Our question is then: what is the difference be-
tween an IE system with access to syntax, as com-
pared to a semantic parser, when both are targeting
a factoid-extraction style task? While our conclu-
sions should hold generally for similar KBs, we
will focus on Freebase, such as explored by Kr-
ishnamurthy and Mitchell (2012), and then others
such as Cai and Yates (2013a) and Berant et al.
(2013). We compare two open-source, state-of-
the-art systems on the task of Freebase QA: the
semantic parsing system SEMPRE (Berant et al.,
2013), and the IE system jacana-freebase (Yao
and Van Durme, 2014).
We find that these two systems are on par with
each other, with no significant differences in terms
of accuracy between them. A major distinction be-
tween the work of Berant et al. (2013) and Yao
and Van Durme (2014) is the ability of the for-
mer to represent, and compose, aggregation oper-
ators (such as argmax, or count), as well as in-
tegrate disparate pieces of information. This rep-
resentational capability was important in previous,
closed-domain tasks such as GeoQuery. The move
to Freebase by the SP community was meant to
1
So-called Open Information Extraction (OIE) is simply
a further blurring of the distinction between IE and SP, where
the schema is allowed to grow with the number of verbs, and
other predicative elements of the language.
82
provide richer, open-domain challenges. While
the vocabulary increased, our analysis suggests
that compositionality and complexity decreased.
We therefore conclude that the semantic parsing
community should target more challenging open-
domain datasets, ones that ?standard IE? methods
are less capable of attacking.
2 IE and SP Systems
jacana-freebase
2
(Yao and Van Durme, 2014)
treats QA from a KB as a binary classification
problem. Freebase is a gigantic graph with mil-
lions of nodes (topics) and billions of edges (re-
lations). For each question, jacana-freebase
first selects a ?view? of Freebase concerning only
involved topics and their close neighbors (this
?view? is called a topic graph). For instance,
for the question ?who is the brother of justin
bieber??, the topic graph of Justin Bieber, con-
taining all related nodes to the topic (think of the
?Justin Bieber? page displayed by the browser), is
selected and retrieved by the Freebase Topic API.
Usually such a topic graph contains hundreds to
thousands of nodes in close relation to the central
topic. Then each of the node is judged as answer
or not by a logistic regression learner.
Features for the logistic regression learner are
first extracted from both the question and the
topic graph. An analysis of the dependency
parse of the question characterizes the question
word, topic, verb, and named entities of the
main subject as the question features, such as
qword=who. Features on each node include the
types of relations and properties the node pos-
sesses, such as type=person. Finally features
from both the question and each node are com-
bined as the final features used by the learner, such
as qword=who|type=person. In this way the as-
sociation between the question and answer type
is enforced. Thus during decoding, for instance,
if there is a who question, the nodes with a per-
son property would be ranked higher as the an-
swer candidate.
SEMPRE
3
is an open-source system for training
semantic parsers, that has been utilized to train a
semantic parser against Freebase by Berant et al.
(2013). SEMPRE maps NL utterances to logical
forms by performing bottom-up parsing. First, a
2
https://code.google.com/p/jacana/
3
http://www-nlp.stanford.edu/software/
sempre/
lexicon is used to map NL phrases to KB predi-
cates, and then predicates are combined to form a
full logical form by a context-free grammar. Since
logical forms can be derived in multiple ways from
the grammar, a log-linear model is used to rank
possible derivations. The parameters of the model
are trained from question-answer pairs.
3 Analysis
3.1 Evaluation Metrics
Both Berant et al. (2013) and Yao and
Van Durme (2014) tested their systems on
the WEBQUESTIONS dataset, which contains
3778 training questions and 2032 test questions
collected from the Google Suggest API. Each
question came with a standard answer from
Freebase annotated by Amazon Mechanical Turk.
Berant et al. (2013) reported a score of 31.4%
in terms of accuracy (with partial credit if inexact
match) on the test set and later in Berant and Liang
(2014) revised it to 35.7%. Berant et al. focused
on accuracy ? how many questions were correctly
answered by the system. Since their system an-
swered almost all questions, accuracy is roughly
identical to F
1
. Yao and Van Durme (2014)?s sys-
tem on the other hand only answered 80% of all
test questions. Thus they report a score of 42%
in terms of F
1
on this dataset. For the purpose of
comparing among all test questions, we lowered
the logistic regression prediction threshold (usu-
ally 0.5) on jacana-freebase for the other 20%
of questions where jacana-freebase had not pro-
posed an answer to, and selected the best-possible
prediction with the highest prediction score as the
answer. In this way jacana-freebase was able
to answer all questions with a lower accuracy of
35.4%. In the following we present analysis re-
sults based on the test questions where the two
systems had very similar performance (35.7% vs.
35.4%).
4
The difference is not significant accord-
ing to the paired permutation test (Smucker et al.,
2007).
3.2 Accuracy vs. Coverage
First, we were interested to see the proportions of
questions SEMPRE and jacana-freebase jointly
and separately answered correctly. The answer to
4
In this setting accuracy equals averaged macro F
1
: first
the F
1
value on each question were computed, then averaged
among all questions, or put it in other words: ?accuracy with
partial credit?. In this section our usage of the terms ?accu-
racy? and ?F
1
? can be exchanged.
83
jacana (F
1
= 1) jacana (F
1
? 0.5)
S
E
M
P
R
E
?
?
?
?
?
153 (0.08) 383 (0.19) 429 (0.21) 321 (0.16)
? 136 (0.07) 1360 (0.67) 366 (0.18) 916 (0.45)
Table 1: The absolute and proportion of ques-
tions SEMPRE and jacana-freebase answered
correctly (
?
) and incorrectly (?) jointly and sep-
arately, running a threshold F
1
of 1 and 0.5.
many questions in the dataset is a set of answers,
for example what to see near sedona arizona?.
Since turkers did not exhaustively pick out all pos-
sible answers, evaluation is performed by comput-
ing the F
1
between the set of answers given by
the system and the answers provided by turkers.
With a strict threshold of F
1
= 1 and a permis-
sive threshold of F
1
? 0.5 to judge the correct-
ness, we list the pair-wise correctness matrix in
Table 1. Not surprisingly, both systems had most
questions wrong given that the averaged F
1
?s were
only around 35%. With the threshold F
1
= 1,
SEMPRE answered more questions exactly cor-
rectly compared to jacana-freebase, while when
F
1
? 0.5, it was the other way around. This
shows that SEMPRE is more accurate in certain
questions. The reason behind this is that SEMPRE
always fires queries that return exactly one set of
answers from Freebase, while jacana-freebase
could potentially tag multiple nodes as the answer,
which may lower the accuracy.
We have shown that both systems can be more
accurate in certain questions, but when? Is there
a correlation between the system confidence and
accuracy? Thus we took the logistic decoding
score (between 0 and 1) from jacana-freebase
and the probability from the log-linear model used
by SEMPRE as confidence, and plotted an ?accu-
racy vs. coverage? curve, which shows the accu-
racy of a QA engine with respect to its coverage
of all questions. The curve basically answers one
question: at a fixed accuracy, what is the propor-
tion of questions that can be answered? A better
system should be able to answer more questions
correctly with the same accuracy.
The curve was drawn in the following way. For
each question, we select the best answer candidate
with the highest confidence score. Then for the
whole test set, we have a list of (question, highest
ranked answer, confidence score) tuples. Running
0 10 20 30 40 50 60 70 80 90 100Percent Answered
20
30
40
50
60
70
Acc
ura
cy
Accuracy vs. Coverage
jacana-freebase
SEMPRE
Figure 1: Precision with respect to proportion of
questions answered
a threshold from 1 to 0, we select those questions
with an answer confidence score above the thresh-
old and compute accuracy at this point. The X-
axis indicates the percentage of questions above
the threshold and the Y-axis the accuracy, shown
in Figure 1.
The two curves generally follow a similar trend,
but while jacana-freebase has higher accuracy
when coverage is low, SEMPRE obtains slightly
better accuracy when more questions are an-
swered.
3.3 Accuracy by Question Length and Type
Do accuracies of the two systems differ with re-
spect to the complexity of questions? Since there
is no clear way to measure question complexity,
we use question length as a surrogate and report
accuracies by question length in Figure 2. Most of
the questions were 5 to 8 words long and there was
no substantial difference in terms of accuracies.
The major difference lies in questions of length 3,
12 and 13. However, the number of such ques-
tions was not high enough to show any statistical
significance.
Figure 3 further shows the accuracies with re-
spect to the question types (as reflected by the
WH-word). Again, there is no significant differ-
ence between the two systems.
3.4 Learned Features
What did the systems learn during training? We
compare them by presenting the top features by
weight, as listed in Table 2. Clearly, the type of
knowledge learned by the systems in these fea-
tures is similar: both systems learn to associate
certain phrases with predicates from the KB.
84
0	 ?0.05	 ?
0.1	 ?0.15	 ?
0.2	 ?0.25	 ?
0.3	 ?0.35	 ?
0.4	 ?0.45	 ?
0.5	 ?
3	 ?(9)	 ? 4	 ?(78)
	 ?
5	 ?(299
)	 ?
6	 ?(432
)	 ?
7	 ?(395
)	 ?
8	 ?(273
)	 ?
9	 ?(122
)	 ?
10	 ?(48
)	 ?
11	 ?(19
)	 ?
12	 ?(10
)	 ? 13	 ?(4)
	 ?
15	 ?(1)
	 ?
<=	 ?5(	 ?
386)	 ?
<=	 ?10
	 ?(1270
)	 ?
<=15	 ?
(34)	 ?
Jacana-??freebase	 ?
SEMPRE	 ?
Figure 2: Accuracy (Y-axis) by question length.
The X-axis specifies the question length in words
and the total number of questions in parenthesis.
0	 ?
0.05	 ?
0.1	 ?
0.15	 ?
0.2	 ?
0.25	 ?
0.3	 ?
0.35	 ?
0.4	 ?
0.45	 ?
what	 ?
(929)	 ?
where	 ?
(357)	 ?
who	 ?
(261)	 ?
which	 ?
(35)	 ?
when	 ?
(100)	 ?
how	 ?	 ?
(8)	 ?
Jacana-??freebase	 ?
SEMPRE	 ?
Figure 3: Accuracy by question type (and the
number of questions).
We note, however, that SEMPRE also obtains in-
formation from the fully constructed logical form.
For instance, SEMPRE learns that logical forms
that return an empty set when executed against the
KB are usually incorrect (the weight for this fea-
ture is -8.88). In this respect the SP approach ?un-
derstands? more than the IE approach.
We did not further compare on other datasets
such as GeoQuery (Tang and Mooney, 2001) and
FREE917 (Cai and Yates, 2013b). The first one
involves geographic inference and multiple con-
traints in queries, directly fitting the compositional
nature of semantic parsing. The second one was
manually generated by looking at Freebase top-
ics. Both datasets were less realistic than the
WEBQUESTIONS dataset. Both datasets were also
less challenging (accuracy/F
1
were between 80%
and 90%) compared to WEBQUESTIONS (around
40%).
4 Discussion and Conclusion
Our analysis of two QA approaches, semantic
parsing and information extraction, has shown no
significant difference between them. Note the
feature weight
qfocus=religion|type=Religion 8.60
qfocus=money|type=Currency 5.56
qverb=die|type=CauseOfDeath 5.35
qword=when|type=datetime 5.11
qverb=border|rel=location.adjoins 4.56
(a) jacana-freebase
feature weight
die from=CauseOfDeath 10.23
die of=CauseOfDeath 7.55
accept=Currency 7.30
bear=PlaceOfBirth 7.11
in switzerland=Switzerland 6.86
(b) SEMPRE
Table 2: Learned top features and their weights for
jacana-freebase and SEMPRE.
similarity between features used in both systems
shown in Table 2: the systems learned the same
?knowledge? from data, with the distinction that
the IE approach acquired this through a direct as-
sociation between dependency parses and answer
properties, while the SP approach acquired this
through optimizing on intermediate logic forms.
With a direct information extraction technol-
ogy easily getting on par with the more sophis-
ticated semantic parsing method, it suggests that
SP-based approaches for QA with Freebase has
not yet shown its power from a ?deeper? under-
standing of the questions, among questions of var-
ious lengths. We suggest that more compositional
open-domain datasets should be created, and that
SP researchers should focus on utterances in exist-
ing datasets that are beyond the reach of direct IE
methods.
5 Acknowledgement
We thank the Allen Institute for Artificial Intelli-
gence for assistance in funding this work. This
material is partially based on research sponsored
by the NSF under grant IIS-1249516 and DARPA
under agreements number FA8750-13-2-0017 and
FA8750-13-2-0040 (the DEFT program).
85
References
S?oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. DBPedia: A nucleus for a web of open data.
In The semantic web, pages 722?735. Springer.
Jonathan Berant and Percy Liang. 2014. Semantic
parsing via paraphrasing. In Proceedings of ACL.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic Parsing on Freebase from
Question-Answer Pairs. In Proceedings of EMNLP.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247?1250. ACM.
Qingqing Cai and Alexander Yates. 2013a. Large-
scale semantic parsing via schema matching and lex-
icon extension. In Proceedings of ACL.
Qingqing Cai and Alexander Yates. 2013b. Large-
scale semantic parsing via schema matching and lex-
icon extension. In Proceedings of ACL.
Johannes Hoffart, Fabian M Suchanek, Klaus
Berberich, Edwin Lewis-Kelham, Gerard De Melo,
and Gerhard Weikum. 2011. Yago2: exploring and
querying world knowledge in time, space, context,
and many languages. In Proceedings of the 20th
international conference companion on World Wide
Web, pages 229?232. ACM.
Jayant Krishnamurthy and Tom Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of EMNLP.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proceedings of EMNLP, pages
1223?1233.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in CCG grammar induction for semantic pars-
ing. In Proceedings of EMNLP.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling Semantic Parsers with
On-the-fly Ontology Matching. In Proceedings of
EMNLP.
Percy Liang, Michael I. Jordan, and Dan Klein.
2011. Learning Dependency-Based Compositional
Semantics. In Proceedings of ACL.
M.D. Smucker, J. Allan, and B. Carterette. 2007. A
comparison of statistical significance tests for in-
formation retrieval evaluation. In Proceedings of
the sixteenth ACM conference on Conference on in-
formation and knowledge management, pages 623?
632. ACM.
Lappoon R Tang and Raymond J Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In Machine
Learning: ECML 2001. Springer.
Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In Proceedings of ACL.
Luke S Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. Uncertainty in Artificial Intelligence
(UAI).
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of EMNLP-CoNLL.
Luke S Zettlemoyer and Michael Collins. 2009.
Learning context-dependent mappings from sen-
tences to logical form. In Proceedings of ACL-
CoNLL.
86
