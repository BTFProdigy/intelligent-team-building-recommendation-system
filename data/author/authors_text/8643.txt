Learning to Predict Problematic Situations in a Spoken Dialogue 
System: Experiments with How May I Help You? 
Mar i lyn  Walker ,  I rene  Langk i lde ,  Je r ry  Wr ight ,  A l ien  Gor in ,  D iane  L i tman 
AT&T Labs- -Research 
180 Park Avenue 
F lorham Park, NJ 07932-0971 USA 
walker, jwright, algor, diane@research, art. corn, ilangkil@isi, edu 
Abst rac t  
Current spoken dialogue systems are deficient in 
their strategies for preventing, identifying and re- 
pairing problems that arise in the conversation. This 
paper reports results on learning to automatically 
identify and predict problematic human-computer 
dialogues in a corpus of 4774 dialogues collected with 
the How May I Help You spoken dialogue system. 
Our expectation is that the ability to predict prob- 
lematic dialogues will allow the system's dialogue 
manager to modify its behavior to repair problems, 
and even perhaps, to prevent them. We train a 
problematic dialogue classifier using automatically- 
obtainable features that can identify problematic 
dialogues ignificantly better (23%) than the base- 
line. A classifier trained with only automatic fea- 
tures from the first exchange in the dialogue can 
predict problematic dialogues 7% more accurately 
than the baseline, and one trained with automatic 
features from the first two exchanges can perform 
14% better than the baseline. 
1 In t roduct ion  
Spoken dialogue systems promise fficient and nat- 
ural access to a large variety of information sources 
and services from any phone. Systems that sup- 
port short utterances to select a particular function 
(through a statement such as "Say credit card, col- 
lect or person-to-person") are saving companies mil- 
lions of dollars. Research prototypes exist for appli- 
cations uch as personal email and calendars, travel 
and restaurant information, and personal banking 
(Baggia et al, 1998; Walker et al, 1998; Seneff et 
al., 1995; Sanderman et al, 1998; Chu-Carroll and 
Carpenter, 1999) inter alia. Yet there are still many 
research challenges: current systems are limited in 
the interaction they support and brittle in many re- 
spects. We show how spoken dialogue systems can 
learn to support more natural interaction on the ba- 
sis of their previous experience. 
One way that current spoken dialogue systems are 
quite limited is in their strategies for detecting and 
repairing problems that arise in conversation. If 
a problem can be detected, the system can either 
transfer the call to a human operator or modify its 
dialogue strategy in an attempt to repair the prob- 
lem. We can train systems to improve their ability to 
detect problems by exploiting dialogues collected in 
interactions with human users. The initial segments 
of these dialogues can be used to predict hat a prob- 
lem is likely to occur. We expect hat the ability to 
predict that a dialogue is likely to be problematic 
will allow the system's dialogue manager to apply 
more sophisticated strategies to repairing problems, 
and even perhaps, to prevent hem. 
This paper reports experiments on predicting 
problems in spoken dialogue interaction by train- 
ing a problematic dialogue predictor on a corpus of 
4774 dialogues collected in an experimental trial of 
AT~;T's How May I Help You (HMIHY) spoken dia- 
logue system (Gorin et al, 1997; Riccardi and Gorin, 
to appear; E. Ammicht and Alonso, 1999). In this 
trial, the HMIHY system was installed at an AT&T 
customer care center. HMIHY answered calls from 
live customer traffic and successfully automated a 
large number of customer requests. An example of 
a dialogue that HMIHY completed successfully is
shown in Figure 13 
St: AT&T How may I help you? 
UI: I need to \[ uh \] put a call on my calling card please 
$2: May I have your card number, please? 
U2:765432 10987654 
$3: What number would you like to call? 
U3:8 1 4 7 7 7 6 6 6 6 (misrecognized) 
$4: May I have that number again? 
U4:8147776666 
$5: Thank you. 
Figure 1: Sample TASKSUCCESS Dialogue 
We shall refer to the dialogues with a TASKSUC- 
CESS outcome, in which HMIHY successfully auto- 
mates the customer's call, as the TASKSUCCESS dia- 
logues. In addition to the TASKSUCCESS dialogues, 
there are several other call outcomes, to be described 
in detail below, which we consider problematic. 
tThe phone numbers, card numbers, and pin numbers in 
the sample dialogues are artificial. 
210 
This paper reports results from a set of experi- 
ments that test whether we can learn to automat- 
ically predict that a dialogue will be problematic 
on the basis of information the system has: (1) 
early on in the dialogue; and (2) in real time. We 
train an automatic lassifer for predicting problem- 
atic dialogues from features that can be automat- 
ically extracted from the HMIHY corpus. The re- 
sults show that we can learn to predict problematic 
dialogues using fully automatic features with an ac- 
curacy ranging from 72% to 87%, depending on how 
much of the dialogue the system has seen so far. 
Section 2 describes HMIHY and the dialogue corpus 
which the experiments are based on. Section 3 de- 
tails the encoding of the dialogues and the methods 
used for utilizing the machine learning program RIP- 
PER to train an automatic problematic dialogue pre- 
dictor (Cohen, 1996). Section 4 presents the results 
and section 5 summarizes the paper and describes 
future work. 
2 Exper imenta l  Sys tem and Data  
HMIHY is a spoken dialogue system based on the no- 
tion of call routing (Gorin et al, 1997; Chu-Carroll 
and Carpenter, 1999). In the HMIHY call rout- 
ing system, services that the user can access are 
classified into 14 categories, plus a category called 
other for calls that cannot be automated and must 
be transferred to a human operator (Gorin et al, 
1997). ~ Each category describes a different task, 
such as person-to-person dialing, or receiving credit 
for a misdialed number. The system determines 
which task the caller is requesting on the basis of 
its understanding of the cMler's response to the 
open-ended system greeting A T~ T, How May I Help 
You?. Once the task has been determined, the infor- 
mation needed for completing the caller's request is 
obtained using dialogue submodules that are specific 
for each task (Abella and Gorin, 1999). 
In addition to the TASKSUCCESS dialogues in 
which HMIHY successfully automates the cus- 
tomer's call, illustrated in Figure 1, and the calls 
that are transferred to a human operator, there are 
three other possible outcomes for a call, all of which 
are problematic. The first category, which we call 
HANGUP, results from a customer's decision to hang 
up on the system. A sample HANGUP dialogue is 
in Figure 2. A caller may hang up because s/he is 
frustrated with the system; our goal is to learn from 
the corpus which system behaviors led to the caller's 
frustration. 
The second problematic ategory (WIZARD), re- 
suits from a human customer care agent's decision to 
take over the call from the system. Because HMIHY 
is experimental, each call was monitored by a human 
2 One potential use of our results is to provide better ules 
for making the transfer decision. 
SI: AT&T How may I help you? 
UI: I need credit please. 
$2: Okay. What is the reason for the credit? 
U2: Miss, uh, Miss, different area code than I needed. 
$3: Sorry. I need to know whether you need credit for 
a wrong number, bad connection or a call that was cut  
off. 
U3: It is a wrong number. (impatient humming) 
$4: What was the number that you dialed? 
USER HANGS UP 
Figure 2: Sample HANGUP Dialogue 
agent serving as a wizard who could override the 
system. There were a number of agents who partici- 
pated as wizards during the trial of HMIHY and each 
wizard was simply told to take over the call if s/he 
perceived problems with the system's performance. 
The wizard's decision was logged by the experimen- 
tal setup, resulting in labelling the call as one that 
the wizard took over. s A dialogue where the wizard 
decided that the dialogue was problematic and took 
over the call is shown in Figure 3. 
Sh AT&T How may I help you? 
Uh (silence) 
$2: Sorry. Please briefly tell me how I may  help you? 
U2: I'm trying to call 1 8 hundred call A T T. 
$3: Do you want to place a call? 
U3: Through my calling card. 
$4: May I have your card number, please? 
U4: It's 8 7 6 5 4 3 2 1 0 and then my pin number is 8 
7 6 5. (misrecognized) 
$5: Sorry. Please enter or say your card number again.  
WIZARD STEPS IN 
Figure 3: Sample WIZARD Dialogue 
The third problematic category, the TASKFAILURE 
dialogues, are cases where the system completed the 
call, but carried out a task that was not the task 
that the customer was actually requesting. An ex- 
ample TASKFAILURE dialogue is given in Figure 4: 
HMIHY interpreted utterance U2 as a request to 
make a third-party call e.g. to bill it to my home 
phone. HMIHY then asked the caller for the infor- 
mation it needed to carry out this task, the caller 
complied, and the system completed the call. 
The corpus of 4774 dialogues used in our exper- 
iments was collected in severM experimental trials 
of HMIHY on live customer traffic (Pdccardi and 
Gorin, to appear; E. Ammicht and Alonso, 1999), 
and is referred to as HM2 in (Riccardi and Gorin, to 
appear)). During the trial, all of the system behav- 
iors were automatically recorded in a log file, and 
later the dialogues were transcribed by humans and 
labelled with a semantic ategory representing the 
sOf  course we can only infer what might have mot ivated 
the wizard to take over the call, but we assume that the wizard 
had good reason for doing so. 
211 
SI: AT&T How may I help you? 
UI: (silence) 
$2: Sorry. Please briefly tell me how I may help you? 
U2: I wanta find out how much my bill's worth. 
$3: What phone number would you like the call billed 
to? 
U3: Urn the access number. 
$4: Sorry. Could I have that phone number again? 
U4:18hundred2255288.  
$5: What number would you like to call? 
U5:800 225 5288 (touchtone input) 
$6: Thank you. 
Figure 4: Sample TASKFAILURE Dialogue 
task that the caller was asking HMIHY to perform, 
on a per utterance basis. The logfiles also included 
labels indicating whether the wizard had taken over 
the call or the user had hung up. 
3 T ra in ing  an  Automat ic  
P rob lemat ic  D ia logue  Pred ic tor  
Our experiments apply the machine learning pro- 
gram RIPPER (Cohen, 1996) to automatically induce 
a "problematic dialogue" classification model. RIP- 
PER takes as input the names of a set of classes to 
be learned, the names and ranges of values of a fixed 
set of features, and training data specifying the class 
and feature values for each example in a training set. 
Its output is a classification model for predicting the 
class of future examples. In RIPPER, the classifica- 
tion model is learned using greedy search guided by 
an information gain metric, and is expressed as an 
ordered set of if-then rules. 
To apply RIPPER, the dialogues in the corpus must 
be encoded in terms of a set of classes (the output 
classification) and a set of input features that are 
used as predictors for the classes. We start with the 
dialogue categories described above, but since our 
goal is to develop algorithms that predict/identify 
problematic dialogues, we treat HANGUP, WIZARD 
and TASKFAILURE as equivalently problematic. Thus 
we train the classifier to distinguish between two 
classes: TASKSUCCESS and PROBLEMATIC. Note that 
our categorization is inherently noisy because we do 
not know the real reasons why a caller hangs up or a 
wizard takes over the call. The caller may hang up 
because she is frustrated with the system, or she may 
simply dislike automation, or her child may have 
started crying. Similarly, one wizard may have low 
confidence in the system's ability to recover from er- 
rors and use a conservative approach that results in 
taking over many calls, while another wizard may be 
more willing to let the system try to recover. Nev- 
ertheless we take these human actions as a human 
labelling of these calls as problematic. Given this 
classification, approximately 36% of the calls in the 
corpus of 4774 dialogues are PROBLEMATIC and 64% 
are TASKSUCCESS. 
Next, we encoded each dialogue in terms of a set 
of 196 features that were either automatically ogged 
by one of the system modules, hand-labelled by hu- 
mans, or derived from raw features. We use the 
hand-labelled features to produce a TOPLINE, an es- 
timation of how well a classifier could do that had 
access to perfect information. The entire feature set 
is summarized in Figure 5. 
? Acoust ic /ASR Features 
- recog, recog-numwords, ASR-duration, dtmf- 
flag, rg-modality, rg-grammar 
? NLU Features 
- a confidence measure for all of the possible 
tasks that the user could be trying to do 
- salience-coverage, inconsistency, context-shift, 
top-task, nexttop-task, top-confidence, dill- 
confidence 
? D ia logue  Manager  Features  
- sys-label, utt-id, prompt, reprompt, confirma- 
tion, subdial 
- running tallies: num-reprompts, num- 
confirms, num-subdials, reprompt%, confir- 
mation%, subdialogue% 
? Hand-Labe l led  Features  
- tscript, human-label, age, gender, user- 
modality, clean-tscript, cltscript-numwords, 
rsuccess 
? Whole-Dialogue Features 
num-utts, num-reprompts, percent-reprompts, 
num-confirms, percent-confirms, num- 
subdials, percent-subdials, dial-duration. 
Figure 5: Features for spoken dialogues. 
There are 8 features that describe the whole dia- 
logue, and 47 features for each of the first four ex- 
changes. We encode features for the first four ex- 
changes because we want to predict failures before 
they happen. Since 97% of the dialogues in our cor- 
pus are five exchanges or less, in most cases, any 
potential problematic outcome will have occurred 
by the time the system has participated in five ex- 
changes. Because the system needs to be able to 
predict whether the dialogue will be problematic us- 
ing information it has available in the initial part of 
the dialogue, we train classifiers that only have ac- 
cess to input features from exchange 1, or only the 
features from exchange 1 and exchange 2. To see 
whether our results generalize, we also experiment 
with a subset of features that are task-independent. 
We compare results for predicting problematic din- 
212 
logues, with results for identifying problematic di- 
alogues, when the classifier has access to features 
representing the whole dialogue. 
We utilized features logged by the system because 
they are produced automatically, and thus could be 
used during runtime to alter the course of the dia- 
logue. The system modules that we collected infor- 
mation from were the acoustic processer/automatic 
speech recognizer (ASR) (Riccardi and Gorin, to ap- 
pear), the natural anguage understanding (NLU) 
module (Gorin et al, 1997), and the dialogue man- 
ager (DM) (Abella and Gorin, 1999). Below we de- 
scribe each module and the features obtained from 
it. 
ASR takes as input the acoustic signal and 
outputs a potentially errorful transcription of what 
it believes the caller said. The ASR features for 
each of the first four exchanges were the output 
of the speech recognizer (recog), the number of 
words in the recognizer output (recog-numwords), 
the duration in seconds of the input to the 
recognizer (asr-duration), a flag for touchtone 
input (dtmf-flag), the input modality expected 
by the recognizer (rg-modality) (one of: none, 
speech, touchtone, speech+touchtone, touchtone- 
card, speech+touchtone-card, touchtone-date, 
speech+touchtone-date, or none-final-prompt), and 
the grammar used by the recognizer (rg-grammar). 
The motivation for the ASR features is that any 
one of them may have impacted performance. For 
example, it is well known that longer utterances 
are less likely to be recognized correctly, thus asr- 
duration could be a clue to incorrect recognition re- 
suits. In addition, the larger the grammar is, the 
more likely an ASR error is, so the name of the 
grammar vg-grammar could be a predictor of incor- 
rect recognition. 
The natural language understanding (NLU) mod- 
ule takes as input a transcription ofthe user's utter- 
ance from ASR and produces 15 confidence scores 
representing the likelihood that the caller's task is 
one of the 15 task types. It also extracts other 
relevant information, such as phone or credit card 
numbers. Thus 15 of the NLU features for each ex- 
change represent the 15 confidence scores. There 
are also features that the NLU module calculates 
based on processing the utterance. These include 
an intra-utterance measure of the inconsistency be- 
tween tasks that the user appears to be requesting 
(inconsistency), a measure of the coverage of the 
utterance by salient grammar fragments (salience- 
coverage), a measure of the shift in context between 
utterances (context-shift), he task with the highest 
confidence score (top-task), the task with the second 
highest confidence score (nexttop-task), the value of 
the highest confidence score (top-confidence), and 
the difference in values between the top and next- 
to-top confidence scores (diff-confidence). 
The motivation for these NLU features i  to make 
use of information that the NLU module has based 
on processing the output of ASR and the current dis- 
course context. For example, for utterances that fol- 
low the first utterance, the NLU module knows what 
task it believes the caller is trying to complete. If it 
appears that the caller has changed her mind, then 
the NLU module may have misunderstood a previ- 
ous utterance. The context-shift feature indicates 
the NLU module's belief that it may have made an 
error (or be making one now). 
The dialogue manager (DM) takes the output of 
NLU and the dialogue history and decides what it 
should say to the caller next. It decides whether it 
believes there is a single unambiguous task that the 
user is trying to accomplish, and how to resolve any 
ambiguity. The DM features for each of the first four 
exchanges are the task-type label which includes a 
label that indicates task ambiguity (sys-label), utter- 
ance id within the dialogue (implicit in the encod- 
ing), the name of the prompt played before the user 
utterance (prompt), and whether that prompt was a 
reprompt (reprompt), a confirmation (confirm), or a 
subdialogue prompt (subdia O, a superset of the re- 
prompts and confirmation prompts. 
The DM features are primarily motivated by pre- 
vious work. The task-type label feature is to cap- 
ture the fact that some tasks may be harder than 
others. The utterance id feature is motivated by the 
idea that the length of the dialogue may be impor- 
tant, possibly in combination with other features like 
task-type. The different prompt features for initial 
prompts, reprompts, confirmation prompts and sub- 
dialogue prompts are motivated by results indicating 
that reprompts and confirmation prompts are frus- 
trating for callers and that callers are likely to hy- 
perarticulate when they have to repeat hemselves, 
which results in ASR errors (Shriberg et al, 1992; 
Levow, 1998). 
The DM features also include running tallies for 
the number of reprompts (num-reprompts), number 
of confirmation prompts (num.confirms), and num- 
ber of subdialogue prompts (num-subdials), that had 
been played up to each point in the diMogue, as well 
as running percentages (percent-reprompts, ercent- 
confirms, percent-subdials). The use of running tal- 
lies and percentages is based on the assumption that 
these features are likely to produce generalized pre- 
dictors (Litman et al, 1999). 
The features obtained via hand-labelling were hu- 
man transcripts of each user utterance (tscript), a 
set of semantic labels that are closely related to the 
system task-type labels (human-label), age (age) and 
gender (gender) of the user, the actual modality of 
the user utterance (user-modality) (one of: nothing, 
speech, touchtone, speech+touchtone, on-speech), 
213 
and a cleaned transcript with non-word noise infor- 
mation removed (clean-tscript). From these features 
we calculated two derived features. The first was the 
number of words in the cleaned transcript (cltscript 
numwords), again on the assumption that utterance 
length is strongly correlated with ASR and NLU er- 
rors. The second derived feature was based on cal- 
culating whether the human-label matches the sys- 
label from the dialogue manager (rsuccess). There 
were four values for rsuccess: rcorrect, rmismatch, 
rpartial-match and rvacuous-match, indicating re- 
spectively correct understanding, incorrect under- 
standing, partial understanding, and the fact that 
there had been no input for ASR and NLU to oper- 
ate on, either because the user didn't say anything 
or because she used touch-tone. 
The whole-dialogue f atures derived from the per- 
utterance features were: num-utts, num-reprompts, 
percent-reprampts, hum.confirms, percent-confirms, 
num-subdials, and per-cent-subdials for the whole di- 
alogue, and the duration of the entire dialogue in 
seconds (dial-duration). 
In the experiments, the features in Figure 5 except 
the Hand-Labelled features are referred to as the AU- 
TOMATIC feature set. We examine how well we can 
identify or predict problematic dialogues using these 
features, compared to the full feature set including 
the Hand-Labelled features. As mentioned earlier, 
we wish to generalize our problematic dialogue pre- 
dictor to other systems. Thus we also discuss how 
well we can predict problematic dialogues using only 
features that are both automatically acquirable dur- 
ing runtime and independent of the HMIHY task. 
The subset of features from Figure 5 that fit this 
qualification are in Figure 6. We refer to them as 
the AUTO, TASK-INDEP feature set. 
The output of each RIPPER. experiment is a clas- 
sification model learned from the training data. To 
evaluate these results, the error rates of the learned 
classification models are estimated using the resam- 
pling method of cross-validation. In 5-fold cross- 
validation, the total set of examples is randomly di- 
vided into 5 disjoint test sets, and 5 runs of the learn- 
ing program are performed. Thus, each run uses the 
examples not in the test set for training and the re- 
maining examples for testing. An estimated error 
rate is obtained by averaging the error rate on the 
testing portion of the data from each of the 5 runs. 
Since we intend to integrate the rules learned 
by RIPPER into the HMIHY system, we examine 
the precision and recall performance of specific hy- 
potheses. Because hypotheses from different cross- 
validation experiments cannot readily be combined 
together, we apply the hypothesis learned on one 
randomly selected training set (80% of the data) to 
that set's respective test data. Thus the precision 
and recall results reported below are somewhat less 
? Acoust ic /ASR Features 
- recog, recog-numwords, ASR-duration, dtmf- 
flag, rg-modality 
? NLU Features 
- salience-coverage, inconsistency, context-shift, 
top-confidence, dig-confidence 
? D ia logue  Manager  Features  
- utt-id, reprompt, confirmation, subdial 
- running tallies: num-reprompts, num- 
confirms, num-subdials, reprompt%, confir- 
mation%, subdialogue% 
Figure 6: Automatic task independent (AUTO, 
TASK-INDEP) features available at runtime. 
reliable than the error rates from cross-validation. 
4 Results 
We present results for both predicting and identi- 
fying problematic dialogues. Because we are inter- 
ested in predicting that a dialogue will be problem- 
atic at a point in the dialogue where the system can 
do something about it, we compare prediction ac- 
curacy after having only seen the first exchange of 
the diMogue with prediction accuracy after having 
seen the first two exchanges, with identification ac- 
curacy after having seen the whole dialogue. For 
each of these situations we also compare results for 
the AUTOMATIC and AUTO, TASK-INDEP feature sets 
(as described earlier), with results for the whole fea- 
ture set including hand-labelled features. Table 1 
summarizes the results. 
The baseline on the first line of Table 1 repre- 
sents the prediction accuracy from always guess- 
ing the majority class. Since 64% of the dialogues 
are TASKSUCCESS dialogues, we can achieve 64% ac- 
curacy from simply guessing TASKSUCCESS without 
having seen any of the dialogue yet. 
The first EXCHANGE 1 row shows the results of 
using the AUTOMATIC features from only the first 
exchange to predict whether the dialogue outcome 
will be TASKSUCCESS or PROBLEMATIC. The results 
show that the machine-learned classifier can predict 
problematic dialogues 8% better than the baseline 
after having seen only the first user utterance. Using 
only task-independent automatic features (Figure 6) 
the EXCHANGE 1 classifier can still do nearly as well. 
The ALL row for EXCHANGE 1 indicates that even 
if we had access to human perceptual ability (the 
hand-labelled features) we would still only be able 
to distinguish between TASKSUCCESS and PROBLEM- 
ATIC dialogues with 77% accuracy after having seen 
the first exchange. 
214 
Features Used 
BASELINE (majority class) 
EXCHANGE 1 AUTOMATIC 
AUTO, TASK-INDEP 
ALL 
EXCHANGES l&2 AUTOMATIC 
AUTO, TASK-INDEP 
ALL 
FULL DIALOGUE AUTOMATIC 
AUTO, TASK-INDEP 
TOPLINE ALL 
\]Accuracy (SE) 
64.0 % 
72.3 % 1.04 % 
71.6 % 1.05 % 
77.0 % 0.56 % 
79.9 % 0.58 % 
78.6 % 0.37 % 
86.7 % 0.33 % 
87.0 % 0.72 % 
86.7 % 0.82 % 
92.3 % 0.72 % 
Table 1: Results for predicting and identifying problematic dialogues (SE --- Standard Error) 
The EXCHANGE l&2 rows of Table 1 show the re- 
suits using features from the first two exchanges in 
the dialogue to predict he outcome of the dialogue. 4 
The additional exchange gives roughly an additional 
7% boost in predictive accuracy using either of the 
AUTOMATIC feature sets. This is only 8% less than 
the accuracy we can achieve using these features af- 
ter having seen the whole dialogue (see below). The 
ALL row for EXCHANGE l&2 shows that we could 
achieve over 86% accuracy if we had the ability to 
utilize the hand-labelled features. 
The FULL DIALOGUE row in Table 1 for AUTO- 
MATIC and AUTO, TASK-INDEP features hows the 
ability of the classifier to identify problematic dia- 
logues, rather than predict them, using features for 
the whole dialogue. The ALL row for the FULL DI- 
ALOGUE shows that we could correctly identify over 
92% of the outcomes accurately if we had the ability 
to utilize the hand-labelled features. 
Note that the task-independent automatic fea- 
tures always perform within 2% error of the auto- 
matic features, and the hand-labelled features con- 
sistently perform with accuracies ranging from 6-8% 
greater. 
The rules that RIPPER learned on the basis of the 
Exchange 1 automatic features are below. 
Exchange 1, Automat ic  Features:  
i f  (el-top-confidence _< .924) A (el-dtmf-f lag = '1') 
then  problematic, 
if (el-cliff-confidence _<.916) A (el-asr-duration > 6.92) 
then problematic, 
default is tasksuccess. 
According to these rules, a dialogue will be prob- 
lematic if the confidence score for the top-ranked 
4Since 23% of the dialogues consisted of only two ex- 
changes, we exclude the second exchange features for those 
dialogues where the second exchange consists only of the sys- 
tem playing a closing prompt. We also excluded any features 
that indicated to the classifier that the second exchange was 
the last exchange in the dialogue. 
task (given by the NLU module) is moderate or low 
and there was touchtone input in the user utterance. 
The second rule says that if the difference between 
the top confidence score and the second-ranked con- 
fidence score is moderate or low, and the duration 
of the user utterance is more than 7 seconds, predict 
PROBLEMATIC. 
The performance of these rules is summarized in
Table 2. These results show that given the first ex- 
change, this ruleset predicts that 22% of the dia- 
logues will be problematic, while 36% of them ac- 
tually will be. Of the dialogues that actually will 
be problematic, it can predict 41% of them. Once 
it predicts that a dialogue will be problematic, it is 
correct 69% of the time. As mentioned earlier, this 
reflects an overMl improvement in accuracy of 8% 
over the baseline. 
The rules learned by training on the automatic 
task-independent features for exchanges 1 and 2 are 
given below. As in the first rule set, the features that 
the classifier appears to be exploiting are primarily 
those from the ASR and NLU modules. 
Exchanges l&2, Automatic Task- 
Independent Features: 
i f  (e2-recog-numwords < 0) A (el-cliff-confidence < .95) 
then  problematic. 
if (el-salience-coverage < .889) A (e2-recog contains 
"I') A (e2-asr-duration > 7.48) then problematic. 
if (el-top-confidence < .924) A (e2-asr-duration >_ 5.36) 
A (el-asr-duration > 8.6) then problematic. 
if (e2-recog is blank) A (e2-asr-duration > 2.8) then 
problematic. 
if (el-salience-coverage < .737) A (el-recog contains 
"help") A (el-asr-duration < 7.04) then problematic. 
if (el-cliff-confidence < .924) A (el-dtmf-flag = '1') A 
(el-asr-duration < 6.68) then problematic. 
default is tasksuccess. 
The performance of this ruleset is summarized in
Table 3. These results show that, given the first 
two exchanges, this ruleset predicts that 26% of the 
215 
Class 
Success 
Problematic 
Occur red  Pred ic ted  Recal l  P rec i s ion  
64.1% 78.3 % 89.44 % 73.14 % 
35.9 % 21.7 % 41.47 % 68.78 % 
Table 2: Precision and Recall with Exchange 1 Automatic Features 
Class 
Success 
Problematic 
Occur red  Pred ic ted  Recal l  Prec is ion  
64.1% 75.3 % 91.42 % 77.81% 
35.9 % 24.7 %' 53.53 % 77.78 % 
Table 3: Precision and Recall with Exchange l&2 Automatic, Task-Independent Features 
dialogues will be problematic, while 36% of them 
actually will be. Of the problematic dialogues, it 
can predict 57% of them. Once it predicts that a 
dialogue will be problematic, it is correct 79% of 
the time. Compared with the classifier for the first 
utterance alone, this classifier has an improvement 
of 16% in recall and 10% in precision, for an overall 
improvement in accuracy of 7% over using the first 
exchange alone. 
One observation from these hypotheses i  the clas- 
sifier's preference for the asr-duration feature over 
the feature for the number of words recognized 
(recog-numwords). One would expect longer utter- 
ances to be more difficult, but the learned rulesets 
indicate that duration is a better measure of utter- 
ance length than the number of words. Another ob- 
servation is the usefulness of the NLU confidence 
scores and the NLU salience-coverage in predicting 
problematic dialogues. These features eem to pro- 
vide good general indicators of the system's uccess 
in recognition and understanding. The fact that the 
main focus of the rules is detecting ASR and NLU 
errors and that none of the DM behaviors are used 
as predictors also indicates that, in all likelihood, the 
DM is performing as well as it can, given the noisy 
input that it is getting from ASR and NLU. 
To identify potential improvements in the prob- 
lematic dialogue predictor, we analyzed which hand- 
labelled features made large performance improve- 
ments, under the assumption that future work can 
focus on developing automatic features that ap- 
proximate the information provided by these hand- 
labelled features. The analysis indicated that the 
vsuceess feature alone improves the performance of 
the TOPLINE from 88.5%, as reported in (Langkilde 
et al, 1999), to 92.3%. Using rsuccess as the only 
feature results in 73.75% accuracy for exchange 1, 
81.9% accuracy for exchanges 18z2 and 85.3% accu- 
racy for the full dialogue. In addition, for Exchanges 
l&2, the accuracy of the AUTOMATIC, TASK-INDEP 
feature set plus the rsuccess feature is 86.5%, which 
is only 0.2% less than the accuracy of ALL the lea- 
tures for Exchanges l&2 as shown in Table 1. The 
rules that RIPPER learns for Exchanges 1&52 when 
the AUTOMATIC, TASK-INDEP feature set is aug- 
mented with the single hand-labelled rsuccess fea- 
ture is shown below. 
Exchanges  1~2,  Rsuccess -b Automat ic  
Task - Independent  Features: 
ife2-salience-coverage ~ 0.651 A e2-asr-duration >_0.04 
A e2-rsuccess=Rvacuous-match then problematic, 
if e2-rsuccess=Rmismatch A el-top-confidence < 0.909 
then problematic, 
if e2-rsuccess=Rmismatch A e2-context-shift < 0.014 A 
e2-salience-coverage ~ 0.2 A e2-recog-numwords < 12 ( 
then problematic, 
if e2-rsuccess=Rmismatch ^ el-rsuccess=Rmismatch 
then problematic, 
if e2-rsuccess=Rmismatch A e2-top-confidence < 0.803 
^ e2-asr-duration >__2.68 ^  e2-asr-duration < 6.32 then 
problematic, 
if el-rsuccess=Rmismatch A el-diff-confidence > 0.83 
then problematic, 
if e2-rsuccess=Rmismatch A e2-context-shift >_ 0.54 
then problematic, 
ife2-asr-duration > 5.24 A e2-salience-coverage < 0.833 
A e2-top-confidence < 0.801 A e2-recog-numwords < 7 
A e2-asr-duration < 16.08 then problematic, 
if el-diff-confidence < 0.794 A el-asr-duration > 7.2 
A el-inconsistency > 0.024 A el-inconsistency > 0.755 
then problematic, 
default is tasksuccess 
Note that the rsuccess feature is frequently used in 
the rules and that RIPPER learns rules that combine 
the rsuccess feature with other features, such as the 
confidence, asr-duration, and salience-coverage fea- 
tures. 
5 D iscuss ion  and Future  Work  
In summary, our results show that: (1) All feature 
sets significantly improve over the baseline; (2) Us- 
ing automatic features from the whole dialogue, we 
can identify problematic dialogues 23% better than 
the baseline; (3) Just the first exchange provides ig- 
216 
nificantly better prediction (8%) than the baseline; 
(4) The second exchange provides an additional sig- 
nificant (7%) improvement, (5) A classifier based on 
task-independent automatic features performs with 
less than 1% degradation in error rate relative to 
the automatic features. Even with current accuracy 
rates, the improved ability to predict problematic 
dialogues means that it may be possible to field the 
system without human agent oversight, and we ex- 
pect to be able to improve these results. 
The research reported here is the first that we 
know of to automatically analyze a corpus of logs 
from a spoken dialogue system for the purpose of 
learning to predict problematic situations. Our work 
builds on earlier research on learning to identify di- 
alogues in which the user experienced poor speech 
recognizer performance (Litman et al, 1999). How- 
ever, that work was based on a much smaller set of 
experimental dialogues where the notion of a good or 
bad dialogue was automatically approximated rather 
than being labelled by humans. In addition, because 
that work was based on features ynthesized over the 
entire dialogues, the hypotheses that were learned 
could not be used for prediction during runtime. 
We are exploring several ways to improve the per- 
formance of and test the problematic dialogue pre- 
dictor. First, we noted above the extent to which 
the hand-labelled feature rsuccess improves classifier 
performance. In other work we report results from 
training an rsuccess classifier on a per-utterance level 
(Walker et al, 2000), where we show that we can 
achieve 85% accuracy using only fully automatic fea- 
tures. In future work we intend to use the (noisy) 
output from this classifier as input to our problem- 
atic dialogue classifier with the hope of improving 
the performance of the fully automatic feature sets. 
In addition, since it is more important o minimize 
errors in predicting PROBLEMATIC dialogues than er- 
rors in predicting TASKSUCCESS dialogues, we intend 
to experiment with RIPPER'S loss ratio parameter, 
which instructs RIPPER to achieve high accuracy for 
the PROBLEMATIC class, while potentially reducing 
overall accuracy. Finally, we plan to integrate the 
learned rulesets into the HMIHY dialogue system to 
improve the system's overall performance. 
Re ferences  
A. Abella and A.L. Gorin. 1999. Construct algebra: 
An analytical method for dialog management. In
Proc. of the Association for Computational Lin- 
guistics. 
P. Baggia, G. Castagneri, and M. Danieli. 1998. 
Field trials of the Italian Arise Train Timetable 
System. In Interactive Voice Technology for 
Telecommunications Applications, IVTTA, pages 
97-102. 
J. Chu-Carroll and R. Carpenter. 1999. Vector- 
based natural language call routing. Computa- 
tional Linguistics, 25-3:361-387. 
W. Cohen. 1996. Learning trees and rules with set- 
valued features. In l~th Conference of the Amer- 
ican Association of Artificial Intelligence, AAAI. 
A.L. Gorin E. Ammicht and T. Alonso. 1999. 
Knowledge collection for natural anguage spoken 
dialog systems. In Proc. of EUROSPEECH 99. 
A.L. Gorin, G. Riccardi, and J.H. Wright. 1997. 
How may I Help You? Speech Communication, 
23:113-127. 
I. Langkilde, M. Walker, J. Wright, A. Gorin, and 
D. Litman. 1999. Automatic prediction of prob- 
lematic human-computer dialogues in How May 
I Help You? In Proc. IEEE Workshop on Auto- 
matic Speech Recognition and Understanding. 
G. A. Levow. 1998. Characterizing and recogniz- 
ing spoken corrections in human-computer dia- 
logue. In Proc. of the 36th Annual Meeting of the 
Association of Computational Linguistics, COL- 
ING/ACL 98, pages 736-742. 
D. J. Litman, M. A. Walker, and M. J. Kearns. 1999. 
Automatic detection of poor speech recognition at 
the dialogue level. In Proc. of the 37th Annual 
Meeting of the Association of Computational Lin- 
guistics, ACL99, pages 309-316. 
G. Riccardi and A.L. Gorin. to appear. Spoken lan- 
guage adaptation over time and state in a natu- 
ral spoken dialog system. IEEE Transactions on 
Speech and Audio. 
A. Sanderman, J. Sturm, E. den Os, L. Bores, and 
A. Cremers. 1998. Evaluation of the Dutch Train 
Timetable Information System developed in the 
ARISE project. In Interactive Voice Technology 
for Telecommunications Applications, pages 91- 
96. 
S. Seneff, V. Zue, J. Polifroni, C. Pao, L. Hethering- 
ton, D. Goddeau, and J. Glass. 1995. The pre- 
liminary development of a displayless PEGASUS 
system. In ARPA SLT Workshop. 
E. Shriberg, E. Wade, and P. Price. 1992. Human- 
machine problem solving using spoken language 
systems (SLS): Factors affecting performance and 
user satisfaction. In Proc. of the DARPA Speech 
and NL Workshop, pages 49-54. 
M. A. Walker, J. C. Fromer, and S. Narayanan. 
1998. Learning optimal dialogue strategies: A 
ease study of a spoken dialogue agent for email. 
In Proc. of the 36th Annual Meeting of the 
Association of Computational Linguistics, COL- 
ING/ACL 98, pages 1345-1352. 
M. Walker, I. Langkilde, and J. Wright. 2000. Us- 
ing NLP and Discourse features to identify under- 
standing errors in the How May I Help You spoken 
dialogue system. In Submission. 
217 
Automatic Optimization of Dialogue Management 
Diane J. Litman, Michael S. Kearns, Satinder Singh, Mari lyn A. Walker  
AT&T Labs -  Research  
180 Park  Avenue 
F lo rham Park ,  N J  07932 USA 
{d iane ,mkearns ,bave j  a,walker} @research.ar t .corn  
Abstract 
Designing the dialogue strategy of a spoken dialogue 
system involves many nontrivial choices. This pa- 
per I)resents a reinforcement learning approach for 
automatically optimizing a dialogue strategy that 
addresses the technical challenges in applying re- 
inforcement learning to a working dialogue system 
with hulnan users. \?e then show that our approach 
measurably improves performance in an experimen- 
tal system. 
1 I n t roduct ion  
Recent advances in spoken language understanding 
have made it 1)ossible to develop dialogue systems 
tbr many applications. The role of the dialogue man- 
ager in such systems is to interact in a naturM w~y 
to hel 1 ) the user complete the tasks that the system 
is designed to support. Tyl)ically, an expert designs 
a dialogue manager by hand, and has to make m~ny 
nontrivial design choices that can seriously impact 
system performance. This paper applies reintbrce- 
ment learning (RL) to automatically learn design 
choices that optimize system pertbrnmnee for a cho- 
seLL pertbrmance measure (Levin et al, 2000; Walker 
et al, 1998). 
Consider the spoken dialogue system named N J- 
Fun, wlfich we built to help users find fun places 
to go in New Jersey. A sample dialogue with N J- 
Fun is shown in Figure 1, with system utterances 
labeled Si and user utterances labeled Ui. When 
NJFun greets the user, it can provide an open greet- 
ing such as utterance S1, expecting that the user 
will take the initiative. Alternatively, it can pro- 
vide a directive greeting in which the system takes 
the initiative, as in "Welcome to NJFun. Please say 
an activity name or say 'list activities' for a list of 
activities I know about." After tile user's response 
U1, NJFun must decide whether it should explicitly 
confirm its understanding, as in utterances $2 and 
$3. NJFun can also simply continue on with the di- 
alogue, as when it does not explicitly confirm that 
the user wants to find out M)out wineries. In NJFun, 
as shown in more detail below, decisions about ini- 
tiative and confirmation strategies alone result in a 
SI: Welcome to N.llSm. How may I help you? 
UI: I'd like to find mn winetasting in Lambertville 
in the morning. (ASR output: I'd like to find 
out wirterics the in the Lambertvillc in the 
mornin.q. ) 
$2: Did you say you are interested in Lambertville? 
U2: Yes. 
$3: Did you say you want to go in the morning? 
U3: Yes. 
$4: I found a winery near l~ambertville that is 
open in the morning, it is \[...\] Please give 
lne feedback by saying 'good', ~so-so', or 'bad'. 
U4: Good. 
Figure 1: An example dialogue with NJFun. 
search space of 242 potential globnl dialogue strate- 
gies. Furthermore, the performance of a dialogue 
strategy depends on many other factors, such as 
the user population, the robustness of the automatic 
speech recognizer (ASR), and task difficulty (Kamm 
et al, 1998; DanMi and Gerbino, 1995). 
In the main, previous research has treated the 
specification of the dialogue management s rategy 
as an iterative design problem: several versions of a 
system are created, dialogue corpora are collected 
with human users interacting with different ver- 
sions of tile system, a number of evaluation met- 
rics are collected ibr each dialogue, and the differ- 
ent versions are statistically compared (Danieli and 
Gerbino, 1995; Sanderman et al, 1998). Due to the 
costs of experimentation, only a few global strategies 
are typically explored in any one experiment. 
However, recent work has suggested that dialogue 
strategy can be designed using tile formalism of 
Markov decision processes (MDPs) and the algo- 
rithms of RL (Biermann and Long, 1996; Levin et 
al., 2000; Walker et nl., 1998; Singh et al, 1999). 
More specifically, the MDP formalism suggests a 
method for optimizing dialogue strategies from sam- 
ple dialogue data. The main advantage of this ap- 
proach is the 1)otential tbr computing an optilnal di- 
alogue strategy within a much larger search space, 
using a relatively small nmnber of training dialogues. 
This paper presents an application of RL to the 
502 
problem of oi)timizing dialogue strategy selection in 
the NJFnn system, and exl)erimentally demonstrates 
the utility of the ~l)proach. Section 2 exl)lahls how 
we apply RL to dialogue systems, then Se('tion 3 
describes t.he NJFun system in detail. Section 4 dee 
scribes how NJFun optimizes its dialogue strategy 
from experimentally obtained dialogue data. Sec- 
tion 5 reports results from testing the learned strat- 
egy demonstrating that our al)l)roach improves task 
coml)letion rates (our chosen measure for 1)erfor- 
mance optimization). A conll)alliOll paper provides 
only an al)brevi~tted system and dialogue manager 
description, but includes additional results not pre- 
sented here (Singh et al, 2000), such as analysis es- 
tablishing the veracity of the MDP we learn, and 
comparisons of our learned strategy to strategies 
hand-picked by dialogue xperts. 
2 Reinforcement Learning for 
Dialogue 
Due to space limitations, we 1)resent only a 1)rief 
overview of how di~dogue strategy optimization can 
be viewed as an llL 1)roblem; for more details, 
see Singh ctal .  (\]999), Walker el; a.1. (\]998), Levin 
et al (2000). A dialogue strategy is a mapl)ing h'om 
a set ot! states (which summarize the entire dialogue 
so far) to a set of actions (such as the system's utter- 
mines and database queries). There are nmltil)l(~ rea- 
sonable action choices in each state; tyl)ically these 
choices are made by the system designer. Our RL- 
I)ased at)l)roach is to build a system that explores 
these choices in a systematic way through experi- 
ments with rel)resentative human us(!rs. A scalar 
i)erf()rmanee l l leasllre, called a rewal'd, is t h(m (;al- 
eulated for each Cxl)erimental diMogue. (We dis- 
cuss various choices for this reward measure later, 
but in our experiments only terminal dialogue states 
ha,re nonzero  rewi-l,rds, s l id  the reward lneasul'(}s arc  
quantities directly obtMnable from the experimental 
set-up, such as user satisfaction or task coml)letion. ) 
This experimental data is used to construct an MDP 
which models the users' intera(:tion with the system. 
The l)roblem of learning the best dialogue strategy 
from data is thus reduced to computing the optimal 
policy tbr choosing actions in an MDP - that is, the 
system's goal is to take actions so as to maximize 
expected reward. The comput~ttion of the ol)timal 
policy given the MDP can be done etficiently using 
stan&trd RL algorithms. 
How do we build the desired MDP from sample 
dialogues? Following Singh et al (1999), we can 
view a dialogue as a trajectory in the chosen state 
space determined by the system actions and user 
resl) onses: 
S1 -~a l , r l  '5'2 --}a~,rs 83 "-~aa,ra " '"  
Here si -%,,,.~ si+l indicates that at the ith ex- 
change, the system was in state si, executed action 
ai, received reward ri, and then the state changed 
to si+~. Dialogue sequences obtained froln training 
data can be used to empirically estimate the transi- 
tion probabilities P(.s"la', a) (denoting the probabil- 
ity of a mmsition to state s', given that the system 
was in state .s and took ;ration a), and the reward 
function R(.s, (t). The estilnated transition 1)tel)abil- 
ities and rewi~rd flmction constitute an MDP model 
of the nser population's interaction with the system. 
Given this MDP, the exl)ected cumnlative reward 
(or Q-value) Q(s, a) of taking action a from state s 
can be calculated in terms of the Q-wdues of succes- 
sor states via the following recursive quation: 
Q(.% = ,) + r(,,'l,% ,)n,a:,: Q(,,", , ' ) .  
t ;  ! 
These Q-values can be estimated to within a desired 
threshold using the standard RL value iteration al- 
gorithm (Sutton, 1.991.), which iteratively updates 
the estimate of Q(s, a) based on the current Q-vahms 
of neighboring states. Once value iteration is con> 
pleted, the optima\] diah)gue strategy (according to 
our estimated model) is obtained by selecting the 
action with the maximum Q-value at each dia.logue 
state. 
While this apl)roach is theoretically appealing, the 
cost of obtaining sample human dialogues makes it 
crucial to limit the size of the state space, to mini- 
mize data sparsity problems, while retaining enough 
information in the state to learn an accurate model. 
Our approad~ is to work directly in a minimal but 
carefully designed stat;e space (Singh et al, 1999). 
The contribution of this paper is to eml)irically 
vMi(tate a practical methodology for using IlL to 
build a dialogue system that ol)timizes its behav- 
ior from dialogue data. Our methodology involves 
1) representing a dialogue strategy as a mapl)il~g 
fronl each state in the chosen state space S to a 
set of dialogue actions, 2) deploying an initial trah> 
ing system that generates exploratory training data 
with respect o S, 3) eonstrncting an MDP model 
from the obtained training data, 4) using value iter- 
ation to learn the optimal dialogue strategy in the 
learned MDP, and 4) redeploying the system using 
the learned state/~mtion real)ping. The next section 
details the use of this methodology to design the 
NJFun system. 
3 The N JFun  System 
NJFnn is a real-time spoken dialogue system that 
provides users with intbrmation about things to do 
in New Jersey. NJFun is built using a general pur- 
pose 1)latt'ornl tbr spoken dialogue systems (Levin 
et al, 1.999), with support tbr modules tbr attto- 
rustic speech recognition (ASI/.), spoken language 
503 
Action Prompt 
Greets Welcome to NJIqm. Please say an activity name or say 'list activities' for a list of activities I know 
about. 
GreetU \Velcome to NdPun. How may I help you? 
ReAsklS Iknow about amusement parks, aquariums, cruises, historic sites, museums, parks, theaters, 
wineries, and zoos. Please say an activity name from this list. 
ReAsklM Please tell me the activity type.You can also tell me the location and time. 
Ask2S Please say the name of the town or city that you are interested in. 
Ask2U Please give me more information. 
ReAsk2S Please teli me the name of the town or city that you are interested in. 
ReAsk2M Please tell me the location that you are interested in. You call also tell me the time. 
Figure 2: Sample initiative strategy choices. 
understanding, text-to-speech (TTS), database ac- 
cess, and dialogue management. NJFnn uses a 
speech recognizer with stochastic language and un- 
derstanding models trained from example user ut- 
terances, and a TTS system based on concatena- 
tive diphone synthesis. Its database is populated 
from the nj .  on l ine  webpage to contain information 
about activities. NJFun indexes this database using 
three attributes: activity type, location, and time of 
day (which can assume values morning, afternoon, 
or evening). 
hffornmlly, the NJFun dialogue manager sequen- 
tially queries the user regarding the activity, loca- 
tion and time attributes, respectively. NJFun first 
asks the user for the current attribute (and 1)ossibly 
the other attributes, depending on the initiative). 
If the current attribute's value is not obtained, N.J- 
Fun asks for the attrilmte (and possibly the later 
attributes) again. If NJFun still does not obtain 
a value, NJFun moves on to the next attribute(s). 
Whenever NJFun successihlly obtains a value, it 
can confirm the vMue, or move on to the next at- 
tribute(s). When NJFun has finished acquiring at- 
tributes, it queries the database (using a wildcard 
for each unobtained attribute value). The length of 
NJFun dialogues ranges from 1 to 12 user utterances 
before the database query. Although the NJFun di- 
alogues are fairly short (since NJFun only asks for 
an attribute twice), the information access part of 
the dialogue is similar to more complex tasks. 
As discussed above, our methodology for using RL 
to optimize dialogue strategy requires that all poten- 
tim actions tbr each state be specified. Note that at 
some states it is easy for a human to make the cor- 
rect action choice. We made obvious dialogue strat- 
egy choices in advance, and used learIfing only to 
optimize the difficult choices (Walker et al, 1998). 
Ill NJFun, we restricted the action choices to 1) the 
type of initiative to use when asking or reasking for 
an attribute, and 2) whether to confirm an attribute 
value once obtained. The optimal actions may vary 
with dialogue state, and are subject o active debate 
in the literature. 
Tile examples in Figure 2 show that NJFun can 
ask the user about the first 2 attributes I using three 
types of initiative, based on the combination of tile 
wording of the system prompt (open versus direc- 
tive), and the type of grammar NJFun uses during 
ASR (restrictivc versus non-restrictive). If NJFun 
uses an open question with m~ unrestricted gram- 
mar, it is using v.scr initiative (e.g., Greet\[l). If N J- 
Fun instead uses a directive prompt with a restricted 
grammar, the system is using systcm initiative (e.g., 
GreetS). If NJFun uses a directive question with a 
non-restrictive granmlar, it is using mizcd initiative, 
because it allows the user to take the initiative by 
supplying extra intbrnlation (e.g., ReAsklM). 
NJFun can also vary the strategy used to confirm 
each attribute. If NJFun asks the user to explicitly 
verify an attribute, it is using czplicit confirmation 
(e.g., ExpConf2 for the location, exemplified by $2 
in Figure 1). If NJFun does not generate any COlt- 
firnmtion prompt, it is using no confirmation (the 
NoConf action). 
Solely tbr the purposes of controlling its operation 
(as opposed to the le~trning, which we discuss in a 
moment), NJNm internally maintains an opcratio'ns 
vector of 14 variables. 2 variables track whether the 
system has greeted the user, and which attribute 
the system is currently attempting to obtain. For 
each of the 3 attributes, 4 variables track whether 
the system has obtained the attribute's value, the 
systent's confidence in the value (if obtained), the 
number of times the system has asked the user about 
the attribute, and the type of ASR grammar most 
recently used to ask for the attribute. 
The formal state space S maintained by NJFun 
for tile purposes of learning is nmch silnt)ler than 
the operations vector, due to the data sparsity con- 
cerns already discussed. The dialogue state space 
$ contains only 7 variables, as summarized in Fig- 
sire 3. S is computed from the operations vector us- 
ing a hand-designed algorithm. The "greet" variable 
1 "Greet" is equivalent toasking for the first attribute. N J- 
Fun always uses system initiative for the third attribute, be- 
cause at that point the user can only provide the time of (lay. 
504 
greet attr conf val times gram hist \] 
0,1 1,2,3,4 0,1,2,3,4 0,1 0,1,2 0,1 0,1 \] 
Figure 3: State features and vahles. 
tracks whether tile system has greeted tile user or  
not (no=0, yes=l). "Attr ~: specifies which attrihute 
NJFun is ('urrently ~tttelnpting to obtain or ver- 
ify (activity=l, location=2, time=a, done with at- 
tributes=4). "Conf" tel)resents the confidence that 
NaFun has after obtaining a wdue for an attribute. 
The values 0, 1, and 2 represent the lowest, middle 
and highest ASR confidence vahms? The wdues 3 
and 4 are set when ASR hears "yes" or "no" after a 
confirmation question. "Val" tracks whether NaFun 
has obtained a value, for tile attribute (no=0, yes=l). 
"Times" tracks the number of times that N,lFun has 
aske(1 the user ~d)out he attribute. "(4ram" tracks 
the type of grammar most ree(mtly used to obtain 
the attribute (0=non-restrictive, 1=restrictive). Fi- 
nally, "hist" (history) represents whether Nalflm had 
troullle understanding the user ill the earlier p~trt of 
the conversation (bad=0, good=l).  We omit the full 
detinition, but as a,n ex~unl>le , when N.lFun is work- 
ing on the secon(1 attribute (location), the history 
variable is set to 0 if NJFun does not have an ac- 
tivity, has an activity but has no confidence in the 
value, or needed two queries to obtain the activity. 
As mentioned above, the goal is to design a small 
state space that makes enough critical distin('tions to 
suPi)ort learning. The use of 6" redu(:es the mmfl)er 
of states to only 62, and SUl)l)orts the constru('tion of
mt MI)P model that is not sparse with respect o ,g, 
even using limite(1 trMning (btta. :~ Tit(.' state sp~t(;e 
that we utilize here, although minimal, allows us 
to make initiative decisions based on the success of 
earlier ex(:ha.nges, and confirmation de(:isions based 
on ASR. confidence scores and gralnmars. 
'.Phe state/~t('tiol: real)ping r(-`l)resenting NaFun's 
initial dialogue strategy EIC (Explor:ttory for Ini- 
tiative and Confirmation) is in Figure 4. Only the 
exploratory portion of the strategy is shown, namely 
those states for which NaFun has an action choice. 
~klr each such state, we list tile two choices of actions 
available. (The action choices in boldfime are the 
ones eventually identified as el)ritual 1)y the learning 
process, an(1 are discussed in detail later.) The EIC 
strategy chooses random, ly between these two ac- 
21"or each uttermme, the ASH. outfmt includes 11o|, only the 
recognized string, but also aIl asso(:ia.ted acoustic (:onJld(mce 
score, iBased on data obtaintM dm'ing system deveJolmmnt , 
we defined a mapl)ing from raw confidence, values into 3 ap- 
proximately equally potmlated p~rtitions. 
362 refers to those states that can actually occur in a di- 
alogue. \[<)r example, greet=0 is only possible in the initial 
dialogue state "0 1 0 0 0 0 0". Thus, all other states beginning 
with 0 (e.g. "0 I 0 0 I 0 0") will never occur. 
g 
1 0 0 (} 0 0 
1 1 0 (} 1 0 0 
1 1 0 1 0 0 0 
1 1 0 1 0 1 0 
1 1 1 1 0 0 0 
1 1 1 1 0 \] 0 
1 1 2 1 0 0 0 
1 1 2 1 0 1 0 
1 1 4 0 0 0 0 
\] 1 4 (} 1 0 0 
St~tte Action Choices 
a c v t g it 
1 2 0 0 0 0 0 
1 2 0 0 0 0 1 
1 2 (} 0 1 (} 0 
1 2 0 (} 1 0 1 
1 2 0 1 0 (} 0 
1 2 0 \] 0 0 1 
1 2 0 1 0 1 0 
1 2 0 1 0 1 1 
1 2 1 1 0 (1 0 
1 2 1 \] 0 0 1 
1 2 1 1 0 1 0 
\] 2 1 1 0 \] 1 
1 2 2 1 0 0 0 
1 2 2 \] 0 0 1 
1 2 2 \] 0 1 0 
\] 2 2 1 0 1 1 
1 2 4 0 0 (1 0 
1 2 4 0 0 0 1 
1 2 4 0 1 0 0 
1 2 4 0 1 0 1 
1 3 () 1 () () 0 
\] 3 0 1 0 0 1 
1 3 (1 1 0 1 0 
1 3 \[) 1 0 \] \] 
1. 3 1 1 0 0 0 
1 3 1 1 0 0 1 
1 3 1 1. 0 1 0 
1 3 1 1 0 \] 1 
1 3 2 1 0 0 0 
1 3 2 1 0 0 1 
1 3 2 1 0 \] 0 
\] 3 2 1 0 1 1 
GreetS,GreetU 
ReAsklS,ReAsklM 
NoConf, ExpConf l  
NoConf, ExpColffl 
NoCont, Exp Confl  
NoConf, ExpConf l  
NoConf~ExpConfl 
NoConf, ExpConfl 
ReAsklS,ReAsklM 
ReAsklS,RcAsklM 
Ask2S,Ask2U 
Ask2S,Ask2U 
I{eAsk2S,ReAsk2M 
ReAsk2S,ReAsk2M 
NoConf, ExpCont'2 
NoConf, ExpConP2 
NoConf~ExpCont)2 
NoConf,ExpConf2 
NoCoiff, Exp C oaF2 
NoConf, ExpConf2 
NoConf, ExpCont2 
NoConf,ExpCong2 
NoConf~ExpConf2 
NoConf, ExpConf2 
NoConf, ExpConf2 
NoConf, ExpCon\[2 
ReAsk2S,RcAsk2M 
I/.eAsk2S,ReAsk2M 
RcAsk2S,ReAsk2M 
ReAsk2S,ReAsk2M 
NoConf, ExpCon\[3 
NoConf, Exl) Conf3 
NoConf, ExpConf3 
NoConf,Ext)ConF3 
NoConf, ExpCont~ 
NoConf, ExpConf3 
NoConf, ExpConf3 
NoConf,ExpConF3 
NoConf, ExpConi~J 
NoConf, ExpConf3 
NoColff, ExpConf3 
NoConf, ExpConf3 
Figure 4: Exploratory portion of EIC strategy. 
tions in the indicated state, to maximize xploration 
and minimize data sparseness when constructing our 
model. Since there are 42 states with 2 choices each, 
there is n search space of 242 potential global di- 
alogue strategies; the goal of RL is to identify an 
apparently optimal strategy fl'om this large search 
space. Note that due to the randomization of the 
EIC strategy, the prompts are designed to ensure 
the coherence of all possible action sequences. 
Figure 5 illustrates how the dialogue strategy in 
Figure 4 generates the diMogue in Figure 1. Each 
row indicates the state that NJFun is in, the ac- 
505 
State Action %Irn Reward 
gacvtgh  
0100000 GreetU $1 0 
1121000 NoConf 0 
1221001 ExpConf2 $2 0 
1 3 2 1 0 0 1 ExpConf3 $3 0 
1400000 Tell $4 1 
Figure 5: Generating the dialogue in Figure 1. 
tion executed in this state, the corresponding turn 
in Figure 1, and the reward received. The initial 
state represents that NaFun will first attempt o ob- 
tain attribute 1. NJFun executes GreetU (although 
as shown in Figure 4, GreetS is also possible), gen- 
erating the first utterance in Figure 1. Alter the 
user's response, the next state represents that N J- 
Fun has now greeted the user and obtained the ac- 
tivity value with high confidence, by using a non- 
restrictive grmnmar. NJFnn then chooses the No- 
Conf strategy, so it does not attempt to confirm 
the activity, which causes the state to change but 
no prompt to be generated. The third state repre- 
sents that NJFun is now working on the second at- 
tribute (location), that it already has this vahle with 
high confidence (location was obtained with activity 
after the user's first utterance), and that the dia- 
logue history is good. 4 This time NaFun chooses the 
ExpConf2 strategy, and confirms the attribute with 
the second NJFun utterance, and the state changes 
again. The processing of time is similar to that of lo- 
cation, which leads NJFun to the final state, where it 
performs the action "Tell" (corresponding to query- 
ing the database, presenting the results to the user, 
and asking the user to provide a reward). Note that 
in NJFun, the reward is always 0 except at the ter- 
minal state, as shown in the last column of Figure 5. 
4 Experimental ly Optimizing a 
Strategy 
We collected experimental dialogues for both train- 
ing and testing our system. To obtain training di- 
alogues, we implemented NJFun using the EIC dia- 
logue strategy described in Section 3. We used these 
dialogues to build an empirical MDP, and then com- 
puted the optimal dialogue strategy in this MDP (as 
described in Section 2). In this section we describe 
our experimental design and the learned dialogue 
strategy. In the next section we present results from 
testing our learned strategy and show that it im- 
proves task completion rates, the performance mea- 
sure we chose to optimize. 
Experimental subjects were employees not associ- 
a, ted with the NJFun project. There were 54 sub- 
4Recall that only the current attribute's features are ill the 
state, lIowever, the operations vector contains information 
regarding previous attributes. 
jects for training and 21 for testing. Subjects were 
distributed so tile training and testing pools were 
balanced for gender, English as a first language, and 
expertise with spoken dialogue systems. 
During both training and testing, subjects carried 
out free-form conversations with NJFun to complete 
six application tasks. For examl)le , the task exe- 
cuted by the user in Figure 1 was: "You feel thirsty 
and want to do some winetasting in the morning. 
Are there any wineries (;lose by your house in Lam- 
bertville?" Subjects read task descriptions on a web 
page, then called NJFun from their office phone. 
At the end of the task, NJFun asked for feedback 
on their experience (e.g., utterance $4 in Figure 1). 
Users then hung up the phone and filled out a user 
survey (Singh et al, 2000) on the web. 
The training phase of the experiment resulted in 
311 complete dialogues (not all subjects completed 
all tasks), for which NJFun logged the sequence 
of states and the corresponding executed actions. 
The number of samples per st~tte for the initi~fl ask 
choices are: 
0 1 0 0 0 0 0 GreetS=IS5 GreetU=156 
1 2 0 0 0 0 0 Ask2S=93 Ask2U=72 
1 2 0 0 0 0 1 Ask2S=36 Ask2U=48 
Such data illustrates that the random action choice 
strategy led to a fairly balanced action distribution 
per state. Similarly, the small state space, and the 
fact that we only allowed 2 action choices per state, 
prevented a data sparseness problem. The first state 
in Figure 4, the initial state for every dialogue, was 
the most frequently visited state (with 311 visits). 
Only 8 states that occur near the end of a dialogue 
were visited less tlmn 10 times. 
The logged data was then used to construct he 
empirical MDP. As we have mentioned, the measure 
we chose to optinfize is a binary reward flmction 
based on the strongest possible measure of task com- 
pletion, called S t rongComp,  that takes on value 
1 if NJFun queries the database using exactly the 
attributes pecified in the task description, and 0 
otherwise. Then we eoml)uted the optimal dialogue 
strategy in this MDP using RL (cf. Section 2). The 
action choices constituting the learned strategy are 
in boldface in Figure 4. Note that no choice was 
fixed for several states, inealfing that the Q-values 
were identical after value iteration. Thus, even when 
using the learned strategy, NJFun still sometimes 
chooses randomly between certain action pairs. 
Intuitively, the learned strategy says that the op- 
timal use of initiative is to begin with user initia- 
tive, then back off to either mixed or system ini- 
tiative when reasking for an attribute. Note, how- 
ever, that the specific baekoff method differs with 
attribute (e.g., system initiative for attribute 1, but 
gcnerMly mixed initiative for attribute 2). With 
respect to confirmation, the optimal strategy is to 
506 
mainly contirm at lower contidenee -values. Again, 
however, the point where contirlnation becomes un- 
necessary difl'ers across attributes (e.g., confidence 
level 2 for attribute 1, but sometimes lower levels 
for attributes 2 and 3), and  also dt!txmds on other 
features of the state besides confidence (e.g., gram- 
mar and history). This use (if ASP, (:ontidence. by the 
dialogue strategy is more Sol)hisli('ated than previ- 
ous al)proaches, e.g. (Niimi and Kot)ayashi, 1996; 
Lit\]nan and Pan, 2000). N.lI,'un ('an learn such line- 
grained distinctions l}ecause the el)ritual strategy is 
based on a eonll)arisoi) of 24~ l}ossible exl}h)ratory 
strategies. Both the initiative and confirmation re- 
suits sugge.sl that the begimfing of the dialogue was 
the most problenmtie for N.lli'un. Figure I ix an ex- 
ample dialogue using the Ol)tilnal strategy. 
5 Experimentally Evaluating the 
Strategy 
For the testing i)\]tase, NJFun was reilnplemented to
use the learned strategy. 2:t test sul)je(;Is then per- 
formed the same 6 tasks used during training, re- 
sulling in 124 complete test dialogues. ()he of our 
main resull;s is that task completion its measured by 
StrongCom 11 increased front 52cX} in training 1o 64% 
in testing (p < .06)) 
There is also a signilicant in~twaction (!II'(~c.t 
between strategy nnd task (p<.01) for Strong- 
Colnl).  \]'revious work has suggested l;hat novic(~ 
users l)erform (:Oml)arably to eXl)erts after only 2 
tasks (Kamm et ill., \] 9!18). Sill('e Ollr \]oarllt}d sl.rat- 
egy was based on 6 tasks with each user, one (?xpla- 
nation of the interaction eft'cot is that the learnc.d 
strategy is slightly optimized for expert users. ~lb 
explore this hyi)othesis, we divided our corpus into 
diah)gues with "novice" (tasks \] and 2) and "ex- 
pert" (tasks 3-6) users. We fOltltd that the learned 
strategy did in fact lc'a(l to a large an(1 significant 
improvement in StrongComp tbr (;Xl)erts (EIC=.d6, 
learned==.69, 11<.001), and a non-signilieant degra- 
dation for novices (1,31C=.66, learned=.55, 11<.3). 
An apparent limitation of these results is that EIC 
may not 1)e the best baseline strategy tbr coral)arisen 
to our learned strategy. A more standard alternative 
would be comparison to the very best hand-designed 
fixed strategy. However, there is no itgreement in the 
literature, nor amongst he authors, its to what the 
1)est hand-designed strategy might have been. There 
is agreement, however, that the best strategy ix sen- 
sitive to lnally unknown and unmodeled factors: the 
aThe ('.xlmrimental design (lescribed above Colmists of 2 
factors: the within-in groul) fa(:tor sl~ntefly aim the l)etween- 
groui)s facl;or task. \,Ve 11812, ~1, l,WO-~,g~l,y D.llO.ly,qiS of variance 
(ANOVA) to comtmte wlmtlmr main and int(!raction (!flk!cts 
of strategy are statistically signitica nt (t)<.05) or indicative 
of a statistical trend (p < .101. Main effe.cts of strategy are 
task-in(lel)endent , while interaction eIt'(!cts involving strat(%y 
are task-dependent. 
~4(~aSIlIX~ 
StrongComp 
\VcakComp 
ASR 
Fecdlmck 
UserSat 
EIC 
(n=:3111 
0.52 
1.75 
2.50 
0.18 
1.3.38 
v _ _  
l~eatned p 
(n=124) 
0.64 
2.19 .02 
2.67 .04 
0.11 .d2 
13.29 .86 
Table 1: Main ett'ects of dialogue strategy. 
user 1)olmlation, the specitics of the, task, the 1)ar- 
ticular ASR used, etc. Furthernlore, \]P, IC was (:are- 
fully designed so that the random choices it makes 
never results in tm unnatural dialogue. Finally, a 
companion paper (Singh et al, 2000) shows that the 
1)erforntanee of the learned strategy is better tha l l  
several "stmtdard" fixed strategies (such as always 
use system-initiative and no-confirmation). 
Although many types of measures have been used 
to evaluate dialogue systems (e.g., task success, 
dialogue quality, ettit:ieney, usability (l)anieli and 
Gerbino, 1995; Kamm et al, 11998)), we optimized 
only tbr one task success measure, StrongConll). 
Ilowever, we also examined the 1)erl 'ornmnee of the 
learned strategy using other ewduation measures 
(which t)ossibly could have llo011 used its our  reward 
function). WeakComp is a relaxed version of task 
comt)letion that gives partial credit: if all attribute 
values are either correct or wihh:ards, the value is the 
sum of the correct munl)er of attrilmtes. ()tlmrwise, 
at least one attribute is wrong (e.g., the user says 
"Lanfl)ertvilhf' but the system hears "Morristown"), 
and the wdue is -1. ASR is a dialogue quality lllea- 
sure that itl)l)roxinmtes Sl)eech recognition act:uracy 
for tl,e datM)ase query, a.nd is computed 1:) 3, adding 
1 for each correct attribute value altd .5 for every 
wihtca.rd. Thus, if the task ix to go winetasting 
near Lambertville in the morning, and the systenl 
queries the database for an activity in New Jersey 
in the morning, StrongComp=0, \VeakComp=l, and 
ASR=2. In addition to the objective measures dis- 
cussed a,bove, we also COmlmted two subjective us- 
ability measures. Feedback  is obtained front the 
dialogue (e.g. $4 in Figure 5), by mapping good, 
so-so, bad to 1, 0, m~d -1, respectively. User satis- 
faction (UserSat, ranging front 0-20) is obtained by 
summing the answers of the web-based user survey. 
Table I summarizes the diflhrence in performance 
of NJFun tbr our original reward flmction and the 
above alternative valuation measures, from trail> 
ing (EIC) to test (learned strategy for StrongComp). 
For WeakComp, the average reward increased from 
1.75 to 2.19 (p < 0.02), while tbr ASll the average 
reward increased from 2.5 to 2.67 (p < 0.04). Again, 
these iml)rovements occur even though the learned 
strategy was not optilnized for these measures. 
The last two rows of the table show that for the 
507 
subjective measures, i)erformmme does not signifi- 
cantly differ for the EIC and learned strategies. In- 
terestingly, the distributions of the subjective mea- 
sures move to the middle from training to testing, 
i.e., test users reply to the survey using less extreme 
answers than training users. Explaining the subjec- 
tire results is an area for future work. 
6 Discussion 
This paper presents a practical methodology for ap- 
plying RL to optimizing dialogue strategies in spo- 
ken dialogue systems, and shows empirically that the 
method improves performance over the EIC strategy 
in NJFun. A companion paper (Singh et al, 2000) 
shows that the learned strategy is not only better 
than EIC, but also better than other fixed choices 
proposed in the literature. Our results demonstrate 
that the application of RL allows one to empirically 
optimize a system's dialogue strategy by searching 
through a much larger search space than can be ex- 
plored with more traditional lnethods (i.e. empiri- 
cally testing several versions of a systent). 
RL has been appled to dialogue systems in pre- 
vious work, but our approach ditlhrs from previous 
work in several respects. Biermann and Long (1996) 
did not test RL in an implemented system, and the 
experiments of Levin et 31. (2000) utilized a simu- 
lated user model. Walker et al (1998)'s methodol- 
ogy is similar to that used here, in testing RL with 
an imt)lelnented system with human users. However 
that work only explored strategy choices at 13 states 
in the dialogue, which conceivably could have been 
explored with more traditional methods (~ts com- 
pared to the 42 choice states explored here). 
We also note that our learned strategy made di- 
alogue decisions based on ASR confidence in con- 
junction with other features, mid alto varied initia- 
tive and confirmation decisions at a finer grain than 
previous work; as such, our learned strategy is not; 
a standard strategy investigated in the dialogue sys- 
teln literature. For example, we would not have pre- 
dicted the complex and interesting back-off strategy 
with respect o initiative when reasking for an at- 
tribute. 
To see how our method scales, we are al)plying RL 
to dialogue systems for eustolner care and tbr travel 
planning, which are more complex task-oriented do- 
mains. As fllture work, we wish to understand 
the aforementioned results on the subjective reward 
measures, explore the potential difference between 
optimizing tbr expert users and novices, automate 
the choice of state space for dialogue systems, ilwes- 
tigate the use of a learned reward function (Walker 
et al, 1998), and explore the use of more informative 
non-terminal rewards. 
Acknowledgements 
The authors thank Fan Jiang for his substantial effort 
in implenmnting NJFun, Wieland Eckert, Esther Levin, 
Roberto Pieraccini, and Mazin R.ahinl for their technical 
help, Julia Hirsehberg for her comments on a draft of this 
paper, and David McAllester, I~ichard Sutton, Esther 
Levin and Roberto Pieraccini for hell)tiff conversations. 
References 
A. W. Biermann and P. M. Long. 1996. The composition 
of messages in sl)eeeh-graphies interactive systems. In 
Proe. of the International Symposium on Spoken Dia- 
logue, pages 97 100. 
M. Danieli and E. Gerbino. 1995. Metrics for evaluating 
dialogue strategies in a spoken language system. In 
P~vc. of the AAAI  Spring Symposium on Empirical 
Methods in Discourse Interpretation and Generation, 
pages 34 39. 
C. Kamm, D. Litman, and M. A. Walker. 1998. From 
novice to expert: The effect of tutorials on user exl)er- 
tise with spoken dialogue systems. In P~vc. of the In- 
ternational Conference on Spolccn Language P~vccss- 
in.q, ICSLP98. 
E. Levin, R. Pieraccini, W. Eekere, G. Di Fabbrizio, and 
S. Narayanan. 1999. Spoken language dialogue: lh'om 
theory to practice. In Pwc. IEEE Workshop on Au- 
tomatic Speech R.ecognition and Understanding, AS- 
R U U99. 
E. Levin, R. Pieraccini, and W. Eckert. 2000. A stochas- 
tic model of human machine interaction for learning 
dialog strategies. IEEE TTnnsactions on Speech and 
Audio Processing, 8(1):11-23. 
D. J. Litman and S. Pan. 2000. Predicting and adapting 
to poor Sl)eech recognition in a spoken dialogue sys- 
tern. In Proc. of the Scv('ntccnth National Confl:rcncc 
on Artificial Intclligcncc, AAAI-2000. 
Y. Niimi and Y. Kobayashi. 1996. A dialog control strat- 
egy based on the reliability of speech recognition. In 
Proc. of the International Symposium on Spoken Dia- 
loguc, pages 157--160. 
A. Sanderman, J. Sturm, E. den Os, L. Boves, and 
A. Cremers. 1998. Evaluation of the dutchtrain 
timetable inibrmation system developed in the arise 
project. In Interactive Voice Technology for Tclccom- 
munications Applications, IVT2'A, pages 91-96. 
S. Singh, M. S. Kearns, D. J. Litman, and M. A. \Valker. 
1999. Reinforcement learning for spoken dialogue sys- 
tems. In Proc. NIPS99. 
S. B. Singh, M. S. Kearns, D. J. Litman, and 
M. A. Walker. 2000. Empirical evaluation of a rein- 
forccment learning spoken dialogue system. In Proc. 
of thc Scvcntccnth National Conference on Artificial 
Intelligence, AAAI-2000. 
R. S. Sutton. 1991. Plamfing by incremental dynamic 
programming. In Proc. Ninth Confcwztcc on Machine 
Learning, pages 353-357. 
M. A. Walker, J. C. Promer, and S. Narayanan. 1998. 
Learning optimal dialogue strategies: A ease study of 
a Sl)oken dialogue agent br email. In P~vc. of the 36th 
Annual Meeting of the Association of Computational 
Linguistics, COLING//ACL 98, pages 1345 1352. 
508 
Towards Automatic Generation of Natural Language Generation
Systems
John Chen?, Srinivas Bangalore?, Owen Rambow?, and Marilyn A. Walker?
Columbia University? AT&T Labs?Research?
New York, NY 10027 Florham Park, NJ 07932
{jchen,rambow}@cs.columbia.edu {srini,walker}@research.att.com
Abstract
Systems that interact with the user via natural
language are in their infancy. As these systems
mature and become more complex, it would be
desirable for a system developer if there were
an automatic method for creating natural lan-
guage generation components that can produce
quality output efficiently. We conduct experi-
ments that show that this goal appears to be
realizable. In particular we discuss a natural
language generation system that is composed of
SPoT, a trainable sentence planner, and FER-
GUS, a stochastic surface realizer. We show
how these stochastic NLG components can be
made to work together, that they can be ported
to new domains with apparent ease, and that
such NLG components can be integrated in a
real-time dialog system.
1 Introduction
Systems that interact with the user via natural
language are in their infancy. As these systems
mature and become more complex, it would
be desirable for a system developer if there
were automatic methods for creating natural
language generation (NLG) components that
can produce quality output efficiently. Stochas-
tic methods for NLG may provide such auto-
maticity, but most previous work (Knight and
Hatzivassiloglou, 1995), (Langkilde and Knight,
1998), (Oh and Rudnicky, 2000), (Uchimoto et
al., 2000), (Bangalore and Rambow, 2000) con-
centrate on the specifics of individual stochastic
methods, ignoring other issues such as integra-
bility, portability, and efficiency. In contrast,
this paper investigates how different stochastic
NLG components can be made to work together
effectively, whether they can easily be ported to
new domains, and whether they can be inte-
grated in a real-time dialog system.
Request(DEPART?DATE)
Surface Generator
FERGUS
TTS
SPoT
Dialog Manager
Sentence Planner
DM
Imp?conf(N)
soft?merge
Text to Speech
Implicit?confirm(NEWARK)
Implicit?confirm(DALLAS)
period
Imp?conf(D)
Flying from Newark to
Dallas.  What date would
you like to leave?
Request(D?D)
Figure 1: Components of an NLG system.
Recall the basic tasks in NLG. During text
planning, content and structure of the target
text are determined to achieve the overall com-
municative goal. During sentence planning, lin-
guistic means?in particular, lexical and syn-
tactic means?are determined to convey smaller
pieces of meaning. During realization, the spec-
ification chosen in sentence planning is trans-
formed into a surface string by linearizing and
inflecting words in the sentence (and typically,
adding function words). Figure 1 shows how
such components cooperate to generate text
corresponding to a set of communicative goals.
Our work addresses both the sentence plan-
ning stage and the realization stage. The sen-
tence planning stage is embodied by the SPoT
sentence planner (Walker et al, 2001), while
the surface realization stage is embodied by the
FERGUS surface realizer (Bangalore and Ram-
bow, 2000). We extend the work of (Walker et
al., 2001) and (Bangalore and Rambow, 2000)
in various ways. We show that apparently each
of SPoT and FERGUS can be ported to differ-
ent domains with little manual effort. We then
show that these two components can work to-
gether effectively. Finally, we show the on-line
integration of FERGUS with a dialog system.
2 Testing the Domain Independence
of Sentence Planning
In this section, we address the issue of the
amount of effort that is required to port a sen-
tence planner to new domains. In particular,
we focus on the SPoT sentence planner. The
flexibility of the training mechanism that SPoT
employs allows us to perform experiments that
provide evidence for its domain independence.
Being a sentence planner, SPoT chooses ab-
stract linguistic resources (meaning-bearing lex-
emes, syntactic constructions) for a text plan. A
text plan is a set of communicative goals which
is assumed to be output by a dialog manager of
a spoken dialog system. The output of SPoT is
a set of ranked sentence plans, each of which is
a binary tree with leaves labeled by the commu-
nicative goals of the text plan.
SPoT divides the sentence planning task into
two stages. First, the sentence-plan-generator
(SPG) generates 12-20 possible sentence plans
for a given input text plan. These are gener-
ated randomly by incrementally building each
sentence plan according to some probability
distribution. Second, the sentence-plan-ranker
(SPR) ranks the resulting set of sentence plans.
SPR is trained for this task via RankBoost (Fre-
und et al, 1998), a machine learning algorithm,
using as training data sets of sentence plans
ranked by human judges.
In porting SPoT to a new domain, this last
point seems to be a hindrance. New train-
ing data in the new domain ranked by hu-
man judges might be needed in order to train
SPoT. To the contrary, our experiments that
show that this need not be the case. We par-
tition the set of all features used by (Walker et
al., 2001) to train SPoT into three subsets ac-
cording to their level of domain and task de-
pendence. Domain independent features are
features whose names include only closed-class
words, e.g. ?in,? or names of operations that in-
crementally build the sentence plan, e.g. merge.
Domain-dependent, task-independent features
are those whose names include open class words
Features Used Mean Score S.D.
all 4.56 0.68
domain-independent 4.55 0.69
task-independent 4.20 0.99
task-dependent 3.90 1.19
Table 1: Results for subsets of features used to
train SPoT
specific to this domain, e.g. ?travel? or the
names of the role slots, e.g. $DEST-CITY. Do-
main dependent, task dependent features are
features whose names include the value of a role
filler for the domain, e.g. ?Albuquerque.?
We have trained and tested SPoT with these
different feature subsets using the air-travel do-
main corpus of 100 text plans borrowed from
(Walker et al, 2001), using five fold cross-
validation. Results are shown in Table 2 us-
ing t-tests with the modified Bonferroni statis-
tic for multiple comparisons. Scores can range
from 1.0 (worst) to 5.0 (best). The results in-
dicate that the domain independent feature set
performs as well as all the features (t = .168, p
= .87), but that both the task independent (t
= 6.25, p = 0.0) and the task dependent (t =
4.58, p = 0.0) feature sets perform worse.
3 Automation in Training a Surface
Realizer
As with the sentence planning task, there is the
possibility that the task of surface realization
may be made to work in different domains with
relatively little manual effort. Here, we perform
experiments using the FERGUS surface realizer
to determine whether this may be so. We re-
view the FERGUS architecture, enumerate re-
sources required to train FERGUS, recapitulate
previous experiments that indicate how these
resources can be automatically generated, and
finally show how similar ideas can be used to
port FERGUS to different domains with little
manual effort.
3.1 Description of the FERGUS
Surface Realizer
Given an underspecified dependency tree repre-
senting one sentence as input, FERGUS outputs
the best surface string according to its stochas-
tic modeling. Each node in the input tree corre-
sponds to a lexeme. Nodes that are related by
grammatical function are linked together. Sur-
face ordering of the lexemes remains unspecified
in the tree.
FERGUS consists of three models: tree
chooser, unraveler, and linear precedence
chooser. The tree chooser associates a su-
pertag (Bangalore and Joshi, 1999) from a tree-
adjoining grammar (TAG) with each node in
the underspecified dependency tree. This par-
tially specifies the output string?s surface order;
it is constrained by grammatical constraints en-
coded by the supertags (e.g. subcategorization
constraints, voice), but remains free otherwise
(e.g. ordering of modifiers). The tree chooser
uses a stochastic tree model (TM) to select a
supertag for each node in the tree based on lo-
cal tree context. The unraveler takes the re-
sulting semi-specified TAG derivation tree and
creates a word lattice corresponding to all of
the potential surface orderings consistent with
this tree. Finally, the linear precedence (LP)
chooser finds the best path through the word
lattice according to a trigram language model
(LM), specifying the output string completely.
Certain resources are required in order to
train FERGUS. A TAG grammar is needed?
the source of the supertags with which the
semi-specified TAG derivation tree is annotated.
There needs to be a treebank in order to ob-
tain the stochastic model TM driving the tree
chooser. There also needs to be a corpus of sen-
tences in order to train the language model LM
required for the LP chooser.
3.2 Labor-Minimizing Approaches to
Training FERGUS
The resources that are needed to train FER-
GUS seem quite labor intensive to develop. But
(Bangalore et al, 2001) show that automati-
cally generated version of these resources can
be used by FERGUS to obtain quality output.
Two kinds of TAG grammar are used in (Ban-
galore et al, 2001). One kind is a manually de-
veloped, broad-coverage grammar for English:
the XTAG grammar (XTAG-Group, 2001). It
consists of approximately 1000 tree frames. Dis-
advantages of using XTAG are the consider-
able amount of human labor expended in its
development and the lack of a treebank based
on XTAG?the only way to estimate parame-
ters in the TM is to rely on a heuristic map-
ping of XTAG tree frames onto a pre-existing
treebank (Bangalore and Joshi, 1999). Another
kind of grammar is a TAG automatically ex-
tracted from a treebank using the techniques of
(Chen, 2001) (cf. (Chiang, 2000), (Xia, 1999)).
These techniques extract a linguistically mo-
tivated TAG using heuristics programmed us-
ing a modicum of human labor. They nullify
the disadvantages of using the XTAG grammar,
but they introduce potential complications?
notably, an extracted grammar?s size is often
much larger than that of XTAG, typically more
than 2000 tree frames, potentially leading to a
larger sparse data problem, and also the result-
ing grammar is not hand-checked.
Two kinds of treebank are used in (Bangalore
et al, 2001). One kind is the Penn Treebank
(Marcus et al, 1993). It consists of approxi-
mately 1,000,000 words of hand-checked, brack-
eted text. The text consists of Wall Street Jour-
nal news articles. The other kind of treebank is
the BLLIP corpus (Charniak, 2000). It con-
sists of approximately 40,000,000 words of text
that has been parsed by a broad-coverage sta-
tistical parser. The text consists of Wall Street
Journal news and newswire articles. The ad-
vantage of the former is that it has been hand-
checked, whereas the latter has the advantage
of being easily produced and hence can easily
be enlarged.
(Bangalore et al, 2001) experimentally de-
termine how the quality and quantity of the
resources used in training FERGUS affect the
output quality of the generator. They find that
while a better quality annotated corpus (Penn
Treebank) results in better model accuracy than
a lower quality corpus (BLLIP) of the same size,
an (easily-obtained) larger lower quality corpus
results in a model that eclipses a smaller, better
quality treebank. Also, the model that is ob-
tained when using an automatically extracted
grammar yields comparable output quality to
the model that is obtained when using a hand-
crafted (XTAG) grammar.
3.3 Automating Adaptation of
FERGUS to a New Domain
This paper is about minimizing the amount of
manual labor that is required to port NLG com-
ponents to different domains. (Bangalore et
al., 2001) perform all of their experiments on
the same domain of Wall Street Journal news
articles. In contrast, in this section we show
that FERGUS can be adapted to the domain of
air-travel reservation dialogs with minimal hu-
man effort. We show that out-of-domain train-
ing data can be used instead of in-domain data
without drastically compromising output qual-
ity. We also show that partially parsed in-
domain training data can be effectively used
to train the TM. Finally, we show that using
an in-domain corpus to train the LM can help
the output quality, even if that corpus is of
small size. In this section, we first describe the
training resources that are used in these exper-
iments. We subsequently describe the experi-
ments themselves and their results.
Various corpora are used in these experi-
ments. For training, there are two distinct
corpora. First, there is the previously in-
troduced Penn Treebank (PTB). As the
alternative, there is a human-human corpus of
dialogs (HH) from Carnegie Mellon University.
The HH corpus consists of approximately
13,000 words in the air-travel reservation
domain. This is not exactly the target domain
because human-human interaction differs
from human-computer interaction which is
our true target domain. From this raw text,
an LDA parser (Bangalore and Joshi, 1999)
trained using the XTAG-based Penn Treebank
creates a partially-parsed, non-hand-checked
treebank. Test data consists of about 2,200
words derived from Communicator template
data. Communicator templates are hand-
crafted surface strings of words interspersed
with slot names. An example is ?What time
would you, traveling from $ORIG-CITY
to $DEST-CITY like to leave?? The test
data is derived from all strings like these, with
duplicates, in the Communicator system by
replacing the slot names with fillers according
to a probability distribution. Furthermore,
dependency parses are assigned to the resulting
strings by hand.
In the first series of experiments, we ascertain
the output quality of FERGUS using the XTAG
grammar on different training corpora. We vary
the TM?s training corpus to be either PTB or
HH. We do the same for the LM?s training cor-
pus. Assessing the output quality of a generator
is a complex issue. Here, we select as our met-
ric understandability accuracy, defined in (Ban-
galore et al, 2000) as quantifying the differ-
PTB TM HH TM
PTB LM 0.30 0.38
HH LM 0.37 0.41
Table 2: Average understandability accuracies
using XTAG-Based FERGUS for various kinds
of training data
PTB TM
PTB LM 0.39
HH LM 0.33
Table 3: Average understandability accuracies
using automatically-extracted grammar based
FERGUS for various kinds of training data
ence between the generator output, in terms of
both dependency tree and surface string, and
the desired reference output. (Bangalore et al,
2000) finds this metric to correlate well with hu-
man judgments of understandability and qual-
ity. Understandability accuracy varies between
a high score of 1.0 and a low score which may
be less than zero.
The results of our experiments are shown in
Table 2. We conclude that despite its smaller
size, and despite its being only automatically-
and partially- parsed, using the in-domain HH
is more effective than using the out-of-domain
PTB for training the TM. Similarly, HH is more
effective than PTB for training the LM. The
best result is obtained by using HH to train both
the TM and the LM; this result (0.41) is com-
parable to the result obtained by using matched
PTB training and test data (0.43) that is used
in (Bangalore et al, 2001).
The second series of experiments investi-
gates the output quality of FERGUS using
automatically-extracted grammars. In these ex-
periments, the TM is always trained on PTB
but not HH. It is the type of training data that
is used to train the LM, either PTB or HH,
that is varied. The results are shown in Ta-
ble 3. Note that these scores are in the same
range as those obtained when training FER-
GUS using XTAG. Also, these scores show that
when using automatically-extracted grammars,
training LM using a large, out-of-domain cor-
pus (PTB) is more beneficial than training LM
using a small, in-domain corpus (HH).
We can now draw various conclusions about
training FERGUS in a new domain. Con-
sider training the TM. It is not necessary
to use a handwritten TAG in the new do-
main; a broad-coverage hand-written TAG or
an automatically-extracted TAG will give com-
parable results. Also, instead of requiring a
hand-checked treebank in the new domain, par-
tially parsed data in the new domain is ade-
quate. Now consider training the LM. Our ex-
periments show that a small corpus in the new
domain is a viable alternative to a large corpus
that is out of the domain.
4 Integration of SPoT with
FERGUS
We have seen evidence that both SPoT and
FERGUS may be easily transferable to a new
domain. Because the output of a sentence plan-
ner usually becomes the input of a surface real-
izer, questions arise such as whether SPoT and
FERGUS can be made to work together in a
new domain and what is the output quality of
the combined system. We will see that an ad-
dition of a rule-based component to FERGUS
will be necessary in order for this integration
to occur. We will subsequently see that the
output quality of the resulting combination of
SPoT and FERGUS is quite good.
Integration of SPoT as described in Section 2
and FERGUS as described in Section 3 is not
automatic. The reason is that the output of
SPoT is a deep syntax tree (Mel?c?uk, 1998)
whereas hitherto the input of FERGUS has
been a surface syntax tree. The primary dis-
tinguishing characteristic of a deep syntax tree
is that it contains features for categories such as
definiteness for nouns, or tense and aspect for
verbs. In contrast, a surface syntax tree real-
izes these features as function words. However,
there is a one-to-one mapping from features of
a deep syntax tree to function words in the cor-
responding surface syntax tree. Therefore, inte-
grating SPoT with FERGUS is basically a mat-
ter of performing this mapping. We have added
a rule-based component (RB) as the new first
stage of FERGUS to do just that. Note that it
is erroneous to think that RB makes choices be-
tween different generation options because there
is a one-to-one mapping between features and
function words.
PTB TM HH TM
PTB LM 0.48 0.47
HH LM 0.73 0.68
Table 4: Average understandability accuracies
of SPoT-integrated, XTAG-Based FERGUS for
various kinds of training data
After the addition of RB to FERGUS, we
evaluate the output quality of the combination
of SPoT and FERGUS. Only the XTAG gram-
mar is used in this experiment. As in previous
experiments with the XTAG grammar, there is
either the option of training using HH or PTB
derived data for either the TM or LM, giving a
total of four possibilities.
Test data is obtained by output strings that
are produced by the combination of SPoT and
the RealPro surface realizer (Lavoie and Ram-
bow, 1998). RealPro has the advantage of pro-
ducing high quality surface strings, but at the
cost of having to be hand-tuned to a particu-
lar domain. It is this cost we are attempting to
minimize by using FERGUS. Only those sen-
tence plans produced by SPoT ranked 3.0 or
greater by human judges are used. The surface
realization of these sentence plans yields a test
corpus of 2,200 words.
As shown in Table 4, the performance of
SPoT and FERGUS combined is quite high.
Also note that in terms of training the LM, out-
put quality is markedly better when HH is used
rather than PTB. Furthermore, note that there
is a smaller difference between using PTB or
HH to train TM when compared to previous re-
sults shown in Table 2. This seems to indicate
that the TM?s effect on output quality dimin-
ishes because of addition of RB to FERGUS.
5 On-line Integration of FERGUS
with a Dialog System
Certain statistical natural language processing
systems can be quite slow, usually because of
the large search space that these systems must
explore. It is therefore uncertain whether a sta-
tistical NLG component can be integrated into a
real-time dialog system. Investigating the mat-
ter in FERGUS?s case, we have experimented
with integrating FERGUS into Communicator,
a mixed-initiative, airline travel reservation sys-
tem. We begin by explaining how Communica-
tor manages surface generation without FER-
GUS. We then delineate several possible kinds
of integration. Finally, we describe our experi-
ences with one kind of integration.
Communicator performs only a rudimentary
form of surface generation as follows. The
dialog manager of Communicator issues a set
of communicative goals that are to be realized.
Surface template strings are selected based
on this set, such as ?What time would you,
traveling from $ORIG-CITY to $DEST-CITY
like to leave?? The slot names in these
strings are then replaced with fillers according
to the dialog manager?s state. The resulting
strings are then piped to a text-to-speech
component (TTS) for output.
There are several possibilities as to how FER-
GUS may supplant this system. One possibility
is off-line integration. In this case, the set of all
possible sets communicative goals for which the
dialog manager requires realization are matched
with a set of corresponding surface syntax trees.
The latter set is input to FERGUS, which gen-
erates a set of surface template strings, which in
turn is used to replace the manually created sur-
face template strings that are an original part
of Communicator. Since these changes are pre-
compiled, the resulting version of Communica-
tor is therefore as fast the original. On the other
hand, off-line integration may be unmanageable
if the set of sets of communicative goals is very
large. In that case, only the alternative of on-
line integration is palatable. In this approach,
each surface template string in Communicator is
replaced with its corresponding surface syntax
tree. At points in a dialog where Communicator
requires surface generation, it sends the appro-
priate surface syntax trees to FERGUS, which
generates surface strings.
We have implemented the on-line integration
of FERGUS with Communicator. Our experi-
ments show that FERGUS is fast enough to be
used in for this purpose, the average time for
FERGUS to generate output strings for one di-
alog turn being only 0.28 seconds.
6 Conclusions and Future Work
We have performed experiments that provide
evidence that components of a statistical NLG
system may be ported to different domains
without a huge investment in manual labor.
These components include a sentence planner,
SPoT, and a surface realizer, FERGUS. SPoT
seems easily portable to different domains be-
cause it can be trained well using only domain-
independent features. FERGUS may also be
said to be easily portable because our experi-
ments show that the quality and quantity of in-
domain training data need not be high and plen-
tiful for decent results. Even if in-domain data
is not available, we show that out-of-domain
training data can be used with adequate results.
By integrating SPoT with FERGUS, we have
also shown that different statistical NLG com-
ponents can be made to work well together. In-
tegration was achieved by adding a rule-based
component to FERGUS which transforms deep
syntax trees into surface syntax trees. The com-
bination of SPoT and FERGUS performs with
high accuracy. Post-integration, there is a di-
minishing effect of TM on output quality.
Finally, we have shown that a statistical NLG
component can be integrated into a dialog sys-
tem in real time. In particular, we replace the
hand-crafted surface generation of Communica-
tor with FERGUS. We show that the resulting
system performs with low latency.
This work may be extended in different di-
rections. Our experiments showed promising re-
sults in porting to the domain of air travel reser-
vations. Although this is a reasonably-sized do-
main, it would be interesting to see how our
findings vary for broader domains. Our experi-
ments used a partially parsed version of the HH
corpus. We would like to compare its use as
TM training data in relation to using a fully
parsed version of HH, and also a hand-checked
treebank version of HH. We would also like to
investigate the possibility of interpolating mod-
els based on different kinds of training data in
order to ameliorate data sparseness. Our ex-
periments focused on integration between the
NLG components of sentence planning and sur-
face generation. We would like to explore the
possibility of further integration, in particular
integrating these components with TTS. This
would provide the benefit of enabling the use of
syntactic and semantic information for prosody
assignment. Also, although FERGUS was inte-
grated with SPoT relatively easily, it does not
necessarily follow that FERGUS can be inte-
grated easily with other kinds of components.
It may be worthwhile to envision a redesigned
version of FERGUS whose input can be flexibly
underspecified in order to accommodate differ-
ent kinds of modules.
7 Acknowledgments
This work was partially funded by DARPA un-
der contract MDA972-99-3-0003.
References
Srinivas Bangalore and A. K. Joshi. 1999. Su-
pertagging: An approach to almost parsing.
Computational Linguistics, 25(2).
Srinivas Bangalore and Owen Rambow. 2000.
Exploiting a probabilistic hierarchical model
for generation. In Proceedings of the 18th
International Conference on Computational
Linguistics (COLING 2000).
Srinivas Bangalore, Owen Rambow, and Steve
Whittaker. 2000. Evaluation metrics for gen-
eration. In Proceedings of the First Interna-
tional Conference on Natural Language Gen-
eration, Mitzpe Ramon, Israel.
Srinivas Bangalore, John Chen, and Owen
Rambow. 2001. Impact of quality and quan-
tity of corpora on stochastic generation. In
Proceedings of the 2001 Conference on Em-
pirical Methods in Natural Langauge Process-
ing, Pittsburgh, PA.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of First An-
nual Meeting of the North American Chap-
ter of the Association for Computational Lin-
guistics, Seattle, WA.
John Chen. 2001. Towards Efficient Statis-
tical Parsing Using Lexicalized Grammati-
cal Information. Ph.D. thesis, University of
Delaware.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining gram-
mar. In Proceedings of the the 38th Annual
Meeting of the Association for Computational
Linguistics, pages 456?463, Hong Kong.
Yoav Freund, Raj Iyer, Robert E. Schapire, and
Yoram Singer. 1998. An efficient boosting
algorithm for combining preferences. In Ma-
chine Learning: Proceedings of the Fifteenth
International Conferece.
Kevin Knight and V. Hatzivassiloglou. 1995.
Two-level many-paths generation. In Pro-
ceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics,
Boston, MA.
Irene Langkilde and Kevin Knight. 1998. Gen-
eration that exploits corpus-based statisti-
cal knowledge. In Proceedings of the 17th
International Conference on Computational
Linguistics and the 36th Annual Meeting of
the Association for Computational Linguis-
tics, Montreal, Canada.
Benoit Lavoie and Owen Rambow. 1998. A
framework for customizable generation of
multi-modal presentations. In Proceedings of
the 17th International Conference on Com-
putational Linguistics and the 36th Annual
Meeting of the Association for Computational
Linguistics, Montreal, Canada.
Mitchell Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of english: the
penn treebank. Computational Linguistics,
19(2):313?330.
Igor A. Mel?c?uk. 1998. Dependency Syntax:
Theory and Practice. State University of New
York Press, New York, NY.
Alice H. Oh and Alexander I. Rudnicky.
2000. Stochastic language generation for spo-
ken dialog systems. In Proceedings of the
ANLP/NAACL 2000 Workshop on Conver-
sational Systems, pages 27?32, Seattle, WA.
Kiyotaka Uchimoto, Masaki Murata, Qing Ma,
Satoshi Sekine, and Hitoshi Isahara. 2000.
Word order acquisition from corpora. In Pro-
ceedings of the 18th International Confer-
ence on Computational Linguistics (COLING
?00), Saarbru?cken, Germany.
Marilyn A. Walker, Owen Rambow, and Mon-
ica Rogati. 2001. Spot: A trainable sentence
planner. In Proceedings of the Second Meeting
of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages
17?24.
Fei Xia. 1999. Extracting tree adjoining gram-
mars from bracketed corpora. In Fifth Natu-
ral Language Processing Pacific Rim Sympo-
sium (NLPRS-99), Beijing, China.
The XTAG-Group. 2001. A Lexicalized
Tree Adjoining Grammar for English.
Technical report, University of Penn-
sylvania. Updated version available at
http://www.cis.upenn.edu/?xtag.
Amount of Information Presented in a Complex List:   
Effects on User Performance 
Dawn Dutton 
AT&T Labs - Research 
180 Park Avenue 
D103, Bldg. 103 
Florham Park, NJ 07932 
+1.973.236.6522 
dldutton@att.com 
Selina Chu 
Information and Computer Science 
444 Computer Sci. Bldg. 
UC Irvine 
Irvine, CA 92697 
+1.949.509.9762 
selina@ics.uci.edu 
James Hubbell 
12 Lawrence Avenue 
West Long Branch, NJ 07932 
+1.732.728.9351 
jameshubbell@hotmail.com 
Marilyn Walker 
AT&T Labs - Research 
180 Park Avenue 
E103, Bldg. 103 
Florham Park, NJ 07932 
+1.973.360.8956 
walker@research.att.com 
Shrikanth Narayanan 
Speech and Image Proc. Institute 
Department of EE-Systems 
EEB 430, USC 
Los Angeles, CA 90089 
+1.213.740.6432 
shri@sipi.usc.edu 
 
 
ABSTRACT 
AT&T Communicator is a state-of-the-art speech-enabled 
telephony-based application that allows the end-user to, among 
other things, select and reserve airline itineraries.  This 
experiment explores how the amount and structure of information 
presented in complex lists influences the user experience and the 
ability of subjects to successfully complete a selection task.  
Presenting all the relevant information needed for a decision at 
once was the factor that most positively influenced successful task 
completion and the user experience.  Subjects preferred hearing 
all of the relevant information about each flight, without initiating 
additional dialog with the system.  Additionally, successful task 
completion rates improved when all of the flights were presented 
at once, without any intervening questions from the system.  
Keywords 
dialogue, user interface, human factors, spoken language, lists, 
selection 
1. INTRODUCTION 
AT&T Communicator is a state-of-the-art spoken dialogue system 
that allows the end-user to select and reserve various travel related 
resources, in particular, airfare, hotel, and rental cars [2].  One of 
its most challenging dialog modules is that for presenting 
information that allows the user to select a desired air travel 
itinerary. While selecting between multiple possible itineraries in 
a visual domain is a relatively simple task because most of the 
selection criteria are listed in a single scrollable page, the same 
task is likely to have a much higher cognitive load in an audio-
only domain [1, 3]. The selection criteria for each candidate flight 
must be presented to the user serially, leading to higher cognitive 
demands that could result in errors in itinerary selection.     
A sample AT&T Communicator dialog with the prototype 
used in this experiment, showing the outbound flight only, 
follows. 
System: Hello, this is AT&T Communicator.   
How may I help with your travel plans? 
User:   Yeah, I?d like to fly from Newark to San Francisco. 
System: OK, a flight from Newark, New Jersey to San 
Francisco, California.  And will this be a round 
trip? 
User:   Yes, and I need to arrive in San Francisco by 4 PM 
on November 22nd. 
System: I found five outbound United Airlines flights.   
Option 1 is flight number 635.  It has one stop; it 
departs at 10:15 A.M. and arrives at 2:42 P.M. 
<pause>   
Option 2 is flight number 7535.  It has one stop; It 
departs at 12:20 P.M., and arrives at 4:37 P.M.  
<pause>   
Option 3 is flight number 837.  It' s a non-stop 
flight; it departs at 2:00 P.M.  and arrives at 5:01 
P.M.  <pause>   
Would you like to hold any of these flights? 
 
 
 
User:   I guess I?ll take option one. 
System: Got it.   
Note that the second to the last system utterance, in which a 
set of itineraries are presented to the user for selection, is by far 
the longest system utterance in the dialog.  This utterance 
includes a complex list of information.  Unlike a more simple list, 
each list item (option) contains a number of sub-items, for 
example flight number, number of stops, etc.  In early dialog 
evaluations of the Communicator system, the selection of a single 
itinerary from such a complex list of flights was a source of 
substantial usability problems; users frequently became frustrated.  
In fact, many users dropped out of the dialog before successfully 
selecting an itinerary.   
This paper describes an experiment in which we vary the 
amount and structure of information presented about available 
itineraries.  We predict that the amount and structure of 
information presented affects the ability of users to successfully 
select the optimal itinerary within a set, and influences subjective 
measures such as user satisfaction. 
2. METHODS AND PROCEDURES 
2.1 Subjects 
Sixty-four subjects were run at a local shopping mall over a five 
day period.  Subjects were recruited from the shoppers 
frequenting the mall. 
2.2 Wizard of Oz 
A Wizard of Oz (WOZ) experiment was run to determine the 
optimal way for the end-user to select a desired itinerary in the 
Communicator project. 
A Wizard of Oz experiment is one in which no real automatic 
speech recognition (ASR) or natural language understanding 
(NLU) is used.  Instead, the user interface is prototyped and a 
?wizard,? or experimenter, acts in place of the ASR and NLU.  
Consequently, subjects believe that ASR/NLU is being used.  The 
WOZ methodology allows competing user interface strategies to 
be prototyped and tested with end users in a shorter period of time 
than would be required to implement multiple fully-functioning 
systems with competing user interfaces. 
2.3 Apparatus & Materials 
Relevant aspects of the AT&T Communicator user interface were 
prototyped using the Unisys Natural Language Speech Assistant 
(NLSA) software.  NLSA runs on a PC using the Windows NT 
operating system.  Subjects called into the Communicator 
prototype using an analog telephone and interacted with the 
system by voice.  The wizard categorized the subject?s speech 
using the NLSA Wizard graphical user interface (GUI).  Each 
subject completed 5 surveys in pen and paper format.  During the 
course of the experiment, subjects also had access to a pad of 
paper. 
2.4 Experimental Design 
All itineraries presented to the subjects were round-trip. 
2.4.1 Independent Variables 
This was a factorial experiment with two factors, one factor 
between subjects and the other within subject (see Table 1). 
Selection Itinerary Content.
  There were two levels of this 
between subjects factor: 
--Terse.  The presented itineraries included:  airline, number 
of stops, and departure time1.  In order to get additional 
information, the user could ask the system questions (e.g. 
?When does that flight arrive??). 
--Verbose.  The presented itineraries included:  airline, flight 
number, number of stops, departure time, and arrival time.  
All the information relevant to the tasks specified in the 
experiment are presented about each flight; the user did not 
need to ask questions to get additional information. 
Number of Flights Before Question.
  Each level is actually a 
combination of two separate, but related, factors. 
--Combined vs. Separate.  Whether outbound and return 
flights are presented separately or in combination.  
--Number of flights.  The number of flights that are presented 
before asking the subject to make a decision. 
Four levels of this factor were chosen.  In all cases (1) the 
total number of flights ?found? was 5, and, (2) the question was, 
?Would you like to hold [that flight/any of  those flights]??. 
--Separate 1.  The outbound and return flights of the trip are 
presented separately and after each flight the subject is asked 
the question. 
--Separate 3.  The outbound and return flights of the trip are 
presented separately and after the third flight the subject is 
asked the question. 
--Separate 5.  The outbound and return flights of the trip are 
presented separately and after the last flight the subject is 
asked the question.   
--Combined.  The outbound and return flights of the trip are 
presented at the same time and after each set of two flights 
the subject is asked the question.   
Table 1:  Factors used in this experiment. 
  
Example.
  The following example could have been used in 
the Separate 3 condition.  Text that is unformatted is common to 
both the terse and verbose conditions.  Text in italics is found 
only in the verbose condition. 
?I found 5 outbound Delta flights. Option 1 is
 flight number 
323.  It?s a non-stop leaving at 9:10 and arriving at 2:01.   
Option 2
 is flight number 798.  It has one stop; it departs at 
11:13 and arrives at 5:07. Option 3 is flight number 295.  It 
                                                                
1
 All times in this experiment were presented to the minute, with either 
AM or PM, e.g. ??departs at 6:01 AM.? 
Selection Itinerary Content (Between) 
Terse Verbose 
Outbound 
/ Return 
#  of 
Flights 
Before 
Question 
Outbound 
/ Return 
# of Flights 
Before 
Question 
(Within) (Within) 
Separate 1 Separate 1 
Separate 3 Separate 3 
Separate 5 Separate 5 
Combined 2 Combined 2 
has two stops; it departs at 1:52 and arrives at 6:57. Would 
you like to hold any of those flights?? 
2.4.2 Mixed Initiative and Flow of Control 
The dialog strategy was mixed initiative.  The first prompt was 
open-ended, e.g. ?How may I help you with your travel plans??  
All subsequent prompts requested specific information from the 
user (e.g. ?What date did you want to depart??)  The prototypes 
were built to allow the user to provide multiple informational 
elements (e.g. departure city and departure date) to either open-
ended or specific requests.  Subsequent steps in the flow of 
control could be skipped if multiple pieces of information were 
presented at a single dialog point. 
2.4.3 Tasks 
Each subject was asked to complete four tasks in the course of 
this experiment.  In each task the subject was given a set of 
criteria that the subject had to meet in selecting both an outbound 
and a return flight.  The tasks used in this experiment exercise 
selection criteria that are representative of selection criteria 
typically used by individuals actually purchasing airline tickets.  
The four tasks given to subjects follow: 
Departure Only.  The task criteria for both the outbound and 
return flights require the subject to choose flights based on 
departure time only.  
Arrival Only.  The task criteria for both the outbound and return 
flights require the subject to choose flights based on arrival time 
only. 
Departure & Arrival.   The task criteria require the subject to 
choose the outbound flight based on departure time and the return 
flight based on arrival time. 
Specific Flight.  The task requires the subject to book a particular 
flight for both the outbound and return flights. 
Example.  The following example was used for the Departure 
& Arrival task (it has been edited for presentation here). 
You want a round trip ticket from Boston to Charleston.  
You want to leave Boston about 5 in the evening of 
Friday November 10th.  You want to arrive in Boston no 
later than 8 PM on Tuesday November 14th. 
An important selection criterion for many purchasers of 
airline tickets is price.  The price of the ticket was not a selection 
criterion used in this experiment because it would introduce 
possible confounds.  Many users are willing to trade-off other 
important selection criteria, e.g. arrival time and departure time, 
in order to minimize price.  Therefore, it was decided, a priori, to 
postpone the use of price as a selection criterion to a later 
experiment. 
2.4.4 Counterbalancing 
A Balanced Greco-Latin Square was used to counterbalance the 
orders of the conditions and tasks. 
2.4.5 Dependent Measures 
A rich set of dependent measures were gathered in this 
experiment:   
-- After each system prompt was played, NLSA recorded what 
subjects said. 
-- At the end of each task, the wizard determined whether that 
task was successfully completed. 
-- At the end of each task, subjects completed paper and pen 
surveys rating the overall dialog for that task. 
-- After experiencing all four tasks, subjects told the 
experimenter which of the flight selection criteria were 
important to them. 
Objective measure.  Successful task completion was the one 
objective measure used in determining the optimal method for 
presenting complex lists in an audio-only domain.  For each task 
the subject was given a set of required criteria for selecting both 
the outbound and a return flight.  Task completion was binary, 
successful or unsuccessful, and was determined by the 
experimenter (wizard) at the time the subject completed each task.  
In order for a subject to successfully complete a task, the subject 
had to select both the outbound and return flight that best fit the 
clear criteria given to subjects in the task description. 
Subjective measures.  Other data gathered in this experiment 
included a number of subjective measures.  After each task, 
subjects were asked: 
Overall, how satisfied were you with AT&T Communicator while 
booking this flight?  
[1] Very Satisfied  
[2] Somewhat Satisfied  
[3] Neither Satisfied, Nor Dissatisfied  
[4] Somewhat Dissatisfied  
[5] Very Dissatisfied  
Overall, how easy was it to use AT&T Communicator while 
booking this flight?  
[1] Very Easy  
[2] Somewhat Easy  
[3] Neither Easy, Nor Difficult  
[4] Somewhat Difficult  
[5] Very Difficult  
Overall, how quickly did AT&T Communicator respond to your 
speech?   
[1] Much Too Fast  
[2] A Little Too Fast  
[3] Just the Right Speed  
[4] A Little Too Slow  
[5] Much Too Slow  
After you told Communicator the date and time to book your 
flight, Communicator responded with possible flights to choose 
from. For EACH of the possible flights, did Communicator 
present the right amount of information?   
[1] Too Much Information about Each Flight  
[2] Just the Right Amount of Information About Each 
Flight  
[3] Too Little Information about Each Flight  
After completing all four tasks, subjects were asked to (1) 
rank order the criteria they personally use when selecting between 
multiple itineraries, and (2) specify the information that 
Communicator should present about every flight for selection 
purposes in the future. 
3. RESULTS AND CONCLUSIONS 
3.1 Terse or Verbose? 
A two-way, 2x4, Analysis of Variance (ANOVA) was run for 
each of 5 dependent measures:  successful task completion, 
amount of information presented about each flight, satisfaction, 
ease of use, and speed of interaction.   For each dependent 
measure, no significant interactions were found2.  A significant 
main effect for Terse/Verbose was found for the subjective 
measure of the amount of information presented about each flight 
(p=.001), see Fig. 1.  
 
Amount of Information
1.80
1.90
2.00
2.10
2.20
2.30
2.40
1-Sep 3-Sep 5-Sep Comb
# of Flights
verbose (N=30)
terse (N=29)
 
Figure 1:  Verbose and Terse subject ratings to the Amount of 
Information question (2=Just the Right Amount of 
Information about each flight). 
No other significant main effects were found for any of the 
dependent measures.  The optimum value for the dependent 
measure amount of information is ?2? (Just the right amount of 
information about each flight).  The average value for the Verbose 
condition (across the 4 levels of # of Flights) was 2.06, while the 
equivalent average for the Terse condition was 2.24.   
Rank Order of 
Personal Selection Criteria
0 100 200 300 400 500
Price
Stops - Number
Time - Arrival
Time - Departure
Airline
Stops - Length
Flight Number
Weighted ScoreVerbose
Terse
 
Figure 2:  Summed Weighted Scores of subjects? rank 
ordering of their personal selection criteria. 
Related to these results is a question that was asked of all 
subjects at the end of the experiment.  Figure 2 shows the 
weighted scores based on the  rank ordering of the selection 
                                                                
2
 Throughout the experiment, the alpha level used to determine 
significance of an effect was p<.05. 
criteria subjects personally use when selecting among multiple 
flights.   A rank order of 1 was given a score 7 points, a rank 
order of 7 was given a score of 1 point, etc.  The Weighted Score 
for each selection criteria shown in Figure 2 is the sum of the 
Weighted Scores for all  subjects. 
Similarly, a second question was asked of all subjects at 
the end of the experiment:  ?In the future, what information 
should AT&T Communicator present about each flight 
when you are choosing between multiple flights?? Figure 3 
shows the compiled responses to this question.  
 
What Information Should AT&T 
Communicator Present?
0 10 20 30 40 50 60 70
Price
Time - Arrival
Time - Departure
Stops - Number
Airline
Stops - Length
Flight Number
Number of Subjects SelectingVerbose
Terse
 
Figure 3:  Number of subjects indicating that each selection 
criterion should, by default, be presented by AT&T 
Communicator. 
Information that should definitely be presented to subjects 
when selecting between multiple flights includes:  price, arrival 
time, departure time, number of stops and airline.    The value to 
users of the length of stops is ambiguous.  It probably should not 
be presented by default, although it might be useful to present the 
length of stops if they will be inordinately long, e.g. greater than 
2 hours, or inordinately short, e.g. less than 45 minutes.  Flight 
number was judged to be least valuable and should not be 
presented. 
3.2 Number of Flights? 
The above analyses indicate that the amount of information 
presented in the Verbose condition better met the expectations of 
subjects.   The next question then was, within the verbose 
condition, which level of the number of flights before the question 
factor showed the best performance.  A one-way, 1x4, ANOVA 
was run for the verbose condition for each of five dependent 
measures:  successful task completion, amount of information 
about each flight, satisfaction, ease of use, and speed of 
interaction3.  A significant main effect was found for successful 
                                                                
3
 As noted at the beginning of the Results section, subject responses to the 
satisfaction, ease of use, and speed of the interaction questions may be 
attributable to the subject?s reactions to the novel user-system 
task completion (p=.005).  Figure 4 shows the percentage of 
successful task completions in the Verbose condition only.  No 
significant effects were found for the other four dependent 
measures. 
Successful Task Completion 
- Verbose Condition Only
0.00
0.20
0.40
0.60
0.80
1.00
1-Sep 3-Sep 5-Sep Comb
# of Flights
 
Figure 4:  Successful task completion (in percent) across the 
four levels of the # of Flights Before Question condition 
(Verbose only). 
The significant main effect was probed using the Tukey test4.  
Separate 5 was the condition with the highest successful task 
completion rate.  Only one pairwise comparison was significant 
(p<.05).  Tasks attempted in the Separate 5 condition were 
significantly more likely to be completed successfully than tasks 
attempted in the Separate 3 condition.   
Table 2:  Results of Tukey test for successful task completion 
across the four levels of # of Flights Before Question condition 
(Verbose only). 
 Sep. 3 
=.567 
Sep. 1 
=.600 
Comb. 
=.833 
Sep. 5 
=.900 
Sep. 3 
=.567 
 p>=.05 p>=.05 p<.05* 
Sep. 1 
=.600 
  p>=.05 p>=.05 
Comb. 
=.833 
   p>=.05 
Sep. 5 
=.900 
    
 
Among the three Separate conditions (Separate 1, Separate 3, 
and Separate 5), subjects were much more likely to successfully 
complete a task in Separate 5.  That is, when all the flights for a 
given flight (outbound or return) were presented at once, without 
any intervening questions. Also, based on subject comments, it 
appeared that at least some subjects in the Separate 3 condition 
were confused about the number of flights they had available to 
select between.  These subjects didn?t realize that there were more 
flights available after the system presented them with the first 
                                                                                                          
interaction style, rather than to the experimentally varied presentation of 
the flight selection criteria. 
4
 The Tukey is a test of significance for pairwise comparisons of treatment 
means that controls for familywise error. 
three in a total set of five flights.  This is in spite of the fact that in 
all tasks, including the Separate 3 condition, the subjects heard a 
sentence like ?I found five outbound Northwest Airlines flights,? 
before the options were presented for selection.   
It not possible, on the basis of the experimental data gathered 
in this study, to unambiguously choose one of the # of flights 
before question conditions over the others.  It may be that a more 
difficult set of tasks would elicit stronger differences in both the 
objective and subjective measures for the levels of this factor.  
However, in absolute terms, the task completion rates with 
Separate 5 and Combined were both high (90% and 83%, 
respectively), relative to the Separate 1 and Separate 3 conditions 
(60% and 57%, respectively). 
Anecdotal evidence sheds some additional light on the issue 
of which condition (Separate 5 or Combined) is preferred by 
subjects.  In the Verbose condition, the last 17 subjects run in the 
experiment were asked a few questions that provide evidence 
concerning their subjective impressions of the four levels of the 
number of flights before question factor.  The first question was 
?Did you notice any difference between the different versions of 
the system??  Twelve of seventeen subjects stated that they had 
noticed a difference between the four versions.  Those 12 subjects 
were then asked to choose the version they liked the best, and 
then the version they considered to be the worst.   
Best? 
3-Did not specify a ?best? version 
2-Combined only 
2-Separate 1 only 
1-Separate 3 only 
1-Separate 5 only 
2-Any of the Separate versions 
1-Either Separate 3 or Separate 5 
Worst? 
7-Did not specify a ?worst? version 
5-Combined only 
0-Separate 1 only 
0-Separate 3 only 
0-Separate 5 only 
In response to the question of which version of the system 
was best, the subjects stated no consistent preference for any of 
the versions of the system.  On the other hand, the responses to 
the question concerning which version of the system was ?worst? 
resulted in a more consistent set of responses; the Combined 
version was selected by 5 of 12 of the subjects as the version they 
considered to be the ?worst.?  From subject comments, it appeared 
that subjects didn?t like it when they heard one flight that matched 
their constraints (e.g. outbound), while the other flight did not 
match their constraints (e.g. return).  Some subjects found this to 
be frustrating, confusing, and/or tedious. 
4. DISCUSSION 
Presenting all the relevant information about a given flight at once 
seemed to be the single overarching factor that most positively 
influenced successful task completion and the user experience.  
Subjects wanted to hear all of the relevant information about a 
flight needed to make the best choice.    
Within the Separate conditions (Separate 1, Separate 3 and 
Separate 5), the task completion rate was highest for the Separate 
5 condition.  That is, when all of the flights were presented at 
once, without any intervening system questions. The Separate 5 
and Combined conditions had similar task completion rates and 
were not significantly different.  However, the Combined 
condition was the only condition considered ?worst? by subjects. 
Thus, the condition that maximized both successful task 
completion and user experience was the Verbose Separate 5 
condition.   
A major concern in the design of this experiment was that the 
audio presentation of lists of complex information, in this case 
lists of multiple airline flights each containing multiple pieces of 
information, would result in cognitive overload.  These findings 
argue that, for this task, our concern about the increased cognitive 
load in an audio-only domain was unfounded.  There are a couple 
of possible reasons that cognitive load did not appear to have the 
influence that we anticipated.  First, users knew their constraints 
when listening to the flights.  Some users may have used a 
strategy of only remembering the options that potentially matched 
their constraints.  Second, many subjects apparently dealt with the 
increased cognitive load by taking notes, with flight times, etc., 
while completing the experimental tasks.  Such behaviour is 
certainly common when making airline reservations with a human 
travel agent over the telephone.  Further, it is anticipated that 
users would generally book flights in a hands-free environment, 
and would be able to take notes, when it is desirable.  Although 
less ecologically valid, future experiments primarily concerned 
with the issue of cognitive load should explicitly preclude 
subjects from taking notes. 
From the above data, it appears that the standard user 
interface practices for Prompt & Collect (in which the system 
?prompts? the user for a specific piece of information and then 
?collects? their response) should be different than the standard 
practice for Selecting from a List.  For Prompt & Collect, the 
standard UI practice is for the system to be as terse as possible 
when prompting the user.  The user generally knows the 
information the system is asking for, they just need to be 
prompted so that they know what piece of information the system 
is currently ready to accept.  When Selecting from a List, 
however, the level of knowledge of the system and user are very 
different.  The system knows the possible items that can be 
selected.  The user begins the interaction only knowing their own 
constraints (and their relative priority).  The system must present 
all the relevant information so that the user can select the optimal 
item from the list based on their constraints and priorities.  
Consequently, when the user must select an item from a list, the 
system should be as verbose as necessary and present all the 
information relevant to their decision at once. 
For the task of selecting between multiple airline flights, the 
following information should definitely be presented to users 
about each flight: price, arrival time, departure time, number of 
stops and airline.  While there was agreement between subjects on 
the important selection criteria for flights, there are also some 
individual differences.  For example, some users like to fly only a 
particular airline or have strong preferences concerning the 
locations of layovers.  This experiment indicates that users like to 
hear all of the information relevant to their decision at once.  
Therefore, if a user asks a question concerning information that is 
not by default presented by the system about each flight, the 
information presented to that user for each subsequent flight on 
that call, should include the information they have requested.  For 
example, if after the first flight the user asks ?Where does that 
flight stop??, the information for all subsequent flights in that call 
should include the location of any stops. 
It thus appears likely that subjects in this experiment were 
using AT&T Communicator as a tool, rather than as a 
conversational or negotiating partner.  That is, their goal was to 
use Communicator to quickly and efficiently select the single 
flight that best matched the criteria given to them in each task.  
Asking the system questions in order to get information relevant 
to this selection process, would decrease the speed and efficiency 
with which they were able to accomplish this task.  So, subjects 
preferred the most verbose presentation of information in order to 
increase the speed and efficiency of the overall task. 
Nevertheless, eighty percent of the subjects stated that the 
most important criterion when personally selecting a flight was 
price.   A number of subjects commented that they were willing to 
trade off other important criterion, e.g. airline, number of stops, in 
order to get a better price.  In a more complex selection task, 
where the user is choosing a set of flights based on multiple, 
competing selection criteria, the user may wish to use AT&T 
Communicator as a negotiating partner in order to get the best 
flight available.  Such a negotiation might lend itself well to 
exploring machine-user dialog in a natural language telephony-
based system.   
5. ACKNOWLEDGMENTS 
We would like to thank the AT&T Communicator team for their 
assistance on this project. 
6. REFERENCES 
[1] Blanchard, H.E. & Lewis, S.H. (1999), The Voice 
messaging user interface, in D. Gardner-Bonneau (ed.), 
Human factors and voice interactive systems, Kluwer 
Academic Publishers, pp.257-284. 
[2] Levin, E., Narayanan, S., Pieraccini, R., Biatov, K., 
Bocchieri, E., Di Fabbrizio, G., Eckert, W., Lee, S., 
Pokrovsky, A., Rahim, M., Ruscitti, P., and Walker, M. 
(2000), The AT&T-DARPA Communicator mixed-
initiative spoken dialog system, Proc. of the 
International Conference of Spoken Langurage 
Processing, (Beijing, China), pp. 122-125. 
[3] Schneiderman, B. (1992).  Designing the user interface 
(2nd ed.).  Reading MA:  Addison Wesley. 
 
 
 
DATE: A Dialogue Act Tagging Scheme for Evaluation of
Spoken Dialogue Systems
Marilyn Walker and Rebecca Passonneau
AT&T Shannon Labs
180 Park Ave.
Florham Park, N.J. 07932 fwalker,beckyg@research.att.com
ABSTRACT
This paper describes a dialogue act tagging scheme developed for
the purpose of providing finer-grained quantitative dialogue met-
rics for comparing and evaluating DARPA COMMUNICATOR spo-
ken dialogue systems. We show that these dialogue act metrics can
be used to quantify the amount of effort spent in a dialogue main-
taining the channel of communication or, establishing the frame
for communication, as opposed to actually carrying out the travel
planning task that the system is designed to support. We show that
the use of these metrics results in a 7% improvement in the fit in
models of user satisfaction. We suggest that dialogue act metrics
can ultimately support more focused qualitative analysis of the role
of various dialogue strategy parameters, e.g. initiative, across di-
alogue systems, thus clarifying what development paths might be
feasible for enhancing user satisfaction in future versions of these
systems.
1. INTRODUCTION
Recent research on dialogue is based on the assumption that di-
alogue acts provide a useful way of characterizing dialogue behav-
iors in human-human dialogue, and potentially in human-computer
dialogue as well [16, 27, 11, 7, 1]. Several research efforts have
explored the use of dialogue act tagging schemes for tasks such
as improving recognition performance [27], identifying important
parts of a dialogue [12], and as a constraint on nominal expres-
sion generation [17]. This paper reports on the development and
use of a dialogue act tagging scheme for a rather different task:
the evaluation and comparison of spoken dialogue systems in the
travel domain. We call this scheme DATE: Dialogue Act Tagging
for Evaluation.
Our research on the use of dialogue act tagging for evaluation
focuses on the corpus of DARPA COMMUNICATOR dialogues col-
lected in the June 2000 data collection [28]. This corpus consists of
662 dialogues from 72 users calling the nine different COMMUNI-
CATOR travel planning systems. Each system implemented a log-
file standard for logging system behaviors and calculating a set of
core metrics. Each system utterance and each recognizer result was
logged, and user utterances were transcribed and incorporated into
.
the logfiles. The logfile standard supported the calculation of met-
rics that were hypothesized to potentially affect the user?s percep-
tion of the system; these included task duration, per turn measures,
response latency measures and ASR performance measures. Each
dialogue was also hand labelled for task completion.
The hypothesis underlying our approach is that a system?s di-
alogue behaviors have a strong effect on the user?s perception of
the system. Yet the core metrics that were collected via the logfile
standard represent very little about dialogue behaviors. For exam-
ple, the logging counts system turns and tallies their average length,
but doesn?t distinguish turns that reprompt the user, or give in-
structions, from those that present flight information. Furthermore,
each COMMUNICATOR system had a unique dialogue strategy and a
unique way of achieving particular communicative goals. Thus, in
order to explore our hypothesis about the differential effect of these
strategies, we needed a way to characterize system dialogue behav-
iors that would capture such differences yet be applied uniformly to
all nine systems. While some sites logged system dialogue behav-
iors using site-specific dialogue act naming schemes, there existed
no scheme that could be applied across sites.
Our goal was thus to develop a dialogue act tagging scheme that
would capture important distinctions in this set of dialogues; these
distinctions must be useful for testing particular hypotheses about
differences among dialogue systems. We also believed that it was
important for our tagging scheme to allow for multiple views of
each dialogue act. This would allow us, for example, to investigate
what part of the task an utterance contributes to separately from
what speech act function it serves. A central claim of the paper is
that these goals require a tagging scheme that makes distinctions
within three orthogonal dimensions of utterance classification: (1)
a SPEECH-ACT dimension; (2) a TASK-SUBTASK dimension; and
(3) a CONVERSATIONAL-DOMAIN dimension. Figure 1 shows a
COMMUNICATOR dialogue with each system utterance classified
on these three dimensions. The labels on each utterance are fully
described in the remainder of the paper.
Sections 2, 3, and 4, describe the three dimensions of DATE. In
these sections, we describe two aspects of our annotation scheme
that are not captured in existing tagging schemes, which we be-
lieve are important for characterizing how much effort in a dialogue
is devoted to the task versus different kinds of dialogue mainte-
nance. Section 5 describes how the dialogue act labels are assigned
to system utterances and section 6 discusses results showing that
the DATE dialogue act metrics improve models of user satisfaction
by an absolute 7% (an increase from 38% to 45%). The dialogueue
act metrics that are important predictors of user satisfaction are var-
ious kinds of meta-dialogue, apologies and acts that may be land-
marks for achieving particular dialogueue subtasks. In section 7 we
summarize the paper, discuss our claim that a dialogue annotation
scheme is a partial model of a natural class of dialogues, and dis-
cuss the ways in which the DATE scheme may be generalizable to
other dialogue corpora.
2. CONVERSATIONAL DOMAINS
The CONVERSATIONAL-DOMAIN dimension characterizes each
utterance as primarily belonging to one of three arenas of conver-
sational action. The first arena is the domain task, which in this
case is air travel booking, and which we refer to below as ABOUT-
TASK. The second domain of conversational action is the manage-
ment of the communication channel, which we refer to as ABOUT-
COMMUNICATION. This distinction has been widely adopted [19,
2, 9]. In addition, we identify a third domain of talk that we refer
to as ABOUT-SITUATION-FRAME. This domain is particularly rel-
evant for distinguishing human-computer from human-human dia-
logues, and for distinguishing dialogue strategies across the 9 COM-
MUNICATOR systems. Each domain is described in this section.
2.1 About-Task
The ABOUT-TASK domain reflects the fact that many utterances
in a task-oriented dialogue originate because the goal of the dia-
logue is to complete a particular task to the satisfaction of both
participants. Typically an about-task utterance directly asks for or
presents task-related information, or offers a solution to a task goal.
As Figure 1 shows, most utterances are in the ABOUT-TASK di-
mension, reflecting the fact that the primary goal of the dialogue
is to collaborate on the task of making travel arrangements. The
task column of Figure 1 specifies the subtask that each task-related
utterance contributes to. DATE includes a large inventory of sub-
tasks in the task/subtask dimension in order to make fine-grained
distinctions regarding the dialogue effort devoted to the task or its
subcomponents. Section 4 will describe the task model in more
detail.
2.2 About-Communication
The ABOUT-COMMUNICATION domain reflects the system goal
of managing the verbal channel and providing evidence of what
has been understood [29, 8, 25]. Although utterances of this type
occur in human-human dialogue, they are more frequent in human-
computer dialogue, where they are motivated by the need to avoid
potentially costly errors arising from imperfect speech recognition.
In the COMMUNICATOR corpus, many systems use a conserva-
tive strategy of providing feedback indicating the system?s under-
standing of the information provided by the user after each user
turn. A typical example is the repetition of the origin and des-
tination cities in Figures 1 and 6. This type of repetition is the
IMPLICIT-CONFIRMATION speech-act (see Section 3 below). Some
systems used a variable confirmation strategy where some informa-
tion items may be confirmed as they are understood, but the system
requests explicit confirmation of all task parameters before search-
ing the database for matching flights. An example is in Figure 2.
Here the system asks for explicit confirmation in SYS3 before go-
ing to the database. This is the first opportunity that the user has
for making a correction, which he does in USER3. The system
then again asks for explicit confirmation of its new understanding,
which the user provides in USER4. After the user informs the sys-
tem that it is a one-way flight in USER6, the system accesses the
database. These explicit confirmations have the goal of avoiding
a costly database lookup, where the retrieval is conditioned on the
wrong parameters.
All implicit and explicit confirmation speech-acts are catego-
rized as ABOUT-COMMUNICATION because they are motivated by
the potential errors that the system might make in understanding
Speaker Utterance
SYS1 ....What are your travel plans? ?
USER1 I WISH TO FLY FROM LEXICON KENTUCKY TO
SPRINGFIELD MISSOURI THE MORNING OF OC-
TOBER FIFTH
SYS2 do you have a preferred airline?
USER2 NO
SYS3 I have you going from Kentucky to Gainesville Missouri
on October fifth . Is that correct?
USER3 IT?S LEXINGTON KENTUCKY TO SPRINGFIELD
MISSOURI
SYS4 I have you going from Lexington to Springfield Missouri
on October fifth, Is that correct?
USER4 YES
SYS5 Will you return to Lexington from Springfield?
USER5 NO
SYS6 Is Springfield your final destination?
USER6 YES
SYS7 Hold on while I check availability..... Three options were
returned. Option one has a fare of four hundred and thirty
three dollars.....
Figure 2: Dialogue Illustrating Variable Confirmation Strategy
the caller, or in diagnosing the causes of misunderstandings. In
general, any utterance that reflects the system?s understanding of
something the user said is classified as ABOUT-COMMUNICATION.
A second set of ABOUT-COMMUNICATION utterances are APOLO-
GIES that the system makes for misunderstandings (see Section 3
below), i.e. utterances such as I?m sorry. I?m having trouble under-
standing you., or My mistake again. I didn?t catch that. or I can see
you are having some problems.
The last category of ABOUT-COMMUNICATION utterances are
the OPENINGS/CLOSINGS by which the system greets or says good-
bye to the caller. (Again, see Section 3 below.)
2.3 About Situation-Frame
The SITUATION-FRAME domain pertains to the goal of manag-
ing the culturally relevant framing expectations. The term is in-
spired by Goffman?s work on the organization and maintenance of
social interaction [13, 14]. An obvious example of a framing as-
sumption is that the language of the interaction will be English [13,
14]. Another is that there is an asymmetry between the knowledge
and/or agency of the system (or human travel agent) and that of the
user (or caller): the user cannot issue an airline ticket.
In developing the DATE tagging scheme, we compared human-
human travel planning dialogues collected by CMU with the human-
machine dialogues of the June 2000 data collection and noticed
a striking difference in the ABOUT-FRAME dimension. Namely,
very few ABOUT-FRAME utterances occur in the human-human di-
alogues, whereas they occur frequently enough in human-computer
dialogues that to ignore them is to risk obscuring significant differ-
ences in habitability of different systems. In other words, certain
differences in dialogue strategies across sites could not be fully rep-
resented without such a distinction. Figure 3 provides examples
motivating this dimension.
Dialogue acts that are ABOUT-FRAME are cross-classified as one
of three types of speech-acts, PRESENT-INFO, INSTRUCTION or
APOLOGY. They are not classified as having a value on the TASK-
SUBTASK dimension. Most of the ABOUT-FRAME dialogue acts
fall into the speech-act category of INSTRUCTIONS, utterances di-
rected at shaping the user?s behavior and expectations about how to
interact with a machine. Sites differ regarding how much instruc-
tion is provided up-front versus within the dialogue; most sites have
different utterance strategies for dialogue-initial versus dialogue-
Speech-Act Example
PRESENT-
INFO
You are logged in as a guest user of A T and T Commu-
nicator.
PRESENT-
INFO
I?ll enroll you temporarily as a guest user.
PRESENT-
INFO
I know about the top 150 cities worldwide.
PRESENT-
INFO
This call is being recorded for development purposes,
and may be shared with other researchers.
PRESENT-
INFO
I cannot handle rental cars or hotels yet. Please restrict
your requests to air travel.
PRESENT-
INFO
I heard you ask about fares. I can only price an
itinerary. I cannot provide information on published
fares for individual flights.
INSTRUCTION First, always wait to hear the beep before you say any-
thing
INSTRUCTION You can always start over again completely just by say-
ing: start over.
INSTRUCTION Before we begin, let?s go over a few simple instructions.
INSTRUCTION Please remember to speak after the tone. If you get con-
fused at any point you can say start over to cancel your
current itinerary.
APOLOGY Sorry, an error has occurred. We?ll have to start over.
APOLOGY I am sorry I got confused. Thanks for your patience. Let
us try again.
APOLOGY Something is wrong with the flight retrieval.
APOLOGY I have trouble with my script.
Figure 3: Example About-Frame Utterances
medial instructions. One site gives minimal up-front framing in-
formation; further, the same utterances that can occur up-front also
occur dialogue-medially. A second site gives no up-front fram-
ing information, but it does provide framing information dialogue-
medially. Yet a third site gives framing information dialogue-initially,
but not dialogue-medially. The remaining sites provide different
kinds of general instructions dialogue-initially, e.g. (Welcome. ...You
may say repeat, help me out, start over, or, that?s wrong, you can
also correct and interrupt the system at any time.) versus dialogue-
medially: (Try changing your departure dates or times or a nearby
city with a larger airport.) This category also includes statements
to the user about the system?s capabilities. These occur in response
to a specific question or task that the system cannot handle: I can-
not handle rental cars or hotels yet. Please restrict your requests
to air travel. See Figure 3.
Another type of ABOUT-FRAME utterance is the system?s at-
tempt to disambiguate the user?s utterance; in response to the user
specifying Springfield as a flight destination, the system indicates
that this city name is ambiguous (I know of three Springfields, in
Missouri, Illinois and Ohio. Which one do you want?). The sys-
tem?s utterance communicates to the user that Springfield is am-
biguous, and goes further than a human would to clarify that there
are only three known options. It is important for evaluation pur-
poses to distinguish the question and the user?s response from a
simple question-answer sequence establishing a destination. A di-
rect question, such as What city are you flying to?, functions as a
REQUEST-INFO speech act and solicits information about the task.
The context here contrasts with a direct question in that the system
has already asked for and understood a response from the caller
about the destination city. Here, the function of the system turn is
to remediate the caller?s assumptions about the frame by indicating
the system?s confusion about the destination. Note that the question
within this pattern could easily be reformulated as a more typical
instruction statement, such as Please specify which Springfield you
mean, or Please say Missouri, Illinois or Ohio..
3. THE SPEECH-ACT DIMENSION
The SPEECH-ACT dimension characterizes the utterance?s com-
municative goal, and is motivated by the need to distinguish the
communicative goal of an utterance from its form. As an exam-
ple, consider the functional category of a REQUEST for information,
found in many tagging schemes that annotate speech-acts [24, 18,
6]. Keeping the functional category of a REQUEST separate from
the sentence modality distinction between question and statement
makes it possible to capture the functional similarity between ques-
tion and statement forms of requests, e.g., Can you tell me what
time you would like to arrive? versus Please tell me what time you
would like to arrive.
In DATE, the speech-act dimension has ten categories. We use
familiar speech-act labels, such as OFFER, REQUEST-INFO, PRESENT-
INFO, ACKNOWLEDGMENT, and introduce new ones designed to
help us capture generalizations about communicative behavior in
this domain, on this task, given the range of system and human
behavior we see in the data. One new one, for example, is STATUS-
REPORT, whose speech-act function and operational definition are
discussed below. Examples of each speech-act type are in Figure 4.
Speech-Act Example
REQUEST-INFO And, what city are you flying to?
PRESENT-INFO The airfare for this trip is 390 dollars.
OFFER Would you like me to hold this option?
ACKNOWLEDGMENT I will book this leg.
STATUS-REPORT Accessing the database; this might take a few sec-
onds.
EXPLICIT-
CONFIRM
You will depart on September 1st. Is that correct?
IMPLICIT-CONFIRM Leaving from Dallas.
INSTRUCTION Try saying a short sentence.
APOLOGY Sorry, I didn?t understand that.
OPENINGS/CLOSINGS Hello. Welcome to the C M U Communicator.
Figure 4: Example Speech Acts
In this domain, the REQUEST-INFO speech-acts are designed to
solicit information about the trip the caller wants to book, such as
the destination city (And what city are you flying to?), the desired
dates and times of travel (What date would you like to travel on), or
information about ground arrangements, such as hotel or car rental
(Will you need a hotel in Chicago?).
The PRESENT-INFO speech-acts also often pertain directly to the
domain task of making travel arrangements: the system presents
the user with a choice of itinerary (There are several flights from
Dallas Fort Worth to Salisbury Maryland which depart between
eight in the morning and noon on October fifth. You can fly on
American departing at eight in the morning or ten thirty two in the
morning, or on US Air departing at ten thirty five in the morning.),
as well as a ticket price (Ticket price is 495 dollars), or hotel or car
options.
OFFERS involve requests by the caller for a system action, such
as to pick a flight (I need you to tell me whether you would like to
take this particular flight) or to confirm a booking (If this itinerary
meets your needs, please press one; otherwise, press zero.) They
typically occur after the prerequisite travel information has been
obtained, and choices have been retrieved from the database.
The ACKNOWLEDGMENT speech act characterizes system utter-
ances that follow a caller?s acceptance of an OFFER, e.g. I will book
this leg or I am making the reservation.
The STATUS-REPORT speech-act is used to inform the user about
the status of the part of the domain task pertaining to the database
retrieval, and can include apologies, mollification, requests to be
patient, and so on. Their function is to let the user know what is
happening with the database lookup, whether there are problems
with it, and what types of problems. While the form of these acts
are typically statements, their communication function is different
than typical presentations of information; they typically function to
keep the user apprised of progress on aspects of the task that the
user has no direct information about, e.g. Accessing the database;
this might take a few seconds. There is also a politeness function
to utterances like Sorry this is taking so long, please hold., and
they often provide the user with error diagnostics: The date you
specified is too far in advance.; or Please be aware that the return
date must be later than the departure date.; or No records satisfy
your request.; or There don?t seem to be any flights from Boston.
The speech-act inventory also includes two types of speech acts
whose function is to confirm information that has already been pro-
vided by the caller. In order to identify and confirm the parameters
of the trip, systems may ask the caller direct questions, as in SYS3
and SYS4 in Figure 2. These EXPLICIT-CONFIRM speech acts are
sometimes triggered by the system?s belief that a misunderstand-
ing may have occurred. A typical example is Are you traveling
to Dallas?. An alternative form of the same EXPLICIT-CONFIRM
speech-act type asserts the information the system has understood
and asks for confirmation in an immediately following question: I
have you arriving in Dallas. Is that correct? In both cases, the
caller is intended to provide a response.
A less intrusive form of confirmation, which we tag as IMPLICIT-
CONFIRM, typically presents the user with the system?s understand-
ing of one travel parameter immediately before asking about the
next parameter. Depending on the site, implicit information can ei-
ther precede the new request for information, as in Flying to Tokyo.
What day are you leaving?, or can occur within the same utter-
ance, as in What day do you want to leave London? More rarely,
an implicit confirmation is followed by PRESENT-INFO: a flight
on Monday September 25. Delta has a flight departing Atlanta at
nine thirty. One question about the use of implicit confirmation
strategy is whether the caller realizes they can correct the system
when necessary [10]. Although IMPLICIT-CONFIRMS typically oc-
cur as part of a successful sequence of extracting trip information
from the caller, they can also occur in situations where the system
is having trouble understanding the caller. In this case, the system
may attempt to instruct the user on what it is doing to remediate
the problem in between an IMPLICIT-CONFIRM and a REQUEST-
INFO: So far, I have you going from Tokyo. I am trying to assemble
enough information to pick a flight. Right now I need you to tell me
your destination.
We have observed that INSTRUCTIONS are a speech-act type
that distinguishes these human-computer travel planning dialogues
from corresponding human-human travel planning dialogues. In-
structions sometimes take the form of a statement or an imperative,
and are characterized by their functional goal of clarifying the sys-
tem?s own actions, correcting the user?s expectations, or changing
the user?s future manner of interacting with the system. Dialogue
systems are less able to diagnose a communication problem than
human travel agents, and callers are less familiar with the capa-
bilities of such systems. As noted above, some systems resort to
explicit instructions about what the system is doing or is able to do,
or about what the user should try in order to assist the system: Try
asking for flights between two major cities; or You can cancel the
San Antonio, Texas, to Tampa, Florida flight request or change it.
To change it, you can simply give new information such as a new
departure time. Note that INSTRUCTIONS, unlike the preceding di-
alogue act types, do not directly involve a domain task.
Like the INSTRUCTION speech-acts, APOLOGIES do not address
a domain task. They typically occur when the system encoun-
ters problems, for example, in understanding the caller (I?m sorry,
I?m having trouble understanding you), in accessing the database
(Something is wrong with the flight retrieval), or with the connec-
tion (Sorry, we seem to have a bad connection. Can you please call
me back later?).
The OPENING/CLOSING speech act category characterizes utter-
ances that open and close the dialogue, such as greetings or good-
byes [26]. Most of the dialogue systems open the interactions with
some sort of greeting?Hello, welcome to our Communicator flight
travel system, and end with a sign-off or salutation?Thank you
very much for calling. This session is now over. We distinguish
these utterances from other dialogue acts, but we do not tag open-
ings separate from closings because they have a similar function,
and can be distinguished by their position in the discourse. We also
include in this category utterances in which the systems survey the
caller as to whether s/he got the information s/he needed or was
happy with the system.
4. THE TASK-SUBTASK DIMENSION
The TASK-SUBTASK dimension refers to a task model of the do-
main task that the system is designed to support and captures dis-
tinctions among dialogue acts that reflect the task structure.1 Our
domain is air travel reservations, thus the main communicative task
is to specify information pertaining to an air travel reservation, such
as the destination city. Once a flight has been booked, ancillary
tasks such as arranging for lodging or a rental car become relevant.
The fundamental motivation for the TASK-SUBTASK dimension in
the DATE scheme is to derive metrics related to subtasks in order to
quantify how much effort a system expends on particular subtasks.2
This dimension distinguishes among 13 subtasks, some of which
can also be grouped at a level below the top level task. The subtasks
and examples are in Figure 5. The TOP-LEVEL-TRIP task describes
the task which contains as its subtasks the ORIGIN, DESTINATION,
DATE, TIME, AIRLINE, TRIP-TYPE, RETRIEVAL and ITINERARY
tasks. The GROUND task includes both the HOTEL and CAR sub-
tasks.
Typically each COMMUNICATOR dialogue system acts as though
it utilizes a task model, in that it has a particular sequence in which
it will ask for task information if the user doesn?t take the initia-
tive to volunteer this information. For example, most systems ask
first for the origin and destination cities, then for the date and time.
Some systems ask about airline preference and others leave it to the
caller to volunteer this information. A typical sequence of tasks for
the flight planning portion of the dialogue is illustrated in Figure 6.
As Figure 6 illustrates, any subtask can involve multiple speech
acts. For example, the DATE subtask can consist of acts requesting,
or implicitly or explicitly confirming the date. A similar exam-
ple is provided by the subtasks of CAR (rental) and HOTEL, which
include dialogue acts requesting, confirming or acknowledging ar-
rangements to rent a car or book a hotel room on the same trip.
1This dimension is used as an elaboration of each speech-act type
in other tagging schemes [24].
2It is tempting to also consider this dimension as a means of in-
ferring discourse structure on the basis of utterance level labels,
since it is widely believed that models of task structure drive the
behavior of dialogue systems [23, 3, 22], and the relationship be-
tween discourse structure and task structure has been a core topic
of research since Grosz?s thesis [15]. However, we leave the infer-
ence of discourse structure as a topic for future work because the
multifunctionality of many utterances suggests that the correspon-
dence between task structure and dialogue structure may not be as
straightforward as has been proposed in Grosz?s work [30].
Task Example
TOP-LEVEL-
TRIP
What are your travel plans?
ORIGIN And, what city are you leaving from?
DESTINATION And, where are you flying to?
DATE What day would you like to leave?
TIME Departing at what time?.
AIRLINE Did you have an airline preference?
TRIP-TYPE Will you return to Boston from San Jose?
RETRIEVAL Accessing the database; this might take a few seconds.
ITINERARY The airfare for this trip is 390 dollars.
GROUND Did you need to make any ground arrangements?.
HOTEL Would you like a hotel near downtown or near the air-
port?.
CAR Do you need a car in San Jose?
Figure 5: Example Utterances for each Subtask
Speaker Utterance Task
SYS Welcome .... What trip would you like to take? TOP-
LEVEL-
TRIP
USER STARTING FROM ATLANTA GEORGIA TO
LONDON ENGLAND
SYS flying from atlanta to london. ORIGIN,
DESTINA-
TION
SYS What day are you leaving Atlanta? DATE
USER MONDAY SEPTEMBER TWENTY FIVE
SYS on monday, september twenty fifth. DATE
SYS About what time do you want to leave? TIME
USER [UH] DAYTIME
SYS Leaving in the daytime. TIME
SYS Please stand by while I look up the flight sched-
ules on the web.
RETRIEVAL
SYS OK, got them. I have 13 flights. The first flight
is on American at six fifty nine eh M, arriving at
ten forty five PM, with a connection in Chicago.
. Is that OK?
ITINERARY
USER [UH] YES OKAY
SYS Do you want a return flight from London to At-
lanta?
TRIP-TYPE
USER [UH] YES
SYS Returning from london to atlanta. TRIP-TYPE
SYS What day are you leaving London? DATE
USER [UH] LEAVING [UM] MONDAY OCTOBER
THE SECOND
SYS on monday, october second. DATE
SYS About what time do you want to leave? TIME
USER ANY TIME?S OKAY
SYS Please stand by while I look up the flight sched-
ules on the web... OK, got them.
RETRIEVAL,
ITINERARY
Figure 6: Dialogue Illustrating a Typical Task Sequence
There are also differences in how each site?s dialogue strategy
reflects it conceptualization of the travel planning task. For exam-
ple, some systems ask the user explicitly for their airline prefer-
ences whereas others do not (the systems illustrated in Figures 1
and 6 do not, wherase the one in Figure 2 does). Another differ-
ence is whether the system asks the user explicitly whether s/he
wants a round-trip ticket. Some systems ask this information early
on, and search for both the outbound and the return flights at the
same time. Other systems do not separately model round-trip and
multi-leg trips. Instead they ask the user for information leg by leg,
and after requesting the user to select an itinerary for one leg of
the flight, they ask whether the user has an additional destination.
A final difference was that, in the June 2000 data collection, some
systems such as the one illustrated in Figure 1 included the ground
arrangements subtasks, and others did not.
5. IMPLEMENTATION
Our focus in this work is in labelling the system side of the di-
alogue; our goal was to develop a fully automatic 100% correct
dialogue parser for the limited range of utterances produced by the
9 COMMUNICATOR systems. While we believe that it would be
useful to be able to assign dialogue acts to both sides of the con-
versation, we expect that to require hand-labelling [1]. We also
believe that in many cases the system behaviors are highly corre-
lated with the user behaviors of interest; for example when a user
has to repeat himself because of a misunderstanding, the system
has probably prompted the user multiple times for the same item of
information and has probably apologized for doing so. Thus this
aspect of the dialogue would also be likely to be captured by the
APOLOGY dialogue act and by counts of effort expended on the
particular subtask.
We implemented a pattern matcher that labels the system side
of each dialogue. An utterance or utterance sequence is identifed
automatically from a database of patterns that correspond to the di-
alogue act classification we arrived at in cooperation with the site
developers. Where it simplifies the structure of the dialogue parser,
we assign two adjacent utterances that are directed at the same goal
the same DATE label, thus ignoring the utterance level segmenta-
tion, but we count the number of characters used in each act. Since
some utterances are generated via recursive or iterative routines,
some patterns involve wildcards.
The current implementation labels the utterances with tags that
are independent of any particular markup-language or representa-
tion format. We have written a transducer that takes the labelled
dialogues and produces HTML output for the purpose of visual-
izing the distribution of dialogue acts and meta-categories in the
dialogues. An additional summarizer program is used to produce a
summary of the percentages and counts of each dialogue act as well
as counts of meta-level groupings of the acts related to the different
dimensions of the tagging scheme. We intend to use our current
representation to generate ATLAS compliant representations [4].
6. RESULTS
Our primary goal was to achieve a better understanding of the
qualitative aspects of each system?s dialogue behavior. We can
quantify the extent to which the dialogue act metrics have the po-
tential to improve our understanding by applying the PARADISE
framework to develop a model of user satisfaction and then exam-
ining the extent to which the dialogue act metrics improve these
models [31]. In other work, we show that given the standard met-
rics collected for the COMMUNICATOR dialogue systems, the best
model accounts for 38% of the variance in user satisfaction [28].
When we retrain these models with the dialogue act metrics ex-
tracted by our dialogue parser, we find that many metrics are signif-
icant predictors of user satisfaction, and that the model fit increases
from 38% to 45%. When we examine which dialogue metrics are
significant, we find that they include several types of meta-dialogue
such as explicit and implicit confirmations of what the user said,
and acknowledgments that the system is going to go ahead and do
the action that the user has requested. Significant negative predic-
tors include apologies. On interpretation of many of the significant
predictors is that they are landmarks in the dialogue for achieve-
ment of particular subtasks. However the predictors based on the
core metrics included a ternary task completion metric that captures
succinctly whether any task was achieved or not, and whether the
exact task that the user was attempting to accomplish was achieved.
A plausible explanation for the increase in the model fits is that user
satisfaction is sensitive to exactly how far through the task the user
got, even when the user did not in fact complete the task. The
role of the other significant dialogue metrics are plausibly inter-
preted as acts important for error minimization. As with the task-
related dialogue metrics, there were already metrics related to ASR
performance in the core set of metrics. However, several of the
important metrics count explicit confirmations, one of the desired
date of travel, and the other of all information before searching the
database, as in utterances SYS3 and SYS4 in Figure 2.
7. DISCUSSION
This paper has presented DATE, a dialogue act tagging scheme
developed explicitly for the purpose of comparing and evaluating
spoken dialogue systems. We have argued that such a scheme needs
to make three important distinctions in system dialogue behaviors
and we are investigating the degree to which any given type of dia-
logue act belongs in a single category or in multiple categories.
We also propose the view that a tagging scheme be viewed as a
partial model of a natural class of dialogues. It is a model to the de-
gree that it represents claims about what features of the dialogue are
important and are sufficiently well understood to be operationally
defined. It is partial in that the distributions of the features and
their relationship to one another, i.e., their possible manifestations
in dialogues within the class, are an empirical question.
The view that a dialogue tagging scheme is a partial model of a
class of dialogues implies that a pre-existing tagging scheme can be
re-used on a different research project, or by different researchers,
only to the degree that it models the same natural class with respect
to similar research questions, is sufficient for expressing observa-
tions about what actually occurs within the current dialogues of
interest, and is sufficiently well-defined that high reliability within
and across research sites can be achieved. Thus, our need to modify
existing schemes was motivated precisely to the degree that exist-
ing schemes fall short of these requirements. Other researchers who
began with the goal of re-utilizing existing tagging schemes have
also found it necessary to modify these schemes for their research
purposes [11, 18, 7].
The most substantial difference between our dialogue act tag-
ging scheme and others that have been proposed is in our expan-
sion of the two-way distinction between dialogue tout simple vs.
meta-dialogue, into a three-way distinction among the immediate
dialogue goals, meta-dialogue utterances, and meta-situation utter-
ances. Depending on further investigation, we might decide these
three dimensions have equal status within the overall tagging scheme
(or within the overall dialogue-modeling enterprise), or that there
are two types of meta-dialogue: utterances devoted to maintaining
the channel, versus utterances devoted to establishing/maintaining
the frame. Further, in accord with our view that a tagging scheme
is a partial model, and that it is therefore necessarily evolving as
our understanding of dialogue evolves, we also believe that our for-
mulation of any one dimension, such as the speech-act dimension,
will necessarily differ from other schemes that model a speech-act
dimension.
Furthermore, because human-computer dialogue is at an early
stage of development, any such tagging scheme must be a moving
target, i.e., the more progress is made, the more likely it is we may
need to modify along the way the exact features used in an annota-
tion scheme to characterize what is going on. In particular, as sys-
tem capabilities become more advanced in the travel domain, it will
probably be necessary to elaborate the task model to capture differ-
ent aspects of the system?s problem solving activities. For example,
our task model does not currently distinguish between different as-
pects of information about an itinerary, e.g. between presentation
of price information and presentation of schedule information.
We also expect that some domain-independent modifications are
likely to be necessary as dialogue systems become more success-
ful, for example to address the dimension of ?face?, i.e. the posi-
tive politeness that a system shows to the user [5]. As an example,
consider the difference between the interpretation of the utterance,
There are no flights from Boston to Boston, when produced by a
system vs. when produced by a human travel agent. If a human
said this, it would be be interpretable by the recipient as an in-
sult to their intelligence. However when produced by a system, it
functions to identify the source of the misunderstanding. Another
distinction that we don?t currently make which might be useful is
between the initial presentation of an item of information and its
re-presentation in a summary. Summaries arguably have a differ-
ent communicative function [29, 7]. Another aspect of function
our representation doesn?t capture is rhetorical relations between
speech acts [20, 21].
While we developed DATE to answer particular research ques-
tions in the COMMUNICATOR dialogues, there are likely to be as-
pects of DATE that can be applied elsewhere. The task dimension
tagset reflects our model of the domain task. The utility of a task
model may be general across domains and for this particular do-
main, the categories we employ are presumably typical of travel
tasks and so, may be relatively portable.
The speech act dimension includes categories typically found in
other classifications of speech acts, such as REQUEST-INFO, OF-
FER, and PRESENT-INFO. We distinguish information presented to
the user about the task, PRESENT-INFO, from information provided
to change the user?s behavior, INSTRUCTION, and from information
presented in explanation or apology for an apparent interruption in
the dialogue, STATUS-REPORT. The latter has some of the flavor
of APOLOGIES, which have an inter-personal function, along with
OPENINGS/CLOSINGS. We group GREETINGS and SIGN-OFFS into
the single category of OPENINGS/CLOSINGS on the assumption that
politeness forms make less contribution to perceived system suc-
cess than the system?s ability to carry out the task, to correct mis-
understandings, and to coach the user.
Our third dimension, conversational-domain, adds a new cate-
gory, ABOUT-SITUATION-FRAME, to the more familiar distinction
between utterances directed at a task goal vs. utterances directed
at a maintaining the communication. This distinction supports the
separate classification of utterances directed at managing the user?s
assumptions about how to interact with the system on the air travel
task. As we mention above, the ABOUT-SITUATION-FRAME utter-
ances that we find in the human-computer dialogues typically did
not occur in human-human air travel dialogues. In addition, as we
note above, one obvious difference in the dialogue strategies im-
plemented at different sites had to do with whether these utterances
occurred upfront, within the dialogue, or both.
In order to demonstrate the utility of dialogue act tags as metrics
for spoken dialogue systems, we show that the use of these metrics
in the application of PARADISE [31] improves our model of user
satisfaction by an absolute 7%, from 38% to 45%. This is a large
increase, and the fit of these models are very good for models of
human behavior. We believe that we have only begun to discover
the ways in which the output of the dialogue parser can be used. In
future work we will examine whether other representations derived
from the metrics we have applied, such as sequences or structural
relations between various types of acts might improve our perfor-
mance model further. We are also collaborating with other mem-
bers of the COMMUNICATOR community who are investigating the
use of dialogue act and initiative tagging schemes for the purpose
of comparing human-human to human-computer dialogues [1].
8. ACKNOWLEDGMENTS
This work was supported under DARPA GRANT MDA 972 99 3
0003 to AT&T Labs Research. Thanks to Payal Prabhu and Sung-
bok Lee for their assistance with the implementation of the dia-
logue parser. We also appreciate the contribution of J. Aberdeen,
E. Bratt, S. Narayanan, K. Papineni, B. Pellom, J. Polifroni, A.
Potamianos, A. Rudnicky, S. Seneff, and D. Stallard who helped
us understand how the DATE classification scheme applied to their
COMMUNICATOR systems? dialogues.
9. REFERENCES
[1] J. Aberdeen and C. Doran. Human-computer and
human-human dialogues. DARPA Communicator Principle
Investigators Meeting (Philadelphia, PA USA).
http://www.dsic-web.net/ito/meetings/communicator
sep2000/, September, 2000.
[2] J. Allen and M. Core. Draft of DAMSL: Dialog act markup
in several layers. Coding scheme developed by the
MultiParty group, 1st Discourse Tagging Workshop,
University of Pennsylvania, March 1996, 1997.
[3] J. F. Allen. Recognizing intentions from natural language
utterances. In M. Brady and R. Berwick, editors,
Computational Models of Discourse. MIT Press, 1983.
[4] S. Bird and M. Liberman. A formal framework for linguistic
annotation. Speech Communication, 33(1,2):23?60, 2001.
[5] P. Brown and S. Levinson. Politeness: Some universals in
language usage. Cambridge University Press, 1987.
[6] J. C. Carletta, A. Isard, S. Isard, J. C. Kowtko,
G. Dowerty-Sneddon, and A. H. Anderson. The reliability of
a dialogue structure coding scheme. Computational
Linguistics, 23-1:13?33, 1997.
[7] R. Cattoni, M. Danieli, A. Panizza, V. Sandrini, and C. Soria.
Building a corpus of annotated dialogues: the ADAM
experience. In Proc. of the Conference
Corpus-Linguistics-2001, Lancaster, U.K., 2001.
[8] H. H. Clark and E. F. Schaefer. Contributing to discourse.
Cognitive Science, 13:259?294, 1989.
[9] S. L. Condon and C. G. Cech. Functional comparison of
face-to-face and computer-mediated decision-making
interactions. In S. Herring, editor, Computer-Mediated
Converstaion. John Benjamins, 1995.
[10] M. Danieli and E. Gerbino. Metrics for evaluating dialogue
strategies in a spoken language system. In Proceedings of the
1995 AAAI Spring Symposium on Empirical Methods in
Discourse Interpretation and Generation, pages 34?39,
1995.
[11] B. Di Eugenio, P. W. Jordan, J. D. Moore, and R. H.
Thomason. An empirical investigation of collaborative
dialogues. In ACL-COLING98, Proceedings of the
Thirty-sixth Conference of the Association for
Computational Linguistics, 1998.
[12] M. Finke, M. Lapata, A. Lavie, L. Levin, L. M. Tomokiyo,
T. Polzin, K. Ries, A. Waibel, and K. Zechner. Clarity:
Inferring discourse structure from speech. In American
Association for Artificial Intelligence (AAAI) Symposium on
Applying Machine Learning to Discourse Processing
Proceedings, Stanford, California, March 1998.
[13] E. Goffman. Frame Analysis: An Essay on the Organization
of Experience. Harper and Row, New York, 1974.
[14] E. Goffman. Forms of Talk. University of Pennsylvania
Press, Philadelphia, Pennsylvania, USA, 1981.
[15] B. J. Grosz. The representation and use of focus in dialogue
understanding. Technical Report 151, SRI International, 333
Ravenswood Ave, Menlo Park, Ca. 94025, 1977.
[16] A. Isard and J. C. Carletta. Replicability of transaction and
action coding in the map task corpus. In M. Walker and
J. Moore, editors, AAAI Spring Symposium: Empirical
Methods in Discourse Interpretation and Generation, pages
60?67, 1995.
[17] P. W. Jordan. Intentional Influences on Object Redescriptions
in Dialogue: Evidence from an Empirical Study. PhD thesis,
Intelligent Systems Program, University of Pittsburgh, 2000.
[18] D. Jurafsky, E. Shriberg, and D. Biasca. Swbd-damsl
labeling project coder?s manual. Technical report, University
of Colorado, 1997. available as
http://stripe.colorado.edu/ jurafsky/manual.august1.html.
[19] D. Litman. Plan recognition and discourse analysis: An
integrated approach for understanding dialogues. Technical
Report 170, University of Rochester, 1985.
[20] D. Marcu. Perlocutions: The achilles? heel of speech act
theory. Journal of Pragmatics, 1999.
[21] M. G. Moser, J. Moore, and E. Glendening. Instructions for
coding explanations: Identifying segments, relations and
minimal units. Technical Report 96-17, University of
Pittsburgh, Department of Computer Science, 1996.
[22] R. Perrault and J. Allen. A plan-based analysis of indirect
speech acts. American Journal of Computational Linguistics,
6:167?182, 1980.
[23] R. Power. A Computer Model of Conversation. PhD thesis,
University of Edinburgh, 1974.
[24] N. Reithinger and E. Maier. Utilizing statistical speech act
processing in verbmobil. In ACL 95, 1995.
[25] D. R.Traum and E. A. Hinkelman. Conversation acts in
task-oriented spoken dialogue. Computational Intelligence,
8(3):575?599, 1992.
[26] E. A. Schegloff and H. Sacks. Opening up closings.
Semiotica, 8:289?327, 1977.
[27] E. Shriberg, P. Taylor, R. Bates, A. Stolcke, K. Ries,
D. Jurafsky, N. Coccaro, R. Martin, M. Meteer, and C. V.
Ess-Dykema. Can prosody aid the automatic classification of
dialog acts in conversational speech. Language and Speech:
Special Issue on Prosody and Conversation, 2000.
[28] M. Walker, J. Aberdeen, J. Boland, E. Bratt, J. Garofolo,
L. Hirschman, A. Le, S. Lee, S. Narayanan, K. Papineni,
B. Pellom, J. Polifroni, A. Potamianos, P. Prabhu,
A. Rudnicky, G. Sanders, S. Seneff, D. Stallard, and
S. Whittaker. Darpa communicator dialog travel planning
systems: The june 2000 data collection. In Submitted to
EUROSPEECH 2001, 2001.
[29] M. A. Walker. Redundancy in collaborative dialogue. In
Fourteenth International Conference on Computational
Linguistics, pages 345?351, 1992.
[30] M. A. Walker. Limited attention and discourse structure.
Computational Linguistics, 22-2:255?264, 1996.
[31] M. A. Walker, C. A. Kamm, and D. J. Litman. Towards
developing general models of usability with PARADISE.
Natural Language Engineering: Special Issue on Best
Practice in Spoken Dialogue Systems, 2000.
Speaker Utterance Speech Act Task C-Domain
SYS Welcome. opening/closing nil task
You are logged in as a guest user. instruct nil frame
You may say repeat, help me out, start over, or, that?s wrong, instruct nil frame
you can also correct and interrupt the system at any time. instruct nil frame
What airport woodja like to fly out of? request-info origin task
USER MIAMI FLORIDA
SYS Leaving from Miami, implicit-conf origin communication
And, what city are you flying to? request-info dest task
USER MINNEAPOLIS MINNESOTA
SYS Flying from Miami to Minneapolis, implicit-conf origin, dest communication
What date would you like to fly? request-info date task
USER UH MONDAY OCTOBER TWENTY THIRD
SYS Leaving on the 23rd of October, implicit-conf date communication
And what time didja wanna leave? request-info time task
USER UH LATE MORNING
SYS From Minneapolis. implicit-conf origin communication
Are you returning to Miami? request-info trip-type task
USER UM
SYS Are you going back to Miami? request-info trip-type task
Please say yes or no. instruct nil frame
USER NO
SYS Okay, it will just take a moment while I look. status-report retrieval task
SYS I found 3 flights from Miami to Minneapolis on October 23rd . For option one I have a
flight with United departing at 10 A M and arriving at 3 P M with one stopover and
present-info itinerary task
Ticket price is 390 dollars. present-info price task
Please say next option, or, flight details, or, I?ll take it. offer itinerary task
USER NEXT OPTION
SYS For option two I have a flight with Delta departing at 12 and arriving at 4 P M non-stop. present-info itinerary task
Ticket price is 450 dollars. present-info itinerary task
Would you like me to hold this option? offer itinerary task
USER NEXT OPTION
SYS For the last option I have a flight with Northwest departing at 2 P M and arriving at 7 P M
with 1 stopover and
present-info itinerary task
Ticket price is 470 dollars. present-info itinerary task
say next option, or, flight details, or, I?ll take it. offer itinerary task
USER NEXT OPTION
SYS Here is your third option again. a flight with Northwest departing at 2 P M and arriving at
7 P M with 1 stopover and
present-info itinerary task
Ticket price is 470 dollars. present-info itinerary task
say next option, or, flight details, or, I?ll take it. offer itinerary task
USER I?LL TAKE IT
SYS Great! I am adding this flight to your itinerary. acknowledge booking task
Okay, Is Minneapolis your final destination? request-info trip-type task
USER YES IT IS
SYS Would you like to make any ground arrangements? offer ground task
USER NO
SYS Now, would you like to hear a summary of your itinerary? offer itinerary task
USER YES PLEASE
SYS Okay, Here?s your itinerary. On October 23rd Northwest Flight 123 departs Miami at 2 P
M, arrives Pittsburgh at 4 30 P M connecting to Northwest Flight 146 that departs Pittsburgh
at 5 P M and arrives Minneapolis at 7 P M.
present-info itinerary task
Figure 1: Dialogue Illustrating the Speech Act, Task-Subtask and Conversational Domain Dimensions of DATE
Natural Language Generation in Dialog Systems
Owen Rambow Srinivas Bangalore Marilyn Walker
AT&T Labs ? Research
Florham Park, NJ, USA
rambow@research.att.com
ABSTRACT
Recent advances in Automatic Speech Recognition technology have
put the goal of naturally sounding dialog systems within reach.
However, the improved speech recognition has brought to light
a new problem: as dialog systems understand more of what the
user tells them, they need to be more sophisticated at responding
to the user. The issue of system response to users has been ex-
tensively studied by the natural language generation community,
though rarely in the context of dialog systems. We show how re-
search in generation can be adapted to dialog systems, and how
the high cost of hand-crafting knowledge-based generation systems
can be overcome by employing machine learning techniques.
1. DIALOG SYSTEMS AND GENERATION
Recent advances in Automatic Speech Recognition (ASR) tech-
nology have put the goal of naturally sounding dialog systems within
reach.1 However, the improved ASR has brought to light a new
problem: as dialog systems understand more of what the user tells
them, they need to be more sophisticated at responding to the user.
If ASR is limited in quality, dialog systems typically employ a
system-initiative dialog strategy in which the dialog system prompts
the user for specific information and then presents some informa-
tion to the user. In this paradigm, the range of user input at any time
is limited (thus facilitating ASR), and the range of system output at
any time is also limited. However, such interactions are not very
natural. In a more natural interaction, the user can supply more and
different information at any time in the dialog. The dialog system
must then support a mixed-initiative dialog strategy. While this
strategy places greater requirements on ASR, it also increases the
range of system responses and the requirements on their quality in
terms of informativeness and of adaptation to the context.
For a long time, the issue of system response to users has been
studied by the Natural Language Generation (NLG) community,
though rarely in the context of dialog systems. What have emerged
from this work are a ?consensus architecture? [17] which modu-
larizes the large number of tasks performed during NLG in a par-
 
The work reported in this paper was partially funded by DARPA
contract MDA972-99-3-0003.
.
ticular way, and a range of linguistic representations which can be
used in accomplishing these tasks. Many systems have been built
using NLG technology, including report generators [8, 7], system
description generators [10], and systems that attempt to convince
the user of a particular view through argumentation [20, 4].
In this paper, we claim that the work in NLG is relevant to dia-
log systems as well. We show how the results can be incorporated,
and report on some initial work in adapting NLG approaches to di-
alog systems and their special needs. The dialog system we use is
the AT&T Communicator travel planning system.We use machine
learning and stochastic approaches where hand-crafting appears to
be too complex an option, but we also use insight gained during
previous work on NLG in order to develop models of what should
be learned. In this respect, the work reported in this paper differs
from other recent work on generation in the context of dialog sys-
tems [12, 16], which does not modularize the generation process
and proposes a single stochastic model for the entire process. We
start out by reviewing the generation architecture (Section 2). In
Section 3, we discuss the issue of text planning for Communicator.
In Section 4, we summarize some initial work in using machine
learning for sentence planning [19]. Finally, in Section 5 we sum-
marize work using stochastic tree models in generation [2].
2. TEXT GENERATION ARCHITECTURE
.
NLG is conceptualized as a process leading from a high-level
communicative goal to a sequence of communicative acts which
accomplish this communicative goal. A communicative goal is a
goal to affect the user?s cognitive state, e.g., his or her beliefs about
the world, desires with respect to the world, or intentions about
his or her actions in the world. Following (at least) [13], it has
been customary to divide the generation process into three phases,
the first two of which are planning phases. Reiter [17] calls this
architecture a ?consensus architecture? in NLG.
 During text planning, a high-level communicative goal is
broken down into a structured representation of atomic com-
municative goals, i.e., goals that can be attained with a single
communicative act (in language, by uttering a single clause).
The atomic communicative goals may be linked by rhetori-
cal relations which show how attaining the atomic goals con-
tributes to attaining the high-level goal.
 During sentence planning, abstract linguistic resources are
chosen to achieve the atomic communicative goals. This
includes choosing meaning-bearing lexemes, and how the
meaning-bearing lexemes are connected through abstract gram-
matical constructions (basically, lexical predicate-argument
Realizer
Sentence
Planner
Text
Manager
Dialog
Natural Language Generation
Planner
Prosody
Utterance
User
Utterance
System
Assigner
TTS
Natural Language Understanding ASR
Figure 1: Architecture of a dialog system with natural language generation
structure and modification). As a side-effect, sentence plan-
ning also determines sentence boundaries: there need not
be a one-to-one relation between elementary communicative
goals and sentences in the final text.
 During realization, the abstract linguistic resources chosen
during sentence planning are transformed into a surface lin-
guistic utterance by adding function words (such as auxil-
iaries and determiners), inflecting words, and determining
word order. This phase is not a planning phase in that it only
executes decisions made previously, by using grammatical
information about the target language. (Prosody assignment
can be treated as a separate module which follows realization
and which draws on all previous levels of representation. We
do not discuss prosody further in this paper.)
Note that sentence planning and realization use resources spe-
cific to the target-language, while text planning is language-independent
(though presumably it is culture-dependent).
In integrating this approach into a dialog system, we see that the
dialog manager (DM) no longer determines surface strings to send
to the TTS system, as is often the case in current dialog systems.
Instead, the DM determines high-level communicative goals which
are sent to the NLG component. Figure 1 shows a complete archi-
tecture. An advantage of such an architecture is the possibility for
extended plug-and-play: not only can the entire NLG system be
replaced, but also modules within the NLG system, thus allowing
researchers to optimize the system incrementally.
The main objection to the use of NLG techniques in dialog sys-
tems is that they require extensive hand-tuning of existing sys-
tems and approaches for new domains. Furthermore, because of
the relative sophistication of NLG techniques as compared to sim-
pler techniques such as templates, the hand-tuning requires spe-
cialized knowledge of linguistic representations; hand-tuning tem-
plates only requires software engineering skills. An approach based
on machine learning can provide a solution to this problem: it
draws on previous research in NLG and uses the same sophisti-
cated linguistic representations, but it learns the domain-specific
rules that use these representation automatically from data. It is the
goal of our research to show that for dialog systems, approaches
based on machine learning can do as well as or outperform hand-
crafted approaches (be they NLG- or template-based), while requir-
ing far less time for tuning. In the following sections, we summa-
rize the current state of our research on an NLG system for the
Communicator dialog system.
3. TEXT PLANNER
Based on observations from the travel domain of the Communi-
cator system, we have categorized system responses into two types.
The first type occurs during the initial phase when the system is
gathering information from the user. During this phase, the high-
level communicative goals that the system is trying to achieve are
fairly complex: the goals include getting the hearer to supply in-
formation, and to explicitly or implicitly confirm information that
the hearer has just supplied. (These latter goals are often motivated
by the still not perfect quality of ASR.) The second type occurs
when the system has obtained information that matches the user?s
requirements and the options (flights, hotel, or car rentals) need to
be presented to the user. Here, the communicative goal is mainly to
make the hearer believe a certain set of facts (perhaps in conjunc-
tion with a request for a choice among these options).
In the past, NLG systems typically have generated reports or
summaries, for which the high-level communicative goal is of the
type ?make the hearer/reader believe a given set of facts?, as it is
in the second type of system response discussed above. We believe
that NLG work in text planning can be successfully adapted to bet-
ter plan these system responses, taking into account not only the
information to be conveyed but also the dialog context and knowl-
edge about user preferences. We leave this to ongoing work.
In the first type of system response, the high-level communica-
tive goal typically is an unordered list of high-level goals, all of
which need to be achieved with the next turn of the system. An ex-
ample is shown in Figure 2. NLG work in text planning has not ad-
dressed such complex communicative goals in the past. However,
we have found that for the Communicator domain, no text planning
is needed, and that the sentence planner can act directly on a rep-
resentation of the type shown in Figure 2, because the number of
goals is limited (to five, in our studies). We expect that further work
in other dialog domains will require an extension of existing work
in text planning to account better for communicative goals other
than those that simply aim to affect the user?s (hearer?s) beliefs.
implicit-confirm(orig-city:NEWARK)
implicit-confirm(dest-city:DALLAS)
implicit-confirm(month:9)
implicit-confirm(day-number:1)
request(depart-time)
Figure 2: Sample text plan (communicative goals)
Realization Score
What time would you like to travel on September the 1st to Dallas from Newark? 5
Leaving on September the 1st. What time would you like to travel from Newark to Dallas? 4.5
Leaving in September. Leaving on the 1st. What time would you, traveling from Newark
to Dallas, like to leave?
2
Figure 3: Sample alternate realizations of the set of communicative goals shown in Figure 2 suggested by our sentence planner, with
human scores
-
--
-
SP
R
.
.
Sentence Planner
SP
G
H
RealPro
Realizer
Text Plan Chosen sp?tree with associated DSyntS
-
H
Sp?trees with associated DSyntSs
aDialog
System .
Figure 4: Architecture of our sentence planner
4. SENTENCE PLANNER
The principal challenge facing sentence planning for dialog sys-
tems is that there is no good corpus of naturally occurring interac-
tions of the type that need to occur between a dialog system and hu-
man users. This is because of the not-yet perfect ASR and the need
for implicitly or explicitly confirming most or all of the informa-
tion provided by the user. In conversations between two humans,
communicative goals such as implicit or explicit confirmations are
rare, and thus transcripts of human-human interactions in the same
domain cannot be used for the purpose of learning good strategies
to attain communicative goals. And of course we do not want to
use transcripts of existing systems, as we want to improve on their
performance, not mirror it.
We have therefore taken the approach of randomly generating a
set of solutions and having human judges score each of the options.
Each turn of the system is, as described in Section 3, characterized
by a set of high-level goals such as that shown in Figure 2. In the
turns we consider, no text planning is needed. To date, we have
concentrated on the issue of choosing abstract syntactic construc-
tions (rather than lexical choice), so we map each elementary com-
municative goal to a canonical lexico-syntactic structure (called a
DSyntS [11]). We then randomly combine these DSyntSs into
larger DSyntSs using a set of clause-combining operations iden-
tified previously in the literature [14, 18, 5], such as RELATIVE-
CLAUSE, CONJUNCTION, and MERGE.2 The way in which the ele-
mentary DSyntSs are combined is represented in a structure called
the sp-tree. Each sp-tree is then realized using an off-the-shelf re-
alizer, RealPro [9]. Some sample realizations for the same text plan
are shown in Figure 3, along with the average of the scores assigned
by two human judges.

MERGE identifies the verbs and arguments of two lexico-syntactic
structures which differ only in adjuncts. For example, you are flying
from Newark and you are flying on Monday are merged to you are
flying from Newark on Monday.
Using the human scores on each of the up to twenty variants per
turn, we use RankBoost [6] to learn a scoring function which uses
a large set of syntactic and lexical features. The resulting sentence
planner consists of two components: the sentence plan generator
(SPG) which generates candidate sentence plans and the sentence
plan ranker (SPR) which scores each one of them using the rules
learned by RankBoost and which then chooses the best sentence
plan. This architecture is shown in Figure 4.
We compared the performance of our sentence planner to a ran-
dom choice of sentence plans, and to the sentence plans chosen
as top-ranked by the human judges. The mean score of the turns
judged best by the human judges is 4.82 as compared with the
mean of 4.56 for the turns generated by our sentence planner, for
a mean difference of 0.26 (5%) on a scale of 1 to 5. The mean of
the scores of the turns picked randomly is 2.76, for a mean differ-
ence of 1.8 (36%). We validated these results in an independent
experiment in which 60 subjects evaluated different realizations for
a given turn [15]. (Recall that our trainable sentence planner was
trained on the scores of only two human judges.) This evaluation
revealed that the choices made by our trainable sentence planner
were not statistically distinguishable from the choices ranked at the
top by the two human judges. More importantly, they were also not
distinguishable statistically from the current hand-crafted template-
based output of the AT&T Communicator system, which has been
developed and fine-tuned over an extended period of time (the train-
able sentence planner is based on judgments that took about three
person-days to make).
5. REALIZER
At the level of the surface language, the difference in commu-
nicative intention between human-human travel advisory dialogs
and the intended dialogs is not as relevant: we can try and mimic
the human-human transcripts as closely as possible. To show this,
we have performed some initial experiments using FERGUS (Flex-
ible Empiricist-Rationalist Generation Using Syntax), a stochastic
surface realizer which incorporates a tree model and a linear lan-
guage model [2]. We have developed a metric which can be com-
puted automatically from the syntactic dependency structure of the
sentence and the linear order chosen by the realizer, and we have
shown that this metric correlates with human judgments of the fe-
licity of the sentence [3]. Using this metric, we have shown that the
use of both the tree model and the linear language model improves
the quality of the output of FERGUS over the use of only one or
the other of these resources.
FERGUS was originally trained on the Penn Tree Bank cor-
pus consisting of Wall Street Journal text (WSJ). The results on
an initial set of Communicator sentences were not encouraging,
presumably because there are few questions in the WSJ corpus,
and furthermore, specific constructions (including what as deter-
miner) appear to be completely absent (perhaps due to a newspaper
style file). In an initial experiment, we replaced the linear language
model (LM) trained on 1 million words of WSJ by an LM trained
on 10,000 words of human-human travel planning dialogs collected
at CMU. This resulted in a dramatic improvement, with almost all
questions being generated correctly. Since the CMU corpus is rel-
atively small for a LM, we intend to experiment with finding the
ideal combination of WSJ and CMU corpora. Furthermore, we are
currently in the process of syntactically annotating the CMU cor-
pus so that we can derive a tree model as well. We expect further
improvements in quality of the output, and we expect to be able
to exploit the kind of limited lexical variation allowed by the tree
model [1].
6. CONCLUSION
We have discussed how work in NLG can be applied in the
development of dialog systems, and we have presented two ap-
proaches to using stochastic models and machine learning in NLG.
Of course, the final justification for using a more sophisticated NLG
architecture must come from user trials of an integrated system.
However, we suspect that, as in the case of non-dialog NLG sys-
tems, the strongest arguments in favor of NLG often come from
software engineering issues of maintainability and extensibility, which
can be difficult to quantify in research systems.
7. REFERENCES
[1] S. Bangalore and O. Rambow. Corpus-based lexical choice
in natural language generation. In 38th Meeting of the
Association for Computational Linguistics (ACL?00), Hong
Kong, China, 2000.
[2] S. Bangalore and O. Rambow. Exploiting a probabilistic
hierarchical model for generation. In Proceedings of the 18th
International Conference on Computational Linguistics
(COLING 2000), Saarbru?cken, Germany, 2000.
[3] S. Bangalore, O. Rambow, and S. Whittaker. Evaluation
metrics for generation. In Proceedings of the First
International Natural Language Generation Conference
(INLG2000), Mitzpe Ramon, Israel, 2000.
[4] G. Carenini and J. Moore. A strategy for generating
evaluative arguments. In Proceedings of the First
International Natural Language Generation Conference
(INLG2000), Mitzpe Ramon, Israel, 2000.
[5] L. Danlos. G-TAG: A lexicalized formalism for text
generation inspired by tree adjoining grammar. In A. Abeille?
and O. Rambow, editors, Tree Adjoining Grammars:
Formalisms, Linguistic Analysis, and Processing. CSLI
Publications, 2000.
[6] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An efficient
boosting algorithm for combining preferences. In Machine
Learning: Proceedings of the Fifteenth International
Conference, 1998. Extended version available from
http://www.research.att.com/ schapire.
[7] E. Goldberg, N. Driedger, and R. Kittredge. Using
natural-language processing to produce weather forecasts.
IEEE Expert, pages 45?53, 1994.
[8] K. Kukich. Knowledge-Based Report Generation: A
Knowledge Engineering Approach to Natural Language
Report Generation. PhD thesis, University of Pittsuburgh,
1983.
[9] B. Lavoie and O. Rambow. RealPro ? a fast, portable
sentence realizer. In Proceedings of the Conference on
Applied Natural Language Processing (ANLP?97),
Washington, DC, 1997.
[10] B. Lavoie, O. Rambow, and E. Reiter. Customizable
descriptions of object-oriented models. In Proceedings of the
Conference on Applied Natural Language Processing
(ANLP?97), Washington, DC, 1997.
[11] I. A. Mel?c?uk. Dependency Syntax: Theory and Practice.
State University of New York Press, New York, 1988.
[12] A. H. Oh and A. I. Rudnicky. Stochastic language generation
for spoken dialog systems. In Proceedings of the
ANL/NAACL 2000 Workshop on Conversational Systems,
pages 27?32, Seattle, 2000. ACL.
[13] O. Rambow and T. Korelsky. Applied text generation. In
Third Conference on Applied Natural Language Processing,
pages 40?47, Trento, Italy, 1992.
[14] O. Rambow and T. Korelsky. Applied text generation. In
Proceedings of the Third Conference on Applied Natural
Language Processing, ANLP92, pages 40?47, 1992.
[15] O. Rambow, M. Rogati, and M. Walker. A trainable sentence
planner for spoken dialogue systems. In 39th Meeting of the
Association for Computational Linguistics (ACL?01),
Toulouse, France, 2001.
[16] A. Ratnaparkhi. Trainable methods for surface natural
language generation. In Proceedings of First North American
ACL, Seattle, USA, May 2000.
[17] E. Reiter. Has a consensus NL generation architecture
appeared, and is it psycholinguistically plausible? In
Proceedings of the 7th International Workshop on Natural
Language Generation, pages 163?170, Maine, 1994.
[18] J. Shaw. Clause aggregation using linguistic knowledge. In
Proceedings of the 8th International Workshop on Natural
Language Generation, Niagara-on-the-Lake, Ontario, 1998.
[19] M. Walker, O. Rambow, and M. Rogati. A trainable sentence
planner for spoken dialogue systems. In 2nd Meeting of the
North American Chapter of the Association for
Computational Linguistics (NAACL?01), Pittsburgh, PA,
2001.
[20] I. Zukerman, R. McConachy, and K. Korb. Bayesian
reasoning in an abductive mechanism for argument
generation and analysis. In AAAI98 Proceedings ? the
Fifteenth National Conference on Artificial Intelligence,
pages 833?838, Madison, Wisconsin, 1998.
SPoT: A Trainable Sentence Planner
Marilyn A. Walker
AT&T Labs ? Research
Florham Park, NJ, USA
walker@research.att.com
Owen Rambow
AT&T Labs ? Research
Florham Park, NJ, USA
rambow@research.att.com
Monica Rogati
Carnegie Mellon University
Pittsburgh, PA, USA
mrogati+@cs.cmu.edu
Abstract
Sentence planning is a set of inter-related but distinct
tasks, one of which is sentence scoping, i.e. the choice
of syntactic structure for elementary speech acts and
the decision of how to combine them into one or more
sentences. In this paper, we present SPoT, a sentence
planner, and a new methodology for automatically train-
ing SPoT on the basis of feedback provided by human
judges. We reconceptualize the task into two distinct
phases. First, a very simple, randomized sentence-plan-
generator (SPG) generates a potentially large list of pos-
sible sentence plans for a given text-plan input. Second,
the sentence-plan-ranker (SPR) ranks the list of output
sentence plans, and then selects the top-ranked plan. The
SPR uses ranking rules automatically learned from train-
ing data. We show that the trained SPR learns to select a
sentence plan whose rating on average is only 5% worse
than the top human-ranked sentence plan.
1 Introduction
Sentence planning is a set of inter-related but distinct
tasks, one of which is sentence scoping, i.e. the choice
of syntactic structure for elementary speech acts and the
decision of how to combine them into sentences.1 For
example, consider the required capabilities of a sentence
planner for a mixed-initiative spoken dialog system for
travel planning:
(D1) System1: Welcome.... What airport would you like to
fly out of?
User2: I need to go to Dallas.
System3: Flying to Dallas. What departure airport was
that?
User4: from Newark on September the 1st.
System5: What time would you like to travel on Septem-
ber the 1st to Dallas from Newark?
Utterance System1 requests information about the
caller?s departure airport, but in User2, the caller takes
the initiative to provide information about her destina-
tion. In System3, the system?s goal is to implicitly con-
firm the destination (because of the possibility of error
1We would like to thank Michael Collins and Rob Schapire for their
help, comments, and encouragement, and Noemie Elhadad and three
anonymous reviewers for very useful feedback. This work was partially
funded by DARPA under contract MDA972-99-3-0003.
in the speech recognition component), and request in-
formation (for the second time) of the caller?s departure
airport. In User4, the caller provides this information
but also provides the month and day of travel. Given the
system?s dialog strategy, the communicative goals for its
next turn are to implicitly confirm all the information that
the user has provided so far, i.e. the departure and desti-
nation cities and the month and day information, as well
as to request information about the time of travel. The
system?s representation of its communicative goals for
utterance System5 is in Figure 1. The job of the sentence
planner is to decide among the large number of potential
realizations of these communicative goals. Some exam-
ple alternative realizations are in Figure 2.2
implicit-confirm(orig-city:NEWARK)
implicit-confirm(dest-city:DALLAS)
implicit-confirm(month:9)
implicit-confirm(day-number:1)
request(depart-time)
Figure 1: The text plan (communicative goals) for utter-
ance System5 in dialog D1
Alt Realization H RB
0 What time would you like to travel on
September the 1st to Dallas from Newark?
5 .85
5 Leaving on September the 1st. What time
would you like to travel from Newark to Dal-
las?
4.5 .82
8 Leaving in September. Leaving on the 1st.
What time would you, traveling from Newark
to Dallas, like to leave?
2 .39
Figure 2: Alternative sentence plan realizations for the
text plan for utterance System5 in dialog D1. H = human
rating, RB = RankBoost score.
In this paper, we present SPoT, for ?Sentence Plan-
ner, Trainable?. We also present a new methodology
for automatically training SPoT on the basis of feed-
back provided by human judges. In order to train SPoT,
we reconceptualize its task as consisting of two distinct
phases. In the first phase, the sentence-plan-generator
2The meaning of the human ratings and RankBoost scores in Fig-
ure 2 are discussed below.
(SPG) generates a potentially large sample of possible
sentence plans for a given text-plan input. In the second
phase, the sentence-plan-ranker (SPR) ranks the sample
sentence plans, and then selects the top-ranked output to
input to the surface realizer. Our primary contribution is
a method for training the SPR. The SPR uses rules au-
tomatically learned from training data, using techniques
similar to (Collins, 2000; Freund et al, 1998).
Our method for training a sentence planner is unique
in neither depending on hand-crafted rules, nor on the
existence of a text or speech corpus in the domain of the
sentence planner obtained from the interaction of a hu-
man with a system or another human. We show that the
trained SPR learns to select a sentence plan whose rating
on average is only 5% worse than the top human-ranked
sentence plan. In the remainder of the paper, section 2
describes the sentence planning task in more detail. We
then describe the sentence plan generator (SPG) in sec-
tion 3, the sentence plan ranker (SPR) in section 4, and
the results in section 5.
2 The Sentence Planning Task
The term ?sentence planning? comprises many distinct
tasks and many ways of organizing these tasks have been
proposed in the literature. In general, the role of the sen-
tence planner is to choose abstract linguistic resources
(meaning-bearing lexemes, syntactic constructions) for a
text plan. In our case, the output of the dialog manager of
a spoken dialog system provides the input to our sentence
planner in the form of a single spoken dialog text plan
for each of the turns. (In contrast, the dialog managers
of most dialog systems today simply output completely
formed utterances which are passed on to the TTS mod-
ule.) Each text plan is an unordered set of elementary
speech acts encoding all of the system?s communicative
goals for the current turn, as illustrated in Figure 1. Each
elementary speech act is represented as a type (request,
implicit confirm, explicit confirm), with type-specific pa-
rameters. The sentence planner must decide among al-
ternative abstract linguistic resources for this text plan;
surface realizations of some such alternatives are in Fig-
ure 2.
As already mentioned, we divide the sentence plan-
ning task into two phases. In the first phase, the sentence-
plan-generator (SPG) generates 12-20 possible sentence
plans for a given input text plan. Each speech act
is assigned a canonical lexico-structural representation
(called a DSyntS ? Deep Syntactic Structure (Mel?c?uk,
1988)). The sentence plan is a tree recording how these
elementary DSyntS are combined into larger DSyntSs;
the DSyntS for the entire input text plan is associated
with the root node of the tree. In the second phase, the
sentence plan ranker (SPR) ranks sentence plans gener-
ated by the SPG, and then selects the top-ranked out-
put as input to the surface realizer, RealPro (Lavoie and
Rambow, 1997). The architecture is summarized in Fig-
ure 3.
-
--
-
S
P
R
.
.
Sentence Planner
S
P
G
H
RealPro
Realizer
Text Plan Chosen sp?tree with associated DSyntS
-
H
Sp?trees with associated DSyntSs
aDialog
System .
Figure 3: Architecture of SPoT
3 The Sentence Plan Generator
The research presented here is primarily concerned
with creating a trainable SPR. A strength of our ap-
proach is the ability to use a very simple SPG, as we
explain below. The basis of our SPG is a set of clause-
combining operations that incrementally transform a list
of elementary predicate-argument representations (the
DSyntSs corresponding to elementary speech acts, in
our case) into a single lexico-structural representation,
by combining these representations using the following
combining operations. Examples can be found in Fig-
ure 4.
  MERGE. Two identical main matrix verbs can be iden-
tified if they have the same arguments; the adjuncts are
combined.
  MERGE-GENERAL. Same as MERGE, except that one of
the two verbs may be embedded.
  SOFT-MERGE. Same as MERGE, except that the verbs
need only to be in a relation of synonymy or hyperonymy
(rather than being identical).
  SOFT-MERGE-GENERAL. Same as MERGE-GENERAL,
except that the verbs need only to be in a relation of syn-
onymy or hyperonymy.
  CONJUNCTION. This is standard conjunction with con-
junction reduction.
  RELATIVE-CLAUSE. This includes participial adjuncts to
nouns.
  ADJECTIVE. This transforms a predicative use of an ad-
jective into an adnominal construction.
  PERIOD. Joins two complete clauses with a period.
These operations are not domain-specific and are sim-
ilar to those of previous aggregation components (Ram-
bow and Korelsky, 1992; Shaw, 1998; Danlos, 2000), al-
though the various MERGE operations are, to our knowl-
edge, novel in this form.
The result of applying the operations is a sentence
plan tree (or sp-tree for short), which is a binary tree
with leaves labeled by all the elementary speech acts
Rule Sample first argument Sample second argument Result
MERGE You are leaving from Newark. You are leaving at 5 You are leaving at 5 from Newark
MERGE-GENERAL What time would you like to
leave?
You are leaving from Newark. What time would you like to leave
from Newark?
SOFT-MERGE You are leaving from Newark You are going to Dallas You are traveling from Newark to
Dallas
SOFT-MERGE-
GENERAL
What time would you like to
leave?
You are going to Dallas. What time would you like to fly to
Dallas?
CONJUNCTION You are leaving from Newark. You are going to Dallas. You are leaving from Newark and
you are going to Dallas.
RELATIVE-
CLAUSE
Your flight leaves at 5. Your flight arrives at 9. Your flight, which leaves at 5, ar-
rives at 9.
ADJECTIVE Your flight leaves at 5. Your flight is nonstop. Your nonstop flight leaves at 5.
PERIOD You are leaving from Newark. You are going to Dallas. You are leaving from Newark.
You are going to Dallas
Figure 4: List of clause combining operations with examples from our domain; an explanation of the operations is
given in Section 3.
from the input text plan, and with its interior nodes la-
beled with clause-combining operations3. Each node is
also associated with a DSyntS: the leaves (which corre-
spond to elementary speech acts from the input text plan)
are linked to a canonical DSyntS for that speech act (by
lookup in a hand-crafted dictionary). The interior nodes
are associated with DSyntSs by executing their clause-
combing operation on their two daughter nodes. (A PE-
RIOD node results in a DSyntS headed by a period and
whose daughters are the two daughter DSyntSs.) If a
clause combination fails, the sp-tree is discarded (for ex-
ample, if we try to create a relative clause of a struc-
ture which already contains a period). As a result, the
DSyntS for the entire turn is associated with the root
node. This DSyntS can be sent to RealPro, which returns
a sentence (or several sentences, if the DSyntS contains
period nodes). The SPG is designed in such a way that
if a DSyntS is associated with the root node, it is a valid
structure which can be realized.
2
1
3imp?confirm(month) request(time) soft?merge?general
imp?confirm(dest?city) imp?confirm(orig?city)
imp?confirm(day)
soft?merge
soft?merge?general
soft?merge?general
Figure 5: Alternative 0 Sentence Plan Tree
Figure 2 shows some of the realizations of alternative
sentence plans generated by our SPG for utterance Sys-
3The sp-tree is inspired by (Lavoie and Rambow, 1998). The rep-
resentations used by Danlos (2000), Gardent and Webber (1998), or
Stone and Doran (1997) are similar, but do not (always) explicitly rep-
resent the clause combining operations as labeled nodes.
21
3
soft?merge?general
imp?confirm(day)
period
soft?merge?general
imp?confirm(month)
imp?confirm(orig?city) imp?confirm(dest?city)
request(time)soft?merge?general
Figure 6: Alternative 5 Sentence Plan Tree
2
1
soft?merge?general
imp?confirm(orig?city)
request(time)
imp?confirm(dest?city)
relative?clause
imp?confirm(day)
period
period
imp?confirm(month)
Figure 7: Alternative 8 Sentence Plan Tree
tem5 in Dialog D1. Sp-trees for alternatives 0, 5 and
8 are in Figures 5, 6 and 7. For example, consider the
sp-tree in Figure 7. Node soft-merge-general merges
an implicit-confirmations of the destination city and the
origin city. The row labelled SOFT-MERGE in Figure 4
shows the result of applying the soft-merge operation
when Args 1 and 2 are implicit confirmations of the ori-
gin and destination cities. Figure 8 illustrates the rela-
tionship between the sp-tree and the DSyntS for alter-
native 8. The labels and arrows show the DSyntSs as-
sociated with each node in the sp-tree (in Figure 7), and
the diagram also shows how structures are composed into
larger structures by the clause combining operations.
number: sg
mood: question
mood: inf?to
AT1
time
travel
IN1
September
PRONOUN ON1
1 WHAT2
person: 2
Dallasimp?confirm(moth)
imp?confirm(day)
soft?merge?general
relative?clause request(time)
imp?confirm(dest?city)imp?confirm(orig?city)
TO1
mood:prespartmood:prespart
period
travel
mood:prespart
Newark
FROM1PRONOUN
leave
period
leave
like
PRONOUN
PRONOUN
Figure 8: Alternative 8 DSyntS (not all linguistic features are shown)
The complexity of most sentence planners arises from
the attempt to encode constraints on the application of,
and ordering of, the operations, in order to generate a sin-
gle high quality sentence plan. In our approach, we do
not need to encode such constraints. Rather, we gener-
ate a random sample of possible sentence plans for each
text plan, up to a pre-specified maximum number of sen-
tence plans, by randomly selecting among the operations
according to some probability distribution.4
4 The Sentence-Plan-Ranker
The sentence-plan-ranker SPR takes as input a set of sen-
tence plans generated by the SPG and ranks them. In
order to train the SPR we applied the machine learning
program RankBoost (Freund et al, 1998), to learn from
a labelled set of sentence-plan training examples a set of
rules for scoring sentence plans.
4.1 RankBoost
RankBoost is a member of a family of boosting algo-
rithms (Schapire, 1999). Freund et al (1998) describe
the boosting algorithms for ranking in detail: for com-
pleteness, we give a brief description in this section.
Each example  is represented by a set of  indicator
functions

	 for 
 . The indicator functions
are calculated by thresholding the feature values (counts)
described in section 4.2. For example, one such indicator
function might be


 
		
ffEvaluating a Trainable Sentence Planner for a Spoken Dialogue System
Owen Rambow
AT&T Labs ? Research
Florham Park, NJ, USA
rambow@research.att.com
Monica Rogati
Carnegie Mellon University
Pittsburgh, PA, USA
mrogati+@cs.cmu.edu
Marilyn A. Walker
AT&T Labs ? Research
Florham Park, NJ, USA
walker@research.att.com
Abstract
Techniques for automatically training
modules of a natural language gener-
ator have recently been proposed, but
a fundamental concern is whether the
quality of utterances produced with
trainable components can compete with
hand-crafted template-based or rule-
based approaches. In this paper We ex-
perimentally evaluate a trainable sen-
tence planner for a spoken dialogue sys-
tem by eliciting subjective human judg-
ments. In order to perform an ex-
haustive comparison, we also evaluate
a hand-crafted template-based genera-
tion component, two rule-based sen-
tence planners, and two baseline sen-
tence planners. We show that the train-
able sentence planner performs better
than the rule-based systems and the
baselines, and as well as the hand-
crafted system.
1 Introduction
The past several years have seen a large increase
in commercial dialog systems. These systems
typically use system-initiative dialog strategies,
with system utterances highly scripted for style
and register and recorded by voice talent. How-
ever several factors argue against the continued
use of these simple techniques for producing the
system side of the conversation. First, text-to-
speech has improved to the point of being a vi-
able alternative to pre-recorded prompts. Second,
there is a perceived need for spoken dialog sys-
tems to be more flexible and support user initia-
tive, but this requires greater flexibility in utter-
ance generation. Finally, systems to support com-
plex planning are being developed, which will re-
quire more sophisticated output.
As we move away from systems with pre-
recorded prompts, there are two possible ap-
proaches to producing system utterances. The
first is template-based generation, where ut-
terances are produced from hand-crafted string
templates. Most current research systems use
template-based generation because it is concep-
tually straightforward. However, while little or
no linguistic training is needed to write templates,
it is a tedious and time-consuming task: one or
more templates must be written for each combi-
nation of goals and discourse contexts, and lin-
guistic issues such as subject-verb agreement and
determiner-noun agreement must be repeatedly
encoded for each template. Furthermore, main-
tenance of the collection of templates becomes a
software engineering problem as the complexity
of the dialog system increases.1
The second approach is natural language gen-
eration (NLG), which customarily divides the
generation process into three modules (Rambow
and Korelsky, 1992): (1) Text Planning, (2) Sen-
tence Planning, and (3) Surface Realization. In
this paper, we discuss only sentence planning; the
role of the sentence planner is to choose abstract
lexico-structural resources for a text plan, where
a text plan encodes the communicative goals for
an utterance (and, sometimes, their rhetorical
structure). In general, NLG promises portability
across application domains and dialog situations
by focusing on the development of rules for each
generation module that are general and domain-
1Although we are not aware of any software engineering
studies of template development and maintenance, this claim
is supported by abundant anecdotal evidence.
independent. However, the quality of the output
for a particular domain, or a particular situation
in a dialog, may be inferior to that of a template-
based system without considerable investment in
domain-specific rules or domain-tuning of gen-
eral rules. Furthermore, since rule-based systems
use sophisticated linguistic representations, this
handcrafting requires linguistic knowledge.
Recently, several approaches for automatically
training modules of an NLG system have been
proposed (Langkilde and Knight, 1998; Mel-
lish et al, 1998; Walker, 2000). These hold
the promise that the complex step of customiz-
ing NLG systems by hand can be automated,
while avoiding the need for tedious hand-crafting
of templates. While the engineering benefits of
trainable approaches appear obvious, it is unclear
whether the utterance quality is high enough.
In (Walker et al, 2001) we propose a new
model of sentence planning called SPOT. In
SPOT, the sentence planner is automatically
trained, using feedback from two human judges,
to choose the best from among different options
for realizing a set of communicative goals. In
(Walker et al, 2001), we evaluate the perfor-
mance of the learning component of SPOT, and
show that SPOT learns to select sentence plans
that are highly rated by the two human judges.
While this evaluation shows that SPOT has in-
deed learned from the human judges, it does not
show that using only two human judgments is
sufficient to produce more broadly acceptable re-
sults, nor does it show that SPOT performs as well
as optimized hand-crafted template or rule-based
systems. In this paper we address these questions.
Because SPOT is trained on data from a work-
ing system, we can directly compare SPOT to the
hand-crafted, template-based generation compo-
nent of the current system. In order to perform an
exhaustive comparison, we also implemented two
rule-based and two baseline sentence-planners.
One baseline simply produces a single sentence
for each communicative goal. Another baseline
randomly makes decisions about how to combine
communicative goals into sentences. We directly
compare these different approaches in an evalua-
tion experiment in which 60 human subjects rate
each system?s output on a scale of 1 to 5.
The experimental design is described in section
System1: Welcome.... What airport would you like to fly
out of?
User2: I need to go to Dallas.
System3: Flying to Dallas. What departure airport was
that?
User4: from Newark on September the 1st.
System5: Flying from Newark to Dallas, Leaving on the
1st of September, And what time did you want
to leave?
Figure 1: A dialog with AMELIA
2. The sentence planners used in the evaluation
are described in section 3. In section 4, we present
our results. We show that the trainable sentence
planner performs better than both rule-based sys-
tems and as well as the hand-crafted template-
based system. These four systems outperform the
baseline sentence planners. Section 5 summarizes
our results and discusses related and future work.
2 Experimental Context and Design
Our research concerns developing and evaluat-
ing a portable generation component for a mixed-
initiative travel planning system, AMELIA, de-
veloped at AT&T Labs as part of DARPA Com-
municator. Consider the required generation ca-
pabilities of AMELIA, as illustrated in Figure 1.
Utterance System1 requests information about
the caller?s departure airport, but in User2, the
caller takes the initiative to provide information
about her destination. In System3, the system?s
goal is to implicitly confirm the destination (be-
cause of the possibility of error in the speech
recognition component), and request information
(for the second time) of the caller?s departure air-
port. This combination of communicative goals
arises dynamically in the dialog because the sys-
tem supports user initiative, and requires differ-
ent capabilities for generation than if the system
could only understand the direct answer to the
question that it asked in System1.
In User4, the caller provides this information
but takes the initiative to provide the month and
day of travel. Given the system?s dialog strategy,
the communicative goals for its next turn are to
implicitly confirm all the information that the user
has provided so far, i.e. the departure and desti-
nation cities and the month and day information,
as well as to request information about the time
of travel. The system?s representation of its com-
municative goals for System5 is in Figure 2. As
before, this combination of communicative goals
arises in response to the user?s initiative.
implicit-confirm(orig-city:NEWARK)
implicit-confirm(dest-city:DALLAS)
implicit-confirm(month:9)
implicit-confirm(day-number:1)
request(depart-time:whatever)
Figure 2: The text plan (communicative goals) for
System5 in Figure 1
Like most working research spoken dialog
systems, AMELIA uses hand-crafted, template-
based generation. Its output is created by choos-
ing string templates for each elementary speech
act, using a large choice function which depends
on the type of speech act and various context con-
ditions. Values of template variables (such as ori-
gin and destination cities) are instantiated by the
dialog manager. The string templates for all the
speech acts of a turn are heuristically ordered and
then appended to produce the output. In order to
produce output that is not highly redundant, string
templates must be written for every possible com-
bination of speech acts in a text plan. We refer to
the output generated by AMELIA using this ap-
proach as the TEMPLATE output.
System Realization
TEMPLATE Flying from Newark to Dallas, Leaving on
the 1st of September, And what time did
you want to leave?
SPoT What time would you like to travel on
September the 1st to Dallas from Newark?
RBS (Rule-
Based)
What time would you like to travel on
September the 1st to Dallas from Newark?
ICF (Rule-
Based)
What time would you like to fly on
September the 1st to Dallas from Newark?
RANDOM Leaving in September. Leaving on the
1st. What time would you, traveling from
Newark to Dallas, like to leave?
NOAGG Leaving on the 1. Leaving in September.
Going to Dallas. Leaving from Newark.
What time would you like to leave?
Figure 3: Sample outputs for System5 of Figure 1
for each type of generation system used in the
evaluation experiment.
We perform an evaluation using human sub-
jects who judged the TEMPLATE output of
AMELIA against five NLG-based approaches:
SPOT, two rule-based approaches, and two base-
lines. We describe them in Section 3. An exam-
ple output for the text plan in Figure 2 for each
system is in Figure 3. The experiment required
human subjects to read 5 dialogs of real inter-
actions with AMELIA. At 20 points over the 5
dialogs, AMELIA?s actual utterance (TEMPLATE)
is augmented with a set of variants; each set of
variants included a representative generated by
SPOT, and representatives of the four compari-
son sentence planners. At times two or more of
these variants coincided, in which case sentences
were not repeated and fewer than six sentences
were presented to the subjects. The subjects rated
each variation on a 5-point Likert scale, by stating
the degree to which they agreed with the state-
ment The system?s utterance is easy to under-
stand, well-formed, and appropriate to the dialog
context. Sixty colleagues not involved in this re-
search completed the experiment.
3 Sentence Planning Systems
This section describes the five sentence planners
that we compare. SPOT, the two rule-based
systems, and the two baseline sentence planners
are all NLG based sentence planners. In Sec-
tion 3.1, we describe the shared representations
of the NLG based sentence planners. Section 3.2
describes the baselines, RANDOM and NOAGG.
Section 3.3 describes SPOT. Section 3.4 de-
scribes the rule-based sentence planners, RBS
and ICF.
3.1 Aggregation in Sentence Planning
In all of the NLG sentence planners, each speech
act is assigned a canonical lexico-structural rep-
resentation (called a DSyntS ? Deep Syntactic
Structure (Melc?uk, 1988)). We exclude issues of
lexical choice from this study, and restrict our at-
tention to the question of how elementary struc-
tures for separate elementary speech acts are as-
sembled into extended discourse. The basis of all
the NLG systems is a set of clause-combining op-
erations that incrementally transform a list of el-
ementary predicate-argument representations (the
DSyntSs corresponding to the elementary speech
acts of a single text plan) into a list of lexico-
structural representations of one or more sen-
tences, that are sent to a surface realizer. We uti-
lize the RealPro Surface realizer with all of the
Rule Arg 1 Arg 2 Result
MERGE You are leaving from Newark. You are leaving at 5 You are leaving at 5 from Newark
SOFT-MERGE You are leaving from Newark You are going to Dallas You are traveling from Newark to
Dallas
CONJUNCTION You are leaving from Newark. You are going to Dallas. You are leaving from Newark and
you are going to Dallas.
RELATIVE-
CLAUSE
Your flight leaves at 5. Your flight arrives at 9. Your flight, which leaves at 5, ar-
rives at 9.
ADJECTIVE Your flight leaves at 5. Your flight is nonstop. Your nonstop flight leaves at 5.
PERIOD You are leaving from Newark. You are going to Dallas. You are leaving from Newark.
You are going to Dallas
RANDOM CUE-
WORD
What time would yo like to
leave?
n/a Now, what time would you like to
leave?
Figure 4: List of clause combining operations with examples
sentence planners (Lavoie and Rambow, 1997).
DSyntSs are combined using the operations ex-
emplified in Figure 4. The result of applying the
operations is a sentence plan tree (or sp-tree
for short), which is a binary tree with leaves la-
beled by all the elementary speech acts from the
input text plan, and with its interior nodes la-
beled with clause-combining operations. As an
example, Figure 5 shows the sp-tree for utterance
System5 in Figure 1. Node soft-merge-general  
merges an implicit-confirmation of the destina-
tion city and the origin city. The row labelled
SOFT-MERGE in Figure 4 shows the result when
Args 1 and 2 are implicit confirmations of the ori-
gin and destination. See (Walker et al, 2001) for
more detail on the sp-tree. The experimental sen-
tence planners described below vary how the sp-
tree is constructed.
2
1
3imp?confirm(month) request(time) soft?merge?general
imp?confirm(dest?city) imp?confirm(orig?city)
imp?confirm(day)
soft?merge
soft?merge?general
soft?merge?general
Figure 5: A Sentence Plan Tree for Utterance Sys-
tem 5 in Dialog D1
3.2 Baseline Sentence Planners
In one obvious baseline system the sp-tree is con-
structed by applying only the PERIOD operation:
each elementary speech act is realized as its own
sentence. This baseline, NOAGG, was suggested
by Hovy and Wanner (1996). For NOAGG, we
order the communicative acts from the text plan
as follows: implicit confirms precede explicit
confirms precede requests. Figure 3 includes a
NOAGG output for the text plan in Figure 2.
A second possible baseline sentence planner
simply applies combination rules randomly ac-
cording to a hand-crafted probability distribution
based on preferences for operations such as the
MERGE family over CONJUNCTION and PERIOD.
In order to be able to generate the resulting sen-
tence plan tree, we exclude certain combinations,
such as generating anything other than a PERIOD
above a node labeled PERIOD in a sentence plan.
The resulting sentence planner we refer to as
RANDOM. Figure 3 includes a RANDOM output
for the text plan in Figure 2.
In order to construct a more complex, and
hopefully better, sentence planner, we need to en-
code constraints on the application of, and order-
ing of, the operations. It is here that the remaining
approaches differ. In the first approach, SPOT,
we learn constraints from training material; in the
second approach, rule-based, we construct con-
straints by hand.
3.3 SPoT: A Trainable Sentence Planner
For the sentence planner SPOT, we reconceptu-
alize sentence planning as consisting of two dis-
tinct phases as in Figure 6. In the first phase, the
sentence-plan-generator (SPG) randomly gener-
ates up to twenty possible sentence plans for a
given text-plan input. For this phase we use the
RANDOM sentence-planner. In the second phase,
the sentence-plan-ranker (SPR) ranks the sample
-
--
-
S
P
R
.
.
Sentence Planner
S
P
G
H
RealPro
Realizer
Text Plan Chosen sp?tree with associated DSyntS
-
H
Sp?trees with associated DSyntSs
aDialog
System .
Figure 6: Architecture of SPoT
sentence plans, and then selects the top-ranked
output to input to the surface realizer. The SPR
is automatically trained by applying RankBoost
(Freund et al, 1998) to learn ranking rules from
training data. The training data was assembled
by using RANDOM to randomly generate up to
20 realizations for 100 turns; two human judges
then ranked each of these realizations (using the
setup described in Section 2). Over 3,000 fea-
tures were discovered from the generated trees
by routines that encode structural and lexical as-
pects of the sp-trees and the DSyntS. RankBoost
identified the features that contribute most to a
realization?s ranking. The SPR uses these rules
to rank alternative sp-trees, and then selects the
top-ranked output as input to the surface realizer.
Walker et al (2001) describe SPOT in detail.
3.4 Two Rule-Based Sentence Planners
It has not been the object of our research to con-
struct a rule-based sentence planner by hand, be
it domain-independent or optimized for our do-
main. Our goal was to compare the SPOT sen-
tence planner with a representative rule-based
system. We decided against using an existing off-
the-shelf rule-based system, since it would be too
complex a task to port it to our application. In-
stead, we constructed two reasonably representa-
tive rule-based sentence planners. This task was
made easier by the fact that we could reuse much
of the work done for SPOT, in particular the data
structure of the sp-tree and the implementation of
the clause-combining operations. We developed
the two systems by applying heuristics for pro-
ducing good output, such as preferences for ag-
gregation. They differ only in the initial ordering
of the communicative acts in the input text plan.
In the first rule-based system, RBS (for ?Rule-
Based System?), we order the speech acts with
explicit confirms first, then requests, then implicit
confirms. Note that explicit confirms and requests
do not co-occur in our data set. The second rule-
based system is identical, except that implicit con-
firms come first rather than last. This system we
call ICF (for ?Rule-based System with Implicit
Confirms First?).
In the initial step of both RBS and ICF,
we take the two leftmost members of the text
plan and try to combine them using the follow-
ing preference ranking of the combination op-
erations: ADJECTIVE, the MERGEs, CONJUNC-
TION, RELATIVE-CLAUSE, PERIOD. The first
operation to succeed is chosen. This yields a bi-
nary sp-tree with three nodes, which becomes the
current sp-tree. As long as the root node of
the current sp-tree is not a PERIOD, we iterate
through the list of remaining speech acts on the
ordered text plan, combining each one with the
current sp-tree using the preference-ranked opera-
tions as just described. The result of each iteration
step is a binary, left-branching sp-tree. However,
if the root node of the current sp-tree is a PERIOD,
we start a new current sp-tree, as in the initial step
described above. When the text plan has been ex-
hausted, all partial sp-trees (all of which except
for the last one are rooted in PERIOD) are com-
bined in a left-branching tree using PERIOD. Cue
words are added as follows: (1) The cue word
now is attached to utterances beginning a new
subtask; (2) The cue word and is attached to ut-
terances continuing a subtask; (3) The cue words
alright or okay are attached to utterances contain-
ing implicit confirmations. Figure 3 includes an
RBS and an ICF output for the text plan in Fig-
ure 2. In this case ICF and RBS differ only in
the verb chosen as a more general verb during the
SOFT-MERGE operation.
We illustrate the RBS procedure with an ex-
ample for which ICF works similarly. For RBS,
the text plan in Figure 2 is ordered so that the re-
quest is first. For the request, a DSyntS is cho-
sen that can be paraphrased as What time would
you like to leave?. Then, the first implicit-confirm
is translated by lookup into a DSyntS which on
its own could generate Leaving in September.
We first try the ADJECTIVE aggregation opera-
tion, but since neither tree is a predicative ad-
jective, this fails. We then try the MERGE fam-
ily. MERGE-GENERAL succeeds, since the tree
for the request has an embedded node labeled
leave. The resulting DSyntS can be paraphrased
as What time would you like to leave in Septem-
ber?, and is attached to the new root node of
the resulting sp-tree. The root node is labeled
MERGE-GENERAL, and its two daughters are the
two speech acts. The implicit-confirm of the
day is added in a similar manner (adding an-
other left-branching node to the sp-tree), yielding
a DSyntS that can be paraphrased as What time
would you like to leave on September the 1st? (us-
ing some special-case attachment for dates within
MERGE). We now try and add the DSyntS for
the implicit-confirm, whose DSyntS might gener-
ate Going to Dallas. Here, we again cannot use
ADJECTIVE, nor can we use MERGE or MERGE-
GENERAL, since the verbs are not identical. In-
stead, we use SOFT-MERGE-GENERAL, which
identifies the leave node with the go root node of
the DSyntS of the implicit-confirm. When soft-
merging leave with go, fly is chosen as a general-
ization, resulting in a DSyntS that can be gener-
ated as What time would you like to fly on Septem-
ber the 1st to Dallas?. The sp-tree has added a
layer but is still left-branching. Finally, the last
implicit-confirm is added to yield a DSyntS that
is realized as What time would you like to fly on
September the 1st to Dallas from Newark?.
4 Experimental Results
All 60 subjects completed the experiment in a half
hour or less. The experiment resulted in a total
of 1200 judgements for each of the systems be-
ing compared, since each subject judged 20 ut-
terances by each system. We first discuss overall
differences among the different systems and then
make comparisons among the four different types
of systems: (1) TEMPLATE, (2) SPOT, (3) two
rule-based systems, and (4) two baseline systems.
All statistically significant results discussed here
had p values of less than .01.
We first examined whether differences in hu-
man ratings (score) were predictable from the
System Min Max Mean S.D.
TEMPLATE 1 5 3.9 1.1
SPoT 1 5 3.9 1.3
RBS 1 5 3.4 1.4
ICF 1 5 3.5 1.4
No Aggregation 1 5 3.0 1.2
Random 1 5 2.7 1.4
Figure 7: Summary of Overall Results for all Sys-
tems Evaluated
type of system that produced the utterance be-
ing rated. A one-way ANOVA with system as the
independent variable and score as the dependent
variable showed that there were significant differ-
ences in score as a function of system. The overall
differences are summarized in Figure 7.
As Figure 7 indicates, some system outputs re-
ceived more consistent scores than others, e.g.
the standard deviation for TEMPLATE was much
smaller than RANDOM. The ranking of the sys-
tems by average score is TEMPLATE, SPOT, ICF,
RBS, NOAGG, and RANDOM. Posthoc compar-
isons of the scores of individual pairs of systems
using the adjusted Bonferroni statistic revealed
several different groupings.2
The highest ranking systems were TEMPLATE
and SPOT, whose ratings were not statistically
significantly different from one another. This
shows that it is possible to match the quality of a
hand-crafted system with a trainable one, which
should be more portable, more general and re-
quire less overall engineering effort.
The next group of systems were the two rule-
based systems, ICF and RBS, which were not
statistically different from one another. However
SPOT was statistically better than both of these
systems (p   .01). Figure 8 shows that SPOT
got more high rankings than either of the rule-
based systems. In a sense this may not be that
surprising, because as Hovy and Wanner (1996)
point out, it is difficult to construct a rule-based
sentence planner that handles all the rule interac-
tions in a reasonable way. Features that SPoT?s
SPR uses allow SPOT to be sensitive to particular
discourse configurations or lexical collocations.
In order to encode these in a rule-based sentence
2The adjusted Bonferroni statistic guards against acci-
dentally finding differences between systems when making
multiple comparisons among systems.
planner, one would first have to discover these
constraints and then determine a way of enforc-
ing them. However the SPR simply learns that
a particular configuration is less preferred, result-
ing in a small decrement in ranking for the cor-
responding sp-tree. This flexibility of increment-
ing or decrementing a particular sp-tree by a small
amount may in the end allow it to be more sensi-
tive to small distinctions than a rule-based system.
Along with the TEMPLATE and RULE-BASED
systems, SPOT also scored better than the base-
line systems NOAGG and RANDOM. This is also
somewhat to be expected, since the baseline sys-
tems were intended to be the simplest systems
constructable. However it would have been a pos-
sible outcome for SPOT to not be different than
either system, e.g. if the sp-trees produced by
RANDOM were all equally good, or if the ag-
gregation rules that SPOT learned produced out-
put less readable than NOAGG. Figure 8 shows
that the distributions of scores for SPOT vs. the
baseline systems are very different, with SPOT
skewed towards higher scores.
Interestingly NOAGG also scored better than
RANDOM (p   .01), and the standard deviation
of its scores was smaller (see Figure 7). Remem-
ber that RANDOM?s sp-trees often resulted in ar-
bitrarily ordering the speech acts in the output.
While NOAGG produced redundant utterances, it
placed the initiative taking speech act at the end of
the utterance in its most natural position, possibly
resulting in a preference for NOAGG over RAN-
DOM. Another reason to prefer NOAGG could be
its predictability.
5 Discussion and Future Work
Other work has also explored automatically train-
ing modules of a generator (Langkilde and
Knight, 1998; Mellish et al, 1998; Walker, 2000).
However, to our knowledge, this is the first re-
ported experimental comparison of a trainable
technique that shows that the quality of system
utterances produced with trainable components
can compete with hand-crafted or rule-based tech-
niques. The results validate our methodology;
SPOT outperforms two representative rule-based
sentence planners, and performs as well as the
hand-crafted TEMPLATE system, but is more eas-
ily and quickly tuned to a new domain: the train-
ing materials for the SPOT sentence planner can
be collected from subjective judgements from a
small number of judges with little or no linguistic
knowledge.
Previous work on evaluation of natural lan-
guage generation has utilized three different ap-
proaches to evaluation (Mellish and Dale, 1998).
The first approach is a subjective evaluation
methodology such as we use here, where human
subjects rate NLG outputs produced by different
sources (Lester and Porter, 1997). Other work has
evaluated template-based spoken dialog genera-
tion with a task-based approach, i.e. the genera-
tor is evaluated with a metric such as task com-
pletion or user satisfaction after dialog comple-
tion (Walker, 2000). This approach can work
well when the task only involves one or two ex-
changes, when the choices have large effects over
the whole dialog, or the choices vary the con-
tent of the utterance. Because sentence plan-
ning choices realize the same content and only
affect the current utterance, we believed it impor-
tant to get local feedback. A final approach fo-
cuses on subproblems of natural language gener-
ation such as the generation of referring expres-
sions. For this type of problem it is possible to
evaluate the generator by the degree to which it
matches human performance (Yeh and Mellish,
1997). When evaluating sentence planning, this
approach doesn?t make sense because many dif-
ferent realizations may be equally good.
However, this experiment did not show that
trainable sentence planners produce, in general,
better-quality output than template-based or rule-
based sentence planners. That would be im-
possible: given the nature of template and rule-
based systems, any quality standard for the output
can be met given sufficient person-hours, elapsed
time, and software engineering acumen. Our prin-
cipal goal, rather, is to show that the quality of the
TEMPLATE output, for a currently operational dia-
log system whose template-based output compo-
nent was developed, expanded, and refined over
about 18 months, can be achieved using a train-
able system, for which the necessary training data
was collected in three person-days. Furthermore,
we wished to show that a representative rule-
based system based on current literature, without
massive domain-tuning, cannot achieve the same
1 1.5 2 2.5 3 3.5 4 4.5 5
0
200
400
600
800
1000
1200
Score
Nu
mb
er 
of 
pla
ns
 w
ith
 th
at 
sc
ore
 or
 m
ore
AMELIA
SPOT  
IC    
RB    
NOAGG 
RAN   
Figure 8: Chart comparing distribution of human ratings for SPOT, RBS, ICF, NOAGG and RANDOM.
level of quality. In future work, we hope to extend
SPoT and integrate it into AMELIA.
6 Acknowledments
This work was partially funded by DARPA under
contract MDA972-99-3-0003.
References
Y. Freund, R. Iyer, R. E. Schapire, and Y.Singer. 1998.
An efficient boosting algorithm for combining pref-
erences. In Machine Learning: Proc. of the Fif-
teenth International Conference.
E.H. Hovy and L. Wanner. 1996. Managing sentence
planning requirements. In Proc. of the ECAI?96
Workshop Gaps and Bridges: New Directions in
Planning and Natural Language Generation.
I. Langkilde and K. Knight. 1998. Generation that ex-
ploits corpus-based statistical knowledge. In Proc.
of COLING-ACL.
Benoit Lavoie and Owen Rambow. 1997. A fast and
portable realizer for text generation systems. In
Proc. of the Third Conference on Applied Natural
Language Processing, ANLP97, pages 265?268.
J. Lester and B. Porter. 1997. Developing and em-
pirically evaluating robust explanation generators:
The knight experiments. Computational Linguis-
tics, 23-1:65?103.
C. Mellish and R. Dale. 1998. Evaluation in the
context of natural language generation. Computer
Speech and Language, 12(3).
C. Mellish, A. Knott, J. Oberlander, and M.
O?Donnell. 1998. Experiments using stochas-
tic search for text planning. In Proc. of Interna-
tional Conference on Natural Language Genera-
tion, pages 97?108.
I. A. Melc?uk. 1988. Dependency Syntax: Theory and
Practice. SUNY, Albany, New York.
O. Rambow and T. Korelsky. 1992. Applied text
generation. In Proc. of the Third Conference on
Applied Natural Language Processing, ANLP92,
pages 40?47.
M. Walker, O. Rambow, and M. Rogati. 2001. Spot:
A trainable sentence planner. In Proc. of the North
American Meeting of the Association for Computa-
tional Linguistics.
M. A. Walker. 2000. An application of reinforcement
learning to dialogue strategy selection in a spoken
dialogue system for email. Journal of Artificial In-
telligence Research, 12:387?416.
C.L. Yeh and C. Mellish. 1997. An empirical study
on the generation of anaphora in chinese. Compu-
tational Linguistics, 23-1:169?190.
Quantitative and Qualitative Evaluation of Darpa Communicator
Spoken Dialogue Systems
Marilyn A. Walker
AT&T Labs ? Research
180 Park Ave, E103
Florham Park, NJ. 07932
walker@research.att.com
Rebecca Passonneau
AT&T Labs ?Research
180 Park Ave, D191
Florham Park, NJ. 07932
becky@research.att.com
Julie E. Boland
Institute of Cognitive Science
University of Louisiana at Lafayette
Lafayette, LA 70504
boland@louisiana.edu
Abstract
This paper describes the application of
the PARADISE evaluation framework
to the corpus of 662 human-computer
dialogues collected in the June 2000
Darpa Communicator data collection.
We describe results based on the stan-
dard logfile metrics as well as results
based on additional qualitative metrics
derived using the DATE dialogue act
tagging scheme. We show that per-
formance models derived via using the
standard metrics can account for 37%
of the variance in user satisfaction, and
that the addition of DATE metrics im-
proved the models by an absolute 5%.
1 Introduction
The objective of the DARPA COMMUNICATOR
program is to support research on multi-modal
speech-enabled dialogue systems with advanced
conversational capabilities. In order to make this
a reality, it is important to understand the con-
tribution of various techniques to users? willing-
ness and ability to use a spoken dialogue system.
In June of 2000, we conducted an exploratory
data collection experiment with nine participating
communicator systems. All systems supported
travel planning and utilized some form of mixed-
initiative interaction. However the systems var-
ied in several critical dimensions: (1) They tar-
geted different back-end databases for travel in-
formation; (2) System modules such as ASR,
NLU, TTS and dialogue management were typ-
ically different across systems.
The Evaluation Committee chaired by Walker
(Walker, 2000), with representatives from the
nine COMMUNICATOR sites and from NIST, de-
veloped the experimental design. A logfile stan-
dard was developed by MITRE along with a set
of tools for processing the logfiles (Aberdeen,
2000); the standard and tools were used by all
sites to collect a set of core metrics for making
cross system comparisons. The core metrics were
developed during a workshop of the Evaluation
Committee and included all metrics that anyone
in the committee suggested, that could be imple-
mented consistently across systems. NIST?s con-
tribution was to recruit the human subjects and to
implement the experimental design specified by
the Evaluation Committee.
The experiment was designed to make it possi-
ble to apply the PARADISE evaluation framework
(Walker et al, 2000), which integrates and unifies
previous approaches to evaluation (Price et al,
1992; Hirschman, 2000). The framework posits
that user satisfaction is the overall objective to be
maximized and that task success and various in-
teraction costs can be used as predictors of user
satisfaction. Our results from applying PARADISE
include that user satisfaction differed consider-
ably across the nine systems. Subsequent model-
ing of user satisfaction gave us some insight into
why each system was more or less satisfactory;
four variables accounted for 37% of the variance
in user-satisfaction: task completion, task dura-
tion, recognition accuracy, and mean system turn
duration.
However, when doing our analysis we were
struck by the extent to which different aspects of
the systems? dialogue behavior weren?t captured
by the core metrics. For example, the core met-
rics logged the number and duration of system
turns, but didn?t distinguish between turns used
to request or present information, to give instruc-
tions, or to indicate errors. Recent research on
dialogue has been based on the assumption that
dialogue acts provide a useful way of character-
izing dialogue behaviors (Reithinger and Maier,
1995; Isard and Carletta, 1995; Shriberg et al,
2000; Di Eugenio et al, 1998). Several research
efforts have explored the use of dialogue act tag-
ging schemes for tasks such as improving recog-
nition performance (Reithinger and Maier, 1995;
Shriberg et al, 2000), identifying important parts
of a dialogue (Finke et al, 1998), and as a con-
straint on nominal expression generation (Jordan,
2000). Thus we decided to explore the applica-
tion of a dialogue act tagging scheme to the task
of evaluating and comparing dialogue systems.
Section 2 describes the corpus. Section 3 de-
scribes the dialogue act tagging scheme we de-
veloped and applied to the evaluation of COM-
MUNICATOR dialogues. Section 4 first describes
our results utilizing the standard logged metrics,
and then describes results using the DATE met-
rics. Section 5 discusses future plans.
2 The Communicator 2000 Corpus
The corpus consists of 662 dialogues from nine
different travel planning systems with the num-
ber of dialogues per system ranging between 60
and 79. The experimental design is described
in (Walker et al, 2001). Each dialogue consists
of a recording, a logfile consistent with the stan-
dard, transcriptions and recordings of all user ut-
terances, and the output of a web-based user sur-
vey. Metrics collected per call included:
  Dialogue Efficiency: Task Duration, System turns,
User turns, Total Turns
  Dialogue Quality: Word Accuracy, Response latency,
Response latency variance
  Task Success: Exact Scenario Completion
  User Satisfaction: Sum of TTS performance, Task
ease, User expertise, Expected behavior, Future use.
The objective metrics focus on measures that
can be automatically logged or computed and a
web survey was used to calculate User Satisfac-
tion (Walker et al, 2001). A ternary definition
of task completion, Exact Scenario Completion
(ESC) was annotated by hand for each call by an-
notators at AT&T. The ESC metric distinguishes
between exact scenario completion (ESC), any
scenario completion (ANY) and no scenario com-
pletion (NOCOMP). This metric arose because
some callers completed an itinerary other than
the one assigned. This could have been due to
users? inattentiveness, e.g. users didn?t correct the
system when it had misunderstood them. In this
case, the system could be viewed as having done
the best that it could with the information that it
was given. This would argue that task completion
would be the sum of ESC and ANY. However,
examination of the dialogue transcripts suggested
that the ANY category sometimes arose as a ratio-
nal reaction by the caller to repeated recognition
error. Thus we decided to distinguish the cases
where the user completed the assigned task, ver-
sus completing some other task, versus the cases
where they hung up the phone without completing
any itinerary.
3 Dialogue Act Tagging for Evaluation
The hypothesis underlying the application of di-
alogue act tagging to system evaluation is that
a system?s dialogue behaviors have a strong ef-
fect on the usability of a spoken dialogue sys-
tem. However, each COMMUNICATOR system has
a unique dialogue strategy and a unique way of
achieving particular communicative goals. Thus,
in order to explore this hypothesis, we needed a
way of characterizing system dialogue behaviors
that could be applied uniformly across the nine
different communicator travel planning systems.
We developed a dialogue act tagging scheme for
this purpose which we call DATE (Dialogue Act
Tagging for Evaluation).
In developing DATE, we believed that it was
important to allow for multiple views of each
dialogue act. This would allow us, for ex-
ample, to investigate what part of the task an
utterance contributes to separately from what
speech act function it serves. Thus, a cen-
tral aspect of DATE is that it makes distinc-
tions within three orthogonal dimensions of ut-
terance classification: (1) a SPEECH-ACT dimen-
sion; (2) a TASK-SUBTASK dimension; and (3) a
CONVERSATIONAL-DOMAIN dimension. We be-
lieve that these distinctions are important for us-
ing such a scheme for evaluation. Figure 1 shows
a COMMUNICATOR dialogue with each system ut-
terance classified on these three dimensions. The
tagset for each dimension are briefly described in
the remainder of this section. See (Walker and
Passonneau, 2001) for more detail.
3.1 Speech Acts
In DATE, the SPEECH-ACT dimension has ten cat-
egories. We use familiar speech-act labels, such
as OFFER, REQUEST-INFO, PRESENT-INFO, AC-
KNOWLEDGE, and introduce new ones designed
to help us capture generalizations about commu-
nicative behavior in this domain, on this task,
given the range of system and human behavior
we see in the data. One new one, for example,
is STATUS-REPORT. Examples of each speech-act
type are in Figure 2.
Speech-Act Example
REQUEST-INFO And, what city are you flying to?
PRESENT-INFO The airfare for this trip is 390 dol-
lars.
OFFER Would you like me to hold this op-
tion?
ACKNOWLEDGE I will book this leg.
STATUS-REPORT Accessing the database; this
might take a few seconds.
EXPLICIT-
CONFIRM
You will depart on September 1st.
Is that correct?
IMPLICIT-
CONFIRM
Leaving from Dallas.
INSTRUCTION Try saying a short sentence.
APOLOGY Sorry, I didn?t understand that.
OPENING/CLOSING Hello. Welcome to the C M U
Communicator.
Figure 2: Example Speech Acts
3.2 Conversational Domains
The CONVERSATIONAL-DOMAIN dimension in-
volves the domain of discourse that an utterance
is about. Each speech act can occur in any of three
domains of discourse described below.
The ABOUT-TASK domain is necessary for
evaluating a dialogue system?s ability to collab-
orate with a speaker on achieving the task goal of
making reservations for a specific trip. It supports
metrics such as the amount of time/effort the sys-
tem takes to complete a particular phase of mak-
ing an airline reservation, and any ancillary ho-
tel/car reservations.
The ABOUT-COMMUNICATION domain re-
flects the system goal of managing the verbal
channel and providing evidence of what has been
understood (Walker, 1992; Clark and Schaefer,
1989). Utterances of this type are frequent in
human-computer dialogue, where they are moti-
vated by the need to avoid potentially costly er-
rors arising from imperfect speech recognition.
All implicit and explicit confirmations are about
communication; See Figure 1 for examples.
The SITUATION-FRAME domain pertains to the
goal of managing the culturally relevant framing
expectations (Goffman, 1974). The utterances in
this domain are particularly relevant in human-
computer dialogues because the users? expecta-
tions need to be defined during the course of the
conversation. About frame utterances by the sys-
tem attempt to help the user understand how to in-
teract with the system, what it knows about, and
what it can do. Some examples are in Figure 1.
3.3 Task Model
The TASK-SUBTASK dimension refers to a task
model of the domain task that the system sup-
ports and captures distinctions among dialogue
acts that reflect the task structure.1 The motiva-
tion for this dimension is to derive metrics that
quantify the effort expended on particular sub-
tasks.
This dimension distinguishes among 14 sub-
tasks, some of which can also be grouped at
a level below the top level task.2, as described
in Figure 3. The TOP-LEVEL-TRIP task de-
scribes the task which contains as its subtasks the
ORIGIN, DESTINATION, DATE, TIME, AIRLINE,
TRIP-TYPE, RETRIEVAL and ITINERARY tasks.
The GROUND task includes both the HOTEL and
CAR subtasks.
Note that any subtask can involve multiple
speech acts. For example, the DATE subtask can
consist of acts requesting, or implicitly or explic-
itly confirming the date. A similar example is pro-
vided by the subtasks of CAR (rental) and HOTEL,
which include dialogue acts requesting, confirm-
ing or acknowledging arrangements to rent a car
or book a hotel room on the same trip.
1This dimension elaborates of each speech-act type in
other tagging schemes (Reithinger and Maier, 1995).
2In (Walker and Passonneau, 2001) we didn?t distinguish
the price subtask from the itinerary presentation subtask.
Task Example
TOP-LEVEL-
TRIP
What are your travel plans?
ORIGIN And, what city are you leaving from?
DESTINATION And, where are you flying to?
DATE What day would you like to leave?
TIME Departing at what time?.
AIRLINE Did you have an airline preference?
TRIP-TYPE Will you return to Boston from San Jose?
RETRIEVAL Accessing the database; this might take
a few seconds.
ITINERARY I found 3 flights from Miami to Min-
neapolis.
PRICE The airfare for this trip is 390 dollars.
GROUND Did you need to make any ground ar-
rangements?.
HOTEL Would you like a hotel near downtown
or near the airport?.
CAR Do you need a car in San Jose?
Figure 3: Example Utterances for each Subtask
3.4 Implementation and Metrics Derivation
We implemented a dialogue act parser that clas-
sifies each of the system utterances in each dia-
logue in the COMMUNICATOR corpus. Because
the systems used template-based generation and
had only a limited number of ways of saying the
same content, it was possible to achieve 100% ac-
curacy with a parser that tags utterances automat-
ically from a database of patterns and the corre-
sponding relevant tags from each dimension.
A summarizer program then examined each di-
alogue?s labels and summed the total effort ex-
pended on each type of dialogue act over the
dialogue or the percentage of a dialogue given
over to a particular type of dialogue behavior.
These sums and percentages of effort were calcu-
lated along the different dimensions of the tagging
scheme as we explain in more detail below.
We believed that the top level distinction be-
tween different domains of action might be rel-
evant so we calculated percentages of the to-
tal dialogue expended in each conversational do-
main, resulting in metrics of TaskP, FrameP and
CommP (the percentage of the dialogue devoted
to the task, the frame or the communication do-
mains respectively).
We were also interested in identifying differ-
ences in effort expended on different subtasks.
The effort expended on each subtask is repre-
sented by the sum of the length of the utterances
contributing to that subtask. These are the met-
rics: TripC, OrigC, DestC, DateC, TimeC, Air-
lineC, RetrievalC, FlightinfoC, PriceC, GroundC,
BookingC. See Figure 3.
We were particularly interested developing
metrics related to differences in the system?s di-
alogue strategies. One difference that the DATE
scheme can partially capture is differences in con-
firmation strategy by summing the explicit and
implicit confirms. This introduces two metrics
ECon and ICon, which represent the total effort
spent on these two types of confirmation.
Another strategy difference is in the types of
about frame information that the systems pro-
vide. The metric CINSTRUCT counts instances
of instructions, CREQAMB counts descriptions
provided of what the system knows about in the
context of an ambiguity, and CNOINFO counts
the system?s descriptions of what it doesn?t know
about. SITINFO counts dialogue initial descrip-
tions of the system?s capabilities and instructions
for how to interact with the system
A final type of dialogue behavior that the
scheme captures are apologies for misunderstand-
ing (CREJECT), acknowledgements of user re-
quests to start over (SOVER) and acknowledg-
ments of user corrections of the system?s under-
standing (ACOR).
We believe that it should be possible to use
DATE to capture differences in initiative strate-
gies, but currently only capture differences at the
task level using the task metrics above. The TripC
metric counts open ended questions about the
user?s travel plans, whereas other subtasks typi-
cally include very direct requests for information
needed to complete a subtask.
We also counted triples identifying dialogue
acts used in specific situations, e.g. the utterance
Great! I am adding this flight to your itinerary
is the speech act of acknowledge, in the about-
task domain, contributing to the booking subtask.
This combination is the ACKBOOKING metric.
We also keep track of metrics for dialogue acts
of acknowledging a rental car booking or a hotel
booking, and requesting, presenting or confirm-
ing particular items of task information. Below
we describe dialogue act triples that are signifi-
cant predictors of user satisfaction.
Metric Coefficient P value
ESC 0.45 0.000
TaskDur -0.15 0.000
Sys Turn Dur 0.12 0.000
Wrd Acc 0.17 0.000
Table 1: Predictive power and significance of
Core Metrics
4 Results
We initially examined differences in cumulative
user satisfaction across the nine systems. An
ANOVA for user satisfaction by Site ID using the
modified Bonferroni statistic for multiple com-
parisons showed that there were statistically sig-
nificant differences across sites, and that there
were four groups of performers with sites 3,2,1,4
in the top group (listed by average user satisfac-
tion), sites 4,5,9,6 in a second group, and sites 8
and 7 defining a third and a fourth group. See
(Walker et al, 2001) for more detail on cross-
system comparisons.
However, our primary goal was to achieve a
better understanding of the role of qualitative as-
pects of each system?s dialogue behavior. We
quantify the extent to which the dialogue act
metrics improve our understanding by applying
the PARADISE framework to develop a model of
user satisfaction and then examining the extent
to which the dialogue act metrics improve the
model (Walker et al, 2000). Section 4.1 describes
the PARADISE models developed using the core
metrics and section 4.2 describes the models de-
rived from adding in the DATE metrics.
4.1 Results using Logfile Standard Metrics
We applied PARADISE to develop models of user
satisfaction using the core metrics; the best model
fit accounts for 37% of the variance in user sat-
isfaction. The learned model is that User Sat-
isfaction is the sum of Exact Scenario Comple-
tion, Task Duration, System Turn Duration and
Word Accuracy. Table 1 gives the details of the
model, where the coefficient indicates both the
magnitude and whether the metric is a positive or
negative predictor of user satisfaction, and the P
value indicates the significance of the metric in
the model.
The finding that metrics of task completion and
Metric Coefficient P value
ESC (Completion) 0.40 0.00
Task Dur -0.31 0.00
Sys Turn Dur 0.14 0.00
Word Accuracy 0.15 0.00
TripC 0.09 0.01
BookingC 0.08 0.03
PriceC 0.11 0.00
AckRent 0.07 0.05
EconTime 0.05 0.13
ReqDate 0.10 0.01
ReqTripType 0.09 0.00
Econ 0.11 0.01
Table 2: Predictive power and significance of Di-
alogue Act Metrics
recognition performance are significant predic-
tors duplicates results from other experiments ap-
plying PARADISE (Walker et al, 2000). The fact
that task duration is also a significant predictor
may indicate larger differences in task duration in
this corpus than in previous studies.
Note that the PARADISE model indicates that
system turn duration is positively correlated with
user satisfaction. We believed it plausible that this
was due to the fact that flight presentation utter-
ances are longer than other system turns. Thus
this metric simply captures whether or not the sys-
tem got enough information to present some po-
tential flight itineraries to the user. We investigate
this hypothesis further below.
4.2 Utilizing Dialogue Parser Metrics
Next, we add in the dialogue act metrics extracted
by our dialogue parser, and retrain our models of
user satisfaction. We find that many of the dia-
logue act metrics are significant predictors of user
satisfaction, and that the model fit for user sat-
isfaction increases from 37% to 42%. The dia-
logue act metrics which are significant predictors
of user satisfaction are detailed in Table 2.
When we examine this model, we note that sev-
eral of the significant dialogue act metrics are cal-
culated along the task-subtask dimension, namely
TripC, BookingC and PriceC. One interpretation
of these metrics are that they are acting as land-
marks in the dialogue for having achieved a par-
ticular set of subtasks. The TripC metric can
be interpreted this way because it includes open
ended questions about the user?s travel plans both
at the beginning of the dialogue and also after
one itinerary has been planned. Other signif-
icant metrics can also be interpreted this way;
for example the ReqDate metric counts utterances
such as Could you tell me what date you wanna
travel? which are typically only produced after
the origin and the destination have been under-
stood. The ReqTripType metric counts utterances
such as From Boston, are you returning to Dal-
las? which are only asked after all the first infor-
mation for the first leg of the trip have been ac-
quired, and in some cases, after this information
has been confirmed. The AckRental metric has a
similar potential interpretation; the car rental task
isn?t attempted until after the flight itinerary has
been accepted by the caller. However, the predic-
tors for the models already include a ternary exact
scenario completion metric (ESC) which speci-
fies whether any task was achieved or not, and
whether the exact task that the user was attempt-
ing to accomplish was achieved. The fact that the
addition of these dialogue metrics improves the fit
of the user satisfaction model suggests that per-
haps a finer grained distinction on how many of
the subtasks of a dialogue were completed is re-
lated to user satisfaction. This makes sense; a user
who the system hung up on immediately should
be less satisfied than one who never could get the
system to understand his destination, and both of
these should be less satisfied than a user who was
able to communicate a complete travel plan but
still did not complete the task.
Other support for the task completion related
nature of some of the significant metrics is that
the coefficient for ESC is smaller in the model
in Table 2 than in the model in Table 1. Note
also that the coefficient for Task Duration is much
larger. If some of the dialogue act metrics that are
significant predictors are mainly so because they
indicate the successful accomplishment of partic-
ular subtasks, then both of these changes would
make sense. Task Duration can be a greater nega-
tive predictor of user satisfaction, only when it is
counteracted by the positive coefficients for sub-
task completion.
The TripC and the PriceC metrics also have
other interpretations. The positive contribution of
the TripC metric to user satisfaction could arise
from a user?s positive response to systems with
open-ended initial greetings which give the user
the initiative. The positive contribution of the
PriceC metric might indicate the users? positive
response to getting price information, since not
all systems provided price information.
As mentioned above, our goal was to de-
velop metrics that captured differences in dia-
logue strategies. The positive coefficient of the
Econ metric appears to indicate that an explicit
confirmation strategy overall leads to greater user
satisfaction than an implicit confirmation strategy.
This result is interesting, although it is unclear
how general it is. The systems that used an ex-
plicit confirmation strategy did not use it to con-
firm each item of information; rather the strategy
seemed to be to acquire enough information to go
to the database and then confirm all of the param-
eters before accessing the database. The other use
of explicit confirms was when a system believed
that it had repeatedly misunderstood the user.
We also explored the hypothesis that the rea-
son that system turn duration was a predictor of
user satisfaction is that longer turns were used
to present flight information. We removed sys-
tem turn duration from the model, to determine
whether FlightInfoC would become a significant
predictor. However the model fit decreased and
FlightInfoC was not a significant predictor. Thus
it is unclear to us why longer system turn dura-
tions are a significant positive predictor of user
satisfaction.
5 Discussion and Future Work
We showed above that the addition of dialogue act
metrics improves the fit of models of user satis-
faction from 37% to 42%. Many of the significant
dialogue act metrics can be viewed as landmarks
in the dialogue for having achieved particular sub-
tasks. These results suggest that a careful defi-
nition of transaction success, based on automatic
analysis of events in a dialogue, such as acknowl-
edging a booking, might serve as a substitute for
the hand-labelling of task completion.
In current work we are exploring the use of tree
models and boosting for modeling user satisfac-
tion. Tree models using dialogue act metrics can
achieve model fits as high as 48% reduction in
error. However, we need to test both these mod-
els and the linear PARADISE models on unseen
data. Furthermore, we intend to explore methods
for deriving additional metrics from dialogue act
tags. In particular, it is possible that sequential or
structural metrics based on particular sequences
or configurations of dialogue acts might capture
differences in dialogue strategies.
We began a second data collection of dialogues
with COMMUNICATOR travel systems in April
2001. In this data collection, the subject pool will
use the systems to plan real trips that they intend
to take. As part of this data collection, we hope
to develop additional metrics related to the qual-
ity of the dialogue, how much initiative the user
can take, and the quality of the solution that the
system presents to the user.
6 Acknowledgements
This work was supported under DARPA GRANT
MDA 972 99 3 0003 to AT&T Labs Research.
Thanks to the evaluation committee members:
J. Aberdeen, E. Bratt, J. Garofolo, L. Hirschman,
A. Le, S. Narayanan, K. Papineni, B. Pellom,
A. Potamianos, A. Rudnicky, G. Sanders, S. Sen-
eff, and D. Stallard who contributed to 2000
COMMUNICATOR data collection.
References
John Aberdeen. 2000. Darpa communicator logfile
standard. http://fofoca.mitre.org/logstandard.
Herbert H. Clark and Edward F. Schaefer. 1989. Con-
tributing to discourse. Cognitive Science, 13:259?
294.
Barbara Di Eugenio, Pamela W. Jordan, Johanna D.
Moore, and Richmond H. Thomason. 1998. An
empirical investigation of collaborative dialogues.
In ACL-COLING98, Proc. of the 36th Conference
of the Association for Computational Linguistics.
M. Finke, M. Lapata, A. Lavie, L. Levin, L. May-
field Tomokiyo, T. Polzin, K. Ries, A. Waibel, and
K. Zechner. 1998. Clarity: Inferring discourse
structure from speech. In AAAI Symposium on
Applying Machine Learning to Discourse Process-
ing.
Erving Goffman. 1974. Frame Analysis: An Essay on
the Organization of Experience. Harper and Row,
New York.
Lynette Hirschman. 2000. Evaluating spoken lan-
guage interaction: Experiences from the darpa spo-
ken language program 1990?1995. In S. Luperfoy,
editor, Spoken Language Discourse. MIT Press,
Cambridge, Mass.
Amy Isard and Jean C. Carletta. 1995. Replicabil-
ity of transaction and action coding in the map task
corpus. In AAAI Spring Symposium: Empirical
Methods in Discourse Interpretation and Genera-
tion, pages 60?67.
Pamela W. Jordan. 2000. Intentional Influences on
Object Redescriptions in Dialogue: Evidence from
an Empirical Study. Ph.D. thesis, Intelligent Sys-
tems Program, University of Pittsburgh.
Patti Price, Lynette Hirschman, Elizabeth Shriberg,
and Elizabeth Wade. 1992. Subject-based evalu-
ation measures for interactive spoken language sys-
tems. In Proc. of the DARPA Speech and NL Work-
shop, pages 34?39.
Norbert Reithinger and Elisabeth Maier. 1995. Utiliz-
ing statistical speech act processing in verbmobil.
In ACL 95.
E. Shriberg, P. Taylor, R. Bates, A. Stolcke, K. Ries,
D. Jurafsky, N. Coccaro, R. Martin, M. Meteer, and
C. Van Ess-Dykema. 2000. Can prosody aid the
automatic classification of dialog acts in conversa-
tional speech. Language and Speech: Special Issue
on Prosody and Conversation.
M. Walker and R. Passonneau. 2001. Date: A dia-
logue act tagging scheme for evaluation. In Human
Language Technology Conference.
Marilyn A. Walker, Candace A. Kamm, and Diane J.
Litman. 2000. Towards developing general models
of usability with PARADISE. Natural Language
Engineering: Special Issue on Best Practice in Spo-
ken Dialogue Systems.
M. Walker, J. Aberdeen, J. Boland, E. Bratt, J. Garo-
folo, L. Hirschman, A. Le, S. Lee, S. Narayanan,
K. Papineni, B. Pellom, J. Polifroni, A. Potami-
anos, P. Prabhu, A. Rudnicky, G. Sanders, S. Sen-
eff, D. Stallard, and S. Whittaker. 2001. Darpa
communicator dialog travel planning systems: The
june 2000 data collection. In Submitted to EU-
ROSPEECH 2001.
Marilyn A. Walker. 1992. Redundancy in collabora-
tive dialogue. In Fourteenth International Confer-
ence on Computational Linguistics, pages 345?351.
Marilyn Walker. 2000. Communi-
cator evaluation committee webpage.
http://www.research.att.com/ walker/eval/eval.html.
Speaker Utterance Speech Act Task C-Domain
SYS Welcome. opening/closing nil task
You are logged in as a guest user. instruct nil frame
You may say repeat, help me out, start over, or, that?s wrong, instruct nil frame
you can also correct and interrupt the system at any time. instruct nil frame
What airport woodja like to fly out of? request-info origin task
USER MIAMI FLORIDA
SYS Leaving from Miami, implicit-conf origin communication
And, what city are you flying to? request-info dest task
USER MINNEAPOLIS MINNESOTA
SYS Flying from Miami to Minneapolis, implicit-conf origin, dest communication
What date would you like to fly? request-info date task
USER UH MONDAY OCTOBER TWENTY THIRD
SYS Leaving on the 23rd of October, implicit-conf date communication
And what time didja wanna leave? request-info time task
USER UH LATE MORNING
SYS From Minneapolis. implicit-conf origin communication
Are you returning to Miami? request-info trip-type task
USER UM
SYS Are you going back to Miami? request-info trip-type task
Please say yes or no. instruct nil frame
USER NO
SYS Okay, it will just take a moment while I look. status-report retrieval task
SYS I found 3 flights from Miami to Minneapolis on October 23rd .
For option one I have a flight with United departing at 10 A M
and arriving at 3 P M with one stopover and
present-info itinerary task
Ticket price is 390 dollars. present-info price task
Please say next option, or, flight details, or, I?ll take it. offer itinerary task
USER NEXT OPTION
SYS For option two I have a flight with Delta departing at 12 and
arriving at 4 P M non-stop.
present-info itinerary task
Ticket price is 450 dollars. present-info price task
Would you like me to hold this option? offer itinerary task
USER NEXT OPTION
SYS For the last option I have a flight with Northwest departing at 2
P M and arriving at 7 P M with 1 stopover and
present-info itinerary task
Ticket price is 470 dollars. present-info price task
say next option, or, flight details, or, I?ll take it. offer itinerary task
USER NEXT OPTION
SYS Here is your third option again. a flight with Northwest departing
at 2 P M and arriving at 7 P M with 1 stopover and
present-info itinerary task
Ticket price is 470 dollars. present-info price task
say next option, or, flight details, or, I?ll take it. offer itinerary task
USER I?LL TAKE IT
SYS Great! I am adding this flight to your itinerary. acknowledge booking task
Okay, Is Minneapolis your final destination? request-info trip-type task
USER YES IT IS
SYS Would you like to make any ground arrangements? offer ground task
USER NO
SYS Now, would you like to hear a summary of your itinerary? offer itinerary task
USER YES PLEASE
SYS Okay, Here?s your itinerary. On October 23rd Northwest Flight
123 departs Miami at 2 P M, arrives Pittsburgh at 4 30 P M con-
necting to Northwest Flight 146 that departs Pittsburgh at 5 P M
and arrives Minneapolis at 7 P M.
present-info itinerary task
Figure 1: Dialogue Illustrating the Speech Act, Task-Subtask and Conversational Domain Dimensions
of DATE
MATCH: An Architecture for Multimodal Dialogue Systems
Michael Johnston, Srinivas Bangalore, Gunaranjan Vasireddy, Amanda Stent
Patrick Ehlen, Marilyn Walker, Steve Whittaker, Preetam Maloor
AT&T Labs - Research, 180 Park Ave, Florham Park, NJ 07932, USA
johnston,srini,guna,ehlen,walker,stevew,pmaloor@research.att.com
Now at SUNY Stonybrook, stent@cs.sunysb.edu
Abstract
Mobile interfaces need to allow the user
and system to adapt their choice of com-
munication modes according to user pref-
erences, the task at hand, and the physi-
cal and social environment. We describe a
multimodal application architecture which
combines finite-state multimodal language
processing, a speech-act based multimodal
dialogue manager, dynamic multimodal
output generation, and user-tailored text
planning to enable rapid prototyping of
multimodal interfaces with flexible input
and adaptive output. Our testbed appli-
cation MATCH (Multimodal Access To
City Help) provides a mobile multimodal
speech-pen interface to restaurant and sub-
way information for New York City.
1 Multimodal Mobile Information Access
In urban environments tourists and residents alike
need access to a complex and constantly changing
body of information regarding restaurants, theatre
schedules, transportation topology and timetables.
This information is most valuable if it can be de-
livered effectively while mobile, since places close
and plans change. Mobile information access devices
(PDAs, tablet PCs, next-generation phones) offer
limited screen real estate and no keyboard or mouse,
making complex graphical interfaces cumbersome.
Multimodal interfaces can address this problem by
enabling speech and pen input and output combining
speech and graphics (See (Andre?, 2002) for a detailed
overview of previous work on multimodal input and
output). Since mobile devices are used in different
physical and social environments, for different tasks,
by different users, they need to be both flexible in in-
put and adaptive in output. Users need to be able to
provide input in whichever mode or combination of
modes is most appropriate, and system output should
be dynamically tailored so that it is maximally effec-
tive given the situation and the user?s preferences.
We present our testbed multimodal application
MATCH (Multimodal Access To City Help) and the
general purpose multimodal architecture underlying
it, that: is designed for highly mobile applications;
enables flexible multimodal input; and provides flex-
ible user-tailored multimodal output.
Figure 1: MATCH running on Fujitsu PDA
Highly mobile MATCH is a working city guide
and navigation system that currently enables mobile
users to access restaurant and subway information for
New York City (NYC). MATCH runs standalone on
a Fujitsu pen computer (Figure 1), and can also run
in client-server mode across a wireless network.
Flexible multimodal input Users interact with a
graphical interface displaying restaurant listings and
a dynamic map showing locations and street infor-
mation. They are free to provide input using speech,
by drawing on the display with a stylus, or by us-
ing synchronous multimodal combinations of the two
modes. For example, a user might ask to see cheap
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 376-383.
                         Proceedings of the 40th Annual Meeting of the Association for
Italian restaurants in Chelsea by saying show cheap
italian restaurants in chelsea, by circling an area on
the map and saying show cheap italian restaurants
in this neighborhood; or, in a noisy or public envi-
ronment, by circling an area and writing cheap and
italian (Figure 2). The system will then zoom to the
appropriate map location and show the locations of
restaurants on the map. Users can ask for information
about restaurants, such as phone numbers, addresses,
and reviews. For example, a user might circle three
restaurants as in Figure 3 and say phone numbers for
these three restaurants (or write phone). Users can
also manipulate the map interface directly. For exam-
ple, a user might say show upper west side or circle
an area and write zoom.
Figure 2: Unimodal pen command
Flexible multimodal output MATCH provides
flexible, synchronized multimodal generation and
can take initiative to engage in information-seeking
subdialogues. If a user circles the three restaurants in
Figure 3 and writes phone, the system responds with
a graphical callout on the display, synchronized with
a text-to-speech (TTS) prompt of the phone number,
for each restaurant in turn (Figure 4).
Figure 3: Two area gestures
Figure 4: Phone query callouts
The system also provides subway directions. If the
user says How do I get to this place? and circles one
of the restaurants displayed on the map, the system
will ask Where do you want to go from? The user
can then respond with speech (e.g., 25th Street and
3rd Avenue), with pen by writing (e.g., 25th St & 3rd
Ave), or multimodally ( e.g, from here with a circle
gesture indicating location). The system then calcu-
lates the optimal subway route and dynamically gen-
erates a multimodal presentation of instructions. It
starts by zooming in on the first station and then grad-
ually zooms out, graphically presenting each stage of
the route along with a series of synchronized TTS
prompts. Figure 5 shows the final display of a sub-
way route heading downtown on the 6 train and trans-
ferring to the L train Brooklyn bound.
Figure 5: Multimodal subway route
User-tailored generation MATCH can also pro-
vide a user-tailored summary, comparison, or rec-
ommendation for an arbitrary set of restaurants, us-
ing a quantitative model of user preferences (Walker
et al, 2002). The system will only discuss restau-
rants that rank highly according to the user?s dining
preferences, and will only describe attributes of those
restaurants the user considers important. This per-
mits concise, targeted system responses. For exam-
ple, the user could say compare these restaurants and
circle a large set of restaurants (Figure 6). If the user
considers inexpensiveness and food quality to be the
most important attributes of a restaurant, the system
response might be:
Compare-A: Among the selected restaurants, the following
offer exceptional overall value. Uguale?s price is 33 dollars. It
has excellent food quality and good decor. Da Andrea?s price is
28 dollars. It has very good food quality and good decor. John?s
Pizzeria?s price is 20 dollars. It has very good food quality and
mediocre decor.
Figure 6: Comparing a large set of restaurants
2 Multimodal Application Architecture
The multimodal architecture supporting MATCH
consists of a series of agents which communicate
through a facilitator MCUBE (Figure 7).
Figure 7: Multimodal Architecture
MCUBE is a Java-based facilitator which enables
agents to pass messages either to single agents or
groups of agents. It serves a similar function to sys-
tems such as OAA (Martin et al, 1999), the use of
KQML for messaging in Allen et al(2000), and the
Communicator hub (Seneff et al, 1998). Agents may
reside either on the client device or elsewhere on the
network and can be implemented in multiple differ-
ent languages. MCUBE messages are encoded in
XML, providing a general mechanism for message
parsing and facilitating logging.
Multimodal User Interface Users interact with
the system through the Multimodal UI, which is
browser-based and runs in Internet Explorer. This
greatly facilitates rapid prototyping, authoring, and
reuse of the system for different applications since
anything that can appear on a webpage (dynamic
HTML, ActiveX controls, etc.) can be used in
the visual component of a multimodal user inter-
face. A TCP/IP control enables communication with
MCUBE.
MATCH uses a control that provides a dynamic
pan-able, zoomable map display. The control has ink
handling capability. This enables both pen-based in-
teraction (on the map) and normal GUI interaction
(on the rest of the page) without requiring the user to
overtly switch ?modes?. When the user draws on the
map their ink is captured and any objects potentially
selected, such as currently displayed restaurants, are
identified. The electronic ink is broken into a lat-
tice of strokes and sent to the gesture recognition
and handwriting recognition components which en-
rich this stroke lattice with possible classifications of
strokes and stroke combinations. The UI then trans-
lates this stroke lattice into an ink meaning lattice
representing all of the possible interpretations of the
user?s ink and sends it to MMFST.
In order to provide spoken input the user must tap
a click-to-speak button on the Multimodal UI. We
found that in an application such as MATCH which
provides extensive unimodal pen-based interaction, it
is preferable to use click-to-speak rather than pen-
to-speak or open-mike. With pen-to-speak, spurious
speech results received in noisy environments can
disrupt unimodal pen commands.
The Multimodal UI also provides graphical output
capabilities and performs synchronization of multi-
modal output. For example, it synchronizes the dis-
play actions and TTS prompts in the answer to the
route query mentioned in Section 1.
Speech Recognition MATCH uses AT&T?s Wat-
son speech recognition engine. A speech manager
running on the device gathers audio and communi-
cates with a recognition server running either on the
device or on the network. The recognition server pro-
vides word lattice output which is passed to MMFST.
Gesture and handwriting recognition Gesture
and handwriting recognition agents provide possible
classifications of electronic ink for the UI. Recogni-
tions are performed both on individual strokes and
combinations of strokes in the input ink lattice. The
handwriting recognizer supports a vocabulary of 285
words, including attributes of restaurants (e.g. ?chi-
nese?,?cheap?) and zones and points of interest (e.g.
?soho?,?empire?,?state?,?building?). The gesture rec-
ognizer recognizes a set of 10 basic gestures, includ-
ing lines, arrows, areas, points, and question marks.
It uses a variant of Rubine?s classic template-based
gesture recognition algorithm (Rubine, 1991) trained
on a corpus of sample gestures. In addition to classi-
fying gestures the gesture recognition agent also ex-
tracts features such as the base and head of arrows.
Combinations of this basic set of gestures and hand-
written words provide a rich visual vocabulary for
multimodal and pen-based commands.
Gestures are represented in the ink meaning lat-
tice as symbol complexes of the following form: G
FORM MEANING (NUMBER TYPE) SEM. FORM
indicates the physical form of the gesture and has val-
ues such as area, point, line, arrow. MEANING indi-
cates the meaning of that form; for example an area
can be either a loc(ation) or a sel(ection). NUMBER
and TYPE indicate the number of entities in a selec-
tion (1,2,3, many) and their type (rest(aurant), the-
atre). SEM is a place holder for the specific content
of the gesture, such as the points that make up an area
or the identifiers of objects in a selection.
When multiple selection gestures are present
an aggregation technique (Johnston and Bangalore,
2001) is employed to overcome the problems with
deictic plurals and numerals described in John-
ston (2000). Aggregation augments the ink meaning
lattice with aggregate gestures that result from com-
bining adjacent selection gestures. This allows a de-
ictic expression like these three restaurants to com-
bine with two area gestures, one which selects one
restaurant and the other two, as long as their sum is
three. For example, if the user makes two area ges-
tures, one around a single restaurant and the other
around two restaurants (Figure 3), the resulting ink
meaning lattice will be as in Figure 8. The first ges-
ture (node numbers 0-7) is either a reference to a
location (loc.) (0-3,7) or a reference to a restaurant
(sel.) (0-2,4-7). The second (nodes 7-13,16) is either
a reference to a location (7-10,16) or to a set of two
restaurants (7-9,11-13,16). The aggregation process
applies to the two adjacent selections and adds a se-
lection of three restaurants (0-2,4,14-16). If the user
says show chinese restaurants in this neighborhood
and this neighborhood, the path containing the two
locations (0-3,7-10,16) will be taken when this lat-
tice is combined with speech in MMFST. If the user
says tell me about this place and these places, then
the path with the adjacent selections is taken (0-2,4-
9,11-13,16). If the speech is tell me about these or
phone numbers for these three restaurants then the
aggregate path (0-2,4,14-16) will be chosen.
Multimodal Integrator (MMFST) MMFST re-
ceives the speech lattice (from the Speech Manager)
and the ink meaning lattice (from the UI) and builds
a multimodal meaning lattice which captures the po-
tential joint interpretations of the speech and ink in-
puts. MMFST is able to provide rapid response times
by making unimodal timeouts conditional on activity
in the other input mode. MMFST is notified when the
user has hit the click-to-speak button, when a speech
result arrives, and whether or not the user is inking on
the display. When a speech lattice arrives, if inking
is in progress MMFST waits for the ink meaning lat-
tice, otherwise it applies a short timeout (1 sec.) and
treats the speech as unimodal. When an ink meaning
lattice arrives, if the user has tapped click-to-speak
MMFST waits for the speech lattice to arrive, other-
wise it applies a short timeout (1 sec.) and treats the
ink as unimodal.
MMFST uses the finite-state approach to multi-
modal integration and understanding proposed by
Johnston and Bangalore (2000). Possibilities for
multimodal integration and understanding are cap-
tured in a three tape device in which the first tape
represents the speech stream (words), the second the
ink stream (gesture symbols) and the third their com-
bined meaning (meaning symbols). In essence, this
device takes the speech and ink meaning lattices as
inputs, consumes them using the first two tapes, and
writes out a multimodal meaning lattice using the
third tape. The three tape finite-state device is sim-
ulated using two transducers: G:W which is used to
align speech and ink and G W:M which takes a com-
posite alphabet of speech and gesture symbols as in-
put and outputs meaning. The ink meaning lattice
G and speech lattice W are composed with G:W and
the result is factored into an FSA G W which is com-
posed with G W:M to derive the meaning lattice M.
In order to capture multimodal integration using
finite-state methods, it is necessary to abstract over
specific aspects of gestural content (Johnston and
Bangalore, 2000). For example, all possible se-
quences of coordinates that could occur in an area
gesture cannot be encoded in the finite-state device.
We employ the approach proposed in (Johnston and
Bangalore, 2001) in which the ink meaning lattice is
converted to a transducer I:G, where G are gesture
symbols (including SEM) and I contains both gesture
symbols and the specific contents. I and G differ only
in cases where the gesture symbol on G is SEM, in
which case the corresponding I symbol is the specific
interpretation. After multimodal integration a pro-
jection G:M is taken from the result G W:M machine
and composed with the original I:G in order to rein-
corporate the specific contents that were left out of
the finite-state process (I:G o G:M = I:M).
The multimodal finite-state transducers used at
runtime are compiled from a declarative multimodal
context-free grammar which captures the structure
Figure 8: Ink Meaning Lattice
and interpretation of multimodal and unimodal com-
mands, approximated where necessary using stan-
dard approximation techniques (Nederhof, 1997).
This grammar captures not just multimodal integra-
tion patterns but also the parsing of speech and ges-
ture, and the assignment of meaning. In Figure 9 we
present a small simplified fragment capable of han-
dling MATCH commands such as phone numbers for
these three restaurants. A multimodal CFG differs
from a normal CFG in that the terminals are triples:
W:G:M, where W is the speech stream (words), G
the ink stream (gesture symbols) and M the meaning
stream (meaning symbols). An XML representation
for meaning is used to facilate parsing and logging
by other system components. The meaning tape sym-
bols concatenate to form coherent XML expressions.
The epsilon symbol (eps) indicates that a stream is
empty in a given terminal.
When the user says phone numbers for these
three restaurants and circles two groups of restau-
rants (Figure 3). The gesture lattice (Figure 8) is
turned into a transducer I:G with the same sym-
bol on each side except for the SEM arcs which are
split. For example, path 15-16 SEM([id1,id2,id3])
becomes [id1,id2,id3]:SEM. After G and the speech
W are integrated using G:W and G W:M. The G path
in the result is used to re-establish the connection
between SEM symbols and their specific contents
in I:G (I:G o G:M = I:M). The meaning read off
I:M is<cmd><phone><restaurant> [id1,id2,id3]
</restaurant> </phone> </cmd>. This is passed
to the multimodal dialog manager (MDM) and from
there to the Multimodal UI resulting in a display like
Figure 4 with coordinated TTS output. Since the
speech input is a lattice and there is also potential
for ambiguity in the multimodal grammar, the output
from MMFST to MDM is an N-best list of potential
multimodal interpretations.
Multimodal Dialog Manager (MDM) The MDM
is based on previous work on speech-act based mod-
els of dialog (Stent et al, 1999; Rich and Sidner,
1998). It uses a Java-based toolkit for writing dialog
managers that is similar in philosophy to TrindiKit
(Larsson et al, 1999). It includes several rule-based
S ! eps:eps:<cmd> CMD eps:eps:</cmd>
CMD ! phone:eps:<phone> numbers:eps:eps
for:eps:eps DEICTICNP
eps:eps:</phone>
DEICTICNP ! DDETPL eps:area:eps eps:selection:eps
NUM RESTPL eps:eps:<restaurant>
eps:SEM:SEM eps:eps:</restaurant>
DDETPL ! these:G:eps
RESTPL ! restaurants:restaurant:eps
NUM ! three:3:eps
Figure 9: Multimodal grammar fragment
processes that operate on a shared state. The state
includes system and user intentions and beliefs, a di-
alog history and focus space, and information about
the speaker, the domain and the available modalities.
The processes include interpretation, update, selec-
tion and generation processes.
The interpretation process takes as input an N-best
list of possible multimodal interpretations for a user
input from MMFST. It rescores them according to a
set of rules that encode the most likely next speech
act given the current dialogue context, and picks the
most likely interpretation from the result. The update
process updates the dialogue context according to the
system?s interpretation of user input. It augments the
dialogue history, focus space, models of user and sys-
tem beliefs, and model of user intentions. It also al-
ters the list of current modalities to reflect those most
recently used by the user.
The selection process determines the system?s next
move(s). In the case of a command, request or ques-
tion, it first checks that the input is fully specified
(using the domain ontology, which contains informa-
tion about required and optional roles for different
types of actions); if it is not, then the system?s next
move is to take the initiative and start an information-
gathering subdialogue. If the input is fully specified,
the system?s next move is to perform the command or
answer the question; to do this, MDM communicates
with the UI. Since MDM is aware of the current set
of preferred modalities, it can provide feedback and
responses tailored to the user?s modality preferences.
The generation process performs template-based
generation for simple responses and updates the sys-
tem?s model of the user?s intentions after generation.
The text planner is used for more complex genera-
tion, such as the generation of comparisons.
In the route query example in Section 1, MDM first
receives a route query in which only the destination
is specified How do I get to this place? In the se-
lection phase it consults the domain model and de-
termines that a source is also required for a route.
It adds a request to query the user for the source to
the system?s next moves. This move is selected and
the generation process selects a prompt and sends it
to the TTS component. The system asks Where do
you want to go from? If the user says or writes 25th
Street and 3rd Avenue then MMFST will assign this
input two possible interpretations. Either this is a re-
quest to zoom the display to the specified location or
it is an assertion of a location. Since the MDM dia-
logue state indicates that it is waiting for an answer
of the type location, MDM reranks the assertion as
the most likely interpretation. A generalized overlay
process (Alexandersson and Becker, 2001) is used to
take the content of the assertion (a location) and add
it into the partial route request. The result is deter-
mined to be complete. The UI resolves the location
to map coordinates and passes on a route request to
the SUBWAY component.
We found this traditional speech-act based dia-
logue manager worked well for our multimodal inter-
face. Critical in this was our use of a common seman-
tic representation across spoken, gestured, and multi-
modal commands. The majority of the dialogue rules
operate in a mode-independent fashion, giving users
flexibility in the mode they choose to advance the di-
alogue. On the other hand, mode sensitivity is also
important since user modality choice can be used to
determine system mode choice for confirmation and
other responses.
Subway Route Constraint Solver (SUBWAY)
This component has access to an exhaustive database
of the NYC subway system. When it receives a route
request with the desired source and destination points
from the Multimodal UI, it explores the search space
of possible routes to identify the optimal one, using a
cost function based on the number of transfers, over-
all number of stops, and the walking distance from
the station at each end. It builds a list of actions re-
quired to reach the destination and passes them to the
multimodal generator.
Multimodal Generator and Text-to-speech The
multimodal generator processes action lists from
SUBWAY and other components and assigns appro-
priate prompts for each action using a template-based
generator. The result is a ?score? of prompts and ac-
tions which is passed to the Multimodal UI. The Mul-
timodal UI plays this ?score? by coordinating changes
in the interface with the corresponding TTS prompts.
AT&T?s Natural Voices TTS engine is used to pro-
vide the spoken output. When the UI receives a mul-
timodal score, it builds a stack of graphical actions
such as zooming the display to a particular location
or putting up a graphical callout. It then sends the
prompts to be rendered by the TTS server. As each
prompt is synthesized the TTS server sends progress
notifications to the Multimodal UI, which pops the
next graphical action off the stack and executes it.
Text Planner and User Model The text plan-
ner receives instructions from MDM for execution
of ?compare?, ?summarize?, and ?recommend? com-
mands. It employs a user model based on multi-
attribute decision theory (Carenini and Moore, 2001).
For example, in order to make a comparison between
the set of restaurants shown in Figure 6, the text
planner first ranks the restaurants within the set ac-
cording to the predicted ranking of the user model.
Then, after selecting a small set of the highest ranked
restaurants, it utilizes the user model to decide which
restaurant attributes are important to mention. The
resulting text plan is converted to text and sent to TTS
(Walker et al, 2002). A user model for someone who
cares most highly about cost and secondly about food
quality and decor leads to a system response such as
that in Compare-A above. A user model for someone
whose selections are driven by food quality and food
type first, and cost only second, results in a system
response such as that shown in Compare-B.
Compare-B: Among the selected restaurants, the following of-
fer exceptional overall value. Babbo?s price is 60 dollars. It has
superb food quality. Il Mulino?s price is 65 dollars. It has superb
food quality. Uguale?s price is 33 dollars. It has excellent food.
Note that the restaurants selected for the user who
is not concerned about cost includes two rather more
expensive restaurants that are not selected by the text
planner for the cost-oriented user.
Multimodal Logger User studies, multimodal data
collection, and debugging were accomplished by in-
strumenting MATCH agents to send details of user
inputs, system processes, and system outputs to a log-
ger agent that maintains an XML log designed for
multimodal interactions. Our critical objective was
to collect data continually throughout system devel-
opment, and to be able to do so in mobile settings.
While this rendered the common practice of video-
taping user interactions impractical, we still required
high fidelity records of each multimodal interaction.
To address this problem, MATCH logs the state of
the UI and the user?s ink, along with detailed data
from other components. These components can in
turn dynamically replay the user?s speech and ink as
they were originally received, and show how the sys-
tem responded. The browser- and component-based
architecture of the Multimodal UI facilitated its reuse
in a Log Viewer that reads multimodal log files, re-
plays interactions between the user and system, and
allows analysis and annotation of the data. MATCH?s
logging system is similar in function to STAMP (Ovi-
att and Clow, 1998), but does not require multimodal
interactions to be videotaped and allows rapid re-
configuration for different annotation tasks since it
is browser-based. The ability of the system to log
data standalone is important, since it enables testing
and collection of multimodal data in realistic mobile
environments without relying on external equipment.
3 Experimental Evaluation
Our multimodal logging infrastructure enabled
MATCH to undergo continual user trials and evalu-
ation throughout development. Repeated evaluations
with small numbers of test users both in the lab and
in mobile settings (Figure 10) have guided the design
and iterative development of the system.
Figure 10: Testing MATCH in NYC
This iterative development approach highlighted
several important problems early on. For example,
while it was originally thought that users would for-
mulate queries and navigation commands primarily
by specifying the names of New York neighborhoods,
as in show italian restaurants in chelsea, early field
test studies in the city revealed that the need for
neighborhood names in the grammar was minimal
compared to the need for cross-streets and points of
interest; hence, cross-streets and a sizable list of land-
marks were added. Other early tests revealed the
need for easily accessible ?cancel? and ?undo? fea-
tures that allow users to make quick corrections. We
also discovered that speech recognition performance
was initially hindered by placement of the ?click-to-
speak? button and the recognition feedback box on
the bottom-right side of the device, leading many
users to speak ?to? this area, rather than toward the
microphone on the upper left side. This placement
also led left-handed users to block the microphone
with their arms when they spoke. Moving the but-
ton and the feedback box to the top-left of the device
resolved both of these problems.
After initial open-ended piloting trials, more struc-
tured user tests were conducted, for which we devel-
oped a set of six scenarios ordered by increasing level
of difficulty. These required the test user to solve
problems using the system. These scenarios were left
as open-ended as possible to elicit natural responses.
Sample scenario:You have plans to meet your aunt for dinner
later this evening at a Thai restaurant on the Upper West Side
near her apartment on 95th St. and Broadway. Unfortunately,
you forgot what time you?re supposed to meet her, and you can?t
reach her by phone. Use MATCH to find the restaurant and write
down the restaurant?s telephone number so you can check on the
reservation time.
Test users received a brief tutorial that was inten-
tionally vague and broad in scope so the users might
overestimate the system?s capabilities and approach
problems in new ways. Figure 11 summarizes re-
sults from our last scenario-based data collection for
a fixed version of the system. There were five sub-
jects (2 male, 3 female) none of whom had been in-
volved in system development. All of these five tests
were conducted indoors in offices.
exchanges 338 asr word accuracy 59.6%
speech only 171 51% asr sent. accuracy 36.1%
multimodal 93 28% handwritten sent. acc. 64%
pen only 66 19% task completion rate 85%
GUI actions 8 2% average time/scenario 6.25m
Figure 11: MATCH study
There were an average of 12.75 multimodal ex-
changes (pairs of user input and system response) per
scenario. The overall time per scenario varied from
1.5 to to 15 minutes. The longer completion times
resulted from poor ASR performance for some of the
users. Although ASR accuracy was low, overall task
completion was high, suggesting that the multimodal
aspects of the system helped users to complete tasks.
Unimodal pen commands were recognized more suc-
cessfully than spoken commands; however, only 19%
of commands were pen only. In ongoing work, we
are exploring strategies to increase users? adoption of
more robust pen-based and multimodal input.
MATCH has a very fast system response time.
Benchmarking a set of speech, pen, and multimodal
commands, the average response time is approxi-
mately 3 seconds (time from end of user input to sys-
tem response). We are currently completing a larger
scale scenario-based evaluation and an independent
evaluation of the functionality of the text planner.
In addition to MATCH, the same multimodal ar-
chitecture has been used for two other applications:
a multimodal interface to corporate directory infor-
mation and messaging and a medical application to
assist emergency room doctors. The medical proto-
type is the most recent and demonstrates the utility of
the architecture for rapid prototyping. System devel-
opment took under two days for two people.
4 Conclusion
The MATCH architecture enables rapid develop-
ment of mobile multimodal applications. Combin-
ing finite-state multimodal integration with a speech-
act based dialogue manager enables users to interact
flexibly using speech, pen, or synchronized combina-
tions of the two depending on their preferences, task,
and physical and social environment. The system
responds by generating coordinated multimodal pre-
sentations adapted to the multimodal dialog context
and user preferences. Features of the system such
as the browser-based UI and general purpose finite-
state architecture for multimodal integration facili-
tate rapid prototyping and reuse of the technology for
different applications. The lattice-based finite-state
approach to multimodal understanding enables both
multimodal integration and dialogue context to com-
pensate for recognition errors. The multimodal log-
ging infrastructure has enabled an iterative process
of pro-active evaluation and data collection through-
out system development. Since we can replay multi-
modal interactions without video we have been able
to log and annotate subjects both in the lab and in
NYC throughout the development process and use
their input to drive system development.
Acknowledgements
Thanks to AT&T Labs and DARPA (contract MDA972-99-3-
0003) for financial support. We would also like to thank Noemie
Elhadad, Candace Kamm, Elliot Pinson, Mazin Rahim, Owen
Rambow, and Nika Smith.
References
J. Alexandersson and T. Becker. 2001. Overlay as the ba-
sic operation for discourse processing in a multimodal
dialogue system. In 2nd IJCAI Workshop on Knowl-
edge and Reasoning in Practical Dialogue Systems.
J. Allen, D. Byron, M. Dzikovska, G. Ferguson,
L. Galescu, and A. Stent. 2000. An architecture for
a generic dialogue shell. JNLE, 6(3).
E. Andre?. 2002. Natural language in multime-
dia/multimodal systems. In Ruslan Mitkov, editor,
Handbook of Computational Linguistics. OUP.
G. Carenini and J. D. Moore. 2001. An empirical study of
the influence of user tailoring on evaluative argument
effectiveness. In IJCAI, pages 1307?1314.
M. Johnston and S. Bangalore. 2000. Finite-state mul-
timodal parsing and understanding. In Proceedings of
COLING 2000, Saarbru?cken, Germany.
M. Johnston and S. Bangalore. 2001. Finite-state meth-
ods for multimodal parsing and integration. In ESSLLI
Workshop on Finite-state Methods, Helsinki, Finland.
M. Johnston. 2000. Deixis and conjunction in mul-
timodal systems. In Proceedings of COLING 2000,
Saarbru?cken, Germany.
S. Larsson, P. Bohlin, J. Bos, and D. Traum. 1999.
TrindiKit manual. Technical report, TRINDI Deliver-
able D2.2.
D. Martin, A. Cheyer, and D. Moran. 1999. The Open
Agent Architecture: A framework for building dis-
tributed software systems. Applied Artificial Intelli-
gence, 13(1?2):91?128.
M-J. Nederhof. 1997. Regular approximations of CFLs:
A grammatical view. In Proceedings of the Interna-
tional Workshop on Parsing Technology, Boston.
S. L. Oviatt and J. Clow. 1998. An automated tool for
analysis of multimodal system performance. In Pro-
ceedings of ICSLP.
C. Rich and C. Sidner. 1998. COLLAGEN: A collabora-
tion manager for software interface agents. User Mod-
eling and User-Adapted Interaction, 8(3?4):315?350.
D. Rubine. 1991. Specifying gestures by example. Com-
puter graphics, 25(4):329?337.
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and
V. Zue. 1998. Galaxy-II: A reference architecture for
conversational system development. In ICSLP-98.
A. Stent, J. Dowding, J. Gawron, E. Bratt, and R. Moore.
1999. The CommandTalk spoken dialogue system. In
Proceedings of ACL?99.
M. A. Walker, S. J. Whittaker, P. Maloor, J. D. Moore,
M. Johnston, and G. Vasireddy. 2002. Speech-Plans:
Generating evaluative responses in spoken dialogue. In
In Proceedings of INLG-02.
What?s the Trouble: Automatically Identifying Problematic Dialogues in
DARPA Communicator Dialogue Systems
Helen Wright Hastie, Rashmi Prasad, Marilyn Walker
AT& T Labs - Research
180 Park Ave, Florham Park, N.J. 07932, U.S.A.
hhastie,rjprasad,walker@research.att.com
Abstract
Spoken dialogue systems promise effi-
cient and natural access to information
services from any phone. Recently, spo-
ken dialogue systems for widely used ap-
plications such as email, travel informa-
tion, and customer care have moved from
research labs into commercial use. These
applications can receive millions of calls
a month. This huge amount of spoken
dialogue data has led to a need for fully
automatic methods for selecting a subset
of caller dialogues that are most likely
to be useful for further system improve-
ment, to be stored, transcribed and further
analyzed. This paper reports results on
automatically training a Problematic Di-
alogue Identifier to classify problematic
human-computer dialogues using a corpus
of 1242 DARPA Communicator dialogues
in the travel planning domain. We show
that using fully automatic features we can
identify classes of problematic dialogues
with accuracies from 67% to 89%.
1 Introduction
Spoken dialogue systems promise efficient and nat-
ural access to a large variety of information services
from any phone. Deployed systems and research
prototypes exist for applications such as personal
email and calendars, travel and restaurant informa-
tion, personal banking, and customer care. Within
the last few years, several spoken dialogue systems
for widely used applications have moved from re-
search labs into commercial use (Baggia et al, 1998;
Gorin et al, 1997). These applications can receive
millions of calls a month. There is a strong require-
ment for automatic methods to identify and extract
dialogues that provide training data for further sys-
tem development.
As a spoken dialogue system is developed, it is
first tested as a prototype, then fielded in a limited
setting, possibly running with human supervision
(Gorin et al, 1997), and finally deployed. At each
stage from research prototype to deployed commer-
cial application, the system is constantly undergoing
further development. When a system is prototyped
in house or first tested in the field, human subjects
are often paid to use the system and give detailed
feedback on task completion and user satisfaction
(Baggia et al, 1998; Walker et al, 2001). Even
when a system is deployed, it often keeps evolving,
either because customers want to do different things
with it, or because new tasks arise out of develop-
ments in the underlying application. However, real
customers of a deployed system may not be willing
to give detailed feedback.
Thus, the widespread use of these systems has
created a data management and analysis problem.
System designers need to constantly track system
performance, identify problems, and fix them. Sys-
tem modules such as automatic speech recognition
(ASR), natural language understanding (NLU) and
dialogue management may rely on training data col-
lected at each phase. ASR performance assessment
relies on full transcription of the utterances. Dia-
logue manager assessment relies on a human inter-
face expert reading a full transcription of the dia-
logue or listening to a recording of it, possibly while
examining the logfiles to understand the interaction
between all the components. However, because of
the high volume of calls, spoken dialogue service
providers typically can only afford to store, tran-
scribe, and analyze a small fraction of the dialogues.
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 384-391.
                         Proceedings of the 40th Annual Meeting of the Association for
Therefore, there is a great need for methods for
both automatically evaluating system performance,
and for extracting subsets of dialogues that provide
good training data for system improvement. This is
a difficult problem because by the time a system is
deployed, typically over 90% of the dialogue inter-
actions result in completed tasks and satisfied users.
Dialogues such as these do not provide very use-
ful training data for further system development be-
cause there is little to be learned when the dialogue
goes well.
Previous research on spoken dialogue evaluation
proposed the application of automatic classifiers for
identifying and predicting of problematic dialogues
(Litman et al, 1999; Walker et al, 2002) for the
purpose of automatically adapting the dialogue man-
ager. Here we apply similar methods to the dialogue
corpus data-mining problem described above. We
report results on automatically training a Problem-
atic Dialogue Identifier (PDI) to classify problem-
atic human-computer dialogues using the October-
2001 DARPA Communicator corpus.
Section 2 describes our approach and the dialogue
corpus. Section 3 describes how we use the DATE
dialogue act tagging scheme to define input features
for the PDI. Section 4 presents a method and results
for automatically predicting task completion. Sec-
tion 5 presents results for predicting problematic di-
alogues based on the user?s satisfaction. We show
that we identify task failure dialogues with 85% ac-
curacy (baseline 59%) and dialogues with low user
satisfaction with up to 89% accuracy. We discuss the
application of the PDI to data mining in Section 6.
Finally, we summarize the paper and discuss future
work.
2 Corpus, Methods and Data
Our experiments apply CLASSIFICATION and RE-
GRESSION trees (CART) (Brieman et al, 1984) to
train a Problematic Dialogue Identifier (PDI) from
a corpus of human-computer dialogues. CLASSI-
FICATION trees are used for categorical response
variables and REGRESSION trees are used for con-
tinuous response variables. CART trees are binary
decision trees. A CLASSIFICATION tree specifies
what queries to perform on the features to maximize
CLASSIFICATION ACCURACY, while REGRESSION
trees derive a set of queries to maximize the COR-
RELATION of the predicted value and the original
value. Like other machine learners, CART takes as
input the allowed values for the response variables;
the names and ranges of values of a fixed set of input
features; and training data specifying the response
variable value and the input feature values for each
example in a training set. Below, we specify how
the PDI was trained, first describing the corpus, then
the response variables, and finally the input features
derived from the corpus.
Corpus: We train and test the PDI on the DARPA
Communicator October-2001 corpus of 1242 dia-
logues. This corpus represents interactions with
real users, with eight different Communicator travel
planning systems, over a period of six months from
April to October of 2001. The dialogue tasks range
from simple domestic round trips to multileg inter-
national trips requiring both car and hotel arrange-
ments. The corpus includes logfiles with logged
events for each system and user turn; hand transcrip-
tions and automatic speech recognizer (ASR) tran-
scription for each user utterance; information de-
rived from a user profile such as user dialect region;
and a User Satisfaction survey and hand-labelled
Task Completion metric for each dialogue. We ran-
domly divide the corpus into 80% training (894 dia-
logues) and 20% testing (248 dialogues).
Defining the Response Variables: In principle,
either low User Satisfaction or failure to complete
the task could be used to define problematic dia-
logues. Therefore, both of these are candidate re-
sponse variables to be examined. The User Satisfac-
tion measure derived from the user survey ranges be-
tween 5 and 25. Task Completion is a ternary mea-
sure where no Task Completion is indicated by 0,
completion of only the airline itinerary is indicated
by 1, and completion of both the airline itinerary and
ground arrangements, such as car and hotel book-
ings, is indicated by 2. We also defined a binary ver-
sion of Task Completion, where Binary Task Com-
pletion=0 when no task or subtask was complete
(equivalent to Task Completion=0), and Binary Task
Completion=1 where all or some of the task was
complete (equivalent to Task Completion=1 or Task
Completion=2).
Figure 1 shows the frequency of dialogues for
varying User Satisfaction for cases where Task
Completion is 0 (solid line) and Task Completion
is greater than 0 (dotted lines). Note that Task Com-
pletion is 1 or 2 for a number of dialogues for which
User Satisfaction is low. Figure 2 illustrates such a
dialogue (system turns are labelled S, user turns as
U, and ASR hypotheses as REC). Here, low User
Satisfaction may be due to the fact that the user had
to repeat herself many times before the system un-
derstood the departure city. An automatic surrogate
for ASR accuracy (such as ASR confidence) would
Figure 1: User Satisfaction for dialogues where Task
Completion is 0, 1 or 2
not be adequate for identifying this problematic di-
alogue, because here either the dialogue manager or
the SLU component is at fault. Another dialogue
subset of interest in Figure 1 is one for which Task
Completion is 0 but User Satisfaction is high. A
common cause for non-Task Completion in these di-
alogues is database access problems. The fact that
the interaction went well until this point is not cap-
tured by the Task Completion metric. Thus we de-
cided to use both User Satisfaction and Task Com-
pletion as response variables in separate experiments
in order to allow the PDI to capture the relationship
between these two variables.
Input Features: Both User Satisfaction and Task
Completion predictors are trained using two types of
automatically extractable features: (1) logfile fea-
tures; and (2) system dialogue act types. The
logfile features include efficiency metrics such as
the number of system and user turns spent on the
task; the total time of the dialogue in milliseconds;
and situational metrics, such as phone type1 which
affects ASR. The system dialogue act types are
extracted from the logfiles using the DATE (Dia-
logue Act Tagging for Evaluation) automatic tag-
1We assume this is automatically derivable by automatic
number identification (ANI).
S13: Sorry, I must have misunderstood. I just heard you
say that the departure city was Minto, Arkansas.
Was the departure city Minneapolis-St-Paul, Minto,
Arkansas or Milan, Italy?
U14: THE DEPARTURE CITY IS MINNEAPOLIS
SAINT PAUL
REC: the departure city in minneapolis saint paul
S15: Here?s a summary of the second flight segment:
I?ve got you leaving from Minto, Arkansas on Fri-
day, October twelfth in the morning and arriving in
newark flying on continental. Is this correct?
U16: NO I WANT TO LEAVE FROM MINNEAPOLIS
REC: no i want to leave from minneapolis
Repeat dialogue segment 3 times
S25: Sorry, I must have misunderstood. I just heard you
say that the departure city was Minto, Arkansas. Was
the departure city Minto, Arkansas or Minneapolis-
St-Paul ?
U26: THE DEPARTURE CITY IS MINNEAPOLIS
SAINT PAUL
REC: the departure city minneapolis by all
S27: Recapping the second flight segment: I have you
leaving out of minneapolis-st-paul on Friday, Octo-
ber twelfth in the morning and arriving in newark
flying on continental. Is this correct?
U28: YES
REC: yes
Figure 2: Sample low user satisfaction dialogue
ging scheme (Walker et al, 2001). The purpose
of these features is to extract numerical correlates
of system dialogue behaviors. This dialogue act la-
belling procedure is detailed in Section 3.
Figure 3 summarizes the types of features used
to train the User Satisfaction predictor. In addition
to the efficiency metrics and the DATE labels, Task
Success can itself be used as a predictor. This can
either be the hand-labelled feature or an approxima-
tion as predicted by the Task Completion Predictor,
described in Section 4. Figure 4 shows the system
design for automatically predicting User Satisfac-
  Efficiency Measures
? Hand-labelled: WERR, SERR
? Automatic: TimeOnTask, TurnsOnTask, Nu-
mOverlaps, MeanUsrTurnDur, MeanWrdsPerUs-
rTurn, MeanSysTurnDur, MeanWrdsPerSysTurn,
DeadAlive, Phone-type, SessionNumber
  Qualitative Measures
? Automatic: DATE Unigrams, e.g. present-
info:flight, acknowledgement:flight booking etc.
? Automatic: DATE Bigrams, e.g. present-
info:flight+acknowledgement:flight booking etc.
  Task Success Features
? Hand-labelled: HL Task Completion
? Automatic: Auto Task Completion
Figure 3: Features used to train the User Satisfaction
Prediction tree
tion with the three types of input features.
DATE
Output 
of
SLS
Completion
Auto Task Completion
CART
Predictor
UserSatisfaction
Task 
Predictor
TAGGER
Automatic
Logfile 
Features
DATE
Rules
Figure 4: Schema for User Satisfaction prediction
3 Extracting DATE Features
The dialogue act labelling of the corpus follows
the DATE tagging scheme (Walker et al, 2001).
In DATE, utterance classification is done along
three cross-cutting orthogonal dimensions. The
CONVERSATIONAL-DOMAIN dimension specifies
the domain of discourse that an utterance is about.
The SPEECH ACT dimension captures distinctions
between communicative goals such as requesting
information (REQUEST-INFO) or presenting infor-
mation (PRESENT-INFO). The TASK-SUBTASK di-
mension specifies which travel reservation subtask
the utterance contributes to. The SPEECH ACT and
CONVERSATIONAL-DOMAIN dimensions are gen-
eral across domains, while the TASK-SUBTASK di-
mension is domain- and sometimes system-specific.
Within the conversational domain dimension,
DATE distinguishes three domains (see Figure 5).
The ABOUT-TASK domain is necessary for evaluat-
ing a dialogue system?s ability to collaborate with
a speaker on achieving the task goal. The ABOUT-
COMMUNICATION domain reflects the system goal
of managing the verbal channel of communication
and providing evidence of what has been under-
stood. All implicit and explicit confirmations are
about communication. The ABOUT-SITUATION-
FRAME domain pertains to the goal of managing the
user?s expectations about how to interact with the
system.
DATE distinguishes 11 speech acts. Examples of
each speech act are shown in Figure 6.
The TASK-SUBTASK dimension distinguishes
among 28 subtasks, some of which can also be
grouped at a level below the top level task. The
TOP-LEVEL-TRIP task describes the task which con-
tains as its subtasks the ORIGIN, DESTINATION,
Conversational Domain Example
ABOUT-TASK And what time didja wanna
leave?
ABOUT-
COMMUNICATION
Leaving from Miami.
ABOUT-SITUATION-
FRAME
You may say repeat, help me
out, start over, or, that?s wrong
Figure 5: Example utterances distinguished within
the Conversational Domain Dimension
Speech-Act Example
REQUEST-INFO And, what city are you flying to?
PRESENT-INFO The airfare for this trip is 390 dol-
lars.
OFFER Would you like me to hold this op-
tion?
ACKNOWLEDGMENT I will book this leg.
BACKCHANNEL Okay.
STATUS-REPORT Accessing the database; this
might take a few seconds.
EXPLICIT-
CONFIRM
You will depart on September 1st.
Is that correct?
IMPLICIT-
CONFIRM
Leaving from Dallas.
INSTRUCTION Try saying a short sentence.
APOLOGY Sorry, I didn?t understand that.
OPENING-
CLOSING
Hello. Welcome to the C M U
Communicator.
Figure 6: Example speech act utterances
DATE, TIME, AIRLINE, TRIP-TYPE, RETRIEVAL
and ITINERARY tasks. The GROUND task includes
both the HOTEL and CAR-RENTAL subtasks. The
HOTEL task includes both the HOTEL-NAME and
HOTEL-LOCATION subtasks.2
For the DATE labelling of the corpus, we imple-
mented an extended version of the pattern matcher
that was used for tagging the Communicator June
2000 corpus (Walker et al, 2001). This method
identified and labelled an utterance or utterance se-
quence automatically by reference to a database of
utterance patterns that were hand-labelled with the
DATE tags. Before applying the pattern matcher,
a named-entity labeler was applied to the system
utterances, matching named-entities relevant in the
travel domain, such as city, airport, car, hotel, airline
names etc.. The named-entity labeler was also ap-
plied to the utterance patterns in the pattern database
to allow for generality in the expression of com-
municative goals specified within DATE. For this
named-entity labelling task, we collected vocabulary
lists from the sites, which maintained such lists for
2ABOUT-SITUATION-FRAME utterances are not specific to
any particular task and can be used for any subtask, for example,
system statements that it misunderstood. Such utterances are
given a ?meta? dialogue act status in the task dimension.
developing their system.3 The extension of the pat-
tern matcher for the 2001 corpus labelling was done
because we found that systems had augmented their
inventory of named entities and utterance patterns
from 2000 to 2001, and these were not accounted
for by the 2000 tagger database. For the extension,
we collected a fresh set of vocabulary lists from the
sites and augmented the pattern database with ad-
ditional 800 labelled utterance patterns. We also
implemented a contextual rule-based postprocessor
that takes any remaining unlabelled utterances and
attempts to label them by looking at their surround-
ing DATE labels. More details about the extended
tagger can be found in (Prasad and Walker, 2002).
On the 2001 corpus, we were able to label 98.4 
of the data. A hand evaluation of 10 randomly se-
lected dialogues from each system shows that we
achieved a classification accuracy of 96  at the ut-
terance level.
For User Satisfaction Prediction, we found that
the distribution of DATE acts were better captured
by using the frequency normalized over the total
number of dialogue acts. In addition to these un-
igram proportions, the bigram frequencies of the
DATE dialogue acts were also calculated. In the fol-
lowing two sections, we discuss which DATE labels
are discriminatory for predicting Task Completion
and User Satisfaction.
4 The Task Completion Predictor
In order to automatically predict Task Comple-
tion, we train a CLASSIFICATION tree to catego-
rize dialogues into Task Completion=0, Task Com-
pletion=1 or Task Completion=2. Recall that a
CLASSIFICATION tree attempts to maximize CLAS-
SIFICATION ACCURACY, results for Task Comple-
tion are thus given in terms of percentage of dia-
logues correctly classified. The majority class base-
line is 59.3% (dialogues where Task Completion=1).
The tree was trained on a number of different in-
put features. The most discriminatory ones, how-
ever, were derived from the DATE tagger. We
use the primitive DATE tags in conjunction with a
feature called GroundCheck (GC), a boolean fea-
ture indicating the existence of DATE tags related
to making ground arrangements, specifically re-
quest info:hotel name, request info:hotel location,
offer:hotel and offer:rental.
Table 1 gives the results for Task Completion pre-
diction accuracy using the various types of features.
3The named entities were preclassified into their respective
semantic classes by the sites.
Baseline Auto ALF + ALF +
Logfile GC GC+ DATE
TC 59% 59% 79% 85%
BTC 86% 86% 86% 92%
Table 1: Task Completion (TC) and Binary Task
Completion (BTC) prediction results, using auto-
matic logfile features (ALF), GroundCheck (GC)
and DATE unigram frequencies
The first row is for predicting ternary Task Comple-
tion, and the second for predicting binary Task Com-
pletion. Using automatic logfile features (ALF) is
not effective for the prediction of either types of Task
Completion. However, the use of GroundCheck re-
sults in an accuracy of 79% for the ternary Task
Completion which is significantly above the base-
line (df = 247, t = -6.264, p  .0001). Adding in the
other DATE features yields an accuracy of 85%. For
Binary Task Completion it is only the use of all the
DATE features that yields an improvement over the
baseline of 92%, which is significant (df = 247, t =
5.83, p  .0001).
A diagram of the trained decision tree for ternary
Task Completion is given in Figure 7. At any junc-
tion in the tree, if the query is true then one takes
the path down the right-hand side of the tree, oth-
erwise one takes the left-hand side. The leaf nodes
contain the predicted value. The GroundCheck fea-
ture is at the top of the tree and divides the data
into Task Completion  2 and Task Completion  2.
If GroundCheck  1, then the tree estimates that
Task Completion is 2, which is the best fit for the
data given the input features. If GroundCheck  0
and there is an acknowledgment of a booking, then
probably a flight has been booked, therefore, Task
Completion is predicted to be 1. Interestingly, if
there is no acknowledgment of a booking then Task
Completion  0, unless the system got to the stage of
asking the user for an airline preference and if re-
quest info:top level trip  2. More than one of these
DATE types indicates that there was a problem in the
dialogue and that the information gathering phase
started over from the beginning.
The binary Task Completion decision tree simply
checks if an acknowledgement:flight booking
has occurred. If it has, then Binary Task Com-
pletion=1, otherwise it looks for the DATE act
about situation frame:instruction:meta situation info,
which captures the fact that the system has told
the user what the system can and cannot do, or
has informed the user about the current state of the
task. This must help with Task Completion, as the
tree tells us that if one or more of these acts are
observed then Task Completion=1, otherwise Task
Completion=0.
TC=1
GroundCheck =0
TC=2
request_info:airline <1
request_info:top_level_trip < 2
acknow.: flight_booking< 1
TC=0TC=1
TC=0 TC=1
Figure 7: Classification Tree for predicting Task
Completion (TC)
5 The User Satisfaction Predictor
Feature Log LF + LF +
used features unigram bigram
HL TC 0.587 0.584 0.592
Auto TC 0.438 0.434 0.472
HL BTC 0.608 0.607 0.614
Auto BTC 0.477 0.47 0.484
Table 2: Correlation results using logfile fea-
tures (LF), adding unigram proportions and bigram
counts, for trees tested on either hand-labelled (HL)
or automatically derived Task Completion (TC) and
Binary Task Completion (BTC)
Quantitative Results: Recall that REGRESSION
trees attempt to maximize the CORRELATION of the
predicted value and the original value. Thus, the re-
sults of the User Satisfaction predictor are given in
terms of the correlation between the predicted User
Satisfaction and actual User Satisfaction as calcu-
lated from the user survey. Here, we also provide R 
for comparison with previous studies. Table 2 gives
the correlation results for User Satisfaction for dif-
ferent feature sets. The User Satisfaction predictor
is trained using the hand-labelled Task Completion
feature for a topline result and using the automati-
cally obtained Task Completion (Auto TC) for the
fully automatic results. We also give results using
Binary Task Completion (BTC) as a substitute for
Task Completion. The first column gives results us-
ing features extracted from the logfile; the second
column indicates results using the DATE unigram
proportions and the third column indicates results
when both the DATE unigram and bigram features
are available.
The first row of Table 2 indicates that perfor-
mance across the three feature sets is indistinguish-
able when hand-labelled Task Completion (HL TC)
is used as the Task Completion input feature. A
comparison of Row 1 and Row 2 shows that the
PDI performs significantly worse using only auto-
matic features (z = 3.18). Row 2 also indicates that
the DATE bigrams help performance, although the
difference between R = .438 and R = .472 is not
significant. The third and fourth rows of Table 1
indicate that for predicting User Satisfaction, Bi-
nary Task Completion is as good as or better than
Ternary Task Completion. The highest correlation of
0.614 (   	

 ) uses hand-labelled Binary Task
Completion and the logfile features and DATE uni-
gram proportions and bigram counts. Again, we see
that the Automatic Binary Task Completion (Auto
BTC) performs significantly worse than the hand-
labelled version (z = -3.18). Row 4 includes the best
totally automatic system: using Automatic Binary
Task Completion and DATE unigrams and bigrams
yields a correlation of 0.484 ( 	 ).
Regression Tree Interpretation: It is interest-
ing to examine the trees to see which features are
used for predicting User Satisfaction. A metric
called Feature Usage Frequency indicates which fea-
tures are the most discriminatory in the CART tree.
Specifically, Feature Usage Frequency counts how
often a feature is queried for each data point, nor-
malized so that the sum of Feature Usage Frequency
values for all the features sums to one. The higher a
feature is in the tree, the more times it is queried. To
calculate the Feature Usage Frequency, we grouped
the features into three types: Task Completion, Log-
file features and DATE frequencies. Feature Us-
age Frequency for the logfile features is 37%. Task
Completion occurs only twice in the tree, however,
it makes up 31because it occurs at the top of the
tree. The Feature Usage Frequency for DATE cat-
egory frequency is 32%. We will discuss each of
these three groups of features in turn.
The most used logfile feature is TurnsOnTask
which is the number of turns which are task-
oriented, for example, initial instructions on how
to use the system are not taken as a TurnOnTask.
Shorter dialogues tend to have a higher User Sat-
isfaction. This is reflected in the User Satisfaction
scores in the tree. However, dialogues which are
long (TurnsOnTask  79 ) can be satisfactory (User
Satisfaction = 15.2) as long as the task that is com-
pleted is long, i.e., if ground arrangements are made
in that dialogue (Task Completion=2). If ground ar-
rangements are not made, the User Satisfaction is
lower (11.6). Phone type is another important fea-
ture queried in the tree, so that dialogues conducted
over corded phones have higher satisfaction. This
is likely to be due to better recognition performance
from corded phones.
As mentioned previously, Task Completion is at
the top of the tree and is therefore the most queried
feature. This captures the relationship between Task
Completion and User Satisfaction as illustrated in
Figure 1.
Finally, it is interesting to examine which DATE
tags the tree uses. If there have been more than
three acknowledgments of bookings, then several
legs of a journey have been successfully booked,
therefore User Satisfaction is high. In particular,
User Satisfaction is high if the system has asked
if the user would like a price for their itinerary
which is one of the final dialogue acts a system
does before the task is completed. The DATE act
about comm:apology:meta slu reject is a measure
of the system?s level of misunderstanding. There-
fore, the more of these dialogue act types the lower
User Satisfaction. This part of the tree uses length
in a similar way described earlier, whereby long di-
alogues are only allocated lower User Satisfaction
if they do not involve ground arrangements. Users
do not seem to mind longer dialogues as long as
the system gives a number of implicit confirma-
tions. The dialogue act request info:top level trip
usually occurs at the start of the dialogue and re-
quests the initial travel plan. If there are more than
one of this dialogue act, it indicates that a START-
OVER occurred due to system failure, and this leads
to lower User Satisfaction. A rule containing the
bigram request info:depart day month date+USER
states that if there is more than one occurrence of this
request then User Satisfaction will be lower. USER
is the single category used for user-turns. No auto-
matic method of predicting user speech act is avail-
able yet for this data. A repetition of this DATE
bigram indicates that a misunderstanding occurred
the first time it was requested, or that the task is
multi-leg in which case User Satisfaction is gener-
ally lower.
The tree that uses Binary Task Completion is
identical to the tree described above, apart from
one binary decision which differentiates dialogues
where Task Completion=1 and Task Completion=2.
Instead of making this distinction, it just uses dia-
logue length to indicate the complexity of the task.
In the original tree, long dialogues are not penalized
if they have achieved a complex task (i.e. if Task
Completion=2). The Binary Task Completion tree
has no way of making this distinction and therefore
just penalizes very long dialogues (where TurnsOn-
Task  110). The Feature Usage Frequency for the
Task Completion features is reduced from 31% to
21%, and the Feature Usage Frequency for the log-
file features increases to 47%. We have shown that
this more general tree produces slightly better re-
sults.
6 Results for Identifying Problematic
Dialogues for Data Mining
So far, we have described a PDI that predicts User
Satisfaction as a continuous variable. For data min-
ing, system developers will want to extract dialogues
with predicted User Satisfaction below a particular
threshold. This threshhold could vary during dif-
ferent stages of system development. As the sys-
tem is fine tuned there will be fewer and fewer dia-
logues with low User Satisfaction, therefore in order
to find the interesting dialogues for system develop-
ment one would have to raise the User Satisfaction
threshold. In order to illustrate the potential value
of our PDI, consider an example threshhold of 12
which divides the data into 73.4% good dialogues
where User Satisfaction  12 which is our baseline
result.
Table 3 gives the recall and precision for the PDIs
described above which use hand-labelled Task Com-
pletion and Auto Task Completion. In the data,
26.6% of the dialogues are problematic (User Sat-
isfaction is under 12), whereas the PDI using hand-
labelled Task Completion predicts that 21.8% are
problematic. Of the problematic dialogues, 54.5%
are classified correctly (Recall). Of the dialogues
that it classes as problematic 66.7% are problematic
(Precision). The results for the automatic system
show an improvement in Recall: it identifies more
problematic dialogues correctly (66.7%) but the pre-
cision is lower.
What do these numbers mean in terms of our orig-
inal goal of reducing the number of dialogues that
need to be transcribed to find good cases to use
Task Completion Dialogue Recall Prec.
Hand-labelled Good 90% 84.5%
Hand-labelled Problematic 54.5% 66.7%
Automatic Good 88.5% 81.3%
Automatic Problematic 66.7% 58.0%
Table 3: Precision and Recall for good and prob-
lematic dialogues (where a good dialogue has User
Satisfaction  12) for the PDI using hand-labelled
Task Completion and Auto Task Completion
for system improvement? If one had a budget to
transcribe 20% of the dataset containing 100 dia-
logues, then by randomly extracting 20 dialogues,
one would transcribe 5 problematic dialogues and 15
good dialogues. Using the fully automatic PDI, one
would obtain 12 problematic dialogues and 8 good
dialogues. To look at it another way, to extract 15
problematic dialogues out of 100, 55% of the data
would need transcribing. To obtain 15 problem-
atic dialogues using the fully automatic PDI, only
26% of the data would need transcribing. This is a
massive improvement over randomly choosing dia-
logues.
7 Discussion and Future Developments
This paper presented a Problematic Dialogue Identi-
fier which system developers can use for evaluation
and to extract problematic dialogues from a large
dataset for system development. We describe PDIs
for predicting both Task Completion and User Satis-
faction in the DARPA Communicator October 2001
corpus.
There has been little previous work on recogniz-
ing problematic dialogues. However, a number of
studies have been done on predicting specific errors
in a dialogue, using a variety of automatic and hand-
labelled features, such as ASR confidence and se-
mantic labels (Aberdeen et al, 2001; Hirschberg et
al., 2000; Levow, 1998; Litman et al, 1999). Pre-
vious work on predicting problematic dialogues be-
fore the end of the dialogue (Walker et al, 2002)
achieved accuracies of 87% using hand-labelled fea-
tures (baseline 67%). Our automatic Task Comple-
tion PDI achieves an accuracy of 85%.
Previous work also predicted User Satisfaction
by applying multi-variate linear regression features
with and without DATE features and showed that
DATE improved the model fit from   	
 to
 (Walker et al, 2001). Our best model
has an NJFun: A Reinforcement Learning Spoken Dialogue System 
Diane  L i tman,  Sat inder  S ingh ,  M ichae l  Kearns  and  Mar i l yn  Walker  
AT&T Labs - -  Research 
180 Park  Avenue 
F lo rham Park,  NJ  07932 USA 
{diane,bavej  a,mkearns,walker} @research.att .com 
Abst rac t  
This paper describes NJFun, a real-time spoken dia- 
logue systemthat-provides users with information 
about things to d~ in New Jersey. NJFun auto- 
matically optimizes its dialogue strategy over time, 
by using a methodology for applying reinforcement 
learning to a working dialogue system with human 
users .  
1 In t roduct ion  
Using the formalism of Markov decision processes 
(MDPs) and the algorithms of reinforcement learn- 
ing (RL) has become a standard approach to many 
AI problems that involve an agent learning to 
optimize reward by interaction with its environ- 
ment (Sutton and Barto, 1998). We have adapted 
the methods of RL to the problem of automatically 
learning a good dialogue strategy in a fielded spoken 
dialogue system. Here is a summary of our proposed 
methodology for developing and evaluating spoken 
dialogue systems using R.L: 
? Choose an appropriate reward measure for di- 
alogues, and an appropriate representation for 
dialogue states. 
? Build an initial state-based training system that 
creates an exploratory data set. Despite being 
exploratory, this system should provide the de- 
sired basic functionality. 
? Use these training dialogues to build an empir- 
ical MDP model on the state space. 
? Compute the optimal dialogue policy according 
to this MDF, using RL. 
? Reimplement the system using the learned dia- 
logue policy. 
In this demonstration session paper, we briefly de- 
scribe our system, present some sample dialogues, 
and summarize our main contributions and limita- 
tions. Full details of our work (e.g. our reinforce- 
ment learning methodology, analysis establishing the 
veracity of the MDP we learn, a description of an 
experimental evaluation of NJFun, analysis of our 
learned ialogue strategy) can be found in two forth- 
coming technical papers (Singh et al, 2000; Litman 
et al, 2000). 
2 The  N JFun  System 
NJFun is a reM-time spoken dialogue system that 
provides users with information about things to do 
in New Jersey. 1 An example dialogue with NJFun is 
shown in Figure 1. NJFun is built using an internal 
platform for spoken dialogue systems. NJFun uses 
a speech recognizer with stochastic language models 
trained from example user utterances, and a TTS 
system based on concatenative diphone synthesis. 
Its database is populated from the nj.  onl ine web- 
page to contain information about activities. NJFun 
indexes this database using three attributes: activity 
type, location, and time of day. 
Informally, the NJFun dialogue manager sequen- 
tially queries the user regarding the activity, location 
and time attributes, respectively. NJFun first asks 
the user for the current attribute (and possibly the 
other attributes, depending on the initiative). If the 
current attribute's value is not obtained, NJFun asks 
for the attribute (and possibly the later attributes) 
again. If NJFun still does not obtain a value, N J- 
Fun moves on to the next attribute(s). Whenever 
NJFun successfully obtains a value, it can confirm 
the value, or move on and attempt o obtain the 
next attribute(s)? When NJFun has finished asking 
about the attributes, it queries the database (using 
a wildcard for each unobtained attribute value). 
We use reinforcement learning (RL) to optimize 
dialogue strategy, lq.L requires that all potential ac- 
tions for each state be specified. Note that at some 
states it is easy for a human to make the correct 
action choice. We made obvious dialogue strategy 
choices in advance, and used learning only to opti- 
mize the difficult choices. In NJFun, we restricted 
the action choices to 1) the type of initiative to use 
1This system description section is taken from (Litman et 
al., 2000) .
2Note that it is possible for users to specify multiple at- 
tributes, in any order, in a single utterance. However, NJFun 
will always process multiple attributes using its predefined 
sequential ordering. 
17 
SI: Welcome to N JFun .  How may i help you? ._ 
UI :  I 'd like to find um winetas t ing  in Lambertv i l le  in the morn ing .  
(ASR output: I 'd like to f ind out wineries the in the Lambertville in the morning.) 
$2: Did you say you are interested in Lambertv i l le?  
U2: Yes. 
$3: Did you say you want  to go in the morn ing?  
U3: Yes. 
$4: I found a winery near  Lambertv i l le  that  is open in the morn ing .  It is \ [ . . .  \] 
P lease give me feedback by say ing 'good ' ,  'so-so',  or  'bad ' .  
U4: Good.  
Figure 1: An example dialogue with NJFun (after optimization via RL). 
Act ion  
GreetS 
GreetU  
ReAsk 1 S 
ReAsk  1 M 
Ask2S 
Ask2U 
ReAsk2S 
ReAsk2~ 
m 
Welcome to N JFun.  P lease say an act iv i ty  name or say 'l ist act iv i t ies '  for a list of act iv i t ies I know about .  
Welcome to N JFun.  How may I help you?  
I know about  amusement  parks ,  aquar iums,  cruises, histor ic sites, museums,  parks ,  theaters ,  wineries,  
and  zoos. P lease say an act iv i ty  name f rom this  list. 
P lease tell me the act iv i ty  type.You can  also tell me the locat ion  and time. 
P lease say the name of the town or city that  you are interested in. 
P lease give me more in format ion.  
P lease tell me the name of the town or c i ty  that  you are interested in. 
"P lease  tell me the locat ion that  you are interested in. You can  also tell me the t ime. 
Figure 2: Sample initiative strategy choices. 
when asking or reasking for an attribute, and 2) 
whether to confirm an attribute value once obtained. 
The optimal actions may vary with dialogue state, 
and are subject o active debate in the literature. 
The examples in Figure 2 shows that NJFun can 
ask the user about the first 2 attributes 3 using three 
types of initiative, based on the combination of the 
wording of the system prompt (open versus direc- 
tive), and the type of grammar NJFun uses during 
ASR (restrictive versus non-restrictive). If NJFun 
uses an open question with an unrestricted gram- 
mar, it is using user initiative (e.g., GreetU). If N J- 
Fun instead uses a directive prompt with a restricted 
grammar, the system is using system initiative (e.g., 
GreetS). If NJFun uses a directive question with a 
non-restrictive grammar, it is using mixed initiative, 
because it is giving the user an opportunity to take 
the initiative by supplying extra information (e.g., 
ReAsklM). 
NJFun can also vary the strategy used to confirm 
each attribute. If NJFun asks the user to explicitly 
verify an attribute, it is using explicit confirmation 
(e.g., ExpConf2 for the location, exemplified by $2 
in Figure 1). If NJFun does not generate any con- 
firmation prompt, it is using no confirmation (an 
action we call NoConf). 
Solely for the purposes of controlling its operation 
(as opposed to the learning, which we discuss in a 
moment), NJFun internally maintains an operations 
vector of 14 variables. 2variables track whether the 
system has greeted the user, and which attribute 
the system is currently attempting to obtain. For 
each of the 3 attributes, 4 variables track whether 
'~ "Greet"  is equ iva lent  to  ask ing  for the f i rs t  a t t r ibute .  N J -  
Fun  a lways  uses  sys tem in i t ia t ive  fo r  the  th i rd  a t t r ibute ,  be -  
cause  a t  that  po in t  the  user  can  on ly  prov ide  the time of day .  
the system has obtained the attribute's value, the 
system's confidence in the value (if obtained), the 
number of times the system has asked the user about 
the attribute, and the type of ASR grammar most 
recently used to ask for the attribute. 
The formal state space S maintained by NJFun 
for the purposes of learning is much simpler than 
the operations vector, due to data sparsity concerns. 
The dialogue state space $ contains only 7 variables, 
which are summarized in Figure 3, and is easily com- 
puted from the operations vector. The "greet" vari- 
able tracks whether the system has greeted the user 
or not (no=0, yes=l). "Attr" specifies which at- 
tribute NJFun is currently attempting to obtain or 
verify (activity=l, location=2, time=3, done with 
attributes=4). "Conf" represents the confidence 
that NJFun has after obtaining a value for an at- 
tribute. The values 0, 1, and 2 represent low, 
medium and high ASR confidence. The values 3 
and 4 are set when ASR hears "yes" or "no" after a 
confirmation question. "Val" tracks whether NJFun 
has obtained avalue for the attribute (no=0, yes=l). 
"Times" tracks the number of times that NJFun has 
asked the user about the attribute. "Gram" tracks 
the type of grammar most recently used to obtain 
the attribute (0=non-restrictive, l=restrictive). Fi- 
nally, "history" represents whether NJFun had trou- 
ble understanding the user in the earlier part of the 
conversation (bad=0, good=l). We omit the full 
definition, but as an example, when NJFun is work- 
ing on the second attribute (location), the history 
variable is set to 0 if NJFun does not have an ac- 
tivity, has an activity but has no confidence in the 
value, or needed two queries to obtain the activity. 
In order to apply RL with a limited amount of 
training data, we need to design a small state space 
18 
I greet  a t t r  conf  val t imes gram history \[ 
0,1 1,2,3,4 0,1,2,3,4 0,1 0,1,2 0,1 0,1 I 
Figure 3: State features and values. 
that makes enough critical distinctions to support 
learning. The use of S yields a state space of size 
62. The state space that we utilize here, although 
minimal, allows us to make initiative decisions based 
on the success of earlier exchanges, and confirmation 
decisions based on ASR confidence scores and gram- 
mars. 
In order to learn a good dialogue strategy via RL 
we have to explore the state action space. The 
state/action mapping representing NJFun's initial 
exploratory dialog@ strategy EIC (Exploratory for 
Initiative and Confirmation) is given in Figure 4. 
Only the exploratory portion of the strategy is 
shown, namely all those states for which NJFun has 
an action choice. For each such state, we list the 
two choices of actions available. (The action choices 
in boldface are the ones eventually identified as op- 
timal by the learning process.) The EIC strategy 
chooses randomly between these two actions when in 
the indicated state, in order to maximize xploration 
and minimize data sparseness when constructing our 
model. Since there are 42 states with 2 choices 
each, there is a search space of 242 potential dia- 
logue strategies; the goal of the RL is to identify an 
apparently optimal strategy from this large search 
space. Note that due to the randomization of the 
EIC strategy, the prompts are designed to ensure 
the coherence of all possible action sequences. 
Figure 5 illustrates how the dialogue strategy in 
Figure 4 generates the dialogue in Figure 1. Each 
row indicates the state that NJFun is in, the ac- 
tion executed in this state, the corresponding turn 
in Figure 1, and the reward received. The initial 
state represents hat NJFun will first attempt to ob- 
tain attribute 1. NJFun executes GreetU (although 
as shown in Figure 4, Greets is also possible), gen- 
erating the first utterance in Figure 1. After the 
user's response, the next state represents that N J- 
Fun has now greeted the user and obtained the ac- 
tivity value with high confidence, by using a non- 
restrictive grammar. NJFun chooses not to confirm 
the activity, which causes the state to change but no 
prompt o be generated. The third state represents 
that NJFun is now working on the second attribute 
(location), that it already has this value with high 
confidence (location was obtained with activity af- 
ter the user's first utterance), and that the dialogue 
history is good. This time NJFun chooses to confirm 
the attribute with the second NJFun utterance, and 
the state changes again. The processing of time is 
similar to that of location, which leads NJFun to the 
final state, where it performs the action "Tell" (cor- 
State  
C V t g 
0 1 0 0 0 0 0 
1 1 0 0 1 0 0 
1 1 0 1 0 0 0 
1 1 0 1 0 1 0 
1 1 1 1 0 0 0 
1 1 1 1 0 1 0 
1 1 2 I 0 0 0 
1 1 2 1 0 1 0 
1 1 4 0 0 0 0 
1 1 4 0 1 0 
1 2 0 0 0 0 
1 2 0 0 0 0 
1 2 0 0 1 0 
1 2 0 0 1 0 
1 2 0 1 0 0 
1 2 0 1 0 0 
1 2 0 1 0 1 
I 2 0 I 0 I 
1 2 i 1 0 0 
1 2 1 1 0 0 
1 2 1 1 0 1 
1 2 1 1 0 1 
1 2 2 1 0 0 
1 2 2 1 0 0 
I 2 2 1 0-  1 
1 2 2 1 0 1 
I 2 4 0 0 0 
1 2 4 0 0 0 
1 2 4 0 I 0 
1 2 4 0 1 0 
1 3 0 1 0 0 
1 3 0 1 0 0 
1 3 0 1 0 1 
1 3 0 1 0 1 
1 3 1 1 0 0 
1 3 1 1 0 0 
1 3 1 1 0 1 
i 3 1 I 0 i 
1 3 2 1 0 0 
1 3 2 1 0 0 
1 3 2 1 0 1 
i 3 2 1 0 I 
Act ion  Choices  
GreetS ,GreetU  
ReAsk  1 S ,ReAsk  1 M 
NoConf ,  ExpConf l  
NoConf ,  ExpConf l  
NoConf ,ExpConf l  
NoConf ,  ExpConf l  
NoConf ,  ExpConf l  
NoConf ,  ExpConf l  
ReAsk lS ,ReAsk lM 
0 ReAsk lS ,ReAsk lM 
0 - Ask2S,Ask2U 
1 Ask2S,Ask2U 
0 ReAsk2S,ReAsk2M 
1 ReAsk2S,ReAsk2  M 
0 NoConf ,  ExpConf2  
1 NoConf, ExpConf2  
0 NoConf, ExpConf2  
1 NoConf ,  ExpConf2  
0 NoConf ,  ExpConf2  
1 NoConf ,ExpConf2  
0 NoConf ,  ExpConf2  
1 NoConf ,  ExpConf2  
0 NoConf ,  ExpConf2  
1 NoConf ,  ExpConf2  
0 NoConf ,  ExpConf2  
1 NoConf, ExpConf2 
0 ReAsk2S,ReAsk2M 
1 ReAsk2S,ReAsk2M 
0 ReAsk2S,R .eAsk2M 
1 ReAsk2S,ReAskSM 
0 NoConf ,  ExpConf3  
1 NoConf ,  ExpConf3  
0 NoConf, ExpConf3 
I NoConf, ExpConf3 
0 NoConf, ExpConf3  
1 NoConf ,  ExpConf3  
0 NoConf ,  ExpConf3  
1 NoConf ,  ExpConf3  
0 NoConf ,ExpCon: f3  
1 NoConf ,ExpConf3  
0 NoConf ,  ExpConf3  
I NoConf, ExpConf3  
Figure 4: Exploratory portion of EIC strategy. 
State  Act ion Turn Reward  
gaevtgh  
0100000 GreetU S1 0 
I 121000 NoConf  0 
1 2 2 1 0 0 1 ExpConf2  $2 0 
1 3 2 1 0 0 1 ExpConf3  $3 0 
1 4 0 0 0 0 0 Tell S4 1 
Figure 5: Generating the dialogue in Figure 1. 
responding to querying the database, presenting the 
results to the user, and asking the user to provide a 
reward). Note that in NJFun, the reward is always 
0 except at the terminal state, as shown in the last 
column of Figure 5. 
3 Sample Dialogues 
Figures 1 and 6 present two dialogues that were gen- 
erated using different dialogue strategies in the EIC 
search space. For example, although NJFun begins 
both dialogues in the first state in Figure 4, NJFun 
executes GreetU in one dialogue and GreetS in the 
other. Similarly, although the next state of NJFun 
19 
SI: Welcome to N JFun.  P lease say an act iv i ty  name or say 'list act iv i t ies '  for  a list of act iv i t ies I know about .  
U I :  I would like to visit a h istor ic  site in S tanhope this  morn ing.  
(ASR output: I would like to the zoo historic sites in stanhope historic) 
$2: Did you say you are interested in going to a zoo?  
U2: No. 
$3: I know about  amusement  parks ,  aquar iums,  cruises, histor ic sites, museums,  parks,  theaters ,  wineries, and  zoos. 
P lease say an act iv i ty  name f rom this list. 
U3: I would like to visit a h istor ic  site. (ASR output: I would like to visit historic sites) 
$4: Did you say you are interested in going to a h istor ic  site? 
U4: Yes. 
$5: P lease give me more in format ion .  
U5: (user says nothing) 
S6: P lease tell me the locat ion that  you are interested in. You can also tell me the time. 
U6: Visit a histor ic .  (ASR output: What  historic) 
$7: What  t ime of  the day  do you want  to go? 
U7: S tanhope.  (no ASR output) 
$8: Do you want  to go in the morn ing ,  in the a f ternoon,  or  in the evening? 
US: Morn ing.  
S9: Did you say you want  to go in the morn ing?  
U9: Yes. 
S l0 :  I found 11 histor ic  sites in New Jersey that  are open in the morn ing .  The  first 3 \ [ . . .  \] Would you like to  hear  more?  
UiO: No. 
$11: Thank~ou for using the system.  P lease give me feedback by say ing 'good ' ,  'so-so',  or  'bad ' .  
U I I :  Bad'. _. 
Figure 6: An example training dialogue with NJFun. 
is the same in both dialogues ("1 1 2 1 0 0 0"), the 
activity is not confirmed in the first dialogue. 
4 Contributions 
The main contribution of this work is that we 
have developed and empirically validated a practi- 
cal methodology for using RL to build a real dia- 
logue system that optimizes its behavior from dia- 
logue data. Unlike traditional approaches to learn- 
ing dialogue strategy from data, which are limited 
to searching a handful of policies, our RL approach 
is able to search many tens of thousands of dialogue 
strategies. In particular, the traditional approach 
is to pick a handful of strategies that experts in- 
tuitively feel are good, implement each policy as a 
separate system, collect data from representative hu- 
man users for each system, and then use standard 
statistical tests on that data to pick the best sys- 
tem, e.g. (Danieli and Gerbino, 1995). In contrast, 
our use of RL allowed us to explore 242 strategies 
that were left in our search space after we excluded 
strategies that were clearly suboptimal. 
An empirical validation of our approach is de- 
tailed in two forthcoming technical papers (Singh 
et al, 2000; Litman et al, 2000). We obtained 311 
dialogues with the exploratory (i.e., training) ver- 
sion of NJFun, constructed an MDP from this train- 
ing data, used RL to compute the optimal dialogue 
strategy in this MDP, reimplemented NJFun such 
that it used this learned dialogue strategy, and ob- 
tained 124 more dialogues. Our main result was 
that task completion improved from 52% to 64% 
from training to test data. Furthermore, analysis 
of our MDP showed that the learned strategy was 
not only better than EIC, but also better than other 
fixed choices proposed in the literature (Singh et al, 
2000). 
5 Limitations 
The main limitation of this effort to automate the 
design of a good dialogue strategy is that our current 
framework has nothing to say about how to choose 
the reward measure, or how to best represent dia- 
logue state. In NJFun we carefully but manually de- 
signed the state space of the dialogue. In the future, 
we hope to develop a learning methodology to auto- 
mate the choice of state space for dialogue systems. 
With respect o the reward function, our empirical 
evaluation investigated the impact of using a number 
of reward measures (e.g., user feedback such as U4 in 
Figure 1, task completion rate, ASR accuracy), and 
found that some rewards worked better than others. 
We would like to better understand these differences 
among the reward measures, investigate the use of 
a learned reward function, and explore the use of 
non-terminal rewards. 
Re ferences  
M. Danieli and E. Gerbino. 1995. Metrics for eval- 
uating dialogue strategies in a spoken language 
system. In Proceedings of the 1995 AAA1 Spring 
Symposium on Empirical Methods in Discourse 
Interpretation and Generation, pages 34-39. 
D. Litman, M. Kearns, S. Singh, and M. Walker. 
2000. Automatic optimization of dialogue man- 
agement. Manuscript submitted for publication. 
S. Singh, M. Kearns, D. Litman, and M. Walker. 
2000. Empirical evaluation of a reinforcement 
learning spoken dialogue system. In Proceedings 
of AAAI 2000. 
R. S. Sutton and A. G. Barto. 1998. Reinforcement 
Learning: An Introduction. MIT Press. 
20 
Training a Dialogue Act Tagger For Human-Human and Human-Computer
Travel Dialogues
Rashmi Prasad and Marilyn Walker
AT&T Labs - Research
180 Park Avenue, Florham Park, NJ 07932, U.S.A.
rjprasad, walker@research.att.com
Abstract
While dialogue acts provide a useful
schema for characterizing dialogue be-
haviors in human-computer and human-
human dialogues, their utility is limited
by the huge effort involved in hand-
labelling dialogues with a dialogue act la-
belling scheme. In this work, we exam-
ine whether it is possible to fully auto-
mate the tagging task with the goal of en-
abling rapid creation of corpora for eval-
uating spoken dialogue systems and com-
paring them to human-human dialogues.
We report results for training and test-
ing an automatic classifier to label the in-
formation provider?s utterances in spoken
human-computer and human-human dia-
logues with DATE (Dialogue Act Tagging
for Evaluation) dialogue act tags. We
train and test the DATE tagger on var-
ious combinations of the DARPA Com-
municator June-2000 and October-2001
human-computer corpora, and the CMU
human-human corpus in the travel plan-
ning domain. Our results show that we
can achieve high accuracies on the human-
computer data, and surprisingly, that the
human-computer data improves accuracy
on the human-human data, when only
small amounts of human-human training
data are available.
1 Introduction
Recent research on dialogue is based on the as-
sumption that dialogue acts provide a useful way
of characterizing dialogue behaviors in both human-
human (HH) and human-computer (HC) dialogue
(Isard and Carletta, 1995; Shriberg et al, 2000; Di
Eugenio et al, 1998; Cattoni et al, 2001). Previous
research has used dialogue act tagging for tasks such
as improving recognition performance (Shriberg et
al., 2000), identifying important parts of a dialogue
(Finke et al, 1998), evaluating and comparing spo-
ken dialogue systems (Walker et al, 2001c; Cattoni
et al, 2001; Hastie et al, 2002), as a constraint on
nominal expression generation (Jordan, 2000), and
for comparing HH to HC dialogues (Doran et al,
2001).
Our work builds directly on the previous applica-
tion of the DATE (Dialogue Act Tagging for Eval-
uation) tagging scheme to the evaluation and com-
parison of DARPA Communicator dialogues. The
hypothesis underlying the use of dialogue act tag-
ging in spoken dialogue evaluation is that a system?s
dialogue behaviors have a strong effect on its usabil-
ity. Because Communicator systems have unique
dialogue strategies, and a unique way of represent-
ing and achieving particular communicative goals,
DATE was developed to consistently label dialogue
behaviors across systems so that the potential util-
ity of dialogue act tagging could be explored. In
previous work, Walker and Passonneau defined the
DATE scheme, and labelled the system utterances in
the June 2000 data collection of 663 dialogues from
nine participating Communicator systems (Walker
et al, 2001c; Walker et al, 2001a). They then
derived dialogue act metrics from the DATE tags
and showed that when these metrics were used in
the PARADISE evaluation framework (Walker et al,
1997) that they improved models of user satisfaction
by an absolute 5   , and that the new metrics could be
used to understand which system?s dialogue strate-
gies were most effective.
     Philadelphia, July 2002, pp. 162-173.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
A major part of evaluation effort using dialogue
act tagging, however, is to actually label the dia-
logues with the dialogue act tags. In previous work
(Walker et al, 2001c), the DATE labelling of the
June-2000 corpus was done using a semi-automatic
method that involved collection of a large number
of utterance patterns from the different sites par-
ticipating in the collection and subsequent hand la-
belling of these patterns. The 100   coverage and
accuracy achieved by the pattern matcher that was
implemented for labelling the system utterances was
crucially at the cost of maintaining a large labelled
pattern database.1 Furthermore, since the collected
patterns were very specific and often exact dupli-
cates of the system utterances in the dialogues, slight
variations in the system utterances over time led to
a reduction in coverage of the pattern matcher. For
example, on the October-2001 collection, the tagger
was able to label only 60   of the data. Accounting
for the unmatched (and thus unlabeled) utterances
turned out again to be a tedious process of manually
augmenting the pattern database with new utterance
patterns.
We examine whether it is possible to fully au-
tomate the tagging task with the goal being to en-
able rapid creation of new spoken dialogue corpora
that can be submitted to evaluation procedures that
rely on tagging schemes like DATE. Our hypothesis,
arising from rough qualitative studies, is that within
the same domain there are large similarities not only
between different HC dialogues (across system as
well as over a period of time) such as the Commu-
nicator dialogues, but also between HC dialogues
and HH dialogues. Figure 1 shows a HC dialogue
and Figure 2 shows a HH dialogue from the CMU-
corpus (U=user, A=human agent, S=computer sys-
tem) in which the information providers? utterances
for information gathering and information presenta-
tion across both dialogues are very similar. Thus,
based on this and other similar observations, we de-
cided to test the extent to which sufficiently general
tagging rules can be learned from HC data to label
other HC data and HH data in similar domains with
high accuracy.
We focus on labelling only the information
providers? side of the dialogues. The primary mo-
tivation for this is to be able to characterize system
dialogue strategies for cross-site evaluation. System
dialogue behaviors have a large impact on users? ex-
1It should be noted, however, that the task of labelling the
utterances in the pattern database was still a considerable sim-
plification over entire dialogues being labelled by human anno-
tators, which is the usual practice in tagging tasks.
S: Leaving from Phoenix, And, what city are you flying to?
U: SEOUL KOREA
S: Flying from Phoenix to Seoul, What date would you like
to fly?
U: WEDNESDAY OCTOBER ELEVENTH
S: Leaving on the 11th of October, What time woodja like to
depart?
U: ANY TIME
S: From Seoul. Are you returning to Phoenix?
U: YES
S: And what date didja wanna return?
U: SUNDAY OCTOBER FIFTEENTH
S: I may have misheard You saying the 5th of October.
Please be aware that the return date must be later than the
departure date. Please tell me what date you would like too
return.
Figure 1: Excerpt of a HC Dialogue from the June-
2000 Corpus
U: hello
A: people?s travel. what city do you want to fly to
U: chicago
A: on what date
U: on the twenty second of may there will be two people
travelling
A: what time do you need to depart
U: as soon as possible after five p.m.
A: the first flight after five p.m. on that date is at five thiry
five p.m. arriving in chicago at six oh six p.m. on u.s. air
U: is that o?hare
A: yes it is
U: okay. i?ll take that
A: on what date do you want to return
U: the following sunday
A: at what time do you need to depart
U: i want to arrive no later than seven in the evening
Figure 2: Excerpt of a HH Dialogue from the CMU-
Corpus
perience and on users? behavior. Furthermore, users
in the HC dialogues rarely took initiative and their
utterances showed very little variation (Doran et al,
2001). In addition, we believe that once the system
side of the dialogues is labelled, it will be much eas-
ier to label the user side of the dialogues.
We report the results of applying a rule-induction
method to train and test DATE taggers on various
combinations of the DARPA Communicator June-
2000 and October-2001 HC corpora, and the CMU
HH corpus in the travel planning domain. The ac-
curacy of a DATE tagger trained and tested on the
June-2000 corpus is 98.5   . On the October-2001
corpus, this tagger achieves an accuracy of only
71.8   , but adding 2000 utterances from the 2001
corpus to the training data improves accuracy on the
rest of the 2001 corpus to 93.8   . The accuracy of
a tagger trained on the HC corpora and tested on
the CMU-corpus is 36.7   (a significant improve-
ment over the baseline of 28   ). A DATE tagger
trained on 305 examples of the HH data achieves
an accuracy of 48.75   , but the addition of the HC
training data improves accuracy to 55.5   (majority
class baseline=28   ). This pair of results demon-
strates quantitatively that the HC data can be used to
improve performance of a tagger for HH data. How-
ever, a larger training corpus of HH data improves
performance to 76.6   accuracy, as estimated by 20-
fold cross-validation on the CMU-corpus.
Section 2 describes the corpora, the DATE dia-
logue act tagging scheme, methods for tagging the
corpora for the experiments, and the features used
to train a DATE dialogue act tagger for DATE la-
belling of the corpora. Section 3 presents our re-
sults. We postpone discussion and comparison with
related work till Section 4.
2 Corpus, Data, Methods
Our experiments apply the rule learning program
RIPPER (Cohen, 1996) to train a DATE dialogue act
tagger for the utterances of the information provider
in HC and HH travel planning dialogues. Like
other automatic classifiers, RIPPER takes as input the
names of a set of classes to be learned, the names
and ranges of values of a fixed set of features, and
training data specifying the class and feature val-
ues for each example in a training set. Its output is
a classication model for predicting the class of fu-
ture examples. In RIPPER, the classification model is
learned using greedy search guided by an informa-
tion gain metric, and is expressed as an ordered set
of if-then rules. Although any of several automatic
classifiers could be used to train an automatic DATE
tagger, RIPPER supports textual features, which are
important for this problem, and outputs if-then rules
that are easy to understand and which make clear
which features are useful to the DATE tagger when
classifying utterances.
To apply RIPPER, the utterances in the corpus
must be encoded in terms of a set of classes (the out-
put classification) and a set of input features that are
used as predictors for the classes. Below we describe
the corpora, the classes derived from the DATE tag-
ging scheme, the methods used for tagging the cor-
pora using the DATE scheme, and the features that
are extracted from the dialogue in which each utter-
ance occurs.
2.1 Travel Planning Corpora
Our experiments utilize both HC and HH dialogues
in the travel planning domain. The DARPA Com-
municator HC dialogue corpus consists of the June-
2000 corpus and the October-2001 corpus. The
June-2000 corpus contains 663 experimental dia-
logues collected during a three week period in June
of 2000 for conversations between human users and
9 different Communicator travel planning systems.
The October-2001 corpus contains 1252 experimen-
tal dialogues collected between April and October of
2001 for conversations between human users and 8
different COMMUNICATOR travel planning systems.
The dialogues were quite complex, ranging between
simple one way trips requiring no ground arrange-
ments to multileg trips to international or domes-
tic destinations that required car and hotel arrange-
ments. The dialogues typically lasted between 2 and
10 minutes. There was a great deal of variation in
the dialogue strategies implemented by the different
systems, both between the sites during each collec-
tion as well as within a single site across the differ-
ent collections, from 2000 to 2001. There were a to-
tal of 22930 system utterances in the June-2000 cor-
pus and a total of 69766 utterances in the October-
2001 corpus. Each dialogue interaction was logged
by each system using a shared logfile standard. We
were primarily interested in three logged features:
(1) the text of each system utterance; (2) what the
recognizer understood for each user utterance; and
(3) the transcription that each site provided for what
the user actually said. We describe below in Section
2.4 how we used these three logfile features to derive
the features used to train the DATE tagger.
The HH dialogue corpus consists of the CMU-
corpus (Eskenazi et al, 1999). Dialogues in the
travel planning domain were collected by the Com-
municator group at Carnegie Mellon University
(CMU), who arranged with the onsite travel agency
People?s Travel to record calls from a number of vol-
unteer subjects who called the human travel agent
to plan intended trips. These calls were then tran-
scribed and the recordings and the transcriptions
were made available to members of the Communi-
cator community. Labellers at our site subsequently
segmented the travel agent side of the conversation
into utterances where each utterance realized a sin-
gle dialogue act. We used this utterance level seg-
mentation to define the unit for tagging in the exper-
iments described below. The CMU-corpus consists
of 38 dialogues with a total of 1062 travel agent ut-
terances.
2.2 Class Assignment
The classes used to train the DATE tagger are
derived directly from the DATE tagging scheme
(Walker et al, 2001c). DATE classifies each ut-
terance along three cross-cutting orthogonal dimen-
sions of utterance classification: (1) a SPEECH ACT
dimension; (2) a CONVERSATIONAL-DOMAIN di-
mension; and (3) a TASK-SUBTASK dimension. The
SPEECH ACT and CONVERSATIONAL-DOMAIN di-
mensions should be general across domains, while
the TASK-SUBTASK dimension involves a task
model that is not only domain specific, but could
vary from system to system because some systems
might make finer-grained subtask distinctions.
The SPEECH ACT dimension captures distinc-
tions between distinct communicative goals such as
requesting information (REQUEST-INFO), present-
ing information (PRESENT-INFO) and making offers
(OFFER) to act on behalf of the caller. The types of
speech acts are specified and illustrated in Figure 3.
Speech-Act Example
REQUEST-INFO And, what city are you flying to?
PRESENT-INFO The airfare for this trip is 390 dol-
lars.
OFFER Would you like me to hold this op-
tion?
ACKNOWLEDGMENT I will book this leg.
BACKCHANNEL Okay.
STATUS-REPORT Accessing the database; this might
take a few seconds.
EXPLICIT-
CONFIRM
You will depart on September 1st. Is
that correct?
IMPLICIT-
CONFIRM
Leaving from Dallas.
INSTRUCTION Try saying a short sentence.
APOLOGY Sorry, I didn?t understand that.
OPENING-
CLOSING
Hello. Welcome to the C M U Com-
municator.
Figure 3: Example Speech Acts in DATE
The CONVERSATIONAL-DOMAIN dimension dis-
tinguishes between talk devoted to the task of book-
ing airline reservations (?about-task?) versus talk
devoted to maintaining the verbal channel of com-
munication (?about-communication?) (Allen and
Core, 1997). DATE adds a third domain called
?about-situation-frame?, to distinguish utterances
that provide information about the interactional con-
text, e.g. Try saying a short sentence, or I know
about 500 international destinations.
The TASK-SUBTASK dimension focusses on spec-
ifying which subtask of the travel reservation task
the utterance contributes to. Some examples are
given in Figure 4. This dimension distinguishes
among 28 subtasks, some of which can also be
grouped at a level below the top level task. The
TOP-LEVEL-TRIP task describes the task which con-
tains as its subtasks the ORIGIN, DESTINATION,
DATE, TIME, AIRLINE, TRIP-TYPE, RETRIEVAL
and ITINERARY tasks. The GROUND task includes
both the HOTEL and CAR subtasks. The HOTEL
task includes both the HOTEL-NAME and HOTEL-
LOCATION subtasks.
Some utterances, especially about-situation-
frame utterances such as instructions and apologies
are not specific to any task. For example, apologies
made by the system about a misunderstanding
can be made within any subtask. We give these
utterances a ?meta? value in the task dimension.
Task Example
TOP-LEVEL-
TRIP
What are your travel plans?
ORIGIN And, what city are you leaving from?
DESTINATION And, where are you flying to?
DATE What day would you like to leave?
TIME Departing at what time?.
AIRLINE Did you have an airline preference?
RETRIEVAL Accessing the database; this might take a
few seconds.
ITINERARY The airfare for this trip is 390 dollars.
GROUND Did you need to make any ground arrange-
ments?.
HOTEL Did you need a hotel?.
HOTEL-
NAME
Do you have a preferred hotel chain?.
HOTEL-
LOCATION
Would you like a hotel near downtown or
near the airport?.
CAR Do you need a car in San Jose?
CAR-TYPE What kind of car did you want?
CAR-RENTAL Do you have a preferred rental agency?
Figure 4: Example Subtasks in DATE
It is possible to achieve very specific labelling
of system utterances by applying all three dimen-
sions simultaneously. For example, one set of out-
put classes for the DATE tagger consists of the
combination of all three classes so that an utter-
ance such as I found three ights that match your
request is classified as ABOUT-TASK:PRESENT-
INFO:FLIGHT.2 However, the DATE scheme also
makes it possible to train and test a DATE tag-
ger for just the SPEECH-ACT dimension or just the
TASK dimension. Figure 5 shows utterances from
a June-2000 dialogue fragment that are classified
along each of the three DATE dimensions.
Tagging utterances along the SPEECH ACT dimen-
sion provides the most general tagging. This level of
categorization is task-independent and possibly sit-
uation independent, ie. from HC to HH dialogues.
One set of experiments simply tests performance of
a DATE tagger for the speech-act dimension on the
HC dialogue data. In addition, we also train a DATE
tagger on the HC dialogues using only the speech
2DATE labels that are specified for all the three dimensions
have the dimension values given in three fields separated by ?:?.
The first field contains the value for the Conversational-Domain
Dimension, the second for the Speech-Act Dimension, and the
third for the Task-Subtask Dimension.
act dimension for the purpose of applying it to a test
set of the CMU-corpus of HH dialogues.3
2.3 Preparation of Training and Test Data via
DATE Tagging
The DATE labelling of the June-2000 data was done
with a semi-automatic tagger: an utterance or ut-
terance sequence is identified and labelled automat-
ically by reference to a database of utterance pat-
terns hand-labelled with DATE tags. The collection
and DATE labelling of the utterance patterns was
done in cooperation with site developers. As dis-
cussed above, these patterns for the 2000 data set
were often quite specific, and often involved whole
utterances. However, since the systems use template
based generation and have only a limited number
of ways of saying the same content, relatively few
utterance patterns needed to be hand-labelled when
compared to the actual number of utterances occur-
ring in the corpus. Further abstraction on the pat-
terns was done with a named-entity labeller which
replaces specific tokens of city names, airports, ho-
tels, airlines, dates, times, cars, and car rental com-
panies with their generic type labels. For example,
what time do you want to leave  AIRPORT  on
 DATE-TIME  ? is the typed utterance for what
time do you want to leave Newark International on
Monday?. For the 2000 tagging, the number of ut-
terances in the pattern database was 1700 whereas
the total number of utterances in the 663 dialogues
was 22930. The named-entity labeller was also ap-
plied to the system utterances in the corpus. We
collected vocabulary lists from all the sites for the
named-entity labelling task. In most cases, systems
had preclassified the individual tokens into generic
types.
The tagger implements a simple pattern match-
ing algorithm to do the dialogue act labelling: for
each utterance pattern in the pattern database, the
tagger attempts to find a match in the dialogues;
if the match succeeds, the DATE label of that pat-
tern is assigned to the matching utterance in the dia-
logue. The matching ignores punctuation since sys-
tems vary in the way they record punctuation.4
Certain utterances have different communicative
functions depending on the context in which they
3Tagging utterances along the TASK dimension may pro-
vide a rough notion of discourse segmentation in that utterances
about the same task may be grouped together. Due to lack of
space, however, we do not present results for task tagging.
4Ignoring punctuation does not, however, create an utterance
segmentation problem for the tagger. We assume that the utter-
ances in the pattern database provide the reference points for
utterance boundaries.
occur. For example, phrases like leaving in the
 DATE-TIME  are implicit confirmations when
they constitute an utterance on their own, but are part
of the flight information presentation when they oc-
cur embedded in utterances such as I have one ight
leaving in the  DATE-TIME  . To prevent incor-
rect labelling for such ambiguous cases, the pattern
database is sorted so that sub-patterns are listed later
than the patterns within which they are embedded,
and the pattern matcher is forced to match patterns
in their order of occurrence in the database.
    Systems 
   Lists from 
Named?Entity
Named?Entity
DATE contextual rules
Dialogues with
       Labels
Named Entity
  from Systems
Dialogue Logfiles
   Classification 
   Labeller
Pattern Database
    with DATE 
  
DATE
Labelled Dialogues
       DATE
Pattern Matcher
Figure 6: The DATE Dialogue Act Tagger
While this tagger achieved 100   accuracy for the
2000 data by using many specific patterns, when ap-
plied to the 2001 corpus it was able to label only
60   of the data. On examination of the unlabelled
utterances, we found that many systems had aug-
mented their inventory of named-entity items as well
as system utterances from the 2000 to the 2001 data
collection. As a result, there were many new pat-
terns unaccounted for in the existing named-entity
lists as well as in the pattern database. In an at-
tempt to cover the remaining 40   of the data, we
therefore augmented the named-entity lists by ob-
taining a new set of preclassified vocabulary items
from the sites, and added 800 hand-labelled pat-
terns to the pattern database. For the labelling of
any additional unaccounted-for patterns, we imple-
mented a contextual rule-based postprocessor that
looks at the surrounding dialogue acts of an un-
matched utterance within a turn and attempts to la-
bel it. The contextual rules are intended to capture
rigid system dialogue behaviors that are reflected
in the DATE sequences within a turn.5 For exam-
ple, one very frequently occurring DATE sequence
within system turns is about task:present info:flight,
about task:present info:price, about task:offer:flight. The
rule using this contextual information can be infor-
mally stated as follows: if in a turn, the first two
utterances are labelled as about task:present info:flight
and about task:present info:price, and the third utterance
is unlabelled, assign the third utterance the label
about task:offer:flight. Not all turn-internal DATE se-
quences are used as contextual rules, however, be-
cause many of them are highly ambiguous. For ex-
ample, about communicaton:apology:meta slu reject can be
followed by a system instruction as well as any kind
of request for information (typically) repeated from
the previous system utterance. Figure 6 shows the
current DATE tagging system, augmented with the
DATE rule-based postprocessor.
With the 2000 tagger augmented with the addi-
tional named-entity items, utterance patterns, and
the postprocessor, we were able to label 98.4   of
the (69766) utterances in the 2001 corpus.
We conducted a hand evaluation of 10 dialogues
which we selected randomly from each system. The
evaluation of the total 80 dialogues shows that we
achieved 96   accuracy on the 2001 tagging.
In order to label the HH corpus of 1062 ut-
terances, we started with 10 dialogues (305 utter-
ances) labelled with the CSTAR dialogue act tag-
ging scheme (Finke et al, 1998; Doran et al, 2001).
We automatically converted the labels to DATE, and
then hand-corrected them. We labelled the rest of
the HH data by training a DATE tagger, applying it
to the remainder of the corpus, and hand-correcting
the results.
2.4 Feature Extraction
The corpus is used to construct the machine learning
features as follows. In RIPPER, feature values are
continuous (numeric), set-valued (textual), or sym-
bolic. We encoded each utterance in terms of a set
of 19 features that were either derived from the log-
files, derived from human transcription of the user
utterances, or represent aspects of the dialogue con-
text in which each utterance occurs.
The complete feature set used by the machine
learner is described in Figure 7. The features fall
into three categories: (1) target utterance features
; (2) context features ; and (3) whole dialogue fea-
tures.
5The logfile standard distinguishes system and user turns
within the dialogues.
 target utterance features: utt-string, contains-word-
FLIGHT-or-AIRLINE, contains-word-HOTEL-or-
ROOM, contains-word-RENTAL-or-CAR, contains-
word-CITY-or-AIRPORT, contains-word-DATE-TIME,
pattern-length.
 context features: left-sys-utt-string, right-sys-utt-
string, da-num, position-in-turn, left?dacontext1,
left-da-context2, usr-orig-string, usr-typed-string,
rec-orig-string, rec-typed-string usr-rec-string-identity.
 whole dialogue features: system-name, turn-number.
Figure 7: Features used by the Machine Learner
The target utterance features include the target
utterance string for which the dialogue act is to
be predicted (utt-string), and a set of features
derived from the named-entity labelling about what
semantic types are instantiated in the target string.
For example the feature contains-word-FLIGHT-
or-AIRLINE is represented by a boolean variable
specifying whether the utterance string contains
the words FLIGHT or AIRLINE. Similar features
are contains-word-HOTEL-or-ROOM, contains-
word-RENTAL-or-CAR, contains-word-CITY-or-
AIRPORT, and contains-word-DATE-TIME. The
pattern-length feature encodes the character length
of the target utterance. The motivation for these
features is to represent basic aspects of the target
utterance, e.g. its length, and the lexical items and
semantic types that appear in the utterance.
The context features encode simple aspects
about the context in which the target utterance oc-
curs. Two of these represent the system utter-
ance strings to the left and right of the target ut-
terance (left-sys-utt-string and right-sys-utt-string).
The left-da-context1 and left-da-context2 features
represent the left unigram and bigram dialogue act
context of the target utterance; this goes beyond
the target turn to only the last dialogue act in the
previous system turn. The da-num feature encodes
the number of dialogue acts in the target turn and
the position-in-turn feature encodes the position of
the target utterance in its turn. In addition, the
user?s previous utterance is represented as part of the
context, both in terms of automatically extractable
features like what the automatic speech recognizer
thought the user said (rec-orig-string), and a version
of this on which the named-entity labeller has been
run (rec-typed-string), as well as in terms of hu-
man generated transcriptions of the user?s utterance.
Features based on the transcriptions include the
original human transcription (usr-orig-string) and
the transcription after named-entity tagging (usr-
typed-string). The usr-rec-string-identity feature is a
Training Data Test Data Dim Maj. Cl. Baseline Acc. (SE)
JUNE-2000 4fold Xval JUNE-2000 All 6.45% 98.5% 0.11%
JUNE-2000 OCTOBER-2001 All 9.52   71.82   0.17  
JUNE-2000 & 2000 ex-
amples of October-2001
October-2001 w/out 2000 All 10.18   93.82   0.09  
Table 1: Results for Identifying Three-Way DATE Tags in the October-2001 Communicator Corpus, (Dim
= Dimension of Date used for output classification (Maj. Cl. = Majority Class, Acc = Accuracy, SE =
Standard Error)
boolean feature based on comparing the user?s tran-
scribed utterance with the recognizer?s hypothesis
of what the user said, using simple string-identity.
Some applications of DATE tagging would not use
features derived from human generated transcrip-
tions so the experiments below report accuracy fig-
ures for DATE taggers which ignore these features.
The motivation for the context features is to repre-
sent aspects of the context in which the utterance
occurs in terms of a window of surrounding lexical
items and dialogue acts.
The whole dialogue features are the name of the
site whose system generated the dialogue (system-
name), and the turn number of the target utterance
within the whole dialogue (turn-number). For HH
dialogues the system-name has the value ?human?.
The motivation for including the system-name fea-
ture is to see whether there are any aspects of the di-
alogue act realizations that are specific to particular
systems. The motivation for the turn-number fea-
ture is that particular types of dialogue acts are more
likely to occur in particular phases of the dialogue.
3 Results
Given the corpora and features described above, we
constructed a set of training and test files for use
with the RIPPER engine. Each spoken dialogue ut-
terance by the system or by the human travel agent
in the corpora are represented in terms of the fea-
tures and class values described above. One of the
primary goals in these experiments is to test the abil-
ity of the trained DATE tagger to learn and apply
general rules for dialogue act tagging. In the HC
data, we examine how a DATE tagger trained on
the June-2000 corpus performs on the October-2001
corpus, with and without 2000 labelled examples of
October-2001 training data. For the HH data, we
examine how a DATE tagger trained on the two HC
corpora (June-2000 and October-2001) performs on
the CMU-corpus, with and without 305 utterances of
HH labelled training data. We first report accuracy
results for a DATE tagger trained and tested on the
HC June-2000 and October-2001 corpora and then
report results for the HH CMU-corpus.
Human-Computer Results: Table 1 shows that
the reported accuracies for the HC experiments are
signifcantly better than the baseline in each case and
the differences between the rows are also statisti-
cally significant. The first row shows that the ac-
curacy of a DATE tagger trained and tested using
four-fold cross-validation on the June-2000 data is
98.5   with a standard error of only 0.11   . This
indicates that after training on 75   of the data,
there are few unexpected utterances in the remain-
ing 25   . However, the second row shows that a
DATE tagger trained on the 9 systems represented
in the June-2000 corpus and tested on the (sub-
set) 8 systems represented in the October-2001 cor-
pus only achieves 71.82   accuracy. This roughly
matches our earlier finding in Section 2.3 that dur-
ing the interval from June-2000 to April-2001 when
the 2001 data collection began, many changes had
been made to the Communicator systems and that
the learned rules from the June-2000 data were not
able to generalize as well to the October-2001 cor-
pus.. The third row shows that the overall variation
in the data is still low: when 2000 labelled examples
of the October-2001 data are added to the June-2000
data for training, the accuracy increases to 93.82   .
This suggests that adding a small amount of new la-
belled training data for successive versions of a sys-
tem would support high accuracy DATE tagging for
the new version of the system.
Some of the rules that RIPPER learned from the
HC corpora for predicting the DATE tag for ut-
terances requesting information about the origin
city, e.g. What city are you departing from?,
and requesting information about the destination
city, e.g. Where are you traveling to?, are shown
in Figure 8. The figure shows that all of the
rules for both about task:request info:orig city and small
about task:request info:dest city utilize the utter-
ance string feature. This suggests that single words
in utterances can be regarded as reliable indicators
Training Data Test Data Dim Maj. Cl. Baseline Acc. (SE)
JUNE-2000 4fold Xval JUNE-2000 SPA 31.28   99.1   .09  
JUNE-2000 OCTOBER-2001 SPA 31.28   82.57   0.14  
JUNE-2000 & 2000 ex-
amples of October-2001
October-2001 w/out 2000 SPA 30.88   95.68   0.08  
Table 2: Results for Identifying Speech-Act DATE tags in the October-2001 Communicator Corpus, (Dim
= Dimension of Date used for output classification (SPA = Speech Act, Maj. Cl. = Majority Class, Acc =
Accuracy, SE = Standard Error)
of DATE tags. More interestingly, the words uti-
lized are intuitively plausible for the travel planning
domain. For example, the learned question words
such as which, where and would are significant for
utterances that have request info as their SPEECH-
ACT dimension. The words city, airport, from, des-
tination and departing are significant predictors of
utterances that have orig city and dest city as their
task dimension.
if utt-string contains city  utt-string contains from 
pattern-length  38
or if utt-string contains airport  pattern-length  38
or if utt-string contains city  pattern-length  17  pattern-
length  15
or if utt-string contains from  pattern-length  66  utt-
string contains Where
or if utt-string contains city  utt-string contains say
or if utt-string contains DEPARTING
or if utt-string contains which  utt-string contains From
or if utt-string contains city  system-name=IBM  utt-
string contains departure
or if utt-string contains fly  utt-string contains which 
left-sys-utt-string contains city
or if utt-string contains fly  utt-string contains O
then about task:request info:orig city
if utt-string contains where  utt-string contains must
or if utt-string contains city  pattern-length  35
or if utt-string contains Where
or if utt-string contains destination
or if utt-string contains DESTINATION
or if utt-string contains which  utt-string contains city
or if utt-string contains where
or if utt-string contains WOULD
then about task:request info:dest city
Figure 8: Rules for DATE tags
about task:request info:orig city and
about task:request info:dest city for Training
on the June-2000 Corpus and 2000 Examples of
October-2001 Corpus.
Human-Computer Speech-Act Results: Be-
cause the DATE scheme describes utterances
in terms of SPEECH-ACT, CONVERSATIONAL-
DOMAIN and TASK dimensions, it is also possible to
extract from the composite labels and examine the
DATE tagger performance for the individual dimen-
sions. Here we focus on the SPEECH-ACT dimension
since, as mentioned above, it is more likely to gen-
eralize to HH travel dialogues and to other task do-
mains. Table 2 shows the results for a DATE tagger
trained and tested on only the SPEECH-ACT dimen-
sion. The reported accuracies are signifcantly better
than the baseline in each case and the differences be-
tween the rows are also statistically significant. The
results support our original hypothesis, showing that
the June-2000 SPEECH-ACT DATE tagger general-
izes more readily to the October-2001 corpus, with
an accuracy of 82.57   (Row 2). Furthermore, as
before, even a small amount of training data from
the 2001 corpus makes a significant improvement in
accuracy to 95.68   (Row 3), which is close to the
99.1   accuracy (Row 1) reported for training and
testing on the June-2000 corpus as estimated by 4-
fold cross-validation.
Human-Human Results: In order to examine
whether there is any generalization from labelled
HC data to HH data for the same task, we apply a
DATE tagger trained on only the SPEECH-ACT di-
mension. The first row of Table 3 shows that when
a DATE tagger is trained on only the HC corpus and
tested on the HH corpus that the accuracy is 36.72  
(a significant improvement over the baseline). This
result demonstrates quantitatively that the HC data
can be used to improve performance of a tagger for
HH data.
Now, let us consider a situation where we only
have 305 HH labelled utterances from 10 of the HH
dialogues to train a DATE tagger. Row 2 shows that
we achieve 48.75   accuracy when testing on the re-
mainder of the HH corpus. However if we add the
HC data to the training set, the accuracy improves
significantly to 55.48   (Row 3). Again this result
demonstrates quantitatively that the HC data can im-
prove performance of a tagger for HH data.
Row 4 shows that the utility of the HC corpus de-
creases if larger amounts of HH labelled data are
available; using 95   of the data to train and test-
Training Data Test Data Maj. Cl. Baseline Acc. (SE)
JUNE-2000 & OCTOBER-
2001
CMU-CORPUS 28.07   36.72   2.76  
305 CMU-CORPUS CMU-CORPUS - 305 43.93   48.75   1.82  
JUNE-2000, OCTOBER-
2001 & 305 CMU-CORPUS
CMU-CORPUS - 305 28.04   55.48   1.81  
CMU-CORPUS 20fold Xval CMU-CORPUS 54.14   76.56   1.03  
Table 3: Results for Identifying DATE Speech-Act Tags in the CMU Human-Human Corpus (Maj. Cl. =
Majority Class, Acc. = Accuracy, SE = Standard Error)
ing on 5   with 20-fold cross-validation achieves an
accuracy of 76.56   .
Examination of the errors that the tagger makes
indicates both similarities and differences between
HH and HC dialogues. For example, information is
presented in small installments in the HH dialogues
whereas information presentation utterances in the
HC dialogues tend to be very long. The information
presentation utterances in HH dialogues then appear
to be syntactically similar to the implicit confirma-
tions in the HC data. Finally, some utterance types
that are very frequent in the HC data such as instruc-
tions rarely occur in the HH dialogues.
The rules that are learned for a DATE tagger
trained on the HC corpora and the HH CMU-corpus
for the offer SPEECH-ACT are in Figures 9 and 10.
There are two main conclusions that can be drawn
from these figures about the generalization from HC
to HH corpora in the SPEECH-ACT dimension. First,
in general, a larger number of rules are learned for
the HH data, suggesting that there is greater varia-
tion for the same speech act in HH dialogues. While
this is not surprising, there is also significant over-
lap in the features and values used in the rules. For
example, the utterance string feature utilizes words
such as select, ight, do, okay, ne, these in both rule
sets.
4 Discussion and Future Work
In summary our results show that: (1) It is possi-
ble to assign DATE dialogue act tags to system ut-
terances in HC dialogues from many different sys-
tems for the same domain with high accuracy; (2) A
DATE tagger trained on data from an earlier version
of the system only achieves moderate accuracy on a
later version of the system without a small amount
labelled training data from that later version; (3) La-
belled training data from HC dialogues can improve
the performance of a DATE tagger for HH dialogue
when only a small amount of HH training data is
available.
Previous work has also reported results for di-
alogue act taggers, using similar features to those
we use, with accuracies ranging from 62   to 75  
(Reithinger and Klesen, 1997; Shriberg et al, 2000;
Samuel et al, 1998). Our best accuracy for the HC
data is 98   . The best performance for the HH cor-
pus is 76   accuracy for the cross-validation study
using only HH data. However, accuracies reported
for previous work are not directly comparable to
ours for several reasons. First, some of our results
concern labelling the system side of utterances in
HC dialogues for the purpose of automatic evalua-
tion of system performance. It is much easier to de-
velop a high accuracy tagger for HC dialogue than it
is for HH dialogue.
We also applied the DATE tagger to HH dialogue,
and focused on the travel agent side of the dialogue.
Here the accuracies that we report are more compa-
rable with that of other researchers, but large differ-
ences should nevertheless be expected due to differ-
ences in the types of corpora, dialogue act tagging
schemes, and features used.
We considered the possibility of generating dia-
logue acts automatically in the logfiles. This idea
was attractive because it is possible to easily im-
plement the generation of dialogue acts tags in the
logfiles. Large amounts of human-computer data
would then be available for the human-human la-
belling task or for evaluation efforts. However, this
turned out to be impractical because we found it dif-
ficult to get dialogue designers across the different
participating sites to agree on a labelling standard.
We therefore believe that machine learning meth-
ods for classification such as the one discussed here
might still be necessary to automate the tagging task
for rapid evaluation and labelling efforts.
As part of the ISLE NSF/EU project, the labelled
corpus that we developed for this work will soon be
released by the LDC, and other researchers will then
be able to utilize it to improve upon our results. In
addition, we believe this corpus could be useful as a
training resource for spoken response generation in
dialogue systems. For example, the dialogue act rep-
resentation can be used to provide a broad range of
text-planning inputs for a stochastic sentence plan-
ner in the travel domain (Walker et al, 2001b), or
to represent the systems? dialogue strategies for re-
inforcement learning (Walker, 2000; Scheffler and
Young, 2002). In future work, we hope to demon-
strate that features derived from the labelling of the
system side of the dialogue can also improve per-
formance of a dialogue act tagger for the human ut-
terances in the dialogue, and to conduct additional
analyses demonstrating the utility of this representa-
tion for cross-site evaluation.
5 Acknowledgments
Thanks to John Aberdeen and Christy Doran for
their contribution of a labelled set of 10 HH dia-
logues from the CMU-corpus. The work reported in
this paper was partially funded by DARPA contract
MDA972-99-3-0003.
References
J. Allen and M. Core. 1997. Draft of DAMSL: Dialog
act markup in several layers. Coding scheme devel-
oped by the MultiParty group, 1st Discourse Tagging
Workshop, Univ. of Penn, March 1996.
R. Cattoni, M. Danieli, A. Panizza, V. Sandrini, and
C. Soria. 2001. Building a corpus of annotated dia-
logues: the ADAM experience. In Proc. of the Con-
ference Corpus-Linguistics-2001, Lancaster, U.K.
W. Cohen. 1996. Learning trees and rules with set-
valued features. In 14th Conference of AAAI.
B. Di Eugenio, P. W. Jordan, J. D. Moore, and
R. H. Thomason. 1998. An empirical investigation
of collaborative dialogues. In ACL-COLING98, Proc.
of the 36th ACL Conference.
C. Doran, J. Aberdeen, L. Damianos, and L. Hirschman.
2001. Comparing several aspects of human-computer
and human-human dialogues. In SIGDIAL Workshop
in conjuction with Eurospeech 2001.
M. Eskenazi, A. Rudnicky, K. Gregory, P. Constantinides,
R. Brennan, K. Bennett, and J. Allen. 1999. Data col-
lection and processing in the carnegie mellon commu-
nicator. In Proc. of Eurospeech-99, pages 2695?2698.
M. Finke, M. Lapata, A. Lavie, L. Levin, L. Mayfield
Tomokiyo, T. Polzin, K. Ries, A. Waibel, and K. Zech-
ner. 1998. Clarity: Inferring discourse structure from
speech. In AAAI Symposium on Applying Machine
Learning to Discourse Processing Proceedings, Stan-
ford, California.
H. Hastie, R. Prasad, and M. A. Walker. 2002. Auto-
matic evaluation: Using a date dialogue act tagger for
user satisfaction and task completion prediction. In
LREC 2002.
A. Isard and J. C. Carletta. 1995. Replicability of trans-
action and action coding in the map task corpus. In
M. A. Walker and J. Moore, eds., AAAI Spring Sympo-
sium: Empirical Methods in Discourse Interpretation
and Generation, pages 60?67.
P. W. Jordan. 2000. Intentional Influences on Object Re-
descriptions in Dialogue: Evidence from an Empirical
Study. Ph.D. thesis, Intelligent Systems Program, Uni-
versity of Pittsburgh.
N. Reithinger and M. Klesen. 1997. Dialogue act clas-
sification using language models. In Proc. of Eu-
rospeech ?97, pages 2235?2238, Rhodes, Greece.
K. Samuel, S. Carberry, and K. Vijay-Shanker. 1998. Di-
alogue act tagging with transformation-based learning.
In Proc. of COLING-ACL, pages 1150?1156.
K. Scheffler and S. Young. 2002. Automatic learning of
dialogue strategy using dialogue simulation and rein-
forcement learning. In HLT Conference.
E. Shriberg, P. Taylor, R. Bates, A. Stolcke, K. Ries,
D. Jurafsky, N. Coccaro, R. Martin, M. Meteer, and
C. Van Ess-Dykema. 2000. Can prosody aid the au-
tomatic classification of dialog acts in conversational
speech. Language and Speech: Special Issue on
Prosody and Conversation.
M. A. Walker, D. Litman, C. A. Kamm, and A. Abella.
1997. PARADISE: A general framework for evaluat-
ing spoken dialogue agents. In Proc. of ACL/EACL 97,
pages 271?280.
M. A. Walker, J. Aberdeen, J. Boland, E. Bratt, J. Garo-
folo, L. Hirschman, A. Le, S. Lee, S. Narayanan,
K. Papineni, B. Pellom, J. Polifroni, A. Potamianos,
P. Prabhu, A. Rudnicky, G. Sanders, S. Seneff, D. Stal-
lard, and S. Whittaker. 2001a. Darpa communica-
tor dialog travel planning systems: The june 2000 data
collection. In Proc. of EUROSPEECH 2001.
M. A. Walker, O. Rambow, and M. Rogati. 2001b. Spot:
A trainable sentence planner. In Proc. of NAACL.
M. A. Walker, R. Passonneau, and J. E. Boland. 2001c.
Quantitative and qualitative evaluation of DARPA
communicator spoken dialogue systems. In Proc. of
ACL 2001.
M. A. Walker. 2000. An application of reinforcement
learning to dialogue strategy selection in a spoken di-
alogue system for email. Journal of Artificial Intelli-
gence Research, 12:387?416.
Speaker Utterance C-Domain Speech Act Task
SYS Welcome. about-task opening/closing meta
You are logged in as a guest user. about-frame instruction meta
You may say repeat, help me out, start over, or, that?s wrong, about-frame instruction meta
you can also correct and interrupt the system at any time. about-frame instruction meta
What airport woodja like to fly out of? about-task request-info orig-city
USER MIAMI FLORIDA
SYS Leaving from Miami, about-comm implicit-confirm orig-city
And, what city are you flying to? about-task request-info dest-city
USER MINNEAPOLIS MINNESOTA
SYS Flying from Miami to Minneapolis, about-comm implicit-confirm orig-city, dest-
city
What date would you like to fly? about-task request-info depart-date
USER UH MONDAY OCTOBER TWENTY THIRD
SYS Leaving on the 23rd of October, about-comm implicit-conf date
And what time didja wanna leave? about-task request-info time
USER UH LATE MORNING
SYS From Minneapolis. about-comm implicit-conf origin
Are you returning to Miami? about-task request-info trip-type
USER UM
SYS Are you going back to Miami? about-task request-info trip-type
Please say yes or no. about-frame instruct meta
USER NO
SYS Okay, it will just take a moment while I look. about-task status-report retrieval
SYS I found 3 flights from Miami to Minneapolis on October 23rd
. For option one I have a flight with United departing at 10
A M and arriving at 3 P M with one stopover and
about-task present-info itinerary
Ticket price is 390 dollars. about-task present-info price
Please say next option, or, flight details, or, I?ll take it. about-task offer itinerary
USER I?LL TAKE IT
SYS Great! I am adding this flight to your itinerary. about-task acknowledge booking
Okay, Is Minneapolis your final destination? about-task request-info trip-type
USER YES IT IS
SYS Would you like to make any ground arrangements? about-task offer ground
USER NO
SYS Now, would you like to hear a summary of your itinerary? about-task offer itinerary
USER YES PLEASE
SYS Okay, Here?s your itinerary. On October 23rd Northwest
Flight 123 departs Miami at 2 P M, arrives Pittsburgh at 4 30
P M connecting to Northwest Flight 146 that departs Pitts-
burgh at 5 P M and arrives Minneapolis at 7 P M.
about-task present-info itinerary
Figure 5: Dialogue Illustrating the Conversational-Domain, Speech-Act, and Task-Subtask Dimensions of
DATE
if left-context1=SPA-present-info  pattern-length  25  position-in-turn  2  right-sys-utt-string contains none
or if left-context2=SPA-present-info-SPA-present-info  system-name=ATT
or if right-sys-utt-string contains also  left-sys-utt-string contains at
or if utt-string contains select
or if utt-string contains confirm
or if utt-string contains fine
or if right-sys-utt-string contains locations  utt-string contains If
or if left-context2=SPA-implicit-confirm-SPA-instruction  utt-string contains Which
or if utt-string contains Okay  utt-string contains flight
or if left-context2=SPA-explicit-confirm-SPA-acknowledgement  utt-string contains flight
or if utt-string contains these  utt-string contains Are
or if rec-orig-string contains sixteenth  utt-string contains Do
then offer
Figure 9: Rules learned for DATE SPEECH-ACT offer using June-2000 plus 2000 Examples of October-2001
as Training
if left-sys-utt-string contains ?NUMBER?  pattern-length  25  right-sys-utt-string contains none  utt-string contains OK
or if position-in-turn  2  left-sys-utt-string contains dollars  pattern-length  55  contains-word-CITY-or-AIRPORT=false
or if utt-string contains this  pattern-length  37  contains-word-FLIGHT-or-AIRLINE=true
or if left-sys-utt-string contains per  da-num  2
or if right-sys-utt-string contains rate
or if utt-string contains these
or if utt-string contains itinerary  pattern-length  41
or if utt-string contains reservation
or if utt-string contains select
or if utt-string contains book  utt-string contains it
or if utt-string contains whether
or if utt-string contains OK  utt-string contains Is
or if utt-string contains MAKE
or if utt-string contains one  right-sys-utt-string contains none
or if utt-string contains fine
or if utt-string contains Kay
or if right-sys-utt-string contains locations
or if utt-string contains THE  utt-string contains LIKE
or if left-sys-utt-string contains over  utt-string contains flight  utt-string contains would
or if utt-string contains take  utt-string contains Do
or if left-sys-utt-string contains yes  utt-string contains what  utt-string contains flight
then offer
Figure 10: Rules learned for DATE SPEECH-ACT offer using 305 CMU-Corpus Utterances as Training
POLLy: A Conversational System that uses a Shared Representation to 
Generate Action and Social Language
Swati Gupta
Department of Computer 
Science, Regent Court
211 Portobello Street
University of Sheffield
Sheffield, UK
s.gupta@dcs.shef.ac
.uk
Marilyn A. Walker
Department of Computer 
Science, Regent Court
211 Portobello Street
University of Sheffield
Sheffield, UK 
m.walker@dcs.shef.ac
.uk
Daniela M. Romano
Department of Computer 
Science, Regent Court
211 Portobello Street
University of Sheffield
Sheffield, UK 
d.romano@dcs.shef.ac
.uk
Abstract
We present a demo of our conversational 
system POLLy (POliteness in Language 
Learning) which uses a common planning 
representation to generate actions to be per-
formed by embodied agents in a virtual en-
vironment and to generate spoken utter-
ances for dialogues about the steps in-
volved in completing the task. In order to 
generate socially appropriate dialogue, 
Brown and Levinson?s theory of politeness 
is used to constrain the dialogue generation 
process.
1 Introduction
Research in Embodied Conversational Agents 
(ECAs) has explored embedding ECAs in domain-
specific Virtual Environments (VE) where users 
interact with them using different modalities, in-
cluding Spoken Language. However, in order to 
support dialogic interaction in such environments, 
an important technical challenge is the synchroni-
zation of the ECA Spoken Interaction module with 
the ECA non-verbal actions in the VE. We propose 
an approach that uses a common high level repre-
sentation which is broken down to simpler levels to 
generate the agents? verbal interaction and the 
agents? non-verbal actions synchronously for task-
oriented applications that involve performing some 
actions to achieve a goal, while talking about the 
actions using natural language. 
In previous work, Bersot et al(1998) present a 
conversational agent called Ulysses embedded in a 
collaborative VE which accepts spoken input from 
the user and enables him or her to navigate within 
the VE. They use a ?reference resolver? which 
maps the entities mentioned in utterances to geo-
metric objects in the VE and to actions.
Figure 1. A user interacting with the Agents
Max, a VR based conversational agent by Kopp 
et al(2003) allows multimodal conversational 
abilities for task-oriented dialogues in virtual con-
struction tasks. It builds on a database of utterance 
templates which contain the verbal part, aug-
mented with accompanying gestures and their 
cross-modal affiliation. In order to deal with the 
vagueness of language in specifying spatial rela-
tions in virtual space, the K2 system (Takenobu et 
al 2003) proposed a bilateral symbolic and numeric
representation of locations, to bridge the gap be-
tween language processing (a symbolic system), 
and animation generation (a continuous system). 
K2 extracts a user?s goal from the utterance and 
967
translates it into animation data. The FearNot! 
demonstrator by Paiva et al(2005) provides train-
ing to kids against bullying via virtual drama in 
which one virtual character plays the role of a 
bully and the other plays the role of victim, who 
asks the child for advice. FearNot!?s spoken inter-
action is template-based where the incoming text 
from the child is matched against a set of language 
templates. The information about the character?s 
action is defined in a collection which contains the 
utterance to be spoken as well as the animation. 
Eichner et al(2007) describe an application in 
which life-like characters present MP3 players in a 
virtual showroom. An XML scripting language is 
used to define the content of the presentation as 
well as the animations of the agents. A more ex-
pressive agent, Greta, developed by Pelachaud et al
(Poggi et al 2005) is capable of producing socially 
appropriate gestures and facial expressions, and 
used is in an evaluation of gesture and politeness 
as reported in Rehm and Andr? (2007).
Since these ECAs function in scenarios where 
they interact with the world, other agents, and the 
user, they must be ?socially intelligent? (Dauten-
hahn, 2000) and exhibit social skills. Our work is 
based on the hypothesis that the relevant social 
skills include the ability to communicate appropri-
ately, according to the social situation, by building 
on theories about the norms of human social be-
haviour. We believe that an integral part of such 
skills is the correct use of politeness (Brown & 
Levinson, 1987; Walker et al1997). For instance, 
note the difference in the effect of requesting the 
hearer to clean the floor by saying ?You must clean 
the spill on the floor now!? and ?I know I?m asking 
you for a big favour but could you kindly clean the 
spill on the floor??
According to Brown and Levinson (1987) 
(henceforth B&L), choices of these different forms 
are driven by sociological norms among human 
speakers. Walker et al(1997) were the first to pro-
pose and implement B&L?s theory in ECAs to 
provide interesting variations of character and per-
sonality in an interactive narrative application. 
Since then B&L?s theory has been used in many
conversational applications e.g. animated presenta-
tion teams (Andr? et al2000; Rehm & Andr?, 
2007), real estate sales (Cassell & Bickmore, 2003), 
and tutorials (Johnson et al 2004; Johnson et al 
2005; Porayska-Pomsta 2003; Wang et al2003). 
Rehm & Andr? (2007) show that gestures are used 
consistently with verbal politeness strategies and 
specific gestures can be used to mitigate face 
threats. Work in literary analysis has also argued 
for the utility of B&L?s theory, e.g. Culpeper 
(1996) argues that a notion of ?impoliteness? in 
dramatic narratives creates conflict by portraying 
verbal events that are inappropriate in real life. 
Thus impoliteness often serves as a key to move 
the plot forward in terms of its consequences.
This demo presents our Conversational System 
POLLy which produces utterances with a socially 
appropriate level of politeness as per the theory of 
Brown and Levinson. We have implemented 
POLLy in a VE for the domain of teaching English 
as a second language (ESL). It is rendered in our 
VE RAVE at Sheffield University as well as on a 
normal computer screen, as explained in section 3.
Figure 1 shows a user interacting with POLLy in 
RAVE. Since RAVE is not portable, we will dem-
onstrate POLLy on the computer screen where the 
user will be able to verbally communicate with the 
agents and the agents will respond with computa-
tionally generated utterances with an appropriate 
level of politeness as per a given situation.
2 POLLy?s Architecture
POLLy uses a shared representation for generating 
actions to be performed by the ECAs in the virtual 
domain on one hand, and on the other, for generat-
ing dialogues to communicate about the actions to 
be performed. It consists of three components: A 
Virtual Environment (VE), a Spoken Language 
Generation (SLG) system and a Shared AI Plan-
ning Representation for VE and SLG as illustrated 
in Figure 2. 
A classic STRIPS-style planner called Graph-
Plan (Blum & Furst, 1997) produces, given a goal 
e.g. cook pasta, a plan of the steps involved in do-
ing so (Gupta et al, 2007). POLLy then allocates 
this plan to the Embodied Conversational Agents 
(ECA) in the VE as a shared collaborative plan to 
achieve the cooking task with goals to communi-
cate about the plan via speech acts (SAs), needed 
to accomplish the plan collaboratively, such as Re
quests, Offers, Informs, Acceptances and rejections
(Grosz, 1990; Sidner, 1994; Walker, 1996). It also 
allocates this plan to the SLG which generates 
variations of the dialogue based on B&L?s theory 
of politeness that realizes this collaborative plan as 
in (Andr? et al2000;Walker et al 1997).
968
Figure 2: POLLY?s Architecture
The SLG (Gupta et al, 2007) is based on a 
standard architecture (Dale & Reiter, 1995) with 
three components: Content planning, utterance
planning and surface realization. See Figure 2. The 
politeness strategies are implemented through a 
combination of content selection and utterance
planning. The linguistic realizer RealPro is used 
for realization of the resulting utterance plan (La-
voie & Rambow, 1997), which takes a dependency 
structure called the Deep-Syntactic Structure 
(DSyntS) as input and realizes it as a sentence 
string.  The Content Planner interfaces to the AI 
Planner, selecting content from the preconditions, 
steps and effects of the plan. According to B&L, 
direct strategies are selected from the steps of the 
plan, while realizations of preconditions and negat-
ing the effects of actions are techniques for imple-
menting indirect strategies. The content planner 
extracts the components of the utterances to be 
created, from the plan and assigns them their re-
spective categories, for example, lexeme get/add 
under category verb, knife/oil under direct object 
etc and sends them as input to the Utterance Plan-
ner. The Utterance Planner then converts the utter-
ance components to the lexemes of DSyntS nodes 
to create basic DsyntS for simple sentences, which 
are then transformed to create variations as per 
B&L?s politeness strategies, with the ?politeness 
manipulator script?. For realizing these B&L 
strategies, transformations to add lexical items 
such as ?please?, ?if you don?t mind?, and ?mate?  
were added to the DSyntS  to make a sentence less 
or more polite.
Some example dialogues are shown in section
3. In the VE, the human English language learner 
is able to interact with the Embodied Conversa-
tional Agent and plays the part of one of the agents 
in order to practice politeness real-time.
2.1 Brown and Levinson?s theory
B&L?s theory states that speakers in conversation 
attempt to realize their speech acts (SAs) to avoid 
threats to one another?s face, which consists of two 
components. Positive face is the desire that at least 
some of the speaker?s and hearer?s goals and 
desires are shared by other speakers. Negative face 
is the want of a person that his action be 
unimpeded by others. Utterances that threaten the 
conversants? face are called Face Threatening Acts 
(FTAs). B&L predict a universal of language usage 
that the choice of linguistic form can be 
determined by the predicted Threat ? as a sum of 3 
variables: P: power that the hearer has over the 
speaker; D: social distance between speaker & 
hearer; and R: a ranking of imposition of the 
speech act. Linguistic strategy choice is made 
according to the value of the Threat ?. We follow 
Walker et al?s (1997) four part classification of 
strategy choice. 
The Direct strategy is used when ? is low and 
executes the SA in the most direct, clear and 
969
unambiguous way. It is usually carried out either in 
urgent situations (Please Help!), or where the face 
threat is small as in ?I have chopped the 
vegetables? or if the speaker has power over the 
hearer, ?Did you finish your homework today??
The Approval strategy (Positive Politeness) is 
used for the next level of threat ? - this strategy is 
oriented towards the need for the hearer to 
maintain a positive self-image. Positive politeness 
is primarily based on how the speaker approaches 
the hearer, by treating him as a friend, a person 
whose wants and personality traits are liked, for ex. 
by using friendly markers ?Friend, would you 
close the door?? 
The Autonomy Strategy (Negative Politeness) 
is used for high face threats, when the speaker may 
be imposing on the hearer, intruding on their space 
or violating their freedom of action. These face 
threats can be mitigated by using hedges, ?I 
wonder if you would mind closing the door for 
me,? or by minimizing imposition, ?I just want to 
ask you if you could close the door.? 
The Indirect Strategy (Off Record) is the 
politest strategy and is used when ? is greatest. It 
usually has more than one attributable intention so 
that the speaker removes himself from any 
imposition. For ex. using metaphor and irony, 
rhetorical questions, understatement, hints etc. For 
instance, ?Its cold in here,? which implies a request 
to close the door.
2.2 Example Dialogues
Here are some example dialogues that illustrate the 
difference in the politeness strategies used in dis-
course contexts of varying power. Two Microsoft 
Agents, Merlin and Genie are involved in a con-
versation while cooking and cleaning in a kitchen 
together. Consider the difference in the degree of 
politeness in each situation.
Conversation 1: This is an example conversa-
tion in which Merlin is Genie?s boss.
Agent (Speech act: Politeness strategy): Utterance
Merlin: Would you mind washing the dishes? 
(Approval: RequestAct)
Genie: Sure, I?ll wash the dishes. (Direct:AcceptRequest)
Genie: I?m wondering whether I should boil the 
pasta.(Autonomy:Offer)
Merlin: Yeah you can. (Direct: AcceptOffer)
Merlin: You?ve burnt the vegetables. (Direct: Inform)
Genie: Yeah. (Direct: AcceptInform)
Genie: The oven is dirty. (Indirect: RequestAct)
Merlin: I?m sorry I can?t clean the oven. 
(Approval: RejectRequest)
Genie: Ok. (Direct: AcceptReject)
Genie: If you don?t mind, can I clean the burner? 
(Autonomy: Offer)
Merlin: No, forget it. (Direct: RejectOffer)
Merlin: I must wipe the counter now. (Direct: Offer)
Genie: Alright, if you insist. (Autonomy: AcceptOffer)
Genie: Do you know that you?ve broken the baking dish 
mate? (Approval: Inform)
Merlin: Yes. (Direct: AcceptInform)
Merlin: I?ll cook the vegetables. (Direct: Offer)
Genie: No please don?t bother yourself. 
(Autonomy: RejectOffer)
Genie: The vegetables are healthy. (Indirect: RequestAct)
Merlin: Alright, I?ll add the vegetables to the pasta. 
(Direct: AcceptRequest)
Merlin: Could you please add the cheese to the pasta for 
me? (Approval: RequestAct)
Genie: Please don?t mind but I can?t do that. 
(Autonomy: RejectRequest)
Merlin: Alright. (Direct: AcceptReject)
Conversation 2: In this case Genie and Merlin 
are colleagues at an equal level in hierarchy.
Agent (Speech act: Politeness strategy): Utterance
Merlin: Could you possibly clean the oven for 
me?(Approval:RequestAct)
Genie: Sure. (Direct:AcceptRequest)
Genie: I?ll clean the burner. (Direct:Offer)
Merlin: Ok. (Direct:AcceptOffer)
Merlin: You?ve burnt the vegetables. (Direct:Inform)
Genie: Yeah. (Direct:AcceptInform)
Genie: Would you mind washing the dishes?
(Approval:RequestAct)
Merlin: I?m sorry but I can?t wash the dishes. 
(Approval:RejectRequest)
Genie: Alright. (Direct:AcceptReject)
Genie: I must boil the pasta. (Direct:Offer)
Merlin: No, thanks. (Direct:RejectOffer)
Merlin: I can wipe the counter. (Direct:Offer)
Genie: Yeah you can. (Direct:AcceptOffer)
Genie: You?ve broken the baking dish. (Direct:Inform)
Merlin: Yes. (Direct:AcceptInform)
Merlin: I?ll cook the vegetables. (Direct:Offer)
Genie: No, forget it. (Direct:RejectOffer)
Merlin: Could you please add the vegetables to the pasta? 
(Approval:RequestAct)
Genie: Please don?t mind but I can?t do that. 
(Approval:RejectRequest)
Merlin: Ok. (Direct:AcceptReject)
Genie: Will you please wipe the table mate? 
(Approval:RequestAct)
Merlin: Sure. (Direct:AcceptRequest)
970
3 Virtual Environment
We rendered POLLy with Microsoft Agent Char-
acters (Microsoft, 1998) in our Virtual Environ-
ment RAVE at Sheffield University as well as on a 
desktop computer screen. RAVE consists of a 3-
dimensional visualisation of computer-generated 
scenes onto a 10ft x 8ft screen and a complete 3D 
surround sound system driven by a dedicated com-
puter. Since Microsoft Agents are 2D, they are not 
rendered 3D, but a life size image of the characters 
is visible to the users on the screen to make them 
appear believable. Figure 1 showed a user interact-
ing with POLLy in RAVE. The MS Agent package 
provides libraries to program control using various 
developing environments like the .NET framework 
and visual studio and includes a voice recognizer 
and a text-to-speech engine. It also provides con-
trols to embed predefined animations which make 
the characters? behaviour look more interesting and 
believable (Cassell & Th?risson, 1999). We have
programmed MS agent in Visual C++ and have
embedded these animations like gesturing in a di-
rection, looking towards the other agents, blinking, 
tilting the head, extending arms to the side, raising 
eyebrows, looking up and down etc while the 
agents speak and listen to the utterances and hold-
ing the hand to the ear, extending the ear, turning 
the head left or right etc when the agents don?t un-
derstand what the user says or the user doesn?t 
speak anything. 
The Agents share the AI plan to collaborate on 
it together to achieve the cooking task. Goals to 
communicate about the plan are also allocated to 
the agents as speech acts (SAs) such as Requests, 
Offers, Informs, Acceptances and Rejections, 
needed to accomplish the plan collaboratively. 
While interacting with the system using a high 
quality microphone, the user sees one or two 
agents on the screen and plays the part of the sec-
ond or the third agent, as per the role given to 
him/her. 
When we extend this to a real-time immersive 
Virtual Reality environment, a Virtual Kitchen in 
this case, the ECAs will actually perform the task 
of cooking a recipe together in the virtual kitchen 
while conversing about the steps involved in doing 
so, as laid out by the AI plan. 
This setup makes it possible to design a 2x2x2 
experiment to test three conditions: Interactivity, 
i.e. whether the user only sees the agents interact-
ing on the screen vs. the user interacts with the 
agents by playing a role; immersiveness of the en-
vironment, i.e. rendering in RAVE vs. rendering on 
a desktop computer; and culture, i.e. the difference 
between the perception of politeness by people 
from different cultures as in (Gupta et al, 2007). 
We are now in the process of completing the de-
sign of this experiment and running it.
4 Conclusion
We presents a demo of our conversational system 
POLLy which implements MS Agent characters in 
a VE and uses an AI Planning based shared repre-
sentation for generating actions to be performed by 
the agents and utterances to communicate about 
the steps involved in performing the action. The 
utterances generated by POLLy are socially appro-
priate in terms of their politeness level. The user 
will be given a role play situation and he/she will 
be able to have a conversation with the agents on a 
desktop computer, where some dialogic utterances
would be allocated to the user. An evaluation of 
POLLy (Gupta et al 2007; Gupta et al 2008) 
showed that (1) politeness perceptions of POLLy?s 
output are generally consistent with B&L?s predic-
tions for choice of form for discourse situation, i.e. 
utterances to strangers or a superior person need to 
be very polite, preferably autonomy oriented (2) 
our indirect strategies which should be the politest 
forms, are the rudest (3) English and Indian speak-
ers of English have different perceptions of polite-
ness (4) B&L implicitly state the equality of the P 
& D variables in their equation (? = P + D + R), 
whereas we observe that not only their weights are
different as they appear to be subjectively deter-
mined, but they are also not independent.
References
Andr?, E., Rist, T., Mulken, S.v., Klesen, M., & Baldes, 
S. 2000. The automated design of believable dia-
logues for animated presentation teams. In Embodied 
Conversational Agents (pp. 220?255). MIT Press.
Bersot, O., El-Guedj, P.O., God?ereaux, C. and Nugues. 
P. 1998. A conversational agent to help navigation & 
collaboration in virtual worlds. Virtual Real-
ity,3(1):71?82.
Blum, A., Furst, M. 1997. Fast Planning Through Plan-
ning Graph Analysis. Artificial Intelligence 90.
Cassell, J. and Th?risson, K.R. 1999. The Power of a
Nod and a Glance: Envelope vs. Emotional Feedback
971
in Animated Conversational Agents. Applied Artificial 
Intelligence 13: 519-538.
Cassell, J. and Bickmore, Timothy W. Negotiated Col-
lusion. 2003. Modeling Social Language and its Re-
lationship Effects in Intelligent Agents. User Model. 
User-Adapt.Interact. 13(1-2):89-132.
Culpeper, J. 1996. (Im)politeness in dramatic dialogue. 
Exploring the Language of Drama: From text to con-
text. Routledge, London.
Dale, R. and Reiter, E. 1995. Building Natural Lan-
guage Generation Systems. Studies in Natural Lan-
guage Processing. Cambridge University Press.
Dautenhahn, K. 2000. Socially Intelligent Agents: The 
Human in the Loop (Papers from the 2000 AAAI Fall 
Symposium). The AAAI Press, Technical Report.
Eichner, T., Prendinger, H., Andr?, E. and Ishizuka, M. 
2007. Attentive presentation agents. Proc. 7th Inter-
national Conference on Intelligent Virtual Agents 
(IVA-07), Springer LNCS 4722. pp 283-295.
Grosz, B.J., Sidner, C.L. 1990. Plans for discourse. In: 
Cohen, P.R., Morgan, J.L., Pollack, M.E. (eds.) In-
tentions in Communication, MIT Press, Cambridge.
Gupta, S., Walker, M.A., Romano, D.M. 2007. How 
Rude are You?: Evaluating Politeness and Affect in 
Interaction. Affective Computing & Intelligent Inter-
action (ACII-2007).
Gupta , S., Walker, M.A., Romano, D.M. 2008 (to be 
published). Using a Shared Representation to Gener-
ate Action and Social Language for a Virtual Dia-
logue Environment. AAAI Spring Symposium on 
Emotion, Personality and Social Behavior.
Johnson, L.W. and Rizzo, P. and Bosma, W.E. and Ghi-
jsen, M. and van Welbergen, H. 2004. Generating 
socially appropriate tutorial dialog. In: ISCA Work-
shop on Affective Dialogue Systems.  pp. 254-264.
Johnson, L., Mayer, R., Andr?, E., & Rehm, M. 2005. 
Cross-cultural evaluation of politeness in tactics for 
pedagogical agents. Proc. of the 12th Int. Conf. on 
Artificial Intelligence in Education.
Kopp, S., Jung, B., Lessmann, N. and Wachsmuth, I. 
2003.  Max ? A multimodal assistant in virtual reality 
construction. KI Zeitschift (German Magazine of Ar-
tificial Intelligence), Special Issue on Embodied 
Conversational Agents, vol.4, pp.11?17.
Lavoie, B., and Rambow, O. 1997. RealPro ? a fast, 
portable sentence realizer. In Proc. Conference on 
Applied Natural Language Processing (ANLP?97).
Microsoft. 1998. Developing for Microsoft Agent. Mi-
crosoft Press.
op den Akker, H.J.A. and Nijholt, A. 2000. Dialogues
for Embodied Agents in Virtual Environments. In: 
Natural Language Processing - NLP 2000, 2nd Int. 
Conf. pp. 358-369. LNAI 1835.
Paiva, A., Dias, J., & Aylett, R.S. 2005. Learning by 
feeling: evoking empathy with synthetic characters.
Applied Artificial Intelligence: 19 (3-4), 235-266.
Poggi, I., Pelachaud, C., de Rosis, F., Carofiglio, V., De 
Carolis, B. 2005. GRETA. A Believable Embodied 
Conversational Agent. in O. Stock and M. Zanca-
rano, eds, Multimodal Intelligent Information 
Presentation, Kluwer.
Prendinger, Helmut and Ishizuka, Mitsuru. 2001. Let's 
talk! Socially intelligent agents for language conver-
sation training. IEEE Transactions on xSystems, 
Man, and Cybernetics - Part A: Systems and Hu-
mans, Vol. 31, No. 5, pp 465-471.
Porayska-Pomsta, K. 2003. Influence of Situational 
Context on Language Production: Modelling Teach-
ers' Corrective Responses. PhD Thesis. School of In-
formatics, University of Edinburgh.
Rehm, M. and Andr?, E. 2007. Informing the Design of 
Agents by Corpus Analysis. Conversational Informat-
ics, Edited by T. Nishida.
Sidner, C.L. 1994. An artificial discourse language for 
collaborative negotiation. In: Proc. 12th National 
Conf. on AI, pp. 814?819.
Takenobu, T., Tomofumi, K., Suguru, S., Manabu, O. 
2003. Bridging the Gap between Language and Ac-
tion. IVA 2003, LNAI 2792, pp. 127-135.
Traum, D., Rickel, J., Gratch, J., Marsella, S. 2003. Ne-
gotiation over Tasks in Hybrid Human-Agent Teams 
for Simulation-Based Training. Proceedings of the 
2nd Int. Joint Conf. on Autonomous Agents and Mul-
tiagent Systems.
Walker, M.A. 1996. The effect of resource limits and 
task complexity on collaborative planning in dia-
logue. Artificial Intelligence Journal 85, 1?2.
Walker, M., Cahn, J. and Whittaker, S. J. 1997. Improv-
ing linguistic style: Social and affective bases for 
agent personality. In Proc. Autonomous Agents'97. 
96?105. ACM Press.
Wang, N., Johnson, W.L., Rizzo, P., Shaw,E., & Mayer, 
R. 2005. Experimental evaluation of polite interac-
tion tactics for pedagogical agents. Proceedings of 
IUI ?05. ACM Press.
Watts, Richard J. Ide, S. and Ehlich, K. 1992. Introduc-
tion, in Watts, R, Ide, S. and Ehlich, K. (eds.), Po-
liteness in Language: Studies in History, Theory and 
Practice. Berlin: Mouton de Gruyter, pp.1-17.
972
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 85?88,
New York, June 2006. c?2006 Association for Computational Linguistics
Automatic Recognition of Personality in Conversation
Franc?ois Mairesse
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, United Kingdom
F.Mairesse@sheffield.ac.uk
Marilyn Walker
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, United Kingdom
M.A.Walker@sheffield.ac.uk
Abstract
The identification of personality by auto-
matic analysis of conversation has many
applications in natural language process-
ing, from leader identification in meetings
to partner matching on dating websites.
We automatically train models of the main
five personality dimensions, on a corpus
of conversation extracts and personality
ratings. Results show that the models per-
form better than the baseline, and their
analysis confirms previous findings link-
ing language and personality, while re-
vealing many new linguistic and prosodic
markers.
1 Introduction
It is well known that utterances convey information
about the speaker in addition to their semantic con-
tent. One such type of information consists of cues
to the speaker?s personality traits, typically assessed
along five dimensions known as the Big Five (Nor-
man, 1963):
? Extraversion (sociability, assertiveness)
? Emotional stability (vs. neuroticism)
? Agreeableness to other people (friendliness)
? Conscientiousness (discipline)
? Intellect (openness to experience)
Findings include that extraverts talk more, louder,
and faster, with fewer pauses and hesitations, and
more informal language (Scherer, 1979; Furnham,
1990; Heylighen and Dewaele, 2002; Gill and Ober-
lander, 2002). Neurotics use more 1st person sin-
gular pronouns and negative emotion words, while
conscientious people avoid negations and negative
emotion words (Pennebaker and King, 1999). The
use of words related to insight and the avoidance of
past tense indicate intellect, and swearing and neg-
ative emotion words mark disagreeableness. Cor-
relations are higher in spoken language, possibly
especially in informal conversation (Mehl et al, in
press).
Previous work has modeled emotion and person-
ality in virtual agents, and classified emotions from
actor?s speech (Andre? et al, 1999; Liscombe et al,
2003). However, to our knowledge no one has tested
whether it is possible to automatically recognize per-
sonality from conversation extracts of unseen sub-
jects. Our hypothesis is that automatic analysis of
conversation to detect personality has application
in a wide range of language processing domains.
Identification of leaders using personality dimen-
sions could be useful in analyzing meetings and the
conversations of suspected terrorists (Hogan et al,
1994; Tucker and Whittaker, 2004; Nunn, 2005).
Dating websites could analyze text messages to try
to match personalities and increase the chances of a
successful relationship (Donnellan et al, 2004). Di-
alogue systems could adapt to the user?s personality,
like humans do (Reeves and Nass, 1996; Funder and
Sneed, 1993). This work is a first step toward indi-
vidual adaptation in dialogue systems.
We present non-linear statistical models for rank-
ing utterances based on the Big Five personality
traits. Results show that the models perform sig-
nificantly better than a random baseline, and that
prosodic features are good indicators of extraver-
sion. A qualitative analysis confirms previous find-
ings linking language and personality, while reveal-
ing many new linguistic markers.
2 Experimental method
Our approach can be summarized in five steps: (1)
collect individual corpora; (2) collect personality
85
ratings for each participant; (3) extract relevant fea-
tures from the texts; (4) build statistical models of
the personality ratings based on the features; and (5)
test the learned models on the linguistic outputs of
unseen individuals.
2.1 Spoken language and personality ratings
The data consists of daily-life conversation extracts
of 96 participants wearing an Electronically Acti-
vated Recorder (EAR) for two days, collected by
Mehl et al (in press). To preserve the participants?
privacy, random bits of conversation were recorded,
and only the participants? utterances were tran-
scribed, making it impossible to reconstruct whole
conversations. The corpus contains 97,468 words
and 15,269 utterances. Table 1 shows utterances for
two participants judged as introvert and extravert.
Introvert:
- Yeah you would do kilograms. Yeah I see what you?re saying.
- On Tuesday I have class. I don?t know.
- I don?t know. A16. Yeah, that is kind of cool.
- I don?t know. I just can?t wait to be with you and not have
to do this every night, you know?
- Yeah. You don?t know. Is there a bed in there? Well ok just...
Extravert:
- That?s my first yogurt experience here. Really watery. Why?
- Damn. New game.
- Oh.
- Yeah, but he, they like each other. He likes her.
- They are going to end up breaking up and he?s going to be like.
Table 1: Extracts from the corpus, for participants
rated as extremely introvert and extravert.
Between 5 and 7 independent observers scored
each extract using the Big Five Inventory (John and
Srivastava, 1999). Mehl et al (in press) report
strong inter-observer reliabilities for all dimensions
(r = 0.84, p < 0.01). Average observers? ratings
were used as the scores for our experiments.
2.2 Feature selection
Features are automatically extracted from each ex-
tract (see Table 2). We compute the ratio of words
in each category from the LIWC utility (Pennebaker
et al, 2001), as those features are correlated with the
Big Five dimensions (Pennebaker and King, 1999).
Additional psychological characteristics were com-
puted by averaging word feature counts from the
MRC psycholinguistic database (Coltheart, 1981).
In an attempt to capture initiative-taking in conversa-
tion (Walker and Whittaker, 1990; Furnham, 1990),
we introduce utterance type features using heuristics
on the parse tree to tag each utterance as a command,
prompt, question or assertion. Overall tagging accu-
racy over 100 randomly selected utterances is 88%.
As personality influences speech, we also use Praat
LIWC FEATURES (Pennebaker et al, 2001):
? STANDARD COUNTS:
- Word count (WC), words per sentence (WPS), type/token ratio (Unique),
words captured (Dic), words longer than 6 letters (Sixltr), negations
(Negate), assents (Assent), articles (Article), prepositions (Preps), num-
bers (Number)
- Pronouns (Pronoun): 1st person singular (I), 1st person plural (We), total
1st person (Self), total 2nd person (You), total 3rd person (Other)
? PSYCHOLOGICAL PROCESSES:
- Affective or emotional processes (Affect): positive emotions (Posemo),
positive feelings (Posfeel), optimism and energy (Optim), negative
emotions (Negemo), anxiety or fear (Anx), anger (Anger), sadness (Sad)
- Cognitive Processes (Cogmech): causation (Cause), insight (Insight),
discrepancy (Discrep), inhibition (Inhib), tentative (Tentat), certainty
(Certain)
- Sensory and perceptual processes (Senses): seeing (See), hearing (Hear),
feeling (Feel)
- Social processes (Social): communication (Comm), other references to
people (Othref), friends (Friends), family (Family), humans (Humans)
? RELATIVITY:
- Time (Time), past tense verb (Past), present tense verb (Present),
future tense verb (Future)
- Space (Space): up (Up), down (Down), inclusive (Incl), exclusive (Excl)
- Motion (Motion)
? PERSONAL CONCERNS:
- Occupation (Occup): school (School), work and job (Job),
achievement (Achieve)
- Leisure activity (Leisure): home (Home), sports (Sports), television and
movies (TV), music (Music)
- Money and financial issues (Money)
- Metaphysical issues (Metaph): religion (Relig), death (Death), physical
states and functions (Physcal), body states and symptoms (Body),
sexuality (Sexual), eating and drinking (Eating), sleeping (Sleep),
grooming (Groom)
? OTHER DIMENSIONS:
- Punctuation (Allpct): period (Period), comma (Comma), colon
(Colon), semi-colon (Semic), question (Qmark), exclamation (Exclam),
dash (Dash), quote (Quote), apostrophe (Apostro), parenthesis
(Parenth), other (Otherp)
- Swear words (Swear), nonfluencies (Nonfl), fillers (Fillers)
MRC FEATURES (Coltheart, 1981):
Number of letters (Nlet), phonemes (Nphon), syllables (Nsyl), Kucera-
Francis written frequency (K-F-freq), Kucera-Francis number of categories
(K-F-ncats), Kucera-Francis number of samples (K-F-nsamp), Thorndike-
Lorge written frequency (T-L-freql), Brown verbal frequency (Brown-
freq), familiarity rating (Fam), concreteness rating (Conc), imageability
rating (Imag), meaningfulness Colorado Norms (Meanc), meaningfulness
Paivio Norms (Meanp), age of acquisition (AOA)
UTTERANCE TYPE FEATURES:
Ratio of commands (Command), prompts or back-channels (Prompt),
questions (Question), assertions (Assertion)
PROSODIC FEATURES:
Average, minimum, maximum and standard deviation of the voice?s pitch
in Hz (Pitch-mean, Pitch-min, Pitch-max, Pitch-stddev) and intensity in dB
(Int-mean, Int-min, Int-max, Int-stddev), voiced time (Voiced) and speech
rate (Word-per-sec)
Table 2: Description of all features, with feature la-
bels in brackets.
(Boersma, 2001) to compute prosodic features char-
acterizing the voice?s pitch, intensity, and speech
rate.
2.3 Statistical model
By definition, personality evaluation assesses rela-
tive differences between individuals, e.g. one per-
86
son is described as an extravert because the average
population is not. Thus, we formulate personality
recognition as a ranking problem: given two indi-
viduals? extracts, which shows more extraversion?
Personality models are trained using RankBoost,
a boosting algorithm for ranking, for each Big Five
trait using the observers? ratings of personality (Fre-
und et al, 1998). RankBoost expresses the learned
models as rules, which support the analysis of dif-
ferences in the personality models (see section 3).
Each rule modifies the conversation extract?s rank-
ing score by ? whenever a feature value exceeds ex-
perimentally learned thresholds, e.g. Rule 1 of the
extraversion model in Table 4 increases the score of
an extract by ? = 1.43 if the speech rate is above
0.73 words per second. Models are evaluated by a
ranking error function which reports the percentage
of misordered pairs of conversation extracts.
3 Results
The features characterize many aspects of lan-
guage production: utterance types, content and syn-
tax (LIWC), psycholinguistic statistics (MRC), and
prosody. To evaluate how each feature set con-
tributes to the final result, we trained models with
the full feature set and with each set individually.
Results are summarized in Table 3. The baseline is a
model ranking extracts randomly, producing a rank-
ing error of 0.5 on average. Results are averaged
over a 10 fold cross-validation.
Feature set All LIWC MRC Type Pros
Set size 117 88 14 4 11
Extraversion 0.35? 0.36? 0.45 0.55 0.26?
Emot. stability 0.40 0.41 0.39? 0.43 0.45
Agreeableness 0.31? 0.32? 0.44 0.45 0.54
Conscientious. 0.33? 0.36? 0.41? 0.44 0.55
Intellect 0.38? 0.37? 0.41 0.49 0.44
? statistically significant improvement over the random
ordering baseline (two-tailed paired t-test, p < 0.05)
Table 3: Ranking errors over a 10 fold cross-
validation for different feature sets (Type=utterance
type, Pros=prosody). Best models are in bold.
Paired t-tests show that models of extraversion,
agreeableness, conscientiousness and intellect using
all features are better than the random ordering base-
line (two-tailed, p < 0.05)1. Emotional stability is
the most difficult trait to model, while agreeableness
1We also built models of self-reports of personality, but none
of them significantly outperforms the baseline.
and conscientiousness produce the best results, with
ranking errors of 0.31 and 0.33 respectively. Table 3
shows that LIWC features perform significantly bet-
ter than the baseline for all dimensions but emo-
tional stability, while emotional stability is best pre-
dicted by MRC features. Interestingly, prosodic fea-
tures are very good predictors of extraversion, with
a lower ranking error than the full feature set (0.26),
while utterance type features on their own never out-
perform the baseline.
The RankBoost rules indicate the impact of each
feature on the recognition of a personality trait by
the magnitude of the parameter ? associated with
that feature. Table 4 shows the rules with the most
impact on each best model, with the associated ?
values. The feature labels are in Table 2. For ex-
ample, the model of extraversion confirms previous
findings by associating this trait with a high speech
rate (Rules 1 and 4) and longer conversations (Rule
5). But many new markers emerge: extraverts speak
with a high pitch (Rules 2, 6 and 7), while introverts?
pitch varies a lot (Rules 15, 18 and 20). Agreeable
people use longer words but shorter sentences (Rule
1 and 20), while swear words reduce the agreeable-
ness score (Rules 12, 18 and 19). As expected, con-
scientious people talk a lot about their job (Rule 1),
while unconscientious people swear a lot and speak
loudly (Rules 19 and 20). Our models contain many
additional personality cues which aren?t identified
through a typical correlational analysis.
4 Conclusion
We showed that personality can be recognized auto-
matically in conversation. To our knowledge, this is
the first report of experiments testing trained mod-
els on unseen subjects. There are models for each
dimension that perform significantly better than the
baseline. Combinations of these models may be use-
ful to identify important personality types in dif-
ferent NLP applications, e.g. a combination of
extraversion, emotional stability and intellect indi-
cates leadership, while low intellect, extraversion
and agreeableness are correlated with perceptions of
trustworthiness.
One limitation for applications involving speech
recognition is that recognition errors will introduce
noise in all features except prosodic features, and
prosodic features on their own are only effective in
the extraversion model. However, our data set is rel-
atively small (96 subjects) so we expect that more
87
# Extraversion Emotional stability Agreeableness Conscientiousness Intellect
with prosody ? with MRC ? with all ? with all ? with LIWC ?
1 Word-per-sec? 0.73 1.43 Nlet? 3.28 0.53 Nphon? 2.66 0.56 Occup? 1.21 0.37 Colon? 0.03 0.49
2 Pitch-mean? 194.61 0.41 T-L-freq? 28416 0.25 Tentat? 2.83 0.50 Insight? 2.15 0.36 Insight? 1.75 0.37
3 Voiced? 647.35 0.41 Meanc? 384.17 0.24 Colon? 0.03 0.41 Posfeel? 0.30 0.30 Job? 0.29 0.33
4 Word-per-sec? 2.22 0.36 AOA? 277.36 0.24 Posemo? 2.67 0.32 Int-stddev? 7.83 0.29 Music? 0.18 0.32
5 Voiced? 442.95 0.31 K-F-nsamp? 322 0.22 Voiced? 584 0.32 Nlet? 3.29 0.27 Optim? 0.19 0.24
6 Pitch-max? 599.88 0.30 Meanp? 654.57 0.19 Relig? 0.43 0.27 Comm? 1.20 0.26 Inhib? 0.15 0.24
7 Pitch-mean? 238.99 0.26 Conc? 313.55 0.17 Insight? 2.09 0.25 Nphon? 2.66 0.25 Tentat? 2.23 0.22
8 Int-stddev? 6.96 0.24 K-F-ncats? 14.08 0.15 Prompt? 0.06 0.25 Nphon? 2.67 0.22 Posemo? 2.67 0.19
9 Int-max? 85.87 0.24 Nlet? 3.28 0.14 Comma? 4.60 0.23 Nphon? 2.76 0.20 Future? 0.87 0.17
10 Voiced? 132.35 0.23 Nphon? 2.64 0.13 Money? 0.38 0.20 K-F-nsamp? 329 0.19 Certain? 0.92 0.17
11 Pitch-max? 636.35 -0.05 Fam? 601.98 -0.19 Fam? 601.61 -0.16 Swear? 0.20 -0.18 Affect? 5.07 -0.16
12 Pitch-slope? 312.67 -0.06 Nphon? 2.71 -0.19 Swear? 0.41 -0.18 WPS? 6.25 -0.19 Achieve? 0.62 -0.17
13 Int-min? 54.30 -0.06 AOA? 308.39 -0.23 Anger? 0.92 -0.19 Pitch-mean? 229 -0.20 Othref? 7.67 -0.17
14 Word-per-sec? 1.69 -0.06 Brown-freq? 1884 -0.25 Time? 3.71 -0.20 Othref? 7.64 -0.20 I? 7.11 -0.19
15 Pitch-stddev? 115.49 -0.06 Fam? 601.07 -0.25 Negate? 3.52 -0.20 Humans? 0.83 -0.21 WPS? 5.60 -0.20
16 Pitch-max? 637.27 -0.06 K-F-nsamp? 329 -0.26 Fillers? 0.54 -0.22 Swear? 0.93 -0.21 Social? 10.56 -0.20
17 Pitch-slope? 260.51 -0.12 Imag? 333.50 -0.27 Time? 3.69 -0.23 Swear? 0.17 -0.24 You? 3.57 -0.21
18 Pitch-stddev? 118.10 -0.15 Meanp? 642.81 -0.28 Swear? 0.61 -0.27 Relig? 0.32 -0.27 Incl? 4.30 -0.33
19 Int-stddev? 6.30 -0.18 K-F-ncats? 14.32 -0.35 Swear? 0.45 -0.27 Swear? 0.65 -0.31 Physcal? 1.79 -0.33
20 Pitch-stddev? 119.73 -0.47 Nsyl? 1.17 -0.63 WPS? 6.13 -0.45 Int-max? 86.84 -0.50 Family? 0.08 -0.39
Table 4: Best RankBoost models for each trait. Rows 1-10 represent the rules producing the highest score
increase, while rows 11-20 indicate evidence for the other end of the scale, e.g. introversion.
training data would improve model accuracies and
might also make additional features useful. In fu-
ture work, we plan to integrate these models in a di-
alogue system to adapt the system?s language gener-
ation; we will then be able to test whether the accu-
racies we achieve are sufficient and explore methods
for improving them.
Acknowledgements
Thanks to Matthias Mehl and James Pennebaker for
sharing their data.
References
E. Andre?, M. Klesen, P. Gebhard, S. Allen, and T. Rist. 1999.
Integrating models of personality and emotions into lifelike
characters. In Proc. of the International Workshop on Affect
in Interactions, p. 136?149.
P. Boersma. 2001. Praat, a system for doing phonetics by com-
puter. Glot International, 5(9/10):341?345.
M. Coltheart. 1981. The MRC psycholinguistic database.
Quarterly J. of Experimental Psychology, 33A:497?505.
B. Donnellan, R. D. Conger, and C. M. Bryant. 2004. The Big
Five and enduring marriages. J. of Research in Personality,
38:481?504.
Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. 1998. An
efficient boosting algorithm for combining preferences. In
Proc. of the 15th ICML, p. 170?178.
D. Funder and C. Sneed. 1993. Behavioral manifestations of
personality: An ecological approach to judgmental accuracy.
J. of Personality and Social Psychology, 64(3):479?490.
A. Furnham, 1990. Handbook of Language and Social Psychol-
ogy, chapter Language and Personality. Winley.
A. J. Gill and J. Oberlander. 2002. Taking care of the linguistic
features of extraversion. In Proc. of the 24th Annual Confer-
ence of the Cognitive Science Society, p. 363?368.
F. Heylighen and J.-M. Dewaele. 2002. Variation in the contex-
tuality of language: an empirical measure. Context in Con-
text, Special issue of Foundations of Science, 7:293?340.
R. Hogan, G. J. Curphy, and J. Hogan. 1994. What we know
about leadership: Effectiveness and personality. American
Psychologist, 49(6):493?504.
O. P. John and S. Srivastava. 1999. The Big Five trait taxon-
omy: History, measurement, and theoretical perspectives. In
L. A. Pervin and O. P. John, editors, Handbook of personality
theory and research. New York: Guilford Press.
J. Liscombe, J. Venditti, and J. Hirschberg. 2003. Classifying
subject ratings of emotional speech using acoustic features.
In Proc. of Eurospeech - Interspeech 2003, p. 725?728.
M. R. Mehl, S. D. Gosling, and J. W. Pennebaker. In press.
Personality in its natural habitat: Manifestations and implicit
folk theories of personality in daily life. J. of Personality and
Social Psychology.
W. T. Norman. 1963. Toward an adequate taxonomy of person-
ality attributes: Replicated factor structure in peer nomina-
tion personality rating. J. of Abnormal and Social Psychol-
ogy, 66:574?583.
S. Nunn. 2005. Preventing the next terrorist attack: The theory
and practice of homeland security information systems. J. of
Homeland Security and Emergency Management, 2(3).
J. W. Pennebaker and L. A. King. 1999. Linguistic styles: Lan-
guage use as an individual difference. J. of Personality and
Social Psychology, 77:1296?1312.
J. W. Pennebaker, L. E. Francis, and R. J. Booth, 2001. LIWC:
Linguistic Inquiry and Word Count.
B. Reeves and C. Nass. 1996. The Media Equation. University
of Chicago Press.
K. R. Scherer. 1979. Personality markers in speech. In K. R.
Scherer and H. Giles, editors, Social markers in speech, p.
147?209. Cambridge University Press.
S. Tucker and S. Whittaker. 2004. Accessing multimodal meet-
ing data: Systems, problems and possibilities. Lecture Notes
in Computer Science, 3361:1?11.
M. Walker and S. Whittaker. 1990. Mixed initiative in dia-
logue: an investigation into discourse segmentation. In Proc.
of the 28th Annual Meeting of the ACL, p. 70?78.
88
Trainable Sentence Planning for Complex Information
Presentation in Spoken Dialog Systems
Amanda Stent
Stony Brook University
Stony Brook, NY 11794
U.S.A.
stent@cs.sunysb.edu
Rashmi Prasad
University of Pennsylvania
Philadelphia, PA 19104
U.S.A.
rjprasad@linc.cis.upenn.edu
Marilyn Walker
University of Sheffield
Sheffield S1 4DP
U.K.
M.A.Walker@sheffield.ac.uk
Abstract
A challenging problem for spoken dialog sys-
tems is the design of utterance generation mod-
ules that are fast, flexible and general, yet pro-
duce high quality output in particular domains.
A promising approach is trainable generation,
which uses general-purpose linguistic knowledge
automatically adapted to the application do-
main. This paper presents a trainable sentence
planner for the MATCH dialog system. We
show that trainable sentence planning can pro-
duce output comparable to that of MATCH?s
template-based generator even for quite com-
plex information presentations.
1 Introduction
One very challenging problem for spoken dialog
systems is the design of the utterance genera-
tion module. This challenge arises partly from
the need for the generator to adapt to many
features of the dialog domain, user population,
and dialog context.
There are three possible approaches to gener-
ating system utterances. The first is template-
based generation, used in most dialog systems
today. Template-based generation enables a
programmer without linguistic training to pro-
gram a generator that can efficiently produce
high quality output specific to different dialog
situations. Its drawbacks include the need to
(1) create templates anew by hand for each ap-
plication; (2) design and maintain a set of tem-
plates that work well together in many dialog
contexts; and (3) repeatedly encode linguistic
constraints such as subject-verb agreement.
The second approach is natural language gen-
eration (NLG), which divides generation into:
(1) text (or content) planning, (2) sentence
planning, and (3) surface realization. NLG
promises portability across domains and dialog
contexts by using general rules for each genera-
tion module. However, the quality of the output
for a particular domain, or a particular dialog
context, may be inferior to that of a template-
based system unless domain-specific rules are
developed or general rules are tuned for the par-
ticular domain. Furthermore, full NLG may be
too slow for use in dialog systems.
A third, more recent, approach is trainable
generation: techniques for automatically train-
ing NLG modules, or hybrid techniques that
adapt NLG modules to particular domains or
user groups, e.g. (Langkilde, 2000; Mellish,
1998; Walker, Rambow and Rogati, 2002).
Open questions about the trainable approach
include (1) whether the output quality is high
enough, and (2) whether the techniques work
well across domains. For example, the training
method used in SPoT (Sentence Planner Train-
able), as described in (Walker, Rambow and Ro-
gati, 2002), was only shown to work in the travel
domain, for the information gathering phase of
the dialog, and with simple content plans in-
volving no rhetorical relations.
This paper describes trainable sentence
planning for information presentation in the
MATCH (Multimodal Access To City Help) di-
alog system (Johnston et al, 2002). We pro-
vide evidence that the trainable approach is
feasible by showing (1) that the training tech-
nique used for SPoT can be extended to a
new domain (restaurant information); (2) that
this technique, previously used for information-
gathering utterances, can be used for infor-
mation presentations, namely recommendations
and comparisons; and (3) that the quality
of the output is comparable to that of a
template-based generator previously developed
and experimentally evaluated with MATCH
users (Walker et al, 2002; Stent et al, 2002).
Section 2 describes SPaRKy (Sentence Plan-
ning with Rhetorical Knowledge), an extension
of SPoT that uses rhetorical relations. SPaRKy
consists of a randomized sentence plan gen-
erator (SPG) and a trainable sentence plan
ranker (SPR); these are described in Sections 3
strategy:recommend
items: Chanpen Thai
relations:justify(nuc:1;sat:2); justify(nuc:1;sat:3); jus-
tify(nuc:1;sat:4)
content: 1. assert(best(Chanpen Thai))
2. assert(has-att(Chanpen Thai, decor(decent)))
3. assert(has-att(Chanpen Thai, service(good))
4. assert(has-att(Chanpen Thai, cuisine(Thai)))
Figure 1: A content plan for a recommendation
for a restaurant in midtown Manhattan
strategy:compare3
items: Above, Carmine?s
relations:elaboration(1;2); elaboration(1;3); elabora-
tion(1,4); elaboration(1,5); elaboration(1,6);
elaboration(1,7); contrast(2;3); contrast(4;5);
contrast(6;7)
content: 1. assert(exceptional(Above, Carmine?s))
2. assert(has-att(Above, decor(good)))
3. assert(has-att(Carmine?s, decor(decent)))
4. assert(has-att(Above, service(good)))
5. assert(has-att(Carmine?s, service(good)))
6. assert(has-att(Above, cuisine(New Ameri-
can)))
7. assert(has-att(Carmine?s, cuisine(italian)))
Figure 2: A content plan for a comparison be-
tween restaurants in midtown Manhattan
and 4. Section 5 presents the results of two
experiments. The first experiment shows that
given a content plan such as that in Figure 1,
SPaRKy can select sentence plans that commu-
nicate the desired rhetorical relations, are sig-
nificantly better than a randomly selected sen-
tence plan, and are on average less than 10%
worse than a sentence plan ranked highest by
human judges. The second experiment shows
that the quality of SPaRKy?s output is compa-
rable to that of MATCH?s template-based gen-
erator. We sum up in Section 6.
2 SPaRKy Architecture
Information presentation in the MATCH sys-
tem focuses on user-tailored recommendations
and comparisons of restaurants (Walker et al,
2002). Following the bottom-up approach to
text-planning described in (Marcu, 1997; Mel-
lish, 1998), each presentation consists of a set of
assertions about a set of restaurants and a spec-
ification of the rhetorical relations that hold be-
tween them. Example content plans are shown
in Figures 1 and 2. The job of the sentence
planner is to choose linguistic resources to real-
ize a content plan and then rank the resulting
alternative realizations. Figures 3 and 4 show
alternative realizations for the content plans in
Figures 1 and 2.
Alt Realization H SPR
2 Chanpen Thai, which is a Thai restau-
rant, has decent decor. It has good
service. It has the best overall quality
among the selected restaurants.
3 .28
5 Since Chanpen Thai is a Thai restau-
rant, with good service, and it has de-
cent decor, it has the best overall qual-
ity among the selected restaurants.
2.5 .14
6 Chanpen Thai, which is a Thai restau-
rant, with decent decor and good ser-
vice, has the best overall quality among
the selected restaurants.
4 .70
Figure 3: Some alternative sentence plan real-
izations for the recommendation in Figure 1. H
= Humans? score. SPR = SPR?s score.
Alt Realization H SPR
11 Above and Carmine?s offer exceptional
value among the selected restaurants.
Above, which is a New American
restaurant, with good decor, has good
service. Carmine?s, which is an Italian
restaurant, with good service, has de-
cent decor.
2 .73
12 Above and Carmine?s offer exceptional
value among the selected restaurants.
Above has good decor, and Carmine?s
has decent decor. Above and Carmine?s
have good service. Above is a New
American restaurant. On the other
hand, Carmine?s is an Italian restau-
rant.
2.5 .50
13 Above and Carmine?s offer exceptional
value among the selected restaurants.
Above is a New American restaurant.
It has good decor. It has good service.
Carmine?s, which is an Italian restau-
rant, has decent decor and good service.
3 .67
20 Above and Carmine?s offer exceptional
value among the selected restaurants.
Carmine?s has decent decor but Above
has good decor, and Carmine?s and
Above have good service. Carmine?s is
an Italian restaurant. Above, however,
is a New American restaurant.
2.5 .49
25 Above and Carmine?s offer exceptional
value among the selected restaurants.
Above has good decor. Carmine?s is
an Italian restaurant. Above has good
service. Carmine?s has decent decor.
Above is a New American restaurant.
Carmine?s has good service.
NR NR
Figure 4: Some of the alternative sentence plan
realizations for the comparison in Figure 2. H
= Humans? score. SPR = SPR?s score. NR =
Not generated or ranked
The architecture of the spoken language gen-
eration module in MATCH is shown in Figure 5.
The dialog manager sends a high-level commu-
nicative goal to the SPUR text planner, which
selects the content to be communicated using a
user model and brevity constraints (see (Walker
Synthesizer
How to Say It
Realizer
Surface
Assigner
Prosody
Speech
  
UTTERANCE
SYSTEM
Sentence
SPUR
Planner
Communicative
DIALOGUE
MANAGER
Goals
Text
Planner
What to Say
Figure 5: A dialog system with a spoken lan-
guage generator
et al, 2002)). The output is a content plan for
a recommendation or comparison such as those
in Figures 1 and 2.
SPaRKy, the sentence planner, gets the con-
tent plan, and then a sentence plan generator
(SPG) generates one or more sentence plans
(Figure 7) and a sentence plan ranker (SPR)
ranks the generated plans. In order for the
SPG to avoid generating sentence plans that are
clearly bad, a content-structuring module first
finds one or more ways to linearly order the in-
put content plan using principles of entity-based
coherence based on rhetorical relations (Knott
et al, 2001). It outputs a set of text plan
trees (tp-trees), consisting of a set of speech
acts to be communicated and the rhetorical re-
lations that hold between them. For example,
the two tp-trees in Figure 6 are generated for
the content plan in Figure 2. Sentence plans
such as alternative 25 in Figure 4 are avoided;
it is clearly worse than alternatives 12, 13 and
20 since it neither combines information based
on a restaurant entity (e.g Babbo) nor on an
attribute (e.g. decor).
The top ranked sentence plan output by the
SPR is input to the RealPro surface realizer
which produces a surface linguistic utterance
(Lavoie and Rambow, 1997). A prosody as-
signment module uses the prior levels of linguis-
tic representation to determine the appropriate
prosody for the utterance, and passes a marked-
up string to the text-to-speech module.
3 Sentence Plan Generation
As in SPoT, the basis of the SPG is a set of
clause-combining operations that operate on tp-
trees and incrementally transform the elemen-
tary predicate-argument lexico-structural rep-
resentations (called DSyntS (Melcuk, 1988))
associated with the speech-acts on the leaves
of the tree. The operations are applied in a
bottom-up left-to-right fashion and the result-
ing representation may contain one or more sen-
tences. The application of the operations yields
two parallel structures: (1) a sentence plan
tree (sp-tree), a binary tree with leaves labeled
by the assertions from the input tp-tree, and in-
terior nodes labeled with clause-combining op-
erations; and (2) one or more DSyntS trees
(d-trees) which reflect the parallel operations
on the predicate-argument representations.
We generate a random sample of possible
sentence plans for each tp-tree, up to a pre-
specified number of sentence plans, by ran-
domly selecting among the operations accord-
ing to a probability distribution that favors pre-
ferred operations1. The choice of operation is
further constrained by the rhetorical relation
that relates the assertions to be combined, as
in other work e.g. (Scott and de Souza, 1990).
In the current work, three RST rhetorical rela-
tions (Mann and Thompson, 1987) are used in
the content planning phase to express the rela-
tions between assertions: the justify relation
for recommendations, and the contrast and
elaboration relations for comparisons. We
added another relation to be used during the
content-structuring phase, called infer, which
holds for combinations of speech acts for which
there is no rhetorical relation expressed in the
content plan, as in (Marcu, 1997). By explicitly
representing the discourse structure of the infor-
mation presentation, we can generate informa-
tion presentations with considerably more inter-
nal complexity than those generated in (Walker,
Rambow and Rogati, 2002) and eliminate those
that violate certain coherence principles, as de-
scribed in Section 2.
The clause-combining operations are general
operations similar to aggregation operations
used in other research (Rambow and Korelsky,
1992; Danlos, 2000). The operations and the
1Although the probability distribution here is hand-
crafted based on assumed preferences for operations such
as merge, relative-clause and with-reduction, it
might also be possible to learn this probability distribu-
tion from the data by training in two phases.
nucleus:<3>assert-com-decor
contrast
nucleus:<2>assert-com-decor nucleus:<6>assert-com-cuisine
nucleus:<7>assert-com-cuisine
contrast
nucleus:<4>assert-com-service
nucleus:<5>assert-com-service
contrast
elaboration
nucleus:<1>assert-com-list_exceptional infer
nucleus:<3>assert-com-decor
nucleus:<5>assert-com-service
nucleus:<7>assert-com-cuisine
inferinfer
nucleus:<2>assert-com-decor nucleus:<6>assert-com-cuisine
nucleus:<4>assert-com-service
elaboration
nucleus:<1>assert-com-list_exceptional contrast
Figure 6: Two tp-trees for alternative 13 in Figure 4.
constraints on their use are described below.
merge applies to two clauses with identical
matrix verbs and all but one identical argu-
ments. The clauses are combined and the non-
identical arguments coordinated. For example,
merge(Above has good service;Carmine?s has
good service) yields Above and Carmine?s have
good service. merge applies only for the rela-
tions infer and contrast.
with-reduction is treated as a kind of
?verbless? participial clause formation in which
the participial clause is interpreted with the
subject of the unreduced clause. For exam-
ple, with-reduction(Above is a New Amer-
ican restaurant;Above has good decor) yields
Above is a New American restaurant, with good
decor. with-reduction uses two syntactic
constraints: (a) the subjects of the clauses must
be identical, and (b) the clause that under-
goes the participial formation must have a have-
possession predicate. In the example above, for
instance, the Above is a New American restau-
rant clause cannot undergo participial forma-
tion since the predicate is not one of have-
possession. with-reduction applies only for
the relations infer and justify.
relative-clause combines two clauses with
identical subjects, using the second clause to
relativize the first clause?s subject. For ex-
ample, relative-clause(Chanpen Thai is a
Thai restaurant, with decent decor and good ser-
vice;Chanpen Thai has the best overall quality
among the selected restaurants) yields Chanpen
Thai, which is a Thai restaurant, with decent
decor and good service, has the best overall qual-
ity among the selected restaurants. relative-
clause also applies only for the relations infer
and justify.
cue-word inserts a discourse connective
(one of since, however, while, and, but, and on
the other hand), between the two clauses to be
combined. cue-word conjunction combines
two distinct clauses into a single sentence with a
coordinating or subordinating conjunction (e.g.
Above has decent decor BUT Carmine?s has
good decor), while cue-word insertion inserts
a cue word at the start of the second clause, pro-
ducing two separate sentences (e.g. Carmine?s
is an Italian restaurant. HOWEVER, Above
is a New American restaurant). The choice of
cue word is dependent on the rhetorical relation
holding between the clauses.
Finally, period applies to two clauses to be
treated as two independent sentences.
Note that a tp-tree can have very different
realizations, depending on the operations of the
SPG. For example, the second tp-tree in Fig-
ure 6 yields both Alt 11 and Alt 13 in Figure 4.
However, Alt 13 is more highly rated than Alt
11. The sp-tree and d-tree produced by the SPG
for Alt 13 are shown in Figures 7 and 8. The
composite labels on the interior nodes of the sp-
PERIOD_elaboration
PERIOD_contrast
RELATIVE_CLAUSE_inferPERIOD_infer
PERIOD_infer <4>assert-com-service <7>assert-com-cuisine MERGE_infer
<3>assert-come-decor <5>assert-com-service<2>assert-com-decor<6>assert-com-cuisine
<1>assert-com-list_exceptional
Figure 7: Sentence plan tree (sp-tree) for alternative 13 in Figure 4
offer
exceptional
among
restaurant
selected
Above_and_Carmine?s
Carmine?s
BE3
restaurantCarmine?s
Italian
decor
decent AND2
service
good
HAVE1
PERIOD
New_American
BE3
Above Above decor
good
HAVE1
restaurant
Above
good
HAVE1
service
PERIOD
PERIOD
value
PERIOD
Figure 8: Dependency tree (d-tree) for alternative 13 in Figure 4
tree indicate the clause-combining relation se-
lected to communicate the specified rhetorical
relation. The d-tree for Alt 13 in Figure 8 shows
that the SPG treats the period operation as
part of the lexico-structural representation for
the d-tree. After sentence planning, the d-tree
is split into multiple d-trees at period nodes;
these are sent to the RealPro surface realizer.
Separately, the SPG also handles referring ex-
pression generation by converting proper names
to pronouns when they appear in the previous
utterance. The rules are applied locally, across
adjacent sequences of utterances (Brennan et
al., 1987). Referring expressions are manipu-
lated in the d-trees, either intrasententially dur-
ing the creation of the sp-tree, or intersenten-
tially, if the full sp-tree contains any period op-
erations. The third and fourth sentences for Alt
13 in Figure 4 show the conversion of a named
restaurant (Carmine?s) to a pronoun.
4 Training the Sentence Plan
Ranker
The SPR takes as input a set of sp-trees gener-
ated by the SPG and ranks them. The SPR?s
rules for ranking sp-trees are learned from a la-
beled set of sentence-plan training examples us-
ing the RankBoost algorithm (Schapire, 1999).
Examples and Feedback: To apply Rank-
Boost, a set of human-rated sp-trees are en-
coded in terms of a set of features. We started
with a set of 30 representative content plans for
each strategy. The SPG produced as many as 20
distinct sp-trees for each content plan. The sen-
tences, realized by RealPro from these sp-trees,
were then rated by two expert judges on a scale
from 1 to 5, and the ratings averaged. Each sp-
tree was an example input for RankBoost, with
each corresponding rating its feedback.
Features used by RankBoost: RankBoost
requires each example to be encoded as a set of
real-valued features (binary features have val-
ues 0 and 1). A strength of RankBoost is that
the set of features can be very large. We used
7024 features for training the SPR. These fea-
tures count the number of occurrences of certain
structural configurations in the sp-trees and the
d-trees, in order to capture declaratively de-
cisions made by the randomized SPG, as in
(Walker, Rambow and Rogati, 2002). The fea-
tures were automatically generated using fea-
ture templates. For this experiment, we use
two classes of feature: (1) Rule-features: These
features are derived from the sp-trees and repre-
sent the ways in which merge, infer and cue-
word operations are applied to the tp-trees.
These feature names start with ?rule?. (2) Sent-
features: These features are derived from the
DSyntSs, and describe the deep-syntactic struc-
ture of the utterance, including the chosen lex-
emes. As a result, some may be domain specific.
These feature names are prefixed with ?sent?.
We now describe the feature templates used
in the discovery process. Three templates were
used for both sp-tree and d-tree features; two
were used only for sp-tree features. Local feature
templates record structural configurations local
to a particular node (its ancestors, daughters
etc.). Global feature templates, which are used
only for sp-tree features, record properties of the
entire sp-tree. We discard features that occur
fewer than 10 times to avoid those specific to
particular text plans.
Strategy System Min Max Mean S.D.
Recommend SPaRKy 2.0 5.0 3.6 .71
HUMAN 2.5 5.0 3.9 .55
RANDOM 1.5 5.0 2.9 .88
Compare2 SPaRKy 2.5 5.0 3.9 .71
HUMAN 2.5 5.0 4.4 .54
RANDOM 1.0 5.0 2.9 1.3
Compare3 SPaRKy 1.5 4.5 3.4 .63
HUMAN 3.0 5.0 4.0 .49
RANDOM 1.0 4.5 2.7 1.0
Table 1: Summary of Recommend, Compare2
and Compare3 results (N = 180)
There are four types of local feature
template: traversal features, sister features,
ancestor features and leaf features. Local
feature templates are applied to all nodes in a
sp-tree or d-tree (except that the leaf feature is
not used for d-trees); the value of the resulting
feature is the number of occurrences of the
described configuration in the tree. For each
node in the tree, traversal features record the
preorder traversal of the subtree rooted at
that node, for all subtrees of all depths. An
example is the feature ?rule traversal assert-
com-list exceptional? (with value 1) of the
tree in Figure 7. Sister features record all
consecutive sister nodes. An example is the fea-
ture ?rule sisters PERIOD infer RELATIVE
CLAUSE infer? (with value 1) of the
tree in Figure 7. For each node in the
tree, ancestor features record all the ini-
tial subpaths of the path from that node
to the root. An example is the feature
?rule ancestor PERIOD contrast*PERIOD
infer? (with value 1) of the tree in Figure 7.
Finally, leaf features record all initial substrings
of the frontier of the sp-tree. For example, the
sp-tree of Figure 7 has value 1 for the feature
?leaf #assert-com-list exceptional#assert-com-
cuisine?.
Global features apply only to the sp-
tree. They record, for each sp-tree and for
each clause-combining operation labeling a non-
frontier node, (1) the minimal number of leaves
dominated by a node labeled with that op-
eration in that tree (MIN); (2) the maximal
number of leaves dominated by a node la-
beled with that operation (MAX); and (3)
the average number of leaves dominated by
a node labeled with that operation (AVG).
For example, the sp-tree in Figure 7 has
value 3 for ?PERIOD infer max?, value 2 for
?PERIOD infer min? and value 2.5 for ?PE-
RIOD infer avg?.
5 Experimental Results
We report two sets of experiments. The first ex-
periment tests the ability of the SPR to select a
high quality sentence plan from a population of
sentence plans randomly generated by the SPG.
Because the discriminatory power of the SPR is
best tested by the largest possible population of
sentence plans, we use 2-fold cross validation for
this experiment. The second experiment com-
pares SPaRKy to template-based generation.
Cross Validation Experiment: We re-
peatedly tested SPaRKy on the half of the cor-
pus of 1756 sp-trees held out as test data for
each fold. The evaluation metric is the human-
assigned score for the variant that was rated
highest by SPaRKy for each text plan for each
task/user combination. We evaluated SPaRKy
on the test sets by comparing three data points
for each text plan: HUMAN (the score of the
top-ranked sentence plan); SPARKY (the score
of the SPR?s selected sentence); and RANDOM
(the score of a sentence plan randomly selected
from the alternate sentence plans).
We report results separately for comparisons
between two entities and among three or more
entities. These two types of comparison are gen-
erated using different strategies in the SPG, and
can produce text that is very different both in
terms of length and structure.
Table 1 summarizes the difference between
SPaRKy, HUMAN and RANDOM for recom-
mendations, comparisons between two entities
and comparisons between three or more enti-
ties. For all three presentation types, a paired
t-test comparing SPaRKy to HUMAN to RAN-
DOM showed that SPaRKy was significantly
better than RANDOM (df = 59, p < .001) and
significantly worse than HUMAN (df = 59, p
< .001). This demonstrates that the use of a
trainable sentence planner can lead to sentence
plans that are significantly better than baseline
(RANDOM), with less human effort than pro-
gramming templates.
Comparison with template generation:
For each content plan input to SPaRKy, the
judges also rated the output of a template-
based generator for MATCH. This template-
based generator performs text planning and sen-
tence planning (the focus of the current pa-
per), including some discourse cue insertion,
clause combining and referring expression gen-
eration; the templates themselves are described
in (Walker et al, 2002). Because the templates
are highly tailored to this domain, this genera-
tor can be expected to perform well. Example
template-based and SPaRKy outputs for a com-
parison between three or more items are shown
in Figure 9.
Strategy System Min Max Mean S.D.
Recommend Template 2.5 5.0 4.22 0.74
SPaRKy 2.5 4.5 3.57 0.59
HUMAN 4.0 5.0 4.37 0.37
Compare2 Template 2.0 5.0 3.62 0.75
SPaRKy 2.5 4.75 3.87 0.52
HUMAN 4.0 5.0 4.62 0.39
Compare3 Template 1.0 5.0 4.08 1.23
SPaRKy 2.5 4.25 3.375 0.38
HUMAN 4.0 5.0 4.63 0.35
Table 2: Summary of template-based genera-
tion results. N = 180
Table 2 shows the mean HUMAN scores for
the template-based sentence planning. A paired
t-test comparing HUMAN and template-based
scores showed that HUMAN was significantly
better than template-based sentence planning
only for compare2 (df = 29, t = 6.2, p < .001).
The judges evidently did not like the template
for comparisons between two items. A paired
t-test comparing SPaRKy and template-based
sentence planning showed that template-based
sentence planning was significantly better than
SPaRKy only for recommendations (df = 29, t
= 3.55, p < .01). These results demonstrate
that trainable sentence planning shows promise
for producing output comparable to that of a
template-based generator, with less program-
ming effort and more flexibility.
The standard deviation for all three template-
based strategies was wider than for HUMAN
or SPaRKy, indicating that there may be
content-specific aspects to the sentence plan-
ning done by SPaRKy that contribute to out-
put variation. The data show this to be cor-
rect; SPaRKy learned content-specific prefer-
ences about clause combining and discourse cue
insertion that a template-based generator can-
System Realization H
Template Among the selected restaurants, the fol-
lowing offer exceptional overall value.
Uguale?s price is 33 dollars. It has good
decor and very good service. It?s a
French, Italian restaurant. Da Andrea?s
price is 28 dollars. It has good decor and
very good service. It?s an Italian restau-
rant. John?s Pizzeria?s price is 20 dollars.
It has mediocre decor and decent service.
It?s an Italian, Pizza restaurant.
4.5
SPaRKy Da Andrea, Uguale, and John?s Pizze-
ria offer exceptional value among the se-
lected restaurants. Da Andrea is an Ital-
ian restaurant, with very good service, it
has good decor, and its price is 28 dol-
lars. John?s Pizzeria is an Italian , Pizza
restaurant. It has decent service. It has
mediocre decor. Its price is 20 dollars.
Uguale is a French, Italian restaurant,
with very good service. It has good decor,
and its price is 33 dollars.
4
Figure 9: Comparisons between 3 or more
items, H = Humans? score
not easily model, but that a trainable sentence
planner can. For example, Table 3 shows the
nine rules generated on the first test fold which
have the largest negative impact on the final
RankBoost score (above the double line) and
the largest positive impact on the final Rank-
Boost score (below the double line), for com-
parisons between three or more entities. The
rule with the largest positive impact shows that
SPaRKy learned to prefer that justifications in-
volving price be merged with other information
using a conjunction.
These rules are also specific to presentation
type. Averaging over both folds of the exper-
iment, the number of unique features appear-
ing in rules is 708, of which 66 appear in the
rule sets for two presentation types and 9 ap-
pear in the rule sets for all three presentation
types. There are on average 214 rule features,
428 sentence features and 26 leaf features. The
majority of the features are ancestor features
(319) followed by traversal features (264) and
sister features (60). The remainder of the fea-
tures (67) are for specific lexemes.
To sum up, this experiment shows that the
ability to model the interactions between do-
main content, task and presentation type is a
strength of the trainable approach to sentence
planning.
6 Conclusions
This paper shows that the training technique
used in SPoT can be easily extended to a new
N Condition ?s
1 sent anc PROPERNOUN RESTAURANT
*HAVE1 ? 16.5
-0.859
2 sent anc II Upper East Side*ATTR IN1*
locate ? 4.5
-0.852
3 sent anc PERIOD infer*PERIOD infer
*PERIOD elaboration ? -?
-0.542
4 rule anc assert-com-service*MERGE infer
? 1.5
-0.356
5 sent tvl depth 0 BE3 ? 4.5 -0.346
6 rule anc PERIOD infer*PERIOD infer
*PERIOD elaboration ? -?
-0.345
7 rule anc assert-com-decor*PERIOD infer
*PERIOD infer*PERIOD contrast *PE-
RIOD elaboration? -?
-0.342
8 rule anc assert-com-food quality*MERGE
infer ? 1.5
0.398
9 rule anc assert-com-price*CW
CONJUNCTION infer*PERIOD justify
? -?
0.527
Table 3: The nine rules generated on the first
test fold which have the largest negative impact
on the final RankBoost score (above the dou-
ble line) and the largest positive impact on the
final RankBoost score (below the double line),
for Compare3. ?s represents the increment or
decrement associated with satisfying the condi-
tion.
domain and used for information presentation
as well as information gathering. Previous work
on SPoT also compared trainable sentence plan-
ning to a template-based generator that had
previously been developed for the same appli-
cation (Rambow et al, 2001). The evalua-
tion results for SPaRKy (1) support the results
for SPoT, by showing that trainable sentence
generation can produce output comparable to
template-based generation, even for complex in-
formation presentations such as extended com-
parisons; (2) show that trainable sentence gen-
eration is sensitive to variations in domain ap-
plication, presentation type, and even human
preferences about the arrangement of particu-
lar types of information.
7 Acknowledgments
We thank AT&T for supporting this research,
and the anonymous reviewers for their helpful
comments on this paper.
References
I. Langkilde. Forest-based statistical sentence gen-
eration. In Proc. NAACL 2000, 2000.
S. E. Brennan, M. Walker Friedman, and C. J. Pol-
lard. A centering approach to pronouns. In Proc.
25th Annual Meeting of the ACL, Stanford, pages
155?162, 1987.
L. Danlos. 2000. G-TAG: A lexicalized formal-
ism for text generation inspired by tree ad-
joining grammar. In Tree Adjoining Grammars:
Formalisms, Linguistic Analysis, and Processing.
CSLI Publications.
M. Johnston, S. Bangalore, G. Vasireddy, A. Stent,
P. Ehlen, M. Walker, S. Whittaker, and P. Mal-
oor. MATCH: An architecture for multimodal di-
alogue systems. In Annual Meeting of the ACL,
2002.
A. Knott, J. Oberlander, M. O?Donnell and C. Mel-
lish. Beyond Elaboration: the interaction of rela-
tions and focus in coherent text. In Text Repre-
sentation: linguistic and psycholinguistic aspects,
pages 181-196, 2001.
B. Lavoie and O. Rambow. A fast and portable re-
alizer for text generation systems. In Proc. of the
3rd Conference on Applied Natural Language Pro-
cessing, ANLP97, pages 265?268, 1997.
W.C. Mann and S.A. Thompson. Rhetorical struc-
ture theory: A framework for the analysis of texts.
Technical Report RS-87-190, USC/Information
Sciences Institute, 1987.
D. Marcu. From local to global coherence: a
bottom-up approach to text planning. In Proceed-
ings of the National Conference on Artificial In-
telligence (AAAI?97), 1997.
C. Mellish, A. Knott, J. Oberlander, and M.
O?Donnell. Experiments using stochastic search
for text planning. In Proceedings of INLG-98.
1998.
I. A. Melc?uk. Dependency Syntax: Theory and Prac-
tice. SUNY, Albany, New York, 1988.
O. Rambow and T. Korelsky. Applied text genera-
tion. In Proceedings of the Third Conference on
Applied Natural Language Processing, ANLP92,
pages 40?47, 1992.
O. Rambow, M. Rogati and M. A. Walker. Evalu-
ating a Trainable Sentence Planner for a Spoken
Dialogue Travel System In Meeting of the ACL,
2001.
R. E. Schapire. A brief introduction to boosting. In
Proc. of the 16th IJCAI, 1999.
D. R. Scott and C. Sieckenius de Souza. Getting
the message across in RST-based text generation.
In Current Research in Natural Language Gener-
ation, pages 47?73, 1990.
A. Stent, M. Walker, S. Whittaker, and P. Maloor.
User-tailored generation for spoken dialogue: An
experiment. In Proceedings of ICSLP 2002., 2002.
M. A. Walker, S. J. Whittaker, A. Stent, P. Mal-
oor, J. D. Moore, M. Johnston, and G. Vasireddy.
Speech-Plans: Generating evaluative responses
in spoken dialogue. In Proceedings of INLG-02.,
2002.
M. Walker, O. Rambow, and M. Rogati. Training a
sentence planner for spoken dialogue using boost-
ing. Computer Speech and Language: Special Is-
sue on Spoken Language Generation, 2002.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 265?272,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning to Generate Naturalistic Utterances Using Reviews in Spoken
Dialogue Systems
Ryuichiro Higashinaka
NTT Corporation
rh@cslab.kecl.ntt.co.jp
Rashmi Prasad
University of Pennsylvania
rjprasad@linc.cis.upenn.edu
Marilyn A. Walker
University of Sheffield
walker@dcs.shef.ac.uk
Abstract
Spoken language generation for dialogue
systems requires a dictionary of mappings
between semantic representations of con-
cepts the system wants to express and re-
alizations of those concepts. Dictionary
creation is a costly process; it is currently
done by hand for each dialogue domain.
We propose a novel unsupervised method
for learning such mappings from user re-
views in the target domain, and test it on
restaurant reviews. We test the hypothesis
that user reviews that provide individual
ratings for distinguished attributes of the
domain entity make it possible to map re-
view sentences to their semantic represen-
tation with high precision. Experimental
analyses show that the mappings learned
cover most of the domain ontology, and
provide good linguistic variation. A sub-
jective user evaluation shows that the con-
sistency between the semantic representa-
tions and the learned realizations is high
and that the naturalness of the realizations
is higher than a hand-crafted baseline.
1 Introduction
One obstacle to the widespread deployment of
spoken dialogue systems is the cost involved
with hand-crafting the spoken language generation
module. Spoken language generation requires a
dictionary of mappings between semantic repre-
sentations of concepts the system wants to express
and realizations of those concepts. Dictionary cre-
ation is a costly process: an automatic method
for creating them would make dialogue technol-
ogy more scalable. A secondary benefit is that a
learned dictionary may produce more natural and
colloquial utterances.
We propose a novel method for mining user re-
views to automatically acquire a domain specific
generation dictionary for information presentation
in a dialogue system. Our hypothesis is that re-
views that provide individual ratings for various
distinguished attributes of review entities can be
used to map review sentences to a semantic rep-
An example user review (we8there.com)
Ratings Food=5, Service=5, Atmosphere=5,
Value=5, Overall=5
Review
comment
The best Spanish food in New York. I am
from Spain and I had my 28th birthday
there and we all had a great time. Salud!
?
Review comment after named entity recognition
The best {NE=foodtype, string=Spanish} {NE=food,
string=food, rating=5} in {NE=location, string=New
York}. . . .
?
Mapping between a semantic representation (a set of
relations) and a syntactic structure (DSyntS)
? Relations:
RESTAURANT has FOODTYPE
RESTAURANT has foodquality=5
RESTAURANT has LOCATION
([foodtype, food=5, location] for shorthand.)
? DSyntS:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
lexeme : food
class : common noun
number : sg
article : def
ATTR
[
lexeme : best
class : adjective
]
ATTR
?
?
lexeme : FOODTYPE
class : common noun
number : sg
article : no-art
?
?
ATTR
?
?
?
?
?
lexeme : in
class : preposition
II
?
?
lexeme : LOCATION
class : proper noun
number : sg
article : no-art
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Example of procedure for acquiring a
generation dictionary mapping.
resentation. Figure 1 shows a user review in the
restaurant domain, where we hypothesize that the
user rating food=5 indicates that the semantic rep-
resentation for the sentence ?The best Spanish
food in New York? includes the relation ?RESTAU-
RANT has foodquality=5.?
We apply the method to extract 451 mappings
from restaurant reviews. Experimental analyses
show that the mappings learned cover most of the
domain ontology, and provide good linguistic vari-
ation. A subjective user evaluation indicates that
the consistency between the semantic representa-
tions and the learned realizations is high and that
the naturalness of the realizations is significantly
higher than a hand-crafted baseline.
265
Section 2 provides a step-by-step description of
the method. Sections 3 and 4 present the evalua-
tion results. Section 5 covers related work. Sec-
tion 6 summarizes and discusses future work.
2 Learning a Generation Dictionary
Our automatically created generation dictionary
consists of triples (U ,R,S) representing a map-
ping between the original utterance U in the user
review, its semantic representation R(U), and its
syntactic structure S(U). Although templates are
widely used in many practical systems (Seneff and
Polifroni, 2000; Theune, 2003), we derive syn-
tactic structures to represent the potential realiza-
tions, in order to allow aggregation, and other
syntactic transformations of utterances, as well as
context specific prosody assignment (Walker et al,
2003; Moore et al, 2004).
The method is outlined briefly in Fig. 1 and de-
scribed below. It comprises the following steps:
1. Collect user reviews on the web to create a
population of utterances U .
2. To derive semantic representations R(U):
? Identify distinguished attributes and
construct a domain ontology;
? Specify lexicalizations of attributes;
? Scrape webpages? structured data for
named-entities;
? Tag named-entities.
3. Derive syntactic representations S(U).
4. Filter inappropriate mappings.
5. Add mappings (U ,R,S) to dictionary.
2.1 Creating the corpus
We created a corpus of restaurant reviews by
scraping 3,004 user reviews of 1,810 restau-
rants posted at we8there.com (http://www.we8-
there.com/), where each individual review in-
cludes a 1-to-5 Likert-scale rating of different
restaurant attributes. The corpus consists of
18,466 sentences.
2.2 Deriving semantic representations
The distinguished attributes are extracted from the
webpages for each restaurant entity. They in-
clude attributes that the users are asked to rate,
i.e. food, service, atmosphere, value, and over-
all, which have scalar values. In addition, other
attributes are extracted from the webpage, such
as the name, foodtype and location of the restau-
rant, which have categorical values. The name
attribute is assumed to correspond to the restau-
rant entity. Given the distinguished attributes, a
Dist. Attr. Lexicalization
food food, meal
service service, staff, waitstaff, wait staff, server,
waiter, waitress
atmosphere atmosphere, decor, ambience, decoration
value value, price, overprice, pricey, expensive,
inexpensive, cheap, affordable, afford
overall recommend, place, experience, establish-
ment
Table 1: Lexicalizations for distinguished at-
tributes.
simple domain ontology can be automatically de-
rived by assuming that a meronymy relation, rep-
resented by the predicate ?has?, holds between the
entity type (RESTAURANT) and the distinguished
attributes. Thus, the domain ontology consists of
the relations:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
RESTAURANT has foodquality
RESTAURANT has servicequality
RESTAURANT has valuequality
RESTAURANT has atmospherequality
RESTAURANT has overallquality
RESTAURANT has foodtype
RESTAURANT has location
We assume that, although users may discuss
other attributes of the entity, at least some of the
utterances in the reviews realize the relations spec-
ified in the ontology. Our problem then is to iden-
tify these utterances. We test the hypothesis that,
if an utterance U contains named-entities corre-
sponding to the distinguished attributes, thatR for
that utterance includes the relation concerning that
attribute in the domain ontology.
We define named-entities for lexicalizations of
the distinguished attributes, starting with the seed
word for that attribute on the webpage (Table 1).1
For named-entity recognition, we use GATE (Cun-
ningham et al, 2002), augmented with named-
entity lists for locations, food types, restaurant
names, and food subtypes (e.g. pizza), scraped
from the we8there webpages.
We also hypothesize that the rating given for the
distinguished attribute specifies the scalar value
of the relation. For example, a sentence contain-
ing food or meal is assumed to realize the re-
lation ?RESTAURANT has foodquality.?, and the
value of the foodquality attribute is assumed to be
the value specified in the user rating for that at-
tribute, e.g. ?RESTAURANT has foodquality = 5? in
Fig. 1. Similarly, the other relations in Fig. 1 are
assumed to be realized by the utterance ?The best
Spanish food in New York? because it contains
1In future, we will investigate other techniques for boot-
strapping these lexicalizations from the seed word on the
webpage.
266
filter filtered retained
No Relations Filter 7,947 10,519
Other Relations Filter 5,351 5,168
Contextual Filter 2,973 2,195
Unknown Words Filter 1,467 728
Parsing Filter 216 512
Table 2: Filtering statistics: the number of sen-
tences filtered and retained by each filter.
one FOODTYPE named-entity and one LOCATION
named-entity. Values of categorical attributes are
replaced by variables representing their type be-
fore the learned mappings are added to the dictio-
nary, as shown in Fig. 1.
2.3 Parsing and DSyntS conversion
We adopt Deep Syntactic Structures (DSyntSs) as
a format for syntactic structures because they can
be realized by the fast portable realizer RealPro
(Lavoie and Rambow, 1997). Since DSyntSs are a
type of dependency structure, we first process the
sentences with Minipar (Lin, 1998), and then con-
vert Minipar?s representation into DSyntS. Since
user reviews are different from the newspaper ar-
ticles on which Minipar was trained, the output
of Minipar can be inaccurate, leading to failure in
conversion. We check whether conversion is suc-
cessful in the filtering stage.
2.4 Filtering
The goal of filtering is to identify U that realize
the distinguished attributes and to guarantee high
precision for the learned mappings. Recall is less
important since systems need to convey requested
information as accurately as possible. Our proce-
dure for deriving semantic representations is based
on the hypothesis that if U contains named-entities
that realize the distinguished attributes, thatRwill
include the relevant relation in the domain ontol-
ogy. We also assume that if U contains named-
entities that are not covered by the domain ontol-
ogy, or words indicating that the meaning of U de-
pends on the surrounding context, that R will not
completely characterizes the meaning of U , and so
U should be eliminated. We also require an accu-
rate S for U . Therefore, the filters described be-
low eliminate U that (1) realize semantic relations
not in the ontology; (2) contain words indicating
that its meaning depends on the context; (3) con-
tain unknown words; or (4) cannot be parsed ac-
curately.
No Relations Filter: The sentence does not con-
tain any named-entities for the distinguished
attributes.
Other Relations Filter: The sentence contains
named-entities for food subtypes, person
Rating
Dist.Attr.
1 2 3 4 5 Total
food 5 8 6 18 57 94
service 15 3 6 17 56 97
atmosphere 0 3 3 8 31 45
value 0 0 1 8 12 21
overall 3 2 5 15 45 70
Total 23 15 21 64 201 327
Table 3: Domain coverage of single scalar-valued
relation mappings.
names, country names, dates (e.g., today, to-
morrow, Aug. 26th) or prices (e.g., 12 dol-
lars), or POS tag CD for numerals. These in-
dicate relations not in the ontology.
Contextual Filter: The sentence contains index-
icals such as I, you, that or cohesive markers
of rhetorical relations that connect it to some
part of the preceding text, which means that
the sentence cannot be interpreted out of con-
text. These include discourse markers, such
as list item markers with LS as the POS tag,
that signal the organization structure of the
text (Hirschberg and Litman, 1987), as well
as discourse connectives that signal semantic
and pragmatic relations of the sentence with
other parts of the text (Knott, 1996), such as
coordinating conjunctions at the beginning of
the utterance like and and but etc., and con-
junct adverbs such as however, also, then.
Unknown Words Filter: The sentence contains
words not in WordNet (Fellbaum, 1998)
(which includes typographical errors), or
POS tags contain NN (Noun), which may in-
dicate an unknown named-entity, or the sen-
tence has more than a fixed length of words,2
indicating that its meaning may not be esti-
mated solely by named entities.
Parsing Filter: The sentence fails the parsing to
DSyntS conversion. Failures are automati-
cally detected by comparing the original sen-
tence with the one realized by RealPro taking
the converted DSyntS as an input.
We apply the filters, in a cascading manner, to the
18,466 sentences with semantic representations.
As a result, we obtain 512 (2.8%) mappings of
(U ,R,S). After removing 61 duplicates, 451 dis-
tinct (2.4%) mappings remain. Table 2 shows the
number of sentences eliminated by each filter.
3 Objective Evaluation
We evaluate the learned expressions with respect
to domain coverage, linguistic variation and gen-
erativity.
2We used 20 as a threshold.
267
# Combination of Dist. Attrs Count
1 food-service 39
2 food-value 21
3 atmosphere-food 14
4 atmosphere-service 10
5 atmosphere-food-service 7
6 food-foodtype 4
7 atmosphere-food-value 4
8 location-overall 3
9 food-foodtype-value 3
10 food-service-value 2
11 food-foodtype-location 2
12 food-overall 2
13 atmosphere-foodtype 2
14 atmosphere-overall 2
15 service-value 1
16 overall-service 1
17 overall-value 1
18 foodtype-overall 1
19 food-foodtype-location-overall 1
20 atmosphere-food-service-value 1
21 atmosphere-food-overall-
service-value
1
Total 122
Table 4: Counts for multi-relation mappings.
3.1 Domain Coverage
To be usable for a dialogue system, the mappings
must have good domain coverage. Table 3 shows
the distribution of the 327 mappings realizing a
single scalar-valued relation, categorized by the
associated rating score.3 For example, there are 57
mappings with R of ?RESTAURANT has foodqual-
ity=5,? and a large number of mappings for both
the foodquality and servicequality relations. Al-
though we could not obtain mappings for some re-
lations such as price={1,2}, coverage for express-
ing a single relation is fairly complete.
There are also mappings that express several re-
lations. Table 4 shows the counts of mappings
for multi-relation mappings, with those contain-
ing a food or service relation occurring more fre-
quently as in the single scalar-valued relation map-
pings. We found only 21 combinations of rela-
tions, which is surprising given the large poten-
tial number of combinations (There are 50 com-
binations if we treat relations with different scalar
values differently). We also find that most of the
mappings have two or three relations, perhaps sug-
gesting that system utterances should not express
too many relations in a single sentence.
3.2 Linguistic Variation
We also wish to assess whether the linguistic
variation of the learned mappings was greater
than what we could easily have generated with a
hand-crafted dictionary, or a hand-crafted dictio-
nary augmented with aggregation operators, as in
3There are two other single-relation but not scalar-valued
mappings that concern LOCATION in our mappings.
(Walker et al, 2003). Thus, we first categorized
the mappings by the patterns of the DSyntSs. Ta-
ble 5 shows the most common syntactic patterns
(more than 10 occurrences), indicating that 30%
of the learned patterns consist of the simple form
?X is ADJ? where ADJ is an adjective, or ?X is RB
ADJ,? where RB is a degree modifier. Furthermore,
up to 55% of the learned mappings could be gen-
erated from these basic patterns by the application
of a combination operator that coordinates mul-
tiple adjectives, or coordinates predications over
distinct attributes. However, there are 137 syntac-
tic patterns in all, 97 with unique syntactic struc-
tures and 21 with two occurrences, accounting for
45% of the learned mappings. Table 6 shows ex-
amples of learned mappings with distinct syntactic
structures. It would be surprising to see this type
of variety in a hand-crafted generation dictionary.
In addition, the learned mappings contain 275 dis-
tinct lexemes, with a minimum of 2, maximum of
15, and mean of 4.63 lexemes per DSyntS, indi-
cating that the method extracts a wide variety of
expressions of varying lengths.
Another interesting aspect of the learned map-
pings is the wide variety of adjectival phrases
(APs) in the common patterns. Tables 7 and 8
show the APs in single scalar-valued relation map-
pings for food and service categorized by the as-
sociated ratings. Tables for atmosphere, value and
overall can be found in the Appendix. Moreover,
the meanings for some of the learned APs are very
specific to the particular attribute, e.g. cold and
burnt associated with foodquality of 1, attentive
and prompt for servicequality of 5, silly and inat-
tentive for servicequality of 1. and mellow for at-
mosphere of 5. In addition, our method places the
adjectival phrases (APs) in the common patterns
on a more fine-grained scale of 1 to 5, similar to
the strength classifications in (Wilson et al, 2004),
in contrast to other automatic methods that clas-
sify expressions into a binary positive or negative
polarity (e.g. (Turney, 2002)).
3.3 Generativity
Our motivation for deriving syntactic representa-
tions for the learned expressions was the possibil-
ity of using an off-the-shelf sentence planner to
derive new combinations of relations, and apply
aggregation and other syntactic transformations.
We examined how many of the learned DSyntSs
can be combined with each other, by taking ev-
ery pair of DSyntSs in the mappings and apply-
ing the built-in merge operation in the SPaRKy
generator (Walker et al, 2003). We found that
only 306 combinations out of a potential 81,318
268
# syntactic pattern example utterance count ratio accum.
1 NN VB JJ The atmosphere is wonderful. 92 20.4% 20.4%
2 NN VB RB JJ The atmosphere was very nice. 52 11.5% 31.9%
3 JJ NN Bad service. 36 8.0% 39.9%
4 NN VB JJ CC JJ The food was flavorful but cold. 25 5.5% 45.5%
5 RB JJ NN Very trendy ambience. 22 4.9% 50.3%
6 NN VB JJ CC NN VB JJ The food is excellent and the atmosphere is great. 13 2.9% 53.2%
7 NN CC NN VB JJ The food and service were fantastic. 10 2.2% 55.4%
Table 5: Common syntactic patterns of DSyntSs, flattened to a POS sequence for readability. NN, VB,
JJ, RB, CC stand for noun, verb, adjective, adverb, and conjunction, respectively.
[overall=1, value=2] Very disappointing experience for
the money charged.
[food=5, value=5] The food is excellent and plentiful at a
reasonable price.
[food=5, service=5] The food is exquisite as well as the
service and setting.
[food=5, service=5] The food was spectacular and so was
the service.
[food=5, foodtype, value=5] Best FOODTYPE food with
a great value for money.
[food=5, foodtype, value=5] An absolutely outstanding
value with fantastic FOODTYPE food.
[food=5, foodtype, location, overall=5] This is the best
place to eat FOODTYPE food in LOCATION.
[food=5, foodtype] Simply amazing FOODTYPE food.
[food=5, foodtype] RESTAURANTNAME is the best of the
best for FOODTYPE food.
[food=5] The food is to die for.
[food=5] What incredible food.
[food=4] Very pleasantly surprised by the food.
[food=1] The food has gone downhill.
[atmosphere=5, overall=5] This is a quiet little place
with great atmosphere.
[atmosphere=5, food=5, overall=5, service=5, value=5]
The food, service and ambience of the place are all fabu-
lous and the prices are downright cheap.
Table 6: Acquired generation patterns (with short-
hand for relations in square brackets) whose syn-
tactic patterns occurred only once.
combinations (0.37%) were successful. This is
because the merge operation in SPaRKy requires
that the subjects and the verbs of the two DSyntSs
are identical, e.g. the subject is RESTAURANT and
verb is has, whereas the learned DSyntSs often
place the attribute in subject position as a definite
noun phrase. However, the learned DSyntS can
be incorporated into SPaRKy using the semantic
representations to substitute learned DSyntSs into
nodes in the sentence plan tree. Figure 2 shows
some example utterances generated by SPaRKy
with its original dictionary and example utterances
when the learned mappings are incorporated. The
resulting utterances seem more natural and collo-
quial; we examine whether this is true in the next
section.
4 Subjective Evaluation
We evaluate the obtained mappings in two re-
spects: the consistency between the automatically
derived semantic representation and the realiza-
food=1 awful, bad, burnt, cold, very ordinary
food=2 acceptable, bad, flavored, not enough, very
bland, very good
food=3 adequate, bland and mediocre, flavorful but
cold, pretty good, rather bland, very good
food=4 absolutely wonderful, awesome, decent, ex-
cellent, good, good and generous, great, out-
standing, rather good, really good, tradi-
tional, very fresh and tasty, very good, very
very good
food=5 absolutely delicious, absolutely fantastic, ab-
solutely great, absolutely terrific, ample, well
seasoned and hot, awesome, best, delectable
and plentiful, delicious, delicious but simple,
excellent, exquisite, fabulous, fancy but tasty,
fantastic, fresh, good, great, hot, incredible,
just fantastic, large and satisfying, outstand-
ing, plentiful and outstanding, plentiful and
tasty, quick and hot, simply great, so deli-
cious, so very tasty, superb, terrific, tremen-
dous, very good, wonderful
Table 7: Adjectival phrases (APs) in single scalar-
valued relation mappings for foodquality.
tion, and the naturalness of the realization.
For comparison, we used a baseline of hand-
crafted mappings from (Walker et al, 2003) ex-
cept that we changed the word decor to at-
mosphere and added five mappings for overall.
For scalar relations, this consists of the realiza-
tion ?RESTAURANT has ADJ LEX? where ADJ is
mediocre, decent, good, very good, or excellent for
rating values 1-5, and LEX is food quality, service,
atmosphere, value, or overall depending on the re-
lation. RESTAURANT is filled with the name of
a restaurant at runtime. For example, ?RESTAU-
RANT has foodquality=1? is realized as ?RESTAU-
RANT has mediocre food quality.? The location
and food type relations are mapped to ?RESTAU-
RANT is located in LOCATION? and ?RESTAU-
RANT is a FOODTYPE restaurant.?
The learned mappings include 23 distinct se-
mantic representations for a single-relation (22 for
scalar-valued relations and one for location) and
50 for multi-relations. Therefore, using the hand-
crafted mappings, we first created 23 utterances
for the single-relations. We then created three ut-
terances for each of 50multi-relations using differ-
ent clause-combining operations from (Walker et
al., 2003). This gave a total of 173 baseline utter-
ances, which together with 451 learned mappings,
269
service=1 awful, bad, great, horrendous, horrible,
inattentive, forgetful and slow, marginal,
really slow, silly and inattentive, still
marginal, terrible, young
service=2 overly slow, very slow and inattentive
service=3 bad, bland and mediocre, friendly and
knowledgeable, good, pleasant, prompt,
very friendly
service=4 all very warm and welcoming, attentive,
extremely friendly and good, extremely
pleasant, fantastic, friendly, friendly and
helpful, good, great, great and courteous,
prompt and friendly, really friendly, so
nice, swift and friendly, very friendly, very
friendly and accommodating
service=5 all courteous, excellent, excellent and
friendly, extremely friendly, fabulous,
fantastic, friendly, friendly and helpful,
friendly and very attentive, good, great,
great, prompt and courteous, happy and
friendly, impeccable, intrusive, legendary,
outstanding, pleasant, polite, attentive and
prompt, prompt and courteous, prompt
and pleasant, quick and cheerful, stupen-
dous, superb, the most attentive, unbeliev-
able, very attentive, very congenial, very
courteous, very friendly, very friendly and
helpful, very friendly and pleasant, very
friendly and totally personal, very friendly
and welcoming, very good, very helpful,
very timely, warm and friendly, wonderful
Table 8: Adjectival phrases (APs) in single scalar-
valued relation mappings for servicequality.
yielded 624 utterances for evaluation.
Ten subjects, all native English speakers, eval-
uated the mappings by reading them from a web-
page. For each system utterance, the subjects were
asked to express their degree of agreement, on a
scale of 1 (lowest) to 5 (highest), with the state-
ment (a) The meaning of the utterance is consis-
tent with the ratings expressing their semantics,
and with the statement (b) The style of the utter-
ance is very natural and colloquial. They were
asked not to correct their decisions and also to rate
each utterance on its own merit.
4.1 Results
Table 9 shows the means and standard deviations
of the scores for baseline vs. learned utterances for
consistency and naturalness. A t-test shows that
the consistency of the learned expression is signifi-
cantly lower than the baseline (df=4712, p < .001)
but that their naturalness is significantly higher
than the baseline (df=3107, p < .001). However,
consistency is still high. Only 14 of the learned
utterances (shown in Tab. 10) have a mean consis-
tency score lower than 3, which indicates that, by
and large, the human judges felt that the inferred
semantic representations were consistent with the
meaning of the learned expressions. The correla-
tion coefficient between consistency and natural-
ness scores is 0.42, which indicates that consis-
Original SPaRKy utterances
? Babbo has the best overall quality among the selected
restaurants with excellent decor, excellent service and
superb food quality.
? Babbo has excellent decor and superb food quality
with excellent service. It has the best overall quality
among the selected restaurants.
?
Combination of SPaRKy and learned DSyntS
? Because the food is excellent, the wait staff is pro-
fessional and the decor is beautiful and very com-
fortable, Babbo has the best overall quality among the
selected restaurants.
? Babbo has the best overall quality among the selected
restaurants because atmosphere is exceptionally nice,
food is excellent and the service is superb.
? Babbo has superb food quality, the service is excep-
tional and the atmosphere is very creative. It has the
best overall quality among the selected restaurants.
Figure 2: Utterances incorporating learned
DSyntSs (Bold font) in SPaRKy.
baseline learned stat.
mean sd. mean sd. sig.
Consistency 4.714 0.588 4.459 0.890 +
Naturalness 4.227 0.852 4.613 0.844 +
Table 9: Consistency and naturalness scores aver-
aged over 10 subjects.
tency does not greatly relate to naturalness.
We also performed an ANOVA (ANalysis Of
VAriance) of the effect of each relation in R on
naturalness and consistency. There were no sig-
nificant effects except that mappings combining
food, service, and atmosphere were significantly
worse (df=1, F=7.79, p=0.005). However, there
is a trend for mappings to be rated higher for
the food attribute (df=1, F=3.14, p=0.08) and the
value attribute (df=1, F=3.55, p=0.06) for consis-
tency, suggesting that perhaps it is easier to learn
some mappings than others.
5 Related Work
Automatically finding sentences with the same
meaning has been extensively studied in the field
of automatic paraphrasing using parallel corpora
and corpora with multiple descriptions of the same
events (Barzilay and McKeown, 2001; Barzilay
and Lee, 2003). Other work finds predicates of
similar meanings by using the similarity of con-
texts around the predicates (Lin and Pantel, 2001).
However, these studies find a set of sentences with
the same meaning, but do not associate a specific
meaning with the sentences. One exception is
(Barzilay and Lee, 2002), which derives mappings
between semantic representations and realizations
using a parallel (but unaligned) corpus consisting
of both complex semantic input and correspond-
ing natural language verbalizations for mathemat-
270
shorthand for relations and utterance score
[food=4] The food is delicious and beautifully
prepared.
2.9
[overall=4] A wonderful experience. 2.9
[service=3] The service is bland and mediocre. 2.8
[atmosphere=2] The atmosphere here is eclec-
tic.
2.6
[overall=3] Really fancy place. 2.6
[food=3, service=4] Wonderful service and
great food.
2.5
[service=4] The service is fantastic. 2.5
[overall=2] The RESTAURANTNAME is once a
great place to go and socialize.
2.2
[atmosphere=2] The atmosphere is unique and
pleasant.
2.0
[food=5, foodtype] FOODTYPE and FOODTYPE
food.
1.8
[service=3] Waitstaff is friendly and knowl-
edgeable.
1.7
[atmosphere=5, food=5, service=5] The atmo-
sphere, food and service.
1.6
[overall=3] Overall, a great experience. 1.4
[service=1] The waiter is great. 1.4
Table 10: The 14 utterances with consistency
scores below 3.
ical proofs. However, our technique does not re-
quire parallel corpora or previously existing se-
mantic transcripts or labeling, and user reviews are
widely available in many different domains (See
http://www.epinions.com/).
There is also significant previous work on min-
ing user reviews. For example, Hu and Liu (2005)
use reviews to find adjectives to describe products,
and Popescu and Etzioni (2005) automatically find
features of a product together with the polarity of
adjectives used to describe them. They both aim at
summarizing reviews so that users can make deci-
sions easily. Our method is also capable of finding
polarities of modifying expressions including ad-
jectives, but on a more fine-grained scale of 1 to
5. However, it might be possible to use their ap-
proach to create rating information for raw review
texts as in (Pang and Lee, 2005), so that we can
create mappings from reviews without ratings.
6 Summary and Future Work
We proposed automatically obtaining mappings
between semantic representations and realizations
from reviews with individual ratings. The results
show that: (1) the learned mappings provide good
coverage of the domain ontology and exhibit good
linguistic variation; (2) the consistency between
the semantic representations and realizations is
high; and (3) the naturalness of the realizations are
significantly higher than the baseline.
There are also limitations in our method. Even
though consistency is rated highly by human sub-
jects, this may actually be a judgement of whether
the polarity of the learned mapping is correctly
placed on the 1 to 5 rating scale. Thus, alter-
nate ways of expressing, for example foodqual-
ity=5, shown in Table 7, cannot be guaranteed to
be synonymous, which may be required for use in
spoken language generation. Rather, an examina-
tion of the adjectival phrases in Table 7 shows that
different aspects of the food are discussed. For
example ample and plentiful refer to the portion
size, fancy may refer to the presentation, and deli-
cious describes the flavors. This suggests that per-
haps the ontology would benefit from represent-
ing these sub-attributes of the food attribute, and
sub-attributes in general. Another problem with
consistency is that the same AP, e.g. very good
in Table 7 may appear with multiple ratings. For
example, very good is used for every foodquality
rating from 2 to 5. Thus some further automatic
or by-hand analysis is required to refine what is
learned before actual use in spoken language gen-
eration. Still, our method could reduce the amount
of time a system designer spends developing the
spoken language generator, and increase the natu-
ralness of spoken language generation.
Another issue is that the recall appears to be
quite low given that all of the sentences concern
the same domain: only 2.4% of the sentences
could be used to create the mappings. One way
to increase recall might be to automatically aug-
ment the list of distinguished attribute lexicaliza-
tions, using WordNet or work on automatic iden-
tification of synonyms, such as (Lin and Pantel,
2001). However, the method here has high pre-
cision, and automatic techniques may introduce
noise. A related issue is that the filters are in some
cases too strict. For example the contextual fil-
ter is based on POS-tags, so that sentences that do
not require the prior context for their interpreta-
tion are eliminated, such as sentences containing
subordinating conjunctions like because, when, if,
whose arguments are both given in the same sen-
tence (Prasad et al, 2005). In addition, recall is
affected by the domain ontology, and the automat-
ically constructed domain ontology from the re-
view webpages may not cover all of the domain.
In some review domains, the attributes that get
individual ratings are a limited subset of the do-
main ontology. Techniques for automatic feature
identification (Hu and Liu, 2005; Popescu and Et-
zioni, 2005) could possibly help here, although
these techniques currently have the limitation that
they do not automatically identify different lexi-
calizations of the same feature.
A different type of limitation is that dialogue
systems need to generate utterances for informa-
tion gathering whereas the mappings we obtained
271
can only be used for information presentation.
Thus these would have to be constructed by hand,
as in current practice, or perhaps other types of
corpora or resources could be utilized. In addi-
tion, the utility of syntactic structures in the map-
pings should be further examined, especially given
the failures in DSyntS conversion. An alternative
would be to leave some sentences unparsed and
use them as templates with hybrid generation tech-
niques (White and Caldwell, 1998). Finally, while
we believe that this technique will apply across do-
mains, it would be useful to test it on domains such
as movie reviews or product reviews, which have
more complex domain ontologies.
Acknowledgments
We thank the anonymous reviewers for their help-
ful comments. This work was supported by a
Royal Society Wolfson award to Marilyn Walker
and a research collaboration grant from NTT to
the Cognitive Systems Group at the University of
Sheffield.
References
Regina Barzilay and Lillian Lee. 2002. Bootstrapping lex-
ical choice via multiple-sequence alignment. In Proc.
EMNLP, pages 164?171.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proc. HLT/NAACL, pages 16?23.
Regina Barzilay and Kathleen McKeown. 2001. Extracting
paraphrases from a parallel corpus. In Proc. 39th ACL,
pages 50?57.
Hamish Cunningham, Diana Maynard, Kalina Bontcheva,
and Valentin Tablan. 2002. GATE: A framework and
graphical development environment for robust NLP tools
and applications. In Proc. 40th ACL.
Christiane Fellbaum. 1998. WordNet: An Electronic Lexical
Database (Language, Speech, and Communication). The
MIT Press.
Julia Hirschberg and Diane. J. Litman. 1987. Now let?s talk
about NOW: Identifying cue phrases intonationally. In
Proc. 25th ACL, pages 163?171.
Minqing Hu and Bing Liu. 2005. Mining and summarizing
customer reviews. In Proc. KDD, pages 168?177.
Alistair Knott. 1996. A Data-Driven Methodology for Moti-
vating a Set of Coherence Relations. Ph.D. thesis, Univer-
sity of Edinburgh, Edinburgh.
Benoit Lavoie and Owen Rambow. 1997. A fast and portable
realizer for text generation systems. In Proc. 5th Applied
NLP, pages 265?268.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language En-
gineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of MINI-
PAR. In Workshop on the Evaluation of Parsing Systems.
Johanna D. Moore, Mary Ellen Foster, Oliver Lemon, and
Michael White. 2004. Generating tailored, comparative
descriptions in spoken dialogue. In Proc. 7th FLAIR.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with re-
spect to rating scales. In Proc. 43st ACL, pages 115?124.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Proc.
HLT/EMNLP, pages 339?346.
Rashmi Prasad, Aravind Joshi, Nikhil Dinesh, Alan Lee,
Eleni Miltsakaki, and Bonnie Webber. 2005. The Penn
Discourse TreeBank as a resource for natural language
generation. In Proc. Corpus Linguistics Workshop on Us-
ing Corpora for NLG.
Stephanie Seneff and Joseph Polifroni. 2000. Formal and
natural language generation in the mercury conversational
system. In Proc. ICSLP, volume 2, pages 767?770.
Marie?t Theune. 2003. From monologue to dialogue: natural
language generation in OVIS. In AAAI 2003 Spring Sym-
posium on Natural Language Generation in Written and
Spoken Dialogue, pages 141?150.
Peter D. Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classification
of reviews. In Proc. 40th ACL, pages 417?424.
Marilyn Walker, Rashmi Prasad, and Amanda Stent. 2003.
A trainable generator for recommendations in multimodal
dialog. In Proc. Eurospeech, pages 1697?1700.
Michael White and Ted Caldwell. 1998. EXEMPLARS: A
practical, extensible framework for dynamic text genera-
tion. In Proc. INLG, pages 266?275.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004.
Just how mad are you? finding strong and weak opinion
clauses. In Proc. AAAI, pages 761?769.
Appendix
Adjectival phrases (APs) in single scalar-valued
relation mappings for atmosphere, value, and
overall.
atmosphere=2 eclectic, unique and pleasant
atmosphere=3 busy, pleasant but extremely hot
atmosphere=4 fantastic, great, quite nice and simple,
typical, very casual, very trendy, wonder-
ful
atmosphere=5 beautiful, comfortable, excellent, great,
interior, lovely, mellow, nice, nice and
comfortable, phenomenal, pleasant, quite
pleasant, unbelievably beautiful, very
comfortable, very cozy, very friendly,
very intimate, very nice, very nice and
relaxing, very pleasant, very relaxing,
warm and contemporary, warm and very
comfortable, wonderful
value=3 very reasonable
value=4 great, pretty good, reasonable, very good
value=5 best, extremely reasonable, good, great,
reasonable, totally reasonable, very good,
very reasonable
overall=1 just bad, nice, thoroughly humiliating
overall=2 great, really bad
overall=3 bad, decent, great, interesting, really
fancy
overall=4 excellent, good, great, just great, never
busy, not very busy, outstanding, recom-
mended, wonderful
overall=5 amazing, awesome, capacious, delight-
ful, extremely pleasant, fantastic, good,
great, local, marvelous, neat, new, over-
all, overwhelmingly pleasant, pampering,
peaceful but idyllic, really cool, really
great, really neat, really nice, special,
tasty, truly great, ultimate, unique and en-
joyable, very enjoyable, very excellent,
very good, very nice, very wonderful,
warm and friendly, wonderful
272
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 496?503,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
PERSONAGE: Personality Generation for Dialogue
Franc?ois Mairesse
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, United Kingdom
F.Mairesse@sheffield.ac.uk
Marilyn Walker
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, United Kingdom
M.A.Walker@sheffield.ac.uk
Abstract
Over the last fifty years, the ?Big Five?
model of personality traits has become a
standard in psychology, and research has
systematically documented correlations be-
tween a wide range of linguistic variables
and the Big Five traits. A distinct line of
research has explored methods for automati-
cally generating language that varies along
personality dimensions. We present PER-
SONAGE (PERSONAlity GEnerator), the
first highly parametrizable language gener-
ator for extraversion, an important aspect
of personality. We evaluate two personal-
ity generation methods: (1) direct genera-
tion with particular parameter settings sug-
gested by the psychology literature; and (2)
overgeneration and selection using statistical
models trained from judge?s ratings. Results
show that both methods reliably generate ut-
terances that vary along the extraversion di-
mension, according to human judges.
1 Introduction
Over the last fifty years, the ?Big Five? model of per-
sonality traits has become a standard in psychology
(extraversion, neuroticism, agreeableness, conscien-
tiousness, and openness to experience), and research
has systematically documented correlations between
a wide range of linguistic variables and the Big Five
traits (Mehl et al, 2006; Norman, 1963; Oberlan-
der and Gill, 2006; Pennebaker and King, 1999). A
distinct line of research has explored methods for
automatically generating language that varies along
personality dimensions, targeting applications such
as computer gaming and educational virtual worlds
(Andre? et al, 2000; Isard et al, 2006; Loyall and
Bates, 1997; Piwek, 2003; Walker et al, 1997) inter
alia. Other work suggests a clear utility for gener-
ating language manifesting personality (Reeves and
Nass, 1996). However, to date, (1) research in gener-
ation has not systematically exploited the psycholin-
guistic findings; and (2) there has been little evalua-
tion showing that automatic generators can produce
language with recognizable personality variation.
Alt Realization Extra
5 Err... it seems to me that Le Marais isn?t as bad
as the others.
1.83
4 Right, I mean, Le Marais is the only restaurant
that is any good.
2.83
8 Ok, I mean, Le Marais is a quite french, kosher
and steak house place, you know and the atmo-
sphere isn?t nasty, it has nice atmosphere. It has
friendly service. It seems to me that the service
is nice. It isn?t as bad as the others, is it?
5.17
9 Well, it seems to me that I am sure you would
like Le Marais. It has good food, the food is
sort of rather tasty, the ambience is nice, the at-
mosphere isn?t sort of nasty, it features rather
friendly servers and its price is around 44 dol-
lars.
5.83
3 I am sure you would like Le Marais, you know.
The atmosphere is acceptable, the servers are
nice and it?s a french, kosher and steak house
place. Actually, the food is good, even if its price
is 44 dollars.
6.00
10 It seems to me that Le Marais isn?t as bad as
the others. It?s a french, kosher and steak house
place. It has friendly servers, you know but it?s
somewhat expensive, you know!
6.17
2 Basically, actually, I am sure you would like Le
Marais. It features friendly service and accept-
able atmosphere and it?s a french, kosher and
steak house place. Even if its price is 44 dollars,
it just has really good food, nice food.
6.17
Table 1: Recommendations along the extraver-
sion dimension, with the average extraversion rating
from human judges on a scale from 1 to 7. Alt-2 and
3 are from the extravert set, Alt-4 and 5 are from the
introvert set, and others were randomly generated.
Our aim is to produce a highly parameterizable
generator whose outputs vary along personality di-
mensions. We hypothesize that such language can
496
be generated by varying parameters suggested by
psycholinguistic research. So, we must first map
the psychological findings to parameters of a natural
language generator (NLG). However, this presents
several challenges: (1) The findings result from
studies of genres of language, such as stream-of-
consciousness essays (Pennebaker and King, 1999),
and informal conversations (Mehl et al, 2006), and
thus may not apply to fixed content domains used in
NLG; (2) Most findings are based on self-reports of
personality, but we want to affect observer?s percep-
tions; (3) The findings consist of weak but signifi-
cant correlations, so that individual parameters may
not have a strong enough effect to produce recog-
nizable variation within a single utterance; (4) There
are many possible mappings of the findings to gen-
eration parameters; and (5) It is unclear whether
only specific speech-act types manifest personality
or whether all utterances do.
Thus this paper makes several contributions.
First, Section 2 summarizes the linguistic reflexes of
extraversion, organized by the modules in a standard
NLG system, and propose a mapping from these
findings to NLG parameters. To our knowledge this
is the first attempt to put forward a systematic frame-
work for generating language manifesting personal-
ity. We start with the extraversion dimension be-
cause it is an important personality factor, with many
associated linguistic variables. We believe that our
framework will generalize to the other dimensions
in the Big Five model. Second, Sections 3 and 4
describe the PERSONAGE (PERSONAlity GEner-
ator) generator and its 29 parameters. Table 1 shows
examples generated by PERSONAGE for recom-
mendations in the restaurant domain, along with
human extraversion judgments. Third, Sections 5
and 6 describe experiments evaluating two genera-
tion methods. We first show that (1) the parame-
ters generate utterances that vary significantly on the
extraversion dimension, according to human judg-
ments; and (2) we can train a statistical model that
matches human performance in assigning extraver-
sion ratings to generation outputs produced with ran-
dom parameter settings. Section 7 sums up and dis-
cusses future work.
2 Psycholinguistic Findings and
PERSONAGE Parameters
We hypothesize that personality can be made man-
ifest in evaluative speech acts in any dialogue do-
main, i.e. utterances responding to requests to REC-
OMMEND or COMPARE domain entities, such as
restaurants or movies (Isard et al, 2006; Stent et al,
2004). Thus, we start with the SPaRKy genera-
tor1, which produces evaluative recommendations
and comparisons in the restaurant domain, for a
database of restaurants in New York City. There
are eight attributes for each restaurant: the name and
address, scalar attributes for price, food quality, at-
mosphere, and service and categorical attributes for
neighborhood and type of cuisine. SPaRKy is based
on the standard NLG architecture (Reiter and Dale,
2000), and consists of the following modules:
1. Content Planning: refine communicative goals, select and
structure content;
2. Sentence planning; choose linguistic resources (lexicon,
syntax) to achieve goals;
3. Realization: use grammar (syntax, morphology) to gen-
erate surface utterances.
Given the NLG architecture, speech-act types,
and domain, the first step then is to summarise psy-
chological findings on extraversion and map them
to this architecture. The column NLG modules of
Table 2 gives the proposed mapping. The first row
specifies findings for the content planning module
and the other rows are aspects of sentence planning.
Realization is achieved with the RealPro surface re-
alizer (Lavoie and Rambow, 1997). An examina-
tion of the introvert and extravert findings in Table 2
highlights the challenges above, i.e. exploiting these
findings in a systematic way within a parameteriz-
able NLG system.
The column Parameter in Table 2 proposes pa-
rameters (explained in Sections 3 and 4) that are ma-
nipulated within each module to realize the findings
in the other columns. Each parameter varies con-
tinuously from 0 to 1, where end points are meant
to produce extreme but plausible output. Given the
challenges above, it is important to note that these
parameters represent hypotheses about how a find-
ing can be mapped into any NLG system. The Intro
and Extra columns at the right hand side of the Pa-
rameter column indicate a range of settings for this
parameter, suggested by the psychological findings,
to produce introverted vs. extraverted language.
SPaRKy produces content plans for restaurant
recommendations and comparisons that are modi-
fied by the parameters. The sample content plan
for a recommendation in Figure 1 corresponds to
the outputs in Table 1. While Table 1 shows that
PERSONAGE?s parameters have various pragmatic
effects, they preserve the meaning at the Gricean in-
tention level (dialogue goal). Each content plan con-
tains a claim (nucleus) about the overall quality of
1Available for download from
www.dcs.shef.ac.uk/cogsys/sparky.html
497
NLG modules Introvert findings Extravert findings Parameter Intro Extra
Content Single topic Many topics VERBOSITY low high
selection Strict selection Think out loud* RESTATEMENTS low high
and REPETITIONS low low
structure Problem talk, Pleasure talk, agreement, CONTENT POLARITY low high
dissatisfaction compliment REPETITIONS POLARITY low high
CLAIM POLARITY low high
CONCESSIONS avg avg
CONCESSIONS POLARITY low high
POLARISATION low high
POSITIVE CONTENT FIRST low high
Syntactic Few self-references Many self-references SELF-REFERENCES low high
templates Elaborated constructions Simple constructions* CLAIM COMPLEXITY high low
selection Many articles Few articles
Aggregation Many words per Few words per RELATIVE CLAUSES high low
Operations sentence/clause sentence/clause WITH CUE WORD high low
CONJUNCTION low high
Many unfilled pauses Few unfilled pauses PERIOD high low
...
Pragmatic
transformations
Many nouns, adjectives, prepo-
sitions (explicit)
Many verbs, adverbs, pronouns
(implicit)
SUBJECT IMPLICITNESS low high
Many negations Few negations NEGATION INSERTION high low
Many tentative words Few tentative words DOWNTONER HEDGES:
?SORT OF, SOMEWHAT, QUITE, RATHER,
ERR, I THINK THAT, IT SEEMS THAT, IT
SEEMS TO ME THAT, I MEAN
high low
?AROUND avg avg
Formal Informal ?KIND OF, LIKE low high
ACKNOWLEDGMENTS:
?YEAH low high
?RIGHT, OK, I SEE, WELL high low
Realism Exaggeration* EMPHASIZER HEDGES:
?REALLY, BASICALLY, ACTUALLY, JUST
HAVE, JUST IS, EXCLAMATION low high
?YOU KNOW low high
No politeness form Positive face redressment* TAG QUESTION INSERTION low high
Lower word count Higher word count HEDGE VARIATION low avg
HEDGE REPETITION low low
Lexical Rich Poor LEXICON FREQUENCY low high
choice Few positive emotion words Many positive emotion words see polarity parameters
Many negative emotion words Few negative emotion words see polarity parameters
Table 2: Summary of language cues for extraversion, based on Dewaele and Furnham (1999); Furnham
(1990); Mehl et al (2006); Oberlander and Gill (2006); Pennebaker and King (1999), as well as PERSON-
AGE?s corresponding generation parameters. Asterisks indicate hypotheses, rather than results. For details
on aggregation parameters, see Section 4.2.
Relations: JUSTIFY (nuc:1, sat:2); JUSTIFY (nuc:1, sat:3);
JUSTIFY (nuc:1, sat:4); JUSTIFY (nuc:1, sat:5);
JUSTIFY (nuc:1, sat:6)
Content: 1. assert(best (Le Marais))
2. assert(is (Le Marais, cuisine (French)))
3. assert(has (Le Marais, food-quality (good)))
4. assert(has (Le Marais, service (good)))
5. assert(has (Le Marais, decor (decent)))
6. assert(is (Le Marais, price (44 dollars)))
Figure 1: A content plan for a recommendation.
the selected restaurant(s), supported by a set of satel-
lite content items describing their attributes. See Ta-
ble 1. Claims can be expressed in different ways,
such as RESTAURANT NAME is the best, while
the attribute satellites follow the pattern RESTAU-
RANT NAME has MODIFIER ATTRIBUTE NAME,
as in Le Marais has good food. Recommendations
are characterized by a JUSTIFY rhetorical relation
associating the claim with all other content items,
which are linked together through an INFER relation.
In comparisons, the attributes of multiple restaurants
are compared using a CONTRAST relation. An op-
tional claim about the quality of all restaurants can
also be expressed as the nucleus of an ELABORATE
relation, with the rest of the content plan tree as a
satellite.
3 Content Planning
Content planning selects and structures the content
to be communicated. Table 2 specifies 10 param-
eters hypothesized to affect this process which are
explained below.
Content size: Extraverts are more talkative than
introverts (Furnham, 1990; Pennebaker and King,
1999), although it is not clear whether they actu-
ally produce more content, or are just redundant and
wordy. Thus various parameters relate to the amount
and type of content produced. The VERBOSITY pa-
rameter controls the number of content items se-
lected from the content plan. For example, Alt-5 in
Table 1 is terse, while Alt-2 expresses all the items in
the content plan. The REPETITION parameter adds
an exact repetition: the content item is duplicated
and linked to the original content by a RESTATE
498
rhetorical relation. In a similar way, the RESTATE-
MENT parameter adds paraphrases of content items
to the plan, that are obtained from the initial hand-
crafted generation dictionary (see Section 4.1) and
by automatically substituting content words with the
most frequent WordNet synonym (see Section 4.4).
Alt-9 in Table 1 contains restatements for the food
quality and the atmosphere attributes.
Polarity: Extraverts tend to be more positive; in-
troverts are characterized as engaging in more ?prob-
lem talk? and expressions of dissatisfaction (Thorne,
1987). To control for polarity, content items are
defined as positive or negative based on the scalar
value of the corresponding attribute. The type of cui-
sine and neighborhood attributes have neutral polar-
ity. There are multiple parameters associated with
polarity. The CONTENT POLARITY parameter con-
trols whether the content is mostly negative (e.g.
X has mediocre food), neutral (e.g. X is a Thai
restaurant), or positive. From the filtered set of
content items, the POLARISATION parameter deter-
mines whether the final content includes items with
extreme scalar values (e.g. X has fantastic staff).
In addition, polarity can also be implied more sub-
tly through rhetorical structure. The CONCESSIONS
parameter controls how negative and positive infor-
mation is presented, i.e. whether two content items
with different polarity are presented objectively, or if
one is foregrounded and the other backgrounded. If
two opposed content items are selected for a con-
cession, a CONCESS rhetorical relation is inserted
between them. While the CONCESSIONS param-
eter captures the tendency to put information into
perspective, the CONCESSION POLARITY parameter
controls whether the positive or the negative content
is concessed, i.e. marked as the satellite of the CON-
CESS relation. The last sentence of Alt-3 in Table 1
illustrates a positive concession, in which the good
food quality is put before the high price.
Content ordering: Although extraverts use more
positive language (Pennebaker and King, 1999;
Thorne, 1987), it is unclear how they position the
positive content within their utterances. Addition-
ally, the position of the claim affects the persuasive-
ness of an argument (Carenini and Moore, 2000):
starting with the claim facilitates the hearer?s under-
standing, while finishing with the claim is more ef-
fective if the hearer disagrees. The POSITIVE CON-
TENT FIRST parameter therefore controls whether
positive content items ? including the claim ? appear
first or last, and the order in which the content items
are aggregated. However, some operations can still
impose a specific ordering (e.g. BECAUSE cue word
to realize the JUSTIFY relation, see Section 4.2).
4 Sentence Planning
Sentence planning chooses the linguistic resources
from the lexicon and the syntactic and discourse
structures to achieve the communicative goals spec-
ified in the input content plan. Table 2 specifies four
sets of findings and parameters for different aspects
of sentence planning discussed below.
4.1 Syntactic template selection
PERSONAGE?s input generation dictionary is made
of 27 Deep Syntactic Structures (DSyntS): 9 for
the recommendation claim, 12 for the comparison
claim, and one per attribute. Selecting a DSyntS re-
quires assigning it automatically to a point in a three
dimensional space described below. All parameter
values are normalized over all the DSyntS, so the
DSyntS closest to the target value can be computed.
Syntactic complexity: Furnham (1990) suggests
that introverts produce more complex constructions:
the CLAIM COMPLEXITY parameter controls the
depth of the syntactic structure chosen to represent
the claim, e.g. the claim X is the best is rated as less
complex than X is one of my favorite restaurants.
Self-references: Extraverts make more self-
references than introverts (Pennebaker and King,
1999). The SELF-REFERENCE parameter controls
whether the claim is made in the first person, based
on the speaker?s own experience, or whether the
claim is reported as objective or information ob-
tained elsewhere. The self-reference value is ob-
tained from the syntactic structure by counting the
number of first person pronouns. For example, the
claim of Alt-2 in Table 1, i.e. I am sure you would
like Le Marais, will be rated higher than Le Marais
isn?t as bad as the others in Alt-5.
Polarity: While polarity can be expressed by con-
tent selection and structure, it can also be directly
associated with the DSyntS. The CLAIM POLARITY
parameter determines the DSyntS selected to realize
the claim. DSyntS are manually annotated for po-
larity. For example, Alt-4?s claim in Table 1, i.e. Le
Marais is the only restaurant that is any good, has a
lower polarity than Alt-2.
4.2 Aggregation operations
SPaRKy aggregation operations are used (See Stent
et al (2004)), with additional operations for conces-
sions and restatements. See Table 2. The probabil-
ity of the operations biases the production of com-
plex clauses, periods and formal cue words for in-
troverts, to express their preference for complex syn-
499
tactic constructions, long pauses and rich vocabulary
(Furnham, 1990). Thus, the introvert parameters fa-
vor operations such as RELATIVE CLAUSE for the
INFER relation, PERIOD HOWEVER CUE WORD for
CONTRAST, and ALTHOUGH ADVERBIAL CLAUSE
for CONCESS, that we hypothesize to result in more
formal language. Extravert aggregation produces
longer sentences with simpler constructions and in-
formal cue words. Thus extravert utterances tend to
use operations such as a CONJUNCTION to realize
the INFER and RESTATE relations, and the EVEN IF
ADVERBIAL CLAUSE for CONCESS relations.
4.3 Pragmatic transformations
This section describes the insertion of markers in the
DSyntS to produce various pragmatic effects.
Hedges: Hedges correlate with introversion (Pen-
nebaker and King, 1999) and affect politeness
(Brown and Levinson, 1987). Thus there are param-
eters for inserting a wide range of hedges, both af-
fective and epistemic, such as kind of, sort of, quite,
rather, somewhat, like, around, err, I think that, it
seems that, it seems to me that, and I mean. Alt-5 in
Table 1 shows hedges err and it seems to me that.
To model extraverts use of more social language,
agreement and backchannel behavior (Dewaele and
Furnham, 1999; Pennebaker and King, 1999), we
use informal acknowledgments such as yeah, right,
ok. Acknowledgments that may affect introversion
are I see, expressing self-reference and cognitive
load, and the well cue word implying reservation
from the speaker (see Alt-9).
To model social connection and emotion we
added mechanisms for inserting emphasizers such as
you know, basically, actually, just have, just is, and
exclamations. Alt-3 in Table 1 shows the insertion
of you know and actually.
Although similar hedges can be grouped together,
each hedge has a unique pragmatic effect. For ex-
ample, you know implies positive-face redressment,
while actually doesn?t. A parameter for each hedge
controls the likelihood of its selection.
To control the general level of hedging, a HEDGE
VARIATION parameter defines how many different
hedges are selected (maximum of 5), while the fre-
quency of an individual hedge is controlled by a
HEDGE REPETITION parameter, up to a maximum
of 2 identical hedges per utterance.
The syntactic structure of hedges are defined as
well as constraints on their insertion point in the ut-
terance?s syntactic structure. Each time a hedge is
selected, it is randomly inserted at one of the inser-
tion points respecting the constraints, until the spec-
ified frequency is reached. For example, a constraint
on the hedge kind of is that it modifies adjectives.
Tag questions: Tag questions are also polite-
ness markers (Brown and Levinson, 1987). They
redress the hearer?s positive face by claiming com-
mon ground. A TAG QUESTION INSERTION param-
eter leads to negating the auxiliary of the verb and
pronominalizing the subject, e.g. X has great food
results in the insertion of doesn?t it?, as in Alt-8.
Negations: Introverts use significantly more
negations (Pennebaker and King, 1999). Although
the content parameters select more negative polarity
content items for introvert utterances, we also ma-
nipulate negations, while keeping the content con-
stant, by converting adjectives to the negative of
their antonyms, e.g. the atmosphere is nice was
transformed to not nasty in Alt-9 in Table 1.
Subject implicitness: Heylighen and Dewaele
(2002) found that extraverts use more implicit lan-
guage than introverts. To control the level of implic-
itness, the SUBJECT IMPLICITNESS parameter deter-
mines whether predicates describing restaurant at-
tributes are expressed with the restaurant in the sub-
ject, or with the attribute itself (e.g., it has good food
vs. the food is tasty in Alt-9).
4.4 Lexical choice
Introverts use a richer vocabulary (Dewaele and
Furnham, 1999), so the LEXICON FREQUENCY pa-
rameter selects lexical items by their normalized fre-
quency in the British National Corpus. WordNet
synonyms are used to obtain a pool of synonyms, as
well as adjectives extracted from a corpus of restau-
rant reviews for all levels of polarity (e.g. the ad-
jective tasty in Alt-9 is a high polarity modifier of
the food attribute). Synonyms are manually checked
to make sure they are interchangeable. For example,
the content item expressed originally as it has decent
service is transformed to it features friendly service
in Alt-2, and to the servers are nice in Alt-3.
5 Experimental Method and Hypotheses
Our primary hypothesis is that language generated
by varying parameters suggested by psycholinguis-
tic research can be recognized as extravert or in-
trovert. To test this hypothesis, three expert judges
evaluated a set of generated utterances as if they had
been uttered by a friend responding in a dialogue to a
request to recommend restaurants. These utterances
had been generated to systematically manipulate ex-
traversion/introversion parameters.
The judges rated each utterance for perceived ex-
traversion, by answering the two questions measur-
500
ing that trait from the Ten-Item Personality Inven-
tory, as this instrument was shown to be psychome-
trically superior to a ?single item per trait? question-
naire (Gosling et al, 2003). The answers are aver-
aged to produce an extraversion rating ranging from
1 (highly introvert) to 7 (highly extravert). Because
it was unclear whether the generation parameters in
Table 2 would produce natural sounding utterances,
the judges also evaluated the naturalness of each ut-
terance on the same scale. The judges rated 240 ut-
terances, grouped into 20 sets of 12 utterances gen-
erated from the same content plan. They rated one
randomly ordered set at a time, but viewed all 12
utterances in that set before rating them. The ut-
terances were generated to meet two experimental
goals. First, to test the direct control of the per-
ception of extraversion. 2 introvert utterances and
2 extravert utterances were generated for each con-
tent plan (80 in total) using the parameter values
in Table 2. Multiple outputs were generated with
both parameter settings normally distributed with a
15% standard deviation. Second, 8 utterances for
each content plan (160 in total) were generated with
random parameter values. These random utterances
make it possible to: (1) improve PERSONAGE?s di-
rect output by calibrating its parameters more pre-
cisely; and (2) build a statistical model that selects
utterances matching input personality values after an
overgeneration phase (see Section 6.2). The inter-
rater agreement for extraversion between the judges
over all 240 utterances (average Pearson?s correla-
tion of 0.57) shows that the magnitude of the differ-
ences of perception between judges is almost con-
stant (? = .037). A low agreement can yield a high
correlation (e.g. if all values differ by a constant
factor), so we also compute the intraclass correla-
tion coefficient r based on a two-way random effect
model. We obtain a r of 0.79, which is significant
at the p < .001 level (reliability of average mea-
sures, identical to Cronbach?s alpha). This is com-
parable to the agreement of judgments of personality
in Mehl et al (2006) (mean r = 0.84).
6 Experimental Results
6.1 Hypothesized parameter settings
Table 1 provides examples of PERSONAGE?s out-
put and extraversion ratings. To assess whether
PERSONAGE generates language that can be rec-
ognized as introvert and extravert, we did a indepen-
dent sample t-test between the average ratings of the
40 introvert and 40 extravert utterances (parameters
with 15% standard deviation as in Table 2). Table 3
Rating Introvert Extravert Random
Extraversion 2.96 5.98 5.02
Naturalness 4.93 5.78 4.51
Table 3: Average extraversion and naturalness rat-
ings for the utterances generated with introvert, ex-
travert, and random parameters.
shows that introvert utterances have an average rat-
ing of 2.96 out of 7 while extravert utterances have
an average rating of 5.98. These ratings are signifi-
cantly different at the p < .001 level (two-tailed).
In addition, if we divide the data into two equal-
width bins around the neutral extravert rating (4 out
of 7), then PERSONAGE?s utterance ratings fall in
the bin predicted by the parameter set 89.2% of the
time. Extravert utterance are also slightly more nat-
ural than the introvert ones (p < .001).
Table 3 also shows that the 160 random parame-
ter utterances produce an average extraversion rating
of 5.02, both significantly higher than the introvert
set and lower than the extravert set (p < .001). In-
terestingly, the random utterances, which may com-
bine linguistic variables associated with both intro-
verts and extraverts, are less natural than the intro-
vert (p = .059) and extravert sets (p < .001).
6.2 Statistical models evaluation
We also investigate a second approach: overgener-
ation with random parameter settings, followed by
ranking via a statistical model trained on the judges?
feedback. This approach supports generating utter-
ances for any input extraversion value, as well as de-
termining which parameters affect the judges? per-
ception.
We model perceived personality ratings (1 . . . 7)
with regression models from the Weka toolbox (Wit-
ten and Frank, 2005). We used the full dataset of
160 averaged ratings for the random parameter utter-
ances. Each utterance was associated with a feature
vector with the generation decisions for each param-
eter in Section 2. To reduce data sparsity, we select
features that correlate significantly with the ratings
(p < .10) with a coefficient higher than 0.1.
Regression models are evaluated using the mean
absolute error and the correlation between the pre-
dicted score and the actual average rating. Table 4
shows the mean absolute error on a scale from 1 to
7 over ten 10-fold cross-validations for the 4 best
regression models: Linear Regression (LR), M5?
model tree (M5), and Support Vector Machines (i.e.
SMOreg) with linear kernels (SMO1) and radial-
501
basis function kernels (SMOr). All models signif-
icantly outperform the baseline (0.83 mean absolute
error, p < .05), but surprisingly the linear model
performs the best with a mean absolute error of 0.65.
The best model produces a correlation coefficient of
0.59 with the judges? ratings, which is higher than
the correlations between pairs of judges, suggesting
that the model performs as well as a human judge.
Metric LR M5 SMO1 SMOr
Absolute error 0.65 0.66 0.72 0.70
Correlation 0.59 0.56 0.54 0.57
Table 4: Mean absolute regression errors (scale from
1 to 7) and correlation coefficients over ten 10-fold
cross-validations, for 4 models: Linear Regression
(LR), M5? model tree (M5), Support Vector Ma-
chines with linear kernels (SMO1) and radial-basis
function kernels (SMOr). All models significantly
outperform the mean baseline (0.83 error, p < .05).
The M5? regression tree in Figure 2 assigns a rat-
ing given the features. Verbosity plays the most im-
portant role: utterances with 4 or more content items
are modeled as more extravert. Given a low ver-
bosity, lexical frequency and restatements determine
the extraversion level, e.g. utterances with less than
4 content items and infrequent words are perceived
as very introverted (rating of 2.69 out of 7). For
verbose utterances, the you know hedge indicates
extraversion, as well as concessions, restatements,
self-references, and positive content. Although rel-
atively simple, these models are useful for identify-
ing new personality markers, as well as calibrating
parameters in the direct generation model.
7 Discussion and Conclusions
We present and evaluate PERSONAGE, a parame-
terizable generator that produces outputs that vary
along the extraversion personality dimension. This
paper makes four contributions:
1. We present a systematic review of psycholinguistic find-
ings, organized by the NLG reference architecture;
2. We propose a mapping from these findings to generation
parameters for each NLG module and a real-time imple-
mentation of a generator using these parameters2. To our
knowledge this is the first attempt to put forward a sys-
tematic framework for generating language that manifests
personality;
3. We present an evaluation experiment showing that we can
control the parameters to produce recognizable linguis-
tic variation along the extraversion personality dimen-
sion. Thus, we show that the weak correlations reported
2An online demo is available at
www.dcs.shef.ac.uk/cogsys/personage.html
in other genres of language, and for self-reports rather
than observers, carry over to the production of single eval-
uative utterances with recognizable personality in a re-
stricted domain;
4. We present the results of a training experiment showing
that given an output, we can train a model that matches
human performance in assigning an extraversion rating to
that output.
Some of the challenges discussed in the introduc-
tion remain. We have shown that evaluative utter-
ances in the restaurant domain can manifest person-
ality, but more research is needed on which speech
acts recognisably manifest personality in a restricted
domain. We also showed that the mapping we hy-
pothesised of findings to generation parameters was
effective, but there may be additional parameters
that the psycholinguistic findings could be mapped
to.
Our work was partially inspired by the ICONO-
CLAST and PAULINE parameterizable generators
(Bouayad-Agha et al, 2000; Hovy, 1988), which
vary the style, rather than the personality, of the gen-
erated texts. Walker et al (1997) describe a gen-
erator intended to affect perceptions of personality,
based on Brown and Levinson?s theory of polite-
ness (Brown and Levinson, 1987), that uses some
of the linguistic constructions implemented here,
such as tag questions and hedges, but it was never
evaluated. Research by Andre? et al (2000); Piwek
(2003) uses personality variables to affect the lin-
guistic behaviour of conversational agents, but they
did not systematically manipulate parameters, and
their generators were not evaluated. Reeves and
Nass (1996) demonstrate that manipulations of per-
sonality affect many aspects of user?s perceptions,
but their experiments use handcrafted utterances,
rather than generated utterances. Cassell and Bick-
more (2003) show that extraverts prefer systems uti-
lizing discourse plans that include small talk. Paiva
and Evans? trainable generator (2005) produces out-
puts that correspond to a set of linguistic variables
measured in a corpus of target texts. Their method
is similar to our statistical method using regression
trees, but provides direct control. The method re-
ported in Mairesse and Walker (2005) for training
individualized sentence planners ranks the outputs
produced by an overgeneration phase, rather than di-
rectly predicting a scalar value, as we do here. The
closest work to ours is probably Isard et al?s CRAG-
2 system (2006), which overgenerates and ranks us-
ing ngram language models trained on a corpus la-
belled for all Big Five personality dimensions. How-
ever, CRAG-2 has no explicit parameter control, and
it has yet to be evaluated.
502
Max BNC Frequency Restatements
Verbosity
> 0.02
2.69 3.52 4.47
4.12
3.26
> 0.1
> 2.5
> 0.64<= 0.64
3.74
Max BNC Frequency
Concessions
> 0.87
Self?references Restatements
> 0.5 > 0.5
5.33 Verbosity
5.08 5.53
> 5.5
5.85
> 0.5
5.00
> 0.5
.
. <= 0.02
<= 2.5
<= 0.5
<= 0.5
<= 0.5
<= 0.5
<= 0.5
<= 5.5
> 0.5
> 3.5
<= 0.87
> 0.5<= 0.1
Max BNC Frequency
Verbosity
Verbosity
> 4.5<= 4.5
.
Infer aggregation:
       Period
<= 0.5
4.52
<= 3.5
Hedge: ?you know?
5.93Content
5.54 5.78
Polarity
Figure 2: M5? regression tree. The output ranges from 1 to 7, where 7 means strongly extravert.
In future work, we hope to directly compare the
direct generation method of Section 6.1 with the
overgenerate and rank method of Section 6.2, and to
use these results to refine PERSONAGE?s parame-
ter settings. We also hope to extend PERSONAGE?s
generation capabilities to other Big Five traits, iden-
tify additional features to improve the model?s per-
formance, and evaluate the effect of personality vari-
ation on user satisfaction in various applications.
References
E. Andre?, T. Rist, S. van Mulken, M. Klesen, and S. Baldes.
2000. The automated design of believable dialogues for
animated presentation teams. In Embodied conversational
agents, p. 220?255. MIT Press, Cambridge, MA.
N. Bouayad-Agha, D. Scott, and R. Power. 2000. Integrating
content and style in documents: a case study of patient in-
formation leaflets. Information Design Journal, 9:161?176.
P. Brown and S. Levinson. 1987. Politeness: Some universals
in language usage. Cambridge University Press.
G. Carenini and J. D. Moore. 2000. A strategy for generating
evaluative arguments. In Proc. of International Conference
on Natural Language Generation, p. 47?54.
J. Cassell and T. Bickmore. 2003. Negotiated collusion: Model-
ing social language and its relationship effects in intelligent
agents. User Modeling and User-Adapted Interaction, 13
(1-2):89?132.
J-M. Dewaele and A. Furnham. 1999. Extraversion: the unloved
variable in applied linguistic research. Language Learning,
49(3):509?544.
A. Furnham. 1990. Language and personality. In Handbook of
Language and Social Psychology. Winley.
S. D. Gosling, P. J. Rentfrow, and W. B. Swann Jr. 2003. A very
brief measure of the big five personality domains. Journal of
Research in Personality, 37:504?528.
F. Heylighen and J-M. Dewaele. 2002. Variation in the con-
textuality of language: an empirical measure. Context in
Context, Foundations of Science, 7(3):293?340.
E. Hovy. 1988. Generating Natural Language under Pragmatic
Constraints. Lawrence Erlbaum Associates.
A. Isard, C. Brockmann, and J. Oberlander. 2006. Individuality
and alignment in generated dialogues. In Proc. of INLG.
B. Lavoie and O. Rambow. 1997. A fast and portable realizer
for text generation systems. In Proc. of ANLP.
A. Loyall and J. Bates. 1997. Personality-rich believable agents
that use language. In Proc. of the First International Confer-
ence on Autonomous Agents, p. 106?113.
F. Mairesse and M. Walker. 2005. Learning to personalize spo-
ken generation for dialogue systems. In Proc. of the Inter-
speech - Eurospeech, p. 1881?1884.
M. Mehl, S. Gosling, and J. Pennebaker. 2006. Personality in
its natural habitat: Manifestations and implicit folk theories
of personality in daily life. Journal of Personality and Social
Psychology, 90:862?877.
W. T. Norman. 1963. Toward an adequate taxonomy of per-
sonality attributes: Replicated factor structure in peer nom-
ination personality rating. Journal of Abnormal and Social
Psychology, 66:574?583.
J. Oberlander and A. Gill. 2006. Language with character: A
stratified corpus comparison of individual differences in e-
mail communication. Discourse Processes, 42:239?270.
D. Paiva and R. Evans. 2005. Empirically-based control of nat-
ural language generation. In Proc. of ACL.
J. W. Pennebaker and L. A. King. 1999. Linguistic styles: Lan-
guage use as an individual difference. Journal of Personality
and Social Psychology, 77:1296?1312.
P. Piwek. 2003. A flexible pragmatics-driven language genera-
tor for animated agents. In Proc. of EACL.
B. Reeves and C. Nass. 1996. The Media Equation. University
of Chicago Press.
E. Reiter and R. Dale. 2000. Building Natural Language Gen-
eration Systems. Cambridge University Press.
A. Stent, R. Prasad, and M. Walker. 2004. Trainable sentence
planning for complex information presentation in spoken di-
alog systems. In Proc. of ACL.
A. Thorne. 1987. The press of personality: A study of conver-
sations between introverts and extraverts. Journal of Person-
ality and Social Psychology, 53:718?726.
M. Walker, J. Cahn, and S. Whittaker. 1997. Improvising lin-
guistic style: Social and affective bases for agent personality.
In Proc. of the Conference on Autonomous Agents.
I. H. Witten and E. Frank. 2005. Data Mining: Practical ma-
chine learning tools and techniques. Morgan Kaufmann.
503
Proceedings of ACL-08: HLT, pages 165?173,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Trainable Generation of Big-Five Personality Styles
through Data-driven Parameter Estimation
Franc?ois Mairesse
Cambridge University Engineering Department
Trumpington Street
Cambridge, CB2 1PZ, United Kingdom
farm2@eng.cam.ac.uk
Marilyn Walker
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, United Kingdom
lynwalker@gmail.com
Abstract
Previous work on statistical language gen-
eration has primarily focused on grammat-
icality and naturalness, scoring generation
possibilities according to a language model
or user feedback. More recent work has
investigated data-driven techniques for con-
trolling linguistic style without overgenera-
tion, by reproducing variation dimensions ex-
tracted from corpora. Another line of work
has produced handcrafted rule-based systems
to control specific stylistic dimensions, such
as politeness and personality. This paper
describes a novel approach that automati-
cally learns to produce recognisable varia-
tion along a meaningful stylistic dimension?
personality?without the computational cost
incurred by overgeneration techniques. We
present the first evaluation of a data-driven
generation method that projects multiple per-
sonality traits simultaneously and on a contin-
uous scale. We compare our performance to a
rule-based generator in the same domain.
1 Introduction
Over the last 20 years, statistical language models
(SLMs) have been used successfully in many tasks
in natural language processing, and the data avail-
able for modeling has steadily grown (Lapata and
Keller, 2005). Langkilde and Knight (1998) first
applied SLMs to statistical natural language genera-
tion (SNLG), showing that high quality paraphrases
can be generated from an underspecified representa-
tion of meaning, by first applying a very undercon-
strained, rule-based overgeneration phase, whose
outputs are then ranked by an SLM scoring phase.
Since then, research in SNLG has explored a range
of models for both dialogue and text generation.
One line of work has primarily focused on gram-
maticality and naturalness, scoring the overgener-
ation phase with a SLM, and evaluating against
a gold-standard corpus, using string or tree-match
metrics (Langkilde-Geary, 2002; Bangalore and
Rambow, 2000; Chambers and Allen, 2004; Belz,
2005; Isard et al, 2006).
Another thread investigates SNLG scoring mod-
els trained using higher-level linguistic features
to replicate human judgments of utterance quality
(Rambow et al, 2001; Nakatsu and White, 2006;
Stent and Guo, 2005). The error of these scoring
models approaches the gold-standard human rank-
ing with a relatively small training set.
A third SNLG approach eliminates the overgen-
eration phase (Paiva and Evans, 2005). It applies
factor analysis to a corpus exhibiting stylistic vari-
ation, and then learns which generation parameters
to manipulate to correlate with factor measurements.
The generator was shown to reproduce intended fac-
tor levels across several factors, thus modelling the
stylistic variation as measured in the original corpus.
Our goal is a generation technique that can tar-
get multiple stylistic effects simultaneously and
over a continuous scale, controlling stylistic di-
mensions that are commonly understood and thus
meaningful to users and application developers.
Our intended applications are output utterances
for intelligent training or intervention systems,
video game characters, or virtual environment
avatars. In previous work, we presented PERSON-
AGE, a psychologically-informed rule-based genera-
tor based on the Big Five personality model, and we
showed that PERSONAGE can project extreme per-
sonality on the extraversion scale, i.e. both intro-
verted and extraverted personality types (Mairesse
and Walker, 2007). We used the Big Five model
to develop PERSONAGE for several reasons. First,
the Big Five has been shown in psychology to ex-
165
Trait High Low
Extraversion warm, assertive, sociable, excitement seeking, active,
spontaneous, optimistic, talkative
shy, quiet, reserved, passive, solitary, moody
Emotional stability calm, even-tempered, reliable, peaceful, confident neurotic, anxious, depressed, self-conscious
Agreeableness trustworthy, considerate, friendly, generous, helpful unfriendly, selfish, suspicious, uncooperative, ma-
licious
Conscientiousness competent, disciplined, dutiful, achievement striving disorganised, impulsive, unreliable, forgetful
Openness to experience creative, intellectual, curious, cultured, complex narrow-minded, conservative, ignorant, simple
Table 1: Example adjectives associated with extreme values of the Big Five trait scales.
plain much of the variation in human perceptions of
personality differences. Second, we believe that the
adjectives used to develop the Big Five model pro-
vide an intuitive, meaningful definition of linguis-
tic style. Table 1 shows some of the trait adjec-
tives associated with the extremes of each Big Five
trait. Third, there are many studies linking person-
ality to linguistic variables (Pennebaker and King,
1999; Mehl et al, 2006, inter alia). See (Mairesse
and Walker, 2007) for more detail.
In this paper, we further test the utility of basing
stylistic variation on the Big Five personality model.
The Big Five traits are represented by scalar val-
ues that range from 1 to 7, with values normally
distributed among humans. While our previous
work targeted extreme values of individual traits,
here we show that we can target multiple person-
ality traits simultaneously and over the continuous
scales of the Big Five model. Section 2 describes
a novel parameter-estimation method that automat-
ically learns to produce recognisable variation for
all Big Five traits, without overgeneration, imple-
mented in a new SNLG called PERSONAGE-PE.
We show that PERSONAGE-PE generates targets for
multiple personality dimensions, using linear and
non-linear parameter estimation models to predict
generation parameters directly from the scalar tar-
gets. Section 3.2 shows that humans accurately per-
ceive the intended variation, and Section 3.3 com-
pares PERSONAGE-PE (trained) with PERSONAGE
(rule-based; Mairesse and Walker, 2007). We delay
a detailed discussion of related work to Section 4,
where we summarize and discuss future work.
2 Parameter Estimation Models
The data-driven parameter estimation method con-
sists of a development phase and a generation phase
(Section 3). The development phase:
1. Uses a base generator to produce multiple utter-
ances by randomly varying its parameters;
2. Collects human judgments rating the personality of
each utterance;
3. Trains statistical models to predict the parameters
from the personality judgments;
7.006.005.004.003.002.001.00 Agreeableness rating
30
20
10
0
Freq
uenc
y
Figure 1: Distribution of average agreeableness ratings
from the 2 expert judges for 160 random utterances.
4. Selects the best model for each parameter via cross-
validation.
2.1 Base Generator
We make minimal assumptions about the input to
the generator to favor domain independence. The
input is a speech act, a potential content pool that
can be used to achieve that speech act, and five scalar
personality parameters (1. . .7), specifying values for
the continuous scalar dimensions of each trait in
the Big Five model. See Table 1. This requires a
base generator that generates multiple outputs ex-
pressing the same input content by varying linguis-
tic parameters related to the Big Five traits. We
start with the PERSONAGE generator (Mairesse and
Walker, 2007), which generates recommendations
and comparisons of restaurants. We extend PER-
SONAGE with new parameters for a total of 67 pa-
rameters in PERSONAGE-PE. See Table 2. These
parameters are derived from psychological studies
identifying linguistic markers of the Big Five traits
(Pennebaker and King, 1999; Mehl et al, 2006, in-
ter alia). As PERSONAGE?s input parameters are
domain-independent, most parameters range contin-
uously between 0 and 1, while pragmatic marker in-
sertion parameters are binary, except for the SUB-
JECT IMPLICITNESS, STUTTERING and PRONOMI-
166
Parameters Description
Content parameters:
VERBOSITY Control the number of propositions in the utterance
RESTATEMENTS Paraphrase an existing proposition, e.g. ?Chanpen Thai has great service, it has fantastic waiters?
REPETITIONS Repeat an existing proposition
CONTENT POLARITY Control the polarity of the propositions expressed, i.e. referring to negative or positive attributes
REPETITIONS POLARITY Control the polarity of the restated propositions
CONCESSIONS Emphasise one attribute over another, e.g. ?even if Chanpen Thai has great food, it has bad service?
CONCESSIONS POLARITY Determine whether positive or negative attributes are emphasised
POLARISATION Control whether the expressed polarity is neutral or extreme
POSITIVE CONTENT FIRST Determine whether positive propositions?including the claim?are uttered first
Syntactic template selection parameters:
SELF-REFERENCES Control the number of first person pronouns
CLAIM COMPLEXITY Control the syntactic complexity (syntactic embedding)
CLAIM POLARITY Control the connotation of the claim, i.e. whether positive or negative affect is expressed
Aggregation operations:
PERIOD Leave two propositions in their own sentences, e.g. ?Chanpen Thai has great service. It has nice decor.?
RELATIVE CLAUSE Aggregate propositions with a relative clause, e.g. ?Chanpen Thai, which has great service, has nice decor?
WITH CUE WORD Aggregate propositions using with, e.g. ?Chanpen Thai has great service, with nice decor?
CONJUNCTION Join two propositions using a conjunction, or a comma if more than two propositions
MERGE Merge the subject and verb of two propositions, e.g. ?Chanpen Thai has great service and nice decor?
ALSO CUE WORD Join two propositions using also, e.g. ?Chanpen Thai has great service, also it has nice decor?
CONTRAST - CUE WORD Contrast two propositions using while, but, however, on the other hand, e.g. ?While Chanpen Thai has great
service, it has bad decor?, ?Chanpen Thai has great service, but it has bad decor?
JUSTIFY - CUE WORD Justify a proposition using because, since, so, e.g. ?Chanpen Thai is the best, because it has great service?
CONCEDE - CUE WORD Concede a proposition using although, even if, but/though, e.g. ?Although Chanpen Thai has great service, it
has bad decor?, ?Chanpen Thai has great service, but it has bad decor though?
MERGE WITH COMMA Restate a proposition by repeating only the object, e.g. ?Chanpen Thai has great service, nice waiters?
CONJ. WITH ELLIPSIS Restate a proposition after replacing its object by an ellipsis, e.g. ?Chanpen Thai has . . . , it has great service?
Pragmatic markers:
SUBJECT IMPLICITNESS Make the restaurant implicit by moving the attribute to the subject, e.g. ?the service is great?
NEGATION Negate a verb by replacing its modifier by its antonym, e.g. ?Chanpen Thai doesn?t have bad service?
SOFTENER HEDGES Insert syntactic elements (sort of, kind of, somewhat, quite, around, rather, I think that, it seems that, it seems
to me that) to mitigate the strength of a proposition, e.g. ?Chanpen Thai has kind of great service? or ?It seems
to me that Chanpen Thai has rather great service?
EMPHASIZER HEDGES Insert syntactic elements (really, basically, actually, just) to strengthen a proposition, e.g. ?Chanpen Thai has
really great service? or ?Basically, Chanpen Thai just has great service?
ACKNOWLEDGMENTS Insert an initial back-channel (yeah, right, ok, I see, oh, well), e.g. ?Well, Chanpen Thai has great service?
FILLED PAUSES Insert syntactic elements expressing hesitancy (like, I mean, err, mmhm, you know), e.g. ?I mean, Chanpen
Thai has great service, you know? or ?Err... Chanpen Thai has, like, great service?
EXCLAMATION Insert an exclamation mark, e.g. ?Chanpen Thai has great service!?
EXPLETIVES Insert a swear word, e.g. ?the service is damn great?
NEAR-EXPLETIVES Insert a near-swear word, e.g. ?the service is darn great?
COMPETENCE MITIGATION Express the speaker?s negative appraisal of the hearer?s request, e.g. ?everybody knows that . . . ?
TAG QUESTION Insert a tag question, e.g. ?the service is great, isn?t it??
STUTTERING Duplicate the first letters of a restaurant?s name, e.g. ?Ch-ch-anpen Thai is the best?
CONFIRMATION Begin the utterance with a confirmation of the restaurant?s name, e.g. ?did you say Chanpen Thai??
INITIAL REJECTION Begin the utterance with a mild rejection, e.g. ?I?m not sure?
IN-GROUP MARKER Refer to the hearer as a member of the same social group, e.g. pal, mate and buddy
PRONOMINALIZATION Replace occurrences of the restaurant?s name by pronouns
Lexical choice parameters:
LEXICAL FREQUENCY Control the average frequency of use of each content word, according to BNC frequency counts
WORD LENGTH Control the average number of letters of each content word
VERB STRENGTH Control the strength of the selected verbs, e.g. ?I would suggest? vs. ?I would recommend?
Table 2: The 67 generation parameters whose target values are learned. Aggregation cue words, hedges, acknowl-
edgments and filled pauses are learned individually (as separate parameters), e.g. kind of is modeled differently than
somewhat in the SOFTENER HEDGES category. Parameters are detailed in previous work (Mairesse and Walker, 2007).
NALIZATION parameters.
2.2 Random Sample Generation and Expert
Judgments
We generate a sample of 160 random utterances by
varying the parameters in Table 2 with a uniform dis-
tribution. This sample is intended to provide enough
training material for estimating all 67 parameters
for each personality dimension. Following Mairesse
and Walker (2007), two expert judges (not the au-
thors) familiar with the Big Five adjectives (Table 1)
evaluate the personality of each utterance using the
Ten-Item Personality Inventory (TIPI; Gosling et al,
2003), and also judge the utterance?s naturalness.
Thus 11 judgments were made for each utterance for
a total of 1760 judgments. The TIPI outputs a rating
on a scale from 1 (low) to 7 (high) for each Big Five
trait. The expert judgments are approximately nor-
167
mally distributed; Figure 1 shows the distribution for
agreeableness.
2.3 Statistical Model Training
Training data is created for each generation
parameter?i.e. the output variable?to train statis-
tical models predicting the optimal parameter value
from the target personality scores. The models are
thus based on the simplifying assumption that the
generation parameters are independent. Any person-
ality trait whose correlation with a generation deci-
sion is below 0.1 is removed from the training data.
This has the effect of removing parameters that do
not correlate strongly with any trait, which are set to
a constant default value at generation time. Since
the input parameter values may not be satisfiable
depending on the input content, the actual genera-
tion decisions made for each utterance are recorded.
For example, the CONCESSIONS decision value is
the actual number of concessions produced in the
utterance. To ensure that the models? output can
control the generator, the generation decision values
are normalized to match the input range (0. . .1) of
PERSONAGE-PE. Thus the dataset consists of 160
utterances and the corresponding generation deci-
sions, each associated with 5 personality ratings av-
eraged over both judges.
Parameter estimation models are trained to predict
either continuous (e.g. VERBOSITY) or binary (e.g.
EXCLAMATION) generation decisions. We compare
various learning algorithms using the Weka toolkit
(with default values unless specified; Witten and
Frank, 2005). Continuous parameters are modeled
with a linear regression model (LR), an M5? model
tree (M5), and a model based on support vector ma-
chines with a linear kernel (SVM). As regression
models can extrapolate beyond the [0, 1] interval, the
output parameter values are truncated if needed?at
generation time?before being sent to the base gen-
erator. Binary parameters are modeled using clas-
sifiers that predict whether the parameter is enabled
or disabled. We test a Naive Bayes classifier (NB), a
j48 decision tree (J48), a nearest-neighbor classifier
using one neighbor (NN), a Java implementation of
the RIPPER rule-based learner (JRIP), the AdaBoost
boosting algorithm (ADA), and a support vector ma-
chines classifier with a linear kernel (SVM).
Figures 2, 3 and 4 show the models learned for
the EXCLAMATION (binary), STUTTERING (contin-
uous), and CONTENT POLARITY (continuous) pa-
rameters in Table 2. The models predict generation
parameters from input personality scores; note that
Condition Class Weight
--------- ----- ------
if extraversion > 6.42 then 1 else 0 1.81
if extraversion > 4.42 then 1 else 0 0.38
if extraversion <= 6.58 then 1 else 0 0.22
if extraversion > 4.71 then 1 else 0 0.28
if agreeableness > 5.13 then 1 else 0 0.42
if extraversion <= 6.58 then 1 else 0 0.14
if extraversion > 4.79 then 1 else 0 0.19
if extraversion <= 6.58 then 1 else 0 0.17
Figure 2: AdaBoost model predicting the EXCLAMATION
parameter. Given input trait values, the model outputs
the class yielding the largest sum of weights for the rules
returning that class. Class 0 = disabled, class 1 = enabled.
(normalized) Content polarity =
0.054
- 0.102 * (normalized) emotional stability
+ 0.970 * (normalized) agreeableness
- 0.110 * (normalized) conscientiousness
+ 0.013 * (normalized) openness to
experience
Figure 3: SVM model with a linear kernel predicting the
CONTENT POLARITY parameter.
sometimes the best performing model is non-linear.
Given input trait values, the AdaBoost model in Fig-
ure 2 outputs the class yielding the largest sum of
weights for the rules returning that class. For ex-
ample, the first rule of the EXCLAMATION model
shows that an extraversion score above 6.42 out of
7 would increase the weight of the enabled class by
1.81. The fifth rule indicates that a target agreeable-
ness above 5.13 would further increase the weight
by .42. The STUTTERING model tree in Figure 4
lets us calculate that a low emotional stability (1.0)
together with a neutral conscientiousness and open-
ness to experience (4.0) yield a parameter value of
.62 (see LM2), whereas a neutral emotional stabil-
ity decreases the value down to .17. Figure 4 also
shows how personality traits that do not affect the
parameter are removed, i.e. emotional stability, con-
scientiousness and openness to experience are the
traits that affect stuttering. The linear model in Fig-
ure 3 shows that agreeableness has a strong effect
on the CONTENT POLARITY parameter (.97 weight),
but emotional stability, conscientiousness and open-
ness to experience also have an effect.
2.4 Model Selection
The final step of the development phase identifies
the best performing model(s) for each generation
parameter via cross-validation. For continuous pa-
168
? 3.875 > 3.875
Conscientiousness
Emotional stability
? 4.375 > 4.375
Stuttering =
-0.0136 * emotional stability
+ 0.0098 * conscientiousness
+ 0.0063 * openness to experience
+ 0.0126
Stuttering =
-0.1531 * emotional stability
+ 0.004 * conscientiousness
+ 0.1122 * openness to experience
+ 0.3129
Stuttering =
-0.0142 * emotional stability
+ 0.004 * conscientiousness
+ 0.0076 * openness to experience
+ 0.0576
Figure 4: M5? model tree predicting the STUTTERING parameter.
Continuous parameters LR M5 SVM
Content parameters:
VERBOSITY 0.24 0.26 0.21
RESTATEMENTS 0.14 0.14 0.04
REPETITIONS 0.13 0.13 0.08
CONTENT POLARITY 0.46 0.46 0.47
REPETITIONS POLARITY 0.02 0.15 0.06
CONCESSIONS 0.23 0.23 0.12
CONCESSIONS POLARITY -0.01 0.16 0.07
POLARISATION 0.20 0.21 0.20
Syntactic template selection:
CLAIM COMPLEXITY 0.10 0.33 0.26
CLAIM POLARITY 0.04 0.04 0.05
Aggregation operations:
INFER - WITH CUE WORD 0.03 0.03 0.01
INFER - ALSO CUE WORD 0.10 0.10 0.06
JUSTIFY - SINCE CUE WORD 0.03 0.07 0.05
JUSTIFY - SO CUE WORD 0.07 0.07 0.04
JUSTIFY - PERIOD 0.36 0.35 0.21
CONTRAST - PERIOD 0.27 0.26 0.26
RESTATE - MERGE WITH COMMA 0.18 0.18 0.09
CONCEDE - ALTHOUGH CUE WORD 0.08 0.08 0.05
CONCEDE - EVEN IF CUE WORD 0.05 0.05 0.03
Pragmatic markers:
SUBJECT IMPLICITNESS 0.13 0.13 0.04
STUTTERING INSERTION 0.16 0.23 0.17
PRONOMINALIZATION 0.22 0.20 0.17
Lexical choice parameters:
LEXICAL FREQUENCY 0.21 0.21 0.19
WORD LENGTH 0.18 0.18 0.15
Table 3: Pearson?s correlation between parameter model
predictions and continuous parameter values, for differ-
ent regression models. Parameters that do not correlate
with any trait are omitted. Aggregation operations are as-
sociated with a rhetorical relation (e.g. INFER). Results
are averaged over a 10-fold cross-validation.
rameters, Table 3 evaluates modeling accuracy by
comparing the correlations between the model?s pre-
dictions and the actual parameter values in the test
folds. Table 4 reports results for binary parameter
classifiers, by comparing the F-measures of the en-
abled class. Best performing models are identified
in bold; parameters that do not correlate with any
trait or that produce a poor modeling accuracy are
omitted.
The CONTENT POLARITY parameter is modeled
Binary parameters NB J48 NN ADA SVM
Pragmatic markers:
SOFTENER HEDGES
kind of 0.00 0.00 0.16 0.11 0.10
rather 0.00 0.00 0.02 0.01 0.01
quite 0.14 0.08 0.09 0.07 0.06
EMPHASIZER HEDGES
basically 0.00 0.00 0.02 0.01 0.01
ACKNOWLEDGMENTS
yeah 0.00 0.00 0.04 0.03 0.03
ok 0.13 0.07 0.06 0.05 0.05
FILLED PAUSES
err 0.32 0.20 0.24 0.22 0.19
EXCLAMATION 0.23 0.34 0.36 0.38 0.34
EXPLETIVES 0.27 0.18 0.24 0.17 0.15
IN-GROUP MARKER 0.40 0.31 0.31 0.24 0.21
TAG QUESTION 0.32 0.21 0.21 0.15 0.13
CONFIRMATION 0.00 0.00 0.07 0.04 0.04
Table 4: F-measure of the enabled class for classifica-
tion models of binary parameters. Parameters that do
not correlate with any trait are omitted. Results are av-
eraged over a 10-fold cross-validation. JRIP models are
not shown as they never perform best.
the most accurately, with the SVM model in Fig-
ure 3 producing a correlation of .47 with the true pa-
rameter values. Models of the PERIOD aggregation
operation also perform well, with a linear regression
model yielding a correlation of .36 when realizing
a justification, and .27 when contrasting two propo-
sitions. CLAIM COMPLEXITY and VERBOSITY are
also modeled successfully, with correlations of .33
and .26 using a model tree. The model tree control-
ling the STUTTERING parameter illustrated in Fig-
ure 4 produces a correlation of .23. For binary pa-
rameters, Table 4 shows that the Naive Bayes classi-
fier is generally the most accurate, with F-measures
of .40 for the IN-GROUP MARKER parameter, and
.32 for both the insertion of filled pauses (err) and
tag questions. The AdaBoost algorithm best predicts
the EXCLAMATION parameter, with an F-measure of
.38 for the model in Figure 2.
169
# Traits End Rating Nat Output utterance
1.a Extraversion high 4.42 4.79 Radio Perfecto?s price is 25 dollars but Les Routiers provides adequate food. Iimagine they?re alright!Agreeableness high 4.94
1.b
Emotional stability high 5.35
5.04
Let?s see, Les Routiers and Radio Perfecto... You would probably appreciate them.
Radio Perfecto is in the East Village with kind of acceptable food. Les Routiers is
located in Manhattan. Its price is 41 dollars.Conscientiousness high 5.21
2.a Extraversion low 3.65 3.21 Err... you would probably appreciate Trattoria Rustica, wouldn?t you? It?s inManhattan, also it?s an italian restaurant. It offers poor ambience, also it?s quite costly.Agreeableness low 4.02
2.b
Emotional stability low 4.13
4.50 Trattoria Rustica isn?t as bad as the others. Err... even if it?s costly, it offers kind ofadequate food, alright? It?s an italian place.Openness to low 3.85experience
Table 5: Example outputs controlled by the parameter estimation models for a comparison (#1) and a recommendation
(#2), with the average judges? ratings (Rating) and naturalness (Nat). Ratings are on a scale from 1 to 7, with 1 = very
low (e.g. neurotic or introvert) and 7 = very high on the dimension (e.g. emotionally stable or extraverted).
3 Evaluation Experiment
The generation phase of our parameter estimation
SNLG method consists of the following steps:
1. Use the best performing models to predict parame-
ter values from the desired personality scores;
2. Generate the output utterance using the predicted
parameter values.
We then evaluate the output utterances using naive
human judges to rate their perceived personality and
naturalness.
3.1 Evaluation Method
Given the best performing model for each genera-
tion parameter, we generate 5 utterances for each
of 5 recommendation and 5 comparison speech acts.
Each utterance targets an extreme value for two traits
(either 1 or 7 out of 7) and neutral values for the re-
maining three traits (4 out of 7). The goal is for each
utterance to project multiple traits on a continuous
scale. To generate a range of alternatives, a Gaus-
sian noise with a standard deviation of 10% of the
full scale is added to each target value.
Subjects were 24 native English speakers (12
male and 12 female graduate students from a range
of disciplines from both the U.K. and the U.S.). Sub-
jects evaluate the naturalness and personality of each
utterance using the TIPI (Gosling et al, 2003). To
limit the experiment?s duration, only the two traits
with extreme target values are evaluated for each
utterance. Subjects thus answered 5 questions for
50 utterances, two from the TIPI for each extreme
trait and one about naturalness (250 judgments in
total per subject). Subjects were not told that the
utterances were intended to manifest extreme trait
values. Table 5 shows several sample outputs and
the mean personality ratings from the human judges.
For example, utterance 1.a projects a high extraver-
sion through the insertion of an exclamation mark
based on the model in Figure 2, whereas utterance
2.a conveys introversion by beginning with the filled
pause err. The same utterance also projects a low
agreeableness by focusing on negative propositions,
through a low CONTENT POLARITY parameter value
as per the model in Figure 3. This evaluation ad-
dresses a number of open questions discussed below.
Q1: Is the personality projected by models trained on
ratings from a few expert judges recognised by a
larger sample of naive judges? (Section 3.2)
Q2: Can a combination of multiple traits within a single
utterance be detected by naive judges? (Section 3.2)
Q3: How does PERSONAGE-PE compare to PERSON-
AGE, a psychologically-informed rule-based gen-
erator for projecting extreme personality? (Sec-
tion 3.3)
Q4: Does the parameter estimation SNLG method pro-
duce natural utterances? (Section 3.4)
3.2 Parameter Estimation Evaluation
Table 6 shows that extraversion is the dimension
modeled most accurately by the parameter estima-
tion models, producing a .45 correlation with the
subjects? ratings (p < .01). Emotional stability,
agreeableness, and openness to experience ratings
also correlate strongly with the target scores, with
correlations of .39, .36 and .17 respectively (p <
.01). Additionally, Table 6 shows that the magni-
tude of the correlation increases when considering
the perception of a hypothetical average subject, i.e.
smoothing individual variation by averaging the rat-
ings over all 24 judges, producing a correlation ravg
up to .80 for extraversion. These correlations are
unexpectedly high; in corpus analyses, significant
correlations as low as .05 to .10 are typically ob-
served between personality and linguistic markers
(Pennebaker and King, 1999; Mehl et al, 2006).
Conscientiousness is the only dimension whose
ratings do not correlate with the target scores. The
170
comparison with rule-based results in Section 3.3
suggests that this is not because conscientiousness
cannot be exhibited in our domain or manifested in
a single utterance, so perhaps this arises from dif-
fering perceptions of conscientiousness between the
expert and naive judges.
Trait r ravg e
Extraversion .45 ? .80 ? 1.89
Emotional stability .39 ? .64 ? 2.14
Agreeableness .36 ? .68 ? 2.38
Conscientiousness -.01 -.02 2.79
Openness to experience .17 ? .41 ? 2.51
? statistically significant correlation
p < .05, ? p = .07 (two-tailed)
Table 6: Pearson?s correlation coefficient r and mean ab-
solute error e between the target personality scores and
the 480 judges? ratings (20 ratings per trait for 24 judges);
ravg is the correlation between the personality scores and
the average judges? ratings.
Table 6 shows that the mean absolute error varies
between 1.89 and 2.79 on a scale from 1 to 7. Such
large errors result from the decision to ask judges to
answer just the TIPI questions for the two traits that
were the extreme targets (See Section 3.1), because
the judges tend to use the whole scale, with approx-
imately normally distributed ratings. This means
that although the judges make distinctions leading to
high correlations, they do so on a compressed scale.
This explains the large correlations despite the mag-
nitude of the absolute error.
Table 7 shows results evaluating whether utter-
ances targeting the extremes of a trait are perceived
differently. The ratings differ significantly for all
traits but conscientiousness (p ? .001). Thus pa-
rameter estimation models can be used in applica-
tions that only require discrete binary variation.
Trait Low High
Extraversion 3.69 5.06 ?
Emotional stability 3.75 4.75 ?
Agreeableness 3.42 4.33 ?
Conscientiousness 4.16 4.15
Openness to experience 3.71 4.06 ?
? statistically significant difference
p ? .001 (two-tailed)
Table 7: Average personality ratings for the utterances
generated with the low and high target values for each
trait on a scale from 1 to 7.
It is important to emphasize that generation pa-
rameters were predicted based on 5 target person-
ality values. Thus, the results show that individ-
ual traits are perceived even when utterances project
other traits as well, confirming that the Big Five the-
ory models independent dimensions and thus pro-
vides a useful and meaningful framework for mod-
eling variation in language. Additionally, although
we do not directly evaluate the perception of mid-
range values of personality target scores, the results
suggest that mid-range personality is modeled cor-
rectly because the neutral target scores do not affect
the perception of extreme traits.
3.3 Comparison with Rule-Based Generation
PERSONAGE is a rule-based personality generator
based on handcrafted parameter settings derived
from psychological studies. Mairesse and Walker
(2007) show that this approach generates utterances
that are perceptibly different along the extraversion
dimension. Table 8 compares the mean ratings of
the utterances generated by PERSONAGE-PE with
ratings of 20 utterances generated by PERSONAGE
for each extreme of each Big Five scale (40 for ex-
traversion, resulting in 240 handcrafted utterances in
total). Table 8 shows that the handcrafted parame-
ter settings project a significantly more extreme per-
sonality for 6 traits out of 10. However, the learned
parameter models for neuroticism, disagreeableness,
unconscientiousness and openness to experience do
not perform significantly worse than the handcrafted
generator. These findings are promising as we dis-
cuss further in Section 4.
Method Rule-based Learned parameters
Trait Low High Low High
Extraversion 2.96 5.98 3.69 ? 5.05 ?
Emotional stability 3.29 5.96 3.75 4.75 ?
Agreeableness 3.41 5.66 3.42 4.33 ?
Conscientiousness 3.71 5.53 4.16 4.15 ?
Openness to experience 2.89 4.21 3.71 ? 4.06
?,? significant increase or decrease of the variation range
over the average rule-based ratings (p < .05, two-tailed)
Table 8: Pair-wise comparison between the ratings of
the utterances generated using PERSONAGE-PE with ex-
treme target values (Learned Parameters), and the ratings
for utterances generated with Mairesse andWalker?s rule-
based PERSONAGE generator, (Rule-based). Ratings are
averaged over all judges.
3.4 Naturalness Evaluation
The naive judges also evaluated the naturalness of
the outputs of our trained models. Table 9 shows
that the average naturalness is 3.98 out of 7, which is
significantly lower (p < .05) than the naturalness of
handcrafted and randomly generated utterances re-
ported by Mairesse and Walker (2007). It is possi-
ble that the differences arise from judgments of ut-
terances targeting multiple traits, or that the naive
171
judges are more critical.
Trait Rule-based Random Learned
All 4.59 4.38 3.98
Table 9: Average naturalness ratings for utterances gen-
erated using (1) PERSONAGE, the rule-based generator,
(2) the random utterances (expert judges) and (3) the out-
puts of PERSONAGE-PE using the parameter estimation
models (Learned, naive judges). The means differ sig-
nificantly at the p < .05 level (two-tailed independent
sample t-test).
4 Conclusion
We present a new method for generating linguis-
tic variation projecting multiple personality traits
continuously, by combining and extending previous
research in statistical natural language generation
(Paiva and Evans, 2005; Rambow et al, 2001; Is-
ard et al, 2006; Mairesse and Walker, 2007). While
handcrafted rule-based approaches are limited to
variation along a small number of discrete points
(Hovy, 1988; Walker et al, 1997; Lester et al, 1997;
Power et al, 2003; Cassell and Bickmore, 2003; Pi-
wek, 2003; Mairesse and Walker, 2007; Rehm and
Andre?, in press), we learn models that predict pa-
rameter values for any arbitrary value on the varia-
tion dimension scales. Additionally, our data-driven
approach can be applied to any dimension that is
meaningful to human judges, and it provides an ele-
gant way to project multiple dimensions simultane-
ously, by including the relevant dimensions as fea-
tures of the parameter models? training data.
Isard et al (2006) and Mairesse and Walker
(2007) also propose a personality generation
method, in which a data-driven personality model
selects the best utterance from a large candidate set.
Isard et al?s technique has not been evaluated, while
Mairesse and Walker?s overgenerate and score ap-
proach is inefficient. Paiva and Evans? technique
does not overgenerate (2005), but it requires a search
for the optimal generation decisions according to
the learned models. Our approach does not require
any search or overgeneration, as parameter estima-
tion models predict the generation decisions directly
from the target variation dimensions. This tech-
nique is therefore beneficial for real-time genera-
tion. Moreover the variation dimensions of Paiva
and Evans? data-driven technique are extracted from
a corpus: there is thus no guarantee that they can
be easily interpreted by humans, and that they gen-
eralise to other corpora. Previous work has shown
that modeling the relation between personality and
language is far from trivial (Pennebaker and King,
1999; Argamon et al, 2005; Oberlander and Now-
son, 2006; Mairesse et al, 2007), suggesting that the
control of personality is a harder problem than the
control of data-driven variation dimensions.
We present the first human perceptual evaluation
of a data-driven stylistic variation method. In terms
of our research questions in Section 3.1, we show
that models trained on expert judges to project mul-
tiple traits in a single utterance generate utterances
whose personality is recognized by naive judges.
There is only one other similar evaluation of an
SNLG (Rambow et al, 2001). Our models perform
only slightly worse than a handcrafted rule-based
generator in the same domain. These findings are
promising as (1) parameter estimation models are
able to target any combination of traits over the full
range of the Big Five scales; (2) they do not benefit
from psychological knowledge, i.e. they are trained
on randomly generated utterances.
This work also has several limitations that should
be addressed in future work. Even though the
parameters of PERSONAGE-PE were suggested by
psychological studies (Mairesse and Walker, 2007),
some of them are not modeled successfully by our
approach, and thus omitted from Tables 3 and 4.
This could be due to the relatively small develop-
ment dataset size (160 utterances to optimize 67 pa-
rameters), or to the implementation of some param-
eters. The strong parameter-independence assump-
tion could also be responsible, but we are not aware
of any state of the art implementation for learn-
ing multiple dependent variables, and this approach
could further aggravate data sparsity issues.
In addition, it is unclear why PERSONAGE per-
forms better for projecting extreme personality
and produces more natural utterances, and why
PERSONAGE-PE fails to project conscientiousness
correctly. It might be possible to improve the pa-
rameter estimation models with a larger sample of
random utterances at development time, or with ad-
ditional extreme data generated using the rule-based
approach. Such hybrid models are likely to perform
better for extreme target scores, as they are trained
on more uniformly distributed ratings (e.g. com-
pared to the normal distribution in Figure 1). In ad-
dition, we have only shown that personality can be
expressed by information presentation speech-acts
in the restaurant domain; future work should assess
the extent to which the parameters derived from psy-
chological findings are culture, domain, and speech
act dependent.
172
References
S. Argamon, S. Dhawle, M. Koppel, and J. Pennebaker.
Lexical predictors of personality type. In Proceedings
of the Joint Annual Meeting of the Interface and the
Classification Society of North America, 2005.
S. Bangalore and O. Rambow. Exploiting a probabilistic
hierarchical model for generation. In Proceedings of
the 18th International Conference on Computational
Linguistics (COLING), pages 42?48, 2000.
A. Belz. Corpus-driven generation of weather forecasts.
In Proceedings of the 3rd Corpus Linguistics Confer-
ence, 2005.
J. Cassell and T. Bickmore. Negotiated collusion: Mod-
eling social language and its relationship effects in in-
telligent agents. User Modeling and User-Adapted In-
teraction, 13:89?132, 2003.
N. Chambers and J. Allen. Stochastic language genera-
tion in a dialogue system: Toward a domain indepen-
dent generator. In Proceedings 5th SIGdial Workshop
on Discourse and Dialogue, 2004.
S. D. Gosling, P. J. Rentfrow, and W. B. Swann. A
very brief measure of the big five personality domains.
Journal of Research in Personality, 37:504?528, 2003.
E. Hovy. Generating Natural Language under Pragmatic
Constraints. Lawrence Erlbaum Associates, 1988.
A. Isard, C. Brockmann, and J. Oberlander. Individuality
and alignment in generated dialogues. In Proceedings
of the 4th International Natural Language Generation
Conference (INLG), pages 22?29, 2006.
I. Langkilde and K. Knight. Generation that exploits
corpus-based statistical knowledge. In Proceedings of
the 36th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 704?710, 1998.
I. Langkilde-Geary. An empirical verification of coverage
and correctness for a general-purpose sentence genera-
tor. In Proceedings of the 1st International Conference
on Natural Language Generation, 2002.
M. Lapata and F. Keller. Web-based models for natu-
ral language processing. ACM Transactions on Speech
and Language Processing, 2:1?31, 2005.
J. Lester, S. Converse, S. Kahler, S. Barlow, B. Stone,
and R. Bhogal. The persona effect: affective impact
of animated pedagogical agents. Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 359?366, 1997.
F. Mairesse and M. A. Walker. PERSONAGE: Personal-
ity generation for dialogue. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 496?503, 2007.
F. Mairesse, M. A. Walker, M. R. Mehl, and R. K. Moore.
Using linguistic cues for the automatic recognition of
personality in conversation and text. Journal of Artifi-
cial Intelligence Research (JAIR), 30:457?500, 2007.
M. R. Mehl, S. D. Gosling, and J. W. Pennebaker. Person-
ality in its natural habitat: Manifestations and implicit
folk theories of personality in daily life. Journal of
Personality and Social Psychology, 90:862?877, 2006.
C. Nakatsu and M. White. Learning to say it well:
Reranking realizations by predicted synthesis quality.
In Proceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
1113?1120, 2006.
J. Oberlander and S. Nowson. Whose thumb is it any-
way? classifying author personality from weblog text.
In Proceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), 2006.
D. S. Paiva and R. Evans. Empirically-based control of
natural language generation. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 58?65, 2005.
J. W. Pennebaker and L. A. King. Linguistic styles: Lan-
guage use as an individual difference. Journal of Per-
sonality and Social Psychology, 77:1296?1312, 1999.
P. Piwek. A flexible pragmatics-driven language gener-
ator for animated agents. In Proceedings of Annual
Meeting of the European Chapter of the Association
for Computational Linguistics (EACL), 2003.
R. Power, D. Scott, and N. Bouayad-Agha. Generating
texts with style. In Proceedings of the 4th Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics, 2003.
O. Rambow, M. Rogati, and M. A. Walker. Evaluating a
trainable sentence planner for a spoken dialogue travel
system. In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics (ACL),
2001.
M. Rehm and E. Andre?. From annotated multi-
modal corpora to simulated human-like behaviors.
In I. Wachsmuth and G. Knoblich, editors, Model-
ing Communication with Robots and Virtual Humans.
Springer, Berlin, Heidelberg, in press.
A. Stent and H. Guo. A new data-driven approach
for multimedia presentation generation. In Proc. Eu-
roIMSA, 2005.
M. A. Walker, J. E. Cahn, and S. J. Whittaker. Improvis-
ing linguistic style: Social and affective bases for agent
personality. In Proceedings of the 1st Conference on
Autonomous Agents, pages 96?105, 1997.
I. H. Witten and E. Frank. Data Mining: Practical ma-
chine learning tools and techniques. Morgan Kauf-
mann, San Francisco, CA, 2005.
173
Proceedings of ACL-08: HLT, pages 479?487,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Intensional Summaries as Cooperative Responses in Dialogue:
Automation and Evaluation
Joseph Polifroni
Nokia Research Center
3 Cambridge Center
Cambridge, MA 02142, USA
joseph.polifroni@nokia.com
Marilyn Walker
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, United Kingdom
M.A.Walker@sheffield.ac.uk
Abstract
Despite its long history, and a great deal
of research producing many useful algo-
rithms and observations, research in coop-
erative response generation has had little
impact on the recent commercialization of
dialogue technologies, particularly within
the spoken dialogue community. We hy-
pothesize that a particular type of cooper-
ative response, intensional summaries, are
effective for when users are unfamiliar with
the domain. We evaluate this hypothe-
sis with two experiments with cruiser,
a DS for in-car or mobile users to access
restaurant information. First, we compare
cruiser with a baseline system-initiative
DS, and show that users prefer cruiser.
Then, we experiment with four algorithms
for constructing intensional summaries in
cruiser, and show that two summary
types are equally effective: summaries that
maximize domain coverage and summaries
that maximize utility with respect to a
user model.
1 Introduction
Research in cooperative response generation has
had a long history and produced many useful
algorithms and observations (Mays 1980; Pol-
lack et al1982; Joshi et al, 1986; Kalita et
al., 1986) inter alia. However, it has had little
impact on the recent commercialization of dia-
logue technologies, particularly within the spo-
ken dialogue community. We believe that this
lack of interest in cooperative response genera-
tion arises from two limitations of previous work:
(1) There has been relatively little empirical re-
search showing that cooperative responses lead
U1: Tell me about restaurants in London.
SI1: What kind of cuisine are you interested in?
C1: I know of 596 restaurants in London. I know
of 3 inexpensive vegetarian restaurants and 14
inexpensive Chinese restaurants. I also know
of 4 inexpensive Greek restaurants.
U2: Chinese.
SI2: Do you have a price range in mind?
C2: I know of 27 restaurants in London that
serve Chinese food. There are 8 inexpen-
sive Chinese restaurants in Chinatown and
2 inexpensive Chinese restaurants in Hamp-
stead/Kilburn. I also know of 1 inexpensive
Chinese restaurant in Soho.
U3: How about a cheap one?
SI3: What neighborhood would you like?
C3: I know of 1 inexpensive Chinese restaurant
in Hampstead/Kilburn with very good food
quality and 1 in Bayswater with good food
quality. I also know of 2 in Chinatown with
medium food quality.
Figure 1: Intensional summaries (C = cruiser) as
compared with a system initiative (SI) strategy in
the London restaurant domain. U = User
to more natural, effective, or efficient dialogues
(Litman et al1998; Demberg and Moore, 2006);
and (2) Previous work has hand-crafted such re-
sponses, or hand-annotated the database to sup-
port them (Kaplan, 1984; Kalita et al, 1986;
Cholvy, 1990; Polifroni et al, 2003; Benamara,
2004), which has made it difficult to port and
scale these algorithms.
Moreover, we believe that there is an even
greater need today for cooperative response gen-
eration. Larger and more complex datasets are
daily being created on the Web, as information
479
is integrated across multiple sites and vendors.
Many users will want to access this information
from a mobile device and will have little knowl-
edge of the domain. We hypothesize that these
users will need cooperative responses that select
and generalize the information provided.
In particular, we hypothesize that a partic-
ular type of cooperative response, intensional
summaries, when provided incrementally dur-
ing a dialogue, are effective for large or com-
plex domains, or when users are unfamiliar
with the domain. These intensional summaries
have the ability to describe the data that forms
the knowledge base of the system, as well as
relationships among the components of that
database. We have implemented intensional
summaries in cruiser (Cooperative Responses
Using Intensional Summaries of Entities and Re-
lations), a DS for in-car or mobile users to access
restaurant information (Becker et al2006; Weng
et al2005; Weng et al2006). Figure 1 contrasts
our proposed intensional summary strategy with
the system initiative strategy used in many di-
alogue systems (Walker et al, 2002; VXML,
2007).
Previous research on cooperative responses
has noted that summary strategies should
vary according to the context (Sparck Jones,
1993), and the interests and preferences of the
user (Gaasterland et al, 1992; Carenini and
Moore, 2000; Demberg and Moore, 2006).
A number of proposals have emphasized the
importance of making generalizations (Kaplan,
1984; Kalita et al, 1986; Joshi et al, 1986).
In this paper we explore different methods for
constructing intensional summaries and inves-
tigate their effectiveness. We present fully
automated algorithms for constructing inten-
sional summaries using knowledge discovery
techniques (Acar, 2005; Lesh and Mitzen-
macher, 2004; Han et al, 1996), and decision-
theoretic user models (Carenini and Moore,
2000).
We first explain in Sec. 2 our fully automated,
domain-independent algorithm for constructing
intensional summaries. Then we evaluate our
intensional summary strategy with two experi-
ments. First, in Sec. 3, we test the hypothesis
that users prefer summary responses in dialogue
systems. We also test a refinement of that hy-
pothesis, i.e., that users prefer summary type
responses when they are unfamiliar with a do-
main. We compare several versions of cruiser
with the system-initiative strategy, exemplified
in Fig. 1, and show that users prefer cruiser.
Then, in Sec. 4, we test four different algo-
rithms for constructing intensional summaries,
and show in Sec. 4.1 that two summary types
are equally effective: summaries that maximize
domain coverage and summaries that maximize
utility with respect to a user model. We also
show in Sec. 4.2 that we can predict with 68%
accuracy which summary type to use, a signifi-
cant improvement over the majority class base-
line of 47%. We sum up in Sec. 5.
2 Intensional Summaries
This section describes algorithms which result in
the four types of intensional summaries shown in
Fig. 2. We first define intensional summaries as
follows. Let D be a domain comprised of a set R
of database records {ri, ...rn}. Each record con-
sists of a set of attributes {Aj , ..., An}, with as-
sociated values v: D(Ai)={vi,1, vi,2, ..., vi,n}. In
a dialogue system, a constraint is a value intro-
duced by a user with either an explicit or implied
associated attribute. A constraint c is a func-
tion over records in D such that cj(R) returns a
record r if r ? D and r : Ai = c. The set of all
dialogue constraints {ci, ..., cn} is the context C
at any point in the dialogue. The set of records
R in D that satisfy C is the focal information:
R is the extension of C in D. For example, the
attribute cuisine in a restaurant domain has val-
ues such as ?French? or ?Italian?. A user utter-
ance instantiating a constraint on cuisine, e.g.,
?I?m interested in Chinese food?, results in a set
of records for restaurants serving Chinese food.
Intensional summaries as shown in Fig. 2 are
descriptions of the focal information, that high-
light particular subsets of the focal information
and make generalizations over these subsets.
The algorithm for constructing intensional
summaries takes as input the focal information
R, and consists of the following steps:
? Rank attributes in context C, using one of two
ranking methods (Sec. 2.1);
480
Type Ranking #atts Clusters Scoring Summary
Ref-
Sing
Refiner 3 Single
value
Size I know of 35 restaurants in London serving Indian
food. All price ranges are represented. Some of the
neighborhoods represented are Mayfair, Soho, and
Chelsea. Some of the nearby tube stations are Green
Park, South Kensington and Piccadilly Circus.
Ref-
Assoc
Refiner 2 Associative Size I know of 35 restaurants in London serving Indian
food. There are 3 medium-priced restaurants in May-
fair and 3 inexpensive ones in Soho. There are also
2 expensive ones in Chelsea.
UM-
Sing
User
model
3 Single
value
Utility I know of 35 restaurants in London serving Indian
food. There are 6 with good food quality. There are
also 12 inexpensive restaurants and 4 with good ser-
vice quality.
UM-
Assoc
User
model
2 Associative Utility I know of 35 restaurants in London serving Indian
food. There are 4 medium-priced restaurants with
good food quality and 10 with medium food quality.
There are also 4 that are inexpensive but have poor
food quality.
Figure 2: Four intensional summary types for a task specifying restaurants with Indian cuisine in London.
? Select top-N attributes and construct clusters
using selected attributes (Sec. 2.2);
? Score and select top-N clusters (Sec. 2.3);
? Construct frames for generation, perform aggre-
gation and generate responses.
2.1 Attribute Ranking
We explore two candidates for attribute ranking:
User model and Refiner.
User model: The first algorithm utilizes
decision-theoretic user models to provide an at-
tribute ranking specific to each user (Carenini
and Moore, 2000). The database contains 596
restaurants in London, with up to 19 attributes
and their values. To utilize a user model, we
first elicit user ranked preferences for domain
attributes. Attributes that are unique across
all entities, or missing for many entities, are
automatically excluded, leaving six attributes:
cuisine, decor quality, food quality, price, ser-
vice, and neighborhood. These are ranked using
the SMARTER procedure (Edwards and Bar-
ron, 1994). Rankings are converted to weights
(w) for each attribute, with a formula which
guarantees that the weights sum to 1:
wk =
1
K
K?
i=k
1
i
where K equals the number of attributes in the
ranking. The absolute rankings are used to se-
lect attributes. The weights are also used for
cluster scoring in Sec. 2.3. User model ranking
is used to produce UM-Sing and UM-Assoc
in Fig. 2.
Refiner method: The second attribute
ranking method is based on the Refiner algo-
rithm for summary construction (Polifroni et al,
2003). The Refiner returns values for every at-
tribute in the focal information in frames or-
dered by frequency. If the counts for the top-N
(typically, 4) values for a particular attribute,
e.g., cuisine, exceeded M% (typically 80%) of
the total counts for all values, then that at-
tribute is selected. For example, 82% of In-
dian restaurants in the London database are in
the neighborhoods Mayfair, Soho, and Chelsea.
Neighborhood would, therefore, be chosen as an
attribute to speak about for Indian restaurants.
The thresholds M and N in the original Refiner
were set a priori, so it was possible that no at-
tribute met or exceeded the thresholds for a par-
ticular subset of the data. In addition, some en-
tities could have many unknown values for some
attributes.
Thus, to insure that all user queries result in
some summary response, we modify the Refiner
481
method to include a ranking function for at-
tributes. This function favors attributes that
contain fewer unknown values but always re-
turns a ranked set of attributes. Refiner ranking
is used to produce Ref-Sing and Ref-Assoc in
Fig. 2.
2.2 Subset Clustering
Because the focal information is typically too
large to be enumerated, a second parameter at-
tempts to find interesting clusters representing
subsets of the focal information to use for the
content of intensional summaries. We assume
that the coverage of the summary is important,
i.e., the larger the cluster, the more general the
summary.
The simplest algorithm for producing clusters
utilizes a specified number of the top-ranked at-
tributes to define a cluster. Single attributes,
as in the Ref-Sing and UM-Sing examples in
Fig. 2, typically produce large clusters. Thus
one algorithm uses the top three attributes to
produce clusters, defined by either a single value
(e.g., UM-Sing) or by the set of values that
comprise a significant portion of the total (e.g.,
Ref-Sing).
price_range
medium inexpensive
food_quality food_quality
good medium poor(4) (10) (4)
Figure 3: A partial tree for Indian restaurants in
London, using price range as the predictor variable
and food quality as the dependent variable. The
numbers in parentheses are the size of the clusters
described by the path from the root.
However, we hypothesize that more informa-
tive and useful intensional summaries might be
constructed from clusters of discovered associ-
ations between attributes. For example, as-
sociations between price and cuisine produce
summaries such as There are 49 medium-priced
restaurants that serve Italian cuisine. We apply
c4.5 decision tree induction to compute associ-
ations among attributes (Kamber et al, 1997;
Quinlan, 1993). Each attribute in turn is desig-
nated as the dependent variable, with other at-
tributes used as predictors. Thus, each branch
in the tree represents a cluster described by the
attribute/value pairs that predict the leaf node.
Fig. 3 shows clusters of different sizes induced
from Indian restaurants in London. The cluster
size is determined by the number of attributes
used in tree induction. With two attributes, the
average cluster size at the leaf node is 60.4, but
drops to 4.2 with three attributes. Thus, we use
two attributes to produce associative clusters, as
shown in Fig. 2 (i.e., the Ref-Assoc and UM-
Assoc responses), to favor larger clusters.
2.3 Cluster Scoring
The final parameter scores the clusters. One
scoring metric is based on cluster size. Single
attributes produce large clusters, while associa-
tion rules produce smaller clusters.
The second scoring method selects clusters
of high utility according to a user model. We
first assign scalar values to the six ranked at-
tributes (Sec. 2.1), using clustering methods as
described in (Polifroni et al, 2003) The weights
from the user model and the scalar values for
the attributes in the user model yield an overall
utility U for a cluster h, similar to utilities as
calculated for individual entities (Edwards and
Barron, 1994; Carenini and Moore, 2000):
Uh =
K?
k=1
wk(xhk)
We use cluster size scoring with Refiner rank-
ing and utility scoring with user model ranking.
For conciseness, all intensional summaries are
based on the three highest scoring clusters.
2.4 Summary
The algorithms for attribute selection and clus-
ter generation and scoring yield the four sum-
mary types in Table 2. Summary Ref-Sing is
constructed using (1) the Refiner attribute rank-
ing; and (2) no association rules. (The quanti-
fier (e.g., some, many) is based on the cover-
482
age.) Summary Ref-Assoc is constructed us-
ing (1) the Refiner attribute ranking; and (2)
association rules for clustering. Summary UM-
Sing is constructed using (1) a user model with
ranking as above; and (2) no association rules.
Summary UM-Assoc is constructed using (1) a
user model with ranking of price, food, cuisine,
location, service, and decor; and (2) association
rules.
3 Experiment One
This experiment asks whether subjects prefer
intensional summaries to a baseline system-
initiative strategy. We compare two types of in-
tensional summary responses from Fig. 2, Ref-
Assoc and UM-Assoc to system-initiative.
The 16 experimental subjects are asked to as-
sume three personas, in random order, chosen to
typify a range of user types, as in (Demberg and
Moore, 2006). Subjects were asked to read the
descriptions of each persona, which were avail-
able for reference, via a link, throughout the ex-
periment.
The first persona is the Londoner, represent-
ing someone who knows London and its restau-
rants quite well. The Londoner persona typi-
cally knows the specific information s/he is look-
ing for. We predict that the system-initiative
strategy in Fig. 1 will be preferred by this per-
sona, since our hypothesis is that users prefer
intensional summaries when they are unfamiliar
with the domain.
The second persona is the Generic tourist
(GT), who doesn?t know London well and does
not have strong preferences when it comes to
selecting a restaurant. The GT may want to
browse the domain, i.e. to learn about the struc-
ture of the domain and retrieve information by
recognition rather than specification (Belkin et
al., 1994). We hypothesize that the Ref-Assoc
strategy in Fig. 2 will best fit the GT, since the
corresponding clusters have good domain cover-
age.
The third persona is the UM tourist (UMT).
This persona may also want to browse the
database, since they are unfamiliar with Lon-
don. However, this user has expressed prefer-
ences about restaurants through a previous in-
teraction. The UMT in our experiment is con-
cerned with price and food quality (in that or-
der), and prefers restaurants in Central London.
After location, the UMT is most concerned with
cuisine type. The intensional summary labelled
Um-Assoc in Fig. 2 is based on this user model,
and is computed from discovered associations
among preferred attributes.
As each persona, subjects rate responses on
a Likert scale from 1-7, for each of four dia-
logues, each containing between three and four
query/response pairs. We do not allow tie votes
among the three choices.
3.1 Experimental results
The primary hypothesis of this work is that
users prefer summary responses in dialogue sys-
tems, without reference to the context. To test
this hypothesis, we first compare Londoner re-
sponses (average rating 4.64) to the most highly
rated of the two intensional summaries (average
rating 5.29) for each query/response pair. This
difference is significant (df = 263, p < .0001),
confirming that over users prefer an intensional
summary strategy to a system-initiative strat-
egy.
Table 1 shows ratings as a function of persona
and response type. Overall, subjects preferred
the responses tailored to their persona. The
Londoner persona signifcantly preferred Lon-
doner over UMT responses (df = 95, p < .05),
but not more than GT responses. This con-
firms our hypothesis that users prefer incremen-
tal summaries in dialogue systems. Further,
it disconfirms our refinement of that hypothe-
sis, that users prefer summaries only when they
are unfamiliar with the domain. The fact that
no difference was found between Londoner and
GT responses indicates that GT responses con-
tain information that is perceived as useful even
when users are familiar with the domain.
The Generic Tourist persona also preferred
the GT responses, significantly more than the
Londoner responses (df = 95, p < .05), but
not significantly more than the UMT responses.
We had hypothesized that the optimal summary
type for users completely new to a domain would
describe attributes that have high coverage of
the focal information. This hypothesis is discon-
firmed by these findings, that indicate that user
483
Response Type
Persona London GT UMT
London 5.02 4.55 4.32
GT 4.14 4.67 4.39
UM tourist 3.68 4.86 5.23
Table 1: Ratings by persona assumed. London =
Londoner persona, GT = Generic tourist, UMT =
User Model tourist
model information is helpful when constructing
summaries for any user interested in browsing.
Finally, the UM Tourist persona overwhelm-
ingly preferred UMT responses over Londoner
responses (df = 95, p < .0001). However, UMT
responses were not significantly preferred to GT
responses. This confirms our hypothesis that
users prefer summary responses when they are
unfamiliar with the domain, but disconfirms
the hypothesis that users will prefer summaries
based on a user model. The results for both the
Generic Tourist and the UM Tourist show that
both types of intensional summaries contain use-
ful information.
4 Experiment Two
The first experiment shows that users prefer in-
tensional summaries; the purpose of the sec-
ond experiment is to investigate what makes a
good intensional summary. We test the different
ways of constructing such summaries described
in Sec. 2, and illustrated in Fig. 2.
Experimental subjects were 18 students whose
user models were collected as described in
Sec. 2.3. For each user, the four summary types
were constructed for eight tasks in the London
restaurant domain, where a task is defined by a
query instantiating a particular attribute/value
combination in the domain (e.g., I?m interested
in restaurants in Soho). The tasks were selected
to utilize a range of attributes. The focal in-
formation for four of the tasks (large set tasks)
were larger than 100 entities, while the focal in-
formation for the other four tasks were smaller
than 100 entities (small set tasks). Each task
was presented to the subject on its own web
page with the four intensional summaries pre-
sented as text on the web page. Each subject
was asked to carefully read and rate each al-
User model Refiner
Association rules 3.4 2.9
Single attributes 3.0 3.4
User model Refiner
Small dataset 3.1 3.4
Large dataset 3.2 2.9
Table 2: User ratings showing the interaction be-
tween clustering method, attribute ranking, and
dataset size in summaries.
ternative summary response on a Likert scale
of 1 . . . 5 in response to the statement, This re-
sponse contains information I would find useful
when choosing a restaurant. The subjects were
also asked to indicate which response they con-
sidered the best and the worst, and to provide
free-text comments about each response.
4.1 Hypothesis Testing Results
We performed an analysis of variance with at-
tribute ranking (user model vs. refiner), clus-
tering method (association rules vs. single at-
tributes), and set size (large vs. small) as in-
dependent variables and user ratings as the de-
pendent variable. There was a main effect for set
size (df = 1, f = 6.7, p < .01), with summaries
describing small datasets (3.3 average rating)
rated higher than those for large datasets (3.1
average rating).
There was also a significant interaction be-
tween attribute ranking and clustering method
(df = 1, f = 26.8, p < .001). Table 2 shows
ratings for the four summary types. There are
no differences between the two highest rated
summaries: Ref-Sing (average 3.4) and UM-
Assoc (average 3.4). See Fig. 2. This suggests
that discovered associations provide useful con-
tent for intensional summaries, but only for at-
tributes ranked highly by the user model.
In addition, there was another significant in-
teraction between ranking method and setsize
(df = 1, f = 11.7, p < .001). The ratings at the
bottom of Table 2 shows that overall, users rate
summaries of small datasets higher, but users
rate summaries higher for large datasets when a
user model is used. With small datasets, users
prefer summaries that don?t utilize user model
information.
484
We also calculate the average utility for each
response (Sec. 2.1) and find a strong correlation
between the rating and its utility (p < .005).
When considering this correlation, it is impor-
tant to remember that utility can be calculated
for all responses, and there are cases where the
Refiner responses have high utility scores.
4.2 Summary Type Prediction
Our experimental data suggest that characteris-
tics associated with the set of restaurants being
described are important, as well as utility in-
formation derived from application of a a user
model. The performance of a classifier in pre-
dicting summary type will indicate if trends we
discovered among user judgements carry over to
an automated means of selecting which response
type to use in a given context.
In a final experiment, for each task, we use the
highest rated summary as a class to be predicted
using C4.5 (Quinlan, 1993). Thus we have 4
classes: Ref-Sing, Ref-Assoc, UM-Sing, and
UM-Assoc. We derive two types of feature sets
from the responses: features derived from each
user model and features derived from attributes
of the query/response pair itself. The five fea-
ture sets for the user model are:
? umInfo: 6 features for the rankings for each at-
tribute for each user?s model, e.g. a summary
whose user had rated food quality most highly
would receive a ?5? for the feature food quality;
? avgUtility: 4 features representing an average
utility score for each alternative summary re-
sponse, based on its clusters (Sec. 2.3).
? hiUtility: 4 features representing the highest
utility score among the three clusters selected
for each response;
? loUtility: 4 features representing the lowest util-
ity score among the three clusters selected for
each response;
? allUtility: 12 features consisting of the high,
low, and average utility scores from the previous
three feature sets.
Three feature sets are derived from the query
and response pair:
? numRests: 4 features for the coverage of each
response. For summary Ref-Assoc in Ta-
ble 2, numRests is 43; for summary UM-
Assoc, numrests is 53.;
Sys Feature Sets Acc(%)
S1 allUtility 47.1
S2 task, numRests 51.5
S3 allUtility,umInfo 62.3?
S4 allUtility,umInfo,numRests,task 63.2?
S5 avgUtility,umInfo,numRests,task 62.5?
S6 hiUtility,umInfo,numRests,task 66.9?
S7 hiUtility,umInfo,numRests,task,dataset68.4?
S8 loUtility,umInfo,numRests,task 60.3?
S9 hiUtility,umInfo 64.0?
Table 3: Accuracy of feature sets for predicting pre-
ferred summary type. ? = p < .05 as compared to
the Baseline (S1)).
? task: A feature for the type of constraint used
to generate the focal information (e.g., cuisine,
price range).
? dataset: A feature for the size of the focal in-
formation subset (i.e., big, small), for values
greater and less than 100.
Table 3 shows the relative strengths of the two
types of features on classification accuracy. The
majority class baseline (System S1) is 47.1%.
The S2 system uses only features associated
with the query/response pair, and its accuracy
(51.5%) is not significantly higher than the base-
line. User model features perform better than
the baseline (S3 in Table 3), and combining
features from the query/response pair and the
user model significantly increases accuracy in all
cases. We experimented with using all the utility
scores (S4), as well as with using just the aver-
age (S5), the high (S6), and the low (S8). The
best performance (68.4%)is for the (S7) system
combination of features.
The classification rules in Table 4 for the best
system (S7) suggests some bases for users? deci-
sions. The first rule is very simple, simply stat-
ing that, if the highest utility value of the Ref-
Sing response is lower than a particular thresh-
old, then use the UM-Assoc response. In other
words, if one of the two highest scoring response
types has a low utility, use the other.
The second rule in Table 4 shows the effect
that the number of restaurants in the response
has on summary choice. In this rule, the Ref-
Sing response is preferred when the highest util-
485
IF (HighestUtility: Ref-Sing) < 0.18
THEN USE UM-Assoc
----------------------------------------
IF (HighestUtility: Ref-Assoc) > 0.18) &&
(NumRestaurants: UM-Assoc < 400) &&
(HighestUtility: UM-Assoc < .47)
THEN USE Ref-Sing
----------------------------------------
IF (NumRestaurants: UM-Assoc < 400) &&
(HighestUtility: UM-Assoc < .57) &&
(HighestUtility: Ref-Assoc > .2)
THEN USE Ref-Assoc
Table 4: Example classification rules from System 7
in Table 3.
ity value of that response is over a particular
threshold.
The final rule in Table 4 predicts Ref-Assoc,
the lowest overall scoring response type. When
the number of restaurants accounted for by
UM-Assoc, as well as the highest utility for
that response, are both below a certain thresh-
old, and the highest utility for the Ref-Assoc
response is above a certain threshold, then use
Ref-Assoc. The utility for any summary type
using the Refiner method is usually lower than
those using the user model, since overall utility is
not taken into account in summary construction.
However, even low utility summaries may men-
tion attributes the user finds important. That,
combined with higher coverage, could make that
summary type preferable over one constructed
to maximize user model utility.
5 Conclusion
We first compared intensional summary coop-
erative responses against a system initiative di-
alogue strategy in cruiser. Subjects assumed
three ?personas?, a native Londoner, a tourist
who was interacting with the system for the first
time (GT), or a tourist for which the system
has a user model (UMT). The personas were
designed to reflect differing ends of the spectra
defined by Belkin to characterize information-
seeking strategies (Belkin et al, 1994). There
was a significant preference for intensional sum-
maries across all personas, but especially when
the personas were unfamiliar with the domain.
This preference indicates that the benefits of
intensional summaries outweigh the increase in
verbosity.
We then tested four algorithms for summary
construction. Results show that intensional
summaries based on a user model with associa-
tion rules, or on the Refiner method (Polifroni et
al., 2003), are equally effective. While (Dem-
berg and Moore, 2006) found that their user
model stepwise refinement (UMSR) method was
superior to the Refiner method, they also found
many situations (70 out of 190) in which the
Refiner method was preferred. Our experiment
was structured differently, but it suggests that,
in certain circumstances, or within certain do-
mains, users may wish to hear about choices
based on an analysis of focal information, irre-
spective of user preferences.
Our intensional summary algorithms auto-
matically construct summaries from a database,
along with user models collected via a domain-
independent method; thus we believe that
the methods described here are domain-
independent. Furthermore, in tests to deter-
mine whether a classifier can predict the best
summary type to use in a given context, we
achieved an accuracy of 68% as compared to a
majority class baseline of 47%, using dialogue
context features. Both of these results point
hopefully towards a different way of automating
dialogue design, one based on a combination of
user modelling and an analysis of contextual in-
formation. In future work we hope to test these
algorithms in other domains, and show that in-
tensional summaries can not only be automati-
cally derived but also lead to reduced task times
and increased task success.
References
A.C. Acar and A. Motro. 2005. Intensional Encapsu-
lations of Database Subsets via Genetic Program-
ming. Proc, 16th Int. Conf. on Database and Ex-
pert Systems Applications. Copenhagen.
Tilman Becker, Nate Blaylock, Ciprian Gersten-
berger, Ivana Kruijff-Korbayova?, Andreas Ko-
rthauer, Manfred Pinkal, Michael Pitz, Peter
Poller, and Jan Schehl. Natural and intuitive mul-
timodal dialogue for in-car applications: The sam-
mie system. In ECAI, pages 612?616, 2006.
486
N. J. Belkin, C. Cool, A. Stein and U. Thiel. 1994.
Cases, Scripts, and Information Seeking Strate-
gies: On the Design of Interactive Information Re-
trieval Systems. Expert Systems and Applications,
9(3):379?395.
F. Benamara. 2004. Generating Intensional Answers
in Intelligent Question Answering Systems. Proc.
3rd Int. Conf. on Natural Language Generation
INLG.
G. Carenini and J. Moore. 2000. A Strategy for Gen-
erating Evaluative Arguments. Proc. First Int?l
Conf. on Natural Language Generation. 1307?
1314.
Brant Cheikes and Bonnie Webber. Elements of a
computational model of cooperative response gen-
eration. In Proc. Speech and Natural Language
Workshop, pages 216?220, Philadelphia, 1989.
X. Chen and Y-F. Wu. 2006. Personalized Knowl-
edge Discovery: Mining Novel Association Rules
from Text. Proc., SIAM Conference on Data Min-
ing.
L. Cholvy. 1990. Answering Queries Addressed
to a Rule Base. Revue d?Intelligence Artificielle.
1(1):79?98.
V. Demberg and J. Moore. 2006 Information Pre-
sentation in Spoken Dialogue Systems. Proc. 11th
Conf. EACL..
W. Edwards and F. Hutton Barron. 1994. Smarts
and smarter: Improved simple methods for mul-
tiattribute utility measurement. Organizational
Behavior and Human Decision Processes. 60:306?
325.
T. Gaasterland and P. Godfrey and J. Minker. 1992.
An Overview of Cooperative Answering. Journal
of Intelligent Information Systems. 1(2):387?416.
J. Han, Y. Huang and N. Cercone. 1996. Intelligent
Query Answering by Knowledge Discovery Tech-
niques. IEEE Transactions on Knowledge and
Data Engineering. 8(3):373?390.
Aravind Joshi, Bonnie Webber, and Ralph M.
Weischedel. Living up to expectations: computing
expert responses. In HLT ?86: Proceedings of the
workshop on Strategic computing natural language,
pages 179?189, Morristown, NJ, USA, 1986. Asso-
ciation for Computational Linguistics.
J. Kalita and M.J. Colburn and G. McCalla. 1984.
A response to the need for summary responses.
COLING-84). 432?436.
M. Kamber, L. Winstone, W. Gong, S. Cheng and
J Han. 1997. Generalization and decision tree
induction: efficient classification in data mining.
Proc. 7th Int. Workshop on Research Issues in
Data Engineering (RIDE ?97). 111?121.
S.J.Kaplan. 1984. Designing a Portable Natural
Language Database Query System. ACM Trans-
actions on Database Systems, 9(1):1?19.
N. Lesh and M. Mitzenmacher. Interactive data
summarization: an example application. Proc.,
Working Conference on Advanced Visual Inter-
faces. Gallipoli, Italy. pages 183?187.
Diane J. Litman, Shimei Pan, and Marilyn A.
Walker. Evaluating response strategies in a web-
based spoken dialogue agent. In COLING-ACL,
pages 780?786, 1998.
J. Polifroni, G. Chung, and S. Seneff. 2003. Towards
the Automatic Generation of Mixed-Initiative Di-
alogue Systems from Web Content. Proc. Eu-
rospeech. 2721?2724.
E. Mays. Correcting misconceptions about database
structure. In Proceedings of the CSCSI ?80, 1980.
Martha E. Pollack, Julia Hirschberg, and Bonnie L.
Webber. User participation in the reasoning pro-
cesses of expert systems. In AAAI, pages 358?361,
1982.
J.R. Quinlan 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann. San Mateo, CA.
K. Sparck Jones. 1998. Automatic summarising:
factors and directions. I. Mani and M. Maybury,
eds. Advances in Automatic Text Summarization.
MIT Press.
M. Walker, A. Rudnicky, J. Aberdeen, E. Bratt, J.
Garofolo, H. Hastie, A. Le, B. Pellom, A. Potami-
anos, R. Passonneau, R. Prasad, S. Roukos, G.
Sanders, S. Seneff and D. Stallard. 2002. DARPA
Communicator Evaluation: Progress from 2000 to
2001. Proc, ICSLP 2002.
F. Weng, L. Cavedon, B. Raghunathan, D. Mirkovic,
H. Cheng, H. Schmidt, H. Bratt, R. Mishra,
S. Peters, L. Zhao, S. Upson, E. Shriberg, and
C. Bergmann. Developing a conversational dia-
logue system for cognitively overloaded drivers. In
Proceedings, International Congress on Intelligent
Transportation Systems, 2005.
F. Weng, S. Varges, B. Raghunathan, F. Ratiu,
H. Pon-Barry, B. Lathrop, Q. Zhang, T. Schei-
deck, H. Bratt, K. Xu, M. Purver, R. Mishra,
M. Raya, S. Peters, Y. Meng, L. Cavedon, and
L. Shriberg. Chat: A conversational helper for
automotive tasks. In Proceedings, Interspeech: In-
ternational Conference on Spoken Language Pro-
cessing, 2006.
Voxeo. VoiceXML Development Guide.
http://voicexml.org.
487
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 369?379,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Unsupervised Induction of Contingent Event Pairs from Film Scenes
Zhichao Hu, Elahe Rahimtoroghi, Larissa Munishkina, Reid Swanson and Marilyn A. Walker
Natural Language and Dialogue Systems Lab
Department of Computer Science, University of California, Santa Cruz
Santa Cruz, CA, 95064
{zhu, elahe, mlarissa, reid, maw}@soe.ucsc.edu
Abstract
Human engagement in narrative is partially
driven by reasoning about discourse relations
between narrative events, and the expectations
about what is likely to happen next that results
from such reasoning. Researchers in NLP
have tackled modeling such expectations from
a range of perspectives, including treating it as
the inference of the CONTINGENT discourse
relation, or as a type of common-sense causal
reasoning. Our approach is to model likeli-
hood between events by drawing on several of
these lines of previous work. We implement
and evaluate different unsupervised methods
for learning event pairs that are likely to be
CONTINGENT on one another. We refine event
pairs that we learn from a corpus of film scene
descriptions utilizing web search counts, and
evaluate our results by collecting human judg-
ments of contingency. Our results indicate that
the use of web search counts increases the av-
erage accuracy of our best method to 85.64%
over a baseline of 50%, as compared to an av-
erage accuracy of 75.15% without web search.
1 Introduction
Human engagement in narrative is partially driven
by reasoning about discourse relations between nar-
rative events, and the expectations about what is
likely to happen next that results from such reason-
ing (Gerrig, 1993; Graesser et al, 1994; Lehnert,
1981; Goyal et al, 2010). Thus discourse relations
are one of the primary means to structure narrative
in genres as diverse as weblogs, search queries, sto-
ries, film scripts and news articles (Chambers and
Jurafsky, 2009; Manshadi et al, 2008; Gordon and
Swanson, 2009; Gordon et al, 2011; Beamer and
Girju, 2009; Riaz and Girju, 2010; Do et al, 2011).
DOUGLAS QUAIL and his wife KRISTEN, are
asleep in bed.
Gradually the room lights brighten. the clock chimes
and begins speaking in a soft, feminine voice.
They don?t budge. Shortly, the clock chimes again.
Quail?s wife stirs. Maddeningly, the clock chimes a
third time.
CLOCK (continuing)Tick, tock ?.
Quail reaches out and shuts the clock off. Then he sits
up in bed.
He swings his legs out from under the covers and sits
on the edge of the bed. He puts on his glasses and sits,
lost in thought.
He is a good-looking but conventional man in his early
thirties. He seems rather in awe of his wife, who is
attractive and rather off-hand towards him.
Kirsten pulls on her robe, lights a cigarette, sits fishing
for her slippers.
Figure 1: Opening Scene from Total Recall
Recent work in NLP has tackled the inference of
relations between events from a broad range of per-
spectives: (1) as inference of a discourse relations
(e.g. the Penn Discourse Treebank (PDTB) CON-
TINGENT relation and its specializations); (2) as a
type of common sense reasoning; (3) as part of text
understanding to support question-answering; and
(4) as way of learning script-like or plot-like knowl-
edge structures. All these lines of work aim to model
narrative understanding, i.e. to enable systems to in-
fer which events are likely to have happened even
though they have not been mentioned in the text
(Schank et al, 1977), and which events are likely
to happen in the future. Such knowledge has prac-
tical applications in commonsense reasoning, infor-
369
mation retrieval, question answering, narrative un-
derstanding and inferring discourse relations.
We model this likelihood between events by
drawing on the PTDB?s general definition of the
CONTINGENT relation, which encapsulates relations
elsewhere called CAUSE, CONDITION and ENABLE-
MENT (Prasad et al, 2008a; Lin et al, 2010; Pitler et
al., 2009; Louis et al, 2010). Our aim in this paper
is to implement and evaluate a range of different un-
supervised methods for learning event pairs that are
likely to be CONTINGENT on one another.
We first utilize a corpus of scene descriptions
from films because they are guaranteed to have an
explicit narrative structure. Moreover, screenplay
scene descriptions tend to be told in temporal or-
der (Beamer and Girju, 2009; Gordon and Swan-
son, 2009), which makes them a good resource for
learning about contingencies between events. In
addition, scenes in film represent many typical se-
quences from real life, while providing a rich source
of event clusters related to battles, love and mys-
tery. We carry out separate experiments for the ac-
tion movie genre and the romance movie genre. For
example, in the scene from Total Recall, from the
action movie genre (See Fig. 1), we might learn that
the event of sits up is CONTINGENT on the event
of clock chimes. The subset of the corpus we
use comprises 123,869 total unique event pairs.
We produce initial scalar estimates of poten-
tial CONTINGENCY between events using four
previously defined measures of distributional co-
occurrence. We then refine these estimates through
web searches that explicitly model the patterns of
narrative event sequences that were previously ob-
served to be likely within a particular genre. There
are several advantages of this method: (1) events in
the same genre tend to be more similar than events
across genres, so less data is needed to estimate
co-occurrence; (2) film scenes are typically nar-
rated via simple tenses in the correct temporal order,
which allows the ordering of events to contribute to
the estimation of the CONTINGENCY relation; (3)
The web counts focus on validating event pairs al-
ready deemed to be likely to be CONTINGENT in
the smaller, more controlled, film scene corpus. To
test our method, we conduct perceptual experiments
with human subjects on Mechanical Turk by asking
them to select which of two pairs of events are the
most likely. For example, given the scene from To-
tal Recall in Fig. 1, Mechanical Turkers are asked
to select whether the sequential event pair clock
chimes, sits up is more likely than clock
chimes followed by a randomly selected event
from the action film genre. Our experimental data
and annotations are available at http://nlds.
soe.ucsc.edu/data/EventPairs.
Sec. 2 describes our experimental method in de-
tail. Sec. 3 describes how we set up our evaluation
experiments and the results. We show that none of
the methods from previous work perform better on
our data than 75.15% average accuracy as measured
by human perceptions of CONTINGENCY. But after
web search refinement, we achieve an average accu-
racy of 85.64%. We delay a more detailed compari-
son to previous work to Sec. 4 where we summarize
our results and compare previous work to our own.
2 Experimental Method
Our method uses a combination of estimating
the likelihood of a CONTINGENT relation between
events in a corpus of film scenes (Walker et al,
2012b), with estimates then revised through web
search. Our experiments are based on two sub-
sets of 862 film screen plays collected from the
IMSDb website using its ontology of film genres
(Walker et al, 2012b): a set of action movies of 115
screenplays totalling 748 MB, and a set of romance
movies of 71 screenplays totalling 390 MB. Fig. 1
provided an example scene from the action movie
genre from the IMSDb corpus.
We assume that the relation we are aiming to learn
is the PDTB CONTINGENT relation, which is de-
fined as a relation that exists when one of the sit-
uations described in the text spans that are identi-
fied as the two arguments of the relation, i.e. Arg1
and Arg2, causally influences the other (Prasad et
al., 2008b). As Girju notes, it is notoriously dif-
ficult to define causality without making the defi-
nition circular, but we follow Beamer and Girju?s
work in assuming that if events A, B are causally
related then B should occur less frequently when it
is not preceded by A and that B?A should be much
less frequent than A? B. We assume that both the
CAUSE and CONDITION subtypes of the CONTIN-
GENCY relation will result in pairs of events that are
likely to occur together and in a particular order. In
particular we assume that the subtypes of the PDTB
taxonomy of Contingency.Cause.Reason and Con-
tingency.Cause.Result are the most likely to occur
together as noted in previous work. Other related
370
work has made use of discourse connectives or dis-
course taggers (implicit discourse relations) to pro-
vide additional evidence of CONTINGENCY (Do et
al., 2011; Gordon et al, 2011; Chiarcos, 2012; Pitler
et al, 2009; Lin et al, 2010), but we do not because
the results have been mixed. In particular these dis-
course taggers are trained on The Wall Street Journal
(WSJ) and are unlikely to work well on our data.
We define an event as a verb lemma with its sub-
ject and object. Two events are considered equal if
they have the same verb. We do not believe word
ambiguities to be a primary concern, and previous
work also defines events to be the same if they have
the same surface verb, in some cases with a restric-
tion that the dependency relations should also be
the same (Chambers and Jurafsky, 2008; Chambers
and Jurafsky, 2009; Do et al, 2011; Riaz and Girju,
2010; Manshadi et al, 2008). Word sense ambigu-
ities are also reduced in specific genres (Action and
Romance) of film scenes.
Our method for estimating the likelihood of a
CONTINGENT relations between events consists of
four steps:
1. TEXT PROCESSING: We use Stanford
CoreNLP to annotate the corpus docu-
ment by document and store the annotated text
in XML format (Sec. 2.1);
2. COMPUTE EVENT REPRESENTATIONS: We
form intermediate artifacts such as events, pro-
tagonists and event pairs from the annotated
documents. Each event has its arguments (sub-
ject and object). We calculate the frequency of
the event across the relevant genre (Sec. 2.2);
3. CALCULATE CONTINGENCY MEASURES: We
define 4 different measures of contingency and
calculate each one separately using the results
from Steps 1 and 2 above. We call each re-
sult a PREDICTED CONTINGENT EVENT PAIR
(PCEP). All measures return scalar values that
we use to rank the PCEPs (Sec. 2.3);
4. WEB SEARCH REFINEMENT: We select the top
100 event pairs calculated by each contingency
measure, and construct a RANDOM EVENT
PAIR (REP) for each PCEP that preserves the
first element of the PCEP, and replaces the sec-
ond element with another event selected ran-
domly from within the same genre. We then
define web search patterns for both PCEP and
REPs and compare the counts (Sec. 2.4).
2.1 Text Processing
We first separate our screen plays into two sets of
documents, one for the action genre and one for the
romance genre. Because we are interested in the
event descriptions that are part of the scene descrip-
tions, we excise the dialog from each screen play.
Then using the Stanford CoreNLP pipeline, we an-
notate the film scene files. Annotations include tok-
enization, lemmatization, named entity recognition,
parsing and coreference resolution.
We extract the events by keeping all tokens whose
POS tags begin with VB. We then use the depen-
dency parse to find the subject and object of each
verb (if any), considering only nsubj, agent,
dobj, iobj, nsubjpass. We keep the orig-
inal tokens of the subject and the object for further
processing.
2.2 Compute Event Representations
Given the results of Step 1 we start by generalizing
the subject and object stored with each event by sub-
stituting tokens with named entities if there are any
named entities tagged. Otherwise we generalize the
subjects and the objects using their lemmas. For ex-
ample, person UNLOCK door, as illustrated in
Table 1.
We then integrate all the subjects and objects
across all film scene files, keeping a record of the
frequency of each subject and object. For example,
[person (115), organization (14),
door (3)] UNLOCK [door (127),
person (5), bars (2)]. The most frequent
subject and object are selected as representative ar-
guments for the event. We then count the frequency
of each event across all the film scene files.
Within each film scene file, we count adjacent
events as potential CONTINGENT event pairs. Two
event pairs are defined as equal if they have the same
verbs in the same order. We also count the frequency
of each event pair.
2.3 Calculate Contingency Measures
We calculate four different measures of CONTIN-
GENCY based on previous work using the results
of Steps 1 and 2 (Sec. 2.1 and Sec. 2.2). These
measures are pointwise mutual information, causal
potential, bigram probability and protagonist-based
371
causal potential as described in detail below. We
calculate each measure separately by genre for the
action and romance genres of the film corpus.
Pointwise Mutual Information. The majority of
related work uses pointwise mutual information
(PMI) in some form or another (Chambers and Ju-
rafsky, 2008; Chambers and Jurafsky, 2009; Riaz
and Girju, 2010; Do et al, 2011). Given a set of
events (a verb and its collected set of subjects and
objects), we calculate the PMI using the standard
definition:
pmi(e1, e2) = log
P (e1, e2)
P (e1)P (e2)
(1)
in which e1 and e2 are two events. P (e1) is the
probability that event e1 occur in the corpus:
P (e1) =
count(e1)
?
x count(ex)
(2)
where count(e1) is the count of how many times
event e1 occurs in the corpus, and
?
x count(ex) is
the count of all the events in the corpus. The nu-
merator is the probability that the two events occur
together in the corpus:
P (e1, e2) =
count(e1, e2)
?
x
?
y count(ex, ey)
(3)
in which count(e1, e2) is the number of times the
two events e1 and e2 occur together in the corpus
regardless of their order. Only adjacent events in
each document are paired up. PMI is a symmetric
measurement for the relationship between two
events. The order of the events does not matter.
Causal Potential. Beamer and Girju proposed a
measure called Causal Potential (CP) based on pre-
vious work in philosophy and logic, along with an
annotation test for causality. An annotator decid-
ing whether event A causes event B asks herself the
following questions, where answering yes to both
means the two events are causally related:
? Does event A occur before (or simultaneously)
with event B?
? Keeping constant as many other states of affairs
of the world in the given text context as possi-
ble, does modifying event A entail predictably
modifying event B?
As Beamer & Girju note, this annotation test is
objective, and it is simple to execute mentally. It
only assumes that the average person knows a lot
about how things work in the world and can reliably
answer these questions. CP is then defined below,
where the arrow notation means ordered bigrams,
i.e. event e1 occurs before event e2:
?(e1, e2) = pmi(e1, e2) + log
P (e1 ? e2)
P (e2 ? e1)
(4)
where pmi(e1, e2) = log
P (e1, e2)
P (e1)P (e2)
The causal potential consists of two terms: the
first is pair-wise mutual information (PMI) and
the second is relative ordering of bigrams. PMI
measures how often events occur as a pair; whereas
relative ordering counts how often event order
occurs in the bigram. If there is no ordering of
events, the relative ordering is zero. We smooth
unseen event pairs by setting their frequency equal
to 1 to avoid zero probabilities. For CP as with PMI,
we restrict these calculations to adjacent events.
Column CP of Table 1 below provides sample
values for the CP measure.
Probabilistic Language Models. Our third method
models event sequences using statistical language
models (Manshadi et al, 2008). A language model
estimates the probability of a sequence of words us-
ing a sample corpus. To identify contingent event
sequences, we apply a bigram model which esti-
mates the probability of observing the sequence of
two words w1 and w2 as follows:
P (w1, w2) ?= P (w2|w1) =
count(w1, w2)
count(w1)
(5)
Here, the words are events. Each verb is a single
event and each film scene is treated as a sequence of
verbs. For example, consider the following sentence
from Total Recall:
Quail and Kirsten sit at a small table,
eating breakfast.
This sentence is represented as the sequence of its
two verbs: sit, eat. We estimate the probability
of verb bigrams using Equation 5 and hypothesize
that the verb sequences with higher probability are
372
Row # Causal Potential Pair CP PCEP Search pat-
tern
NumHits Random Pair REP Search pat-
tern
NumHits
1 person KNOW person -
person MEAN what
2.18 he knows * means 415M person KNOW person -
person PEDDLE papers
he knows * ped-
dles
2
2 person COME - person
REST head
2.12 he comes * rests 158M person COME - person
GLANCE window
he comes *
glances
41
3 person SLAM person -
person SHUT door
2.11 he slams * shuts 11 person SLAM person -
person CHUCKLE
he slams * chuck-
les
0
4 person UNLOCK door -
person ENTER room
2.11 he unlocks * en-
ters
80 person UNLOCK door -
person ACT shot
he unlocks * acts 0
5 person SLOW person -
person STOP person
2.10 he slows * stops 697K person SLOW person -
eyes RIVET eyes
he slows * rivets 0
6 person LOOK window -
person WONDER thing
2.06 he looks * won-
ders
342M person LOOK window -
person EDGE hardness
he looks * edges 98
7 person TAKE person -
person LOOK window
2.01 he takes * looks 163M person TAKE person -
person CATCH person
he takes * catches 311M
8 person MANAGE smile -
person GET person
2.01 he manages * gets 80M person MANAGE smile
- person APPROACH
person
he manages * ap-
proaches
16
9 person DIVE escape -
person SWIM way
2.00 he dives * swims 1.5M person DIVE escape -
gun JAM person
he dives * jams 6
10 person STAGGER person
- person DROP person
2.00 he staggers *
drops
33 person STAGGER per-
son - plain WHEEL per-
son
he staggers *
wheels
1
11 person SHOOT person -
person FALL feet
1.99 he shoots * falls 55.7M person SHOOT person -
person PREVENT per-
son
he shoots * pre-
vents
6
12 person SQUEEZE person
- person SHUT door
1.87 he squeezes *
shuts
5 person SQUEEZE per-
son - person MARK per-
son
he squeezes *
marks
1
13 person SEE person - per-
son GO
1.87 he sees * goes 184M person SEE person - im-
age QUIVER hips
he sees * quivers 2
Table 1: Sample web search patterns and values used in web search refinement algorithm from action genre
more likely to be contingent. We apply a threshold
of 20 for count(w1, w2) to avoid infrequent and
uncommon bigrams.
Protagonist-based Models. We also used a method
of generating event pairs based not only on the con-
secutive events in text but on their protagonist. This
is based on the assumption that the agent, or protag-
onist, will tend to perform actions that further her
own goals, and are thus causally related. We called
this method protagonist-based because all events
were partitioned into multiple sets where each set of
events has one protagonist. This method is roughly
based on previous work using chains of discourse
entities to induce narrative schemas (Chambers and
Jurafsky, 2009).
Events that share one protagonist were extracted
from text according to co-referring mentions pro-
vided by the Stanford CoreNLP toolkit.1 A man-
ual examination of coreference results on a sample
of movie scripts suggests that the accuracy is only
around 60%: most of the time the same entity (in its
1http://nlp.stanford.edu/software/corenlp.shtml
nominal and pronominal forms) was not recognized
and was assigned as a new entity.
We preserve the order of events based on their tex-
tual order assuming as above that film scripts tend
to preserve temporal order. An ordered event pair
is generated if both events share a protagonist. We
further filter event pairs by eliminating those whose
frequency is less than 5 to filter insignificant and rare
event pairs. This also tends to catch errors generated
by the Stanford parser.
CP was then calculated accordingly to Equa-
tion 4. To calculate the PMI part of CP, we combine
the frequencies of event pairs in both orders.
2.4 Web Search Refinement
The final step of our method is WEB SEARCH RE-
FINEMENT. Our hypothesis is that using the film
corpus within a particular genre to do the initial esti-
mates of contingency takes advantage of genre prop-
erties such as similar events and narration of scenes
in chronological order. However the film corpus is
necessarily small, and we can augment the evidence
for a particular contingent relation by defining spe-
cific narrative sequence patterns and collecting web
373
counts.
Recall that PCEP stands for predicted contin-
gent event pair and that REP stands for random
event pair. We first select the top 100 event pairs
calculated by each CONTINGENCY measure, and
construct a RANDOM EVENT PAIR (REP) for each
PCEP that preserves the first element of the PCEP,
and replaces the second element with another event
selected randomly from within the same genre. We
then define web search patterns for both PCEP and
REPs and compare the counts. PCEPs should be fre-
quent in web search and REPs should be infrequent.
Our web refinement procedure is:
? For each event pair, PCEPs and REPs, create a
Google search pattern as illustrated by Table 1,
and described in more detail below.
? Search for the exact match in Google gen-
eral web search using incognito browsing and
record the estimated count of results returned;
? Remove all the PCEP/REP pairs with PCEP
Google search count less than 100: highly con-
tingent events should be frequent in a general
web search;
? Remove all PCEP/REP pairs with REP Google
search count greater than 100: events that are
not contingent on one another should not be
frequent in a general web search.
The motivation for this step is to provide addi-
tional evidence for or against the contingency of a
pair of events. Table 1 shows a selection of the top
100 PCEPs learned using the Causal potential (CP)
Metric, the web search patterns that are automati-
cally derived from the PCEPs (Column 4), the REPs
that were constructed for each PCEP (Column 6),
the web search patterns that were automatically de-
rived from the REPs (Column 7). Column 5 shows
the results of web search hits for the PCEP patterns
and Column 8 shows the results of web search hits
for the REP patterns. These hit counts were then
used in refining our estimates of CONTINGENCY for
the learned patterns as described above.
Note that the web search patterns do not aim to
find every possible match of the targeted CONTIN-
GENT relation that could possibly occur. Instead,
they are generalizations of the instances of PCEPs
that we found in the films corpus that are targeted at
finding hits that are the most likely to occur in nar-
rative sequences. Narrative sequences are most re-
liably signalled by use of the historical present tense,
e.g. as instantiated in the search patterns He knows
in Row 1 and He comes in Row 2 of Table 1 (Swan-
son and Gordon, 2012; Beamer and Girju, 2009;
Labov and Waletzky, 1997). In addition, we use
the ?*? operator in Google Search to limit search
to pairs of events reported in the historical present
tense, that are ?near? one another, and in a particular
sequence. We don?t care whether the events are in
the same utterance or in sequential utterances, thus
for the second verb (event) we do not include a sub-
ject pronoun he. These search patterns are not in-
tended to match the original instances in the film
corpus and in general they are unlikely to match
those instances.
For example, consider the search patterns and
results shown in Row 1 of Table 1. The
PCEP is (person KNOW person, person
MEAN what). The REP is (person KNOW
person, person PEDDLE papers). Our
prediction is that the REP should be much less likely
in web search counts and the results validate that
predication. A paired t-test over the 100 top PCEP
pairs for the CP measure comparing the hit counts
for the PCEP pairs vs. the REP pairs was highly
significant (p < .00001). However, consider Row
7. Even though in general the PCEP pairs are more
likely (as measured by the paired t-test compar-
ing web search counts for PCEPs vs REPs), there
are cases where the REP is highly likely as shown
by the REP (person take person, person
CATCH person) in Row 7. Alternatively there are
cases where the web search counts provide evidence
against one of the PCEPs. Consider Rows 3, 4, 10
and 12. In all of these cases the web counts NumHits
for the PCEP number in the tens.
After the web search refinement, we retain the
PCEP/REP pairs with initially high PCEP estimates,
for which we found good evidence for contingency
and for randomness, e.g. Row 1 and 2 in Table 1.
We use 100 as a threshold because most of the time
the estimate result count from Google is either a
very large number (millions) or a very small num-
ber (tens), as illustrated by the NumHits columns in
Table 1.
We experimented with different types of patterns
with a development set of PCEPs before we settled
on the search pattern template shown in Table 1. We
374
decided to use third person rather than first person
patterns, because first person patterns are only one
type of narrative (Swanson and Gordon, 2012). We
also decided to utilize event patterns without typical
objects, such as head in person REST head in
Row 2 of Table 1. We do not have any evidence that
this is the optimal search pattern template because
we did not systematically try other types of search
patterns.
3 Evaluation and Results
While other work uses a range of methods for evalu-
ating accuracy, to our knowledge our work is the first
to use human judgments from Mechanical Turk to
evaluate the accuracy of the learned PCEPs. We first
describe the evaluation setup in Sec. 3.1 and then re-
port the results in Sec. 3.2
3.1 Mechanical Turk Contingent Pair
Evaluations
We used three different types of HITs (Human Intel-
ligence Tasks) on Mechanical Turk for our evalua-
tion. Two of the HITS are in Fig. 2 and Fig. 3. The
differences in the different types of HITS involve:
(1) whether the arguments of events were given in
the HIT, as in Fig. 2 and (2): whether the Turkers
were told that the order of the events mattered, as
in Fig. 3. We initially thought that providing the
arguments to the events as shown in Fig. 2 would
help Turkers to reason about which event was more
likely. We tested this hypothesis only in the action
genre for the Causal Potential Measure. For CP, Bi-
gram and Protag the order of events always matters.
For the PMI task, the order of the events doesn?t
matter because PMI is a symmetric measure. Fig. 2
illustrates the instructions that were given with the
HIT when the event order doesn?t matter. In all the
other cases, the instructions that were given with the
HIT are those in Fig. 3 where the Turkers are in-
structed to pay attention to the order of the events.
For all types of HITS, for all measures of CON-
TINGENCY, we set up the task as a choice over
two alternatives, where for each predicted contin-
gent pair (PCEP), we generate a random event pair
(REP), with the first event the same and the second
one randomly chosen from all the events in the same
film genre. The REPs are constructed the same way
as we construct REPs for web search refinement,
as illustrated by Table 1. This is illustrated in both
Fig. 2 and Fig. 3. For all types of HITS, we ask 15
Turkers from a pre-qualified group to select which
pair (the PCEP or the REP) is more likely to oc-
cur together. Thus, the framing of these Mechani-
cal Turk tasks only assumes that the average person
knows how the world works; we do not ask them to
explicitly reason about causality as other work does
(Beamer and Girju, 2009; Gordon et al, 2011; Do et
al., 2011).
For each measure of CONTINGENCY, we take 100
event pairs with highest PCEP scores, and put them
in 5 HITs with twenty items per HIT. Previous work
has shown that for many common NLP tasks, 7
Turkers? average score can match expert annotations
(Snow et al, 2008). However, we use 15 Turkers
because we had no gold-standard data and because
we were not sure how difficult the task is. It is
clearly subjective. To calculate the accuracy of each
method, we computed the average correlation coef-
ficient between each pair of raters and eliminate the
5 lowest scoring workers. We then used the percep-
tions of the 10 remaining workers to calculate accu-
racy as # of correct answers / total # of answers.
In general, deciding when a MTurk worker is un-
reliable when the data is subjective is a difficult
problem. In the future we plan to test other solu-
tions to measuring annotator reliability as proposed
in related work (Callison-Burch, 2009; Snow et al,
2008; Karger et al, 2011; Dawid and Skene, 1979;
Welinder et al, 2010; Liu et al, 2012).
3.2 Results
We report our results in terms of overall accuracy.
Because the Mechanical Turk task is a choose-
one question rather than a binary classification,
Precision = Recall in our experimental results:
True Positive = Number of Correct Answers
True Negative = Number of Correct Answers
False Positive = Number of Incorrect Answers
False Positive = Number of Incorrect Answers
Precision =
True Positive
True Positive + False Positive
Recall =
True Positive
True Positive + False Negative
The accuracies of all the methods are shown in
Table 2. The results of using event arguments
(person KNOW person) in the Mechanical Turk
evaluation task (i.e. Fig. 2) is given in Rows 1 and
2 of Table 2. The accuracies for Rows 1 and 2 are
375
Figure 2: Mechanical Turk HIT with event arguments provided. This HIT also illustrates instructions where Turkers
are told that the order of the events does not matter.
Row
#
Contingency Estimation Method Action
Acc%
Romance
Acc%
Average
Acc%
1 CP with event arguments 69.30 NA 69.30
2 CP with event arguments + Web search 77.57 NA 77.57
3 CP no args 75.20 75.10 75.15
4 CP no args +Web Search 87.67 83.61 85.64
5 PMI no args 68.70 79.60 74.15
6 PMI no args +Web Search 72.11 88.52 80.32
7 Bigram no args 67.10 66.50 66.80
8 Bigram no args +Web Search 72.40 70.14 71.27
9 Protag CP no args 65.40 68.20 66.80
10 Protag CP no args +Web Search 76.59 64.10 70.35
Table 2: Evaluation results for the top 100 event pairs using all methods.
considerably lower than when the PCEPs are tested
without arguments. Comparing Rows 1 and 2 with
Rows 3 and 4 suggests that even if the arguments
provide extra information that help to ground the
type of event, in some cases these constraints on
events may mislead the Turkers or make the eval-
uation task more difficult. There is an over 10% in-
crease in CP + Web search accuracy for the task that
omits the event arguments (i.e. Fig. 3 as can be seen
by comparing Row 2 with Row 4. Thus omitting the
arguments of events in evaluations actually appears
to allow Turkers to make better judgments.
In addition, Table 2 shows clearly that for ev-
ery single method, accuracy is improved by refin-
ing the initial estimates of contingency using the
narrative-based web search patterns. Web search in-
376
Figure 3: Mechanical Turk HIT for evaluation with no event arguments provided. This HIT also illustrates instructions
where Turkers are told that the order of the events does matter.
creases the accuracy of almost all evaluation tasks,
with increases ranging from 3.45% to 12.5% when
averaged over both film genres (Column 5 Average
Acc%). The best performing method for the Ac-
tion genre is CP+Web Search at 87.67%, while the
best performing method for the Romance genre is
PMI+Web search at 88.52%. However PMI+Web
Search does not beat CP+Web Search on average
over both genres we tested, even though the Me-
chanical Turk HIT for CP specifies that the order of
the events matters: a more stringent criterion. Also
overall the CP+WebSearch method achieves a very
high 85.64% average accuracy.
It is also interesting to note the variation across
the different methods. For example, while it is
well known that PMI typically requires very large
corpora to make good estimates, the PMI method
without web search refinement has an initially high
accuracy of 79.60% for the romance genre, while
only achieving 68.70% for action. Perhaps this dif-
ference arises because the romance genre is more
highly causal, or because situations are more struc-
tured in romance, providing better estimates with a
small corpus. However even in this case of romance
with PMI, adding web search refinement provides
an almost 10% increase in absolute accuracy to the
highest accuracy of any combination, i.e. 88.52%.
There is also an interesting case of Protag CP for
the romance genre where web search refinement ac-
tually decreases accuracy by 4.1%. In future work
we plan to examine more genres from the film cor-
pus and also examine the role of corpus size in more
detail.
4 Discussion and Future Work
We induced event pairs using several methods from
previous work with similar aims but widely different
problem formulations and evaluation methods. We
used a verb-rich film scene corpus where events are
normally narrated in temporal order. We used Me-
chanical Turk to evaluate the learned pairs of CON-
TINGENT events using human perceptions. In the
first stage drawing on previous measures of distribu-
tional co-occurrence, we achieved an overall average
accuracy of around 70%, over a 50% baseline. We
then implemented a novel method of defining narra-
tive sequence patterns using the Google Search API,
and used web counts to further refine our estimates
of the contingency of the learned event pairs. This
increased the overall average accuracy to around
77%, which is 27% above the baseline. Our results
indicate that the use of web search counts increases
the average accuracy of our Causal Potential-based
method to 85.64% as compared to an average accu-
racy of 75.15% without web search. To our knowl-
edge this is the highest accuracy achieved in tasks of
377
this kind to date.
Previous work on recognition of the PDTB CON-
TINGENT relation has used both supervised and un-
supervised learning, and evaluation typically mea-
sures precision and recall against a PDTB annotated
corpus (Do et al, 2011; Pitler et al, 2009; Zhou et
al., 2010; Chiarcos, 2012; Louis et al, 2010). We
use an unsupervised approach and measure accuracy
using human perceptions. Other work by Girju and
her students defined a measure called causal poten-
tial and then used film screen plays to learn a knowl-
edge base of causal pairs of events. They evaluate
the pairs by asking two trained human annotators to
label whether occurrences of those pairs in their cor-
pus are causally related (Beamer and Girju, 2009;
Riaz and Girju, 2010). We also make use of their
causal potential measure. Work on commonsense
causal reasoning aims to learn causal relations be-
ween pairs of events using a range of methods ap-
plied to a large corpus of weblog narratives (Gordon
et al, 2011; Gordon and Swanson, 2009; Manshadi
et al, 2008). One form of evaluation aimed to pre-
dict the last event in a sequence (Manshadi et al,
2008), while more recent work uses the learned pairs
to improve performance on the COPA SEMEVAL
task (Gordon et al, 2011).
Related work on SCRIPT LEARNING induces
likely sequences of temporally ordered events in
news, rather than CONTINGENCY or CAUSALITY
(Chambers and Jurafsky, 2008; Chambers and Ju-
rafsky, 2009). Chambers & Jurafsky also evaluate
against a corpus of existing documents, by leaving
one event out of a document (news story), and then
testing the system?s ability to predict the missing
event. To our knowledge, our method is the first
to augment distributional semantics measures from
a corpus with web search data. We are also the first
to evaluate the learned event pairs with a human per-
ceptual evaluation with native speakers.
We hypothesize that there are several advantages
to our method: (1) events in the same genre tend to
be more similar than events across genres, so less
data is needed to estimate co-occurrence; (2) film
scenes are typically narrated via simple tenses in the
correct temporal order, which allows the ordering
of events to contribute to estimates of the CONTIN-
GENCY relation; (3) The web counts focus on vali-
dating event pairs already deemed to be likely to be
CONTINGENT in the smaller, more controlled, film
scence corpus.
Our work capitalizes on event sequences narrated
in temporal order as a cue to causality. We expect
this approach to generalize to other domains where
these properties hold, such as fables, personal stories
and news articles. We do not expect this technique
to generalize without further refinements to genres
frequently told out of temporal order or when events
are not mentioned consecutively in the text, for ex-
ample in certain types of fiction.
In future work we want to explore in more de-
tail the differences in performance of the different
contingency measures. For example, previous work
would suggest that the the higher the measure is, the
more likely the two events are to be contingent on
one another. To date, while we have only tested the
top 100, we have not found that the bottom set of
20 are less accurate than the top set of 20. This
could be due to corpus size, or the measures them-
selves, or noise from parser accuracy etc. As shown
in Table 2, web search refinement is able to eliminate
most noise in event pairs, but we would still aim to
achieve a better understanding of the circumstances
which lead particular methods to work better.
In future work we also want to explore ways of in-
ducing larger event structures than event pairs, such
as the causal chains, scripts, or narrative schemas of
previous work.
Acknowledgments
We would like to thank Yan Li for setting up au-
tomatic search query. We also thank members of
NLDS for their discussions and suggestions, espe-
cially Stephanie Lukin, Rob Abbort, and Grace Lin.
References
B. Beamer and R. Girju. 2009. Using a bigram event
model to predict causal potential. In Computational
Linguistics and Intelligent Text Processing, p. 430?
441. Springer.
C. Callison-Burch. 2009. Fast, cheap, and creative: eval-
uating translation quality using amazon?s mechanical
turk. In Proc. of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 1 -
Volume 1, p. 286?295. Association for Computational
Linguistics.
N. Chambers and D. Jurafsky. 2008. Unsupervised
learning of narrative event chains. Proc. of ACL-08:
HLT, p. 789?797.
N. Chambers and D. Jurafsky. 2009. Unsupervised
learning of narrative schemas and their participants. In
378
Proc. of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint Confer-
ence on Natural Language Processing of the AFNLP:
Volume 2-Volume 2, p. 602?610.
C. Chiarcos. 2012. Towards the unsupervised acquisi-
tion of discourse relations. In Proc. of the 50th Annual
Meeting of the Association for Computational Linguis-
tics: Short Papers-Volume 2, p. 213?217. Association
for Computational Linguistics.
A. P. Dawid and A. M. Skene. 1979. Maximum likeli-
hood estimation of observer error-rates using the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries C (Applied Statistics), 28(1):20?28.
Q. X. Do, Y. S. Chan, and D. Roth. 2011. Minimally
supervised event causality identification. In Proc. of
the Conference on Empirical Methods in Natural Lan-
guage Processing, p. 294?303. Association for Com-
putational Linguistics.
R.J. Gerrig. 1993. Experiencing narrative worlds: On
the psychological activities of reading. Yale Univ Pr.
A. Gordon and R. Swanson. 2009. Identifying personal
stories in millions of weblog entries. In Third Interna-
tional Conference on Weblogs and Social Media, Data
Challenge Workshop.
A. Gordon, Cosmin Bejan, and Kenji Sagae. 2011. Com-
monsense causal reasoning using millions of personal
stories. In Twenty-Fifth Conference on Artificial Intel-
ligence (AAAI-11).
A. Goyal, E. Riloff, and H. Daume? III. 2010. Automat-
ically producing plot unit representations for narrative
text. In Proc. of the 2010 Conference on Empirical
Methods in Natural Language Processing, p. 77?86.
Association for Computational Linguistics.
A. C. Graesser, M. Singer, and T. Trabasso. 1994. Con-
structing inferences during narrative text comprehen-
sion. Psychological review, 101(3):371.
D. R. Karger, S. Oh, and D. Shah. 2011. Iterative
learning for reliable crowdsourcing systems. In John
Shawe-Taylor, Richard S. Zemel, Peter L. Bartlett,
Fernando C. N. Pereira, and Kilian Q. Weinberger, ed-
itors, NIPS, p. 1953?1961.
W. Labov and J. Waletzky. 1997. Narrative analysis:
Oral versions of personal experience.
W. G. Lehnert. 1981. Plot units and narrative summa-
rization. Cognitive Science, 5(4):293?331.
Z. Lin, M.-Y. Kan, and H. T Ng. 2010. A pdtb-styled
end-to-end discourse parser. In Proc. of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
Q. Liu, J. Peng, and A. Ihler. 2012. Variational inference
for crowdsourcing. In Advances in Neural Information
Processing Systems 25, p. 701?709.
A. Louis, A. Joshi, R. Prasad, and A. Nenkova. 2010.
Using entity features to classify implicit relations. In
Proc. of the 11th Annual SIGdial Meeting on Dis-
course and Dialogue, Tokyo, Japan.
M. Manshadi, R. Swanson, and A. S Gordon. 2008.
Learning a probabilistic model of event sequences
from internet weblog stories. In Proc. of the 21st
FLAIRS Conference.
E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in text.
In Proc. of the 47th Meeting of the Association for
Computational Linguistics.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,
A. Joshi, and B. Webber. 2008a. The penn discourse
treebank 2.0. In Proc. of the 6th International Confer-
ence on Language Resources and Evaluation (LREC
2008), p. 2961?2968.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,
A. Joshi, and B. Webber. 2008b. The Penn Discourse
TreeBank 2.0. In Proc. of 6th International Confer-
ence on Language Resources and Evaluation (LREC
2008).
M. Riaz and R. Girju. 2010. Another look at causal-
ity: Discovering scenario-specific contingency rela-
tionships with no supervision. In Semantic Computing
(ICSC), 2010 IEEE Fourth International Conference
on, p. 361?368. IEEE.
R. Schank and R. Abelson. 1977. Scripts Plans Goals.
Lea.
R. Snow, B. O?Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast?but is it good?: evaluating non-expert
annotations for natural language tasks. In Proc. of
the Conference on Empirical Methods in Natural Lan-
guage Processing, p. 254?263. Association for Com-
putational Linguistics.
R. Swanson and A. S. Gordon. 2012. Say anything:
Using textual case-based reasoning to enable open-
domain interactive storytelling. ACM Transactions on
Interactive Intelligent Systems (TiiS), 2(3):16.
M. A. Walker, G. Lin, and J. Sawyer. 2012b. An anno-
tated corpus of film dialogue for learning and charac-
terizing character style. In Language Resources and
Evaluation Conference, LREC2012.
P. Welinder, S. Branson, S. Belongie, and P. Perona.
2010. The multidimensional wisdom of crowds. In
Advances in Neural Information Processing Systems
23, p. 2424?2432.
Z.-M. Zhou, Y. Xu, Z.Y. Niu, M. Lan, J. Su, , and
C. L. Tan. 2010. Predicting discourse connectives for
implicit discourse relation recognition. In In Coling
2010: Posters, p. 1507?1514.
379
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 592?596,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Stance Classification using Dialogic Properties of Persuasion
Marilyn A. Walker, Pranav Anand, Robert Abbott and Ricky Grant
Baskin School of Engineering & Linguistics Department
University of California Santa Cruz
Santa Cruz, Ca. 95064, USA
maw,panand,abbott,rgrant@soe.ucsc.edu
Abstract
Public debate functions as a forum for both
expressing and forming opinions, an impor-
tant aspect of public life. We present results
for automatically classifying posts in online
debate as to the position, or STANCE that the
speaker takes on an issue, such as Pro or Con.
We show that representing the dialogic struc-
ture of the debates in terms of agreement rela-
tions between speakers, greatly improves per-
formance for stance classification, over mod-
els that operate on post content and parent-
post context alone.
1 Introduction
Public debate functions as a forum for both express-
ing and forming opinions. Three factors affect opin-
ion formation, e.g. the perlocutionary uptake of de-
bate arguments (Cialdini, 2000; Petty and Cacioppo,
1988; Petty et al, 1981). First, there is the ARGU-
MENT itself, i.e. the propositions discussed along
with the logical relations between them. Second is
the SOURCE of the argument (Chaiken, 1980), e.g.
the speaker?s expertise, or agreement relations be-
tween speakers. The third factor consists of proper-
ties of the AUDIENCE such as prior beliefs, social
identity, personality, and cognitive style (Davies,
1998). Perlocutionary uptake in debates primar-
ily occurs in the audience, who may be undecided,
while debaters typically express a particular position
or STANCE on an issue, e.g. Pro or Con, as in the
online debate dialogues in Figs. 1, 2, and 3.
Previous computational work on debate covers
three different debate settings: (1) congressional de-
Post Stance Utterance
P1 PRO I feel badly for your ignorance because although
there maybe a sliver of doubt that mankind may
have evolved from previous animals, there is no
doubt that the Earth and the cosmos have gone
through evolution and are continuing to do so
P2 CON As long as there are people who doubt evolu-
tion, both lay and acedamia, then evolution is in
doubt. And please don?t feel bad for me. I am
perfectly secure in my ?ignorance?.
P3 PRO By that measure, as long as organic chemistry,
physics and gravity are in doubt by both lay and
acedamia, then organic chemistry, physics and
gravity are in doubt. Gravity is a theory. Why
aren?t you giving it the same treatment you do
to evolution? Or is it because you are ignorant?
Angelic Falling anyone?
P4 CON I?m obviously ignorant. Look how many times
i?ve been given the title. ?Gravity is a theory.
Why aren?t you giving it the same treatment you
do to evolution?? Because it doesn?t carry the
same weight. ;P
Figure 1: All posts linked via rebuttal links. The topic
was ?Evolution?, with sides ?Yes, I Believe? vs. ?No, I
Dont Believe?.
bates (Thomas et al, 2006; Bansal et al, 2008;
Yessenalina et al, 2010; Balahur et al, 2009; Bur-
foot et al, 2011); (2) company-internal discussion
sites (Murakami and Raymond, 2010; Agrawal et
al., 2003); and (3) online social and political public
forums (Somasundaran and Wiebe, 2009; Somasun-
daran and Wiebe, 2010; Wang and Rose?, 2010; Bi-
ran and Rambow, 2011). Debates in online public
forums (e.g. Fig. 1) differ from debates in congress
and on company discussion sites in two ways.
First, the language is different. Online debaters
are highly involved, often using emotional and col-
orful language to make their points. These debates
are also personal, giving a strong sense of the indi-
592
vidual making the argument, and whether s/he fa-
vors emotive or factual modes of expression, e.g.
Let me answer.... NO! (P2 in Fig. 3). Other com-
mon features are sarcasm, e.g. I?m obviously igno-
rant. Look how many times i?ve been given the ti-
tle (P4 in Fig. 1), questioning another?s evidence or
assumptions: Yes there is always room for human
error, but is one accident that hasn?t happened yet
enough cause to get rid of a capital punishment? (P2
in Fig. 3), and insults: Or is it because you are ig-
norant? (P3 in Fig. 1). These properties may func-
tion to engage the audience and persuade them to
form a particular opinion, but they make computa-
tional analysis of such debates challenging, with the
best performance to date averaging 64% over several
topics (Somasundaran and Wiebe, 2010).
Post Stance Utterance
P1 Superman Batman is no match for superman. Not
only does he have SUPERnatural powers as
opposed to batman?s wit and gadgetry, but
his powers have increased in number over
the years. For example, when Superman?s
prowess was first documented in the comics
he did not have x-ray vision. It wasn?t until
his story was told on radio that he could see
through stuff. So no matter what new weapon
batman could obtain, Superman would add an-
other SUPERnatural weapon to foil the Caped
crusader.
P2 Batman Superman GAVE Batman a krytonite ring so
that Batman could take him down should he
need to. Superman did this because he knows
Batman is the only guy that could do it.
P3 Superman But, not being privy to private conversations
with S-man, you wouldn?t know that, being the
humble chap that he is, S-man allowed batman
the victory because he likes the bat and wanted
him to mantain some credibility. Honest.
P4 Batman Hmmm, this is confusing. Since we all know
that Supes doesn?t lie and yet at the time of
him being beaten by Batman he was under the
control of Poison Ivy and therefore could NOT
have LET Batman win on purpose. I have to
say that I am beginning to doubt you really are
friends with Supes at all.
Figure 2: All posts linked via rebuttal links. The topic
was ?Superman vs. Batman?
Second, the affordances of different online debate
sites provide differential support for dialogic rela-
tions between forum participants. For example, the
research of Somasundaran and Wiebe (2010), does
not explicitly model dialogue or author relations.
However debates in our corpus vary greatly by topic
on two dialogic factors: (1) the percent of posts that
are rebuttals to prior posts, and (2) the number of
Post Stance Utterance
P1 CON 69 people have been released from death row
since 1973 these people could have been killed
if there cases and evidence did not come up rong
also these people can have lost 20 years or more
to a false coviction. it is only a matter of time till
some one is killed yes u could say there doing
a good job now but it has been shown so many
times with humans that they will make the hu-
man error and cost an innocent person there life.
P2 PRO Yes there is always room for human error, but
is one accident that hasn?t happened yet enough
cause to get rid of a capital punishment? Let me
answer...NO! If you ban the death penalty crime
will skyrocket. It is an effective deterannce for
crime. The states that have strict death penalty
laws have less crime than states that don?t (Texas
vs. Michigan) Texas?s crime rate is lower than
Michigan and Texas has a higher population!!!!
Figure 3: Posts linked via rebuttal links. The topic was
?Capital Punishment?, and the argument was framed as
?Yes we should keep it? vs. ?No we should not?.
posts per author. The first 5 columns of Table 2
shows the variation in these dimensions by topic.
In this paper we show that information about di-
alogic relations between authors (SOURCE factors)
improves performance for STANCE classification,
when compared to models that only have access to
properties of the ARGUMENT. We model SOURCE
relations with a graph, and add this information to
classifiers operating on the text of a post. Sec. 2
describes the corpus and our approach. Our cor-
pus is publicly available, see (Walker et al, 2012).
We show in Sec. 3 that modeling source properties
improves performance when the debates are highly
dialogic. We leave a more detailed comparison to
previous work to Sec. 3 so that we can contrast pre-
vious work with our approach.
2 Experimental Method and Approach
Our corpus consists of two-sided debates from Con-
vinceme.net for 14 topics that range from play-
ful debates such as Superman vs. Batman (Fig. 2
to more heated political topics such as the Death
Penalty (Fig. 3. In total the corpus consists of 2902
two-sided debates (36,307 posts), totaling 3,080,874
words; the topic labelled debates which we use in
our experiments contain 575,818 words. On Con-
vinceme, a person starts a debate by posting a topic
or a question and providing sides such as for vs.
against. Debate participants can then post argu-
ments for one side or the other, essentially self-
593
labelling their post for stance. Convinceme pro-
vides three possible sources of dialogic structure,
SIDE, REBUTTAL LINKS and TEMPORAL CONTEXT.
Timestamps for posts are only available by day and
there are no agreement links. Here, we use the self-
labelled SIDE as the stance to be predicted.
Set/Factor Description
Basic Number of Characters in post, Average Word
Length, Unigrams, Bigrams
Sentiment LIWC counts and frequencies, Opinion De-
pendencies, LIWC Dependencies, negation
Argument Cue Words, Repeated Punctuation, Context,
POS-Generalized Dependencies, Quotes
Table 1: Feature Sets
We construct features from the posts, along with a
representation of the parent post as context, and use
those features in several base classifiers. As shown
in Table 1, we distinguish between basic features,
such as length of the post and the words and bi-
grams in the post, and features capturing sentiment
and subjectivity, including using the LIWC tool for
emotion labelling (Pennebaker et al, 2001) and de-
riving generalized dependency features using LIWC
categories, as well as some limited aspects of the
argument structure, such as cue words signalling
rhetorical relations between posts, POS generalized
dependencies, and a representation of the parent post
(context). Only rebuttal posts have a parent post, and
thus values for the context features.
50-10
Figure 4: Sample maxcut to ConvinceMe siding. Sym-
bols (circle, cross, square, triangles) indicate authors and
fill colors (white,black) indicate true side. Rebuttal links
are marked by black edges, same-author links by red;
weights are 50 and -10, respectively. Edges in the max-
cut are highlighted in yellow, and the nodes in each cut
set are bounded by the green dotted line.
We then construct a graph (V,E) representing the
dialogue structure, using the rebuttal links and au-
thor identifiers from the forums site. Each node V
of the graph is a post, and edges E indicate dialogic
relations of agreement and disagreement between
posts. We assume only that authors always agree
with themselves, and that rebuttal links indicate dis-
agreement. Agreement links based on the inference
that if A, B disagree with C they agree with each
other were not added to the graph.
Maxcut attempts to partition a graph into two
sides. Fig. 4 illustrates a sample result of applying
MaxCut. Edges connecting the partitions are said
to be cut, while those within partitions are not. The
goal is to maximize the sum of cut edge weights. By
making edge weights high we reward the algorithm
for cutting the edge, by making edge weights nega-
tive we penalize the algorithm for cutting the edge.
Rebuttal links were assigned a weight +100/(num-
ber of rebuttals). Same author links were assigned a
weight -60/(number of posts by author). If author A
rebutted author B at some point, then a weight of 50
was assigned to all edges connecting posts by author
A and posts by author B. If author B rebutted author
A as well, that 50 was increased to 100. We applied
the MaxCut partitioning algorithm to this graph, and
then we orient each of the components automati-
cally using a traditional supervised classifier. We
consider each component separately where compo-
nents are defined using the original (pre-MaxCut)
graph. For each pair of partition side p ? {P0, P1}
and classifier label l ? {L0, L1}, we compute a
score Sp,l by summing the margins of all nodes as-
signed to that partition and label. We then compute
and compare the score differences for each partition.
Dp = Sp,L1 ? Sp,L0 If DP0 < DP1 , then nodes in
partition P0 should be assigned label L0 and nodes
in P1 should be assigned label L1. Likewise, if
DP0 > DP1 , then nodes in partition P0 should be as-
signed label L1 and nodes in P1 should be assigned
label L0. If DP0 = DP1 , then we orient the compo-
nent with a coin flip.
3 Results and Discussion
Table 2 summarizes our results for the base classi-
fier (JRIP) compared to using MaxCut over the so-
cial network defined by author and rebuttal links.
We report results for experiments using all the fea-
594
Topic Characteristics MaxCut Algorithm JRIP Algorithm
Topic Posts Rebs P/A A> 1p MLE Acc F1 P R Acc F1 P R
Abortion 607 64% 2.73 42% 53% 82% 0.82 0.78 0.88 55% 0.55 0.52 0.59
Cats v. Dogs 162 40% 1.60 24% 53% 80% 0.78 0.80 0.76 61% 0.55 0.59 0.51
Climate Change 207 65% 2.92 41% 50% 64% 0.66 0.63 0.69 61% 0.62 0.60 0.63
Comm. v. Capitalism 214 62% 2.97 46% 55% 70% 0.67 0.66 0.68 53% 0.49 0.48 0.49
Death Penalty 331 60% 2.40 45% 56% 35% 0.31 0.29 0.34 55% 0.46 0.48 0.44
Evolution 818 66% 3.74 53% 58% 82% 0.78 0.78 0.79 56% 0.49 0.48 0.50
Existence Of God 852 76% 4.16 51% 56% 75% 0.73 0.70 0.76 52% 0.49 0.47 0.51
Firefox v. IE 233 38% 1.27 15% 79% 76% 0.47 0.44 0.49 72% 0.33 0.34 0.33
Gay Marriage 560 56% 2.01 28% 65% 84% 0.77 0.74 0.81 60% 0.43 0.43 0.44
Gun Control 135 59% 2.08 45% 63% 37% 0.24 0.21 0.27 53% 0.24 0.30 0.20
Healthcare 112 79% 3.11 53% 55% 73% 0.71 0.69 0.72 60% 0.49 0.56 0.44
Immigration 78 58% 1.95 33% 54% 33% 0.21 0.23 0.19 53% 0.39 0.48 0.33
Iphone v. Blackberry 25 44% 1.14 14% 67% 88% 0.80 0.86 0.75 71% 0.46 0.60 0.38
Israel v. Palestine 64 33% 3.37 53% 58% 85% 0.82 0.79 0.85 49% 0.48 0.42 0.56
Mac v. PC 126 37% 1.85 24% 52% 19% 0.18 0.17 0.18 46% 0.46 0.45 0.48
Marijuana legalization 229 45% 1.52 25% 71% 73% 0.56 0.52 0.60 63% 0.34 0.35 0.34
Star Wars vs. LOTR 102 44% 1.38 26% 53% 63% 0.62 0.60 0.65 63% 0.62 0.60 0.65
Superman v. Batman 146 30% 1.39 20% 54% 50% 0.40 0.44 0.37 56% 0.47 0.52 0.43
Table 2: Results. KEY: Number of posts on the topic (Posts). Percent of Posts linked by Rebuttal links (Rebs). Posts
per author (P/A). Authors with more than one post (A > 1P). Majority Class Baseline (MLE).
tures with ?2 feature selection; we use JRIP as the
base classifier because margins are used by the auto-
matic MaxCut graph orientation algorithm. Exper-
iments with different learners (NB, SVM) did not
yield significant differences from JRIP. The results
show that, in general, representing dialogic infor-
mation in terms of a network of relations between
posts yields very large improvements. In the few
topics where performance is worse (Death Penalty,
Gun Control, Mac vs. PC, Superman vs. Batman),
the MaxCut graph gets oriented to the stance sides
the wrong way, so that the cut actually groups the
posts correctly into sides, but then assigns them to
the wrong side. For Maxcut, as expected, there are
significant correlations between the % of Rebuttals
in a debate and Precision (R = .16 ) and Recall (R=
.22), as well as between Posts/Author and Precision
(R = .25) and Recall (R = .43). This clearly indi-
cates that the degree of dialogic behavior (the graph
topology) has a strong influence on results per topic.
These results would be even stronger if all MaxCut
graphs were oriented correctly.
(Somasundaran and Wiebe, 2010) present an un-
supervised approach using ICA to stance classifica-
tion, showing that identifying argumentation struc-
ture improves performance, with a best performance
averaging 64% accuracy over all topics, but as high
as 70% for some topics. Other research classifies
the speaker?s side in a corpus of congressional floor
debates (Thomas et al, 2006; Bansal et al, 2008;
Balahur et al, 2009; Burfoot et al, 2011). Thomas
et al(2006) achieved accuracies of 71.3% by using
speaker agreement information in the graph-based
MinCut/Maxflow algorithm, as compared to accura-
cies around 70% via an an SVM classifier operating
on content alone. The best performance to date on
this corpus achieves accuracies around 82% for dif-
ferent graph-based approaches as compared to 76%
accuracy for content only classification (Burfoot et
al., 2011). Other work applies MaxCut to the reply
structure of company discussion forums, showing
that rules for identifying agreement (Murakami and
Raymond, 2010), defined on the textual content of
the post yield performance improvements over using
reply structures alone (Malouf and Mullen, 2008;
Agrawal et al, 2003)
Our results are not strictly comparable since we
use a different corpus with different properties, but
to our knowledge this is the first application of Max-
Cut to stance classification that shows large perfor-
mance improvements from modeling dialogic rela-
tions. In future work, we plan to explore whether
deeper linguistic features can yield large improve-
ments in both the base classifier and in MaxCut re-
sults, and to explore better ways of automatically
orienting the MaxCut graph to stance side. We also
hope to develop much better context features and to
make even more use of dialogue structure.
595
References
R. Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu. 2003.
Mining newsgroups using networks arising from so-
cial behavior. In Proceedings of the 12th international
conference on World Wide Web, pages 529?535. ACM.
A. Balahur, Z. Kozareva, and A. Montoyo. 2009. Deter-
mining the polarity and source of opinions expressed
in political debates. Computational Linguistics and
Intelligent Text Processing, pages 468?480.
M. Bansal, C. Cardie, and L. Lee. 2008. The power
of negative thinking: Exploiting label disagreement in
the min-cut classification framework. Proceedings of
COLING: Companion volume: Posters, pages 13?16.
O. Biran and O. Rambow. 2011. Identifying justifica-
tions in written dialogs. In 2011 Fifth IEEE Inter-
national Conference on Semantic Computing (ICSC),
pages 162?168. IEEE.
C. Burfoot, S. Bird, and T. Baldwin. 2011. Collective
classification of congressional floor-debate transcripts.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 1506?1515. As-
sociation for Computational Linguistics.
S. Chaiken. 1980. Heuristic versus systematic informa-
tion processing and the use of source versus message
cues in persuasion. Journal of personality and social
psychology, 39(5):752.
Robert B. Cialdini. 2000. Influence: Science and Prac-
tice (4th Edition). Allyn & Bacon.
M.F. Davies. 1998. Dogmatism and belief formation:
Output interference in the processing of supporting
and contradictory cognitions. Journal of personality
and social psychology, 75(2):456.
R. Malouf and T. Mullen. 2008. Taking sides: User clas-
sification for informal online political discourse. In-
ternet Research, 18(2):177?190.
A. Murakami and R. Raymond. 2010. Support or
Oppose? Classifying Positions in Online Debates
from Reply Activities and Opinion Expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 869?875.
Association for Computational Linguistics.
J. W. Pennebaker, L. E. Francis, and R. J. Booth, 2001.
LIWC: Linguistic Inquiry and Word Count.
Richard E. Petty and John T. Cacioppo. 1988. The ef-
fects of involvement on responses to argument quan-
tity and quality: Central and peripheral routes to per-
suasion. Journal of Personality and Social Psychol-
ogy, 46(1):69?81.
R.E. Petty, J.T. Cacioppo, and R. Goldman. 1981. Per-
sonal involvement as a determinant of argument-based
persuasion. Journal of Personality and Social Psy-
chology, 41(5):847.
S. Somasundaran and J. Wiebe. 2009. Recognizing
stances in online debates. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1-Volume
1, pages 226?234. Association for Computational Lin-
guistics.
S. Somasundaran and J. Wiebe. 2010. Recognizing
stances in ideological on-line debates. In Proceedings
of the NAACL HLT 2010 Workshop on Computational
Approaches to Analysis and Generation of Emotion in
Text, pages 116?124. Association for Computational
Linguistics.
M. Thomas, B. Pang, and L. Lee. 2006. Get out the
vote: Determining support or opposition from Con-
gressional floor-debate transcripts. In Proceedings of
the 2006 conference on empirical methods in natural
language processing, pages 327?335. Association for
Computational Linguistics.
M. Walker, P. Anand, J. Fox Tree, R. Abbott, and J. King.
2012. A corpus for research on deliberation and
debate. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation,
LREC 2012, Istanbul, Turkey, May 23-25, 2012.
Y.C. Wang and C.P. Rose?. 2010. Making conversational
structure explicit: identification of initiation-response
pairs within online discussions. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 673?676. Association for
Computational Linguistics.
A. Yessenalina, Y. Yue, and C. Cardie. 2010. Multi-
level structured models for document-level sentiment
classification. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1046?1056. Association for Computational
Linguistics.
596
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, page 17,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Invited Talk
Dynamic Adaptation in Dialog Systems
Marilyn Walker
University of California, Santa Cruz
A hallmark of human robust intelligence is the ability to flexibly and dynami-
cally adapt behavior to the current situation. For dialog behavior, this entails adap-
tation to features of both the dialog partner (e.g., relationship, age, personality) and
the dialog situation (e.g., task context, asynchronous interaction, limited modalities
such as voice-only communication). We don?t completely understand how humans
do this, nor do we have the ability to produce such dynamically adaptable behavior
in human computer dialog interaction. In this talk I will discuss our recent work on
dynamic adaptation to the user, and present some experimental results showing that
it is possible to automatically generate both verbal and nonverbal system behaviors
that are perceived by the user as reliably expressing particular system personalities.
I will describe two of my current projects at UCSC that are integrating these ca-
pabilities into mobile dialogue systems: SpyFeet, a role playing augmented reality
game for encouraging girls to exercise, and Skipper, a dialogue system that gives
pedestrians directions in both urban and campus environments.
17
Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 2?11,
Portland, Oregon, 23 June 2011. c?2011 Association for Computational Linguistics
How can you say such things?!?:
Recognizing Disagreement in Informal Political Argument
Rob Abbott, Marilyn Walker, Pranav Anand, Jean E. Fox Tree,
Robeson Bowmani, and Joseph King
University of California Santa Cruz
abbott|maw@soe.ucsc.edu,panand|foxtree@ucsc.edu
Abstract
The recent proliferation of political and so-
cial forums has given rise to a wealth of freely
accessible naturalistic arguments. People can
?talk? to anyone they want, at any time, in
any location, about any topic. Here we use
a Mechanical Turk annotated corpus of forum
discussions as a gold standard for the recog-
nition of disagreement in online ideological
forums. We analyze the utility of meta-post
features, contextual features, dependency fea-
tures and word-based features for signaling
the disagreement relation. We show that us-
ing contextual and dialogic features we can
achieve accuracies up to 68% as compared to
a unigram baseline of 63%.
1 Introduction
The recent proliferation of political and social fo-
rums has given rise to a wealth of freely accessible
naturalistic arguments. People can ?talk? to anyone
they want, at any time, in any location, about any
topic. Their conversations range from current polit-
ical topics such as national health care to religious
questions such as the meaning of biblical passages.
See Figure 1. We aim to automatically derive rep-
resentations of the discourse structure of such argu-
ments and to gain a deeper theoretical and empirical
understanding of the linguistic reflexes of perlocu-
tionary acts such as persuasion (Austin, 1965).
The study of the structure of argumentative com-
munication has a long lineage in psychology (Cial-
dini, 2000) and rhetoric (Hunter, 1987), but the his-
torical lack of a large corpus of naturalistic exam-
Topic Q-R: Post
Evolution Q: How can you say such things? The Bible says that
God CREATED over and OVER and OVER again! And
you reject that and say that everything came about by
evolution? If you reject the literal account of the Cre-
ation in Genesis, you are saying that God is a liar! If you
cannot trust God?s Word from the first verse, how can
you know that the rest of it can be trusted?
R: It?s not a literal account unless you interpret it that
way.
Gay
mar-
riage
Q: Gavin Newsom- I expected more from him when I
supported him in the 2003 election. He showed himself
as a family-man/Catholic, but he ended up being the ex-
act oppisate, supporting abortion, and giving homosexu-
als marriage licenses. I love San Francisco, but I hate the
people. Sometimes, the people make me want to move
to Sacramento or DC to fix things up.
R: And what is wrong with giving homosexuals the right
to settle down with the person they love? What is it to
you if a few limp-wrists get married in San Francisco?
Homosexuals are people, too, who take out their garbage,
pay their taxes, go to work, take care of their dogs, and
what they do in their bedroom is none of your business.
Abortion Q: Equality is not defined by you or me. It is defined by
the Creator who created men.
R: Actually I think it is defined by the creator who cre-
ated all women. But in reality your opinion is gibberish.
Equality is, like every other word, defined by the people
who use the language. Currently it means ?the same?.
People aren?t equal because they are not all the same.
Any attempt to argue otherwise is a display of gross stu-
pidity.
Figure 1: Sample Quote/Response Pairs
ples has limited empirical work to a handful of gen-
res (e.g., editorials or simulated negotiations). Ar-
gumentation is above all tactical. Thus being able
to effectively model it would afford us a glimpse
of pragmatics beyond the conversational turn. More
practically, an increasing portion of information and
opinion exchange online occurs in natural dialogue,
in forums, in webpage comments, and in the back
2
and forth of short messages (e.g., Facebook status
updates, tweets, etc.) Effective models of argumen-
tative discourse thus have clear applications in auto-
matic summarization, information retrieval, or pre-
dicting real-world events such as how well a new
product is being received or the outcome of a popu-
lar vote on a topic (Bollen et al, 2011).
In this paper, we focus on an important initial task
for the recognition of argumentative structure: auto-
matic identification of agreement and disagreement.
We introduce the ARGUE corpus, an annotated col-
lection of 109,553 forum posts (11,216 discussion
threads) from the debate website 4forums.com. On
4forums, a person starts a discussion by posting a
topic or a question in a particular category, such as
society, politics, or religion. Some example topics
can be seen in Table 1. Forum participants can then
post their opinions, choosing whether to respond di-
rectly to a previous post or to the top level topic (start
a new thread). These discussions are essentially di-
alogic; however the affordances of the forum such
as asynchrony, and the ability to start a new thread
rather than continue an existing one, leads to dia-
logic structures that are different than other multi-
party informal conversations (Fox Tree, 2010). An
additional source of dialogic structure in these dis-
cussions, above and beyond the thread structure, is
the use of the quote mechanism, in which partici-
pants often break a previous post down into the com-
ponents of its argument and respond to each compo-
nent in turn. Many posts include quotations of previ-
ous posts. Because we hypothesize that these posts
are more targeted at a particular proposition that the
poster wants to comment on, than posts and replies
in general, we focus here on understanding the rela-
tionship between a quoted text and a response, and
the linguistic reflexes of those relationships. Exam-
ples of quote/response pairs for several of our topics
are provided in Figure 1.
The most similar work to our own is that of
Wang & Rose (2010) who analyzed Usenet fo-
rum quote/response structures. This work did not
distinguish agreement vs. disagreement across
quote/response pairs. Rather they show that they can
use a variant of LSA to improve accuracy for identi-
fying a parent post, given a response post, with 70%
accuracy. Other similar work uses Congressional
debate transcripts or blogs or other social media to
develop methods for distinguishing agreement from
disagreement or to distinguish rebuttals from out-of-
context posts (Thomas et al, 2006; Bansal et al,
2008; Awadallah et al, 2010; Walker et al, ; Bur-
foot, 2008; Mishne and Glance, 2006; Popescu and
Pennacchiotti, 2010). These methods are directly
applicable, but the genre of the language is so dif-
ferent from our informal forums that the results are
not directly comparable. Work by Somasundaran &
Wiebe (2009, 2010) has examined debate websites
and focused on automatically determining the stance
of a debate participant with respect to a particular is-
sue. This work has treated each post as a text to be
classified in terms of stance, for a particular topic,
and shown that discourse relations such as conces-
sions and the identification of argumentation triggers
improves performance . Their work, along with oth-
ers, also indicates that for such tasks it is difficult to
beat a unigram baseline (Pang and Lee, 2008). Other
work has focused on the social network structure
of online forums (Murakami and Raymond, 2010;
Agrawal et al, 2003). However, Agarwal?s work as-
sumed that adjacent posts always disagree, and did
not use any of the information in the text. Murakami
& Raymond (2010) show that simple rules defined
on the textual content of the post can improve over
Agarwal?s results.
Section 2 discusses our corpus in more detail, de-
scribes how we collected annotations using Mechan-
ical Turk, and presents results of a corpus analysis
of the use of particular discourse cues. Section 3 de-
scribes how we set up classification experiments for
distinguishing agreement from disagreement, and
Section 4 presents our results for agreement classifi-
cation. We also characterize the linguistic reflexes of
this relation. We analyze the utility of meta-post fea-
tures, contextual features, dependency features and
word-based features for signaling the disagreement
relation. We show that using contextual and dia-
logic features we can achieve accuracies up to 68%
as compared to a unigram baseline of 63%.
2 Data and Corpus Analysis
Table 1 provides an overview of some of the charac-
teristics of our corpus by topic. Figure 2 shows the
wording of the survey questions that we posted for
each quote/response as Mechanical Turk hits.
3
Topic Discs Posts NumA P/A A>1P PL Agree Sarcasm Emote Attack Nasty
evolution 872 10292 580 17.74 76% 576 10% 6% 16% 13% 9%
gun control 825 7968 411 19.39 66% 521 11% 8% 21% 16% 12%
abortion 564 7354 574 12.81 69 % 454 9% 6% 31% 16% 12%
gay marriage 305 3586 342 10.49 69% 522 13% 9% 23% 12% 8%
existence of God 105 1581 258 6.13 66% 569 11 % 7% 26% 14% 10%
healthcare 81 702 112 6.27 64% 522 14% 10% 34% 17% 17%
communism vs. capitalism 38 585 110 5.32 59% 393 23% 8% 15% 8% 0%
death penalty 25 500 138 3.62 62% 466 25% 5% 5% 5% 5%
climate change 40 361 116 3.11 55% 375 20% 9% 17% 26% 17%
marijuana legalization 13 160 72 2.22 38% 473 5% 2% 20% 5% 5%
Table 1: Characteristics of Different Topics. KEY: Number of discussions and posts on the topic (Discs, Posts).
Number of authors (NumA). Posts per author (P/A). Authors with more than one post (A > 1P). Median post Length
in Characters (PL). The remainder of the columns are the annotations shown in Figure 2. Percentage of posts that
agree (Agree%), use sarcasm (Sarcasm%), are emotional (Emote), attack the previous poster (Attack), and are
nasty (Nasty). The scalar values are threshholded at -1,1.
Our corpus is derived from a debate oriented in-
ternet forum called 4forums.com. It is a typical in-
ternet forum built on the vBulletin software. People
initiate discussions (threads) and respond to others?
posts. Each thread has a tree-like dialogue structure.
Each post has author information and a timestamp
with minute resolution. Many posts include quota-
tions of previous posts. For this work we chose to
focus on quotations because they establish a clear re-
lationship between the quoted text and the response.
Our corpus consists of 11,216 discussions and
109,553 posts by 2764 authors. We hand annotated
discussions for topic from a set of previously identi-
fied contentious political and social issues. The web-
site is tailored to a US audience and our topics are
somewhat US centric. Table 1 describes features of
our topics in order of decreasing discussion count.
When restricted to these topics, the corpus consists
of 2868 discussions, 33,089 posts, and 1302 authors.
Many posts include quotations. Overall 60,382
posts contain one or more quotation. Within our
topics of interest, nearly 20,000 posts contain quota-
tions. We defined a quote-response pair (Q-R pair)
where the response was the portion of the respond-
ing post directly following a quotation but preceding
any additional quotations.
We selected 10,003 Q-R pairs from the topics
of interest for a Mechanical Turk annotation task.
These were biased by cue word to ensure adequate
data for discourse marker analysis (See Section 2.1.
For this task we showed annotators seven Q-R pairs
and asked them to judge Agreement/Disagreement
and a set of other measures as shown in Figure 2.
Most of our measures were scalar; we chose to do
this because previous work on estimating the rela-
tionship between MTurk annotations and expert an-
notations suggest that taking the means of scalar
annotations could be a good way to reduce noise
in MTurk annotations (Snow et al, 2008). For all
of the measures annotated, the Turkers were not
given additional definitions of their meaning. For
example, we let Turkers to use their native intu-
itions about what it means for a post to be sarcas-
tic, since previous work suggests that non-specialists
tend to collapse all forms of verbal irony under the
term sarcastic (Bryant and Fox Tree, 2002). We
did not ask Turkers to distinguish between sarcasm
and other forms of verbal irony such as hyperbole,
understatement, rhetorical questions and jocularity
(Gibbs, 2000).
Agreement was a scalar judgment on an 11 point
scale [-5,5] implemented with a slider. The anno-
tators were also able to signal uncertainty with an
CAN?T TELL option. Each of the pairs was anno-
tated by 5-7 annotators. We showed the first 155
characters of each quote and each response. We also
provided a SHOW MORE button which expanded the
post to its full length. After annotation, we removed
a number of Q-R pairs in cases where a clear link
between the quote and a previous post could not be
established, e.g. the source quoted was not another
post, but the NY Times. This left us with 8,242 Q-R
pairs for our final analysis. Resampling to a natural
distribution left us with 2,847 pairs which we used
to build our machine learning test set. We used the
remaining annotated and unannotated pairs for de-
4
velopment.
Type ? Survey Question
S 0.62 Agree/Disagree: Does the respondent agree or dis-
agree with the prior post?
S 0.32 Fact/Emotion: Is the respondent attempting to
make a fact based argument or appealing to feel-
ings and emotions?
S 0.42 Attack/Insult: Is the respondent being support-
ive/respectful or are they attacking/insulting in
their writing?
B 0.22 Sarcasm: Is the respondent using sarcasm?
S 0.46 Nice/Nasty: Is the respondent attempting to be nice
or is their attitude fairly nasty?
Figure 2: Mechanical Turk Annotations (Binary = B and
Scalar = S) and level of agreement as Krippendorff?s ?.
Figure 3 provides examples from the end
points and means of the annotations for three
of the questions, Respect/Insult, Sarcasm, and
Fact/Emotion. Nice/Nasty and Respect/Insult are
strongly correlated by worker annotations r(54003)
= 0.84, p < 2.2e-16 and both weakly corre-
lated with Agree/Disagree ratings (r(54003) = 0.32
and r(54003)=0.36, respectively; p < 2.2e-16)
and Fact/Emotion ratings (r(54003) = 0.32 and
r(54003)=0.31, respectively; p < 2.2e-16), while
Agree/Disagree and Fact/Emotion ratings show the
smallest correlation, r(54003)=0.11, p < 2.2e-16.
For the linguistic marker correlations discussed be-
low we averaged scores across annotators, a process
which sharpened correlations (e.g., Respect/Insult
means correlate with Agree/Disagree means more
strongly (r(5393) = 0.51) as well as Nice/Nasty
means (r(5393) = 0.91; Agree/Disagree is far less
correlated with Fact/Emotion (r(5393) = 0.07). In-
terannotator agreement was computed using Krip-
pendorff?s ? (due to the variability in number of an-
notators that completed each hit), assuming an ordi-
nal scale for all measures except sarcasm; see Fig-
ure 2. The low agreement for Sarcasm accords with
native intuition ? it is the class with the least de-
pendence on lexicalization and the most subject to
inter-speaker stylistic variation. The relatively low
results for Fact/Emotion is perhaps due to the emo-
tional charge many ideological arguments engender;
informal examination of posts that showed the most
disagreement in this category often showed a cut-
ting comment or a snide remark at the end of a post,
which was was ignored by some annotators and ev-
idence for others (one Emotional post in Figure 3 is
clearly an insult, but was uniformly labeled as -5 by
all annotators).
2.1 Discourse Markers
Both psychological research on discourse processes
(Fox Tree and Schrock, 1999; Fox Tree and Schrock,
2002; Groen et al, 2010) and computational work
on agreement (Galley et al, 2004) indicate that dis-
course markers are strongly associated with partic-
ular pragmatic functions. Because of their salient
position, we test the role of turn-initial markers in
predicting upcoming content (Fox Tree and Schrock,
2002; Groen et al, 2010). Based on manual inspec-
tion of a subset of the corpus, we constructed a list of
20 discourse markers; 17 of these occurred at least
50 times in a quote response (upper bound of 700
samples): actually, and, because, but, I believe, I
know, I see, I think, just, no, oh, really, so, well,
yes, you know, you mean. All of their occurrences
became part of the 10,003 Q-R pairs annotated.
The top discourse markers highlighting disagree-
ment were really (67% read a response beginning
with this marker as prefacing a disagreement with
a prior post), no (66%), actually (60%), but (58%),
so (58%), and you mean (57%). At this point, the
next most disagreeable category was the unmarked
category, with about 50% of respondents interpret-
ing an unmarked post as disagreeing. On the other
hand, the most agreeable marker was yes (73% read
a response beginning with this marker as prefacing
an agreement) followed by I know (64%), I believe
(62%), I think (61%), and just (57%). The other
markers were close to the unmarked category: and
(50%), because (51%), oh (51%), I see (52%), you
know (54%), and well (55%).
The overall agreement on sarcasm was low, as in
other computational work on recognizing sarcasm
(Davidov et al, 2010). At most, only 31% of re-
spondents agreed that the material after a discourse
marker was sarcastic, with the most sarcastic mark-
ers being you mean (31%), oh (29%), really (24%),
so (22%), and I see (21%). Only 15% of respon-
dents rated the unmarked category as sarcastic (e.g.,
fewer than 1 out of 6 respondents). The cues I think
(10%), I believe (9%), and actually (10%) were the
least sarcastic markers.
Taken together, these ratings suggest that the cues
really, you mean, and so can be used to indicate both
5
Class Very High Degree Neutral Very Low Degree
Insult
or
Attack
Well, you have proven yoruself to be a
man with no brain, that is for sure. The
definition that was given was the one that
scientists use, not the layperson.
The empire you defend is tyrannical.
They are responsible for the death of mil-
lions.
Very well put.
Is that what you said right be-
fore they started banning assault
weapons?...Obviously, you?re gullible.
Since you?re such a brainiac and all, why
don?t you visit the UN website and see
what your beloved UN is up to?
Bad comparisons. A fair comparison
would be comparing the total number of
defensive gun uses to the total number
of gun crimes (not just limiting it to gun
homicides).
In some cases yes, in others no. If the
mutation gives a huge advantage, then
there will be a decline in the size of the
gene pool for a while (eg when the Aus-
tralian rabbit population...
Sarcasm My pursuit of happiness is denied by
trees existing. Let?s burn them down and
destroy the environment. It?s much bet-
ter than me being unhappy.
An interesting analysis of that article you
keep quoting from the World Net Daily
[url]
I would suggest you look at the faero is-
land mouse then. That is a new species,
and it is not man doing it, but rather na-
ture itself.
Like the crazy idea the Earth goes around
the Sun.
Indeed there is no diffrence it is still a
dead baby but throwing a baby in a trash
can and leaving it for dead is far more
cruel than abortion.
Too late, drug usage has already created
those epidemics. Legalizing drugs may
increase some of them temporarily, but
they already exist.
Emotion-
based
Argu-
ment
Really! You can prove that most pro-
lifers don?t care about women?...it is id-
iotic thinking like this that makes me re-
spect you less and less.
Fine by me. First, I don?t consider hav-
ing a marriage recognized by govern-
ment to be a ?right?. Second, I?ve said
many times I don?t think government
should be in the marriage business at all.
Sure. Here is an explanation. The 14C
Method. That is from the Radiocarbon
WEB info site by the Waikato Radio-
carbon Dating Lab of the University of
Waikato (New Zeland).
I love Jesus John the Beloved is my most
favorite writer throughout time If you
think I have a problem with a follower
of Jesus your wrong. I have a problem
with the Christians
I agree that the will to survive is an amaz-
ing phenomenon when put to the test.
But I do not agree with your statement
of life at *any* cost. There will always
be a time when the humane/loving thing
to do is to let an infant/child/adult go.
Heller is about determining the answer to
a long standing question on the nature of
the Second Amendment, and how much
gun control is legally allowed. Roe v.
Wade is about finding legal precedent for
the murder of unborn children. I see ab-
solutely no comparison between the two.
Figure 3: Sample Responses for the Insult, Sarcasm, and Fact/Feeling spectrums
disagreement and sarcasm. However, but, no, and
actually can be used for disagreement, but not sar-
casm. And I know (14% sarcastic, similar to None),
I believe, and I think can be used for non-sarcastic
agreement.
From informal analyses, we hypothesized that re-
ally and oh might indicate sarcasm. While we found
evidence supporting this for really, it was not the
case for oh. Instead, oh was used to indicate emo-
tion; it was the discourse marker with the highest
ratings of feeling over fact.
Despite the fact that it would seem that disagree-
ment would be positively correlated with sarcasm,
disagreement and sarcasm were not related. There
were two tests possible. One tested the percentage of
people who identified an item as disagreeing against
the percentage of people who identified it as sar-
casm, r(16) = -.27, p = .27 (tested on 17 discourse
markers plus the None category). The other tested
the degree of disagreement (from -5 to +5) against
the percentage of people who identified the post as
sarcastic, r(16) = -.33, p = .18.
However, we did observe relationships between
sarcasm and other variables. Two results support the
argument that sarcasm is emotional and personal.
The more sarcastic, the nastier (rather than nicer),
r(16) = .87, p < .001. In addition, the more sarcas-
tic, the more emotional (over factual) respondents
were judged to be, r(16) = .62, p = .006 Taken to-
gether, these analyses suggest that sarcasm is emo-
tional and personal, but not necessarily a sign of dis-
agreement.
3 Machine Learning Experimental Setup
For our experiments we used the Weka machine
learning toolkit. All results are from 10 fold cross-
validation on a balanced test set. Unless otherwise
mentioned, we used thresholds of 1 and -1 on the
mean agreement judgment to determine agreement
6
and disagreement respectively. We omitted those Q-
R pairs which were judged neutral (mean annotator
judgment in the (-1,1) range).
As described above, from the original 10,003 Q-
R pairs we applied certain constraints (notably re-
quirement that we be able to identify the originating
post) which left us with 8,242. We then resampled
to obtain a natural distribution leaving us with 2,847
pairs. Applying the (-1,1) threshold and balancing
the result yielded a test set of 682 Q-R pairs.
3.1 Classifiers
Our experiments used two simple classifiers: Naive-
Bayes and JRip. NaiveBayes makes a strict indepen-
dence assumption and can be swamped by the sheer
number of features we used, but it is a solid baseline
and does a decent job of suggesting which features
are more powerful. JRip is a rule based classifier
which produces a compact model suitable for hu-
man consumption and quick application. JRip is not
without its own limitations but, for our task, it shows
better results than NaiveBayes. The model it builds
uses only a handful of features.
3.2 Feature Extraction
Our aim was to develop features for the automatic
identification of agreement and disagreement that
would do well on the task and provide useful base-
lines for comparisons with previous and future work.
Features are grouped into sets as shown in Table 2
and discussed in more detail below.
Set Description/Examples
MetaPost Non-lexical features. E.g. posterid, time be-
tween posts, etc.
Unigrams,
Bigrams
Word and Word Pair frequencies
Cue Words Initial unigram, bigram, and trigram
Punctuation Collapsed into one of the following: ??, !!, ?!
LIWC LIWC measures and frequencies
Dependencies Dependencies derived from the Stanford
Parser.
Generalized De-
pendencies
Dependency features generalized with re-
spect to POS of the head word and opinion
polarity of both words.
Table 2: Feature Sets, Descriptions, and Examples
Unigrams, Bigrams, Trigrams. Results of pre-
vious work suggest that a unigram baseline can be
difficult to beat for certain types of debates (Walker
et al, ; Somasundaran and Wiebe, 2010). Thus we
derived both unigrams and bigrams as features. We
captured the final token as a feature by padding with
-nil- tokens when building the bigrams. See below
for comments on initial uni/bi/tri-grams.
MetaPost Info. Previous work suggested that
non-lexical features like poster ids and the time be-
tween posts might contain indicators of disagree-
ment. People on these forums get to know one an-
other and often enjoy repeatedly arguing with the
same person. In addition, we hypothesized that the
?heat? of a particular conversation could be corre-
lated with rapid-fire exchanges, as indicated by short
time periods between posts.
Thus these features involve structure outside of
the quote/response text. This includes author infor-
mation, time between posts, the log10 of the time
between posts, the number of other quotes in the
response, whether the quote responds to a post by
the response?s author, the percent of the quoted post
which is actually quoted, whether the quoted post is
by the same author as the response (there were only
an handful of these), whether the response mentions
the quote author by name, and whether the response
is longer than the quote.
The forum software effectively does this annota-
tion for us so there is no reason not to consider it as
a clue in our quest to understand and interpret online
dialogue.
Discourse Markers. Previous work on dialogue
analysis has repeatedly noted the discourse func-
tions of particular discourse markers, and our corpus
analysis above also suggests their use in this par-
ticular dataset (Hirschberg and Litman, 1993; Fox
Tree, 2010; Schiffrin, 1987; Di Eugenio et al, 1997;
Moser and Moore, 1995). However, because dis-
course markers can be stacked up Oh, so really we
decided to represent this feature as post initial uni-
grams, bigrams and trigrams.
Repeated Punctuation. Informal analyses of our
data suggested that repeated sequential use of partic-
ular types of punctuation such as !! and ?? did not
mean the same thing as simple counts or frequen-
cies of punctuation across a whole post. Thus we
developed distinct features for a subset of these rep-
etitions.
LIWC. We also derived features using the Lin-
guistics Inquiry Word Count tool (LIWC-2001)
(Pennebaker et al, 2001). LIWC classifies words
7
into 69 categories and counts how many words get
classified into each category. Some LIWC features
that we expect to be important are words per sen-
tence (WPS), pronominal forms, and positive and
negative emotion words.
Dependency and Generalized Dependency. We
used the Stanford parser to extract dependency fea-
tures for each quote and response (De Marneffe et
al., 2006; Klein and Manning, 2003). The depen-
dency parse for a given sentence is a set of triples,
composed of a grammatical relation and the pair
of words for which the grammatical relation holds
(reli, wj , wk), where reli is the dependency relation
among words wj and wk. The word wj is the HEAD
of the dependency relation.
Following (Joshi and Penstein-Rose?, 2009) we ex-
tracted generalized dependency features by leaving
one dependency element lexicalized and generaliz-
ing the other to part of speech. Joshi & Rose?s re-
sults suggested that this approach would work better
than either fully lexicalized or fully generalized de-
pendency features.
Opinion Dependencies. Somasundaran & Wiebe
(2009) introduce the concept of features that iden-
tify the TARGET of opinion words. Inspired by this
approach, we used the MPQA dictionary of opinion
words to select the subset of dependency and gen-
eralized dependency features in which those opin-
ion words appear. For these features we replace the
opinion words with their positive or negative polar-
ity equivalents.
Cosine Similarity. This feature is based on previ-
ous work on threading. We derive cosine-similarity
measure using tf-idf vectors where the document
frequency was derived from the entire topic re-
stricted corpus.
Annotations. We also add features represent-
ing information that we do not currently derive au-
tomatically, but which might be automatically de-
rived in future work based on annotations in the cor-
pus. These include the topic and Mechanical Turk
annotations for Fact/Emotion, Respect/Insult, Sar-
casm, and Nasty/Nice, which could reasonably be
expected to be recognized independently of Agree-
ment/Disagreement.
Feature
type
Selected Features
Meta number-of-other-quotes, percent-quoted, author-quote-
USERNAME
Initial
n-gram
yes, so, I agree, well said, really?, I don?t know
Bigram that you, ? -nil-, you have, evolution is
Depend-
ency
dep-nsubj(agree, i), dep-nsubj(think, you), dep-prep-
with(agree, you)
Opinion
Depen-
dency
dep-opinion-nsubj(negative, you), dep-opinion-
dep(proven, negative), dep-opinion-aux(positive,
to)
Anno-
tations
topic-gay marriage, mean-response-nicenasty, mean-
unsure-sarcasm
Table 3: Some of the more useful features for each cate-
gory, using ?2 for feature selection.
Figure 4: Sample model learned using JRip. The num-
bers represent (total instances covered by a rule / number
incorrectly labeled). This particular model was built on
development data.
4 Results
Table 3 shows features which were selected for each
of our feature categories using a ?2 test for fea-
ture selection. These results vindicate our interest
in discourse markers as cues to argument structure,
as well as the importance of the generalized depen-
dency features and opinion target pairs (Wang and
Rose?, 2010; Somasundaran and Wiebe, 2009). Fig-
ure 4 shows a sample model learned using JRip.
We limit our pair-wise comparisons between clas-
sifiers and feature sets to those corresponding to par-
8
Feats NB JRip?2
Uni,UniCue 0.578 0.626
BOW 0.598 0.654
Meta 0.579 0.588
Response Local 0.600 0.666
Quote Local 0.531 0.588
Both Local 0.601 0.682
Meta+Local 0.603 0.654
All 0.603 0.632
Just Annotations 0.765 0.814
All+Annotations 0.603 0.795
Table 4: Accuracies on a balanced test set (random base-
line: 0.5). NB = NaiveBayes. JRip?2 = Jripper with ?2
feature selection on the training set during cross valida-
tion. BOW = Unigrams, CueWords, Bigrams, Trigrams,
LIWC, Repeated Punctuation. Response/Quote/Both
Local uses only those features which exist in the text of
the response or quote respectively. It consists of LIWC,
dependencies, generalized dependencies, the various n-
grams, and length measures.
ticular hypotheses. We conducted five tests with
Bonferroni correction to .01 for a .05 level of sig-
nificance.
While we hypothesized that more sophisticated
linguistic features would improve over unigram fea-
tures alone, a paired t-test using the results in Table 4
indicate that there is no statistical difference be-
tween the performance of JRip using only response
local features (JRip,ResponseLocal), as compared to
the Unigram,UniCue features (t(9) = 2.18, p = .06).
However, a paired t-test using the results in
Table 4 indicate that there is a statistical dif-
ference between the performance of JRip using
local features from both the quote and the re-
sponse, (JRip,BothLocal) as compared to the Uni-
gram,UniCue features (t(9) = 3.94, p =.003). This
shows that the contextual features do matter, even
though (JRip,BothLocal) does not provide signifi-
cant improvements over (JRip,Response Local) (t(9)
= .92, p = .38).
In general, examination of the table suggests
that the JRip classifier performs better than Naive
Bayes. A paired t-test indicates that there is a sta-
tistical difference between the performance of JRip
using local features from both the quote and the
response, (JRip,BothLocal) (JRip,BothLocal) and
Naive Bayes using local features from both the quote
and the response, (NB,BothLocal) (t(9) = 3.43, p =
.007).
In addition, with an eye toward the future, we ex-
amined whether automatic recognition of sarcasm,
attack/insult, fact/feeling nice/nasty could possibly
improve results for recognizing disagreement. Us-
ing the human annotations as a proxy for automatic
results, we get classification accuracies of over 81%
(JRip,JustAnnotations). This suggests it might be
possible to improve results over our best current re-
sults (JRip,BothLocal) (t(9) = 6.09, p < .001).
Another interesting fact, is that despite its use in
previous work for threading, the cosine similarity
between the quote and response did not improve ac-
curacy for the classifiers we tested, over and above
the use of text-based contextual features. Further
investigation is required to draw conclusions about
this or similar metrics (LSA, PMI, etc.).
5 Discussion and Conclusion
In this paper, we have introduced a new collection
of internet forum posts, the ARGUE corpus, col-
lected across a range of ideological topics, and con-
taining scalar Agreement/Disagreement annotations
over quote-response pairs within a post. We have
demonstrated that we can achieve a significant im-
provement over a unigram baseline agreement de-
tection system using features from both a response
and the quote being responded to.
Beyond agreement, the ARGUE corpus contains
finer-grained annotations for degrees of insult, nas-
tiness, and emotional appeal, as well as the pres-
ence of sarcasm. We have demonstrated that these
classes (especially insult and nastiness) correlate
with agreement. While the utility of these classes
as features for agreement detection is dependent on
how easily they are learned, in closing we note that
they also afford us a richer understanding of how ar-
gumentative conversation flows. In section 2.1.2, we
outlined how they can yield understanding of the po-
tential functions of a discourse particle within a par-
ticular post. They may as allow us to understand the
extent to which participants react in kind, rewarding
insult with insult or kindness in turn. In future work,
we hope to turn to these conversational dynamics.
In future work, it would be useful to
build a ternary classifier which labels
9
Agree/Disagree/Neutral, thus reflecting the true
distribution of these dialogue acts in the data.
Additionally, the proportion of agreeing utterances
varies widely across media so it may be desirable to
add an appropriate prior when adapting the model
to a new dataset.
Acknowledgments
This work was funded by Grant NPS-BAA-03 to
UCSC and and through the Intelligence Advanced
Research Projects Activity (IARPA) through the
Army Research Laboratory. We?d like to thank
Craig Martell for helpful discussions over the course
of this project, and the anonymous reviewers for use-
ful feedback. We would also like to thank Michael
Minor and Jason Aumiller for their contributions to
scripting and the database.
References
R. Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu. 2003.
Mining newsgroups using networks arising from so-
cial behavior. In Proceedings of the 12th international
conference on World Wide Web, pages 529?535. ACM.
J.L. Austin. 1965. How to do things with words. Oxford
University Press, New York.
R. Awadallah, M. Ramanath, and G. Weikum. 2010.
Language-model-based pro/con classification of polit-
ical text. In Proceeding of the 33rd international ACM
SIGIR conference on Research and development in in-
formation retrieval, pages 747?748. ACM.
M. Bansal, C. Cardie, and L. Lee. 2008. The power
of negative thinking: Exploiting label disagreement in
the min-cut classification framework. Proceedings of
COLING: Companion volume: Posters, pages 13?16.
J. Bollen, H. Mao, and X. Zeng. 2011. Twitter mood
predicts the stock market. Journal of Computational
Science.
G.A. Bryant and J.E. Fox Tree. 2002. Recognizing ver-
bal irony in spontaneous speech. Metaphor and sym-
bol, 17(2):99?119.
C. Burfoot. 2008. Using multiple sources of agree-
ment information for sentiment classification of polit-
ical transcripts. In Australasian Language Technology
Association Workshop 2008, volume 6, pages 11?18.
Robert B. Cialdini. 2000. Influence: Science and Prac-
tice (4th Edition). Allyn & Bacon.
D. Davidov, O. Tsur, and A. Rappoport. 2010. Semi-
supervised recognition of sarcastic sentences in twitter
and amazon. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning,
pages 107?116. Association for Computational Lin-
guistics.
M.C. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of LREC, vol-
ume 6, pages 449?454. Citeseer.
Barbara Di Eugenio, Johanna D. Moore, and Massimo
Paolucci. 1997. Learning features that predict cue
usage. In Proceedings of the 35th Annual Meet-
ing of the Association for Computational Linguistics,
ACL/EACL 97, pages 80?87.
J.E. Fox Tree and J.C. Schrock. 1999. Discourse Mark-
ers in Spontaneous Speech: Oh What a Difference
an Oh Makes. Journal of Memory and Language,
40(2):280?295.
J.E. Fox Tree and J.C. Schrock. 2002. Basic mean-
ings of you know and I mean. Journal of Pragmatics,
34(6):727?747.
J. E. Fox Tree. 2010. Discourse markers across speak-
ers and settings. Language and Linguistics Compass,
3(1):113.
M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg.
2004. Identifying agreement and disagreement in
conversational speech: Use of bayesian networks to
model pragmatic dependencies. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, pages 669?es. Association for Com-
putational Linguistics.
R.W. Gibbs. 2000. Irony in talk among friends.
Metaphor and Symbol, 15(1):5?27.
M. Groen, J. Noyes, and F. Verstraten. 2010. The Effect
of Substituting Discourse Markers on Their Role in
Dialogue. Discourse Processes: A Multidisciplinary
Journal, 47(5):33.
Julia Hirschberg and Diane Litman. 1993. Empirical
studies on the disambiguation of cue phrases. Com-
putational Linguistics, 19(3):501?530.
John E. Hunter. 1987. A model of compliance-
gaining message selection. Communication Mono-
graphs, 54(1):54?63.
M. Joshi and C. Penstein-Rose?. 2009. Generalizing de-
pendency features for opinion mining. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, pages 313?316. Association for Computational
Linguistics.
D. Klein and C.D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics-
Volume 1, pages 423?430. Association for Computa-
tional Linguistics.
G. Mishne and N. Glance. 2006. Leave a reply: An anal-
ysis of weblog comments. In Third annual workshop
on the Weblogging ecosystem. Citeseer.
10
Margaret G. Moser and Johanna Moore. 1995. Inves-
tigating cue selection and placement in tutorial dis-
course. In ACL 95, pages 130?137.
A. Murakami and R. Raymond. 2010. Support or
Oppose? Classifying Positions in Online Debates
from Reply Activities and Opinion Expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 869?875.
Association for Computational Linguistics.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1?135.
J. W. Pennebaker, L. E. Francis, and R. J. Booth, 2001.
LIWC: Linguistic Inquiry and Word Count.
A.M. Popescu and M. Pennacchiotti. 2010. Detecting
controversial events from twitter. In Proceedings of
the 19th ACM international conference on Information
and knowledge management, pages 1873?1876. ACM.
Deborah Schiffrin. 1987. Discourse Markers. Cam-
bridge University Press, Cambridge, U.K.
R. Snow, B. O?Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast?but is it good?: evaluating non-expert
annotations for natural language tasks. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 254?263. Association for
Computational Linguistics.
S. Somasundaran and J. Wiebe. 2009. Recognizing
stances in online debates. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1-Volume
1, pages 226?234. Association for Computational Lin-
guistics.
S. Somasundaran and J. Wiebe. 2010. Recognizing
stances in ideological on-line debates. In Proceedings
of the NAACL HLT 2010 Workshop on Computational
Approaches to Analysis and Generation of Emotion in
Text, pages 116?124. Association for Computational
Linguistics.
M. Thomas, B. Pang, and L. Lee. 2006. Get out the
vote: Determining support or opposition from Con-
gressional floor-debate transcripts. In Proceedings of
the 2006 conference on empirical methods in natural
language processing, pages 327?335. Association for
Computational Linguistics.
Marilyn Walker, Rob Abbott, Pranav Anand, Jean E. Fox
Tree, Robeson Bowmani, and Michael Minor. Cats
Rule and Dogs Drool: Classifying Stance in Online
Debate.
Y.C. Wang and C.P. Rose?. 2010. Making conversational
structure explicit: identification of initiation-response
pairs within online discussions. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 673?676. Association for
Computational Linguistics.
11
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 1?9,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Cats Rule and Dogs Drool!: Classifying Stance in Online Debate
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E. Fox Tree,
Robeson Bowmani, and Michael Minor
University of California Santa Cruz
Abstract
A growing body of work has highlighted the
challenges of identifying the stance a speaker
holds towards a particular topic, a task that in-
volves identifying a holistic subjective dispo-
sition. We examine stance classification on
a corpus of 4873 posts across 14 topics on
ConvinceMe.net, ranging from the playful to
the ideological. We show that ideological de-
bates feature a greater share of rebuttal posts,
and that rebuttal posts are significantly harder
to classify for stance, for both humans and
trained classifiers. We also demonstrate that
the number of subjective expressions varies
across debates, a fact correlated with the per-
formance of systems sensitive to sentiment-
bearing terms. We present results for iden-
tifing rebuttals with 63% accuracy, and for
identifying stance on a per topic basis that
range from 54% to 69%, as compared to un-
igram baselines that vary between 49% and
60%. Our results suggest that methods that
take into account the dialogic context of such
posts might be fruitful.
1 Introduction
Recent work has highlighted the challenges of iden-
tifying the STANCE that a speaker holds towards a
particular political, social or technical topic. Clas-
sifying stance involves identifying a holistic subjec-
tive disposition, beyond the word or sentence (Lin
et al, 2006; Malouf and Mullen, 2008; Greene and
Resnik, 2009; Somasundaran and Wiebe, 2009; So-
masundaran and Wiebe, 2010). Our work is inspired
by the large variety of such conversations now freely
available online, and our observation that the contex-
tual affordances of different debate and discussion
websites vary a great deal. One important contex-
tual variable, discussed at length below, is the per-
centage of posts that are rebuttals to previous posts,
which varies in our data from 34% to 80%. The abil-
ity to explicitly rebut a previous post gives these de-
bates both monologic and dialogic properties (Biber,
1991; Crystal, 2001; Fox Tree, 2010); Compare Fig-
ure 1 to Figure 2. We believe that discussions con-
taining many rebuttal links require a different type of
analysis than other types of debates or discussions.
Dialogic Capital Punishment
Studies have shown that using the death penalty saves 4 to 13 lives
per execution. That alone makes killing murderers worthwhile.
What studies? I have never seen ANY evidence that capital pun-
ishment acts as a deterrant to crime. I have not seen any evidence
that it is ?just? either.
When Texas and Florida were executing people one after the other
in the late 90?s, the murder rates in both states plunged, like Rosie
O?donnel off a diet.. .
That?s your evidence? What happened to those studies? In the
late 90s a LOT of things were different than the periods preceding
and following the one you mention. We have no way to determine
what of those contributed to a lower murder rate, if indeed there
was one. You have to prove a cause and effect relationship and
you have failed.
Figure 1: Capital Punishment discussions with posts
linked via rebuttal links.
This paper utilizes 1113 two-sided debates (4873
posts) from Convinceme.net for 14 different debate
topics. See Table 1. On Convinceme, a person starts
a debate by posting a topic or a question and provid-
ing sides such as for vs. against. Debate participants
can then post arguments for one side or the other, es-
sentially self-labelling their post for stance. These
debates may be heated and emotional, discussing
weighty issues such as euthanasia and capital pun-
ishment, such as the example in Figure 1. But they
also appear to be a form of entertainment via playful
1
debate. Popular topics on Convinceme.net over the
past 4 years include discussions of the merits of Cats
vs. Dogs, or Pirates vs. Ninjas (almost 1000 posts).
See Figure 3.
Monologic Capital Punishment
I value human life so much that if someone takes one than his
should be taken. Also if someone is thinking about taking a life
they are less likely to do so knowing that they might lose theirs
Death Penalty is only a costlier version of a lifetime prison sen-
tence, bearing the exception that it offers euthanasia to criminals
longing for an easy escape, as opposed to a real punishment.
There is no proof that the death penalty acts as a deterrent, plus
due to the finalty of the sentence it would be impossible to amend
a mistaken conviction which happens with regualrity especially
now due to DNA and improved forensic science.
Actually most hardened criminals are more afraid to live-then die.
I?d like to see life sentences without parole in lieu of capital pun-
ishment with hard labor and no amenities for hard core repeat
offenders, the hell with PC and prisoner?s rights-they lose priv-
eledges for their behaviour.
Figure 2: Posts on the topic Capital punishment without
explicit link structure. The discussion topic was ?Death
Penalty?, and the argument was framed as yes we should
keep it vs. no we should not.
Our long term goal is to understand the dis-
course and dialogic structure of such conversations.
This could be useful for: (1) creating automatic
summaries of each position on an issue (Sparck-
Jones, 1999); (2) gaining a deeper understanding
of what makes an argument persuasive (Marwell
and Schmitt, 1967); and (3) identifying the lin-
guistic reflexes of perlocutionary acts such as per-
suasion and disagreement (Walker, 1996; Greene
and Resnik, 2009; Somasundaran and Wiebe, 2010;
Marcu, 2000). As a first step, in this paper we aim
to automatically identify rebuttals, and identify the
speaker?s stance towards a particular topic.
Dialogic Cats vs. Dogs
Since we?re talking much of $hit, then Dogs rule! Cat poo is ex-
tremely foul to one?s nostrils you?ll regret ever handling a cat.
Stick with dogs, they?re better for your security, and poo?s not too
bad. Hah!
Dog owners seem infatuated with handling sh*t. Cat owners don?t
seem to share this infatuation.
Not if they?re dog owners who live in the country. If your dog
sh*ts in a field you aren?t going to walk out and pick it up.
Cat owners HAVE to handle sh*t, they MUST clean out a litter
box...so suck on that!
Figure 3: Cats vs. Dogs discussions with posts linked by
rebuttal links.
The most similar work to our own is that of So-
masundaran & Wiebe (2009, 2010) who also focus
on automatically determining the stance of a debate
participant with respect to a particular issue. Their
data does not provide explicit indicators of dialogue
structure such as are provided by the rebuttal links
in Convinceme. Thus, this work treats each post as
a monologic text to be classified in terms of stance,
for a particular topic. They show that discourse re-
lations such as concessions and the identification of
argumentation triggers improves performance over
sentiment features alone (Somasundaran and Wiebe,
2009; Somasundaran and Wiebe, 2010). This work,
along with others, indicates that for such tasks it is
difficult to beat a unigram baseline (Pang and Lee,
2008).
Other similar related work analyzes Usenet forum
quote/response structures (Wang and Rose?, 2010).
We believe quote/response pairs have a similar dis-
course structure to the rebuttal post pairs in Con-
vinceme, but perhaps with the linguistic reflexes
of stance expressed even more locally. However
agreement vs. disagreement is not labelled across
quote/response pairs and Wang & Rose (2010) do
not attempt to distinguish these different discourse
relations. Rather they show that they can use a vari-
ant of LSA to identify a parent post, given a response
post, with approximately 70% accuracy. A recent
paper by (Abbott et al, 2011) examines agreement
and disagreement in quote/response pairs in idealog-
ical and nonidealogical online forum discussions,
and shows that you can distinguish the agreement
relation with 68% accuracy. Their results indicate
that contextual features do improve performance for
identifying the agreement relation between quotes
and responses.
Other work has utilized the social network struc-
ture of online forums, either with or without tex-
tual features of particular posts (Malouf and Mullen,
2008; Mishne and Glance, 2006; Murakami and
Raymond, 2010; Agrawal et al, 2003). However
this work does not examine the way that the dia-
logic structure varies by topic, as we do, and the
threading structure of their debates does not dis-
tinguish between agreement and disagreement re-
sponses. (Mishne and Glance, 2006) show that most
replies to blog posts are disagreements, while Agar-
wal?s work assumed that adjacent posts always dis-
agree, and did not use any of the information in the
text. Murakami & Raymond (2010) show that sim-
ple rules for identifying disagreement, defined on
the textual content of the post, can improve over
Agarwal?s results and (Malouf and Mullen, 2008)
show that a combination of textual and social net-
2
work features provides the best performance. We
leave the incorporation of social network informa-
tion for stance classification to future work.
Section 3 discusses our corpus in more detail, and
presents the results of a human debate-side classi-
fication task conducted on Mechanical Turk. Sec-
tion 3 describes two different machine learning ex-
periments: one for identifying rebuttals and the other
for automatically determining stance. Section 4
presents our results. We show that we can iden-
tify rebuttals with 63% accuracy, and that using sen-
timent, subjectivity and dialogic features, we can
achieve debate-side classification accuracies, on a
per topic basis, that range from 54% to 69%, as com-
pared to unigram baselines that vary between 49%
and 60%.
2 Corpus Description and Analysis
Table 1 provides an overview of our corpus. Our
corpus consists of 1113 two-sided debates (4873
posts) from Convinceme.net for 12 topics ranging
from playful debates such as Cats vs. Dogs to more
heated political topics such as Capital Punishment.
In Table 1, the topics above the line are either tech-
nical or playful, while the topics below the line are
ideological. In total the corpus consists of 2,722,340
words; the topic labeled debates which we use in our
experiments contain 507,827 words.
Convinceme provides three possible sources of
dialogic structure: (1) the SIDE that a post is placed
on indicates the poster?s stance with respect to the
original debate topic, and thus can be considered as a
response to that post; (2) REBUTTAL LINKS between
posts which are explicitly indicated by the poster us-
ing the affordances of the site; and (3) the TEMPO-
RAL CONTEXT of the debate, i.e. the state of the
debate at a particular point in time, which a debate
participant orients to in framing their post.
Topics vary a great deal in terms of their dialogic
structure and linguistic expression. In Table 1, the
columns providing counts for different variables are
selected to illustrate ways in which topics differ in
the form and style of the argument and in its sub-
jective content. One important variable is the per-
centage of the topic posts that are linked into a re-
buttal dialogic structure (Rebuttals). Some of these
differences can be observed by comparing the dia-
logic and monologic posts for the Capital Punish-
ment topic in Figures 1 and 2 to those for the Cats
vs. Dogs topic in Figures 3 and 4. Ideological
Monologic Cats vs. Dogs
First of all, cats are about a thousand times easier to care for.
You don?t have to walk them or bathe them because they?re smart
enough to figure out all that stuff on their own. Plus, they have the
common courtesy to do their business in the litter box, instead of
all over your house and yard. Just one of the many reasons cats
rule and dogs, quite literally drool!
Say, you had a bad day at work, or a bad breakup, you just wanna
go home and cry. A cat would just look at you like ?oh ok, you?re
home? and then walk away. A dog? Let?s see, the dog would most
likely wiggle its tail, with tongue sticking out and head tilted - the
?you?re home! i missed you so much, let?s go snuggle in front of
the TV and eat ice-cream? look. What more do I need to say?
Figure 4: Posts on the topic Cats vs. Dogs without ex-
plicit rebuttal links.
topics display more author investment; people feel
more strongly about these issues. This is shown by
the fact that there are more rebuttals per topic and
more posts per author (P/A) in the topics below the
line in Table 1. It follows that these topics have a
much higher degree of context-dependence in each
post, since posts respond directly to the parent post.
Rebuttals exhibit more markers of dialogic interac-
tion: greater pronominalization (especially you as
well as propositional anaphora such as that and it),
ellipsis, and dialogic cue words; Figure 5 shows the
difference in counts of ?you? between rebuttals and
non-rebuttals (Rebuttals x? = 9.6 and Non-Rebuttals
x? = 8.5, t(27) = 24.94, p < .001). Another indi-
cation of author investment is the percentage of au-
thors with more than one post (A > 1P). Post Length
(PL), on the other hand, is not significantly corre-
lated with degree of investment in the topic.
Figure 5: Kernel density estimates for ?you? counts across
rebuttals (green) and non-rebuttals (red).
Other factors we examined were words per sen-
3
Post and Threading Variables Normalized LIWC Variables
Topic Posts Rebuttals P/A A > 1p PL Pro WPS 6LTR PosE NegE
Cats v. Dogs 148 40% 1.68 26% 242 3.30 -1.95 -2. 43 1.70 .30
Firefox vs. IE 218 40% 1.28 16% 167 -0.11 -0.84 0.53 1.23 -0.81
Mac vs. PC 126 47% 1.85 24% 347 0.52 0.28 -0.85 -0.11 -1.05
Superman/Batman 140 34% 1.41 21% 302 -0.57 -1.78 -0.43 1.21 .99
2nd Amendment 134 59% 2.09 45% 385 -1.38 1.74 0.58 -1.04 0.38
Abortion 594 70% 2.82 43% 339 0.63 -0.27 -0.41 -0.95 0.68
Climate Change 202 69% 2.97 40% 353 -0.74 1.23 0.57 -1.25 -0.63
Communism vs. Capitalism 212 70% 3.03 47% 348 -0.76 -0.15 1.09 0.39 -0.55
Death Penalty 324 62% 2.44 45% 389 -0.15 -0.40 0.49 -1.13 2.90
Evolution 798 76% 3.91 55% 430 -0.80 -1.03 1.34 -0.57 -0.94
Exist God 844 77% 4.24 52% 336 0.43 -0.10 0.34 -0.24 -0.32
Gay Marriage 505 65% 2.12 29% 401 -0.13 .86 .85 -0.42 -0.01
Healthcare 110 80% 3.24 56% 280 0.28 1.54 .99 0.14 -0.42
Marijuana Legalization 214 52% 1.55 26% 423 0.14 0.37 0.53 -0.86 0.50
Table 1: Characteristics of Different Topics. Topics below the line are considered ?ideological?. Normalized LIWC
variable z-scores are significant when more than 1.94 standard deviations away from the mean (two-tailed).
KEY: Number of posts on the topic (Posts). Percent of Posts linked by Rebuttal links (Rebuttals). Posts per author
(P/A). Authors with more than one post (A > 1P). Post Length in Characters (PL). Pro = percent of the words as
pronominals. WPS = Words per sentence. 6LTR = percent of words that are longer than 6 letters. PosE positive
emotion words. NegE negative emotion words.
tence (WPS), the length of words used (6LTR)
which typically indicates scientific or low frequency
words, the use of pronominal forms (Pro), and
the use of positive and negative emotion words
(PosE,NegE) (Pennebaker et al, 2001). For exam-
ple, Table 1 shows that discussions about Cats vs.
Dogs consist of short simple words in short sen-
tences with relatively high usage of positive emo-
tion words and pronouns, whereas 2nd amendment
debates use relatively longer sentences, and death
penalty debates (unsurprisingly) use a lot of nega-
tive emotion words.
Human Topline. The best performance for sid-
ing ideological debates in previous work is approx-
imately 64% accuracy over all topics, for a collec-
tion of 2nd Amendment, Abortion, Evolution, and
Gay Rights debate posts (Somasundaran and Wiebe,
2010). Their best performance is 70% for the 2nd
amendment topic. The website that these posts were
collected from apparently did not support dialogic
threading, and thus there are no explicitly linked re-
buttals in this data set. Given the dialogic nature
of our data, as indicated by the high percentage of
rebuttals in the ideological debates, we first aim to
determine how difficult it is for humans to side an
individual post from a debate without context. To
our knowledge, none of the previous work on de-
bate side classification has attempted to establish a
human topline.
We set up a Mechanical Turk task by randomly se-
lected a subset of our data excluding the first post on
each side of a debate and debates with fewer than 6
posts on either side. Each of our 12 topics consists of
more than one debate: each debate was mapped by
hand to the topic and topic-siding (as in (Somasun-
daran and Wiebe, 2010)). We selected equal num-
bers of posts for each topic for each side, and cre-
ated 132 tasks (Mechanical Turk HITs). Each HIT
consisted of choosing the correct side for 10 posts
divided evenly, and selected randomly without re-
placement, from two debates. For each debate we
presented a title, side labels, and the initial post on
each side. For each post we presented the first 155
characters with a SEE MORE button which expanded
the post to its full length. Each HIT was judged by 9
annotators using Mechanical Turk with each anno-
tator restricted to at most 30 HITS (300 judgments).
Since many topics were US specific and we wanted
annotators with a good grasp of English, we required
Turkers to have a US IP address.
Figure 6 plots the number of annotators over all
topics who selected the ?true siding? as the side that
the post was on. We defined ?true siding? for this
purpose as the side that the original poster placed
their post. Figure 6 illustrates that humans often
placed the post on the wrong side. The majority of
posters agreed with the true siding 78.26% of the
time. The Fleiss? kappa statistic was 0.2656.
Importantly and interestingly, annotator accuracy
varied across topics in line with rebuttal percentage.
Annotators correctly labeled 94 of 100 posts for Cats
vs. Dogs but only managed 66 of 100 for the Cli-
4
Figure 6: Accuracies of Human Mechanical Turk judges
at selecting the True Siding of a post without context.
mate Change topic. This suggests that posts may
be difficult to side without context, which is what
one might expect given their dialogic nature. Rebut-
tals were clearly harder to side: annotators correctly
sided non-rebuttals 87% of the time, but only man-
aged 73% accuracy for rebuttals. Since all of the less
serious topics consisted of ?50% rebuttals while all
of the more serious ideological debates had >50%
rebuttals, 76% of ideological posts were sided cor-
rectly, while 85% of non-ideological posts were cor-
rectly sided. See Table 2.
Class Correct Total Accuracy
Rebuttal 606 827 0.73
Non-Rebuttal 427 493 0.87
Table 2: Human Agreement on Rebuttal Classification
Looking at the data by hand revealed that when
nearly all annotators agreed with each other but dis-
agreed with the self-labeled side, the user posted on
the wrong side (either due to user error, or because
the user was rebutting an argument the parent post
raised, not the actual conclusion).
The difficult-to-classify posts (where only 4-6 an-
notators were correct) were more complex. Our
analysis suggests that in 28% of these cases, the an-
notators were simply wrong, perhaps only skimming
a post when the stance indicator was buried deep in-
side it. Our decision to show only the first 155 char-
acters of each post by default (with a SHOW MORE
button) may have contributed to this error. An ad-
ditional 39% were short comments or ad hominem
responses, that showed disagreement, but no indi-
cation of side and 17% were ambiguous out of con-
text. A remaining 10% were meta-debate comments,
either about whether there were only two sides, or
whether the argument was meaningful. Given the
differences in siding difficulty depending on rebut-
tal status, in Section 4 we present results for both
rebuttal and stance classification.
3 Features and Learning Methods
Our experiments were conducted with the Weka
toolkit. All results are from 10 fold cross-validation
on a balanced test set. In the hand examination of
annotators siding performance, 101 posts were de-
termined to have incorrect self-labeling for side. We
eliminated these posts and their descendants from
the experiments detailed below. This resulted in a
dataset of 4772 posts. We used two classifiers with
different properties: NaiveBayes and JRip. JRip is
a rule based classifier which produces a compact
model suitable for human consumption and quick
application. Table 3 provides a summary of the fea-
tures we extract for each post. We describe and mo-
tivate these feature sets below.
Set Description/Examples
Post Info IsRebuttal, Poster
Unigrams Word frequencies
Bigrams Word pair frequencies
Cue Words Initial unigram, bigram, and trigram
Repeated
Punctuation
Collapsed into one of the following: ??, !!, ?!
LIWC LIWC measures and frequencies
Dependencies Dependencies derived from the Stanford Parser.
Generalized
Dependen-
cies
Dependency features generalized with respect to
POS of the head word and opinion polarity of
both words.
Opinion De-
pendencies
Subset of Generalized Dependencies with opin-
ion words from MPQA.
Context Fea-
tures
Matching Features used for the post from the par-
ent post.
Table 3: Feature Sets, Descriptions, and Examples
Counts, Unigrams, Bigrams. Previous work
suggests that the unigram baseline can be difficult to
beat for certain types of debates (Somasundaran and
Wiebe, 2010). Thus we derived both unigrams and
bigrams as features. We also include basic counts
such as post length.
Cue Words. We represent each posts initial un-
igram, bigram and trigram sequences to capture the
useage of cue words to mark responses of particular
type, such as oh really, so, and well; these features
were based on both previous work and our exami-
nation of the corpus (Fox Tree and Schrock, 1999;
Fox Tree and Schrock, 2002; Groen et al, 2010).
5
Repeated Punctuation. Our informal analyses
suggested that repeated sequential use of particular
types of punctuation such as !! and ?? did not mean
the same thing as simple counts or frequencies of
punctuation across a whole post. Thus we developed
distinct features for a subset of these repetitions.
LIWC. We also derived features using the Lin-
guistics Inquiry Word Count tool (LIWC-2001)
(Pennebaker et al, 2001). LIWC provides meta-
level conceptual categories for words to use in word
counts. Some LIWC features that we expect to be
important are words per sentence (WPS), pronomi-
nal forms (Pro), and positive and negative emotion
words (PosE) and (NegE). See Table 1.
Syntactic Dependency. Previous research in
this area suggests the utility of dependency struc-
ture to determine the TARGET of an opinion word
(Joshi and Penstein-Rose?, 2009; Somasundaran and
Wiebe, 2009; Somasundaran and Wiebe, 2010). The
dependency parse for a given sentence is a set of
triples, composed of a grammatical relation and the
pair of words for which the grammatical relation
holds (reli, wj , wk), where reli is the dependency
relation among words wj and wk. The word wj is
the HEAD of the dependency relation. We use the
Stanford parser to parse the utterances in the posts
and extract dependency features (De Marneffe et al,
2006; Klein and Manning, 2003).
Generalized Dependency. To create generalized
dependencies, we ?back off? the head word in each
of the above features to its part-of-speech tag (Joshi
and Penstein-Rose?, 2009). Joshi & Rose?s results
suggested that this approach would work better than
either fully lexicalized or fully generalized depen-
dency features. We call these POS generalized de-
pendencies in the results below.
Opinion Dependencies. Somasundaran & Wiebe
(2009) introduced features that identify the TAR-
GET of opinion words. Inspired by this approach,
we used the MPQA dictionary of opinion words
to select the subset of dependency and generalized
dependency features in which those opinion words
appear. For these features we replace the opinion
words with their positive or negative polarity equiv-
alents (Lin et al, 2006).
Context Features. Given the difficulty annota-
tors had in reliably siding rebuttals as well as their
prevalence in the corpus, we hypothesize that fea-
tures representing the parent post could be helpful
for classification. Here, we use a naive represen-
tation of context, where for all the feature types in
Table 3, we construct both parent features and post
features. For top-level parentless posts, the parent
features were null.
Figure 7: Model for distinguishing rebuttals vs. nonre-
buttals across all topics.
4 Results
The primary aim of our experiments was to deter-
mine the potential contribution, to debate side clas-
sification performance, of contextual dialogue fea-
tures, such as linguistic reflexes indicating a poster?s
orientation to a previous post or information from a
parent post. Because we believed that identification
of whether a post is a rebuttal or not might be help-
ful in the long term for debate-side classification, we
also establish a baseline for rebuttal classification.
4.1 Rebuttal Classification Results
The differences in human performance for siding de-
pended on rebuttal status. Our experiments on re-
buttal classification using the rule-based JRip clas-
sifer on a 10-fold cross-validation of our dataset pro-
6
duced 63% accuracy. Figure 7 illustrates a sample
model learned for distinguishing rebuttals from non-
rebuttals across all topics. The Figure shows that,
although we used the full complement of lexical and
syntactic features detailed above, the learned rules
were almost entirely based on LIWC and unigram
lexical features, such as 2nd person pronouns (7/8
rules), quotation marks (4/8 rules), question marks
(3/8), and negation (4/8), all of which correlated
with rebuttals. Other features that are used at several
places in the tree are LIWC Social Processes, LIWC
references to people, and LIWC Inclusive and Ex-
clusive. One tree node reflects the particular concern
with bodily functions that characterizes the Cats vs.
Dogs debate as illustrated in Figure 3.
4.2 Automatic Debate-Side Classification
Results
We first compared accuracies using Naive Bayes to
JRip for all topics for all feature sets. A paired t-test
showed that Naive Bayes over all topics and feature
sets was consistently better than JRip (p < .0001).
Thus the rest of our analysis and the results in Ta-
ble 4 focus on the Naive Bayes results.
Table 4 presents results for automatic debate
side classification using different feature sets and the
Naive Bayes learner which performs best over all
topics. In addition to classifying using only post-
internal features, we ran a parallel set of experiments
adding contextual features representing the parent
post, as described in Section 3. The results in Table
4 are divided under the headers Without Context and
With Context depending on whether features from
the parent post were used if it existed (e.g. in the
case of rebuttals).
We conducted paired t-tests over all topics simul-
taneously to examine the utility of different feature
sets. We compared unigrams to LIWC, opinion gen-
eralized dependencies, POS generalized dependen-
cies, and all features. We also compared experi-
ments using context features to experiments using
no contextual features. In general, our results in-
dicate that if the data are aggregated over all top-
ics, that indeed it is very difficult to beat the uni-
gram baseline. Across all topics there are generally
no significant differences between experiments con-
ducted with unigrams and other features. The mean
accuracies across all topics for unigrams vs. LIWC
features was 54.35% for unigrams vs. 52.83% for
LIWC. The mean accuracies for unigram vs POS
generalized dependencies was 54.35% vs. 52.64%,
and for unigrams vs. all features was Unigram
54.35% vs 54.62%. The opinion generalized de-
pendencies features actually performed significantly
worse than unigrams with an accuracy of 49% vs.
54.35% (p < .0001).
It is interesting to note that in general the unigram
accuracies are significantly below what Somasun-
daran and Wiebe achieve (who report overall uni-
gram of 62.5%). This suggests a difference between
the debate posts in their corpus and the Convinceme
data we used which may be related to the proportion
of rebuttals.
The overal lack of impact for either the POS gen-
eralized dependency features (GDepP) or the Opin-
ion generalized dependency features (GDep0) is
surprising given that they improve accuracy for other
similar tasks (Joshi and Penstein-Rose?, 2009; Soma-
sundaran and Wiebe, 2010). While our method of
extracting the GDepP features is identical to (Joshi
and Penstein-Rose?, 2009), our method for extracting
GDepO is an approximation of the method of (So-
masundaran and Wiebe, 2010), that does not rely on
selecting particular patterns indicating the topics of
arguing by using a development set.
The LIWC feature set, which is based on a lexi-
cal hierarchy that includes social features, negative
and positive emotion, and psychological processes,
is the only feature set that appears to have the po-
tential to systematically show improvement over a
good range of topics. We believe that further analy-
sis is needed; we do not want to handpick topics for
which particular feature sets perform well.
Our results also showed that context did not seem
to help uniformly over all topics. The mean per-
formance over all topics for contextual features us-
ing the combination of all features and the Naive
Bayes learner was 53.0% for context and 54.62%
for no context (p = .15%, not significant). Interest-
ing, the use of contextual features provided surpris-
ingly greater performance for particular topics. For
example for 2nd Amendment, unigrams with con-
text yield a performance of 69.23% as opposed to
the best performing without context features using
LIWC of 64.10%. The best performance of (So-
masundaran and Wiebe, 2010) is also 70% for the
2nd amendment topic. For the Healthcare topic,
LIWC with context features corresponds to an accu-
racy of 60.64% as opposed to GDepP without con-
text performance of 54.26%. For Communism vs.
Capitism, LIWC with context features gives an ac-
curacy of 56.55% as opposed to accuracies actually
7
Without Context With Context
Turk Uni LIWC GdepO GdepP All Uni LIWC GdepO GdepP All
Cats v. Dogs 94 59.23 55.38 56.15 61.54 62.31 50.77 56.15 55.38 60.77 50.00
Firefox vs. IE 74 51.25 53.75 43.75 48.75 50.00 51.25 53.75 52.50 52.50 51.25
Mac vs. PC 76 53.33 56.67 55.00 50.83 56.67 53.33 55.83 56.67 49.17 54.17
Superman Batman 89 54.84 45.97 42.74 45.97 54.03 50.00 57.26 43.55 50.81 53.23
2nd Amendment 69 56.41 64.10 51.28 58.97 57.69 69.23 61.54 44.87 52.56 67.95
Abortion 75 50.97 51.56 50.58 52.14 51.17 51.36 53.70 51.75 53.70 50.78
Climate Change 66 53.65 58.33 38.02 46.35 50.52 48.96 56.25 38.02 38.54 48.96
Comm vs. Capitalism 68 48.81 47.02 46.43 47.02 48.81 45.83 56.55 47.02 51.19 48.81
Death Penalty 79 51.80 53.96 46.76 49.28 52.52 51.80 56.12 56.12 57.55 53.24
Evolution 72 57.24 48.36 54.93 56.41 57.24 54.11 46.22 50.82 52.14 52.96
Existence of God 73 52.71 51.14 49.72 52.42 51.99 52.28 52.28 50.14 53.42 51.42
Gay Marriage 88 60.28 56.11 56.11 58.61 59.44 56.94 52.22 54.44 53.61 54.72
Healthcare 86 52.13 51.06 51.06 54.26 52.13 45.74 60.64 59.57 57.45 53.19
MJ Legalization 81 57.55 46.23 43.40 53.77 59.43 52.83 46.23 49.06 49.06 50.94
Table 4: Accuracies achieved using different feature sets and 10-fold cross validation as compared to the human
topline from MTurk. Best accuracies are shown in bold for each topic in each row. KEY: Human topline results
(Turk). Unigram features (Uni). Linguistics Inquiry Word Count features (LIWC). Generalized dependency features
containing MPQA terms (GdepO) & POS tags (GdepP). NaiveBayes was used, no attribute selection was applied.
below the majority class baseline for all of the fea-
tures without context.
Should we conclude anything from the fact that
6 of the topics are idealogical, out of the 7 topics
where contextual features provide the best perfor-
mance? We believe that the significantly greater per-
centage of rebuttals for these topics should give a
greater weight to contextual features, so it would be
useful to examine stance classification performance
on the subset of the posts that are rebuttals. We be-
lieve that context is important; our conclusion is that
our current contextual features are naive ? they are
not capturing the relationship between a post and a
parent post. Sequential models or at least better con-
textual features are needed.
The fact that we should be able to do much better
is indicated clearly by the human topline, shown in
the column labelled Turk in Table 4. Even without
context, and with the difficulties siding rebuttals, the
human annotators achieve accuracies ranging from
66% to 94%.
5 Discussion
This paper examines two problems in online-
debates: rebuttal classification and debate-side or
stance classification. Our results show that we can
identify rebuttals with 63% accuracy, and that using
lexical and contextual features such as those from
LIWC, we can achieve debate-side classification ac-
curacies on a per topic basis that range from 54% to
69%, as compared to a unigram baselines that vary
between 49% and 60%. These are the first results
that we are aware of that establish a human topline
for debate side classification. These are also the first
results that we know of for identifying rebuttals in
such debates.
Our results for stance classification are mixed.
While we show that for many topics we can beat
a unigram baseline given more intelligent features,
we do not beat the unigram baseline when we com-
bine our data across all topics. In addition, we are
not able to show across all topics that our contex-
tual features make a difference, though clearly use of
context should make a difference in understanding
these debates, and for particular topics, classifica-
tion results using context are far better than the best
feature set without any contextual features. In fu-
ture work, we hope to develop more intelligent fea-
tures for representing context and improve on these
results. We also plan to make our corpus available
to other researchers in the hopes that it will stimu-
late further work analyzing the dialogic structure of
such debates.
Acknowledgments
This work was funded by Grant NPS-BAA-03 to
UCSC and Intelligence Advanced Research Projects
Activity (IARPA) through the Army Research Lab-
oratory to UCSC by subcontract from the University
of Maryland. We?d like to thank Craig Martell and
Joseph King for helpful discussions over the course
of this project, and the anonymous reviewers for use-
ful feedback. We would also like to thank Jason Au-
miller for his contributions to the database.
8
References
Rob Abbott, Marilyn Walker, Jean E. Fox Tree, Pranav
Anand, Robeson Bowmani, and Joseph King. 2011.
How can you say such things?!?: Recognizing Dis-
agreement in Informal Political Argument. In Pro-
ceedings of the ACL Workshop on Language and So-
cial Media.
R. Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu. 2003.
Mining newsgroups using networks arising from so-
cial behavior. In Proceedings of the 12th international
conference on World Wide Web, pages 529?535. ACM.
D. Biber. 1991. Variation across speech and writing.
Cambridge Univ Pr.
David Crystal. 2001. Language and the Internet. Cam-
bridge University Press.
M.C. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of LREC, vol-
ume 6, pages 449?454. Citeseer.
J.E. Fox Tree and J.C. Schrock. 1999. Discourse Mark-
ers in Spontaneous Speech: Oh What a Difference
an Oh Makes. Journal of Memory and Language,
40(2):280?295.
J.E. Fox Tree and J.C. Schrock. 2002. Basic mean-
ings of you know and I mean. Journal of Pragmatics,
34(6):727?747.
J. E. Fox Tree. 2010. Discourse markers across speak-
ers and settings. Language and Linguistics Compass,
3(1):113.
S. Greene and P. Resnik. 2009. More than words: Syn-
tactic packaging and implicit sentiment. In Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
503?511. Association for Computational Linguistics.
M. Groen, J. Noyes, and F. Verstraten. 2010. The Effect
of Substituting Discourse Markers on Their Role in
Dialogue. Discourse Processes: A Multidisciplinary
Journal, 47(5):33.
M. Joshi and C. Penstein-Rose?. 2009. Generalizing de-
pendency features for opinion mining. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, pages 313?316. Association for Computational
Linguistics.
D. Klein and C.D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics-
Volume 1, pages 423?430. Association for Computa-
tional Linguistics.
W.H. Lin, T. Wilson, J. Wiebe, and A. Hauptmann. 2006.
Which side are you on?: identifying perspectives at
the document and sentence levels. In Proceedings
of the Tenth Conference on Computational Natural
Language Learning, pages 109?116. Association for
Computational Linguistics.
R. Malouf and T. Mullen. 2008. Taking sides: User clas-
sification for informal online political discourse. In-
ternet Research, 18(2):177?190.
Daniel Marcu. 2000. Perlocutions: The Achilles?
heel of Speech Act Theory. Journal of Pragmatics,
32(12):1719?1741.
G. Marwell and D. Schmitt. 1967. Dimensions of
compliance-gaining behavior: An empirical analysis.
sociomety, 30:350?364.
G. Mishne and N. Glance. 2006. Leave a reply: An anal-
ysis of weblog comments. In Third annual workshop
on the Weblogging ecosystem. Citeseer.
A. Murakami and R. Raymond. 2010. Support or
Oppose? Classifying Positions in Online Debates
from Reply Activities and Opinion Expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 869?875.
Association for Computational Linguistics.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1?135.
J. W. Pennebaker, L. E. Francis, and R. J. Booth, 2001.
LIWC: Linguistic Inquiry and Word Count.
S. Somasundaran and J. Wiebe. 2009. Recognizing
stances in online debates. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1-Volume
1, pages 226?234. Association for Computational Lin-
guistics.
S. Somasundaran and J. Wiebe. 2010. Recognizing
stances in ideological on-line debates. In Proceedings
of the NAACL HLT 2010 Workshop on Computational
Approaches to Analysis and Generation of Emotion in
Text, pages 116?124. Association for Computational
Linguistics.
Karen Sparck-Jones. 1999. Automatic summarizing;
factors and directions. In Inderjeet Mani and Mark
Maybury, editors, Advances in Automatic Text Summa-
rization. MIT Press.
Marilyn A. Walker. 1996. Inferring acceptance and re-
jection in dialogue by default rules of inference. Lan-
guage and Speech, 39-2:265?304.
Y.C. Wang and C.P. Rose?. 2010. Making conversational
structure explicit: identification of initiation-response
pairs within online discussions. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 673?676. Association for
Computational Linguistics.
9
Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 30?40,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Really? Well. Apparently Bootstrapping Improves the Performance of
Sarcasm and Nastiness Classifiers for Online Dialogue
Stephanie Lukin
Natural Language and Dialogue Systems
University of California, Santa Cruz
1156 High Street, Santa Cruz, CA 95064
slukin@soe.ucsc.edu
Marilyn Walker
Natural Language and Dialogue Systems
University of California, Santa Cruz
1156 High Street, Santa Cruz, CA 95064
maw@soe.ucsc.edu
Abstract
More and more of the information on the web
is dialogic, from Facebook newsfeeds, to fo-
rum conversations, to comment threads on
news articles. In contrast to traditional, mono-
logic Natural Language Processing resources
such as news, highly social dialogue is fre-
quent in social media, making it a challenging
context for NLP. This paper tests a bootstrap-
ping method, originally proposed in a mono-
logic domain, to train classifiers to identify
two different types of subjective language in
dialogue: sarcasm and nastiness. We explore
two methods of developing linguistic indica-
tors to be used in a first level classifier aimed
at maximizing precision at the expense of re-
call. The best performing classifier for the first
phase achieves 54% precision and 38% recall
for sarcastic utterances. We then use general
syntactic patterns from previous work to cre-
ate more general sarcasm indicators, improv-
ing precision to 62% and recall to 52%. To
further test the generality of the method, we
then apply it to bootstrapping a classifier for
nastiness dialogic acts. Our first phase, using
crowdsourced nasty indicators, achieves 58%
precision and 49% recall, which increases to
75% precision and 62% recall when we boot-
strap over the first level with generalized syn-
tactic patterns.
1 Introduction
More and more of the information on the web is
dialogic, from Facebook newsfeeds, to forum con-
versations, to comment threads on news articles. In
contrast to traditional, monologic Natural Language
Processing resources such as news, highly social di-
alogue is very frequent in social media, as illustrated
in the snippets in Fig. 1 from the publicly avail-
able Internet Argument Corpus (IAC) (Walker et al,
Quote Q, Response R Sarc Nasty
Q1: I jsut voted. sorry if some people actu-
ally have, you know, LIVES and don?t sit around
all day on debate forums to cater to some athe-
ists posts that he thiks they should drop every-
thing for. emoticon-rolleyes emoticon-rolleyes
emoticon-rolleyes As to the rest of your post, well,
from your attitude I can tell you are not Christian
in the least. Therefore I am content in knowing
where people that spew garbage like this will end
up in the End.
R1: No, let me guess . . . er . . . McDonalds. No,
Disneyland. Am I getting closer?
1 -3.6
Q2: The key issue is that once children are born
they are not physically dependent on a particular
individual.
R2 Really? Well, when I have a kid, I?ll be sure to
just leave it in the woods, since it can apparently
care for itself.
1 -1
Q3: okay, well i think that you are just finding
reasons to go against Him. I think that you had
some bad experiances when you were younger or
a while ago that made you turn on God. You are
looking for reasons, not very good ones i might
add, to convince people.....either way, God loves
you. :)
R3: Here come the Christians, thinking they can
know everything by guessing, and commiting the
genetic fallacy left and right.
0.8 -3.4
Figure 1: Sample Quote/Response Pairs from
4forums.com with Mechanical Turk annotations
for Sarcasm and Nasty/Nice. Highly negative values
of Nasty/Nice indicate strong nastiness and sarcasm is
indicated by values near 1.
2012). Utterances are frequently sarcastic, e.g., Re-
ally? Well, when I have a kid, I?ll be sure to just
leave it in the woods, since it can apparently care
for itself (R2 in Fig. 1 as well as Q1 and R1), and are
often nasty, e.g. Here come the Christians, thinking
they can know everything by guessing, and commit-
ing the genetic fallacy left and right (R3 in Fig. 1).
Note also the frequent use of dialogue specific dis-
course cues, e.g. the use of No in R1, Really? Well
in R2, and okay, well in Q3 in Fig. 1 (Fox Tree
and Schrock, 1999; Bryant and Fox Tree, 2002; Fox
Tree, 2010).
30
The IAC comes with annotations of different
types of social language categories including sarcas-
tic vs not sarcastic, nasty vs nice, rational vs emo-
tional and respectful vs insulting. Using a conser-
vative threshold of agreement amongst the annota-
tors, an analysis of 10,003 Quote/Response pairs
(Q/R pairs) from the 4forums portion of IAC sug-
gests that social subjective language is fairly fre-
quent: about 12% of posts are sarcastic, 23% are
emotional, and 12% are insulting or nasty. We select
sarcastic and nasty dialogic turns to test our method
on more than one type of subjective language and
explore issues of generalization; we do not claim any
relationship between these types of social language
in this work.
Despite their frequency, expanding this corpus of
sarcastic or nasty utterances at scale is expensive:
human annotation of 100% of the corpus would be
needed to identify 12% more examples of sarcasm
or nastiness. An explanation of how utterances are
annotated in IAC is detailed in Sec. 2.
Our aim in this paper is to explore whether it is
possible to extend a method for bootstrapping a clas-
sifier for monologic, subjective sentences proposed
by Riloff & Wiebe, henceforth R&W (Riloff and
Wiebe, 2003; Thelen and Riloff, 2002), to automat-
ically find sarcastic and nasty utterances in unanno-
tated online dialogues. Sec. 3 provides an overview
of R&W?s bootstrapping method. To apply boot-
strapping, we:
1. Explore two different methods for identifying
cue words and phrases in two types of subjec-
tive language in dialogues: sarcasm and nasty
(Sec. 4);
2. Use the learned indicators to train a sarcastic
(nasty) dialogue act classifier that maximizes
precision at the expense of recall (Sec. 5);
3. Use the classified utterances to learn general
syntactic extraction patterns from the sarcastic
(nasty) utterances (Sec. 6);
4. Bootstrap this process on unannotated text to
learn new extraction patterns to use for classifi-
cation.
We show that the Extraction Pattern Learner im-
proves the precision of our sarcasm classifier by
17% and the recall by 24%, and improves the pre-
cision of the nastiness classifier by 14% and recall
by 13%. We discuss previous work in Sec. 2 and
compare to ours in Sec. 7 where we also summarize
our results and discuss future work.
2 Previous Work
IAC provides labels for sarcasm and nastiness that
were collected with Mechanical Turk on Q/R pairs
such as those in Fig. 1. Seven Turkers per Q/R pair
answered a binary annotation question for sarcasm
Is the respondent using sarcasm? (0,1) and a scalar
annotation question for nastiness Is the respondent
attempting to be nice or is their attitude fairly nasty?
(-5 nasty . . . 5 nice). We selected turns from IAC
Table 1 with sarcasm averages above 0.5, and nasty
averages below -1 and nice above 1. Fig. 1 included
example nastiness and sarcasm values.
Previous work on the automatic identification
of sarcasm has focused on Twitter using the
#sarcasm (Gonza?lez-Iba?n?ez et al, 2011) and
#irony (Reyes et al, 2012) tags and a combined
variety of tags and smileys (Davidov et al, 2010).
Another popular domain examines Amazon product
reviews looking for irony (Reyes and Rosso, 2011),
sarcasm (Tsur et al, 2010), and a corpus collec-
tion for sarcasm (Filatova, 2012). (Carvalho et al,
2009) looks for irony in comments in online newpa-
pers which can have a thread-like structure. This
primary focus on monologic venues suggests that
sarcasm and irony can be detected with a relatively
high precision but have a different structure from di-
alogues (Fox Tree and Schrock, 1999; Bryant and
Fox Tree, 2002; Fox Tree, 2010), posing the ques-
tion, can we generalize from monologic to dialogic
structures? Each of these works use methods in-
cluding LIWC unigrams, affect, polarity, punctua-
tion and more, and achieve on average a precision of
75% or accuracy of between 45% and 85%.
Automatically identifying offensive utterances is
also of interest. Previous work includes identifying
flames in emails (Spertus, 1997) and other messag-
ing interfaces (Razavi et al, 2010), identifying in-
sults in Twitter (Xiang et al, 2012), as well as com-
ments from new sites (Sood et al, 2011). These
approaches achieve an accuracy between 64% and
83% using a variety of approaches. The accuracies
for nasty utterances has a much smaller spread and
higher average than sarcasm accuracies. This sug-
gests that nasty language may be easier to identify
than sarcastic language.
3 Method Overview
Our method for bootstrapping a classifier for sarcas-
tic (nasty) dialogue acts uses R&W?s model adapted
to our data as illustrated for sarcasm in Fig. 2. The
31
Figure 2: Bootstrapping Flow for Classifying Subjective
Dialogue Acts, shown for sarcasm, but identical for nas-
tiness.
overall idea of the method is to find reliable cues and
then generalize. The top of Fig. 2 specifies the input
to the method as an unannotated corpus of opinion
dialogues, to illustrate the long term aim of building
a large corpus of the phenomenon of interest with-
out human annotation. Although the bootstrapping
method assumes that the input is unannotated text,
we first need utterances that are already labeled for
sarcasm (nastiness) to train it. Table 1 specifies how
we break down into datasets the annotations on the
utterances in IAC for our various experiments.
The left circle of Fig. 2 reflects the assump-
tion that there are Sarcasm or Nasty Cues that can
identify the category of interest with high preci-
sion (R&W call this the ?Known Subjective Vocab-
ulary?). The aim of first developing a high preci-
sion classifier, at the expense of recall, is to select
utterances that are reliably of the category of inter-
est from unannotated text. This is needed to ensure
that the generalization step of ?Extraction Pattern
Learner? does not introduce too much noise.
R&W did not need to develop a ?Known Sub-
jective Vocabulary? because previous work provided
one (Wilson et al, 2005; Wiebe et al, 1999; Wiebe
et al, 2003). Thus, our first question with applying
R&W?s method to our data was whether or not it is
possible to develop a reliable set of Sarcasm (Nas-
tiness) Cues (O1 below). Two factors suggest that
it might not be. First, R&W?s method assumes that
the cues are in the utterance to be classified, but it
has been claimed that sarcasm (1) is context depen-
dent, and (2) requires world knowledge to recognize,
SARCASM #sarc #notsarc total
MT exp dev 617 NA 617
HP train 1407 1404 2811
HP dev test 1614 1614 3228
PE eval 1616 1616 3232
All 5254 4635 9889
NASTY #nasty #nice total
MT exp dev 510 NA 510
HP train 1147 1147 2294
HP dev test 691 691 1382
PE eval 691 691 1382
All 3039 2529 5568
Table 1: How utterances annotated for sarcasm (top) and
nastiness (bottom) in IAC were used. MT = Mechanical
Turk experimental development set. HP train = utter-
ances used to test whether combinations of cues could be
used to develop a High precision classifier. HP dev test
= ?Unannotated Text Collection? in Fig. 2. PE eval =
utterances used to train the Pattern Classifier.
at least in many cases. Second, sarcasm is exhibited
by a wide range of different forms and with differ-
ent dialogue strategies such as jocularity, understate-
ment and hyberbole (Gibbs, 2000; Eisterhold et al,
2006; Bryant and Fox Tree, 2002; Filatova, 2012).
In Sec. 4 we devise and test two different methods
for acquiring a set of Sarcasm (Nastiness) Cues on
particular development sets of dialogue turns called
the ?MT exp dev? in Table 1.
The boxes labeled ?High Precision Sarcastic Post
Classifier? and ?High Precision Not Sarcastic Post
Classifier? in Fig. 2 involves using the Sarcasm
(Nastiness) Cues in simple combinations that max-
imize precision at the expense of recall. R&W
found cue combinations that yielded a High Preci-
sion Classifier (HP Classifier) with 90% precision
and 32% recall on their dataset. We discuss our test
of these steps in Sec. 5 on the ?HP train? develop-
ment sets in Table 1 to estimate parameters for the
High Precision classifier, and then test the HP classi-
fier with these parameters on the test dataset labeled
?HP dev test? in Table 1.
R&W?s Pattern Based classifier increased recall
to 40% while losing very little precision. The open
question with applying R&W?s method to our data,
was whether the cues that we discovered, by what-
ever method, would work at high enough precision
to support generalization (O2 below). In Sec. 6 we
32
describe how we use the ?PE eval? development set
(Table 1) to estimate parameters for the Extraction
Pattern Learner, and then test the Pattern Based Sar-
castic (Nasty) Post classifier on the newly classified
utterances from the dataset labeled ?HP dev test?
(Table 1). Our final open question was whether the
extraction patterns from R&W, which worked well
for news text, would work on social dialogue (O3
below). Thus our experiments address the following
open questions as to whether R&W?s bootstrapping
method improves classifiers for sarcasm and nasti-
ness in online dialogues:
? (O1) Can we develop a ?known sarcastic
(nasty) vocabulary?? The LH circle of Fig. 2
illustrates that we use two different methods to
identify Sarcasm Cues. Because we have ut-
terances labeled as sarcastic, we compare a sta-
tistical method that extracts important features
automatically from utterances, with a method
that has a human in the loop, asking annotators
to select phrases that are good indicators of sar-
casm (nastiness) (Sec. 5);
? (O2) If we can develop a reliable set of sarcasm
(nastiness) cues, is it then possible to develop
an HP classifier? Will our precision be high
enough? Is the fact that sarcasm is often con-
text dependent an issue? (Sec. 5);
? (O3) Will the extraction patterns used in
R&W?s work allow us to generalize sarcasm
cues from the HP Classifiers? Are R&W?s pat-
terns general enough to work well for dialogue
and social language? (Sec. 6).
4 Sarcasm and Nastiness Cues
Because there is no prior ?Known Sarcastic Vocabu-
lary? we pilot two different methods for discovering
lexical cues to sarcasm and nastiness, and experi-
ment with combinations of cues that could yield a
high precision classifier (Gianfortoni et al, 2011).
The first method uses ?2 to measure whether a word
or phrase is statistically indicative of sarcasm (nasti-
ness) in the development sets labeled ?MT exp dev?
(Table 1). This method, a priori, seems reasonable
because it is likely that if you have a large enough
set of utterances labeled as sarcastic, you could be
able to automatically learn a set of reliable cues for
sarcasm.
The second method introduces a step of human
annotation. We ask Turkers to identify sarcastic
(nasty) indicators in utterances (the open question
unigram
?2 MT IA FREQ
right ah .95 2
oh relevant .85 2
we amazing .80 2
same haha .75 2
all yea .73 3
them thanks .68 6
mean oh .56 56
bigram
?2 MT IA FREQ
the same oh really .83 2
mean like oh yeah .79 2
trying to so sure .75 2
that you no way .72 3
oh yeah get real .70 2
I think oh no .66 4
we should you claim .65 2
trigram
?2 MT IA FREQ
you mean to I get it .97 3
mean to tell I?m so sure .65 2
have to worry then of course .65 2
sounds like a are you saying .60 2
to deal with well if you .55 2
I know I go for it .52 2
you mean to oh, sorry .50 2
Table 2: Mechanical Turk (MT) and ?2 indicators for
Sarcasm
O1) from the development set ?MT exp dev? (Ta-
ble 1). Turkers were presented with utterances pre-
viously labeled sarcastic or nasty in IAC by 7 dif-
ferent Turkers, and were told ?In a previous study,
these responses were identified as being sarcastic by
3 out of 4 Turkers. For each quote/response pair,
we will ask you to identify sarcastic or potentially
sarcastic phrases in the response?. The Turkers then
selected words or phrases from the response they be-
lieved could lead someone to believing the utterance
was sarcastic or nasty. These utterances were not
used again in further experiments. This crowdsourc-
ing method is similar to (Filatova, 2012), but where
their data is monologic, ours is dialogic.
4.1 Results from Indicator Cues
Sarcasm is known to be highly variable in form, and
to depend, in some cases, on context for its inter-
pretation (Sperber and Wilson, 1981; Gibbs, 2000;
Bryant and Fox Tree, 2002). We conducted an ini-
tial pilot on 100 of the 617 sarcastic utterances in
33
unigram
?2 MT IA FREQ
like idiot .90 3
them unfounded .85 2
too babbling .80 2
oh lie .72 11
mean selfish .70 2
just nonsense .69 9
make hurt .67 3
bigram
?2 MT IA FREQ
of the don?t expect .95 2
you mean get your .90 2
yes, you?re an .85 2
oh, what?s your .77 4
you are prove it .77 3
like a get real .75 2
I think what else .70 2
trigram
?2 MT IA FREQ
to tell me get your sick .75 2
would deny a your ignorance is .70 2
like that? make up your .70 2
mean to tell do you really .70 2
sounds like a do you actually .65 2
you mean to doesn?t make it .63 3
to deal with what?s your point .60 2
Table 3: Mechanical Turk (MT) and ?2 indicators for
Nasty
Figure 3: Interannotator Agreement for sarcasm trigrams
the development set ?MT exp dev? to see if this was
necessarily the case in our dialogues. (Snow et al,
2008) measures the quality of Mechanical Turk an-
notations on common NLP tasks by comparing them
to a gold standard. Pearson?s correlation coefficient
shows that very few Mechanical Turk annotators
were required to beat the gold standard data, often
less than 5. Because our sarcasm task does not have
gold standard data, we ask 100 annotators to partic-
ipate in the pilot. Fig. 3 plots the average interan-
notator agreement (ITA) as a function of the number
of annotators, computed using Pearson correlation
counts, for 40 annotators and for trigrams which re-
quire more data to converge. In all cases (unigrams,
bigrams, trigrams) ITA plateaus at around 20 anno-
tators and is about 90% with 10 annotators, showing
that the Mechanical Turk tasks are well formed and
there is high agreement. Thus we elicited only 10
annotations for the remainder of the sarcastic and all
the nasty utterances from the development set ?MT
exp dev?.
We begin to form our ?known sarcastic vocab-
ulary? from these indicators, (open question O1).
Each MT indicator has a FREQ (frequency): the
number of times each indicator appears in the train-
ing set; and an IA (interannotator agreement): how
many annotators agreed that each indicator was sar-
castic or nasty. Table 2 shows the best unigrams,
bigrams, and trigrams from the ?2 test and from the
sarcasm Mechanical Turk experiment and Table 3
shows the results from the nasty experiment. We
compare the MT indicators to the ?2 indicators as
part of investigating open question O1.
As a pure statistical method, ?2 can pick out
things humans might not. For example, if it just hap-
pened that the word ?we? only occurs in sarcastic
utterances in the development set, then ?2 will se-
lect it as a strong sarcastic word (row 3 of Table 2).
However, no human would recognize this word as
corresponding to sarcasm. ?2 could easily be over-
trained if the ?MT exp dev? development set is not
large enough to eliminate such general words from
consideration, ?MT exp dev? only has 617 sarcastic
utterances and 510 nasty utterances (Table 1).
Words that the annotators select as indicators
(columns labeled MT in Table 2 and Table 3) are
much more easily identifiable although they do not
appear as often. For example, the IA of 0.95 for ?ah?
in Table 2 means that of all the annotators who saw
?ah? in the utterance they annotated, 95% selected it
to be sarcastic. However the FREQ of 2 means that
?ah? only appeared in 2 utterances in the ?MT exp
dev? development set.
We test whether any of the methods for select-
ing indicators provide reliable cues that generalize
to a larger dataset in Sec. 5. The parameters that
we estimate on the development sets are exactly how
frequent (compared to a ?1) and how reliable (com-
34
pared to a ?2) a cue has to be to be useful in R&W?s
bootstrapping method.
5 High-Precision Classifiers
R&W use their ?known subjective vocabulary? to
train a High Precision classifier. R&W?s HP classi-
fier searches for exact surface matches of the sub-
jective indicators and classifies utterances as sub-
jective if two subjective indicators are present. We
follow similar guidelines to train HP Sarcasm and
Nasty Classifiers. To test open question O1, we
use a development set called ?HP train? (Table 1)
to test three methods for measuring the ?goodness?
of an indicator that could serve as a high precision
cue: (1) interannotator agreement based on anno-
tators consensus from Mechanical Turk, on the as-
sumption that the number of annotators that select
a cue indicates its strength and reliability (IA fea-
tures); (2) percent sarcastic (nasty) and frequency
statistics in the HP train dataset as R&W do (percent
features); and (3) the ?2 percent sarcastic (nasty)
and frequency statistics (?2 features).
The IA features use the MT indicators and the IA
and FREQ calculations introduced in Sec. 4 (see
Tables 2 and 3). First, we select indicators such
that ?1 <= FREQ where ?1 is a set of possible
thresholds. Then we introduce two new parameters
? and ? to divide the indicators into three ?good-
ness? groups that reflect interannotator agreement.
indicatorstrength =
{
weak if 0 ? IA < ?
medium if ? ? IA < ?
strong if ? ? IA < 1
For IA features, an utterance is classified as sar-
castic if it contains at least one strong or two medium
indicators. Other conditions were piloted. We first
hypothesized that weak cues might be a way of
classifying ?not sarcastic? utterances. But HP train
showed that both sarcastic and not sarcastic utter-
ances contain weak indicators yielding no informa-
tion gain. The same is true for Nasty?s counter-
class Nice. Thus we specify that counter-class utter-
ances must have no strong indicators or at most one
medium indicator. In contrast, R&W?s counter-class
classifier looks for a maximum of one subjective in-
dicator.
The percent features also rely on the FREQ of
each MT indicator, subject to a ?1 threshold, as
well as the percentage of the time they occur in
a sarcastic utterance (%SARC) or nasty utterance
(%NASTY). We select indicators with various pa-
rameters for ?1 and ?2 ? %SARC. At least two in-
dicators must be present and above the thresholds to
be classified and we exhaust all combinations. Less
than two indicators are needed to be classified as the
counter-class, as in R&W.
Finally, the ?2 features use the same method as
percent features only using the ?2 indicators instead
of the MT indicators.
After determining which parameter settings per-
forms the best for each feature set, we ran the HP
classifiers, using each feature set and the best param-
eters, on the test set labeled ?HP dev test?. The HP
Classifiers classify the utterances that it is confident
on, and leave others unlabeled.
5.1 Results from High Precision Classifiers
The HP Sarcasm and Nasty Classifiers were trained
on the three feature sets with the following parame-
ters: IA features we exhaust all combinations of ? =
[.70, .75, .80, .85, .90, .95, 1.00], ? = [.35, .40, .45,
.50, .55, .60, .65, .7], and ?1 = [2, 4, 6, 8, 10]; for the
percent features and ?2 features we again exhaust ?1
= [2, 4, 6, 8, 10] and ?2 = [.55, .60, .65, .70, .75, .80,
.85, .90, .95, 1.00].
Tables 4 and 5 show a subset of the experiments
with each feature set. We want to select parame-
ters that maximize precision without sacrificing too
much recall. Of course, the parameters that yield
the highest precision also have the lowest recall, e.g.
Sarcasm percent features, parameters ?1 = 4 and
?2 = 0.75 achieve 92% precision but the recall is
1% (Table 4), and Nasty percent features with pa-
rameters ?1 = 8 and ?2 = 0.8 achieves 98% preci-
sion but a recall of 3% (Table 5). On the other end of
the spectrum, the parameters that achieve the highest
recall yield a precision equivalent to random chance.
Examining the parameter combinations in Ta-
bles 4 and 5 shows that percent features do better
than IA features in all cases in terms of precision.
Compare the block of results labeled % in Tables 4
and 5 with the IA and ?2 blocks for column P. Nasty
appears to be easier to identify than Sarcasm, espe-
cially using the percent features. The performance
of the ?2 features is comparable to that of percent
features for sarcasm, but lower than percent features
for Nasty.
The best parameters selected from each feature
set are shown in the PARAMS column of Table 6.
With the indicators learned from these parameters,
we run the Classifiers on the test set labeled ?HP
35
SARC PARAMS P R N (tp)
% ?1 =4, ?2 =.55 62% 55% 768
4, .6 72% 32% 458
4, .65 84% 12% 170
4, .75 92% 1% 23
IA ?1 =2, ? =.90, ? =.35 51% 73% 1,026
2, .95, .55 62% 13% 189
2, .9, .55 54% 34% 472
4, .75, .5 64% 7% 102
4, .75, .6 78% 1% 22
?2 ?1 =8, ?2 =.55 59% 64% 893
8, .6 67% 31% 434
8, .65 70% 12% 170
8, .75 93% 1% 14
Table 4: Sarcasm Train results; P: precision, R: recall, tp:
true positive classifications
NASTY PARAMS P R N (tp)
% ?1 =2, ?2 =.55 65% 69% 798
4, .65 80% 44% 509
8, .75 95% 11% 125
8, .8 98% 3% 45
IA ?1 =2, ? =.95, ? =.35 50% 96% 1,126
2, .95, .45 60% 59% 693
4, .75, .45 60% 50% 580
2, .7, .55 73% 12% 149
2, .9, .65 85% 1% 17
?2 ?1 =2, ?2 =.55 73% 15% 187
2, .65 78% 8% 104
2, .7 86% 3% 32
Table 5: Nasty Train results; P: precision, R: recall, tp:
true positive classifications
dev test? (Table 1). The performance on test set ?HP
dev test? (Table 6) is worse than on the training set
(Tables 4 and 5). However we conclude that both
the % and ?2 features provide candidates for sar-
casm (nastiness) cues that are high enough precision
(open question O2) to be used in the Extraction Pat-
tern Learner (Sec. 6), even if Sarcasm is more con-
text dependent than Nastiness.
PARAMS P R F
Sarc % ?1 =4, ?2 =.55 54% 38% 0.46
Sarc IA ?1 =2, ? =.95, ? =.55 56% 11% 0.34
Sarc ?2 ?1 =8, ?2 =.60 60% 19% 0.40
Nasty % ?1 =2, ?2 =.55 58% 49% 0.54
Nasty IA ?1 =2, ? =.95, ? =.45 53% 35% 0.44
Nasty ?2 ?1 =2, ?2 =.55 74% 14% 0.44
Table 6: HP Dev test results; PARAMS: the best pa-
rameters for each feature set P: precision, R: recall, F:
f-measure
6 Extraction Patterns
R&W?s Pattern Extractor searches for instances of
the 13 templates in the first column of Table 7 in ut-
terances classified by the HP Classifier. We reim-
plement this; an example of each pattern as in-
stantiated in test set ?HP dev test? for our data is
shown in the second column of Table 7. The tem-
plate <subj> active-verb <dobj> matches ut-
terances where a subject is followed by an active
verb and a direct object. However, these matches
are not limited to exact surface matches as the HP
Classifiers required, e.g. this pattern would match
the phrase ?have a problem?. Table 10 in the Ap-
pendix provides example utterances from IAC that
match the instantiated template patterns. For exam-
ple, the excerpt from the first row in Table 10 ?It
is quite strange to encounter someone in this day
and age who lacks any knowledge whatsoever of the
mechanism of adaptation since it was explained 150
years ago? matches the <subj> passive-verb
pattern. It appears 2 times (FREQ) in the test set
and is sarcastic both times (%SARC is 100%). Row
11 in Table 10 shows an utterance matching the
active-verb prep <np> pattern with the phrase
?At the time of the Constitution there weren?t ex-
actly vast suburbs that could be prowled by thieves
looking for an open window?. This phrase appears
14 times (FREQ) in the test set and is sarcastic
(%SARC) 92% of the time it appears.
Synactic Form Example Pattern
<subj> passive-verb <subj> was explained
<subj> active-verb <subj> appears
<subj> active-verb dobj <subj> have problem
<subj> verb infinitive <subj> have to do
<subj> aux noun <subj> is nothing
active-verb <dobj> gives <dobj>
infinitive <dobj> to force <dobj>
verb infinitive <dobj> want to take <dobj>
noun aux <dobj> fact is <dobj>
noun prep <np> argument against <np>
active-verb prep <np> looking for <np>
passive-verb prep <np> was put in <np>
infinitive prep <np> to go to <np>
Table 7: Syntactic Templates and Examples of Patterns
that were Learned for Sarcasm. Table. 10 in the Appendix
provides example posts that instantiate these patterns.
The Pattern Based Classifiers are trained on a de-
velopment set labeled ?PE eval? (Table 1). Utter-
ances from this development set are not used again
36
Figure 4: Recall vs. Precision for Sarcasm PE eval
in any further experiments. Patterns are extracted
from the dataset and we again compute FREQ and
%SARC and %NASTY for each pattern subject to
?1 ? FREQ and ?2 ? %SARC or % NASTY.
Classifications are made if at least two patterns are
present and both are above the specified ?1 and ?2,
as in R&W. Also following R&W, we do not learn
?not sarcastic? or ?nice? patterns.
To test the Pattern Based Classifiers, we use as in-
put the classifications made by the HP Classifiers.
Using the predicted labels from the classifiers as the
true labels, the patterns from test set ?HP test dev?
are extracted and compared to those patterns found
in development set ?PE eval?. We have two feature
sets for both sarcasm and nastiness: one using the
predictions from the MT indicators in the HP clas-
sifier (percent features) and another using those in-
stances from the ?2 features.
6.1 Results from Pattern Classifier
The Pattern Classifiers classify an utterance as Sar-
castic (Nasty) if at least two patterns are present and
above the thresholds ?1 and ?2, exhausting all com-
binations of ?1 = [2, 4, 6, 8, 10] and ?2 = [.55, .60,
.65, .70, .75, .80, .85, .90, .95, 1.00]. The counter-
classes are predicted when the utterance contains
less than two patterns. The exhaustive classifica-
tions are first made using the utterances in the de-
velopment set labeled ?PE eval?. Fig. 4 shows the
precision and recall trade-off for ?1 = [2, 10] and all
?2 values on sarcasm development set?PE eval?. As
recall increases, precision drops. By including pat-
terns that only appear 2 times, we get better recall.
Limiting ?1 to 10 yields fewer patterns and lower
recall.
Table 8 shows the results for various parameters.
The PE dev dataset learned a total of 1,896 sarcas-
tic extraction patterns above a minimum threshold of
?1 < 2 and ?2 < 0.55, and similarly 847 nasty ex-
traction patterns. Training on development set ?PE
dev? yields high precision and good recall. To se-
lect the best parameters, we again look for a balance
between precision and recall. Both Classifiers have
very high precision. In the end, we select parame-
ters that have a better recall than the best parame-
ter from the HP Classifiers which is recall = 38%
for sarcasm and recall = 49% for nastiness. The
best parameters and their test results are shown in
Table 9.
PARAMS P R F N (tp)
SARC ?1 =2, ?2 =.60 65% 49% 0.57 792
2, .65 71% 44% 0.58 717
2, .70 80% 38% 0.59 616
2, 1.0 97% 24% 0.60 382
NASTY ?1 =2, ?2 =.65 71% 49% 0.60 335
2, .75 83% 42% 0.62 289
2, .90 96% 30% 0.63 209
Table 8: Pattern Classification Training; P: precision, R:
recall, F: F-measure, tp: true positive classifications
The Pattern Classifiers are tested on ?HP dev test?
with the labels predicted by our HP Classifiers, thus
we have two different sets of classifications for both
Sarcasm and Nastiness: percent features and ?2 fea-
tures. Overall, the Pattern Classification performs
better on Nasty than Sarcasm. Also, the percent fea-
tures yield better results than ?2 features, possibly
because the precision for ?2 is high from the HP
Classifiers, but the recall is very low. We believe
that ?2 selects statistically predictive indicators that
are tuned to the dataset, rather than general. Having
a human in the loop guarantees more general fea-
tures from a smaller dataset. Whether this remains
true on the size as the dataset increases to 1000 or
more is unknown. We conclude that R&W?s patterns
generalize well on our Sarcasm and Nasty datasets
(open question O3), but suspect that there may be
better syntactic patterns for bootstrapping sarcasm
and nastiness, e.g. involving cue words or semantic
categories of words rather than syntactic categories,
as we discuss in Sec. 7.
This process can be repeated by taking the newly
classified utterances from the Pattern Based Clas-
sifiers, then applying the Pattern Extractor to learn
new patterns from the newly classified data. This
37
PARAMS P R F
Sarc % ?1 =2, ?2 =.70 62% 52% 0.57
Sarc ?2 ?1 =2, ?2 =.70 31% 58% 0.45
Nasty % ?1 =2, ?2 =.65 75% 62% 0.69
Nasty ?2 ?1 =2, ?2 =.65 30% 70% 0.50
Table 9: The results for Pattern Classification on HP dev
test dataset ; PARAMS: the best parameters for each fea-
ture set P: precision, R: recall, F: f-measure
can be repeated for multiple iterations. We leave this
for future work.
7 Discussion and Future Work
In this work, we apply a bootstrapping method to
train classifiers to identify particular types of subjec-
tive utterances in online dialogues. First we create
a suite of linguistic indicators for sarcasm and nas-
tiness using crowdsourcing techniques. Our crowd-
sourcing method is similar to (Filatova, 2012). From
these new linguistic indicators we construct a classi-
fier following previous work on bootstrapping sub-
jectivity classifiers (Riloff and Wiebe, 2003; Thelen
and Riloff, 2002). We compare the performance of
the High Precision Classifier that was trained based
on statistical measures against one that keeps human
annotators in the loop, and find that Classifiers us-
ing statistically selected indicators appear to be over-
trained on the development set because they do not
generalize well. This first phase achieves 54% preci-
sion and 38% recall for sarcastic utterances using the
human selected indicators. If we bootstrap by using
syntactic patterns to create more general sarcasm in-
dicators from the utterances identified as sarcastic in
the first phase, we achieve a higher precision of 62%
and recall of 52%.
We apply the same method to bootstrapping a
classifier for nastiness dialogic acts. Our first phase,
using crowdsourced nasty indicators, achieves 58%
precision and 49% recall, which increases to 75%
precision and 62% recall when we bootstrap with
syntactic patterns, possibly suggesting that nastiness
(insults) are less nuanced and easier to detect than
sarcasm.
Previous work claims that recognition of sarcasm
(1) depends on knowledge of the speaker, (2) world
knowledge, or (3) use of context (Gibbs, 2000; Eis-
terhold et al, 2006; Bryant and Fox Tree, 2002;
Carvalho et al, 2009). While we also believe that
certain types of subjective language cannot be de-
termined from cue words alone, our Pattern Based
Classifiers, based on syntactic patterns, still achieves
high precision and recall. In comparison to previous
monologic works whose sarcasm precision is about
75%, ours is not quite as good with 62%. While the
nasty works do not report precision, we believe that
they are comparable to the 64% - 83% accuracy with
our precision of 75%.
Open question O3 was whether R&W?s patterns
are fine tuned to subjective utterances in news. How-
ever R&W?s patterns improve both precision and re-
call of our Sarcastic and Nasty classifiers. In fu-
ture work however, we would like to test whether
semantic categories of words rather than syntactic
categories would perform even better for our prob-
lem, e.g. Linguistic Inquiry and Word Count cat-
egories. Looking again at row 1 in Table 10, ?It
is quite strange to encounter someone in this day
and age who lacks any knowledge whatsoever of the
mechanism of adaptation since it was explained 150
years ago?, the word ?quite? matches the ?cogmech?
and ?tentative? categories, which might be interest-
ing to generalize to sarcasm. In row 11 ?At the time
of the Constitution there weren?t exactly vast sub-
urbs that could be prowled by thieves looking for an
open window?, the phrase ?weren?t exactly? could
also match the LIWC categories ?cogmech? and ?cer-
tain? or, more specifically, certainty negated.
We also plan to extend this work to other cate-
gories of subjective dialogue acts, e.g. emotional
and respectful as mentioned in the Introduction, and
to expand our corpus of subjective dialogue acts. We
will experiment with performing more than one iter-
ation of the bootstrapping process (R&W complete
two iterations) as well as create a Hybrid Classifier
combining the subjective cues and patterns into a
single Classifier that itself can be bootstrapped.
Finally, we would like to extend our method to
different dialogue domains to see if the classifiers
trained on our sarcastic and nasty indicators would
achieve similar results or if different social media
sites have their own style of displaying sarcasm or
nastiness not comparable to those in forum debates.
References
G.A. Bryant and J.E. Fox Tree. 2002. Recognizing ver-
bal irony in spontaneous speech. Metaphor and sym-
bol, 17(2):99?119.
P. Carvalho, L. Sarmento, M.J. Silva, and E. de Oliveira.
2009. Clues for detecting irony in user-generated con-
38
tents: oh...!! it?s so easy;-). In Proc. of the 1st inter-
national CIKM workshop on Topic-sentiment analysis
for mass opinion, p. 53?56. ACM.
D. Davidov, O. Tsur, and A. Rappoport. 2010. Semi-
supervised recognition of sarcastic sentences in twitter
and amazon. In Proc. of the Fourteenth Conference on
Computational Natural Language Learning, p. 107?
116. Association for Computational Linguistics.
J. Eisterhold, S. Attardo, and D. Boxer. 2006. Reactions
to irony in discourse: Evidence for the least disruption
principle. Journal of Pragmatics, 38(8):1239?1256.
E. Filatova. 2012. Irony and sarcasm: Corpus genera-
tion and analysis using crowdsourcing. In Language
Resources and Evaluation Conference, LREC2012.
J.E. Fox Tree and J.C. Schrock. 1999. Discourse Mark-
ers in Spontaneous Speech: Oh What a Difference
an Oh Makes. Journal of Memory and Language,
40(2):280?295.
J. E. Fox Tree. 2010. Discourse markers across speak-
ers and settings. Language and Linguistics Compass,
3(1):1?13.
P. Gianfortoni, D. Adamson, and C.P. Rose?. 2011. Mod-
eling of stylistic variation in social media with stretchy
patterns. In Proc. of the First Workshop on Algo-
rithms and Resources for Modelling of Dialects and
Language Varieties, p. 49?59. ACL.
R.W. Gibbs. 2000. Irony in talk among friends.
Metaphor and Symbol, 15(1):5?27.
R. Gonza?lez-Iba?n?ez, S. Muresan, and N. Wacholder.
2011. Identifying sarcasm in twitter: a closer look.
In Proc. of the 49th Annual Meeting of the ACL: Hu-
man Language Technologies: short papers, volume 2,
p. 581?586.
A. Razavi, D. Inkpen, S. Uritsky, and S. Matwin. 2010.
Offensive language detection using multi-level classi-
fication. Advances in Artificial Intelligence, p. 16?27.
A. Reyes and P. Rosso. 2011. Mining subjective knowl-
edge from customer reviews: a specific case of irony
detection. In Proc. of the 2nd Workshop on Computa-
tional Approaches to Subjectivity and Sentiment Anal-
ysis (WASSA 2.011), ACL, p. 118?124.
A. Reyes, P. Rosso, and D. Buscaldi. 2012. From humor
recognition to irony detection: The figurative language
of social media. Data & Knowledge Engineering.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proc. of the 2003
conference on Empirical methods in Natural Lan-
guage Processing-V. 10, p. 105?112. ACL.
R. Snow, B. O?Conner, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast?but is it good?: evaluating non-expert
annotations for natural language tasks In Proc. of
the Conference on Empirical Methods in Natural Lan-
guage Processing, p. 254?263. ACM.
S.O. Sood, E.F. Churchill, and J. Antin. 2011. Auto-
matic identification of personal insults on social news
sites. Journal of the American Society for Information
Science and Technology.
Dan Sperber and Deidre Wilson. 1981. Irony and the
use-mention distinction. In Peter Cole, editor, Radical
Pragmatics, p. 295?318. Academic Press, N.Y.
E. Spertus. 1997. Smokey: Automatic recognition of
hostile messages. In Proc. of the National Conference
on Artificial Intelligence, p. 1058?1065.
M. Thelen and E. Riloff. 2002. A bootstrapping method
for learning semantic lexicons using extraction pattern
contexts. In Proc. of the ACL-02 conference on Empir-
ical methods in natural language processing-Volume
10, p. 214?221. ACL.
O. Tsur, D. Davidov, and A. Rappoport. 2010. Icwsm?
a great catchy name: Semi-supervised recognition of
sarcastic sentences in online product reviews. In Proc.
of the fourth international AAAI conference on we-
blogs and social media, p. 162?169.
Marilyn Walker, Pranav Anand, , Robert Abbott, and
Jean E. Fox Tree. 2012. A corpus for research on
deliberation and debate. In Language Resources and
Evaluation Conference, LREC2012.
J.M. Wiebe, R.F. Bruce, and T.P. O?Hara. 1999. Devel-
opment and use of a gold-standard data set for subjec-
tivity classifications. In Proc. of the 37th annual meet-
ing of the Association for Computational Linguistics,
p. 246?253. ACL.
J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis,
B. Fraser, D. Litman, D. Pierce, E. Riloff, T. Wilson,
et al 2003. Recognizing and organizing opinions ex-
pressed in the world press. In Working Notes-New Di-
rections in Question Answering (AAAI Spring Sympo-
sium Series).
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Pat-
wardhan. 2005. Opinionfinder: A system for subjec-
tivity analysis. In Proc. of HLT/EMNLP on Interactive
Demonstrations, p. 34?35. ACL.
G. Xiang, B. Fan, L. Wang, J. Hong, and C. Rose. 2012.
Detecting offensive tweets via topical feature discov-
ery over a large scale twitter corpus. In Proc. of
the 21st ACM international conference on Information
and knowledge management, p. 1980?1984. ACM.
8 Appendix A. Instances of Learned
Patterns
39
Pattern Instance FREQ %SARC Example Utterance
<subj> was explained 2 100% Well, I incorrectly assumed that anyone attempting to enter the discus-
sion would at least have a grasp of the most fundamental principles. It
is quite strange to encounter someone in this day and age who lacks any
knowledge whatsoever of the mechanism of adaptation since it was ex-
plained 150 years ago.
<subj> appears 1 94% It appears this thread has been attacked by the ?line item ? poster.
<subj> have problem 4 50% I see your point, Iangb but I?m not about to be leaving before you?ve had
a chance to respond. I won?t be ?leaving ? at all. You challenged me
to produce an argument, so I?m going to produce my argument. I will
then summarize the argument, and you can respond to it and we can then
discuss / debate those specifics that you have a problem with.
<subj> have to do 15 86% How does purchasing a house have to do with abortion? Ok, so what if
the kid wants to have the baby and the adults want to get rid of it? What
if the adults want her to have the baby and the kid wants to get rid of it?
You would force the kid to have a child (that doesn?t seem responsible at
all), or you would force the kid to abort her child (thereby taking away
her son or daughter). Both of those decisions don?t sound very consitent
or responsible. The decision is best left up to the person that is pregnant,
regardless of their age.
<subj> is nothing 10 90% Even though there is nothing but ad hoc answers to the questions, cre-
ationists touted the book as ?proof ? that Noahs? ark was possible. They
never seem to notice that no one has ever tried to build and float an ark.
They prefer to put the money into creation museums and amusement
parks.
gives <dobj> 25 88% Just knowing that there are many Senators and Congressmen who would
like to abolish gun rights gives credence to the fact that government could
actually try to limit or ban the 2nd Amendment in the future.
to force <dobj> 9 89% And I just say that it would be unjust and unfair of you to force meta-
physical belief systems of your own which constitute religious belief
upon your follows who may believe otherwise than you. Get pregnant
and treat your fetus as a full person if you wish, nobody will force you
to abort it. Let others follow their own beliefs differing or the same.
Otherwise you attempt to obtain justice by doing injustice
want to take <dobj> 5 80% How far do you want to take the preemptive strike thing? Should we
make it illegal for people to gather in public in groups of two or larger
because anything else might be considered a violent mob assembly for
the basis of creating terror and chaos?
fact is <dobj> 6 83% No, the fact is PP was founded by an avowed racist and staunch supporter
of Eugenics.
argument against <np> 4 75% Perhaps I am too attached to this particular debate that you are having
but if you actually have a sensible argument against gay marriage then
please give it your best shot here. I look forward to reading your com-
ments.
looking for <np> 14 92% At the time of the Constitution there weren?t exactly vast suburbs that
could be prowled by thieves looking for an open window.
was put in <np> 3 66% You got it wrong Daewoo. The ban was put in place by the 1986 Firearm
Owners Protection Act, designed to correct the erronius Gun Control Act
of 1968. The machinegun ban provision was slipped in at the last minute,
during a time when those that would oppose it werent? there to debate it.
to go to <np> 8 63% Yes that would solve the problem wouldn?t it,worked the first time
around,I say that because we (U.S.)are compared to the wild west. But
be they whites,Blacks,Reds,or pi** purple shoot a few that try to detain
or threaten you, yeah I think they will back off unless they are prepared
to go to war.
Table 10: Sarcastic patterns and example instances
40
Proceedings of the SIGDIAL 2013 Conference, pages 41?50,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Topic Independent Identification of
Agreement and Disagreement in Social Media Dialogue
Amita Misra & Marilyn A. Walker
Natural Language and Dialogue Systems Lab
Computer Science Department
University of California, Santa Cruz
maw|amitamisra@soe.ucsc.edu
Abstract
Research on the structure of dialogue has
been hampered for years because large di-
alogue corpora have not been available.
This has impacted the dialogue research
community?s ability to develop better the-
ories, as well as good off-the-shelf tools
for dialogue processing. Happily, an in-
creasing amount of information and opin-
ion exchange occur in natural dialogue in
online forums, where people share their
opinions about a vast range of topics. In
particular we are interested in rejection
in dialogue, also called disagreement and
denial, where the size of available dia-
logue corpora, for the first time, offers
an opportunity to empirically test theo-
retical accounts of the expression and in-
ference of rejection in dialogue. In this
paper, we test whether topic-independent
features motivated by theoretical predic-
tions can be used to recognize rejection in
online forums in a topic-independent way.
Our results show that our theoretically mo-
tivated features achieve 66% accuracy, an
improvement over a unigram baseline of
an absolute 6%.
1 Introduction
Research on the structure of dialogue has been
hampered for years because large dialogue corpora
have not been publicly available. This has im-
pacted the dialogue research community?s ability
to develop better theories, as well as good off-the-
shelf tools for dialogue processing that account for
the richness of human dialogue. Happily, an in-
creasing amount of information and opinion ex-
change occurs in natural dialogue in online fo-
rums, where people can express their opinion on
a vast range of topics from Should there be more
stringent gun laws? to Are school uniforms a good
idea? (Walker et al, 2012a). For example, con-
sider the dialogic exchange in Fig. 1.
Post P, Response R
P1: Can the government force abortion clinics to carry
anti-abortion articles and papers? Or maybe force them
provide a sonogram? Force them to have a psychologist
on staff? Force them to have 3x3 foot posters of aborted
babies on the wall? Seems like it makes more sense for a
state to restrict something from the people rather than force
the people to have something. No?
R1: I don?t see why this matters. Could you please elab-
orate a little more, and in that elaboration, could you ad-
dress why the government may require a private company
to provide this commonly recommended medical remedy
(plan b) when it does not do so with countless other com-
mon medically recommended remedies?
Figure 1: Disagreement from 4forums.com. Pos-
sible features in bold.
In particular we are interested in the phe-
nomenon of REJECTION in dialogue (Horn, 1989;
Walker, 1996a), also called disagreement and de-
nial. Our data show that the amount of disagree-
ment in online ideological dialogues ranges from
80% to 90% across topic. Such data provides a
rich resource for testing theoretical accounts of re-
jection, as well as for developing computational
models of how to recognize rejection in dialogue.
To date, rejection has received relatively little at-
tention in computational models of discourse be-
cause of its rareness in task-oriented, tutorial or
SwitchBoard style dialogue. Computational mod-
els of argumentative discourse do not typically at-
tempt to account for rejection in dialogue, focus-
ing instead on monologic sources displaying legal
reasoning, logical accounts of rejection, or how to
produce good arguments using natural language
generation (Zukerman et al, 2000; Carenini and
Moore, 2000; Wiley, 2005; Sadock, 1977).
Moreover, the theoretical literature strongly
suggests that there should be topic-independent in-
dicators of rejection. In work on politeness the-
ory, rejection is a dispreferred response, predict-
ing that rejection should be associated with mark-
ers of dispreferred responses such as disfluencies
and hedging (Brown and Levinson, 1987). Work
on negation specifies markers of negation and con-
trast such as but or only for different types of re-
jection, and work on discourse relations and their
41
Type Context Rejection
DENIAL Pigs can fly. No, you idiot, pigs can?t fly! (Horn?s 29)
LOGICAL CONTRADICTION Kim and Lee have been partners since
1989.
But Lee said they met in 1990.
IMPLICIT DENIAL Julia?s daughter is a genius. Julia doesn?t have any children.
REFUSAL Come and play ball with me. No, I don?t want to. (Horn?s 33)
IMPLICATURE REJECTION There?s a man in the garage. There?s something in the garage. (Walker?s 6)
DENYING BELIEF TRANSFER B: Well ah he uh ... he belongs to a
money market fund now and uh they
will do that for him. H: The money
market fund will invest it in govern-
ment securities as part of their individ-
ual retirement account ? is that what
you?re saying? B: Right.
H: I?m not so sure of that. (Walker?s 31)
INCONSISTENT PAST BELIEF H: Then they are remiss in not sending
it to you because that money is taxable
sir.
M: I know it?s taxable, but I thought they
would wait until the end of the 30 months.
CITING CONTRADICTORY
AUTHORITY
H: No sir.... R: That?s what they told me.
Figure 2: Classification and Examples of the Types of Rejections.
markers suggests that DENIAL is a type of COM-
PARISON relation (Horn, 1989; Groen et al, 2010;
Webber and Prasad, 2008). These observations,
among others, suggest a range of theoretically mo-
tivated features for the classification of rejection in
online dialogue, e.g. phrases such as I think, but, I
don?t see, and Can you. See Fig. 1.
Our aim is to test whether theoretical predic-
tions and topic-independent features motivated by
them can be used to recognize rejection in online
forums. We generalize our topic independent fea-
tures using a development set on the topic Evolu-
tion. We then test a rejection (disagreement) clas-
sifier trained on Evolution on 1757 posts covering
a collection of other topics, and compare our re-
sults to a ngram model trained on Evolution and
tested on the same test set. See Table 1.
We first describe our corpus in Sec. 2, and then
review previous work characterizing the theoreti-
cal basis of rejection in dialogue in Sec. 3. Sec. 4
describes our method for classifying rejections
and Sec. 5 presents our results, showing that our
theoretically motivated rejection cues are reliable
across topic. We show that cue words, polarity,
punctuation, denial and claim features motivated
by the theoretical literature provide a significant
improvement over a 50% baseline, and that all
of the theoretically motivated features combined
achieve 66% accuracy as compared to a unigram
accuracy of 60%. We delay reviewing previous
computational work rejection to Sec. 6 when we
can compare it with our own work.
2 Corpus
We utilize the publicly available Internet Ar-
gument Corpus (IAC), an annotated collec-
Topic Agr DisAgr Total
Evolution 460 460 920
Abortion 250 280 530
Climate Change 17 10 27
Communism vs. Capitalism 10 13 23
Death Penalty 15 19 34
Existence Of God 53 48 101
Gay Marriage 173 134 307
Gun Control 334 331 665
HealthCare 21 37 58
Marijuana Legalization 6 6 12
All Topics (test set) 879 878 1757
Table 1: Distribution of (Dis)Agreement by Topic.
The Evolution topic is for development and train-
ing. The test set of other topics is balanced overall,
but not by topic.
tion of 109,553 forum posts (11,216 discussion
threads)(Walker et al, 2012a). We use the
portion of the IAC containing dialogues from
http://4forums.com. On 4forums, a person
starts a discussion by posting a topic or a question
in a particular category, such as society, politics,
or religion. Forum participants can then post their
opinions, choosing whether to respond directly to
a previous post or to the top level topic (start a
new thread). Conversants may simply agree or dis-
agree with a previous post or they may provide a
reasoned argument.
The corpus contains posts on topics such as
Abortion, Evolution, Existence of God, Gay Mar-
riage and Gun control along with a range of use-
ful annotations. First, there are annotations that
collapse different discussions into a single topic
for 14 topics. For example, the Evolution and
Gun Control topics include discussions initiated
with the range of titles in Table 2, which guaran-
42
First Post (P), Response (R)
Disagreements
P1: No I didn?t miss it, I was hoping you?d actually put forward an argument against what I said, not what you think I
said. See what I actually said was the tautology. Then make your argument. Note Post 30 He said evolution is a tautology.
I said that Darwin preferred a tautology to ?Natural Selection? You may have mixed up who it is you?re arguing against.
R1: I?m wondering. What do we call someone who debates feverishly on scientific theories, yet admittedly does not
understand the concepts they are arguing against? Is it productive to debate something that you don?t understand
the concepts of when it?s a fairly involved theory based on scientific evidence? What if you convinced someone NOT
to believe in it, but you did so using falsifiable reasons, since you aren?t an expert and might not know any better?
Irresponsible, is one such word, that comes to mind.
P2: What in Vishnu?s name does this have ANYTHING to do with evolution vs creation???
R2: Well, many have argued that if you don?t except a literal Genesis, you?re damned. Perhaps not in this particular
thread, but the arguments are essentially the same. I believe that the theological implications of that position are fair
game for discussing the validity of creationism.
P4: You have this backwards. The word theory was originally a scientific word, and then it was adapted into common
speech to mean a range of things not originally designated to that word. Words like evolve, gravity and congruent have
different meanings within the realm of science than they have outside. If you can?t appreciate the difference between
the definition of a word in the context of science as opposed to the context of common speech, then maybe you have no
business in science.
R4: When it comes to all the examples that Behe had provided in both his first book, and his second book , it has been
shown to be able to evolve naturally. That means, in principle, IC systems can evolve. If you don?t believe so, bring forth
the I.C. system of your choice. To say ?you don?t know all the answers? is just the logical fallacy known as ?argument
from ignorance?. Behe brings a system up that he claims is IC. the pathway for evolution is discovered, and Behe trys
another one. How dishonest can you get? The concept is falsified.
P5: Well, Genesis has God making all the animals ?and their kind?, and then when he?s done with that he makes humans.
So I would assume that humans don?t fit into the ?kind? schema, or perhaps are a kind unto themselves........
R5: : So we can?t base our definition of ?kind? on mere appearances? I mean if we are going to put things into
categories and call the category ?kind?, we should do this by common appearances. A penguin is in the same kind as a
hummingbird, but is a lobster in the same kind as an oyster? ........
Agreements
P6: I think its nonsense interpretation developed by people who were afraid that if they fought for guns as valiantly as
they did for free speech, they wouldn?t receive any donations.
R6: I think you are entirely correct. From the page VOR linked: There is no evidence ANYWHERE that the second
amendment is a collective right. We have been over this multiple times, and the evidence simply does not exist, and an
organization like the ACLU should be well aware of this.
P7: Correction: If one isn?t a fundementalist, literal christian, jew or muslim, then marc considers them a atheist. He?s
never going to deal with the fact that he?s quite wrong on that subject. It?s obvious to everyone that he?s constantly
avoiding it even when asked point blank several times. A sign of argumental failure is constant avoidance of a simple
question.
R7: Quite right. My mistake. Once again, quite right...
P8: thats pretty neat. Did they finish up the feeder?
R8: yeah, this is clearly the best thread on these forums in probably the past year....give us some more pics length)
P9: This is probably the most rational site in all of the creationist?s online arguments. Arguments we think creationists
should NOT use
R9: Thanks, DuoMax, for this link. How delightful to see here mention of this solid gesture, on the part of a major
creationist organization, in the direction of intellectual integrity..... ....Each time a Christian stands in the pulpit and pours
out poor argument, s/he loses ground for the faith. Thanks again.
Figure 3: Disagreements and Agreements from 4forums.com. Theoretically motivated features are in
bold.
Evolution Evolution in school, Dinosaurs and Hu-
man Footprints, Can Evolution & Reli-
gion Coexist, Did Charles Darwin Re-
cant, Shrinking Sun, Bombardier beetle,
Moon Dust, Second Law of Thermody-
namics, Magnetic Field, Nebraska Man
Gun Control Gun Control, Trigger Locks, Guns in the
Home, Right to Carry, Assault Weapons,
One gun a month, Gun Buy Back, Gun-
Seizure Laws, Plastic Guns, Does gun
ownership deter crime, Second Amend-
ment, Enforced Gun Control Laws?,
Gun Registration, Armor piercing bul-
lets, Background Checks at Gun Shows
Table 2: Discussions Mapped to the Evolution and
Gun Control Topics.
tees variation in the focus of the discussion even
within topic. The topics we use are in Table 1.
Each discussion is threaded so that we can iden-
tify direct responses. Discussions may have a tree-
like structure, so a post may have multiple di-
rect responses. In addition to the adjacency pairs
yielded by threading, 4forums also provides a
quote/response Q/R mechanism where a post may
include a quote of part or all of a previous post.
We do not use the Q/R pairs here.
The IAC also includes annotations collected via
Mechanical Turk on these dialogue pairs. There
are 20,000 pairs from threads of 3 posts P1,P2,P3
with annotations for (dis)agreement for pairs (P1,
P2) and (P2, P3). Agreement was a scalar judg-
43
ment on an 11 point scale [-5,5] implemented with
a slider. The annotators were also able to signal
uncertainty with a CAN?T TELL option. Each of
the pairs was annotated by 5-7 annotators, in re-
sponse to the annotation question Does the respon-
dent agree or disagree with the prior post?. Anno-
tators achieved high agreement on dis(agreement)
annotation with an ? of 0.62. We used thresholds
of 1 and -1 on the mean agreement judgment to de-
termine agreement and disagreement respectively.
We omitted dialogue adjacency pairs with mean
annotator judgment in the (-1,1) range. Table 1
provides the distribution of topics for the 1757
posts in the test set.
3 Theories of Rejection in Dialogue
A common view of dialogue is that the conversa-
tional record is part of the COMMON GROUND of
the conversants. As conversants A and B partici-
pate in a dialogue, A and B communicate through
dialogue speech acts such as PROPOSALS, ASSER-
TIONS, ACCEPTANCES and REJECTIONS. If A
asserts a proposition ? and B accepts A?s asser-
tion, the ? becomes a mutual belief in the com-
mon ground. If B rejects A?s assertion or proposal,
the common ground remains as it was (Stalnaker,
1978). For conversants to remain coordinated
(Thomason, 1990), they must monitor whether
their utterances are accepted or rejected by their
conversational partners.
Computational models of dialogue also must
track what is in the common ground (Traum, 1994;
Stent, 2002). This would be simple if conversants
always explicitly indicated rejection with forms
such as I reject your assertion. However recog-
nizing rejection typically relies on making infer-
ences. Horn categorizes rejections into: DENIAL
a straightforward negation of the other?s assertion;
LOGICAL CONTRADICTION following from logi-
cal inference; IMPLICIT DENIAL where B denies
a presupposition of A?s; and REFUSAL, also called
REJECTION where B refuses an offer or proposal
of A?s (Horn, 1989). See Fig. 2. All of Horn?s
forms can be identified as rejections by recogniz-
ing logical inconsistency either directly from what
was said, or via an inferential chain.
However subsequent work by Walker on the
Harry Gross Corpus (henceforth HGC) of advice-
giving dialogues (Pollack et al, 1982) demon-
strated that REJECTION IMPLICATURES as seen in
the 5th row of Fig. 2, are common in natural di-
alogue (Walker, 1996a). A number of similar ex-
amples can also be found in (Hirschberg, 1985).
Here, the proposition realized by the response fol-
lows from the original assertion as an entailment
via existential generalization. Thus the REJEC-
TION IMPLICATURE is logically consistent with
the original assertion.
Walker argues that the fact that an implicature
can function as a rejection clearly indicates that
inference rules about what gets added to the com-
mon ground must have the same logical status as
implicatures, i.e. they must be default rules of
inference that can be defeated by context. She
then goes on to identify additional types of rejec-
tions in HGC that rely on detecting conflicts in
the default inferences triggered by the epistemic
inference rules used in speech act theory. Walker
uses a compressed version of rules from (Perrault,
1990; Appelt and Konolige, 1988), assuming that
conflicting defaults can arise between these in-
ferences and implicature inferences (Hirschberg,
1985). The first rule is given in 1:
(1) BELIEF TRANSFER RULE:
Say(A,B,p)? Bel (B,p)
The Belief Transfer Rule states that if one agent
A makes an assertion that p then by default another
agent B will come to believe that p. The second
rule is in 2:
(2) BELIEF PERSISTENCE RULE:
Bel (B,p,t0)? Bel (B,p,t1)
The Belief Persistence Rule states that if an
agent B believes p at time t0 then by default agent
B still believes p at a later time t1. These rules pro-
vide the basis for inferring three additional types
of rejections:
? DENYING BELIEF TRANSFER: Agent B can
deny the consequent of the Belief Transfer
Rule by negatively evaluating A?s assertion or
expressing doubt as to its truth.
? INCONSISTENT PAST BELIEF: Inferring that
B?s expression of an inconsistent past belief
is a type of rejection relies on detecting con-
flicting defaults with the Belief Transfer Rule
and the Belief Persistence Rule. The two be-
liefs may directly conflict, or the conflict may
arise via an inferential chain.
? CITING CONTRADICTORY AUTHORITY: In-
ferring that citing a contradictory authority
is a type of rejection relies on recognizing
two inconsistent instantiations of the Belief
Transfer rule. For example, agent A1 as-
serted p and agent A2 asserted ?p, leaving
B in an inconsistent belief state caused by the
conflicting defaults generated by the alternate
instantiations of the Belief Transfer Rule.
44
Fig. 2 provides Walker?s examples of these
new types of rejection and Fig. 3 illustrates dis-
agreements and agreements in the IAC corpus.1
While we see many instances of the rejection
types in Fig. 2 in IAC, especially CITING CON-
TRADICTORY AUTHORITY and DENYING BELIEF
TRANSFER, we also find new types such as ad-
hominem attacks on the other speaker as the
source of particular propositions (e.g. R1 in Fig. 3,
which would not have occurred in HGC talk show
context. Other cases that we have noted are a
different type of DENYING BELIEF TRANSFER,
which occurs when a previous speaker?s asserted
proposition is marked by the hearer as hypotheti-
cal using a conditional, e.g. If capital punishment
is a deterrent, then ..... In future work we aim to
expand the taxonomy of rejections using IAC.
4 Empirical Method
Our primary hypothesis is that certain expres-
sions and phrases are reliable cues to the auto-
matic identification of the speech acts of REJEC-
TION and ACCEPTANCE, i.e. (dis)agreement, in-
dependently of the topic. We assume that it will
not always be possible to get annotated data for a
particular topic, given the ever-burgeoning range
of topics discussed online. We use the Evolu-
tion topic as our development set, and ask: given
(dis)agreement annotations for only one topic, is it
possible to develop features that perform well on
another arbitrary topic?
There is limited previous research on disagree-
ment, thus it is an open issue what types of fea-
tures might be useful. One line of previous work
suggests that various pragmatic features might
help (Galley et al, 2004). Another line suggests
that disagreement is subtype of the COMPARISON
(CONTRAST) discourse relation, in the Penn Dis-
course TreeBank taxonomy, suggesting that fea-
tures for identifying COMPARISON, such as polar-
ity and discourse cues might also be useful (Hahn
et al, 2006; Prasad et al, 2010; Louis et al, 2010).
We began by selecting and manually inspecting
460 agreements and 460 disagreements from the
Evolution topic, and extracting their most frequent
unigrams, bigrams and trigrams. This showed that
features suggested by theoretical work on rejec-
tion were indeed highly frequent: our aim was
to generalize what we observed in the Evolution
dataset and then test whether the generalized fea-
tures can distinguish agreements from disagree-
ments. We first observed that very few unigrams
1Since participants are not generally making plans to-
gether in these dialogues, we leave aside Walker?s classifi-
cation of rejections of proposals.
were useful for disagreements, e.g. liar, no, don?t,
while bigrams such as I don?t, How can, If I, how
could, show me seemed to be better indicators.
Furthermore, trigrams such as I don?t agree, how
can you, point is that, and I do not understand
are even stronger indicators of disagreement, but
of course these higher order ngrams are less fre-
quent and are more likely to contain topic-specific
words. In order to provide better generalization,
we generalized the ngrams that we observed, e.g.
an instance such as how can you would also result
in how can we and how can they being added to the
same feature set. We also generalized over hedges
and other categories of features on the basis of the
theoretical literature. The total set of features we
developed are grouped into the sets in Table 3 dis-
cussed in detail below.
Feature Description Examples
Agreement Ngrams in-
dicative of
accepting
others claim.
right, yes, yeah, correct,
accepted, thanks, good,
agree, acknowledge
Cue Words Cues as Ngrams
and their LIWC
CogMech gen-
eralizations
oh, so, uh, yes, no, dont,
cogmech, claim, i, yeah,
because, well, just, and,
you, you mean, i see, i
COGMECH
Denial Ngrams indica-
tive of denying
another?s claim
You don?t know, That
does not, I don?t think,
what is, This has noth-
ing, I don?t see, You
do not, do you mean,
I don?t know, we don?t
have, Problem with
that, I do not, Does not,
why do, But I don?t,
how can
Hedges Unigrams,
bigrams, and
trigrams that
include hedge
terms.
Im wondering, I am
wondering, whatever,
somewhat, may be,
possibly, anyway, it
seems to me, my view,
actually, my opinion,
essentially, somewhat,
my perspective, rather,
although, really, I
suppose, perhaps
Duration Sentence, word and post lengths
Polarity Means of positive and negative polarity
terms.
Punctuation Counts of question marks and exclamation
points.
Table 3: Feature Sets, Descriptions, and Exam-
ples. The unigrams features are our baseline case;
these features are not theoretically motivated.
Unigrams. Results of previous work on stance
identification in argumentative discourse suggest
that a unigram baseline can be difficult to beat
(Thomas et al, 2006; Somasundaran and Wiebe,
2010). Thus we test our theoretically moti-
vated features against unfiltered unigrams and un-
45
igrams+bigrams as baselines.
Agreement and Denial. As described above we
used Evolution to manually develop generaliza-
tions of the observed unigrams, bigrams and tri-
grams that were consistent with theoretical pre-
dictions. We split the indicator features into two
categories Agreement and Denial. See Table 3.
Our manual analysis suggested that agreements
have few topic independent markers. Unigrams
such as agree correct and right were also present
in disagreements, and trigrams such as I agree
but, You may be correct however I do not agree,
I don?t agree were better indicators of disagree-
ment. Our agreement markers are thus a small
category where we check that the keywords agree,
correct and right are not preceded by a negation
marker and not followed by discourse markers
such as but, yet, or however. However, the denial
category at present has more than 300 ngrams ex-
tracted and generalized from the Evolution topic.
Pitler et al (2009) also used ngrams consisting of
the first and last three words for recognition of the
PDTB COMPARISON relation. Other work on the
PDTB also suggests that DENIAL can be indicated
by contrast (Webber and Prasad, 2008).
Cue Words. Both psychological research on dis-
course processes (Fox Tree and Schrock, 1999;
Groen et al, 2010) and computational work on
agreement and discourse markers (Galley et al,
2004; Louis et al, 2010) indicate that discourse
markers are strongly associated with particular
pragmatic functions such as stating a personal
opinion (Asher et al, 2008; Webber and Prasad,
2008). Based on manual inspection of the Evo-
lution devset we selected 18 items for the CUE
WORDS feature set, as in Table 3. Examples are
well in R2 and so and but in R5.
Durational Features. Brown and Levinson?s the-
ory of politeness would suggest that disagree-
ments are dispreferred responses and thus that the
length of the post could indicate disagreement; it
predicts that people will elaborate more and pro-
vide reasons and justifications for disagreement
(Brown and Levinson, 1987). Our durational fea-
tures measure the length of the utterance in terms
of characters, words and sentences.
Hedges. In Brown and Levinson?s theory of po-
liteness, hedges are one of many possible strate-
gies for mitigating a face-threatening act (Brown
and Levinson, 1987; Lakoff, 1973). Hedges can be
used to be deliberately vague or simply to soften
a claim. We see many examples of hedges in on-
line dialogue, e.g. the speaker of R2 in Fig. 3 uses
the hedges Perhaps and essentially, and I mean in
R5. Thus hedges are hypothesized to be useful
feature for distinguishing (dis)agreement, yielding
the hedge features in Table 3.
Polarity. Work on discourse relations in the PDTB
also suggests that differences in polarity across
adjacent utterances might be an indicator of the
COMPARISON relation. In addition, Horn?s classes
of REJECTIONS shown in Fig. 2 all include mark-
ers of negation. Thus to capture the overall senti-
ment of the post we used the MPQA subjectivity
lexicon (Wiebe et al, 2003; Wilson et al, 2005).
Each word is POS tagged and then categorized as
strongly or weakly subjective. The positive po-
larity feature is the sum of the strongly subjective
words of positive polarity, and the negative polar-
ity feature represents the sum of strongly subjec-
tive words of negative polarity.
Punctuation. Another indication of DENYING
BELIEF TRANSFER rejections are the question
marks and exclamation marks that conversants fre-
quently use to express their disbelief and doubt
about another conversant?s claim. For example,
R1 and R5 in Fig. 3 have a high frequency of ques-
tion marks.
5 Results
Our aim was to test how well we can distinguish
agreements and disagreements in IAC using clas-
sifiers trained with theoretically motivated fea-
tures. As described above, we developed our fea-
tures by manual inspection of (dis)agreements in
920 posts on the topic Evolution. We do not train
on a mixture of topics for any feature set, includ-
ing unigrams, because we assume that in general,
new topics are always arising so there will not be
annotated data for every topic. We evaluate the
performance of all types of features on classify-
ing (dis)agreements on other topics combined. We
do not report per-topic results because our test set
baseline accuracies vary a great deal by topic as do
the size of the topic sets. See Table 1.
Features Random Forest J 48
ALL-TM 63.1 66.0
Unigram 56.6 59.8
Bigram 59.3 60.1
Table 4: Accuracies for Theoretically Motivated
Features (ALL-TM), Unigrams and Bigrams with
Random Forest and J48 Trees over a 50% base-
line. No interesting differences observed in preci-
sion and recall.
Table 3 summarizes our theoretically-motivaed
topic-independent features, and Table 4 compares
the accuracies of classifiers using these features to
unigrams and bigrams when we train on Evolu-
46
tion and then test on our mixed-topic test set, using
the Weka learners for Random forest and J48 Tree.
Although unigrams and unigram+bigram achieves
approximately 60% accuracy over a 50% baseline,
paired t-tests on the result vectors show that the
differences in accuracies are statistically signifi-
cant when we compare ALL-TM features with un-
igrams and unigram+bigrams: Random Forest (p
= .004) and J48 Trees (p < .0001).
Ngram N
Feats
Acc Feats Selected
Uni 2K 62.5 understand, fail, never,
nothing, catholic, gene,
irrelevant, acceptable, show,
didn?t, geologist, creationist
Bigram 4k 62.7 ? you, do we, understand
that, ? just, really?, is based,
well said, ? did, can the, the
nature, the church, failed to,
then what
Table 5: Accuracy when fitting to test set for num-
ber of features selected for ngrams, with sample
features.
Moreover even if we optimize on the test set
by examining the variations in performance as a
function of the number of features selected, ALL-
TM still beats both unigram and unigram+bigram,
when features are selected according to ranking by
Gain Ratio. ALL-TM is significantly more accu-
rate when compared to unigrams (p = .003) best
accuracy of 62.5 with 2000 features, and better
than unigram+bigram best accuracy of 62.7 for
4000 features (p = .007). See Table 5.
More interestingly though, if we look at what
features get selected ( Table 5), we see many fea-
tures reminiscent of our theoretically motivated
features. Features highly ranked by the Gain Ra-
tio were topic-independent cues for disagreement
such as understand, fail, nothing, never and Bi-
grams such as ? how, perhaps you, would you,
never said. However there were few high ranked
unigrams and bigrams for agreement. Also note
that topic specific cues such as gene, catholic, cre-
ationist, geologist and the church are selected over
any topic-independent cues for agreement. This
corroborates our manual construction of a com-
bined denial category with more than 300 words
and a very limited agreement category.
To test which features make the most difference,
we also conducted ablation experiments (Table 6),
as well as tests with individual features (Table 7).
Table 6 shows that the CUE WORDS (p = .0008)
and PUNCTUATION features (p = .01) have the
biggest impact on performance. The decrease in
performance when ablating agreement features is
Ablated Feature Random Forest J 48
No Agreement 62.2 65.0
No Cue Words 59.1 62.1
No Denial 63.3 66.0
No Duration 63.6 66.3
No Hedges 64.2 66.5
No Polarity 64.4 66.8
No Punctuation 60.3 61.6
Table 6: Accuracy when Ablating each Theoreti-
cally Motivated Feature with Random Forest and
J48 Trees over a 50% baseline .
not statistically significant (p = .20).
Feature Acc Prec Recall
Agreement 54.4 .55 .54
Cue words 62.5 .63 .62
Denial 52.0 .54 .52
Duration 53.6 .54 .53
Hedges 50.4 .51 .50
Polarity 53.4 .53 .53
Punctuation 65.3 .65 .65
Table 7: Results for Individual Features for J48
Trees over a 50% baseline .
Since the J48 learner performs consistently bet-
ter, we restrict our comparison of individual fea-
tures in Table 7 to that learner. Table 7 shows
that PUNCTUATION and CUE WORDS features by
themselves provide significant performance im-
provements over the unigram baseline, and that
the POLARITY, AGREEMENT, DENIAL and DU-
RATION feature sets provide significant improve-
ments on their own over the majority class base-
line of 50%. A paired t-test shows these differ-
ences are significant at p =.02. To our surprise,
the HEDGE feature was not effective, and we plan
further refinements of it. These results support the
hypothesis that there are clearly markers for agree-
ment and disagreement that are suggested by the
theoretical literature and which are not topic spe-
cific.
6 Discussion and Future Work
We develop topic-independent features for classi-
fying (dis)agreement in online dialogue, and show
that we can beat an unfiltered unigram baseline
by 6%, and even beat the best feature-selection
ngram-based classifers fitted to the test set.
Features we didn?t use from previous work in-
clude word pairs as introduced by (Marcu and
Echihabi, 2002), and used subsequently by (Pitler
et al, 2009) and (Biran and Rambow, 2011). The
issue of whether word pairs are topic-dependent
has never been addressed, but the examples given
in previous work suggest that they may indicate
topic-specific comparisons. Previous work also
47
suggests that context might be helpful in recog-
nizing disagreement (Walker et al, 2012b), but we
did not test the effect of context.
The most similar work to our own trains a dis-
agreement classifier for Q/R response pairs in on-
line forums (Abbott et al, 2011). Their work used
ngrams, MPQA opinion words (Stoyanov et al,
2005), LIWC (Pennebaker et al, 2001), and a dif-
ferent dataset (Q/R instead of P1,P2 datasets), and
does not aim to develop a classifier that works
well independently of topic. Their best accuracy
is about 68% for a feature set called BothLocal
for the JRip classifier using ?2 feature selection.
BothLocal includes unigrams, bigrams, trigrams,
LIWC, punctuation, cue words, dependency fea-
tures, generalized dependency features and utter-
ance length measures, and it is unclear whether
these features are specific to topic. It is also dif-
ficult to directly compare the results because they
do not report accuracies for individual feature sets
or ablated feature experiments. For example, their
unigram accuracy of 63% includes cue words, and
is reported for training and testing on a mixture of
topics without any held-out topics.
Other work on disagreement recognition in-
cludes that of (Wang et al, 2011) who de-
scribe conditional random field model for detect-
ing (dis)agreement between speakers in English
broadcast conversations. They use sampling and
prosodic features such as pause, duration and
speech rate on an unbalanced dataset. They re-
port an increase in F-measure of 4.5% for agree-
ment and 4.7% for disagreement over a baseline of
lexical, structural, and durational features. (Hahn
et al, 2006) show that a contrast classifier im-
proves the accuracy of dis(agreement) classifica-
tion in the ICSI meetings corpus, and that their re-
sults are less affected by imbalanced data. They
improve the F-measure to .755 over a baseline
SVM with F-measure .726. (Yin et al, 2012)
use sentiment, emotion and durational features for
(dis)agreement classification in online forums, and
they show that aggregating local positions over
posts yields 3 to 4% better performance than non-
aggregating baselines.
While recognizing (dis)agreement can be use-
ful in its own right, it has also been shown to
be useful for the identification of stance (Gawron
et al, 2012; Hassan et al, 2010; Thomas et al,
2006; Bansal et al, 2008; Murakami and Ray-
mond, 2010; Agrawal et al, 2003). Work that
focuses on the social network structure of on-
line forums as a way to improve stance classifi-
cation has either assumed that adjacent posts al-
ways disagree, or used simple rules for identify-
ing agreement based on patterns in the reply post
(Murakami and Raymond, 2010; Agrawal et al,
2003). Previous work by Somasundaran & Wiebe
(2009, 2010) develops positive and negative argu-
ing features for the classification of stance, that
at least in motivation, resemble our denial fea-
tures . They show that arguing features are help-
ful in stance classification. Work by (Galley et
al., 2004) on detecting disagreement in meetings
corpora similarly shows that pragmatic features
are useful for detecting disagreement using mod-
els based on Bayesian Networks. (Walker et al,
2012b) use a number of linguistic features such as
unigrams, bigrams, and repeated punctuation and
proposed a supervised model for stance classifica-
tion in online debates. Related work by (Hassan et
al., 2010) focuses on identifying the attitude of the
participants towards one another in online debates.
They relate the polarity of words to the second per-
son pronoun for classification, while related work
by (Abu-Jbara et al, 2012) uses the polarity of
expressions and named entity recognition to iden-
tify a subgroup of participants, where participants
within a subgroup are inclined to agree with one
another. Methods for stance classification in con-
gressional debates do not separately evaluate the
accuracy of (dis)agreement classification (Thomas
et al, 2006; Bansal et al, 2008; Awadallah et al,
2010; Burfoot, 2008).
In future work, we plan to develop more de-
tailed patterns based on LIWC categories and syn-
tactic parses (Thelen and Riloff, 2002). For ex-
ample, an error analysis suggests that sometimes
two people mutually reject the proposal or claim
of a third person, e.g. How can they say that....
In such cases our classifier finds the disagreement
marker how can and classifies it as disagreement.
More detailed syntactic processing would allow us
to refine our patterns to identify particular classes
of targets such as third person vs. first person.
Similarly, here we extended patterns by hand, e.g.
generalizations over pronouns such as I can?t, we
can?t, can you, can we. In future we aim to gen-
eralize such patterns automatically using tagsets.
We expect that more general patterns should im-
prove the accuracy of the topic-independent fea-
ture sets. We also plan to carry out further annota-
tion of the IAC corpus using the classes of rejec-
tions summarized in Fig. 2 to determine whether
there are forms for indicating each type that are
not represented by our features, and to determine
the frequency across a sample of our corpus of the
different types.
48
References
R. Abbott, M. A. Walker, J. E. Fox Tree, P. Anand, R.
Bowmani, and J. King. 2011. How can you say
such things?!?: Recognizing Disagreement in Infor-
mal Political Argument. In Proc. of the ACL Work-
shop on Language in Social Media.
A. Abu-Jbara, M. Diab, P. Dasigi, and D. Radev. 2012.
Subgroup detection in ideological discussions. In
Proc. of the 50th Annual Meeting of the Association
for Computational Linguistics, ACL ?12, p. 399?
409.ACL.
R. Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu.
2003. Mining newsgroups using networks arising
from social behavior. In Proc. of the 12th interna-
tional conference on World Wide Web, p. 529?535.
ACM.
D. Appelt and K. Konolige. 1988. A practical non-
monotonic theory for reasoning about speech acts.
In Proc. of the 26th Annual Meeting of the Associa-
tion for Computational Linguistics.
N. Asher, F. Benamara, and Y. Yannick Mathieu. 2008.
Distilling opinion in discourse: A preliminary study.
In COLING 2008, p. 7?10.
R. Awadallah, M. Ramanath, and G. Weikum. 2010.
Language-model-based pro/con classification of po-
litical text. In Proc. of the 33rd international ACM
SIGIR conference on Research and development in
information retrieval, p. 747?748. ACM.
M. Bansal, C. Cardie, and L. Lee. 2008. The power of
negative thinking: Exploiting label disagreement in
the min-cut classification framework. Proc. of COL-
ING: Companion volume: Posters, p. 13?16.
O. Biran and O. Rambow. 2011. Identifying justifica-
tions in written dialogs. In 2011 Fifth IEEE Interna-
tional Conference on Semantic Computing (ICSC),
p. 162?168. IEEE.
Penelope Brown and Steve Levinson. 1987. Polite-
ness: Some universals in language usage. Cam-
bridge University Press.
C. Burfoot. 2008. Using multiple sources of agree-
ment information for sentiment classification of po-
litical transcripts. In Australasian Language Tech-
nology Association Workshop 2008, v.6, p. 11?18.
G. Carenini and J. Moore. 2000. A strategy for gener-
ating evaluative arguments. In Proc. of the 1st Inter-
national Conference on Natural Language Genera-
tion (INLG-00).
J.E. Fox Tree and J.C. Schrock. 1999. Discourse
Markers in Spontaneous Speech: Oh What a Dif-
ference an Oh Makes. Journal of Memory and Lan-
guage, 40(2):280?295.
M. Galley, K. McKeown, J. Hirschberg, and
E. Shriberg. 2004. Identifying agreement and
disagreement in conversational speech: Use of
bayesian networks to model pragmatic dependen-
cies. In Proc. of the 42nd Annual Meeting on Asso-
ciation for Computational Linguistics, p. 669. ACL.
J.M. Gawron, D. Gupta, K. Stephens, M.H. Tsou,
B. Spitzberg, and L. An. 2012. Using group mem-
bership markers for group identification in web logs.
In The 6th International AAAI Conference on We-
blogs and Social Media (ICWSM 2012).
M. Groen, J. Noyes, and F. Verstraten. 2010. The
Effect of Substituting Discourse Markers on Their
Role in Dialogue. Discourse Processes: A Multidis-
ciplinary Journal, 47(5):33.
S. Hahn, R. Ladner, and M. Ostendorf. 2006. Agree-
ment/disagreement classification: exploiting unla-
beled data using contrast classifiers. In Proc. of
the Human Language Technology Conference of the
NAACL, NAACL06, p. 53?56. ACL.
A. Hassan, V. Qazvinian, and D. Radev. 2010. What?s
with the attitude?: identifying sentences with atti-
tude in online discussions. In Proc. of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, p 1245?1255. ACL.
J.B. Hirschberg. 1985. A Theory of Scalar Implicature.
Ph.D. thesis, University of Pennsylvania, Computer
and Information Science.
L. R. Horn. 1989. A natural history of negation.
Chicago University Press.
G. Lakoff. 1973. Hedges: A study in meaning criteria
and the logic of fuzzy concepts. Journal of Philo-
sophical Logic, 2(4):458?508.
A. Louis, A. Joshi, R. Prasad, and A. Nenkova. 2010.
Using entity features to classify implicit relations.
In Proc. of the 11th Annual SIGdial Meeting on Dis-
course and Dialogue, Tokyo, Japan.
D. Marcu and A. Echihabi. 2002. An unsupervised ap-
proach to recognizing discourse relations. In Proc.
of the 40th Annual Meeting on Association for Com-
putational Linguistics, p. 368?375. ACL.
A. Murakami and R. Raymond. 2010. Support or Op-
pose? Classifying Positions in Online Debates from
Reply Activities and Opinion Expressions. In Proc.
of the 23rd International Conference on Computa-
tional Linguistics: Posters, p. 869?875. ACL.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Informa-
tion Retrieval, 2(1-2):1?135.
J. W. Pennebaker, L. E. Francis, and R. J. Booth, 2001.
LIWC: Linguistic Inquiry and Word Count.
49
R. Perrault. 1990. An application of default logic to
speech-act theory. In Philip Cohen, Jerry Morgan,
and Martha Pollack, editors, Intentions in Commu-
nication, p. 161?187. MIT Press.
E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. In Proc. of the Joint Conference of the 47th An-
nual Meeting of the ACL: Vol. 2, p. 683?691. ACL.
M. Pollack, J. Hirschberg, and B. Webber. 1982. User
participation in the reasoning process of expert sys-
tems. In Proc. First National Conference on Artifi-
cial Intelligence, pages pp. 358?361.
R. Prasad, A. Joshi, and B. Webber. 2010. Exploiting
scope for shallow discourse parsing. In Language
Resources and Evaluation Conference.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proc. of the 2003
conference on Empirical methods in Natural Lan-
guage Processing-Vol.10, p. 105?112. ACL.
J. M. Sadock. 1977. Modus brevis: The truncated ar-
gument. In Papers from the 13th meeting of the CLS.
Chicago Linguistic Society.
S. Somasundaran and J. Wiebe. 2009. Recognizing
stances in online debates. In Proc. of the Joint Con-
ference of the 47th Annual Meeting of the ACL, p.
226?234. ACL.
S. Somasundaran and J. Wiebe. 2010. Recognizing
stances in ideological on-line debates. In Proc. of
the NAACL HLT 2010 Workshop on Computational
Approaches to Analysis and Generation of Emotion
in Text, p. 116?124. ACL.
R. C. Stalnaker. 1978. Assertion. In Peter Cole, editor,
Syntax and Semantics, Vol. 9: Pragmatics, p. 315?
332. Academic Press.
A. Stent. 2002. A conversation acts model for gen-
erating spoken dialogue contributions. Computer
Speech and Language: Special Issue on Spoken
Language Generation.
V. Stoyanov, C. Cardie, and J. Wiebe. 2005. Multi-
perspective question answering using the MPQA
corpus. In Proc. of the conference on Human Lan-
guage Technology and Empirical Methods in Nat-
ural Language Processing, HLT ?05, p. 923?930,
ACL.
M. Thelen and E. Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extrac-
tion pattern contexts. In Proc. of the ACL-02 con-
ference on Empirical methods in Natural Language
Processing. Vol. 10, p. 214?221. ACL.
M. Thomas, B. Pang, and L. Lee. 2006. Get out the
vote: Determining support or opposition from Con-
gressional floor-debate transcripts. In Proc. of the
2006 conference on empirical methods in natural
language processing, p. 327?335. ACL.
R. Thomason. 1990. Propagating epistemic coordi-
nation through mutual defaults i. In R. Parikh, ed-
itor, Proc. of the Third Conference on Theoretical
Aspects of Reasoning about Knowledge, p. 29?39.
D. Traum. 1994. A Computational Model of Ground-
ing in Natural Language Conversation. Ph.D. the-
sis, University of Rochester.
M. Walker, P. Anand, R. Abbott, and J. E. Fox Tree.
2012a. A corpus for research on deliberation and de-
bate. In Language Resources and Evaluation Con-
ference, LREC2012.
M.A. Walker, P. Anand, R. Abbott, and R. Grant.
2012b. Stance classification using dialogic proper-
ties of persuasion. In Meeting of the North American
Association for Computational Linguistics. NAACL-
HLT12.
M. A. Walker. 1996a. Inferring acceptance and rejec-
tion in dialogue by default rules of inference. Lan-
guage and Speech, 39-2:265?304.
W. Wang, S. Yaman, K. Precoda, C. Richey, and G.
Raymond. 2011. Detection of agreement and dis-
agreement in broadcast conversations. In The 49th
Annual Meeting of the Association for Computa-
tional Linguistics, p. 374?378. ACL.
B. Webber and R. Prasad. 2008. Sentence-initial dis-
course connectives, discourse structure and seman-
tics. In Proc. of the Workshop on Formal and Ex-
perimental Approaches to Discourse Particles and
Modal Adverbs, Hamburg, Germany.
J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis,
B. Fraser, D. Litman, D. Pierce, E. Riloff, T. Wil-
son, et al 2003. Recognizing and organizing
opinions expressed in the world press. In Working
Notes-New Directions in Question Answering (AAAI
Spring Symposium Series).
J. Wiley. 2005. A fair and balanced look at the news:
What affects memory for controversial arguments?
Journal of Memory and Language, 53(1):95?109.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Pat-
wardhan. 2005. Opinionfinder: A system for sub-
jectivity analysis. In Proc. of HLT/EMNLP on Inter-
active Demonstrations, p. 34?35. ACL.
J. Yin, P. Thomas, N. Narang, and C. Paris. 2012. Uni-
fying local and global agreement and disagreement
classification in online debates. In Proc. of the 3rd
Workshop in Computational Approaches to Subjec-
tivity and Sentiment Analysis, WASSA ?12, p. 61?
69, ACL.
I. Zukerman, R. McConachy, and K. Korb. 2000. Us-
ing argumentation strategies in automated argument
generation. In Proc. of the 1st International Natural
Language Generation Conference, p. 55?62.
50
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 109?117,
Baltimore, Maryland USA, 27 June 2014.
c?2014 Association for Computational Linguistics
Collective Stance Classification of Posts in Online Debate Forums
Dhanya Sridhar
Computer Science Dept.
UC Santa Cruz
dsridhar@soe.ucsc.edu
Lise Getoor
Computer Science Dept.
UC Santa Cruz
getoor@soe.ucsc.edu
Marilyn Walker
Computer Science Dept.
UC Santa Cruz
maw@soe.ucsc.edu
Abstract
Online debate sites are a large source of
informal and opinion-sharing dialogue on
current socio-political issues. Inferring
users? stance (PRO or CON) towards dis-
cussion topics in domains such as politics
or news is an important problem, and is
of utility to researchers, government or-
ganizations, and companies. Predicting
users? stance supports identification of so-
cial and political groups, building of better
recommender systems, and personaliza-
tion of users? information preferences to
their ideological beliefs. In this paper, we
develop a novel collective classification
approach to stance classification, which
makes use of both structural and linguis-
tic features, and which collectively labels
the posts? stance across a network of the
users? posts. We identify both linguistic
features of the posts and features that cap-
ture the underlying relationships between
posts and users. We use probabilistic soft
logic (PSL) (Bach et al., 2013) to model
post stance by leveraging both these local
linguistic features as well as the observed
network structure of the posts to reason
over the dataset. We evaluate our approach
on 4FORUMS (Walker et al., 2012b), a col-
lection of discussions from an online de-
bate site on issues ranging from gun con-
trol to gay marriage. We show that our col-
lective classification model is able to eas-
ily incorporate rich, relational information
and outperforms a local model which uses
only linguistic information.
1 Introduction
Modeling user stance (PRO, CON) in discussion
topics in online social media debate is of inter-
est to researchers, corporations and governmental
organizations alike. Predicting a user?s stance to-
wards a given issue can support the identification
of social or political groups (Gawron et al., 2012;
Abu-Jbara et al., 2012; Anand et al., 2011; Qiu et
al., 2013; Hasan and Ng, 2013), help develop bet-
ter recommendation systems, or tailor users? infor-
mation preferences to their ideologies and beliefs.
Stance classification problems consist of a collec-
tion of debate-style discussions by authors on dif-
ferent controversial, political topics.
While these may be spoken as in the Congres-
sional Debates corpus (Thomas et al., 2006; Bur-
foot, 2008), we focus on forum posts on social
media debate sites. Users on debate sites share
their opinions freely, using informal and social
language, providing a rich and much more chal-
lenging domain for stance prediction.
Social media debate sites contain online discus-
sions with posts from various authors, where each
post is either a response to another post or the root
of the discussion (Anand et al., 2011; Walker et
al., 2012a). Posts are linked to one another by ei-
ther rebuttal or agreement links and are labelled
for stance, either PRO or CON, depending on the
framing of the issue under discussion. Each post
reflects the stance and sentiment of its author. Au-
thors may participate in multiple discussions in the
same topic, and may discuss multiple topics. For
example consider the sample posts from the online
discussion forum 4forums.com shown in Fig.
1. Here, we see discussion topics, together with
sample quotes and responses, where the response
is a direct reply to the quote text. The annotations
for stance were gathered using Amazon?s Mechan-
ical Turk service with an interface that allowed an-
notators to see complete discussions. Quotes pro-
vide additional context that were used by human
annotators in a separate task for annotating agree-
ment and disagreement (Misra and Walker, 2013).
Responses can be labeled as either PRO or CON to-
ward the topic. For the example shown in Fig. 1,
109
Quote Q, Response R Stance Topic
Q: I thought I?d start a new thread for those newcomers who don?t want to be shocked by sick
minded nazi XXXX. Anyway... When are fetuses really alive, and how many fetuses are actually
aborted (murdered) before that time?
R: The heart starts beating 3 weeks after conception, and you can?t live without a beating heart,
but me personally, I think that as soon as the miracle starts, (egg and sperm combine) that is
when life begins. I know it?s more of a spiritual thing for me instead of a fact. :)
CON Abortion
Q2: Most americans support a Federal Marriage Amendment. Defining Marriage as a union
between a man and a woman. Federal Marriage Amendment. This is the text of the Amend:
Marriage in the United States shall consist only of the union of a man and a woman. Neither
this constitution or the constitution of any state, nor state or federal law, shall be construed to
require that marital status or the legal incidents thereof be conferred upon unmarried couples or
groups.
R2: Debator, why does it bother you so much that some people are gay? Its a sexual prefference.
People like certain things when they have sex. Example: A man likes a women with small boobs.
Or, a man likes a women with nice legs. People like the way certain things feel (I?m not giving
te example for that one;) ). So why does it bother people that someone?s sexual prefference is
just a little kinkier than thiers?
PRO Gay
Mar-
riage
Figure 1: Sample Quote/Response Pair from 4forums.com with Mechanical Turk annotations for stance.
Both response posts are from the same author.
both response posts are from the same author. We
describe the dataset further in Section 4.1.
We believe that models of post stance in on-
line debate should capture both the content and the
context of author posts. By jointly reasoning over
both the content of the post and its relationships
with other posts in the discussion, we perform col-
lective classification, as we further define in Sec-
tion 3 (Sen et al., 2008). Previous work has shown
that collective classification models often perform
better than content-only approaches. (Burfoot et
al., 2011; Hasan and Ng, 2013; Thomas et al.,
2006; Bansal et al., 2008; Walker et al., 2012c).
Here, we develop a collective classification ap-
proach for stance prediction which leverages the
sentiment conveyed in a post through its language,
and the reply links consisting of agreements or re-
buttals between posts in a discussion. We imple-
ment our approach using Probabilistic Soft Logic
(PSL) (Bach et al., 2013), a recently introduced
tool for collective inference in relational data. We
evaluate our model on data from the 4FORUMS
online debate site (Walker et al., 2012b).
Section 2 first presents an overview of our ap-
proach and then in Section 3.1 we describe the
PSL framework in more detail. Section 4 de-
scribes the evaluation data and our results show-
ing that the PSL model improves prediction of post
stance in the 4Forums dataset. In Section 5 we
describe related work, and compare with our pro-
posed approach. Section 6 summarizes our ap-
proach and results.
2 Overview of Approach
Given a set of topics {t
1
. . . t
n
}, where each topic
t
i
consists of a set of discussions {d
i1
. . . d
ij
}, we
model each discussion d
k
as a collection of posts
{p
k0
, . . . , p
km
}, where each post p
ki
is mapped to
its author a
i
.
A discussion d
i
? D is a tree of posts, starting
with the initial post p
i0
. We distinguish between
posts that start a new thread (root) and others (non-
root). Each non-root post p
ij
is the response to
some previous post p
ik
, where k < j, and we refer
to p
ik
as the parent of p
ij
. For a subset of the posts,
p
ij
has been annotated with a real valued number
in the interval [?5, 5] that denotes whether the post
disagrees or agrees with its parent. Values ? 0 are
considered disagreement and values? 1, as agree-
ment. We discard the posts where the annotations
are in the interval (0, 1) since those indicate high
annotator uncertainty about agreement.
Fig. 2 illustrates an example of three discussion
trees for two topics where author a
2
participates
in multiple discussions of the same topic and a
3
and a
4
participate in multiple topics. An author
directly replies with a post to another author?s post
and either disagrees or agrees.
Each post p
ij
in discussion d
i
is also mapped to
{x
ij
1
, . . . , x
ij
N
} linguistic features as described in
Section 3.2.1 as well as y
ij
, the stance label (PRO,
CON) towards the discussion topic t
i
.
We say that a
j
participates in topic t
i
if there
exist any posts p
j
? d
i
with author a
j
.
Using the tree structure and posts that have an-
notations for agreement or disagreement, we con-
110
Figure 2: Example of 3 discussions in (a), (b) and (c). Dotted lines denote the ?writes? relation between
authors and posts and dashed lines denote the ?disagrees? relation between posts and between authors.
Authors can participate in multiple discussions of the same topic, shown by a
2
in both (a) and (b).
Moreover, authors may post in multiple topics, as shown by a
3
and a
4
in both (b) and (c), and may
interact with the same authors multiple times, as shown again in (b) and (c).
sider the network graph G of disagreement and
agreement between posts and between authors,
where the vertices are posts {p
0
, . . . , p
m
} and au-
thors {a
0
, . . . , a
n
}. A disagreement edge exists
from post p
u
to p
v
if p
u
disagrees with p
v
.
A disagreement edge exists from a
w
to a
y
if any
of the posts {p
w
, . . . , p
x
} mapped to a
w
disagree
with any posts {p
y
, . . . p
z
}mapped to a
y
. We sim-
ilarly define agreement edges for both posts and
authors.
3 Collective Classification of Stance
Given the discussion structure defined in the pre-
vious section, our task is to infer the stance of each
post. We make use of both linguistic features and
the relational structure in order to collectively or
jointly infer the stance labels. This corresponds to
a collective classification setting (Sen et al., 2008),
in which we are given a multi-relational network
and some partially observed labels, and we wish
to infer all of the unobserved labels, conditioned
on observed attributes and links. Collective clas-
sification refers to the combined classification of
a set of interdependent objects (posts, in our do-
main) using information given by both the local
features of the objects and the properties of the
objects? neighbors in a network. For the stance
classification problem, we infer stance labels for
posts using both the correlation between a post and
its linguistic attributes {x
ij
1
, . . . , x
ij
N
}, and the
labels and attributes of its neighbors in observed
network graph G. We use PSL, described below,
to perform collective classification.
3.1 Probabilistic Soft Logic
Probabilistic soft logic (PSL) is a framework for
probabilistic modeling and collective reasoning in
relational domains (Kimmig et al., 2012; Bach et
al., 2013). PSL provides a declarative syntax and
uses first-order logic to define a templated undi-
rected graphical model over continuous random
variables. Like other statistical relational learn-
ing methods, dependencies in the domain are cap-
tured by constructing rules with weights that can
be learned from data.
But unlike other statistical relational learning
methods, PSL relaxes boolean truth values for
atoms in the domain to soft truth values in the in-
terval [0,1]. In this setting, finding the most proba-
ble explanation (MPE), a joint assignment of truth
values to all random variable ground atoms, can be
done efficiently.
For example, a typical PSL rule looks like the
following:
P (A,B) ?Q(B,C)? R(A,C)
where P, Q and R are predicates that represent
observed or unobserved attributes in the domain,
and A, B, and C are variables. For example, in
our 4FORUMS domain, we consider an observed
attribute such as writesPost(A, P) and infer an un-
observed attribute (or label) such as isProPost(P,
T). Instantiation of predicates with data is called
grounding (e.g. writesPost(A2, P7)), and each
ground predicate, often called ground atom, has a
soft truth value in the interval [0,1]. To build a PSL
model for stance classification, we represent posts
111
isProPost(P, T) ? writesPost(A, P) ? isProAuth(A, T)
? isProPost(P, T) ? writesPost(A, P) ? ? isProAuth(A, T)
agreesPost(P, P2) ? isProPost(P, T) ? isProPost(P2, T)
agreesPost(P, P2) ?? isProPost(P, T) ? ? isProPost(P2, T)
disagreesPost(P, P2) ? isProPost(P, T) ? ? isProPost(P2, T)
disagreesPost(P, P2) ?? isProPost(P, T) ? isProPost(P2, T)
agreesAuth(A, A2) ? isProAuth(A, T) ? isProAuth(A, T)
agreesAuth(A, A2) ?? isProAuth(A, T) ? ? isProAuth(A2, T)
disagreesAuth(A, A2) ? isProAuth(A, T) ? ? isProAuth(A2, T)
disagreesAuth(A, A2) ?? isProAuth(A, T) ? isProAuth(A2, T)
hasLabelPro(P, T) ? isProPost(P, T)
? hasLabelPro(P, T) ? ? isProPost(P, T)
Table 1: Rules for PSL model, where P = post, T = Topic, and A = Author.
and authors as variables and specify predicates to
encode different interactions, such as writes, be-
tween them. Domain knowledge is captured by
writing rules with weights that govern the rela-
tive importance of the dependencies between pred-
icates. The groundings of all the rules result in
an undirected graphical model that represents the
joint probability distribution of assignments for all
unobserved atoms, conditioned on the observed
atoms.
Triangular norms, which are continuous relax-
ations of logical AND and OR, are used to com-
bine the atoms in the first-order clauses. As a
result of the soft truth values and the triangu-
lar norms, the underlying probabilistic model is
a hinge-loss Markov Random Field (HL-MRF).
Inference in HL-MRFs is a convex optimization,
which leads to a significant improvement in effi-
ciency over discrete probabilistic graphical mod-
els. Thus, PSL offers a very natural interface to
compactly represent stance classification as a col-
lective classification problem, along with methods
to reason about our domain.
3.2 Features
We extract both linguistic features that capture the
content of a post and features that capture multiple
relations from our dataset.
3.2.1 Linguistic Features
To capture the content of a post, on top of a bag-of-
words representation with unigrams and bigrams,
we also consider basic lengths, discourse cues,
repeated punctuation counts and counts of lex-
ical categories based on the Linguistic Inquiry
and Word Count tool (LIWC) (Pennebaker et al.,
2001). Basic length features capture the number
of sentences, words, and characters, along with
the average word and sentence lengths for each
post. The discourse cues feature captures fre-
quency counts for the first few words of the post,
which often contain discourse cues. To capture
the information in repeated punctuation like ?!!?,
???? or ??!? we include the frequency count of the
given punctuation patterns as a feature of each post
(Anand et al., 2011). LIWC counts capture senti-
ment by giving the degree to which the post uses
certain categories of subjective language.
3.2.2 Relational Information
As our problem domain contains relations be-
tween both authors and posts, for our PSL model,
we consider the relations between authors, be-
tween posts and between authors and posts. As de-
scribed above, in PSL, we model these relations as
first-order predicates. In Section 3.3, we describe
how we populate the predicates with observations
from our data.
Author Information We observe that authors
participate in discussions by writing posts. For
a subset of authors, we have annotations for their
interactions with other authors as either disagree-
ment or agreement, as given by network graph
G. We encode this with the following predi-
cates: writesPost(A, P), disagreesAuth(A1, A2),
agreesAuth(A1, A2).
Post Information Posts are linked to the topic
of their given discussion, and to other posts in
their discussion through disagreement or agree-
ment. Additionally, we include a predicate for post
stance towards its topic as predicted by a classifier
112
that only uses linguistic features, as described in
Section 3.3, as prior information. We capture these
relations with the following predicates: hasLabel-
Pro(P, T), hasTopic(P, T), disagreesPost(P1, P2),
agreesPost(P1, P2).
3.2.3 Target attributes
Our goal is to 1) predict the stance relation be-
tween a post and its topic, namely, PRO or CON and
2) predict the stance relation between an author
and a topic. In our PSL model, our target predi-
cates are isProPost(P, T) and isProAuth(A, T).
3.3 PSL Model
We construct our collective stance classification
model in PSL using the predicates listed above.
For disagreement/agreement annotations in the in-
terval [-5, 5], we consider values [-5,0] as evidence
for the disagreesAuth relation and values [1, 5] as
evidence for the agreesAuth relation. We discard
observations with annotations in the interval [0,1]
because it indicates a very weak signal of agree-
ment, which is already rare on debate sites. We
populate disagreesPost and agreesPost in the same
way as described above.
For each relation, we populate the correspond-
ing predicate with all the instances that we observe
in data and we fix the truth value of each observa-
tion as 1. For all such predicates where we observe
instances in the data, we say that the predicate is
closed. For the relations isPostPro and isAuthPro
that we predict through inference, a truth value of
1 denotes a PRO stance and a truth value of 0 de-
notes a CON stance. We say that those predicates
are open, and the goal of inference is to jointly as-
sign truth values to groundings of those predicates.
We use our domain knowledge to describe rules
that relate these predicates to one another. We fol-
low our intuition that agreement between nodes
implies that they have the same stance, and dis-
agreement between nodes implies that they have
opposite stances. We relate post and author nodes
to each other by supposing that if a post is PRO
towards its topic, then its author will also be PRO
towards that topic.
We construct a classifier that takes as input the
linguistic features of the posts and outputs predic-
tions for stance label of each post. We then con-
sider the labels predicted by the local classifier as
a prior for the inference of the target attributes in
our PSL model. Table 1 shows the rules in our
PSL model.
Topic Authors Posts
Abortion 385 8114
Evolution 325 6186
Gun Control 319 3899
Gay Marriage 316 7025
Death Penalty 170 572
Table 2: Overview of topics in 4FORUMSdataset.
4 Experimental Evaluation
We first describe the dataset we use for evaluation
and then describe our evaluation method and re-
sults.
4.1 Dataset
We evaluate our proposed approach on discus-
sions from https://www.4forums.com, an
online debate site on social and political issues.
The dataset is publicly available as part of the
Internet Argument Corpus, an annotated collec-
tion of 109,533 forum posts (Walker et al., 2012b;
Walker et al., 2012c). On 4FORUMS, a user ini-
tiates a discussion by posting a new question or
comment under a topic, or participate in an ongo-
ing discussion by replying to any of the posts in
the thread. The discussions were given to English
speaking Mechanical Turk annotators for a num-
ber of annotation tasks to get labels for the stances
of discussion participants towards the topic, and
scores for each post in a discussion indicating
whether it is in agreement or disagreement with
the preceding post.
The scores for agreement and disagreement
were on a 11 point scale [-5, 5] implemented using
a slider, and annotators were given quote/response
pairs to determine if the response text agreed
or disagreed with the quote text. We use the
mean score across the 5-7 annotators used in the
task. A more negative value indicates higher
inter-annotator confidence of disagreement, and a
more positive value indicates higher confidence of
agreement. The gold-standard annotation used for
the stance label of each post is given by the ma-
jority annotation among 3-8 Mechanical Turk an-
notators performed as a separate task, using en-
tire discussions to determine the stance of the au-
thors in the discussion towards the topic. We use
the stance of each post?s author to determine the
post?s stance. For our experiments, we use all
posts with annotations for stance, and about 90%
of these posts also have annotations for agree-
113
ment/disagreement.
The discussions span many topics, and Table 2
gives a summary of the topics we consider in our
experiments and the distribution of posts across
these topics. Each post in a discussion comes as
a quote-response pair, where the quote is the text
that the post is in response to, and the response is
the post text. We refer to (Walker et al., 2012b) for
a full description of the corpus and the annotation
process.
4.2 Evaluation
In order to evaluate our methods, we split the
dataset into training and testing sets by randomly
selecting half the authors from each topic and their
posts for the training set and using the remaining
authors and their posts for the test set. This way,
we ensure that no two authors appear in both train-
ing and test sets for the same topic, since stance
is topic-dependent. We create 10 randomly sam-
pled train/test splits for evaluation. Each split con-
tains about 18,000 posts. For each train/test split,
we train a linear SVM for each topic, with the
L2-regularized-L1-loss SVM implemented in the
LibLINEAR package (Fan et al., 2008). We use
only the linguistic features from the posts, for each
topic in the training set. We refer to the baseline
model which only uses the the output of the SVM
as the LOCAL model. We output the predictions
from LOCAL model and get stance labels for posts
in both the training and test sets. We use the pre-
dictions as prior information for the true stance la-
bel in our PSL model, with the hasLabel predicate.
We use the gold standard stance annotation
(PRO, CON) for each post as ground truth for
weight learning and inference. A truth value of 1
for isPostPro and isAuthPro denotes a PRO stance
and a truth value of 0 denotes a CON stance. We
learn the weights of our PSL model (initially set to
1) for each of our training sets and perform infer-
ence on each of the test sets.
Table 3 shows averages for F1 score for the pos-
itive class (PRO), area under the precision-recall
curve (AUC-PR) for the negative class (CON) and
area under the ROC curve (AUROC) over the 10
train/test splits. For the PSL model, the measures
are computed for joint inference over all topics
in the test sets. For the per-topic linear SVMs
(LOCAL model), we compute the measures indi-
vidually for the predictions of each topic in the
test sets and take a weighted average over the
topics. Our PSL model outperforms the LOCAL
model, with statistically significant improvements
in the F1 score and AUC-PR for the negative class.
Moreover, our model completes weight learning
and inference on the order of seconds, boasting an
advantage in computational efficiency, while also
maintaining model interpretability.
Table 4 shows the weights learned by the PSL
model for the rules in one of the train/test splits
of the experiment. The first two rules relating
post stance and author stance are weighted more
heavily, in part because the writesPost predicate
has a grounding for each author-post pair and con-
tributes to lots of groundings of the rule. The rules
that capture the alternating disagreement stance
also have significant weight, while the rules denot-
ing agreement both between posts and between au-
thors are weighted least heavily since there are far
fewer instances of agreement than disagreement.
This matches our intuition of political debates.
We also explored variations of the PSL model
by removing the first two rules relating post stance
and author stance and found that the weight learn-
ing algorithm drove the weights of the other
rules close to 0, worsening the performance.
We also removed rules 3-10 that capture agree-
ment/disagreement from the model, and found that
the model performs poorly when disregarding the
links between nodes entirely. The PSL model
learns to weight the first and second rule very
highly, and does worse than when considering the
prior alone. Thus, the combination of the rules
gives the model its advantage, allowing the PSL
model to make use of a richer structure that has
multiple types of relations and more information.
5 Related Work
Over the last ten years, there has been significant
progress on modeling stance. Previous work cov-
ers three different debate settings: (1) congres-
sional debates (Thomas et al., 2006; Bansal et
al., 2008; Yessenalina et al., 2010; Balahur et al.,
2009); (2) company-internal discussion sites (Mu-
rakami and Raymond, 2010; Agrawal et al., 2003);
and (3) online social and political public forums
(Somasundaran and Wiebe, 2009; Somasundaran
and Wiebe, 2010; Wang and Ros?e, 2010; Biran
and Rambow, 2011; Walker et al., 2012c; Anand
et al., 2011). Debates in online public forums
(e.g. Fig. 1) differ from debates in congress and
on company discussion sites because the posts are
114
Classifier F1 Score AUC-PR negative class AUROC
LOCAL 0.66 ? 0.015 0.44 ? 0.04 0.54 ? 0.02
PSL 0.74 ? 0.04 0.511 ? 0.04 0.59 ? 0.05
Table 3: Averages and standard deviations for F1 score for the positive class, area under PR curve for the
negative class, and area under ROC curve for post stance over 10 train/test splits.
isProPost(P, T) ? writesPost(A, P) ? isProAuth(A, T) : 10.2
? isProPost(P, T) ? writesPost(A, P) ? ? isProAuth(A, T) : 8.5
agreesPost(P, P2) ? isProPost(P, T) ? isProPost(P2, T) : 0.003
agreesPost(P, P2) ?? isProPost(P, T) ? ? isProPost(P2, T) : 0.003
disagreesPost(P, P2) ? isProPost(P, T) ? ? isProPost(P2, T) : 0.06
disagreesPost(P, P2) ?? isProPost(P, T) ? isProPost(P2, T) : 0.11
agreesAuth(A, A2) ? isProAuth(A, T) ? isProAuth(A, T) : 0.001
agreesAuth(A, A2) ?? isProAuth(A, T) ? ? isProAuth(A2, T) : 0.0
disagreesAuth(A, A2) ? isProAuth(A, T) ? ? isProAuth(A2, T) : 0.23
disagreesAuth(A, A2) ?? isProAuth(A, T) ? isProAuth(A2, T) : 0.6
hasLabelPro(P, T) ? isProPost(P, T) : 2.2
? hasLabelPro(P, T) ? ? isProPost(P, T) : 4.8
Table 4: Weights learned by the model for the PSL rules in train/test split 2 of experiments
shorter and the language is more informal and so-
cial. We predict that this difference makes it more
difficult to achieve accuracies as high for 4FO-
RUMS discussions as can be achieved for the con-
gressional debates corpus.
Work by (Somasundaran and Wiebe, 2009) on
idealogical debates very similar to our own show
that identifying argumentation structure improves
performance; their best performance is approxi-
mately 64% accuracy over all topics. Research by
(Thomas et al., 2006; Bansal et al., 2008; Yesse-
nalina et al., 2010; Balahur et al., 2009) classifies
the speaker?s stance in a corpus of congressional
floor debates. This work combines graph-based
and text-classification approaches to achieve 75%
accuracy on Congressional debate siding over all
topics. Other work applies MaxCut to the re-
ply structure of company discussion forums (Mal-
ouf and Mullen, 2008; Murakami and Raymond,
2010; Agrawal et al., 2003). Murakami & Ray-
mond (2010) show that rules for identifying agree-
ment, defined on the textual content of the post
improve performance.
More recent work has explicitly focused on the
benefits of collective classification in these set-
tings (Burfoot et al., 2011; Hasan and Ng, 2013;
Walker et al., 2012c), and has shown in each
case that collective classification improves perfor-
mance. The results reported here are the first to
apply the PSL collective classification framework
to the forums conversations from the IAC corpus
(Anand et al., 2011; Walker et al., 2012c).
6 Discussion and Future Work
Here, we introduce a novel approach to classify
stance of posts from online debate forums with a
collective classification framework. We formally
construct a model, using PSL, to capture relational
information in the network of authors and posts
and our intuition that agreement or disagreement
between users correlates to their stance towards a
topic. Our initial results are promising, showing
that by incorporating more complex interactions
between authors and posts, we gain improvements
over a content-only approach. Our approach is
ideally suited to collective inference in social me-
dia. It easily extendable to use additional rela-
tional information, and richer behavioral and lin-
guistic information.
Acknowledgments
Thanks to Pranav Anand for providing us with the
stance annotations for the 4forums dataset. This
work is supported by National Science Foundation
under Grant Nos. IIS1218488, CCF0937094 and
CISE-RI 1302668.
115
References
Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi, and
Dragomir Radev. 2012. Subgroup detection in ide-
ological discussions. In Association for Computa-
tional Linguistics (ACL), pages 399?409.
R. Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu.
2003. Mining newsgroups using networks arising
from social behavior. In International Conference
on World Wide Web (WWW), pages 529?535. ACM.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.
Fox Tree, Robeson Bowmani, and Michael Minor.
2011. Cats Rule and Dogs Drool: Classifying
Stance in Online Debate. In ACL Workshop on Sen-
timent and Subjectivity.
Stephen H. Bach, Bert Huang, Ben London, and Lise
Getoor. 2013. Hinge-loss markov random fields:
Convex inference for structured prediction. In Un-
certainty in Artificial Intelligence (UAI).
A. Balahur, Z. Kozareva, and A. Montoyo. 2009. De-
termining the polarity and source of opinions ex-
pressed in political debates. Computational Linguis-
tics and Intelligent Text Processing, pages 468?480.
M. Bansal, C. Cardie, and L. Lee. 2008. The power
of negative thinking: Exploiting label disagreement
in the min-cut classification framework. COLING,
pages 13?16.
O. Biran and O. Rambow. 2011. Identifying justifi-
cations in written dialogs. In IEEE International
Conference on Semantic Computing (ICSC), pages
162?168.
Clinton Burfoot, Steven Bird, and Timothy Baldwin.
2011. Collective classification of congressional
floor-debate transcripts. In Association for Compu-
tational Linguistics (ACL), pages 1506?1515.
C. Burfoot. 2008. Using multiple sources of agree-
ment information for sentiment classification of po-
litical transcripts. In Australasian Language Tech-
nology Association Workshop, volume 6, pages 11?
18.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
J.M. Gawron, D. Gupta, K. Stephens, M.H. Tsou,
B. Spitzberg, and L. An. 2012. Using group mem-
bership markers for group identification in web logs.
In AAAI Conference on Weblogs and Social Media
(ICWSM).
Kazi Saidul Hasan and Vincent Ng. 2013. Stance clas-
sification of ideological debates: Data, models, fea-
tures, and constraints. International Joint Confer-
ence on Natural Language Processing.
Angelika Kimmig, Stephen H. Bach, Matthias
Broecheler, Bert Huang, and Lise Getoor. 2012.
A short introduction to probabilistic soft logic.
In NIPS Workshop on Probabilistic Programming:
Foundations and Applications.
R. Malouf and T. Mullen. 2008. Taking sides: User
classification for informal online political discourse.
Internet Research, 18(2):177?190.
Amita Misra and Marilyn A Walker. 2013. Topic in-
dependent identification of agreement and disagree-
ment in social media dialogue. In Conference of the
Special Interest Group on Discourse and Dialogue,
page 920.
A. Murakami and R. Raymond. 2010. Support or Op-
pose? Classifying Positions in Online Debates from
Reply Activities and Opinion Expressions. In Inter-
national Conference on Computational Linguistics
(ACL), pages 869?875.
J. W. Pennebaker, L. E. Francis, and R. J. Booth, 2001.
LIWC: Linguistic Inquiry and Word Count.
Minghui Qiu, Liu Yang, and Jing Jiang. 2013. Mod-
eling interaction features for debate side clustering.
In ACM International Conference on Information &
Knowledge Management (CIKM), pages 873?878.
Prithviraj Sen, Galileo Mark Namata, Mustafa Bilgic,
Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad.
2008. Collective classification in network data. AI
Magazine, 29(3):93?106.
S. Somasundaran and J. Wiebe. 2009. Recogniz-
ing stances in online debates. In ACL and AFNLP,
pages 226?234.
S. Somasundaran and J. Wiebe. 2010. Recognizing
stances in ideological on-line debates. In NAACL
HLT 2010 Workshop on Computational Approaches
to Analysis and Generation of Emotion in Text,
pages 116?124.
M. Thomas, B. Pang, and L. Lee. 2006. Get out the
vote: Determining support or opposition from Con-
gressional floor-debate transcripts. In Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 327?335.
Marilyn Walker, Pranav Anand, Rob Abbott, Jean E.
Fox Tree, Craig Martell, and Joseph King. 2012a.
That?s your evidence?: Classifying stance in online
political debate. Decision Support Sciences.
Marilyn Walker, Pranav Anand, Robert Abbott, and
Jean E. Fox Tree. 2012b. A corpus for research
on deliberation and debate. In Language Resources
and Evaluation Conference, LREC2012.
Marilyn Walker, Pranav Anand, Robert Abbott, and
Richard Grant. 2012c. Stance classification using
dialogic properties of persuasion. In Meeting of the
North American Association for Computational Lin-
guistics. NAACL-HLT12.
116
Y.C. Wang and C.P. Ros?e. 2010. Making conversa-
tional structure explicit: identification of initiation-
response pairs within online discussions. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 673?676.
A. Yessenalina, Y. Yue, and C. Cardie. 2010.
Multi-level structured models for document-level
sentiment classification. In Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1046?1056.
117
Proceedings of the SIGDIAL 2014 Conference, pages 171?180,
Philadelphia, U.S.A., 18-20 June 2014.
c
?2014 Association for Computational Linguistics
Identifying Narrative Clause Types in Personal Stories
Reid Swanson, Elahe Rahimtoroghi, Thomas Corcoran and Marilyn A. Walker
Natural Language and Dialog Systems Lab
University of California Santa Cruz
Santa Cruz, CA 95064, USA
{reid,elahe,maw}@soe.ucsc.edu, tcorcora@ucsc.edu
Abstract
This paper describes work on automatically
identifying categories of narrative clauses
in personal stories written by ordinary peo-
ple about their daily lives and experiences.
We base our approach on Labov & Walet-
zky?s theory of oral narrative which catego-
rizes narrative clauses into subtypes, such as
ORIENTATION, ACTION and EVALUATION.
We describe an experiment where we an-
notate 50 personal narratives from weblogs
and experiment with methods for achieving
higher annotation reliability. We use the re-
sulting annotated corpus to train a classi-
fier to automatically identify narrative cat-
egories, achieving a best average F-score of
.658, which rises to an F-score of .767 on
the cases with the highest annotator agree-
ment. We believe the identified narrative
structure will enable new types of compu-
tational analysis of narrative discourse.
1 Introduction
Sharing personal experiences by storytelling is
a fundamental aspect of human social behavior
(Fivush et al., 2005; Fivush and Nelson, 2004;
Habermas and Bluck, 2000; Bamberg, 2006;
Thorne, 2004; Bohanek et al., 2008; Thorne and
Nam, 2009; McLean and Thorne, 2003; Pratt and
Fiese, 2004). Humans appear to be wired to en-
gage with information that is narratively structured
(Gerrig, 1993; Bamberg, 2006; Bruner, 1991), and
telling stories provides a critical developmental and
societal function, by serving as a means to reinforce
community value systems and to define individual
identity (Thorne and Shapiro, 2011; Thorne et al.,
2007). This has led some theorists to claim that ?the
stories they tell? is the defining aspect of both indi-
viduals and cultures.
Unlike any prior time in human history, personal
narratives about many life experiences are being
told online, and are widely available in social me-
dia sources such as weblogs. A personal narrative
about an arrest is shown in Fig. 1, and one about a
protest is in Fig. 4. Narratives such as these provide
a valuable resource for learning a wealth of com-
monsense knowledge about people, the types of ac-
tivities they engage in, and the attitudes they hold.
They are also well suited to learning about causal
and temporal relationships between events because
narrative interpretation explicitly depends on the co-
herence of these relationships (Graesser et al., 1994;
Elson, 2012; Gordon et al., 2011; Hu et al., 2013).
This paper applies and tests a narrative clause la-
beling scheme to personal narratives. Our scheme
is derived from Labov & Waletzky?s (henceforth
L&W) theory of oral narrative (Labov, 1997; Labov
and Waletzky, 1967). L&W?s theory distinguishes
(1) clauses that indicate causal relationships (AC-
TION), from (2) clauses that provide traits or prop-
erties of the setting or characters (ORIENTATION),
from (3) clauses describing the story characters?
emotional reactions to the events (EVALUATION).
We adopt L&W?s theory for three reasons. First,
we believe that the narrative structure of personal
narratives posted on weblogs will be more similar
to oral narrative than they are to classical stories.
Second, we believe that any narrative discourse ty-
pology must at least distinguish ACTION, from ORI-
ENTATION, and EVALUATION. Third, personal sto-
ries found on the web are often noisy and diffi-
cult to interpret; they do not always clearly follow
well defined narrative conventions. A deep analysis
171
# Category Story Clause
1 Orientation
Now, on with this week?s story...
2 Orientation
The last month has been hectic.
3 Orientation
Turbo charged.
4 Orientation
Lot?s of work because I was learning from Tim, my partner in crime.
5 Orientation
This hasn?t been helped by the intense pressure in town due to the political transition coming to an end.
6 Orientation
This week things started alright and on schedule.
7 Action
But I managed to get myself arrested by the traffic police (rouleage) early last Wednesday.
8 Action
After yelling excessively at their outright corrupted methods
9 Action
and asking incessently for what law I actually broke,
10 Action
they managed to bring me in at the police HQ.
11 Action
I was drawing too much of a curious crowd for the authorities.
12 Action
In about half an hour at police HQ I had charmed every one around.
13 Action
I had prepared my ?gift? as they wished.
14 Evaluation
Decision witheld, they decided that I neednt to bother,
15 Evaluation
they liked me too much.
16 Evaluation
I should go free.
17 Action
I even managed to meet famous Raus, the big chief.
18 Evaluation
He was too happy to let me go when he realized I was no one.
19 Action
But then, a Major at his side noticed my Visa was expired.
20 Evaluation
Damn!
21 Orientation
My current Visa is being renewed in my other passport at Immigration?s.
22 Evaluation
Fuck.
23 Evaluation
In custody, for real.
Figure 1: An excerpt from an example story from our corpus annotated with the L&W categories.
and annotation scheme, such as the one employed
by DramaBank (Elson and McKeown, 2010; Elson,
2012) that extends theories of narrative structure and
plot units (Stein et al., 2000; Lehnert, 1981), offers
many advantages. However, acquiring this level of
analysis on user generated content, such as blog sto-
ries, is resource intensive.
Research on computational models of narrative
structure typically focus on inferring the causal and
temporal relationships between events (Goyal et al.,
2010; Chambers and Jurafsky, 2009; Riaz and Girju,
2010; Beamer and Girju, 2009; Do et al., 2011;
Manshadi et al., 2008; Gordon et al., 2011; Hu et al.,
2013). Yet L&W point out that stories are not just
about the events that occur. In fact, L&W say that
stories that are only about events are boring. Current
methods for inferring narrative structure, including
our own (Hu et al., 2013), do not distinguish event
clauses from other narrative clause types. But note
that actions only constitute about one third of the
clauses in the narratives in Fig. 1 and Fig. 4.
Sec. 2 provides more detail about L&W?s the-
ory. Sec. 3 describes the annotation experiments
and efforts to improve annotation reliability. Sec. 4
presents experiments on learning to automatically
classify L&W categories, where we examine the the
most predictive features, and the effect of annotator
agreement on classification accuracy. We achieve a
best average F-score of .658, which rises to an F-
score of .767 on the cases with the highest annotator
agreement. We analyze the types of errors the clas-
sifier makes in Sec. 5.1 and conclude in Sec. 6.
2 Labov & Waletzky?s Theory of
Narrative Discourse
L&W?s theory of oral narrative defines a story as
a series of ACTION clauses (events), of which at
least two must be temporally joined (e.g., clauses
7-13 in Fig. 1 and clauses 7-11 in Fig. 4) (Labov
and Waletzky, 1967; Labov, 1997) Stories also con-
tain ORIENTATIONS (setting the scene, describing
the characters), e.g. utterances 1-6 in Fig. 1. An
orientation clause introduces the time and place of
the events of the story, and identifies the participants
of the story and their initial behavior. To properly
understand narrative structure, orientations need to
be identified as a separate type of utterance distinct
from events. L&W define two other structural types
called ABSTRACT and CODA. The ABSTRACT is an
initial narrative clause summarizing the entire se-
quence of events. A CODA is final clause which
returns the narrative to the time of speaking, indi-
cating the end of the narrative. The CODA often pro-
vides the ?moral? of the story.
The final element of a story according to L&W
is EVALUATION, which L&W identify as essen-
tial to every story. According to L&W, evaluation
gives the reason for telling the story, or the point of
the story: without EVALUATION there is no story,
merely a boring recitation of events. L&W state
that the EVALUATION clauses may also provide in-
formation on the consequences of the events as they
172
relate to the goals and desires of the participants,
and can be used to describe the events that did not
occur, may have occurred, or could occur in the fu-
ture in the story. Clauses 14-16 and 18 in Fig. 1
provide the narrator?s evaluation of the transpiring
events as well as introducing possible but unreal-
ized alternative timelines. In theories of narrative
identity (McAdams, 2003; Thorne, 2004), evalua-
tion performs two additional functions: (1) it ex-
presses the tellers opinion and in doing so reflects
the value system of that person and their commu-
nity; (2) it constructs and maintains relations be-
tween the teller and the listener. Clauses 20 and 22
illustrate these functions where the narrator directly
reveals his affective response to the prior events.
3 Dataset
Corpus of Personal Stories. Previous work (Gor-
don and Swanson, 2009) showed that about 5% of
all weblog entries are personal stories describing an
event in the author?s daily life. They developed an
automatic classifier for identifying personal narra-
tives from a random sample of 5,000 posts from
a corpus of 44 million entries available as part of
the ICWSM 2010 dataset challenge (Burton et al.,
2009). 229 of these posts were manually labeled as
personal stories. Our experiments are based on 50
of these 229 stories.
Annotation Process. L&W?s theory applies to sub-
sentence discourse units in a narrative. It is an
open question what level of phrasal granularity is
appropriate to apply to written narratives. Here, we
treat each independent clause as the basic unit of
discourse and manually segment each story in our
dataset using this definition. This results in a collec-
tion of 1,602 independent clauses. We then divided
the 50 stories into 4 groups and annotated them in
batches among 3 annotators in order to refine our
annotation guidelines and process. This dataset is
freely available at https://nlds.soe.ucsc.edu/lw.
Previous work has applied L&W?s theory to Ae-
sop?s fables and achieved high levels of interanno-
tator agreement and extremely high machine learn-
ing accuracies (Rahimtoroghi et al., 2013). How-
ever personal narratives clearly provide a more chal-
lenging context for annotation. There was a high
level of disagreement after the initial round of an-
notation. We found at least 6 primary sources of
disagreements:
? Clauses of more than one category are common
with rising action and evaluation, e.g. a clause
containing elements of orientation, action, and
evaluation: After leaving the apartment at 6:45
AM, flying 2 hours, taking a cab to Seattle, and
then driving seven hours up to Whistler including
a border crossing, it?s safe to say that I felt pretty
much like a dick with legs.
? Actions that are implied but not explicitly stated
in the text.
? Stative descriptions of the world as a result of an
action that are not intuitively orientation.
? Stative descriptions of the world that are localized
to a specific place in the narrative, which is prob-
lematic to L&W?s definition of orientation.
? Subjective clauses in scene setting are usually ori-
entation, but are lexically similar to evaluation.
? Disambiguating the functional purpose of clauses
that describe actions, but may be intended to set
the scene as opposed to the rising action.
? Disambiguating the functional purpose of subjec-
tive language in the description of an event or
state, e.g., The gigantic tree outside my window,
The radiant blue of the sky.
After several rounds of annotation we stabilized
on a labeling scheme that hierarchically extends
the original L&W categories, along with annotation
guidelines that annotators could use to disambiguate
recurring problematic cases. The final set of ex-
tended category labels along with two reduced hi-
erarchical mappings are shown in Table 1.
STATIVE-LOCAL CONTEXT is a category for dis-
tinguishing stative descriptions of the world, that are
not intuitively orientation. For example:
? I saw the sign I expected to turn south on Hwy
138. The traffic sign pointed to Sutherlin and
Roseburg,
The clause in italics is a stative that describes the
sign seen in the previous action. It is clearly not an
action or evaluation, but is not intuitively an orien-
tation, because it is so locally dependent.
STATIVE-IMPLIED ACTIONS are clauses, which
do not explicitly mention an action or event, but im-
ply one that is necessary to maintain the causal or
temporal coherence of the remaining story. For ex-
ample: After that, we decided to walk some more.
In the context of the story it is necessary to know
that they actually did walk some more in order to
interpret the other actions described in the narrative.
Implied actions are often passive constructions that
describe a state of the world that could only be true
173
Label Set ? Labels
Extended 0.582 ? Story Orient Action Eval Local Context Implied Action Consequence
Stative 0.597 ? Story Orient Action Eval Stative Stative Stative
L&W 0.630 ? Story Orient Action Eval Orient Action Eval
Table 1: The extended L&W label categories and two reduced hierarchical mappings.
if an action had taken place. For example: We were
at the convention center in about 10 minutes.
STATIVE-CONSEQUENCE is a category that de-
scribes the state of the world that has resulted as
a consequence of an action, but does not directly
evaluate the goals, intentions or desires of the par-
ticipants. For example, clause 23 in Fig. 1.
Using this extended label set we were able to
achieve an inter-annotator agreement between the
3 annotators of 0.582 using Fleiss? ? on assign-
ing categories to clauses. We also mapped the full
set of labels to a smaller subsets to see if the finer
grained distinctions helped improve reliability on
more coarse grained labeling schemes. The ex-
tended labels we included were generally different
types of stative descriptions of the world, which
were all mapped to a single category for the Stative
label set. Finally, we mapped each extended label
to an original L&W category that we thought best
fit the original definitions. When mapping back to
these reduced label sets we were able to increase the
? to 0.597 for the stative set and 0.630 for the orig-
inal L&W categories. This result indicates that we
can achieve higher reliability by ensuring that the
annotators think carefully about particular kinds of
distinctions between different stative clauses.
Gold standard labels were selected based on a sim-
ple majority of the annotator assignments. When no
annotators agreed on a label, one of the selected la-
bels was chosen at random. Once completed there
were 424 action clauses, 702 evaluations, 26 not sto-
ries, 306 orientation, 17 stative consequences, 12
implied actions and 115 local contexts. Note that
EVALUATION and ORIENTATION clauses that would
not be distinguished from ACTION by previous work
constitute two thirds of the clauses.
4 Experiments
The triply annotated dataset described above was
used as training and test data for experiments on
learning to automatically label narrative clauses. 40
narratives were randomly selected to be used as
training and development data and the remaining
10 narratives for test data. The average story in
the training data had 29.3 clauses with the shortest
Feature Set Description
Linguistic Parts of Speech, Number of Charac-
ters in post, Average Word Length,
Unigrams, Bigrams
Lexical and Senti-
ment Categories
LIWC counts and frequencies, nega-
tion
Story Position First Clause, Last Clause, Position in
the story binned into ten story regions
Table 2: Feature Sets for L&W Classification.
story consisting of 4 and the maximum consisting
of 100. The average story in the test data had 43
clauses with the shortest story consisting of 4 and
the maximum consisting of 114.
To derive feature representations of each type of
narrative clause we started with the features pre-
sented in (Rahimtoroghi et al., 2013). We refined
these by examining L&W?s descriptions of distin-
guishing features of each category. Table 2 summa-
rizes the features we automatically extracted from
all narrative clauses in the weblogs.
First, we used the Stanford Parser to distinguish
independent and dependent clauses and kept track
separately of features that occurred in both types
of clause. This is because L&W state that the
unit of analysis should be an independent clause
with its subordinate clauses, but we felt that these
were exactly the cases that often caused difficul-
ties during annotation. However distinguishing
between the features occurring in the two clause
types would allow us to determine if and when
the features of the subordinate clause were rele-
vant or more informative for automatic classifica-
tion. Then, within both dependent and independent
clauses, we distinguished the part-of-speech of the
main verb (POS), whether the clause contained a
negation (Negate), lexical semantic categories from
LIWC (Pennebaker et al., 2001), dependency rela-
tions (DEP), lexical unigrams (STEM), and whether
the verb was one of a class of verbs that are likely to
be stative.
We also developed a set of features describing the
relative position of the clause in the story (Bin-
Position, FirstClause, LastClause), because differ-
ent story regions are often associated with different
174
Feature Gain Act Ori Eval ?
POS:IND-VBD 0.128 0.084 0.002 0.031 0.011
BinPosition0 0.076 0.017 0.042 0.014 0.003
FirstClause 0.044 0.010 0.019 0.011 0.003
POS:IND-VBZ 0.042 0.029 0.008 0.002 0.003
IND-Negate 0.040 0.025 0.000 0.013 0.002
IND-Copula 0.039 0.030 0.004 0.005 0.001
POS:IND-: 0.036 0.001 0.000 0.002 0.033
IND-FirstPerson 0.035 0.017 0.004 0.002 0.013
IND-LIWC Motion 0.034 0.021 0.003 0.006 0.004
POS:IND-VBP 0.033 0.023 0.001 0.007 0.002
Table 3: The 10 most highly correlated features with
each label and cumulatively over all the labels using
mutual information and information gain.
clause types. For example, in Fig. 1 and Fig. 4, the
beginning of the story contains more ORIENTATION
clauses, while ACTION clauses are concentrated in
the middle of the story. The EVALUATION clauses
typically occur part-way through the story where
they provide the narrator?s reaction to story events.
See Table 2.
In total there were 3,510 unique binary valued fea-
tures extracted from our training dataset. We used
mutual information to find the features that had the
highest correlation with each category and the in-
formation gain over all the labels. The 10 highest
valued features are in Table 3, e.g. the top feature is
when the part-of-speech (POS) of the main verb in
the independent clause (IND) is past tense (VBD).
This feature encoding was used for machine learn-
ing experiments with classification algorithms from
Mallet (McCallum, 2002): Naive Bayes (NB) (Wit-
ten and Frank, 2005), Confidence Weighted Linear
Classifier (CWLC) (Dredze et al., 2008), Maximum
Entropy (ME)(Witten and Frank, 2005) and a se-
quential classifier (CRF) (Lafferty et al., 2001).
5 Evaluation and Results
We evaluate the performance of our classifiers with
experiments using the 50 annotated stories. Us-
ing the 40 stories in the training set we calculated
the information gain for each feature (see Table 3).
For each subset of the highest valued features (in
the range of 2
2
-2
12
), we performed a 10-fold cross-
validation on the training data and assessed the per-
formance of each classifier to find the right number
of features. Within each fold of the cross-validation
we also perform a simple grid search for the optimal
hyper-parameters of the model (e.g., the prior in the
ME and CRF models) using only the data within the
training fold.
The feature selection experimental results are
Extended Stative L&W
Classifier # F1 # F1 # F1
CRF 2
7
0.61 2
9
0.61 2
7
0.65
CWLC 2
11
0.67 2
11
0.68
?
2
11
0.73
?
ME 2
11
0.67 2
10
0.68
?
2
10
0.73
?
NB 2
9
0.68
?
2
9
0.70
?
2
10
0.76
?
Table 4: The optimal number of features found for
each model and the average F-score obtained using
a 10-fold cross-validation on the training data.
shown in Table 4. We report the optimal number
of features and the corresponding macro F-score,
weighted by the relative frequency of each category,
for each algorithm and label set. For all algorithms,
performance increases for label sets with higher lev-
els of abstraction. The Naive Bayes and CRF mod-
els perform better with a small subset of the fea-
tures, while the ME and CWLC algorithms use a
much larger subset. Surprisingly the sequential clas-
sifier has the lowest F-score and Naive Bayes per-
forms the best. A
?
indicates a significant improve-
ment over CRF at p < 0.05 using a two-sided t-test.
No other differences were significant.
Using the optimal number of features obtained
from this search we trained a model for each algo-
rithm using the entire training dataset and selecting
the hyper-parameters as before. We applied these
models to the unseen test data and evaluated the per-
formance of each classifier as applied to the entire
set of clauses and to individual narratives.
We first computed the precision, recall and F-score
aggregated over all the clauses in the test set. Ta-
ble 5 summarizes the results for each classifier and
label set. The left hand side of the table shows the
macro precision, recall and F-score weighted by the
relative frequency of each label. The right hand
side of the table shows the F-score of each indi-
vidual label separately. On this evaluation, Naive
Bayes outperforms all other methods on all label
sets. Overall, precision and recall are relatively bal-
anced achieving a maximum F-score of 0.689 when
the labels are mapped back to the original L&W cat-
egories. The CRF does surprisingly well consider-
ing its poor performance during the feature selection
search. The classifiers perform the poorest on orien-
tation clauses and the best on evaluation clauses.
As mentioned above, the annotation task is highly
subjective, requiring interpreting the narrative and
the author?s intention, which prevents us from ob-
taining high levels of inter-rater agreement. Because
of the noise in the annotations, the standard evalua-
175
Overall Per Label
L&W Stative
Label Set Model Prec Rec F1 ? Ori Eva Act ? Imp Loc Con
Extended
CRF 0.568 0.626 0.593 0.419 0.532 0.727 0.651 0.000 0.000 0.041 0.000
CWLC 0.567 0.616 0.582 0.398 0.377 0.763 0.612 0.000 0.000 0.087 0.000
ME 0.597 0.649 0.614 0.450 0.496 0.767 0.667 0.000 0.000 0.085 0.000
NB 0.625 0.656 0.623 0.459 0.478 0.781 0.650 0.000 0.000 0.174 0.000
Stative
CRF 0.563 0.591 0.574 0.370 0.412 0.695 0.628 0.000 0.235
CWLC 0.572 0.621 0.587 0.403 0.417 0.760 0.614 0.000 0.077
ME 0.610 0.644 0.611 0.441 0.464 0.759 0.673 0.000 0.118
NB 0.650 0.667 0.638 0.477 0.496 0.779 0.676 0.000 0.226
L&W
CRF 0.650 0.665 0.656 0.468 0.556 0.742 0.640 0.000
CWLC 0.624 0.647 0.632 0.424 0.480 0.747 0.609 0.000
ME 0.681 0.700 0.688 0.517 0.580 0.780 0.670 0.000
NB 0.687 0.705 0.689 0.514 0.565 0.780 0.687 0.000
Table 5: The performance of each of classifier on the test set when all clauses are aggregated together.
Agreement Total # Prec Rec F1 ? Ori Eva Act ?
1 of 3 15 0.333 0.400 0.339 0.069 0.000 0.625 0.333 0.000
2 of 3 146 0.597 0.610 0.580 0.405 0.472 0.700 0.622 0.000
3 of 3 269 0.770 0.773 0.767 0.607 0.667 0.824 0.746 0.000
All 430 0.687 0.705 0.689 0.514 0.565 0.780 0.687 0.000
Adjusted 430 0.646 0.660 0.643 0.447 0.516 0.745 0.623 0.000
Table 6: Performance measures for different levels of agreement among the annotators.
tion metrics may hide information about the ability
of the classifiers to learn from our feature set. For
example, the best performing classifier (NB) incor-
rectly labeled 127 clauses out of 430 possible in the
test set. However, 44 of these errors agreed with
at least one annotator, but were counted as entirely
incorrect in the previous evaluations.
To address these concerns we also evaluated the
performance of the the best performing classifier
based on the level of agreement of each instance us-
ing two different approaches. See Table 6. The first
approach was inspired by the approach in (Louis
and Nenkova, 2011) where the clauses in the test
set are binned based on the number of annotators
who agreed with the gold standard label. The per-
formance is then calculated for each bin. The first
three rows of Table 6 show the performance for the
different levels of agreement in the dataset. There
were only 15 clauses in the test set where there was
no agreement at all. It is unsurprising that when
the annotators could not agree on a label the sys-
tem performed near chance levels. However, when
all three annotators agreed on the gold standard
label the F-score improved to 0.767. As a compar-
ison, the F-score of the entire test set was 0.689 as
shown in the row labeled All.
Our second approach is based on the proposal of
Tetreault et al. (Tetreault et al., 2013). They intro-
Label Set Model Min Max Mean ? CI
Extended
CRF 0.333 0.763 0.540 ? 0.080
CWLC 0.276 0.763 0.582 ? 0.099
ME 0.333 0.753 0.572 ? 0.088
NB 0.333 0.741 0.573 ? 0.093
Stative
CRF 0.298 0.762 0.521 ? 0.099
CWLC 0.345 0.758 0.591 ? 0.090
ME 0.333 0.753 0.562 ? 0.098
NB 0.333 0.758 0.582 ? 0.088
L&W
CRF 0.333 0.837 0.609 ? 0.097
CWLC 0.458 0.877 0.658 ? 0.081
ME 0.333 0.830 0.649 ? 0.095
NB 0.333 0.851 0.647 ? 0.096
Table 7: Summary statistics of the F-score, with
95% confidence intervals, when evaluating stories.
duce a modification to the standard precision, re-
call and F-scores that takes into account the level
of agreement of each instance, where the values of
true-positives and false-negatives are assigned frac-
tional counts based on the proportion of annotators
who assigned that label. The final row of Table 6
provides the results using these adjusted values.
We also investigated the performance of the classi-
fiers when evaluating each story separately. Table 7
summarizes these results. Each classifier was ap-
plied to the clauses of the 10 narratives in the test
set and the F-score was computed for each narrative
individually. The table shows the minimum, maxi-
mum and average F-score with 95% confidence in-
176
ll l
l l
l l l
l l l
0.50
0.55
0.60
0.65
0.70
0.75
250 500 750 1000# Training Clauses
F?Sc
ore
Model
l CRFCWLCMaxEntNB
Figure 2: Learning curves of the Naive Bayes clas-
sifier using the optimal number of features.
tervals over the 10 narratives.
The CWLC performed the best on this test and
the performance of all the algorithms generally im-
proved using the higher-level label sets. The results
also show that there is a high variance in perfor-
mance between stories, with a minimum F-score of
0.458 and a maximum of 0.877 for the CWLC on
the L&W label set. This indicates that some clauses
are ambiguous and difficult to label, but also that
some stories are more difficult to classify.
To assess whether more annotated data could im-
prove performance, we ran a series of learning
curves in Fig. 2. Only the training data was used
for these experiments. The curves were created by
randomly sampling 90% of the data for training and
10% for testing. A model was trained, using the
same process as above, on successively larger sub-
sets of the data and applied to the 10% held out
clauses. This process was repeated 10 times and the
mean F-Score is reported. In nearly all cases, the
performance of classifiers is still increasing when
all of the data is used indicating that we have not
exhausted the expressive power of our features and
more annotated data would be useful. However, we
also see we can reach about 93% of our maximum
performance with only a few hundred examples. We
plan to increase the size of our annotated data set in
future work.
5.1 Error Analysis
Our results to date indicate that we achieve an over-
all F-score of 0.689, and that our classifiers are most
accurate for the evaluation and action categories.
See Table 6. Fig. 3 presents a confusion matrix
0036
0521242
096826
01712183EvaluationAction
Orientation
Not Story
Evaluation Action Orientation Not StoryGold
Pred
icted
Figure 3: Confusion matrix for the best classifier.
showing the frequency of predicted labels against
the gold standard labels for the Naive Bayes classi-
fier on the L&W label set. With the exception of not
story there are cases of confusion between all cate-
gories. However, in the vast majority of cases both
action and orientation are confused with evaluation
and the classifier overpredicts evaluation.
We also categorized errors for the Naive Bayes
classifier into the the 4 sources of errors in the pre-
dictions shown in Table 8. The most common errors
involved clauses with lexical INDICATORS that are
highly correlated with one category, but in the con-
text and interpretation of the story actually function
as a different type. For example, unfortunately,
could and n?t are all strong indicators of evaluation,
but in this case the primary function of the clause
is to set the scene for the rest of the story, i.e., ori-
entation. The interpretation of these clauses is clear
to a human, despite the lexical items misleading the
classifier.
Another source of error is when the function of the
clause in the narrative is ambiguous (PURPOSE in
Table 8). While there may be some misleading lex-
ical indicators in these clauses, there were often no
strongly correlated words, such as the adjectives and
modal verbs in EVALUATIONS. The distinction in
these cases is that the primary function of the clause
within the story is unclear, even to a human reader.
Unsurprisingly, most of the examples in this cate-
gory had low inter-rater agreement.
Some of the clauses contain figurative language
or complex constructions that require a significant
amount of world knowledge and INFERENCE to in-
terpret. For example, understanding the INFERENCE
clause in Table 8 requires recognizing the metaphor
about rabbit food in order to identify the subjective
evaluation the narrator is making.
There are also cases of clauses that contain MULTI-
PLE categories, at least partially because of the gran-
ularity of our segmentation. In the example in Ta-
ble 8 a new character, Alejandrio, is introduced and
a rising action is described, trekking to the waterfall.
177
Error Type Freq Gold Pred Example
Indicators 57 Ori Eva So, unfortunately I couldn?t make the Gamesindustry.biz party tonight.
Purpose 20 Ori Eva I know it is a remarkable haircut because on the way home a handsome young Mo-
roccan man nearly died to tell me how beautiful I was.
Inference 6 Eva Ori That?s that rabbit food that all of those Northeastern Kerry voters...
Multiple 4 Act Ori We trekked to a waterfall in the park with the help of Alejandrio a 65 year old
Honduran guy who not only walked quicker than us but also carried all the water.
Unclear 39 Ori Eva We have diners out east,
Not Story 7 Not Act scroll down to the Hobbit post,
Table 8: Several common sources of errors with a prototypical example.
Our annotation guidelines instructed us to prefer ac-
tions in these types of clauses, however, both ORI-
ENTATION and ACTION are present in this situation.
There were also 39 clauses that were labeled in-
correctly that had no clear reason (UNCLEAR) for
being mislabelled. We also explicitly excluded the
7 clauses marked not part of the story.
The types of errors described above are not mutu-
ally exclusive and in some cases are causally related.
For example, the purpose of a clause may be am-
biguous because it contains conflicting lexical indi-
cators. Similarly, a clause containing multiple cate-
gories will likely have strong lexical indicators from
each of these categories so that the classification al-
gorithms cannot disambiguate among possible la-
bels. This might be improved by more data, more
sophisticated semantic features, or possibly an anal-
ysis focused on discourse relations, such as those in
the PDTB (Louis et al., 2010; Prasad et al., 2008),
or Elson?s STORY INTENTION GRAPH (Rishes et al.,
2013; Elson and McKeown, 2010; Elson, 2012).
6 Discussion
This paper describes work on categorization of nar-
rative clauses based on Labov & Waletzky?s theory
of oral narrative, applied to personal narratives writ-
ten by ordinary people. We show that we can auto-
matically classify narrative clauses with these cate-
gories achieving an overall F-score of 0.689, which
is substantially higher than a random (0.250) or ma-
jority class (0.437) baseline, which increases to an
F-score of .767 on the cases where all three annota-
tors agreed. The learning curves plotted in Fig. 2
clearly suggest that more training data would be
beneficial before we investigate more complex fea-
tures and learning algorithms.
We believe the ability to automatically perform this
type of simple narrative analysis will enable and
improve many other types of deeper narrative un-
derstanding (Rahimtoroghi et al., 2014; Hu et al.,
2013). For example, causal and temporal relation-
ship extraction methods that focus only on clauses in
the same action sequence be more accurate, because
they exclude disconnected events from the orienta-
tion or evaluation sections. This type of analysis
will also enable new methods for learning attitudes
and values of societal groups based on the different
affective evaluations that are provided in response
to action clauses.
Our results also highlight several properties of the
data. Performance is different for results by story
rather than over all clauses. This indicates that
some stories are more difficult to classify than oth-
ers and that ambiguous clauses are not uniformly
distributed but are likely to be correlated with par-
ticular authors or writing styles. In other work, we
have started to investigate whether we can automat-
ically rate the temporal coherence of personal narra-
tives (Ryan et al., 2014). We can use this to identify
stories with utterances that are likely to be difficult
to classify because of the poor quality of the narra-
tive input. These cases are unlikely to have usable
narrative structure.
Acknowledgments
This research was supported by NSF Grants IIS
#1002921 and IIS #123855. The content of this
publication does not necessarily reflect the position
or policy of the government, and no official en-
dorsement should be inferred.
Appendix A
See Fig. 4 for an additional example labelled with
L&W Categories.
178
#Category
Story Clause
1
Abstract
Today was a very eventful work day.
2
Orientation
Today was the start of the G20 summit.
3
Orientation
It happens every year
4
Orientation
and it is where 20 of the leaders of the world come together to talk about how to run their
governments effectively and what not.
5
Orientation
Since there are so many leaders coming together their are going to be a lot of people who
have different views on how to run the government they follow so they protest.
6
Orientation
This week things started alright and on schedule.
7
Action
There was a protest that happened along the street where I work
8
Action
and at first it looked peaceful until a bunch of people started rebelling
9
Action
and creating a riot.
10
Action
Police cars were burned
11
Action
and things were thrown at cops.
12
Orientation
Police were in full riot gear to alleviate the violence.
13
Action
As things got worse tear gas and bean bag bullets were fired at the rioters
14
Action
while they smash windows of stores.
15
Evaluation
And this all happened right in front of my store
16
Evaluation
which was kind of scary
17
Evaluation
but it was kind of interesting
18
Coda
since I?ve never seen a riot before.
Figure 4: A personal narrative about a protest, with narrative categories of Labov & Waletzky, 1967.
References
Michael Bamberg. 2006. Stories: Big or small: Why do
we care? Narrative inquiry, 16(1):139?147.
Brandon Beamer and Roxana Girju. 2009. Using a bi-
gram event model to predict causal potential. In Com-
putational Linguistics and Intelligent Text Processing,
p. 430?441. Springer.
Jennifer G Bohanek, Kelly A Marin, and Robyn Fivush.
2008. Family narratives, self, and gender in early
adolescence. The Journal of Early Adolescence,
28(1):153?176.
Jerome Bruner. 1991. The narrative construction of re-
ality. Critical Inquiry, 18:1?21.
Kevin Burton, Akshay Java, and Ian Soboroff. 2009.
The ICWSM 2009 spinn3r dataset. In Proc. of the Third
Annual Conf. on Weblogs and Social Media.
N. Chambers and D. Jurafsky. 2009. Unsupervised
learning of narrative schemas and their participants. In
Proc. of the 47th Annual Meeting of the ACL, p. 602?
610.
Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011.
Minimally supervised event causality identification. In
Proc. of the Conf. on Empirical Methods in Natural
Language Processing, p. 294?303.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. In
Proc. of the 25th international conference on Machine
learning, p. 264?271. ACM.
D.K. Elson and K.R. McKeown. 2010. Building a bank
of semantically encoded narratives. In Proc. of the Sev-
enth International Conf. on Language Resources and
Evaluation (LREC 2010).
David K Elson. 2012. Detecting story analogies from
annotations of time, action and agency. In Proc. of
the LREC 2012 Workshop on Computational Models of
Narrative, Istanbul, Turkey.
Robyn Fivush and Katherine Nelson. 2004. Culture and
language in the emergence of autobiographical mem-
ory. Psychological Science, 15(9):573?577.
Robyn Fivush, Jennifer G Bohanek, and Marshall Duke.
2005. The intergenerational self: Subjective perspec-
tive and family history. Individual and collective self-
continuity. Mahwah, NJ: Erlbaum.
R.J. Gerrig. 1993. Experiencing narrative worlds: On
the psychological activities of reading.
Andrew Gordon and Reid Swanson. 2009. Identifying
personal stories in millions of weblog entries. In Third
International Conf. on Weblogs and Social Media, Data
Challenge Workshop.
Andrew Gordon, Cosmin Bejan, and Kenji Sagae. 2011.
Commonsense causal reasoning using millions of per-
sonal stories. In Twenty-Fifth Conf. on Artificial Intelli-
gence (AAAI-11).
Amit Goyal, Ellen Riloff, and Hal Daum?e III. 2010. Au-
tomatically producing plot unit representations for nar-
rative text. In Proc. of the 2010 Conf. on Empirical
Methods in Natural Language Processing, p. 77?86.
Arthur C Graesser, Murray Singer, and Tom Trabasso.
1994. Constructing inferences during narrative text
comprehension. Psychological review, 101(3):371.
179
T. Habermas and S. Bluck. 2000. Getting a life: the
emergence of the life story in adolescence. Psychol
Bull, 126(5):748?69.
Zhichao Hu, Elahe Rahimtoroghi, Larissa Munishkina,
Reid Swanson, and Marilyn A Walker. 2013. Unsu-
pervised induction of contingent event pairs from film
scenes. In Proc. of Conf. on Empirical Methods in Nat-
ural Language Processing, p. 370?379.
W. Labov and J. Waletzky. 1967. Narrative analysis:
Oral versions of personal experience. In J. Helm, ed.,
Essays on the Verbal and Visual Arts, p. 12?44.
W. Labov. 1997. Some further steps in narrative analy-
sis. Journal of narrative and life history, 7:395?415.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data.
Wendy G Lehnert. 1981. Plot units and narrative sum-
marization. Cognitive Science, 5(4):293?331.
Annie Louis and Ani Nenkova. 2011. Automatic identi-
fication of general and specific sentences by leveraging
discourse annotations. In International Joint Conf. on
Natural Language Processing, p. 605?613.
Annie Louis, Aravind Joshi, Rashmi Prasad, and Ani
Nenkova. 2010. Using entity features to classify im-
plicit relations. In Proc. of the 11th Annual SIGdial
Meeting on Discourse and Dialogue.
Mehdi Manshadi, Reid Swanson, and Andrew S Gor-
don. 2008. Learning a probabilistic model of event
sequences from internet weblog stories. In Proc. of the
21st FLAIRS Conf. .
Dan P McAdams. 2003. Identity and the life story. Au-
tobiographical memory and the construction of a nar-
rative self: Developmental and cultural perspectives,
9:187?207.
Andrew Kachites McCallum. 2002. MAL-
LET: a machine learning for language toolkit.
http://mallet.cs.umass.edu.
K.C. McLean and A. Thorne. 2003. Adolescents? self-
defining memories about relationships. Developmental
Psychology, (39):635?645.
J. W. Pennebaker, M. E. Francis, and R. J. Booth. 2001.
Inquiry and Word Count: LIWC 2001.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie Web-
ber. 2008. The Penn Discourse TreeBank 2.0. In Proc.
of 6th International Conf. on Language Resources and
Evaluation (LREC 2008).
Michael W Pratt and Barbara H Fiese. 2004. Families,
Stories, and the Life Course: An Ecological Context.
Elahe Rahimtoroghi, Reid Swanson, and Marilyn A.
Walker. 2013. Evaluation, orientation, and action in in-
teractive storytelling. In Proc. of Intelligent Narrative
Technologies 6.
Elahe Rahimtoroghi, Thomas Corcoran, Reid Swanson,
Marilyn A. Walker, Kenji Sagae, and Andrew S. Gor-
don. 2014. Minimal narrative annotation schemes
and their applications. In Proc. of Intelligent Narrative
Technologies 7.
Mehwish Riaz and Roxana Girju. 2010. Another look
at causality: Discovering scenario-specific contingency
relationships with no supervision. In Semantic Com-
puting (ICSC), p. 361?368. IEEE.
Elena Rishes, Stephanie Lukin, David K. Elson, and
Marilyn A. Walker. 2013. Generating dierent story
tellings from semantic representations of narrative. In
Int. Conf. on Interactive Digital Storytelling, ICIDS?13.
James Owen Ryan, Marilyn A. Walker, and Noah
Wardrip-Fruin. 2014. Toward recombinant dialogue
in interactive narrative. In 7th Workshop on Intelligent
Narrative Technologies.
Nancy L. Stein, Tom Trabasso, and Maria D. Liwag.
2000. A goal appraisal theory of emotional understand-
ing: Implications for development and learning. In M.
Lewis and J. M. Haviland-Jones, ed, Handbook of emo-
tions (2nd ed.), p. 436?457.
Joel Tetreault, Martin Chodorow, and Nitin Madnani.
2013. Bucking the trend: improved evaluation and an-
notation practices for esl error detection systems. Lan-
guage Resources and Evaluation, p. 1?27.
A. Thorne and V. Nam. 2009. The storied construction
of personality. In Kitayama S. and Cohen D., ed, Hand-
book of Cultural Psychology, p. 491?505.
A. Thorne and L. A. Shapiro. 2011. Testing, testing:
Everyday storytelling and the construction of adoles-
cent identity. Adolescent Vulnerabilities and Opportu-
nities: Developmental and Constructivist Perspectives,
38:117.
A. Thorne, N. Korobov, and E. M. Morgan. 2007. Chan-
neling identity: A study of storytelling in conversations
between introverted and extraverted friends. Journal of
research in personality, 41(5):1008?1031.
Avril Thorne. 2004. Putting the person into social iden-
tity. Human Development, 47(6):361?365.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical machine learning tools and techniques. Mor-
gan Kaufmann, San Francisco, CA.
180
