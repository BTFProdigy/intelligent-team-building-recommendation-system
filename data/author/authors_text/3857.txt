Japanese Case Structure Analysis 
by Unsupervised Construction of a Case Frame Dictionary 
Daisuke Kawahara, Nobuhiro Kaji and Sadao Kurohashi 
Graduate  School of Intbrm~tics, Kyoto  University 
Yoshida-Honmachi ,  S~kyo-ku, Kyoto,  606-8501, Japan  
{kawahara, kaj i ,  kuro }@p inc. kuee. kyoto-u, ac. jp 
Abstract 
In Japanese, case structure analysis is very im- 
t)ortant to handle several troublesome charac- 
teristics of Japanese snch as scrambling, onfis- 
sion of ease components, mid disappearance of
case markers. However, fi)r lack of a wide- 
coverage ase frame dictionary, it has been dif- 
ficult to perfornl case structure analysis accu- 
rat;ely. Although several methods to construct 
a ease fl'mne dictionary from analyzed corpora 
have been proposed, they cannot avoid data 
sparseness 1)rol)lem. This paper proposes an un- 
supervised method of constructing a case frame 
dictionary from an enormous raw corpus by us- 
ing a robust and accurate parser. It also pro- 
rides a case structure analysis method based on 
the constructed ictionary. 
1 I n t roduct ion  
Syntactic analysis, or parsing has been a main 
objective in Natural Language Processing. In 
case of Jat)anese , however, syntactic analysis 
cannot clarify relations between words ill sen- 
tences because of several troublesome character- 
istics of Japanese such as scrambling, omission 
of case components, and disappearance of case 
markers. Therefore, in Japanese sentence analy- 
sis, case structure analysis is an important issue, 
and a case frame dictionary is necessary for the 
analysis. 
Some research institutes have constructed 
Japanese case frmne dictiouaries manually (Ike- 
hara et al, 1997; Infbrmation-Technology Pro- 
motion Agency, Japan, 1987). However, it is 
quite expensive, or almost impossible to con- 
struct a wide-coverage ease fl'anm dictionary by 
hand. 
Others have tried to construct a case fl'mne 
dictionary automatically from analyzed corpora 
(Utsuro et al, 1998). However, existing syntac- 
tically analyzed corpora are too small to learn a 
dictionary, since case fl'ame iuformation consists 
of relations between ouns and verbs, which rnul- 
tiplies to millions of combinations. 
Based on such a consideration, we took the 
fbllowing unsupervised learning strategy to the 
.Japanese case structure analysis: 
1. At first, a robust and accurate parser is de- 
veloped, which does not utilize a case fl'mne 
dictionary, 
2. a very large corI)us is parsed by the parser, 
3. reliable noun-verb relations are extracted 
from the parse results, and a case frmne dic- 
tionary is constructed from them, and 
4. the dictionary is utilized for case structure 
analysis. 
2 Characteristics of Japanese 
language and necessity of case 
s t ructure  ana lys i s  
In Japanese, postpositions function as case 
markers ( (Ms)  mid a verb is final in a sentence. 
The basic structure of a Japanese sentence is as 
fbllows: 
(1) kate  9a coat wo ki~'u. 
he nominative-CM coat accusative-CM wear 
(lie wears a coat) 
A clause modifier is left to the modified noun 
as follows: 
(2) kate  9 a k i te - i ru  coat 
lie nom-CM wear coat 
(the coat he wears) 
The modified noun followed by a postposition 
then becomes a case component of a matrix verb. 
The typical structure of a Japanese complex sen- 
tence is as fbllows: 
432 
(3) boush, i no irv wa kitc-ir'u 
hat of color tol)ic-marker wear 
coat ni a'wa~'cr"~,. 
coal; dative-CM harmonize 
(c/) harmonizes the color of his/her hat with 
the coat he/she wears) 
In terms of autolnatic analysis, the problen> 
atic characteristics of Japanese sentences can be 
summarized as follows: 
1. Case componenl;s are often scrambled or 
omitted. 
2. Case-marking postpositions disappear when 
case components are accompanied by topic- 
markers or other special 1)ostpositions 
meaning 'just', 'also' and others. 
cx) karv 'wa coat me ti:itc-iv'u. 
he tol}iC-lna.rker coat also wear 
(Ile wears a coat a.lso) 
3. A noun modified 1)y a clause is usually a case 
component for the verb of the mo(litlying 
clause. However, there is no case-marker for 
their relation. In case of sentence 3, there is 
no case-marker for coat in relation to kite- 
ir'u 'wear'. Note that 'hi (dative-CM) of coat: 
ni does not show the case to kitc-ivu 'wear', 
lint to awascr'v, 'harmonize'. 
4. Sentence 3 exhibits a typical structural am- 
1)iguity in a ,lalmnese sentence. That is, 
ir'o "~va 'color topic-marker' possit)ly modi- 
ties kit, c- iru 'wear' or awa.scv'u qmrmonize'. 
In English, sentence structure is rather rigid, 
and word order (the position in relation to the 
verb) clearly defines cases. In Japanese, how- 
ew% the l)roblem 1 above makes word order use- 
less, and CMs constitute the only int'ormation for 
detecting cases. 
Nevertheless, CMs often disapl)ear because of 
the problems 2 and 3, whidl means that sim- 
ple syntactic analysis cmmot clari(5~ cases sui\[i- 
cientl> For eXalnple, given an inlmt sentence: 
(4) har'c wa Dcv, tsch,-go me hano, sv,. 
he topic-marker Germall also sl)eak 
(he speaks German also) 
a simple syntactic analysis just detects both kar'c 
'he' and Dcutsch-go 'German' modifies \]ta'aas~t 
'speak', but tells nothing about which is subject 
and object. This analysis result is not sufficient 
for subsequent NLP applieations like Japanese 
to English machine l;rmmlation. 
Then, what we need to do is a case structure. 
analysis based on a case fl'ame dictiolmry, or a 
subcat, of each verb as follows: 
hanasu 'speak': 
ga (nora) ks're 'he', hire 'person' 
'we (ace) cigo 'English', kotoba 'language' 
~;i?'U, ~%vear': 
ga (nora) kavc qm', hil, o 'person' 
we (ace) fuhu 'cloth', coat 'coat' 
a'tl Jasel'~t 'har l l lonize' :  
ga (nora) kar'c 'he', hito 'person' 
'we (ace) ire ~color' 
ni (dat) fltku 'cloth' 
Consultation of such a dictionary can easily find 
that kar'c 'he' is a nomilmtive case and Dev, tsch,- 
90 'German' is an accusative (:as(', in the sentence 
4. 
Furthermore, a (:ase frame dictionary Call so lve  
the problem 4 above, that is, some part of struc- 
tural ambiguity in sentences. In case of sentence 
3, a t)r()l)er head for 'ir'o wa 'color topic-marker' 
(:all })e selected by consulting case slots of kir'u 
~wear' and those of a'wascru 'harmonize'. 
3 Unsupe,  rv i sed  const ruct ion  o f  a 
case  f ra lne  d ic t ionary  
This s(x:tion explains how to construct a case 
fralll(*, dictionary fl'om corl)ora autonmtica.lly. 
As mentioned in the introduction section, it; 
is quite expensive, or ahnost ilnl)ossible to con- 
struct a wide-coverage case frame dictionary by 
lmnd. In Japanese,, some noun q- copula works 
like an adjective. For example, sa~tsei da 'posi- 
tiveness + Colmla' can take 9a case and 'hi case. 
However, such case frames are rarely covered t)y 
the existing handmade dictionaries 1.
Fm'thermore, existing halldmade dictionaries 
cover typical obligatory cases like ga (nomina- 
tive), wo (accusative), ni (dative), but do not 
cover compound case markers uch as ni-kandz.itc 
'in terms of', 'wo-rncqutte 'concerning' and oth- 
ers. 
Then, we tried to construct an example-based 
case frmne dictionary from corpora, which de- 
lOut method collects case frames not only tbr verbs, 
but also tbr adjectives mM nouns-kcopula. In this paper, 
we use 'verb' instead of 'w;rb/adjective. orllOllll -{- copula.' 
for simplicity. 
433 
Table 1: The accuracy of KNP. 
'wa~ 7tto c lause  clause 
9 a 'we n i  ka'r'a rr~.ade ?lori topic- modif~ying modifying 
noln. ace. dative from to from marker verbs nouIIS 
'lbtal 
91.2% 97.7% 94.2% 83.8% 85.3% 82.8% 88.0% 84.3% 95.5% 91.3% 
scribes what kind of cases each verb has and 
what kind of nouns can fill a case slot. Very large 
syntactically analyzed corpora could be useful to 
construct such a dictionary. However, corpus an- 
notation costs very much and existing analyzed 
corpora are too small from the view point of case 
frame learning. For exmnple, in Kyoto Univer- 
sity Corpus which consists of about 40,000 ana- 
lyzed sentences of newspaper articles, very basic 
verbs like te tsudau 'help' or v, ketsv ,  kcr 'u 'accept' 
appear only 10 times or 15 times respectively. It
is obvious that such small data are insufficient 
for automatic ase frmne learning. That is, case 
frame learning must be done from enormous un- 
analyzed corpora, in unsupervised way 2. 
3.1 Good parser 
NLP research group at Kyoto University has 
been developing a robust and accurate parsing 
system, KNP, over the last ten yem's (Kurohashi 
and Nagao, 1994; Kurohashi and Nagao, 1998). 
This parser has the following advantages: 
? .Japanese is an agglutinative language, and 
several Nnction words (auxiliary verbs, suf- 
fixes, and postpositions) often appear to- 
gether and in many cases compositionality 
does not hold among them. KNP treats 
such function words careflflly and precisely. 
? KNP detects scopes of coordination struc- 
tures well based on their parallelism. 
? KNP employs everal heuristic rules to pro- 
duce mfique parses for the input sentences. 
The accuracy of KNP is shown in Table 1, 
which counted whether each phrase modifies a 
proper head or not. The overall accuracy was 
around 90%, and the accuracy concerning case 
components varies from 82% to 98%. 
21n English, several unsupervised methods have been 
proposed (Manning, 1993; Briscoe and Carroll, 19!)7). 
However, as mentioned in Section 3, automatic Japanese 
case analysis is much harder than English. 
We can collect pairs of verbs and case compo- 
nents from the automatic analyses of large cor- 
pora by KNP. 
3.2 Cop ing wi th  two problems 
The quality of automatic ase frame learning 
could be negatively influenced by the %llowing 
two problems: 
Word sense ambiguity:  A verb sometimes 
has w~rious usages and possibly has several 
case frames depending on its usages. 
Structura l  ambiguity:  KNP performs fairly 
well, but automatic parse results inevitably 
contt~in errors. 
The tbllowing sections explain how to solve 
these problems. 
3.2.1 Word sense ambigu i ty  
If a verb has two or more meanings and their 
case fl'ame patterns differ, we htwe to disam- 
biguate the sense of each occurrence of the verb 
in a corpus first, and collect case components for 
each sense respectively. However, unsupervised 
word sense disambiguation f fl'ee texts is one of 
the most ditficult problems in NLP. At the very 
begimfing, even the definition of word senses is 
open to question. 
To cope with this problem, we made a very 
simple but usefltl assumption: a light verb has 
diffbrent case frames det)ending on its main case 
component; an ordinary verb has a unique case 
frmne even if it has two or more meanings. For 
example, the case frmne of the verb narn  'be- 
come' differs depending on its ni (dative) case 
as %llows: 
. . .  ga  b?}ouki n i  na ' ru  
nora. become ill 
? . .  ga  . . .  to  to rnodach i  n i  na'r"u 
nora. with become a fliend 
In most cases, the main case components are 
placed just in front of the light verbs so that 
the automatic parser can detect their relations 
434 
Tal/le 2: EXmnl)les of the constructed ease frames. 
verl)s 
t, aS?t\]gCl"~l, 
'help' 
yomu 
l'ead' 
case lnarkel"s 
(1,o111) 
,,,,o (it(:(:) 
'r~,i (dat) 
ae (op) 
.qa (no\]n) 
'wo (at(;) 
hi, (dat) 
& (o10 
example  nomls  
husband, person, child, staff, I, SUSl)eet, faculty, ... 
.jol), shol) , farmwork, preparation, election, move, ... 
son, friend, ambassador, meml~er, thank, holid~\y, ... 
volunte(,r, aft'air, otfice, rewar(l, house, headquarters, ...
lX;rson, \], chihl, adult, parent, teacher, ... 
newspaper, book, magazine, article, nov(J, letter, ... 
chiht, person, daughter, teacher, student, reader, ... 
newspaper, book, magazine, library, classroom, bathroom, ... 
reliably. Therefore, as for five major and trou- 
t)lesome light verbs (.~'.,r'u 'do', 'nwr'u, q)ceomo?, 
ar'u 'is . . . ' ,  iu ~s~w', nai 'not'), their case fl'mnes 
are distinguished epending (m their left neigh- 
bouring case components. For other verbs, we 
aSStlllle a \] lnique ease f rame.  
3.2 .2  St ructura l  ambigu i ty  
As shown in '_\['~dfle 1, KNP detects heads of case 
conlt~onents in faMy high accuracy. However, 
in order to collect nmch reliable data, we dis- 
carded moditier-hcad relations in the aul;onmti- 
tal ly Imrsed corpora in the following cases: 
? When CMs of ease conqxments disappear 
because oi" topic markers or others. 
? When the verb is followed 1)y a causative 
auxil iary o1' a passive auxiliary, l;he case tm.t- 
t(:rn is e\]mnged and the 1;race in KNI '  is not 
so rclial)le. 
Based on the conditions al)ove, case compo- 
nents of each verb are collected froln the 1)arscd 
corpora, and the collected ata arc considered as 
case frames of verbs. However, if the f lcquency 
of a CM is very low compared to other CMs, it 
might t)e collected because of parse errors. So, 
we set the threshold for the CM flequency as 
2~,  where m.f means the frequency of the 
1nest folln(t ChJ. i f  the fl'equeney of ~t CM is less 
t lmn the threshold, it is discarded, l.~br exalnple, 
suppose the most frequent CM fin' a verb is we, 
100 times, and the frequency of ni CM tbr the 
verb is 1.6, ni CM is discarded (since it is less 
than the threshold, 20). 
a.3  Const ructed  case  f rmne d ic t ionary  
We applied the al)ow', procedure to Mainichi 
Newst)al)er Corpus (7 years, 3,600,00(} sen- 
tences). Fronl the cortms , case franws of 23,497 
verbs are constructed; the average number of 
ease slots of a verb is 2.8; the average munber 
of cxanqflc nouns in a (:as(: slot is 33.6. Table 2 
shows exmnlfles of constructed ease Dames. 
Although the constructed ata look apl)ropri- 
ate in most cases, it is hard to evaluate a (lictio- 
nary statica.ll> In the next section, we use the 
dictiomu'y in case structure analysis and eval- 
uate the analysis result, wlfich also im\])lies an 
cvahu~.ti(m of the dictionary itself. 
4 Case  s t ructure  ana lys i s  us ing  the  
const ructed  case  f rame d ic t ionary  
4.1. Match ing  o f  an  input  sentence  and  
a case  f ra l l le  
'Jl~e basic 1)ro(:cdure in ('ase strucl;ul"e analysis is 
lo match an inlml sentence with a case frame, 
aS show11 ill lqgUl'C, 1. 
The matching of case conq)onenl:s in an input 
and case slots in a case  fl'alllO is (\[Olle Oll the 
following conditions: 
I. When a ease component has a CM, it must 
be assigned to 1;11o case slot with the same 
CM.  
. When a case COml)Onent does nol: have a 
CM, it can 1)e assigned to the 9a, we, or ni 
CM slot. 
. ()nly one case component can be assigned 
to a case slot (unique case assiglmmnt con- 
straint). 
The conditions above may produce nmltil)le 
matching patterns, and to select the proper one 
alllOng {,llclll, 11Oll118 of case COlllpon(',lltS al'o COlll- 
pared with examph',s in case slots of the (tictio- 
nary. 
435 
syorui wa . 
(5) document topic-marker / 
ka,'e .i .___1 
! 
,,e 1 
~" walashila 
" hand 
\[ (1 handed the document to him.) 
WaRlSU 'hand' 
ga defendam, president .... 
we money, nlelllO, bribe .... 
ni person, suspect, ... 
de affair, office, room .... 
(6) Deutsch-go me 
/ Gcl'nlan also q 
hHI I ( IS I I  
speak " -7  
{';cq/;i'i;ir 
a teacher who speaks also German) 
~ professor, president 1
ni person, friend .... 
- -  we reason English Japanese 
to (sentence) 
Figure 1: Matching of an inl)ut sentence and a case fl:ame. 
Even though a 3,600,000 sentences corpus was 
used for learning, examples in case slots are still 
sparse, and an input noun mostly does not match 
exactly an example in the dictionary. Then, a 
thesaurus i employed to solve this problem. 
In our experiments, NTT Semantic Feature 
Dictionary (Ikehara et al, 1997) is employed as 
a thesaurus. Suppose we calculate the silnilar- 
ity between Wl and w2, their depth is dl and d2 
in the thesaurus, and the depth of their lowest 
(most specitic) common ode is de, the similarity 
score between them is calculated as follows: 
= (4  ? + 
If W 1 and w2 are in the same node of the the- 
saurus, the similarity is 1.0, the maximum score 
based on this criteria. If Wl and w2 are identical, 
the similarity is 1.0, of course. 
The score of case assigmnent is the best sim- 
ilarity between the input noun and examples in 
the case slots. The score of a matching pattern 
is the sum of scores of case assignments in it. If 
two or more patterns meet the above conditions, 
one which has the best score is selected as a final 
result. 
In the case of sentence 5 in Figure 1, karc 7ti 
'he dativc-CM' is assigned to the ni case slot. 
Then, syorui wa 'document topic-marker' can be 
assigned to the ga or wo case slot. By calculating 
similarity between syorui and 9a-slot examples 
and wo-slot exmnples, it; is considered to be as- 
signed to the wo slot. 
In case of sentence 6, none of the case compo- 
nents has a CM. Based on similarity calculation, 
Deutsch,-go is assigned to 'wo, sensei is assigned 
to ga. 
4.2 Pars ing with case structure analysis 
A complex sentence which contains a clausal 
modifier exhit)its a typical structural ambiguity 
of Japanese; case components left to a verb of 
a clausal modifier, Vc, possibly modify V~: or a 
matrix verb Vm. 
For example, in sentence 3, ir'o 'w~L 'color 
topic-inarker' possibly modifies kite-iru 'wear' or 
(l,~l)(\],Sel'~l, q lar i i l on ize ' .  
KNP, a rule-based parser, handles this type of 
ambiguity ~s follows. If a case component is fol- 
lowed by a comma, it is treated as modif\[ying Vm ; 
if not, it is treated as modif\[ying 1~:. Although 
this heuristic rule usually explains real data very 
well, sentence 3 will be analyzed incorrectly. 
Parsing which utilizes a case frame dictionary 
can consider which is a proper head, V~ or Vm, 
tbr an ambiguous case compolmnt by comparing 
examples in the case slots of V~ and 14~. Such 
a consideration nmst be done considering wlmt 
other case components modifly Vc and Vm, since 
the assigned case slot of a case component might 
differ depending on the candidate structure of 
the sentence due to the unique case assignment 
constraint. 
Therefore, it is necessary to expand the struc- 
tural ambiguity and consider all the possible 
structures fbr an input. So, we calculate the 
matching score of all pairs of case components 
and verbs in all possible structures of the sen- 
tence, and select the best structure based on the 
436 
boushi tic; ire wa 
hat color 
bott.~\]ti no
hal ~,~ 
~-- - __  ire wa 
co lo r - -  I 
C(;?lt It\[ 
COat ~5"?rll 
harlllOlliZe 
We ClOth, tllli\]'/)l'lll, CI)la .... ~ WO \] pOWCI', face, }l}ind 
~ . I  
I de party, oily, home .... ni I prcl)lcnce, cItlth .... 
kite-it'lt Co(It I1i HWCLTCI'll 
weal" coat hllrnlolliZC 
hat 
i re wa -2 (distance penalty) 
c? l ? r  ~ \] 
kite-iru \[ 
co.t ,,i ~ 
~ COat IdilA't!rll 
k i ru  'wear' ~. \  awa,~emt 'harllloilizc' 
2 ,:2,,,,,,6 ....
Figure 2: Parsing with east structure analysis. 
sum of the matching scores in it. 
Since the heuristic rule employed ill KNP is 
actually very useful, we in(:orporate it, that is, 
l)enalty score is imposed to the modifier-hea(l re- 
la.tion depending on the distraint between ~t mod- 
ifi(;l" and a head. If a moditier is not followed by 
a comma, the penalty score, 0 , -2 , -4 , -6 ,  ... is 
imposed when a moditler modifie.s the first (nea.r- 
est), second, third, tburth, ... verbs ill a sentence 
respectively; if with a comma, the tmnalty score, 
-2 ,  0, -2,  -4,  ... is impose& 
For example, sentence 3 was analyzed t)y our 
method as shown ill Figure 2. Since the simi- 
lm'ity score between fro ~color' a.nd the 'we-slot 
of uwa.s'cr'u hmunonize is nmch larger t;\]iall theft 
l)etween ire 'eoloff and the ga-slot of lci'r'u. 'wear', 
the correct structure of the selltellee was de- 
tected (the right-lmnd parse of Figure 2). Note 
that, furthermore, both the ease of ire ill reb> 
tion to awascru  'harmonize', and the case of coal, 
in relation to kite-iru 'wear' were dete(:ted cor- 
rectly. 
Structm'al ambiguities often cause a combina- 
torial explosion when a sentence is long. How- 
ever, by detecting the SeOl)eS of coordinate struc- 
tures 1)e%rehand, which off;ell aPl)ear in long 
'l'td)le 3: The at:curacy of case detection. 
(;orre(:I; ill(:orl'e(;t 
\])arsing ease case 
er ror  (lel;e(:l;ion (tete(:tion 
topic-marleer 82 13 5 
clausal modifier 73 18 9 
senl;ences, we can reasonably limit the possil)le 
sl;ructures of the sentence. 
The ~werage analysis peed of tile ext)criments 
described in the next section was about 50 sen- 
tenets/aria. 'File tinm-oul, of one rain. was only 
employed to 7 out of 4,272 test Selltellces. 
4.3 Exper i lnents  and discuss ion 
We used 4,272 sentences of Kyoto University co l  
pus as a test set. We parsed them by our new 
lnethod (Figure 3 shows several examt)les) and 
cheekc, d two 1)oints: case detection of mnbiguous 
case (-omponents and syntactic analysis. 
First, we randomly selected ambiguous ease 
components: 100 l,ol)ic-markcA case components 
all(t 100 (:ase coral)orients moditied by clausal 
437 
ookllrasyo ha 
the Treasury 
3gatsuki kes:~an de 
settlenlellt ill March ,, 
.l'hintakuginkotl kakukmt ga I impr<n'ed bycase iajbmtatiml 
each trust bank 
tsttmitareteirtt 
save lip 
tokubetsu t3,uhokil~ ,1o 
specially reserved money 
mrikuzushi wo 
collstllllptioll 
gai,,'yoltha 
the Foreign Minislcr 
m 
tnitonwru 
allow 
hm~shhula. 
policy 
mikka ni 
on tile third \] 
4 Mexico ga Mexico 
\[Iglppyolt shiRl imln'ored hy cave i@~tmltli?~*i 
illltl(RlnCC ~ ~ \ ]  
i@lre Ixmshi mulo ? 
prevention o1" inl\]alion \[ 
I 
L'eizai misaku ni 
fimmclal pllllcy 
t,wtite 
al~(Rl\[ 
st'Lvlltttl'i ~hifa 
cxphdn 
Figure 3: Exmnt)les of the mmlysis results. 
modifiers, and checked whether their cases were 
correctly detected or not. As shown in Table 3, 
the accuracy of the analysis was fairly good: that 
tbr topic-markers was 82% and that tbr clausal 
modifiers was 73%. 
Then, we compared the parse results of our 
method with those of the original KNP. As a re- 
sult, 565 modifier-head relations differed; in 260 
cases, our method was correct and the original 
KNP was incorrect (by considering the struc- 
tures in the Kyoto University Corpus as a golden 
standard); in 224 cases, vice versa. That is, 
our method was superior to KNP by 36 cases, 
and increased the overall accuracy from 89.8% 
to 89.9%. Since the heuristic rule used in KNP 
is very strong, the improvement was not big. 
The improvement of the accuracy, though small, 
is valuable, because the accuracy around 90% 
seems close to the ceiling of this task. 
5 Conc lus ion  
We proposed an unsupervised construction 
method of a case frame dictionary. We obtained 
a large case fl'alne dictionary, which consists 
of 23,497 verbs. Using this dictionary, we can 
detect ambiguous case components accurately. 
Also since our method employs unsupervised dic- 
tionary learning, it can be easily scaled up. 
Re ferences  
Ted Briscoe and John Carroll. 1997. Automatic 
extraction of subcategorization from corpora. 
In Prvccedings of ANLP-97. 
Satoru Ikehara, Masahiro Miyazaki, Satoshi 
Shirai, Akio Yokoo, Hiromi Nakaiwa, Ken- 
tarou Ogura, and Yoshiflmfi Oyama Yoshi- 
hiko Hayashi, editors. 1997. Japanese Lexi- 
con. Iwananfi Publishing. 
Information-q~chnology Promotion Agency, 
,Japan. 1987. Japanese Verbs : A Guide to 
the H~A Lea:icon of Basic ,Japa~tcsc Verbs. 
S. Kurohashi and M. Nagao. 1994. A syntac- 
tic analysis method of long japanese sentences 
based on the detection of conjunctive struc- 
tures. Computational Linguistics, 20(4). 
S. Kurohashi and M. Nagao. 1998. Build- 
ing a jal)anese parsed corpus while improv- 
ing the t)arsing system. In Prvcccdin.qs of" Th.c 
Fir;st h~,tcr'national Co't@r~ncc on Lwnguage 
R.csources 64 Evaluation, pages 719 724. 
Christopher D. Maturing. 1993. Automatic ac- 
quisition of a large snbcategorization dictio- 
nary froln corpora. In Pr'occcding s of A CL-93. 
Takehito Utsuro, Takashi Miyata, and Yuji Mat- 
sumoto. 1998. General-to-simeific model se- 
lection tbr subcategorization preference. In 
Proceedings of th.c 17th International ConJ'cr- 
cncc on Computational Li'n.quistics and the 
36th Annual Mectin.q of the Association for 
Computational Lin.quistics. 
438 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 401?408
Manchester, August 2008
Using Hidden Markov Random Fields to Combine Distributional and
Pattern-based Word Clustering
Nobuhiro Kaji and Masaru Kitsuregawa
Institute of Industrial Science, University of Tokyo
4-6-1 Komaba, Meguro-ku, Tokyo 153-8505 Japan
{kaji,kitsure}@tkl.iis.u-tokyo.ac.jp
Abstract
Word clustering is a conventional and im-
portant NLP task, and the literature has
suggested two kinds of approaches to this
problem. One is based on the distribu-
tional similarity and the other relies on
the co-occurrence of two words in lexico-
syntactic patterns. Although the two meth-
ods have been discussed separately, it is
promising to combine them since they are
complementary with each other. This pa-
per proposes to integrate them using hid-
den Markov random fields and demon-
strates its effectiveness through experi-
ments.
1 Introduction
Word clustering is a technique of grouping similar
words together, and it is important for various NLP
systems. Applications of word clustering include
language modeling (Brown et al, 1992), text clas-
sification (Baker and McCallum, 1998), thesaurus
construction (Lin, 1998) and so on. Furthermore,
recent studies revealed that word clustering is use-
ful for semi-supervised learning in NLP (Miller et
al., 2004; Li and McCallum, 2005; Kazama and
Torisawa, 2008; Koo et al, 2008).
A well-known approach to grouping similar
words is to use distribution of contexts in which
target words appear. It is founded on the hypothe-
sis that similar words tend to appear in similar con-
texts (Harris, 1968). Based on this idea, some stud-
ies proposed probabilistic models for word cluster-
ing (Pereira et al, 1993; Li and Abe, 1998; Rooth
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
et al, 1999; Torisawa, 2002). Others proposed
distributional similarity measures between words
(Hindle, 1990; Lin, 1998; Lee, 1999; Weeds et al,
2004). Once such similarity is defined, it is trivial
to perform clustering.
On the other hand, some researchers utilized
co-occurrence for word clustering. The idea be-
hind it is that similar words tend to co-occur in
certain patterns. Considerable efforts have been
devoted to measure word similarity based on co-
occurrence frequency of two words in a window
(Church and Hanks, 1989; Turney, 2001; Terra and
Clarke, 2003; Matsuo et al, 2006). In addition to
the classical window-based technique, some stud-
ies investigated the use of lexico-syntactic patterns
(e.g., X or Y) to get more accurate co-occurrence
statistics (Chilovski and Pantel, 2004; Bollegala et
al., 2007).
These two approaches are complementary with
each other, because they are founded on different
hypotheses and utilize different corpus statistics.
Consider to cluster a set of words based on the dis-
tributional similarity. It is likely that some words
are difficult to cluster due to the data sparseness or
some other problems, while we can still expect that
those words are correctly classified using patterns.
This consideration leads us to combine distribu-
tional and pattern-based word clustering. In this
paper we propose to combine them using mixture
models based on hidden Markov random fields.
This model was originally proposed by (Basu et
al., 2004) for semi-supervised clustering. In semi-
supervised clustering, the system is provided with
supervision in the form of pair-wise constraints
specifying data points that are likely to belong to
the same cluster. These constraints are directly in-
corporated into the clustering process as a prior
knowledge. Our idea is to view the co-occurrence
401
of two words in lexico-syntactic patterns as con-
straints, and incorporate them into distributional
word clustering.
In summary, this paper discusses the problem
of integrating multiple approaches for word clus-
tering. We consider that the clustering results are
improved if multiple approaches are successfully
combined and if they are complementary with each
other. Our contribution is to provide a proba-
bilistic framework for this problem. Although our
proposal aims at combining the distributional and
pattern-based approaches, it is also applicable to
combine other approaches like (Lin et al, 2003),
as we will discuss in Section 5.4.
2 Distributional Clustering
This and next section describe distributional and
pattern-based word clustering respectively. Sec-
tion 4 will explain how to combine them.
2.1 Probabilistic model
In distributional word clustering, similarity be-
tween words (= nouns) is measured by the distribu-
tion of contexts in which they appear. As a context,
verbs that appear in certain grammatical relations
with the target nouns are typically used. Using the
distribution of such verbs, we can express a noun
n by a feature vector ?(n):
?(n) = (f
nv
1
, f
nv
2
, ...f
nv
V
)
where f
nv
i
denotes the frequency of noun-verb
pair (n, v
i
), and V denotes the number of distinct
verbs. The basic idea of using the distribution for
clustering is to group n and n? together if?(n) and
?(n
?
) are similar.
Let us consider a soft clustering model. We hy-
pothesize that ?(n) is a mixture of multinomial,
and the probability of n is defined by1
p(n) =
Z
?
z=1
p(z)p(?(n)|z)
=
Z
?
z=1
?
z
f
n
!
?
v
f
nv
!
?
v
?
f
nv
vz
where Z is the number of mixture components,
?
z
is the mixing coefficient (?
z
?
z
= 1), f
n
=
?
v
f
nv
is the total number of occurrence of n, and
1We ignored p(f
n
) by assuming that it is independent of
hidden variables. See (McCallum and Nigam, 1998) for detail
discussion.
?
vz
is the parameter of the multinomial distribu-
tion (?
v
?
vz
= 1). In this model the hidden vari-
ables can be interpreted as semantic class of nouns.
Now consider a set of nouns n = {n
i
}
N
i=1
. Let
z = {z
i
}
N
i=1
be a set of hidden variables corre-
sponding to n. Assuming that the hidden variables
are independent and n
i
is also independent of other
nouns given the hidden variables, the probability of
n is defined by
p(n) =
?
z
p(z)p(n|z)
where
p(z) =
N
?
i=1
p(z
i
)
p(n|z) =
N
?
i=1
p(n
i
|z
i
).
Hereafter, we use p(n|z) instead of p(?(n)|z) to
keep the notation simple. p(n|z) is the conditional
distribution on all nouns given all the hidden vari-
ables, and p(z) is the prior distribution on the hid-
den variables. Computing the log-likelihood of the
complete data (n, z), we found
log p(n, z) =
N
?
i=1
log p(z
i
)p(n
i
|z
i
). (1)
2.2 Parameter estimation
The parameters can be estimated by the EM algo-
rithm. In the E-step, p(z
i
|n
i
) is computed based
on current parameters. It is computed by
p(z
i
= k|n
i
) =
p(z
i
= k)p(n
i
|z
i
= k)
?
z
p(z)p(n
i
|z)
=
?
k
?
v
?
f
n
i
v
vk
?
z
?
z
?
v
?
f
n
i
v
vz
.
In the M-step, the parameters are re-estimated by
using the result of the E-step:
?
?k
=
? +
?
i
f
n
i
?
p(z
i
= k|n
i
)
?V +
?
v
?
i
f
n
i
v
p(z
i
= k|n
i
)
?
k
=
? +
?
i
p(z
i
= k|n
i
)
?Z +
?
z
?
i
p(z
i
= z|n
i
)
where ? is a smoothing factor.2 Both steps are
repeated until a convergence criteria is satisfied.
The important point to note is that the E-step can
be computed using the above equation because the
hidden variables are independent.
2
?=1.0 in our experiment.
402
X ya Y X mo Y mo X to Y to X, Y nado
(X or Y) (X and Y) (X and Y) (X, Y etc.)
Table 1: Four lexico-syntactic patterns, where X
and Y are extracted as co-occurring words. Note
that ya, mo, and to are Japanese postpositions, and
they correspond to or or and in English.
3 Pattern-based Clustering
A graph-based algorithm was employed in order to
cluster words using patterns.
3.1 Graph Construction
We first construct the graph in which vertices
and edges correspond to words and their co-
occurrences in patterns respectively (Figure 1). We
employed four lexico-syntactic patterns (Table 1)
to extract co-occurrence of two words from cor-
pus. Note that we target Japanese in this paper al-
though our proposal is independent of languages.
The edges are weighted by the strength of co-
occurrence that is computed by the Point-wise Mu-
tual Information (PMI):
PMI(n
i
, n
j
) = log
f(n
i
, n
j
)f(?, ?)
f(n
i
, ?)f(?, n
j
)
where f(n
i
, n
j
) is the co-occurrence frequency
of two nouns, and ??? means summation over all
nouns. If PMI is less than zero, the edge is re-
moved.
3.2 Graph Partitioning
Assuming that similar words tend to co-occur in
the lexico-syntactic patterns, it is reasonable to
consider that a dense subgraph is a good cluster
(Figure 1). Following (Matsuo et al, 2006), we
exploit the Newman clustering (Newman, 2004) to
partition the graph into such dense subgraphs.
We start by describing Newman?s algorithm for
unweighted graphs and we will generalize it to
weighted graphs later. The Newman clustering is
an algorithm that divides a graph into subgraphs
based on connectivity. Roughly speaking, it di-
vides a graph such that there are a lot of edges be-
tween vertices in the same cluster. In the algorithm
goodness of clustering is measured by score Q:
Q =
?
i
(
e
ii
? a
2
i
)
ramen
dumpling
pasta
steak
Japan U.S.A.
Germany
China
France
Figure 1: An example of the graph consisting of
two dense subgraphs.
where
e
ij
=
# of edges between two vertices in cluster i and j
# of all edges
a
i
=
?
k
e
ik
.
The term e
ij
is the fraction of edges between clus-
ter i and j. a
i
is the sum of e
ik
over all clusters,
and a2
i
represents the expected number of fraction
of edges within the cluster i when edges are given
at random. See (Newman, 2004) for the detail.
The Newman clustering optimizes Q in an ag-
glomerative fashion. At the beginning of the algo-
rithm every vertex forms a singleton cluster, and
we repeatedly merge two clusters so that the join
results in the largest increase in Q. The change in
Q when cluster i and j are merged is given by
?Q = e
ij
+ e
ji
? 2a
i
a
j
= 2(e
ij
? a
i
a
j
).
The above procedure is repeated until Q reaches
local maximum.
The algorithm can be easily generalized to
weighted graphs by substituting ?sum of weights
of edges? for ?# of edges? in the definition of e
ij
.
The other part of the algorithm remains the same.
4 Integration based on Hidden Markov
Random Fields
This section represents how to integrate the distri-
bution and pattern for word clustering.
4.1 Background and idea
Clustering has long been discussed as an unsu-
pervised learning problem. In some applications,
however, it is possible to provide some form of
supervision by hand in order to improve the clus-
tering result. This motivated researchers to inves-
tigate semi-supervised clustering, which uses not
only unlabeled data but supervision in the form of
pair-wise constraints (Basu et al, 2004). In this
403
framework, the clustering system is provided with
a set of pair-wise constraints specifying data points
that are likely to belong to the same cluster. These
constraints are directly incorporated into the clus-
tering process as a prior knowledge.
Our idea is to view the co-occurrence of two
words in lexico-syntactic patterns as constraints,
and incorporate them into the distributional clus-
tering. The rest of this section describes how to ex-
tend the distributional clustering so as to incorpo-
rate the constraints, and how to generate the con-
straints using the patterns.
4.2 Probabilistic model
Let C be a set of pair-wise constraints, and con-
sider to incorporate the constraints into the distri-
butional clustering (Section 2). In what follows we
assume each constraint ?i, j? ? C represents that
z
i
and z
j
are likely to have the same value, and it is
associated with a weight w
ij
(> 0) corresponding
to a penalty for constraint violation.
It is easy to extend the distributional cluster-
ing algorithm so as to incorporate the constraints.
This is done by just changing the prior distribution
on hidden variables p(z). Following (Basu et al,
2004), we construct the Markov random field on
the hidden variables so as to incorporate the con-
straints. The new prior distribution is defined as
p(z) =
N
?
i=1
p(z
i
) ?
1
G
exp{?
?
?i,j??C
?(z
i
6= z
j
)w
ij
}
where ?(?) is the delta function. ?(z
i
6= z
j
) takes
one if the constraint ?i, j? is violated and otherwise
zero. G is the normalization factor of the Markov
random field (the second term).
By examining the log-likelihood of the complete
data, we can see how violation of constraints is pe-
nalized. Using the new prior distribution, we get
log p(n, z) =
N
?
i=1
log p(z
i
)p(n
i
|z
i
)
?
?
?i,j??C
?(z
i
6= z
j
)w
ij
? log G.
The first term in the right-hand side is equal to the
log-likelihood of the multinomial mixture, namely
equation (1). The second term can be interpreted
as the penalty for constraint violation. The last
term is a constant.
It is worth pointing out that the resulting algo-
rithm makes a soft assignment and polysemous
words can belong to more than one clusters.
4.3 Parameter estimation
The parameters are estimated by the EM algo-
rithm. The M-step is exactly the same as discussed
in Section 2.2. The problem is that the hidden vari-
ables are no longer independent and the E-step re-
quires the calculation of
p(z
i
|n) =
?
z
?i
p(z
?i
, z
i
|n)
?
?
z
?i
p(z
?i
, z
i
)p(n|z
?i
, z
i
)
where z
?i
means all hidden variables but z
i
. The
computation of the above equation is intractable
because the summation in it requires O(ZN?1) op-
erations.
Instead of exactly computing p(z
i
|n), we ap-
proximate it by using the mean field approximation
(Lange et al, 2005). In the mean field approxima-
tion, p(z|n) is approximated by a factorized distri-
bution q(z), in which all hidden variables are inde-
pendent:
q(z) =
N
?
i=1
q
i
(z
i
). (2)
Using q(z) instead of p(z|n), computation of the
E-step can be written as follows:
p(z
i
|n) '
?
z
?i
q(z
?i
, z
i
) = q
i
(z
i
). (3)
The parameters of q(z) are determined such that
the KL divergence between q(z) and p(z|n) is
minimized. In other words, the approximate dis-
tribution q(z) is determined by minimizing
?
z
q(z) log
q(z)
p(z|n)
(4)
under the condition that
?
k
q
i
(z
i
= k) = 1
for all i. This optimization problem can be re-
solved by introducing Lagrange multipliers. Be-
cause we cannot get the solution in closed form, an
iterative method is employed. Taking the deriva-
tive of equation (4) with respect to a parameter
q
ik
= q
i
(z
i
= k) and setting it to zero, we get
the following updating formula:
q
(t+1)
ik
? p(n
i
, k) exp{?
?
j?N
i
(1 ? q
(t)
jk
)w
ij
} (5)
404
where N
i
= {j|?i, j? ? C} and q(t)
ik
is the value of
q
ik
at t-th iteration. The derivation of this formula
is found in Appendix.
4.4 Generation of constraints
It is often pointed out that even small amounts of
misspecified constraints significantly decrease the
performance of semi-supervised clustering. This
is because the error of misspecified constraints is
propagated to the entire transitive neighborhoods
of the constrained data (Nelson and Cohen, 2007).
As an example, consider that we have two con-
straints ?i, j? and ?j, k?. If the former is misspeci-
fied one, the error propagate to k through j.
To tackle this problem, we propose a technique
to put an upper bound ? on the size of the transitive
neighborhoods. Our constraint generation process
is as follows. To begin with, we modified the New-
man clustering so that the maximum cluster size
does not exceed ?. This can be done by prohibit-
ing such merge that results in larger cluster than
?. Given the result of the modified Newman clus-
tering, it is straightforward to generate constraints.
Constraints are generated between two nouns in
the same cluster if they co-occur in the lexico-
syntactic patterns at least one time. The penalty
for constraint violation w
ij
was set to PMI(n
i
, n
j
).
This procedure obviously ensures that the size of
the transitive neighborhoods is less than ?.
5 Experiments
5.1 Data sets
We parsed 15 years of news articles by KNP3 so
as to obtain data sets for the distributional and
pattern-based word clustering (Table 2). The num-
ber of distinct nouns in total was 297,719. Note
that, due to the computational efficiency, we re-
moved such nouns that appeared less than 10 times
with verbs and did not appear at all in the patterns.
A test set was created using manually tailored
Japanese thesaurus (Ikehara et al, 1997). We ran-
domly selected 500 unambiguous nouns from 25
categories (20 words for each category).
5.2 Baselines
For comparison we implemented the following
baseline systems.
? The multinomial mixture (Section 2).
? The Newman clustering (Newman, 2004).
3http://nlp.kuee.kyoto-u.ac.jp/nl-resource/
nouns 208,934
verbs 64,954
noun-verb pairs 4,804,715
nouns 245,465
noun-noun pairs 633,302
Table 2: Data sets statistics. The first and second
row shows the number of distinct words (and word
pairs) used for the distributional and pattern-based
word clustering respectively.
? Three K-means algorithms using different
distributional similarity or dissimilarity mea-
sures: cosine, ?-skew divergence (Lee,
1999)4, and Lin?s similarity (Lin, 1998).
? The CBC algorithm (Lin and Pantel, 2002;
Pantel and Lin, 2002).
5.3 Evaluation procedure
All the nouns in the data set were clustered by the
proposed and baseline systems.5 For the mixture
models and K-means, the number of clusters was
set to 1,000. The parameter ? was set to 100.
The result was assessed by precision and recall
using the test data. The precision and recall were
computed by the B-CUBED algorithm as follows
(Bagga and Baldwin, 1998). For each noun n
i
in
the test data, precision
i
and recall
i
are defined as
precision
i
=
|S
i
? T
i
|
|S
i
|
recall
i
=
|S
i
? T
i
|
| T
i
|
where S
i
is the system generated cluster contain-
ing n
i
and T
i
is the goldstandard cluster containing
n
i
. The precision and recall are defined as an av-
erage of precision
i
and recall
i
for all the nouns in
the test data respectively. The result of soft clus-
tering models cannot be directly evaluated by the
precision and recall. In such cases, each noun is
assigned to the cluster that maximizes p(z|n).
5.4 The result and discussion
Table 3 shows the experimental results. The best
results for each statistic are shown in bold. For the
mixture models and K-means, the precision and re-
call are an average of 10 trials.
Table 3 demonstrates the impact of combining
distribution and pattern. Our method outperformed
4
? = 0.99 in our experiment.
5Our implementation is available from
http://www.tkl.iis.u-tokyo.ac.jp/?kaji/clustering.
405
P R F
1
proposed .383 .437 .408
multinomial mixture .360 .374 .367
Newman (2004) .318 .353 .334
cosine .603 .114 .192
?-skew divergence (Lee, 1999) .730 .155 .255
Lin?s similarity (Lin, 1998) .691 .096 .169
CBC (Lin and Pantel, 2002) .981 .060 .114
Table 3: Precision, recall, and F-measure.
all the baseline systems. It was statistically signif-
icantly better than the multinomial mixture (P <
0.01, Mann-Whitney U-test). Note that it is possi-
ble to improve some baseline systems, especially
CBC, by tuning the parameters. For CBC we sim-
ply used the same parameter values as reported in
(Lin and Pantel, 2002).
Compared with the multinomial mixture, one
advantage of our method is that it has broad cov-
erage. Our method can successfully handle un-
known words, which do not appear with verbs at
all (i.e., f
n
= 0 and ?(n) is zero vector), if they
co-occur with other words in the lexico-syntactic
patterns. For unknown words, the hidden variables
are determined based only on p(z) because p(n|z)
takes the same value for all hidden variables. This
means that our method clusters unknown words
using pair-wise constraints. On the other hand,
the multinomial mixture assigns all the unknown
words to the cluster that maximizes p(z).
The test set included 51 unknown words.6 We
split the test set into two parts: f
n
= 0 and f
n
6= 0,
and calculated precision and recall for each subset
(Table 4). Although the improvement is especially
significant for the unknown words, we can clearly
confirm the improvement for both subsets. For the
Newman clustering we can discuss similar things
(Table 5). Different from the Newman clustering,
our method can handle nouns that do not co-occur
with other nouns if 0 < f
n
. In this case the test set
included 64 unknown words.
It is interesting to point out that our framework
can further incorporate lexico-syntactic patterns
for dissimilar words (Lin et al, 2003). Namely,
we can use patterns so as to prevent distribution-
ally similar but semantically different words (e.g.,
ally and supporter (Lin et al, 2003)) from being as-
signed to the same cluster. This can be achieved by
using cannot-link constraints, which specify data
points that are likely to belong to different clus-
6The baseline systems assigned the unknown words to a
default cluster as the multinomial mixture does.
f
n
= 0 f
n
6= 0
P R F
1
P R F
1
proposed .320 .632 .435 .412 .450 .430
multi. .099 1.000 .181 .402 .394 .398
Table 4: Detail comparison with the multinomial
mixture.
f(n
i
, ?) = 0 f(n
i
, ?) 6= 0
P R F
1
P R F
1
proposed .600 .456 .518 .380 .479 .424
Newman .071 1.000 .133 .354 .412 .381
Table 5: Detail comparison with the Newman clus-
tering.
ters (Basu et al, 2004). The remaining problem
is which patterns to use so as to extract dissimilar
words. Although this problem has already been
discussed by (Lin et al, 2003), they mainly ad-
dressed antonyms. We believe that a more exhaus-
tive investigation is required. In addition, it is still
unclear whether dissimilar words are really useful
to improve clustering results.
One problem that we did not examine is how to
determine optimal number of clusters. In the ex-
periment, the number was decided with trial-and-
error through our initial experiment. We leave it
as our future work to test methods of automat-
ically determining the cluster number (Pedersen
and Kulkarni, 2006; Blei and Jordan, 2006).
6 Related work
As far as we know, the distributional and pattern-
based word clustering have been discussed inde-
pendently (e.g., (Pazienza et al, 2006)). One of
the most relevant work is (Bollegala et al, 2007),
which proposed to integrate various patterns in or-
der to measure semantic similarity between words.
Although they extensively discussed the use of pat-
terns, they did not address the distributional ap-
proach.
Mirkin (2006) pointed out the importance of
integrating distributional similarity and lexico-
syntactic patterns, and showed how to combine the
two approaches for textual entailment acquisition.
Although their work inspired our research, we dis-
cussed word clustering, which is related to but dif-
ferent from entailment acquisition.
Lin (2003) also proposed to use both distribu-
tional similarity and lexico-syntactic patterns for
finding synonyms. However, they present an oppo-
site viewpoint from our research. Their proposal
is to exploit patterns in order to filter dissimilar
406
words. As we have already discussed, the integra-
tion of such patterns can also be formalized using
similar probabilistic model to ours.
A variety of studies discussed determining po-
larity of words. Because this problem is ternary
(positive, negative, and neutral) classification of
words, it can be seen as one kind of word clus-
tering. The literature suggested two methods of
determining polarity, and they are analogous to the
distributional and co-occurrence-based approaches
in word clustering (Takamura et al, 2005; Hi-
gashiyama et al, 2008). We consider it is also
promising to integrate them for polarity determi-
nation.
7 Conclusion
The distributional and pattern-based word cluster-
ing have long been discussed separately despite
the potentiality for their integration. In this paper,
we provided a probabilistic framework for com-
bining the two approaches, and demonstrated that
the clustering result is significantly improved.
Our important future work is to extend current
framework so as to incorporate patterns for dissim-
ilar words using cannot-link constraints. We con-
sider such patterns further improve the clustering
result.
Combining distribution and pattern is important
for other NLP problems as well (e.g., entailment
acquisition, polarity determination). Although this
paper examined word clustering, we consider a
part of our idea can be applied to other problems.
Acknowledgement
This work was supported by the Comprehensive
Development of e-Society Foundation Software
program of the Ministry of Education, Culture,
Sports, Science and Technology, Japan.
References
Bagga, Amit and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector space
model. In Proceedings of ACL, pages 79?85.
Baker, L. Douglas and Andrew Kachites McCallum.
1998. Distributional clustering of words for text
classification. In Proceedings of SIGIR, pages 96?
103.
Basu, Sugato, Mikhail Bilenko, and Raymond J.
Mooney. 2004. A probabilistic framework for semi-
supervised clustering. In Proceedings of SIGKDD,
pages 59?68.
Blei, David M. and Michael I. Jordan. 2006. Vari-
ational inference for Dirichlet process mixtures.
Bayesian Analysis, 1(1):121?144.
Bollegala, Danushka, Yutaka Matsuo, and Mitsuru
Ishizuka. 2007. An integrated approach to mea-
suring semantic similarity between words using in-
formation available on the web. In Proceedings of
NAACL, pages 340?347.
Brown, Peter F., Vincent J. Della Pietra, Peter V. deS-
ouza, Jenifer C. Lai, and Rober L. Mercer. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467?479.
Chilovski, Timothy and Patrick Pantel. 2004. VER-
BOCEAN: Mining the web for fine-grained semantic
verb relations. In Proceedings of EMNLP, pages 33?
40.
Church, KennethWard and Patrick Hanks. 1989. Word
association norms, mutual information, and lexicog-
raphy. In Proceedings of ACL, pages 76?83.
Harris, Zellig. 1968. Mathematical Structure of Lan-
guage. New York: Wiley.
Higashiyama, Masahiko, Kentaro Inui, and Yuji Mat-
sumoto. 2008. Learning polarity of nouns by se-
lectional preferences of predicates (in Japanese). In
Proceedings of the Association for NLP, pages 584?
587.
Hindle, Donald. 1990. Noun classification from
predicate-argument structure. In Proceedings of
ACL, pages 268?275.
Ikehara, Satoru, Masahiro Miyazaki, Satoshi Shirai,
Akio Yokoo, Hiromi Nakaiwa, Kentarou Ogura,
and Yoshifumi Oyama Yoshihiko Hayashi, editors.
1997. Japanese Lexicon. Iwanami Publishing.
Kazama, Jun?ichi and Kentaro Torisawa. 2008. Induc-
ing gazetteers for named entity recognition by large-
scale clustering of dependency relations. In Pro-
ceedings of ACL, pages 407?415.
Koo, Terry, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL, pages 595?603.
Lange, Tilman, Martin H.C. Law, Anil K. Jain, and
Joachim M. Buhmann. 2005. Learning with con-
strained and unlabelled data. In Proceedings of
CVPR, pages 731?738.
Lee, Lillian. 1999. Measures of distributional similar-
ity. In Proceedings of ACL, pages 25?32.
Li, Hang and Naoki Abe. 1998. Word clustering and
disambiguation based on co-occurrence. InProceed-
ings of ACL-COLING, pages 749?755.
Li, Wei and Andrew McCallum. 2005. Semi-
supervised sequence modeling with syntactic topic
models. In Proceedings of AAAI, pages 813?818.
407
Lin, Dekang and Patrick Pantel. 2002. Concept discov-
ery from text. In Proceeodings of COLING, pages
577?583.
Lin, Dekang, Shaojun Zhao, Lijuan Qin, and Ming
Zhou. 2003. Identifying synonyms among distri-
butionally similar words. In Proceedings of IJCAI,
pages 1492?1493.
Lin, Dekang. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of ACL-COLING,
pages 768?774.
Matsuo, Yutaka, Takeshi Sakaki, Koki Uchiyama, and
Mitsuru Ishizuka. 2006. Graph-based word cluster-
ing using a web search engine. In Proceedings of
EMNLP, pages 542?550.
McCallum, Andrew and Kamal Nigam. 1998. A com-
parison of event models for naive Bayes text classifi-
cation. In Proceedings of AAAI Workshop on Learn-
ing for Text Categorization, pages 41?48.
Miller, Scott, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In Proceedings of NAACL, pages
579?586.
Mirkin, Shachar, Ido Dagan, andMaayan Geffet. 2006.
Integrating pattern-based and distributional similar-
ity methods for lexical entailment acquisition. In
Proceedings of COLING-ACL Poster Sessions, pages
579?586.
Nelson, Blaine and Ira Cohen. 2007. Revisiting prob-
abilistic models for clustering with pair-wise con-
straints. In Proceedings of ICML, pages 673?680.
Newman, Mark. 2004. Fast algorithm for detecting
community structure in networks. In Phys. Rev. E
69.
Pantel, Patrick and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of SIGKDD,
pages 613?619.
Pazienza, Maria Teresa, Marco Pennacchiotti, and
Fabio Massimo Zanzotto. 2006. Discovering
verb relations in corpora: Distributional versus
non-distributional approaches. In Proceedings of
IEA/AIE, pages 1042?1052.
Pedersen, Ted and Anagha Kulkarni. 2006. Automatic
cluster stopping with criterion functions and the gap
statistic. In Proceedings of HLT/NAACL, Compan-
ion Volume, pages 276?279.
Pereira, Fernando, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
Proceedings of ACL, pages 183?190.
Rooth, Mats, Stefan Riezler, Detlef Prescher, Glenn
Garrroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via EM-based clustering. In
Proceedings of ACL, pages 104?111.
Takamura, Hiroya, Takashi Inui, and Manabu Oku-
mura. 2005. Extracting semantic orientations of
words using spin model. In Proceedings of ACL,
pages 133?140.
Terra, Egidio and C.L.A. Clarke. 2003. Frequency es-
timates for statistical word similarity measures. In
Proceedings of NAACL, pages 165?172.
Torisawa, Kentaro. 2002. An unsupervised learning
method for associative relationships between verb
phrases. In Proceedings of COLING, pages 1009?
1015.
Turney, Peter. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
ECML, pages 491?502.
Weeds, Julie, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of COLING, pages 1015?
1021.
Appendix. Derivation of the updating
formula
We can rewrite equation (4) as follows:
(4) =
?
z
q(z) log q(z) (6)
?
?
z
q(z)
N
?
i=1
log p(n
i
, z
i
) (7)
+
?
z
q(z)
?
?i,j??C
?(z
i
6= z
j
)w
ij
(8)
+ const (9)
where we made use of the fact that log p(z|n) =
log p(n|z)p(z) + const. Taking the derivative of
equation (6), (7), and (8) with respect to q
ik
, we
found
?(6)
?q
ik
= log q
ik
+ const
?(7)
?q
ik
= ? log p(n
i
, k) + const
?(8)
?q
ik
=
?
z
?i
q(z
?i
)
?
j?N
i
?(z
j
6= k)w
ij
+ const
=
?
j?N
i
?
z
?i
q(z
?i
)?(z
j
6= k)w
ij
+ const
=
?
j?N
i
(1 ? q
jk
)w
ij
+ const
where const denotes terms independent of k. Mak-
ing use of these results, the updating formula can
be derived by taking the derivative of equation (4)
with respect to q
ik
and setting it to zero.
408
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1075?1083, Prague, June 2007. c?2007 Association for Computational Linguistics
Building Lexicon for Sentiment Analysis from Massive Collection of HTML
Documents
Nobuhiro Kaji and Masaru Kitsuregawa
Institute of Industrial Science, University of Tokyo
4-6-1 Komaba, Meguro-ku, Tokyo 153-8505 Japan
 kaji,kitsure@tkl.iis.u-tokyo.ac.jp
Abstract
Recognizing polarity requires a list of po-
lar words and phrases. For the purpose of
building such lexicon automatically, a lot of
studies have investigated (semi-) unsuper-
vised method of learning polarity of words
and phrases. In this paper, we explore to
use structural clues that can extract polar
sentences from Japanese HTML documents,
and build lexicon from the extracted po-
lar sentences. The key idea is to develop
the structural clues so that it achieves ex-
tremely high precision at the cost of recall.
In order to compensate for the low recall,
we used massive collection of HTML docu-
ments. Thus, we could prepare enough polar
sentence corpus.
1 Introduction
Sentiment analysis is a recent attempt to deal with
evaluative aspects of text. In sentiment analysis, one
fundamental problem is to recognize whether given
text expresses positive or negative evaluation. Such
property of text is called polarity. Recognizing po-
larity requires a list of polar words and phrases such
as ?good?, ?bad? and ?high performance? etc. For
the purpose of building such lexicon automatically, a
lot of studies have investigated (semi-) unsupervised
approach.
So far, two kinds of approaches have been pro-
posed to this problem. One is based on a the-
saurus. This method utilizes synonyms or glosses of
a thesaurus in order to determine polarity of words
(Kamps et al, 2004; Hu and Liu, 2004; Kim and
Hovy, 2004; Esuli and Sebastiani, 2005). The sec-
ond approach exploits raw corpus. Polarity is de-
cided by using co-occurrence in a corpus. This is
based on a hypothesis that polar phrases conveying
the same polarity co-occur with each other. Typ-
ically, a small set of seed polar phrases are pre-
pared, and new polar phrases are detected based on
the strength of co-occurrence with the seeds (Hatzi-
vassiloglous and McKeown, 1997; Turney, 2002;
Kanayama and Nasukawa, 2006).
As for the second approach, it depends on the
definition of co-occurrence whether the hypothe-
sis is appropriate or not. In Turney?s work, the
co-occurrence is considered as the appearance in
the same window (Turney, 2002). Although this
idea is simple and feasible, there is a room for im-
provement. According to Kanayama?s investiga-
tion, the hypothesis is appropriate in only 60% of
cases if co-occurrence is defined as the appearance
in the same window1. In Kanayama?s method, the
co-occurrence is considered as the appearance in
intra- or inter-sentential context (Kanayama and Na-
sukawa, 2006). They reported that the precision was
boosted to 72.2%, but it is still not enough. There-
fore, we think that the above hypothesis is often in-
appropriate in practice, and this fact is the biggest
obstacle to learning lexicon from corpus.
In this paper, we explore to use structural clues
that can extract polar sentences from Japanese
HTML documents, and build lexicon from the ex-
1To be exact, the precision depends on window size and
ranges from 59.7 to 64.1%. See Table 4 in (Kanayama and Na-
sukawa, 2006) for the detail.
1075
Figure 1: Overview of the proposed method.
kono
this
software-no
software-POST
riten-ha
advantage-POST
hayaku
quickly
ugoku
run
koto-desu
to-POST
The advantage of this software is to run quickly.
Figure 2: Language structure.
tracted polar sentences. An overview of the pro-
posed method is represented in Figure 1. First, po-
lar sentences are extracted from HTML documents
by using structural clues (step 1). The set of po-
lar sentences is called polar sentence corpus. Next,
from the polar sentence corpus, candidates of po-
lar phrases are extracted together with their counts
in positive and negative sentences (step 2). Finally,
polar phrases are selected from the candidates and
added to our lexicon (step 3).
The key idea is to develop the structural clues so
that it achieves extremely high precision at the cost
of recall. As we will see in Section 2.3, the precision
was extremely high. It was around 92% even if am-
biguous cases were considered as incorrect. In order
to compensate for the low recall, we used massive
collection of HTML documents. Thus, we could
build enough polar sentence corpus. To be specific,
we extracted 500,000 polar sentences from one bil-
lion HTML documents.
The contribution of this paper is to empirically
show the effectiveness of an approach that makes
use of the strength of massive data. Nowadays, ter-
abyte is not surprisingly large, and larger corpus
would be obtained in the future. Therefore, we think
this kind of research direction is important.
2 Extracting Polar Sentences
Our method begins by automatically constructing
polar sentence corpus with structural clues (step 1).
The basic idea is exploiting certain language and
layout structures as clues to extract polar sentences.
The clues were carefully chosen so that it achieves
high precision. The original idea was represented in
our previous paper (Kaji and Kitsuregawa, 2006).
2.1 Language structure
Some polar sentences are described by using char-
acteristic language structures. Figure 2 illustrates
such Japanese polar sentence attached with English
translations. Japanese are written in italics and ?-
? denotes that the word is followed by postposi-
tional particles. For example, ?software-no? means
that ?software? is followed by postpositional particle
?no?. The arrow represents dependency relationship.
Translations are shown below the Japanese sentence.
?-POST? means postpositional particle.
What characterizes this sentence is the singly un-
derlined phrase. In this phrase, ?riten (advantage)?
is followed by postpositional particle ?-ha?, which is
Japanese topic marker. And hence, we can recognize
that something positive is the topic of the sentence.
This kind of linguistic structure can be recognized
by lexico-syntactic pattern. Hereafter, such words
like ?riten (advantage)? are called cue words.
1076
In order to handle the language structures, we uti-
lized lexico-syntactic patterns as illustrated below.
riten-ha
advantage-POST
(polar) koto-desu
to-POST
A sub-tree that matches (polar) is extracted as po-
lar sentence. It is obvious whether the polar sen-
tence is positive or negative one. In case of Figure
2, the doubly underlined part is extracted as polar
sentence2.
Besides ?riten (advantage)?, other cue words were
also used. A list of cue words (and phrases) were
manually created. For example, we used ?pros? or
?good point? for positive sentences, and ?cons?, ?bad
point? or ?disadvantage? for negative ones. This list
is also used when dealing with layout structures.
2.2 Layout structure
Two kinds of layout structures are utilized as clues.
The first clue is the itemization. In Figure 3, the
itemizations have headers and they are cue words
(?pros? and ?cons?). Note that we illustrated trans-
lations for the sake of readability. By using the cue
words, we can recognize that polar sentences are de-
scribed in these itemizations.
The other clue is table structure. In Figure 4, a
car review is summarized in the table format. The
left column acts as a header and there are cue words
(?plus? and ?minus?) in that column.
Pros:
  The sound is natural.
  Music is easy to find.
  Can enjoy creating my favorite play-lists.
Cons:
  The remote controller does not have an LCD dis-
play.
  The body gets scratched and fingerprinted easily.
  The battery drains quickly when using the back-
light.
Figure 3: Itemization structure.
2To be exact, the doubly underlined part is polar clause.
However, it is called polar sentence because of the consistency
with polar sentences extracted by using layout structures.
Mileage(urban) 7.0km/litter
Mileage(highway) 9.0km/litter
Plus This is a four door car, but it?s
so cool.
Minus The seat is ragged and the light
is dark.
Figure 4: Table structure.
It is easy to extract polar sentences from the item-
ization. Such itemizations as illustrated in Figure 3
can be detected by using the list of cue words and
HTML tags such as  h1 and  ul etc. Three
positive and negative sentences are extracted respec-
tively from Figure 3.
As for table structures, two kinds of tables are
considered (Figure 5). In the Figure,   and   rep-
resent positive and negative polar sentences, and 
 
and 
 
represent cue words. Type A is a table in
which the leftmost column acts as a header. Figure
4 is categorized into this type. Type B is a table in
which the first row acts as a header.
Type A
 
 
 
 
 

Type B
 
 
 
 
  
Figure 5: Two types of table structures.
In order to extract polar sentences, first of all, it
is necessary to determine the type of the table. The
table is categorized into type A if there are cue words
in the leftmost column. The table is categorized into
type B if it is not type A and there are cue words in
the first row. After the type of the table is decided,
we can extract polar sentences from the cells that
correspond to   and   in the Figure 5.
2.3 Result of corpus construction
The method was applied to one billion HTML doc-
uments. In order to get dependency tree, we used
KNP3. As the result, 509,471 unique polar sentences
were obtained. 220,716 are positive and the others
are negative4. Table 1 illustrates some translations
of the polar sentences.
3http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html
4The polar sentence corpus is available from
http://www.tkl.iis.u-tokyo.ac.jp/?kaji/acp/.
1077
Table 1: Examples of polar sentences.
Polarity Polar sentence
It becomes easy to compute cost.
positive It?s easy and can save time.
The soup is rich and flavorful.
Cannot use mails in HTML format.
negative The lecture is really boring.
There is no impressive music.
In order to investigate the quality of the corpus,
two human judges (judge A/B) assessed 500 polar
sentences in the corpus. According to the judge
A, the precision was 91.4%. 459 out of 500 polar
sentences were regarded as valid ones. According
to the judge B, the precision was 92.0% (460/500).
The agreement between the two judges was 93.5%
(Kappa value was 0.90), and thus we can conclude
that the polar sentence corpus has enough quality
(Kaji and Kitsuregawa, 2006).
After error analysis, we found that most of the er-
rors are caused by the lack of context. The following
is a typical example.
There is much information.
This sentence is categorized into positive one in the
corpus, and it was regarded as invalid by both judges
because the polarity of this sentence is ambiguous
without context.
As we described in Section 1, the hypothesis of
co-occurrence based method is often inappropriate.
(Kanayama and Nasukawa, 2006) reported that it
was appropriate in 72.2% of cases. On the other
hand, by using extremely precise clues, we could
build polar sentence corpus that have high preci-
sion (around 92%). Although the recall of structural
clues is low, we could build large corpus by using
massive collection of HTML documents. Of course,
we cannot directly compare these two percentages.
We think, however, the high precision of 92% im-
plies the strength of our approach.
3 Acquisition of Polar Phrases
The next step is to acquire polar phrases from the
polar sentence corpus (step 2 and 3 in Figure 1).
3.1 Counting candidates
From the corpus, candidates of polar phrases are ex-
tracted together with their counts (step 2).
As is often pointed out, adjectives are often used
to express evaluative content. Considering that po-
larity of isolate adjective is sometimes ambiguous
(e.g. high), not only adjectives but also adjective
phrases (noun + postpositional particle + adjective)
are treated as candidates. Adjective phrases are ex-
tracted by the dependency parser. To handle nega-
tion, an adjective with negation words such as ?not?
is annotated by  NEGATION tag. For the sake of
readability, we simply represent adjective phrases in
the form of ?noun-adjective? by omiting postposi-
tional particle, as in the Figure 1.
For each candidate, we count the frequency in
positive and negative sentences separately. Intu-
itively, we can expect that positive phrases often ap-
pear in positive sentences, and vice versa. However,
there are exceptional cases as follows.
Although the price is high, its shape is
beautiful.
Although this sentence as a whole expresses posi-
tive evaluation and it is positive sentence, negative
phrase ?price is high? appears in it. To handle this,
we hypothesized that positive/negative phrases tend
to appear in main clause of positive/negative sen-
tences, and we exploited only main clauses to count
the frequency.
3.2 Selecting polar phrases
For each candidate, we determine numerical value
indicating the strength of polarity, which is referred
as polarity value. On the basis of this value, we se-
lect polar phrases from the candidates and add them
to our lexicon (step 3).
For each candidate , we can create a contingency
table as follows.
Table 2: Contingency table
 
 	
  	
 
 	
  	
 
 	 is the frequency of  in positive sentences.
 	 is that of all candidates but .  

and  
 are similarly decided.
From this contingency table, ?s polarity value is
determined. Two ideas are examined for compari-
1078
son. One is based on chi-square value and the other
is based on Pointwise Mutual Information (PMI).
Chi-square based polarity value The chi-square
value is a statistical measure used to test the null hy-
pothesis that, in our case, the probability of a candi-
date in positive sentences is equal to the probability
in negative sentences. Given Table 2, the chi-square
value is calculated as follows.


 
 
 
 
	

   

 


 
Here,   is the expected value of   under
the null hypothesis.
Although   indicates the strength of
bias toward positive or negative sentences, its di-
rection is not clear. We determined polarity value
so that it is greater than zero if  appears in posi-
tive sentences more frequently than in negative sen-
tences and otherwise it is less than zero.


 



  
    	
 

 	

 	 is ?s probability in positive sentences, and
 
 is that in negative sentences. They are es-
timated by using Table 2.
 	 
 	
 	    	
 
 
 

 
    

PMI based polarity value Using PMI, the
strength of association between  and positive sen-
tences (and negative sentences) is defined as follows
(Church and Hanks, 1989).
 	  	

  	
  	
 
  	

  

  

PMI based polarity value is defined as their differ-
ence. This idea is the same as (Turney, 2002).


   	    

 	

  	 	
  
 

 	

 	
 

 	 and  
 are estimated in the same
way as shown above. 

 is (log of) the ra-
tio of ?s probability in positive sentences to that in
negative sentences. This formalization follows our
intuition. Similar to 


, 

 is greater
than zero if  
    	, otherwise it is
less than zero.
Selecting polar phrases By using polarity value
and threshold  , it is decided whether a can-
didate  is polar phrase or not. If     , the
candidate is regarded as positive phrase. Similarly, if
     , it is regarded as negative phrase. Oth-
erwise, it is regarded as neutral. Only positive and
negative phrases are added to our lexicon. By chang-
ing , the trade-off between precision and recall can
be adjusted. In order to avoid data sparseness prob-
lem, if both  	 and  
 are less than
three, such candidates were ignored.
4 Related Work
As described in Section 1, there have been two ap-
proaches to (semi-) unsupervised learning of polar-
ity. This Section introduces the two approaches and
other related work.
4.1 Thesaurus based approach
Kamps et al built lexical network by linking syn-
onyms provided by a thesaurus, and polarity was de-
fined by the distance from seed words (?good? and
?bad?) in the network (Kamps et al, 2004). This
method relies on a hypothesis that synonyms have
the same polarity. Hu and Liu used similar lexi-
cal network, but they considered not only synonyms
but antonyms (Hu and Liu, 2004). Kim and Hovy
proposed two probabilistic models to estimate the
strength of polarity (Kim and Hovy, 2004). In their
models, synonyms are used as features. Esuli et al
utilized glosses of words to determine polarity (Esuli
and Sebastiani, 2005; Esuli and Sebastiani, 2006).
Compared with our approach, the drawback of us-
ing thesaurus is the lack of scalability. It is diffi-
cult to handle such words that are not contained in
a thesaurus (e.g. newly-coined words or colloquial
words). In addition, phrases cannot be handled be-
cause the entry of usual thesaurus is not phrase but
word.
1079
4.2 Corpus based approach
Another approach is based on an idea that polar
phrases conveying the same polarity co-occur with
each other in corpus.
(Turney, 2002) is one of the most famous work
that discussed learning polarity from corpus. Turney
determined polarity value5 based on co-occurrence
with seed words (?excellent? and ?poor?). The co-
occurrence is measured by the number of hits re-
turned by a search engine. The polarity value pro-
posed by (Turney, 2002) is as follows.
	


  

		

  		



 means the number of hits returned by a
search engine when query  is issued.  
means NEAR operator, which enables to retrieve
only such documents that contain two queries within
ten words.
Hatzivassiloglou and McKeown constructed lex-
ical network and determine polarity of adjectives
(Hatzivassiloglous and McKeown, 1997). Although
this is similar to thesaurus based approach, they built
the network from intra-sentential co-occurrence.
Takamura et al built lexical network from not only
such co-occurrence but other resources including
thesaurus (Takamura et al, 2005). They used spin
model to predict polarity of words.
Popescu and Etzioni applied relaxation labeling to
polarity identification (Popescu and Etzioni, 2005).
This method iteratively assigns polarity to words by
using various features including intra-sentential co-
occurrence and synonyms of a thesaurus.
Kanayama and Nasukawa used both intra- and
inter-sentential co-occurrence to learn polarity of
words and phrases (Kanayama and Nasukawa,
2006). Their method covers wider range of co-
occurrence than other work such as (Hatzivas-
siloglous and McKeown, 1997). An interesting
point of this work is that they discussed building do-
main oriented lexicon. This is contrastive to other
work including ours that addresses to build domain
independent lexicon.
In summary, the strength of our approach is to ex-
ploit extremely precise structural clues, and to use
5Semantic Orientation in (Turney, 2002).
massive collection of HTML documents to compen-
sate for the low recall. Although Turney?s method
also uses massive collection of HTML documents,
his method does not make much of precision com-
pared with our method. As we will see in Section
5, our experimental result revealed that our method
overwhelms Turney?s method.
4.3 Other related work
In some review sites, pros and cons are stated using
such layout that we introduced in Section 2. Some
work examined the importance of such layout (Liu et
al., 2005; Kim and Hovy, 2006). However, they re-
garded layout structures as clues specific to a certain
review site. They did not propose to use layout struc-
ture to extract polar sentences from arbitrary HTML
documents.
Some studies addressed supervised approach to
learning polarity of phrases (Wilson et al, 2005;
Takamura et al, 2006). These are different from
ours in a sense that they require manually tagged
data.
Kobayashi et al proposed a framework to reduce
the cost of manually building lexicon (Kobayashi et
al., 2004). In the experiment, they compared the
framework with fully manual method and investi-
gated the effectiveness.
5 Experiment
A test set consisting of 405 adjective phrases were
created. From the test set, we extract polar phrases
by looking up our lexicon. The result was evaluated
through precision and recall6.
5.1 Setting
The test set was created in the following manner.
500 adjective phrases were randomly extracted from
the Web text. Note that there is no overlap between
our polar sentence corpus and this text. After remov-
ing parsing error and duplicates, 405 unique adjec-
tive phrases were obtained. Each phase was man-
ually annotated with polarity tag (positive, negative
and neutral), and we obtained 158 positive phrases,
150 negative phrases and 97 neutral phrases. In or-
der to check the reliability of annotation, another
6The lexicon is available from http://www.tkl.iis.u-
tokyo.ac.jp/?kaji/polardic/.
1080
Table 3: The experimental result (chi-square).
 0 10 20 30 40 50 60
Precision/Recall Positive 76.4/92.4 84.0/86.7 84.1/83.5 86.2/79.1 88.7/74.7 86.7/65.8 86.7/65.8
Negative 68.5/84.0 65.5/63.3 64.3/60.0 62.7/57.3 81.1/51.3 80.0/48.0 80.0/48.0
# of polar words and phrases 9,670 2,056 1,047 698 533 423 335
Table 4: The experimental result (PMI).
 0 0.5 1.0 1.5 2.0 2.5 3.0
Precision/Recall Positive 76.4/92.4 79.6/91.1 86.1/89.9 87.2/86.1 90.9/82.3 92.4/76.6 92.9/65.8
Negative 68.5/84.0 75.8/81.3 82.3/77.3 84.8/74.7 85.8/72.7 86.8/70.0 87.9/62.7
# of polar words and phrases 9,670 9,320 9,039 8,804 8,570 8,398 8,166
Table 5: The effect of data size (PMI, =1.0).
size 1/20 1/15 1/10 1/5 1
Precision/Recall Positive 87.0/63.9 84.6/65.8 85.1/75.9 85.4/84.8 86.1/89.9
Negative 76.9/55.8 86.2/50.0 82.1/58.0 80.3/62.7 82.3/77.3
human judge annotated the same data. The Kappa
value between the two judges was 0.73, and we think
the annotation is reliable.
From the test set, we extracted polar phrases by
looking up our lexicon. As for adjectives in the lex-
icon, partial match is allowed. For example, if the
lexicon contains an adjective ?excellent?, it matches
every adjective phrase that includes ?excellent? such
as ?view-excellent? etc.
As a baseline, we built lexicon similarly by using
polarity value of (Turney, 2002). As seed words, we
used ?saikou (best)? and ?saitei (worst)?. Some seeds
were tested and these words achieved the best result.
As a search engine, we tested Google and our local
engine, which indexes 150 millions Japanese docu-
ments. Its size is compatible to (Turney and Littman,
2002). Since Google does not support NEAR, we
used AND. Our local engine supports NEAR.
5.2 Results and discussion
We evaluated the result of polar phrase extraction.
By changing the threshold , we investigated recall-
precision curve (Figure 6 and 7). The detail is rep-
resented in Table 3 and 4. The second/third row
represents precision and recall of positive/negative
phrases. The fourth row is the size of the lexicon.
The Figures show that both of the proposed meth-
ods outperform the baselines. The best F-measure
was achieved by PMI (=1.0). Although Turney?s
method may be improved with minor configurations
(e.g. using other seeds etc.), we think this results
indicate the feasibility of the proposed method. Al-
Figure 6: Recall-precision curve (positive phrases)
though the size of lexicon is not surprisingly large, it
would be possible to make the lexicon larger by us-
ing more HTML documents. In addition, notice that
we focus on only adjectives and adjective phrases.
Comparing the two proposed methods, PMI is al-
ways better than chi-square. Especially, chi-square
suffers from low recall, because the size of lexicon
is extremely small. For example, when the thresh-
old is 60, the precision is 80% and the recall is 48%
for negative phrases. On the other hand, PMI would
achieve the same precision when recall is around
80% ( is between 0.5 and 1.0).
Turney?s method did not work well although they
reported 80% accuracy in (Turney and Littman,
2002). This is probably because our experimental
setting is different. Turney examined binary classi-
fication of positive and negative words, and we dis-
cussed extracting positive and negative phrases from
the set of positive, negative and neutral phrases.
1081
Figure 7: Recall-precision curve (negative phrases)
Error analysis revealed that most of the errors are
related to neutral phrases. For example, PMI (=1.0)
extracted 48 incorrect polar phrases, and 37 of them
were neutral phrases. We think one reason is that
we did not use neutral corpus. It is one future work
to exploit neutral corpus. The importance of neutral
category is also discussed in other literatures (Esuli
and Sebastiani, 2006).
To further assess our method, we did two addi-
tional experiments. In the first experiment, to inves-
tigate the effect of data size, the same experiment
was conducted using 1/n (n=1,5,10,15,20) of the en-
tire polar sentence corpus (Table 5). PMI (=1.0)
was also used. As the size of corpus increases, the
performance becomes higher. Especially, the re-
call is improved dramatically. Therefore, the recall
would be further improved using more corpus.
In the other experiment, the lexicon was evalu-
ated directly so that we can examine polar words and
phrases that are not in the test set. We think it is diffi-
cult to fully assess low frequency words in the previ-
ous setting. Two human judges assessed 200 unique
polar words and phrases in the lexicon (PMI, =1.0).
The average precision was 71.3% (Kappa value was
0.66). The precision is lower than the result in Table
4. This result indicates that it is difficult to handle
low frequency words.
The Table 6 illustrates examples of polar phrases
and their polarity values. We can see that both
phrases and colloquial words such as ?uncool? are
appropriately learned. They are difficult to handle
for thesaurus based approach, because such words
are not usually in thesaurus.
It is important to discuss how general our frame-
Table 6: Examples
polar phrase 
 
 
 


kenkyoda (modest) 38.3 12.1
exiting (exiting) 13.5 10.4
more-sukunai (leak-small) 9.2 9.8
dasai (uncool) -2.9 -3.3
yakkaida (annoying) -11.9 -3.9
shomo-hayai (consumption-quick) -17.7 -4.4
work is. Although the lexico-syntactic patterns
shown in Section 2 are specific to Japanese, we
think that the idea of exploiting language structure
is applicable to other languages including English.
Roughly speaking, the pattern we exploited can be
translated into ?the advantage/weakness of some-
thing is to ...? in English. It is worth pointing out
that lexico-syntactic patterns have been widely used
in English lexical acquisition (Hearst, 1992). Obvi-
ously, other parts of the proposed method does not
depend on Japanese.
6 Conclusion
In this paper, we explore to use structural clues that
can extract polar sentences from Japanese HTML
documents, and build lexicon from the extracted po-
lar sentences. The key idea is to develop the struc-
tural clues so that it achieves extremely high preci-
sion at the cost of recall. In order to compensate for
the low recall, we used massive collection of HTML
documents. Thus, we could prepare enough polar
sentence corpus. Experimental result demonstrated
the feasibility of our approach.
Acknowledgement This work was supported by
the Comprehensive Development of e-Society Foun-
dation Software program of the Ministry of Edu-
cation, Culture, Sports, Science and Technology,
Japan. We would like to thank Assistant Researcher
Takayuki Tamura for his development of the Web
crawler.
References
Kenneth Ward Church and Patric Hanks. 1989. Word
association norms, mutual information, and lexicogra-
phy. In Proceedings of ACL, pages 76?83.
Andrea Esuli and Fabrizio Sebastiani. 2005. Determin-
ing the semantic orientation of terms throush gloss
classification. In Proceedings of CIKM, pages 617?
624.
1082
Andrea Esuli and Fabrizio Sebastiani. 2006. Determin-
ing term subjectivity and term orientation for opinion
mining. In Proceedings of EACL, pages 193?200.
Vasileios Hatzivassiloglous and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of ACL, pages 174?181.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING,
pages 539?545.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of KDD, pages
168?177.
Nobuhiro Kaji and Masaru Kitsuregawa. 2006. Au-
tomatic construction of polarity-tagged corpus from
html documents. In Proceedings of COLING/ACL,
poster sessions, pages 452?459.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten de Rijke. 2004. Using wordnet to measure
semantic orientations of adjectives. In Proceedings of
LREC.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully
automatic lexicon expansion for domain-oriented sen-
timent analysis. In Proceedings of ENMLP, pages
355?363.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of COLING,
pages 1367?1373.
Soo-Min Kim and Eduard Hovy. 2006. Automatic iden-
tification of pro and con reasons in online reviews. In
Proceedings of COLING/ACL Poster Sessions, pages
483?490.
Nozomi Kobayashi, Kentaro Inui, Yuji Matsumoto, Kenji
Tateishi, and Toshikazu Fukushima. 2004. Collecting
evaluative expressions for opinion extraction. In Pro-
ceedings of IJCNLP, pages 584?589.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: Analyzing and comparing opinions
on the web. In Proceedings of WWW.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings of HLT/EMNLP.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientation of words using
spin model. In Proceedings of ACL, pages 133?140.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2006. Latent variable mdels for semantic orientations
of phrases. In Proceedings of EACL, pages 201?208.
Peter D. Turney and Michael L. Littman. 2002. Unsuper-
vised learning of semantic orientation from a hundred-
billion-word corpus. Technical report, National Re-
search Council Canada.
Peter D. Turney. 2002. Thumbs up or thumbs down ?
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of ACL, pages 417?
424.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT/EMNLP.
1083
Lexical Choice via Topic Adaptation for
Paraphrasing Written Language to Spoken
Language
Nobuhiro Kaji1 and Sadao Kurohashi2
1 Institute of Industrial Science, The University of Tokyo,
4-6-1 Komaba, Meguro-ku, Tokyo 153-8505, Japan
kaji@tkl.iis.u-tokyo.ac.jp
2 Graduate School of Information Science and Technology,
The University of Tokyo, 7-3-1 Hongo,
Bunkyo-ku, Tokyo 113-8656, Japan
kuro@kc.t.u-tokyo.ac.jp
Abstract. Our research aims at developing a system that paraphrases
written language text to spoken language style. In such a system, it is
important to distinguish between appropriate and inappropriate words
in an input text for spoken language. We call this task lexical choice
for paraphrasing. In this paper, we describe a method of lexical choice
that considers the topic. Basically, our method is based on the word
probabilities in written and spoken language corpora. The novelty of our
method is topic adaptation. In our framework, the corpora are classified
into topic categories, and the probability is estimated using such corpora
that have the same topic as input text. The result of evaluation showed
the effectiveness of topic adaptation.
1 Introduction
Written language is different from spoken language. That difference has various
aspects. For example, spoken language is often ungrammatical, or uses simplified
words rather than difficult ones etc. Among these aspects this paper examines
difficulty. Difficult words are characteristic of written language and are not ap-
propriate for spoken language.
Our research aims at developing a system that paraphrases written language
text into spoken language style. It helps text-to-speech generating natural voice
when the input is in written language. In order to create such a system, the
following procedure is required: (1) the system has to detect inappropriate words
in the input text for spoken language, (2) generate paraphrases of inappropriate
words, and (3) confirm that the generated paraphrases are appropriate. This
paper examines step (1) and (3), which we call lexical choice for paraphrasing
written language to spoken language.
Broadly speaking, lexical choice can be defined as binary classification task:
the input is a word and a system outputs whether it is appropriate for spoken
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 981?992, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
982 N. Kaji and S. Kurohashi
language or not. This definition is valid if we can assume that the word diffi-
culty is independent of such factors as context or listeners. However, we think
such assumption is not always true. One example is business jargon (or technical
term). Generally speaking, business jargon is difficult and inappropriate for spo-
ken language. Notwithstanding, it is often used in business talk. This example
implies that the word difficulty is dependent on the topic of text/talk.
In this paper, we define the input of lexical choice as a word and text where it
occurs (= the topic). Such definition makes it possible for a system to consider
the topic. We think the topic plays an important role in lexical choice, when
dealing with such words that are specific to a certain topic, e.g., business jargon.
Hereafter, those words are called topical words, and others are called non-topical
words. Of course, in addition to the topic, we have to consider other factors such
as listeners and so on. But, the study of such factors lies outside the scope of
this paper.
Based on the above discussion, we describe a method of lexical choice that
considers the topic. Basically, our method is based on the word probabilities in
written and spoken language corpora. It is reasonable to assume that these two
probabilities reflect whether the word is appropriate or not. The novelty of the
method is topic adaptation. In order to adapt to the topic of the input text,
the corpora are classified into topic categories, and the probability is estimated
using such corpora that have the same topic category as the input text. This
process enables us to estimate topic-adapted probability. Our method was evalu-
ated by human judges. Experimental results demonstrated that our method can
accurately deal with topical words.
This paper is organized as follows. Section 2 represents method overview.
Section 3 and Section 4 describe the corpora construction. Section 5 represents
learning lexical choice. Section 6 reports experimental results. Section 7 describes
related works. We conclude this paper in Section 8.
2 Method Overview
Our method uses written and spoken language corpora classified into topic cat-
egories. They are automatically constructed from the WWW. The construction
procedure consists of the following two processes (Figure 1).
1. Style Classification
Web pages are downloaded from the WWW, and are classified into written
and spoken language style. Those pages classified as written/spoken language
are referred as written/spoken language corpus. In this process, we discarded
ambiguous pages that are difficult to classify.
2. Topic Classification
The written and spoken language corpora are classified into 14 topic cate-
gories, such as arts, computers and so on.
Both classification methods are represented in Section 3 and Section 4.
Given an input word and a text where it occurs, it is decided as follows
whether the input word is appropriate or inappropriate for spoken language.
Lexical Choice via Topic Adaptation for Paraphrasing Written Language 983
1. The topic category of the input text is decided by the same method as the
one used to classify Web pages into topic categories.
2. We estimate the probabilities of the input word in the written and spoken
language corpora. We use such corpora that have the same topic as the input
text.
3. Using the two probabilities, we decide whether the input word is appropriate
or not. Section 5 describes this method.
Web Pages
Spoken AmbiguousWritten
Topic classification
Style classification
(Section 3)
(Section 4)
Written
Arts category
Spoken Written
Recreation category
Spoken
.........
Written
Science category
Spoken
Fig. 1. Written and spoken language corpora construction
3 Style Classification
In order to construct written and spoken language corpora classified into topic
categories, first of all, Web pages are classified into written and spoken language
pages (Figure 1). Note that what is called spoken language here is not real
utterance but chat like texts. Although it is not real spoken language, it works
as a good substitute, as some researchers pointed out [2,11].
We follow a method proposed by Kaji et al(2004). Their method classifies
Web pages into three types: (1) written language page, (2) spoken language page,
and (3) ambiguous page. Then, Web pages classified into type (1) or (2) are
used. Ambiguous pages are discarded because classification precision decreases
if such pages are used. This Section summarizes their method. See [11] for detail.
Note that for this method the target language is Japanese, and its procedure is
dependent on Japanese characteristics.
3.1 Basic Idea
Web pages are classified based on interpersonal expressions, which imply an at-
titude of a speaker toward listeners, such as familiarity, politeness, honor or con-
tempt etc. Interpersonal expressions are often used in spoken language, although
984 N. Kaji and S. Kurohashi
not frequently used in written language. For example, when spoken language is
used, one of the most basic situations is face-to-face communication. On the
other hand, such situation hardly happens when written language is used.
Therefore, Web pages containing many interpersonal expressions are classi-
fied as spoken language, and vice versa. Among interpersonal expressions, such
expressions that represent familiarity or politeness are used, because:
? Those two kinds of interpersonal expressions frequently appear in spoken
language,
? They are represented by postpositional particle in Japanese and, therefore,
are easily recognized as such.
Hereafter, interpersonal expression that represents familiarity/politeness is
called familiarity/politeness expression.
3.2 Style Classification Procedure
Web pages are classified into the three types based on the following two ratios:
? Familiarity ratio (F-ratio): ?# of sentences including familiarity expressions?
divided by ?# of all the sentences in the page?.
? Politeness ratio (P-ratio): ?# of sentences including politeness expressions?
divided by ?# of all the sentences in the page?.
The procedure is as follows. First, Web pages are processed by Japanese
morphological analyzer JUMAN3. And then, in order to calculate F-ratio and P-
ratio, sentences which include familiarity or politeness expressions are recognized
in the following manner. A sentence is considered to include the familiarity
expression, if it has one of the following six postpositional particles: ne, yo,
wa, sa, ze, na. A sentence is considered to include the politeness expression, if
it has one of the following four postpositional particles: desu, masu, kudasai,
gozaimasu.
After calculating the two ratios, the page is classified according to the rules
illustrated in Figure 2. If F-ratio and P-ratio are equal to 0, the page is classified
as written language page. If F-ratio is more than 0.2, or if F-ratio is more than
0.1 and P-ratio is more than 0.2, the page is classified as spoken language page.
The other pages are regarded as ambiguous and are discarded.
3.3 The Result
Table 1 shows the number of pages and words (noun, verb, and adjective) in the
corpora constructed from the WWW. About 8,680k pages were downloaded from
the WWW, and 994k/1,338k were classified as written/spoken language. The
rest were classified as ambiguous page and they were discarded. The precision of
this method was reported by Kaji et al(2004). According to their experiment,
the precision was 94%.
3 http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman-e.html
Lexical Choice via Topic Adaptation for Paraphrasing Written Language 985
0
0.2
F-ratio
P-ratio0.2
0.1
Written language
Ambiguous
Spoken language
Fig. 2. Style classification rule
Table 1. The size of the written and spoken
language corpora
# of pages # of words
Written language 989k 432M
Spoken language 1,337k 907M
Table 2. The size of the training and test
data
Topic category Training Test
Arts 2,834 150
Business & Economy 5,475 289
Computers & Internet 6,156 325
Education 2,943 155
Entertainment 6,221 328
Government 3,131 165
Health 1,800 95
News 2,888 152
Recreation 4,352 230
Reference 1,099 58
Regional 4,423 233
Science 3,868 204
Social Science 5,410 285
Society & Culture 5,208 275
4 Topic Classification
The written and spoken language corpora are classified into 14 topic categories
(Figure 1). This task is what is called text categorization. We used Support Vec-
tor Machine because it is reported to achieve high performance in this task.The
training data was automatically built from Yahoo! Japan4.
The category provided by Yahoo! Japan have hierarchy structure. For exam-
ple, there are Arts and Music categories, and Music is one of the subcategories
of Arts. We used 14 categories located at the top level of the hierarchy. We
downloaded Web pages categorized in one of the 14 categories. Note that we
did not use Web pages assigned more than one categories. And then, the Web
pages were divided them into 20 segments. One of them was used as the test
data, and the others were used as the training data (Table 2). In the Table, the
4 http://www.yahoo.co.jp/
986 N. Kaji and S. Kurohashi
0
50000
100000
150000
200000
250000
300000
350000
400000
A
r
t
s
B
u
s
i
n
e
s
s
&
E
c
o
n
o
m
y
C
o
m
p
u
t
e
r
s
&
I
n
t
e
r
n
e
t
E
d
u
c
a
t
i
o
n
E
n
t
e
r
t
a
i
n
m
e
n
t
G
o
v
e
r
n
m
e
n
t
H
e
a
l
t
h
N
e
w
s
R
e
c
r
e
a
t
i
o
n
R
e
f
e
r
e
n
c
e
R
e
g
i
o
n
a
l
S
c
i
e
n
c
e
S
o
c
i
a
l
 
S
c
i
e
n
c
e
S
o
c
i
e
t
y
&
C
u
l
t
u
r
e
Topic category
#
 
o
f
 
p
a
g
e
s
Written language page Spoken language page
Fig. 3. The size of written and spoken language corpora in each topic category
first column shows the name of the 14 topic categories. The second/third column
shows the number of pages in the training/test data.
SVM was trained using the training data. In order to build multi-class clas-
sifier, we used One-VS-Rest method. Features of SVM are probabilities of nouns
in a page. Kernel function was linear. After the training, it was applied to the
test data. The macro-averaged accuracy was 86%.
The written and spoken language corpora constructed from the WWW were
classified into 14 categories by SVM. Figure 3 depicts the number of pages in
each category.
5 Learning Lexical Choice
We can now construct the written and spoken language corpora classified into
topic categories. The next step is discrimination between inappropriate and ap-
propriate words for spoken language using the probabilities in written and spoken
language corpora (Section 2). This paper proposes two methods: one is based
on Decision Tree (DT), and the other is based on SVM. This Section first de-
scribes the creation of gold standard data, which is used for both training and
evaluation. Then, we describe the features given to DT and SVM.
5.1 Creation of Gold Standard Data
We prepared data consisting of pairs of a word and binary tag. The tag represents
whether that word is inappropriate or appropriate for spoken language. This data
is referred as gold standard data. Note that the gold standard is created for each
topic category.
Gold standard data of topic T is created as follows.
1. Web pages in topic T are downloaded from Yahoo! Japan, and we sampled
words (verbs, nouns, and adjectives) from those pages at random.
2. Three human judges individually mark each word as INAPPROPRIATE,
APPROPRIATE or NEUTRAL. NEUTRAL tag is used when a judge cannot
mark a word as INAPPROPRIATE or APPROPRIATE with certainty.
Lexical Choice via Topic Adaptation for Paraphrasing Written Language 987
3. The three annotations are combined, and single gold standard data is cre-
ated. A word is marked as INAPPROPRIATE/APPROPRIATE in the gold
standard, if
? All judges agree that it is INAPPROPRIATE/APPROPRIATE, or
? Two judges agree that it is INAPPROPRIATE/APPROPRIATE and
the other marked it as NEUTRAL.
The other words are not used in the gold standard data.
5.2 The Features
Both DT and SVM use the same three features: the word probability in written
language corpus, the word probability in spoken language corpus, and the ratio
of the word probability in spoken language corpus to that in written language.
Note that when DT and SVM are trained on the gold standard of topic T, the
probability is estimated using the corpus in topic T.
6 Evaluation
This Section first reports the gold standard creation. Then, we show that DT and
SVM can successfully classify INAPPROPRIATE and APPROPRIATE words
in the gold standard. Finally, the effect of topic adaptation is represented.
6.1 The Gold Standard Data
The annotation was performed by three human judges (Judge1, Judge2 and
Judge3) on 410 words sampled from Business category, and 445 words sampled
from Health category. Then, we created the gold standard data in each category
(Table 3). The average Kappa value [3] between the judges was 0.60, which
corresponds to substantial agreement.
Table 3. Gold standard data
Business Health
INAPPROPRIATE 49 38
APPROPRIATE 267 340
Total 316 378
Table 4. # of words in Business and Health
categories corpora
Business Health
Written language 29,891k 30,778k
Spoken language 9,018k 32,235k
6.2 Lexical Choice Evaluation
DT and SVM were trained and tested on the gold standard data using Leave-
One-Out (LOO) cross validation. DT and SVM were implemented using C4.55
and TinySVM6 packages. The kernel function of SVM was Gaussian RBF. Table
5 http://www.rulequest.com/Personal/
6 http://chasen.org/ taku/software/TinySVM/
988 N. Kaji and S. Kurohashi
Table 5. The result of LOO cross validation
Topic Method Accuracy # of correct answers Precision Recall
Business DT .915 (289/316) 31 + 258 = 289 .775 .660
SVM .889 (281/316) 21 + 260 = 281 .750 .429
MCB .845 (267/316) 0 + 267 = 267 ? .000
Health DT .918 (347/378) 21 + 326 = 347 .600 .552
SVM .918 (347/378) 13 + 334 = 347 .684 .342
MCB .899 (340/378) 0 + 340 = 340 ? .000
4 shows the number of words in Business and Health categories corpora. Three
features described in Section 5 were used.
The result is summarized in Table 5. For example, in Business category, the
accuracy of DT was 91.5%. 289 out of 316 words were classified successfully, and
the 289 consists of 31 INAPPROPRIATE and 258 APPROPRIATE words. The
last two columns show the precision and recall of INAPPROPRIATE words.
MCB is Majority Class Baseline, which marks every word as APPROPRIATE.
Judging from the accuracy in Health category, one may think that our
method shows only a little improvement over MCB. However, considering other
evaluation measures such as recall of INAPPROPRIATE words, it is obvious
that the proposed method overwhelms MCB. We would like to emphasize the
fact that MCB is not at all practical lexical choice method. If MCB is used, all
words in the input text are regarded as appropriate for spoken language and the
input is never paraphrased.
One problem of our method is that the recall of INAPPROPRIATE words is
low. We think that the reason is as follows. The number of INAPPROPRIATE
words in the gold standard is much smaller than that of APPROPRIATE words.
Hence, we think a system that is biased to classify words as APPROPRIATE
often achieves high accuracy. It is one of future works to improve the recall while
keeping high accuracy.
We examined discrimination rules learned by DT. Figure 4 depicts the rules
learned by DT when the whole gold standard data of Business category is used
as a training data. In the Figure, the horizontal/vertical axis corresponds to the
probability in the written/spoken language corpus. Words in the gold standard
can be mapped into this two dimension space. INAPPROPRIATE/ APPROPRI-
ATE words are represented by a cross/square. The line represents discrimination
rules. Words below the line are classified as INAPPROPRIATE, and the others
are classified as APPROPRIATE.
6.3 Effect of Topic Adaptation
Finally, we investigated the effect of topic adaptation by comparing our method
to a baseline method that does not consider topic.
Our method consists of two steps: (1) mapping from a word to features, and
(2) applying discrimination rules to the features. In the step (1), the probability
is estimated using the written and spoken language corpora in a certain topic
Lexical Choice via Topic Adaptation for Paraphrasing Written Language 989
0
0.0001
0 0.0001 0.0002
Probability in written language
P
r
o
b
a
b
i
l
i
t
y
 
i
n
 
s
p
o
k
e
n
 
l
a
n
g
u
a
g
e
Fig. 4. Decision tree rules in Business cat-
egory
customer
customer
management
management
0
0.0002
0.0004
0 0.0002 0.0004 0.0006 0.0008
Probability in written language
P
r
o
b
a
b
i
l
i
t
y
 
i
n
 
s
p
o
k
e
n
 
l
a
n
g
u
a
g
e
Fig. 5. Examples in Business category
T. In the step (2), discrimination rules are learned by DT using the whole gold
standard data of topic T. We used DT rather than SVM because rules are easy
for humans to understand. On the other hand, the baseline uses the same dis-
crimination rules as our method, but uses the whole written and spoken language
corpora to map a word to features. Hereafter, the two methods are referred as
Proposed and Baseline. Both methods use the same discrimination rules but
map a word to features in a different way. Therefore, there are such words that
are classified as INAPPROPRIATE by Proposed and are classified as AP-
PROPRIATE by Baseline, and vice versa. In the evaluation, we compared the
classification results of such words.
We evaluated the results of topical words and non-topical words separately.
This is because we think Proposed is good at dealing with topical words and
hence we can clearly confirm the effectiveness of topic adaptation. Here, a word is
regarded as topical word in topic T, if its probabilities in the written and spoken
language corpora assigned topic category T are larger than those in the whole
corpora with statistical significance (the 5% level). Otherwise it is regarded as
non-topical word in topic T. As a statistical test log-likelihood ratio [4] was used.
The evaluation procedure is as follows.
1. Web pages in Business category were downloaded from Yahoo! Japan, and
words in those pages were classified by the two methods. If the results of the
two methods disagree, such words were stocked.
2. From the stocked words, we randomly sampled 50 topical words in Business
and 50 non-topical words. Note that we did not use such words that are
contained in the gold standard.
3. Using Web pages in Health category, we also sampled 50 topical words in
Health and 50 non-topical words in the same manner.
4. As a result, 100 topical words and 100 non-topical words were prepared. For
each word, two judges (Judge-A and Judge-B) individually assessed which
method successfully classified the word. Some classification results were dif-
ficult even for human judges to assess. In such cases, the results of the both
methods were regarded as correct.
990 N. Kaji and S. Kurohashi
Table 6 represents the classification accuracy of the 100 topical words. For
example, according to assessment by Judge-A, 75 out of 100 words were classified
successfully by Proposed. Similarly, Table 7 represents the accuracy of the 100
non-topical words. The overall agreement between the two judges according to
the Kappa value was 0.56. We compared the result of the two methods using
McNemar?s test [8], and we found statistically significant difference (the 5%
level) in the results. There was no significant difference in the result of non-
topical words assessed by the Judge-A.
Table 6. Accuracy of topical words classi-
fication
Judge Method Accuracy
Judge-A Proposed 75% (75/100)
Baseline 52% (52/100)
Judge-B Proposed 72% (72/100)
Baseline 53% (53/100)
Table 7. Accuracy of non-topical words
classification
Judge Method Accuracy
Judge-A Proposed 48% (48/100)
Baseline 66% (66/100)
Judge-B Proposed 38% (38/100)
Baseline 78% (78/100)
6.4 Discussion and Future Work
Proposed outperformed Baseline in topical words classification. This result
indicates that the difficulty of topical words depends on the topic and we have
to consider the topic. On the other hand, the result of Proposed was not good
when applied to non-topical words. We think this result is caused by two reasons:
(1) the difficulty of non-topical words is independent of the topic, and (2) Base-
line uses larger corpora than Proposed (see Table 1 and Table 4). Therefore,
we think this result does not deny the effectiveness of topic adaptation. These
results mean that Proposed and Baseline are complementary to each other,
and it is effective to combine the two methods: Proposed/Baseline is applied
to topical/non-topical words. It is obvious from the experimental results that
such combination is effective.
We found that Baseline is prone to classify topical words as inappropriate
and such bias decreases the accuracy. Figure 5 depicts typical examples sam-
pled from topical words in Business. Both judges regarded ?management? and
?customer?7 as appropriate for spoken language in Business topic. The white tri-
angle and diamond in the Figure represent their features when the probability
is estimated using the corpora in Business category. They are located above the
line, which corresponds to discrimination rules, and are successfully classified as
appropriate by Proposed. However, if the probability is estimated using the
whole corpora, the features shift to the black triangle and diamond, and Base-
line wrongly classified the two as inappropriate. In Health category, we could
observe similar examples such as ?lung cancer? or ?metastasis?.
7 Our target language is Japanese. Examples illustrated here are translation of the
original Japanese words.
Lexical Choice via Topic Adaptation for Paraphrasing Written Language 991
These examples can be explained in the following way. Consider topical words
in Business. When the probability is estimated using the whole corpora, it is
influenced by the topic but Business, where topical words in Business are often
inappropriate for spoken language. Therefore, we think that Baseline is biased
to classify topical words as inappropriate.
Besides the lexical choice method addressed in this paper, we proposed lex-
ical paraphrase generation method [10]. Our future direction is to apply these
methods to written language texts and evaluate the output of text-to-speech. So
far, the methods were tested on a small set of reports.
Although the main focus of this paper is lexical paraphrases, we think that
it is also important to deal with structural paraphrases. So far, we implemented
a system that paraphrases compound nouns into nominal phrases. It is our
future work to build a system that generates other kinds of structural para-
phrases.
7 Related Work
Lexical choice has been widely discussed in both paraphrasing and natural lan-
guage generation (NLG). However, to the best of our knowledge, no researches
address topic adaptation. Previous approaches are topic-independent or specific
to only certain topic.
Lexical choice has been one of the central issues in NLG. However, the main
focus is mapping from concepts to words, (e.g., [1]). In NLG, a work by Edmonds
and Hirst is related to our research [5]. They proposed a computational model
that represents the connotation of words.
Some paraphrasing researches focus on lexical choice. Murata and Isahara
addressed paraphrasing written language to spoken language. They used only
probability in spoken language corpus [12]. Kaji et al also discussed paraphras-
ing written language to spoken language, and they used the probabilities in
written and spoken language corpora [11]. On the other hand, Inkpen et al ex-
amined paraphrasing positive and negative text [9]. They used the computational
model proposed by Edmonds and Hirst [5].
The proposed method is based on the probability, which can be considered
as a simple language model. In language model works, many researchers have
discussed topic adaptation in order to precisely estimate the probability of topical
words [6,7,13]. Our work can be regarded as one application of such language
model technique.
8 Conclusion
This paper proposed lexical choice method that considers the topic. The method
utilizes written and spoken language corpora classified into topic categories, and
estimate the word probability that is adapted to the topic of the input text.
From the experimental result we could confirm the effectiveness of topic adap-
tation.
992 N. Kaji and S. Kurohashi
References
1. Berzilay, R., Lee, L.: Bootstrapping Lexical Choice via Multiple-Sequence Align-
ment. Proceedings of EMNLP. (2002) 50?57
2. Bulyko, I., Ostendorf, M., and Stolcke, A.: Getting More Mileage from Web Text
Sources for Conversational Speech Language Modeling using Class-Dependent Mix-
tures. Proceedings of HLT-NAACL (2003) 7?9
3. Carletta, J.: Assessing Agreement on Classification Tasks: The Kappa Statistic.
Computational Linguistics. 22 (2). (1996) 249?255
4. Dunning, T.: Accurate Methods for the Statistics of Surprise and Coincidence.
Computational Linguistics. 19 (1). (1993) 61?74
5. Edmonds, P., Hirst, G.: Near-Synonymy and Lexical Choice. Computational Lin-
guistics. 28 (2). (2002) 105?144
6. Florian, R., Yarowsky, D.: Dynamic Nonlocal Language Modeling via Hierarchical
Topic-Based Adaptation: Proceedings of ACL. (1999) 167?174
7. Gildea, D., Hofmann, T.; TOPIC-BASED LANGUAGE MODELS USING EM.
Proceedings of EUROSPEECH. (1999) 2167?2170
8. Gillick, L., Cox, S.: Some Statistical Issues in the Comparison of Speech Recogni-
tion Algorithms. Proceedings of ICASSP. (1989) 532?535
9. Inkpen, D., Feiguina, O., and Hirst, G.: Generating more-positive and more-
negative text. Proceedings of AAAI Spring Symposium on Exploring Attitude and
Affect in Text. (2004)
10. Kaji, N., Kawahara, D., Kurohashi, S., and Satoshi, S. : Verb Paraphrase based
on Case Frame Alignment. Proceedings of ACL. (2002) 215?222
11. Kaji, N., Okamoto, M., and Kurohasih, S.: Paraphrasing Predicates from Written
Language to Spoken Language Using the Web. Proceedings of HLT-NAACL. (2004)
241?248
12. Murata, M., Isahara, H.: Automatic Extraction of Differences Between Spoken and
Written Languages, and Automatic Translation from the Written to the Spoken
Language. Proceedings of LREC. (2002)
13. Wu, J., Khudanpur, S.: BUILDING A TOPIC-DEPENDENT MAXIMUM EN-
TROPY MODEL FOR VERY LARGE CORPORA. Proceedings of ICASSP.
(2002) 777?780
Paraphrasing Predicates from Written Language
to Spoken Language Using the Web
Nobuhiro Kaji and Masashi Okamoto and Sadao Kurohashi
Graduate School of Information Science and Technology, the University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
 kaji,okamoto,kuro@kc.t.u-tokyo.ac.jp
Abstract
There are a lot of differences between expres-
sions used in written language and spoken lan-
guage. It is one of the reasons why speech syn-
thesis applications are prone to produce unnat-
ural speech. This paper represents a method
of paraphrasing unsuitable expressions for spo-
ken language into suitable ones. Those two
expressions can be distinguished based on the
occurrence probability in written and spoken
language corpora which are automatically col-
lected from the Web. Experimental results indi-
cated the effectiveness of our method. The pre-
cision of the collected corpora was 94%, and
the accuracy of learning paraphrases was 76 %.
1 Introduction
Information can be provided in various forms, and one of
them is speech form. Speech form is familiar to humans,
and can convey information effectively (Nadamoto et al,
2001; Hayashi et al, 1999). However, little electronic
information is provided in speech form so far. On the
other hand, there is a lot of information in text form, and
it can be transformed into speech by a speech synthesis.
Therefore, a lot of attention has been given to applications
which uses speech synthesis, for example (Fukuhara et
al., 2001).
In order to enhance such applications, two problems
need to be resolved. The first is that current speech syn-
thesis technology is still insufficient and many applica-
tions often produce speech with unnatural accents and in-
tonations. The second one is that there are a lot of differ-
ences between expressions used in written language and
spoken language. For example, Ohishi indicated that dif-
ficult words and compound nouns are more often used in
written language than in spoken language (Ohishi, 1970).
Therefore, the applications are prone to produce unnatu-
ral speech, if their input is in written language.
Although the first problem is well-known, little atten-
tion has been given to the second one. The reason why the
second problem arises is that the input text contains Un-
suitable Expressions for Spoken language (UES). There-
fore, the problem can be resolved by paraphrasing UES
into Suitable Expression for Spoken language (SES).
This is a new application of paraphrasing. There are no
similar attempts, although a variety of applications have
been discussed so far, for example question-answering
(Lin and Pantel, 2001; Hermjakob et al, 2002; Duclaye
and Yvon, 2003) or text-simplification (Inui et al, 2003).
(1) Written (2) Spoken
(3) Unnatural
Figure 1: Paraphrasing UES into SES
Figure 1 illustrates paraphrasing UES into SES. In the
figure, three types of expressions are shown: (1) expres-
sions used in written language, (2) expressions used in
spoken language, and (3) unnatural expressions. The
overlap between two circles represents expressions used
both in written language and spoken language. UES is
the shaded portion: unnatural expressions, and expres-
sions used only in written language. SES is the non-
shaded portion. The arrows represent paraphrasing UES
into SES, and other paraphrasing is represented by broken
arrows. Paraphrasing unnatural expressions is not consid-
ered, since such expressions are not included in the input
text. The reason why unnatural expressions are taken into
consideration is that paraphrasing into such expressions
should be avoided.
In order to paraphrase UES into SES, this paper pro-
poses a method of learning paraphrase pairs in the form
of ?UES   SES?. The key notion of the method is to
distinguish UES and SES based on the occurrence prob-
ability in written and spoken language corpora which are
automatically collected from the Web. The procedure of
the method is as follows:1
(step 1) Paraphrase pairs of predicates2 are learned from
a dictionary using a method proposed by (Kaji et al,
2002).
(step 2) Written and spoken language corpora are auto-
matically collected from the Web.
(step 3) From the paraphrase pairs learned in step 1,
those in the form of ?UES  SES? are selected using
the corpora.
This paper deals with only paraphrase pairs of predicates,
although UES includes not only predicates but also other
categories such as nouns.
This paper is organized as follows. In Section 2 related
works are illustrated. Section 3 summarizes the method
of Kaji et al In Section 4, we describe the method of
collecting corpora form the Web and report the experi-
mental result. In Section 5, we describe the method of
selecting suitable paraphrases pairs and the experimental
result. Our future work is described in Section 6, and we
conclude in Section 7.
2 Related Work
Paraphrases are different expressions which convey the
same or almost the same meaning. However, there are
few paraphrases that have exactly the same meaning, and
almost all have subtle differences such as style or formal-
ity etc. Such a difference is called a connotational dif-
ference. This paper addresses one of the connotational
differences, that is, the difference of whether an expres-
sion is suitable or unsuitable for spoken language.
Although a large number of studies have been made
on learning paraphrases, for example (Barzilay and Lee,
2003), there are only a few studies which address the con-
notational difference of paraphrases. One of the studies
is a series of works by Edmonds et al and Inkpen et
al (Edmonds and Hirst, 2002; Inkpen and Hirst, 2001).
Edmonds et al proposed a computational model which
represents the connotational difference, and Inkpen et
al. showed that the parameters of the model can be
learned from a synonym dictionary. However, it is doubt-
ful whether the connotational difference between para-
phrases is sufficiently described in such a lexical re-
source. On the other hand, Inui et al discussed read-
1Note that this paper deals with Japanese.
2A predicate is a verb or an adjective.
ability, which is one of the connotational differences,
and proposed a method of learning readability ranking
model of paraphrases from a tagged corpus (Inui and Ya-
mamoto, 2001). The tagged corpus was built as follows:
a large amount of paraphrase pairs were prepared and an-
notators tagged them according to their readability. How-
ever, they focused only on syntactic paraphrases. This
paper deals with lexical paraphrases.
There are several works that try to learn paraphrase
pairs from parallel or comparable corpora (Barzilay and
McKeown, 2001; Shinyama et al, 2002; Barzilay and
Lee, 2003; Pang et al, 2003). In our work, paraphrase
pairs are not learned from corpora but learned from a dic-
tionary. Our corpora are neither parallel nor comparable,
and are used to distinguish UES and SES.
There are several studies that compare two corpora
which have different styles, for example, written and spo-
ken corpora or British and American English corpora,
and try to find expressions unique to either of the styles
(Kilgarriff, 2001). However, those studies did not deal
with paraphrases.
Bulyko et al also collected spoken language corpora
from the Web (Bulyko et al, 2003). The method of Bu-
lyko et al used N-grams in a training corpus and is dif-
ferent from ours (the detail of our method is described in
Section 4).
In respect of automatically collecting corpora which
have a desired style, Tambouratzis et al proposed a
method of dividing Modern Greek corpus into Demokiti
and Katharevoua, which are variations of Modern Greek
(Tambouratzis et al, 2000).
3 Learning Predicate Paraphrase Pairs
Kaji et al proposed a method of paraphrasing predi-
cates using a dictionary (Kaji et al, 2002). For example,
when a definition sentence of ?chiratsuku (to shimmer)?
is ?yowaku hikaru (to shine faintly)?, his method para-
phrases (1a) into (1b).
(1) a. ranpu-ga chiratsuku
a lamp to shimmer
b. ranpu-ga yowaku hikaru
a lamp faintly to shine
As Kaji et al discussed, this dictionary-based paraphras-
ing involves three difficulties: word sense ambiguity, ex-
traction of the appropriate paraphrase from a definition
sentence, transformation of postposition3. In order to
solve those difficulties, he proposed a method based on
case frame alignment.
If paraphrases can be extracted from the definition sen-
tences appropriately, paraphrase pairs can be learned. We
extracted paraphrases from definition sentences using the
3Japanese noun is attached with a postposition.
method of Kaji et al However, it is beyond the scope of
this paper to describe his method as a whole. Instead, we
represent an overview and show examples.
(predicate) (definition sentence)
(2) a. chiratsuku [ kasukani hikaru ]
to shimmer faintly to shine
to shine faintly
b. chokinsuru [ okane-wo tameru ]
to save money money to save
to save money
c. kansensuru byouki-ga [ utsuru ]
to be infected disease to be infected
to be infected with a disease
In almost all cases, a headword of a definition sentence of
a predicate is also a predicate, and the definition sentence
sometimes has adverbs and nouns which modify the head
word. In the examples, headwords are ?hikaru (to shine)?,
?tameru (to save)?, and ?utsuru (to be infected)?. The ad-
verbs are underlined, the nouns are underlined doubly,
paraphrases of the predicates are in brackets. The head-
word and the adverbs can be considered to be always in-
cluded in the paraphrase. On the other hand, the nouns
are not, for example ?money? in (2b) is included but ?dis-
ease? in (2c) is not. It is decided by the method of Kaji et
al. whether they are included or not.
The paraphrase includes one noun at most, and is in
the form of ?adverb noun+ predicate? 4. Hereafter, it
is assumed that a paraphrase pair which is learned is in
the form of ?predicate   adverb noun+ predicate?. The
predicate is called source, the ?adverb noun+ predicate?
is called target.
We used reikai-shougaku-dictionary (Tadika, 1997),
and 5,836 paraphrase pairs were learned. The main prob-
lem dealt with in this paper is to select paraphrase pairs
in the form of ?UES   SES? from those 5,836 ones.
4 Collecting Written and Spoken
Language Corpora from the Web
We distinguish UES and SES (see Figure 1) using the oc-
currence probability in written and spoken language cor-
pora. Therefore, large written and spoken corpora are
necessary. We cannot use existing Japanese spoken lan-
guage corpora, such as (Maekawa et al, 2000; Takezawa
et al, 2002), because they are small.
Our solution is to automatically collect written and
spoken language corpora from the Web. The Web con-
tains various texts in different styles. Such texts as news
articles can be regarded as written language corpora, and
such texts as chat logs can be regarded as spoken lan-
guage corpora. Since we do not need information such as
4
  means zero or more, and + means one or more.
accents or intonations, speech data of real conversations
is not always required.
This papepr proposes a method of collecting written
and spoken language corpora from the Web using inter-
personal expressions (Figure 2). Our method is as fol-
lows. First, a corpus is created by removing useless parts
such as html tags from the Web. It is called Web corpus.
Note that the Web corpus consist of Web pages (hereafter
page). Secondly, the pages are classified into three types
(written language corpus, spoken language corpus, and
ambiguous corpus) based on interpersonal expressions.
And then, only written and spoken language copora are
used, and the ambiguous corpus is abandoned. This is
because:
 Texts in the same page tend to be described in the
same style.
 The boundary between written and spoken language
is not clear even for humans, and it is almost im-
possible to precisely classify all pages into written
language or spoken language.
written 
language corpus
spoken 
language corpus
The Web corpus
.
.
.
.
.
.
.
.
.
ambiguous corpus
pages
Figure 2: Collecting written and spoken language corpora
4.1 Interpersonal expressions
Each page in the Web corpus is classified based on inter-
personal expressions.
Spoken language is often used as a medium of informa-
tion which is directed to a specific listener. For example,
face-to-face communication is one of the typical situa-
tions in which spoken language is used. Due to this fact,
spoken language tends to contain expressions which im-
ply an certain attitude of a speaker toward listeners, such
as familiarity, politeness, honor or contempt etc. Such
an expression is called interpersonal expression. On the
other hand, written language is mostly directed to unspe-
cific readers. For example, written language is often used
in news articles or books or papers etc. Therefore, inter-
personal expressions are not used so frequently in written
language as in spoken language.
Among interpersonal expressions, we utilized familiar-
ity and politeness expressions. The familiarity expression
is one kind of interpersonal expressions, which implies
the speaker?s familiarity toward the listener. It is repre-
sented by a postpositional particle such as ?ne? or ?yo?
etc. The following is an example:
(3) watashi-wa ureshikatta yo
I was happy (familiarity)
I was happy
(3) implies familiarity using the postpositional particle
?yo?.
The politeness expression is also one kind of inter-
personal expressions, which implies politeness to the lis-
tener. It is represented by a postpositional particle. For
example:
(4) watashi-wa eiga-wo mi masu
I a movie to watch (politeness)
I watch a movie
(4) implies politeness using the postpositional particle
?masu?.
Those two interpersonal expressions often appear in
spoken language, and are easily recognized as such by
a morphological analyzer and simple rules. Therefore, a
page in the Web corpus can be classified into the three
types based the following two ratios.
 Familiarity ratio (F-ratio):
# of sentences which include familiarity expressions
# of all the sentences in the page
 Politeness ratio (P-ratio):
# of sentences which include politeness expressions
# of all the sentences in the page.
4.2 Algorithm
After the Web corpus is processed by a Japanese mor-
phological analyzer (JUMAN)5, sentences which include
familiarity or politeness expressions are recognized in the
following manner in order to calculate F-ratio and P-ratio.
If a sentence has one of the following six postpositional
particles, it is considered to include the familiarly expres-
sion.
ne, yo, wa, sa, ze, na
A sentence is considered to include the politeness expres-
sion, if it has one of the following four postpositional par-
ticles.
desu, masu, kudasai, gozaimasu
5http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman-e.html
If F-ratio and P-ratio of a page are very low, the page
is in written language, and vice versa. We observed a
part of the Web corpus, and empirically decided the rules
illustrated in Table 1. If F-ratio and P-ratio are equal to
0, the page is classified as written language. If F-ratio is
more than 0.2, or if F-ratio is more than 0.1 and P-ratio is
more than 0.2, the page is classified as spoken language.
The other pages are regarded as ambiguous.
Table 1: Page classification rules
F-ratio   
  Written languageP-ratio   
F-ratio   
  Spoken languageorF-ratio   
P-ratio   
Otherwise   Ambiguous
4.3 Evaluation
The Web corpus we prepared consists of 660,062 pages
and contains 733M words. Table 2 shows the size of
the written and spoken language corpora which were col-
lected from the Web corpus.
Table 2: The size of the corpora
# of pages # of words
The Web corpus 660,062 733M
Written language corpus 80,685 77M
Spoken language corpus 73,977 113M
Size comparison The reason why written and spo-
ken language corpora were collected from the Web is
that Japanese spoken language corpora available are too
small. As far as we know, the biggest Japanese one
is Spontaneous Speech Corpus of Japanese, which con-
tains 7M words (Maekawa et al, 2000). Our corpus is
about ten times as big as Spontaneous Speech Corpus of
Japanese.
Precision of our method What is important for our
method is not recall but precision. Even if the recall is
not high we can collect large corpora, because the Web
corpus is very huge. However, if the precision is low, it is
impossible to collect corpora with high quality.
240 pages of the written and spoken language cor-
pora were extracted at random, and the precision of our
method was evaluated. The 240 pages consist of 125
pages collected as written language corpus and 115 pages
collected as spoken language corpus. Two judges (here-
after judge 1 and 2) respectively assessed how many of
the 240 pages were classified properly.
The result is shown in Table 3. The judge 1 identified
228 pages as properly classified ones; the judge 2 iden-
tified 221 pages as properly classified ones. The average
precision of the total was 94% (=228+221/240+240) and
we can say that our corpora have sufficient quality.
Table 3: # of pages properly collected
Judge 1 Judge 2
Written language corpus 119/125 110/125
Spoken language corpus 109/115 111/115
Total 228/240 221/240
Discussion Pages which were inappropriately collected
were examined, and it was found that lexical information
is useful in order to properly classify them. (5) is an ex-
ample which means ?A new software is exciting?.
(5) atarashii
new
sohuto-ha
software
wakuwakusuru
exiting
(5) is in spoken language, although it does not include any
familiarity and politeness expressions. This is because of
the word ?wakuwakusuru?, which is informal and means
?exiting?.
On way to deal with such pages is to use words charac-
teristic of written or spoken language. Such words will be
able to be gathered form our written and spoken language
corpora. It is our future work to improve the quality of
our corpora in an iterative way.
5 Paraphrase Pair Selection
A paraphrase pair we want is one in which the source
is UES and the target is SES. From the paraphrase pairs
learned in Section 3, such paraphrase pairs are selected
using the written and spoken language corpora.
Occurrence probabilities (OPs) of expressions in the
written and spoken language corpora can be used to dis-
tinguish UES and SES. This is because:
 An expression is likely to be UES if its OP in spoken
language corpora is very low.
 An expression is likely to be UES, if its OP in writ-
ten language corpora is much higher than that in spo-
ken language corpora.
For example, Table 4 shows OP of ?jikaisuru?. It is a
difficult verb which means ?to admonish oneself?, and
rarely used in a conversation. The verb ?jikaisuru? ap-
peared 14 times in the written language corpus, which
contains 6.1M predicates, and 7 times in the spoken lan-
guage corpus, which contains 11.7M predicates. The OP
of jikaisuru in spoken language corpus is low, compared
Table 4: Occurrence probability of ?jikaisuru?
written language spoken language
corpus corpus
# of jikaisuru 14 7
# of predicates 6.1M 11.7M
OP of jikaisuru 14 6.1M 7 11.7M
with that in written language corpus. Therefore, we can
say that ?jikaisuru? is UES.
The paraphrase pair we want can be selected based on
the following four OPs.
(1) OP of source in the written language corpus
(2) OP of source in the spoken language corpus
(3) OP of target in the written language corpus
(4) OP of target in the spoken language corpus
The selection can be considered as a binary classification
task: paraphrase pairs in which source is UES and target
is SES are treated as positive, and others are negative.
We propose a method based on Support Vector Machine
(Vapnik, 1995). The four OPs above are used as features.
5.1 Feature calculation
The method of calculating OP of an expression  ( 
 ) in a corpus is described. According to the
method, those four features can be calculated. The
method is broken down into two steps: counting the fre-
quency of , and calculation of   using the fre-
quency.
Frequency After a corpus is processed by the Japanese
morphological analyzer (JUMAN) and the parser
(KNP)6, the frequency of e ( ) is counted. Although
the frequency is often obvious from the analysis result,
there are several issues to be discussed.
The frequency of a predicate is sometimes quite differ-
ent from that of the same predicate in the different voice.
Therefore, the same predicates which have different voice
should be treated as different predicates.
As already mentioned in Section 3, the form of source
is ?predicate? and that of target is ?adjective noun+ pred-
icate?. If e is target and contains adverbs and nouns, it is
difficult to count the frequency because of the sparse data
problem. In order to avoid the problem, an approximation
that the adverbs are ignored is used. For example, the fre-
quency of ?run fast? is approximated by that of ?run?. We
did not ignore the noun because of the following reason.
As a noun and a predicate forms an idiomatic phrase more
often than an adverb and a predicate, the meaning of such
idiomatic phrase completely changes without the noun.
6http://www.kc.t.u-tokyo.ac.jp/nl-resource/knp-e.html
If the form of target is ?adverb noun predicate?, the
frequency is approximated by that of ?noun predicate?,
which is counted based on the parse result. However,
generally speaking, the accuracy of Japanese parser is
low compared with that of Japanese morphological an-
alyzer; the former is about 90% while the latter about
99%. Therefore, only reliable part of the parse result is
used in the same way as Kawahara et al did. See (Kawa-
hara and Kurohashi, 2001) for the details. Kawahara et
al. reported that 97% accuracy is achieved in the reliable
part.
Occurrence probability In general,   is defined
as:
      # of expressions in a corpus.
  tends to be small when  contains a noun, because
only a reliable part of the parsed corpus is used to count
 . Therefore, the value of the denominator ?# of ex-
pressions in a corpus? should be changed depending on
whether  contains a noun or not. The occurrence proba-
bility is defined as follows:
if  does not contain any nouns
      # of predicates in a corpus.
otherwise
      # of noun-predicates in a
corpus.
Table 5 illustrates # of predicates and # of noun-
predicates in our corpora.
Table 5: # of predicates, and # of noun-predicates
# of predicates # of noun-predicates
written language corpus 6.1M 1.5M
spoken language corpus 11.7M 1.9M
5.2 Evaluation
The two judges built a data set, and 20-hold cross valida-
tion was used.
Data set 267 paraphrase pairs were extracted at random
form the 5,836 paraphrase pairs learned in section 3. Two
judges independently tagged each of the 267 paraphrase
pairs as positive or negative. Then, only such paraphrase
pairs that were agreed upon by both of them were used as
data set. The data set consists of 200 paraphrase pairs (70
positive pairs and 130 negative pairs).
Experimental result We implemented the system us-
ing Tiny SVM package7.The Kernel function explored
was the polynomial function of degree 2.
Using 20-hold cross validation, two types of feature
sets (F-set1 and F-set2) were evaluated. F-set1 is a fea-
ture set of all the four features, and F-set2 is that of only
two features: OP of source in the spoken language cor-
pus, and OP of target in the spoken language corpus.
The results were evaluated through three measures: ac-
curacy of the classification (positive or negative), preci-
sion of positive paraphrase pairs, and recall of positive
paraphrase pairs. Table 6 shows the result. The accuracy,
precision and recall of F-set1 were 76 %, 70 % and 73 %
respectively. Those of F-set2 were 75 %, 67 %, and 69
%.
Table 6: Accuracy, precision and recall
F-set1 F-set2
Accuracy 76% 75%
Precision 70% 67%
Recall 73% 69%
Table 7 shows examples of classification. The para-
phrase pair (1) is positive example and the paraphrase
pair (2) is negative, and both of them were successfully
classified. The source of (1) appears only 10 times in the
spoken language corpus, on the other hand, the source of
(2) does 67 times.
Discussion It is challenging to detect the connotational
difference between lexical paraphrases, and all the fea-
tures were not explicitly given but estimated using the
corpora which were prepared in the unsupervised man-
ner. Therefore, we think that the accuracy of 76 % is very
high.
The result of F-set1 exceeds that of F-set2. This in-
dicates that comparing   in the written and spoken
language corpus is effective.
Calculated   was occasionally quite far from our
intuition. One example is that of ?kangekisuru?, which is
a very difficult verb that means ?to watch a drama?. Al-
though the verb is rarely used in real spoken language,
its occurrence probability in the spoken language corpus
was very high: the verb appeared 9 times in the writ-
ten language corpus and 69 times in the spoken language
corpus. We examined those corpora, and found that the
spoken language corpus happens to contain a lot of texts
about dramas. Such problems caused by biased topics
will be resolved by collecting corpora form larger Web
corpus.
7http://cl.aist-nara.ac.jp/?taku-ku/software/TinySVM/
Table 7: Successfully classified paraphrase pairs
Occurrence probabilities
Paraphrase pair source target
written language spoken language written language spoken language
(1) denraisuru  tsutawaru 43/6.1M 10/11.7M 1,927/6.1M 4,213/11.7Mto descend to be transmitted
(2) hebaru  hetohetoni tsukareru 18/6.1M 67/11.7M 1,026/6.1M 7,829/11.7Mto be tired out to be exhausted
6 Future Work
In order to estimate more reliable features, we are going
to increase the size of our corpora by preparing larger
Web corpus.
Although the paper has discussed paraphrasing from
the point of view that an expression is UES or SES, there
are a variety of SESs such as slang or male/female speech
etc. One of our future work is to examine what kind of
spoken language is suitable for such a kind of application
that was illustrated in the introduction.
This paper has focused only on paraphrasing predi-
cates. However, there are other kinds of paraphrasing
which are necessary in order to paraphrase written lan-
guage text into spoken language. For example, para-
phrasing compound nouns or complex syntactic structure
is the task to be tackled.
7 Conclusion
This paper represented the method of learning paraphrase
pairs in which source is UES and target is SES. The key
notion of the method is to identify UES and SES based
on the occurrence probability in the written and spoken
language corpora which are automatically collected from
the Web. The experimental result indicated that reliable
corpora can be collected sufficiently, and the occurrence
probability calculated from the corpora is useful to iden-
tify UES and SES.
References
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL
2003.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting of the Association
for Computational Linguistics, pages 50?57.
Ivan Bulyko, Mari Ostenforf, and Andreas Stolcke.
2003. Getting more mileage from web text sources
for conversational speech language modeling using
class-dependent mixtures. In Proceedings of HLT-
NAACL2003, pages 7?9.
Florence Duclaye and FranC?ois Yvon. 2003. Learn-
ing paraphrases to improve a question-answering sys-
tem. In Proceedings of the 10th Conference of EACL
Workshop Natural Language Processing for Question-
Answering.
Philip Edmonds and Graeme Hirst. 2002. Near-
synonymy and lexical choice. Computational Linguis-
tics, 28(2):105?144.
Tomohiro Fukuhara, Toyoaki Nishida, and Shunsuke Ue-
mura. 2001. Public opinion channel: A system for
augmenting social intelligence of a community. In
Workshop notes of the JSAI-Synsophy International
Conference on Social Intelligence Design, pages 22?
25.
Masaki Hayashi, Hirotada Ueda, Tsuneya Kurihara,
Michiaki Yasumura, Mamoru Douke, and Kyoko
Ariyasu. 1999. Tvml (tv program making language) -
automatic tv program generation from text-based script
-. In ABU Technical Review.
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural language based reformulation
resource and web exploitation for question answering.
In Proceedings of TREC 2002 Conference.
Diana Zaiu Inkpen and Graeme Hirst. 2001. Building a
lexical knowledge-base of near-synonym differences.
In Proceedings of Workshop on WordNet and Other
Lexical Sources, pages 47?52.
Kentaro Inui and Satomi Yamamoto. 2001. Corpus-
based acquisition of sentence readability ranking mod-
els for deaf people. In Proceedings of NLPRS 2001.
Kentaro Inui, Atsushi Fujita, Tetsuro Takahashi, Ryu
Iida, and Tomoya Iwakura. 2003. Text simplification
for reading assistance: A project note. In Proceedings
of the Second International Workshop on Paraphras-
ing, pages 9?16.
Nobuhiro Kaji, Daisuke Kawahara, Sadao Kurohashi,
and Satoshi Sato. 2002. Verb paraphrase based on
case frame alignment. In Proceedings of ACL 2002,
pages 215?222.
Daisuke Kawahara and Sadao Kurohashi. 2001.
Japanese case frame construction by coupling the verb
and its closest case component. In Proceedings of HLT
2001, pages 204?210.
Adam Kilgarriff. 2001. Comparing corpora. Interna-
tional Journal of Corpus Linguistics.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Journal of Natural
Language Engneering, 7(4):343?360.
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-
toshi Isahara. 2000. Spontaneous speech corpus of
japanese. In Proceedings of LREC 2000, pages 947?
952.
Akiyo Nadamoto, Hiroyuki Kondo, and Katsumi Tanaka.
2001. Webcarousel: Restructuring web search results
for passive viewing in mobile environments. In 7th
International ?Conference on Database Systems for
Advanced Applications, pages 164?165.
Hatsutaroh Ohishi, editor. 1970. Hanashi Ko-
toba(Spoken Language). Bunkacho.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating sentences. In Pro-
ceedings of HLT-NAACL 2003.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proceedings of HLT 2002.
Jyunichi Tadika, editor. 1997. Reikai Shougaku Kokugo-
jiten (Japanese dictionary for children). Sanseido.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,
Hirofumi Yamamoto, and Seiichi Yamamoto. 2002.
Toward a broad-coverage bilingual corpus for speech
translation of travel conversations in the real world. In
Proceedings of LREC 2002, pages 147?152.
George Tambouratzis, Stella Markantonatou, Nikolaos
Hairetakis, Marina Vassiliou, Dimitrios Tambouratzis,
and George Carayannis. 2000. Discriminating the reg-
isters and styles in the modern greek language. In Pro-
ceedings of Workshop on Comparing Corpora 2000.
Vladimir Vapnik. 1995. The Nature of Statistical Learn-
ing Theory. Springer.
 
				Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 452?459,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic Construction of Polarity-tagged Corpus from HTML
Documents
Nobuhiro Kaji and Masaru Kitsuregawa
Institute of Industrial Science
the University of Tokyo
4-6-1 Komaba, Meguro-ku, Tokyo 153-8505 Japan
 kaji,kitsure@tkl.iis.u-tokyo.ac.jp
Abstract
This paper proposes a novel method
of building polarity-tagged corpus from
HTML documents. The characteristics of
this method is that it is fully automatic and
can be applied to arbitrary HTML docu-
ments. The idea behind our method is
to utilize certain layout structures and lin-
guistic pattern. By using them, we can
automatically extract such sentences that
express opinion. In our experiment, the
method could construct a corpus consist-
ing of 126,610 sentences.
1 Introduction
Recently, there has been an increasing interest in
such applications that deal with opinions (a.k.a.
sentiment, reputation etc.). For instance, Mori-
naga et al developed a system that extracts and
analyzes reputations on the Internet (Morinaga et
al., 2002). Pang et al proposed a method of clas-
sifying movie reviews into positive and negative
ones (Pang et al, 2002).
In these applications, one of the most important
issue is how to determine the polarity (or semantic
orientation) of a given text. In other words, it is
necessary to decide whether a given text conveys
positive or negative content.
In order to solve this problem, we intend to
take statistical approach. More specifically, we
plan to learn the polarity of texts from a cor-
pus in which phrases, sentences or documents
are tagged with labels expressing the polarity
(polarity-tagged corpus).
So far, this approach has been taken by a lot of
researchers (Pang et al, 2002; Dave et al, 2003;
Wilson et al, 2005). In these previous works,
polarity-tagged corpus was built in either of the
following two ways. It is built manually, or created
from review sites such as AMAZON.COM. In some
review sites, the review is associated with meta-
data indicating its polarity. Those reviews can be
used as polarity-tagged corpus. In case of AMA-
ZON.COM, the review?s polarity is represented by
using 5-star scale.
However, both of the two approaches are not
appropriate for building large polarity-tagged cor-
pus. Since manual construction of tagged corpus
is time-consuming and expensive, it is difficult to
build large polarity-tagged corpus. The method
that relies on review sites can not be applied to
domains in which large amount of reviews are not
available. In addition, the corpus created from re-
views is often noisy as we discuss in Section 2.
This paper proposes a novel method of building
polarity-tagged corpus from HTML documents.
The idea behind our method is to utilize certain
layout structures and linguistic pattern. By using
them, we can automatically extract sentences that
express opinion (opinion sentences) from HTML
documents. Because this method is fully auto-
matic and can be applied to arbitrary HTML doc-
uments, it does not suffer from the same problems
as the previous methods.
In the experiment, we could construct a corpus
consisting of 126,610 sentences. To validate the
quality of the corpus, two human judges assessed
a part of the corpus and found that 92% opinion
sentences are appropriate ones. Furthermore, we
applied our corpus to opinion sentence classifica-
tion task. Naive Bayes classifier was trained on
our corpus and tested on three data sets. The re-
sult demonstrated that the classifier achieved more
than 80% accuracy in each data set.
The following of this paper is organized as fol-
452
lows. Section 2 shows the design of the corpus
constructed by our method. Section 3 gives an
overview of our method, and the detail follows in
Section 4. In Section 5, we discuss experimen-
tal results, and in Section 6 we examine related
works. Finally we conclude in Section 7.
2 Corpus Design
This Section explains the design of our corpus that
is built automatically. Table 1 represents a part
of our corpus that was actually constructed in the
experiment. Note that this paper treats Japanese.
The sentences in the Table are translations, and the
original sentences are in Japanese.
The followings are characteristics of our corpus:
  Our corpus uses two labels,   and . They
denote positive and negative sentences re-
spectively. Other labels such as ?neutral? are
not used.
  Since we do not use ?neutral? label, such sen-
tence that does not convey opinion is not
stored in our corpus.
  The label is assigned to not multiple sen-
tences (or document) but single sentence.
Namely, our corpus is tagged at sentence
level rather than document level.
It is important to discuss the reason that we in-
tend to build a corpus tagged at sentence level
rather than document level. The reason is that one
document often includes both positive and nega-
tive sentences, and hence it is difficult to learn
the polarity from the corpus tagged at document
level. Consider the following example (Pang et
al., 2002):
This film should be brilliant. It sounds
like a great plot, the actors are first
grade, and the supporting cast is good as
well, and Stallone is attempting to de-
liver a good performance. However, it
can?t hold up.
This document as a whole expresses negative
opinion, and should be labeled ?negative? if it is
tagged at document level. However, it includes
several sentences that represent positive attitude.
We would like to point out that polarity-tagged
corpus created from reviews prone to be tagged at
document-level. This is because meta-data (e.g.
stars in AMAZON.COM) is usually associated with
one review rather than individual sentences in a
review. This is one serious problem in previous
works.
Table 1: A part of automatically constructed
polarity-tagged corpus.
label opinion sentence
  It has high adaptability.
 The cost is expensive.
 The engine is powerless and noisy.
  The usage is easy to understand.
  Above all, the price is reasonable.
3 The Idea
This Section briefly explains our basic idea, and
the detail of our corpus construction method is
represented in the next Section.
Our idea is to use certain layout structures and
linguistic pattern in order to extract opinion sen-
tences from HTML documents. More specifically,
we used two kinds of layout structures: the item-
ization and the table. In what follows, we ex-
plain examples where opinion sentences can be
extracted by using the itemization, table and lin-
guistic pattern.
3.1 Itemization
The first idea is to extract opinion sentences from
the itemization (Figure 1). In this Figure, opinions
about a music player are itemized and these item-
izations have headers such as ?pros? and ?cons?.
By using the headers, we can recognize that opin-
ion sentences are described in these itemizations.
Pros:
  The sound is natural.
  Music is easy to find.
  Can enjoy creating my favorite play-lists.
Cons:
  The remote controller does not have an LCD dis-
play.
  The body gets scratched and fingerprinted easily.
  The battery drains quickly when using the back-
light.
Figure 1: Opinion sentences in itemization.
Hereafter, such phrases that indicate the pres-
453
ence of opinion sentences are called indicators.
Indicators for positive sentences are called positive
indicators. ?Pros? is an example of positive indi-
cator. Similarly, indicators for negative sentences
are called negative indicators.
3.2 Table
The second idea is to use the table structure (Fig-
ure 2). In this Figure, a car review is summarized
in the table.
Mileage(urban) 7.0km/litter
Mileage(highway) 9.0km/litter
Plus This is a four door car, but it?s
so cool.
Minus The seat is ragged and the light
is dark.
Figure 2: Opinion sentences in table.
We can predict that there are opinion sentences
in this table, because the left column acts as a
header and there are indicators (plus and minus)
in that column.
3.3 Linguistic pattern
The third idea is based on linguistic pattern. Be-
cause we treat Japanese, the pattern that is dis-
cussed in this paper depends on Japanese gram-
mar although we think there are similar patterns in
other languages including English.
Consider the Japanese sentences attached with
English translations (Figure 3). Japanese sen-
tences are written in italics and ?-? denotes that
the word is followed by postpositional particles.
For example, ?software-no? means that ?software?
is followed by postpositional particle ?no?. Trans-
lations of each word and the entire sentence are
represented below the original Japanese sentence.
?-POST? means postpositional particle.
In the examples, we focused on the singly un-
derlined phrases. Roughly speaking, they corre-
spond to ?the advantage/weakness is to? in En-
glish. In these phrases, indicators (?riten (ad-
vantage)? and ?ketten (weakness)?) are followed
by postpositional particle ?-ha?, which is topic
marker. And hence, we can recognize that some-
thing good (or bad) is the topic of the sentence.
Based on this observation, we crafted a linguis-
tic pattern that can detect the singly underlined
phrases. And then, we extracted doubly under-
lined phrases as opinions. They correspond to ?run
quickly? and ?take too much time?. The detail of
this process is discussed in the next Section.
4 Automatic Corpus Construction
This Section represents the detail of the corpus
construction procedure.
As shown in the previous Section, our idea uti-
lizes the indicator, and it is important to recognize
indicators in HTML documents. To do this, we
manually crafted lexicon, in which positive and
negative indicators are listed. This lexicon con-
sists of 303 positive and 433 negative indicators.
Using this lexicon, the polarity-tagged corpus is
constructed from HTML documents. The method
consists of the following three steps:
1. Preprocessing
Before extracting opinion sentences, HTML
documents are preprocessed. This process
involves separating texts form HTML tags,
recognizing sentence boundary, and comple-
menting omitted HTML tags etc.
2. Opinion sentence extraction
Opinion sentences are extracted from HTML
documents by using the itemization, table
and linguistic pattern.
3. Filtering
Since HTML documents are noisy, some of
the extracted opinion sentences are not ap-
propriate. They are removed in this step.
For the preprocessing, we implemented simple
rule-based system. We cannot explain its detail
for lack of space. In the remainder of this Section,
we describe three extraction methods respectively,
and then examine filtering technique.
4.1 Extraction based on itemization
The first method utilizes the itemization. In order
to extract opinion sentences, first of all, we have
to find such itemization as illustrated in Figure 1.
They are detected by using indicator lexicon and
HTML tags such as  h1 and  ul etc.
After finding the itemizations, the sentences in
the items are extracted as opinion sentences. Their
polarity labels are assigned according to whether
the header is positive or negative indicator. From
the itemization in Figure 1, three positive sen-
tences and three negative ones are extracted.
The problem here is how to treat such item that
has more than one sentences (Figure 4). In this
itemization, there are two sentences in each of the
454
(1) kono software-no riten-ha hayaku ugoku koto
this software-POST advantage-POST quickly run to
The advantage of this software is to run quickly.
(2) ketten-ha jikan-ga kakarisugiru koto-desu
weakness-POST time-POST take too much to-POST
The weakness is to take too much time.
Figure 3: Instances of the linguistic pattern.
third and fourth item. It is hard to precisely pre-
dict the polarity of each sentence in such items,
because such item sometimes includes both posi-
tive and negative sentences. For example, in the
third item of the Figure, there are two sentences.
One (?Has high pixel...?) is positive and the other
(?I was not satisfied...?) is negative.
To get around this problem, we did not use such
items. From the itemization in Figure 4, only two
positive sentences are extracted (?the color is re-
ally good? and ?this camera makes me happy while
taking pictures?).
Pros:
  The color is really good.
  This camera makes me happy while taking pic-
tures.
  Has high pixel resolution with 4 million pixels. I
was not satisfied with 2 million.
  EVF is easy to see. But, compared with SLR, it?s
hard to see.
Figure 4: Itemization where more than one sen-
tences are written in one item.
4.2 Extraction based on table
The second method extracts opinion sentences
from the table. Since the combination of  table
and other tags can represent various kinds of ta-
bles, it is difficult to craft precise rules that can
deal with any table.
Therefore, we consider only two types of tables
in which opinion sentences are described (Figure
5). Type A is a table in which the leftmost column
acts as a header, and there are indicators in that
column. Similarly, type B is a table in which the
first row acts as a header. The table illustrated in
Figure 2 is categorized into type A.
The type of the table is decided as follows. The
table is categorized into type A if there are both
type A
 
 
     
 
 
  
type B
?  
 
 
 
?
?    ?
?    ?
?    ?

 
:positive indicator  :positive sentence

 
:negative indicator :negative sentence
Figure 5: Two types of tables.
positive and negative indicators in the leftmost col-
umn. The table is categorized into type B if it is
not type A and there are both positive and negative
indicators in the first row.
After the type of the table is decided, we can
extract opinion sentences from the cells that cor-
respond to   and  in the Figure 5. It is obvi-
ous which label (positive or negative) should be
assigned to the extracted sentence.
We did not use such cell that contains more than
one sentences, because it is difficult to reliably
predict the polarity of each sentence. This is simi-
lar to the extraction from the itemization.
4.3 Extraction based on linguistic pattern
The third method uses linguistic pattern. The char-
acteristic of this pattern is that it takes dependency
structure into consideration.
First of all, we explain Japanese dependency
structure. Figure 6 depicts the dependency rep-
resentations of the sentences in the Figure 3.
Japanese sentence is represented by a set of de-
pendencies between phrasal units called bunsetsu-
phrases. Broadly speaking, bunsetsu-phrase is an
unit similar to baseNP in English. In the Fig-
ure, square brackets enclose bunsetsu-phrase and
arrows show modifier  head dependencies be-
tween bunsetsu-phrases.
In order to extract opinion sentences from these
dependency representations, we crafted the fol-
lowing dependency pattern.
455
[ kono
this
] [ software-no
software-POST
] [ riten-ha
advantage-POST
] [ hayaku
quickly
] [ ugoku
run
] [ koto
to
]
[ ketten-ha
weakness-POST
] [ jikan-ga
time-POST
] [ kakari sugiru
take too much
] [ koto-desu
to-POST
]
Figure 6: Dependency representations.
[ INDICATOR-ha ] [ koto-POST* ]
This pattern matches the singly underlined
bunsetsu-phrases in the Figure 6. In the modi-
fier part of this pattern, the indicator is followed
by postpositional particle ?ha?, which is topic
marker1. In the head part, ?koto (to)? is followed
by arbitrary numbers of postpositional particles.
If we find the dependency that matches this pat-
tern, a phrase between the two bunsetsu-phrases
is extracted as opinion sentence. In the Figure 6,
the doubly underlined phrases are extracted. This
heuristics is based on Japanese word order con-
straint.
4.4 Filtering
Sentences extracted by the above methods some-
times include noise text. Such texts have to be fil-
tered out. There are two cases that need filtering
process.
First, some of the extracted sentences do not ex-
press opinions. Instead, they represent objects to
which the writer?s opinion is directed (Table 7).
From this table, ?the overall shape? and ?the shape
of the taillight? are wrongly extracted as opinion
sentences. Since most of the objects are noun
phrases, we removed such sentences that have the
noun as the head.
Mileage(urban) 10.0km/litter
Mileage(highway) 12.0km/litter
Plus The overall shape.
Minus The shape of the taillight.
Figure 7: A table describing only objects to which
the opinion is directed.
Secondly, we have to treat duplicate opinion
sentences because there are mirror sites in the
1To be exact, some of the indicators such as ?strong point?
consists of more than one bunsetsu-phrase, and the modifier
part sometimes consists of more than one bunsetsu-phrase.
HTML documents. When there are more than one
sentences that are exactly the same, one of them is
held and the others are removed.
5 Experimental Results and Discussion
This Section examines the results of corpus con-
struction experiment. To analyze Japanese sen-
tence we used Juman and KNP2.
5.1 Corpus Construction
About 120 millions HTML documents were pro-
cessed, and 126,610 opinion sentences were ex-
tracted. Before the filtering, there were 224,002
sentences in our corpus. Table2 shows the statis-
tics of our corpus. The first column represents the
three extraction methods. The second and third
column shows the number of positive and nega-
tive sentences by extracted each method. Some
examples are illustrated in Table 3.
Table 2: # of sentences in the corpus.
Positive Negative Total
Itemization 18,575 15,327 33,902
Table 12,103 11,016 23,119
Linguistic Pattern 34,282 35,307 69,589
Total 64,960 61,650 126,610
The result revealed that more than half of the
sentences are extracted by linguistic pattern (see
the fourth row). Our method turned out to be ef-
fective even in the case where only plain texts are
available.
5.2 Quality assessment
In order to check the quality of our corpus,
500 sentences were randomly picked up and two
judges manually assessed whether appropriate la-
bels are assigned to the sentences.
The evaluation procedure is the followings.
2http://www.kc.t.u-tokyo.ac.jp/nl-resource/top.html
456
Table 3: Examples of opinion sentences.
label opinion sentence
 
cost keisan-ga yoininaru
cost computation-POST become easy
It becomes easy to compute cost.
 
kantan-de jikan-ga setsuyakudekiru
easy-POST time-POST can save
It?s easy and can save time.
 
soup-ha koku-ga ari oishii
soup-POST rich flavorful
The soup is rich and flavorful.

HTML keishiki-no mail-ni taioshitenai
HTML format-POST mail-POST cannot use
Cannot use mails in HTML format.

jugyo-ga hijoni tsumaranai
lecture-POST really boring
The lecture is really boring.

kokoro-ni nokoru ongaku-ga nai
impressive music-POST there is no
There is no impressive music.
  Each of the 500 sentences are shown to the
two judges. Throughout this evaluation, We
did not present the label automatically tagged
by our method. Similarly, we did not show
HTML documents from which the opinion
sentences are extracted.
  The two judges individually categorized each
sentence into three groups: positive, negative
and neutral/ambiguous. The sentence is clas-
sified into the third group, if it does not ex-
press opinion (neutral) or if its polarity de-
pends on the context (ambiguous). Thus, two
goldstandard sets were created.
  The precision is estimated using the goldstan-
dard. In this evaluation, the precision refers
to the ratio of sentences where correct la-
bels are assigned by our method. Since we
have two goldstandard sets, we can report
two different precision values. A sentence
that is categorized into neutral/ambiguous by
the judge is interpreted as being assigned in-
correct label by our method, since our corpus
does not have a label that corresponds to neu-
tral/ambiguous.
We investigated the two goldstandard sets, and
found that the judges agree with each other in 467
out of 500 sentences (93.4%). The Kappa value
was 0.901. From this result, we can say that the
goldstandard was reliably created by the judges.
Then, we estimated the precision. The precision
was 459/500 (91.5%) when one goldstandard was
used, and 460/500 (92%) when the other was used.
Since these values are nearly equal to the agree-
ment between humans (467/500), we can conclude
that our method successfully constructed polarity-
tagged corpus.
After the evaluation, we analyzed errors and
found that most of them were caused by the lack
of context. The following is a typical example.
You see, there is much information.
In our corpus this sentence is categorized into pos-
itive one. The below is a part of the original docu-
ment from which this sentence was extracted.
I recommend this guide book. The Pros.
of this book is that, you see, there is
much information.
On the other hand, both of the two judges catego-
rized the above sentence into neutral/ambiguous,
probably because they can easily assume context
where much information is not desirable.
You see, there is much information. But,
it is not at all arranged, and makes me
confused.
In order to precisely treat this kind of sentences,
we think discourse analysis is inevitable.
5.3 Application to opinion classification
Next, we applied our corpus to opinion sentence
classification. This is a task of classifying sen-
tences into positive and negative. We trained a
classifier on our corpus and investigated the result.
Classifier and data sets As a classifier, we
chose Naive Bayes with bag-of-words features,
because it is one of the most popular one in this
task. Negation was processed in a similar way as
previous works (Pang et al, 2002).
To validate the accuracy of the classifier, three
data sets were created from review pages in which
the review is associated with meta-data. To build
data sets tagged at sentence level, we used such re-
views that contain only one sentence. Table 4 rep-
resents the domains and the number of sentences
in each data set. Note that we confirmed there is
no duplicate between our corpus and the these data
sets.
The result and discussion Naive Bayes classi-
fier was trained on our corpus and tested on the
three data sets (Table 5). In the Table, the sec-
ond column represents the accuracy of the clas-
sification in each data set. The third and fourth
457
Table 5: Classification result.
Accuracy Positive Negative
Precision Recall Precision Recall
Computer 0.831 0.856 0.804 0.804 0.859
Restaurant 0.849 0.905 0.859 0.759 0.832
Car 0.833 0.860 0.844 0.799 0.819
Table 4: The data sets.
Domain # of sentences
Positive Negative
Computer 933 910
Restaurant 753 409
Car 1,056 800
columns represent precision and recall of positive
sentences. The remaining two columns show those
of negative sentences. Naive Bayes achieved over
80% accuracy in all the three domains.
In order to compare our corpus with a small
domain specific corpus, we estimated accuracy in
each data set using 10 fold crossvalidation (Ta-
ble 6). In two domains, the result of our corpus
outperformed that of the crossvalidation. In the
other domain, our corpus is slightly better than the
crossvalidation.
Table 6: Accuracy comparison.
Our corpus Crossvalidation
Computer 0.831 0.821
Restaurant 0.849 0.848
Car 0.833 0.808
One finding is that our corpus achieved good ac-
curacy, although it includes various domains and is
not accustomed to the target domain. Turney also
reported good result without domain customiza-
tion (Turney, 2002). We think these results can be
further improved by domain adaptation technique,
and it is one future work.
Furthermore, we examined the variance of the
accuracy between different domains. We trained
Naive Bayes on each data set and investigate the
accuracy in the other data sets (Table 7). For ex-
ample, when the classifier is trained on Computer
and tested on Restaurant, the accuracy was 0.757.
This result revealed that the accuracy is quite poor
when the training and test sets are in different do-
mains. On the other hand, when Naive Bayes is
trained on our corpus, there are little variance in
different domains (Table 5). This experiment in-
dicates that our corpus is relatively robust against
the change of the domain compared with small do-
main specific corpus. We think this is because our
corpus is large and balanced. Since we cannot al-
ways get domain specific corpus in real applica-
tion, this is the strength of our corpus.
Table 7: Cross domain evaluation.
Training
Computer Restaurant Car
Computer ? 0.701 0.773
Test Restaurant 0.757 ? 0.755
Car 0.751 0.711 ?
6 Related Works
6.1 Learning the polarity of words
There are some works that discuss learning the po-
larity of words instead of sentences.
Hatzivassiloglou and McKeown proposed a
method of learning the polarity of adjectives from
corpus (Hatzivassiloglou and McKeown, 1997).
They hypothesized that if two adjectives are con-
nected with conjunctions such as ?and/but?, they
have the same/opposite polarity. Based on this hy-
pothesis, their method predicts the polarity of ad-
jectives by using a small set of adjectives labeled
with the polarity.
Other works rely on linguistic resources such
as WordNet (Kamps et al, 2004; Hu and Liu,
2004; Esuli and Sebastiani, 2005; Takamura et al,
2005). For example, Kamps et al used a graph
where nodes correspond to words in the Word-
Net, and edges connect synonymous words in the
WordNet. The polarity of an adjective is defined
by its shortest paths from the node corresponding
to ?good? and ?bad?.
Although those researches are closely related to
our work, there is a striking difference. In those
researches, the target is limited to the polarity of
words and none of them discussed sentences. In
addition, most of the works rely on external re-
sources such as the WordNet, and cannot treat
words that are not in the resources.
458
6.2 Learning subjective phrases
Some researchers examined the acquisition of sub-
jective phrases. The subjective phrase is more gen-
eral concept than opinion and includes both posi-
tive and negative expressions.
Wiebe learned subjective adjectives from a set
of seed adjectives. The idea is to automatically
identify the synonyms of the seed and to add them
to the seed adjectives (Wiebe, 2000). Riloff et
al. proposed a bootstrapping approach for learn-
ing subjective nouns (Riloff et al, 2003). Their
method learns subjective nouns and extraction pat-
terns in turn. First, given seed subjective nouns,
the method learns patterns that can extract sub-
jective nouns from corpus. And then, the pat-
terns extract new subjective nouns from corpus,
and they are added to the seed nouns. Although
this work aims at learning only nouns, in the sub-
sequent work, they also proposed a bootstrapping
method that can deal with phrases (Riloff and
Wiebe, 2003). Similarly, Wiebe also proposes a
bootstrapping approach to create subjective and
objective classifier (Wiebe and Riloff, 2005).
These works are different from ours in a sense
that they did not discuss how to determine the po-
larity of subjective words or phrases.
6.3 Unsupervised sentiment classification
Turney proposed the unsupervised method for sen-
timent classification (Turney, 2002), and similar
method is utilized by many other researchers (Yu
and Hatzivassiloglou, 2003). The concept behind
Turney?s model is that positive/negative phrases
co-occur with words like ?excellent/poor?. The co-
occurrence statistic is measured by the result of
search engine. Since his method relies on search
engine, it is difficult to use rich linguistic informa-
tion such as dependencies.
7 Conclusion
This paper proposed a fully automatic method of
building polarity-tagged corpus from HTML doc-
uments. In the experiment, we could build a cor-
pus consisting of 126,610 sentences.
As a future work, we intend to extract more
opinion sentences by applying this method to
larger HTML document sets and enhancing ex-
traction rules. Another important direction is to
investigate more precise model that can classify or
extract opinions, and learn its parameters from our
corpus.
References
Kushal Dave, Steve Lawrence, and David M.Pennock.
2003. Mining the peanut gallery: Opinion extrac-
tion and semantic classification of product revews.
In Proceedings of the WWW, pages 519?528.
Andrea Esuli and Fabrizio Sebastiani. 2005. Deter-
mining the semantic orientation of terms throush
gloss classification. In Proceedings of the CIKM.
Vasileios Hatzivassiloglou and Katheleen R. McKe-
own. 1997. Predicting the semantic orientation of
adjectives. In Proceedings of the ACL, pages 174?
181.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of the
KDD, pages 168?177.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten de Rijke. 2004. Using wordnet to measure
semantic orientations of adjectives. In Proceedings
of the LREC.
Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi,
and Toshikazu Fukushima. 2002. Mining product
reputations on the web. In Proceedings of the KDD.
Bo Pang, Lillian Lee, and Shivakumar Vaihyanathan.
2002. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the
EMNLP.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the EMNLP.
Ellen Riloff, JanyceWiebe, and TheresaWilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the CoNLL.
Hiroya Takamura, Takashi Inui, andManabu Okumura.
2005. Extracting semantic orientation of words us-
ing spin model. In Proceedings of the ACL, pages
133?140.
Peter D. Turney. 2002. Thumbs up or thumbs down?
senmantic orientation applied to unsupervised clas-
sification of reviews. In Proceedings of the ACL,
pages 417?424.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In Proceedings of the CICLing.
Janyce M. Wiebe. 2000. Learning subjective adjec-
tives from corpora. In Proceedings of the AAAI.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the
HLT/EMNLP.
Hong Yu and Yasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opinion
sentences. In Proceedings of the EMNLP.
459
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 61?64,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
A Combination of Active Learning and Semi-supervised Learning 
Starting with Positive and Unlabeled Examples for Word Sense   
Disambiguation: An Empirical Study on Japanese Web Search Query 
Makoto Imamura 
and Yasuhiro Takayama 
Information Technology R&D Center,  
Mitsubishi Electric Corporation 
5-1-1 Ofuna, Kamakura, Kanagawa, Japan 
{Imamura.Makoto@bx,Takayama.Yasu 
hiro@ea}.MitsubishiElectric.co.jp 
Nobuhiro Kaji, Masashi Toyoda  
and Masaru Kitsuregawa 
Institute of Industrial Science, 
The University of Tokyo 
4-6-1 Komaba, Meguro-ku Tokyo, Japan 
{kaji,toyoda,kitsure} 
@tkl.iis.u-tokyo.ac.jp 
 
 
Abstract 
This paper proposes to solve the bottle-
neck of finding training data for word 
sense disambiguation (WSD) in the do-
main of web queries, where a complete set 
of ambiguous word senses are unknown. 
In this paper, we present a combination of 
active learning and semi-supervised learn-
ing method to treat the case when positive 
examples, which have an expected word 
sense in web search result, are only given. 
The novelty of our approach is to use 
?pseudo negative examples? with reliable 
confidence score estimated by a classifier 
trained with positive and unlabeled exam-
ples. We show experimentally that our 
proposed method achieves close enough 
WSD accuracy to the method with the 
manually prepared negative examples in 
several Japanese Web search data. 
1 Introduction 
In Web mining for sentiment or reputation 
analysis, it is important for reliable analysis to 
extract large amount of texts about certain prod-
ucts, shops, or persons with high accuracy. When 
retrieving texts from Web archive, we often suf-
fer from word sense ambiguity and WSD system 
is indispensable. For instance, when we try to 
analyze reputation of "Loft", a name of variety 
store chain in Japan, we found that simple text 
search retrieved many unrelated texts which con-
tain "Loft" with different senses such as an attic 
room, an angle of golf club face, a movie title, a 
name of a club with live music and so on. The 
words in Web search queries are often proper 
nouns. Then it is not trivial to discriminate these 
senses especially for the language like Japanese 
whose proper nouns are not capitalized. 
To train WSD systems we need a large 
amount of positive and negative examples. In the 
real Web mining application, how to acquire 
training data for a various target of analysis has 
become a major hurdle to use supervised WSD.  
Fortunately, it is not so difficult to create posi-
tive examples. We can retrieve positive examples 
from Web archive with high precision (but low 
recall) by manually augmenting queries with hy-
pernyms or semantically related words (e.g., 
"Loft AND shop" or "Loft AND stationary").  
On the other hand, it is often costly to create 
negative examples. In principle, we can create 
negative examples in the same way as we did to 
create positive ones. The problem is, however, 
that we are not sure of most of the senses of a 
target word. Because target words are often 
proper nouns, their word senses are rarely listed 
in hand-crafted lexicon. In addition, since the 
Web is huge and contains heterogeneous do-
mains, we often find a large number of unex-
pected senses. For example, all the authors did 
not know the music club meaning of Loft. As the 
result, we often had to spend much time to find 
such unexpected meaning of target words. 
This situation motivated us to study active 
learning for WSD starting with only positive ex-
amples. The previous techniques (Chan and Ng, 
2007; Chen et al 2006) require balanced positive 
and negative examples to estimate the score. In 
our problem setting, however, we have no nega-
tive examples at the initial stage. To tackle this 
problem, we propose a method of active learning 
for WSD with pseudo negative examples, which 
are selected from unlabeled data by a classifier 
trained with positive and unlabeled examples. 
McCallum and Nigam (1998) combined active 
learning and semi-supervised learning technique 
61
by using EM with unlabeled data integrated into 
active learning, but it did not treat our problem 
setting where only positive examples are given. 
The construction of this paper is as follows; 
Section 2 describes a proposed learning algo-
rithm. Section 3 shows the experimental results.  
2 Learning Starting with Positive and 
Unlabeled Examples for WSD 
We treat WSD problem as binary classification 
where desired texts are positive examples and 
other texts are negative examples. This setting is 
practical, because ambiguous senses other than 
the expected sense are difficult to know and are 
no concern in  most Web mining applications. 
2.1 Classifier 
For our experiment, we use naive Bayes classifi-
ers as learning algorithm. In performing WSD, 
the sense ?s? is assigned to an example charac-
terized with the probability of linguistic features 
f1,...,fn so as to maximize: 
?
=
n
j
pp
1
)|(f)( ss j               (1) 
The sense s is positive when it is the target 
meaning in Web mining application, otherwise s 
is negative. We use the following typical linguis-
tic features for Japanese sentence analysis, (a) 
Word feature within sentences, (b) Preceding 
word feature within bunsetsu (Japanese base 
phrase), (c) Backward word feature within bun-
setsu, (d) Modifier bunsetsu feature and (e) 
Modifiee bunsetsu feature. 
Using naive Bayes classifier, we can estimate 
the confidence score c(d, s) that the sense of a 
data instance ?d?, whose features are f1, f2, ..., fn, 
is predicted sense ?s?.  
?
=
+=
n
j
pp
1
)|(f log)( logs)c(d, ss j      (2) 
2.2 Proposed Algorithm 
At the beginning of our algorithm, the system is 
provided with positive examples and unlabeled 
examples. The positive examples are collected 
by full text queries with hypernyms or semanti-
cally related words. 
First we select positive dataset P from initial 
dataset by manually augmenting full text query.      
At each iteration of active learning, we select 
pseudo negative dataset Np (Figure 1 line 15). In 
selecting pseudo negative dataset, we predict 
word sense of each unlabeled example using the 
naive Bayes classifier with all the unlabeled ex-
amples as negative examples (Figure 2). In detail, 
if the prediction score (equation(3)) is more than 
?, which means the example is very likely to be 
negative, it is considered as the pseudo negative 
example (Figure 2 line 10-12). 
pos)c(d,neg)c(d,psdNeg)c(d, ?=          (3) 
 
01    # Definition 
02   ?(P, N): WSD system trained on P as Positive  
03                   examples, N as Negative examples.  
04   ?EM(P, N, U): WSD system trained on P as  
05   Positive examples, N as Negative examples, 
06   U as Unlabeled examples by using EM  
07   (Nigam et. all 2000) 
08    # Input 
09    T ? Initial unlabeled dataset which contain  
10            ambiguous words 
11    # Initialization 
12    P ?  positive training dataset by full text search on T 
13    N ? ? (initial negative training dataset) 
14    repeat 
15      # selecting pseudo negative examples Np  
16          by   the score of  ?(P, T-P)  (see figure 2) 
17      # building a classifier with  Np 
18      ?new ? ?EM (P,  N+Np, T-N-P)   
19      #  sampling data by using the score of ?new 
20      cmin   ? ? 
21      foreach d ? (T ? P ? N )  
22         classify d by WSD system?new 
23         s(d) ? word sense prediction for d using?new 
24         c(d, s(d)) ? the confidence of  prediction of d 
25         if c(d, s(d))  ? cmin   then  
26             cmin  ? c(d),   d min ? d 
27      end 
28    end 
29     provide correct sense s for d min  by human 
30     if s is positive then add d min   to P 
31                             else  add d min   to N 
32   until Training dataset reaches desirable size 
33   ?new  is the output classifier 
 Figure 1: A combination of active learning and 
semi-supervised learning starting with positive 
and unlabeled examples 
Next we use Nigam?s semi-supervised learning 
method using EM and a naive Bayes classifier 
(Nigam et. all, 2000) with pseudo negative data-
set Np  as negative training dataset to build the 
refined classifier ?EM (Figure 1 line 17).  
In building training dataset by active learning, 
we use uncertainty sampling like (Chan and Ng, 
2007) (Figure 1 line 30-31). This step selects the 
most uncertain example that is predicted with the 
lowest confidence in the refined classifier ?EM. 
Then, the correct sense for the most uncertain 
62
example is provided by human and added to the 
positive dataset P or the negative dataset N ac-
cording to the sense of d. 
The above steps are repeated until dataset 
reaches the predefined desirable size. 
 
01    foreach d ? ( T ? P ? N ) 
02       classify d by WSD system?(P, T-P) 
03       c(d, pos) ? the confidence score that d is  
04           predicted as positive defined in equation (2) 
05       c(d, neg) ? the confidence score that d is  
06           predicted as negative defined in equation (2) 
07       c(d, psdNeg) =  c(d, neg)  - c(d, pos)    
08                       (the confidence score that d is  
09                         predicted as pseudo negative)               
10        PN ? d ? ( T ? P ? N ) |  s(d) = neg ?  
11                                                  c(d, psdNeg)  ??} 
12                        (PN is pseudo negative dataset ) 
13     end 
Figure 2: Selection of pseudo negative examples 
3 Experimental Results 
3.1 Data and Condition of Experiments 
We select several example data sets from Japa-
nese blog data crawled from Web. Table 1 shows 
the ambiguous words and each ambiguous senses. 
Word Positive sense Other ambiguous senses 
Wega product name 
(TV) 
Las Vegas, football team 
name, nickname, star, horse 
race, Baccarat glass, atelier, 
wine, game, music 
Loft store name attic room, angle of golf 
club face, club with live 
music,  movie 
Honda personal name 
(football player) 
Personal names (actress, 
artists, other football play-
ers, etc.) hardware store, car 
company name 
Tsubaki product name 
(shampoo) 
flower name, kimono, horse 
race, camellia ingredient, 
shop name 
 Table 1: Selected examples for evaluation 
Table 2 shows the ambiguous words, the num-
ber of its senses, the number of its data instances, 
the number of feature, and the percentage of 
positive sense instances for each data set. 
Assigning the correct labels of data instances is 
done by one person and 48.5% of all the labels 
are checked by another person. The percentage 
of agreement between 2 persons for the assigned 
labels is 99.0%. The average time of assigning 
labels is 35 minutes per 100 instances. 
Selected instances for evaluation are randomly 
divided 10% test set and 90% training set. Table 
3 shows the each full text search query and the 
number of initial positive examples and the per-
centage of it in the training data set. 
word No. of 
senses
No. of  
instances
No. of  
features 
Percentage of  
positive sense
Wega 11 5,372 164,617 31.1%
Loft 5 1,582   38,491 39.4%
Honda 25 2,100   65,687 21.2%
Tsubaki 6 2,022   47,629 40.2%
Table 2: Selected examples for evaluation 
word Full text query for initial 
positive examples 
No. of positive 
examples (percent-
age in trainig set)  
Wega Wega  AND TV 316  (6.5%) 
Loft Loft AND (Grocery OR-
Stationery) 
64  (4.5%) 
Honda Honda AND Keisuke 86 (4.6%) 
Tsubaki Tsubaki AND Shiseido 380 (20.9%) 
Table 3: Initial positive examples 
The threshold value?in figure 2 is set to em-
pirically optimized value 50. Dependency on 
threshold value ? will be discussed in 3.3. 
3.2 Comparison Results 
Figure 3 shows the average WSD accuracy of 
the following 6 approaches. 
 
Figure 3: Average active learning process  
B-clustering is a standard unsupervised WSD, a 
clustering using naive Bayes classifier learned 
with two cluster numbers via EM algorithm. The 
given number of the clusters are two, negative 
and positive datasets.  
  M-clustering is a variant of b-clustering where 
the given number of clusters are each number of 
ambiguous word senses in table 2. 
Human labeling, abbreviated as human, is an 
active learning approach starting with human 
labeled negative examples. The number of hu-
56
58
60
62
64
66
68
70
72
0 10 20 30 40 50 60 70 80 90 100
75
77
79
81
83
85
87
89
91
human
with-EM
without-EM
random
m-clustering
b-clustering
63
man labeled negative examples in initial training 
data is the same as that of positive examples in 
figure 3. Human labeling is considered to be the 
upper accuracy in the variants of selecting 
pseudo negative examples.  
Random sampling with EM, abbreviated as 
with-EM, is the variant approach where dmin  in 
line 26 of figure 1 is randomly selected without 
using confidence score.  
Uncertainty sampling without EM (Takayama 
et al 2009), abbreviated as without-EM, is a vari-
ant approach where ?EM (P,  N+Np, T-N-P) in 
line 18 of figure 1 is replaced by ?(P, N+Np).  
Uncertainty Sampling with EM, abbreviated as un-
certain, is a proposed method described in figure 1. 
The accuracy of the proposed approach with-
EM is gradually increasing according to the per-
centage of added hand labeled examples. 
The initial accuracy of with-EM, which means 
the accuracy with no hand labeled negative ex-
amples, is the best score 81.4% except for that of 
human. The initial WSD accuracy of with-EM is 
23.4 and 4.2 percentage points higher than those 
of b-clustering (58.0%) and m-clustering 
(77.2%), respectively. This result shows that the 
proposed selecting method of pseudo negative 
examples is effective.  
The initial WSD accuracy of with-EM is 1.3 
percentage points higher than that of without-EM 
(80.1%). This result suggests semi-supervised 
learning using unlabeled examples is effective.  
The accuracies of with-EM, random and with-
out-EM are gradually increasing according to the 
percentage of added hand labeled examples and 
catch up that of human and converge at 30 per-
centage added points. This result suggests that 
our proposed approach can reduce the labor cost 
of assigning correct labels.  
The curve with-EM are slightly upper than the 
curve random at the initial stage of active learn-
ing. At 20 percentage added point, the accuracy 
with-EM is 87.0 %, 1.1 percentage points higher 
than that of random (85.9%). This result suggests 
that the effectiveness of proposed uncertainty 
sampling method is not remarkable depending on 
the word distribution of target data.  
There is really not much difference between the 
curve with-EM and without-EM. As a classifies 
to use the score for sampling examples in adapta-
tion iterations, it is indifferent whether with-EM 
or without-EM.  
Larger evaluation is the future issue to confirm 
if the above results could be generalized beyond 
the above four examples used as proper nouns. 
3.3 Dependency on Threshold Value ? 
Figure 4 shows the average WSD accuracies of 
with-EM at 0, 25, 50 and 75 as the values of ?.  
The each curve represents our proposed algorithm 
with threshold value ? in the parenthesis.  The 
accuracy in the case of ? = 75 is higher than that 
of? = 50 over 20 percentage data added point. 
This result suggests that as the number of hand 
labeled negative examples increasing, ? should 
be gradually decreasing, that is, the number of 
pseudo negative examples should be decreasing. 
Because, if sufficient number of hand labeled 
negative examples exist, a classifier does not need 
pseudo negative examples. The control of?
depending on the number of hand labeled examples 
during active learning iterations is a future issue. 
76
78
80
82
84
86
88
90
92
0 10 20 30 40 50 60 70 80 90 100
?=   0.0 
?= 25.0
?= 50.0 
?= 75.0 
 
Figure 4: Dependency of threshold value ? 
References  
Chan, Y. S. and Ng, H. T. 2007. Domain Adaptation 
with Active Learning for Word Sense Disambigua-
tion. Proc. of ACL 2007, 49-56. 
Chen, J., Schein, A., Ungar, L., and Palmer, M. 2006. 
An Empirical Study of the Behavior of Active 
Learning for Word Sense Disambiguation, Proc. of 
the main conference on Human Language Tech-
nology Conference of the North American Chapter 
of ACL, pp. 120-127. 
McCallum, A. and Nigam, K. 1998. Employing EM 
and Pool-Based Active Learning for Text Classifi-
cation. Proceedings of the Fifteenth international 
Conference on Machine Learning, 350-358. 
Nigam, K., McCallum, A., Thrun, S., and Mitchell, T. 
2000. Text Classification from Labeled and Unla-
beled Documents using EM, Machine Learning, 39, 
103-134.  
Takayama, Y., Imamura, M., Kaji N., Toyoda, M. and 
Kitsuregawa, M. 2009. Active Learning with 
Pseudo Negative Examples for Word Sense Dis-
ambiguation in Web Mining (in Japanese), Journal 
of IPSJ (in printing). 
64
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 959?969,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Splitting Noun Compounds via Monolingual and Bilingual Paraphrasing:
A Study on Japanese Katakana Words
Nobuhiro Kaji
Institute of Industrial Science
University of Tokyo, Tokyo, Japan
kaji@tkl.iis.u-tokyo.ac.jp
Masaru Kitsuregawa
Institute of Industrial Science
University of Tokyo, Tokyo, Japan
kitsure@tkl.iis.u-tokyo.ac.jp
Abstract
Word boundaries within noun compounds are
not marked by white spaces in a number of
languages, unlike in English, and it is benefi-
cial for various NLP applications to split such
noun compounds. In the case of Japanese,
noun compounds made up of katakana words
(i.e., transliterated foreign words) are par-
ticularly difficult to split, because katakana
words are highly productive and are often out-
of-vocabulary. To overcome this difficulty,
we propose using monolingual and bilingual
paraphrases of katakana noun compounds for
identifying word boundaries. Experiments
demonstrated that splitting accuracy is sub-
stantially improved by extracting such para-
phrases from unlabeled textual data, the Web
in our case, and then using that information for
constructing splitting models.
1 Introduction
1.1 Japanese katakana words and noun
compound splitting
Borrowing is a major type of word formation
in Japanese, and numerous foreign words (proper
names or neologisms etc.) are continuously being
imported from other languages (Tsujimura, 2006).
Most borrowed words in modern Japanese are
transliterations1 from English and they are referred
to as katakana words because transliterated foreign
words are primarily spelled by using katakana char-
acters in the Japanese writing system.2 Compound-
1Some researchers use the term transcription rather than
transliteration (Breen, 2009). Our terminology is based on stud-
ies on machine transliteration (Knight and Graehl, 1998).
2The Japanese writing system has four character types: hi-
ragana, katakana, kanji, and Latin alphabet.
ing is another type of word formation that is com-
mon in Japanese (Tsujimura, 2006). In particu-
lar, noun compounds are frequently produced by
merging two or more nouns together. These two
types of word formation yield a significant amount
of katakana noun compounds, making Japanese a
highly productive language.
In Japanese as well as some European and Asian
languages (e.g., German, Dutch and Korean), con-
stituent words of compounds are not separated by
white spaces, unlike in English. In those languages,
it is beneficial for various NLP applications to split
such compounds. For example, compound splitting
enables SMT systems to translate a compound on a
word-by-word basis, even if the compound itself is
not found in the translation table (Koehn and Knight,
2003; Dyer, 2009). In the context of IR, decom-
pounding has an analogous effect to stemming, and
it significantly improves retrieval results (Braschler
and Ripplinger, 2004). In abbreviation recognition,
the definition of an abbreviation is often in the form
of a noun compound, and most abbreviation recogni-
tion algorithms assume that the definition is properly
segmented; see e.g., (Schwartz and Hearst, 2003;
Okazaki et al, 2008).
This has led NLP researchers to explore meth-
ods for splitting compounds, especially noun com-
pounds, in various languages (Koehn and Knight,
2003; Nakazawa et al, 2005; Alfonseca et al,
2008a). While many methods have been presented,
they basically require expensive linguistic resources
to achieve high enough accuracy. For example, Al-
fonseca et al (2008b) employed a word dictionary,
which is obviously useful for this task. Other stud-
ies have suggested using bilingual resources such as
parallel corpora (Brown, 2002; Koehn and Knight,
959
2003; Nakazawa et al, 2005). The idea behind those
methods is that compounds are basically split into
constituent words when they are translated into En-
glish, where the compounded words are separated
by white spaces, and hence splitting rules can be
learned by discovering word alignments in bilingual
resources.
The largest obstacle that makes compound split-
ting difficult is the existence of out-of-vocabulary
words, which are not found in the abovemen-
tioned linguistic resources. In the Japanese case,
it is known that katakana words constitute a large
source of out-of-vocabulary words (Brill et al, 2001;
Nakazawa et al, 2005; Breen, 2009). As we have
discussed, katakana words are very productive, and
thus we can no longer expect existent linguistic re-
sources to have sufficient coverage. According to
(Breen, 2009), as many as 20% of katakana words
in news articles, which we think include less out-of-
vocabulary words than Web and other noisy textual
data, are out-of-vocabulary. Those katakana words
often form noun compounds, and pose a substantial
difficulty for Japanese text processing (Nakazawa et
al., 2005).
1.2 Paraphrases as implicit word boundaries
To alleviate the errors caused by out-of-vocabulary
words, we explored the use of unlabeled textual
data for splitting katakana noun compounds. Since
the amount of unlabeled text available is generally
much larger than word dictionaries and other expen-
sive linguistic resources, it is crucial to establish a
methodology for taking full advantage of such eas-
ily available textual data. While several approaches
have already been proposed, their accuracies are still
unsatisfactory (section 2.1).
From a broad perspective, our approach can be
seen as using paraphrases of noun compounds. As
we will see in section 4 and 5, katakana noun com-
pounds can be paraphrased into various forms that
strongly indicate word boundaries within the origi-
nal noun compound. This paper empirically demon-
strates that splitting accuracy can be significantly
improved by extracting such paraphrases from un-
labeled text, the Web in our case, and then using that
information for constructing splitting models.
Specifically, two types of paraphrases are inves-
tigated in this paper. Section 4 explores monolin-
gual paraphrases that can be generated by inserting
certain linguistic markers between constituent words
of katakana noun compounds. Section 5, in turn,
explores bilingual paraphrases (specifically, back-
transliteration). Since katakana words are basically
transliterations from English, back-transliterating
katakana noun compounds is also useful for split-
ting. To avoid terminological confusion, mono-
lingual paraphrases are simply referred to as para-
phrases and bilingual paraphrases are referred to as
back-transliterations hereafter.
We did experiments to empirically evaluate our
method. The results demonstrated that both para-
phrase and back-transliteration substantially im-
proved the performance in terms of F1-score, and
the best performance was achieved when they
were combined. We also confirmed that our
method outperforms the previously proposed split-
ting methods by a wide margin. All these results
strongly suggest the effectiveness of paraphrasing
and back-transliteration for identifying word bound-
aries within katakana noun compounds.
2 Related Work
2.1 Compound splitting
A common approach to splitting compounds with-
out expensive linguistic resources is an unsuper-
vised method based on word or string frequen-
cies estimated from unlabeled text (Koehn and
Knight, 2003; Ando and Lee, 2003; Schiller, 2005;
Nakazawa et al, 2005; Holz and Biemann, 2008).
Amongst others, Nakazawa et al (2005) also in-
vestigated ways of splitting katakana noun com-
pounds. Although the frequency-based method gen-
erally achieves high recall, its precision is not satis-
factory (Koehn and Knight, 2003; Nakazawa et al,
2005). Our experiments empirically compared our
method with the frequency-based methods, and the
results demonstrate the advantage of our method.
Our approach can be seen as augmenting discrim-
inative models of compound splitting with large ex-
ternal linguistic resources, i.e., textual data on the
Web. In a similar spirit, Alfonseca et al (2008b) pro-
posed the use of query logs for compound splitting.3
Their experimental results, however, did not clearly
3Although they also proposed using anchor text, this slightly
degraded the performance.
960
demonstrate their method?s effectiveness. Without
the query logs, the accuracy is reported to drop
only slightly from 90.55% to 90.45%. In contrast,
our experimental results showed statistically signifi-
cant improvements as a result of using additional re-
sources. Moreover, we used only textual data, which
is easily available, unlike query logs.
Holz and Biemann (2008) proposed a method
for splitting and paraphrasing German compounds.
While their work is related to ours, their algorithm
is a pipeline model and paraphrasing result is not
employed during splitting.
2.2 Other research topics
Our study is closely related to word segmentation,
which is an important research topic in Asian lan-
guages including Japanese. Although we can use
existing word segmentation systems for splitting
katakana noun compounds, it is difficult to reach the
desired accuracy, as we will empirically demonstrate
in section 6. One reason for this is that katakana
noun compounds often include out-of-vocabulary
words, which are difficult for the existing segmen-
tation systems to deal with. See (Nakazawa et al,
2005) for a discussion of this point. From a word
segmentation perspective, our task can be seen as
a case study focusing on a certain linguistic phe-
nomenon of particular difficulty. More importantly,
we are unaware of any attempts to use paraphrases
or transliterations for word segmentation in the same
way as we do.
Recent studies have explored using paraphrase
statistics for parsing (Nakov and Hearst, 2005a;
Nakov and Hearst, 2005b; Bansal and Klein, 2011).
Although these studies successfully demonstrated
the usefulness of paraphrases for improving parsers,
the connection between paraphrases and word seg-
mentation (or noun compound splitting) was not at
all discussed.
Our method of using back-transliterations for
splitting katakana noun compounds (section 5) is
closely related to methods for mining transliteration
from the Web text (Brill et al, 2001; Cao et al,
2007; Oh and Isahara, 2008; Wu et al, 2009). What
most differentiates these studies from our work is
that their primary goal is to build a machine translit-
eration system or to build a bilingual dictionary it-
self; none of them explored splitting compounds.
Table 1: Basic features.
ID Feature Description
1 yi constituent word 1-gram
2 yi?1yi constituent word 2-gram
3 LEN(yi) #characters of yi (1, 2, 3, 4, or ?5)
4 DICT(yi) true if yi is in the dictionary
3 Supervised Approach
The task we examine in this paper is splitting
a katakana noun compound x into its constituent
words, y = (y1, y2 . . . y|y|). Note that the output
can be a single word, i.e., |y| = 1. Since it is pos-
sible that the input is an out-of-vocabulary word, it
is not at all trivial to identify a single word as such.
A naive method would erroneously split an out-of-
vocabulary word into multiple constituent words.
We formalize our task as a structure prediction
problem that, given a katakana noun compound x,
predicts the most probable splitting y?.
y? = argmax
y?Y(x)
w ? ?(y),
where Y(x) represents the set of all splitting options
of x, ?(y) is a feature vector representation of y,
and w is a weight vector to be estimated from la-
beled data.
Table 1 summarizes our basic feature set. Fea-
tures 1 and 2 are word 1-gram and 2-gram features,
respectively. Feature 3 represents the length of the
constituent word. LEN(y) returns the number of
characters of y (1, 2, 3, 4, or ?5). Feature 4 indi-
cates whether the constituent word is registered in
an external dictionary (see section 6.1). DICT(y) re-
turns true if the word y is in the dictionary.
In addition to those basic features, we also employ
paraphrases and back-transliterations of katakana
noun compounds as features. The features are de-
tailed in sections 4 and 5, respectively.
We can optimize the weight vector w using an ar-
bitrary training algorithm. Here we adopt the aver-
aged perceptron algorithm for the sake of time effi-
ciency (Freund and Schapire, 1999). The perceptron
offers efficient online training, and it performs com-
paratively well with batch algorithms such as SVMs.
Since we use only factored features (see table 1, sec-
tion 4 and section 5), dynamic programming can be
used to locate y?.
961
Table 2: Paraphrase rules and examples. The first column represents the type of linguistic marker to be inserted, the
second column shows the paraphrase rules, and the last column gives examples.
Type Rule Example
Centered dot X1X2 ? X1 ? X2 ????????
(anchovy pasta)
??????????
(anchovy ? pasta)
Possessive marker X1X2 ? X1 ? X2 ????????
(anchovy pasta)
? ??????
(with anchovy)
???
(pasta)
Verbal suffix X1X2 ? X1 ?? X2
X1X2 ? X1?? X2
??????????
(download file)
?????????
(downloaded)
????
(file)
Adjectival suffix X1X2 ? X1 ? X2
X1X2 ? X1 ? X2
X1X2 ? X1 ?? X2
????????
(surprise gift)
???????
(surprising)
???
(gift)
4 Paraphrasing
In this section, we argue that paraphrases of
katakana noun compounds provides useful informa-
tion on word boundaries. Consequently, we propose
using paraphrase frequencies as features for training
the discriminative model.
4.1 Paraphrasing noun compounds
A katakana noun compound can be paraphrased into
various forms, some of which provide information
on the word boundaries within the original com-
pound.
(1) a. ????????
(anchovy pasta)
b. ?????????
(anchovy ? pasta)
c. ??????
(with anchovy)
???
(pasta)
These examples are paraphrases of each other. (1a)
is in the form of a noun compound, within which
the word boundary is ambiguous. In (1b), on the
other hand, a centered dot ? is inserted between
the constituent words. In the Japanese writing sys-
tem, the centered dot is sometimes, but not always,
used to separate long katakana compounds for the
sake of readability. (1c) is the noun phrase gener-
ated from (1a) by inserting the possessive marker
???, which can be translated as with in this context,
between the constituent words. If we observe para-
phrases of (1a) such as (1b) and (1c), we can guess
that a word boundary exists between ??????
(anchovy)? and ???? (pasta)?.
4.2 Paraphrase rules
The above discussion led us to use paraphrase
frequencies estimated from Web text for splitting
katakana noun compounds. For this purpose, we
established the seven paraphrase rules illustrated in
Table 2. The rules are in the form of X1X2 ?
X1MX2, where X1 and X2 represent nouns, and
M is a certain linguistic marker (e.g., the posses-
sive marker ???). The left-hand term corresponds
to a compound to be paraphrased and the right-hand
term represents its paraphrase. For instance, X1 =
?????? (anchovy)?, X2 = ???? (pasta)?, and
M = ???. The paraphrase rules we use are based on
the rules proposed by Kageura et al (2004) for ex-
panding complex terms, primarily noun compounds,
into their variants.
4.3 Web-based frequency as features
We introduce a new feature using the paraphrase
rules and Web text. As preprocessing, we use reg-
ular expressions to count the frequencies of all po-
tential paraphrases of katakana noun compounds on
the Web in advance.
(katakana)+? (katakana)+
(katakana)+? (katakana)+
(katakana)+?? (katakana)+
. . .
where (katakana) corresponds to one katakana char-
acter. Given a candidate segmentation y at test time,
we generate paraphrases of the noun compound by
setting X1 = yi?1 and X2 = yi, and applying the
paraphrase rules. We then use log(F + 1), where F
is the sum of the Web-based frequencies of the gen-
962
erated paraphrases, as the feature of the boundary
between yi?1 and yi.
As the feature value, we use the logarithmic fre-
quency, rather than the raw frequency, for scaling.
Since the other features have binary value, we found,
in initial experiments, that the importance of this
feature is overemphasized if we use the raw fre-
quency. Note that we use log(F + 1) rather than
log F so as to avoid the feature value being zero
when F = 1.
5 Back-transliteration
Most katakana words are transliterations from En-
glish, where words are separated by white spaces.
It is, therefore, reasonable to think that back-
transliterating katakana noun compounds into En-
glish would provide information on word bound-
aries, in a similar way to paraphrasing.
This section presents a method for extracting
back-transliterations of katakana words from mono-
lingual Web text, and establishing word alignments
between those katakana and English words (Table
3). In what follows, the pair of katakana words
and its English back-transliteration is referred to as a
transliteration pair. If the transliteration pair is an-
notated with word alignment information as in Table
3, it is referred to as a word-aligned transliteration
pair.
Using word-aligned transliteration pairs extracted
from the Web text, we derive a binary feature in-
dicating whether katakana word yi corresponds to
a single English word. Additionally, we derive an-
other feature indicating whether a katakana word 2-
gram yi?1yi corresponds to an English word 2-gram.
5.1 Parenthetical expressions
In Japanese and other Asian languages, transliter-
ated words are sometimes followed by their English
back-transliterations inside parentheses:
(2) a. ?????? ?????
(junk)
???
(food)
(junk food)?...
b. ??????? ? ???
(spam)
(spam)?????...
where the underline indicates the Japanese text
that is followed by English back-transliteration.
We extract word-aligned transliteration pairs from
Table 3: Word-aligned transliteration pairs. The number
indicates the word alignment.
Japanese English
????1??? 2 junk1 food2
???3 spam3
such parenthetical expressions by establishing the
correspondences between pre-parenthesis and in-
parenthesis words.
To accomplish this, we have to resolve three prob-
lems: (a) English words inside parenthesis do not
always provide a back-transliteration of the pre-
parenthesis text, (b) the left boundary of the pre-
parenthesis text, denoted as ??? in the example, has
to be identified, and (c) pre-parenthesis text, which
is a katakana noun compound in our case, has to be
segmented into words.
Although several studies have explored mining
transliterations from such parenthetical expressions
(Cao et al, 2007; Wu et al, 2009), the last problem
has not been given much attention. In the past stud-
ies, the pre-parenthesis text is assumed to be cor-
rectly segmented by, typically, using existent word
segmentation systems. This is, however, not appro-
priate for our purpose, because pre-parenthesis text
is a katakana noun compound, which is hard for ex-
isting systems to handle, and hence the alignment
quality is inevitably affected by segmentation errors.
To handle these three problems, we use the pho-
netic properties of the transliterations. For the pur-
pose of explanation, we shall first focus on problem
(c). Since transliterated katakana words preserve the
pronunciation of the original English words to some
extent (Knight and Graehl, 1998), we can discover
the correspondences between substrings of the two
languages based on phonetic similarity:
(3) a. [???]1[?]2[??]3[? ]4
b. [jun]1[k]2 [foo]3[d]4
Note that these are the pre-parenthesis and in-
parenthesis text in (2a). The substrings surrounded
by square brackets with the same number corre-
spond to each other. Given such a correspondence,
we can segment the pre-parenthesis text (3a) accord-
ing to its English counterpart (3b), in which words
963
Table 4: Example of the substring alignment A between
f =???????? ? and e =?junkfood? (|A| = 4).
(fi, ei) log p(fi, ei)
(???, jun) ?10.767
(?, k) ?5.319
(??, foo) ?11.755
(? , d) ?5.178
are separated by white space. We can recognize that
the katakana string ??????, which is the con-
catenation of the first two substrings in (3a), forms
a single word because it corresponds to the English
word junk, and so on. Consequently, (3a) can be seg-
mented into two words, ????? (junk)? and ???
? (food)?. The word alignment is trivially estab-
lished.
For problems (a) and (b), we can also use the
phonetic similarity between pre-parenthesis and in-
parenthesis text. If the parenthetical expression does
not provide the transliteration, or if the left boundary
is erroneously identified, we can expect the phonetic
similarity to become small. Such situations thus can
be identified.
The remainder of this section details this ap-
proach. Section 5.2 presents a probabilistic model
for discovering substring alignment such as (3). Sec-
tion 5.3 shows how to extract word-aligned translit-
eration pairs by using the probabilistic model.
5.2 Phonetic similarity model
To establish the substring alignment between
katakana and Latin alphabet strings, we use the
probabilistic model proposed by (Jiampojamarn et
al., 2007). Let f and e be katakana and alphabet
strings, and A be the substring alignment between
them. More precisely, A is a set of corresponding
substring pairs (fi, ei) such that f = f1f2 . . . f|A|
and e = e1e2 . . . e|A|. The probability of such align-
ment is defined as
log p(f, e, A) =
?
(fi,ei)?A
log p(fi, ei).
Since A is usually unobservable, it is treated as a
hidden variable. Table 4 illustrates an example of
the substring alignment between f =???????
? ? and e =?junkfood?, and the likelihood of each
substring pair estimated in our experiment.
The model parameters are estimated from a set of
transliteration pairs (f, e) using the EM algorithm.
In the E-step, we estimate p(A|f, e) based on the
current parameters. In the parameter estimation, we
restrict both fi and ei to be at most three characters
long. Doing this not only makes the E-step compu-
tationally efficient but avoids over-fitting by forbid-
ding too-long substrings to be aligned. In the M-
step, the parameter is re-estimated using the result
of the E-step. We can accomplish this by using an
extension of the forward-backward algorithm. See
(Jiampojamarn et al, 2007) for details.
Given a new transliteration pair (f, e), we can de-
termine the substring alignment as
A? = argmax
A
log p(f, e, A).
In finding the substring alignment, a white space on
the English side is used as a constraint, so that the
English substring ei does not span a white space.
5.3 Extracting word-aligned transliteration
pairs
The word-aligned transliteration pairs are extracted
using the phonetic similarity model, as follows.
First, candidate transliteration pairs (f, e) are ex-
tracted from the parenthetical expressions. This is
done by extracting English words inside parenthe-
ses and pre-parenthesis text written in katakana. En-
glish words are normalized by lower-casing capital
letters.
Second, we determine the left boundary by using
the confidence score: 1N log p(f, e, A?), where N is
the number of English words. The term 1N prevents
the score from being unreasonably small when there
are many words. We truncate f by removing the
leftmost characters one by one, until the confidence
score exceeds a predefined threshold ?. If f becomes
empty, the pair is regarded as a non-transliteration
and discarded.
Finally, for the remaining pairs, the Japanese side
is segmented and the word alignment is established
according to A?. This results in a list of word-
aligned transliteration pairs (Table 3).
6 Experiments and Discussion
We conducted experiments to investigate how the
use of the paraphrasing and the back-transliteration
964
improves the performance of the discriminative
model.
6.1 Experimental setting
To train the phonetic similarity model, we used
a set of transliteration pairs extracted from the
Wikipedia.4 Since person names are almost always
transliterated when they are imported from English
into Japanese, we made use of the Wikipedia arti-
cles that belong to the Living people category. From
the titles of those articles, we automatically ex-
tracted person names written in katakana, together
with their English counterparts obtainable via the
multilingual links provided by the Wikipedia. This
yielded 17,509 transliteration pairs for training. In
performing the EM algorithm, we tried ten differ-
ent initial parameters and selected the model that
achieved the highest likelihood.
The data for training and testing the percep-
tron was built using a Japanese-English dictionary
EDICT.5 We randomly extracted 5286 entries writ-
ten in katakana from EDICT and manually anno-
tated word boundaries by establishing word corre-
spondences to their English transliterations. Since
English transliterations are already provided by
EDICT, the annotation can be trivially done by na-
tive speakers of Japanese. Using this data set, we
performed 2-fold cross-validation for testing the per-
ceptron. The number of iterations was set to 20 in all
the experiments.
To compute the dictionary-based feature DICT(y)
in our basic feature set, we used NAIST-jdic.6 It is
the largest dictionary used for Japanese word seg-
mentation, and it includes 19,885 katakana words.
As Web corpora, we used 1.7 G sentences of
blog articles. From the corpora, we extracted
14,966,205 (potential) paraphrases of katakana noun
compounds together with their frequencies. We
also extracted 151,195 word-aligned transliteration
pairs. In doing this, we ranged the threshold ? in
{?10,?20, ? ? ? ? 150} and chose the value that per-
formed the best (? = ?80).
The results were evaluated using precision, recall,
F1-score, and accuracy. Precision is the number of
correctly identified words divided by the number of
4http://ja.wikipedia.org/
5http://www.csse.monash.edu.au/?jwb/edict doc.html
6http://sourceforge.jp/projects/naist-jdic
all identified words, recall is the number of correctly
identified words divided by the number of all ora-
cle words, the F1-score is their harmonic mean, and
accuracy is the number of correctly split katakana
noun compounds divided by the number of all the
katakana noun compounds.
6.2 Baseline systems
We compared our system with three frequency-
based baseline system, two supervised baselines,
and two state-of-the-art word segmentation base-
lines. The first frequency-based baseline, UNI-
GRAM, performs compound splitting based on a
word 1-gram language model (Schiller, 2005; Al-
fonseca et al, 2008b):
y? = argmax
y?Y(x)
?
i
p(yi),
where p(yi) represents the probability of yi. The
second frequency-based baseline, GMF, outputs the
splitting option with the highest geometric mean fre-
quency of the constituent words (Koehn and Knight,
2003):
y? = argmax
y?Y(x)
GMF(y) = argmax
y?Y(x)
{?
i
f(yi)
}1/|y|
,
where f(yi) represents the frequency of yi. The
third frequency-based baseline, GMF2, is a mod-
ification of GMF proposed by Nakazawa et al
(2005). It is based on the following score instead
of GMF(y):
GMF2(y) =
?
??
??
GMF(y) (|y| = 1)
GMF(y)
C
Nl +?
(|y| ? 2),
where C , N , and ? are hyperparameters and l is the
average length of the constituent words. Following
(Nakazawa et al, 2005), the hyperparameters were
set as C = 2500, N = 4, and ? = 0.7. We estimated
p(y) and f(y) from the Web corpora.
The first supervised baseline, AP, is the aver-
aged perceptron model trained using only the ba-
sic feature set. The second supervised baseline,
AP+GMF2 is a combination of AP and GMF2,
which performed the best amongst the frequency-
based baselines. Following (Alfonseca et al,
965
Table 5: Comparison with baseline systems.
Type System P R F1 Acc
Frequency UNIGRAM 64.2 49.7 56.0 63.0
GMF 42.9 62.0 50.7 47.5
GMF2 67.4 76.0 71.5 72.5
Supervised AP 81.9 82.5 82.2 83.4
AP+GMF2 83.0 83.9 83.4 84.2
PROPOSED 86.4 87.4 87.1 87.6
Word seg. JUMAN 71.4 60.1 65.3 69.8
MECAB 72.4 73.7 67.8 71.6
2008b), GMF2 is integrated into AP as two bi-
nary features indicating whether GMF2(y) is larger
than any other candidates, and whether GMF2(y) is
larger than the non-split candidate. Although Alfon-
seca et al (2008b) also proposed using (the log of)
the geometric mean frequency as a feature, doing so
degraded performance in our experiment.
Regarding the two state-of-the-art word segmen-
tation systems, one is JUMAN,7 a rule-based word
segmentation system (Kurohashi and Nagao, 1994),
and the other is MECAB,8 a supervised word seg-
mentation system based on CRFs (Kudo et al,
2004). These two baselines were chosen in order to
show how well existing word segmentation systems
perform this task. Although the literature states that
it is hard for existing systems to deal with katakana
noun compounds (Nakazawa et al, 2005), no empir-
ical data on this issue has been presented until now.
6.3 Splitting result
Table 5 compares the performance of our system
(PROPOSED) with the baseline systems. First of all,
we can see that PROPOSED clearly improved the per-
formance of AP, demonstrating the effectiveness of
using paraphrases and back-transliterations.
Our system also outperformed all the frequency-
based baselines (UNIGRAM, GMF, and GMF2). This
is not surprising, since the simple supervised base-
line, AP, already outperformed the unsupervised
frequency-based ones. Indeed similar experimental
results were also reported by Alfonseca (2008a). An
interesting observation here is the comparison be-
tween PROPOSED and AP+GMF2. It reveals that
our approach improved the performance of AP more
than the frequency-based method did. These results
7http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html
8http://sourceforge.net/projects/mecab
indicate that paraphrasing and back-transliteration
are more informative clues than the simple fre-
quency of constituent words. We would like to
note that the higher accuracy of PROPOSED in com-
parison with the baselines is statistically significant
(p < 0.01, McNemar?s test).
The performance of the two word segmenta-
tion baselines (JUMAN and MECAB) is significantly
worse in our task than in the standard word segmen-
tation task, where nearly 99% precision and recall
are reported (Kudo et al, 2004). This demonstrates
that splitting a katakana noun compound is not at
all a trivial task to resolve, even for the state-of-the-
art word segmentation systems. On the other hand,
PROPOSED outperformed both JUMAN and MECAB
in this task, meaning that our technique can suc-
cessfully complement the weaknesses of the existing
word segmentation systems.
By analyzing the errors, we interestingly found
that some of the erroneous splitting results are still
acceptable to humans. For example, while ????
??? (upload)? was annotated as a single word in
the test data, our system split it into ???? (up)?
and ???? (load)?. Although the latter splitting
may be useful in some applications, it is judged as
wrong in our evaluation framework. This implies
the importance of evaluating the splitting results in
some extrinsic tasks. We leave it to a future work.
6.4 Investigation on out-of-vocabulary words
In our test data, 2681 out of the 5286 katakana noun
compounds contained at least one out-of-vocabulary
word that are not registered in NAIST-jdic. Table 6
illustrates the results of the supervised systems for
those 2681 and the remaining 2605 katakana noun
compounds (referred to as w/ OOV and w/o OOV
data, respectively). While the accuracy exceeds 90%
for w/o OOV data, it is substantially degraded for w/
OOV data. This is consistent with our claim that out-
of-vocabulary words are a major source of errors in
splitting noun compounds.
The three supervised systems performed almost
equally for w/o OOV data. This is because AP triv-
ially performs very well on this subset, and it is dif-
ficult to get any further improvement. On the other
hand, we can see that there are substantial perfor-
mance gaps between the systems for w/ OOV data.
This result reflects the effect of the additional fea-
966
Table 6: Splitting results of the supervised systems for w/ OOV and w/o OOV data.
w/ OOV data w/o OOV data
System P R F1 Acc P R F1 Acc
AP 66.9 69.9 68.3 72.8 95.4 93.2 94.3 94.2
AP+GMF2 69.7 73.7 71.6 75.2 95.2 92.4 93.7 93.6
PROPOSED 76.8 79.3 78.0 80.9 95.3 94.2 94.8 94.5
tures more directly than is shown in table 5.
6.5 Effect of the two new features
To see the effect of the new features in more detail,
we looked at the performances of our system using
different feature sets (Table 7). The first column
represents the feature set we used: BASIC, PARA,
TRANS, and ALL represent the basic features, the
paraphrase feature, the back-transliteration feature,
and all the features. The results demonstrate that
adding either of the new features improved the per-
formance, and the best result was when they were
used together. In all cases, the improvement over
BASIC was statistically significant (p < 0.01, Mc-
Nemar?s test).
Next, we investigated the coverage of the features.
Our test data comprised 7709 constituent words,
4937 (64.0%) of which were covered by NAIST-
jdic. The coverage was significantly improved when
using the back-transliteration feature. We observed
that 6216 words (80.6%) are in NAIST-jdic or word-
aligned transliteration pairs extracted from the Web
text. This shows that the back-transliteration fea-
ture successfully reduced the number of out-of-
vocabulary words. On the other hand, we observed
that the paraphrase and back-transliteration features
were activated for 79.5% (1926/2423) and 15.5%
(376/2423) of the word boundaries in our test data.
Overall, we see that the coverage of these fea-
tures is reasonably good, although there is still room
for further improvement. It would be beneficial to
use larger Web corpora or more paraphrase rules,
for example, by having a system that automatically
learns rules from the corpora (Barzilay and McKe-
own, 2001; Bannard and Callison-Burch, 2005).
6.6 Sensitivity on the threshold ?
Finally we investigated the influence of the thresh-
old ? (Figure 1 and 2). Figure 1 illustrates the system
performance in terms of F1-score for different values
Table 7: Effectiveness of paraphrase (PARA) and back-
transliteration feature (TRANS).
Feature set P R F1 Acc
BASIC 81.9 82.5 82.2 83.4
BASIC+PARA 85.1 85.3 85.2 85.9
BASIC+TRANS 85.1 86.3 85.7 86.5
ALL 86.4 87.4 87.1 87.6
of ?. While the F1-score drops when the value of ?
is too large (e.g., ?20), the F1-score is otherwise al-
most constant. This demonstrates it is generally easy
to set ? near the optimal value. More importantly,
the F1-score is consistently higher than BASIC irre-
spective of the value of ?. Figure 2 represents the
number of distinct word-aligned transliteration pairs
that were extracted from the Web corpora. We see
that most of the extracted transliteration pairs have
high confidence score.
7 Conclusion
In this paper, we explored the idea of using monolin-
gual and bilingual paraphrases for splitting katakana
noun compounds in Japanese. The experiments
demonstrated that our method significantly im-
proves the splitting accuracy by a large margin
in comparison with the previously proposed meth-
ods. This means that paraphrasing provides a sim-
ple and effective way of using unlabeled textual
data for identifying implicit word boundaries within
katakana noun compounds.
Although our investigation was restricted to
katakana noun compounds, one might expect that a
similar approach would be useful for splitting other
types of noun compounds (e.g., German noun com-
pounds), or for identifying general word boundaries,
not limited to those between nouns, in Asian lan-
guages. We think these are research directions worth
exploring in the future.
967
87
88
85
86
co
re
83
84F 1
-s
c
81
82
Threshold
Figure 1: Influence of the threshold ? (x-axis) on the F1-
score (y-axis). The triangles and squares represent sys-
tems using the ALL and BASIC feature sets, respectively.
200000
pa
irs
100000
150000
er
at
io
n 
p
50000f t
ra
ns
lit
e
0
# 
of
Threshold
Figure 2: The number of distinct word-aligned transliter-
ations pairs that were extracted from the Web corpora for
different values of ?.
Acknowledgement
This work was supported by the Multimedia Web
Analysis Framework towards Development of Social
Analysis Software program of the Ministry of Ed-
ucation, Culture, Sports, Science and Technology,
Japan.
References
Enrique Alfonseca, Slaven Bilac, and Stefan Pharies.
2008a. Decompoundig query keywords from com-
pounding languages. In Proceedings of ACL, Short
Papers, pages 253?256.
Enrique Alfonseca, Slaven Bilac, and Stefan Pharies.
2008b. German decompounding in a difficult corpus.
In Proceedings of CICLing, pages 128?139.
Rie Kubota Ando and Lillian Lee. 2003. Mostly-
unsupervised statistical segmentation of Japanese
Kanji sequences. Natural Language Engineering,
9(2):127?149.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL, pages 597?604.
Mohit Bansal and Dan Klein. 2011. Web-scale features
for full-scale parsing. In Proceedings of ACL, pages
693?702.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL, pages 50?57.
Martin Braschler and Ba?rbel Ripplinger. 2004. How ef-
fective is stemming and decompounding for German
text retrieval? Information Retrieval, 7:291?316.
Jamese Breen. 2009. Identification of neologisms in
Japanese by corpus analysis. In Proceedings of eLexi-
cography in the 21st centry conference, pages 13?22.
Eric Brill, Gray Kacmarcik, and Chris Brockett. 2001.
Automatically harvesting katakana-English term pairs
from search engine query logs. In Proceedings of NL-
PRS, pages 393?399.
Ralf D. Brown. 2002. Corpus-driven splitting of com-
pound words. In Proceedings of TMI.
Guihong Cao, Jianfeng Gao, and Jian-Yun Nie. 2007. A
system to mine large-scale bilingual dictionaries from
monolingual Web pages. In Proceedings of MT Sum-
mit, pages 57?64.
Chris Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for MT. In Proceedings of
NAACL, pages 406?414.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning, 37(3):277?296.
Florian Holz and Chris Biemann. 2008. Unsupervised
and knowledge-free learning of compound splits and
periphrases. In CICLing, pages 117?127.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignment and
hidden Markov models to letter-to-phoneme conver-
sion. In HLT-NAACL, pages 372?379.
Kyo Kageura, Fuyuki Yoshikane, and Takayuki Nozawa.
2004. Parallel bilingual paraphrase rules for noun
compounds: Concepts and rules for exploring Web
language resources. In Proceedings of Workshop on
Asian Language Resources, pages 54?61.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):599?
612.
Philip Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of EACL,
pages 187?193.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to Japanese
morphological analysis. In Proceedings of EMNLP,
pages 230?237.
Sadao Kurohashi and Makoto Nagao. 1994. Im-
provements of Japanese morphological analyzer JU-
MAN. In Proceedings of the International Workshop
on Sharable Natural Language Resources, pages 22?
38.
968
Toshiaki Nakazawa, Daisuke Kawahara, and Sadao
Kurohashi. 2005. Automatic acquisition of basic
Katakana lexicon from a given corpus. In Proceedings
of IJCNLP, pages 682?693.
Preslav Nakov and Marti Hearst. 2005a. Search en-
gine statistics beyond the n-gram: Application to noun
compound bracketing. In Proceedings of CoNLL,
pages 17?24.
Preslav Nakov and Marti Hearst. 2005b. Using the Web
as an implicit training set: Application to structural
ambiguity resolution. In Proceedings of HLT/EMNLP,
pages 835?342.
Jong-Hoon Oh and Hitoshi Isahara. 2008. Hypothesis
selection in machine transliteration: A Web mining
approach. In Proceedings of IJCNLP, pages 233?240.
Naoaki Okazaki, Sophia Ananiadou, and Jin?ichi Tsujii.
2008. A discriminative alignment model for abbrevi-
ation recognition. In Proceedings of COLING, pages
657?664.
Anne Schiller. 2005. German compound analysis with
wfsc. In Proceedings of Finite State Methods and Nat-
ural Language Processing, pages 239?246.
Ariel S. Schwartz and Marti A. Hearst. 2003. A sim-
ple algorithm for identifying abbreviation definitions
in biomedical text. In Proceedings of PSB, pages 451?
462.
Natsuko Tsujimura. 2006. An Introduction to Japanese
Linguistics. Wiley-Blackwell.
Xianchao Wu, Naoaki Okazaki, and Jun?ichi Tsujii.
2009. Semi-supervised lexicon mining from paren-
thetical expressions in monolingual Web pages. In
Proceedings of NAACL, pages 424?432.
969
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 883?892, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Identifying Constant and Unique Relations by using Time-Series Text
Yohei Takaku?
Toyo Keizai Inc.
Chuo-ku, Tokyo 103-8345, Japan
takaku.yohei@gmail.com
Nobuhiro Kaji and Naoki Yoshinaga
Institute of Industrial Science,
University of Tokyo
Meguro-ku, Tokyo 153-8505, Japan
{kaji,ynaga}@tkl.iis.u-tokyo.ac.jp
Masashi Toyoda
Institute of Industrial Science,
University of Tokyo
Meguro-ku, Tokyo 153-8505, Japan
toyoda@tkl.iis.u-tokyo.ac.jp
Abstract
Because the real world evolves over time, nu-
merous relations between entities written in
presently available texts are already obsolete
or will potentially evolve in the future. This
study aims at resolving the intricacy in con-
sistently compiling relations extracted from
text, and presents a method for identifying
constancy and uniqueness of the relations in
the context of supervised learning. We ex-
ploit massive time-series web texts to induce
features on the basis of time-series frequency
and linguistic cues. Experimental results con-
firmed that the time-series frequency distribu-
tions contributed much to the recall of con-
stancy identification and the precision of the
uniqueness identification.
1 Introduction
We have witnessed a number of success stories in
acquiring semantic relations between entities from
ever-increasing text on the web (Pantel and Pennac-
chiotti, 2006; Banko et al2007; Suchanek et al
2007; Wu et al2008; Zhu et al2009; Mintz et al
2009; Wu and Weld, 2010). These studies have suc-
cessfully revealed to us millions of relations between
real-world entities, which have been proven to be
beneficial in solving knowledge-rich problems such
as question answering and textual entailment (Fer-
rucci et al2010).
?This work was conducted while the first author was a grad-
uate student at University of Tokyo.
There exists, however, a great challenge to com-
pile consistently relations extracted from text by
these methods, because they assume a simplifying
assumption that relations are time-invariant. In other
words, they implicitly disregard the fact that state-
ments in texts actually reflect the state of the world
at the time when they were written, which follows
that relations extracted from such texts eventually
become outdated as the real world evolves over time.
Let us consider that relations are extracted from
the following sentences:1
(1) a. 1Q84 is written by Haruki Murakami.
b. Moselle river flows through Germany.
c. U.S.?s president is George Bush.
d. Pentax sells K-5, a digital SLR.
Here, italicized predicates represent the relations,
while underlined entities are their arguments. The
relations in statements 1a and 1b are true across
time, so we can simply accumulate all the relation
instances. The relations in 1c and 1d in contrast
evolve over time. The relation written in 1c be-
comes outdated when the other person takes the
position, so we need to supersede it when a new
relation is extracted from text (e.g., U.S?s president
is Barack Obama). For the relation in 1d, we do not
always need to supersede it with a new relation.
This study is motivated from the above consider-
1Since our task settings are language-independent, we here-
after employ English examples as much as possible to widen
the potential readership of the paper, although we conducted
experiments with relations between entities in Japanese.
883
ations and proposes a method for identifying con-
stancy and uniqueness of relations in order to se-
lect an appropriate strategy to maintain relation in-
stances extracted from text. For example, the rela-
tions written in statements 1a and 1b are constant,
while those in 1c and 1d are non-constant; the re-
lation in 1c is unique,2 whereas the relation in 1d
is non-unique. With these properties of relations in
mind, we can accumulate constant relations while
appropriately superseding non-constant, unique re-
lations with newly acquired relations.
We locate each identification task in the context
of supervised classification. The key challenge in
solving these classification tasks is how to induce an
effective feature that identifies unique, non-constant
relations (statement 1c) that seemingly appear as
non-unique relations on text (statement 1b). We ex-
ploit massive time-series web text to observe actual
evolutions of relation instances and induce features
from the relation instances taken from a time sliding
window and linguistic cues modifying the predicate
and arguments of the target relation.
We evaluated our method on 1000 relations ex-
tracted from 6-year?s worth of Japanese blog posts
with 2.3-billion sentences. We have thereby con-
firmed that the features induced from this time-series
text contributed much to improve the classification
accuracy.
The main contributions of this paper are twofold:
? We have introduced a novel task for identify-
ing constancy relations. Since most of the ex-
isting studies assume that relations are time-
invariant as discussed by Weikum et al2011),
non-constant relations prevalent in their out-
come incur a serious problem in maintaining
the acquired relations. The notion of constancy
is meant to resolve this stalemate.
? We have for the first time demonstrated the
usefulness of a time-series text in relation ac-
quisition and confirmed its impact in the two
relation classification tasks. The features in-
duced from the time-series text have greatly
contributed to the accuracy of the classification
based on uniqueness as well as the recall of the
classification based on constancy.
2This kind of relation is referred to as functional relation in
the literature (Ritter et al2008; Lin et al2010).
Constant Non-constant
arg1 was born in arg2 arg1?s president is arg2
arg1 is a father of arg2 arg1 belongs to arg2
arg1 is written by arg2 arg1 lives in arg2
Table 1: Examples of constant, non-constant relations.
The reminder of this paper is structured as fol-
lows. Section 2 introduces the two properties of
relations (constancy and uniqueness) and then de-
fines the task setting of this study. Sections 3 and 4
describe the features induced from time-series text
for constancy and uniqueness classification, respec-
tively. Section 5 reports experimental results. Sec-
tion 6 addresses work related to this study. Section 7
concludes this study and mentions future work.
2 Classification of Relations based on
Constancy and Uniqueness
2.1 Constancy and uniqueness
We introduce two properties of relations: constancy
and uniqueness.
A relation is constant if, for most values of arg1,
the value of arg2 is independent of time (Table 1).
For example, ?arg1 was born in arg2? is a constant
relation since one?s birthplace never changes. On the
other hand, ?arg1 ?s president is arg2? is an example
of non-constant relations. This can be checked by
noting that, for example, the president of the United
States was Barack Obama in 2011 but was previ-
ously George Bush and Bill Clinton before him.
A relation is unique if, for most values of arg1,
there exists, at any given point in time, only one
value of arg2 that satisfies the relation (Table 2). For
example, ?arg1 was born in arg2? is obviously a
unique relation. The relation ?arg1 is headquartered
in arg2? is also unique, while it is non-constant. No-
tice that there is usually only one headquarters at any
point in time, although the location of a headquarters
can change. In contrast, the relation ?arg1 is funded
by arg2? is a non-unique relation since it is likely
that there exist more than one funder.
2.2 Discussion
Both constancy and uniqueness are properties that
usually, not always, hold for most, not all, of the
arg1?s values. To see this, let us examine the relation
?arg1 ?s president is arg2?. Although this relation is
884
Unique Non-unique
arg1 was born in arg2 arg1 is funded by arg2
arg1 is headquartered in arg2 arg1 consists of arg2
arg1?s president is arg2 arg1 borders on arg2
Table 2: Examples of unique and non-unique relations.
non-constant and unique (Table 1 and 2), it is still
possible to find exceptional cases. For example, a
country might exist in which the president has never
changed; a country might have more than one pres-
ident at the same time during civil war. However,
since such situations are rare, the relation ?arg1 ?s
president is arg2? is considered as neither constant
nor non-unique.
The above discussion implies that the constancy
and uniqueness of relations can not be determined
completely objectively. We, nevertheless, claim that
these properties of relations are intuitively accept-
able and thus they can be identified with moderate
agreement by different people (see section 5).
2.3 Task and our approach
This paper explores classifying given relations on
the basis of constancy and uniqueness. We treat
the problem as two independent binary classification
tasks, and train supervised classifiers.
The technical challenge we address in this paper
is how to design features for the two tasks. Section
3 presents features based on time-series frequency
and linguistic cues for classifying constant and non-
constant relations. Similarly, section 4 presents
analogous features for classifying unique and non-
unique relations.
3 Features for Constancy Classification
3.1 Time-series frequency
It is intuitive to identify constant relations by com-
paring frequency distributions over arg2 in different
time periods. This idea leads us to use frequency
estimates from time-series text as features.
Time-series text For a time-series text, we used
Japanese blog posts that had been gathered from
Feb. 2006 to Sep. 2011 (68 months). These data in-
clude 2.3 billions of sentences. These posts were ag-
gregated on a monthly basis by using time stamps at-
tached with them, i.e., the unit of time is one month
0
2
4
6
8
10
12
Mar-08 Sep-08 Mar-09 Sep-09 Mar-10 Sep-10 Mar-11 Sep-11
Freq
uenc
y
PAO ChelseaChairman Haiberuden Luzhniki StadiumDutch league ItalyVVV VVV VenloCSKA Moscow
Figure 1: Time-series frequency distribution of ?arg1 be-
longs to arg2? when arg1 takes Keisuke Honda.
in our corpus.
Basic idea For constant relations (e.g., ?arg1 was
born in arg2?), we can expect that the frequency dis-
tributions over arg2 for a given arg1 (e.g., Mozart)
are similar to each other irrespective of the time win-
dows that are used to estimate frequency.
In the case of non-constant relations (e.g., ?arg1
belongs to arg2?), on the other hand, the frequency
distributions over arg2 for a given arg1 significantly
differ depending on the time window. For exam-
ple, Figure 1 illustrates the frequency distributions
of arg2s for ?arg1 belongs to arg2? in which arg1
takes Keisuke Honda, a famous football player. We
can clearly observe that due to Keisuke Honda being
sold from VVV Venlo to CSKA Moscow, the distri-
butions differ greatly between 2008 and 2010.
As is evident from the above discussions, the sta-
bility/change in the distribution over arg2 is a good
indicator of constant/non-constant relations. The
following subsection addresses how to encode such
information as features.
Feature computation Let us examine using as
features the cosine similarity between frequency dis-
tributions over arg2. Averaging such similarities
over representative values of arg1, we have
1
N
?
e?EN (r)
cos(Fw1(r, e), Fw2(r, e)),
where r is a relation (e.g., ?arg1 ?s president is
arg2?), e is a named entity (e.g., United States) ap-
pearing in arg1, and Fw(r, e) is the frequency distri-
bution over arg2 when arg1 takes e. The subscripts
885
w1 and w2 denote the time window (e.g., from Jan.
2011 to Feb. 2011) used to estimate the frequency
distribution. EN (r) denotes a set of top N frequent
entities appearing in arg1. We use the entire time-
series text to obtain EN (r).
Unfortunately, this idea is not suitable for our pur-
pose. The problem is that it is not clear how to deter-
mine the two time windows, w1 and w2. To identify
non-constant relations, arg2 must have different val-
ues in the two time periods. Such time windows are,
however, impossible to know of in advance.
We propose avoiding this difficulty by using av-
erage, maximum and minimum similarity over all
possible time windows:
1
N
?
e?EN (r)
ave
w1,w2?WT
cos(Fw1(r, e), Fw2(r, e)),
1
N
?
e?EN (r)
max
w1,w2?WT
cos(Fw1(r, e), Fw2(r, e)),
1
N
?
e?EN (r)
min
w1,w2?WT
cos(Fw1(r, e), Fw2(r, e)),
where WT is a set of all time windows of the size
T . For example, if we set T to 3 (months) in the
68-month?s worth of blog posts, WT consists of 66
(= 68?3+1) time windows. Although we still have
to specify the number of entities N and the window
size T , this is not a serious problem in practice. We
set N to 100. We use four window sizes (1, 3, 6, and
12 months) and induce different features for each
window size. As a result, we have 12 real-valued
features.
3.2 Linguistic cues
This subsection presents two types of linguistically-
motivated features for discriminating between con-
stant and non-constant relations.
Nominal modifiers We observe that non-constant
relations could be indicated by some nominal modi-
fiers:
(2) a. George Bush, ex-president of USA.
b. Lincoln is the first president of the USA.
The use of the prefix ex- and the adjective first im-
plies that the president changes, and hence the rela-
tion ?arg1 ?s president is arg2? is not constant.
? (ex-),? (present),?? (next),? (former),? (new),
? (old),?? (successive),?? (first),? (first)
Table 3: Japanese prefixes and adjectives indicating non-
constant relations. The translations are provided in the
parentheses.
We propose making use of such modifiers as fea-
tures. Although the above examples are in English,
we think modifiers also exist that have similar mean-
ings in other languages including Japanese, our tar-
get language.
Our new features are induced as follows:
? First, we manually list eight nominal modifiers
that indicate the non-constancy (Table 3).
? Next, we extract nouns from a relation to
be classified (e.g., president), and count the
frequency with which each modifier modifies
those nouns. We use the same blog posts as in
section 3.1 for counting the frequency. Since
time information is not important in this case,
the frequency is simply accumulated over the
entire time span.
? We then generate eight features, one for each of
the eight modifiers. The value of the features
is one if the frequency exceeds threshold ?1,3
otherwise it is zero. Note that the value of this
feature is always zero if the relation includes no
nouns.
Tense and aspect Tense and aspect of verbs are
also important indicators of the non-constancy:
(3) The U.S. president was George Bush.
If a relation, such as ?arg1 ?s president is arg2?, can
often be rephrased in the past tense as in (3), it is
likely to be, if not always, a non-constant relation.
It is, fortunately, straightforward to recognize
tense and aspect in Japanese, because they are ex-
pressed by attaching suffixes to verbs. In this study,
we use three common suffixes: ???, ?????, and
????. The first suffix expresses past tense, while
the other two express present continuous or progres-
sive aspects depending on context.
3?1 = 10 in our experiment.
886
A given relation is transformed into different
forms by attaching the suffixes to a verb in the rela-
tion, and their frequencies are counted. By using the
frequency estimates, we generate three new features,
each of which corresponds to one of the three suf-
fixes. The value of the new features is one if the fre-
quency exceeds threshold ?2,4 otherwise it is zero.
The frequency is counted in the same way as in
the case of the nominal modifiers. The value of
this feature is always zero if the relation includes no
verbs.
4 Features for Uniqueness Classification
This section provides features for identifying unique
relations. These features are also based on the time-
series text and linguistic cues, as in the case of con-
stancy classification.
4.1 Time-series frequency
Number of entity types A straightforward ap-
proach to identifying unique relations is, for a given
arg1, to count the number of entity types appear-
ing in arg2 (Lin et al2010). For unique relations,
the number of entity types should be one in an ideal
noiseless situation. Even if the estimate is contam-
inated by noise, a small number of entity types can
still be considered to indicate the uniqueness of the
relation.
A shortcoming of such a simple approach is that
it never considers the (non-)constancy of relations.
Presume counting the number of entity types in arg2
of the relation ?arg1 is headquartered in arg2?,
which is non-constant and unique. If we use large
size of time window to obtain counts, we will ob-
serve multiple types of entities in arg2, not because
the relation is non-unique, but because it is non-
constant. This problem cannot be resolved by triv-
ially using very small windows, since a time win-
dow that is too small in turn causes a data sparseness
problem.
This problem is attributed to the difficulty in de-
termining the appropriate size of the time window.
We tackle this problem by using the same technique
presented in section 3.1. Specifically, we use the fol-
4?2 = 3000 in our experiment.
lowing three measures as features:
1
N
?
e?EN (r)
ave
w?WT
#type(Fw(r, e)),
1
N
?
e?EN (r)
max
w?WT
#type(Fw(r, e)),
1
N
?
e?EN (r)
min
w?WT
#type(Fw(r, e)),
where the function #type(?) denotes the number of
entity types appearing in arg2.
Ratio of entity frequency Since it is not reliable
enough to use only the number of entity types, we
also exploit the frequency of the entity. Let e1st and
e2nd be the most and the second most frequent enti-
ties found in arg2. If the frequency of e1st is much
larger than that of e2nd, the relation is likely to be
constant.
To encode this intuition, the following measures
are used as features:
1
N
?
e?EN (r)
ave
w?WT
fw(e, r, e1st)
fw(e, r, e2nd)
1
N
?
e?EN (r)
max
w?WT
fw(e, r, e1st)
fw(e, r, e2nd)
1
N
?
e?EN (r)
min
w?WT
fw(e, r, e1st)
fw(e, r, e2nd)
where the fw(e, r, e?) is the frequency of the relation
r in which arg1 and arg2 take e and e?, respectively.
The subscript w denotes the time window.
4.2 Linguistic cues
Coordination structures and some keywords indicate
non-unique relations:
(4) a. France borders on Italy and Spain.
b. France borders on Italy etc.
The coordination structure in the first example im-
plies an entity can border on more than one entity,
and hence the relation ?arg1 borders on arg2? is not
unique. The keyword etc. in the second example also
indicates the non-uniqueness.
887
?,??,?,??,??,??,?
Table 4: List of Japanese particles that are used to form
coordination structures.
To capture this intuition, we introduce two types
of linguistic features for classifying unique and non-
unique relations. The first feature checks whether
entities in arg2 form coordination structures. The
feature is fired if the number of times that coordina-
tion structures are found in arg2 exceeds threshold
?3.5 Coordination structures are identified by a list
of Japanese particles, which roughly correspond to
and or or in English (Table 4). If two entities are
connected by one of those particles, they are seen as
forming a coordination structure.
The second feature exploits such keywords as etc.
for identifying non-unique relations. We list four
Japanese keywords that have similar meaning to the
English word etc., and induce another binary fea-
ture6. The feature is fired if the number of times that
an entity in arg2 is followed by one of the four key-
words exceeds threshold ?3.
5 Experiments and discussions
We built labeled data and examine the classification
performance of the proposed method. We also an-
alyzed the influence of window size T on the per-
formance, as well as major errors caused by our
method.
5.1 Data
We built a dataset for evaluation by extracting rela-
tions from the time-series text (section 3.1) and then
manually annotating 1000 relations. The detailed
procedure is as follows.
First, we parsed the time-series text and extracted
as relation dependency paths connecting two named
entities. We used J.DepP,7 an efficient shift-reduce
parser with feature sequence trie (Yoshinaga and
Kitsuregawa, 2009; Yoshinaga and Kitsuregawa,
2010), for parsing. All Japanese words that conju-
gate were normalized into standard forms.
5?3 = 10 in our experiment.
6The keywords we used are?,?,??, and?.
7http://www.tkl.iis.u-tokyo.ac.jp/
?ynaga/jdepp/
0
0.2
0.4
0.6
0.8
1.0
0 0.2 0.4 0.6 0.8 1.0
Pr
ec
isi
on
Recall
Proposed
Baseline
Figure 2: Recall-precision curve (constancy classifica-
tion).
Then, annotators were asked to label 1000 rela-
tions as not only constant or non-constant but also
unique or non-unique. Three annotators were as-
signed to each relation, and the goldstandard label
is determined by majority vote. The Fleiss kappa
(Fleiss, 1971) was 0.346 for constancy classification
and was 0.428 for uniqueness classification. They
indicate fair and moderate agreement, respectively
(Landis and Koch, 1977).
We have briefly investigated the relations whose
labels assigned by the annotators conflicted. The
major cause was that the annotators sometimes as-
sumed different types of named entities as values
of arguments. A typical case in which this problem
arises is that the relation has polysemous meanings,
e.g., ?arg1 was born in arg2?, or a vague meaning,
e.g., ?arg1makes arg2?. For example, arg2 of ?arg1
was born in arg2? can be filled with different types
of entities such as date and place. We can address
this problem by typing arguments (Lin et al2010).
5.2 Result
Using the dataset, we performed 5-fold cross-
validation for both classification tasks. We used
the passive-aggressive algorithm for our classifier
(Crammer et al2006).
Constancy classification Figure 2 illustrates the
recall-precision curve in constancy classification.
Because we are unaware of any previous methods
for classifying constant and non-constant relations,
a simple method based on the cosine similarity was
888
00.2
0.4
0.6
0.8
1.0
0 0.2 0.4 0.6 0.8 1.0
Pr
ec
isi
on
Recall
Proposed
Baseline
Figure 3: Recall-precision curve (uniqueness classifica-
tion).
used as a baseline:
1
N
?
e?EN (r)
cos(Fw1(r, e), Fw2(r, e)),
where the time windows w1 and w2 are determined
as the first and last month in which the relation r
is observed. A given relation is classified as non-
constant if the above similarity exceeds a threshold.
The recall-precision curve was drawn by changing
the threshold.
The results demonstrated that our method outper-
forms the baseline. This indicates the effectiveness
of using time-series frequency and linguistic cues as
features.
The poor performance of the baseline was mainly
due to data sparseness. Since the baseline method is
dependent on the frequency estimates obtained from
only two months of texts, it is less reliable than the
proposed method.
Uniqueness classification Figure 3 illustrates the
recall-precision curve in uniqueness classification.
As a baseline we implemented the method proposed
by Lin et al2010). While they have presented
three methods (KLFUNC, KLDIFF, and their aver-
age), we report the results of the last one because it
performed the best among the three in our experi-
ment.
From the figure, we can again see that the pro-
posed method outperforms the baseline method.
Lin?s method is similar to ours, but differs in that
they do not exploit time-series information at all.
0
0.2
0.4
0.6
0.8
1.0
0 0.2 0.4 0.6 0.8 1.0
Pr
ec
isi
on
Recall
N = 2N = 10N = 20N = 100
Figure 4: Comparison with the methods varying a value
of N for constancy classification.
0
0.2
0.4
0.6
0.8
1.0
0 0.2 0.4 0.6 0.8 1.0
Pr
ec
isi
on
Recall
N = 2N = 10N = 20N = 100
Figure 5: Comparison with the methods varying a value
of N for uniqueness classification.
We hence conclude time-series information is use-
ful for classifying not only constant but also unique
relations.
5.3 Investigation into the number of entities, N
We ranged the value of N in {2, 10, 20, 100}. Set-
ting N to a larger value yields the better recall for
constancy classification and the better precision for
uniqueness classification (Figures 4 and 5). These
results meet our expectations, since features derived
from frequency distributions of arg2 over various
arg1s capture the generic nature of the target rela-
tion.
889
00.2
0.4
0.6
0.8
1.0
0 0.2 0.4 0.6 0.8 1.0
Pr
ec
isi
on
Recall
T = 1, 3, 6, 12T = 1T = 3T = 6T = 12
Figure 6: Comparison with the methods using only a sin-
gle value of T for constancy classification.
0
0.2
0.4
0.6
0.8
1.0
0 0.2 0.4 0.6 0.8 1.0
Pr
ec
isi
on
Recall
T = 1, 3, 6, 12T = 1T = 3T = 6T = 12
Figure 7: Comparison with the methods using only a sin-
gle value of T for uniqueness classification.
5.4 Investigation into the window size, T
Our method uses multiple time windows of different
sizes (i.e., different values of T ) to induce features,
as detailed in sections 3.1 and 4.1. To confirm the
effect of this technique, we investigated the perfor-
mance when we use only a single value of T (Fig-
ures 6 and 7).
The results in the uniqueness classification task
demonstrated that our method achieves better over-
all results than the methods using a single value of
T . We can therefore consider that using multiple
values of T as features is a reasonable strategy. On
the other hand, we could not confirm the effect of
using multiple time windows of different sizes in the
constancy classification task.
5.5 Error analysis
We randomly selected and analyzed 200 misclassi-
fied relations for both tasks. The analysis revealed
four types of errors.
Paraphrases We observed that constant relations
are prone to be miss-classified as non-constant when
more than one paraphrase appear in arg2 and thus
the value of arg2 is pretended to change. For exam-
ple, America was also referred to as USA or United
States of America. A similar problem was observed
for unique relations as well.
Topical bias Topics mentioned in the blog posts
are sometimes biased, and such bias can have a neg-
ative effect on classification, especially when a rela-
tion takes a small number of entity types in arg2 for
given arg1. For example, Jaden Smith, who is one
of Will Smith?s sons, is frequently mentioned in our
time-series text because he co-starred with his father
in a movie, while Will Smith?s other sons never ap-
peared in our text. We consider this a possible rea-
son for our method wrongly identifying ?arg1 ?s son
is arg2? as a unique relation.
Short-/Long-term evolution Since we have ag-
gregated on a monthly basis the 6-year?s worth of
blog posts, the induced features cannot capture evo-
lutions that occur in shorter or longer intervals. For
example, consider relation ?arg1 beats arg2? tak-
ing Real Madrid as arg1. Since Real Madrid usually
have more than one football match in a month, they
can beat several teams in a month, which misleads
the classifier to recognize the relation as non-unique.
Similarly when a relation takes more than 6 years to
evolve, it will be regarded as constant.
Reference to past, future, or speculative facts
The blog authors sometimes refer to relations that do
not occur around when they write their posts; such
relations actually occurred in the past, will occur in
the future, or even speculative. Since our method
exploits the time stamps attached to the posts to as-
sociate the relations with time, those relations in-
troduce noises in the frequency distributions. Al-
though our robust feature induction could in most
cases avoid an adverse effect caused by these noises,
they sometimes leaded to misclassification.
890
6 Related Work
In recent years, much attention has been given to
extracting relations from a massive amount of tex-
tual data, especially the web (cf. section 1). Most of
those studies, however, explored just extracting re-
lations from text. Only a few studies, as described
below, have discussed classifying those relations.
There has been no previous work on identify-
ing the constancy of relations. The most relevant
research topic is the temporal information extrac-
tion (Verhagen et al2007; Verhagen et al2010;
Ling and Weld, 2010; Wang et al2010; Hovy et
al., 2012). This is the task of extracting from textual
data an event and the time it happened, e.g., Othello
was written by Shakespeare in 1602. Such tempo-
ral information alone is not sufficient for identifying
the constancy of relations, while we think it would
be helpful.
On the other hand, the uniqueness of relations has
so far been discussed in some studies. Ritter et al
(2008) have pointed out the importance of identi-
fying unique relations for various NLP tasks such
as contradiction detection, quantifier scope disam-
biguation, and synonym resolution. They proposed
an EM-style algorithm for scoring the uniqueness
of relations. Lin et al2010) also proposed three
algorithms for identifying unique relations. While
those studies discussed the same problem as this pa-
per, they did not point out the importance of the
constancy in identifying unique relations (cf. sec-
tion 4.1).
7 Conclusion
This paper discussed that the notion of constancy
is essential in compiling relations between enti-
ties extracted from real-world text and proposed a
method for classifying relations on the basis of con-
stancy and uniqueness. The time-series web text
was fully exploited to induce frequency-based fea-
tures from time-series frequency distribution on re-
lation instances as well as language-based features
tailored for individual classification tasks. Exper-
imental results confirmed that the frequency-based
features contributed much to the precision and recall
in both identification tasks.
We will utilize the identified properties of the re-
lations to adopt an appropriate strategy to compile
their instances. We also plan to start a spin-off re-
search that acquires paraphrases by grouping values
of arg2s for each value of arg1 in a constant, unique
relation.
We consider that the notion of constancy will even
be beneficial in acquiring world knowledge, other
than relations between entities, from text; we aim
at extending the notion of constancy to other types
of knowledge involving real-world entities, such as
concept-instance relations.
Acknowledgments
This work was supported by the Multimedia Web
Analysis Framework towards Development of So-
cial Analysis Software program of the Ministry of
Education, Culture, Sports, Science and Technol-
ogy, Japan. The authors thank the annotators for
their hard work. The authors are also indebted to the
three anonymous reviewers for their valuable com-
ments.
References
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI, pages 2670?2676.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shawartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?583.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
Fan, David Gondek, Aditya A. Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John Prager,
Nico Schlaefer, and Chris Welty. 2010. Building Wat-
son: An overview of the DeepQA project. AI Maga-
zine, 31(3):59?79.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
Dirk Hovy, James Fan, Alfio Gliozzo, Siddharth Patward-
han, and Christopher Welty. 2012. When did that hap-
pen? ? linking events and relations to timestamps. In
Proceedings of EACL, pages 185?193.
Richard J. Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 1(33):159?174.
Thomas Lin, Mausam, and Oren Etzioni. 2010. Identify-
ing functional relation in web text. In Proceedings of
EMNLP, pages 1266?1276.
891
Xiao Ling and Daniel S. Weld. 2010. Temporal informa-
tion extraction. In Proceedings of AAAI, pages 1385?
1390.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of ACL-IJCNLP,
pages 1003?1011.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of ACL, pages
113?120.
Alan Ritter, Doug Downey, Stephen Soderland, and Oren
Etzioni. 2008. It?s a contradiction?no, it?s not: A
case study using functional relations. In Proceedings
of EMNLP, pages 11?20.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A core of semantic knowl-
edge unifying WordNet and Wikipedia. In Proceed-
ings of WWW, pages 697?706.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. SemEval-2007 task 15: TempEval temporal
relation identification. In Proceedings of SemEval,
pages 75?80.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. SemEval-2010 task 13:
TempEval-2. In Proceedings of SemEval, pages 57?
62.
Yafang Wang, Mingjie Zhu, Lizhen Qu, Marc Spaniol,
and Gerhard Weikum. 2010. Timely YAGO: har-
vesting, querying, and visualizing temporal knowledge
from Wikipedia. In Proceedings of EDBT, pages 697?
700.
Gerhard Weikum, Srikanta Bedathur, and Ralf Schenkel.
2011. Temporal knowledge for timely intelligence. In
Proceedings of BIRTE, pages 1?6.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using Wikipedia. In Proceedings of ACL,
pages 118?127.
Fei Wu, Raphael Hoffmann, and Daniel S. Weld. 2008.
Information extraction from Wikipedia: moving down
the long tail. In Proceedings of KDD, pages 731?739.
Naoki Yoshinaga and Masaru Kitsuregawa. 2009. Poly-
nomial to linear: Efficient classification with conjunc-
tive features. In Proceedings of EMNLP, pages 1542?
1551.
Naoki Yoshinaga andMasaru Kitsuregawa. 2010. Kernel
slicing: Scalable online training with conjunctive fea-
tures. In Proceedings of COLING, pages 1245?1253.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-
Rong Wen. 2009. StatSnowball: a statistical approach
to extracting entity relationships. In Proceedings of
WWW, pages 101?110.
892
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 99?109,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Accurate Word Segmentation and POS Tagging for Japanese Microblogs:
Corpus Annotation and Joint Modeling with Lexical Normalization
Nobuhiro Kaji?? and Masaru Kitsuregawa??
?National Institute of Information and Communications Technology
?Institute of Industrial Science, The University of Tokyo
?National Institute of Informatics
{kaji, kitsure}@tkl.iis.u-tokyo.ac.jp
Abstract
Microblogs have recently received
widespread interest from NLP re-
searchers. However, current tools for
Japanese word segmentation and POS
tagging still perform poorly on microblog
texts. We developed an annotated corpus
and proposed a joint model for over-
coming this situation. Our annotated
corpus of microblog texts enables not
only training of accurate statistical models
but also quantitative evaluation of their
performance. Our joint model with lexical
normalization handles the orthographic
diversity of microblog texts. We con-
ducted an experiment to demonstrate
that the corpus and model substantially
contribute to boosting accuracy.
1 Introduction
Microblogs, such as Twitter1 and Weibo2, have re-
cently become an important target of NLP tech-
nology. Since microblogs offer an instant way of
posting textual messages, they have been given
increasing attention as valuable sources for such
actions as mining opinions (Jiang et al., 2011)
and detecting sudden events such as earthquake
(Sakaki et al., 2010).
However, many studies have reported that cur-
rent NLP tools do not perform well on microblog
texts (Foster et al., 2011; Gimpel et al., 2011). In
the case of Japanese text processing, the most se-
rious problem is poor accuracy of word segmen-
tation and POS tagging. Since these two tasks
are positioned as the fundamental step in the text
processing pipeline, their accuracy is vital for all
downstream applications.
1https://twitter.com
2https://www.weibo.com
1.1 Development of annotated corpus
The main obstacle that makes word segmentation
and POS tagging in the microblog domain chal-
lenging is the lack of annotated corpora. Because
current annotated corpora are from other domains,
such as news articles, it is difficult to train models
that perform well on microblog texts. Moreover,
system performance cannot be evaluated quantita-
tively.
We remedied this situation by developing an an-
notated corpus of Japanese microblogs. We col-
lected 1831 sentences from Twitter and manually
annotated these sentences with word boundaries,
POS tags, and normalized forms of words (c.f.,
Section 1.2).
We, for the first time, present a comprehen-
sive empirical study of Japanese word segmenta-
tion and POS tagging on microblog texts by us-
ing this corpus. Specifically, we investigated how
well current models trained on existing corpora
perform in the microblog domain. We also ex-
plored performance gains achieved by using our
corpus for training, and by jointly performing lex-
ical normalization (c.f., Section 1.2).
1.2 Joint modeling with lexical normalization
Orthographic diversity in microblog texts causes a
problem when training a statistical model for word
segmentation and POS tagging. Microblog texts
frequently contain informal words that are spelled
in a non-standard manner, e.g., ?oredi (already)?,
?b4 (before)?, and ?talkin (talking)? (Han and
Baldwin, 2011). Such words, hereafter referred
to as ill-spelled words, are so productive that they
considerably increase the vocabulary size. This
makes training of statistical models difficult.
We address this problem by jointly conducting
lexical normalization. Although a wide variety
of ill-spelled words are used in microblog texts,
many can be normalized into well-spelled equiva-
lents, which conform to standard rules of spelling.
99
A joint model with lexical normalization is able
to handle orthographic diversity by exploiting in-
formation obtainable from the well-spelled equiv-
alents.
The proposed joint model was empirically eval-
uated on the microblog corpus we developed. Our
experiment demonstrated that the proposed model
can perform word segmentation and POS tag-
ging substantially better than current state-of-the-
art models.
1.3 Summary
Contributions of this paper are the following:
? We developed a microblog corpus that en-
ables not only training of accurate models but
also quantitative evaluation for word segmen-
tation and POS tagging in the microblog do-
main.3
? We propose a joint model with lexical nor-
malization for better handling of ortho-
graphic diversity in microblog texts. In par-
ticular, we present a new method of training
the joint model using a partially annotated
corpus (c.f., Section 7.4).
? We, for the first time, present a comprehen-
sive empirical study of word segmentation
and POS tagging for microblogs. The experi-
mental results demonstrated that both the mi-
croblog corpus and joint model greatly con-
tributes to training accurate models for word
segmentation and POS tagging.
The remainder of this paper is organized as fol-
lows. Section 2 reviews related work. Section 3
discusses the task of lexical normalization and in-
troduces terminology. Section 4 presents our mi-
croblog corpus and results of our corpus analysis.
Section 5 presents an overview of our joint model
with lexical normalization, and Sections 6 and 7
provide details of the model. Section 8 presents
experimental results and discussions, and Section
9 presents concluding remarks.
2 Related Work
Researchers have recently developed various mi-
croblog corpora annotated with rich linguistic in-
formation. Gimpel et al. (2011) and Foster et
al. (2011) annotated English microblog posts with
3Please contact the first author for this corpus.
POS tags. Han and Baldwin (2011) released a mi-
croblog corpus annotated with normalized forms
of words. A Chinese microblog corpus annotated
with word boundaries was developed for SIGHAN
bakeoff (Duan et al., 2012). However, there are
no microblog corpora annotated with word bound-
aries, POS tags, and normalized sentences.
There has been a surge of interest in lexical nor-
malization with the advent of microblogs (Han and
Baldwin, 2011; Liu et al., 2012; Han et al., 2012;
Wang and Ng, 2013; Zhang et al., 2013; Ling et
al., 2013; Yang and Eisenstein, 2013; Wang et al.,
2013). However, these studies did not address en-
hancing word segmentation.
Wang et al. (2013) proposed a method of joint
ill-spelled word recognition and word segmenta-
tion. With their method, informal spellings are
merely recognized and not normalized. Therefore,
they did not investigate how to exploit the infor-
mation obtainable from well-spelled equivalents
to increase word segmentation accuracy.
Some studies also explored integrating the lexi-
cal normalization process into word segmentation
and POS tagging (Ikeda et al., 2009; Sasano et al.,
2013). A strength of our joint model is that it uses
rich character-level and word-level features used
in state-of-the-art models of joint word segmenta-
tion and POS tagging (Kudo et al., 2004; Neubig
et al., 2011; Kaji and Kitsuregawa, 2013). Thanks
to these features, our model performed much bet-
ter than Sasano et al.?s system, which is the only
publicly available system that jointly conducts lex-
ical normalization, in the experiments (see Section
8). Another advantage is that our model can be
trained on a partially annotated corpus. Further-
more, we present a comprehensive evaluation in
terms of precision and recall on our microblog cor-
pus. Such an evaluation has not been conducted in
previous work due to the lack of annotated cor-
pora.4
3 Lexical Normalization Task
This section explains the task of lexical normal-
ization addressed in this paper. Since lexical nor-
malization is a relatively new research topic, there
are no precise definitions of a lexical normaliza-
tion task that are widely accepted by researchers.
4Very recently, Saito et al. (2014) conducted similar em-
pirical evaluation on microblog corpus. However, they used
biased dataset, in which every sentence includes at least one
ill-spelled words.
100
Table 1: Examples of our target ill-spelled words
and their well-spelled equivalents. Phonemes are
shown between slashes. English translations are
provided in parentheses.
Ill-spelled word Well-spelled equivalent
??? /sugee/ ??? /sugoi/ (great)
?? /modoro/ ??? /modorou/ (going to return)
?????? /umaiiii/ ??? /umai/ (yummy)
Therefore, it is important to clarify our task setting
before discussing our joint model.
3.1 Target ill-spelled words
Many studies on lexical normalization have
pointed out that phonological factors are deeply
involved in the process of deriving ill-spelled
words. Xia et al. (2006) investigated a Chi-
nese chat corpus and reported that 99.2% of the
ill-spelled words were derived by phonetic map-
ping from well-spelled equivalents. Wang and
Ng (2013) analyzed 200 Chinese messages from
Weibo and 200 English SMS messages from the
NUS SMS corpus (How and Kan, 2005). Their
analysis revealed that most ill-spelled words were
derived from well-spelled equivalents based on
pronunciation similarity.
On top of these investigations, we focused on
ill-spelled words that are derived by phonologi-
cal mapping from well-spelled words by assum-
ing that such ill-spelled words are dominant in
Japanese microblogs as well. We also assume
that these ill-spelled words can be normalized into
well-spelled equivalents on a word-to-word basis,
as assumed in a previous study (Han and Baldwin,
2011). The validity of these two assumptions is
empirically assessed in Section 4.
Table 1 lists examples of our target ill-spelled
words, their well-spelled equivalents, and their
phonemes. The ill-spelled word in the first row
is formed by changing the continuous two vowels
from /oi/ to /ee/. This type of change in pronun-
ciation is often observed in Japanese spoken lan-
guage. The second row presents contractions. The
last vowel character ??? /u/ of the well-spelled
word is dropped. The third row illustrates word
lengthening. The ill-spelled word is derived by re-
peating the vowel character ??? /i/.
3.2 Terminology
We now introduce the terminology that will be
used throughout the remainder of this paper. The
term word surface form (or surface form for short)
is used to refer to the word form observed in an
actual text, while word normal form (or normal
form) refers to the normalized word form. Note
that surface forms of well-spelled words are al-
ways identical to their normal forms.
It is possible that the word surface form and nor-
mal form have distinct POS tags, although they are
identical in most cases. Take the ill-spelled word ?
??? /modoro/ as an example (the second row of
Table 1). According to the JUMAN POS tag set,5
POS of its surface form is CONTRACTED VERB,
while that of its normal form is VERB.6 To handle
such a case, we strictly distinguish between these
two POS tags by referring to them as surface POS
tags and normal POS tags, respectively.
Given these terms, the tasks addressed in this
paper can be stated as follows. Word segmenta-
tion is a task of segmenting a sentence into a se-
quence of word surface forms, and POS tagging
is a task of providing surface POS tags. The task
of joint lexical normalization, word segmentation,
and POS tagging is to map a sentence into a se-
quence of quadruplets: word surface form, surface
POS tag, normal form, and normal POS tag.
4 Microblog Corpus
This section introduces our microblog corpus. We
first explain the process of developing the corpus
then present the results of our agreement study and
corpus analysis.
4.1 Data collection and annotation
The corpus was developed by manually annotating
text messages posted to Twitter.
The posts to be annotated were collected as fol-
lows. 171,386 Japanese posts were collected using
the Twitter API7 on December 6, 2013. Among
these, 1000 posts were randomly selected then
manually split into sentences. As a result, we ob-
tained 1831 sentences as a source of the corpus.
Two human participants annotated the 1831
sentences with surface forms and surface POS
tags. Since much effort has already been done to
annotate corpora with this information, the anno-
tation process here follows the guidelines used to
5http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN
6In this paper, we use simplified POS tags for explana-
tion purposes. Remind that these tags are different from the
original ones defined in JUMAN POS tag set.
7https://stream.twitter.com/1.1/statuses/sample.json
101
develop such corpora in previous studies (Kuro-
hashi and Nagao, 1998; Hashimoto et al., 2011).
The two participants also annotated ill-spelled
words with their normal forms and normal POS
tags. Although this paper targets only infor-
mal phonological variations (c.f., Section 3),
other types of ill-spelled words were also anno-
tated to investigate their frequency distribution
in microblog texts. Specifically, besides infor-
mal phonological variations, spelling errors and
Twitter-specific abbreviations were annotated. As
a result, 833 ill-spelled words were identified (Ta-
ble 2). They were all annotated with normal forms
and normal POS tags.
4.2 Agreement study
We investigated the inter-annotator agreement to
check the reliability of the annotation. During the
annotation process, the two participants collabo-
ratively annotated around 90% of the sentences
(specifically, 1647 sentences) with normal forms
and normal POS tags, and elaborated an annota-
tion guideline through discussion. They then inde-
pendently annotated the remaining 184 sentences
(1431 words), which were used for the agreement
study. Our annotation guideline is shown in the
supplementary material.
We first explored the extent to which the
two participants agreed in distinguishing between
well-spelled words and ill-spelled words. For this
task, we observed Cohen?s kappa of 0.96 (almost
perfect agreement). This results show that it is
easy for humans to distinguish between these two
types of words.
Next, we investigated whether the two partici-
pants could give ill-spelled words with the same
normal forms and normal POS tags. For this pur-
pose, we regarded the normal forms and normal
POS tags annotated by one participant as goldstan-
dards and calculated precision and recall achieved
by the other participant. We observed moder-
ate agreement between the two participants: 70%
(56/80) precision and 73% (56/76) recall. We
manually analyzed the conflicted examples and
found that there were more than one acceptable
normal form in many of these cases. Therefore,
we would like to note that the precision and recall
reported above are rather pessimistic estimations.
4.3 Analysis
We conducted corpus analysis to confirm the fea-
sibility of our approach.
Table 2: Frequency distribution over three types of
ill-spelled words in corpus.
Type Frequency
Informal phonological variation 804 (92.9%)
Spelling error 27 (3.1%)
Twitter-specific abbreviation 34 (3.9%)
Total 865 (100%)
Table 2 illustrates that phonological variations
constitute a vast majority of ill-spelled words in
Japanese microblog texts. In addition, analysis
of the 804 phonological variations showed that
793 of them can be normalized into single words.
These represent the validity of the two assump-
tions we made in Section 3.1.
We then investigated whether lexical normaliza-
tion can decrease the number of out-of-vocabulary
words. For the 793 ill-spelled words, we counted
how many of their surface forms and normal
forms were not registered in the JUMAN dictio-
nary.8 The result suggests that 411 (51.8%) and
74 (9.3%) are not registered in the dictionary. This
indicates the effectiveness of lexical normalization
for decreasing out-of-vocabulary words.
5 Overview of Joint Model
This section gives an overview of our joint model
with lexical normalization for accurate word seg-
mentation and POS tagging.
5.1 Lattice-based approach
A lattice-based approach has been commonly
adopted to perform joint word segmentation and
POS tagging (Jiang et al., 2008; Kudo et al., 2004;
Kaji and Kitsuregawa, 2013). In this approach, an
input sentence is transformed into a word lattice
in which the edges are labeled with surface POS
tags (Figure 1). Given such a lattice, word seg-
mentation and POS tagging can be performed at
the same time by traversing the lattice. A discrim-
inative model is typically used for the traversal.
An advantage of this approach is that, while the
lattice can represent an exponentially large num-
ber of candidate analyses, it can be quickly tra-
versed using dynamic programming (Kudo et al.,
2004; Kaji and Kitsuregawa, 2013) or beam search
(Jiang et al., 2008). In addition, a discriminative
model allows the use of rich word-level features
to find the correct analysis.
8http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN
102
? ? ? ? ??
1RXQ
9HUE
1RXQ
1RXQ 1RXQ 1RXQ
3DUWLFOH6XIIL[
,QSXWVHQWHQFH: ?????? 7ROLYHLQ7RN\RPHWURSROLV
:RUGODWWLFH:
Figure 1: Example lattice (Kudo et al., 2004; Kaji
and Kitsuregawa, 2013). Circle and arrow repre-
sent node and edge, respectively. Bold edges rep-
resent correct analysis.
,QSXWVHQWHQFH:??????? 1RWWRXQGHUVWDQG(QJOLVK
:RUGODWWLFH:
1RUPDOL]HGVHQWHQFH:?? ??? ??
1RXQ
??1RXQ
1RXQ
??1RXQ
6XIIL[
??6XIIL[
9HUE
??? 9HUE
?? ?? ? ??
3DUWLFOH
?3DUWLFOH
6XIIL[
??6XIIL[
Figure 2: Lattice used to perform joint task. Nor-
mal forms and normal POS tags are shown in
parentheses. As indicated by dotted arrows, nor-
malized sentence can be obtained by concatenat-
ing normal forms associated with edges in correct
analysis.
We propose extending the lattice-based ap-
proach to jointly perform lexical normalization,
word segmentation, and POS tagging. We trans-
form an input sentence into a word lattice in which
the edges are labeled with not only surface POS
tags but normal forms and normal POS tags (Fig-
ure 2). By traversing such a lattice, the three
tasks can be performed at the same time. This ap-
proach can not only exploit rich information ob-
tainable from word normal forms, but also achieve
efficiency similar to the original lattice-based ap-
proach.
5.2 Issues
Issues on how to develop this lattice-based ap-
proach is detailed in Sections 6 and 7.
Section 6 describes how to generate a word lat-
tice from an input sentence. This is done us-
ing a hybrid approach that combines a statistical
model and normalization dictionary. The normal-
ization dictionary is specifically a list of quadru-
Table 3: Normalization dictionary. Columns rep-
resent entry ID, surface form, surface POS, normal
form, and normal POS, respectively.
ID Surf. Surf. POS Norm. Norm. POS
A ??? ADJECTIVE ??? ADJECTIVE
B ??? ADJECTIVE ??? ADJECTIVE
C ??? VERB ??? VERB
D ?? CONTR. VERB ??? VERB
E ??? ADJECTIVE ??? ADJECTIVE
F ?????? ADJECTIVE ??? ADJECTIVE
Table 4: Tag dictionary.
ID Surf. form Surf. POS
a ??? (great) ADJECTIVE
b ??? (going to return) VERB
c ?? (gonna return) CONTR. VERB
d ??? (yummy) ADJECTIVE
plets: word surface form, surface POS tag, normal
form, and normal POS tag (Table 3).
Section 7 describes a discriminative model for
the lattice traversal. Our feature design as well as
two training methods are presented.
6 Word Lattice Generation
In this section, we first describe a method of con-
structing a normalization dictionary then present a
method of generating a word lattice from an input
sentence.
6.1 Construction of normalization dictionary
Although large-scale normalization dictionaries
are difficult to obtain, tag dictionaries, which list
pairs of word surface forms and their surface POS
tags (Table 4), are widely available in many lan-
guages including Japanese. Therefore, we use an
existing tag dictionary to construct the normaliza-
tion dictionary.
Due to space limitations, we give only a brief
overview of our construction method, omitting its
details. We note that our method uses hand-crafted
rules similar to those used in (Sasano et al., 2013);
hence, the proposal of this method is not an im-
portant contribution. To make our experimental
results reproducible, our normalization dictionary,
as well as a tool for constructing it, is released as
supplementary material.
Our method of constructing the normalization
dictionary takes three steps. The following ex-
plains each step using Tables 3 and 4 as running
examples.
103
Step 1 A tag dictionary generally contains a
small number of ill-spelled words, although well-
spelled words constitute a vast majority. We iden-
tify such ill-spelled words by using a manually-
tailored list of surface POS tags indicative of in-
formal spelling (e.g., CONTRACTED VERB). For
example, entry (c) in Table 4 is identified as an
ill-spelled word in this step.
Step 2 The tag dictionary is augmented with
normal forms and normal POS tags to construct
a small normalization dictionary. For ill-spelled
words identified in step 1, the normal forms and
normal POS tags are determined by hand-crafted
rules. For example, the normal form is derived by
appending the vowel character ??? /u/ to the sur-
face form, if the surface POS tag is CONTRACTED
VERB. This rule derives entry (D) in Table 3 from
entry (c) in Table 4. For well-spelled words, on
the other hand, the normal forms and normal POS
tags are simply set the same as the surface forms
and surface POS tags. For example, entries (A),
(C), and (E) in Table 3 are generated from entries
(a), (b), and (d) in Table 4, respectively.
Step 3 Because the normalization dictionary
constructed in step 2 contains only a few ill-
spelled words, it is expanded in this step. For this
purpose, we use hand-crafted rules to derive ill-
spelled words from the entries already registered
in the normalization dictionary. Some rules are
taken from (Sasano et al., 2013), while the others
are newly tailored. In Table 3, for example, entry
(B) is derived from entry (A) by applying the rule
that substitutes ???? /goi/ with ???? /gee/.
A small problem that arises in step 3 is how to
handle lengthened words, such as entry (F) in Ta-
ble 3. While lengthened words can be easily de-
rived using simple rules (Brody and Diakopoulos,
2011; Sasano et al., 2013), such rules infinitely
increase the number of entries because an unlim-
ited number of lengthened words can be derived
by repeating characters. To address this problem,
no lengthened words are added to the normaliza-
tion dictionary in step 3. We instead use rules
to skip repetitive characters in an input sentence
when performing dictionary match.
6.2 A hybrid approach
A word lattice is generated using both a statisti-
cal method (Kaji and Kitsuregawa, 2013) and the
normalization dictionary.
We begin by generating a word lattice which en-
codes only word surface forms and surface POS
tags (c.f., Figure 1) using the statistical method
proposed by Kaji and Kitsuregawa (2013). Inter-
ested readers may refer to their paper for details.
Each edge in the lattice is then labeled with nor-
mal forms and normal POS tags. Note that a sin-
gle edge can have more than one candidate normal
form and normal POS tag. In such a case, new
edges are accordingly added to the lattice.
The edges are labeled with normal forms and
normal POS tags in the following manner. First,
every edge is labeled with a normal form and
normal POS tag that are identical with the sur-
face form and surface POS tag. This is based on
our observation that most words are well-spelled
ones. The edge is not provided with further nor-
mal forms and normal POS tags, if the normaliza-
tion dictionary contains a well-spelled word that
has the same surface form as the edge. Otherwise,
we allow the edge to have all pairs of normal forms
and normal POS tags that are obtained by using the
normalization dictionary.
7 Discriminative Lattice Traversal
This section explains a discriminative model for
traversing the word lattice. The lattice traversal
with a discriminative model can formally be writ-
ten as
(w, t,v, s) = argmax
(w,t,v,s)?L(x)
f(x,w, t,v, s) ? ?.
Here, x denotes an input sentence, w, t, v, and s
denote a sequence of word surface forms, surface
POS tags, normal forms, and normal POS tags, re-
spectively, L(x) represents a set of candidate anal-
yses represented by the word lattice, and f(?) and
? are feature and weight vectors.
We now describe features, a decoding method,
and two training methods.
7.1 Features
We use character-level and word-level features
used for word segmentation and POS tagging in
(Kaji and Kitsuregawa, 2013). To take advan-
tage of joint model with lexical normalization, the
word-level features are extracted from not only
surface forms but also normal forms. See (Kaji
and Kitsuregawa, 2013) for the original features.
In addition, several new features are introduced
in this paper. We use the quadruplets (w
i
, t
i
, v
i
, s
i
)
104
and pairs of surface and normal POS tags (t
i
, s
i
)
as binary features to capture probable mappings
between ill-spelled words and their well-spelled
equivalents. We use another binary feature indi-
cating whether a quadruplet (w
i
, t
i
, v
i
, s
i
) is reg-
istered in the normalization dictionary. Also, we
use a bigram language model feature, which pre-
vents sentences from being normalized into un-
grammatical and/or incomprehensible ones. The
language model features are associated with nor-
malized bigrams, (v
i?1
, s
i?1
, v
i
, s
i
), and take as
the values the logarithmic frequency log
10
(f +1),
where f represents the bigram frequency (Kaji and
Kitsuregawa, 2011). Since it is difficult to obtain
a precise value of f , it is approximated by the fre-
quency of the surface bigram, (w
i?1
, t
i?1
, w
i
, t
i
),
calculated from a large raw corpus automatically
analyzed using a system of joint word segmenta-
tion and POS tagging. See Section 8.1 for the raw
corpus and system used in the experiments.
7.2 Decoding
It is easy to find the best analysis (w, t,v, s)
among the candidates represented by the word lat-
tice. Although we use several new features, we
can still locate the best analysis by using the same
dynamic programming algorithm as in previous
studies (Kudo et al., 2004; Kaji and Kitsuregawa,
2013).
7.3 Training on a fully annotated corpus
It is straightforward to train the joint model pro-
vided with a fully annotated corpus, which is la-
beled with word surface forms, surface POS tags,
normal forms, and normal POS tags.
We use structured perceptron (Collins, 2002)
for the training (Algorithm 1). The training be-
gins by initializing ? as a zero vector (line 1).
It then reads the annotated corpus C (line 2-9).
Given a training example, (x,w, t,v, s) ? C, the
algorithm locates the best analysis, ( ?w, ?t, ?v, ?s),
based on the current weight vector (line 4). If
the best analysis differs from the oracle analy-
sis, (w, t,v, s), the weight vector is updated (line
5-7). After going through the annotated corpus
m times (m=10 in our experiment), the averaged
weight vector is returned (line 10).
7.4 Training on a partially annotated corpus
Although the training with the perceptron algo-
rithm requires a fully annotated corpus, it is labor-
intensive to fully annotate sentences. This consid-
Algorithm 1 Perceptron training
1: ? ? 0
2: for i = 1 . . . m do
3: for (x,w, t,v, s) ? C do
4: ( ?w, ?t, ?v, ?s)? DECODING(x, ?)
5: if (w, t,v,s) 6= ( ?w, ?t, ?v, ?s) then
6: ? ? ? + f (x,w, t,v, s)? f (x, ?w, ?t, ?v, ?s)
7: end if
8: end for
9: end for
10: return AVERAGE(?)
Algorithm 2 Latent perceptron training
1: ? ? 0
2: for i = 1 . . . m do
3: for (x,w, t) ? C? do
4: ( ?w, ?t, ?v, ?s)? DECODING(x, ?)
5: (w, t, ?v, ?s)? CONSTRAINEDDECODING(x,?)
6: if w 6= ?w or t 6= ?t then
7: ? ? ? + f (x,w, t, ?v, ?s)? f (x, ?w, ?t, ?v, ?s)
8: end if
9: end for
10: end for
11: return AVERAGE(?)
eration motivates us to explore training our model
with less supervision. We specifically explore us-
ing a corpus annotated with only word boundaries
and POS tags.
We use the latent perceptron algorithm (Sun et
al., 2013) to train the joint model from such a par-
tially annotated corpus (Algorithm 2). In this sce-
nario, a training example is a sentence x paired
with a sequence of word surface forms w and sur-
face POS tags t (c.f., line 3). Similarly to the
perceptron algorithm, we locate the best analy-
sis ( ?w, ?t, ?v, ?s) for a given training example, (line
4). We also locate the best analysis, (w, t, ?v, ?s),
among those having the same surface forms w and
surface POS tags t as the training example (line
5). If the surface forms and surface POS tags of
the former analysis differ from the annotations of
the training example, parameter is updated by re-
garding the latter analysis as an oracle (line 6-8).
8 Experiments
We conducted experiments to investigate how the
microblog corpus and joint model contribute to
improving accuracy of word segmentation and
POS tagging in the microblog domain.
8.1 Setting
We constructed the normalization dictionary from
the JUMAN dictionary 7.0.9 While JUMAN dic-
9http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
105
tionary contains 750,156 entries, the normaliza-
tion dictionary contains 112,458,326 entries.
Some features taken from the previous study
(Kaji and Kitsuregawa, 2013) are induced using a
tag dictionary. For this we used two tag dictionar-
ies. One is JUMAN dictionary 7.0 and the other
is a tag dictionary constructed by listing surface
forms and surface POS tags in the normalization
dictionary.
To compute the language model features, one
billion sentences from Twitter posts were analyzed
using MeCab 0.996.10 We used all bigrams ap-
pearing at least 10 times in the auto-analyzed sen-
tences.
8.2 Results of word segmentation and POS
tagging
We first investigated the performance of models
trained on an existing annotated corpus form news
texts. For this experiment, our joint model as
well as three state-of-the-art models (Kudo et al.,
2004)11(Neubig et al., 2011)12(Kaji and Kitsure-
gawa, 2013) were trained on Kyoto University
Text corpus 4.0 (Kurohashi and Nagao, 1998).
Since this training corpus is not annotated with
normal forms and normal POS tags, our model
was trained using the latent perceptron. Table
5 summarizes the word-level F
1
-scores (Kudo et
al., 2004) on our microblog corpus. The two
columns represent the results for word segmenta-
tion (Seg) and joint word segmentation and POS
tagging (Seg+Tag), respectively.
We also conducted 5-fold crossvalidation on
our microblog corpus to evaluate performance im-
provement when these models are trained on mi-
croblog texts (Table 6). In addition to the models
in Table 5, results of a rule-based system (Sasano
et al., 2013)13 and our joint model trained using
the perceptron algorithm are also presented. No-
tice that Proposed and Proposed (latent) repre-
sent our model trained using perceptron and latent
perceptron, respectively.
From Tables 5 and 6, as expected, we see that
the models trained on news texts performed poorly
on microblog texts, while their performance sig-
nificantly boosted when trained on the microblog
texts. This demonstrates the importance of corpus
annotation. An exception was Kudo04. Its perfor-
10https://code.google.com/p/mecab
11https://code.google.com/p/mecab
12http://www.phontron.com/kytea/
13http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN
Table 5: Performance of models trained on the
news articles.
Seg Seg+Tag
Kudo04 81.8 71.0
Neubig11 80.5 69.1
Kaji13 83.2 73.1
Proposed (latent) 83.0 73.9
mance improved only slightly, even when it was
trained on the microblog texts. We believe this is
because their model uses dictionary-based rules to
prune candidate analyses; thus, it could not per-
form well in the microblog domain, where out-of-
vocabulary words are abundant.
Table 6 also illustrates that our joint models
achieved F
1
-score better than the state-of-the-art
models trained on the microblog texts. This
shows that modeling the derivation process of ill-
spelled words makes training easier. We con-
ducted bootstrap resampling (with 1000 samples)
to investigate the significance of the improvements
achieved with our joint model. The results showed
that all improvements over the baselines were sta-
tistically significant (p < 0.01). The difference
between Proposed and Proposed (latent) were
also statistically significant (p < 0.01).
The results of Proposed (latent) are interest-
ing. Table 5 illustrates that our joint model per-
forms well even when it is trained on a news cor-
pus that rarely contains ill-spelled words and is
not at all annotated with normal forms and nor-
mal POS tags. This indicates the robustness of our
training method and the importance of modeling
word derivation process in the microblog domain.
In Table 6, we observed that Proposed (latent),
which uses less supervision, performed better than
Proposed. The reason for this will be examined
later.
In summary, we can conclude that both the mi-
croblog corpus and joint model significantly con-
tribute to training accurate models for word seg-
mentation and POS tagging in the microblog do-
main.
8.3 Results of lexical normalization
While the main goal with this study was to en-
hance word segmentation and POS tagging in the
microblog domain, it is interesting to explore how
well our joint model can normalize ill-spelled
words.
Table 7 illustrates precision, recall, and F
1
-
score for the lexical normalization task. To put
106
Table 6: Results of 5-fold cross-validation on mi-
croblog corpus.
Seg Seg+Tag
Kudo04 82.7 71.7
Neubig11 88.6 75.9
Kaji13 90.9 82.1
Sasano13 82.7 73.3
Proposed 91.3 83.2
Proposed (latent) 91.4 83.7
Table 7: Results of lexical normalization task in
terms of precision, recall, and F
1
-score.
Precision Recall F
1
Neubig11 69.2 35.9 47.3
Proposed 77.1 44.6 56.6
Proposed (latent) 53.7 24.7 33.9
the results into context, we report on the baseline
results of a tagging model proposed by Neubig et
al. (2011). This baseline conducts lexical normal-
ization by regarding it as two independent tagging
tasks (i.e., tasks of tagging normal forms and nor-
mal POS tags). The result of the baseline model is
also obtained using 5-fold crossvalidation.
Table 7 illustrates that Proposed performed sig-
nificantly better than the simple tagging model,
Neubig11. This suggests the effectiveness of our
joint model. On the other hand, Proposed (latent)
performed poorly in this task. From this result, we
can argue that Proposed (latent) can achieve su-
perior performance in word segmentation and POS
tagging (Table 6) because it gave up correctly nor-
malizing ill-spelled words, focusing on word seg-
mentation and POS tagging.
The experimental results so far suggest the fol-
lowing strategy for training our joint model. If ac-
curacy of word segmentation and POS tagging is
the main concern, we can use the latent percep-
tron. This approach has the advantage of being
able to use a partially annotated corpus. On the
other hand, if performance of lexical normaliza-
tion is crucial, we have to use the standard percep-
tron algorithm.
8.4 Error analysis
We manually analyzed erroneous outputs and ob-
served several tendencies.
We found that a word lattice sometimes missed
the correct output. Such an error was, for example,
observed in a sentence including many ill-spelled
words, e.g., ?????????????? (be
nervous about what other people think!)?, where
the part ???????? is in ill-spelled words.
Improving the lattice generation algorithm is con-
sidered necessary to achieve further performance
gain.
Even if the correct analysis appears in the word
lattice, our model sometimes failed to handle
ill-spelled words, incorrectly analyzing them as
out-of-vocabulary words. For example, the pro-
posed method treated the phrase ????????
(snack time)? as a single out-of-vocabulary word,
even though the correct analysis was found in the
word lattice. More sophisticated features would
be required to accurately distinguish between ill-
spelled and out-of-vocabulary words.
9 Conclusion and Future Work
We presented our attempts towards developing an
accurate model for word segmentation and POS
tagging in the microblog domain. To this end, we,
for the first time, developed an annotated corpus
of microblogs. We also proposed a joint model
with lexical normalization to handle orthographic
diversity in the microblog text. Intensive exper-
iments demonstrated that we could successfully
improve the performance of word segmentation
and POS tagging on microblog texts. We believe
this study will have a large practical impact on a
various research areas that target microblogs.
One limitation of our approach is that it cannot
handle certain types of ill-spelled words. For ex-
ample, the current model cannot handle the cases
in which there are no one-to-one-mappings be-
tween well-spelled and ill-spelled words. Also,
our model cannot handle spelling errors, which
are considered relatively frequent in the microblog
than news domains. The treatment of these prob-
lems would require further research.
Another future research is to speed-up our
model. Since the joint model with lexical normal-
ization significantly increases the search space,
it is much slower than the original lattice-based
model for word segmentation and POS tagging.
Acknowledgments
The authors would like to thank Naoki Yoshinaga
for his help in developing the microblog corpus as
well as fruitful discussions.
107
References
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! using
word lengthening to detect sentiment in microblogs.
In Proceedings of EMNLP, pages 562?570.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1?8.
Huiming Duan, Zhifang Sui, Ye Tian, and Wenjie Li.
2012. The CIPS-SIGHAN CLP 2012 Chinese word
segmentation on microblog corpora bakeoff. In Pro-
ceedings of the Second CIPS-SIGHAN Joint Conr-
erence on Chinese Language Processing, pages 35?
40.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011.
#hardtoparse: POS tagging and parsing the twit-
terverse. In Proceedings of AAAI Workshop on
Analysing Microtext, pages 20?25.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of ACL, pages 42?47.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
ization of short text messages: Makin sens a #twitter.
In Proceedings of ACL, pages 368?378.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary
for microblogs. In Proceedings of EMNLP-CoNLL,
pages 421?432.
Chikara Hashimoto, Sadao Kurohashi, Daisuke Kawa-
hara, Keiji Shinzato, and Masaaki Nagata. 2011.
Construction of a blog corpus with syntac-
tic, anaphoric, and semantic annotations (in
Japanese). Journal of Natural Language Process-
ing, 18(2):175?201.
Yijiu How and Min-Yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mo-
bile phones. In Proceedings of Human Computer
Interfaces International.
Kazushi Ikeda, Tadashi Yanagihara, Kazunori Mat-
sumoto, and Yasuhiro Takishima. 2009. Unsuper-
vised text normalization approach for morphological
analysis of blog documents. In Proceedings of Aus-
tralasian Joint Conference on Advances in Artificial
Intelligence, pages 401?411.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008. Word
lattice reranking for Chinese word segmentation and
part-of-speech tagging. In Proceedings of Coling,
pages 385?392.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent Twitter sen-
timent classification. In Proceedings of ACL, pages
151?160.
Nobuhiro Kaji and Masaru Kitsuregawa. 2011. Split-
ting noun compounds via monolingual and bilingual
paraphrasing: A study on Japanese Katakana words.
In Proceedings of EMNLP, pages 959?969.
Nobuhiro Kaji and Masaru Kitsuregawa. 2013. Effi-
cient word lattice generation for joint word segmen-
tation and POS tagging in Japanese. In Proceedings
of IJCNLP, pages 153?161.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis. In Proceedings of
EMNLP, pages 230?237.
Sadao Kurohashi and Makoto Nagao. 1998. Building a
Japanese parsed corpus while improving the parsing
system. In Proceedings of LREC, pages 719?724.
Wang Ling, Chris Dyer, Alan W Black, and Isabel
Trancoso. 2013. Paraphrasing 4 microblog normal-
ization. In Proceedings of EMNLP, pages 73?84.
Xiaohua Liu, Ming Zhou, Xiangyang Zhou,
Zhongyang Fu, and Furu Wei. 2012. Joint
inference of named entity recognition and normal-
ization for tweets. In Proceedings of ACL, pages
526?535.
Graham Neubig, Yousuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust adaptable
Japanese morphological analysis. In Proceedings of
ACL, pages 529?533.
Itsumi Saito, Kugatsu Sadamitsu, Hisako Asano, and
Yoshihiro Matsuo. 2014. Morphological analysis
for Japanese noisy text based on character-level and
word-level normalization. In Proceedings of COL-
ING, pages 1773?1782.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquak shakes Twitter users: real-time
event detection by social sensors. In Proceedings
of WWW, pages 851?860.
Ryohei Sasano, Sadao Kurohashi, and Manabu Oku-
mura. 2013. A simple approach to unknown word
processing in Japanese morphological analysis. In
Proceedings of IJCNLP, pages 162?170.
Xu Sun, Takuya Matsuzaki, and Wenjie Li. 2013.
Latent structured perceptrons for large-scale learn-
ing with hidden information. IEEE Transactions
on Knowledge and Data Engineering, 25(9):2063?
2075.
Aobo Wang and Min-Yen Kan. 2013. Mining informal
language from Chinese microtext: Joint word recog-
nition and segmentation. In Proceedings of ACL,
pages 731?741.
108
Pidong Wang and Hwee Tou Ng. 2013. A beam-search
decoder for normalization of social media text with
application to machine translation. In Proceedings
of NAACL, pages 471?481.
Aobo Wang, Min-Yen Kan, Daniel Andrade, Takashi
Onishi, and Kai Ishikawa. 2013. Chinese informal
word normalization: an experimental study. In Pro-
ceedings of IJCNLP, pages 127?135.
Yunqing Xia, Kam-Fai Wong, and Wenjie Li. 2006. A
phonetic-based approach to Chinese chat text nor-
malization. In Proceedings of ACL, pages 993?
1000.
Yi Yang and Jacob Eisenstein. 2013. A log-linear
model for unsupervised text normalization. In Pro-
ceedings of EMNLP, pages 61?72.
Congle Zhang, Tyler Baldwin, Howard Ho, Benny
Kimelfeld, and Yunyao Li. 2013. Adaptive parser-
centric text normalization. In Proceedings of ACL,
pages 1159?1168.
109
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 485?494,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Efficient Staggered Decoding for Sequence Labeling
Nobuhiro Kaji Yasuhiro Fujiwara Naoki Yoshinaga Masaru Kitsuregawa
Institute of Industrial Science,
The University of Tokyo,
4-6-1, Komaba, Meguro-ku, Tokyo, 153-8505 Japan
{kaji,fujiwara,ynaga,kisture}@tkl.iis.u-tokyo.ac.jp
Abstract
The Viterbi algorithm is the conventional
decoding algorithm most widely adopted
for sequence labeling. Viterbi decoding
is, however, prohibitively slow when the
label set is large, because its time com-
plexity is quadratic in the number of la-
bels. This paper proposes an exact decod-
ing algorithm that overcomes this prob-
lem. A novel property of our algorithm is
that it efficiently reduces the labels to be
decoded, while still allowing us to check
the optimality of the solution. Experi-
ments on three tasks (POS tagging, joint
POS tagging and chunking, and supertag-
ging) show that the new algorithm is sev-
eral orders of magnitude faster than the
basic Viterbi and a state-of-the-art algo-
rithm, CARPEDIEM (Esposito and Radi-
cioni, 2009).
1 Introduction
In the past decade, sequence labeling algorithms
such as HMMs, CRFs, and Collins? perceptrons
have been extensively studied in the field of NLP
(Rabiner, 1989; Lafferty et al, 2001; Collins,
2002). Now they are indispensable in a wide range
of NLP tasks including chunking, POS tagging,
NER and so on (Sha and Pereira, 2003; Tsuruoka
and Tsujii, 2005; Lin and Wu, 2009).
One important task in sequence labeling is how
to find the most probable label sequence from
among all possible ones. This task, referred to as
decoding, is usually carried out using the Viterbi
algorithm (Viterbi, 1967). The Viterbi algorithm
has O(NL2) time complexity,1 where N is the
input size and L is the number of labels. Al-
though the Viterbi algorithm is generally efficient,
1The first-order Markov assumption is made throughout
this paper, although our algorithm is applicable to higher-
order Markov models as well.
it becomes prohibitively slow when dealing with
a large number of labels, since its computational
cost is quadratic in L (Dietterich et al, 2008).
Unfortunately, several sequence-labeling prob-
lems in NLP involve a large number of labels. For
example, there are more than 40 and 2000 labels
in POS tagging and supertagging, respectively
(Brants, 2000; Matsuzaki et al, 2007). These
tasks incur much higher computational costs than
simpler tasks like NP chunking. What is worse,
the number of labels grows drastically if we jointly
perform multiple tasks. As we shall see later,
we need over 300 labels to reduce joint POS tag-
ging and chunking into the single sequence label-
ing problem. Although joint learning has attracted
much attention in recent years, how to perform de-
coding efficiently still remains an open problem.
In this paper, we present a new decoding algo-
rithm that overcomes this problem. The proposed
algorithm has three distinguishing properties: (1)
It is much more efficient than the Viterbi algorithm
when dealing with a large number of labels. (2) It
is an exact algorithm, that is, the optimality of the
solution is always guaranteed unlike approximate
algorithms. (3) It is automatic, requiring no task-
dependent hyperparameters that have to be manu-
ally adjusted.
Experiments evaluate our algorithm on three
tasks: POS tagging, joint POS tagging and chunk-
ing, and supertagging2. The results demonstrate
that our algorithm is up to several orders of mag-
nitude faster than the basic Viterbi algorithm and a
state-of-the-art algorithm (Esposito and Radicioni,
2009); it makes exact decoding practical even in
labeling problems with a large label set.
2 Preliminaries
We first provide a brief overview of sequence la-
beling and introduce related work.
2Our implementation is available at http://www.tkl.iis.u-
tokyo.ac.jp/?kaji/staggered
485
2.1 Models
Sequence labeling is the problem of predicting la-
bel sequence y = {yn}Nn=1 for given token se-
quence x = {xn}Nn=1. This is typically done by
defining a score function f(x,y) and locating the
best label sequence: ymax = argmax
y
f(x,y).
The form of f(x,y) is dependent on the learn-
ing model used. Here, we introduce two models
widely used in the literature.
Generative models HMM is the most famous
generative model for labeling token sequences
(Rabiner, 1989). In HMMs, the score function
f(x,y) is the joint probability distribution over
(x,y). If we assume a one-to-one correspondence
between the hidden states and the labels, the score
function can be written as:
f(x,y) = log p(x,y)
= log p(x|y) + log p(y)
=
N
?
n=1
log p(xn|yn)+
N
?
n=1
log p(yn|yn?1).
The parameters log p(xn|yn) and log p(yn|yn?1)
are usually estimated using maximum likelihood
or the EM algorithm. Since parameter estimation
lies outside the scope of this paper, a detailed de-
scription is omitted.
Discriminative models Recent years have seen
the emergence of discriminative training methods
for sequence labeling (Lafferty et al, 2001; Tasker
et al, 2003; Collins, 2002; Tsochantaridis et al,
2005). Among them, we focus on the perceptron
algorithm (Collins, 2002). Although we do not
discuss the other discriminative models, our algo-
rithm is equivalently applicable to them. The ma-
jor difference between those models lies in param-
eter estimation; the decoding process is virtually
the same.
In the perceptron, the score function f(x,y) is
given as f(x,y) = w ? ?(x,y) where w is the
weight vector, and ?(x,y) is the feature vector
representation of the pair (x,y). By making the
first-order Markov assumption, we have
f(x,y) = w ? ?(x,y)
=
N
?
n=1
K
?
k=1
wk?k(x, yn?1, yn),
where K = |?(x,y)| is the number of features, ?k
is the k-th feature function, and wk is the weight
corresponding to it. Parameter w can be estimated
in the same way as in the conventional perceptron
algorithm. See (Collins, 2002) for details.
2.2 Viterbi decoding
Given the score function f(x,y), we have to lo-
cate the best label sequence. This is usually per-
formed by applying the Viterbi algorithm. Let
?(yn) be the best score of the partial label se-
quence ending with yn. The idea of the Viterbi
algorithm is to use dynamic programming to com-
pute ?(yn). In HMMs, ?(yn) can be can be de-
fined as
max
y
n?1
{?(yn?1) + log p(yn|yn?1)} + log p(xn|yn).
Using this recursive definition, we can evaluate
?(yn) for all yn. This results in the identification
of the best label sequence.
Although the Viterbi algorithm is commonly
adopted in past studies, it is not always efficient.
The computational cost of the Viterbi algorithm is
O(NL2), where N is the input length and L is
the number of labels; it is efficient enough if L
is small. However, if there are many labels, the
Viterbi algorithm becomes prohibitively slow be-
cause of its quadratic dependence on L.
2.3 Related work
To the best of our knowledge, the Viterbi algo-
rithm is the only algorithm widely adopted in the
NLP field that offers exact decoding. In other
communities, several exact algorithms have al-
ready been proposed for handling large label sets.
While they are successful to some extent, they de-
mand strong assumptions that are unusual in NLP.
Moreover, none were challenged with standard
NLP tasks.
Felzenszwalb et al (2003) presented a fast
inference algorithm for HMMs based on the as-
sumption that the hidden states can be embed-
ded in a grid space, and the transition probabil-
ity corresponds to the distance on that space. This
type of probability distribution is not common in
NLP tasks. Lifshits et al (2007) proposed a
compression-based approach to speed up HMM
decoding. It assumes that the input sequence is
highly repetitive. Amongst others, CARPEDIEM
(Esposito and Radicioni, 2009) is the algorithm
closest to our work. It accelerates decoding by
assuming that the adjacent labels are not strongly
correlated. This assumption is appropriate for
486
some NLP tasks. For example, as suggested in
(Liang et al, 2008), adjacent labels do not provide
strong information in POS tagging. However, the
applicability of this idea to other NLP tasks is still
unclear.
Approximate algorithms, such as beam search
or island-driven search, have been proposed for
speeding up decoding. Tsuruoka and Tsujii (2005)
proposed easiest-first deterministic decoding. Sid-
diqi and Moore (2005) presented the parameter ty-
ing approach for fast inference in HMMs. A simi-
lar idea was applied to CRFs as well (Cohn, 2006;
Jeong et al, 2009).
In general, approximate algorithms have the ad-
vantage of speed over exact algorithms. However,
both types of algorithms are still widely adopted
by practitioners, since exact algorithms have mer-
its other than speed. First, the optimality of the so-
lution is always guaranteed. It is hard for most of
the approximate algorithms to even bound the er-
ror rate. Second, approximate algorithms usually
require hyperparameters, which control the trade-
off between accuracy and efficiency (e.g., beam
width), and these have to be manually adjusted.
On the other hand, most of the exact algorithms,
including ours, do not require such a manual ef-
fort.
Despite these advantages, exact algorithms are
rarely used when dealing with a large number of
labels. This is because exact algorithms become
considerably slower than approximate algorithms
in such situations. The paper presents an exact al-
gorithm that avoids this problem; it provides the
research community with another option for han-
dling a lot of labels.
3 Algorithm
This section presents the new decoding algorithm.
The key is to reduce the number of labels ex-
amined. Our algorithm locates the best label se-
quence by iteratively solving labeling problems
with a reduced label set. This results in signifi-
cant time savings in practice, because each itera-
tion becomes much more efficient than solving the
original labeling problem. More importantly, our
algorithm always obtains the exact solution. This
is because the algorithm allows us to check the op-
timality of the solution achieved by using only the
reduced label set.
In the following discussions, we restrict our fo-
cus to HMMs for presentation clarity. Extension to
A A A A
B B B B
C
D
C
D
C
D
C
D
E E E E
F F F F
G G G G
H H H H
(a)
A
B
C
D
A A
B
A
B
C
D
(b)
Figure 1: (a) An example of a lattice, where the
letters {A, B, C, D, E, F, G, H} represent labels
associated with nodes. (b) The degenerate lattice.
the perceptron algorithm is presented in Section 4.
3.1 Degenerate lattice
We begin by introducing the degenerate lattice,
which plays a central role in our algorithm. Con-
sider the lattice in Figure 1(a). Following conven-
tion, we regard each path on the lattice as a label
sequence. Note that the label set is {A, B, C, D,
E, F, G, H}. By aggregating several nodes in the
same column of the lattice, we can transform the
original lattice into a simpler form, which we call
the degenerate lattice (Figure 1(b)).
Let us examine the intuition behind the degen-
erate lattice. Aggregating nodes can be viewed as
grouping several labels into a new one. Here, a
label is referred to as an active label if it is not ag-
gregated (e.g., A, B, C, and D in the first column
of Figure 1(b)), and otherwise as an inactive label
(i.e., dotted nodes). The new label, which is made
by grouping the inactive labels, is referred to as
a degenerate label (i.e., large nodes covering the
dotted ones). Two degenerate labels can be seen
as equivalent if their corresponding inactive label
sets are the same (e.g., degenerate labels in the first
and the last column). In this approach, each path
of the degenerate lattice can also be interpreted as
a label sequence. In this case, however, the label to
be assigned is either an active label or a degenerate
label.
We then define the parameters associated with
degenerate label z. For reasons that will become
clear later, they are set to the maxima among the
parameters of the inactive labels:
log p(x|z) = max
y??I(z)
log p(x|y?), (1)
log p(z|y) = max
y??I(z)
log p(y?|y), (2)
log p(y|z) = max
y??I(z)
log p(y|y?), (3)
log p(z|z?) = max
y??I(z),y???I(z?)
log p(y?|y??), (4)
487
A A A A
B B B B
C
D
C
D
C
D
C
D
E E E E
F F F F
G G G G
H H H H
(a)
A
B
C
D
A A
B
A
B
C
D
(b)
Figure 2: (a) The path y = {A, E, G, C} of the
original lattice. (b) The path z of the degenerate
lattice that corresponds to y.
where y is an active label, z and z? are degenerate
labels, and I(z) denotes one-to-one mapping from
z to its corresponding inactive label set.
The degenerate lattice has an important prop-
erty which is the key to our algorithm:
Lemma 1. If the best path of the degenerate lat-
tice does not include any degenerate label, it is
equivalent to the best path of the original lattice.
Proof. Let zmax be the best path of the degenerate
lattice. Our goal is to prove that if zmax does not
include any degenerate label, then
?y ? Y, log p(x,y) ? log p(x,zmax) (5)
where Y is the set of all paths on the original lat-
tice. We prove this by partitioning Y into two dis-
joint sets: Y
0
and Y
1
, where Y
0
is the subset of
Y appearing in the degenerate lattice. Notice that
zmax ? Y0. Since zmax is the best path of the
degenerate lattice, we have
?y ? Y
0
, log p(x,y) ? log p(x,zmax).
The equation holds when y = zmax. We next ex-
amine the label sequence y such that y ? Y
1
. For
each path y ? Y
1
, there exists a unique path z on
the degenerate lattice that corresponds to y (Fig-
ure 2). Therefore, we have
?y ? Y
1
, ?z ? Z, log p(x,y) ? log p(x,z)
< log p(x,zmax)
where Z is the set of all paths of the degenerate
lattice. The inequality log p(x,y) ? log p(x,z)
can be proved by using Equations (1)-(4). Using
these results, we can complete (5).
A A A A
(a)
A A
B
A
B
A
BB
(b)
A A
B
C
D
A
B
A
B
C
D
B
C
D
C
D
(c)
Figure 3: (a) The best path of the initial degenerate
lattice, which is denoted by the line, is located. (b)
The active labels are expanded and the best path is
searched again. (c) The best path without degen-
erate labels is obtained.
3.2 Staggered decoding
Nowwe can describe our algorithm, which we call
staggered decoding. The algorithm successively
constructs degenerate lattices and checks whether
the best path includes degenerate labels. In build-
ing each degenerate lattice, labels with high prob-
ability p(y), estimated from training data, are pref-
erentially selected as the active label; the expecta-
tion is that such labels are likely to belong to the
best path. The algorithm is detailed as follows:
Initialization step The algorithm starts by build-
ing a degenerate lattice in which there is only
one active label in each column. We select la-
bel y with the highest p(y) as the active label.
Search step The best path of the degenerate lat-
tice is located (Figure 3(a)). This is done
by using the Viterbi algorithm (and pruning
technique, as we describe in Section 3.3). If
the best path does not include any degenerate
label, we can terminate the algorithm since it
is identical with the best path of the original
lattice according to Lemma 1. Otherwise, we
proceed to the next step.
Expansion step We double the number of the ac-
tive labels in the degenerate lattice. The new
active labels are selected from the current in-
active label set in descending order of p(y).
If the inactive label set becomes empty, we
simply reconstructed the original lattice. Af-
ter expanding the active labels, we go back to
the previous step (Figure 3(b)). This proce-
dure is repeated until the termination condi-
tion in the search step is satisfied, i.e., the best
path has no degenerate label (Figure 3(c)).
Compared to the Viterbi algorithm, staggered
decoding requires two additional computations for
488
training. First, we have to estimate p(y) so as to
select active labels in the initialization and expan-
sion step. Second, we have to compute the pa-
rameters regarding degenerate labels according to
Equations (1)-(4). Both impose trivial computa-
tion costs.
3.3 Pruning
To achieve speed-up, it is crucial that staggered
decoding efficiently performs the search step. For
this purpose, we can basically use the Viterbi algo-
rithm. In earlier iterations, the Viterbi algorithm is
indeed efficient because the label set to be han-
dled is much smaller than the original one. In later
iterations, however, our algorithm drastically in-
creases the number of labels, making Viterbi de-
coding quite expensive.
To handle this problem, we propose a method of
pruning the lattice nodes. This technique is moti-
vated by the observation that the degenerate lattice
shares many active labels with the previous itera-
tion. In the remainder of Section3.3, we explain
the technique by taking the following steps:
? Section 3.3.1 examines a lower bound l such
that l ? max
y
log p(x,y).
? Section 3.3.2 examines the maximum score
MAX(yn) in case token xn takes label yn:
MAX(yn) = max
y?
n
=y
n
log p(x,y?).
? Section 3.3.3 presents our pruning procedure.
The idea is that if MAX(yn) < l, then the
node corresponding to yn can be removed
from consideration.
3.3.1 Lower bound
Lower bound l can be trivially calculated in the
search step. This can be done by retaining the
best path among those consisting of only active
labels. The score of that path is obviously the
lower bound. Since the search step is repeated un-
til the termination criteria is met, we can update
the lower bound at every search step. As the it-
eration proceeds, the degenerate lattice becomes
closer to the original one, so the lower bound be-
comes tighter.
3.3.2 Maximum score
The maximum score MAX(yn) can be computed
from the original lattice. Let ?(yn) be the best
score of the partial label sequence ending with yn.
Presuming that we traverse the lattice from left to
right, ?(yn) can be defined as
max
y
n?1
{?(yn?1) + log p(yn|yn?1)} + log p(xn|yn).
If we traverse the lattice from right to left, an anal-
ogous score ??(yn) can be defined as
log p(xn|yn) + max
y
n+1
{??(yn+1) + log p(yn|yn+1)}.
Using these two scores, we have
MAX(yn) = ?(yn) + ??(yn) ? log p(xn|yn).
Notice that updating ?(yn) or ??(yn) is equivalent
to the forward or backward Viterbi algorithm, re-
spectively.
Although it is expensive to compute ?(yn) and
??(yn), we can efficiently estimate their upper
bounds. Let ?(yn) and ??(yn) be scores analogous
to ?(yn) and ??(yn) that are computed using the
degenerate lattice. We have ?(yn) ? ?(yn) and
??(yn) ? ??(yn), by following similar discussions
as raised in the proof of Lemma 1. Therefore, we
can still check whether MAX(yn) is smaller than l
by using ?(yn) and ??(yn):
MAX(yn) = ?(yn) + ??(yn)? log p(xn|yn)
? ?(yn) + ??(yn) ? log p(xn|yn)
< l.
For the sake of simplicity, we assume that yn is an
active label. Although we do not discuss the other
cases, our pruning technique is also applicable to
them. We just point out that, if yn is an inactive
label, then there exists a degenerate label zn in the
n-th column such that yn ? I(zn), and we can use
?(zn) and ??(zn) instead of ?(yn) and ??(yn).
We compute ?(yn) and ??(yn) by using the
forward and backward Viterbi algorithm, respec-
tively. In the search step immediately following
initialization, we perform the forward Viterbi al-
gorithm to find the best path, that is, ?(yn) is
updated for all yn. In the next search step, the
backward Viterbi algorithm is carried out, and
??(yn) is updated. In the succeeding search steps,
these updates are alternated. As the algorithm pro-
gresses, ?(yn) and ??(yn) become closer to ?(yn)
and ??(yn).
3.3.3 Pruning procedure
We make use of the bounds in pruning the lattice
nodes. To do this, we keep the values of l, ?(yn)
489
and ??(yn). They are set as l = ?? and ?(yn) =
??(yn) = ? in the initialization step, and are up-
dated in the search step. The lower bound l is up-
dated at the end of the search step, while ?(yn)
and ??(yn) can be updated during the running of
the Viterbi algorithm. When ?(yn) or ??(yn) is
changed, we check whether MAX(yn) < l holds
and the node is pruned if the condition is met.
3.4 Analysis
We provide here a theoretical analysis of staggered
decoding. In the following proofs, L, V , and N
represent the number of original labels, the num-
ber of distinct tokens, and the length of input token
sequence, respectively. To simplify the discussion,
we assume that log
2
L is an integer (e.g., L = 64).
We first introduce three lemmas:
Lemma 2. Staggered decoding requires at most
(log
2
L + 1) iterations to terminate.
Proof. We have 2m?1 active labels in the m-th
search step (m = 1, 2 . . . ), which means we have
L active labels and no degenerate labels in the
(log
2
L + 1)-th search step. Therefore, the algo-
rithm always terminates within (log
2
L + 1) itera-
tions.
Lemma 3. The number of degenerate labels is
log
2
L.
Proof. Since we create one new degenerate label
in all but the last expansion step, we have log
2
L
degenerate labels.
Lemma 4. The Viterbi algorithm requires O(L2+
LV ) memory space and has O(NL2) time com-
plexity.
Proof. Since we need O(L2) and O(LV ) space to
keep the transition and emission probability ma-
trices, we need O(L2 + LV ) space to perform
the Viterbi algorithm. The time complexity of the
Viterbi algorithm is O(NL2) since there are NL
nodes in the lattice and it takes O(L) time to eval-
uate the score of each node.
The above statements allow us to establish our
main results:
Theorem 1. Staggered decoding requires O(L2+
LV ) memory space.
Proof. Since we have L original labels and log
2
L
degenerate labels, staggered decoding requires
O((L+log
2
L)2+(L+log
2
L)V ) = O(L2+LV )
A A A A
(a)
A A
B
A
B
A
B
(b)
A A
B
C
D
A
B
A
B
C
D
(c)
Figure 4: Staggered decoding with column-wise
expansion: (a) The best path of the initial degen-
erate lattice, which does not pass through the de-
generate label in the first column. (b) Column-
wise expansion is performed and the best path is
searched again. Notice that the active label in the
first column is not expanded. (c) The final result.
memory space to perform Viterbi decoding in the
search step.
Theorem 2. Staggered decoding has O(N) best
case time complexity and O(NL2)worst case time
complexity.
Proof. To perform the m-th search step, staggered
decoding requires the order of O(N4m?1) time
because we have 2m?1 active labels. Therefore, it
has O(
?M
m=1 N4
m?1) time complexity if it termi-
nates after the M -th search step. In the best case,
M = 1, the time complexity is O(N). In the worst
case, M = log
2
L + 1, the time complexity is the
order of O(NL2) because
?
log
2
L+1
m=1 N4
m?1 <
4
3
NL2.
Theorem 1 shows that staggered decoding
asymptotically requires the same order of mem-
ory space as the Viterbi algorithm. Theorem 2 re-
veals that staggered decoding has the same order
of time complexity as the Viterbi algorithm even
in the worst case.
3.5 Heuristic techniques
We present two heuristic techniques for further
speeding up our algorithm.
First, we can initialize the value of lower bound
l by selecting a path from the original lattice in
some way, and then computing the score of that
path. In our experiments, we use the path lo-
cated by the left-to-right deterministic decoding
(i.e., beam search with a beam width of 1). Al-
though this method requires an additional cost to
locate the path, it is very effective in practice. If
l is initialized in this manner, the best case time
complexity of our algorithm becomes O(NL).
490
The second technique is for the expansion step.
Instead of the expansion technique described in
Section 3.2, we can expand the active labels in a
heuristic manner to keep the number of active la-
bels small:
Column-wise expansion step We double the
number of the active labels in the column
only if the best path of the degenerate lattice
passes through the degenerate label of that
column (Figure 4).
A drawback of this strategy is that the algorithm
requires N(log
2
L+1) iterations in the worst case.
As the result, we can no longer derive a reasonable
upper bound for the time complexity. Neverthe-
less, column-wise expansion is highly effective in
practice as we will demonstrate in the experiment.
Note that Theorem 1 still holds true even if we use
column-wise expansion.
4 Extension to the Perceptron
The discussion we have made so far can be applied
to perceptrons. This can be clarified by comparing
the score functions f(x,y). In HMMs, the score
function can be written as
N
?
n=1
{
log(xn|yn) + log(yn|yn?1)
}
.
In perceptrons, on the other hand, it is given as
N
?
n=1
{
?
k
w1k?
1
k(x, yn) +
?
k
w2k?
2
k(x, yn?1, yn)
}
where we explicitly distinguish the unigram fea-
ture function ?1k and bigram feature function ?2k.
Comparing the form of the two functions, we can
see that our discussion on HMMs can be extended
to perceptrons by substituting
?
k w
1
k?
1
k(x, yn)
and
?
k w
2
k?
2
k(x, yn?1, yn) for log p(xn|yn) and
log p(yn|yn?1).
However, implementing the perceptron algo-
rithm is not straightforward. The problem is
that it is difficult, if not impossible, to compute
?
k w
1
k?
1
k(x, y) and
?
k w
2
k?
2
k(x, y, y
?) offline be-
cause they are dependent on the entire token se-
quence x, unlike log p(x|y) and log p(y|y?). Con-
sequently, we cannot evaluate the maxima analo-
gous to Equations (1)-(4) offline either.
For unigram features, we compute the maxi-
mum, maxy
?
k w
1
k?
1
k(x, y), as a preprocess in
the initialization step (cf. Equation (1)). This pre-
process requires O(NL) time, which is negligible
compared with the cost required by the Viterbi al-
gorithm.
Unfortunately, we cannot use the same tech-
nique for computing maxy,y?
?
k w
2
k?
2
k(x, y, y
?)
because a similar computation would take
O(NL2) time (cf. Equation (4)). For bigram fea-
tures, we compute its upper bound offline. For ex-
ample, the following bound was proposed by Es-
posito and Radicioni (2009):
max
y,y?
?
k
w2k?
2
k(x, y, y
?) ? max
y,y?
?
k
w2k?(0 < w
2
k)
where ?(?) is the delta function and the summa-
tions are taken over all feature functions associated
with both y and y?. Intuitively, the upper bound
corresponds to an ideal case in which all features
with positive weight are activated.3 It can be com-
puted without any task-specific knowledge.
In practice, however, we can compute better
bounds based on task-specific knowledge. The
simplest case is that the bigram features are inde-
pendent of the token sequence x. In such a situ-
ation, we can trivially compute the exact maxima
offline, as we did in the case of HMMs. Fortu-
nately, such a feature set is quite common in NLP
problems and we could use this technique in our
experiments. Even if bigram features are depen-
dent on x, it is still possible to compute better
bounds if several features are mutually exclusive,
as discussed in (Esposito and Radicioni, 2009).
Finally, it is worth noting that we can use stag-
gered decoding in training perceptrons as well, al-
though such application lies outside the scope of
this paper. The algorithm does not support train-
ing acceleration for other discriminative models.
5 Experiments and Discussion
5.1 Setting
The proposed algorithm was evaluated with three
tasks: POS tagging, joint POS tagging and chunk-
ing (called joint tagging for short), and supertag-
ging. To reduce joint tagging into a single se-
quence labeling problem, we produced the labels
by concatenating the POS tag and the chunk tag
(BIO format), e.g., NN/B-NP. In the two tasks
other than supertagging, the input token is the
word. In supertagging, the token is the pair of the
word and its oracle POS tag.
3We assume binary feature functions.
491
Table 1: Decoding speed (sent./sec).
POS tagging Joint tagging Supertagging
VITERBI 4000 77 1.1
CARPEDIEM 8600 51 0.26
SD 8800 850 121
SD+C-EXP. 14,000 1600 300
The data sets we used for the three experiments
are the Penn TreeBank (PTB) corpus, CoNLL
2000 corpus, and an HPSG treebank built from the
PTB corpus (Matsuzaki et al, 2007). We used sec-
tions 02-21 of PTB for training, and section 23 for
testing. The number of labels in the three tasks is
45, 319 and 2602, respectively.
We used the perceptron algorithm for train-
ing. The models were averaged over 10 itera-
tions (Collins, 2002). For features, we basically
followed previous studies (Tsuruoka and Tsujii,
2005; Sha and Pereira, 2003; Ninomiya et al,
2006). In POS tagging, we used unigrams of the
current and its neighboring words, word bigrams,
prefixes and suffixes of the current word, capital-
ization, and tag bigrams. In joint tagging, we also
used the same features. In supertagging, we used
POS unigrams and bigrams in addition to the same
features other than capitalization.
As the evaluation measure, we used the average
decoding speed (sentences/sec) to two significant
digits over five trials. To strictly measure the time
spent for decoding, we ignored the preprocessing
time, that is, the time for loading the model file
and converting the features (i.e., strings) into inte-
gers. We note that the accuracy was comparable to
the state-of-the-art in the three tasks: 97.08, 93.21,
and 91.20% respectively.
5.2 Results and discussions
Table 1 presents the performance of our algo-
rithm. SD represents the proposed algorithm with-
out column-wise expansion, while SD+C-EXP.
uses column-wise expansion. For comparison, we
present the results of two baseline algorithms as
well: VITERBI and CARPEDIEM (Esposito and
Radicioni, 2009). In almost all settings, we see
that both of our algorithms outperformed the other
two. We also find that SD+C-EXP. performed con-
sistently better than SD. This indicates the effec-
tiveness of column-wise expansion.
Following VITERBI, CARPEDIEM is the most
relevant algorithm, for sequence labeling in NLP,
as discussed in Section 2.3. However, our results
Table 2: The average number of iterations.
POS tagging Joint tagging Supertagging
SD 6.02 8.15 10.0
SD+C-EXP. 6.12 8.62 10.6
Table 3: Training time.
POS tagging Joint tagging Supertagging
VITERBI 100 sec. 20 min. 100 hour
SD+C-EXP. 37 sec. 1.5 min. 5.3 hour
demonstrated that CARPEDIEM worked poorly in
two of the three tasks. We consider this is because
the transition information is crucial for the two
tasks, and the assumption behind CARPEDIEM is
violated. In contrast, the proposed algorithms per-
formed reasonably well for all three tasks, demon-
strating the wide applicability of our algorithm.
Table 2 presents the average iteration num-
bers of SD and SD+C-EXP. We can observe
that the two algorithms required almost the same
number of iterations on average, although the
iteration number is not tightly bounded if we
use column-wise expansion. This indicates that
SD+C-EXP. virtually avoided performing extra it-
erations, while heuristically restricting active label
expansion.
Table 3 compares the training time spent by
VITERBI and SD+C-EXP. Although speeding up
perceptron training is a by-product, it is interest-
ing to see that our algorithm is in fact effective at
reducing the training time as well. The result also
indicates that the speed-up is more significant at
test time. This is probably because the model is
not predictive enough at the beginning of training,
and the pruning is not that effective.
5.3 Comparison with approximate algorithm
Table 4 compares two exact algorithms (VITERBI
and SD+E-XP.) with beam search, which is the ap-
proximate algorithm widely adopted for sequence
labeling in NLP. For this experiment, the beam
width, B, was exhaustively calibrated: we tried B
= {1, 2, 4, 8, ...} until the beam search achieved
comparable accuracy to the exact algorithms, i.e.,
the difference fell below 0.1 in our case.
We see that there is a substantial difference in
the performance between VITERBI and BEAM.
On the other hand, SD+C-EXP. reached speeds
very close to those of BEAM. In fact, they
achieved comparable performance in our exper-
iment. These results demonstrate that we could
successfully bridge the gap in the performance be-
492
Table 4: Comparison with beam search (sent./sec).
POS tagging Joint tagging Supertagging
VITERBI 4000 77 1.1
SD+C-EXP. 14,000 1600 300
BEAM 18,000 2400 180
tween exact and approximate algorithms, while re-
taining the advantages of exact algorithms.
6 Relation to coarse-to-fine approach
Before concluding remarks, we briefly examine
the relationship between staggered decoding and
coarse-to-fine PCFG parsing (2006). In coarse-to-
fine parsing, the candidate parse trees are pruned
by using the parse forest produced by a coarse-
grained PCFG. Since the degenerate label can be
interpreted as a coarse-level label, one may con-
sider that staggered decoding is an instance of
coarse-to-fine approach. While there is some re-
semblance, there are at least two essential differ-
ences. First, coarse-to-fine approach is a heuristic
pruning, that is, it is not an exact algorithm. Sec-
ond, our algorithm does not always perform de-
coding at the fine-grained level. It is designed to
be able to stop decoding at the coarse-level.
7 Conclusions
The sequence labeling algorithm is indispensable
to modern statistical NLP. However, the Viterbi
algorithm, which is the standard decoding algo-
rithm in NLP, is not efficient when we have to
deal with a large number of labels. In this paper
we presented staggered decoding, which provides
a principled way of resolving this problem. We
consider that it is a real alternative to the Viterbi
algorithm in various NLP tasks.
An interesting future direction is to extend the
proposed technique to handle more complex struc-
tures than the Markov chains, including semi-
Markov models and factorial HMMs (Sarawagi
and Cohen, 2004; Sutton et al, 2004). We hope
this work opens a new perspective on decoding al-
gorithms for a wide range of NLP problems, not
just sequence labeling.
Acknowledgement
We wish to thank the anonymous reviewers for
their helpful comments, especially on the com-
putational complexity of our algorithm. We also
thank Yusuke Miyao for providing us with the
HPSG Treebank data.
References
Thorsten Brants. 2000. TnT - a statistical part-of-
speech tagger. In Proceedings of ANLP, pages 224?
231.
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph
Austerweil, David Ellis, Isaac Haxton, Catherine
Hill, R. Shrivaths, Jeremy Moore, Michael Pozar,
and Theresa Vu. 2006. Multi-level coarse-to-fine
PCFG parsing. In Proceedings of NAACL, pages
168?175.
Trevor Cohn. 2006. Efficient inference in large con-
ditional random fields. In Proceedings of ECML,
pages 606?613.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1?8.
Thomas G. Dietterich, Pedro Domingos, Lise Getoor,
Stephen Muggleton, and Prasad Tadepalli. 2008.
Structured machine learning: the next ten years.
Machine Learning, 73(1):3?23.
Roberto Esposito and Daniele P. Radicioni. 2009.
CARPEDIEM: Optimizing the Viterbi algorithm
and applications to supervised sequential learning.
Jorunal of Machine Learning Research, 10:1851?
1880.
Pedro F. Felzenszwalb, Daniel P. Huttenlocher, and
Jon M. Kleinberg. 2003. Fast algorithms for large-
state-space HMMs with applications to Web usage
analysis. In Proceedings of NIPS, pages 409?416.
Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.
2009. Efficient inference of CRFs for large-scale
natural language data. In Proceedings of ACL-
IJCNLP Short Papers, pages 281?284.
John Lafferty, Andrew McCallum, and Fernand
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML, pages 282?
289.
Percy Liang, Hal Daume? III, and Dan Klein. 2008.
Structure compilation: Trading structure for fea-
tures. In Proceedings of ICML, pages 592?599.
Yury Lifshits, ShayMozes, OrenWeimann, andMichal
Ziv-Ukelson. 2007. Speeding up HMM decod-
ing and training by exploiting sequence repetitions.
Computational Pattern Matching, pages 4?15.
Dekang Lin and Xiaoyun Wu. 2009. Phrae clustering
for discriminative training. In Proceedings of ACL-
IJCNLP, pages 1030?1038.
493
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2007. Efficient HPSG parsing with supertagging
and CFG-filtering. In Proceedings of IJCAI, pages
1671?1676.
Takashi Ninomiya, TakuyaMatsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006.
Extremely lexicalized models for accurate and fast
HPSG parsing. In Proceedings of EMNLP, pages
155?163.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. In Proceedings of The IEEE, pages
257?286.
Sunita Sarawagi and Willian W. Cohen. 2004. Semi-
Markov conditional random fields for information
extraction. In Proceedings of NIPS, pages 1185?
1192.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
HLT-NAACL, pages 134?141.
Sajid M. Siddiqi and Andrew W. Moore. 2005. Fast
inference and learning in large-state-space HMMs.
In Proceedings of ICML, pages 800?807.
Charles Sutton, Khashayar Rohanimanesh, and An-
drew McCallum. 2004. Dynamic conditional ran-
dom fields: Factorized probabilistic models for la-
beling and segmenting sequence data. In Proceed-
ings of ICML.
Ben Tasker, Carlos Guestrin, and Daphe Koller. 2003.
Max-margin Markov networks. In Proceedings of
NIPS, pages 25?32.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output
variables. Journal of Machine Learning Research,
6:1453?1484.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy
for tagging sequence data. In Proceedings of
HLT/EMNLP, pages 467?474.
Andrew J. Viterbi. 1967. Error bounds for convo-
lutional codes and an asymeptotically optimum de-
coding algorithm. IEEE Transactios on Information
Theory, 13(2):260?267.
494
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 964?972,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Predicting and Eliciting Addressee?s Emotion in Online Dialogue
Takayuki Hasegawa?
GREE Inc.
Minato-ku, Tokyo 106-6101, Japan
takayuki.hasegawa@gree.net
Nobuhiro Kaji and Naoki Yoshinaga
Institute of Industrial Science,
the University of Tokyo
Meguro-ku, Tokyo 153-8505, Japan
{kaji,ynaga}@tkl.iis.u-tokyo.ac.jp
Masashi Toyoda
Institute of Industrial Science,
the University of Tokyo
Meguro-ku, Tokyo 153-8505, Japan
toyoda@tkl.iis.u-tokyo.ac.jp
Abstract
While there have been many attempts to
estimate the emotion of an addresser from
her/his utterance, few studies have ex-
plored how her/his utterance affects the
emotion of the addressee. This has mo-
tivated us to investigate two novel tasks:
predicting the emotion of the addressee
and generating a response that elicits a
specific emotion in the addressee?s mind.
We target Japanese Twitter posts as a
source of dialogue data and automatically
build training data for learning the pre-
dictors and generators. The feasibility of
our approaches is assessed by using 1099
utterance-response pairs that are built by
five human workers.
1 Introduction
When we have a conversation, we usually care
about the emotion of the person to whom we
speak. For example, we try to cheer her/him up
if we find out s/he feels down, or we avoid saying
things that would trouble her/him.
To date, the modeling of emotion in a dialogue
has extensively been studied in NLP as well as re-
lated areas (Forbes-Riley and Litman, 2004; Ayadi
et al, 2011). However, the past attempts are vir-
tually restricted to estimating the emotion of an
addresser1 from her/his utterance. In contrast, few
studies have explored how the emotion of the ad-
dressee is affected by the utterance. We consider
the insufficiency of such research to be fatal for
?This work was conducted while the first author was a
graduate student at the University of Tokyo.
1We use the terms addresser/addressee rather than a
speaker/listener, because we target not spoken but online di-
alogue.
I have had a high fever for 3 days.
JOY
I hope you feel better soon.
I have had a high fever for 3 days.
SADNESS
Sorry, but you can?t join us today.
Figure 1: Two example pairs of utterances and re-
sponses. Those responses elicit certain emotions,
JOY or SADNESS, in the addressee?s mind. The ad-
dressee in this example refers to the left-hand user,
who receives the response.
computers to support human-human communica-
tions or to provide a communicative man-machine
interface.
With this motivation in mind, the paper inves-
tigates two novel tasks: (1) prediction of the ad-
dressee?s emotion and (2) generation of the re-
sponse that elicits a prespecified emotion in the ad-
dressee?s mind.2 In the prediction task, the system
is provided with a dialogue history. For simplic-
ity, we consider, as a history, an utterance and a
response to it (Figure 1). Given the history, the
system predicts the addressee?s emotion that will
be caused by the response. For example, the sys-
tem outputs JOY when the response is I hope you
feel better soon, while it outputs SADNESS when
the response is Sorry, but you can?t join us today
2We adopt Plutchik (1980)?s eight emotional categories in
both tasks.
964
(Figure 1).
In the generation task, on the other hand, the
system is provided with an utterance and an emo-
tional category such as JOY or SADNESS, which is
referred to as goal emotion. Then the system gen-
erates the response that elicits the goal emotion in
the addressee?s mind. For example, I hope you feel
better soon is generated as a response to I have had
a high fever for 3 days when the goal emotion is
specified as JOY, while Sorry, but you can?t join us
today is generated for SADNESS (Figure 1).
Systems that can perform the two tasks not only
serve as crucial components of dialogue systems
but also have interesting applications of their own.
Predicting the emotion of an addressee is use-
ful for filtering flames or infelicitous expressions
from online messages (Spertus, 1997). The re-
sponse generator that is aware of the emotion of
an addressee is also useful for text completion in
online conversation (Hasselgren et al, 2003; Pang
and Ravi, 2012).
This paper explores a data-driven approach to
performing the two tasks. With the recent emer-
gence of social media, especially microblogs, the
amount of dialogue data available is rapidly in-
creasing. Therefore, we are taking this opportu-
nity to building large-scale training data from mi-
croblog posts automatically. This approach allows
us to perform the two tasks in a large-scale with
little human effort.
We employ standard classifiers for predicting
the emotion of an addressee. Our contribution here
is to investigate the effectiveness of new features
that cannot be used in ordinary emotion recog-
nition, the task of estimating the emotion of a
speaker (or writer) from her/his utterance (or writ-
ing) (Ayadi et al, 2011; Bandyopadhyay and Oku-
mura, 2011; Balahur et al, 2011; Balahur et al,
2012). We specifically extract features from the
addressee?s last utterance (e.g., I have had a high
fever for 3 days in Figure 1) and explore the effec-
tiveness of using such features. Such information
is characteristic of a dialogue situation.
To perform the generation task, we build a sta-
tistical response generator by following (Ritter et
al., 2011). To improve on the previous study, we
investigate a method for controlling the contents
of the response for, in our case, eliciting the goal
emotion. We achieve this by using a technique in-
spired by domain adaptation. We learn multiple
models, each of which is adapted for eliciting one
specific emotion. Also, we perform model inter-
polation for addressing data sparseness.
In our experiment, we automatically build train-
ing data consisting of over 640 million dialogues
from Japanese Twitter posts. Using this data set,
we train the classifiers that predict the emotion
of an addressee, and the response generators that
elicit the goal emotion. We evaluate our methods
on the test data that are built by five human work-
ers, and confirm the feasibility of the proposed ap-
proaches.
2 Emotion-tagged Dialogue Corpus
The key in making a supervised approach to pre-
dicting and eliciting addressee?s emotion success-
ful is to obtain large-scale, reliable training data
effectually. We thus automatically build a large-
scale emotion-tagged dialogue corpus from mi-
croblog posts, and use it as the training data in the
prediction and generation tasks.
This section describes a method for construct-
ing the emotion-tagged dialogue corpus. We first
describe how to extract dialogues from posts in
Twitter, a popular microblogging service. We then
explain how to automatically annotate utterances
in the extracted dialogues with the addressers?
emotions by using emotional expressions as clues.
2.1 Mining dialogues from Twitter
We have first crawled utterances (posts) from
Twitter by using the Twitter REST API.3 The
crawled data consist of 5.5 billion utterances in
Japanese tweeted by 770 thousand users from
March 2011 to December 2012. We next cleaned
up the crawled utterances by handling Twitter-
specific expressions; we replaced all URL strings
to ?URL?, excluded utterances with the symbols
that indicate the re-posting (RT) or quoting (QT)
of others? tweets, and erased @user name ap-
pearing at the head and tail of the utterances, since
they are usually added to make a reply. We ex-
cluded utterances given by any user whose name
included ?bot.?
We then extracted dialogues from the resulting
utterances, assuming that a series of utterances
interchangeably made by two users form a dia-
logue. We here exploited ?in reply to status id?
field of each utterance provided by Twitter REST
API to link to the other, if any, utterance to which
it replied.
3https://dev.twitter.com/docs/api/
965
# users 672,937
# dialogues 311,541,839
# unique utterances 1,007,403,858
ave. # dialogues / user 463.0
ave. # utterances / user 1497.0
ave. # utterances / dialogue 3.2
Table 1: Statistics of dialogues extracted from
Twitter.
2,000,000
40,000,000
60,000,000
80,000,000
100,000,000
120,000,000
140,000,000
160,000,000
180,000,000
0 2 3 4 5 6 7 8 9 10 11+
#
Di
alo
gu
es
Dialogue length (# utterances in dialogue)
Figure 2: The number of dialogues plotted against
the dialogue length.
Utterance Emotion
A: Would you like to go for dinner with me?
B: Sorry, I can?t. I have a fever of 38 degrees.
A: Oh dear. I hope you feel better soon. SURPRISE
B: Thanks. I?m happy to hear you say that. JOY
Table 2: An illustration of an emotion-tagged dia-
logue: The first column shows a dialogue (a series
of utterances interchangeably made by two users),
while the second column shows the addresser?s
emotion estimated from the utterance.
Table 1 lists the statistics of the extracted di-
alogues, while Figure 2 plots the number of di-
alogues plotted against the dialogue length (the
number of utterances in dialogue). Most dialogues
(98.2%) consist of at most 10 utterances, although
the longest dialogue includes 1745 utterances and
spans more than six weeks.
2.2 Tagging utterances with addressers?
emotions
We then automatically labeled utterances in the
obtained dialogues with the addressers? emotions
by using emotional expressions as clues (Table 2).
In this study, we have adopted Plutchik (1980)?s
eight emotional categories (ANGER, ANTICIPA-
TION, DISGUST, FEAR, JOY, SADNESS, SUR-
PRISE, and TRUST) as the targets to label, and
manually tailored around ten emotional expres-
sions for each emotional category. Table 3 lists
examples of the emotional expressions, while the
Emotion Emotional expressions
ANGER frustrating, irritating, nonsense
ANTICIPATION exciting, expecting, looking forward
DISGUST disgusting, unpleasant, hate
FEAR afraid, anxious, scary
JOY glad, happy, delighted
SADNESS sad, lonely, unhappy
SURPRISE surprised, oh dear, wow
TRUST relieved, reliable, solid
Table 3: Example of clue emotional expressions.
Emotion # utterances Precision
Worker A Worker B
ANGER 190,555 0.95 0.95
ANTICIPATION 2,548,706 0.99 0.99
DISGUST 475,711 0.93 0.93
FEAR 2,671,222 0.96 0.96
JOY 2,725,235 0.94 0.96
SADNESS 712,273 0.97 0.97
SURPRISE 975,433 0.97 0.97
TRUST 359,482 0.97 0.98
Table 4: Size and precision of utterances labeled
with the addressers? emotions.
rest are mostly their spelling variations.4
Because precise annotation is critical in the su-
pervised learning scenario, we annotate utterances
with the addressers? emotions only when the emo-
tional expressions do not:
1. modify content words.
2. accompany an expression of negation, condi-
tional, imperative, interrogative, concession,
or indirect speech in the same sentence.
For example, I saw a frustrated teacher is re-
jected by the first condition, while I?ll be happy
if it rains is rejected by the second condition. The
second condition was judged by checking whether
the sentence includes trigger expressions such as
??? (not/never)?, ??? (if-clause)?, ???, ???
((al)though)?, and ?? (that-clause)?.
Table 4 lists the size and precision of the utter-
ances labeled with the addressers? emotions. Two
human workers measured the precision of the an-
notation by examining 100 labeled utterances ran-
domly sampled for each emotional category. The
inter-rater agreement was ? = 0.85, indicating al-
most perfect agreement. The precision of the an-
notation exceeded 0.95 for most of the emotional
categories.
4Note that the clue emotional expressions are language-
specific but can be easily tailored for other languages. Here,
Japanese emotional expressions are translated into English to
widen the potential readership of the paper.
966
3 Predicting Addressee?s Emotion
This section describes a method for predicting
emotion elicited in an addressee when s/he re-
ceives a response to her/his utterance. The input
to this task is a pair of an utterance and a response
to it, e.g., the two utterances in Figure 1, while
the output is the addressee?s emotion among the
emotional categories of Plutchik (1980) (JOY and
SADNESS for the top and bottom dialogues in Fig-
ure 1, respectively).
Although a response could elicit multiple emo-
tions in the addressee, in this paper we focus on
predicting the most salient emotion elicited in the
addressee and cast the prediction as a single-label
multi-class classification problem.5 We then con-
struct a one-versus-the-rest classifier6 by combin-
ing eight binary classifiers, each of which predicts
whether the response elicits each emotional cate-
gory. We use online passive-aggressive algorithm
to train the eight binary classifiers.
We exploit the emotion-tagged dialogue corpus
constructed in Section 2 to collect training exam-
ples for the prediction task. For each emotion-
tagged utterance in the corpus, we assume that the
tagged emotion is elicited by the (last) response.
We thereby extract the pair of utterances preced-
ing the emotion-tagged utterance and the tagged
emotion as one training example. Taking the di-
alogue in Table 2 as an example, we obtain one
training example from the first two utterances and
SURPRISE as the emotion elicited in user A.
We extract all the n-grams (n ? 3) in the re-
sponse to induce (binary) n-gram features. The
extracted n-grams could indicate a certain action
that elicits a specific emotion (e.g., ?have a fever?
in Table 2), or a style or tone of speaking (e.g.,
?Sorry?). Likewise, we extract word n-grams from
the addressee?s utterance. The extracted n-grams
activate another set of binary n-gram features.
Because word n-grams themselves are likely to
be sparse, we estimate the addressers? emotions
from their utterances and exploit them to induce
emotion features. The addresser?s emotion has
been reported to influence the addressee?s emotion
5Because microblog posts are short, we expect emotions
elicited by a response post not to be very diverse and a multi-
class classification to be able to capture the essential crux of
the prediction task.
6We should note that a one-versus-the-rest classifier can
be used in the multi-label classification scenario, just by al-
lowing the classifier to output more than one emotional cate-
gory (Ghamrawi and McCallum, 2005).
strongly (Kim et al, 2012), while the addressee?s
emotion just before receiving a response can be a
reference to predict her/his emotion in question af-
ter receiving the response.
To induce emotion features, we exploit the rule-
based approach used in Section 2.2 to estimate
the addresser?s emotion. Since the rule-based ap-
proach annotates utterances with emotions only
when they contain emotional expressions, we in-
dependently train for each emotional category
a binary classifier that estimates the addresser?s
emotion from her/his utterance and apply it to the
unlabeled utterances. The training data for these
classifiers are the emotion-tagged utterances ob-
tained in Section 2, while the features are n-grams
(n ? 3)7 in the utterance.
We should emphasize that the features induced
from the addressee?s utterance are unique to this
task and are hardly available in the related tasks
that predicted the emotion of a reader of news ar-
ticles (Lin and Hsin-Yihn, 2008) or personal sto-
ries (Socher et al, 2011). We will later confirm the
impact of these features on the prediction accuracy
in the experiments.
4 Eliciting Addressee?s Emotion
This section presents a method for generating a re-
sponse that elicits the goal emotion, which is one
of the emotional categories of Plutchik (1980), in
the addressee. In section 4.1, we describe a statis-
tical framework for response generation proposed
by (Ritter et al, 2011). In section 4.2, we present
how to adapt the model in order to generate a
response that elicits the goal emotion in the ad-
dressee.
4.1 Statistical response generation
Following (Ritter et al, 2011), we apply the sta-
tistical machine translation model for generating a
response to a given utterance. In this framework,
a response is viewed as a translation of the input
utterance. Similar to ordinary machine translation
systems, the model is learned from pairs of an ut-
terance and a response by using off-the-shelf tools
for machine translation.
We use GIZA++8 and SRILM9 for learning
translation model and 5-gram language model, re-
7We have excluded n-grams that matched the emotional
expressions used in Section 2 to avoid overfitting.
8http://code.google.com/p/giza-pp/
9http://www.speech.sri.com/projects/
srilm/
967
spectively. As post-processing, some phrase pairs
are filtered out from the translation table as fol-
lows. When GIZA++ is directly applied to di-
alogue data, it frequently finds paraphrase pairs,
learning to parrot back the input (Ritter et al,
2011). To avoid using such pairs for response gen-
eration, a phrase pair is removed if one phrase is
the substring of the other.
We use Moses decoder10 to search for the best
response to a given utterance. Unlike machine
translation, we do not use reordering models, be-
cause the positions of phrases are not considered
to correlate strongly with the appropriateness of
responses (Ritter et al, 2011). In addition, we do
not use any discriminative training methods such
as MERT for optimizing the feature weights (Och,
2003). They are set as default values provided by
Moses (Ritter et al, 2011).
4.2 Model adaptation
The above framework allows us to generate appro-
priate responses to arbitrary input utterances. On
top of this framework, we have developed a re-
sponse generator that elicits a specific emotion.
We use the emotion-tagged dialogue corpus to
learn eight translation models and language mod-
els, each of which is specialized in generating
the response that elicits one of the eight emo-
tions (Plutchik, 1980). Specifically, the models
are learned from utterances preceding ones that are
tagged with emotional category. As an example,
let us examine to learn models for eliciting SUR-
PRISE from the dialogue in Table 2. In this case,
the first two utterances are used to learn the trans-
lation model, while only the second utterance is
used to learn the language model.
However, this simple approach is prone to suf-
fer from the data sparseness problem. Because
not all the utterances are tagged with the emotion
in emotion-tagged dialogue corpus, only a small
fraction of utterances can be used for learning the
adapted models.
We perform model interpolation for addressing
this problem. In addition to the adapted mod-
els described above, we also use a general model,
which is learned from the entire corpus. The two
models are then merged as the weighted linear in-
terpolation.
Specifically, we use tmcombine.py script
provided by Moses for the interpolation of trans-
10http://www.statmt.org/moses/
lation models (Sennrich, 2012). For all the four
features (i.e., two phrase translation probabilities
and two lexical weights) derived from transla-
tion model, the weights of the adapted model are
equally set as ? (0 ? ? ? 1.0). On the other
hand, we use SRILM for the interpolation of lan-
guage models. The weight of the adapted model is
set as ? (0 ? ? ? 1.0).
The parameters ? and ? control the strength of
the adapted models. Only adapted models are used
when ? (or ?)= 1.0, while the adapted models are
not at all used when ? (or ?) = 0. When both ?
and ? are specified as 0, the model becomes equiv-
alent to the original one described in section 4.1.
5 Experiments
5.1 Test data
To evaluate the proposed method, we built, as test
data, sets of an utterance paired with responses
that elicit a certain goal emotion (Table 5). Note
that they were used for evaluation in both of the
two tasks. Each utterance in the test data has
more than one responses that elicit the same goal
emotion, because they are used to compute BLEU
score (see section 5.3).
The data set was built in the following manner.
We first asked five human worker to produce re-
sponses to 80 utterances (10 utterances for each
goal emotion). Note that the 80 utterances do not
have overlap between workers and that the worker
produced only one response to each utterance.
To alleviate the burden on the workers, we ac-
tually provided each worker with the utterances
in the emotion-tagged corpus. Then we asked
each worker to select 80 utterances to which s/he
thought s/he could easily respond. The selected
utterances were removed from the corpus during
training.
As a result, we obtained 400 utterance-response
pairs (= 80 utterance-response pairs ? 5 work-
ers). For each of those 400 utterances, two ad-
ditional responses are produced. We did not al-
low the same worker to produce more than one
response to the same utterance. In this way, we
obtained 1200 responses for the 400 utterances in
total.
Finally, we assessed the data quality to remove
responses that were unlikely to elicit the goal emo-
tion. For each utterance-response pair, we asked
two workers to judge whether the response elicited
the goal emotion. If both workers regarded the
968
Goal emotion: JOY
U: 16???????????????????
?????
(I?m turning 16. Hope to get alng with you as
well as ever!)
R1:??????????????
(Happy birthday!)
R2:?????????????????????
(Congratulations! I?ll give you a birthday present.)
R3:???????????????
(Congratulations! I hope you have a happy year!)
Table 5: Example of the test data. English transla-
tions are attached in the parenthesis.
Emotion # utterance pairs
ANGER 119,881
ANTICIPATION 1,416,847
DISGUST 333,972
FEAR 1,662,998
JOY 1,724,198
SADNESS 436,668
SURPRISE 589,790
TRUST 228,974
GENERAL 646,429,405
Table 6: The number of utterance pairs used
for training classifiers in emotion prediction and
learning the translation models and language mod-
els in response generation.
response as inappropriate, it was removed from
the data. The resulting test data consist of 1099
utterance-response pairs for 396 utterances.
This data set is submitted as supplementary ma-
terial to support the reproducibility of our experi-
mental results.
5.2 Prediction task
We first report experimental results on predicting
the addressee?s emotion within a dialogue. Table 6
lists the number of utterance-response pairs used
to train eight binary classifiers for individual emo-
tional categories, which form a one-versus-the rest
classifier for the prediction task. We used opal11
as an implementation of online passive-aggressive
algorithm to train the individual classifiers.
To investigate the impact of the features that are
uniquely available in a dialogue data, we com-
pared classifiers trained with the following two
sets of features in terms of precision, recall, and
F1 for each emotional category.
RESPONSE The n-gram and emotion features in-
duced from the response.
11http://www.tkl.iis.u-tokyo.ac.jp/
?ynaga/opal/.
Emotion RESPONSE RESPONSE/UTTER.
PREC REC F1 PREC REC F1
ANGER 0.455 0.476 0.465 0.600 0.548 0.573
ANTICIPA. 0.518 0.526 0.522 0.614 0.637 0.625
DISGUST 0.275 0.519 0.359 0.378 0.511 0.435
FEAR 0.484 0.727 0.581 0.459 0.706 0.556
JOY 0.690 0.417 0.519 0.720 0.590 0.649
SADNESS 0.711 0.467 0.564 0.670 0.562 0.611
SURPRISE 0.511 0.348 0.414 0.584 0.437 0.500
TRUST 0.695 0.452 0.548 0.682 0.514 0.586
average 0.542 0.492 0.497 0.588 0.563 0.567
Table 7: Predicting addressee?s emotion: Results.
PREDICTED EMOTION
AN
GE
R
AN
TI
CI
PA
.
DI
SG
US
T
FE
AR
JO
Y
SA
DN
ES
S
SU
RP
RI
SE
TR
US
T
tot
al
ANGER 69 0 26 20 0 8 2 1 126
ANTICIPA. 1 86 11 7 13 0 6 11 135
DISGUST 25 1 68 18 2 8 7 4 133
FEAR 3 0 22 101 1 5 9 2 143
JOY 1 28 9 4 85 1 7 9 144
SADNESS 6 3 25 14 5 77 5 2 137
SURPRISE 7 10 9 32 5 7 59 6 135
TRUST 3 12 10 24 7 9 6 75 146CO
RR
EC
T
EM
OT
IO
N
total 115 140 180 220 118 115 101 110 1099
Table 8: Confusion matrix of predicting ad-
dressee?s emotion, with mostly predicted emo-
tions bold-faced and mostly confused emotions
underlined for each emotional category.
RESPONSE/UTTER. The n-gram and emotion
features induced from the response and the
addressee?s utterance.
Table 7 lists prediction results. We can see that
the features induced from the addressee?s utter-
ance significantly improved the prediction perfor-
mance, F1, for emotions other than FEAR. FEAR is
elicited instantly by the response, and the features
induced from the addressee?s utterance thereby
confused the classifier.
Table 8 shows a confusion matrix of the classi-
fier using all the features, with mostly predicted
emotions bold-faced and mostly confused emo-
tions underlined for each emotional category. We
can find some typical confusing pairs of emotions
from this matrix. The classifier confuses DISGUST
with ANGER and vice versa, while it confuses JOY
with ANTICIPATION. These confusions conform
to our expectation, since they are actually similar
emotions. The classifier was less likely to confuse
positive emotions (JOY and ANTICIPATION) with
negative emotion (ANGER, DISGUST, FEAR, and
SADNESS) vice versa.
969
Goal emotion: ANGER (predicted as SADNESS)
U:????????????????
(You have phone calls every day, I envy you.)
R:????????????????????????
(I envy you have a lot of time ?cause no one calls you.)
Goal emotion: SURPRISE (predicted as FEAR)
U:????????????
(Is it true that dark-haired girls are popular with boys?)
R:???????????????????
(About 80% of boys seem to prefer dark-haired girls.)
Table 9: Examples of utterance-response pairs to
which the system predicted wrong emotions.
We have briefly examined the confusions and
found the two major types of errors, each of which
is exemplified in Table 9. The first (top) one is sar-
casm or irony, which has been reported to be diffi-
cult to capture by lexical features alone (Gonza?lez-
Iba?n?ez et al, 2011). The other (bottom) one is due
to lack of information. In this example, only if the
addressee does not know the fact provided by the
response, s/he will surprise at it.
5.3 Generation task
We next demonstrate the experimental results for
eliciting the emotion of the addressee.
We use the utterance pairs summarized in Ta-
ble 6 to learn the translation models and language
models for eliciting each emotional category. We
also use the 640 million utterances pairs in the
entire emotion-tagged corpus for learning general
models. However, for learning the general transla-
tion models, we currently use 4 millions of utter-
ance pairs sampled from the 640 millions of pairs
due to the computational limitation.
Automatic evaluation
We first use BLEU score (Papineni et al, 2002)
to perform automatic evaluation (Ritter et al,
2011). In this evaluation, the system is pro-
vided with the utterance and the goal emotion
in the test data and the generated responses are
evaluated through BLEU score. Specifically, we
conducted two-fold cross-validation to optimize
the weights of our method. We tried ? and
? in {0.0, 0.2, 0.4, 0.6, 0.8, 1.0} and selected the
weights that achieved the best BLEU score. Note
that we adopted different values of the weights for
different emotional categories.
Table 10 compares BLEU scores of three meth-
ods including the proposed one. The first row
represents a method that does not perform model
adaptation at all. It corresponds to the special case
System BLEU
NO ADAPTATION 0.64
PROPOSED 1.05
OPTIMAL 1.57
Table 10: Comparison of BLEU scores.
(i.e., ? = ? = 0.0) of the proposed method. The
second row represents our method, while the last
row represents the result of our method when the
weights are set as optimal, i.e., those achieving the
best BLEU on the test data. This result can be con-
sidered as an upper bound on BLEU score.
The results demonstrate that model adaptation
is useful for generating the responses that elicit
the goal emotion. We can clearly observe the im-
provement in the BLEU from 0.64 to 1.05.
On the other hand, there still remains a gap be-
tween the last two rows (i.e., proposed and opti-
mal). We think this is partly because the current
test data is too small to reliably tune parameters.
Human evaluation
We next asked two human workers to manually
evaluate the generation results.
In this evaluation, the baseline (no adaptation
in Table 10) and proposed method generated a re-
sponse for each of the 396 utterances in the test
data. For the resulting 792 utterance-response
pairs, the two workers manually assessed the ap-
propriateness of the response. Each response was
judged whether it is grammatical and meaningful.
If the response was regarded as so by either of the
workers, it was further judged whether it elicits the
goal emotion or not. To make the comparison fair,
we did not expose to the workers which system
generated the response. In addition, the responses
generated by the two systems were presented in a
random order.
As the result, 147 and 157 responses of the
baseline and proposed method were regarded as
appropriate, i.e., ecliting the goal emotion, by ei-
ther of the workers; 74 and 92 responses were
regarded as appropirate by both of the workers.
These results suggest the effectiveness of the pro-
posed method. Especially, we can confirm that
the proposed method can generate responses that
elicit addresee?s emotion more clearly. We inves-
tigated the agreement between the two workers in
this evaluation. We found that the ? coefficient is
0.59, which indicates moderate agreement. This
supports the reliability of our evaluation.
970
Goal emotion: JOY
Input: ???????????????????2? 7?????
(I wooooon the outstanding award at the photo competition! The ceremony is on Feb. 7!)
NO ADAPTATION: ?????????????? (Sorry to say, only the first day.)
PROPOSED: ????????????????????
(Congratulations on winning the gold prize!!! Congrats.)
Goal emotion: TRUST
Input: ???????????? (Do you get desperate? )
NO ADAPTATION: ???? (I?m looking forward to it!)
PROPOSED: ???????? (Maybe still OK.)
Goal emotion: ANTICIPATION
Input: ?????????????ww
(Huh! It?s gonna be all right! lol)
?????????????????????????????????? (???)
(I gotta buy the goods, so I?ll be glad if you can take the time :-))
NO ADAPTATION: ????????????????? (Since I?ve not bought it, I feel worried.)
PROPOSED: ???????????????? (Good! I?ll buy it too!!!)
Table 11: Examples of the responses generated by the two systems, NO ADAPTATION and PROPOSED.
Examples
Table 11 illustrates examples of the responses gen-
erated by the no adaptation baseline and proposed
method. In the first two examples, the proposed
method successfully generates responses that elicit
the goal emotions: JOY and TRUST. From these
examples, we can consider that the adapted model
assigns large probability to phrases such as con-
gratulations or OK. In the last example, the sys-
tem also succeeded in eliciting the goal emotion:
ANTICIPATION. For this example, we can interpret
that the speaker of the response (i.e., the system)
feels anticipation, and consequently the emotion
of the addressee is affected by the emotion of the
speaker (i.e., the system). Interestingly, a similar
phenomenon is also observed in real conversation
(Kim et al, 2012).
6 Related Work
There have been a tremendous amount of stud-
ies on predicting the emotion from text or speech
data (Ayadi et al, 2011; Bandyopadhyay and Oku-
mura, 2011; Balahur et al, 2011; Balahur et al,
2012). Unlike our prediction task, most of them
have exclusively focused on estimating the emo-
tion of a speaker (or writer) from her/his utterance
(or writing).
Analogous to our prediction task, Lin and Hsin-
Yihn (2008) and Socher et al (2011) investigated
predicting the emotion of a reader from the text
that s/he reads. Our work differs from them in that
we focus on dialogue data, and we exploit fea-
tures that are not available within their task set-
tings, e.g., the addressee?s previous utterance.
Tokuhisa et al (2008) proposed a method for
extracting pairs of an event (e.g., It rained sud-
denly when I went to see the cherry blossoms) and
an emotion elicited by it (e.g., SADNESS) from the
Web text. The extracted data are used for emotion
classification. A similar technique would be use-
ful for prediction the emotion of an addressee as
well.
Response generation has a long research history
(Weizenbaum, 1966), although it is only very re-
cently that a fully statistical approach was intro-
duced in this field (Ritter et al, 2011). At this mo-
ment, we are unaware of any statistical response
generators that model the emotion of the user.
Some researchers have explored generating
jokes or humorous text (Dybala et al, 2010;
Labtov and Lipson, 2012). Those attempts are
similar to our work in that they also aim at elic-
iting a certain emotion in the addressee. They are,
however, restricted to elicit a specific emotion.
The linear interpolation of translation and/or
language models is a widely-used technique for
adapting machine translation systems to new do-
mains (Sennrich, 2012). However, it has not been
touched in the context of response generation.
7 Conclusion and Future Work
In this paper, we have explored predicting and
eliciting the emotion of an addressee by using a
large amount of dialogue data obtained from mi-
croblog posts. In the first attempt to model the
emotion of an addressee in the field of NLP, we
demonstrated that the response of the dialogue
partner and the previous utterance of the addressee
are useful for predicting the emotion. In the gen-
eration task, on the other hand, we showed that the
971
model adaptation approach successfully generates
the responses that elicit the goal emotion.
For future work, we want to use longer dialogue
history in both tasks. While we considered only
two utterances as a history, a longer history would
be helpful. We also plan to personalize the pro-
posed methods, exploiting microblog posts made
by users of a certain age, gender, occupation, or
even character to perform model adaptation.
Acknowledgment
This work was supported by the FIRST program of
JSPS. The authors thank the anonymous review-
ers for their valuable comments. The authors also
thank the student annotators for their hard work.
References
Moataz El Ayadi, Mohamed S. Kamel, and Fakhri Kar-
ray. 2011. Survey on speech emotion recognition:
Features, classification schemes, and databases.
Pattern Recognition, 44:572?587.
Alexandra Balahur, Ester Boldrini, Andres Montoyo,
and Patricio Martinez-Barco, editors. 2011. Pro-
ceedings of the 2nd Workshop on Computational
Approaches to Subjectivity and Sentiment Analysis.
Association for Computational Linguistics.
Alexandra Balahur, Andres Montoyo, Patricio Mar-
tinez Barco, and Ester Boldrini, editors. 2012. Pro-
ceedings of the 3rd Workshop on Computational
Approaches to Subjectivity and Sentiment Analysis.
Association for Computational Linguistics.
Sivaji Bandyopadhyay and Manabu Okumura, editors.
2011. Proceedings of the Workshop on Sentiment
Analysis where AI meets Psychology. Asian Federa-
tion of Natural Language Processing.
Pawel Dybala, Michal Ptaszynski, Jacek Maciejewski,
Mizuki Takahashi, Rafal Rzepka, and Kenji Araki.
2010. Multiagent system for joke generation: Hu-
mor and emotions combined in human-agent conver-
sation. Journal of Ambient Intelligence and Smart
Environments, 2(1):31?48.
Kate Forbes-Riley and Diane J. Litman. 2004. Pre-
dicting emotion in spoken dialogue from multiple
knowledge sources. In Proceedings of NAACL,
pages 201?208.
Nadia Ghamrawi and Andrew McCallum. 2005. Col-
lective multi-label classification. In Proceedings of
CIKM, pages 195?200.
Roberto Gonza?lez-Iba?n?ez, Smaranda Muresan, and
Nina Wacholder. 2011. Identifying sarcasm in twit-
ter: a closer look. In Proceedings of ACL, pages
581?586.
Jon Hasselgren, Erik Montnemery, Pierre Nugues, and
Markus Svensson. 2003. HMS: A predictive text
entry method using bigrams. In Proceedings of
EACL Workshop on Language Modeling for Text En-
try Methods, pages 43?50.
Suin Kim, JinYeong Bak, and Alice Haeyun Oh. 2012.
Do you feel what I feel? social aspects of emotions
in Twitter conversations. In Proceedings of ICWSM,
pages 495?498.
Igor Labtov and Hod Lipson. 2012. Humor as circuits
in semantic networks. In Proceedings of ACL (Short
Papers), pages 150?155.
Kevin Lin and Hsin-Hsi Hsin-Yihn. 2008. Ranking
reader emotions using pairwise loss minimization
and emotional distribution regression. In Proceed-
ings of EMNLP, pages 136?144.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167.
Bo Pang and Sujith Ravi. 2012. Revisiting the pre-
dictability of language: Response completion in so-
cial media. In Proceedings of EMNLP, pages 1489?
1499.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Robert Plutchik. 1980. A general psychoevolutionary
theory of emotion. In Emotion: Theory, research,
and experience: Vol. 1. Theories of emotion, pages
3?33. New York: Academic.
Alan Ritter, Colin Cherry, andWilliam B. Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of EMNLP, pages 583?593.
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of EACL, pages
539?549.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
EMNLP, pages 151?161.
Ellen Spertus. 1997. Smokey: Automatic recognition
of hostile messages. In Proceedings of IAAI, pages
1058?1065.
Ryoko Tokuhisa, Kentaro Inui, and Yuji Matsumoto.
2008. Emotion classification using massive exam-
ples extracted from the Web. In Proceedings of
COLING, pages 881?888.
JosephWeizenbaum. 1966. ELIZA? a computer pro-
gram for the study of natural language communica-
tion between man and machine. Communications of
the ACM, 9(1):36?45.
972
