Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 218?228,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Negative Training Data can be Harmful to Text Classification 
 
 
Xiao-Li  Li Bing Liu See-Kiong  Ng 
Institute for Infocomm Research University of Illinois at Chicago Institute for Infocomm Research 
1 Fusionopolis Way #21-01,  
Connexis Singapore 138632 
851 South Morgan Street,  
Chicago, IL 60607-7053, USA 
1 Fusionopolis Way #21-01,  
Connexis Singapore 138632 
xlli@i2r.a-star.edu.sg liub@cs.uic.edu skng@i2r.a-star.edu.sg
 
 
Abstract 
This paper studies the effects of training data 
on binary text classification and postulates 
that negative training data is not needed and 
may even be harmful for the task. Traditional 
binary classification involves building a clas-
sifier using labeled positive and negative 
training examples. The classifier is then ap-
plied to classify test instances into positive 
and negative classes. A fundamental assump-
tion is that the training and test data are iden-
tically distributed. However, this assumption 
may not hold in practice. In this paper, we 
study a particular problem where the positive 
data is identically distributed but the negative 
data may or may not be so. Many practical 
text classification and retrieval applications fit 
this model. We argue that in this setting nega-
tive training data should not be used, and that 
PU learning can be employed to solve the 
problem. Empirical evaluation has been con-
ducted to support our claim. This result is im-
portant as it may fundamentally change the 
current binary classification paradigm.  
1 Introduction 
Text classification is a well-studied problem in 
machine learning, natural language processing, and 
information retrieval. To build a text classifier, a 
set of training documents is first labeled with pre-
defined classes. Then, a supervised machine learn-
ing algorithm (e.g., Support Vector Machines 
(SVM), na?ve Bayesian classifier (NB)) is applied 
to the training examples to build a classifier that is 
subsequently employed to assign class labels to the 
instances in the test set. In this paper, we focus on 
binary text classification with two classes (i.e. pos-
itive and negative classes).  
Most learning methods assume that the training 
and test data have identical distributions. However, 
this assumption may not hold in practice, i.e., the 
training and the test distributions can be different. 
The problem is called covariate shift or sample 
selection bias (Heckman 1979; Shimodaira 2000; 
Zadrozny 2004; Huang et al 2007; Sugiyama et al 
2008; Bickel et al 2009). In general, this problem 
is not solvable because the two distributions can be 
arbitrarily far apart from each other. Various as-
sumptions were made to solve special cases of the 
problem. One main assumption was that the condi-
tional distribution of the class given an instance is 
the same over the training and test sets (Shimodai-
ra 2000; Huang et al 2007; Bickel et al 2009).  
In this paper, we study another special case of 
the problem in which the positive training and test 
samples have identical distributions, but the nega-
tive training and test samples may have different 
distributions. We believe this scenario is more ap-
plicable for binary text classification. As the focus 
in many applications is on identifying positive in-
stances correctly, it is important that the positive 
training and the positive test data have the same 
distribution. The distributions of the negative train-
ing and negative test data can be different. We be-
lieve that this special case of the sample selection 
bias problem is also more applicable for machine 
learning. We will show that a partially supervised 
learning model, called PU learning (learning from 
Positive and Unlabeled examples) fits this special 
case quite well (Liu et al 2002).  
Following the notations in (Bickel et al 2009), 
our special case of the sample selection bias prob-
lem can be formulated as follows: We are given a 
training sample matrix XL with row vectors x1, ?, 
xk. The positive and negative training instances are 
governed by different unknown distributions p(x|?) 
218
and p(x|?) respectively. The element yi of vector y 
= (y1,  y2, ?, yk) is the class label for training in-
stance xi (yi ?{+1, -1}, where +1 and -1 denote 
positive and negative classes respectively) and is 
drawn based on an unknown target concept p(y|x). 
In addition, we are also given an unlabeled test set 
in matrix XT with rows xk+1, ?, xk+m. The (hidden) 
positive test instances in XT are also governed by 
the unknown distribution p(x|?), but the (hidden) 
negative test instances in XT are governed by an 
unknown distribution, p(x|?), where ? may or may 
not be the same as ?. p(x|?) and p(x|?) can differ 
arbitrarily, but there is only one unknown target 
conditional class distribution p(y|x).  
This problem setting is common in many appli-
cations, especially in those applications where the 
user is interested in identifying a particular type of 
documents (i.e. binary text classification). For ex-
ample, we want to find sentiment analysis papers 
in the literature. For training a text classifier, we 
may label the papers in some EMNLP proceedings 
as sentiment analysis (positive) and non-sentiment 
analysis (negative) papers. A classifier can then be 
built to find sentiment analysis papers from ACL 
and other EMNLP proceedings. However, this la-
beled training set will not be appropriate for identi-
fying sentiment analysis papers from the WWW, 
KDD and SIGIR conference proceedings. This is 
because although the sentiment analysis papers in 
these proceedings are similar to those in the train-
ing data, the non-sentiment analysis papers in these 
conferences can be quite different. Another exam-
ple is email spam detection. A spam classification 
system built using the training data of spam and 
non-spam emails from a university may not per-
form well in a company. The reason is that al-
though the spam emails (e.g., unsolicited 
commercial ads) are similar in both environments, 
the non-spam emails in them can be quite different.  
One can consider labeling the negative data in 
each environment individually so that only the 
negative instances relevant to the testing environ-
ment are used to train the classifier.  However, it is 
often impractical (if not impossible) to do so. For 
example, given a large blog hosting site, we want 
to classify its blogs into those that discuss stock 
markets (positive), and those that do not (nega-
tive). In this case, the negative data covers an arbi-
trary range of topics. It is clearly impractical to 
label all the negative data. 
Most existing methods for addressing the sam-
ple selection bias problem work as follows.  First, 
they estimate the bias of the training data based on 
the given test data using statistical methods. Then, 
a classifier is trained on a weighted version of the 
original training set based on the estimated bias. In 
this paper, we show that our special case of the 
sample selection bias problem can be solved in a 
much simpler and somewhat radical manner?by 
simply discarding the negative training data alto-
gether. We can use the positive training data and 
the unlabeled test data to build the classifier using 
the PU learning model  (Liu et al 2002).  
PU learning was originally proposed to solve the 
learning problem where no labeled negative train-
ing data exist. Several algorithms have been devel-
oped in the past few years that can learn from a set 
of labeled positive examples augmented with a set 
of unlabeled examples. That is, given a set P of 
positive examples of a particular class (called the 
positive class) and a set U of unlabeled examples 
(which contains both hidden positive and hidden 
negative examples), a classifier is built using P and 
U to classify the data in U as well as future test 
data into two classes, i.e., those belonging to P 
(positive) and those not belonging to P (negative). 
In this paper, we also propose a new PU learning 
method which gives more consistently accurate 
results than the current methods.  
Our experimental evaluation shows that when 
the distributions of the negative training and test 
samples are different, PU learning is much more 
accurate than traditional supervised learning from 
the positive and negative training samples. This 
means that the negative training data actually 
harms classification in this case. In addition, when 
the distributions of the negative training and test 
samples are identical, PU learning is shown to per-
form equally well as supervised learning, which 
means that the negative training data is not needed.   
This paper thus makes three contributions. First, 
it formulates a new special case of the sample se-
lection bias problem, and proposes to solve the 
problem using PU learning by discarding the nega-
tive training data. Second, it proposes a new PU 
learning method which is more accurate than the 
existing methods. Third, it experimentally demon-
strates the effectiveness of the proposed method 
and shows that negative training data is not needed 
and can even be harmful. This result is important 
as it may fundamentally change the way that many 
practical classification problems should be solved.  
219
2 Related Work  
A key assumption made by most machine learning 
algorithms is that the training and test samples 
must be drawn from the same distribution. As 
mentioned, this assumption can be violated in prac-
tice. Some researchers have addressed this problem 
under covariate shift or sample selection bias. 
Sample selection bias was first introduced in the 
econometrics by Heckman (1979). It came into the 
field of machine learning through the work of Za-
drozny (2004). The main approach in machine 
learning is to first estimate the distribution bias of 
the training data based on the test data, and then 
learn using weighted training examples to compen-
sate for the bias (Bickel et al 2009).  
Shimodaira (2000) and Sugiyama and Muller 
(2005) proposed to estimate the training and test 
data distributions using kernel density estimation. 
The estimated density ratio could then be used to 
generate weighted training examples. Dudik et al 
(2005) and Bickel and Scheffer (2007) used maxi-
mum entropy density estimation, while Huang et 
al. (2007) proposed kernel mean matching. Su-
giyama et al (2008) and Tsuboi et al (2008) esti-
mated the weights for the training instances by 
minimizing the Kullback-Leibler divergence be-
tween the test and the weighted training distribu-
tions. Bickel et al (2009) proposed an integrated 
model. In this paper, we adopt an entirely different 
approach by dropping the negative training data 
altogether in learning. Without the negative train-
ing data, we use PU learning to solve the problem 
(Liu et al 2002; Yu et al 2002; Denis et al 2002; 
Li et al 2003; Lee and Liu, 2003; Liu et al 2003; 
Denis et al 2003; Li et al 2007; Elkan and Noto, 
2008; Li et al 2009; Li et al 2010). We will dis-
cuss this learning model further in Section 3.  
Another related work to ours is transfer learning 
or domain adaptation. Unlike our problem setting, 
transfer learning addresses the scenario where one 
has little or no training data for the target domain, 
but has ample training data in a related domain 
where the data could be in a different feature space 
and follow a different distribution. A survey of 
transfer learning can be found in (Pan and Yang 
2009). Several NLP researchers have studied trans-
fer learning for different applications (Wu et al 
2009a; Yang et al 2009; Agirre & Lacalle 2009; 
Wu et al 2009b; Sagae & Tsujii 2008; Goldwasser 
& Roth 2008; Li and Zong 2008; Andrew et al 
2008; Chan and Ng 2007; Jiang and Zhai 2007; 
Zhou et al 2006), but none of them addresses the 
problem studied here.  
3 PU Learning Techniques 
In traditional supervised learning, ideally, there is a 
large number of labeled positive and negative ex-
amples for learning. In practice, the negative ex-
amples can often be limited or unavailable. This 
has motivated the development of the model of 
learning from positive and unlabeled examples, or 
PU learning, where P denotes a set of positive ex-
amples, and U a set of unlabeled examples (which 
contains both hidden positive and hidden negative 
instances). The PU learning problem is to build a 
classifier using P and U in the absence of negative 
examples to classify the data in U or a future test 
data T. In our setting, the test set T will also act as 
the unlabeled set U.  
PU learning has been investigated by several re-
searchers in the past decade. A study of PAC learn-
ing for the setting under the statistical query model 
was given in (Denis, 1998). Liu et al reported the 
sample complexity result and showed how the 
problem may be solved (Liu et al, 2002).  Subse-
quently, a number of practical algorithms (e.g., Liu 
et al, 2002; Yu et al, 2002; Li and Liu, 2003) 
were proposed. They generally follow a two-step 
strategy: (i) identifying a set of reliable negative 
documents RN from the unlabeled set; and then (ii) 
building a classifier using P (positive set), RN (re-
liable negative set) and U-RN (unlabelled set) by 
applying an existing learning algorithm (such as 
naive Bayesian classifier or SVM) iteratively. 
There are also some other approaches based on 
unbalanced errors (e.g., Liu et al 2003; Lee and 
Liu, 2003; Elkan and Noto, 2008). 
In this section, we first introduce a representa-
tive PU learning technique S-EM, and then present 
a new technique called CR-SVM. 
3.1 S-EM Algorithm  
S-EM (Liu et al 2002) is based on na?ve Bayesian 
classification (NB) (Lewis, 1995; Nigam et al, 
2000) and the EM algorithm (Dempster et al 
1977). It has two steps. The first step uses a spy 
technique to identify some reliable negatives (RN) 
from the unlabeled set U and the second step uses 
the EM algorithm to learn a Bayesian classifier 
from P, RN and U?RN. 
220
Step 1: Extracting reliable negatives RN from U 
using a spy technique 
The spy technique in S-EM works as follows (Fig-
ure 1): First, a small set of positive examples (de-
noted by SP) called ?spies? is randomly sampled 
from P (line 2). The default sampling ratio in S-
EM is s = 15%. Then, an NB classifier is built us-
ing P?SP as the positive set and U?SP as the neg-
ative set (lines 3-5). The NB classifier is applied to 
classify each u ? U?SP, i.e., to assign a probabil-
istic class label p(+|u) (+ means positive) to u. The 
idea of the spy technique is as follows. Since the 
spy examples were from P and were put into U as 
negatives in building the NB classifier, they should 
behave similarly to the hidden positive instances in 
U. We thus can use them to find the reliable nega-
tive set RN from U. Using the probabilistic labels 
of spies in SP and an input parameter l (noise lev-
el), a probability threshold t is determined. Due to 
space constraints, we are unable to explain l. De-
tails can be found in (Liu et al 2002). t is then used 
to find RN from U (lines 8-10).  
1.  RN ? ?;                  // Reliable negative set 
2.  SP ? Sample(P, s%);          // spy set 
3.  Assign each example in P ? SP the class label +1; 
4.  Assign each example in U ?SP the class label -1; 
5.  C ?NB(P ? SP, U?SP);   // Produce a NB classifier  
6.  Classify each u ?U?SP using C; 
7.  Decide a probability threshold t using SP and l; 
8.  For each u ?U do 
9.       If its probability p(+|u) < t then 
10.          RN ? RN ? {u}; 
Figure 1. Spy technique for extracting RN from U 
Step 2: Learning using the EM algorithm 
Given the positive set P, the reliable negative set 
RN, and the remaining unlabeled set U?RN, we run 
EM using NB as the base learning algorithm. 
The naive Bayesian (NB) method is an effective 
text classification algorithm. There are two differ-
ent NB models, namely, the multinomial NB and 
the multi-variate Bernoulli NB. In this paper, we 
use the multinomial NB since it has been observed 
to perform consistently better than the multi-
variate Bernoulli NB (Nigam et al, 2000).  
Given a set of training documents D, each doc-
ument di ? D is an ordered list of words. We use 
wdi,k to denote the word in position k of di, where 
each word is from the vocabulary V = {w1, ? , w|v|}, 
which is the set of all words considered in classifi-
cation. We also have a set of classes C = {c1, c2} 
representing positive and negative classes. For 
classification, we compute the posterior probability 
Pr(cj|di). Based on the Bayes rule and multinomial 
model, we have 
      
   (1) 
and with Laplacian smoothing, 
    (2) 
where N(wt,di) is the number of times that the word 
wt occurs in document di, and Pr(cj|di) {0,1} de-
pending on the class label of the document. As-
suming that probabilities of words are independent 
given the class, we have the NB classifier:  
 
(3) 
EM (Dempster et al 1977) is a popular class of 
iterative algorithms for maximum likelihood esti-
mation in problems with incomplete data. It is of-
ten used to address missing values in the data by 
computing expected values using the existing val-
ues. The EM algorithm consists of two steps, the 
E-step and the M-step. The E-step fills in the miss-
ing data, and M-step re-estimated the parameters. 
This process is iterated till satisfaction (i.e. con-
vergence). For NB, the steps used by EM are iden-
tical to those used to build the classifier (equations 
(3) for the E-step, and equations (1) and (2) for the 
? ?
?
= =
=
+
+= |V|
s
|D|
i ijis
|D|
i ijit
jt
d|cd,wN|V|
d|cd,wN
c|w
1 1
1
))Pr((
))Pr((1
)?r(
?
1. Each document in P is assigned the class label 1; 
2. Each document in RN is assigned the class label ?1; 
3. Learn an initial NB classifier f from P and RN, us-
ing Equations (1) and (2); 
4. Repeat 
5. For each document di in U-RN do     // E-Step 
6. Using the current classifier f  compute 
Pr(cj|di) using Equation (3); 
7. Learn a new NB classifier f from P, RN and U-
RN by computing Pr(cj) and Pr(wt|cj), using 
Equations (1) and (2);                       // M-Step 
8. Until the classifier parameters stabilize  
9. The last iteration of EM gives the final classifier f ; 
10. For each document di in U do  
11. If its probability Pr(+|di) ? 0.5 then 
12. Output di as a positive document; 
13. else Output di as a negative document 
Figure 2. EM algorithm with the NB classifier 
||
)|(r
)(r
||
1
D
dc
c
D
i ij
j
? = ?=?
? ?
?
= =
=
??
??=?
||
1
||
1 ,
||
1 ,
)|(r)(r
)|(r)(r
)|(r
C
r
d
k rkdr
d
k jkdj
ij i
i
i
i
cwc
cwc
dc
221
M-step). In EM, Pr(cj|di) takes the value in [0, 1] 
instead of {0, 1} in all the three equations.                                                                               
The algorithm for the second step of S-EM is 
given in Figure 2. Lines 1-3 build a NB classifier f 
using P and RN. Lines 4-8 run EM until conver-
gence. Finally, the converged classifier is used to 
classify the unlabeled set U (lines 10-13).     
3.2 Proposed CR-SVM  
As we will see in the experiment section, the per-
formance of S-EM can be weak in some cases. 
This is due to the mixture model assumption of its 
NB classifier (Nigam et al 2000), which requires 
that the mixture components and classes be of one-
to-one correspondence. Intuitively, this means that 
each class should come from a distinctive distribu-
tion rather than a mixture of multiple distributions. 
In our setting, however, the negative class often 
has documents of mixed topics, e.g., representing 
the broad class of everything else except the top-
ic(s) represented by the positive class.  
There are some existing PU learning methods 
based on SVM which can deal with this problem, 
e.g., Roc-SVM (Li and Liu, 2003). Like S-EM, 
Roc-SVM also has two steps. The first step uses 
Rocchio classification (Rocchio, 1971) to find a set 
of reliable negatives RN from U. In particular, this 
method treats the entire unlabeled set U as negative 
documents and then uses the positive set P and the 
unlabeled set U as the training data to build a Roc-
chio classifier. The classifier is subsequently ap-
plied to classify the unlabeled set U. Those 
documents that are classified as negative are then 
considered as reliable negative examples RN. The 
second step of Roc-SVM runs SVM iteratively 
(instead of EM). Unlike NB, SVM does not make 
any distributional assumption. 
However, Roc-SVM does not do well due to the 
weakness of its first step in finding a good set of 
reliable negatives RN. This motivates us to propose 
a new SVM based method CR-SVM to detect a 
better quality RN set. The second step of CR-SVM 
is similar to that in Roc-SVM.  
Step 1: Extracting reliable negatives RN from U 
using Cosine and Rocchio  
The first step of the proposed CR-SVM algorithm 
for finding a RN set consists of two sub-steps:  
Sub-step 1 (extracting the potential negative set 
PN using the cosine similarity): Given the positive 
set P and the unlabeled set U, we extract a set of 
potential negatives PN from U by computing the 
similarities of the unlabeled documents in U and 
the positive documents in P. The idea is that those 
documents in U that are very dissimilar to the doc-
uments in P are likely to be negative.  
1. PN = ?;  
2. Represent each document in P and U as vectors us-
ing the TF-IDF representation; 
3. For each dj ? P do 
4.  
5. ; 
6. For each dj ? P  do 
7. compute cos(pr, dj) using Equation (4); 
8. Sort all the documents dj?P according to cos(pr, dj) 
in decreasing order; 
9. ? = cos(pr, dp) where dp is ranked in the position of 
(1- l)*|P|; 
10. For each di ? U  do 
11. If cos(pr, di)< ? then 
12. PN = PN ?{di} 
Figure 3. Extracting potential negatives PN from U 
The detailed algorithm is given in Figure 3. 
Each document in P and U is first represented as a 
vector d = (q1, q2, ?, qn) using the TF-IDF scheme 
(Salton 1986). Each element qi (i=1, 2, ?, n) in d 
represents a word feature wi. A positive representa-
tive vector (pr) is built by summing up the docu-
ments in P and normalizing it (lines 3-5). Lines 6-7 
compute the similarities of each document dj in P 
with pr using the cosine similarity, cos(pr, dj).  
Line 8 sorts the documents in P according to 
their cos(pr, dj) values. We want to filter away as 
many as possible hidden positive documents from 
U so that we can obtain a very pure negative set.  
Since the hidden positives in U should have the 
same behaviors as the positives in P in terms of 
their similarities to pr, we set their minimum simi-
larity as the threshold value ? which is the mini-
mum similarity before a document is considered as 
a potential negative document: 
           
Pjj
P
j
?= = ddpr  ),,(cosmin
||
1
?
 
(4) 
In a noiseless scenario, using the minimum simi-
larity is acceptable. However, most real-life appli-
cations contain outliers and noisy artifacts. Using 
the absolute minimum similarity may be unrelia-
ble; the similarity cos(pr, dj) of an outlier docu-
2
||
1 ||||
*
||
1 ?
=
=
P
j j
j
P d
d
pr
2 ||||/ prprpr =
222
ment dj in P could be near 0 or smaller than most 
(or even all) negative documents. It would there-
fore be prudent to ignore a small percentage l of 
the documents in P most dissimilar to the repre-
sentative positive (pr) and assume them as noise or 
outliers.  Since we do not know the noise level of 
the data, to be safe, we use a noise level l = 5% as 
the default. The final classification result is not 
sensitive to l as long as it is not too big. In line 9, 
we use the noise level l to decide on a suitable ?. 
Then, for each document di in U, if its cosine simi-
larity cos(pr, di) < ?, we regard it as a potential 
negative and store it in PN (lines 10-12). 
Our experiment results showed that PN is still 
not sufficient or big enough for accurate PU learn-
ing. Thus, we need to do a bit more work to find 
the final RN.   
Sub-step 2 (extracting the final reliable negative 
set RN from U using Rocchio with PN): At this 
point, we have a positive set P and a potential neg-
ative set PN where PN is a purer negative set than 
U. To extract the final reliable negatives, we em-
ploy the Rocchio classification to build a classifier 
RC using P and PN (We do not use SVM here as it 
is very sensitive to the noise in PN). Those docu-
ments in U that are classified as negatives by RC 
will then be regarded as reliable negatives, and 
stored in set RN.   
The algorithm for this sub-step is given in Fig-
ure 4. Following the Rocchio formula, a positive 
and a negative prototype vectors p and n are built 
(lines 3 and 4), which are used to classify the doc-
uments in U (lines 5-7). ? and ? are parameters for 
adjusting the relative impact of the positive and 
negative examples. In this work, we use ? = 16 and 
? = 4 as recommended in (Buckley et al 1994).  
Step 2:  Learning by running SVM iteratively 
This step is similar to that in Roc-SVM, building 
the final classifier by running SVM iteratively with 
the sets P, RN and the remaining unlabeled set Q 
(Q = U ? RN).  
The algorithm is given in Figure 5. We run 
SVM classifiers Si (line 3) iteratively to extract 
more and more negative documents from Q. The 
iteration stops when no more negative documents 
can be extracted from Q (line 5). There is, howev-
er, a danger in running SVM iteratively, as SVM is 
quite sensitive to noise. It is possible that during 
some iteration, SVM is misled by noisy data to 
extract many positive documents from Q and put 
them in the negative set RN. If this happens, the 
final SVM classifier will be inferior. As such, we 
employ a test to decide whether to keep the first 
SVM classifier or the final one. To do so, we use 
the final SVM classifier obtained at convergence 
(called Slast, line 9) to classify the positive set P to 
see if many positive documents in P are classified 
as negatives. Roc-SVM chooses 5% as the thre-
shold, so CR-SVM also uses this threshold. If there 
are 5% of positive documents (5%*|P|) in P that 
are classified as negative, it indicates that SVM has 
gone wrong and we should use the first SVM clas-
sifier (S1). In our experience, the first classifier is 
always quite strong; good results can therefore be 
achieved even without catching the last (possibly 
better) classifier.  
The main difference between Roc-SVM and 
CR-SVM is that Roc-SVM does not produce PN. It 
simply treats the unlabeled set U as negatives for 
extracting RN. Since PN is clearly a purer negative 
set than U, the use of PN by CR-SVM helps ex-
tract a better quality reliable negative set RN which 
subsequently allows the final classifier of CR-
SVM to give better results than Roc-SVM.   
Note that the methods (S-EM and CR-SVM) are 
all two-step algorithms in which the first step and 
the second step are independent of each other. The 
algorithm for the second step basically needs a 
good set of reliable negatives RN extracted from U. 
This means that one can pick any algorithm for the 
first step to work with any algorithm for the second 
step. For example, we can also have CR-EM which 
uses the algorithm (shown in Figures 3 and 4) of 
the first step of CR-SVM to combine with the al-
gorithm of the second step of S-EM. CR-EM ac-
tually works quite well as it is also able to exploit 
the more accurate reliable negative set RN ex-
tracted using cosine and Rocchio. 
 
1. RN = ?;  
2. Represent each document in P, PN and U as vectors 
using the TF-IDF representation; 
3. ; 
4. ; 
5. For each di ? U  do 
6. If  cos(di, n)> cos(di, p) then 
7. RN  = RN ?{di} 
Figure 4. Identifying RN using the Rocchio classifier 
??
??
?=
PN i
i
P j
j
ij
PNP dd d
d
d
d
p
||||||
1
||||||
1 ??
??
??
?=
P j
j
PN i
i
ji
PPN dd d
d
d
d
n
||||||
1
||||||
1 ??
223
4 Empirical Evaluation 
We now present the experimental results to support 
our claim that negative training data is not needed 
and can even harm text classification. We also 
show the effectiveness of the proposed PU learning 
methods CR-SVM and CR-EM. The following 
methods are compared: (1) traditional supervised 
learning methods SVM and NB which use both 
positive and negative training data; (2) PU learning 
methods, including two existing methods S-EM 
and Roc-SVM and two new methods CR-SVM and 
CR-EM, and (3) one-class SVM (Sch?lkop et al, 
1999) where only positive training data is used in 
learning (the unlabeled set is not used at all).  
We used LIBSVM 1  for SVM and one-class 
SVM, and two publicly available 2  PU learning 
techniques S-EM and Roc-SVM. Note that we do 
not compare with some other PU learning methods 
such as those in (Liu et al 2003, Lee and Liu, 2003 
and Elkan and Noto, 2008) as the purpose of this 
paper is not to find the best PU learning method 
but to show that PU learning can address our spe-
cial sample selection bias problem. Our current 
methods already do very well for this purpose.  
4.1 Datasets and Experimental Settings 
We used two well-known benchmark data collec-
tions for text classification, the Reuters-21578 col-
lection 3  and the 20 Newsgroup collection 4 . 
Reuters-21578 contains 21578 documents. We 
used the most populous 10 out of the 135 catego-
ries following the common practice of other re-
searchers. 20 Newsgroup has 11997 documents 
from 20 discussion groups. The 20 groups were 
also categorized into 4 main categories.  
We have performed two sets of experiments, 
and just used bag-of-words as features since our 
objective in this paper is not feature engineering.  
(1) Test set has other topic documents. This set 
of experiments simulates the scenario in which the 
negative training and test samples have different 
distributions. We select positive, negative and oth-
er topic documents for Reuters and 20 Newsgroup, 
and produce various data sets. Using these data 
sets, we want to show that PU learning can do bet-
                                                          
1 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
2 http://www.cs.uic.edu/~liub/LPU/LPU-download.html 
3 http://www.research.att.com/~lewis/reuters21578.html 
4 http://people.csail.mit.edu/jrennie/20Newsgroups/ 
ter than traditional learning that uses both positive 
and negative training data. 
For the Reuters collection, each of the 10 cate-
gories is used as a positive class. We randomly 
select one or two of the remaining categories as the 
negative class (denoted by Neg 1 or Neg 2), and 
then we randomly choose some documents from 
the rest of the categories as other topic documents. 
These other topic documents are regarded as nega-
tives and added to the test set but not to the nega-
tive training data. They thus introduce a different 
distribution to the negative test data. We generated 
20 data sets (10*2) for our experiments this way. 
The 20 Newsgroup collection has 4 main cate-
gories with sub-categories5; the sub-categories in 
the same main category are relatively similar to 
each other. We are able to simulate two scenarios: 
(1) the other topic documents are similar to the 
negative class documents (similar case), and (2) 
the other topic documents are quite different from 
the negative class documents (different case). This 
allows us to investigate whether the classification 
results will be affected when the other topic docu-
ments are somewhat similar or vastly different 
from the negative training set. To create the train-
ing and test data for our experiments, we randomly 
select one sub-category from a main category (cat 
1) as the positive class, and one (or two) sub-
category from another category (cat 2) as the nega-
tive class (again denoted by Neg 1 or Neg 2). For 
the other topics, we randomly choose some docu-
                                                          
5  The four main categories and their corresponding sub-
categories are: computer (graphics, os, ibmpc.hardware, 
mac.hardware, windows.x), recreation (autos, motorcycles, 
baseball, hockey), science (crypt, electronics, med, space), and 
talk (politics.misc, politics.guns, politics.mideast, religion). 
1. Every document in P is assigned the class label +1; 
2. Every document in RN is assigned the label ?1; 
3. Use P and RN  to train a SVM classifier Si, with i = 
1 initially and i = i+1 with each iteration (line 3-7);  
4. Classify Q using Si. Let the set of documents in Q 
that are classified as negative be W;  
5. If (W = ?) then  stop; 
6. else Q = Q ? W; 
7. RN = RN ?W 
8. goto (3); 
9. Use the last SVM classifier Slast to classify P; 
10. If more than 5% positives are classified as negative  
11. then use S1 as the final classifier; 
12. else use Slast as the final classifier; 
Figure 5.  Constructing the final classifier using SVM 
224
ments from the remaining sub-categories of cat 2 
for the similar case, and some documents from a 
randomly chosen different category (cat 3) (as the 
other topic documents) for the different case. We 
generated 8 data sets (4*2) for the similar case, 
and 8 data sets (4*2) for the different case.   
The training and test sets are then constructed as 
follows: we partition the positive (and similarly for 
the negative) class documents into two standard 
subsets: 70% for training and 30% for testing. In 
order to create different experimental settings, we 
vary the number of the other topic documents that 
are added to the test set as negatives, controlled by 
a parameter ?, which is a percentage of |TN|, where 
|TN| is the size of the negative test set without the 
other topic documents. That is, the number of oth-
er topic documents added is ? ? |TN|.  
(2) Test set has no other topic documents. This 
set of experiments is the traditional classification 
in which the training and test data have the same 
distribution. We employ the same data sets as in 
(1) but without having any other topic documents 
in the test set. Here we want to show that PU learn-
ing can do equally well without using the negative 
training data even in the traditional setting.  
4.2 Results with Other Topic Documents in 
Test Set 
We show the results for experiment set (1), i.e. the 
distributions of the negative training and test data 
are different (caused by the inclusion of other topic 
documents in the test set, or the addition of other 
topic documents to complement existing negatives 
in the test set). The evaluation metric is the F-score 
on the positive class (Bollmann and Cherniavsky, 
1981), which is commonly used for evaluating text 
classification.  
4.2.1  Results on the Reuters data 
Figure 6 shows the comparison results when the 
negative class contains only one category of doc-
uments (Neg 1), while Figure 7 shows the results 
when the negative class contains documents from 
two categories (Neg 2) in the Reuters collection. 
The data points in the figures are the averages of 
the results from the corresponding datasets.  
Our proposed method CR-SVM is shown to per-
form consistently better than the other techniques. 
When the size of the other topic documents (x-
axis) in the test set increases, the F-scores of the 
two traditional learning methods SVM and NB 
decreased much more dramatically as compared 
with the PU learning techniques. The traditional 
learning models were clearly unable to handle dif-
ferent distributions for training and test data. 
Among the PU learning techniques, the proposed 
CR-SVM gave the best results consistently. Roc-
SVM did not do consistently well as it did not 
manage to find high quality reliable negatives RN 
sometimes. The EM based methods (CR-EM and 
S-EM) performed well in the case when we had 
only one negative class (Figure 6). However, it did 
not do well in the situation where there were two 
negative classes (Figure 7) due to the underlying 
mixture model assumption of the na?ve Bayesian 
classifier. One-class SVM (OSVM) performed 
poorly because it did not exploit the useful infor-
mation in the unlabeled set at all.   
 
Figure 6. Results of Neg 1 using the Reuter data 
 
Figure 7. Results of Neg 2 using the Reuter data 
4.2.2  Results on 20 Newsgroup data 
Recall that for the 20 Newsgroup data, we have 
two settings: similar case and different case.  
Similar case: Here, the other topic documents are 
0.6
0.7
0.8
0.9
1
10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
a%*|TN | of other topic documents
F
-s
co
re
SVM NB OSVM S-EM
Roc-SVM CR-EM CR-SVM
0.5
0.6
0.7
0.8
0.9
1
10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
a%*|TN | of other topic documents
F
-s
co
re
SVM NB OSVM S-EM
Roc-SVM CR-EM CR-SVM
225
similar to the negative class documents, as they 
belong to the same main category.  
The comparison results are given in Figure 8 
(Neg 1) and Figure 9 (Neg 2). We observe that 
CR-EM, S-EM and CR-SVM all performed well. 
EM based methods (CR-EM and S-EM) have a 
slight edge over CR-SVM. Again, the F-scores of 
the traditional supervised learning (SVM and NB) 
deteriorated when more other topic documents 
were added to the test set, while CR-EM, S-EM 
and CR-SVM were able to remain unaffected and 
maintained roughly constant F-scores. When the 
negative class contained documents from two cate-
gories (Neg 2), the F-scores of the traditional 
learning dropped even more rapidly. Both Roc-
SVM and One-class SVM (OSVM) performed 
poorly, due to the same reasons given previously.  
 
Figure 8. Results of Neg 1, similar case ? using the 20 
Newsgroup data 
 
Figure 9. Results of Neg 2, similar case ? using the 20 
Newsgroup data 
Different case: In this case, the other topic docu-
ments are quite different from the negative class 
documents, since they are originated from different 
main categories.  
The results are shown in Figures 10 (Neg 1) and 
11 (Neg 2). The trends are similar to those for the 
similar case, except that the performance of the 
traditional supervised learning methods (SVM and 
NB) dropped even more rapidly with more other 
topic documents. As the other topic documents 
have very different distributions from the negatives 
in the training set in this case, they really confused 
the traditional classifiers. In contrast, the three PU 
learning techniques were still able to perform con-
sistently well, regardless of the number of other 
topic documents added to the test data.  
 
Figure 10. Results of Neg 1, different case ? using the 
20 Newsgroup data  
 
Figure 11.  Results of Neg 2, different case ? using the 
20 Newsgroup data 
In summary, the results showed that learning 
with negative training data based on the traditional 
paradigm actually harms classification when the 
identical distribution assumption does not hold.  
4.3 Results without Other Topic Documents in 
Test Set 
Given an application, one may not know whether 
0.6
0.7
0.8
0.9
1
10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
a%*|TN | of other topic documents
F
-s
co
re
SVM NB OSVM S-EM
Roc-SVM CR-EM CR-SVM
0.6
0.7
0.8
0.9
1
10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
a%*|TN | of other topic documents
F
-s
co
re
SVM NB OSVM S-EM
Roc-SVM CR-EM CR-SVM
0.6
0.7
0.8
0.9
1
10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
a%*|TN| of other topic documents
F
-s
co
re
SVM NB OSVM S-EM
Roc-SVM CR-EM CR-SVM
0.6
0.7
0.8
0.9
1
10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
a%*|TN| of other topic documents
F
-s
co
re
SVM NB OSVM S-EM
Roc-SVM CR-EM CR-SVM
226
the identical distribution assumption holds. The 
above results showed that PU learning is better 
when it does not hold.  How about when the as-
sumption does hold? To find out, we compared the 
results of SVM, NB, and three PU learning me-
thods using the datasets without any other topic 
documents added to the test set. In this case, the 
training and test data distributions are the same. 
Table 1 shows the results for this scenario. Note 
that for PU learning, the negative training data 
were not used. The traditional supervised learning 
techniques (SVM and NB), which made full use of 
the positive and negative training data, only per-
formed just about 1-2% better than the PU learning 
method CR-SVM (which is not statistically signifi-
cant based on paired t-test). This suggests that we 
can do away with negative training data, since PU 
learning can perform equally well without them.  
This has practical importance since the full cover-
age of negative training data is hard to find and to 
label in many applications. 
From the results in Figures 6?11 and Table 1, 
we can conclude that PU learning can be used for 
binary text classification without the negative 
training data (which can be harmful for the task). 
CR-SVM is our recommended PU learning method 
based on its generally consistent performance. 
Table 1. Comparison of methods without other docu-
ments in test set 
 Methods 
Reuters 
 (Neg 1) 
Reuters 
 (Neg 2) 
20News 
(Neg 1) 
20News 
(Neg 2) 
SVM 0.971 0.964 0.988 0.990 
NB 0.972 0.947 0.988 0.992 
S-EM 0.952 0.921 0.974 0.975 
CR-EM 0.955 0.897 0.983 0.986 
CR-SVM 0.960 0.959 0.967 0.974 
5 Conclusions 
This paper studied a special case of the sample se-
lection bias problem in which the positive training 
and test distributions are the same, but the negative 
training and test distributions may be different. We 
showed that in this case, the negative training data 
should not be used in learning, and PU learning 
can be applied to this setting. A new PU learning 
algorithm (called CR-SVM) was also proposed to 
overcome the weaknesses of the current two-step 
algorithms.  
 Our experiments showed that the traditional 
classification methods suffered greatly when the 
distributions are different for the negative training 
and test data, but PU learning does not. We also 
showed that PU learning performed equally well in 
the ideal case where the training and test data have 
identical distributions. As such, it can be advanta-
geous to discard the potentially harmful negative 
training data and use PU learning for classification.  
 In our future work, we plan to do more compre-
hensive experiments to compare the classic super-
vised learning and PU learning techniques with 
different kinds of settings, for example, by varying 
the ratio between positive and negative examples, 
as well as their sizes. It is also important to explore 
how to catch the best iteration of the SVM/NB 
classifier in the iterative running process of the 
algorithms. Finally, we would like to point out that 
it is conceivable that negative training data could 
still be useful in many cases. An interesting direc-
tion to explore is to somehow combine the ex-
tracted reliable negative data from the unlabeled 
set and the existing negative training data to further 
enhance learning algorithms.  
 
References  
Agirre E., Lacalle L.O. 2009. Supervised Domain Adap-
tion for WSD. Proceedings of the 12th Conference of 
the European Chapter for Computational Linguistics 
(EACL09), pp 42-50.  
Andrew A., Nallapati R., Cohen W., 2008. Exploiting 
Feature Hierarchy for Transfer Learning in Named 
Entity Recognition, ACL. 
Bickel, S., Bruckner, M., and Scheffer. 2009. T. Dis-
criminative learning under covariate shift. Journal of 
Machine Learning Research.  
Bickel S. and Scheffer T. 2007. Dirichlet-enhanced 
spam filtering based on biased samples. In Advances 
in Neural Information Processing Systems. 
Bollmann, P.,& Cherniavsky, V. 1981. Measurement-
theoretical investigation of the mz-metric. Informa-
tion Retrieval Research. 
Buckley, C., Salton, G., & Allan, J. 1994. The effect of 
adding relevance information in a relevance feed-
back environment, SIGIR. 
Blum, A. and Mitchell, T. 1998. Combining labeled and 
unlabeled data with co-training. In Proc. of Compu-
tational Learning Theory, pp. 92?10. 
Chan Y. S., Ng H. T. 2007. Domain Adaptation with 
Active Learning for Word Sense Disambiguation, 
ACL. 
Dempster A., Laird N. and Rubin D.. 1977. Maximum 
likelihood from incomplete data via the EM algorithm, 
Journal of the Royal Statistical Society. 
Denis F., PAC learning from positive statistical queries. 
ALT, 1998. 
227
Denis F., Laurent A., R?mi G., Marc T. 2003. Text clas-
sification and co-training from positive and unlabeled 
examples. ICML. 
Denis, F, R?mi G, and Marc T. 2002. Text Classifica-
tion from Positive and Unlabeled Examples. In Pro-
ceedings of the 9th International Conference on 
Information Processing and Management of Uncer-
tainty in Knowledge-Based Systems. 
Downey, D., Broadhead, M. and Etzioni, O. 2007. Lo-
cating complex named entities in Web Text. IJCAI.  
Dudik M., Schapire R., and Phillips S. 2005. Correcting 
sample selection bias in maximum entropy density 
estimation. In Advances in Neural Information Proc-
essing Systems.  
Elkan, C. and Noto, K. 2008. Learning classifiers from 
only positive and unlabeled data. KDD, 213-220.  
Goldwasser, D., Roth D. 2008. Active Sample Selection 
for Named Entity Transliteration, ACL. 
Heckman J. 1979. Sample selection bias as a specifica-
tion error. Econometrica, 47:153?161. 
Huang J., Smola A., Gretton A., Borgwardt K., and 
Scholkopf B. 2007. Correcting sample selection bias 
by unlabeled data. In Advances in Neural Informa-
tion Processing Systems. 
Jiang J. and Zhai C. X. 2007. Instance Weighting for 
Domain Adaptation in NLP, ACL. 
Lee, W. S. and Liu, B. 2003. Learning with Positive and 
Unlabeled Examples Using Weighted Logistic Re-
gression. ICML.  
Lewis D. 1995. A sequential algorithm for training text 
classifiers: corrigendum and additional data. SIGIR 
Forum, 13-19. 
Li, S., Zong C., 2008. Multi-Domain Sentiment Classifi-
cation, ACL. 
Li, X., Liu, B. 2003. Learning to classify texts using 
positive and unlabeled data, IJCAI. 
Li, X., Liu, B., 2005. Learning from Positive and Unla-
beled Examples with Different Data Distributions. 
ECML. 
Li, X., Liu, B., 2007. Learning to Identify Unexpected 
Instances in the Test Set. IJCAI.  
Li, X., Yu, P. S., Liu B., and Ng, S. 2009. Positive 
Unlabeled Learning for Data Stream Classification, 
SDM. 
Li, X., Zhang L., Liu B., and Ng, S. 2010. Distribution-
al Similarity vs. PU Learning for Entity Set Expan-
sion, ACL. 
Liu, B, Dai, Y., Li, X., Lee, W-S., and Yu. P. 2003. 
Building text classifiers using positive and unlabeled 
examples. ICDM, 179-188. 
Liu, B, Lee, W-S, Yu, P. S, and Li, X. 2002. Partially 
supervised text classification. ICML, 387-394. 
Nigam, K., McCallum, A., Thrun, S. and Mitchell, T. 
2000. Text classification from labeled and unlabeled 
documents using EM. Machine Learning, 39(2/3), 
103?134.  
Pan, S. J. and Yang, Q. 2009. A survey on transfer 
learning. IEEE Transactions on Knowledge and Da-
ta Engineering, Vol. 99, No. 1. 
Rocchio, J. 1971. Relevant feedback in information 
retrieval. In G. Salton (ed.). The smart retrieval sys-
tem: experiments in automatic document processing, 
Englewood Cliffs, NJ, 1971.Sagae K., Tsujii J. 2008. 
Online Methods for Multi-Domain Learning and 
Adaptation, EMNLP. 
Salton G. and McGill M. J. 1986. Introduction to Mod-
ern Information Retrieval.  
Sch?lkop f B., Platt J.C., Shawe-Taylor J., Smola A.J., 
and Williamson R.C. 1999. Estimating the support 
of a high-dimensional distribution. Technical report, 
Microsoft Research, MSR-TR-99-87. 
Shimodaira H. 2000. Improving predictive inference 
under covariate shift by weighting the log-likelihood 
function. Journal of Statistical Planning and Infer-
ence, 90:227?244. 
Sugiyama M. and Muller K.-R. 2005. Input-dependent 
estimation of generalization error under covariate 
shift. Statistics and Decision, 23(4):249?279. 
Sugiyama M., Nakajima S., Kashima H., von Bunau P., 
and Kawanabe M. 2008. Direct importance estima-
tion with model selection and its application to co-
variate shift adaptation. In Advances in Neural 
Information Processing Systems. 
Tsuboi J., Kashima H., Hido S., Bickel S., and Sugi-
yama M. 2008. Direct density ratio estimation for 
large-scale covariate shift adaptation. In Proceed-
ings of the SIAM International Conference on Data 
Mining, 2008. 
Wu D., Lee W.S., Ye N. and Chieu H. L. 2009. Domain 
adaptive bootstrapping for named entity recognition, 
ACL. 
Wu Q., Tan S. and Cheng X. 2009. Graph Ranking for 
Sentiment Transfer, ACL. 
Yang Q., Chen Y., Xue G., Dai W., Yu Y. 2009. Hete-
rogeneous Transfer Learning for Image Clustering 
via the SocialWeb, ACL  
Yu, H., Han, J., K. Chang. 2002. PEBL: Positive exam-
ple based learning for Web page classification using 
SVM. KDD, 239-248. 
Zadrozny B. 2004. Learning and evaluating classifiers 
under s ample selection bias, ICML.  
Zhou Z., Gao J., Soong F., Meng H. 2006. A Compara-
tive Study of Discriminative Methods for Reranking 
LVCSR N-best Hypotheses in Domain Adaptation 
and Generalization. ICASSP. 
228
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 810?819,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Taxonomy Construction Using Syntactic Contextual Evidence
Luu Anh Tuan
#1
, Jung-jae Kim
#2
, Ng See Kiong
?3
#
School of Computer Engineering, Nanyang Technological University, Singapore
1
anhtuan001@e.ntu.edu.sg,
2
jungjae.kim@ntu.edu.sg
?
Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore
3
skng@i2r.a-star.edu.sg
Abstract
Taxonomies are the backbone of many
structured, semantic knowledge resources.
Recent works for extracting taxonomic
relations from text focused on collect-
ing lexical-syntactic patterns to extract the
taxonomic relations by matching the pat-
terns to text. These approaches, however,
often show low coverage due to the lack of
contextual analysis across sentences. To
address this issue, we propose a novel ap-
proach that collectively utilizes contextual
information of terms in syntactic struc-
tures such that if the set of contexts of
a term includes most of contexts of an-
other term, a subsumption relation be-
tween the two terms is inferred. We ap-
ply this method to the task of taxonomy
construction from scratch, where we intro-
duce another novel graph-based algorithm
for taxonomic structure induction. Our ex-
periment results show that the proposed
method is well complementary with previ-
ous methods of linguistic pattern matching
and significantly improves recall and thus
F-measure.
1 Introduction
Taxonomies that are backbone of structured on-
tology knowledge have been found to be use-
ful for many areas such as question answering
(Harabagiu et al., 2003), document clustering
(Fodeh et al., 2011) and textual entailment (Gef-
fet and Dagan, 2005). There have been an in-
creasing number of hand-crafted, well-structured
taxonomies publicly available, including WordNet
(Miller, 1995), OpenCyc (Matuszek et al., 2006),
and Freebase (Bollacker et al., 2008). However,
the manual curation of those taxonomies is time-
consuming and human experts may miss relevant
terms. As such, there are still needs to extend ex-
isting taxonomies or even to construct new tax-
onomies from scratch.
The previous methods for identifying taxo-
nomic relations (i.e. is-a relations) from text can
be generally classified into two categories: statis-
tical and linguistic approaches. The former in-
cludes co-occurrence analysis (Budanitsky, 1999),
term subsumption (Fotzo and Gallinari, 2004) and
clustering (Wong et al., 2007). The main idea be-
hinds these techniques is that the terms that fre-
quently co-occur may have taxonomic relation-
ships. Such approaches, however, usually suffer
from low accuracy, though relatively high cover-
age, and heavily depend on the choice of feature
types and datasets. Most previous methods of the
linguistic approach, on the other hand, rely on the
lexical-syntactic patterns (e.g. A is a B, A such as
B) (Hearst, 1992). Those patterns can be manu-
ally created (Kozareva et al., 2008; Wentao et al.,
2012), chosen via automatic bootstrapping (Wid-
dows and Dorow, 2002; Girju et al., 2003) or iden-
tified from machine-learned classifiers (Navigli et
al., 2011). The pattern matching methods gen-
erally achieve high precision, but low coverage
due to the lack of contextual analysis across sen-
tences. In this paper, we introduce a novel statisti-
cal method and shows that when combined with a
pattern matching method, it shows significant per-
formance improvement.
The proposed statistical method, called syntac-
tic contextual subsumption (SCS), compares the
syntactic contexts of terms for the taxonomic re-
lation identification, instead of the usage of bag-
of-words model by the previous statistical meth-
ods. We observe that the terms in taxonomic rela-
tions may not occur in the same sentences, but in
similar syntactic structures of different sentences,
and that the contexts of a specific term are often
found in the contexts of a general term but not vice
versa. By context of a term, we mean the set of
words frequently have a particular syntactic rela-
tion (e.g. Subject-Verb-Object) with the term in a
810
given corpus. Given two terms, the SCS method
collects from the Web pre-defined syntactic rela-
tions of each of the terms and checks if the syntac-
tic contexts of a term properly includes that of the
other term in order to determine their taxonomic
relation. The method scores each taxonomic rela-
tion candidate based on the two measures of Web-
based evidence and contextual set inclusion, and
as such, is able to find implicit subsumption rela-
tions between terms across sentences. The SCS
shows itself (Section 3.1) to be complementary to
linguistic pattern matching.
After the relation identification, the identified
taxonomic relations should be integrated into a
graph for the task of taxonomy construction from
scratch or associated with existing concepts of a
given taxonomy via is-a relations (Snow et al.,
2006). In this step of taxonomic structure con-
struction, there is a need for pruning incorrect
and redundant relations. Previous methods for the
pruning task (Kozareva and Hovy, 2010; Velardi et
al., 2012) treat the identified taxonomic relations
equally, and the pruning task is thus reduced to
finding the best trade-off between path length and
the connectivity of traversed nodes. This assump-
tion, however, is not always true due to the fact
that the identified taxonomic relations may have
different confidence values, and the relations with
high confidence values can be incorrectly elimi-
nated during the pruning process. We thus propose
a novel method for the taxonomy induction by uti-
lizing the evidence scores from the relation iden-
tification method and the topological properties of
the graph. We show that it can effectively prune
redundant edges and remove loops while preserv-
ing the correct edges of taxonomy.
We apply the proposed methods of taxonomic
relation identification and taxonomy induction to
the task of constructing a taxonomy from a given
text collection from scratch. The resultant system
consists of three modules: Term extraction and
filtering (Section 2.1), taxonomic relation iden-
tification (Section 2.2), and taxonomy induction
(Section 2.3). The outputs of the term extrac-
tion/filtering module are used as inputs of the tax-
onomic relation identification, such that the tax-
onomic relation identification module checks if
there is a taxonomic relation between each pair
of terms from the term extraction/filtering module.
The taxonomy induction module gets the identi-
fied taxonomic relation set as the input, and out-
puts the final optimal taxonomy by pruning redun-
dant and incorrect relations.
2 Methodology
2.1 Term Extraction and Filtering
The first step to construct taxonomies is to col-
lect candidate terms from text documents in the
domain of interest. Like most of linguistic ap-
proaches, we use pre-defined linguistic filters to
extract candidate terms, including single-word
terms and multi-word terms which are noun or
noun phrases in sentences. These terms are
then preprocessed by removing determiners and
lemmatization.
The candidate terms collected are then filtered
to select the terms that are most relevant to the
domain of interest. Many statistical techniques
are developed for the filtering, such as TF -IDF ,
domain relevance (DR), and domain consensus
(DC) (Navigli and Velardi, 2004). DR measures
the amount of information that a term t captures
within a domain of interest D
i
, compared to other
contrasting domains (D
j
), whileDC measures the
distributed use of a term t across documents d in
a domain D
i
. Since three measures have pros and
cons, and might be complementary to each other,
our term filtering method is thus the linear combi-
nation of them:
TS(t,D
i
) = ?? TFIDF (t,D
i
)
+ ? ?DR(t,D
i
) + ? ?DC(t,D
i
)
(1)
We experimented (see Section 3) with different
values of ?, ? and ?, and found that the method
shows the best performance when the values for ?
and ? are 0.2 and 0.8 and the value for ? is be-
tween 0.15 and 0.35, depending on the size of the
domain corpus.
2.2 Taxonomic Relation Identification
In this section, we present three taxonomic rela-
tion identification methods which are adopted in
our system. First, two methods of string inclusion
with WordNet and lexical-syntactic pattern match-
ing, which were commonly used in the literature
will be introduced with some modifications. Then,
a novel syntactic contextual subsumption method
to find implicit relations between terms across sen-
tences by using contextual evidence from syntactic
structures and Web data will be proposed. Finally,
these three methods will be linearly combined to
811
Notation Meaning
t
1
? t
2
t
1
is a hypernym of t
2
t
1
? t
2
t
1
semantically equals or is sim-
ilar to t
2
t
1
?
WN
t
2
t
1
is a direct or inherited hyper-
nym of t
2
according to WordNet
t
1
?
WN
t
2
t
1
and t
2
belong to the same
synset of WordNet
Table 1: Notations
form an integrating solution for taxonomic rela-
tion identification. Given two terms t
1
and t
2
, Ta-
ble 1 summarizes important notations used in this
paper.
2.2.1 String Inclusion with WordNet (SIWN)
One simple way to check taxonomic relation is to
test string inclusion. For example, ?terrorist orga-
nization? is a hypernym of ?foreign terrorist orga-
nization?, as the former is a substring of the lat-
ter. We propose an algorithm to extend the string
inclusion test by using WordNet, which will be
named SIWN. Given a candidate general term t
g
and a candidate specific term t
s
, the SIWN al-
gorithm examines t
g
from left to right (designat-
ing each word in t
g
to be examined as w
g
) to
check if there is any word (w
s
) in t
s
such that
w
g
?
WN
w
s
or w
g
?
WN
w
s
, and identifies
the taxonomic relation between two terms if ev-
ery word of t
g
has a corresponding word in t
s
(with at least one ?
WN
relation). For example,
consider two terms: ?suicide attack? and ?world
trade center self-destruction bombing?. Because
?attack? ?
WN
?bombing? and ?suicide? ?
WN
?self-destruction?, according to SIWN algorithm,
we conclude that ?suicide attack? is the hypernym
of ?world trade center self-destruction bombing?.
Given two terms t
1
and t
2
, the evidence score
for SIWN algorithm is calculated as follows:
Score
SIWN
(t
1
, t
2
) =
{
1 if t1 ? t
2
via SIWN
0 otherwise
(2)
2.2.2 Lexical-syntactic Pattern
Extending the ideas of Kozareva and Hovy (2010)
and Navigli et al. (2011), we propose a method
of extracting taxonomic relations by matching
lexical-syntactic patterns to the Web data.
Definition 1 (Syntactic patterns). Given two terms
t
1
and t
2
, Pat(t
1
, t
2
) is defined as the set of the
following patterns:
? ?t
1
such as t
2
?
? ?t
1
, including t
2
?
? ?t
2
is [a|an] t
1
?
? ?t
2
is a [kind|type] of t
1
?
? ?t
2
, [and|or] other t
1
?
, where t
1
and t
2
are replaced with actual terms
and [a|b] denotes a choice between a and b.
Given candidate general term t
1
and candi-
date specific term t
2
, the lexical-syntactic pattern
(LSP) method works as follows:
1. Submit each phrase in Pat(t
1
, t
2
) to a Web
search engine as a query. The number of
the search results of the query is denoted as
WH(t
1
, t
2
).
2. Calculate the following evidence score:
Score
LSP
(t
1
, t
2
) =
log(WH(t
1
, t
2
))
1 + log(WH(t
2
, t
1
))
(3)
3. If Score
LSP
(t
1
, t
2
) is greater than a thresh-
old value then t
1
? t
2
.
While most lexical-syntactic pattern meth-
ods in the literature only consider the value of
WH(t
1
, t
2
) in checking t
1
? t
2
(Wentao et al.,
2012), we take into account both WH(t
1
, t
2
) and
WH(t
2
, t
1
). The intuition of formula (3) is that if
t1 is a hypernym of t2 then the size of WH(t
1
, t
2
)
will be much larger than that of WH(t
2
, t
1
),
which means the lexical-syntactic patterns are
more applicable for the ordered pair (t
1
, t
2
) than
(t
2
, t
1
).
2.2.3 Syntactic Contextual Subsumption
The LSP method performs well in recognizing
the taxonomic relations between terms in the
sentences containing those pre-defined syntactic
patterns. This method, however, has a major
shortcoming: it cannot derive taxonomic relations
between two terms occurring in two different
sentences. We thus propose a novel syntactic
contextual subsumption (SCS) method which uti-
lizes contextual information of terms in syntactic
structure (i.e. Subject-Verb-Object in this study)
and Web data to infer implicit taxonomic relations
812
between terms across sentences. Note that the
chosen syntactic structure Subject-Verb-Object
is identical to the definition of non-taxonomic
relations in the literature (Buitelaar et al., 2004),
where the Verb indicate non-taxonomic relations
between Subject and Object. In this subsection,
we first present the method to collect those
non-taxonomic relations. Then we present in
detail the ideas of the SCS method and how we
can use it to derive taxonomic relations in practice.
A. Non-taxonomic Relation Identification
Following previous approaches to non-
taxonomic relation identification, e.g. (Ciaramita
et al., 2005), we use the Stanford parser (Klein
and Manning, 2003) to identify the syntactic
structures of sentences and extract triples of
(Subject, Verb, Object), where Subject and Object
are noun phrases.
We further consider the following issues: First,
if a term (or noun phrase) includes a preposition,
we remove the prepositional phrase. However, if
the headword of a term is a quantitative noun like
?lot?, ?many? or ?dozen? and it is modified by the
preposition ?of?, we replace it with the headword
of the object of the preposition ?of?. For example,
we can extract the triples (people, need, food)
and (people, like, snow) from the following sen-
tences, respectively:
? ?People in poor countries need food?
? ?A lot of people like snow?
Second, if the object of a verb is in a verb form,
we replace it with, if any, the object of the em-
bedded verb. For example, we can extract the
triple (soldier, attack, terrorist) from the fol-
lowing sentence:
? ?The soldiers continue to attack terrorists?
Third, if a term has a coordinate structure with
a conjunction like ?and? or ?or?, we split it into all
coordinated noun phrases and duplicate the triple
by replacing the term with each of the coordinated
noun phrases. For example, we can extract the
triples ofR(girl, like, dog) andR(girl, like, cat)
from the following sentence:
? ?The girl likes both dogs and cats?
Given two terms t
1
, t
2
and a non-taxonomic re-
lation r, some notations which will be used here-
after are shown below:
? R(t
1
, r, t
2
): t
1
, r, and t
2
have a (Subject,
Verb, Object) triple.
? ?(t
1
, t
2
): the set of relations r such that there
exists R(t
1
, r, t
2
) or R(t
2
, r, t
1
).
B. Syntactic Contextual Subsumption Method
The idea of the SCS method derived from the
following two observations.
Observation 1. Given three terms t
1
, t
2
, t
3
, and a
non-taxonomic relation r, if we have two triples
R(t
1
, r, t
3
) and R(t
2
, r, t
3
) (or R(t
3
, r, t
1
) and
R(t
3
, r, t
2
)), t
1
and t
2
may be in taxonomic rela-
tion.
For example, given two triples R(Al-Qaeda, at-
tack, American) and R(Terrorist group, attack,
American), a taxonomic relation Terrorist group
? Al-Qaeda can be induced. However, it is not
always guaranteed to induce a taxonomic rela-
tions from such a pair of triples, for example from
R(animal, eat, meat) and R(animal, eat, grass).
The second observation introduced hereafter will
provide more chance to infer taxonomic relation-
ship.
Definition 2 (Contextual set of a term). Given
a term t
1
and a non-taxonomic relation r,
S(t
1
, r, ?subj?) denotes the set of terms t
2
such
that there exists triple R(t
1
, r, t
2
). Similarly,
S(t
1
, r, ?obj?) is the set of terms t
2
such that
there exists triple R(t
2
, r, t
1
).
Observation 2. Given two terms t
1
, t
2
, and a non-
taxonomic relation r, if S(t
1
, r, ?subj?) mostly
contains S(t
2
, r, ?subj?) but not vice versa, then
most likely t
1
is a hypernym of t
2
. Similarly, if
S(t
1
, r, ?obj?) mostly contains S(t
2
, r, ?obj?) but
not vice versa, then most likely t
1
is a hypernym of
t
2
.
For example, assume that S(animal, eat,
?subj?) = {grass, potato, mouse, insects, meat,
wild boar, deer, buffalo} and S(tiger, eat, ?subj?)
= {meat, wild boar, deer, buffalo}. Since
S(animal, eat, ?subj?) properly contains S(tiger,
eat, ?subj?), we can induce animal ? tiger.
Based on Observation 2, our strategy to infer
taxonomic relations is to first find the contextual
set of terms via the evidence of syntactic structures
and Web data, and then compute the score of the
set inclusion. The detail of the method is presented
hereafter.
813
Definition 3. Given two terms t
1
, t
2
and a non-
taxonomic relation r, C(t
1
, t
2
, r, ?subj?) denotes
the number of terms t
3
such that there exists
both triples R(t
1
, r, t
3
) and R(t
2
, r, t
3
). Simi-
larly, C(t
1
, t
2
, r, ?obj?) is the number of terms
t
3
such that there exists both relations R(t
3
, r, t
1
)
and R(t
3
, r, t
2
).
Given the pair of a candidate general term t
1
and a candidate specific term t
2
, we extract their
non-taxonomic relations from corpora extracted
from the Web, and use them to determine the tax-
onomic relation between t
1
and t
2
as follows:
1. Find from a domain corpus the relation r and
type ? such that:
C(t
1
, t
2
, r,?) = max
r
?
??(t
1
,t
2
)
?
?
?{?subj?,?obj?}
C(t
1
, t
2
, r
?
,?
?
)
2. If type ? is ?subj?, collect the first 1,000
search results of the query ?t
1
r? using
the Google search engine, designated as
Corpus
?
t
1
. In the same way, construct
Corpus
?
t
2
with the query ?t
2
r?. If ? is ?obj?,
two queries ?r t
1
? and ?r t
2
? are submitted
instead to collect Corpus
?
t
1
and Corpus
?
t
2
,
respectively.
3. Find the sets of S(t
1
, r,?) and S(t
2
, r,?)
from Corpus
?
t
1
and Corpus
?
t
2
, respectively,
using the non-taxonomic relation identifica-
tion method above.
4. Calculate the following evidence score for
SCS method:
Score
SCS
=
[
|S(t
1
, r,?)
?
S(t
2
, r,?)|
|S(t
2
, r,?)|
+
(
1?
|S(t
1
, r,?)
?
S(t
2
, r,?)|
|S(t
1
, r,?)|
)
]
? log(|S(t
1
, r,?)|+ |S(t
2
, r,?)|)
(4)
The basic idea of the contextual subsumption
score in our method is that if t
1
is a hyper-
nym of t
2
then the set S(t
1
, r,?) will mostly
contain S(t
2
, r,?) but not vice versa. The in-
tuition of formula (5) is inspired by Jaccard
similarity coefficient. We then multiply the
score with the log value of total size of two
sets to avoid the bias of small set inclusion.
5. If Score
SCS
(t
1
, t
2
) is greater than a thresh-
old value, then we have t1 ? t2.
2.2.4 Combined Method
In our study, we linearly combine three methods
as follows:
1. For each ordered pair of terms (t
1
, t
2
) calcu-
late the total evidence score:
Score(t
1
, t
2
) = ?? Score
SIWN
(t
1
, t
2
)
+ ? ? Score
LSP
(t
1
, t
2
)
+ ? ? Score
SCS
(t
1
, t
2
)
(5)
2. If Score(t
1
, t
2
) is greater than a threshold
value, then we have t
1
? t
2
.
We experimented with various combinations of
values for ?, ? and ?, and found that the method
shows the best performance when the value of ? is
0.5, ? is between 0.35 and 0.45, and ? is between
0.15 and 0.25, depending on the domain corpus
size.
2.3 Taxonomy Induction
The output of the taxonomic relation identifica-
tion module is a set of taxonomic relations T .
In this section, we will introduce a graph-based
algorithm (Algorithm 1) to convert this set into
an optimal tree-structured taxonomy, as well as
to eliminate incorrect and redundant relations.
Denote e(t
1
, t
2
) as an directed edge from t
1
to t
2
,
the algorithm consists of three steps which will be
described hereafter with the corresponding lines
in Algorithm 1.
Step 1: Initial hypernym graph creation
(line 1 - 16) This step is to construct a connected
directed graph from the list of taxonomic rela-
tions. The idea is to add each taxonomic relation
t
1
? t
2
as a directed edge from parent node
t
1
to child node t
2
, and if t
1
does not have any
hypernym term, t
1
will become a child node of
ROOT node. The result of this step is a con-
nected graph containing all taxonomic relations
with the common ROOT node.
Step 2: Edge weighting (line 17) This step
is to calculate the weight of each edge in the
hypernym graph. Unlike the algorithm of Velardi
et al. (2012) and Kozareva and Hovy (2010)
where every taxonomic relation is treated equally,
we assume the confidence of each taxonomic
relation is different, depending on the amount of
814
Algorithm 1 Taxonomy Induction Algorithm
Input: T : the taxonomic relation set
Output: V : the vertex set of resultant taxonomy;
E: the edge set of resultant taxonomy;
1: Initialize V = {ROOT}, E = ?;
2: for each taxonomic relation (t
1
? t
2
) ? T do
3: E = E ? {e(t
1
, t
2
)}
4: if t
1
?? V then
5: V = V ? {t
1
}
6: end if
7: if t
2
?? V then
8: V = V ? {t
2
}
9: end if
10: if ? e(t
3
, t
1
) ? E with t
3
?= ROOT then
11: E = E ? {e(ROOT, t
1
)}
12: end if
13: if ? e(ROOT, t
2
) ? E then
14: E = E \ {e(ROOT, t
2
)}
15: end if
16: end for
17: edgeWeighting(V,E);
18: graphPruning(V,E);
evidence it has. Thus, the hypernym graph edges
will be weighted as follows:
w(e(t
1
, t
2
)) =
{
1 if t
1
= ROOT
Score(t
1
, t
2
) otherwise
(6)
Note that the Score value in formula (6) is de-
termined by the taxonomic relation identification
process described in Section 2.2.4.
Step 3: Graph pruning (line 18) The hy-
pernym graph generated in Step 1 is not an
optimal taxonomy as it may contain many redun-
dant edges or incorrect edges which together form
in a loop. In this step, we aim at producing an
optimal taxonomy by pruning the graph based
on our edge weighting strategy. A maximum
spanning tree algorithm, however, cannot be
applied as the graph is directed. For this purpose,
we apply Edmonds? algorithm (Edmonds, 1967)
for finding a maximum optimum branching of a
weighted directed graph. Using this algorithm,
we can find a subset of the current edge set, which
is the optimized taxonomy where every non-root
node has in-degree 1 and the sum of the edge
weights is maximized. Figure 1 shows an example
of the taxonomy induction process.
3 Experiment Results
We evaluated our methods for taxonomy construc-
tion against the following text collections of five
domains:
? Artificial Intelligence (AI) domain: 4,119 pa-
pers extracted from the IJCAI proceedings
from 1969 to 2011 and the ACL archives
from year 1979 to 2010. The same dataset
used in the work of Velardi et al. (2012).
? Terrorism domain: 104 reports of the US
state department, titled ?Patterns of Global
Terrorism (1991-2002)?
1
. A report contains
about 1,500 words.
? Animals, Plants and Vehicles domains: Col-
lections of Web pages crawled by using
the bootstrapping algorithm described by
Kozareva et al. (2008). Navigli et al. (2011)
and Kozareva and Hovy (2010) used these
datasets to compare their outputs against
WordNet sub-hierarchies.
There are two experiments performed in this sec-
tion: 1) Evaluating the construction of new tax-
onomies for Terrorism and AI domains, and 2)
Comparing our results with the gold-standard
WordNet sub-hierarchies. Note that in the experi-
ments, the threshold value we used for Score
LSP
is 1.9, Score
SCS
is 1.5 and Score is 2.1.
3.1 Constructing new taxonomies for AI and
Terrorism domains
Referential taxonomy structures such as WordNet
or OpenCyc are widely used in semantic analyt-
ics applications. However, their coverage is lim-
ited to common well-known areas, and many spe-
cific domains like Terrorism and AI are not well
covered in those structures. Therefore, an auto-
matic method which can induce taxonomies for
those specific domains from scratch can greatly
contribute to the process of knowledge discovery.
First, we applied our taxonomy construction
system to the AI domain corpus. We compared
the taxonomy constructed by our system with that
obtained by Velardi et al. (2012), and show the
comparison results in Table 2. Notice that in this
comparison, to be fair, we use the same set of
terms that was used in (Velardi et al., 2012). The
result shows that our approach can extract 9.8%
1
http://www.fas.org/irp/threat/terror.htm
815
Figure 1: An example of taxonomy induction. (a) Initial weighted hypernym graph. (b) Final optimal
taxonomy, where we prune two redundant edges (group, International terrorist organization), (Militant
group, Hezbollah) and remove the loop by cutting an incorrect edge (Al-Qaeda, Terrorist organization).
more taxonomic relations and achieve 7% better
term coverage than Velardi?s approach.
Our system Velardi?s system
#vertex 1839 1675
#edge 1838 1674
Average depth 6.2 6
Max depth 10 10
Term coverage 83% 76%
Table 2: Comparison of our system with (Velardi
et al., 2012)
We also applied our system to the Terrorism
corpus. The proposed taxonomic relation identifi-
cation algorithm extracts a total of 976 taxonomic
relations, from which the taxonomy induction al-
gorithm builds the optimal taxonomy. The total
number of vertices in the taxonomy is 281, and the
total number of edges is 280. The average depth
of the trees is 3.1, with the maximum depth 6. In
addition, term coverage (the ratio of the number
of terms in the final optimal trees to the number
of terms obtained by the term suggestion/filtering
method) is 85%.
To judge the contribution of each of taxonomic
relation identification methods described in Sec-
tion 2.2 to the overall system, we alternately run
the system for the AI and Terrorism domains with
different combinations of the three methods (i.e.
SIWN, LSP, and SCS) as shown in Table 3. Note
that we employed only the first two modules of
term suggestion/filtering and taxonomic relation
identification except the last module of taxonomy
No. of extracted relations
Terrorism AI domain
SCS 484 1308
SIWN 301 984
LSP 527 1537
SIWN + LSP 711 2203
SCS + SIWN + LSP 976 3122
Table 3: The number of taxonomic relations ex-
tracted by different methods.
induction for this experiment. Table 3 shows the
number of the taxonomic relations extracted by
each of the combinations. Since SIWN and LSP
are commonly used by previous taxonomic rela-
tion identification systems, we consider the com-
bination of SIWN + LSP as the baseline of the
experiment. The results in Table 3 show that the
three methods are all well complementary to each
other. In addition, the proposed SCS method can
contribute up to about 27% - 29% of all the iden-
tified taxonomic relations, which were not discov-
ered by the other two baseline methods.
Percentage of correct relations
Terrorism AI domain
SCS 91% 88%
SIWN 96% 91%
LSP 93% 93%
SCS + SIWN + LSP 92% 90%
Table 4: Estimated precision of taxonomic relation
identification methods in 100 extracted relations.
816
Animals domain Plants domain Vehicles domain
Our Kozareva Navigli Our Kozareva Navigli Our Kozareva Navigli
#Correct relations 2427 1643 N.A. 1243 905 N.A. 281 246 N.A.
Term coverage 96% N.A. 94% 98% N.A. 97% 97% N.A. 96%
Precision 95% 98% 97% 95% 97% 97% 93% 99% 91%
Recall 56% 38% 44% 53% 39% 38% 69% 60% 49%
F-measure 71% 55% 61% 68% 56% 55% 79% 75% 64%
Table 5: Comparison of (Navigli et al., 2011), (Kozareva and Hovy, 2010) and our system against Word-
Net in three domains: Animals, Plants and Vehicles.
We further evaluated the precision of each in-
dividual taxonomic relation identification method.
For AI and Terrorism domains, we again run the
system with each of the three methods and with all
together, and then randomly select 100 extracted
taxonomic relations each time. These selected tax-
onomic relations are then examined by two do-
main experts to check the correctness. The evalua-
tion results are given in Table 4. Note that only the
first two modules of term suggestion/filtering and
taxonomic relation identification are employed for
this experiment as well. The SIWN and LSPmeth-
ods achieve high precision because they are based
on the gold-standard taxonomy hierarchy Word-
Net and on the well-defined patterns, respectively.
In contrast, the SCS method ambitiously looks
for terms pairs that share similar syntactic con-
texts across sentences, though the contextual ev-
idence is restricted to certain syntactic structures,
and thus has a slightly lower precision compared
to the other two methods.
In short, the SCS method is complementary to
the baseline methods, significantly improving the
coverage of the combined methods, when its pre-
cision is comparable to those of the baseline meth-
ods. We performed next experiments to show that
the SCS method overall has synergistic impact to
improve the F-measure of the combined methods.
3.2 Evaluation against WordNet
In this experiment, we constructed taxonomies
for three domains Animals, Plants and Vehicles,
and then checked whether the identified relations
can be found in the WordNet, and which relations
in WordNet are not found by our method. Note
that in this comparison, to be fair, we changed our
algorithm to avoid using WordNet in identifying
taxonomic relations. Specifically, in the SIWN
algorithm, all operations of ??
WN
? are replaced
with normal string-matching comparison, and all
??
WN
? relations are falsified. The evaluation
uses the following measures:
Precision =
#relations found in WordNet and by the method
#relations found by the method
Recall =
#relations found in WordNet and by the method
#relations found in WordNet
We also compared our results with those ob-
tained by the approaches of Navigli et al. (2011)
and Kozareva and Hovy (2010), where they
also compared their resultant taxonomies against
WordNet. In this comparison, all the three ap-
proaches (i.e. ours, the two previous methods)
use the same corpora and term lists. The com-
parison results are given in Table 5. ?N.A.?
value means that this parameter is not applicable to
the corresponding method. The results show that
our approach achieves better performance than the
other two approaches, in terms of both the num-
ber of correctly extracted taxonomic relations and
the term coverage. Our system has a slightly
lower precision than that of (Navigli et al., 2011)
and (Kozareva and Hovy, 2010) due to the SCS
method, but it significantly contributes to improve
the recall and eventually the F-measure over the
other two systems.
To judge the effectiveness of our proposed tax-
onomy induction algorithm described in Section
2.3, we compared it with the graph-based algo-
rithm of Velardi et al. (2012). Recall that in this al-
gorithm, they treat all taxonomic relations equally,
and the pruning task is reduced to finding the best
trade-off between path length and the connectiv-
ity of traversed nodes. For each of five domains
(i.e. Terrorism, AI, Animals, Plants and Vehicles),
we alternately run the two taxonomy induction
algorithms over the same taxonomic relation set
produced by our taxonomic relation identification
process. For Terrorism and AI domains, we ran-
domly pick up 100 edges in each resultant taxon-
817
omy and ask two domain experts to judge for the
correctness. For Animals, Plants and Vehicles do-
mains, we check the correctness of the edges in re-
sultant taxonomies by comparing them against the
corresponding sub-hierarchies in WordNet. The
evaluation is given in Table 6. The results show
that the proposed taxonomy induction algorithm
can achieve better performance than the algorithm
of Velardi et al. (2012). This may be due to the fact
that our algorithm considers the scores of the iden-
tified taxonomic relations from the relation identi-
fication module, and thus is more precise in elim-
inating incorrect relations during the pruning pro-
cess.
Percentage of correct edges
Our algorithm Velardi?s algorithm
Terrorism 94% 90%
AI 93% 88%
Animals 95% 93%
Plants 95% 92%
Vehicles 93% 92%
Table 6: Comparison of our taxonomy induction
algorithms and that of Velardi et al. (2012).
In addition, when comparing Tables 4 and 6, we
can find that the precision of taxonomic relations
after the pruning process is higher than that before
the pruning process, which proves that the pro-
posed taxonomy induction algorithm effectively
trims the incorrect relations of Terrorism and AI
taxonomies, leveraging the percentage of correct
relations 2% - 3% up.
For the SCS method, besides the triple Subject-
Verb-Object, we also explore other syntactic
structures like Noun-Preposition-Noun and Noun-
Adjective-Noun. For example, from the sentence
?I visited Microsoft in Washington?, the triple
(Microsoft, in, Washington) is extracted using
Noun-Preposition-Noun structure. Similarly, from
the sentence ?Washington is a beautiful city?, the
triple (Washington, beautiful, city) is extracted us-
ing Noun-Adjective-Noun structure. We then use
the triples for the contextual subsumption method
described in Section 2.2.3, and test the method
against the Animals, Plants and Vehicles domains.
The results are then compared against WordNet
sub hierarchies. The experiment results in Table
7 show that the triples of Subject-Verb-Object give
the best performance compared to the other syn-
tactic structures. These can be explained as the
S-V-O N-P-N N-A-N
Animals domain
Precision 95% 68% 72%
Recall 56% 52% 47%
F-measure 71% 59% 57%
Plants domain
Precision 95% 63% 66%
Recall 53% 41% 43%
F-measure 68% 50% 52%
Vehicles domain
Precision 93% 59% 60%
Recall 69% 45% 48%
F-measure 79% 51% 53%
Table 7: Comparison of three syntactic struc-
tures: S-V-O (Subject-Verb-Object), N-P-N
(Noun-Preposition-Noun) and N-A-N (Noun-
Adjective-Noun).
number of triples of two types Noun-Preposition-
Noun and Noun-Adjective-Noun are smaller than
that of Subject-Verb-Object, and the number of
Verb is much greater than number of Preposition
or Adjective.
All experiment results are available at
http://nlp.sce.ntu.edu.sg/wiki/projects/taxogen.
4 Conclusion
In this paper, we proposed a novel method of iden-
tifying taxonomic relations using contextual evi-
dence from syntactic structure and Web data. This
method is proved well complementary with pre-
vious method of linguistic pattern matching. We
also present a novel graph-based algorithm to in-
duce an optimal taxonomy from a given taxo-
nomic relation set. The experiment results show
that our system can generally achieve better per-
formance than the state-of-the-art methods. In
the future, we will apply the proposed taxon-
omy construction method to other domains such
as biomedicine and integrate it into other frame-
works such as ontology authoring.
References
K. Bollacker, C. Evans, P. Paritosh, T. Sturge and J.
Taylor. 2008. Freebase: a collaboratively created
graph database for structuring human knowledge.
In proceedings of the ACM SIGMOD International
Conference onManagement of Data, pp. 1247-1250.
A. Budanitsky. 1999. Lexical semantic relatedness
818
and its application in natural language process-
ing. Technical Report CSRG-390, Computer Sys-
tems Research Group, University of Toronto.
P. Buitelaar, D. Olejnik and M. Sintek. 2004. A
Prot?eg?e Plug-in for Ontology Extraction from Text
Based on Linguistic Analysis. In proceedings of the
1st European Semantic Web Symposium, pp. 31-44.
M. Ciaramita, A. Gangemi, E. Ratsch, J. Saric and I.
Rojas. 2005. Unsupervised Learning of Semantic
Relations Between Concepts of a Molecular Biology
Ontology. In proceedings of the 19th International
Joint Conference on Artificial Intelligence, pp. 659-
664.
J. Edmonds. 1967. Optimum branchings. Journal of
Research of the National Bureau of Standards, 71,
pp. 233-240.
S. Fodeh, B. Punch and P. N. Tan. 2011. On Ontology-
driven Document Clustering Using Core Semantic
Features. Knowledge and information systems,
28(2), pp. 395-421.
H. N. Fotzo and P. Gallinari. 2004. Learning ?Gen-
eralization/Specialization? Relations between Con-
cepts - Application for Automatically Building The-
matic Document Hierarchies. In proceedings of the
7th International Conference on Computer-Assisted
Information Retrieval.
M. Geffet and I. Dagan. 2005. The Distributional In-
clusion Hypotheses and Lexical Entailment. In pro-
ceedings of the 43rd Annual Meeting of the ACL,
pp. 107-114.
R. Girju, A. Badulescu, and D. Moldovan. 2003.
Learning Semantic Constraints for the Automatic
Discovery of Part-Whole Relations. In proceedings
of the NAACL, pp. 1-8.
S. M. Harabagiu, S. J. Maiorano and M. A. Pasca.
2003. Open-Domain Textual Question Answering
Techniques. Natural Language Engineering, 9(3):
pp. 1-38.
M. A. Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. In proceedings
of the 14th Conference on Computational Linguis-
tics, pp. 539-545.
D. Klein and C. D. Manning. 2003. Accurate Unlexi-
calized Parsing. In proceedings of the 41st Annual
Meeting of the ACL, pp. 423-430.
Z. Kozareva, E. Riloff, and E. H. Hovy. 2008. Se-
mantic Class Learning from the Web with Hyponym
Pattern Linkage Graphs. In proceedings of the 46th
Annual Meeting of the ACL, pp. 1048-1056.
Z. Kozareva and E. Hovy. 2010. A Semi-supervised
Method to Learn and Construct Taxonomies Using
the Web. In proceedings of the Conference on Em-
pirical Methods in Natural Language Processing, pp.
1110-1118.
C. Matuszek, J. Cabral, M. J. Witbrock and J. DeO-
liveira. 2006. An Introduction to the Syntax and
Content of Cyc. In proceedings of the AAAI Spring
Symposium: Formalizing and Compiling Back-
ground Knowledge and Its Applications to Knowl-
edge Representation and Question Answering, pp.
44-49.
G. A. Miller. 1995. WordNet: a Lexical Database for
English. Communications of the ACM, 38(11), pp.
39-41.
R. Navigli and P. Velardi, 2004. Learning Domain
Ontologies from Document Warehouses and Dedi-
cated Web Sites. Computational Linguistics, 30(2),
pp. 151-179.
R. Navigli, P. Velardi and S. Faralli. 2011. A Graph-
based Algorithm for Inducing Lexical Taxonomies
from Scratch. In proceedings of the 20th Interna-
tional Joint Conference on Artificial Intelligence,
pp. 1872-1877.
R. Snow, D. Jurafsky and A. Y. Ng. 2006. Semantic
Taxonomy Induction from Heterogenous Evidence.
In proceedings of the 21st International Conference
on Computational Linguistics, pp. 801-808.
P. Velardi, S. Faralli and R. Navigli. 2012. Ontolearn
Reloaded: A Graph-based Algorithm for Taxonomy
Induction. Computational Linguistics, 39(3), pp.
665-707.
W. Wentao, L. Hongsong, W. Haixun, and Q. Zhu.
2012. Probase: A probabilistic taxonomy for text
understanding. In proceedings of the ACM SIG-
MOD International Conference on Management of
Data, pp. 481-492.
D. Widdows and B. Dorow. 2002. A Graph Model for
Unsupervised Lexical Acquisition. In proceedings
of the 19th International Conference on Computa-
tional Linguistics, pp. 1-7.
W. Wong, W. Liu and M. Bennamoun. 2007. Tree-
traversing ant algorithm for term clustering based
on featureless similarities. Data Mining and Knowl-
edge Discovery, 15(3), pp. 349-381.
819
Proceedings of the ACL 2010 Conference Short Papers, pages 359?364,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Distributional Similarity vs. PU Learning for Entity Set Expansion 
 
 
Xiao-Li  Li 
Institute for Infocomm Research,  
1 Fusionopolis Way #21-01 Connexis 
Singapore 138632 
xlli@i2r.a-star.edu.sg 
Lei Zhang 
University of Illinois at Chicago,  
851 South Morgan Street, Chicago, 
Chicago, IL 60607-7053, USA 
zhang3@cs.uic.edu 
 
Bing Liu 
University of Illinois at Chicago,  
851 South Morgan Street, Chicago, 
Chicago, IL 60607-7053, USA 
liub@cs.uic.edu 
See-Kiong  Ng 
Institute for Infocomm Research,  
1 Fusionopolis Way #21-01 Connexis 
Singapore 138632 
skng@i2r.a-star.edu.sg 
 
Abstract 
Distributional similarity is a classic tech-
nique for entity set expansion, where the 
system is given a set of seed entities of a 
particular class, and is asked to expand the 
set using a corpus to obtain more entities 
of the same class as represented by the 
seeds. This paper shows that a machine 
learning model called positive and unla-
beled learning (PU learning) can model 
the set expansion problem better. Based 
on the test results of 10 corpora, we show 
that a PU learning technique outperformed 
distributional similarity significantly.   
1 Introduction 
The entity set expansion problem is defined as 
follows: Given a set S of seed entities of a partic-
ular class, and a set D of candidate entities (e.g., 
extracted from a text corpus), we wish to deter-
mine which of the entities in D belong to S. In 
other words, we ?expand? the set S based on the 
given seeds. This is clearly a classification prob-
lem which requires arriving at a binary decision 
for each entity in D (belonging to S or not). 
However, in practice, the problem is often solved 
as a ranking problem, i.e., ranking the entities in 
D based on their likelihoods of belonging to S.  
The classic method for solving this problem is 
based on distributional similarity (Pantel et al 
2009; Lee, 1998). The approach works by com-
paring the similarity of the surrounding word 
distributions of each candidate entity with the 
seed entities, and then ranking the candidate enti-
ties using their similarity scores.   
In machine learning, there is a class of semi-
supervised learning algorithms that learns from 
positive and unlabeled examples (PU learning for 
short). The key characteristic of PU learning is 
that there is no negative training example availa-
ble for learning. This class of algorithms is less 
known to the natural language processing (NLP) 
community compared to some other semi-
supervised learning models and algorithms.  
PU learning is a two-class classification mod-
el. It is stated as follows (Liu et al 2002): Given 
a set P of positive examples of a particular class 
and a set U of unlabeled examples (containing 
hidden positive and negative cases), a classifier 
is built using P and U for classifying the data in 
U or future test cases. The results can be either 
binary decisions (whether each test case belongs 
to the positive class or not), or a ranking based 
on how likely each test case belongs to the posi-
tive class represented by P. Clearly, the set ex-
pansion problem can be mapped into PU learning 
exactly, with S and D as P and U respectively. 
This paper shows that a PU learning method 
called S-EM (Liu et al 2002) outperforms distri-
butional similarity considerably based on the 
results from 10 corpora. The experiments in-
volved extracting named entities (e.g., product 
and organization names) of the same type or 
class as the given seeds. Additionally, we also 
compared S-EM with a recent method, called 
Bayesian Sets (Ghahramani and Heller, 2005), 
which was designed specifically for set expan-
sion. It also does not perform as well as PU 
learning. We will explain why PU learning per-
forms better than both methods in Section 5. We 
believe that this finding is of interest to the NLP 
community.  
359
There is another approach used in the Web 
environment for entity set expansion. It exploits 
Web page structures to identify lists of items us-
ing wrapper induction or other techniques. The 
idea is that items in the same list are often of the 
same type. This approach is used by Google Sets 
(Google, 2008) and Boo!Wa! (Wang and Cohen, 
2008). However, as it relies on Web page struc-
tures, it is not applicable to general free texts.  
2 Three Different Techniques  
2.1 Distributional Similarity 
Distributional similarity is a classic technique for 
the entity set expansion problem. It is based on 
the hypothesis that words with similar meanings 
tend to appear in similar contexts (Harris, 1985). 
As such, a method based on distributional simi-
larity typically fetches the surrounding contexts 
for each term (i.e. both seeds and candidates) and 
represents them as vectors by using TF-IDF or 
PMI (Pointwise Mutual Information) values (Lin, 
1998; Gorman and Curran, 2006; Pa?ca et al 
2006; Agirre et al 2009; Pantel et al 2009). Si-
milarity measures such as Cosine, Jaccard, Dice, 
etc, can then be employed to compute the simi-
larities between each candidate vector and the 
seeds centroid vector (one centroid vector for all 
seeds). Lee (1998) surveyed and discussed vari-
ous distribution similarity measures.  
2.2 PU Learning and S-EM 
PU learning is a semi-supervised or partially su-
pervised learning model. It learns from positive 
and unlabeled examples as opposed to the model 
of learning from a small set of labeled examples 
of every class and a large set of unlabeled exam-
ples, which we call LU learning (L and U stand 
for labeled and unlabeled respectively) (Blum 
and Mitchell, 1998; Nigam et al 2000)  
There are several PU learning algorithms (Liu 
et al 2002; Yu et al 2002; Lee and Liu, 2003; Li 
et al 2003; Elkan and Noto, 2008). In this work, 
we used the S-EM algorithm given in (Liu et al 
2002). S-EM is efficient as it is based on na?ve 
Bayesian (NB) classification and also performs 
well. The main idea of S-EM is to use a spy 
technique to identify some reliable negatives 
(RN) from the unlabeled set U, and then use an 
EM algorithm to learn from P, RN and U?RN.  
The spy technique in S-EM works as follows 
(Figure 1): First, a small set of positive examples 
(denoted by SP) from P is randomly sampled 
(line 2). The default sampling ratio in S-EM is s 
= 15%, which we also used in our experiments. 
The positive examples in SP are called ?spies?. 
Then, a NB classifier is built using the set P? SP 
as positive and the set U?SP as negative (line 3, 
4, and 5). The NB classifier is applied to classify 
each u ? U?SP, i.e., to assign a probabilistic 
class label p(+|u) (+ means positive). The proba-
bilistic labels of the spies are then used to decide 
reliable negatives (RN). In particular, a probabili-
ty threshold t is determined using the probabilis-
tic labels of spies in SP and the input parameter l 
(noise level). Due to space constraints, we are 
unable to explain l. Details can be found in (Liu 
et al 2002). t is then used to find RN from U 
(lines 8-10). The idea of the spy technique is 
clear. Since spy examples are from P and are put 
into U in building the NB classifier, they should 
behave similarly to the hidden positive cases in 
U. Thus, they can help us find the set RN.  
Algorithm Spy(P, U, s, l) 
1.  RN ? ?;            // Reliable negative set 
2.  SP ? Sample(P, s%); 
3.  Assign each example in P ? SP the class label +1; 
4.  Assign each example in U ? SP the class label -1; 
5.  C ?NB(P ? S, U?SP); // Produce a NB classifier  
6.  Classify each u ?U?SP using C; 
7.  Decide a probability threshold t using SP and l; 
8.  for each u ?U do 
9.       if its probability p(+|u) < t then 
10.          RN ? RN ? {u}; 
Figure 1. Spy technique for extracting reliable 
negatives (RN) from U. 
Given the positive set P, the reliable negative 
set RN and the remaining unlabeled set U?RN, an 
Expectation-Maximization (EM) algorithm is 
run. In S-EM, EM uses the na?ve Bayesian clas-
sification as its base method. The detailed algo-
rithm is given in (Liu et al 2002). 
2.3 Bayesian Sets 
Bayesian Sets, as its name suggests, is based on 
Bayesian inference, and was designed specifical-
ly for the set expansion problem (Ghahramani 
and Heller, 2005). The algorithm learns from a 
seeds set (i.e., a positive set P) and an unlabeled 
candidate set U. Although it was not designed as 
a PU learning method, it has similar characteris-
tics and produces similar results as PU learning. 
However, there is a major difference. PU learn-
ing is a classification model, while Bayesian Sets 
is a ranking method. This difference has a major 
implication on the results that they produce as we 
will discuss in Section 5.3.  
In essence, Bayesian Sets learns a score func-
360
tion using P and U to generate a score for each 
unlabeled case u ? U. The function is as follows:  
                    
)(
)|(
)(
up
Pup
uscore =  (1) 
where p(u|P) represents how probable u belongs 
to the positive class represented by P. p(u) is the 
prior probability of u. Using the Bayes? rule, eq-
uation (1) can be re-written as:              
               
)()(
),(
)(
Ppup
Pup
uscore =                    (2)  
Following the idea, Ghahramani and Heller 
(2005) proposed a computable score function. 
The scores can be used to rank the unlabeled 
candidates in U to reflect how likely each u ? U 
belongs to P. The mathematics for computing the 
score is involved. Due to the limited space, we 
cannot discuss it here. See (Ghahramani and Hel-
ler, 2005) for details. In (Heller and Ghahramani, 
2006), Bayesian Sets was also applied to an im-
age retrieval application.  
3 Data Generation for Distributional 
Similarity, Bayesian Sets and S-EM 
Preparing the data for distributional similarity is 
fairly straightforward. Given the seeds set S, a 
seeds centroid vector is produced using the sur-
rounding word contexts (see below) of all occur-
rences of all the seeds in the corpus (Pantel et al 
2009). In a similar way, a centroid is also pro-
duced for each candidate (or unlabeled) entity.  
Candidate entities: Since we are interested in 
named entities, we select single words or phrases 
as candidate entities based on their correspond-
ing part-of-speech (POS) tags. In particular, we 
choose the following POS tags as entity indica-
tors ? NNP (proper noun), NNPS (plural proper 
noun), and CD (cardinal number). We regard a 
phrase (could be one word) with a sequence of 
NNP, NNPS and CD POS tags as one candidate 
entity (CD cannot be the first word unless it 
starts with a letter), e.g., ?Windows/NNP 7/CD? 
and ?Nokia/NNP N97/CD? are regarded as two 
candidates ?Windows 7? and ?Nokia N97?. 
Context: For each seed or candidate occurrence, 
the context is its set of surrounding words within 
a window of size w, i.e. we use w words right 
before the seed or the candidate and w words 
right after it. Stop words are removed.  
For S-EM and Bayesian Sets, both the posi-
tive set P (based on the seeds set S) and the unla-
beled candidate set U are generated differently. 
They are not represented as centroids.  
Positive and unlabeled sets: For each seed si ?S, 
each occurrence in the corpus forms a vector as a 
positive example in P. The vector is formed 
based on the surrounding words context (see 
above) of the seed mention. Similarly, for each 
candidate d ? D (see above; D denotes the set of 
all candidates), each occurrence also forms a 
vector as an unlabeled example in U. Thus, each 
unique seed or candidate entity may produce 
multiple feature vectors, depending on the num-
ber of times that it appears in the corpus. 
The components in the feature vectors are 
term frequencies for S-EM as S-EM uses na?ve 
Bayesian classification as its base classifier. For 
Bayesian Sets, they are 1?s and 0?s as Bayesian 
Sets only takes binary vectors based on whether 
a term occurs in the context or not.  
4 Candidate Ranking 
For distributional similarity, ranking is done us-
ing the similarity value of each candidate?s cen-
troid and the seeds? centroid (one centroid vector 
for all seeds). Rankings for S-EM and Bayesian 
Sets are more involved. We discuss them below.  
After it ends, S-EM produces a Bayesian clas-
sifier C, which is used to classify each vector u ? 
U and to assign a probability p(+|u) to indicate 
the likelihood that u belongs to the positive class. 
Similarly, Bayesian Sets produces a score 
score(u) for each u (not a probability).  
Recall that for both S-EM and Bayesian Sets, 
each unique candidate entity may generate mul-
tiple feature vectors, depending on the number of 
times that the candidate entity occurs in the cor-
pus. As such, the rankings produced by S-EM 
and Bayesian Sets are not the rankings of the 
entities, but rather the rankings of the entities? 
occurrences. Since different vectors representing 
the same candidate entity can have very different 
probabilities (for S-EM) or scores (for Bayesian 
Sets), we need to combine them and compute a 
single score for each unique candidate entity for 
ranking.  
To this end, we also take the entity frequency 
into consideration. Typically, it is highly desira-
ble to rank those correct and frequent entities at 
the top because they are more important than the 
infrequent ones in applications. With this in 
mind, we define a ranking method. 
Let the probabilities (or scores) of a candidate 
entity d ? D be Vd = {v1 , v2 ?, vn} for the n fea-
ture vectors of the candidate. Let Md be the me-
dian of Vd. The final score (fs) for d is defined as:  
    )1log()( nMdfs d +?=         (3) 
361
The use of the median of Vd can be justified 
based on the statistical skewness (Neter et al 
1993). If the values in Vd are skewed towards the 
high side (negative skew), it means that the can-
didate entity is very likely to be a true entity, and 
we should take the median as it is also high 
(higher than the mean). However, if the skew is 
towards the low side (positive skew), it means 
that the candidate entity is unlikely to be a true 
entity and we should again use the median as it is 
low (lower than the mean) under this condition.  
Note that here n is the frequency count of 
candidate entity d in the corpus. The constant 1 is 
added to smooth the value. The idea is to push 
the frequent candidate entities up by multiplying 
the logarithm of frequency. log is taken in order 
to reduce the effect of big frequency counts. 
The final score fs(d) indicates candidate d?s 
overall likelihood to be a relevant entity. A high 
fs(d) implies a high likelihood that d is in the 
expanded entity set. We can then rank all the 
candidates based on their fs(d) values.  
5 Experimental Evaluation 
We empirically evaluate the three techniques in 
this section. We implemented distribution simi-
larity and Bayesian Sets. S-EM was downloaded 
from http://www.cs.uic.edu/~liub/S-EM/S-EM-
download.html. For both Bayesian Sets and S-
EM, we used their default parameters. EM in S-
EM ran only two iterations. For distributional 
similarity, we tested TF-IDF and PMI as feature 
values of vectors, and Cosine and Jaccard as si-
milarity measures. Due to space limitations, we 
only show the results of the PMI and Cosine 
combination as it performed the best. This com-
bination was also used in (Pantel et al, 2009). 
5.1 Corpora and Evaluation Metrics 
We used 10 diverse corpora to evaluate the tech-
niques. They were obtained from a commercial 
company. The data were crawled and extracted 
from multiple online message boards and blogs 
discussing different products and services. We 
split each message into sentences, and the sen-
tences were POS-tagged using Brill?s tagger 
(Brill, 1995). The tagged sentences were used to 
extract candidate entities and their contexts. Ta-
ble 1 shows the domains and the number of sen-
tences in each corpus, as well as the three seed 
entities used in our experiments for each corpus. 
The three seeds for each corpus were randomly 
selected from a set of common entities in the ap-
plication domain.  
Table 1. Descriptions of the 10 corpora 
Domains # Sentences Seed Entities 
 Bank 17394 Citi, Chase, Wesabe 
 Blu-ray 7093 S300, Sony, Samsung 
 Car 2095 Honda, A3, Toyota 
 Drug 1504 Enbrel, Hurmia, Methotrexate 
 Insurance 12419 Cobra, Cigna, Kaiser 
 LCD 1733 PZ77U, Samsung, Sony 
 Mattress 13191 Simmons, Serta, Heavenly 
 Phone 14884 Motorola, Nokia, N95 
 Stove 25060 Kenmore, Frigidaire, GE 
 Vacuum 13491 Dc17, Hoover, Roomba 
The regular evaluation metrics for named enti-
ty recognition such as precision and recall are not 
suitable for our purpose as we do not have the 
complete sets of gold standard entities to com-
pare with. We adopt rank precision, which is 
commonly used for evaluation of entity set ex-
pansion techniques (Pantel et al, 2009):  
Precision @ N: The percentage of correct enti-
ties among the top N entities in the ranked list.  
5.2 Experimental Results 
The detailed experimental results for window 
size 3 (w=3) are shown in Table 2 for the 10 cor-
pora. We present the precisions at the top 15-, 
30- and 45-ranked positions (i.e., precisions 
@15, 30 and 45) for each corpus, with the aver-
age given in the last column. For distributional 
similarity, to save space Table 2 only shows the 
results of Distr-Sim-freq, which is the distribu-
tional similarity method with term frequency 
considered in the same way as for Bayesian Sets 
and S-EM, instead of the original distributional 
similarity, which is denoted by Distr-Sim. This 
is because on average, Distr-Sim-freq performs 
better than Distr-Sim. However, the summary 
results of both Distr-Sim-freq and Distr-Sim are 
given in Table 3.  
From Table 2, we observe that on average S-
EM outperforms Distr-Sim-freq by about 12 ? 
20% in terms of Precision @ N. Bayesian-Sets 
is also more accurate than Distr-Sim-freq, but S-
EM outperforms Bayesian-Sets by 9 ? 10%. 
To test the sensitivity of window size w, we 
also experimented with w = 6 and w = 9. Due to 
space constraints, we present only their average 
results in Table 3. Again, we can see the same 
performance pattern as in Table 2 (w = 3): S-EM 
performs the best, Bayesian-Sets the second, and 
the two distributional similarity methods the 
third and the fourth, with Distr-Sim-freq slightly 
better than Distr-Sim.  
362
5.3 Why does S-EM Perform Better? 
From the tables, we can see that both S-EM and 
Bayesian Sets performed better than distribution-
al similarity. S-EM is better than Bayesian Sets. 
We believe that the reason is as follows: Distri-
butional similarity does not use any information 
in the candidate set (or the unlabeled set U). It 
tries to rank the candidates solely through simi-
larity comparisons with the given seeds (or posi-
tive cases). Bayesian Sets is better because it 
considers U. Its learning method produces a 
weight vector for features based on their occur-
rence differences in the positive set P and the 
unlabeled set U (Ghahramani and Heller 2005). 
This weight vector is then used to compute the 
final scores used in ranking. In this way, Baye-
sian Sets is able to exploit the useful information 
in U that was ignored by distributional similarity. 
S-EM also considers these differences in its NB 
classification; in addition, it uses the reliable 
negative set (RN) to help distinguish negative 
and positive cases, which both Bayesian Sets and 
distributional similarity do not do. We believe 
this balanced attempt by S-EM to distinguish the 
positive and negative cases is the reason for the 
better performance of S-EM. This raises an inter-
esting question. Since Bayesian Sets is a ranking 
method and S-EM is a classification method, can 
we say even for ranking (our evaluation is based 
on ranking) classification methods produce better 
results than ranking methods? Clearly, our single 
experiment cannot answer this question. But in-
tuitively, classification, which separates positive 
and negative cases by pulling them towards two 
opposite directions, should perform better than 
ranking which only pulls the data in one direc-
tion. Further research on this issue is needed. 
6 Conclusions and Future Work 
Although distributional similarity is a classic 
technique for entity set expansion, this paper 
showed that PU learning performs considerably 
better on our diverse corpora. In addition, PU 
learning also outperforms Bayesian Sets (de-
signed specifically for the task). In our future 
work, we plan to experiment with various other 
PU learning methods (Liu et al 2003; Lee and 
Liu, 2003; Li et al 2007; Elkan and Noto, 2008) 
on this entity set expansion task, as well as other 
tasks that were tackled using distributional simi-
larity. In addition, we also plan to combine some 
syntactic patterns (Etzioni et al 2005; Sarmento 
et al 2007) to further improve the results.  
Acknowledgements: Bing Liu and Lei Zhang 
acknowledge the support of HP Labs Innovation 
Research Grant 2009-1062-1-A, and would like 
to thank Suk Hwan Lim and Eamonn O'Brien- 
Strain for many helpful discussions.   
Table 2.  Precision @ top N (with 3 seeds, and window size w = 3) 
 Bank Blu-ray Car  Drug Insurance LCD Mattress Phone Stove  Vacuum Avg. 
 Top 15 
Distr-Sim-freq 0.466 0.333 0.800 0.666 0.666 0.400 0.666 0.533 0.666 0.733 0.592
Bayesian-Sets 0.533 0.266 0.600 0.666 0.600 0.733 0.666 0.533 0.800 0.800 0.617
S-EM 0.600 0.733 0.733 0.733 0.533 0.666 0.933 0.533 0.800 0.933 0.720 
 Top 30 
Distr-Sim-freq 0.466 0.266 0.700 0.600 0.500 0.333 0.500 0.466 0.600 0.566 0.499 
Bayesian-Sets 0.433 0.300 0.633 0.666 0.400 0.566 0.700 0.333 0.833 0.700 0.556 
S-EM 0.500 0.700 0.666 0.666 0.566 0.566 0.733 0.600 0.600 0.833 0.643 
 Top 45 
Distr-Sim-freq 0.377 0.288 0.555 0.500 0.377 0.355 0.444 0.400 0.533 0.400 0.422 
Bayesian-Sets 0.377 0.333 0.666 0.555 0.377 0.511 0.644 0.355 0.733 0.600 0.515 
S-EM 0.466 0.688 0.644 0.733 0.533 0.600 0.644 0.555 0.644 0.688 0.620 
Table 3. Average precisions over the 10 corpora of different window size (3 seeds) 
Window-size w = 3   Window-size  w = 6  Window-size  w = 9 
Top Results Top 15 Top 30 Top 45  Top 15 Top 30 Top 45  Top 15 Top 30 Top 45
Distr-Sim 0.579 0.466 0.410  0.553 0.483 0.439  0.519 0.473 0.412 
Distr-Sim-freq 0.592 0.499 0.422  0.553 0.492 0.441  0.559 0.476 0.410 
Bayesian-Sets 0.617 0.556 0.515  0.593 0.539 0.524  0.539 0.522 0.497 
S-EM 0.720 0.643 0.620  0.666 0.606 0.597  0.666 0.620 0.604 
 
363
References  
Agirre, E., Alfonseca, E., Hall, K., Kravalova, J., 
Pasca, M., and Soroa, A. 2009. A study on si-
milarity and relatedness using distributional 
and WordNet-based approaches. NAACL 
HLT.  
Blum, A. and Mitchell, T. 1998. Combining la-
beled and unlabeled data with co-training. In 
Proc. of Computational Learning Theory, pp. 
92?100, 1998. 
Brill, E. 1995. Transformation-Based error-
Driven learning and natural language 
processing: a case study in part of speech 
tagging. Computational Linguistics.  
Bunescu, R. and Mooney, R. 2004. Collective 
information extraction with relational Markov 
Networks. ACL.  
Cheng T., Yan X. and Chang C. K. 2007. Entity-
Rank: searching entities directly and holisti-
cally.  VLDB.  
Chieu, H.L. and Ng, H. Tou. 2002. Name entity 
recognition: a maximum entropy approach 
using global information. In The 6th Work-
shop on Very Large Corpora. 
Downey, D., Broadhead, M. and Etzioni, O. 
2007. Locating complex named entities in 
Web Text. IJCAI.  
Elkan, C. and Noto, K. 2008. Learning classifi-
ers from only positive and unlabeled data. 
KDD, 213-220.  
Etzioni, O., Cafarella, M., Downey. D., Popescu, 
A., Shaked, T., Soderland, S., Weld, D. Yates. 
2005. A. Unsupervised named-entity extrac-
tion from the Web: An Experimental Study. 
Artificial Intelligence, 165(1):91-134.  
Ghahramani, Z and Heller, K.A. 2005. Bayesian 
sets. NIPS.  
Google Sets. 2008.  System and methods for au-
tomatically creating lists. US Patent: 
US7350187, March 25. 
Gorman, J. and Curran, J. R. 2006. Scaling dis-
tributional similarity to large corpora. ACL. 
Harris, Z. Distributional Structure. 1985. In: 
Katz, J. J. (ed.), The philosophy of linguistics. 
Oxford University Press.  
Heller, K. and Ghahramani, Z. 2006. A simple 
Bayesian framework for content-based image 
retrieval. CVPR. 
Isozaki, H. and Kazawa, H. 2002. Efficient sup-
port vector classifiers for named entity recog-
nition. COLING.  
Jiang, J. and Zhai, C. 2006. Exploiting domain 
structure for named entity recognition.  HLT-
NAACL.  
Lafferty J., McCallum A., and Pereira F. 2001. 
Conditional random fields: probabilistic 
models for segmenting and labeling sequence 
data. ICML.  
Lee, L. 1999. Measures of distributional similar-
ity. ACL.  
Lee, W-S. and Liu, B. 2003. Learning with Posi-
tive and Unlabeled Examples Using Weighted 
Logistic Regression. ICML.  
Li, X., Liu, B. 2003. Learning to classify texts 
using positive and unlabeled data, IJCAI. 
Li, X., Liu, B., Ng, S. 2007. Learning to identify 
unexpected instances in the test sSet. IJCAI. 
Lin, D. 1998. Automatic retrieval and clustering 
of similar words. COLING/ACL. 
Liu, B, Lee, W-S, Yu, P. S, and Li, X. 2002. 
Partially supervised text classification. ICML, 
387-394. 
Liu, B, Dai, Y., Li, X., Lee, W-S., and Yu. P. 
2003. Building text classifiers using positive 
and unlabeled examples. ICDM, 179-188. 
Neter, J., Wasserman, W., and Whitmore, G. A. 
1993. Applied Statistics. Allyn and Bacon.  
Nigam, K., McCallum, A., Thrun, S. and Mit-
chell, T. 2000. Text classification from la-
beled and unlabeled documents using EM. 
Machine Learning, 39(2/3), 103?134.  
Pantel, P., Eric Crestan, Arkady Borkovsky, 
Ana-Maria Popescu, Vishnu, Vyas. 2009. 
Web-Scale Distributional similarity and entity 
set expansion, EMNLP.  
Pa?ca, M. Lin, D. Bigham, J. Lifchits, A. Jain, A. 
2006. Names and similarities on the web: fast 
extraction in the fast lane. ACL.  
Sarmento, L., Jijkuon, V. de Rijke, M. and 
Oliveira, E. 2007. ?More like these?: growing 
entity classes from seeds. CIKM. 
Wang, R. C. and Cohen, W. W. 2008. Iterative 
set expansion of named entities using the web. 
ICDM.  
Yu, H., Han, J., K. Chang. 2002. PEBL: Positive 
example based learning for Web page classi-
fication using SVM. KDD, 239-248. 
364
