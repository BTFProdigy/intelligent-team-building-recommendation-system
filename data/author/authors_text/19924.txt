Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2087?2096, Dublin, Ireland, August 23-29 2014.
Exploratory Relation Extraction in Large Text Corpora
Alan Akbik Thilo Michael
Database Systems and Information Management Group
Einsteinufer 17, 10587 Berlin, Germany
{firstname.lastname}@tu-berlin.de
Christoph Boden
Abstract
In this paper, we propose and demonstrate Exploratory Relation Extraction (ERE), a novel
approach to identifying and extracting relations from large text corpora based on user-driven
and data-guided incremental exploration. We draw upon ideas from the information seeking
paradigm of Exploratory Search (ES) to enable an exploration process in which users begin with
a vaguely defined information need and progressively sharpen their definition of extraction tasks
as they identify relations of interest in the underlying data. This process extends the application
of Relation Extraction to use cases characterized by imprecise information needs and uncertainty
regarding the information content of available data.
We present an interactive workflow that allows users to build extractors based on entity types
and human-readable extraction patterns derived from subtrees in dependency trees. In order to
evaluate the viability of our approach on large text corpora, we conduct experiments on a dataset
of over 160 million sentences with mentions of over 6 million FREEBASE entities extracted from
the CLUEWEB09 corpus. Our experiments indicate that even non-expert users can intuitively
use our approach to identify relations and create high precision extractors with minimal effort.
1 Introduction
1.1 Motivation and Problem Statement
Relation Extraction (RE) is the task of creating extractors that automatically find instances of semantic
relations in unstructured data such as natural language text (Riloff, 1996). An example extraction task
might be to find instances of the EDUCATEDAT relation, which relates persons to their educational in-
stitution and may include the entity pair <Sigmund Freud, University of Vienna> as relation instance.
Motivated by an explosion of readily available sources of text data such as the Web, RE offers intriguing
possibilities for querying and analyzing data as well as extracting and organizing the contained informa-
tion (Sarawagi, 2008). As scalable computing architectures capable of processing ever larger amounts
of data are being developed (Dean and Ghemawat, 2004) and dependency parsers are becoming more
accurate and more robust (Petrov and McDonald, 2012), so rises the potential of developing means to
directly access the structured information contained in natural language text.
In spite of such positive trends however, currently established methods of creating relation extractors
suffer from a number of limitations. The first is one of cost; the process of creating extractors requires
either labeled data to be produced at sufficient quality and quantity in order to train a supervised machine
learning algorithm (Culotta and Sorensen, 2004; Mintz et al., 2009), or the manual creation of a complex
set of extraction rules (Str?otgen and Gertz, 2010; Reiss et al., 2008). In either case, the process is tedious
and time-consuming and requires trained specialists with an extensive background in NLP, rule-writing
or machine learning (Chiticariu et al., 2013). Worse, this process needs to be repeated for every relation
and domain of interest. Due to this cost, great care must be taken when deciding which relation types to
look for in a given text corpus.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2087
This leads to the second limitation, namely the necessary a priori specification of relations. Current
methods generally require a careful upfront definition of the RE tasks in order to start producing labeled
training data or extraction rule-sets. Practical scenarios, however, are often characterized by imprecise
and rapidly changing information needs and uncertainty regarding the type of information contained in
large, given text corpora (Chiticariu et al., 2013). This severely limits the practicability of currently
established RE methods.
1.2 Exploratory Search for Relations
To address these limitations, we propose a process of exploration for relations of interest in available data.
We propose to substantially reduce entry barriers into RE so that extraction tasks no longer need to be
exactly pre-specified and expensively prepared by generating labeled training data in advance. Instead,
we propose a manual, rule-based approach in which extraction rules are kept very simple so that users
can formulate natural language-like patterns as exploratory queries for relations against a text corpus.
We draw inspiration from the information seeking paradigm of Exploratory Search (ES) (Marchionini,
2006; White and Roth, 2009), where users start with a vaguely defined information need and - with a mix
of look-up, browsing, analysis and exploration - progressively discover information available to address
it and simultaneously concretize their information need. One of the challenges associated with the often
desired capability of ES is the design of interactive interfaces to support users as they navigate through
complex environments. Similarly, our challenge is to create an intuitive workflow that allows non-experts
in NLP to engage in relation exploration.
We propose to simplify the search for information by using natural language-like queries that match
subtrees in large corpora of dependency parsed data while hiding the complexity from the users. Explo-
rative queries return matching relation instances and source sentences, as well as suggestions for further
queries computed from the available data. By following a process of experimental querying and accept-
ing or rejecting pattern suggestions, users identify relations of interest and group patterns into extractors.
Our goal is to make use of such data-guidance to facilitate exploration while giving as much explicit
control to a user as possible.
1.3 Contributions
In this paper, we propose and demonstrate Exploratory Relation Extraction (ERE), a user-driven and
data-guided incremental exploration approach to Relation Extraction. We give details on our relation
extraction pattern language and introduce a guided, interactive workflow aimed at allowing users to
explore parsed text corpora for relations at minimal effort. We conduct two experiments on a large
corpus of over 160 million sentences from the CLUEWEB09 to determine in how far non-experts can use
ERE to discover and extract relations. We discuss the results of the user study, as well as strengths and
weaknesses of our proposed approach.
2 Exploratory Relation Extraction
In this section, we present our approach for Exploratory Relation Extraction. We provide details on how
we define extraction patterns and how we preemptively extract all subtrees in dependency trees from
a given text corpus (Section 2.1). We then outline a data-guided incremental workflow to explore the
indexed data for relations (Section 2.2) and illustrate this with an exemplary execution (Section 2.3).
2.1 Human-Readable Relation Extraction Patterns
Like much previous work in RE (Culotta and Sorensen, 2004; Schutz and Buitelaar, 2005; Uszkoreit,
2011), we define extraction patterns using features from dependency-parsed sentences. As recent work
has shown (Del Corro and Gemulla, 2013; Akbik et al., 2013b), patterns in dependency trees are well-
suited to manual rule based RE, as they enable more succinct and thus more human-readable rule sets.
Following this work, we define RE patterns as subtrees in dependency trees.
In our work, we follow the idea of Preemptive Information Extraction (Shinyama and Sekine, 2006)
in which all possible relations for a given text corpus are preemptively generated in advance. Applied
2088
A. Dependency Parse Sentence 
B. Extract Subtrees for Entity Pair C. Link Entities to Freebase + Retrieve Entity Types 
Entity Text FreebaseID Type Freud m/06myp Person University of Vienna m/0dy04 Educational Institution 
entered entered study 
At young age , Freud entered the University of Vienna to study medicine 
X X Y Y 
At young age entered X Y 
D. Index Subtrees, Entity Pairs,       Types and Sentences 
X-Entity Y-Entity Pattern X-Type Y-Type Sentence Freud University of Vienna X enter Y Person Educational_Institution At young age, Freud entered the ? Freud University of Vienna X enter Y study Person Educational_Institution At young age, Freud entered the ? Freud University of Vienna at young age X enter Y Person Educational_Institution At young age, Freud entered the ? Freud University of Vienna X enter Y study medicine Person Educational_Institution At young age, Freud entered the ? ? ? ? ? ? ? 
Figure 1: Illustration of the subtree generation process. We parse each sentence in a given document
collection using a dependency parser and annotate all entities (A). Then, we generate all possible subtrees
in the dependency tree that span pairs of annotated entities, three of which are illustrated in (B), and link
entities to their FREEBASE IDs to determine their entity types (C). We then generate a lexical, lemmatized
representation of these subtrees which we store along with the entity pair, their entity types and sentence
they are observed with (D).
to our problem this means that we generate all possible dependency subtrees, arguing that depending
on the user?s information need, any such pattern may be valuable. Since we are interested in binary
relations only, we generate only those subtrees that span two named entities in a sentence. In addition,
we also determine the fine-grained entity types for named entities in order to allow users to optionally
restrict patterns to match only entities of certain types. Previous work has shown the benefit of including
fine-grained type restrictions into patterns (Akbik et al., 2013a).
We illustrate this process with an example sentence in Figure 1, for which we determine all subtrees
that span the indicated entity pair. In the subtrees, we replace the entity tokens with the placeholders ?X?
and ?Y?, where the former is the placeholder for the X-entity and the latter the placeholder for the Y-
entity. For better human-readability, we lexicalize the patterns by lemmatizing the words and discarding
information on typed dependencies. We also link the entities in the sentence to entries in the FREEBASE
knowledge base (Bollacker et al., 2008), allowing us to retrieve their fine grained entity types.
We then index the information on lexicalized patterns, the entities they span and their types, as well as
the sentences in which the patterns were found (Figure 1D). This allows users to query for any combi-
nations of patterns and entity type restrictions and retrieve matching entity pairs and sentences from the
index. For instance, a user may query for all entity pairs that match the ?at young age X enter Y? pattern,
and optionally restrict the Y-entity to be only of type ORGANIZATION, or more specific types such as
CHURCH or UNIVERSITY. We argue that because patterns are lexicalized variants of dependency sub-
trees and entity type restrictions can have human readable names, such queries are intuitive to users even
without an NLP background. The use and preemptive indexing of human-readable patterns decreases the
entry barriers into the ERE process, as this enables users to exploratively query parsed text corpora.
2089
X_Entity  Y_Entity  Sentence 
Bill Gates  Harvard  While it has been around since the time Bill Gates dropped out of Harvard , it has just recently become big news.  
Johnny Knoxville  American Academy of Dramatic Art  
Johnny Knoxville attended the American Academy of Dramatic Arts  in California but dropped out after just two weeks.  
Leo Tolstoy  Kazan University Leo Tolstoy also briefly attended Kazan University, although he never took a degree there.  
? ? ? 
Selected Patterns + Types 
X_Type  Person 
Y_Type  Educational_Institution 
Pattern X drop out of Y  OR   X attend Y but drop out OR  X briefly  attend Y 
Pattern Suggestions 
X student at Y 
X left Y Extractor Complete 
accept  
Pattern Suggestions 
X is professor at Y 
X graduate from Y 
X drop out of Y 
accept  
l aunch  
A . Launch Initial Query  B . Accept or Reject  Suggested  Patterns 
C. Mark Extractor Complete  D . Run Extractor on Corpus  
Initial Query 
X_Type  Person 
Y_Type  Educational Institution 
Pattern 
Selected Patterns + Types 
X_Type  Person 
Y_Type  Educational Institution 
Pattern X drop out of Y 
Updated Pattern Suggestions 
X attend Y but drop out 
X left Y 
X briefly  attend Y 
Index  Index  
Index  
Figure 2: Illustration of the exploratory relation extraction process. The user begins with specifying
entity types of interest and receives a set of pattern suggestions (A). Intrigued by the pattern ?X drop out
from Y?, the user affirms this pattern. This prompts updated pattern suggestions which the user affirms
or rejects (B). When no more interesting patters are offered, the user marks the extractor as complete (C)
and runs it on a corpus, retrieving relation instances and matching sentences (D).
2.2 Guidance From Available Data
A second key component is to provide guidance in the exploration process by computing suggestions for
patterns from user input and enabling an interactive workflow that allows users to work with available
data. Such guidance is needed for two reasons: First, though much effort is invested in human-readable
extraction patterns, users may need support in formulating patterns and choosing entity type restrictions.
This is especially the case when users are non-experts in the domain of interest and they strive to identify
a range of appropriate patterns. Second, users may be uncertain of the information content of a given text
corpus. By providing guidance through automatic pattern suggestions that reflect available information,
we help users find patterns for their information need.
Users formulate an entry point to launch the exploration process, either by providing entity types,
patterns or both. We guide the formulation of this initial query through autocomplete options. If the user
enters only types for the entities, the system offers the most common patterns that are observed between
entities of these types. The user can also search for patterns that contain a certain keyword.
In either case, the system suggests patterns that meet the user-defined entry point. Patterns are ordered
by their absolute count in the corpus so that more common patterns are displayed at the top of the list. In
addition, verb-based patterns are favored using a scoring metric that assigns extra points to patterns that
include verbs. To assist a user in understanding a pattern, we optionally display example sentences and
entity pairs in which it matches.
The user then starts a process of selecting (and de-selecting) entity type restrictions and pattern, thus
refining the extractor while being guided by constantly updated pattern suggestions. The user continues
this process until satisfied with the created extractor at which point it can be saved and the discovered
relation instances downloaded. The user can now repeat the workflow to create more extractors.
2090
2.3 Exploration Workflow Example
Suppose we have a user who is given a large text corpus and is asked to link persons to their respec-
tive educational institutions, but is unsure of what type of relevant information may be found in the
corpus. Knowing only that relations should hold between entities of type PERSON and entities of type
EDUCATIONAL INSTITUTION, the user starts an exploration process by providing only these entity type
restrictions. This is illustrated in Figure 2A).
A query is run against the index that identifies common patterns that hold between entities of such
types, including ?X be professor at Y?, ?X study at Y? and ?X drop out from Y?. Recall that each pattern
is a human-readable version of a subtree in a dependency tree with two placeholders for entities, namely
?X? and ?Y?. These placeholders may match named entities of any type, or can be restricted to matching
only entities of certain types such as persons, organizations or locations. By clicking on a pattern, the user
retrieves entity pairs and sentences in which a pattern matches; For example, the user is informed that
the pattern ?X study at Y? finds the relation instance <Bill Gates, Harvard University> in the sentence
?Bill Gates briefly studied at Harvard University.?.
Intrigued by the pattern ?X drop out from Y?, the user affirms this pattern and rejects all other sug-
gestions. This causes a new query to be run against the parsed data, this time consisting of the entity
restrictions as well as the pattern. As the query is now more concrete, the pattern suggestions are updated
to reflect this new information. The user is presented with similar patterns such as ?Y dropout X? and ?X
attend Y but drop out?. This is illustrated in Figure 2B).
The user repeats this, selecting or de-selecting patterns (Figure 2B). At each interaction, suggestions
are updated to reflect the current selection. When the user is satisfied with the identified relation, the
selected set of patterns and restrictions is saved as an extractor (Figure 2C) and executed against the
entire text corpus (Figure 2D). This returns lists of matching relation instances and sentences. The user
has thus started with an imprecise information need and identified a relation of interest in a given text
corpus, namely a relation for persons that attended an educational institution but did not graduate.
3 Experiments
In order to examine in how far our approach indeed contributes to overcoming the limitations of RE
outlined in Section 1.1, namely the significant cost and the necessary a-priory specification of relations,
we conduct a user study with 10 subjects that have little or no NLP background. We ask the users to
apply the workflow for two separate tasks: An extraction task in which users are given four clearly
defined semantic relations and an exploration task in which users are asked to identify relations for more
vaguely defined information needs. We only provide the users with a brief introduction into the workflow.
For the extraction task, we measure the time spent per extractor and estimate the quality of the created
extractors in terms of precision and recall. For the exploration as well as for the extraction task we also
qualitatively inspect discovered relations and evaluate user feedback.
3.1 Datasets
ClueWeb09. As source of text data, we use the English language portion of the well-known
CLUEWEB09
1
reference corpus, consisting of roughly 5 billion crawled Web pages. We use boiler-
plating to remove HTML markup and sentence splitting to determine English language sentences.
FACC1. We use the recently released FACC1 (Gabrilovich et al., 2013) resource, a high quality named
entity linking effort that was executed on the CLUEWEB09 corpus, linking over 6 billion entity mentions
to their corresponding FREEBASE entries. Using this data, we identify over 160 million sentences in
CLUEWEB09 that contain at least two entities we can link to FREEBASE. We parse all such sentences
using the ClearNLP toolkit (Choi and McCallum, 2013).
Gold Standard Relation Annotations. As gold standard, we use the FREEBASE relation annotations
as well as annotations from the ?Relation Extraction Corpus?
2
a large, human-judged dataset of five
relations about public figures on Wikipedia that was released by Google. Four of these relations involve
1
http://lemurproject.org/clueweb09/
2
http://code.google.com/p/relation-extraction-corpus/
2091
EDUCATEDAT GRADUATEDWITHDEGREE
#INST P R #PAT TIME #INST P R #PAT TIME
USER 1 58,611 0.99 0.2 51 12 min 17,698 1.0 0.27 34 17 min
USER 2 48,782 0.99 0.31 34 15 min 12,180 1.0 0.27 27 14 min
USER 3 25,435 0.88 0.12 12 8 min 54,371 0.93 0.53 24 8 min
USER 4 33,095 0.99 0.23 25 12 min 7,196 1.0 0.22 9 10 min
USER 5 47,668 0.76 0.16 29 13 min 34,942 1.0 0.48 3 5 min
USER 6 20,356 0.99 0.15 18 14 min 10,290 1.0 0.25 12 14 min
USER 7 22,889 0.62 0.01 8 4 min 37,119 0.71 0.6 19 4 min
USER 8 31,412 0.98 0.19 13 15 min 1,251 0.46 0.04 10 14 min
USER 9 14,169 0.99 0.1 6 8 min 13,104 0.6 0.17 13 12 min
USER 10 29,289 0.99 0.19 17 15 min 35 1.0 0.02 4 20 min
AVERAGE 33,171 0.92 0.17 21 11.6 min 18,819 0.87 0.29 16 11.8 min
BORNIN DIEDIN
#INST P R #PAT TIME #INST P R #PAT TIME
USER 1 158,222 0.7 0.26 18 9 min 25,779 0.7 0.14 32 9 min
USER 2 72,888 0.79 0.21 23 17 min 13,582 0.86 0.13 12 12 min
USER 3 89,825 0.84 0.22 21 7 min 15,849 0.86 0.13 12 7 min
USER 4 66,899 0.81 0.21 19 14 min 13,542 0.86 0.13 11 8 min
USER 5 65,213 0.82 0.19 19 15 min 21,105 0.85 0.13 10 9 min
USER 6 131,275 0.83 0.25 16 13 min 14,423 0.85 0.13 8 9 min
USER 7 7,851 0.85 0.03 5 4 min 15,980 0.85 0.14 17 4 min
USER 8 52,927 0.82 0.17 10 15 min 25,090 0.74 0.14 8 14 min
USER 9 56,724 0.84 0.18 10 12 min 15,728 0.85 0.14 8 9 min
USER 10 58,347 0.94 0.22 10 15 min 14,112 0.86 0.13 8 10 min
AVERAGE 76,017 0.82 0.19 15 12.1 min 33,171 0.82 0.13 13 9.1 min
Table 1: Evaluation results for the 4 well-defined relations in the extraction task. We note differences
from user to user, especially with regards to the number of found instances (#INST), the number of
selected patterns (#PAT) and the time spent per relation. Extractors generally find large amounts of
relation instances at high precision (P), while recall values (R) are lower. Users are ordered by the total
number of patterns they selected. User 1 selected the most patterns overall and found the most instances
for the BORNIN, DIEDIN and EDUCATEDAT relations (highlighted bold). User 10 both spent the most
time overall while selecting the fewest patterns. User 7 spent the least amount of time overall.
FREEBASE entities, namely BORNIN, DIEDIN, EDUCATEDAT and GRADUATEDWITHDEGREE. We
use these relations in the extraction task.
3.2 Extraction Task
We evaluated the user-created extractors against the gold standard annotations. However, even with
relatively large sources of annotations, only roughly 5% of entity pairs in our 160 million sentences have
a known FREEBASE relation. We therefore compute precision and recall only for labeled entity pairs,
and separately list the absolute number of extracted relation instances.
Large amounts of relation instances at high precision. As Table 1 indicates, many users were able to
create extractors that find very large amounts of instances (over 100.000 instances in some cases) at high
precision in an average time of 9 to 12 minutes, while recall values tend to be lower. This tendency to
favor precision at the cost of recall has been observed in previous works on rule-based RE (Wang et al.,
2012). Nevertheless, we analyzed precision and recall in greater detail by manually evaluating a sample
of 200 false positives and 200 false negatives by hand to discover the reasons for precision and recall
2092
loss.
Mismatch between gold standard and results. As Table 2 shows, false positives are most commonly
due to inconsistencies between extraction results and the gold standard annotations concerning the level
of granularity of a relation instance. For example, we found BORNIN and DIEDIN relation instances
that indicated a person?s place of birth or death at lower or higher granularity than FREEBASE records.
An example of this is given in Table 2 for Abraham Lincoln?s place of death; we find the more granular
<Lincoln, Hildene>, while the gold standard expects <Lincoln, Vermont>. While different from the
gold standard, such instances are not false, which suggests that actual precision may be higher than the
measured values indicate.
Missed patterns and entity types. The most common causes of recall loss are patterns that users failed
to select. In Table 2, we distinguish between ?common? patterns that were found by at least one user and
?long tail? patterns that were found by none. While we did not expect a user-driven approach to identify
long tail patterns, we were surprised that some users failed to find more common patterns. Similarly,
the second most common cause of precision loss are entity type restrictions that users failed to correctly
select, again to our surprise. We proceeded to interview the users to determine reasons for this.
3.3 Exploration Task
We also asked users to explore the corpus for a vaguely defined information need, namely for relations
that pertain to ?celebrities?, as well as one arbitrary relation. Users spent widely varying amounts of time
(between 5 and 50 minutes) on this task due to differences in motivation, as some users had interpreted
the search for ?interesting? relations as a challenge. For each relation, users provided a short description.
Some relations not in Freebase. While the most common types of relations found for entities of type
CELEBRITY regarded different types of romantic involvements with other celebrities such as marriages
and divorces, some relations were identified that are not found in FREEBASE. This included a relation
that connects a celebrity to the sports team they support or the car they drive (see Table 3). This indicates
a potential for using ERE to identify new relations for addition to existing knowledge bases.
Closed-class words can be relevant. Interestingly, one user also worked with patterns that involved
closed-class word classes, such as ?if? and ?whether?. Table 3 shown an example of a relation that
indicates speculative birthplaces using such words.
3.4 User Feedback and Discussion
Approach more suited to exploration than extraction. When interviewing the users, we found that
they generally favored the exploration over the extraction tasks as here the search could be directed to
more fine-granular and specialized relations. One of the main problems encountered was the ?halting
problem?, i.e. the question of when to stop adding patterns to an extractor. For some relations, such
as BORNIN, users already found thousands of relation instances after selecting the first pattern, which
caused two problems; First, they were unsure of the quality of the selected pattern(s), as they were
unable to manually check thousands of relation instances for their validity. Second, they were unsure if
more patterns were even needed if the first few already found such amounts of relation instances. These
problems were not encountered in the exploration tasks, as here users could decide the information need
for themselves and select patterns accordingly.
Difficulties concerning entity types. Another main difficulty related to the precise meaning of FREE-
BASE entity types; For instance, there are several location types, such as LOCATION.LOCATION, LO-
CATION.DATED LOCATION and LOCATION.STATISTICAL REGION, which users found to be confusing,
a problem that was compounded by occasional entity linking errors. Many users expressed the desire
to specify custom entity types as restrictions in order to have a similar level of control here as over the
choice of patterns.
Low entry barriers but allow additional complexity. Overall, we found that users were generally
able to start exploring the corpus using our workflow immediately after the brief introduction. Users
stated the natural language-like representation of patterns to be intuitively readable, although for some
it required a trial and error process to understand how patterns matched entities in sentences. Similarly,
some users wished to understand in greater detail how entity types are determined and whether this could
2093
FALSE POSITIVES
CLASS COUNT EXAMPLE SENTENCE
FB Mismatch 95 Lincoln died at Hildene , his Vermont home, on July 26, 1926.
Type Error 82 [..] the scene where Boromir is killed in The Fellowship of the Ring.
FB Incomplete 14 Later that year, on December 27, Dorr died in Providence, in his native Rhode Island.
Other 9 Brieven van liederen Rascal Flatts die in het schijfcd album omvatten Feels Like Today.
FALSE NEGATIVES
CLASS COUNT EXAMPLE SENTENCE
Common 87 Klein holds a Bachelor of Arts.
Long Tail 79 Roger Blandford is a native of England and took his BA, MA and [..].
Other 34 [..], 1974; MS, 1976; PhD, University of Pierre and Marie Curie, 1982.
Table 2: Analysis of 200 false positives and 200 false negatives to determine error classes for precision
and recall loss. Each error class is listed with an example sentence. Main reasons for false positives
included a mismatch in granularity between extraction results and annotations, wrongly specified types
by the users or cases in which instances were found that were not in FREEBASE. Main reasons for false
negatives were mostly patterns that users failed so select, either common patterns, or more rare patterns
from the long tail.
NAME DESCRIPTION EXAMPLE PATTERNS EXAMPLE INSTANCES
CELEBRITYDIVORCE Divorce between ?X and Y divorce?, <Nicole Kidman, Tom Cruise>
two celebrities ?X divorce Y?, <Federline, Spears>
CELEBRITYDRIVESCAR Finds the cars that ?X drives Y?, <Arnold Schwarzenegger, H1>
celebrities drive ?X ?s car Y?, <Leonardo DiCaprio, Toyota Prius>
CONTESTEDBITHPLACE Relates persons to ?if X born in Y?, <Barack Obama, Kenya>
their speculative birthplace ?whether X born in Y?, <Barack Obama, Nigeria>
Table 3: Examples for relations discovered in the exploration task. CELEBRITYDIVORCE represents a
commonly discovered relation, while CELEBRITYDRIVESCAR represents a relation that is presently not
part of Freebase. CONTESTEDBITHPLACE is an example of a relation that utilizes closed-world words
in patterns.
be influenced. This indicates the need for adding options in future work that give more experienced users
more technical information (and control) on dependency trees and FREEBASE types.
4 Previous Work
While no directly comparable approach to Exploratory Relation Extraction is known to us, we take
inspiration from a number of previous works.
Exploratory Search (Marchionini, 2006; White and Roth, 2009) is an information seeking paradigm
in the field of Information Retrieval, where - like in our proposed approach - users begin an exploration
process with an imprecise information need and progressively discover available information to address
and sharpen it. Unlike our approach, users search for documents and must consume the unstructured
information themselves. We instead apply this paradigm to RE and strive to find structured, relational
information in text corpora of unknown content as well as generate Realtion Extractors in the process.
Preemptive Information Extraction (Shinyama and Sekine, 2006), as well as much work in Open In-
formation Extraction (Yates et al., 2007) that builds on this idea, is the preemptive (or open) extraction
of all possible relations in a text corpus. We draw inspiration from this idea in our preemptive sub-
tree generation approach; however, while we extract all possible subtrees for each relation regardless of
whether they point to a relation or not, Preemptive and OpenIE approaches aim to produce facts and
therefore much more narrowly extract predicates using rule-sets (Del Corro and Gemulla, 2013), classi-
fiers (Schmitz et al., 2012) or both (Etzioni et al., 2011).
Manual Rule-Based RE. We also build our work on the field of manual, rule-based RE, which has been
observed to be predominantly preferred industry solution due to interpretability of extraction rules and
2094
easy adaption to changing domains (Chiticariu et al., 2013; Chiticariu et al., 2010). The lack of tools
to assist rule developers in exploring and choosing between different automatically generated rules has
been stated to be one of the major challenges associated with rule-based RE systems. Recent research
has moved towards more guided (Li et al., 2012) and more interactive (Akbik et al., 2013b) workflows
for the creation of rule-based extractors. Our proposed approach follows this direction, but is the first
approach to combine both with automatic suggestions and enable exploratory search for relations.
Precomputing Resources of Relational Patterns. Our work also bears some resemblance to previous
work that have grouped similar extraction patterns into clusters (Li et al., 2011) or arranged them in a
taxonomy (Nakashole et al., 2012), with the goal of facilitating relation extraction efforts. Contrary to
these works, we do not precompute a static resource but rather continuously re-compute pattern sugges-
tions on the basis of user interactions and the text corpus that the user is working with. In addition, our
suggestions are based on both user-selected patterns as well as entity type restrictions.
5 Conclusion and Future Work
In this paper, we proposed Exploratory Relation Extraction as a method of exploring text corpora of
uncertain content for relations of interest given an imprecise information need. We have presented and
evaluated a user-driven and data-guided incremental exploration workflow that enables non-expert users
to identify relations and create high precision extractors with minimal effort. Our results indicate that
applying ideas from Exploratory Search to RE is beneficial and can extend the application of RE to use
cases characterized by more imprecise information needs and uncertainty regarding the information con-
tent of available data. In order to facilitate the discussion of our approach with the research community,
we release our work publicly through a Web demonstrator
3
.
Future work will investigate extending the approach to relations that hold between an arbitrary number
of entities as well as the detection of custom entity types. We aim to allow users to store and combine ex-
tractors - for example relation extractors that use custom entity type detectors - to address more complex
information needs and distribute the exploration and extraction processes along larger groups of users.
This way we seek to enable collaborative RE approaches for creating large knowledge bases from text.
Acknowledgements
We would like to thank the anonymous reviewers for their helpful comments. This research is funded by the European Union?s
Seventh Framework Programme (FP7/2007-2013) under grant agreement no ICT-2009-4-1 270137 ?Scalable Preservation
Environments? (SCAPE) and the German Federal Ministry of Education and Research (BMBF) under grant no. 01ISI2033
RADAR?.
References
A. Akbik, L. Visengeriyeva, J. Kirschnick, and A. L?oser. 2013a. Effective selectional restrictions for unsupervised relation
extraction. In Proceedings of the 6th International Joint Conference on Natural Language Processing.
Alan Akbik, Oresti Konomi, and Michail Melnikov. 2013b. Propminer: A workflow for interactive information extraction and
exploration using dependency trees. In ACL System Demonstrations. Association for Computational Linguistics.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created
graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on
Management of data, pages 1247?1250. ACM.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li, Sriram Raghavan, Frederick R Reiss, and Shivakumar Vaithyanathan.
2010. Systemt: an algebraic approach to declarative information extraction. In ACL, pages 128?137. Association for
Computational Linguistics.
Laura Chiticariu, Yunyao Li, and Frederick R Reiss. 2013. Rule-based information extraction is dead! long live rule-based
information extraction systems! In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Pro-
cessing, pages 827?832.
Jinho D Choi and Andrew McCallum. 2013. Transitionbased dependency parsing with selectional branching. In Proceedings
of the 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria.
3
The demonstrator is available at http://lucene.textmining.tu-berlin.de/
2095
Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of the 42nd Annual
Meeting on Association for Computational Linguistics, page 423. Association for Computational Linguistics.
Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce: simplified data processing on large clusters. In Proceedings of the 6th
conference on Symposium on Opearting Systems Design & Implementation - Volume 6, OSDI?04, pages 137?150, Berkeley,
CA, USA. USENIX Association.
Luciano Del Corro and Rainer Gemulla. 2013. Clausie: clause-based open information extraction. In Proceedings of the
22nd international conference on World Wide Web, pages 355?366. International World Wide Web Conferences Steering
Committee.
Oren Etzioni, Anthony Fader, Janara Christensen, Stephen Soderland, and Mausam Mausam. 2011. Open information extrac-
tion: The second generation. In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence-
Volume Volume One, pages 3?10. AAAI Press.
Evgeniy Gabrilovich, Michael Ringgaard, and Amarnag Subramanya. 2013. FACC1: freebase annotation of ClueWeb corpora,
version 1 (release date 2013-06-26, format version 1, correction level 0).
Yunyao Li, Vivian Chu, Sebastian Blohm, Huaiyu Zhu, and Howard Ho. 2011. Facilitating pattern discovery for relation ex-
traction with semantic-signature-based clustering. In Proceedings of the 20th ACM international conference on Information
and knowledge management, pages 1415?1424. ACM.
Yunyao Li, Laura Chiticariu, Huahai Yang, Frederick R Reiss, and Arnaldo Carreno-fuentes. 2012. Wizie: a best practices
guided development environment for information extraction. In Proceedings of the ACL 2012 System Demonstrations, pages
109?114. Association for Computational Linguistics.
Gary Marchionini. 2006. Exploratory search: from finding to understanding. Communications of the ACM, 49(4):41?46.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In
ACL/AFNLP, pages 1003?1011.
Ndapandula Nakashole, Gerhard Weikum, and Fabian Suchanek. 2012. Patty: a taxonomy of relational patterns with se-
mantic types. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and
Computational Natural Language Learning, pages 1135?1145. Association for Computational Linguistics.
Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 shared task on parsing the web. In Notes of the First Workshop
on Syntactic Analysis of Non-Canonical Language (SANCL), volume 59.
Frederick Reiss, Sriram Raghavan, Rajasekar Krishnamurthy, Huaiyu Zhu, and Shivakumar Vaithyanathan. 2008. An algebraic
approach to rule-based information extraction. In Data Engineering, 2008. ICDE 2008. IEEE 24th International Conference
on, pages 933?942. IEEE.
Ellen Riloff. 1996. Automatically generating extraction patterns from untagged text. In Proceedings of the national conference
on artificial intelligence, pages 1044?1049.
Sunita Sarawagi. 2008. Information extraction. Foundations and trends in databases, 1(3):261?377.
Michael Schmitz, Robert Bart, Stephen Soderland, Oren Etzioni, et al. 2012. Open language learning for information extrac-
tion. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 523?534. Association for Computational Linguistics.
Alexander Schutz and Paul Buitelaar. 2005. Relext: A tool for relation extraction from text in ontology extension. In The
Semantic Web?ISWC 2005, pages 593?606. Springer.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive information extraction using unrestricted relation discovery. In
Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the
Association of Computational Linguistics, pages 304?311. Association for Computational Linguistics.
Jannik Str?otgen and Michael Gertz. 2010. Heideltime: High quality rule-based extraction and normalization of temporal
expressions. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 321?324. Association for
Computational Linguistics.
Hans Uszkoreit. 2011. Learning relation extraction grammars with minimal human intervention: strategy, results, insights and
plans. In Computational Linguistics and Intelligent Text Processing, pages 106?126. Springer.
Chang Wang, Aditya Kalyanpur, James Fan, Branimir K Boguraev, and DC Gondek. 2012. Relation extraction and scoring in
deepqa. IBM Journal of Research and Development, 56(3.4):9?1.
Ryen W White and Resa A Roth. 2009. Exploratory search: Beyond the query-response paradigm. Synthesis Lectures on
Information Concepts, Retrieval, and Services, 1(1):1?98.
Alexander Yates, Michael Cafarella, Michele Banko, Oren Etzioni, Matthew Broadhead, and Stephen Soderland. 2007. Tex-
trunner: open information extraction on the web. In Proceedings of Human Language Technologies: The Annual Conference
of the North American Chapter of the Association for Computational Linguistics: Demonstrations, pages 25?26. Association
for Computational Linguistics.
2096
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 81?85, Dublin, Ireland, August 23-29 2014.
Nerdle: Topic-Specific Question Answering Using Wikia Seeds
Umar Maqsud, Sebastian Arnold, Michael H?ulfenhaus and Alan Akbik
Database Systems and Information Management Group
Technische Universit?at Berlin
Einsteinufer 17, 10587 Berlin, Germany
Abstract
The WIKIA project maintains wikis across a diverse range of subjects from areas of popular
culture. Each wiki consists of collaboratively authored content and focuses on a particular topic,
including franchises such as ?Star Trek?, ?Star Wars? and ?The Simpsons?. In this paper, we
investigate the use of such wikis to create Question-Answering (QA) systems for a given topic.
Our key idea is to use a wiki as seed to gather large amounts of relevant text and to use semantic
role labeling (SRL) methods to extract N-ary facts from this data. By applying our method to very
large amounts of topically focused text, we propose to address the coverage issues that have been
noted for QA systems built using such techniques. To illustrate the strengths and weaknesses
of the proposed approach, we make a Web demonstrator of our system publicly available; it
provides a QA view that enables users to pose natural language questions to the system and that
visualizes how questions are interpreted and matched to answers. In addition, the demonstrator
provides a graph exploration view in which users can directly browse the fact base in order to
inspect the scope of the extracted information.
1 Introduction
The WIKIA project operates the largest network of collaboratively authored wikis, consisting of over
390.000 wikis on subjects such as games, entertainment and lifestyle, and is available online at
www.wikia.com. Each wiki is focused on one particular topic and may consist of tens of thousands
of pages of text content. Such data, we argue, provides a unique opportunity for extracting structured
relational data confined to one domain of interest.
In this paper, we investigate such an approach; our overall goal is to automatically create Question
Answering (QA) systems that are ?experts? in one field of interest. Our key idea is to use wikis as
seeds to a focused crawler to gather as much text as possible for a given topic. The more text we can
gather, the greater the chance that we can address coverage issues that related works in QA have noted:
Phenomena such as coreferences, synonyms and paraphrasing may negatively affect a QA systems ability
to find answers to questions that are phrased differently from occurrences in text (Fader et al., 2013;
Ravichandran and Hovy, 2002). By gathering large amounts of topically relevant text and by employing
semantic role labeling (SRL) as a means for fact extraction and representation (Shen and Lapata, 2007),
our hypothesis is that we can sidestep many of these issues.
The main purpose of this demonstration is therefore to illustrate and discuss in how far a direct ap-
plication of SRL to domain-specific text can be employed to create a QA system. To this end, we make
publicly available a Web demonstrator of a QA system on three topics of popular culture, namely the
?Star Trek?, ?Star Wars? and ?The Simpsons? franchises. Our system, named NERDLE, supports eight
types of questions, examples of which are given in Table 1. The fact base on these topics is gathered
with our proposed approach. The demonstrator offers two views that highlight different aspects of the
system: The QA view allows users to pose natural language questions and visualizes how questions are
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
81
STAR TREK THE SIMPSONS STAR WARS
WHO Who attacked the Enterprise? Who is Homer?s father? Who destroyed the Death Star?
WHERE Where was Picard born? Where was Homer born? Where was the Death Star destroyed?
WHEN When did James Kirk die? When was Homer born? When was Vader killed?
HOW How did James Kirk die? How does Homer work? How did Luke destroy the Death Star?
WHOM Whom did the Klingons attack? Whom does Homer know? Whom did Luke Skywalker attack?
WHAT What are Klingons? What does Bart think? What is the Rebel Alliance?
WHY Why was Voyager destroyed? Why was Homer sad? Why does Luke Skywalker die?
WHICH Which captain Which food critic Which Jedi
was born on Earth? was born in Springfield? attacked the Death Star?
Table 1: The eight question types currently supported by our system and example questions for each of
the three topics.
interpreted and how they are matched to answers. The graph exploration view enables users to directly
browse the fact base in order to inspect the scope of the extracted information.
In the following sections, we briefly describe our method for using WIKIA to gather text data for a
given topic. We discuss how we employ SRL for fact extraction and representation and give details on
our method for aligning eight types of natural language questions to these facts. Finally, we discuss the
visualization and results of the created QA system.
2 Method
In this section, we describe our method for gathering relevant text using a wiki seed and extracting facts
using SRL. As a running example, we use the ?Star Trek?-universe as the topic of interest.
2.1 Constructing Topic-Specific Corpora
As a first step, we select the appropriate wiki for the given topic and retrieve all its text as well as the
names of the page titles. For the ?Star Trek? example, the WIKIA project offers two wikis, namely
Memory Alpha and Memory Beta
1
, both of which we select as relevant. Using these wikis, we download
over 95.000 pages or 250 MB of text content for the domain of interest.
We then determine combinations of search keywords using the page titles and use these as queries in
search engines like BING
2
or FAROO
3
. Examples of such queries are ?Kirk Spock Star Trek? or ?Picard
Data Star Trek?. While FAROO (unlike BING) does not limit the number of allowed queries per month,
its index is much smaller. We therefore developed the following strategy to use the FAROO index in our
text gathering effort: For each combination of search keywords, we retrieve all matching pages and then
follow their outgoing links to find more possibly related Web pages. We check each Web page to contain
at least one mention of the domain keyword (?Star Trek?) as a simple sanity check to ensure that the
crawler has not left the domain of interest. We download all pages we reach with this method.
Using this method, we find over 500 MB of text for the ?Star Trek?-domain. As this is an ongoing
effort, the corpus size is expected to expand further. The generated corpus is then passed to the fact
extraction step of the pipeline.
2.2 Fact Extraction
We detect English language sentences in the gathered corpus and apply SRL to detect predicate-argument
structures for each verb. We use the ClearNLP toolkit (Choi and Adviser-Palmer, 2012) for this task;
it links each predicate to a PROPBANK (Martha and Palmer, 2002) verb sense and its arguments to
PROPBANK semantic roles. We choose PROPBANK over FRAMENET (Baker et al., 1998) as it models
semantics more broadly and has a more complete coverage of verb frames. For our purpose, we are
especially interested in the argument roles: PROPBANK gives us verb-specific argument roles as well
as universal roles such as temporals (TMP), locatives (LOC), causal adverbials (CAU) and adverbials
1
Available at http://en.memory-alpha.org/ and http://memory-beta.wikia.com/ respectively.
2
http://www.bing.com
3
http://www.faroo.com/
82
Figure 1: The QA-view of the Web demonstrator. The user types in a question and receives a list of
arguments as answers below. For the question ?Who attacked the Enterprise?? the user receives a total
of 38 answers, three of which are shown here, namely ?Taryn?s ship?, ?an Orion scout ship? and ?the
Ngultor?. By clicking on one of the answers, the user inspects the predicate-argument structures of both
question (lower right graph) and the answer (lower left graph), as well as the sentence in which the
answer is found.
of manner (MNR) and purpose (PRP). In this work, we treat the predicate-argument structures as N-ary
facts and store these in a graph database.
To illustrate the fact extraction process, consider the sentence ?In 2254, the Enterprise was attacked
by the Ngultor?. Using SRL, we determine a predicate-argument structure in which ?attack? is the pred-
icate and there are three arguments: Two arguments with verb-specific roles, namely ?by the Ngultor?
(attacker) and ?the Enterprise? (that which is attacked), as well as the argument ?in 2254?, which is
recognized to be of type AM-TMP, meaning that it confers additional temporal information to the ternary
fact. This predicate-argument structure is illustrated in Figure 1.
2.3 Question Parsing
The question parsing process is similar to fact extraction; we apply SRL to a question to determine its
predicate-argument structure. We then try to find matching predicate-argument structures in the fact base
by searching for facts that share the same predicate and as many arguments as possible. The greater the
number of matching arguments, the higher the score of the matching fact. In addition, we require match-
ing facts to also contain an answer argument labeled with a specific semantic role which is determined
through the question type.
We allow seven basic types of questions and one type of composite question. The basic question types
we support are factual questions beginning with the question words ?who?, ?where?, ?when?, ?whom?,
?what?, ?how? and ?why?. Depending on the question type, we require answer arguments to be labeled
with a different semantic role. For ?where?-questions, for example, we require answer arguments to be
labeled as an AM-LOC argument. For ?when?-questions, the answer argument must be labeled as an
AM-TMP argument. For ?who?-questions, we require an argument that shares the semantic role of the
83
question word, which typically will be either A0 or A1. These answer arguments are returned answers to
the question.
We illustrate this in Figure 1. The question ?Who attacked the Enterprise?? is parsed into a predicate-
argument structure. It is aligned with the predicate-argument structure of the sentence ?In 2254 the
Enterprise was attacked by the Ngultor? because they share the same predicate as well as the argument
?the Enterprise? with the role A1. Of the two remaining arguments, namely ?in 2254? and ?the Ngultor?
the latter is selected as the answer argument because it carries the same semantic role as was assigned to
the token ?who? in the question, namely A0. In case of a ?when?-question, ?in 2254? would instead be
selected as answer argument, as it is labeled with the required AM-TMP role.
In addition, we support one type of composite question, namely questions beginning with ?which?,
as in ?Which captain was born on Earth??. Such questions are decomposed into two separate ?who?-
questions, namely ?Who is a captain?? and ?Who was born on Earth??. We then determine answer
arguments that match both questions and return these.
3 Demonstration
We present a Web demonstrator
4
in which users can query the fact base in one of two views:
QA view. In this view, users pose natural language questions and are presented with matching answers
if they exist. For each answer, both the source sentences as well as the URLs to the original Web pages
are displayed. Answers are ranked by a score which is determined through the number of matching
arguments between the predicate-argument structures of the question and the answer. As illustrated in
Figure 1, users inspect a visualization of these predicate-argument structures. This view is primarily
designed to aid with understanding issues with precision, i.e. to understand how answers to questions
come to be and how fact extraction and question parsing function.
Graph exploration view. In this view, users can browse the graph database directly for facts. Together
with the QA view, this view is designed to examine issues of recall, i.e. to help understand the scope of
the extracted information and why some questions are not answered.
4 Discussion
With our method, we find a total of 7 million facts for ?Star Trek?, 6.5 million for ?Star Wars? and, due
to its smaller wiki size, 3.5 million for ?The Simpsons?. Next to the availability of large amounts of Web
text, our focus on topics of popular culture has the advantage that there are a large number of resources
available online that can be used to analyze the QA capabilities of our system. In our analysis, we make
use of ABSURDTRIVIA
5
, a community powered Web site where users write and rate trivia quizzes on
items of popular culture. The trivia quizzes consist of a set of multiple-choice questions. We crawl 50
of these questions on the NERDLE topics that conform to our question types and pose them to NERDLE.
We find that NERDLE chooses the correct answer for 16 questions, a wrong answer for 5 questions and
no answer at all for 29 questions.
Our demonstrator allows us to inspect wrong and unanswered questions. We find that the system often
either lacks the correct facts in the fact base or cannot align questions to answers due to problems of
synonymy, entailment and coreferences. An example of this is the question ?Who played Phlox?? to
which no answer is found, while the correct answer is found for ?Who portrayed Phlox??. This suggests
that the coverage of the system might be improved by adding knowledge on synonymous arguments
as well as synonymous or entailing verbs. Future work will accordingly examine how synonyms and
entailment could be added to improve the coverage of the system. One idea is to leverage wiki page
links to identify synonymous entities similar to the work presented in (Spitkovsky and Chang, 2012). In
addition, we will expand our crawling efforts to gather larger text corpora and add more question types
to the question parser.
Future work will continue to emphasize the visualization of question parsing and answer alignment in
order to aid discussion with the research community about the strengths and limitations of SRL for QA.
4
The demonstrator is available online at http://www.textmining.tu-berlin.de/nerdle/
5
http://www.absurdtrivia.com
84
Acknowledgements
We would like to thank the anonymous reviewers for their helpful comments. Umar Maqsud, Sebastian Arnold, Michael
H?ulfenhaus and Alan Akbik received funding from the European Union?s Seventh Framework Programme (FP7/2007-2013)
under grant agreement no ICT-2009-4-1 270137 ?Scalable Preservation Environments? (SCAPE).
References
Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The berkeley framenet project. In Proceedings of the 36th
Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational
Linguistics-Volume 1, pages 86?90. Association for Computational Linguistics.
Jinho D Choi and Martha Adviser-Palmer. 2012. Optimization of natural language processing components for robustness and
scalability.
Anthony Fader, Luke S Zettlemoyer, and Oren Etzioni. 2013. Paraphrase-driven learning for open question answering. In ACL
(1), pages 1608?1618.
Paul Kingsbury Martha and Martha Palmer. 2002. From treebank to propbank.
Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In Proceedings
of the 40th Annual Meeting on Association for Computational Linguistics, pages 41?47. Association for Computational
Linguistics.
Dan Shen and Mirella Lapata. 2007. Using semantic roles to improve question answering. In EMNLP-CoNLL, pages 12?21.
Citeseer.
Valentin I Spitkovsky and Angel X Chang. 2012. A cross-lingual dictionary for english wikipedia concepts. In LREC, pages
3168?3175.
85
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 157?162,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Propminer: A Workflow for Interactive Information Extraction and
Exploration using Dependency Trees
Alan Akbik, Oresti Konomi and Michail Melnikov
Technische Univerista?t Berlin
Databases and Information Systems Group
Einsteinufer 17, 10587 Berlin, Germany
firstname.lastname@tu-berlin.de
Abstract
The use of deep syntactic information such
as typed dependencies has been shown
to be very effective in Information Ex-
traction. Despite this potential, the pro-
cess of manually creating rule-based in-
formation extractors that operate on de-
pendency trees is not intuitive for persons
without an extensive NLP background. In
this system demonstration, we present a
tool and a workflow designed to enable
initiate users to interactively explore the
effect and expressivity of creating Infor-
mation Extraction rules over dependency
trees. We introduce the proposed five step
workflow for creating information extrac-
tors, the graph query based rule language,
as well as the core features of the PROP-
MINER tool.
1 Introduction
Information Extraction (IE) is the task of gener-
ating structured information, often in the form of
subject-predicate-object relation triples, from un-
structured information such as natural language
text. Although there are well-established methods
for automatically training extractors from anno-
tated data (Mintz et al, 2009), recent years have
seen a renewed interest in manually created and
maintained rule-based IE systems (Doan et al,
2009; Chiticariu et al, 2010). Advantages of such
systems include a better transparency and explain-
ability of extraction rules, and the resulting main-
tainability and customizability of rule sets.
Another trend in IE is to make increasing
use of deep syntactic information in extrac-
tors (Bunescu and Mooney, 2005), as dependency
parsers become faster and more robust on irregular
text (Bohnet, 2010).
Bringing both trends together are recent works
in the field of Open Information Extraction (OIE).
The systems KRAKEN (Akbik and Lo?ser, 2012)
and CLAUSIE (Del Corro and Gemulla, ) use
a set of hand crafted rules on dependency trees
to outperform previous classification based ap-
proaches. The latter system outperforms even OL-
LIE (Mausam et al, 2012), the machine learning
based state-of-the art OIE system on dependency
parses. Not only does CLAUSIE report significant
precision gains over OLLIE, but also finds 2.5 to
3.5 times more relations.
These results indicate a strong potential for
manually creating rule-based Information Extrac-
tion systems using dependency trees. The higher
level syntactic representation, we argue, may even
facilitate rule writing, as - unlike in shallow lexico-
syntactic rules - much linguistic variation such
as inserted clauses and expressions must not be
specifically addressed. This enables the creation
of more succinct IE rules, leading to better ex-
plainability and easier maintenance.
However, despite these advantages, experience
has shown that deep syntactic information is diffi-
cult to read and understand for non NLP-experts.
In this system demonstration, we propose a
workflow designed to tap into this potential, and
present the PROPMINER tool that allows users to
execute this workflow. It is specifically designed
to help persons familiarize themselves with de-
pendency trees and enable exploration and extrac-
tion of relations from parsed document collec-
tions. Core features of PROPMINER are:
Rule generation and modification. Initiate
users are guided by a workflow in which they first
enter and annotate an archetypical sentence with
the desired relation. A rule generation process
then pre-generates an overspecified rule that users
modify along lines suggested by the tool. Our pre-
liminary experiments show that this workflow of
generating and modifying rules eases the learn-
ing curve for non NLP-experts to concepts such
as part-of-speech tags and typed dependencies.
157
Figure 1: Sentence view of PROPMINER, where steps one and two of the workflow are executed. Users
enter (or select) a sentence in the top input field and annotate subject, predicate and object for the desired
relation. A rule is generated and displayed in the upper right panel. The lower right panel is the repository
of already created rules. The parse of the input sentence is displayed in the center panel.
Interactivity and feedback. Each modifica-
tion of a rule is immediately queried against a
large collection of parsed sentences stored in a dis-
tributed graph database. The extraction results of
the current state of the rule are presented at all
times to the user, thereby explaining the rule by
showing its effect.
Intuitive query language. Extraction rules are
formulated as queries against a graph database.
Our query language allows users to formulate sub-
tree queries as path expressions, a concept bor-
rowed from the SerQL query language (Broekstra
and Kampman, 2003) because of its intuitive prop-
erties. We show a visualization of the parse tree of
the current sentence next to the generated rule to
ease users into understanding the query language
(see Figure 1).
Guided workflow. All structured information
generated by the user, such as extraction rules,
sentence annotations and evaluation results, are
stored to build up a repository of structured infor-
mation. This information is used to suggest appro-
priate actions to the user.
A preliminary study shows that users with-
out any NLP background are quickly able to
use PROPMINER to create Information Extraction
rules. We noted that users at first stay true to the
workflow and limit manual effort to generalizing
rules, but tend to more directly modify extraction
rules as they grow more experienced. Further-
more, PROPMINER?s interactive nature eases the
process of understanding typed dependencies and
enables the interactive exploration of parsed doc-
ument collections.
2 Workflow and Query Language
PROPMINER implements a workflow that con-
sists of five steps (Annotate, Generate, General-
ize, Evaluate and Store). It is designed to allow
users that are unfamiliar with syntactic annotation
to create rule-based extractors. In the following
subsections, we explain the five steps in detail. As
a running example, we use the task of creating an
extractor for the PERSONBIRTHPLACE relation.
2.1 Annotate
Users begin the process by constructing an
archetypical sentence for the desired information
type. This sentence constitutes an example that
expresses the desired relation. For instance, a
user interested in the PERSONBIRTHPLACE rela-
tion can choose a sentence such as ?Albert Ein-
stein was born in Germany.?.
In this sentence, the user annotates the words
158
belonging to the relation triple, assigning the roles
of subject, predicate and object. Subject and ob-
ject are the entities in the example between which
the relation holds. The predicate are the words
in the sentence that express the relationship. Ac-
cordingly, the user marks ?Albert Einstein? and
?Germany? as subject and object, and ?born in?
as predicate in the example sentence.
Figure 1 shows the sentence view of PROP-
MINER, with the example sentence entered and an-
notated in the top input fields, and the parsed sen-
tence shown in the center panel.
2.2 Generate
PROPMINER generates a rule from the annotated
sentence by determining the minimal subtree in
the sentence?s dependency tree that connects all
words labeled as subject, predicate and object.
The rule consists of this minimal subtree, as well
as constraints in the part-of-speech (POS) tags and
lexical values of all involved words.
Rules are formulated as queries against a
database in which parsed sentences are stored as
graphs: Nodes represent words and edges repre-
sent typed dependencies. At each node, the POS
tag and the lexical value of the word are stored as
attributes.
A PROPMINER rule (or query) consists mainly
of three parts: A SELECT clause, a FROM clause
and a WHERE clause. The generated rule for the
running example is displayed in Figure 1. Its indi-
vidual parts are discussed in the following subsec-
tions.
2.2.1 SELECT and FROM
The SELECT clause determines the fields of tu-
ple to be returned by the query. Typically, this
consists of a subject-predicate-object triple, but
queries with fewer or more fields are possible.
The FROM clause is a path expression that
specifies the subgraph in the dependency tree the
rule must match, and defines which nodes in the
subgraph correspond to the fields in the SELECT
clause. A path expression is a set of node-edge-
node triples. Each of these triples defines one edge
(typed dependency) that must hold between two
nodes (words). The nodes are denoted in curly
brackets, where the text inside curly brackets as-
signs a variable name to the node.
Consider the SELECT and FROM clauses for
the rule generated for the running example, illus-
trated in the following:
SELECT subject, predicate, objectFROM {_7_}  nsubj {subject}
{_7_}  cop {predicate}{_7_}  amod {object}
SELECT subject, predicate, objectFROM      {predicate.3}   nsubjpass {subject},
      {predicate.3}   prep {predicate.4},      {predicate.4}   pobj {object}
WHERE
AND subject POS ?NNS?
AND predicate.3 POS?VBN?
AND predicate.4 POS ?IN?
AND object POS ?NNP?
AND subject TEXT ?A. Einstein?
AND predicate.3 TEXT ?born?
AND predicate.4 TEXT ?in?
AND object TEXT ?Ulm?
AND subject FULL_ENTITY
Here, the SELECT statement defines the de-
sired result of this query, namely a tuple with a
?subject?, ?object? and a ?predicate? field: The
path expression in this example is specified in the
three lines in the FROM statement. It defines a
subtree that consists of four nodes connected by
three typed dependencies.
The nodes are assigned the variable names
?subject?, ?object?, ?predicate.3? and ?predi-
cate.4?. The node ?subject? is defined to be a
passive subject (typed dependency ?nsubjpass?) of
the node ?predicate.3?. The node ?predicate.3? is
also connected via the dependency ?prep? to the
node ?predicate.4?, which in turn is connected to
?object? with the dependency ?pobj?.
If this rule matches, the lexical values of the
matching nodes are returned. Because the predi-
cate in this example consists of two words (?born?
and ?in?), two nodes are assigned the ?predicate?
value, subtyped per convention with a dot and a
number (?predicate.3? and ?predicate.4?).
2.2.2 WHERE
In the WHERE-clause, the attributes of words
in the subtree can be further restricted. Auto-
generated rules are maximally restricted. The rule
for the running example is initially restricted as
follows:
SELECT subject, predicate, objectFROM {_7_}  nsubj {subject}
{_7_}  cop {predicate}{_7_}  amod {object}
WHERE
.34 subject AND P33AO
.34 predicateS? AND P?V3O
.34 predicateSBAND PI3O
.34 object AND P33AO
.34 subject TEXT PEinsteinO
.34 predicateS? TEXT PbornO
.34 predicateSBTEXT PinO
.34 object TEXT PUermanlO
.34 subject FLYY_E3TIT?
Word attributes are restricted by naming the
variable followed either by ?POS? or ?TEXT? and
the restricting value. Here, for instance, the POS
tag of the ?object? node is restricted to ?NNP? (a
proper noun), and its lexical value is restricted to
?Germany?.
159
a) Generated rule b) Generalize subject text c) Generalize subject and object
SELECT subject, predicate, objectFROM {_7_}  nsubj {subject}{_7_}  cop {predicate}{_7_}  amod {object}
SELECT subject, predicate, objectFROM { collapsed }WHEREsubject POS ?NNP?AND predicate.3 POS ?VBZ?AND predicate.4 POS ?IN?AND object POS ?NNP?AND subject TEXT ?Einstein?AND predicate.3 TEXT ?born?AND predicate.4 TEXT ?in?AND object TEXT ?Germany?AND subject ALLCHILDREN
Subject Predicate ObjectA. Einstein born in Germany Subject Predicate ObjectA. Einstein born in Germany
C. F. Gauss born in Germany
A. Humboldt born in Germany
... ... ...
Subject Predicate ObjectA. Einstein born in Germany
J. Lagrange born in Italy
I. Newton born in England
... ... ...
SELECT subject, predicate, objectFROM { collapsed }WHEREsubject POS ?NNP?AND predicate.3 POS ?VBZ?AND predicate.4 POS ?IN?AND object POS ?NNP?AND subject TEXT ?Einstein?AND predicate.3 TEXT ?born?AND predicate.4 TEXT ?in?AND object TEXT ?Germany?AND subject ALLCHILDREN
SELECT subject, predicate, objectFROM { collapsed }WHEREsubject POS ?NNP?AND predicate.3 POS ?VBZ?AND predicate.4 POS ?IN?AND object POS ?NNP?AND subject TEXT ?Einstein?AND predicate.3 TEXT ?born?AND predicate.4 TEXT ?in?AND object TEXT ?Germany?AND subject ALLCHILDREN
Figure 2: Conceptual example of rule modification through generalization. Below are example relation
triples found for each rule. Rule a) is generated from the annotated sentence in the running example,
and finds only one triple. Rule b) is the same rule without the restriction in the subject text. The rule
now finds a number of relation triples in the document collection, representing different entities born in
Germany. In Rule c) both subject and object text restrictions are removed. This yields a rule that finds
different entities born in any entity.
Additionally, a number of subtree gathering
mechanisms can be specified in the WHERE
clause. For example, the keyword FULL ENTITY
causes the variable binding for the subject to ex-
pand to all children nodes expected to be part of a
named entity.
2.3 Generalize
The rule generated in step two of the workflow is
strongly overspecified to the annotated sentence;
all features, including the shallow syntactic and
lexical values of all words in the subtree, are con-
strained. The resulting rule only finds exact in-
stances of the relations as seen in the archetypical
sentence. Refer to Figure 2 a) for an example.
In step three of the workflow, the user general-
izes the auto-generated rule with the help of sug-
gestions. Common lines of generalizing rules fo-
cus on the WHERE clause; here, users can remove
or modify constraints on the attributes of words.
For example, by removing the restriction on the
lexical value of the subject, the rule is generalized
to finding all entities that were born in ?Germany?,
instead of only entities with the lexical value ?Ein-
stein?. This example is illustrated in Figure 2 b).
The rule can then be further generalized by re-
moving the lexical constraint on the object, yield-
ing the (desired) rule that finds all entities that
were born in any location with an entity name.
Figure 2 c) shows an example of this rule, as well
as example results.
Further options of generalization include re-
moving the lexical constraints in one or both of the
predicate words, or modifying the POS tag con-
straints. At each modification, extraction results
for the current state of the rule are displayed to as-
sist the user. When the results match the desired
relation, the user can proceed to the next step in
the workflow.
2.4 Evaluate
Each rule created by the user is evaluated in the
corpus view of PROPMINER, displayed in Fig-
ure 3. This view shows a sample of extraction
results of the rule in a table. The user can scroll
through the table and in each row see the extracted
information as well as the sentence the informa-
tion was extracted from. If the extracted informa-
tion matches the statement in the sentence, the user
can mark this fact as correct.
2.5 Store
If the user is satisfied with the extraction rule, he
can assign it to a relation and store it in the rule
repository. He can repeat the process with another
sentence to find more patterns for the desired rela-
tion. As the workflow is repeated, the rule reposi-
tory will build up, along with a repository of evalu-
160
Figure 3: Corpus view of PROPMINER, where extraction rules are modified and evaluated. The center
panel is a table that holds the extraction results for the current rule. Users can inspect each extracted
triple by clicking on the row. This will display the sentence in which the triple was found.
ation results. This enables additional functionality
in subsequent executions of the workflow:
Sentence suggestions. Evaluation results are
used to assist the user in finding new sentences
that might be relevant to the relation. For exam-
ple, a user might mark a triple with the subject ?C.
F. Gauss? and object ?Germany? as a correct in-
stance of the PERSONBIRTHPLACE relation dur-
ing evaluation. PROPMINER uses this informa-
tion to retrieve all sentences that contain these two
entities from its database. These sentences are
treated as probable candidates for containing the
PERSONBIRTHPLACE relation, because they con-
tain two entities known to be in this relationship.
Accordingly, they are suggested to the user upon
request.
Conflict resolution. In order to prevent con-
flicts with existing rules, the entire rule set in the
repository is applied to each sentence the work-
flow is started with. If any existing information
extraction rule can be applied, the results of the
extraction are presented to the user as annotations
in the sentence. If this extraction result is already
complete from the point of view of the user, he
can proceed to a new sentence. If not, the user can
proceed to generate a new rule, or modify existing
ones.
3 Previous Work
Previous work on improving the rule creation pro-
cess for IE systems has mainly focused on assist-
ing users with machine learning techniques, such
as pre-generation of regular expressions (Brauer et
al., 2011) or pattern suggestions (Li et al, 2011).
To improve usability, (Li et al, 2012) present a
tool with a wizard-like environment to guide ex-
tractor development. While previous work focuses
on shallow patterns, the focus of PROPMINER is to
help create rules over dependency trees and aid in
the exploration of parsed document collections.
4 Evaluation and Outlook
We conducted a preliminary study in which we
asked 5 computer scientists unfamiliar with com-
putational linguistics to use the tool to cre-
ate extractors for the relations PERSONBIRTH-
PLACE, PERSONMARRIEDTOPERSON and PER-
SONWONPRIZE. The participants were given a
two hour introduction explaining information ex-
traction and subject-predicate-object triples. We
introduced them to the five step workflow using
the PERSONBIRTHPLACE example also used as
running example in this paper, as well as other,
more complex examples. The participants were
given one hour for each relation and asked to cre-
161
ate a rule set for each relation. After the con-
clusion we interviewed the participants and asked
them to rate the usability both for information ex-
traction, as well as for the exploration of depen-
dency tree information.
In the latter category, participants generally
gave positive feedback. Participants stated that the
interactive nature of the tool helped understanding
extraction rules and facilitated exploring informa-
tion stated in the document collection. 4 out of
5 participants deviated from the suggested work-
flow and more directly edited rules as they be-
came more comfortable with the tool. All par-
ticipants consulted information on POS tags and
typed dependencies during the process, in order to
better understand the rules and query results. Par-
ticipants suggested adding an explanation function
for individual syntactic elements to the tool.
While all users were generally able to create
rule sets for each of the relations, two main prob-
lems were cited for the creation of extraction rules.
The first is a problem in conflict resolution; in
some cases, users were not able to discern why
a rule gave imperfect extraction results. We re-
viewed some rules and found that many of these
cases stem from faulty dependency parses, which
non NLP-experts cannot recognize. At present, we
are searching for ways to address this problem.
A second problem were limitations of the rule
language: Participants expressed the need for
named entity types such as PERSON and LOCA-
TION, which in the prototype were not included at
the time of evaluation. However, because of the
design of the query language and the underlying
graph database, such additional operators can be
incorporated easily.
Consequently, current work focuses on extend-
ing the range of user studies to gather more sug-
gestions for the query language and the feature set,
and integrating additional operators into the sys-
tem.
5 Demonstration
In this demonstration we show how PROPMINER
can be used for creating extractors or exploring
the parsed document collection. The hands-on
demonstration allows initiate users to execute the
workflow presented in this paper, but also enables
persons more familiar with syntactic annotation to
more directly query the graph database using our
query language and feature set.
Acknowledgements
We would like to thank the anonymous reviewers for their
helpful comments. Alan Akbik received funding from the Eu-
ropean Union?s Seventh Framework Programme (FP7/2007-
2013) under grant agreement no ICT-2009-4-1 270137 ?Scal-
able Preservation Environments? (SCAPE).
References
Alan Akbik and Alexander Lo?ser. 2012. Kraken: N-ary facts
in open information extraction. In AKBC-WEKEX, pages
52?56. Association for Computational Linguistics.
Bernd Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In COLING, pages
89?97. Association for Computational Linguistics.
Falk Brauer, Robert Rieger, Adrian Mocan, and Wojciech M
Barczynski. 2011. Enabling information extraction by
inference of regular expressions from sample entities. In
CIKM, pages 1285?1294. ACM.
Jeen Broekstra and Arjohn Kampman. 2003. Serql: a second
generation rdf query language. In Proc. SWAD-Europe
Workshop on Semantic Web Storage and Retrieval, pages
13?14.
Razvan C Bunescu and Raymond J Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
EMNLP, pages 724?731. Association for Computational
Linguistics.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li,
Sriram Raghavan, Frederick R Reiss, and Shivakumar
Vaithyanathan. 2010. Systemt: an algebraic approach to
declarative information extraction. In ACL, pages 128?
137. Association for Computational Linguistics.
Luciano Del Corro and Rainer Gemulla. Clausie: Clause-
based open information extraction. In WWW (to appear
in 2013).
AnHai Doan, Jeffrey F Naughton, Raghu Ramakrishnan,
Akanksha Baid, Xiaoyong Chai, Fei Chen, Ting Chen,
Eric Chu, Pedro DeRose, Byron Gao, et al 2009. In-
formation extraction challenges in managing unstructured
data. ACM SIGMOD Record, 37(4):14?20.
Yunyao Li, Vivian Chu, Sebastian Blohm, Huaiyu Zhu, and
Howard Ho. 2011. Facilitating pattern discovery for rela-
tion extraction with semantic-signature-based clustering.
In CIKM, pages 1415?1424. ACM.
Yunyao Li, Laura Chiticariu, Huahai Yang, Frederick R
Reiss, and Arnaldo Carreno-fuentes. 2012. Wizie: a best
practices guided development environment for informa-
tion extraction. In Proceedings of the ACL 2012 System
Demonstrations, pages 109?114. Association for Compu-
tational Linguistics.
Mausam, Michael Schmitz, Stephen Soderland, Robert Bart,
and Oren Etzioni. 2012. Open language learning for in-
formation extraction. In EMNLP-CoNLL, pages 523?534.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction without
labeled data. In ACL/IJCNLP. Volume 2-Volume 2, pages
1003?1011. Association for Computational Linguistics.
162
Proceedings of the AHA! Workshop on Information Discovery in Text, pages 14?18,
Dublin, Ireland, August 23 2014.
Extracting a Repository of Events and Event References
from News Clusters
Silvia Julinda
TU Berlin
Einsteinufer 17
Berlin, Germany
silvia.julinda@gmail.com
Christoph Boden
TU Berlin
Einsteinufer 17
Berlin, Germany
christoph.boden@tu-berlin.de
Alan Akbik
TU Berlin
Einsteinufer 17
Berlin, Germany
alan.akbik@tu-berlin.de
Abstract
In this paper, we prose to build a repository of events and event references from clusters of
news articles. We present an automated approach that is based on the hypothesis that if two
sentences are a) found in the same cluster of news articles and b) contain temporal expressions
that reference the same point in time, they are likely to refer to the same event. This allows
us to group similar sentences together and apply open-domain Information Extraction (OpenIE)
methods to extract lists of textual references for each detected event. We outline our proposed
approach and present a preliminary evaluation in which we extract events and references from 20
clusters of online news. Our experiments indicate that for the largest part our hypothesis holds
true, pointing to a strong potential for applying our approach to building an event repository. We
illustrate cases in which our hypothesis fails and discuss ways for addressing sources or errors.
1 Introduction
We present ongoing work in the automatic creation of a repository of events and event references from
clusters of online news articles. In the context of this work, an event is something that happens at one
specific point in time that can be referenced in text with different text surface forms. An example of
this may be the acquisition of WhatsApp by Facebook, which has a specific timestamp (02-19-2014),
as well as a number of different textual references (such as ?the acquisition of WhatsApp?, ?Facebook?s
landmark deal? etc). Unlike previous work in event extraction (Aone and Ramos-Santacruz, 2000; Ji and
Grishman, 2008), we are less interested in filling slots in a fixed set of event templates. Rather, we aim
to identify an unrestricted set of events (Ritter et al., 2012) and all their possible event mentions. This
means that even noun phrases (?the much-discussed takeover?) and incomplete mentions (?Zuckerberg?s
19 billion bet?) are valid textual references we wish to capture.
We give examples of such events in Table 1. We believe that automatically distilling such events
from news text and hosting them in an event repository could provide a valuable resource to gain a
comprehensive overview of world events and also serve as a resource for event-linking efforts in future
Information Extraction (IE) research.
In this paper, we propose a method for automatically creating such an event repository. Our method
leverages computer-generated news sites that aggregate articles from news sources worldwide and group
similar stories into news clusters. Such news clusters represent an intriguing reservoir for event extrac-
tion: Each cluster typically represents one news item that is reported on by hundreds of different online
sources. Articles in a cluster will therefore describe similar information content - and reference the same
events - using different words. On these news articles, we apply temporal expression taggers to identify
and normalize textual references to specific points in time.
Our main hypothesis is that if two sentences are a) found in the same cluster of news articles and b)
contain temporal expressions that reference the same point in time, they are likely to refer to the same
event. This allows us to group similar sentences together and for each referenced point in time extract an
event with a list of different textual references.
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers.
Licence details: http://creativecommons.org/licenses/by/4.0/
14
ID TIMESTAMP REPRESENTATIVE TEXTUAL REFERENCES
1 2014-02-19 Facebook buys WhatsApp Facebook buying WhatsApp
the landmark deal
Zuckerberg?s acquisition of the mobile messaging-service
2 2014-02-01 Rosetta transmits message Rosetta sends signal to Earth
the spacecraft?s first message
the message from the Rosetta spacecraft
3 2014-02-07 Sinabung volcano erupts Indonesian volcano unleashed a major eruption
the eruption of Mount Sinabung volcano
its biggest eruption yet
Table 1: Examples for events in the event repository. Each extracted event consists of an ID, a timestamp
which indicates on which date the event took place, a short human-readable event representation, and a
list of strings that may be used to reference this event.
In this paper, we present our event extraction system and conduct a preliminary evaluation in order to
determine in how far our hypothesis holds. We discuss the evaluation results and possible improvements
and give an outline of current and future work.
2 Event and Reference Extraction
2.1 Method Overview
Determine sentences likely to reference the same event. We begin the event extraction process by
crawling Google News
1
to retrieve clusters of English language news articles and their publishing date.
Each news article is then boilerplated and segmented into sentences.
We then make use of temporal expression taggers (Str?otgen and Gertz, 2010; Chang and Manning, 2012)
to recognize temporal expressions in text and normalize them into machine-readable timestamps. This
causes expressions such as ?last Friday?, ?winter of 2013?, and ?Saturday morning? to be normalized
to the timestamps ?2013-10-10?, ?2013-WI?, and ?2012-09-24TMO? respectively by using the article?s
publishing date as a reference point. We identify all sentences with temporal expressions and group
sentences together that a) contain the same timestamp and b) are found in the same cluster of documents.
Refer to Table 2 for examples of sentences grouped according to these criteria.
Determine Open-Domain Facts. Because sentences may refer to multiple events
2
, we use OpenIE
methods (Schmitz et al., 2012; Choi, 2012) to determine for each sentence a list of N-ary facts. Each
fact consist of a predicate and a number of arguments. We then discard all facts that do not contain
the temporal expression in order to keep only those facts expressed within each sentence to which the
temporal expression refers. This gives us a list of N-ary facts which we presume to refer to the same
event, together with its timestamp.
Determine Event Representative and Store. For human readability purposes, we then identify a rep-
resentative of the grouped N-ary facts by determining the most common predicate and head arguments.
We assign a global ID to each event and store it along with its timestamp, its representative and a list of
all textual references and their frequency counts in a database.
2.2 Granularity of Timestamps
One question at the onset of this work was which granularity of temporal expressions would be required.
We manually inspected a sample of news clusters and noted that news articles rarely provide time in-
formation that is accurate to the minute. Rather, most temporal expressions refer to specific dates in
past, present or future. We therefore choose the unit ?day? as granularity for the temporal expressions in
this work. We dismiss all expressions that refer to larger and more vague periods of time (?last winter?,
1
http://news.google.com/
2
An example of this is the sentence: ?When asked, he said that WhatApp accepted Facebook?s offer last Sunday?. Here, the
temporal expression ?last Sunday? refers only the ?WhatApp accepted Facebook?s offer? part of the sentence, not the date the
person was asked.
15
TIMESTAMP SENTENCES
2014-02-20 Facebook inked a deal late Wednesday to buy popular texting service WhatsApp.
Yesterday , Facebook Chief Executive Officer Mark Zuckerberg bought their five-year-old company.
Thursday , 20 February 2014 Facebook Inc will buy fast-growing mobile-messaging startup WhatsApp.
Facebook Inc. agreed to buy mobile messaging-service WhatsApp today for as much as 19 billion.
2014-02-01 The European Space Agency received the all-clear message from its Rosetta spacecraft at 7:18 p.m.
[..] a comet-chasing spacecraft sent its first signal back to Earth on Monday
ESA received the all-clear message Hello World from its Rosetta spacecraft [..] away shortly after 7 pm.
Yesterday?s message from the Rosetta spacecraft was celebrated by scientists [..]
2014-02-07 Indonesia?s Mount Sinabung volcano erupted and killed at least 11 people [..] on Saturday
But a day later , Sinabung spewed hot rocks and ash up to 2km in the air.
A giant cloud of hot volcanic ash clouds engulfs villages [..] in Sumatra island on February 1, 2014.
An Indonesian volcano that has been rumbling for months unleashed a major eruption Saturday.
Table 2: Examples of sentences grouped by cluster and timestamp. The temporal expression taggers
enable us to group sentences that refer to the same point of time in very different ways (highlighted
bold). As can be seen in the examples, sentences grouped according to these criteria generally refer to
the same event, albeit in sometimes widely varying words.
?throughout the year?) and generalize all temporal information that refer to the time of day (?later today?,
?at 7:18 p.m.?).
2.3 Improving Event Quality
Upon manual inspection of identified events we find that our hypothesis fails in some cases: A news
item may often summarize a number of smaller events that happened within the same day. An example
of this are news items that deal with unrest in war-torn countries that may reference several bombings,
terrorist attacks and other violence that happened across the country on the day the article was published.
Another example are sports articles that refer to several sport matches that take place during the same
day. This is problematic, as in such cases we erroneously link non-synonymous textual references to the
same event. We experiment with two methods for reducing this error:
Time Window Filter As indicated above, we note that our hypothesis most often fails for events that
occur within 2 days of the publishing date of the articles in the news cluster. Accordingly, we
experiment with filtering out such events, leaving only events to be extracted that lie in the more
distant past or future (such as past or upcoming election days, significant events that impact the
current news story). However, since the largest part of events that are reported on in online news
take place within this 2-day time window, we risk significant recall-loss by discarding too many
events.
Word Alignment Condition For this reason, we investigate requirements for facts to be grouped to-
gether in addition to the requirement of sharing the same timestamp. We experiment with mono-
lingual word-alignment tools (Yao et al., 2013) to determine the ?similarity? of two facts as the
number of aligned content words. We then require at least one content word to be shared by two
facts in order for them to be grouped together into an event.
3 Preliminary Evaluation
We conduct a preliminary evaluation to determine to which extend our hypothesis holds. To this end,
we use our method to extract events from a sample of 20 news clusters with an average size of 200
news articles. We evaluate our method in four setups: 1) The baseline setup in which we apply only the
?same cluster + same timestamp = same event? hypothesis (?BASE?). 2) The baseline setup plus the
time window filter (?TIME?). 3) The baseline setup plus the word alignment condition (?ALIGN?). 4)
The baseline setup plus both the time window filter and the word alignment condition (?ALL?).
We manually evaluate each event found with our method by checking whether all references indeed
refer to the same event. We calculate a purity value that indicates the proportion of the biggest group
16
METHOD TOTAL EVENT REFERENCES CORRECT EVENT REFERENCES PRECISION PURITY
BASE 609 511 0.839 0.698
TIME 109 88 0.807 0.699
ALIGN 609 526 0.864 0.793
ALL 109 89 0.817 0.728
Table 3: The results of our manual evaluation of extracted events and their event references. The main
hypothesis that events mentioned with the same date in one cluster delivers quite promising results with
84% precision. The time window filter does not seem to contribute significant gains, while the ALIGN
filter does boost both precision and purity.
of references that refer to the same event over all references in an event cluster. This means that if all
references indeed refer to the same event, its purity is 1.0. Table 3 lists the average purity over all events.
When a reference accurately represents both the content and the date contained in the original news
sentence and the real world event mentioned actually occurred on this date, we labeled it as a ?correct?
event reference. The precision listed in Table 3 reflects the proportion of correct events references vs. all
extracted event reference in the evaluation data set. This measure indicates how well the extraction itself
performs, apart from the clustering of event references.
Hypothesis produces promising results with a precision of 0.84. In general, we find our underlying
assumption to indeed be a good basis for event extraction. Our baseline approach based on only this
hypothesis produces promising results with a precision of 0.84, albeit at somewhat low overall purity.
Wrong resolution of relative time references biggest source of error. When inspecting sources of er-
rors more closely, we note that the approach fails most often because of erroneous resolution of relative
time references such as ?yesterday?, ?past Saturday? or ?this Sunday?. This may happen because the
wrong publishing date is assigned to a crawled news article, causing temporal taggers to use a wrong
reference point for relative time expressions. With relative references to weekdays, the taggers are often
unsure whether the past or present week is referenced. Consider the expression ?on Saturday? in the
sentence ?John Kerry will meet with opposition leaders on Saturday?. Although the coming Saturday is
meant in this context, the temporal expression tagger normalizes the date to the last Saturday before the
publishing date. We believe that such systematic errors can be addressed in future work through assign-
ing higher confidence to explicit temporal expressions mentions and resolving ambiguities in relative
expressions using this information.
Time Window Filter provides no significant contribution. Contrary to initial assumption filtering out
events within a 2-day time window does not actually boost precision, but rather greatly reduces the total
number of extracted events at slightly lower precision and purity. The likely reason for this behavior is
the above noted most common error source is not addressed by this filter.
Word Alignment Condition boosts both precision and purity significantly. The word alignment
condition on the other hand greatly increases both precision and purity. While the increase in purity is
to be expected as different events occurring on the same date are indeed split into separate clusters, the
increase in precision comes as somewhat of a surprise. Closer inspection of the results revealed that
the word alignment approach aggressively groups similar event mentions, considering also synonyms as
matches, therefore not resulting in redundant event detections as initially feared. Based on these results,
we believe that experimentation with word alignment conditions may further increase event detection
quality.
4 Conclusion
In this paper, we have proposed to create a repository of events and their textural references and presented
an approach to accomplish this automatically by leveraging news clusters and temporal expressions. Our
approach is based on the hypothesis that sentences that are found in the same news cluster and refer to
the same point in time also refer to the same events. We described the implementation of a prototype
system and conducted a preliminary manual evaluation on 20 news clusters to investigate our hypothesis.
Our findings generally point to a strong potential of automatically mining events and references from
17
news clusters. While our hypothesis fails in some cases, our analysis indicates that incorporating mono-
lingual word-alignment techniques can greatly improve extraction quality and appears to be a powerful
tool to disambiguate events that share both timestamp and news cluster.
Present work focuses on further exploring the potential of word alignment as well as the use of cluster-
wide statistics to correct labeling mistakes such as the ones observed for temporal tagging. We aim to
use the system on very large amounts of news clusters crawled from the Web to generate - and make
publicly available - the resource that we have proposed in this paper.
Acknowledgments
The research is funded by the European Union (EU) under grant no. 270137 ?SCAPE? in the 7th Frame-
work Program and the German Federal Ministry of Education and Research (BMBF) under grant no.
01ISI2033 ?RADAR?.
References
Chinatsu Aone and Mila Ramos-Santacruz. 2000. Rees: a large-scale relation and event extraction system. In
Proceedings of the sixth conference on Applied natural language processing, pages 76?83. Association for
Computational Linguistics.
Angel X Chang and Christopher Manning. 2012. Sutime: A library for recognizing and normalizing time expres-
sions. In LREC, pages 3735?3740.
J. D. Choi. 2012. Optimization of natural language processing components for robustness and scalability.
Heng Ji and Ralph Grishman. 2008. Refining event extraction through cross-document inference. In ACL, pages
254?262.
Alan Ritter, Oren Etzioni, Sam Clark, et al. 2012. Open domain event extraction from twitter. In Proceedings of
the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1104?1112.
ACM.
Michael Schmitz, Robert Bart, Stephen Soderland, Oren Etzioni, et al. 2012. Open language learning for infor-
mation extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning, pages 523?534. Association for Computational
Linguistics.
Jannik Str?otgen and Michael Gertz. 2010. Heideltime: High quality rule-based extraction and normalization of
temporal expressions. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 321?
324. Association for Computational Linguistics.
Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch, and Peter Clark. 2013. A lightweight and high perfor-
mance monolingual word aligner. In Proceedings of ACL.
18
