Squibs and Discussions
Evaluating Discourse and Dialogue
Coding Schemes
Richard Craggs?
University of Manchester
Mary McGee Wood?
University of Manchester
Agreement statistics play an important role in the evaluation of coding schemes for discourse and
dialogue. Unfortunately there is a lack of understanding regarding appropriate agreement mea-
sures and how their results should be interpreted. In this article we describe the role of agreement
measures and argue that only chance-corrected measures that assume a common distribution of
labels for all coders are suitable for measuring agreement in reliability studies. We then provide
recommendations for how reliability should be inferred from the results of agreement statistics.
Since Jean Carletta (1996) exposed computational linguists to the desirability of using
chance-corrected agreement statistics to infer the reliability of data generated by apply-
ing coding schemes, there has been a general acceptance of their use within the field.
However, there are prevailing misunderstandings concerning agreement statistics and
the meaning of reliability.
Investigation of new dialogue types and genres has been shown to reveal new
phenomena in dialogue that are ill suited to annotation by current methods and also
new annotation schemes that are qualitatively different from those commonly used
in dialogue analysis. Previously prescribed practices for evaluating coding schemes
become less applicable as annotation schemes become more sophisticated. To compen-
sate, we need a greater understanding of reliability statistics and how they should be
interpreted. In this article we discuss the purpose of reliability testing, address certain
misunderstandings, and make recommendations regarding the way in which coding
schemes should be evaluated.
1. Agreement, Reliability, and Coding Schemes
After developing schemes for annotating discourse or dialogue, it is necessary to
assess their suitability for the purpose for which they are designed. Although no
statistical test can determine whether any form of annotation is worthwhile or how
applications will benefit from it, we at least need to show that coders are capable of
performing the annotation. This often means assessing reliability based on agreement
between annotators applying the scheme. Agreement measures are discussed in detail
in section 2.
Much of the confusion regarding which agreement measures to apply and how their
results should be interpreted stems from a lack of understanding of what it means to
? School of Computer Science, University of Manchester, Manchester, M13 9PL, U.K.
E-mail: richard craggs@yahoo.co.uk; mary mcgee.wood@manchester.ac.uk.
? 2005 Association for Computational Linguistics
Computational Linguistics Volume 31, Number 3
assess reliability. For example, the coding manual for the Switchboard DAMSL
dialogue act annotation scheme (Jurafsky, Shriberg, and Biasca 1997, page 2) states that
kappa is used to ?assess labelling accuracy,? and Di Eugenio and Glass (2004) relate
reliability to ?the objectivity of decisions,? whereas Carletta (1996) regards reliability as
the degree to which we understand the judgments that annotators are asked to make.
Although most researchers recognize that reporting agreement statistics is an important
part of evaluating coding schemes, there is frequently a lack of understanding of what
the figures actually mean.
The intended meaning of reliability should refer to the degree to which the data
generated by coders applying a scheme can be relied upon. If we consider the coding
process to involve mapping units of analysis onto categories, data are reliable if coders
agree on the category onto which each unit should be mapped. The further from perfect
agreement that coders stray, the less we can rely on the resulting annotation.
If data produced by applying a scheme are shown to be reliable, then we have
established two important properties of those data:
1. The categories onto which the units are mapped are not inordinately
dependent on the idiosyncratic judgments of any individual coder.
2. There is a shared understanding of the meaning of the categories and how
data are mapped onto them.
The first of these is important for ensuring the reproducibility of the coding. To be able to
trust the analysis of annotated corpora, we need to be confident that the categorization
of the units of data is not dependent on which individual performed the annotation. The
second governs the value of data resulting from the coding process. For an annotated
corpus or the analysis thereof to be valuable, the phenomenon being annotated must
represent some notion in which we can enjoy a shared understanding.
2. Agreement Measures
There are many ways in which the level of agreement between coders can be evaluated,
and the choice of which to apply in order to assess reliability is the source of much con-
fusion. An appropriate statistic for this purpose must measure agreement as a function
of the coding process and not of the coders, data, or categories. Only if the results of
a test are solely dependent on the degree to which there is a shared understanding of
how the phenomena to be described are mapped to the given categories can we infer the
reliability of the resulting data. Some agreement measures do not behave in this manner
and are therefore unsuitable for evaluating reliability.
A great deal of importance is placed on domain specificity in discourse and dialogue
studies and as such, researchers are often encouraged to evaluate schemes using corpora
from more than one domain. Concerning agreement, this encouragement is misplaced.
Since an appropriate agreement measure is a function of only the coding process, if the
original agreement test is performed in a scientifically sound manner, little more can
be proved by applying it again to different data. Any differences in the results between
corpora are a function of the variance between samples and not of the reliability of the
coding scheme.
Di Eugenio and Glass (2004) identify three general classes of agreement statistics
and suggest that all three should be used in conjunction in order to accurately evaluate
coding schemes. However, this suggestion is founded on some misunderstandings of
290
Craggs and Wood Evaluating Discourse and Dialogue Coding Schemes
the role of agreement measure in reliability studies. We shall now rectify these and
conclude that only one class of agreement measure is suitable.
2.1 Percentage Agreement
The first of the recommended agreement tests, percentage agreement, measures the
proportion of agreements between coders. This is an unsuitable measure for inferring
reliability, and it was the use of this measure that prompted Carletta (1996) to recom-
mend chance-corrected measures.
Percentage agreement is inappropriate for inferring reliability because it excludes
any notion of the level of agreement that we could expect to achieve by chance. Reliabil-
ity should be inferred by locating the achieved level of agreement on a scale between the
best possible (coders agree perfectly) and the worst possible (coders do not understand
or cannot perform the mapping and behave randomly). Without any indication of the
agreement that coders would achieve by behaving randomly, any deviation from perfect
agreement is uninterpretable (Krippendorff 2004b).
The justification given for using percentage agreement is that it does not suffer from
what Di Eugenio and Glass (2004) referred to as the ?prevalence problem.? Prevalence
refers to the unequal distribution of label use by coders. For example, Table 1 shows
an example taken from Di Eugenio and Glass (2004) showing the classification of the
utterance Okay as an acceptance or acknowledgment. It represents a confusion matrix
describing the number of occasions that coders used pairs of labels for a given turn. This
table shows that the two coders favored the use of accept strongly over acknowledge. They
correctly state that this skew in the distribution of categories increases the expected
chance agreement, thus lowering the overall agreement in chance-corrected tests. The
reason for this is that since one category is more popular than others, the likelihood of
coders? agreeing by chance by choosing this category increases. We therefore require a
comparable increase in observed agreement to accommodate this.
Di Eugenio and Glass (2004) perceive this as an ?unpleasant behavior? of chance-
corrected tests, one that prevents us from concluding that the example given in Table 1
shows satisfactory levels of agreement. Instead they use percentage agreement to
arrive at this conclusion. By examining the data, it is clear that this conclusion would
be false.
In Table 1, the coders agree 90 out of 100 times, but all agreements occur when both
coders choose accept. There is not a single case in which they agree on Okay?s being used
as an acknowledgment. The only conclusion one may justifiably draw is that the coders
cannot distinguish the use of Okay as an acceptance from its use as an acknowledgment.
Rather than being an unpleasant behavior, accounting for prevalence in the data is an
Table 1
Prevalence in coding.
Coder 2
Coder 1 Accept Ack
Accept 90 5 95
Acknowledge 5 0 5
95 5 100
291
Computational Linguistics Volume 31, Number 3
important part of accurately reporting the level of agreement. This helps us to avoid
arriving at incorrect conclusions such as believing that the data shown in Table 1 suggest
reliable coding.
2.2 Chance-Corrected Agreement: Unequal Coder Category Distribution
The second class of agreement measure recommended in Di Eugenio and Glass (2004)
is that of chance-corrected tests that do not assume an equal distribution of categories
between coders. Chance-corrected tests compute agreement according to the ratio of
observed (dis)agreement to that which we could expect by chance, estimated from the
data. The measures differ in the way in which this expected (dis)agreement is estimated.
Those that do not assume an equal distribution between coders calculate expected
(dis)agreement based on the individual distribution of each coder.
The concern that in discourse and dialogue coding, coders will differ in the fre-
quency with which they apply labels leads Di Eugenio and Glass to conclude that
Cohen?s (1960) kappa is the best chance-corrected test to apply. To clarify, by unequal
distribution of categories, we do not refer to the disparity in the frequency with which
categories occur (e.g., verbs are more common than pronouns) but rather to the differ-
ence in proclivity between coders (e.g., coder A is more likely to label something a noun
than coder B).
Cohen?s kappa calculates expected chance agreement, based on the individual
coders? distributions, in a manner similar to association measures, such as chi?square.
This means that its results are dependent on the preferences of the individual coders
taking part in the tests. This violates the condition set out at the beginning of this section
whereby agreement must be a function of the coding process, with coders being viewed
as interchangeable. The purpose of assessing the reliability of coding schemes is not to
judge the performance of the small number of individuals participating in the trial, but
rather to predict the performance of the schemes in general. The proposal that in most
discourse and dialogue studies, the assumption of equal distribution between coders
does not hold is, in fact, an argument against the use of Cohen?s kappa. Assessing the
agreement between coders and accounting for their idiosyncratic proclivity toward or
against certain labels tells us little about how the coding scheme will perform when ap-
plied by others. The solution is not to apply a test that panders to individual differences,
but rather to increase the number of coders so that the influence of any individual on
the final result becomes less pronounced.1
Another reason provided for using Cohen?s kappa is that its sensitivity to bias (dif-
ferences in coders? category distribution) can be exploited to improve coding schemes.
However, there is no need to calculate kappa in order to observe bias, since it will
be evident in a contingency table of the data in question. Even if it were necessary to
compute kappa for this purpose, however, this would not justify its use as a reliability
test.
2.3 Chance-Corrected Agreement: Assumed Equal Coder Category Distribution
The remaining class of agreement measure assumes an equal distribution of categories
for all coders. Once we have accepted that this assumption is necessary in order to
1 When there is a single correct label that should be used, such as part-of-speech tags used to describe the
syntactic function of a word or group of words, then training coders may mitigate coder preference.
292
Craggs and Wood Evaluating Discourse and Dialogue Coding Schemes
predict the performance of the scheme in general, there appears to be no objection to
using this type of statistical test for assessing agreement in discourse and dialogue work.
Tests that fall into this class include Siegel and Castellan?s (1988) extension of Scott?s
(1955) pi, confusingly called kappa, and Krippendorff?s (2004a) alpha. Both of these
measures calculate expected (dis)agreement based on the frequency with which each
category is used, estimated from the overall usage by the coders.
Kappa is more frequently described in statistics textbooks and more commonly
implemented in statistical software. In circumstances in which mechanisms other than
nominal labels are used to annotate data, alpha has the benefit of being able to deal with
different degrees of disagreement between pairs of interval, ordinal, and ratio values,
among others.
Di Eugenio and Glass (2004) conclude with the proposal that these three forms of
agreement measure collectively provide better means with which to judge agreement
than any individual test. We would argue, to the contrary, that applying three different
metrics to measure the same property suggests a lack of confidence in any of them.
Percentage agreement and Cohen?s kappa do not provide an insight into a scheme?s
reliability, so reporting their results is potentially misleading.
3. Inferring Reliability
To reiterate, when testing reliability we are assessing whether the data that a scheme
generates can be relied on. This may be inferred from the level of agreement between
coders applying the scheme. In section 1 we described two properties of reliable data
that are important to establish in discourse and dialogue analysis. In this section we
explain how the gap between agreement and reliability may be bridged.
When inferring reliability from agreement, a common error is to believe that there
are a number of thresholds against which agreement scores can be measured in order to
gauge whether or not a coding scheme produces reliable data. Most commonly this
is Krippendorff?s decision criterion, in which scores greater than 0.8 are considered
satisfactory and scores greater than 0.667 allow tentative conclusions to be drawn
(Krippendorff 2004a). The prevalent use of this criterion despite repeated advice that
it is unlikely to be suitable for all studies (Carletta 1996; Di Eugenio and Glass 2004;
Krippendorff 2004a) is probably due to a desire for a simple system that can be easily
applied to a scheme. Unfortunately, because of the diversity of both the phenomena
being coded and the applications of the results, it is impossible to prescribe a scale
against which all coding schemes can be judged.
Instead we provide discussion and some recommendations, all founded on the
premise that reliability must ?correlate with the conditions under which one is willing to
rely on imperfect data? (Krippendorff 2004b, page 6). A common concern regarding the
application of standards from other fields, such as the one described above, to discourse
and dialogue research is that the subjectivity of the phenomena being coded may
mean that we never obtain the necessary agreement levels. In this context, subjectivity
describes the absence of an obvious mapping for each unit of analysis onto categories
that describe the phenomenon in question. However, the fact that we consider these
subjective phenomena worthy of study shows that we are, in fact, ?willing to rely
on imperfect data,? which is fine as long as we recognize the limitations of a scheme
that delivers less-than-ideal levels of reliability and use the resulting annotated corpora
accordingly.
In order to discuss the acceptable levels of agreement for discourse and dialogue
coding, let us consider two popular uses of coded data: to train systems to perform
293
Computational Linguistics Volume 31, Number 3
some automated task and to study the relationship between the coded phenomena and
some other feature of the data.
3.1 Reliability and Training for Automatic Annotation
Considering the effort involved in manually annotating linguistic data, it is un-
surprising that attempts are often made to train a system to perform such annotation
automatically (Mast et al 1996; Wrede and Shriberg 2003). The reliability of manually
annotated data is clearly a concern when they are used to train a system. If the level
of agreement for the annotation scheme is low, then the system is going to replicate
the inconsistent behavior of human annotators. Any deviant behavior by the system
resulting in less than 100% accuracy in comparison with the manual annotation will
compound the problem, possibly leading to meaningless data. Worse still, if a system
is to learn how to annotate from manually annotated data, it will do so based on the
patterns observed in those data. If the manual annotation is not reliable, then those
patterns may be nonexistent or misleading.
Returning to our original premise, we would suggest that if a coding scheme is to
be used to generate data from which a system will learn to perform similar coding, then
we should be ?unwilling to rely on imperfect data.?
3.2 Reliability and Corpus Analysis
Manually annotated corpora can also be used to infer a relationship between the phe-
nomena in question and some other facet of the data. When performing this sort of
analysis, we may be more willing to work with imperfect data and therefore accept
lower levels of agreement. However, the conclusions that are gleaned from the analysis
must be tempered according to the level of agreement achieved. For example, when it is
suggested that a correlation exists between the occurrence of one phenomenon and that
of another, less agreement observed in the sample annotation requires stronger evidence
of the correlation in order for the conclusion to be valid.
To summarize, there are no magic thresholds that, once crossed, entitle us to claim
that a coding scheme is reliable. One must decide for oneself, based on the intended use
of a scheme, whether the observed level of agreement is sufficient and conduct one?s
analysis accordingly.
4. Conclusion
The application of agreement statistics has done much to improve the scientific rigor
of discourse and dialogue research. However, unless we understand what we are
attempting to prove and which tests are appropriate, the results of evaluation can be
unsatisfactory or, worse still, misleading. In this article we have encouraged researchers
to clarify their reasons for assessing agreement and have suggested that in many cases
the most suitable test for this purpose is one that corrects for expected agreement, based
on an assumed equal distribution between coders.
Acknowledgments
The authors acknowledge the help of Klaus
Krippendorff for patiently aiding our
understanding of reliability and University
of Manchester for funding Richard Craggs.
References
Carletta, Jean. 1996. Assessing agreement
on classification tasks: The kappa
statistic. Computational Linguistics,
22(2):249?254.
294
Craggs and Wood Evaluating Discourse and Dialogue Coding Schemes
Cohen, J. 1960. A coefficient of agreement for
nominal scales. Education and Psychological
Measurement, 43(6):37?46.
Di Eugenio, Barbara and Michael Glass.
2004. The kappa statistic: A second look.
Computational Linguistics, 30(1):95?101.
Jurafsky, Daniel, Elizabeth Shriberg, and
Debra Biasca. 1997. Switchboard
SWBD-DAMSL shallow-discourse-function
annotation coders manual. Technical Report
(Draft 13), University of Colorado.
Krippendorff, Klaus. 2004a. Content Analysis:
An Introduction to Its Methodology. 2nd ed.
Sage, Beverly Hills, CA.
Krippendorff, Klaus. 2004b. Reliability in
content analysis: Some common
misconceptions and recommendations.
Human Communication Research,
30(3):411?437.
Mast, Marion, Heinrich Niemann, Elmar
Noth, and Ernst Gunter
Schukat-Talamazzini. 1996. Automatic
classification of dialog acts with semantic
classification trees and polygrams. In
Learning for Natural Language Processing,
edited by Stefan Wermter, Ellen Riloff,
and Gabriele Scheler. Springer,
pages 217?229.
Scott, W. A. 1955. Reliability of content
analysis: The case of nominal scale coding.
Public Opinion Quarterly, 19:127?141.
Siegel, Sidney and John N. Castellan, Jr. 1988.
Nonparametric Statistics. 2nd ed.
McGraw-Hill.
Wrede, Britta and Elizabeth Shriberg. 2003.
Spotting ?hot spots? in meetings: Human
judgments and prosodic cues. In
Proceedings of EUROSPEECH, Geneva.
295

Rare Dialogue Acts Common in Oncology Consultations
Mary McGee Wood, Richard Craggs
Department of Computer Science
University of Manchester
Manchester M13 9PL U.K.
mary, richard.craggs@cs.man.ac.uk
Ian Fletcher, Peter Maguire
Psychological Medicine Group
University of Manchester
Stanley House
Manchester M20 4BX U.K.
ian.fletcher, peter.maguire@cs.man.ac.uk
Abstract
Dialogue Acts (DAs) which explicitly en-
sure mutual understanding are frequent
in dialogues between cancer patients and
health professionals. We present exam-
ples, and argue that this arises from the
health- critical nature of these dialogues.
1 Background
We have described elsewhere (Wood, 2001; Wood
and Craggs, 2002) the use of dialogue analysis in
communication skills training for health profession-
als working with cancer patients. Our initial cor-
pus arises from a study of Macmillan Cancer Care
nurses undertaken by the Psychological Medicine
Group, University of Manchester, funded by the
Cancer Research Campaign. It consists of 37 dia-
logues between nurses and patients, each comprising
200-1200 utterances (mostly 300-600). The nurses?
goal is to learn as much as possible about the pa-
tients? condition, both physical and mental, and to
inform the patients about their condition and treat-
ment. The dialogues are thus genuine, naturally oc-
curring conversations, but occurring in an unusual,
highly significant and emotionally charged situation.
We have not yet fully annotated a statistically
significant sample, but it is clear even from read-
ing through the corpus that a group of themati-
cally related DAs occur frequently which are rare
in previously studied corpora such as Switchboard.
These are DAs which explicitly establish or con-
firm accurate mutual understanding, either factual
or emotional, between the participants (collaborative
completions, summaries), or which build rapport
through courtesy and appreciation (thanks, apolo-
gies). Protracted closing sequences are charac-
teristic, and tend to have elements of both. We
interpret these patterns as direct responses to the
goal-directed and potentially health-critical nature
of these dialogues.
2 Rare dialogue acts
We take as our point of comparison the corpus of
some 200,000+ utterances from the Switchboard
corpus tagged with the SWBD-DAMSL tagset (Ju-
rafsky et al 1998). Of these, 36% are ?Statements?,
19% ?Continuers?, and 13% ?Opinions?, giving a
total of 68% of all utterances in the three most com-
mon categories. At the other end of the scale, an
original tagset of 220 was reduced to 42 because the
rarity of many made statistical analysis impossible.
Even of these 42, 32 occur with less than 1% fre-
quency, 25 less than 0.5%. Four of the five DAs we
will discuss here are among these last 25.
2.1 Mutual understanding
The first goal of the nurse in these dialogues is to as-
certain the subjectively perceived physical condition
of the patient, which reflects the success of previ-
ous treatment, and suggests directions for the future.
Factual accuracy is clearly essential if future treat-
ment is to be appropriate. It is also seen as important
that the patient have accurate knowledge of his / her
condition and treatment.
Secondly, the nurse is trying to elicit the mental
/ emotional state of the patient, and any particular
concerns or worries he or she has. Although this is
a somewhat different type of mutual understanding,
the same DAs - collaborative completions and
     Philadelphia, July 2002, pp. 196-200.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
summaries - can effect both.
Collaborative completions
Collaborative completions are rare in most
dialogue types, where they would probably be seen
as pre-emptive interruptions. (The SWBD/ DAMSL
corpus includes 699/ 205,000, about 0.3%.) Here,
however, they are supportive, and relatively com-
mon. We have identified recurring patterns of both
factual and emotional use.
a. Factual completions: commonly, the patient is
not sure of the name of (e.g.) a drug or procedure,
and hesitates, whereon the nurse provides it:
P20 This is the, this is the ones I?m taking there.
N21 Right.
P21 Deta,
N22 Dexamethasone. (017-166)
P5 MST and,
N6 Antibiotics.
P6 Antibiotics and then by Sunday morning I had
come round a bit because all I did was sleep.
(038-395)
Sometimes the need for information is made ex-
plicit (here - as in the rest of the article - names have
been changed to maintain confidentiality):
P49 I did ask the Registrar, I forget his name Mr,
Mr Ferguson?s Registrar, I don?t, I forget his name.
N50 Birch, Mr Birch?
P50 That?s right, that?s the one. (047-291)
The same effect is also often achieved through an
overt question, more often on the part of the nurse:
N133 And the tablets that you take, the capsules
that you?ve just taken now?
P134 They?re Tylex, they?re a painkiller but I don?t,
I mean I don?t take them all the time they?re just
purely as a little top up. (035-215)
b. Emotional completions: the patient may be
struggling to find the right words, or reluctant to talk
about something. The nurse is showing understand-
ing and empathy with the patient, and encouraging
him/her to continue the conversation:
P80 : But that?s the sort of feeling I get a,
N81 : A tiredness. (042-277)
P328b But it?s not until it happens to you that,
N329 That sudden impact of gosh.
P329 That?s it. (024-113)
P162 I could feel the panic coming in me and before
the operation definitely but I?m,
N163 You?re okay. (057-356)
Summaries
The SWBD/ DAMSL ?summarize/reformulate?
DA would seem laboured and unnatural in most
types of conversation, and indeed their corpus
includes only 919/ 205,000, about 0.5%. In our
domain, however, it is an entirely natural and useful
way of checking correct mutual understanding,
factual or emotional.
a. Repetition: simple repetition signals to the first
speaker that the second has heard and understood:
N297 I would have no problem in recommending
that you go from the 50 to the 75.
P298 Yes to the 75. (035-215)
The first speaker, on the other hand, may repeat in-
formation if the second appears not to have under-
stood (note the use of a collaborative completion in
this example):
P56 : 0 to 10, it went down then to about three.
...
N59 : So it went down to,
P59 : It went down to three. (042-277)
This sequence shows both patterns:
R128 Well she?s on 50mg of,
P129 Durogesic.
N129 Fifty?
R129 Durogesic, fifty yes.
N130 Fifty right. (035-215)
Where there is no apparent problem over informa-
tion, repetition suggests encouragement to continue
the conversation on that topic:
157 And you get very dizzy don?t you.
N158 That?s been an additional problem hasn?t it
that dizziness.
R158 The dizziness. (016-128)
b. Summary / paraphrase: a simple summary asks
for confirmation that the speaker has understood the
preceding dialogue correctly. Repetition may be
used to express the confirmation:
N25 : Mmmm. So yesterday you were sick twice.
P25 : Twice. For the first and only time. (042-277)
Summaries can also be used to bring the conversa-
tion back on track after a digression. The nurse may
initially wish to pursue a digression, in case it leads
to the revealing of a concern, but also needs to keep
the conversation focussed and ensure its goals are
met within an acceptable length of time:
P132 : I mean, and the parent, their separated
parents, I think it?s dreadful.
N133 : But I can also hear that, that from what
you?re saying you?re cross with, with being ne-
glected maybe for five hours or not having the
back-up ... (042-277)
N140 : Right. But coming back to what you were
saying earlier about, I?ve lost my frame of thought,
you mentioned earlier about wanting to make
sure that you get the right information, that it is
consistent. (042-277)
All these examples have been factual, but summaries
are also used to show empathy and understanding of
mental or emotional states:
P425 There?s nothing really honestly, if I took to my
bed or depended on someone.
N426 Right.
P426 That would be the end, oh it would.
N427 Your control, your independence.
P427 Yes, that?s more important to me at the minute.
N428 You want to stay really in the security of your
own home.
P428 Yes. (024-113)
2.2 ?Social glue?
The dialogues in our corpus are not only more
important than most, they also occur in a complex
wider context. The nurse is part of an organisation
which is trying to save the patient?s life, using treat-
ments with painful, embarrassing, and depressing
side-effects. Everyone involved has more than usual
to be thankful or sorry for. Emotions run high. At
the same time, the whole enterprise depends on trust
and cooperation. Explicit courtesy, consideration,
and appreciation are essential ?social glue?. No
wonder that thanks and apologies, both barely
represented in SWBD/DAMSL, are common here.
Thanks
Thanks occur only 67 times in SWBD/DAMSL,
probably not enough to be statistically recognised as
a separate tag, but clearly signalled lexically:
N115 Alright then.
P115 Right. Thank you very much. (027-334)
In our corpus, the nurse thanks the patient with
surprising frequency (even ignoring the artificial
cases of thanking the patient for allowing the con-
versation to be recorded). These seem to be part of
a general pattern of positive attitude and encourage-
ment:
N6 So thanks for doing this Karen, I just wanted
to come down and see you this morning because I
know we changed your medication a couple of days
ago. (027-334)
P80 So I?ve been doing, I?ve done everything you?ve
said and it?s working so far.
N81 Oh you?re wonderful, you?re wonderful, thank
you very much that?s really kind. (024-113)
Sometimes it is hard to draw a clear line between
thanks and appreciation:
N199 Is there anything you?d like to ask me?
P199 Mmmm no I think you?ve been very kind and
helpful. (030-318)
P204 But apart from that I?ve had some excellent
help and advice this year and from this week and
from the nurses, all nursing staff.
N205 Right.
P205 They?ve been excellent.
N206 Good, good.
P206 And it?s very helpful to your recovery.
N207 That?s good...
P207 Very helpful. (042-277)
Apologies
Apologies are similar to thanks (76 in
SWBD/DAMSL). Typically the nurse is reas-
suring the patient of her interest and attention,
perhaps after an interruption or some seeming
oversight:
N1 So go on I?m sorry to have interrupted you but
it was just that she had to get out. So it?s numb and
you can?t, (031- 198)
N105 Barbara I?m sorry I didn?t check out how
you are in here, how are you in this room, is that
alright? (057- 356)
N208 Bearing in mind there are a lot of questions I
get asked that I can?t give the answers to.
P209 Yes.
N209 I?m afraid, but I?ll come and see you next
week. (063-489)
Apologies can also be indirect or implicit:
(after talking about problems)
P314 ...poor girl you?ve got to listen to all that ...
(06-017)
2.3 Closures
?Conventional-closing? is the tenth most frequent
tag in SWBD/DAMSL (although still only 1%
(2486/ 205,000)). Our corpus is distinguished not
so much by the frequency as the nature of closures.
The evidence is incomplete, as in many dialogues
the tape runs out or is switched off before the end,
but typically the closure of a dialogue is long, and
explicitly negotiated. The nurse makes it clear to
the patient that the conversation is ending, in a way
which does not leave the patient feeling cut off or
abandoned (the imminent arrival of lunch is often
given as a reason):
N131 Alright then. I?ll cut it short so you can have
your lunch. (018-168)
Expressions of thanks and/or appreciation are com-
mon:
N114 Right, I wouldn?t envy you that job, you
have loads of problems with Councils. Shall we let
Charlotte back in?
P114 Yes, yes, yes.
N115 And then we?ll be able to have, we?ll see what
she has to do, you can fill the forms in because it
will be getting towards lunch time anyway, what
time is your lunch here?
P115 Oh usually about now.
N116 Right I?ll swap over and let Charlotte in ...
thanks very much that was brilliant. (025-153)
We often find a series of summarising statements
within a closing sequence:
N160 No. Right well I?ll see if there?s anything I
can do about that then and I?ll pop back next week
but give me a ring in the meantime if there?s any
problems. I?ll speak to Dr Clarke and see what he
thinks about this pain in your back.
P161 Yes.
N161 And see if there?s anything else that we can,
bearing in mind you?ve only been on the MST
properly for a short time.
P162 Mmmm.
N162 It might, we?ll just have to see how that works.
P163 Its not too bad today is my back.
N163 Right. Anyway I?ll pop in next week.
P164 Yes.
N164 And, you know, just monitor how things are
going and how you?re managing. Alright?
P165 Yes. (020-139)
N107 Have a great weekend.
P107 Yes.
N108 And we?ll catch up with you.
P108 Yes.
N109 Either in person or on the phone.
P109 Right.
N110 Next week when we know what else is hap-
pening.
P110 Okay.
N111 Other than that we?ll actually now step back.
P111 Right, yes, yes.
N112 Unless you need us for anything specific.
P112 Okay well I know where you are.
N113 Because you?re back in the hands of,
P113 Just the medical team.
N114 That?s right yes for your treatment.
P114 Yes, yes.
N115 Alright then.
P115 Right. Thank you very much.
N116 Okay see you soon.
P116 Yes, yes.
N117 Shall I walk you back down?
P117 Right thank you, I?m not quite straight yet but
I?m getting there. (027-334)
Both participants seem to be taking their last oppor-
tunity to check that mutual understanding is com-
plete. Sometimes contact details are given or new
topics arise at this point:
N558 Oh, I?ll finish off now anyway. (023-111)
The tape runs out at P604, after some repetition of
earlier topics and some new ones.
3 Analysis
Cancer care dialogues are health-critical. Misunder-
standings in casual conversation are unlikely to have
dire consequences: here, they easily could. Both
participants need to be unusually clear, and to ensure
that the clarity is mutual. This results in an unusual
predominance of DAs which establish and monitor
mutual understanding, both factual and emotional.
These dialogues are also emotionally charged, and
are eased by explicit appreciation and courtesy.
(Formal design of patterns to ensure clarity in dia-
logues can be found in safety-critical situations such
as military command and aviation. The use of repe-
tition to check and confirm understanding is charac-
teristic of Air Traffic Control:
Tower: BA117 descend 3,000 feet QNH 1017.
Pilot: Descend 3,000 feet QNH 1017, BA117.
Our dialogues are at the other end of the scale for
openness and unpredictability: it is interesting to see
similar surface devices used for the same purpose in
such different environments.)
These findings are somewhat impressionistic, and
taken from a relatively small corpus. As soon as
we have analysed a larger sample in more detail,
it will be possible to verify and quantify these pat-
terns, and to analyse the linguistic characteristics of
DAs which have previously eluded us. Also, further
comparisons can then be made with other corpora
and previous work on dialogue analyses.
Acknowledgements
Appreciation for their support of this research goes
to Manchester University Computer Science Depart-
ment for funding Richard Craggs and Cancer Re-
search UK for funding Ian Fletcher.
References
Daniel Jurafsky, Elizabeth Shriberg, Barbara Fox
and Traci Curl. 1998. Lexical, Prosodic, and
Syntactic Cues for Dialog Acts. ACL/COLING.
Mary McGee Wood 2001. Dialogue Tagsets in
Oncology. Proceedings of SIGdial2.
Mary McGee Wood and Richard Craggs. 2002.
Language Engineering meets Oncology Training.
Paper submitted to the Edilog workshop on Seman-
tics and Pragmatics of Dialogue, Sept 2002.
Annotating emotion in dialogue
Richard Craggs
Department of Computer Science
Manchester University
craggs@cs.man.ac.uk
Mary McGee Wood
Department of Computer Science
Manchester University
mary@cs.man.ac.uk
Abstract
Communication behaviour is affected by
emotion. Here we discuss how dialogue is
affected by participants? emotion and how
expressions of emotion are manifested in
its content.
Keywords: Dialogue, Emotions, Annotation
1 Introduction
Dialogue annotation is a fundamental stage of much
of the research conducted on both Human-Human
and Human-Machine dialogue.
We are fortunate to have access to a valuable
corpus of 37 dialogues between nurses and pa-
tients, each comprising 200-1200 utterances (Wood,
2001). These consultations contain genuine emo-
tional speech and form the ideal basis for studies of
realistic conversational dialogue.
The emotional state of participants affects the
way in which the dialogue is conducted. I propose
that annotating emotion in dialogue alongside cur-
rently annotated phenomena will reveal interesting
and useful correlations that will improve our under-
standing of dialogue and benefit natural language
applications. The overall aim of this research is
to develop a scheme for annotating expressions of
emotion, to create an annotated corpus of dialogue
containing emotion and to study the effects that a
participant?s emotional state has on their commu-
nicative behaviour.
2 Effects of emotion in dialogue
This research is motivated by observations made on
the consultation dialogues described above. These
are naturally occurring conversational dialogues
conducted under unusual circumstances, in which
the consultant?s goal is to elicit concerns from
the patient. They therefore contain an unusually
high level of emotional speech. When read with
a dialogue analyst?s eye it is apparent that certain
phenomena, interesting to the dialogue analysis
community, are affected by the changing level of
emotion. For example, grounding behaviour is more
protracted when a participant is discussing a subject
about which they feel emotional. This is manifested
in an increase in the number of clarification requests
and repetitions. E.g. -
N. How do you feel when you look at your
scar?
P. Erm, it doesn?t bother me that much
N Okay
P. But I still, When I?m washing and ev-
erything I still get a funny feeling
N. You get a funny feeling, in which way?
P. It just feels strange, hollow,
N. Physically?
P. Physically yes,yes
N. Yes
P. It feels really weird
Turn taking behaviour changes under these cir-
cumstances too. An emotional speaker will hold the
floor for an increased length of time when discussing
a topic about which they feel, for example, anxiety
or joy.
Although these are casual observations of a small
amount of dialogue, other studies have benefited
from investigation into a speaker?s behaviour when
emotional. For the Verbmobil project (Bub and
Schwinn, 1996) it was recognised that anger in
speakers changed the way in which they commu-
nicated (Fischer, 1999). Also applications such as
automated call centres would benefit from recogni-
tion of human emotion so that humans could inter-
vene when a customer becomes angry and frustrated
(Petrushin, 1999). However, these insights are lim-
ited to the vocal expression of the speaker. An anno-
tated corpus of emotional dialogue would allow us
to study all aspects of a speaker?s behaviour.
3 Annotating emotion in dialogue
I envisage that a scheme to annotate dialogue would
constitute one or more layers augmenting an exist-
ing annotation scheme. There are plenty of other
schemes developed for previous dialogue research,
many of which are designed to investigate a particu-
lar phenomenon of communicative behaviour.
In this section we will look at some existing an-
notation schemes. We shall investigate if any of the
layers may accommodate emotion and which may
present interesting correlations with emotional tag-
ging.
Of course when looking for possible indicators of
emotional speech it is important to remember that
people exhibit different behaviours from each other
when they speak. For example some people are
more expressive than others and so a large number of
expletives from one person may be natural, and not
indicative of their emotional state. Prosodic studies
of emotion also suffer from this complexity and it
would be interesting to see if language use, and dia-
logue behaviour are more robust indicators of emo-
tion than prosody.
3.1 Task and conversational dialogue
Most dialogue research concentrates either on task
based dialogue, where the participants converse in
order to achieve some set goal (e.g. Maptask (An-
derson et al, 1991) Coconut (Di Eugenio et al,
1998)), or on conversational dialogue, which is of-
ten less structured and contains a richer use of lan-
guage (e.g. DAMSL (Core and Allen, 1997) and
Chat (MacWhinney, 1998)). It seems likely that we
would see more expressions of emotion in conver-
sational dialogue where people are discussing topics
of personal interest rather than the more mechanical
process of achieving a goal through communication.
These differences are reflected in the types of
phenomena that the schemes are designed to iden-
tify. Task based research may be more interested
in the structure of the dialogue and the way that it
represents the division of the task into sub-goals.
Schemes to annotate conversational dialogue are
more likely to require a greater breadth of dialogue
acts to describe the wider range of illocutionary acts
that may be performed in this type of speech.
3.2 Current dialogue annotation schemes
In order to learn how current annotation schemes
accommodate emotion, we aligned the layers in a
number of schemes. (Core and Allen, 1997; Di Eu-
genio et al, 1998; Traum, 1996; Walker et al, 1996;
MacWhinney, 1996; Jekat et al, 1995; Anderson et
al., 1991; Condon and Cech, 1996; van Vark et al,
1996; Walker and Passonneau, 2001). Layers from
different schemes are grouped according to the sim-
ilar phenomena that they label. Table 1 shows this
alignment.
In this section we will look at these layers and
discuss how they may relate to annotating emotion
in dialogue.
Information level When analysing task dialogue,
we may be interested in knowing whether an utter-
ance pertains to the management of the communi-
cations channel, advancement of a task, discussing
of a task etc. In the previous section we suggested
that we are more likely to find emotional speech in
conversational rather than task dialogue because the
latter is more of a mechanical process than conver-
sation. Perhaps we may consider this layer as an ex-
tension of that distinction where sub-dialogues are
labelled according to how closely related to the task
they are.
This may reveal a correlation where the more re-
lated to the task a sub-dialogue is, the less emotional
speech becomes. There is evidence in our corpus
that when one participant is attempting to achieve
a goal, often the elicitation of information, then the
participants? behaviour becomes more business like
and the language becomes more formal and less
expressive ?
N. Was that Dr Smith who you saw there?
P. Yes but it wasn?t Dr Smith it was an-
other doctor.
N. Right.
P. But I was under Dr Smith.
N. Right. So when did you actually have
those radiotherapy treatments?
P. I had the radiotherapy October 13th.
N. Right, thank you.
Communications status
Communications status indicates whether an ut-
terance was successfully completed. It is used to
tag utterances that are abandoned or unintelligible
rather than whether the intention of a speech act was
achieved.
Although failure to perform a successful utter-
ance may be partly due to the emotional state of
the speaker, annotating such utterances for their
emotional content may be difficult, especially from
the textual content alone. This and the multiplicity
of reasons for unsuccessful communication means
that using communications status as an indication
of emotion in the speaker will produce unreliable
results.
In Human-Machine dialogue failure on behalf of
the machine to communicate can lead to frustration
and anger in the user. In these cases communication
status may signal behaviour that can result in
emotion in the listener which is also applicable to
Human-Human dialogue.
Speech acts
All of the schemes that we examined annotated
the utterances for their illocutionary force. Since
this is the layer that contains most information re-
garding the semantic content of an utterance, this is
likely to be where we shall find the most interesting
correlations. We have already seen that high levels
of emotion in dialogue alters the frequencies of
dialogue acts compared with the more impassive
conversations conducted in the Switchboard corpus
(Wood and Craggs, 2002).
Forward communicative functions describe utter-
ances that intend to evoke some response from the
listener (such as believing a statement or answering
a question), perform an action (such as committing
to something) or similar dialogue advancing func-
tions. These types of utterance are likely to be mo-
tivated by some intention or belief on behalf of the
speaker, providing clues as to their cognitive state.
Forward communicative functions can play an
important role in eliciting emotional responses from
the listener. Open ended questions are more likely to
produce an emotional response than a yes/no ques-
tion (Maguire et al, 1996b). This is partly because
open questions hand the initiative to the listener al-
lowing them to express themselves. The relation-
ship between questions, initiative and emotion is dis-
cussed further in (Wood and Craggs, 2003).
The following extract from our corpus show how
an open question elicits an emotional response from
the listener.
N. How were you coping with that yourself?
P. Oh mentally I?ve never been down men-
tally
Backward communicative functions are used to la-
bel utterances that respond to something that has
been said to them. Some responses are required
by the previous utterance, for example an answer
following a question. In these cases the utterance
wasn?t motivated by a desire on behalf of the speaker
but rather an obligation to adhere to the rules of
engagement for communication. This of course
doesn?t mean that a response can not be emotional.
When faced with a proposal, question or offer the
listener is free to react as they wish and this includes
emotional responses.
Here a backward communicative act responds to
appreciation with sympathy.
N. Good okay. Well thank you as I say for
filling me in and...
P. poor girl you?ve got to listen to all
that
However, from observations of our emotional
dialogues it appears that short Question-Answer,
Offer-Acceptance exchanges tend to be formal.
Emotion tends to build though a sub-dialogue on a
topic that speakers find funny, feel anxious about etc.
Dialogue grammars are used to exploit the ex-
pected sequences of speech acts. These can be used
in dialogue act classification to predict the next act
in a series of utterances (Stolcke et al, 2000). It may
be possible that a complementary approach may be
used to automatically identify emotional utterances.
One way would be to develop grammars based
on patterns discovered in emotional sections of
dialogue where a particular sequence of acts may
indicate the proceedings have become emotional.
Another may be to apply established grammars to
dialogue so that deviations from the grammar may
highlight interesting or emotional passages.
Topic
Several annotation schemes contain a layer that
labels the topic discussed in an utterance. This is
usually in task domains where there is a finite num-
ber of subjects that will be discussed. For exam-
ple in the Alparon scheme for transport dialogues
(van Vark et al, 1996), the topic layer (called ?cod-
ing of information?), labels utterances according to
whether they relate to topics such as timetable, price,
time and locations.
For our corpus of cancer consultations it is ap-
parent that certain topics are more likely to invoke
emotion in people. However topic annotation is only
usually performed in the restricted domains of task
dialogues, where the range of topics that may be dis-
cussed is limited. However it is in these types of
dialogue that we expect the levels of emotion to be
low, and topics are chosen because of their necessity
for the task. Because of this we may not get to see
the correlation between topic and emotion that we
expect.
Topics may play a further role in identification
of emotion in dialogue since in our corpus, patients
tend to remain on the same topic for longer when
they emotional about it. Length of a topic, or return-
ing to a previously discussed topic are indications
of emotion.
Phases
Some schemes distinguish between dialogue
phases such as opening, negotiation and query.
Emotion in dialogue also goes through phases and
it is possible that there are boundaries between the
phases of emotion that correspond to those tagged
using the phase layer.
An interesting area of research would be to iden-
tify how boundaries between the phases of different
levels and types of emotion are manifested in the
use of language. For instance psycho-oncology
research states that open ended questions are more
likely to elicit emotional responses than yes-no
questions (Maguire et al, 1996a). This may cause a
correlation between forward-looking functions and
the onset of phases.
Surface form
Surface form tagging is used in David Traum?s
adaptation of the TRAINS annotation scheme
(Traum, 1996) and the Coconut scheme to tag utter-
ances for certain special features such as cue words
or negation.
It has been shown that certain syntactic features
of an utterance may be indicators of emotion. For
example in German use of modal particles such
as ?eben? and ?denn? colour the utterance with a
particular emotional attitude.
Although the surface form of utterances is depen-
dent on the style of the speaker, it does sometimes
contain indications of emotion.
P. Oh no, no no no no, I?m not in any dis-
comfort
Relatedness and Information relations
The relatedness layer is used to show how utter-
ances relate to one another, usually by tagging an ut-
terance with the distance to the antecedent to which
it refers.
Information relation describes the relationship be-
tween utterances, for instance that one utterance
presents information in support of its antecedent.
These layers are more concerned with the struc-
ture of the dialogue than the semantic content and
are therefore less likely to correlate well with emo-
tional tags.
It would be interesting to see if the emotional
level of the dialogue or its participants has an affect
on the dialogue?s structure. In our corpus it appears
that discussion of emotional topics is often more
protracted, with speakers answering questions with
successive statements, each adding more detail to
their answer. This type of behaviour may show up
in the relatedness and information relations layers.
Grounding
Grounding describes the process by which com-
mon ground between the participants is established.
As with relatedness and information relations,
emotion in the dialogue may be manifested in this
layer by protracted grounding behaviour as people
reiterate points about which they feel emotional. In
our highly emotional corpus this resulted in four
times as many summaries and five times as many
repetitions than in the Switchboard corpus.
Besides the layers listed here there are other lay-
ers included in schemes that do not fit into any
of these categories. For instance Verbmobil (Jekat
et al, 1995) includes a layer for annotating the
propositional content of an utterance, and content
relevance in the Penn multi-party coding scheme
(Walker et al, 1996). Investigation on dialogue an-
notated for emotion will show whether there are any
interesting correlations with these layers.
4 Emotional speech corpora
One of the difficulties in analysing emotion in com-
munication is in obtaining the material to study. For
studies into task dialogues, researchers can simply
record speakers performing the tasks. However cap-
turing conversational dialogue in general and espe-
cially emotional dialogue is a much more difficult
task.
Studies into emotional speech based on acoustic
features use three approaches to attain their data.
Ideally it is preferable to use genuine speech taken
without the speaker?s knowledge since you can be
confident that the resulting data will faithfully repre-
sent human behaviour. An example of research us-
ing this type of data is (Scherer and Ceschi, 2000).
This approach isn?t commonly adopted, partly be-
cause of the ethical issues concerned with recording
people without their consent and also because of the
difficulty in controlling variables such as recording
quality or establishing age, sex, etc. of the speaker.
For dialogue studies this would also be the desired
type of data. If we are interested communicative be-
haviour such as turn-taking and language use rather
than the acoustic features of the speech then we need
not be so concerned with the acoustic quality. If it
were possible to obtain recordings of police inter-
views, legal trails or calls to emergency services then
these would provide suitable material to study. Our
corpus of oncology consultations is a good example
of this type of dialogue.
A more common type of data used in speech stud-
ies is that of acted emotions. Actors deliver lines
expressed with different emotions (e.g. (Dellaert et
al., 1996)). The quality of this data is reliant on
the accuracy with which the emotion is acted. This
is suitable for establishing the prosodic features as-
sociated with various emotions but not for dialogue
studies. It would be much more difficult to recreate
the communicative behaviour of an emotional per-
son through acting than to simply sound emotional.
Finally, induced emotion, where participants are
provoked into an emotional state so that their speech
can be recorded (e.g. (Huber et al, 2000)) . This
provides natural emotion within a laboratory setting.
It is conceivable that this process could be adapted to
obtain induced emotional dialogue. One participant
may try to conduct a conversation during which the
other may behave emotionally. However it is likely
that the data derived from this would be unlike real
conversations.
It is apparent that when studying emotion in dia-
logue it would be desirable to obtain genuine con-
versations that contained some degree of emotion.
Attempting to induce emotion is likely to cause the
communicative behaviour to become unnatural. The
preferable option would be to use natural conversa-
tion in unusually emotional circumstances such as
those described above.
5 Toward an emotion annotation scheme
In developing an annotation scheme our first step
will be to decide on the facets of emotion which we
would like to identify. Emotion is a very vague word
and so it is important that we polarise it into clear
and understandable aspects of human cognition. In
Layer DAMSL Coconut Traum Penn Maptask
Info level Info Level Info Level Info Type
Comm status Comms Status
Topic Topic Topic
Speech act Dialogue acts Comm function Illocutionary func Speech acts Moves
Info relations Info relations Info relations Argumentation
Relatedness Antecedent Link Relatedness Initiative
Grounding Grounding Info status
Surface form Surface features Surface form
Phases
Layer Verbmobil Chat Condon & Cech Alparon Date
Info level Interchange type Metalanguage Domain
Comm status
Topic Info coding Subtasks
Speech act Dialogue acts Illocutionary force Move function Moves Speech acts
Info relations
Relatedness
Grounding
Surface form
Phases Phases Phases
Table 1: Annotation schemes and their layers
order for the annotation to be useful these aspects
must have some influence on their communicative
behaviour. They must also be identifiable from the
language of the dialogue. This will mean that the
scheme may consist of several layers each describ-
ing a different aspect of human emotion.
One of the differences between these types of lay-
ers and those current schemes is that rather than dis-
crete categories such as those used to label speech
acts we can observe varying levels of emotion. A
precedent for this type of annotation exists in the
labelling of expressions of concern in the oncol-
ogy consultation coding scheme of Psychological
Medicine Group at Manchester University (Heaven
and Green, 2001). where these cues are rated 0?3.
If this approach was adopted then we would have to
decide on the number of levels to chose from based
on a trade-off between ease of performing annota-
tion with getting a fine enough distinction between
different levels.
This would allow us to draw conclusions about
communicative behaviour under different levels of
emotion (e.g. ?The length of utterances becomes
longer under increasing levels of anxiety?) and cor-
relations with other layers (e.g. ?People ask open
questions when relaxed but closed questions when
agitated?). It would also allow us to plot the quanti-
tative level of emotions throughout the dialogue, in-
vestigate the way in which this changes and identify
the language phenomena that signal these changes.
If only for pragmatic reasons, it would be wise to
choose utterances as the basic unit for annotation.
By utterances here we refer to the common under-
standing described as ?a sequence of communicative
behaviour bounded by lack of activity? (Allwood,
1996). This would not only allow us to apply other
schemes to emotionally annotated dialogue, but also
to use tools that have been developed to work on ut-
terances. It would therefore be necessary to chose
dimensions of emotion that can be applied to utter-
ances.
There is an interesting question of whether emo-
tion is a property of the participants or the dialogue.
Obviously two or more people participating in a di-
alogue will react differently to the proceedings and
will therefore exhibit different emotions. However
it is apparent from our corpus that the dialogue it-
self has its own levels of emotion. For instance, the
conversation may go through a phase of solemnity
during which the participants may exchange a joke.
The mood of the dialogue outlives this perturbation
and remains serious. It would appear that it may be
useful to track the emotional state of the dialogue
as well as the speakers since one will clearly have
an effect on the other. Quantitative annotation and
analysis of the flow of these levels would therefore
be useful here too.
6 Future work
Our next step will be to design an annotation scheme
based on the observations and principles stated
throughout this paper. We could then start annotat-
ing our corpus for the emotional dimensions that we
had chosen.
In order to assess the correlations that we pro-
posed might exist in section 3, we would have to
annotate these dialogues with the layers of other
schemes. Since none of schemes contain all of the
layers, we would have to combine individual layers
based on our beliefs about which could be most use-
ful and the ease with which we would annotate the
dialogue. It would make sense to select layers from
schemes which have comprehensive coding manu-
als, which have been shown to be reliable and which
would be accommodated by annotation tools.
Before any claims about the effects of emotion in
dialogue can be made, the reliability of the scheme
must be established. Once this has been achieved
than analysis of the results can begin.
An annotated corpus would present us with the
opportunity to investigate correlations and attempt
to identify the effects the various types of emotion
on the behaviour of the participants. It is likely that
along with the possible effects that we have prof-
fered in this paper there will be other interesting pat-
terns that become apparent from the results of our
annotation. This will improve our understanding of
behaviour in dialogue and benefit dialogue applica-
tions.
References
J Allwood. 1996. On dialogue cohesion. In Papers from
Thirteenth Scandinavian Conference of Linguistics.
A Anderson, M Bader, E Bard, Boyle E, G Doherty,
S Garrod, S Isard, J Kowtko, J McAllister, J Miller,
C Sotillo, H Thompson, and R Weinert. 1991. The
HCRC Map Task corpus. Language and Speech,
34:351?66.
T Bub and J Schwinn. 1996. VERBMOBIL: The evo-
lution of a complex large speech-to-speech translation
system. In Proc. ICSLP ?96, volume 4, pages 2371?
2374, Philadelphia, PA.
S Condon and C Cech, 1996. Manual for Coding
Decision-Making Interactions.
Mark G Core and James F Allen. 1997. Coding dialogs
with the damsl annatation scheme. In AAAI Fall Sym-
posium on Communicative Action in Humans and Ma-
chines.
F. Dellaert, T. Polzin, and A. Waibel. 1996. Recognizing
emotions in speech. In Proc. ICSLP ?96, volume 3,
pages 1970?1973, Philadelphia, PA.
B Di Eugenio, P.W Jordan, and L Pylkkanen, 1998. The
COCONUT project: Dialogue Annotation Manual.
ISP Technical Report 98-1.
K Fischer. 1999. Annotating emotional langauge data.
Technical report, Verbmobil - report 236.
C Heaven and C Green, 2001. Medical Interview Aural
Rating Scale, CRC psychological medical group, Oc-
tober.
R. Huber, A. Batliner, J. Buckow, E. No?th, V. Warnke,
and H. Niemann. 2000. Recognition of Emotion in
a Realistic Dialogue Scenario. In Proc. Int. Conf. on
Spoken Language Processing, volume 1, pages 665?
668, Beijing, China, Oktober.
S Jekat, A Klein, E Maier, I Maleck, M Mast, and
J Quantz. 1995. Dialogue acts in Verbmobil.
Technical Report 65, DFKI Saarbrucken, Universitat
Stuttgart, Technische Universitat Berlin, Universitat
des Saarlandes.
B MacWhinney, 1996. The CHILDES System. Carnegie
Mellon University.
B MacWhinney, 1998. The CHILDES Project: Tools for
Analysing Talk. Carnegie Mellon University.
P Maguire, K Booth, C Elliott, and B Jones. 1996a.
Helping health professionals involved in cancer care
work shops acquire key interviewing skills ? the im-
pact of workshops. European journal of cancer,
32A(9):1486?1489.
P Maguire, K Faulkner, K Booth, C Elliot, and V Hillier.
1996b. Helping cancer patients disclose their con-
cerns. European Journal of Cancer, 32(9):1486?1489.
V. Petrushin. 1999. Emotion in speech: Recognition and
application to call centers.
K.R Scherer and G Ceschi. 2000. Criteria for emo-
tion recognition from verbal and nonverbal expression:
studying baggage loss in the airport. Personality & So-
cial Psychology Bulletin, 26(3):327, March.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema,
and M. Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26:339?373.
D Traum, 1996. Coding Schemes for Spoken Dialogue
Structure.
R.J van Vark, J.P.M de Vreught, and L.J.M Rothkrantz,
1996. Analysing OVR dialogue coding scheme 1.0.
Delft University of Technology.
M Walker and R Passonneau. 2001. DATE: a dialogue
act tagging scheme for evaluation of spoken dialogue
systems. In Proceedings: Human Language Technol-
ogy Conference, San Diego, March. AT&T Shannon
Labs.
M Walker, E Maier, J Allen, J Carletta, S Condon,
G Flammia, J Hirschberg, S Isard, M Ishizaki, L Levin,
S Luperfoy, D Traum, and S Whittaker, 1996. Penn
multiparty standard coding scheme, Draft annotation
manual.
M.M Wood and R Craggs. 2002. Rare dialogue acts in
oncology consultations. In Submitted to SIGdial3.
M.M Wood and R Craggs. 2003. Initiative in health care
dialogues. In Submitted to DiaBruck 7th workshop on
the semantics and pragmatics of dialogue.
M.M Wood. 2001. Dialogue tagsets in oncology. In
Proceedings of Sigdial2.
