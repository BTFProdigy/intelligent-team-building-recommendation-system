A More Discerning and Adaptable Multilingual Transliteration Mechanism
for Indian Languages
Harshit Surana
Language Tech. Research Centre
IIIT, Hyderabad, India
surana.h@gmail.com
Anil Kumar Singh
Language Tech. Research Centre
IIIT, Hyderabad, India
anil@research.iiit.ac.in
Abstract
Transliteration is the process of transcribing
words from a source script to a target script.
These words can be content words or proper
nouns. They may be of local or foreign ori-
gin. In this paper we present a more dis-
cerning method which applies different tech-
niques based on the word origin. The tech-
niques used also take into account the prop-
erties of the scripts. Our approach does not
require training data on the target side, while
it uses more sophisticated techniques on the
source side. Fuzzy string matching is used to
compensate for lack of training on the target
side. We have evaluated on two Indian lan-
guages and have achieved substantially bet-
ter results (increase of up to 0.44 in MRR)
than the baseline and comparable to the state
of the art. Our experiments clearly show that
word origin is an important factor in achiev-
ing higher accuracy in transliteration.
1 Introduction
Transliteration is a crucial factor in Cross Lingual
Information Retrieval (CLIR). It is also important
for Machine Translation (MT), especially when the
languages do not use the same scripts. It is the pro-
cess of transforming a word written in a source lan-
guage into a word in a target language without the
aid of a resource like a bilingual dictionary. Word
pronunciation is usually preserved or is modified ac-
cording to the way the word should be pronounced
in the target language. In simple terms, it means
finding out how a source word should be written in
the script of the target languages such that it is ac-
ceptable to the readers of the target language.
One of the main reasons of the importance of
transliteration from the point of view of Natural Lan-
guage Processing (NLP) is that Out Of Vocabulary
(OOV) words are quite common since every lexi-
cal resource is very limited in practical terms. Such
words include named entities, technical terms, rarely
used or ?difficult? words and other borrowed words,
etc. The OOV words present a challenge to NLP ap-
plications like CLIR and MT. In fact, for very close
languages which use different scripts (like Hindi and
Urdu), the problem of MT is almost an extension of
transliteration.
A substantial percentage of these OOV words
are named entities (AbdulJaleel and Larkey, 2003;
Davis and Ogden, 1998). It has also been shown
that cross language retrieval performance (average
precision) reduced by more than 50% when named
entities in the queries were not transliterated (Larkey
et al, 2003).
Another emerging application of transliteration
(especially in the Indian context) is for building in-
put methods which use QWERTY keyboard for peo-
ple who are more comfortable typing in English.
The idea is that the user types Roman letters but
the input method transforms them into letters of In-
dian language (IL) scripts. This is not as simple
as it seems because there is no clear mapping be-
tween Roman letters and IL letters. Moreover, the
output word should be a valid word. Several com-
mercial efforts have been started in this direction
due to the lack of a good (and familiar) input mech-
64
anism for ILs. These efforts include the Google
Transliteration mechanism1 and Quilpad2. (Rathod
and Joshi, 2002) have also developed more intuitive
input mechanisms for phonetic scripts like Devana-
gari.
Our efforts take into account the type of the word,
the similarities among ILs and the characteristics of
the Latin and IL scripts. We use a sophisticated tech-
nique and machine learning on the source language
(English) side, while a simple and light technique on
the target (IL) side. The advantage of our approach
is that it requires no resources except unannotated
corpus (or pages crawled from the Web) on the IL
side (which is where the resources are scarce). The
method easily generalizes to ILs which use Brahmi
origin scripts. Our method has been designed such
that it can be used for more conventional applica-
tions (MT, CLIR) as well as for applications like
building an input mechanism.
Much of the work for transliteration in ILs has
been done from one Indian script to another. One
of the major work is of Punjabi machine transliter-
ation (Malik, 2006). This work tries to address the
problem of transliteration for Punjabi language from
Shahmukhi (Arabic script) to Gurmukhi using a set
of transliteration rules (character mappings and de-
pendency rules). Om transliteration scheme (Gana-
pathiraju et al, 2005) also provides a script repre-
sentation which is common for all Indian languages.
The display and input are in human readable Roman
script. Transliteration is partly phonetic. (Sinha,
2001) had used Hindi Transliteration used to handle
unknowns in MT.
naukri (A popular domain name) 722,000
nokri (domain name) 19,800
naukari 10,500
naukary (domain name) 5,490
nokari 665
naukarii 133
naukaree 102
Table 1: Variations of a Hindi Word nOkarI (job).
The numbers are pages returned when searching on
Google.
1www.google.co.in/press/pressrel/news transliteration.html
2www.quillpad.com
Aswani et. al (Aswani and Gaizauskas, 2005)
have used a transliteration similarity mechanism to
align English-Hindi parallel texts. They used char-
acter based direct correspondences between Hindi
and English to produce possible transliterations.
Then they apply edit distance based similarity to se-
lect the most probable transliteration in the English
text. However, such method can only be appropriate
for aligning parallel texts as the number of possible
candidates is quite small.
The paper is structured as follows. In Section-
2, we discuss the problem of a high degree of vari-
ation in Indian words, especially when written in
Latin script. In Section-3, we explain the idea of
using information about the word origin for improv-
ing transliteration. Then in Section-4 we describe
the method that we use for guessing the word origin.
Once the word origin is guessed, we can apply one
of the two methods for transliteration depending on
the word origin. These two methods are described in
Section-5 and Section-6, respectively. Fuzzy string
matching, which plays an important role in our ap-
proach, is described in Section-7. In Section-8 we
put together all the elements covered in the pre-
ceding sections and explain the Discerning Adapt-
able Transliteration Mechanism. Section-9 presents
the evaluation of our approach in comparison with
two baseline methods, one of which uses knowledge
about word origin. Finally, in Section-10 we present
the conclusions.
2 Variation in Indian Words in Latin
Script
Since the purpose of our work is not only to translit-
erate named entities but to be useful for applications
like input mechanisms, we had to consider some
other issues too which may not be considered di-
rectly related to transliteration. One of these is that
there is a lot of spelling variation in ILs. This vari-
ation is much more when the IL words are written
using the Latin script (Table-1). In other words,
the amount of ambiguity is very high when we try
to build a system that can be used for purposes
like designing input mechanisms, instead of just for
transliteration of NEs etc. for MT or CLIR. One
reason for very high variation in the latter case is
that unlike Romaji for Japanese (which is taught in
65
schools in Japan), there is no widely adopted translit-
eration scheme using the Latin script, although there
are a number of standard schemes, which are not
used by common users. At present the situation is
that most Indians use Indian scripts while writing in
ILs, but use the Latin script when communicating
online. ILs are rarely used for official communica-
tion, except in government offices in some states.
3 Word Origin and Two Ways of
Transliteration
Previous work for other languages has shown that
word origin plays a part in how the word should
be transliterated(Oh and Choi, 2002; May et al,
2004). Llitjos and Black (Llitjos and Black, 2001)
had shown that the knowledge of language origin
can substantially improve pronunciation generation
accuracy. This information has been used to get bet-
ter results (Oh and Choi, 2002). They first checked
whether the word origin is Greek or not before se-
lecting one of the two methods for transliteration.
This approach improved the results substantially.
However, they had used a set of prefixes and suffixes
to identify the word origin. Such an approach is not
scalable. In fact, in a large number of cases, word
origin cannot be identified by using list of affixes.
For ILs, we also define two categories of words:
words which can be roughly considered Indian and
those which can be roughly considered foreign.
Note that ?Indian? and ?foreign? are just loose labels
here. Indian words, which include proper nouns and
also common vocabulary words, are more relevant in
applications like input methods. Two different meth-
ods are used for transliterating, as explained later.
4 Disambiguating Word Origin
Previously (Llitjos and Black, 2001) used probabili-
ties of all trigrams to belong to a particular language
as an measure to disambiguate word origins. We
use a more sophisticated method that has been suc-
cessfully used for language and encoding identifica-
tion (Singh, 2006a).
We first prepare letter based 5-gram models from
the lists of two kinds of words (Indian and foreign).
Then we combine n-grams of all orders and rank
them according to their probability in descending or-
der. Only the top N n-grams are retained and the
rest are pruned. Now we have two probability dis-
tributions which can be compared by a measure of
distributional similarity. The measure used is sym-
metric cross entropy or SCE (Singh, 2006a).
Since the accuracy of identification is low if test
data is very low, which is true in our case because we
are trying to identify the class of a single word, we
had to extend the method used by Singh. One ma-
jor extension was that we add word beginning and
ending markers to all the words in training as well
as test data. This is because n-grams at beginning,
middle and end of words should be treated differ-
ently if we want to identify the ?language? (or class)
of the word.
For every given word, we get a probability about
its origin based on SCE. Based on this probability
measure, transliteration is performed using different
techniques for different classes (Indian or foreign).
In case of ambiguity, transliteration is performed us-
ing both methods and the probabilities are used to
get the final ranking of all possible transliterations.
5 Transliteration of Foreign Words
These words include named entities (George Bush)
and more common nouns (station, computer) which
are regularly used in ILs. To generate translitera-
tion candidates for such words, we first try to guess
the word pronunciation or use a lookup dictionary (if
available) to find it. Then we use some simple man-
ually created mappings, which can be used for all In-
dian languages. Note that these mappings are very
few in number (Figure-1 and Figure-2) and can be
easily created by non-linguistically trained people.
They play only a small role in the method because
other steps (like fuzzy string matching) do most of
the work.
For our experiments, we used the CMU speech
dictionary as the lookup, and also to train pronunci-
ation estimation. If a word is not in the CMU dic-
tionary, we estimate the word pronunciation, as ex-
plained later.
We directly map from English phonemes to IL let-
ters. This is based on our observation that a foreign
word is usually transliterated in almost the same way
as it is pronounced. Almost all English phonemes
can be roughly mapped to specific letters (repre-
senting phonemes, as IL scripts are phonetic in na-
66
ture) in ILs. Similar observations have been made
about Hindi by Su-Youn Yoon, Kyoung-Young Kim
and Richard Sproat (Yoon et al, 2007). We have
prepared our own mappings with help from native
speakers of the languages concerned, which is rel-
atively quite a simple task since the letters in Indic
scripts correspond closely with phonemes.
6 Transliteration of Indian Words
These words include (mainly Indian) named enti-
ties of (e.g. Taj Mahal, Manmohan Singh) and
common vocabulary words (common nouns, verbs)
which need to be transliterated. They also include
words which are spelled similar to the way Indian
words are spelled when written in Latin (e.g. Bagh-
dad, Husain). As stated earlier, this class of words
are much more relevant for an input method using a
QWERTY keyboard.
Since words of Indian origin usually have pho-
netic spellings when they are written in English
(Latin), the issue of pronunciation estimation or
lookup is not important. However, there can be
many possible vowel and consonant segments which
can be formed out of a single word. For example
?ai? can be interpreted as a single vowel with sound
AE (as in Husain), or as two vowels AA IH (as in
Rai). To perform segmentation, we have a simple
program which produces candidates for all possible
segments. This program uses a few rules defining
the possible consonant and vowel combinations.
Now we simply map these segments to their near-
est IL letters (or letter combinations). This is also
done using a simple set of mappings, which do not
contain any probabilities or contexts. This step gen-
erates transliteration candidates. These are then fil-
tered and ranked using fuzzy string matching.
7 Fuzzy String Matching
The initial steps use simpler methods to generate
transliteration candidates on the source as well as
the target side. They also use no resources on the
target (IL) side. The step of fuzzy string matching
compensates for the lack of more language specific
knowledge during the earlier phase. The transliter-
ation candidates are matched with the words in the
target language corpus (actually, words in the word
list extracted from the corpus). The fuzzy string
Figure 1: Mappings for foreign words. The three
columns are for Roman, Devanagari and Telugu
matching algorithm we use is finely tuned for Indian
Languages and performs much better than language
independent approaches like edit distance (Singh et
al., 2007). This method can be used for all the lan-
guages which use Abugida scripts, e.g. Hindi, Ben-
gali, Telugu, Amharic, Thai etc. It uses characteris-
tics of a writing system for fuzzy search and is able
to take care of spelling variation, which is very com-
mon in these languages. This method shows an im-
provement in F-measure of up to 30% over scaled
edit distance.
The method for fuzzy string matching is based
on the Computational Phonetic Model of Scripts
or CPMS (Singh, 2006b), which models scripts
(specifically Indic scripts) in terms of phonetic (ar-
ticulatory) and orthographic features. For calculat-
ing the distance between two letters it uses a Stepped
Distance Function (SDF). Each letter is represented
as a vector of features. Then, to calculate the dis-
tance between two strings, it uses an adapted ver-
sion of the Dynamic Time Warping algorithm (My-
67
Figure 2: Mappings for Indian Words
ers, 1980). In the fuzzy string matching method that
we use (Singh et al, 2007), an akshar (roughly a
syllable) is used as the unit, instead of a letter.
8 Discerning Adaptable Transliteration
Mechanism (DATM)
We use the above mentioned steps to transliterate a
given word based on its origin. In case of ambigu-
ity of word origin both methods are used, and pos-
sible transliterations are ranked. Based on the class
of the word, the possible pronunciations (for foreign
words) and the possible segmentations (for Indian
words) are generated. Then, for foreign words, En-
glish phonemes are mapped to IL segments. For In-
dian words, Latin segments are mapped to IL seg-
ments.
Now, the transliteration candidates are matched
with target language words, using the fuzzy text
search method (Singh et al, 2007). Possible translit-
erations are ranked based on three parameters: word
frequency, text search cost and the probability of
the word belonging to the class through which it
ForeignWords Indian Words
Word Class Identifier
Pronounciation
Guesser
Word
Segmentation
English Phonemes to
IL Segments Maps
Latin Segments to
IL Segments Maps
Possible
Pronounciations
Possible
Segmentations
Fuzzy String Matching
Transliteration
Candidates
Ranked
Transliterations
Figure 3: Block Diagram of the Discerning Adaptive
Transliteration Method (DATM)
is transliterated. A block diagram describing the
method is shown in Figure-3. The ranks are obtained
on the basis of a score which is calculated using the
following formula:
Tt =
log(ft) ? p(C | s)
cost(c, t) + K (1)
where Tt is the transliteration score for the tar-
get word t, ft is the frequency of t in the target lan-
guage corpus, C is the word class (foreign or In-
dian), s is the source word, c is a transliteration can-
didate which has been generated depending on the
predicted class C , p(C|s) is the probability of the
class C given s, cost(c, t) is the cost of fuzzy string
matching between c and t, and finally K is a con-
stant which determines how much weight is given to
the cost of fuzzy string matching.
9 Evaluation
We evaluate our method for two major languages of
India: Hindi and Telugu. We compare our results
with a very commonly used method (Oh and Choi,
2006) based on bilingual dictionary to learn translit-
68
Language ? English-Hindi English-Telugu
Method ? MRR Pr MRR Pr
DATM 0.87 80% 0.82 71%
DBL 0.56 47% 0.53 46%
BL 0.43 35% 0.43 37%
DATM: Discerning Adaptive Transliteration Mechanism
DBL: Discerning Baseline Method
BL: Baseline Method
MRR: Mean Reciprocal Rank
Pr: Precision
Table 2: Evaluation on English-Hindi and English-Telugu
erations. As there are no bilingual transliteration
dictionaries available for ILs, we had to create our
own resources.
9.1 Experimental Setup
We created 2000-word lists which consisted of both
foreign and Indian words written in Latin script
and their transliterations in Hindi and Telugu. This
dictionary was created by people with professional
knowledge in both English and the respective In-
dian language. We only use this list for training
the baseline method, as our method does not need
training data on the target side. The size of bilingual
word lists that we are using is less than those used
for experiments by some other researchers. But our
approach focuses on developing transliterations for
languages with resource scarcity. This setup is more
meaningful for languages with scarce resources.
Since, normal transliteration mechanisms do not
consider word origin, we train the baseline using
the set of 2000 words containing both foreign and
Indian words. Alignments from English to respec-
tive Indian languages were learned by aligning these
lists using GIZA++. The alignments obtained were
fed into a maximum entropy classifier with a con-
text window size of 2 (3 is generally considered
better window size, but because the training size
is not huge, a context window of 3 gave substan-
tially worse results). This method is similar to
the grapheme based model as described by Oh and
Choi (Oh and Choi, 2006). However, unlike in
their approach, the candidate pairs are matched with
words in the target language and are ranked based
on edit distance (BL).
For our method (DATM), we have used CMU dic-
tionary and a collection of Indian named entities
(written in Latin) extracted from web to train the
language identification module. We have consid-
ered n-grams of order 5 and pruned them by 3500
frequency. In case the foreign word is not found in
CMU Speech dictionary, we guess its pronunciation
using the method described by Oh and Choi. How-
ever, in this case, the context window size is 3.
We also use another method (DBL) to check the
validity of our assumptions about word origin. We
use the same technique as BL, but in this case we
train two models of 1000 words each, foreign and
Indian. To disambiguate which model to use, we
use the same language identification method as in
DATM.
9.2 Results
To evaluate our method we have created word lists
of size 200 which were doubly checked by two indi-
viduals. These also contain both Indian and Foreign
words. We use both precision and mean reciprocal
rank (MRR) to evaluate our method against base-
line (BL) and discerning baseline (DBL). MRR is
a measure commonly used in information retrieval
when there is precisely one correct answer (Kandor
and Vorhees, 2000). Results can be seen in Table-
2. The highest scores were obtained for Hindi using
DATM. The MRR in this case was 0.87.
One important fact that comes out from the re-
sults is that determining the class of a word and then
using an appropriate method can lead to significant
increase in performance. This is clear from the re-
sults for BL and DBL. The only difference between
69
English-Hindi
0
20
40
60
80
100
120
140
160
180
1 2 3 4 5
Rank
Nu
m
be
ro
fW
or
ds
DATM
DBL
BL
English-Telugu
0
20
40
60
80
100
120
140
160
1 2 3 4 5
Rank
Nu
m
be
ro
fW
or
ds
DATM
DBL
BL
Figure 4: Number of Correct Words vs. Rank. A significantly higher percentage of correct words occur
at rank 1 for the DATM method, as compared to BL and DBL methods. This percentage indicates a more
practical view of the accuracy transliteration algorithm.
these two was that two different models were trained
for the two classes. Then the class of the word was
identified (in DBL) and the model trained for that
class was used for transliteration.
It should be noted that Yoon et al (Yoon et al,
2007) have also reported MRR score on Hindi. They
have used a number of phonetic and pseudo features,
and trained their algorithm on a winnow classifier.
They tested their algorithm only for named entities.
They have considered a relatively limited number of
candidate words on the target language side (1,500)
which leads to 150k pairs on which they have eval-
uated their method. They have reported the results
as 0.91 and 0.89 under different test conditions. In
case of our evaluation, we do not restrict the candi-
date words on the target side except that it should
be available in the corpus. Because of this formula-
tion, there are over 1000k words for Hindi and over
1800k words from Telugu. This leads to a extremely
high number of pairs possible. But such an approach
is also necessary as we want our algorithm to be
scalable to bigger sizes and also because there are
no high quality tools (like named entity recogniz-
ers) for Indian languages. This is one of the reason
for relatively (compared to figures reported by other
researchers) low baseline scores. Despite all these
issues, our simpler approach yields similar results.
Figure-4 shows how the number of correct words
varies with the rank.
Two possible issues are the out of vocabulary
(OOV) words and misspelled or foreign words in
the IL corpus. The OOV words are not handled
right now by our method, but we plan to extend our
method to at least partially take care of such words.
The second issue is mostly resolved by our use of
fuzzy string matching, although there is scope for
improvement.
10 Conclusions and Further Work
We presented a more general and adaptable method
for transliteration which is especially suitable for In-
dian languages. This method first identifies the class
(foreign or Indian) of the word on the source side.
Based on the class, one of the two methods is used
for transliteration. Easily creatable mapping tables
and a fuzzy string matching algorithm are then used
to get the target word. Our evaluations shows that
the method performs substantially better than the
two baselines we tested against. The results are bet-
ter in terms of both MRR (up to 0.44) and precision
(45%). Our method is designed to be used for other
applications like tolerant input methods for Indian
languages and it uses no resources on the target lan-
guages side except an unannotated corpus. The re-
sults can be further improved if we consider context
information too.
We have also shown that disambiguating word
origin and applying an appropriate method could be
70
critical in getting good transliterations. Currently we
are assuming that the word to be transliterated is in
the target language corpus. We plan to extend the
method so that even those words can be transliter-
ated which are not in the target language corpus. We
are also working on using this method for building
a tolerant input method for Indian languages and on
integrating the transliteration mechanism as well as
the input method with an open source NLP friendly
editor called Sanchay Editor (Singh, 2008).
References
N. AbdulJaleel and L.S. Larkey. 2003. Statistical
transliteration for english-arabic cross language infor-
mation retrieval. Proceedings of the twelfth interna-
tional conference on Information and knowledge man-
agement, pages 139?146.
N. Aswani and R. Gaizauskas. 2005. A hybrid approach
to align sentences and words in English-Hindi paral-
lel corpora. Proceedings of the ACL Workshop on?
Building and Exploiting Parallel Texts.
M.W. Davis and W.C. Ogden. 1998. Free resources
and advanced alignment for cross-language text re-
trieval. Proceedings of the 6th Text Retrieval Confer-
ence (TREC-6), pages 385?402.
M. Ganapathiraju, M. Balakrishnan, N. Balakrishnan,
and R. Reddy. 2005. OM: One Tool for Many (In-
dian) Languages. ICUDL: International Conference
on Universal Digital Library, Hangzhou.
L. Larkey, N. AbdulJaleel, and M. Connell. 2003.
What?s in a Name? Proper Names in Arabic Cross-
Language Information Retrieval. Technical report,
CIIR Technical Report, IR-278.
A. Llitjos and A. Black. 2001. Knowledge of language
origin improves pronunciation of proper names. Pro-
ceedings of EuroSpeech-01, pages 1919?1922.
M.G.A. Malik. 2006. Punjabi Machine Transliteration.
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the ACL, pages 1137?1144.
J. May, A. Brunstein, P. Natarajan, and R. Weischedel.
2004. Surprise! What?s in a Cebuano or Hindi Name?
ACM Transactions on Asian Language Information
Processing (TALIP), 2(3):169?180.
C. S. Myers. 1980. A Comparative Performance Study of
Several Dynamic Time Warping Algorithms for Speech
Recognition. Ph.D. thesis, M.I.T., Cambridge, MA,
Feb. http://gate.ac.uk.
J.H. Oh and K.S. Choi. 2002. An English-Korean
transliteration model using pronunciation and contex-
tual rules. Proceedings of the 19th international con-
ference on Computational linguistics-Volume 1, pages
1?7.
J.H. Oh and K.S. Choi. 2006. An ensemble of translit-
eration models for information retrieval. Information
Processing and Management: an International Jour-
nal, 42(4):980?1002.
A. Rathod and A. Joshi. 2002. A Dynamic Text Input
scheme for phonetic scripts like Devanagari. Proceed-
ings of Development by Design (DYD).
Anil Kumar Singh, Harshit Surana, and Karthik Gali.
2007. More accurate fuzzy text search for languages
using abugida scripts. In Proceedings of ACM SI-
GIR Workshop on Improving Web Retrieval for Non-
English Queries, Amsterdam, Netherlands.
Anil Kumar Singh. 2006a. Study of some distance mea-
sures for language and encoding identification. In Pro-
ceedings of ACL 2006 Workshop on Linguistic Dis-
tance, Sydney, Australia.
Anil Kumar Singh. 2006b. A computational phonetic
model for indian language scripts. In Constraints on
Spelling Changes: Fifth International Workshop on
Writing Systems, Nijmegen, The Netherlands.
Anil Kumar Singh. 2008. A mechanism to provide
language-encoding support and an nlp friendly editor.
In Proceedings of the Third International Joint Con-
ference on Natural Language Processing, Hyderabad,
India.
RMK Sinha. 2001. Dealing with unknowns in machine
translation. Systems, Man, and Cybernetics, 2001
IEEE International Conference on, 2.
S.Y. Yoon, K.Y. Kim, and R. Sproat. 2007. Multilingual
Transliteration Using Feature based Phonetic Method.
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 112?119.
71
Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 25?32,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Aggregating Machine Learning and Rule Based Heuristics for Named 
Entity Recognition 
 Karthik Gali, Harshit Surana, Ashwini Vaidya, Praneeth Shishtla and  
Dipti Misra Sharma 
Language Technologies Research Centre, 
International Institute of Information Technology, 
Hyderabad, India. 
karthikg@students.iiit.ac.in, surana.h@gmail.com,  
ashwini_vaidya@research.iiit.ac.in, praneethms@students.iiit.ac.in, 
dipti@iiit.ac.in 
 
 
 
Abstract 
This paper, submitted as an entry for the 
NERSSEAL-2008 shared task, describes a 
system build for Named Entity Recognition 
for South and South East Asian Languages.  
Our paper combines machine learning 
techniques with language specific heuris-
tics to model the problem of NER for In-
dian languages. The system has been tested 
on five languages: Telugu, Hindi, Bengali, 
Urdu and Oriya. It uses CRF (Conditional 
Random Fields) based machine learning, 
followed by post processing which in-
volves using some heuristics or rules. The 
system is specifically tuned for Hindi and 
Telugu, we also report the results for the 
other four languages. 
1 Introduction 
Named Entity Recognition (NER) is a task that 
seeks to locate and classify entities (?atomic ele-
ments?) in a text into predefined categories such as 
the names of persons, organizations, locations, ex-
pressions of times, quantities, etc. It can be viewed 
as a two stage process: 
  
1. Identification of entity boundaries 
2. Classification into the correct category 
 
For example, if ?Mahatma Gandhi? is a named 
entity in the corpus, it is necessary to identify the 
beginning and the end of this entity in the sentence. 
Following this step, the entity must be classified 
into the predefined category, which is NEP 
(Named Entity Person) in this case. 
This task is the precursor for many natural lan-
guage processing applications. It has been used in 
Question Answering (Toral et al 2005) as well as 
Machine Translation (Babych et al 2004). 
The NERSSEAL contest has used 12 categories 
of named entities to define a tagset. The data has 
been manually tagged for training and testing pur-
poses for the contestants. 
The task of building a named entity recognizer 
for South and South East Asian languages presents 
several problems related to their linguistic charac-
teristics. We will first discuss some of these lin-
guistic issues, followed by a description of the 
method used. Further, we show some of the heuris-
tics used for post-processing and finally an analy-
sis of the results obtained.  
2 Previous Work  
The linguistic methods generally use rules 
manually written by linguists. There are several 
rule based NER systems, containing mainly lexi-
calized grammar, gazetteer lists, and list of trigger 
words, which are capable of providing upto 92% f-
measure accuracy for English (McDonald, 1996; 
Wakao et al, 1996).  
Linguistic approach uses hand-crafted rules 
which need skilled linguistics. The chief disadvan-
tage of these rule-based techniques is that they re-
quire huge experience and grammatical knowledge 
of the particular language or domain and these sys-
tems are not transferable to other languages or do-
mains. However, given the closer nature of many 
Indian languages, the cost of adaptation of a re-
25
source from one language to another could be quite 
less (Singh and Surana, 2007). 
Various machine learning techniques have also 
been successfully used for the NER task. Generally 
hidden markov model (Bikel et al,1997), maxi-
mum entropy (Borthwick, 1999), conditional ran-
dom field (Li and Mccallum, 2004) are more popu-
lar machine learning techniques used for the pur-
pose of NER. 
Hybrid systems have been generally more effec-
tive at the task of NER. Given lesser data and more 
complex NE classes which were present in 
NERSSEAL shared task, hybrid systems make 
more sense. Srihari et al (2000) combines MaxEnt, 
hidden markov model (HMM) and handcrafted 
rules to build an NER system. 
Though not much work has been done for other 
South Asian languages, some previous work fo-
cuses on NER for Hindi. It has been previously 
attempted by Cucerzan and Yarowsky in their lan-
guage independent NER work which used morpho-
logical and contextual evidences (Cucerzan and 
Yarowsky, 1999). They ran their experiment with 
5 different languages. Among these the accuracy 
for Hindi was the worst. For Hindi the system 
achieved 42% f-value with a recall of 28% and 
about 85% precision. A result which highlights 
lack of good training data, and other various issues 
involved with linguistic handling of Indian lan-
guages. 
Later approaches have resulted in better results 
for Hindi. Hindi NER system developed by Wei Li 
and Andrew Mccallum (2004) using conditional 
random fields (CRFs) with feature induction have 
achieved f-value of 71%. (Kumar and Bhat-
tacharyya, 2006) used maximum entropy markov 
model to achieve f-value of upto 80%. 
3 Some Linguistic Issues 
3.1 Agglutinative Nature 
Some of the SSEA languages have agglutinative 
properties.  For example, a Dravidian language like 
Telugu has a number of postpositions attached to a 
stem to form a single word. An example is: 
 
guruvAraMwo = guruvAraM + wo  
up to Wednesday = Wednesday + up to 
 
Most of the NERs are suffixed with several dif-
ferent postpositions, which increase the number of 
distinct words in the corpus.  This in turn affects 
the machine learning process. 
3.2 No Capitalization 
All the five languages have scripts without graphi-
cal cues like capitalization, which could act as an 
important indicator for NER.  For a language like 
English, the NER system can exploit this feature to 
its advantage. 
3.3 Ambiguity 
One of the properties of the named entities in these 
languages is the high overlap between common 
names and proper names. For instance Kamal (in 
Hindi) can mean ?lotus?, which is not a named en-
tity, but it can also be a person?s name, in which 
case, it is a named entity. 
Among the named entities themselves, there is 
ambiguity between a location name Bangalore ek 
badzA shaher heI (Bangalore is a big city) or a per-
son?s surname ?M. Bangalore shikshak heI? (M. 
Bangalore is a teacher). 
3.4 Low POS Tagging Accuracy for Nouns 
For English, the available tools like POS (Part of 
Speech) tagger can be used to provide features for 
machine learning. This is not very helpful for 
SSEA languages because the accuracy for noun 
and proper noun tags is quite low (PVS and G., 
2006) Hence, features based on POS tags cannot 
be used for NER for these languages. 
To illustrate this difficulty, we conducted the 
following experiment. A POS tagger (described in 
PVS & G.,2006) was run on the Hindi test data.  
The data had 544 tokens with NEL, NEP, NEO 
tags.  The POS tagger should have given the NNP 
(proper noun) tag for all those named entities. 
However the tagger was able to tag only 80 tokens 
accurately. This meant that only 14.7% of the 
named entities were correctly recognized. 
3.5 Spelling Variation 
One other important language related issue is the 
variation in the spellings of proper names. For in-
stance the same name Shri Ram Dixit can be writ-
ten as Sri. Ram Dixit, Shree Ram Dixit, Sh. R. Dixit 
and so on. This increases the number of tokens to 
be learnt by the machine and would perhaps also 
require a higher level task like co-reference resolu-
tion. 
 
26
2.6 Pattern of suffixes We have converted this format into the BIO 
format as described in Ramshaw et. al. For exam-
ple, the above format will now be shown as: 
 
Named entities of Location (NEL) or Person 
(NEP) will share certain common suffixes, which 
can be exploited by the learning algorthm. For in-
stance, in Hindi, -pur (Rampur, Manipur) or -giri 
(Devgiri) are suffixes that will appear in the named 
entities for Location. Similarly, there are suffixes 
like -swamy (Ramaswamy, Krishnaswamy) or -
deva (Vasudeva, Mahadeva) which can be com-
monly found in named entities for person. These 
suffixes are cues for some of the named entities in 
the SSEA languages. 
 
Rabindranath  B-NEP 
Tagore   I-NEP 
ne   O 
kahaa   O 
 
The training data set contains (approximately) 
400,000 Hindi, 50,000 Telugu, 35,000 Urdu, 
93,000 Oriya and 120,000 Bengali words respec-
tively.  
A NER system can be rule-based, statistical or 
hybrid. A rule-based NER system uses hand-
written rules to tag a corpus with named entities. A 
statistical NER system learns the probabilities of 
named entities using training data, whereas hybrid 
systems use both. 
5 Conditional Random Fields 
Conditional Random Fields (CRFs) are undirected 
graphical models used to calculate the conditional 
probability of values on designated output nodes 
given values assigned to other designated input 
nodes. Developing rule-based taggers for NER can be cumbersome as it is a language specific process. 
Statistical taggers require large amount of anno-
tated data (the more the merrier) to train.  Our sys-
tem is a hybrid NER tagger which first uses Condi-
tional Random Fields (CRF) as a machine learning 
technique followed by some rule based post-
processing. 
In the special case in which the output nodes of 
the graphical model are linked by edges in a linear 
chain, CRFs make a first-order Markov independ-
ence assumption, and thus can be understood as 
conditionally-trained finite state machines (FSMs). 
Let o = (o,,o
We treat the named entity recognition problem 
as a sequential token-based tagging problem. 
According to Lafferty et. al. CRF outperforms 
other Machine Learning algorithms viz., Hidden 
Markov Models (HMM), Maximum Entropy 
Markov Model (MEMM) for  sequence labeling 
tasks.  
4 Training data 
The training data given by the organizers was in 
SSF format1. For example in SSF format, the 
named entity ?Rabindranath Tagore? will be shown 
in the following way: 
0 (( SSF 
1  ((  NP  <ne=NEP> 
1.1  Rabindranath 
1.2 Tagore 
)) 
2 ne 
3 kahaa 
 )) 
 
                                                          
1 http://shiva.iiit.ac.in/SPSAL2007/ssf-analysis-representation.pdf
2,o3 ,o4 ,... oT  ) be some observed in-
put data sequence, such as a sequence of words in 
text in a document,(the values on n input nodes of 
the graphical model). Let S be a set of FSM states, 
each of which is associated with a label, l ? ?. 
Let s = (s ,s ,s  ,s  ,... s1 2 3 4 T ) be some sequence of 
states, (the values on T output nodes). By the 
Hammersley-Clifford theorem, CRFs define the 
conditional probability of a state sequence given an 
input sequence to be: 
 
where Zo is a normalization factor over all state 
sequences is an arbitrary feature function over its 
arguments, and ?k is a learned weight for each fea-
ture function. A feature function may, for example, 
be defined to have value 0 or 1. Higher ? weights 
make their corresponding FSM transitions more 
likely. CRFs define the conditional probability of a 
label sequence based on the total probability over 
the state sequences, 
 
 
27
 
where l(s) is the sequence of labels correspond-
ing to the labels of the states in sequence s. 
Note that the normalization factor, Zo, (also 
known in statistical physics as the partition func-
tion) is the sum of the scores of all possible states. 
 
And that the number of state sequences is expo-
nential in the input sequence length, T. In arbitrar-
ily-structured CRFs, calculating the partition func-
tion in closed form is intractable, and approxima-
tion methods such as Gibbs sampling or loopy be-
lief propagation must be used. In linear-chain 
structured CRFs (in use here for sequence model-
ing), the partition function can be calculated effi-
ciently by dynamic programming. 
6 CRF Based Machine Learning 
We used the CRF model to perform the initial tag-
ging followed by post-processing. 
6.1 Statistical Tagging 
In the first phase, we have used language inde-
pendent features to build the model using CRF. 
Orthographic features (like capitalization, decimals), 
affixes (suffixes and prefixes), context (previous 
words and following words), gazetteer features, POS 
and morphological features etc. are generally used for 
NER. In English and some other languages, capitali-
zation features play an important role as NEs are 
 generally capitalized for these languages. Unfortu-
nately as explained above this feature is not applica-
ble for the Indian languages. 
Precision Recall F-Measure  
Pm Pn Pl Rm Rn Rl Fm Fn Fl  
Bengali 53.34 49.28 58.27 26.77 25.88 31.19 35.65 33.94 40.63 
Hindi 59.53 63.84 64.84 41.21 41.74 40.77 48.71 50.47 50.06 
Oriya 39.16 40.38 63.70 23.39 19.24 28.15 29.29 26.06 39.04 
Telugu 10.31 71.96 65.45 68.00 30.85 29.78 08.19 43.19 40.94 
Urdu 43.63 44.76 48.96 36.69 34.56 39.07 39.86 39.01 43.46 
Table 1: Evaluation of the NER System for Five Languages 
The exact set of features used are described be-
low. 
6.2 Window of the Words 
Words preceding or following the target word may 
be useful for determining its category. Following a 
few trials we found that a suitable window size is 
five. 
6.3 Suffixes 
Statistical suffixes of length 1 to 4 have been con-
sidered. These can capture information for named 
entities having the NEL tag like Hyderabad, 
Secunderabad, Ahmedabad etc., all of which end 
in -bad. We have collected lists of such suffixes for 
NEP (Named Entity Person) and NEL (Named En-
tity Location) for Hindi. In the machine learning 
model, this resource can be used as a binary fea-
ture. A sample of these lists is as follows: 
 
Type of NE Example suffixes 
(Hindi) 
NE- Location -desa, -vana, -nagara,  
-garh, -rashtra, -giri  
NE ? Person -raja, -natha, -lal, -bhai,-
pathi, -krishnan 
 Table 2: Suffixes for Hindi NER 
28
7 Heuristics Based Post Processing 6.4 Prefixes 
Statistical prefixes of length 1 to 4 have been con-
sidered. These can take care of the problems asso-
ciated with a large number of distinct tokens. As 
mentioned earlier, agglutinative languages can 
have a number of postpositions. The use of pre-
fixes will increase the probability of   Hyderabad 
and Hyderabadlo (Telugu for ?in Hyderabad?) be-
ing treated as the same token. 
Complex named entities like fifty five kilograms 
contain a Named Entity Number within a Named 
Entity Measure. We observed that these were not 
identified accurately enough in the machine learn-
ing based system. Hence, instead of applying ma-
chine learning to handle nested entities we make 
use of rule-based post processing.  
7.1 Second Best Tag 
Table 3: F-Measure (Lexical) for NE Tags 
 Bengali Hindi Oriya Telugu Urdu 
It was observed that the recall of the CRF model is 
low. In order to improve recall, we have used the 
following rule:  if the best tag given by the CRF 
model is O (not a named entity) and the confidence 
of the second best tag is greater than 0.15, then the 
second best tag is considered as the correct tag. 
NEP 35.22 54.05 52.22 01.93 31.22 
NED NA 42.47 01.97 NA 21.27 
NEO 11.59 45.63 14.50 NA 19.13 
NEA NA 61.53 NA NA NA 
We observed an increase of 7% in recall and 3% 
decrease in precision. This resulted in a 4% in-
crease in the F-measure, which is a significant in-
crease in performance. The decrease in precision is 
expected as we are taking the second tag. 
NEB NA NA NA NA NA 
NETP 42.30 NA NA NA NA 
NETO 33.33 13.77 NA 01.66 NA 
NEL 45.27 62.66 48.72 01.49 57.85 
7.2 Nested Entities NETI 55.85 79.09 40.91 71.35 63.47 
NEN 62.67 80.69 24.94 83.17 13.75 One of the important tasks in the contest was to 
identify nested named entities. For example if we 
consider eka kilo (Hindi: one kilo) as NEM 
(Named Entity Measure), it contains a NEN 
(Named Entity Number) within it. 
NEM 60.51 43.75 19.00 26.66 84.10 
NETE 19.17 31.52 NA 08.91 NA
The CRF model tags eka kilo as NEM and in or-
der to tag eka as NEN we have made use of other 
resources like a gazetteer for the list of numbers. 
We used such lists for four languages. 
6.5 Start of a sentence 
There is a possibility of confusing the NEN 
(Named Entity Number) in a sentence with the 
number that appears in a numbered list. The num-
bered list will always have numbers at the begin-
ning of a sentence and hence a feature that checks 
for this property will resolve the ambiguity with an 
actual NEN. 
7.3 Gazetteers 
For Hindi, we made use of three different kinds of 
gazetteers. These consisted of lists for measures 
(entities like kilogram, millimetre, lakh), numerals 
and quantifiers (one, first, second) and time ex-
pressions (January, minutes, hours) etc. Similar 
lists were used for all the other languages except 
Urdu. These gazetteers were effective in identify-
ing this relatively closed class of named entities 
and showed good results for these languages. 
6.6 Presence of digits 
Usually, the presence of digits indicates that the 
token is a named entity. For example, the tokens 
92, 10.1 will be identified as Named Entity Num-
ber based on the binary feature ?contains digits?. 
6.7 Presence of  four digits 8 Evaluation 
If the token is a four digit number, it is likelier to 
be a NETI (Named Entity Time). For example, 
1857, 2007 etc. are most probably years. 
The evaluation measures used for all the five lan-
guages are precision, recall and F-measure. These 
measures are calculated in three different ways: 
 
29
1. Maximal Matches: The largest possible 
named entities are matched with the refer-
ence data. 
The amount of annotated corpus available for 
Hindi was substantially more. This should have 
ideally resulted in better results for Hindi with the 
machine learning approach. But, the results were 
only marginally better than other languages. A ma-
jor reason for this was that a very high percentage 
(44%) of tags in Hindi were NETE. The tagset 
gives examples like ?Horticulture?, ?Conditional 
Random Fields? for the tag NETE. It has also been 
mentioned that even manual annotation is harder 
for NETE as it is domain specific. This affected the 
overall results for Hindi because the performance 
for NETE was low (Table 3). 
2. Nested Matches: The largest possible as 
well as nested named entities are matched. 
3. Lexical Item Matches: The lexical items 
inside largest possible named entities are 
matched. 
9 Results 
The results of evaluation as explained in the previ-
ous section are shown in the Table-1. The F-
measures for nested lexical match are also shown 
individually for each named entity tag separately in 
Table-3 
 Num of 
NE tokens
Num of 
known NE 
% of un-
known NE
Bengali 1185 277 23.37 
10 Unknown Words Hindi 1120 417 37.23 
Table 4 shows the number of unknown words pre-
sent in the test data when compared with the train-
ing data. 
Oriya 1310 563 42.97 
Telugu 1150 145 12.60 
First column shows the number of unique 
Named entity tags present in the test data for each 
language. Second column shows the number of 
unique known named entities present in the test 
data. Third column shows the percentage of unique 
unknown words present in the test data of different 
languages when compared to training data. 
Urdu 631 179 28.36 
Table 4: Unknown Word 
 
Also, the F-measures of NEN, NETI, and NEM 
could have been higher because they are relatively 
closed classes. However, certain NEN can be am-
biguous (Example: eka is a NEN for ?one? in 
Hindi, but in a different context it can be a non-
number. For instance eka-doosra is Hindi for ?each 
other?). 
11 Error Analysis 
We can observe from the results that the maximal 
F-measure for Telugu is very low when compared 
to lexical F-measure and nested F-measure. The 
reason is that the test data of Telugu contains a 
large number of long named entities (around 6 
words), which in turn contain around 4 - 5 nested 
named entities. Our system was able to tag nested 
named entities correctly unlike maximal named 
entity. 
In a language like Telugu, NENs will appear as 
inflected words. For example 2001lo, guru-
vaaramto. 
10     Conclusion and Further Work 
In this paper we have presented the results of using 
a two stage hybrid approach for the task of named 
entity recognition for South and South East Asian 
Languages. We have achieved decent Lexical F-
measures of 40.63, 50.06, 39.04, 40.94, and 43.46 
for Bengali, Hindi, Oriya, Telugu and Urdu respec-
tively without using many language specific re-
sources. 
We can also observe that the maximal F-
measure for Telugu is very low when compared to 
other languages. This is because Telugu test data 
has very few known words. 
Urdu results are comparatively low chiefly be-
cause gazetteers for numbers and measures were 
unavailable.  
We plan to extend our work by applying our 
method to other South Asian languages, and by 
using more language specific constraints and re-
sources. We also plan to incorporate semi-
supervised extraction of rules for NEs (Saha et. al, 
30
2008) and use transliteration techniques to produce 
Indian language gazetteers (Surana and Singh, 
2008). Use of character models for increasing the 
lower recalls (Shishtla et. al, 2008) is also under-
way. We also plan to enrich the Indian dependency 
tree bank (Begum et. al, 2008) by use of our NER 
system. 
 
11 Acknowledgments 
 
   We would like to thank the organizer Mr. Anil 
Kumar Singh deeply for his continuous support 
during the shared task.  
References 
B. Babych, and A. Hartley, Improving Machine transla-
tion Quality with Automatic Named Entity Recognition. 
www.mt-archive.info/EAMT-2003- Babych.pdf 
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti Misra 
Sharma, Lakshmi Bai, and Rajeev Sangal. 2008. De-
pendency annotation scheme for Indian languages. In 
Proceedings of IJCNLP-2008, Hyderabad, India. 
M. Bikel Daniel, Miller Scott, Schwartz Richard and 
Weischedel Ralph. 1997. Nymble: A High Perfor 
mance Learning Name-finder. In Proceedings of the 
Fifth Conference on Applied Natural Language 
Processing. 
S. Cucerzan, and D. Yarowsky, 1999. Language inde-
pendent named entity recognition combining mor-
phological and contextual evidence. Proceedings of 
the Joint SIGDAT Conference on EMNLP and VLC. 
N. Kumar and Pushpak Bhattacharyya. 2006. Named 
Entity Recognition in Hindi using MEMM. In Tech-
nical Report, IIT Bombay, India. 
John Lafferty, Andrew McCallum and Fernando          
Pereira. 2001. Conditional Random Fields: Probabil-
istic Models for Segmenting and Labeling Sequence 
Data. Proc.   18th International Conf. on Machine 
Learning. 
D. McDonald 1996. Internal and external evidence in 
the identification and semantic categorization of 
proper names. In B. Boguraev and J. Pustejovsky, 
editors, Corpus Processing for Lexical Acquisition. 
Avinesh PVS and Karthik G. Part-Of-Speech Tagging 
and Chunking Using Conditional Random Fields and 
Transformation Based Learning. Proceedings of the 
SPSAL workshop during IJCAI?07. 
Lance Ramshaw and Mitch Marcus. Text Chunking 
Using Transformation-Based Learning. Proceedings 
of the Third Workshop on Very Large Corpora. 
S.K. Saha , S. Chatterji , S. Dandapat , S. Sarkar  and P. 
Mitra 2008. A Hybrid Approach for Named Entity 
Recognition in Indian Languages. In Proceedings of 
IJCNLP Workshop on NER for South and South East 
Asian Languages. 
Fei Sha and Fernando Pereira. 2003. Shallow Parsing 
with Conditional Random Fields. In the Proceedings 
of HLT-NAACL. 
P. Shishtla, P. Pingali , V. Varma  2008. A Character n-
gram Based Approach for Improved Recall in Indian 
Language NER. In Proceedings of IJCNLP Work-
shop on NER for South and South East Asian Lan-
guages. 
Cucerzan Silviu and Yarowsky David. 1999. Language 
Independent Named Entity Recognition Combining 
Morphological and Contextual Evidence. In Proceed-
ings of the Joint SIGDAT Conference on EMNLP and 
VLC. 
A. K. Singh and H. Surana  Can Corpus Based Meas-
ures be Used for Comparative Study of Languages? 
In Proceedings of Ninth Meeting of the ACL Special 
Interest Group in Computational Morphology and 
Phonology. ACL. 2007. 
R. Srihari, C. Niu and W. Li  2000. A Hybrid Approach 
for Named Entity and Sub-Type Tagging. In Pro-
ceedings of the sixth conference on Applied natural 
language processing. 
H. Surana and A. K. Singh 2008. A More Discerning 
and Adaptable Multilingual Transliteration Mecha-
nism for Indian Languages. In Proceedings of the 
Third International Joint Conference on Natural 
Language Processing. 
Charles Sutton, An Introduction to Conditional Random 
Fields for Relational Learning. 
T. Wakao , R. Gaizauskas  and Y. Wilks 1996. Evalua-
tion of an algorithm for the recognition and classifi-
cation of proper names. In Proceedings of COLING. 
 
Li Wei and McCallum Andrew. 2004. Rapid Develop-
ment of Hindi Named Entity Recognition using Con-
ditional Random Fields and Feature Induction. In 
ACM Transactions on Computational Logic. 
CRF++:.Yet another Toolkit. 
http://crfpp.sourceforge.net/ 
 
 
31
 32
Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 40?47,
Prague, June 2007. c?2007 Association for Computational Linguistics
Can Corpus Based Measures be Used for Comparative Study of Languages?
Anil Kumar Singh
Language Tech. Research Centre
Int?l Inst. of Information Tech.
Hyderabad, India
anil@research.iiit.net
Harshit Surana
Language Tech. Research Centre
Int?l Inst. of Information Tech.
Hyderabad, India
surana.h@gmail.com
Abstract
Quantitative measurement of inter-language
distance is a useful technique for studying
diachronic and synchronic relations between
languages. Such measures have been used
successfully for purposes like deriving lan-
guage taxonomies and language reconstruc-
tion, but they have mostly been applied to
handcrafted word lists. Can we instead
use corpus based measures for comparative
study of languages? In this paper we try to
answer this question. We use three corpus
based measures and present the results ob-
tained from them and show how these results
relate to linguistic and historical knowledge.
We argue that the answer is yes and that such
studies can provide or validate linguistic and
computational insights.
1 Introduction
Crosslingual and multilingual processing is acquir-
ing importance in the computational linguistics
community. As a result, semi-automatic crosslin-
gual comparison of languages is also becoming
a fruitful area of study. Among the fundamen-
tal tools for crosslingual comparison are measures
of inter-language distances. In linguistics, the
study of inter-language distances, especially for lan-
guage classification, has a long history (Swadesh,
1952; Ellison and Kirby, 2006). Basically, the
work on this problem has been along linguistic,
archaeological and computational streams. Like
in other disciplines, computational methods are in-
creasingly being combined with other more conven-
tional approaches (Dyen et al, 1992; Nerbonne and
Heeringa, 1997; Kondrak, 2002; Ellison and Kirby,
2006). The work being presented in this paper be-
longs to the computational stream.
Even in the computational stream, most of the
previous work on inter-language distances had a
strong linguistic dimension. For example, most
of the quantitative measures of inter-language dis-
tance have been applied on handcrafted word
lists (Swadesh, 1952; Dyen et al, 1992). However,
with increasing use of computational techniques and
the availability of electronic data, a natural ques-
tion arises: Can languages be linguistically com-
pared based on word lists extracted from corpora.
A natural counter-question is whether such compar-
ison will be valid from linguistic and psycholinguis-
tic points of view. The aim of this paper is to exam-
ine such questions.
To calculate inter-language distances on the basis
of words in corpora, we propose two corpus based
distance measures. They internally use a more lin-
guistically grounded distance measure for compar-
ing strings. We also present the results obtained with
one purely statistical measure, just to show that even
naive corpus based measures can be useful. The
main contribution is to show that even noisy corpora
can be used for comparative study of languages. Dif-
ferent measures can give different kinds of insights.
2 Related Work
Typology or history of languages can be studied us-
ing spoken data or text. There has been work on
the former (Remmel, 1980; Kondrak, 2002), but we
40
will focus only on text. An example of a major work
on text based similarity is the paper by Kondrak and
Sherif (Kondrak and Sherif, 2006). They have evalu-
ated various phonetic similarity algorithms for align-
ing cognates. They found that learning based al-
gorithms outperform manually constructed schemes,
but only when large training data is used.
A recent work on applications of such techniques
for linguistic study is by Heeringa et al (Heeringa
et al, 2006). They performed a study on differ-
ent variations of string distance algorithms for di-
alectology and concluded that order sensitivity is
important while scaling with length is not. It may
be noted that Ellison and Kirby (Ellison and Kirby,
2006) have shown that scaling by distance does give
significantly better results. Nakleh et al (Nakleh
et al, 2005) have written about using phyloge-
netic techniques in historical linguistics as men-
tioned by Nerbonne (Nerbonne, 2005) in the review
of the book titled ?Language Classification by Num-
bers? by McMahon and McMahon (McMahon and
McMahon, 2005). All these works are about using
quantitative techniques for language typology and
classification etc.
3 Inter-Language Comparison
Inter-language comparison is more general than
measuring inter-language distance. In addition to
the overall linguistic distance, the comparison can
be of more specific characteristics like the propor-
tion of cognates derived vertically and horizontally.
Or it can be of specific phonetic features (Nerbonne,
2005; McMahon and McMahon, 2005). Quantita-
tive measures for comparing languages can first be
classified according to the form of data being com-
pared, i.e., speech, written text or electronic text.
Assuming that the text is in electronic form, the most
common measures are based on word lists. These
lists are usually prepared by linguists and they are
often in some special notation, e.g. more or less a
phonetic transcription.
The measures can be based on inter-lingual or on
intra-lingual comparison of phonetic forms (Ellison
and Kirby, 2006). They may or may not use statis-
tical techniques like measures of distributional sim-
ilarity (cross entropy, KL-divergence, etc.). These
characteristics of measures may imply some linguis-
tic or psycholinguistic assumptions. One of these is
about a common phonetic space.
4 Common Phonetic Space
Language distance can be calculated through
crosslingual as well as intra-lingual comparison.
Many earlier attempts (Nerbonne and Heeringa,
1997; Kondrak, 2002) were based on crosslingual
comparison of phonetic forms, but some researchers
have argued against the possibility of obtaining
meaningful results from crosslingual comparison of
phonetic forms. This is related to the idea of a
common phonetic space. Port and Leary (Port and
Leary, 2005) have argued against it. Ellison and
Kirby (Ellison and Kirby, 2006) argue that even if
there is a common space, language specific catego-
rization of sound often restructures this space. They
conclude that if there is no language-independent
common phonetic space with an equally common
similarity measure, there can be no principled ap-
proach to comparing forms in one language with
another. They suggest that language-internal com-
parison of forms is better and psychologically more
well-grounded.
This may be true, but should we really abandon
the approach based on crosslingual comparison? As
even Ellison and Kirby say, it is possible to argue
that there is a common phonetic space. After all,
the sounds produced by humans are determined by
human physiology. The only matter of debate is
whether common phonetic space makes sense from
the cognitive point of view. We argue that it does.
In psychology, there has been a long debate about
a similar problem which can be stated in terms of a
common chromatic space. Do humans in different
cultures see the same colors? There is still no con-
clusive answer, but many computational techniques
have been tried to solve real world problems like
classifying human faces, seemingly with the implicit
assumption that there is a common chromatic space.
Such techniques have shown some success (sheng
Chen and kai Liu, 2003).
Could it be that we are defining the notion of a
common chromatic (or phonetic) space too strictly?
Or that the way we define it is not relevant for com-
putational techniques? In our view the answer is
yes. We will give a simple, not very novel, exam-
41
ple. The phoneme t as in the English word battery is
not present in many languages of the world. When
a Thai speaker can not say battery, with the correct
t, he will say battery with t as in the French word
entre. Such substitution will be very regular. The
point is that even if phonetic space is restructured
for a particular language, we can still find which
segments or sections of two differently structured
phonetic spaces are close. Cyan may span different
ranges (on the spectrum) in different cultures, but the
ranges are likely to be near to one another. Even if
some culture has no color which can be called cyan,
one or two of the colors that it does have will be
closer to cyan than the others. The same is true
for all the other colors and also for sounds. If we
use fuzzy similarity measures to take care of such
differently structured cognitive spaces, cross-lingual
comparison may still be meaningful for certain pur-
poses. This argument is in defence of cross-lingual
comparison, not against intra-lingual comparison.
5 Common Orthographic Space
Writing systems used by languages differ very
widely. This can be taken to mean that there
is no common orthographic space for meaning-
ful crosslingual comparison of orthographic forms.
This may be true in general, but for sets of languages
using related scripts, we can assume a similar ortho-
graphic space. For example, most of the major South
Asian languages use scripts derived from Brahmi.
The similarity among these scripts is so much that
crosslingual comparison of text is possible for var-
ious purposes such as identifying cognates without
any phonetic transcription. This is in spite of the fact
that the letter shapes differ so much that they are not
mutually identifiable. Such similarity is relevant for
corpus based measures.
6 Corpus Based Measures
Since we use (non-parallel) corpora of the two lan-
guages for finding out the cognates and hence com-
paring two languages, the validity of the results de-
pends on how representative the corpora are. How-
ever, if they are of enough size, we might still be
able to make meaningful, even if limited, compar-
ison among languages. We restrict ourselves to
word list based comparison. In such a case, cor-
pus based measures can be effective if the corpora
contain a representative portion of the vocabulary,
or even of word segments. The second case (of seg-
ments) is relevant for the n-gram measure described
in section-7.
This category of measures have to incorporate
more linguistic information if they are to provide
good results. Designing such measures can be a
challenging problem as we will be mainly relying
on the corpus for our information. Knowledge about
similarities and differences of writing systems can
play an important role here. The two cognate based
measures described in sections 9 and 10 are an at-
tempt at this. But first we describe a simple n-gram
based measure.
7 Symmetric Cross Entropy (SCE)
The first measure is purely a letter n-gram based
measure similar to the one used by Singh (Singh,
2006b) for language and encoding identification. To
calculate the distance, we first prepare letter 5-gram
models from the corpora of the languages to be com-
pared. Then we combine n-grams of all orders and
rank them according to their probability in descend-
ing order. Only the top N n-grams are retained and
the rest are pruned. 1 Now we have two probability
distributions which can be compared by a measure
of distributional similarity. We have used symmetric
cross entropy as such a measure:
dsce =
?
gl=gm
(p(gl) log q(gm) + q(gm) log p(gl))
(1)
where p and q are the probability distributions for
the two languages and gl and gm are n-grams in lan-
guages l and m, respectively.
The disadvantage of this measure is that it does
not use any linguistic (e.g., phonetic) information,
but the advantage is that it can measure the similar-
ity of distributions of n-grams. Such measures have
proved to be very effective in automatically iden-
tifying languages of text, with accuracies nearing
100% for fairly small amounts of training and test
data (Adams and Resnik, 1997; Singh, 2006b).
1This is based on the results obtained by Cavnar (Cavnar and
Trenkle, 1994) and our own studies, which show that the top N
(300 according to Cavnar) n-grams have a high correlation with
the identity of the language.
42
8 Method for Cognate Identification
The other two measures are based on cognates, in-
herited as well as borrowed. Both of them use an
algorithm for identification of cognates. Many such
algorithms have been proposed. Estimates of sur-
face similarity can be used for finding cognate words
across languages for related languages. By surface
similarity we mean the orthographic, phonetic and
(possibly) morphological similarity of two words or
strings. In spite of the name, surface similarity is
deeper than string similarity as calculated by edit
distances. Ribeiro et al (Ribeiro et al, 2001) have
surveyed some of the algorithms for cognate align-
ment. However, since they studied methods based
on parallel text, we cannot use them directly.
For identifying cognates, we are using the compu-
tational model of scripts or CPMS (Singh, 2006a).
This model takes into account the characteristics of
Brahmi origin scripts and calculates surface simi-
larity in a fuzzy way. This is achieved by using
a stepped distance function (SDF) and a dynamic
programming (DP) algorithm. We have adapted the
CPMS for identifying cognates.
Different researchers have argued about the im-
portance of order sensitivity and scaling in using
string comparison algorithms (Heeringa et al, 2006;
Ellison and Kirby, 2006). The CPMS takes both
of these into account, as well as using knowledge
about the script. In general, the distance between
two strings can be defined as:
clm = fp(wl, wm) (2)
where fp is the function which calculates surface
similarity based cost between the word wl of lan-
guage l and the word wm of language m.
Those word pairs are identified as cognates which
have the least cost.
9 Cognate Coverage Distance (CCD)
The second measure used by us is a corpus based
estimate of the coverage of cognates across two lan-
guages. Cognate coverage is defined as the num-
ber of words (out of the vocabularies of the two lan-
guages) which are of the same origin. The decision
about whether two words are cognates or not is made
on the basis of surface similarity of the two words
as described in the previous section. We use (non-
parallel) corpora of the two languages for identify-
ing the cognates.
The normalized distance between two languages
is defined as:
t?lm = 1?
tlm
max(t) (3)
where tlm and tml are the number of cognates found
when comparing from language l to m and from lan-
guage m to l, respectively.
Since the CPMS based measure of surface lexical
similarity is asymmetric, we calculate the average
number of unidirectional cognates:
dccd = t
?
lm + t?ml
2 (4)
10 Phonetic Distance of Cognates (PDC)
Simply finding the coverage of cognates may in-
dicate the distance between two languages, but a
measure based solely on this information does not
take into account the variation between the cognates
themselves. To include this variation into the esti-
mate of distance, we use another measure based on
the sum of the CPMS based cost of n cognates found
between two languages:
Cpdclm =
n
?
i = 0
clm (5)
where n is the minimum of tlm for all the language
pairs compared.
The normalized distance can be defined as:
C ?lm =
Cpdclm
max(Cpdc) (6)
A symmetric version of this cost is then calcu-
lated:
dpdc =
C ?lm + C ?ml
2 (7)
11 Experimental Setup
For synchronic comparison, we selected ten lan-
guages for our experiment (table-1), mainly be-
cause sufficient corpora were available for these lan-
guages. These languages, though belonging to two
different families (Indo-Iranian and Dravidian), have
43
HI
AS
BN
OR
KN
MR
ML
TE
TA
PA
HI
AS
BN
OR
KN
MR
ML
TE
TA
PA
HI
AS
BN
OR
KN
MR
ML
TE
TA
PA
0.20
0.52 0.32
0.02
0.07 0.20
0.42
0.61
0.61
0.53
0.62
0.850.72
0.16
0.37
0.12
0.05  0.11
 0.16
0.17
 0.25
0.56
0.81
 0.31
0.17
0.45
CCD PDC
Combined
Figure 1: Graphical view of synchronic comparison among ten major South Asian languages using CCD
and PDC measures. The layout of the graph is modeled on the geographical locations of these languages.
The connections among the nodes of the graph are obtained by joining each node to its two closest neighbors
in terms of the values obtained by using the two measures.
a lot of similarities (Emeneau, 1956). The cognate
words among them are loanwords as well as inher-
ited words. In fact, the similarity among these lan-
guages is due to common origin (intra-family) as
well as contact and borrowing over thousands of
years (intra- and inter-family). Moreover, they also
use scripts derived from the same origin (Brahmi),
which allows us to use the CPMS for identifying
cognates. The corpora used for these ten languages
are all part of the CIIL (Central Institute of Indian
Languages) multilingual corpus. This corpus is a
collection of documents from different domains and
is one of best known corpora for Indian languages.
Still, the representativeness of this corpus may be a
matter of debate as it is not as large and diverse as
the BNC (British National Corpus) corpus for En-
glish.
For the cognate measures (CCD and PDC), the
only information we are extracting from the cor-
pora are the word types and their frequencies.
Thus, in a way, we are also working with word
lists, but our word lists are extracted from cor-
pora. Word lists handcrafted by linguists may be
very useful, but they are not always available for
all kinds of inter-language or inter-dialectal compar-
ison, whereas electronic corpora are more likely to
be available. Currently we are not doing any prepro-
cessing or stemming on the word lists before running
the cognate extraction algorithm. For SCE, n-gram
models are being prepared as described in section-
7. For all three measures, we calculate the distances
among all possible pairs of the languages.
For diachronic comparison, we selected modern
standard Hindi, medieval Hindi (actually, Avadhi)
and Sanskrit. The corpus for modern Hindi was the
same as that used for synchronic comparison. The
medieval Hindi we have experimented with is of two
different periods. These are the varieties used by
two great poets of that period, namely Jaayasi (1477-
1542 A.D.) and Tulsidas (1532-1623 A.D.). We took
some of their major works available in electronic
form as the corpora. For Sanskrit, we used the elec-
tronic version of Mahabharata (compiled during the
period 1000 B.C. to 500 A.D. approximately) as the
corpus. We calculate the distances among all pos-
sible pairs of the four varieties using the three mea-
sures. We also compare the ten modern languages
with Sanskrit using the same Mahabharata corpus.
For synchronic comparison, we first extract the
list of word types with frequencies from the corpus.
Then we rank them according to frequency. Top N
of these are retained. This is done because other-
wise a lot of less relevant word types like proper
nouns get included. We are interested in compar-
ing the core vocabulary of languages. The assump-
tion is that words in the core vocabulary are likely
to be more frequent. Another reason for restricting
the experiments to the top N word types is that there
44
BN HI KN ML MR OR PA TA TE
AS 0.02 0.39 0.71 0.86 0.61 0.20 0.61 0.93 0.73
0.12 0.25 0.39 0.61 0.45 0.11 0.58 0.95 0.46
0.05 0.30 0.51 0.50 0.43 0.18 0.42 0.70 0.64
BN 0.32 0.68 0.86 0.57 0.07 0.56 0.96 0.70
0.29 0.42 0.64 0.42 0.05 0.56 0.90 0.50
0.29 0.47 0.45 0.43 0.14 0.42 0.74 0.43
HI 0.61 0.81 0.42 0.40 0.20 0.93 0.61
0.17 0.56 0.16 0.27 0.16 0.87 0.38
0.43 0.46 0.16 0.33 0.20 0.74 0.34
KN 0.77 0.68 0.75 0.73 0.88 0.53
0.45 0.17 0.31 0.50 0.82 0.25
0.18 0.38 0.52 0.58 0.42 0.09
ML 0.89 0.88 0.88 0.62 0.72
0.65 0.59 0.77 0.56 0.31
0.42 0.53 0.55 0.07 0.19
MR 0.64 0.52 0.95 0.68
0.40 0.37 0.94 0.46
0.34 0.39 0.60 0.30
OR 0.63 0.98 0.74
0.45 0.89 0.44
0.65 0.83 0.64
PA 0.90 0.71
0.90 0.59
0.92 0.48
TA 0.85
0.81
0.39
Table 1: Inter-language comparison among ten ma-
jor South Asian languages using three corpus based
measures. The values have been normalized and
scaled to be somewhat comparable. Each cell con-
tains three values: by CCD, PDC and SCE.
are huge differences in sizes of corpora of different
languages. In the next step we identify the cognates
among these word lists. No language specific fea-
tures or thresholds are used. Only common thresh-
olds are used. We now branch out to using either
CCD or PDC.
The method used for diachronic comparison is
similar except that N is much smaller because the
amount of classical corpus being used (Jaayasi, Tul-
sidas) is also much smaller. Two letter codes are
used for ten languages and four varieties2.
12 Analysis of Results
The results of our experiments are shown tables 1
to 3 and figures 1 and 2. Table-1 shows the dis-
tances among pairs of languages using the three
2AS: Assamese, BN: Bengali, HI: Hindi, KN: Kannada,
ML: Malayalam, MR: Marathi, OR: Oriya, PA: Punjabi,
TA: Tamil, TE: Telugu, TL: Avadhi (Tulsidas), JY: Avadhi
(Jaayasi), MB: Sanskrit (Mahabharata)
measures. Figure-1 shows a graph showing the dis-
tances according to CCD and PDC. Figure-2 shows
the effect of the size of word lists (N ) on com-
parison for three linguistically close language pairs.
Table-2 shows the comparison of ten languages with
Sanskrit. Table-3 gives the diachronic comparison
among four historical varieties.
12.1 Synchronic Comparison
As table-1 shows, all three measures give results
which correspond well to the linguistic knowledge
about differences among these languages. Cognate
based measures give better results, but even the n-
gram based measure gives good results. However,
there are some differences among the values ob-
tained with different measures. These differences
are also in accordance with linguistic insights. For
example, the distance between Hindi and Telugu
was given as 0.61 by CCD and 0.38 by PDC. Simi-
larly, the distance between Hindi and Kannada was
given as 0.61 by CCD and 0.17 by PDC. These val-
ues, in relative terms, indicate that the number of
cognates between these languages is in the medium
range as compared to other pairs. But less PDC cost
shows that top N cognates are very similar. This
is because most cognates are tatsam words directly
borrowed from Sanskrit without any change.
The results presented in the table have been nor-
malized on all language pairs using the maximum
and minimum cost. The results would be differ-
ent and more comparable if we normalize over lan-
guage families (Indo-Iranian and Dravidian). With
such normalization, Punjabi-Oriya and Marathi-
Assamese are identified as the farthest language
pairs with costs of 0.92 and 0.90, respectively. This
corresponds well with the actual geographical and
linguistic distances.
While comparing with Sanskrit, it is clear that
different languages have different levels of cognate
coverage. However, except for Punjabi and Tamil,
all languages have very similar PDC cost with the
Mahabharata corpus. This again shows that the
closest cognates among these languages are tatsam
words. These results agree well with linguistic
knowledge, even though the Sanskrit corpus (Ma-
habharata) is highly biased.
Figure-1 makes the results clearer. It shows that
just by connecting each node to its nearest two
45
Distance AS BN HI KN ML MR OR PA TA TE
CCD 0.71 0.70 0.65 0.78 0.87 0.73 0.71 0.78 0.94 0.77
PDC 0.37 0.38 0.40 0.43 0.37 0.41 0.37 0.50 0.63 0.30
Table 2: Comparison with Sanskrit (Mahabharata)
Figure 2: Effect of the size of word lists on inter-
language comparison.
TL JY MB
HI 0.45 0.54 0.82
0.45 0.42 0.70
0.64 0.56 0.49
TL 0.01 0.84
0.02 0.72
0.16 0.91
JY 0.98
0.95
0.81
Table 3: Diachronic comparison among four histor-
ical varieties.
neighbors we can get a very good graphical repre-
sentation of the differences among languages. It also
shows that different measures capture different as-
pects. For example, CCD fails to connect Marathi
with Kannada and Kannada with Malayalam. Sim-
ilarly, PDC fails to connect Bengali with Hindi.
We get this missing information by combining the
graphs obtained with the two measures. More so-
phisticated methods for creating such graphs may
give better results. Note that the Hindi-Telugu and
Marathi-Kannada connections are valid as these lan-
guage pairs are close, even though they are not ge-
netically related. The results indicate closeness be-
tween two languages, but they do not distinguish be-
tween inheritance and borrowing.
We also experimented with several word list sizes.
In figure-2 the CCD values are plotted against word
list sizes for three close language pairs. There is
variation for Hindi-Punjabi and Malayalam-Telugu,
but not for Assamese-Bengali. The following obser-
vations can be derived from the three lines on the
plot. Malayalam-Telugu share a lot of common core
words but not less common words. Hindi-Punjabi
share a lot of less common words, but core words
are not exactly similar. Finally, Assamese-Bengali
share both core as well as less common words.
12.2 Diachronic Comparison
Table-4 shows the results. We can see that Hindi is
closer to Tulsidas than to Jaayasi by the CCD mea-
sure. PDC gives almost similar results for both. Tul-
sidas and Jaayasi are the nearest. Tulsidas is much
nearer to Mahabharata than Jaayasi, chiefly because
Tulsidas? language has more Sanskrit origin words.
Our results put Tulsidas nearest to Hindi, followed
by Jaayasi and then Sanskrit. This is historically as
well as linguistically correct.
13 Conclusions and Further Work
In this paper we first discussed the possibility and
validity of using corpus based measures for compar-
ative study of languages. We presented some ar-
guments in favor of this possibility. We then de-
scribed three corpus based measures for comparative
study of languages. The first measure was symmet-
ric cross entropy of letter n-grams. This measure
uses the least amount of linguistic information. The
second and third measures were cognate coverage
distance and phonetic distance of cognates, respec-
tively. These two are more linguistically grounded.
Using these measures, we presented a synchronic
comparison of ten major South Asian languages and
a diachronic comparison of four historical varieties.
The results of our experiments show that even these
simple measures based on crosslingual comparison
46
and on the data extracted from not very representa-
tive and noisy corpora can be used for obtaining or
validating useful linguistic insights about language
divergence, classification etc.
These measures can be tried for more languages
to see whether they have any validity for less related
languages than the languages we experimented with.
We can also try to design measures and find meth-
ods for distinguishing between borrowed and inher-
ited words. Proper combination of synchronic and
diachronic comparison might help us in doing this.
Other possible applications could be for language re-
construction, classification, dialectology etc.
Better versions of the two cognate based measures
can be defined by using the idea of confusion prob-
abilities (Ellison and Kirby, 2006) and the idea of
distributional similarity. If intra-lingual comparison
is more meaningful than inter-lingual comparison,
then these modified versions should be even more
useful for comparative study of languages.
References
Gary Adams and Philip Resnik. 1997. A language
identification application built on the Java client-server
platform. In Jill Burstein and Claudia Leacock, ed-
itors, From Research to Commercial Applications:
Making NLP Work in Practice, pages 43?47. Associa-
tion for Computational Linguistics.
William B. Cavnar and John M. Trenkle. 1994. N-gram-
based text categorization. In Proceedings of SDAIR-
94, 3rd Annual Symposium on Document Analysis and
Information Retrieval, pages 161?175, Las Vegas, US.
I. Dyen, J.B. Kruskal, and P. Black. 1992. An
indo-european classification: A lexicostatistical exper-
iment. In Transactions of the American Philosophical
Society, 82:1-132.
T. Mark Ellison and Simon Kirby. 2006. Measuring lan-
guage divergence by intra-lexical comparison. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, Sydney,
Australia. Association for Computational Linguistics.
M. B. Emeneau. 1956. India as a linguistic area. In
Linguistics 32:3-16.
W. Heeringa, P. Kleiweg, C. Gooskens, and J. Nerbonne.
2006. Evaluation of String Distance Algorithms for
Dialectology. In Proc. of ACL Workshop on Linguistic
Distances.
G. Kondrak and T. Sherif. 2006. Evaluation of Several
Phonetic Similarity Algorithms on the Task of Cognate
Identification. In Proc. of ACL Workshop on Linguistic
Distances.
Grzegorz Kondrak. 2002. Algorithms for language re-
construction. Ph.D. thesis. Adviser-Graeme Hirst.
April McMahon and Robert McMahon. 2005. Lan-
guage Classification by the Numbers. Oxford Univer-
sity Press, Oxford.
Luay Nakleh, Don Ringe, and Tandy Warnow. 2005.
Perfect phylogentic networks: A new methodology for
reconstructing the evolutionary history of natural lan-
guages. pages 81?2:382?420.
J. Nerbonne and W. Heeringa. 1997. Measuring dialect
distance phonetically. In Proceedings of SIGPHON-
97: 3rd Meeting of the ACL Special Interest Group in
Computational Phonology.
J. Nerbonne. 2005. Review of ?language classification
by the numbers? by april mcmahon and robert mcma-
hon.
B. Port and A. Leary. 2005. Against formal phonology.
pages 81(4):927?964.
M. Remmel. 1980. Computers in the historical phonetics
and phonology of Balto-Finnic languages: problems
and perspectives. In Communication pre?sente?e au 5th
International Finno-Ugric Congress, Turku.
A. Ribeiro, G. Dias, G. Lopes, and J. Mexia. 2001. Cog-
nates alignment. Machine Translation Summit VIII,
Machine Translation in The Information Age, pages
287?292.
Duan sheng Chen and Zheng kai Liu. 2003. A novel
approach to detect and correct highlighted face re-
gion in color image. In AVSS ?03: Proceedings of
the IEEE Conference on Advanced Video and Signal
Based Surveillance, page 7, Washington, DC, USA.
IEEE Computer Society.
Anil Kumar Singh. 2006a. A computational phonetic
model for indian language scripts. In Constraints on
Spelling Changes: Fifth International Workshop on
Writing Systems, Nijmegen, The Netherlands.
Anil Kumar Singh. 2006b. Study of some distance mea-
sures for language and encoding identification. In Pro-
ceedings of ACL 2006 Workshop on Linguistic Dis-
tance, Sydney, Australia.
M. Swadesh. 1952. Lexico-dating of prehistoric ethnic
contacts. In Proceedings of the American philosophi-
cal society, 96(4).
47
