Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 980?989,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Data Driven Grammatical Error Detection
in Transcripts of Children?s Speech
Eric Morley
CSLU
OHSU
Portland, OR 97239
morleye@gmail.com
Anna Eva Hallin
Department of Communicative
Sciences and Disorders
New York University
New York, NY
ae.hallin@nyu.edu
Brian Roark
Google Research
New York, NY 10011
roarkbr@gmail.com
Abstract
We investigate grammatical error detec-
tion in spoken language, and present a
data-driven method to train a dependency
parser to automatically identify and label
grammatical errors. This method is ag-
nostic to the label set used, and the only
manual annotations needed for training are
grammatical error labels. We find that the
proposed system is robust to disfluencies,
so that a separate stage to elide disfluen-
cies is not required. The proposed system
outperforms two baseline systems on two
different corpora that use different sets of
error tags. It is able to identify utterances
with grammatical errors with an F1-score
as high as 0.623, as compared to a baseline
F1 of 0.350 on the same data.
1 Introduction
Research into automatic grammatical error detec-
tion has primarily been motivated by the task of
providing feedback to writers, whether they be na-
tive speakers of a language or second language
learners. Grammatical error detection, however, is
also useful in the clinical domain, for example, to
assess a child?s ability to produce grammatical lan-
guage. At present, clinicians and researchers into
child language must manually identify and clas-
sify particular kinds of grammatical errors in tran-
scripts of children?s speech if they wish to assess
particular aspects of the child?s linguistic ability
from a sample of spoken language. Such manual
annotation, which is called language sample anal-
ysis in the clinical field, is expensive, hindering
its widespread adoption. Manual annotations may
also be inconsistent, particularly between different
research groups, which may be investigating dif-
ferent phenomena. Automated grammatical error
detection has the potential to address both of these
issues, being both cheap and consistent.
Aside from performance, there are at least two
key requirements for a grammatical error detector
to be useful in a clinical setting: 1) it must be able
to handle spoken language, and 2) it must be train-
able. Clinical data typically consists of transcripts
of spoken language, rather than formal written lan-
guage. As a result, a system must be prepared
to handle disfluencies, utterance fragments, and
other phenomena that are entirely grammatical in
speech, but not in writing. On the other hand, a
system designed for transcripts of speech does not
need to identify errors specific to written language
such as punctuation or spelling mistakes. Further-
more, a system designed for clinical data must be
able to handle language produced by children who
may have atypical language due to a developmen-
tal disorder, and therefore may produce grammati-
cal errors that would be unexpected in written lan-
guage. A grammatical error detector appropriate
for a clinical setting must also be trainable be-
cause different groups of clinicians may wish to
investigate different phenomena, and will there-
fore prefer different annotation standards. This
is quite different from grammatical error detectors
for written language, which may have models for
different domains, but which are not typically de-
signed to enable the detection of novel error sets.
We examine two baseline techniques for gram-
matical error detection, then present a simple data-
driven technique to turn a dependency parser into a
grammatical error detector. Interestingly, we find
that the dependency parser-based approach mas-
sively outperforms the baseline systems in terms
of identifying ungrammatical utterances. Further-
more, the proposed system is able to identify spe-
cific error codes, which the baseline systems can-
not do. We find that disfluencies do not degrade
performance of the proposed detector, obviating
the need (for this task) for explicit disfluency de-
tection. We also analyze the output of our system
to see which errors it finds, and which it misses.
980
Code Description Example
[EO] Overgeneralization errors He falled [EO] .
[EW] Other word level errors He were [EW] looking .
[EU] Utterance level errors And they came to stopped .
[OM] Omitted bound morpheme He go [OM] .
[OW] Omitted word She [OW] running .
Table 1: Error codes proposed in the SALT manual. Note that in SALT annotated transcripts, [OM] and
[OW] are actually indicated by ?*? followed by the morpheme or word hypothesized to be omitted.
When treating codes (other than [EU]) as tags, they are attached to the previous word in the string.
Finally, we evaluate our detector on a second set
of data with a different label set and annotation
standards. Although our proposed system does not
perform as well on the second data set, it still out-
performs both baseline systems. One interesting
difference between the two data sets, which does
appear to impact performance, is that the latter set
more strictly follows SALT guidelines (see Sec-
tion 2.1) to collapse multiple errors into a single
label. This yields transcripts with a granularity of
labeling somewhat less amenable to automation,
to the extent that labels are fewer and can be re-
liant on non-local context for aggregation.
2 Background
2.1 Systematic Analysis of Language
Transcripts (SALT)
The Systematic Analysis of Language Transcripts
(SALT) is the de facto standard for clinicians look-
ing to analyze samples of natural language. The
SALT manual includes guidelines for transcrip-
tion, as well as three types of annotations, of
which two are relevant here: maze annotations,
and error codes.
1
Mazes are similar to what is referred to as ?dis-
fluencies? in the speech literature. The SALT
manual defines mazes as ?filled pauses, false
starts, repetitions, reformulations, and interjec-
tions? (Miller et al., 2011, p. 6), without defining
any of these terms. Partial words, which are in-
cluded and marked in SALT-annotated transcripts,
are also included in mazes. Mazes are delimited
by parentheses, and have no internal structure, un-
like disfluencies annotated following the Switch-
board guidelines (Meteer et al., 1995), which are
commonly followed by the speech and language
1
SALT also prescribes annotation of bound morphemes
and clitics, for example -ed in past tense verbs. We preprocess
all of the transcripts to remove bound morpheme and clitic
annotations.
processing communities. An example maze anno-
tation would be: ?He (can not) can not get up.?
The SALT manual proposes the set of error
codes shown (with examples) in Table 1, but re-
search groups may use a subset of these codes, or
augment them with additional codes. For example,
the SALT-annotated Edmonton Narrative Norms
Instrument (ENNI) corpus (Schneider et al., 2006)
rarely annotates omitted morphemes ([OM]), in-
stead using the [EW] code. Other SALT-annotated
corpora include errors that are not described in the
SALT manual. For example the CSLU ADOS cor-
pus (Van Santen et al., 2010) includes the [EX]
tag for extraneous words, and the Narrative Story
Retell corpus (SALT Software, 2014b) uses the
code [EP] to indicate pronominal errors (albeit
inconsistently, as many such errors are coded as
[EW] in this corpus). We note that the definitions
of certain SALT errors, notably [EW] and [EU],
are open to interpretation, and that these codes
capture a wide variety of errors. For example,
some of the errors captured by the [EW] code are:
pronominal case and gender errors; verb tense er-
rors; confusing ?a? and ?an?; and using the wrong
preposition.
The SALT guidelines specify as a general rule
that annotators should not mark utterances with
more than two omissions ([OM] or [OW]) and/or
word-level errors (ex [EW], [EP]) (SALT Soft-
ware, 2014a). Instead, annotators are instructed
to code such utterances with an utterance-level er-
ror ([EU]). How strictly annotators adhere to this
rule affects the distribution of errors, reducing the
number of word-level errors and increasing the
number of utterance-level errors. Following this
rule also increases the variety of errors captured
by the [EU] code. The annotations in different
corpora, including ENNI and NSR, vary in how
strictly they follow this rule, even though this is
not mentioned in the the published descriptions of
981
these corpora.
2.2 Grammatical Error Detection
The most visible fruits of research into grammati-
cal error detection are the spellchecking and gram-
mar checking tools commonly included with word
processors, for example Microsoft Word?s gram-
mar checker. Although developed for handling
written language, many of the techniques used
to address these tasks could still be applicable to
transcripts of speech because many of the same
errors can still occur. The earliest grammatical-
ity tools simply performed pattern matching (Mac-
donald et al., 1982), but this approach is not robust
enough to identify many types of errors, and pat-
tern matching systems are not trainable, and there-
fore cannot be adapted quickly to new label sets.
Subsequent efforts to create grammaticality classi-
fiers and detectors leveraged information extracted
from parsers (Heidorn et al., 1982) and language
models (Atwell, 1987). These systems, however,
were developed for formal written English pro-
duced by well-educated adults, as opposed to spo-
ken English produced by young children, partic-
ularly children with suspected developmental de-
lays.
There have been a few investigations into tech-
niques to automatically identify particular con-
structions in transcripts of spoken English. Bow-
den and Fox (2002) proposed a rule-based sys-
tem to classify many types of errors made by
learners of English. Although their system could
be used on either transcripts of speech, or on
written English, they did not evaluate their sys-
tem in any way. Caines and Buttery (2010) use
a logistic regression model to identify the zero-
auxiliary construction (e.g., ?you going home??)
with over 96% accuracy. Even though the zero-
auxilliary construction is not necessarily ungram-
matical, identifying such constructions may be
useful as a preprocessing step to a grammatical-
ity classifier. Caines and Buttery also demonstrate
that their detector can be integrated into a sta-
tistical parser yielding improved performance, al-
though they are vague about the nature of the parse
improvement (see Caines and Buttery, 2010, p. 6).
Hassanali and Liu (2011) conducted the first in-
vestigation into grammaticality detection and clas-
sification in both speech of children, and speech of
children with language impairments. They identi-
fied 11 types of errors, and compared three types
of systems designed to identify the presence of
each type of error: 1) rule based systems; 2) deci-
sion trees that use rules as features; and 3) naive
Bayes classifiers that use a variety of features.
They were able to identify all error types well
(F1 > 0.9 in all cases), and found that in general
the statistical systems outperformed the rule based
systems. Hassanali and Liu?s system was designed
for transcripts of spoken language collected from
children with impaired language, and is able to
detect the set of errors they defined very well.
However, it cannot be straightforwardly adapted
to novel error sets.
Morley et al. (2013) evaluated how well the
detectors proposed by Hassanali and Liu could
identify utterances with SALT error codes. They
found that a simplified version of one of Has-
sanali and Liu?s detectors was the most effective at
identifying utterances with any SALT error codes,
although performance was very low (F1=0.18).
Their system uses features extracted solely from
part of speech tags with the Bernoulli Naive Bayes
classifier in Scikit (Pedregosa et al., 2012). Their
detector may be adaptable to other annotation
standards, but it does not identify which errors are
in each utterance; it only identifies which utter-
ances have errors, and which do not.
2.3 Redshift Parser
We perform our experiments with the redshift
parser
2
, which is an arc-eager transition-based de-
pendency parser. We selected redshift because of
its ability to perform disfluency detection and de-
pendency parsing jointly. Honnibal and Johnson
(2014) demonstrate that this system achieves state-
of-the-art performance on disfluency detection,
even compared to single purpose systems such as
the one proposed by Qian and Liu (2013). Ra-
sooli and Tetreault (2014) have developed a sys-
tem that performs disfluency detection and depen-
dency parsing jointly, and with comparable perfor-
mance to redshift, but it is not publicly available as
of yet.
Redshift uses an averaged perceptron learner,
and implements several feature sets. The first fea-
ture set, which we will refer to as ZHANG is the
one proposed by Zhang and Nivre (2011). It in-
cludes 73 templates that capture various aspects
of: the word at the top of the stack, along with its
2
Redshift is available at https://github.com/
syllog1sm/redshift. We use the version in the
experiment branch from May 15, 2014.
982
leftmost and rightmost children, parent and grand-
parent; and the word on the buffer, along with
its leftmost children; and the second and third
words on the buffer. Redshift also includes fea-
tures extracted from the Brown clustering algo-
rithm (Brown et al., 1992). Finally, redshift in-
cludes features that are designed to help iden-
tify disfluencies; these capture rough copies, ex-
act copies, and whether neighboring words were
marked as disfluent. We will refer to the feature
set containing all of the features implemented in
redshift as FULL. We refer the reader to Honnibal
and Johnson (2014) for more details.
3 Data, Preprocessing, and Evaluation
Our investigation into using a dependency parser
to identify and label grammatical errors requires
training data with two types of annotations: de-
pendency labels, and grammatical error labels. We
are not aware of any corpora of speech with both
of these annotations. Therefore, we use two dif-
ferent sets of training data: the Switchboard cor-
pus, which contains syntactic parses; and SALT
annotated corpora, which have grammatical error
annotations.
3.1 Switchboard
The Switchboard treebank (Godfrey et al., 1992)
is a corpus of transcribed conversations that have
been manually parsed. These parses include
EDITED nodes, which span disfluencies. We pre-
process the Switchboard treebank by removing all
partial words as well as all words dominated by
EDITED nodes, and converting all words to lower-
case. We then convert the phrase-structure trees to
dependencies using the Stanford dependency con-
verter (De Marneffe et al., 2006) with the basic de-
pendency scheme, which produces dependencies
that are strictly projective.
3.2 SALT Annotated Corpora
We perform two sets of experiments on the two
SALT-annotated corpora described in Table 2. We
carry out the first set of experiments on on the Ed-
monton Narrative Norms Instrument (ENNI) cor-
pus, which contains 377 transcripts collected from
children between the ages of 3 years 11 months
and 10 years old. The children all lived in Edmon-
ton, Alberta, Canada, were typically developing,
and were native speakers of English.
After exploring various system configurations,
ENNI NSR
Words Utts Words Utts
Train 360,912 44,915 103,810 11,869
Dev. 45,504 5,614 12,860 1,483
Test 44,996 5,615 12,982 1,485
% with error 13.2 14.3
(a) Word and utterance counts
ENNI NSR
[EP] 0 20
[EO] 0 495
[EW] 4,916 1,506
[EU] 3,332 568
[OM] 10 297
[OW] 766 569
Total 9,024 3,455
(b) Error code counts
Table 2: Summary of ENNI and NSR Corpora.
There can be multiple errors per utterance. Word
counts include mazes.
we evaluate how well our method works when it
is applied to another corpus with different anno-
tation standards. Specifically, we train and test
our technique on the Narrative Story Retell (NSR)
corpus (SALT Software, 2014b), which contains
496 transcripts collected from typically develop-
ing children living in Wisconsin and California
who were between the ages of 4 years 4 months
and 12 years 8 months old. The ENNI and NSR
corpora were annotated by two different research
groups, and as Table 2 illustrates, they contain
a different distribution of errors. First, ENNI
uses the [EW] (other word-level error) tag to code
both overgeneralization errors instead of [EO], and
omitted morphemes instead of [OM]. The [EU]
code is also far more frequent in ENNI than NSR.
Finally, the NSR corpus includes an error code that
does not appear in the ENNI corpus: [EP], which
indicates a pronominal error, for example using
the wrong person or case. [EP], however, is rarely
used.
We preprocess the ENNI and NSR corpora to
reconstruct surface forms from bound morpheme
annotations (ex. ?go/3S? becomes ?goes?), partial
words, and non-speech sounds. We also either ex-
cise manually identified mazes or remove maze
annotations, depending upon the experiment.
3.3 Evaluation
Evaluating system performance in tagging tasks
on manually annotated data is typically straight-
983
Evaluation Level: ERROR UTTERANCE
Individual error codes Has error?
Gold error codes: [EW] [EW] Yes
Predicted error codes: [EW] [OW] Yes
Evaluation: TP FN FP TP
Figure 1: Illustration of UTTERANCE and ERROR level evaluation
TP = true positive; FP = false positive; FN = false negative
forward: we simply compare system output to the
gold standard. Such evaluation assumes that the
best system is the one that most faithfully repro-
duces the gold standard. This is not necessarily
the case with applying SALT error codes for three
reasons, and each of these reasons suggests a dif-
ferent form of evaluation.
First, automatically detecting SALT error codes
is an important task because it can aid clini-
cal investigations. As Morley et al. (2013) il-
lustrated, even extremely coarse features derived
from SALT annotations, for example a binary fea-
ture for each utterance indicating the presence of
any error codes, can be of immense utility for iden-
tifying language impairments. Therefore, we will
evaluate our system as a binary tagger: each ut-
terance, both in the manually annotated data and
system output either contains an error code, or it
does not. We will label this form of evaluation as
UTTERANCE level.
Second, clinicians are not only interested in
how many utterances have an error, but also which
particular errors appear in which utterances. To
address this issue, we will compute precision, re-
call, and F1 score from the counts of each er-
ror code in each utterance. We will label this
form of evaluation as ERROR level. Figure 1 illus-
trates both UTTERANCE and ERROR level evalua-
tion. Note that the utterance level error code [EU]
is only allowed to appear once per utterance. As
a result, we will ignore any predicted [EU] codes
beyond the first.
Third, the quality of the SALT annotations
themselves is unknown, and therefore evaluation
in which we treat the manually annotated data as a
gold standard may not yield informative metrics.
Morley et al. (2014) found that there are likely
inconsistencies in maze annotations both within
and across corpora. In light of that finding, it is
possible that error code annotations are somewhat
inconsistent as well. Furthermore, our approach
has a critical difference from manual annotation:
we perform classification one utterance at a time,
while manual annotators have access to the context
of an utterance. Therefore certain types of errors,
for example using a pronoun of the wrong gender,
or responding ungrammatically to a question (ex.
?What are you doing?? ?Eat.?) will appear gram-
matical to our system, but not to a human anno-
tator. We address both of these issues with an in-
depth analysis of the output of one of our systems,
which includes manually re-coding utterances out
of context.
4 Detecting Errors in ENNI
4.1 Baselines
We evaluate two existing systems to see how ef-
fectively they can identify utterances with SALT
error codes: 1) Microsoft Word 2010?s gram-
mar check, and 2) the simplified version of Has-
sanali and Liu?s grammaticality detector (2011)
proposed by Morley et al. (2013) (mentioned in
Section 2.2). We configured Microsoft Word
2010?s grammar check to look for the following
classes of errors: negation, noun phrases, subject-
verb agreement, and verb phrases (see http://
bit.ly/1kphUHa). Most error classes in gram-
mar check are not relevant for transcribed speech,
for example capitalization errors or confusing it?s
and its; we selected classes of errors that would
typically be indicated by SALT error codes.
Note that these baseline systems can only give
us an indication of whether there is an error in
the utterance or not; they do not provide the spe-
cific error tags that mimic the SALT guidelines.
Hence we evaluate just the UTTERANCE level per-
formance of the baseline systems on the ENNI de-
velopment and test sets. These results are given
in the top two rows of each section of Table 3.
We apply these systems to utterances in two condi-
tions: with mazes (i.e., disfluencies) excised; and
with unannotated mazes left in the utterances. As
can be seen in Table 3, the performance Microsoft
Word?s grammar checker degrades severely when
984
(a)
Him [EW] (can not) can not get up .
(b)
ROOT him can not can not get up .
nsubj+[EW]
aux
neg
aux
neg
ROOT
prt
P
Figure 2: (a) SALT annotated utterance; mazes indicated by parentheses; (b) Dependency parse of same
utterance parsed with a grammar trained on the Switchboard corpus and augmented dependency labels.
We use a corpus of parses with augmented labels to train our grammaticality detector.
mazes are not excised, but this is not the case for
the Morley et al. (2013) detector.
4.2 Proposed System
Using the ENNI corpus, we now explore various
configurations of a system for grammatical error
code detection. All of our systems use redshift
to learn grammars and to parse. First, we train
an initial grammar G
0
on the Switchboard tree-
bank (Godfrey et al., 1992) (preprocessed as de-
scribed in Section 3.1). Redshift learns a model for
part of speech tagging concurrently with G
0
. We
use G
0
to parse the training portion of the ENNI
corpus. Then, using the SALT annotations, we
append error codes to the dependency arc labels
in the parsed ENNI corpus, assigning each error
code to the word it follows in the SALT annotated
data. Figure 2 shows a SALT annotated utterance,
as well as its dependency parse augmented with
error codes. Finally, we train a grammar G
Err
on
the parse of the ENNI training fold that includes
the augmented arc labels. We can now use G
Err
to automatically apply SALT error codes: they are
simply encoded in the dependency labels. We also
apply the [EW] label to any word that is in a list of
overgeneralization errors
3
.
We modify three variables in our initial trials on
the ENNI development set. First, we change the
proportion of utterances in the training data that
contain an error by removing utterances.
4
Doing
so allows us to alter the operating point of our sys-
3
The list of overgeneralization errors was generously pro-
vided by Kyle Gorman
4
Of course, we never modify the development or test data.
tem in terms of precision and recall. Second, we
again train and test on two versions of the ENNI
corpus: one which has had mazes excised, and the
other which has them present (but not annotated).
Third, we evaluate two feature sets: ZHANG and
FULL.
The plots in Figure 3 show how the per-
formances of our systems at different operating
points vary, while Table 3 shows the performance
of our best system configurations on the ENNI de-
velopment and test sets. Surprisingly, we see that
neither the choice of feature set, nor the presence
of mazes has much of an effect on system per-
formance. This is in strong contrast to Microsoft
Word?s grammar check, which is minimally effec-
tive when mazes are included in the data. The
Morley et al. (2013) system is robust to mazes,
but still performs substantially worse than our pro-
posed system.
4.3 Error Analysis
We now examine the errors produced by our best
performing system for data in which mazes are
present. As shown in Table 3, when we apply our
system to ENNI-development, the UTTERANCE
P/R/F1 is 0.831 / 0.502 / 0.626 and the ERROR
P/R/F1is 0.759 / 0.434 / 0.552. This system?s per-
formance detecting specific error codes is shown
in Table 4. We see that the recall of [EU] errors is
quite low compared with the recall for [EW] and
[OW] errors. This is not surprising, as human an-
notators may need to leverage the context of an ut-
terance to identify [EU] errors, while our system
makes predictions for each utterance in isolation.
985
(a) UTTERANCE level evaluation (b) ERROR level evaluation
Figure 3: SALT error code detection performance at various operating points on ENNI development set
Eval Mazes Excised Mazes Present
System type P R F1 P R F1
Development
MS Word UTT 0.843 0.245 0.380 0.127 0.063 0.084
Morley et al. (2013) UTT 0.407 0.349 0.376 0.343 0.321 0.332
Current paper
UTT 0.943 0.470 0.627 0.831 0.502 0.626
ERR 0.895 0.412 0.564 0.759 0.434 0.552
Test
MS Word UTT 0.824 0.209 0.334 0.513 0.219 0.307
Morley et al. (2013) UTT 0.375 0.328 0.350 0.349 0.252 0.293
Current Paper
UTT 0.909 0.474 0.623 0.809 0.501 0.618
ERR 0.682 0.338 0.452 0.608 0.360 0.452
Table 3: Baseline and current paper systems? performance on ENNI. Evaluation is at the UTTERANCE
(UTT) level except for the current paper?s system, which also presents evaluation at the ERROR (ERR)
level.
Error Code P R F1
EU 0.639 0.193 0.297
EW 0.832 0.582 0.685
OW 0.680 0.548 0.607
Table 4: ERROR level detection performance for
each code (system trained on ENNI; 30% error
utterances; ZHANG feature set; with mazes)
We randomly sampled 200 utterances from the
development set that have a manually annotated
error, are predicted by our system to have an er-
ror, or both. A speech-language pathologist who
has extensive experience with using SALT for re-
search purposes in both clinical and typically de-
veloping populations annotated the errors in each
utterance. She annotated each utterance in isola-
tion so as to ignore contextual errors. We compare
our annotations to the original annotations, and
system performance using our annotations and the
original annotations as different gold standards.
The results of this comparison are shown in Table
5.
Comparing our manual annotations to the orig-
inal annotations, we notice some disagreements.
We suspect there are two reasons for this. First,
unlike the original annotators, we annotate these
utterances out of context. This may explain why
we identify far fewer utterance level error [EU]
codes than the original annotators (20 compared
with 67). Second, we may be using different cri-
teria for each error code than the original anno-
tators. This is an inevitable issue, as the SALT
guidelines do not provide detailed definitions of
the error codes, nor do individual groups of anno-
tators. To illustrate, the ?coding notes? section of
986
Tag Gold Gold Count Disagreement P R F1
[EU] Original 67 52 0.500 0.149 0.230
Revised 20 0.450 0.333 0.383
[EW] Original 137 27 0.859 0.533 0.658
Revised 126 0.800 0.540 0.645
[OW] Original 16 13 0.667 0.275 0.480
Revised 15 0.444 0.267 0.333
Table 5: System performance using ERROR level evaluation on 200 utterances selected from ENNI-dev
using original and revised annotations as gold standard
UTTERANCE level ERROR level
System P R F1 P R F1
ENNI-trained 0.310 0.124 0.178 0.157 0.057 0.084
NSR-trained 0.243 0.249 0.277 0.150 0.195 0.170
MS Word 0.561 0.171 0.261 ? ? ?
Morley et al. (2013) 0.250 0.281 0.264 ? ? ?
NSR ? MS Word 0.291 0.447 0.353 ? ? ?
NSR ? Morley et al. (2013) 0.297 0.387 0.336 ? ? ?
All 3 0.330 0.498 0.397 ? ? ?
Table 6: Error detection performance on NSR-development, mazes included
the description of the ENNI corpus
5
only lists the
error codes that were used consistently, but does
not describe how to apply them. These findings
illustrate the importance of having a rapidly train-
able error code detector: research groups will be
interested in different phenomena, and therefore
will likely have different annotation standards.
5 Detecting Errors in NSR
We apply our system directly to the NSR corpus
with mazes included. We use the same parameters
set on the ENNI corpus in Section 4.2. We apply
the model trained on ENNI to NSR, but find that it
does not perform very well as illustrated in Table
6. These results further underscore the need for
a trainable error code detector in this domain, as
opposed to the static error detectors that are more
common in the grammatical error detection litera-
ture.
We see in Table 6 that retraining our model
on NSR data improves performance substantially
(UTTERANCE F1 improves from 0.178 to 0.277),
but not to the level we observed on the ENNI cor-
pus. The Morley et al. (2013) system also per-
forms worse when trained and tested on NSR, as
compared with ENNI. When mazes are included,
5
http://www.saltsoftware.com/salt/
databases/ENNIRDBDoc.pdf
the performance of Microsoft Word?s grammar
check is higher on NSR than on ENNI (F1=0.261
vs 0.084), but it it still yields the lowest perfor-
mance of the three systems. We find that combin-
ing our proposed system with either or both of the
baseline systems further improves performance.
The NSR corpus differs from ENNI in several
ways: it is smaller, contains fewer errors, and uses
a different set of tags with a different distribution
from the ENNI corpus, as shown in Table 2. We
found that the smaller amount of training data is
not the only reason for the degradation in perfor-
mance; we trained a model for ENNI with a set of
training data that is the same size as the one for
NSR, but did not observe a major drop in perfor-
mance. We found that UTTERANCE F1 drops from
0.626 to 0.581, and ERROR F1 goes from 0.552 to
0.380, not nearly the magnitude drop in accuracy
observed for NSR.
We believe that a major reason for why our sys-
tem performs worse on NSR than ENNI may be
that the ENNI annotations adhere less strictly to
certain SALT recommendations than do the ones
in NSR. The SALT guidelines suggest that utter-
ances with two or more word-level [EW] and/or
omitted word [OW] errors should only be tagged
with an utterance-level [EU] error (SALT Soft-
ware, 2014a). ENNI, however, has many utter-
987
ances with multiple [EW] and [OW] error codes,
along with utterances containing all three error
codes. NSR has very few utterances with [EU] and
other codes, or multiple [EW] and [OW] codes.
The finer grained annotations in ENNI may sim-
ply be easier to learn.
6 Conclusion and Future Directions
We have proposed a very simple method to rapidly
train a grammatical error detector and classifier.
Our proposed system only requires training data
with error code annotations, and is agnostic as to
the nature of the specific error codes. Furthermore,
our system?s performance does not appear to be
affected by disfluencies, which reduces the burden
required to produce training data.
There are several key areas we plan to inves-
tigate in the future. First, we would like to ex-
plore different update functions for the parser; the
predicted error codes are a byproduct of parsing,
but we do not care what the parse itself looks like.
At present, the parser is updated whenever it pro-
duces a parse that diverges from the gold stan-
dard. It may be better to update only when the
error codes predicted for an utterance differ from
the gold standard. Second, we hope to explore fea-
tures that could be useful for identifying grammat-
ical errors in multiple data sets. Finally, we plan
to investigate why our system performed so much
better on ENNI than on NSR.
Acknowledgments
We would like to thank the following people for
valuable input into this study: Joel Tetreault,
Jan van Santen, Emily Prud?hommeaux, Kyle
Gorman, Steven Bedrick, Alison Presmanes Hill
and others in the CSLU Autism research group
at OHSU. This material is based upon work
supported by the National Institute on Deafness
and Other Communication Disorders of the Na-
tional Institutes of Health under award number
R21DC010033. The content is solely the respon-
sibility of the authors and does not necessarily rep-
resent the official views of the National Institutes
of Health.
References
Eric Atwell. 1987. How to detect grammatical er-
rors in a text without parsing it. In Bente Maegaard,
editor, EACL, pages 38?45, Copenhagen, Denmark,
April. The Association for Computational Linguis-
tics.
Mari I Bowden and Richard K Fox. 2002. A diagnos-
tic approach to the detection of syntactic errors in
english for non-native speakers. The University of
Texas?Pan American Department of Computer Sci-
ence Technical Report.
Peter F. Brown, Vincent J. Della Pietra, Peter V.
de Souza, Jennifer C. Lai, and Robert L. Mercer.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479.
Andrew Caines and Paula Buttery. 2010. You talking
to me?: A predictive model for zero auxiliary con-
structions. In Proceedings of the 2010 Workshop on
NLP and Linguistics: Finding the Common Ground,
pages 43?51.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In Proceedings of LREC, volume 6, pages
449?454.
John J Godfrey, Edward C Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In IEEE Interna-
tional Conference on Acoustics, Speech, and Signal
Processing, volume 1, pages 517?520.
Khairun-nisa Hassanali and Yang Liu. 2011. Measur-
ing language development in early childhood educa-
tion: a case study of grammar checking in child lan-
guage transcripts. In Proceedings of the 6th Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications, pages 87?95.
George E. Heidorn, Karen Jensen, Lance A. Miller,
Roy J. Byrd, and Martin S Chodorow. 1982.
The EPISTLE text-critiquing system. IBM Systems
Journal, 21(3):305?326.
Matthew Honnibal and Mark Johnson. 2014. Joint
incremental disfluency detection and dependency
parsing. TACL, 2:131?142.
Nina H Macdonald, Lawrence T Frase, Patricia S Gin-
grich, and Stacey A Keenan. 1982. The writer?s
workbench: Computer aids for text analysis. Edu-
cational psychologist, 17(3):172?179.
Marie W Meteer, Ann A Taylor, Robert MacIntyre,
and Rukmini Iyer. 1995. Dysfluency annotation
stylebook for the switchboard corpus. University of
Pennsylvania.
Jon F Miller, Karen Andriacchi, and Ann Nockerts.
2011. Assessing language production using SALT
software: A clinician?s guide to language sample
analysis. SALT Software, LLC.
988
Eric Morley, Brian Roark, and Jan van Santen. 2013.
The utility of manual and automatic linguistic error
codes for identifying neurodevelopmental disorders.
In Proceedings of the Eighth Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions, pages 1?10, Atlanta, Georgia, June. Associa-
tion for Computational Linguistics.
Eric Morley, Anna Eva Hallin, and Brian Roark. 2014.
Challenges in automating maze detection. In Pro-
ceedings of the First Workshop on Computational
Linguistics and Clinical Psychology, pages 69?77,
Baltimore, Maryland, June.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake VanderPlas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Edouard Duchesnay. 2012.
Scikit-learn: Machine learning in python. CoRR,
abs/1201.0490.
Xian Qian and Yang Liu. 2013. Disfluency detection
using multi-step stacked learning. In Lucy Vander-
wende, Hal Daum?e III, and Katrin Kirchhoff, edi-
tors, HLT-NAACL, pages 820?825, Atlanta, Georgia,
USA, June. The Association for Computational Lin-
guistics.
Mohammad Sadegh Rasooli and Joel R. Tetreault.
2014. Non-monotonic parsing of fluent umm i mean
disfluent sentences. In Gosse Bouma and Yannick
Parmentier, editors, EACL, pages 48?53, Gothen-
burg, Sweden, April. The Association for Compu-
tational Linguistics.
LLC SALT Software. 2014a. Course
1306: Transcription - Conventions Part 3.
http://www.saltsoftware.com/
onlinetraining/section-page?
OnlineTrainingCourseSectionPageId=
76. [Online; accessed 29-May-2104].
LLC SALT Software. 2014b. Narrative
Story Retell Database. http://www.
saltsoftware.com/salt/databases/
NarStoryRetellRDBDoc.pdf. [Online;
accessed 29-May-2104].
Phyllis Schneider, Denyse Hayward, and Rita Vis
Dub?e. 2006. Storytelling from pictures using
the edmonton narrative norms instrument. Jour-
nal of Speech Language Pathology and Audiology,
30(4):224.
Jan PH Van Santen, Emily T Prud?hommeaux, Lois M
Black, and Margaret Mitchell. 2010. Com-
putational prosodic markers for autism. Autism,
14(3):215?236.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
ACL (Short Papers), pages 188?193, Portland, Ore-
gon, USA, June. The Association for Computational
Linguistics.
989
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 1?10,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
The Utility of Manual and Automatic Linguistic Error Codes
for Identifying Neurodevelopmental Disorders?
Eric Morley, Brian Roark and Jan van Santen
Center for Spoken Language Understanding, Oregon Health & Science University
morleye@gmail.com, roarkbr@gmail.com, vansantj@ohsu.edu
Abstract
We investigate the utility of linguistic features
for automatically differentiating between chil-
dren with varying combinations of two po-
tentially comorbid neurodevelopmental disor-
ders: autism spectrum disorder and specific
language impairment. We find that certain
manual codes for linguistic errors are useful
for distinguishing between diagnostic groups.
We investigate the relationship between cod-
ing detail and diagnostic classification perfor-
mance, and find that a simple coding scheme
is of high diagnostic utility. We propose a sim-
ple method to automate the pared down coding
scheme, and find that these automatic codes
are of diagnostic utility.
1 Introduction
In Autism Spectrum Disorders (ASD), language im-
pairments are common, but not universal (American
Psychiatric Association, 2000). Whether these lan-
guage impairments are distinct from those in Spe-
cific Language Impairment (SLI) is an unresolved
issue (Williams et al, 2008; Kjelgaard and Tager-
Flusberg, 2001). Accurate and detailed characteri-
zation of these impairments is important not only for
resolving this issue, but also for diagnostic practice
and remediation.
Language ability is typically assessed with struc-
tured instruments (?tests?) that elicit brief, easy to
?This research was supported in part by NIH NIDCD award
R01DC012033 and NSF award #0826654. Any opinions, find-
ings, conclusions or recommendations expressed in this publi-
cation are those of the authors and do not reflect the views of
the NIH or NSF. Thanks to Emily Prud?hommeaux for useful
discussion on this topic and help with the data.
score, responses to a sequence of items. For exam-
ple, the CELF-4 includes nineteen multi-item sub-
tests with tasks such as object naming, word defini-
tion, reciting the days of the week, or repeating sen-
tences (Semel et al, 2003). Researchers are begin-
ning to discuss the limits of structured instruments in
terms of which language impairments they tap into
and how well they do so, and are advocating the po-
tential benefits of language sample analysis ? an-
alyzing natural language samples ? to complement
structured assessment, specifically for language as-
sessment in ASD where pragmatic and social com-
munication issues are paramount yet are hard to
assess in a conventional test format (e.g. Tager-
Flusberg et al 2009). However, language sample
analysis faces two labor-intensive steps: transcrip-
tion and detailed coding of the transcripts.
To illustrate the latter, consider the Systematic
Analysis of Language Transcripts (SALT) (Miller
and Chapman, 1985; Miller et al, 2011), which is
the de-facto standard choice by clinicians looking
to code elicited language samples. SALT comprises
a scheme for coding transcripts of recorded speech,
together with software that tallies these codes, com-
putes scores describing utterance length and error
counts, and compares these scores with normative
samples. SALT codes indicate bound morphemes,
edits (which are referred to in the clinical literature
as ?mazes?), and several types of errors in transcripts
of natural language, e.g., omitted or inappropriate
words.
Although this has not been formally documented,
our experience with SALT coding has shown that the
codes vary in terms of: 1) difficulty of manual cod-
ing ? e.g., relatively subtle pragmatic errors versus
overgeneralization or marking bound morphemes;
1
2) utility for identifying particular disorders; and 3)
difficulty of automating the code. This raises an im-
portant question: Is there a combination of codes
that jointly discriminate well between relevant diag-
nostic groups, and at the same time are either easy
to code manually or can in principle be automated?
This paper explores, first, how well the various man-
ual SALT codes classify certain diagnostic groups;
and, second, whether we can automate manual codes
that are of diagnostic utility. Our goal is limited: it
is not the automation of all SALT codes, but the au-
tomation of those that in combination are of high di-
agnostic utility. Automating all SALT codes is sub-
stantially more challenging; yet, we note that even
when some of these codes do not aid in classify-
ing groups, they nevertheless may be of importance
for developing remediation strategies for individual
children. We are particularly interested in the im-
pact of Autism in addition to language impairments
for the utility of particular SALT codes.
The diagnostic groups are carefully chosen to
be pairwise matched either on language abilities or
on autism symptomatology, thus enabling a pre-
cise, ?surgical? determination of the degrees to
which SALT codes reflect language-specific vs.
autism-specific factors. Specifically, the groups in-
clude children with ASD with language impairment
(ALI); ASD with no language impairment (ALN);
SLI alone; and typically developing (TD), which is
strictly defined to exclude any neurodevelopmental
disorder. The TD and ALN groups, as well as the
ALI and SLI groups, are matched on language and
overall cognitive abilities, while the ALN and ALI
groups are matched on autism symptomatology but
not on language and overall cognitive abilities; all
groups are matched on chronological age.
Regarding our algorithmic approach, we note that
automatic detection of relatively subtle errors may
be exceedingly difficult, but perhaps such subtle er-
rors are less critical for diagnosis than more obvi-
ous ones. Most prior work in grammaticality de-
tection in spoken language has focused on special-
ized detectors (e.g., Caines and Buttery 2010; Has-
sanali and Liu 2011), such as mis-use of particular
verb constructions rather than coarser detectors for
the presence of diverse classes of errors. We demon-
strate that these specialized error detectors can break
down when confronted with real world dialogue, and
that in general, the features in these detectors re-
stricts their utility in detecting other sorts of errors.
We implement a detector to automatically extract
coarse SALT codes from an uncoded transcript. This
detector only depends upon part of speech tags, as
opposed to the parse features that are often used in
grammaticality detectors. In most cases, these au-
tomatically extracted codes enable us to distinguish
between diagnostic groups more effectively than do
features that can be extracted trivially from an un-
coded transcript.
As far as we know, researchers have not pre-
viously considered the utility of grammatical er-
ror codes to identify ASD or SLI. Prudhommeaux
and Rouhizadeh (2012), however, found that au-
tomatically extracted pragmatic features are useful
for identifying children with ASD, among children
both with and without SLI. Gabani et al (2009)
found that features derived from language models
are useful for distinguishing between children with
and without a language impairment, both in mono-
lingual English speakers, and in children who are
bilingual in English and Spanish.
Improving the characterization of a child?s lan-
guage impairments is a prerequisite to developing a
sound plan for language training and education for
that child. This paper presents a step in the direction
of effective automated analysis of linguistic samples
that can provide useful information even in the face
of comorbid disorders such as ASD and SLI.
2 Systematic Analysis of Language
Transcripts
Here we give an overview of what SALT requires of
transcriptions, and of SALT coding. The approach
has been in wide use for nearly 30 years (Miller and
Chapman, 1985), and now also exists as a software
package1 providing transcription and coding support
along with tools for aggregating statistics for man-
ual codes over the annotated corpora and comparing
with age norms. The SALT software is not the focus
of this investigation, so we do not discuss it further.
2.1 Basic Transcription
We apply the automated methods to what will be
called basic transcripts. Key for this concept is that,
first, these transcripts do not require linguistic ex-
pertise and thus can be performed by standard tran-
scription services; and, second, that ? as we shall
1http://www.saltsoftware.com/
2
see ? useful features can be automatically computed
from them.
Following the SALT guidelines, a basic transcript
should indicate: the speaker of each utterance, par-
tial words (or stuttering), overlapping speech, unin-
telligible words, and non-speech sounds. It should
be verbatim, regardless of whether a child?s utter-
ance contains neologisms (novel words) or gram-
matical errors (for example ?I goed? should be writ-
ten as such).
A somewhat subtle issue is that SALT prescribes
that the basic transcript be broken into communi-
cation units (which in this paper will be synony-
mous with utterance). Communication units are
defined as ?a main clause with all its dependent
clauses? (Miller et al, 2011). One reason for defin-
ing utterance boundaries with communication units,
rather than turns or sentences, is that in addition to
this being standard practice in language sample anal-
ysis, doing so does not reward children for making
long, but rather simple statements, nor does it penal-
ize children for being interrupted. To illustrate the
first point, the utterance ?I like apples, and bananas,
and pears, and oranges, and grapes.? is one sen-
tence long, but has five communication units (one at
each comma). If the sentence were used as the ba-
sic unit, the utterance would indicate the same level
complexity as the obviously more intricate ?for the
past three years we have lived in an apartment?. In
the basic transcript, each communication unit should
be terminated by one of the following punctuation
marks: ??? if it is a question, ??? if the speaker was
interrupted, ?>? if the speaker abandoned the utter-
ance, and ?.? in all other cases. Thus, the above
example would be transcribed as ?C: I like apples.
. . . C: and grapes.?
2.2 Markup
There are three broad categories of SALT codes: in-
dicators of 1) certain bound morphemes, 2) edits
(discussed below), and 3) errors.
Morphology The following inflectional suffixes
must be coded according to the SALT guidelines:
plural -s (/S), possessive -?s (/Z), possessive plural
-s? (/S/Z), past tense -ed (/ED), 3rd person singular
-s (/3S), progressive -ing (/ING). The following cl-
itics must also be delimited with a ?/?, provided the
resulting root is unmodified in the surface form: n?t,
?t, ?d, ?re, ?s, ?ve. Since these morphemes are only in-
dicated if the root is unmodified in the surface form,
?won?t? will remain unsegmented because ?wo? is
not the root; ?can?t? will be segmented ?can/?T? and
?don?t? will be segmented ?do/N?T?, so as to pre-
serve their respective roots. Nominal or verbal forms
with any of the preceding suffixes or clitics are writ-
ten as the base form with the code appended, for ex-
ample hitting? hit/ING, bases? base/S.
Edits Edits consist of filler words such as ?like?,
?um? and ?uh?, false starts, and revisions. There may
be multiple edits in a single utterance, as well as
multiple adjacent edits. Edits are indicated by paren-
theses, for example: ?(And they like) and she (like)
faint/3S.? Note that in the SALT manual, and the lan-
guage sample analysis literature, edits are referred to
as mazes. We use the term edit here because this is
the more widely used term for this phenomenon in
natural language processing.
Error codes The exact set of error codes used de-
pends upon the clinician?s needs and the errors of
interest. Here we consider several key errors out-
lined in the SALT manual. These error codes and
examples are shown in Table 1. Some of these codes
describe precise classes of errors, for example [EO]
or [OW], but others do not. For example, [EW]
can describe using the wrong verb, tense, preposi-
tion or pronoun (in terms of case, person or gender),
as well as other errors. Note that [EU] (and [EC]) er-
ror codes can occur in grammatical utterances. The
[EU] code marks utterances that are ungrammatical
for reasons not captured by the other error codes, for
example severe problems with word order, or utter-
Table 1: SALT error codes and examples
Code Meaning Example Count in Corpus
[EC] Inappropriate response Did you help yourself stop? Mom[EC]. 9
[EO] Overgeneralization Yeah, cuz I almost saw/ED[EO] one. 229
[EW] Error word I play/ED of[EW] the cat. 1,456
[EU] Utterance-level error You can see it very hard because it/?S under my hair[EU]. 532
[EX] Extraneous word Would you like to be[EX] fall down? 322
[OM] Omitted morpheme The cat eat[OM] fish. 881
[OW] Omitted word He [OW] going now. 770
3
ances which are simply nonsensical, as in Table 1.
3 Evaluation of Manual Codes
In this section we use features extracted from SALT-
coded transcripts for classification. We consider two
different types of features: baseline features, which
are easily derived from a basic transcript; and fea-
tures derived from SALT codes. We investigate
these features to determine which SALT codes are
most worth automating for classification.
3.1 Data
Our data is a collection of 144 transcripts of the
Autism Diagnostic Observation Schedule (ADOS),
which is a semi-structured task that includes an
examiner and a child (Lord et al, 2002). Semi-
structured means that the examiner carries out a
sequence of rigorously specified activities, but her
prompts and questions are not scripted verbatim for
all of them. Detailed guidelines exist for scoring
the ADOS, but these are not considered in the cur-
rent paper. All transcripts have been manually coded
with SALT codes, described in Table 1.
Subjects ranged in age between 4 and 8 years and
were required to be intelligible, to have a full-scale
IQ of greater than 70, and to have a mean length of
utterance (MLU) of at least 3. Diagnoses of ASD
and of SLI followed standard procedures, and were
based on clinical consensus in accordance to diag-
nostic criteria outlined in the DSM-IV (American
Psychiatric Association, 2000). Furthermore, ASD
diagnosis required ADOS and Social Communica-
tion Questionnaire scores (SCQ) (Berument et al,
1999) to meet conventional thresholds. Diagnosis
of SLI required a CELF Core Language Score of at
least 1 standard deviation below the mean, in addi-
tion to exclusion of ASD.
Children were partitioned into pairs of groups
matched on certain key measures. Table 2 shows
these pairs and what they were matched on. The
individuals were selected from the initial pool of
all participants using the algorithm proposed by van
Santen et al (2010), in which, for a given pair of
groups, children are iteratively removed from each
group until there is no significant difference (at p <
0.02) on any measure on which we want the pair to
be matched. We combined some groups into com-
posite groups: ASD (ALI and ALN), nASD (SLI
and TD), LN (?language normal?: ALN and TD),
and LI (?language impaired?: ALI and SLI).
Group 1 Group 2
Group N Group N Matched on
ALI 25 ALN 21 Age, ADOS, SCQ
ALI 24 SLI 19 Age, NVIQ, VIQ
ALN 25 TD 27 Age, NVIQ, VIQ
ASD 48 nASD 61 Age
LN 61 LI 39 Age
SLI 15 TD 38 Age
Table 2: Matched measures for paired groups (ADOS =
ADOS score, NVIQ = non-verbal IQ, VIQ = verbal IQ)
3.2 Features
The term ?feature? will be used to refer to instances
of various classes of SALT codes as well as to in-
stances of other events that can be trivially extracted
from the basic transcripts but do not involve SALT
codes (e.g, the ratio of ?uh? to ?um?). We distinguish
between five levels of features, enumerated in Table
3, that vary in the number and complexity of codes
required. This ranges from the baseline features that
require no manual codes to SALT-5 features that re-
quire full SALT coding. We consider two normal-
ized variants of each feature: one normalized by the
number of utterances spoken by the child, and the
other normalized by the number of words spoken
by the child (except for TKCT). The ratios OCRAT
and UMUHRAT are never normalized. Each feature
level includes all features on lower levels. Finally,
to make our investigation into feature combinations
more tractable, we do not consider combining two
different normalizations of the same feature.
3.3 Classification
We perform six classification tasks in our investi-
gation, according to the paired groups in Table 2:
ALI/ALN; ALI/SLI; ALN/TD; ASD/nASD; LN/LI;
and SLI/TD. We extract various features from the
ADOS transcripts, and then classify the children in
a leave-pair-out (LPO) schema (Cortes et al, 2007)
using the scikit logistic regression classifier with de-
fault parameters (Pedregosa et al, 2011). For LPO
analysis, we iterate over all possible pairs that con-
tain one positive and one negative instance (i.e. chil-
dren with different diagnoses), training on all other
instances, and testing on that pair. We count a trial
as a success if the classifier assigns a higher proba-
bility of being positive to the positive instance than
to the negative instance. We then divide the num-
ber of successes by the number of pairs to get an
unbiased estimate of the area under the receiver op-
erating curve (AUC) (Airola et al, 2011). AUC is
4
Group Feature Description
Baseline CEOLP # of times examiner speaks while child is talking
ECOLP # of times child speaks while examiner is talking
INCCT Incomplete word count
OCRAT Ratio of open- to closed-class words
TKCT Token count
TPCT Type count
UMUHRAT Ratio of ?uh? to ?um?
UINTCT Unintelligible word count
SALT-1 All baseline features +
MPCT Morpheme count
EDITCT Edit count
SALT-2 All SALT-1 features +
NERRUTT Number of utterances with any SALT error codes
SALT-3 All SALT-2 features +
ERRCT Count of SALT error codes
SALT-4 All SALT-3 features +
UTLERRCT Count of utterance level errors (EC / EU)
WDLERRCT Count of word level errors (all other error codes)
SALT-5 All SALT-4 features +
XCT Count of individual error codes (X=EC, EO, . . . ; see Table 1)
Table 3: Features by Level
the probability that the classifier will assign a higher
score to a randomly chosen positive example than to
a randomly chosen negative example.
3.4 Determining Relevant Features
We use a t-test based criterion as a simple way to de-
termine which features to investigate for each clas-
sification task. For a given classification task, we
perform a t-test for independent samples on each
feature under both normalization schemes (if ap-
propriate). We retain a feature for investigation if
that feature is significantly different between the two
groups at the ? = 0.10 level. If a particular feature
varies significantly between groups under both nor-
malization schemes, we retain the version that has
the larger T-statistic. For the sake of brevity, we
do not report all of the features that varied between
groups here, but this data is available upon request
from the authors.
3.5 Initial Feature Ablation
We perform feature ablation to see which features
are most useful for performing each classification
task. Figure 1 shows the maximum performance (in
terms of AUC) over all subsets of features at each
feature level (on the x-axis) on each of the six di-
agnostic classification tasks. Missing values for a
particular level of features for any comparison indi-
cate that no features in that level that passed the t-test
based criterion for the two groups being compared.
Figure 1 illustrates two important points. First,
classification difficulty depends heavily on the pair
that is being compared. For example, the AUC
for ALI/SLI is at most 0.723 (SALT-5), while the
AUC for SLI/TD reaches 0.982 (SALT-5). This is
not surprising, as some pairs, most notably SLI/TD,
differ widely in coarse measures of language abil-
ity (such as non-verbal IQ), while other pairs, in-
cluding ALI/SLI, do not. Second, in many of the
tasks, SALT-derived features are of high utility, but
the biggest gain in classification performance comes
with SALT-2, which is a count of the number of
sentences containing any SALT error code. In fact,
for all but one classification task (ASD/nASD), the
AUC achieved with SALT-2 is at least 96% of the
maximum AUC. Furthermore, the best feature set
using SALT-2 features for most of these tasks is ei-
ther the NERRUTT feature alone, or in the case of
ALI/SLI, NERRUTT and TPCT. These results lead
us to conclude that the most important SALT-derived
feature to code is NERRUTT.
Perhaps surprisingly, Figure 1 also shows that for
ALN/TD and SLI/TD, performance at SALT-1 is
lower than the baseline. There are two reasons for
this, which we explain in turn: 1) the SALT-1 fea-
ture set must include a feature that is less useful than
those in the optimal baseline feature set, and 2) the
classifier will not ignore this feature. MPCT must be
included in SALT-1 for both pairs, because the only
5
Figure 1: Maximum classification performance (AUC) at different feature levels (Bln=Baseline, S-N=SALT-N)
other SALT-1 feature, EDITCT, does not vary signif-
icantly between either ALN/TD or SLI/TD. Further-
more, MPCT is highly correlated with TKCT, yet
TKCT is not in the best baseline feature set for ei-
ther of these pairs. Therefore, the SALT-1 feature
set is required to include a feature that is less useful
than the most useful ones in the baseline set, which
results in lower performance. Once MPCT is in-
cluded in the SALT-1 feature set, the logistic regres-
sion classifier will not ignore it by assigning it a zero
coefficient. This is because MPCT distinguishes be-
tween groups, and because the classifier is trained
at each round of LPO classification to maximize the
likelihood of the training data, rather than the AUC
estimate provided by LPO classification.
3.6 Counting Specific Error Codes
The single feature in SALT-2, NERRUTT, counts
how many utterances spoken by the child contain at
least one SALT error code. Some of these heteroge-
nous errors, for example overgeneralization errors
([EO]), should be straightforward to identify auto-
matically. Automatically identifying others, for ex-
ample utterances that are inappropriate in context
([EC]), would be more difficult. Therefore, before
automating the extraction of NERRUTT, we should
see which errors most need to be identified, and
which can safely be ignored. To do this, we repeat
our LPO classification procedure on various tasks
using SALT-2 features.
We perform the following procedure to identify
the most diagnostically informative errors: for each
subset s of SALT error codes, 1) compute the fea-
ture NERRUTTSUBSET by counting the number of
utterances that contain any of the errors in s; then 2)
perform the LPO diagnostic classification task using
NERRUTTSUBSET as the only feature. The results
of this experiment are in Table 4. The ?% Max? col-
umn shows classification performance when a par-
ticular subset of error codes were counted, relative
to the maximum performance yielded by any subset
of error codes for that particular task. We exclude
the ALN/TD and ASD/nASD tasks from this exper-
iment because NERRUTT does not improve perfor-
mance on these tasks. This is perhaps unsurprising,
because SALT codes were designed to be diagnostic
of SLI, not ASD.
We find that in all tasks, ignoring certain error
codes raises performance. These results also show
that it is not necessary, and indeed not ideal, to iden-
tify utterances containing any SALT code. Identi-
fying utterances that contain any of the following
three codes is sufficient to achieve at least 97% of
the maximum AUC enabled by counting any sub-
set of SALT codes: [EW], [OM], [OW]. For clarity,
NERRUTTMOD is the count of utterances that con-
tain any of those three SALT codes.
Table 4: AUC from Counting Subsets of Errors
Classification Errors Counted AUC % Max
ALI/ALN EW, OM 0.762 100
EW, OM, OW 0.739 97
all 0.724 93
ALI/SLI EW, OM 0.715 100
EW, OM, OW 0.704 98
all 0.676 95
LN/LI EW, OM, OW 0.901 100
all 0.881 98
SLI/TD OM, OW 0.984 100
EW, OM, OW 0.970 99
all 0.951 97
6
3.7 Robustness of NERRUTTMOD feature to
noise: a simulation experiment
We will consider two general ways of automatically
extractingNERRUTTMOD. The first way is to build
a detector to identify utterances that contain at least
one relevant error. The second way is to make de-
tectors for the each relevant error, then combine the
output of these detectors. It is unlikely that any error
detector will perform perfectly. Prior to investiga-
tion of automation strategies, we would like to get an
idea of how much such errors will affect diagnostic
classification performance. To this end, we investi-
gate how well we can perform the diagnostic classi-
fication tasks when noise is deliberately introduced
into the NERRUTTMOD values via simulation.
We consider two scenarios. In the first, we as-
sume a single error detector will be used to extract
NERRUTTMOD. We take each manually coded ut-
terance, then randomly change whether or not that
sentence is counted as having an error to simulate
different precision and recall levels of the automated
NERRUTTMOD extractor. We repeat this procedure
100 times for each classification task, and then ex-
amine the mean AUC over all trials. In the sec-
ond scenario, we assume a detector for each error
code that counts a sentence as having an error any
time one of the detectors fires. We randomly cor-
rupt the detection of each error code considered in
NERRUTTMOD in turn to simulate different preci-
sion and recall levels of each individual error detec-
tor. We assume perfect detection of all errors not
being randomly corrupted. Again, we repeat this
procedure 100 times for each classification task, and
consider the mean AUC over all trials.
In both experiments, and in all classification tasks,
we find that the NERRUTTMOD feature is ex-
tremely robust to noise. For example, finding the
NERRUTTMOD feature with a single detector with
a precision/recall of 0.1/0.3 enables SLI/TD clas-
sification with an average AUC of 0.975, as com-
pared to the maximum AUC of 0.984, enabled by
a perfect detector. When we use a cascaded de-
tector to corrupt each of the two errors counted in
NERRUTTMOD for classifying SLI/TD, so long as
one error is detected perfectly, the other error only
needs to be detected with precision and recall of 0.1
to enable a classification AUC within 0.02 of the
maximum.
The extreme robustness of this feature may appear
surprising, but it is easily explained by the data. The
mean value of NERRUTTMOD for the SLI group
is 7.8 times the mean value of this feature for the
TD group. So long as there is a correlation between
the true value of NERRUTTMOD and the estimated
value, as we have assumed in this experiment, then
the estimated value is bound to be of utility in clas-
sification. This bodes well for the utility of automa-
tion, even for a difficult task of discovering some of
the relatively subtle errors coded in SALT.
4 Automatic Feature Extraction
4.1 Evaluating Hassanali and Liu?s System
Hassanali and Liu developed two grammaticality de-
tectors that they used to identify ungrammatical ut-
terances in transcriptions of speech from children
both with and without language impairments (Has-
sanali and Liu, 2011). They tested their grammati-
cality detectors on the Paradise corpus, which con-
sists of conversations with children elicited during
an investigation of otitis media, a hearing disor-
der. They present both a rule-based and a statis-
tical grammaticality detector. Both detectors con-
sist of sub-detectors for the errors shown in Table
5. The rule-based and statistical detectors perform
well, with the statistical detector outperforming the
rule-based one (F1=0.967 vs. 0.929). The statistical
detector, however, requires each error identified by
any of the sub-detectors to be manually identified in
the training data.
We reimplement both the rule based and statis-
tical detectors proposed by Hassanali and Liu, and
apply it to our data, with three modifications. The
first two are minor: 1) we substitute the Charniak-
Johnson reranking parser (2005) for Charniak?s
original parser (Charniak, 2000), and 2) we use the
scikit multinomial naive bayes classifier (Pedregosa
et al, 2011) instead of the one in WEKA (Hall et al,
2009). The third difference is that we use these de-
tectors to identify SALT error codes rather than the
errors these classifiers were originally built to detect.
The mapping of the original errors to SALT error
codes is given in Table 5. To clarify, if we are train-
ing the ?Missing Verb? detector, then any utterance
with an [OW] code is taken to be a positive exam-
ple. This issue does not present itself with the rule-
based detector because it is not trained. Note that the
two verb agreement features may correspond to ei-
ther [EW] or [OM] SALT codes. For example, ?you
does? would be [EW] because of the otiose 3rd per-
7
Error SALT code
Misuse of -ing participle [EW]
Missing copulae [OW]
Missing verb [OW]
Subject-auxilliary agreement [EW]
Subject-verb agreement [EW]/[OM]
Missing infinitive ?to? [OW]
Table 5: Error detectors proposed by Hassanali and Liu
son singular suffix, while ?he do? would be an [OM]
because it is missing that same suffix.
Hassanali and Liu?s error detectors perform
poorly on our data. Table 6 reports the performance
of their detectors detecting utterances with various
error codes. Five of the six statistical error detec-
tors that Hassanali and Liu proposed are unable to
identify any of the errors in our data. The?misuse
of -ing participle? detector, however, is an excep-
tion, and its performance detecting the analogous
error code [EW], using 10-fold cross validation is,
shown in Table 6. To detect the two pairs of er-
ror codes, [EW][OM] and [OM][OW], and all three
relevant error codes ([EW][OM][OW]), we use the
appropriate rule based detectors. For example, to
detect utterances with either [EW] or [OM] errors,
we pool the detectors for the analogous error codes:
?misuse of -ing participle?, ?subject-auxilliary agree-
ment?, and ?subject-verb agreement?.
There are three factors that may explain the poor
performance observed with most of Hassanali and
Liu?s error detectors when used with our data. The
first is that the three SALT codes we try to detect
([EW], [OM], and [OW]) capture a wider variety of
errors than the six in Hassanali and Liu?s system.
This could account for the low recall. Second, there
are many utterances in our data that Hassanali and
Liu?s system would label an error, but which are not
marked with any SALT error codes. For example, if
the examiner asks the child what she is doing, ?eat-
ing spaghetti? is a faultless response, even though it
is missing both the subject and auxiliary verb. Such
utterances may account for the low precision. Fi-
nally, most of Hassanali and Liu?s sub-detectors de-
pend upon features describing the presence or ab-
sence of specific structures in the parses of the input.
The exception to this is the statistical ?misuse of -ing
participle? detector, which uses part of speech (POS)
tag bigrams and skip bigrams as features. It should
come as no surprise then that the ?misuse of -ing par-
ticiple? is the most robust of these detectors. Indeed,
Codes
System Detected P R F1
Hassanali [EW]? 0.074 0.218 0.110
& Liu [EW][OM]* 0.049 0.277 0.083
[OM][OW]* 0.028 0.191 0.049
All three* 0.066 0.354 0.111
POS-tag [EW] 0.074 0.218 0.110
feature- [OM] 0.070 0.191 0.103
based [OW] 0.064 0.210 0.099
classifier [EW][OM] 0.102 0.269 0.148
[OM][OW] 0.102 0.269 0.148
All three 0.127 0.308 0.180
Table 6: Performance on automatic detection of utter-
ances with certain error codes using Hassanali and Liu?s
detectors, and general POS-tag-feature-based classifier.
? = ?misuse of -ing participle?, statistical; * = rule-based
in what follows, we make use of general POS-tag
features (tag n-gram and skip n-grams) as they do in
this detector, for a general purpose detector not tar-
geted specifically at this particular construction, but
rather to detect the presence of arbitrary given sets
of error tags.
4.2 Automatic SALT error code detection
We compare three types of automatic error code de-
tectors: 1) individual error code detectors; 2) pair
detectors, each of which detects a pair of error codes
included in NERRUTTMOD, following Table 4; and
3) a generic detector that identifies any utterance
containing any of the following SALT codes: [EW],
[OM], or [OW]. We investigate four different fea-
tures, all of which are easily derived from the basic
transcript: bigrams and skip bigrams of words, and
POS tags. We use POS tags extracted from the out-
put of the Charniak-Johnson reranking parser (2005)
(also used in our reimplementation of Hassanali and
Liu?s detectors) for simplicity. We use the Bernoulli
Naive Bayes classifier in scikit with the default set-
tings (Pedregosa et al, 2011).
We find that the word features do not aid clas-
sification in any condition, and that using both bi-
grams and skip bigrams of POS tags improves on
using either alone. We report the performance of
the three types of error detectors in Table 6. These
results are from 10-fold cross-validation using POS
tag bigrams and skip bigrams as features. Note that
the general POS-tag-feature-based classifier uses the
same features as Hassanali and Liu?s statistical ?mis-
use of -ing participle? detector, which is why the
performance for detecting [EW] error codes alone
8
Manual features Automatic extraction
Baseline SALT-2 SALT-2 features
Baseline ? Optimized ?
Diagnoses AUC AUC ? AUC ? AUC
ALI/ALN 0.619? 0.723 0.5 0.611 0.94 0.676
ALI/SLI 0.562 0.686 0.5 0.632 0.99 0.671
LN/LI 0.755 0.881 0.5 0.801 0.50 0.801
SLI/TD 0.840 0.951 0.5 0.805 0.99 0.840
? SALT-1; no significantly different baseline features
Table 7: Diagnostic classification AUC using automatically extracted NERRUTTMOD
is identical between the two systems.
The generic error detector yields higher perfor-
mance than either the individual or pair error detec-
tors. Coding training data for the generic detector is
simpler than doing so for the others because it only
involves a single round of binary coding.
4.3 Diagnostic Classification
We repeat the LPO diagnostic classification tasks
using the automatically extracted NERRUTTMOD
feature. We recompute NERRUTTMOD for each
speaker at each iteration, training on all data except
for the two speakers in the test pair, and the speaker
whose NERRUTTMOD feature we are predicting.
The results from this task are shown in Table 7.
As can be seen in Table 7, diagnostic classifica-
tion performance using the automatically extracted
the NERRUTTMOD feature is markedly lower than
when we extracted this feature from manual codes.
However, raising the probability threshold ? at
which utterances are counted as containing an er-
ror from its default value of 0.5, improves diagnos-
tic classification performance for all but one pair
(LN/LI). This is because increasing the probability
threshold at which we count an utterance as hav-
ing an error improves in NERRUTTMOD detection.
For example, in the ALI/SLI group, using the de-
fault ? = 0.5, and a leave-one-out scenario, we can
automatically extract NERRUTTMOD with a preci-
sion/recall score of 0.19/0.47. When we increase ?
to 0.99, the precision and recall become 0.23/0.24.
Even though there is a massive drop in recall, the
improvement in precision is able to boost diagnostic
classification performance.
In all but one pair (SLI/TD), the automati-
cally extracted NERRUTTMOD feature improves
classification over the baseline, even though the
NERRUTTMOD extractor performs poorly in terms
of intrinsic evaluation, with an F1 score of 0.180.
These results are in line with the experiments per-
forming diagnostic classification with an artificially
noisy NERRUTTMOD feature (see Section 3.7).
These results also demonstrate that the automati-
cally extracted values of NERRUTTMOD are suffi-
ciently correlated with the true values of this feature
to be of some diagnostic utility.
5 Conclusions
We have found that the SALT codes provide use-
ful information for distinguishing between certain
diagnostic groups, but not all of them. Specifi-
cally, and not surprisingly given SALT?s focus on
language disorders and not generally on atypical
language use characteristic of ASD, adding SALT-
derived features to baseline features added little
to ASD/nASD, ALI/SLI, or ALN/TD classifica-
tion accuracy, but added substantially to SLI/TD,
ALI/ALN, and LN/LI classification accuracy. Fur-
thermore, we found that a simplified coding schema
is almost as useful as the complete one for differ-
entiating between these groups. Finally, we have
proposed a simple method to automatically extract
a variant of the most useful SALT-derived feature,
NERRUTTMOD, which is a count of sentences that
contain any of three types of errors (omitted mor-
phemes or words, and generic word-level errors).
Although this feature?s utility degrades when ex-
tracted automatically, it still has considerable dis-
criminative value.
In future work, we will investigate the util-
ity of more sophisticated features for extracting
NERRUTTMOD and other SALT-derived features.
We will also investigate the utility of other linguistic
features, for example parse structure, for the diag-
nostic classification task. Finally, we will also con-
sider whether we can perform the diagnostic classi-
fication task more effectively using cascaded binary
classifiers (for example language impaired vs. lan-
guage normal), as opposed to having a classifier for
every diagnostic pair.
9
References
Antti Airola, Tapio Pahikkala, Willem Waegeman,
Bernard De Baets, and Tapio Salakoski. 2011. An ex-
perimental comparison of cross-validation techniques
for estimating the area under the roc curve. Computa-
tional Statistics & Data Analysis, 55(4):1828?1844.
American Psychiatric Association. 2000. DSM-IV-TR:
Diagnostic and Statistical Manual of Mental Disor-
ders. American Psychiatric Publishing, Washington,
DC, 4th edition.
Sibel Kazak Berument, Michael Rutter, Catherine Lord,
Andrew Pickles, and Anthony Bailey. 1999. Autism
screening questionnaire: diagnostic validity. The
British Journal of Psychiatry, 175(5):444?451.
Andrew Caines and Paula Buttery. 2010. You talking to
me?: A predictive model for zero auxiliary construc-
tions. In Proceedings of the 2010 Workshop on NLP
and Linguistics: Finding the Common Ground, pages
43?51. Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 173?
180. Association for Computational Linguistics.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American
chapter of the Association for Computational Linguis-
tics conference, pages 132?139. Morgan Kaufmann
Publishers Inc.
Corinna Cortes, Mehryar Mohri, and Ashish Rastogi.
2007. An alternative ranking problem for search en-
gines. In Proceedings of WEA-2007, LNCS 4525,
pages 1?21. Springer-Verlag.
Keyur Gabani, Melissa Sherman, Thamar Solorio, Yang
Liu, Lisa M Bedore, and Elizabeth D Pena. 2009.
A corpus-based approach for the prediction of lan-
guage impairment in monolingual english and spanish-
english bilingual children. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 46?55. Association
for Computational Linguistics.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The weka data min-
ing software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
K. Hassanali and Y. Liu. 2011. Measuring language de-
velopment in early childhood education: a case study
of grammar checking in child language transcripts. In
Proceedings of the 6th Workshop on Innovative Use
of NLP for Building Educational Applications, pages
87?95. Association for Computational Linguistics.
Margaret M Kjelgaard and Helen Tager-Flusberg. 2001.
An investigation of language impairment in autism:
Implications for genetic subgroups. Language and
cognitive processes, 16(2-3):287?308.
Catherine Lord, Michael Rutter, PC DiLavore, and Susan
Risi. 2002. Autism diagnostic observation schedule:
ADOS. Western Psychological Services.
J. Miller and R. Chapman. 1985. Systematic analysis of
language transcripts. Madison, WI: Language Analy-
sis Laboratory.
Jon F. Miller, Karen Andriacchi, and Ann Nockerts.
2011. Assessing language production using SALT soft-
ware: A Clinician?s Guide to Language Sample Anal-
ysis. SALT Software, LLC.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-
nay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825?
2830.
Emily Prudhommeaux and Masoud Rouhizadeh. 2012.
Automatic detection of pragmatic deficits in children
with autism. In Proceedings of the 3rd Workshop on
Child, Computer and Interaction (WOCCI 2012).
Eleanor Messing Semel, Elisabeth Hemmersam Wiig,
and Wayne Secord. 2003. Clinical evaluation of lan-
guage fundamentals. The Psychological Corporation,
A Harcourt Assessment Company, Toronto, Canada,
fourth edition.
Helen Tager-Flusberg, Sally Rogers, Judith Cooper, Re-
becca Landa, Catherine Lord, Rhea Paul, Mabel Rice,
Carol Stoel-Gammon, Amy Wetherby, and Paul Yoder.
2009. Defining spoken language benchmarks and se-
lecting measures of expressive language development
for young children with autism spectrum disorders.
Journal of Speech, Language and Hearing Research,
52(3):643.
Jan PH van Santen, Emily T Prud?hommeaux, Lois M
Black, and Margaret Mitchell. 2010. Computational
prosodic markers for autism. Autism, 14(3):215?236.
David Williams, Nicola Botting, and Jill Boucher. 2008.
Language in autism and specific language impair-
ment: Where are the links? Psychological Bulletin,
134(6):944.
10
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 82?88,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Experimental Results on the Native Language Identification Shared Task
Amjad Abu-Jbara, Rahul Jha, Eric Morley, Dragomir Radev
Department of EECS
University of Michigan
Ann Arbor, MI, USA
[amjbara, rahuljha, eamorley, radev]@umich.edu
Abstract
We present a system for automatically iden-
tifying the native language of a writer. We
experiment with a large set of features and
train them on a corpus of 9,900 essays writ-
ten in English by speakers of 11 different lan-
guages. our system achieved an accuracy of
43% on the test data, improved to 63% with
improved feature normalization. In this paper,
we present the features used in our system, de-
scribe our experiments and provide an analysis
of our results.
1 Introduction
The task of Native Language Identification (NLI)
is the task of identifying the native language of a
writer or a speaker by analyzing their writing in
English. Previous work in this area shows that
there are several linguistic cues that can be used
to do such identification. Based on their native
language, different speakers tend to make different
kinds of errors pertaining to spelling, punctuation,
and grammar (Garfield, 1964; Wong and Dras, 2009;
Kochmar, 2011). We describe the complete set of
features we considered in Section 4. We evaluate
different combinations of these features, and differ-
ent ways of normalizing them in Section 5.
There are many possible applications for an NLI
system, as noted by Kochmar (2011): finding the
origins of anonymous text; error correction in var-
ious tasks including speech recognition, part-of-
speech tagging, and parsing; and in the field of sec-
ond language acquisition for identifying learner dif-
ficulties. We are most interested in statistical ap-
proaches to this problem because it may point to-
wards fruitful avenues of research in language and
sound transfer, which are how people apply knowl-
edge of their native language, and its phonology
and orthography, respectively, to a second language.
For example, Tsur and Rappoport (2007) found that
character bigrams are quite useful for NLI, which
led them to suggest that second language learners?
word choice may in part be driven by their native
languages. Analysis of such language and sound
translation patterns might be useful in understand-
ing the process of language acquisition in humans.
2 Previous Work
The work presented in this paper was done as part
of the NLI shared task (Tetreault et al, 2013), which
is the first time this problem has been the subject
of a shared task. However, several researchers have
investigated NLI and similar problems. Authorship
attribution, a related problem, has been well stud-
ied in the literature, starting from the seminal work
on disputed Federalist Papers by Mosteller and Wal-
lace (1964). The goal of authorship attribution is
to assign a text to one author from a candidate set
82
of authors. This technique has many applications,
and has recently been used to investigate terrorist
communication (Abbasi and Chen, 2005) and dig-
ital crime (Chaski, 2005). The goal of NLI some-
what similar to authorship attribution, in that NLI
attempts to distinguish between candidate commu-
nities of people who share a common cultural and
linguistic background, while authorship attribution
distinguishes between candidate individuals.
In the earliest treatment of this problem, Koppel
et al (2005) used stylistic text features to identify
the native language of an author. They used features
based on function words, character n-grams and er-
rors and idiosyncrasies such as spelling errors and
non-standard syntactic constructions. They exper-
imented on a dataset with essays written by non-
native English speakers from five countries, Russia,
Czech Republic, Bulgaria, France and Spain, with
258 instances from each dataset. They trained a
multi-class SVM model using the above features and
reported 10-fold cross validation accuracy of 80.2%.
Tsur and Rappoport (2007) studied the problem
of NLI with a focus on language transfer, i.e. how
a seaker?s native language affects the way in which
they acquire a second language, an important area in
Second Language Acquisition research. Their fea-
ture analysis showed that character bigrams alone
can lead to a classification accuracy of about 66%
in a 5-class task. They concluded that the choice of
words people make when writing in a second lan-
guage is highly influenced by the phonology of their
native language.
Wong and Dras (2009) studied syntactic errors de-
rived from contrastive analysis as features for NLI.
They used the five languages from Koppel et al
(2008) along with Chinese and Japanese, but did not
find an improvement in classification accuracy by
adding error features based on contrastive analysis.
Later, Wong and Dras (2011) studied a more gen-
eral set of syntactic features and showed that adding
these features improved the accuracy significantly.
They also investigated classification models based
on LDA (Wong et al, 2011), but did not find them
to be useful overall. They did, however, notice that
some of the topics were capturing information that
would be useful for identifying particular native lan-
guages. They also proposed the use of adaptor gram-
mars (Johnson et al, 2007), which are a generaliza-
tion of probabilistic context-free grammars, to cap-
ture collocational pairings. In a later paper, Wong
et al explored the use of adapter grammars in de-
tail (Wong et al, 2012) and showed that an exten-
sion of adaptor grammars to discover collocations
beyond lexical words can produce features useful for
the NLI task.
3 Dataset
The experiments for this paper were performed us-
ing the TOEFL11 dataset (Blanchard et al, 2013)
provided as part of the shared task. The dataset con-
tains essays written in English from native speakers
of 11 languages (Arabic, Chinese, French, German,
Hindi, Italian, Japanese, Korean, Spanish, Telugu,
and Turkish). The corpus contains 12,099 essays per
language sampled evenly from 8 prompts or topics.
This dataset was designed specifically to support the
task of NLI and addresses some of the shortcom-
ings of earlier datasets used for research in this area.
Specifically, the dataset has been carefully selected
in order to maintain consistency in topic distribu-
tions, character encodings and annotations across
the essays from different native languages. The data
was split into three data sets: a training set com-
prising 9,900 essays, a development set comprising
1,100 essays, and a test set comprising 1,100 essays.
4 Approach
We addressed the problem as a supervised, multi-
class classification task. We trained a Support Vector
Machine (SVM) classifier on a set of lexical, syntac-
tic and dependency features extracted from the train-
ing data. We computed the minimum and maximum
values for each of the features and normalized the
values by the range (max - min). Here we describe
the features in turn.
83
Character and Word N-grams Tsur and Rap-
poport (2007) found that character bigrams were
useful for NLI, and they suggested that this may be
due to the writer?s native language influencing their
choice of words. To reflect this, we compute features
using both characters and word N-grams. For char-
acters, we consider 2,3 and 4-grams, with padding
characters at the beginning and end of each sentence.
The features are generated over the entire training
data, i.e., every n-gram occurring in the training data
is used as a feature. Similarly, we create features
with 1,2 and 3-grams of words. Each word n-gram
is used as a separate feature. We explore both binary
features for each character or word n-gram, as well
as normalized count features.
Part-Of-Speech N-grams Several investigations,
for example those conducted by Kochmar (2011)
and Wong and Dras (2011), have found that part-of-
speech tags can be useful for NLI. Therefore we in-
clude part-of-speech (POS) n-grams as features. We
parse the sentences with the Stanford Parser (Klein
and Manning, 2003) and extract the POS tags. We
use binary features describing the presence or ab-
sence of POS bigrams in a document, as well as nu-
merical features describing their relative frequency
in a document.
Function Words Koppel et al (2005) found that
function words can help identify someone?s native
language. To this end, we include a categorical fea-
ture for the presence of function words that are in-
cluded in list of 321 function words.
Use of punctuation Based on our experience
with speakers of native languages, as well as
Kochmar?s (2011) observations of written English
produced by Germanic and Romance language
speakers, we suspect that speakers of different native
languages use punctuation in different ways, pre-
sumably based on the punctuation patterns in their
native language. For example, comma placement
differs between German and English, and neither
Chinese nor Japanese requires a full stop at the end
of every sentence. To capture these kinds of patterns,
we create two features for each essay: the number of
punctuation marks used per sentence, and the num-
ber of punctuation marks used per word.
Number of Unique Stems Speakers of different
native languages might differ in the amount of vo-
cabulary they use when communicating in English.
We capture this by counting the number of unique
stems in each essay and using this as an additional
feature. The hypothesis here is that depending on the
similarity of the native language with English, the
presence of common words, and other cultural cues,
people with different native language might have ac-
cess to different amounts of vocabulary.
Misuse of Articles We count instances in which
the number of an article is inconsistent with the as-
sociated noun. To do so, we fist identify all the det
dependency relations in the essay. We then com-
pute the ratio of det relations between ?a? or ?an?
and a plural noun (NNS), to all det relations. We
also count the ratio of det relations between ?a? or
?an? and an uncountable noun, to all det relations.
We do this using a list of 288 uncountable nouns.1
Capitalization The writing systems of some lan-
guages in the data set, for example Telugu, do not
include capitalization. Furthermore, other languages
may use capitalization quite differently from En-
glish, for example German, in which all nouns are
capitalized, and French, in which nationalities are
not. Character capitalization mistakes may be com-
mon in the text written by the speakers of such lan-
guages. We compute the ratio of words with at least
two letters that are written in all caps to identify ex-
cessive capitalization. We also count the relative fre-
quency of capitalized words that appear in the mid-
dle of a sentence that are not tagged as proper nouns
by the part of speech tagger.
Tense and Aspect Frequency Verbal tense and
aspect systems vary widely between languages. En-
glish has obligatory tense (past, present, future) and
1http://www.englishclub.com/vocabulary/nouns-
uncountable-list.htm
84
aspect (imperfect, perfect, progressive) marking on
verbs. Other languages, for example French, may
require verbs to be marked for tense, but not as-
pect. Still other languages, for example Chinese,
may use adverbials and temporal phrases to com-
municate temporal and aspectual information. To
attempt to capture some of the ways learners of En-
glish may be influenced by their native language?s
system of tense and aspect, we compute two fea-
tures. First, we compute the relative frequency of
each tense and aspect in the article from the counts
of each verb POS tags (ex. VB, VBD, VBG). We
also compute the percentage of sentences that con-
tain verbs of different tenses or aspect, again using
the verb POS tags.
Missing Punctuation We compute the relative
frequency of sentences that include an introductory
phrase (e.g. however, furthermore, moreover) that is
not followed by a comma. We also count the relative
frequency of sentences that start with a subordinat-
ing conjunction (e.g. sentences starting with if, after,
before, when, even though, etc.), but do not contain
a comma.
Average Number of Syllables We count the num-
ber of syllables per word and the ratio of words with
three or more syllables. To count the number of syl-
lables in a word, we used a perl module that esti-
mates the number of syllables by applying a set of
hand-crafted rules.2.
Arc Length We calculate several features pertain-
ing to dependency arc length and direction. We
parse each sentence separately, using the Stanford
Dependency Parser, and then compute a single value
for each of these features for each document. First,
we simply compute the percentage of arcs that point
left or right (PCTARCL, PCTARCR). We also com-
pute the minumum, maximum, and mean depen-
dency arc length, ignoring arc direction. We also
compute similar features for typed dependencies:
the minimum, maximum, and mean dependency arc
2http://search.cpan.org/dist/Lingua-EN-
Syllable/Syllable.pm
length for each typed dependency; and the percent-
age of arcs for each typed dependency that go to the
left or right.
Downtoners and Intensifiers We compute three
features to describe the use of downtoners, and three
for intensifiers in each document. First, we count the
number of downtoners or intensifiers in a given doc-
ument.3 We normalize this count by the number of
tokens, types, and sentences in the document to yield
the three features capturing the use of downtoners or
intensifiers.
Production Rules We compute a set of features to
describe the relative frequency of production rules
in a given document. First, we parse each sentence
using the Stanford Parser, using the default English
PCFG (Klein and Manning, 2003). We then count
all non-terminal production rules in a given docu-
ment, and report the relative frequency of each pro-
duction rule in that document.
Subject Agreement We count the number of sen-
tences in which there appears to be a mistake in sub-
ject agreement. To do this, we first identify nsubj
and nsubjpass dependency relationships. Of these
dependencies, we count ones meeting the following
criteria as mistakes: a third person singular present
tense verb with a nominal that is not third person
singular, and a third person singular subject with a
present tense verb not marked as third person sin-
gular. We then normalize the count of errors by the
total number of nsubj and nsubj pass dependencies
in the document, and the number of sentences in the
document to produce two features.
Words per Sentence We compute both the num-
ber of tokens per line and the number of types per
3The words we count as downtoners are: ?almost?, ?alot?,
?a lot?, ?barely?, ?a bit?, ?fairly?, ?hardly?, ?just?, ?kind of?,
?least?, ?less?, ?merely?, ?mildly?, ?nearly?, ?only?, ?partially?,
?partly?, ?practically?, ?rather?, ?scarcely?, ?sort of?, ?slightly?,
and ?somewhat?. The intensifiers are: ?a good deal?, ?a great
deal?, ?absolutely?, ?altogether?, ?completely?,?enormously?,
?entirely?, ?extremely?, ?fully?, ?greatly?, ?highly?, ?intensely?,
?more?, ?most?, ?perfectly?, ?quite?, ?really?, ?so?, ?strongly?,
?super?, ?thoroughly?, ?too?, ?totally?, ?utterly?, and ?very?.
85
line.
Topic Scores We construct an unsupervised topic
model for all of the documents using Mallet (Mc-
Callum, 2002) with 100 topics, dirichlet hyperpa-
rameter reestimation every 10 rounds, and all other
options set to default values. We then use the topic
weights as features.
Passive Constructions We count the number of
times an author uses passive constructions by count-
ing the number of nsubjpass dependencies in each
document. We normalize this count in two ways to
produce two different features: dividing by the num-
ber of sentences, and dividing by the total number of
nsubj and nsubjpass dependencies.
5 Experiments and Results
We used weka (Hall et al, 2009) and libsvm (Chang
and Lin, 2011) to run the experiments. The classi-
fication was done using an SVM classifier. We ex-
perimented with different SVM kernels and different
values for the cost parameter. The best performance
was achieved with a linear kernel and cost = 0.001.
We trained the model using the combination of the
training and the development sets. We submitted the
output of the system to the NLI shared task work-
shop. Our system achieved 43.3% accuracy. Table 1
shows the confusion matrix and the precision, recall,
and F-measure for each language. After the NLI
submission deadline, we noticed that we our system
was not handling the normalization of the features
properly which resulted in the poor performance.
After fixing the problem, our system achieved 63%
accuracy on both test data and 10-fold cross valida-
tion on the entire data.
6 Analysis
We did feature analysis on the training and devel-
opment data sets using the Chi-squared test. Our
feature analysis shows that the most important fea-
tures for the classifier were topic models, charac-
ter n-grams of all orders, word unigrams and bi-
grams, POS bigrams, capitalization features, func-
tion words, production rules, and arc length. These
results are consistent with those presented in previ-
ous work done on this task.
Looking at the confusion matrix in Figure 1, we
see that Korean and Japanese were the most com-
monly confused pair of languages. Hindi and Tel-
ugu, two languages from the Indian subcontinent,
were also often confused. To analyze this further,
we did another experiment by training just a binary
classifier on Korean and Japanese using the exact
same feature set as earlier. We achieved a 10-fold
cross validation accuracy of 83.3% on this classifi-
cation task. Thus, given just these two languages,
we were able to obtain high classification accuracy.
This suggests that a potentially fruitful strategy for
NLI systems might be to fuse often-confused pairs,
such as Korean/Japanese and Hindi/Telugu, into sin-
gleton classes for the initial run, and then run a sec-
ond classifier to do a more fine grained classification
within these higher level classes.
When doing feature analysis for these two lan-
guages, we found that the character bigrams rep-
resenting the country names were some of the top
features used for classification. For example ?Kor?
occurred as a trigram frequently in essays from na-
tive language speakers of Korean. Based on this, we
designed a small experiment where we created fea-
tures corresponding to the country name associated
with each native language, e.g., ?Korea?, ?China?,
?India?, ?France?, etc. For Arabic, we used a list of
22 countries where Arabic is spoken. Just using this
feature, we obtained a 10-fold cross validation accu-
racy of 21.3% on the development set. This suggests
that in certain genres, one may be able to leverage in-
formation conveying geographical and demographic
attributes for NLI.
7 Conclusion
In this paper, we presented a supervised system for
the task of Native Language Identification. We de-
scribe and motivate several features for this task
and report results of supervised classification using
these features on a test data set consisting of 11 lan-
86
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision Recall F-measure
ARA 41 7 8 3 6 2 3 5 10 7 8 44.6% 41.0% 42.7%
CHI 6 38 5 2 2 8 15 8 3 3 10 40.0% 38.0% 39.0%
FRE 8 6 43 8 1 14 2 4 6 1 7 39.1% 43.0% 41.0%
GER 3 3 10 49 4 9 1 7 6 0 8 54.4% 49.0% 51.6%
HIN 5 2 6 9 34 0 3 1 3 32 5 47.9% 34.0% 39.8%
ITA 5 3 10 5 1 52 2 1 17 0 4 46.0% 52.0% 48.8%
JPN 3 11 0 1 1 3 49 26 1 1 4 37.4% 49.0% 42.4%
KOR 2 6 6 1 1 2 35 40 1 1 5 38.1% 40.0% 39.0%
SPA 4 6 14 1 1 17 6 2 38 0 11 40.9% 38.0% 39.4%
TEL 9 7 3 4 18 2 2 2 2 48 3 51.1% 48.0% 49.5%
TUR 6 6 5 7 2 4 13 9 6 1 41 38.7% 41.0% 39.8%
Accuracy = 43.0%
Table 1: The results of our original submission to the NLI shared task on the test set. These results reflect the
performance of the system that does not normalize the features properly
guages provided as part of the NLI shared task. We
found that our classifier often confused two pairs
of languages that are spoken near one another, but
are linguistically unrelated: Hindi/Telugu and Ko-
rean/Japanese. We found that we could obtain high
classification accuracy on these two pairs of lan-
guages using a binary classifier trained on just these
pairs. During our feature analysis, we also found
that certain features that happened to convey geo-
graphical and demographic information were also
informative for this task.
References
Ahmed Abbasi and Hsinchun Chen. 2005. Apply-
ing authorship analysis to extremist-group web fo-
rum messages. IEEE Intelligent Systems, 20(5):67?75,
September.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27.
Carole E. Chaski. 2005. Who?s at the keyboard: Au-
thorship attribution in digital evidence investigations.
International Journal of Digital Evidence, 4:2005.
Eugene Garfield. 1964. Can citation indexing be auto-
mated?
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Adaptor grammars: A framework for speci-
fying compositional nonparametric bayesian models.
Advances in neural information processing systems,
19:641.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Associ-
ation for Computational Linguistics.
Ekaterina Kochmar. 2011. Identification of a Writer?s
Native Langauge by Error Analysis. Ph.D. thesis.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proceedings of the eleventh ACM
SIGKDD international conference on Knowledge dis-
covery in data mining, pages 624?628, Chicago, IL.
ACM.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2008. Computational methods in authorship attribu-
tion. Journal of the American Society for information
Science and Technology, 60(1):9?26.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Frederick Mosteller and David L. Wallace. 1964. Infer-
ence and Disputed Authorship: The Federalist Papers.
Addison-Wesley, Reading, Mass.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A report on the first native language identification
shared task. In Proceedings of the Eighth Workshop
87
on Innovative Use of NLP for Building Educational
Applications, Atlanta, GA, USA, June. Association for
Computational Linguistics.
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proceed-
ings of the Workshop on Cognitive Aspects of Com-
putational Language Acquisition, CACLA ?07, pages
9?16, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive
Analysis and Native Language Identification. In Pro-
ceedings of the Australasian Language Technology As-
sociation Workshop 2009, pages 53?61, Sydney, Aus-
tralia, December.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
Parse Structures for Native Language Identification.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1600?1610, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2011. Topic Modeling for Native Language Identifi-
cation. In Proceedings of the Australasian Language
Technology Association Workshop 2011, pages 115?
124, Canberra, Australia, December.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2012. Exploring Adaptor Grammars for Native Lan-
guage Identification. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 699?709, Jeju Island, Korea,
July. Association for Computational Linguistics.
88
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 69?77,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Challenges in Automating Maze Detection
Eric Morley
CSLU
OHSU
Portland, OR 97239
morleye@gmail.com
Anna Eva Hallin
Department of Communicative
Sciences and Disorders
New York University
New York, NY
ae.hallin@nyu.edu
Brian Roark
Google Research
New York, NY 10011
roarkbr@gmail.com
Abstract
SALT is a widely used annotation ap-
proach for analyzing natural language
transcripts of children. Nine annotated
corpora are distributed along with scoring
software to provide norming data. We ex-
plore automatic identification of mazes ?
SALT?s version of disfluency annotations
? and find that cross-corpus generalization
is very poor. This surprising lack of cross-
corpus generalization suggests substantial
differences between the corpora. This is
the first paper to investigate the SALT cor-
pora from the lens of natural language pro-
cessing, and to compare the utility of dif-
ferent corpora collected in a clinical set-
ting to train an automatic annotation sys-
tem.
1 Introduction
Assessing a child?s linguistic abilities is a critical
component of diagnosing developmental disorders
such as Specific Language Impairment or Autism
Spectrum Disorder, and for evaluating progress
made with remediation. Structured instruments
(?tests?) that elicit brief, easy to score, responses
to a sequence of items are a popular way of per-
forming such assessment. An example of a struc-
tured instrument is the CELF-4, which includes
nineteen multi-item subtests with tasks such as
object naming, word definition, reciting the days
of the week, or repeating sentences (Semel et al.,
2003). Over the past two decades, researchers
have discussed the limitations of standardized tests
and how well they tap into different language im-
pairments. Many have advocated the potential
benefits of language sample analysis (LSA) (John-
ston, 2006; Dunn et al., 1996). The analysis of
natural language samples may be particularly ben-
eficial for language assessment in ASD, where
pragmatic and social communication issues are
paramount yet may be hard to assess in a conven-
tional test format (Tager-Flusberg et al., 2009).
At present, the expense of LSA prevents it from
being more widely used. Heilmann (2010), while
arguing that LSA is not too time-consuming, esti-
mates that each minute of spoken language takes
five to manually transcribe and annotate. At this
rate, it is clearly impractical for clinicians to per-
form LSA on hours of speech. Techniques from
natural language processing could be used to build
tools to automatically annotate transcripts, thus fa-
cilitating LSA.
Here, we evaluate the utility of a set of anno-
tated corpora for automating a key annotation in
the de facto standard annotation schema for LSA:
the Systematic Analysis of Language Transcripts
(SALT) (Miller et al., 2011). SALT comprises a
scheme for coding transcripts of recorded speech,
together with software that tallies these codes,
computes scores describing utterance length and
error counts, among a range of other standard mea-
sures, and compares these scores with normative
samples. SALT codes indicate bound morphemes,
several types of grammatical errors (for example
using a pronoun of the wrong gender or case), and
mazes, which are defined as ?filled pauses, false
starts, and repetitions and revisions of words, mor-
phemes and phrases? (Miller et al., 2011, p. 48).
Mazes have sparked interest in the child lan-
guage disorders literature for several reasons.
They are most often analyzed from a language
processing perspective where the disruptions are
viewed as a consequence of monitoring, detect-
ing and repairing language, potentially including
speech errors (Levelt, 1993; Postma and Kolk,
1993; Rispoli et al., 2008). Several studies have
found that as grammatical complexity and utter-
ance length increase, the number of mazes in-
creases in typically developing children and chil-
dren with language impairments (MacLachlan and
69
Chapman, 1988; Nippold et al., 2008; Reuter-
ski?old Wagner et al., 2000; Wetherell et al., 2007).
Mazes in narrative contexts have been shown
to differ between typical children and children
with specific language impairment (MacLachlan
and Chapman, 1988; Thordardottir and Weismer,
2001), though others have not found reliable group
differences (Guo et al., 2008; Scott and Windsor,
2000). Furthermore, outside the potential useful-
ness of looking at mazes in themselves, mazes al-
ways have to be detected and excluded in order
to calculate other standard LSA measures such
as mean length of utterance and type or token
counts. Mazes also must be excluded when ana-
lyzing speech errors, since some mazes are in fact
self-corrections of language or speech errors.
Thus, automatically delimiting mazes could be
clinically useful in several ways. First, if mazes
can be automatically detected, standard measures
such as token and type counts can be calculated
with ease, as noted above. Automatic maze detec-
tion could also be a first processing step for au-
tomatically identifying errors: error codes cannot
appear in mazes, and certain grammatical errors
may be easier to identify once mazes have been
excised. Finally, after mazes have been identified,
further analysis of the mazes themselves (e.g. the
number of word in mazes, and the placement of
mazes in the sentence) can provide supplementary
information about language formulation abilities
and word retrieval abilities (Miller et al., 2011, p.
87-89).
We use the corpora included with the SALT
software to train maze detectors. These are the
corpora that the software uses to compute refer-
ence counts. These corpora share several charac-
teristics we expect to be typical of clinical data:
they were collected under a diverse set of circum-
stances; they were annotated by different groups;
the annotations ostensibly follow the same guide-
lines; and the annotations were not designed with
automation in mind. We will investigate whether
we can extract usable generalizations from the
available data, and explore how well the auto-
mated system performs, which will be of interest
to clinicians looking to expedite LSA.
2 Background
Here we provide an overview of SALT and maze
annotations. We are not aware of any attempts
to automate maze detection, although maze de-
tection closely resembles the well-established task
of edited word detection. We also provide an
overview of the corpora included with the SALT
software, which are the ones we will use to train
maze detectors.
2.1 SALT and Maze Annotations
The approach used in SALT has been in wide use
for nearly 30 years (Miller and Chapman, 1985),
and now also exists as a software package
1
pro-
viding transcription and coding support along with
tools for aggregating statistics for manual codes
over the annotated corpora and comparing with
age norms. The SALT software is not the focus of
this investigation, so we do not discuss it further.
Following the SALT guidelines, speech should
be transcribed orthographically and verbatim. The
transcript must include and indicate: the speaker
of each utterance, partial words or stuttering, over-
lapping speech, unintelligible words, and any non-
speech sounds from the speaker. Even atypical
language, for example neologisms (novel words)
or grammatical errors (for example ?her went?)
should be written as such.
There are three broad categories of SALT anno-
tations: indicators of 1) certain bound morphemes,
2) errors, and 3) mazes. In general, verbal suffixes
that are visible in the surface form (for example
-ing in ?going?) and clitics that appear with an un-
modified root (so for example -n?t in ?don?t?, but
not the -n?t in ?won?t?) must be indicated. SALT
includes various codes to indicate grammatical er-
rors including, but not limited to: overgeneral-
ization errors (?goed?), extraneous words, omit-
ted words or morphemes, and inappropriate ut-
terances (e.g. answering a yes/no question with
?fight?). For more information on these standard
annotations, we refer the reader to the SALT man-
ual (Miller et al., 2011).
Here, we are interested in automatically delim-
iting mazes. In SALT, filled pauses, repetitions
and revisions are included in the umberella term
?mazes? but the manual does not include defini-
tions for any of these categories. In SALT, mazes
are simply delimited by parentheses; they have no
internal structure, and cannot be nested. Contigu-
ous spans of maze words are delimited by a single
set of parentheses, as in the following utterance:
(1) (You have you have um there/?s only)
there/?s ten people
1
http://www.saltsoftware.com/
70
To be clear, we define the task of automatically ap-
plying maze detections as taking unannotated tran-
scripts of speech as input, and then outputting a
binary tag for each word that indicates whether or
not it is in a maze.
2.2 Edited Word Detection
Although we are not aware of any previous work
on automating maze detection, there is a well-
established task in natural language processing
that is quite similar: edited word detection. The
goal of edited word detection is to identify words
that have been revised or deleted by the speaker,
for example ?to Dallas? in the utterance ?I want to
go to Dallas, um I mean to Denver.?. Many in-
vestigations have approached edited word detec-
tion from what Nakatani et al. (1993) have termed
?speech-first? perspective, meaning that edited de-
tection is performed with features from the speech
signal in addition to a transcript. These ap-
proaches, however, are not applicable to the SALT
corpora, because they only contain transcripts. As
a result, we must adopt a text-first approach to
maze detection, using only features extracted from
a transcript.
The text-first approach to edited word detec-
tion is well established. One of the first investi-
gations taking a text-first approach was conducted
by Charniak and Johnson (2001). There, they
used boosted linear classifiers to identify edited
words. Later, Johnson and Charniak (2004) im-
proved upon the linear classifiers? performance
with a tree adjoining grammar based noisy chan-
nel model. Zwarts and Johnson (2011) improve
the noisy channel model by adding in a reranker
that leverages features extracted with the help of a
large language model.
Qian and Liu (2013) have developed what is
currently the best-performing edited word detec-
tor, and it takes a text-first approach. Unlike the
detector proposed by Zwarts and Johnson, Qian
and Liu?s does not rely on any external data. Their
detector operates in three passes. In the first pass,
filler words (?um?, ?uh?, ?I mean?, ?well?, etc.) are
detected. In the second and third passes, edited
words are detected. The reason for the three passes
is that in addition to extracting features (mostly
words and part of speech tags) from the raw tran-
script, the second and third steps use features ex-
tracted from the output of previous steps. An ex-
ample of such features is adjacent words from the
utterance with filler words and some likely edited
words removed.
3 Overview of SALT Corpora
We explore nine corpora included with the SALT
software. Table 1 has a high level overview of
these corpora, showing where each was collected,
the age ranges of the speakers, and the size of each
corpus both in terms of transcripts and utterances.
Note that only utterances spoken by the child are
counted, as we throw out all others.
Table 1 shows several divisions among the cor-
pora. We see that one group of corpora comes
from New Zealand, while the majority come from
North America. All of the corpora, except for Ex-
pository, include children at very different stages
of language development.
Four research groups were responsible for the
transcriptions and annotations of the corpora in
Table 1. One group produced the CONVERSA-
TION, EXPOSITORY, NARRATIVESSS, and NAR-
RATIVESTORYRETELL corpora. Another was
responsible for all of the corpora from New
Zealand. Finally, the ENNI and GILLAMNT cor-
pora were transcribed and annotated by two dif-
ferent groups. For more details on these cor-
pora, how they were collected, and the anno-
tators, we refer the reader to the SALT web-
site at http://www.saltsoftware.com/
resources/databases.html.
Some basic inspection reveals that the corpora
can be put into three groups based on the me-
dian utterance lengths, and the distribution of ut-
Table 1: Description of SALT corpora
Corpus Transcripts Utterances Age Range Speaker Location
CONVERSATION 584 82,643 2;9 ? 13;3 WI & CA
ENNI 377 56,108 3;11 ? 10;0 Canada
EXPOSITORY 242 4,918 10;7 ? 15;9 WI
GILLAMNT 500 40,102 5;0 ? 11;11 USA
NARRATIVESSS 330 16,091 5;2 ? 13;3 WI & CA
NARRATIVESTORYRETELL 500 14,834 4;4 ? 12;8 WI & CA
NZCONVERSATION 248 25,503 4;5 ? 7;7 NZ
NZPERSONALNARRATIVE 248 20,253 4;5 ? 7;7 NZ
NZSTORYRETELL 264 2,574 4;0 ? 7;7 NZ
71
terance
2
lengths, following the groups Figure 1,
with the EXPOSITORY and CONVERSATION cor-
pora in their own groups. Note that the counts
in Figure 1 are of all of the words in each ut-
terance, including those in mazes. We see that
the corpora in Group A have a modal utterance
length ranging from seven to ten words. There are
many utterances in these corpora that are shorter
or longer than the median length. Compared to
the corpora in Group A, those in Group B have
a shorter modal utterance length, and fewer long
utterances. In Figure 1, we see that the CONVER-
SATION corpus consists mostly of very short utter-
ances. At the other extreme is the EXPOSITORY
corpus, which resembles the corpora in Group A
in terms of modal utterance length, but which gen-
erally contains longer utterances than any of the
other corpora.
4 Maze Detection Experiments
4.1 Maze Detector
We carry out our experiments in automatic maze
detection using a statistical maze detector that
learns to identify mazes from manually labeled
data using features extracted from words and auto-
matically predicted part of speech tags. The maze
detector uses the feature set shown in Table 2.
This set of features is identical to the ones used by
the ?filler word? detector in Qian and Liu?s disflu-
ency detector (2013). We also use the same clas-
2
All of these corpora are reported to have been segmented
into c-units, which is defined as ?an independent clause with
its modifiers? (Miller et al., 2011).
Table 2: Feature templates for maze word detection, follow-
ing Qian and Liu (2013). We extract all of the above features
from both words and POS tags, albeit separately. t
0
indicates
the current word or POS tag, while t
?1
is the previous one
and t
1
is the following. The function I(a, b) is 1 if a and b
are identical, and otherwise 0. y
?1
is the tag predicted for the
previous word.
Category Features
Unigrams t
?2
, t
?1
, t
0
, t
1
, t
2
Bigrams t
?1
t
0
, t
0
t
1
Trigrams t
?2
t
?1
t
0
, t
?1
t
0
t
1
, t
0
t
1
t
2
Logic Unigrams I(t
i
, t
0
), I(p
i
, p
0
);
?4 ? i ? 4; i 6= 0
Logic Bigrams I(t
i?2
t
i?1
, t
?1
t
0
)
I(t
i
t
i+1
, t
0
t
i+1
);
?4 ? i ? 4; i 6= 0
Predicted tag y
?1
(a) Group A
(b) Group B
(c) Others
Figure 1: Histograms of utterance length (including words
in mazes) in SALT corpora
sifier as the second and third steps of their system:
the Max Margin Markov Network ?M3N? classi-
fier in the pocketcrf toolkit (available at http://
code.google.com/p/pocketcrf/). The
M3N classifier is a kernel-based classifier that is
able to leverage the sequential nature the data in
this problem (Taskar et al., 2003). We use the fol-
lowing label set: S-O (not in maze); S-M (sin-
gle word maze); B-M (beginning of multi-word
72
maze); I-M (in multi-word maze); and E-M (end
of multi-word maze). The M3N classifier allows
us to set a unique penalty for each pair of con-
fused labels, for example penalizing an erroneous
prediction of S-O (failing to identify maze words)
more heavily than spurious predictions of maze
words (all -M labels). This ability is particularly
useful for maze detection because maze words are
so infrequent compared to words that are not in
mazes.
4.2 Evaluation
We split each SALT corpus into training, develop-
ment, and test partitions. Each training partition
contains 80% of the utterances the corpus, while
the development and test partitions each contain
10% of the utterances. We use the development
portion of each corpus to set the penalty matrix
system to roughly balance precision and recall.
We evaluate maze detection in terms of both
tagging performance and bracketing performance,
both of which are standard forms of evaluation
for various tasks in the Natural Language Pro-
cessing literature. Tagging performance captures
how effectively maze detection is done on a word-
by-word basis, while bracketing performance de-
scribes how well each maze is identified in its en-
tirety. For both tagging and bracketing perfor-
mance, we count the number of true and false
positives and negatives, as illustrated in Figure 2.
In tagging performance, each word gets counted
once, while in bracketing performance we com-
pare the predicted and observed maze spans. We
use these counts to compute the following metrics:
(P)recision =
tp
tp + fp
(R)ecall =
tp
tp + fn
F1 =
2PR
P + R
Note that partial words and punctuation are both
ignored in evaluation. We exclude punctuation be-
cause punctuation does not need to be included
in mazes: it is not counted in summary statistics
(e.g. MLU, word count, etc.), and punctuation er-
rors are not captured by the SALT error codes.
We exclude partial words because they are always
in mazes, and therefore can be detected trivially
with a simple rule. Furthermore, because par-
tial words are excluded from evaluation, the per-
formance metrics are comparable across corpora,
even if they vary widely in the frequency of partial
words.
For both space and clarity, we do not present
the complete results of every experiment in this
paper, although they are available online
3
. In-
stead, we present the complete baseline results,
and then report F1 scores that are significantly
better than the baseline. We establish statistical
significance by using a randomized paired-sample
test (see Yeh (2000) or Noreen (1989)) to com-
pare the baseline system (system A) and the pro-
posed system (system B). First, we compute the
difference d in F1 score between systems A and B.
Then, we repeatedly construct a random set of pre-
dictions for each input item by choosing between
the outputs of system A and B with equal proba-
bility. We compute the F1 score of these random
predictions, and if it exceeds the F1 score of the
baseline system by at least d, we count the itera-
tion as a success. The significance level is at most
the number of successes divided by one more than
the number of trials (Noreen, 1989).
4.3 Baseline Results
For each corpus, we train the maze detector on
the training partition and test it on the devel-
opment partition. The results of these runs are
in Table 3, which also includes the rank of the
size of each corpus (1 = biggest, 9 = smallest).
We see immediately that our maze detector per-
forms far better on some corpora than on oth-
ers, both in terms of tagging and bracketing per-
formance. We note that maze detection perfor-
mance is not solely determined by corpus size:
tagging performance is substantially worse on the
largest corpus (CONVERSATION) than the small-
3
http://bit.ly/1dtFTPl
Figure 2: Tagging and bracketing evaluation for maze detection. TP = True Positive, FP = False Positive, TN = True Negative,
FN = False Negative
Pred. ( and then it ) oh and then it ( um ) put his wings out .
Gold ( and then it oh ) and then it ( um ) put his wings out .
Tag TP ?3 FN TN ?3 TP TN ?4
Brack. FP, FN TP
73
Tagging Bracketing
Corpus Size Rank P R F1 P R F1
CONVERSATION 1 0.821 0.779 0.800 0.716 0.729 0.723
ENNI 2 0.923 0.882 0.902 0.845 0.837 0.841
EXPOSITORY 8 0.703 0.680 0.691 0.620 0.615 0.618
GILLAMNT 3 0.902 0.907 0.904 0.827 0.843 0.835
NARRATIVESSS 6 0.781 0.768 0.774 0.598 0.679 0.636
NARRATIVESTORYRETELL 7 0.799 0.774 0.786 0.627 0.671 0.649
NZCONVERSATION 4 0.832 0.835 0.838 0.707 0.757 0.731
NZPERSONALNARRATIVE 5 0.842 0.835 0.838 0.707 0.757 0.731
NZSTORYRETELL 9 0.905 0.862 0.883 0.773 0.780 0.776
Table 3: Baseline maze detection performance on development sections of SALT corpora: corpus-specific models
est (NZSTORYRETELL).
4.4 Generic Model
We train a generic model for maze detection on
all of the training portions of the nine SALT cor-
pora. We use the combined development sections
of all of the corpora to tune the loss matrix for bal-
anced precision and recall. We then test the re-
sulting model on the development section of each
SALT corpus, and evaluate in terms of tagging and
bracketing accuracy.
We find that the generic model performs worse
than the baseline in terms of both tagging and
bracketing performance on six of the nine corpora
corpora. The generic model significantly improves
tagging (F1=0.925, p ? 0.0022) on the NZSTO-
RYRETELL corpus, but the improvement in brack-
eting performance is not significant (p ? 0.1635).
There is improvement of both tagging (F1=0.805,
p ? 0.0001) and bracketing (F1=0.677, p ?
0.0025) performance on the NARRATIVESSS cor-
pus. The generic model does not perform better
than the baseline corpus-specific models on any
other corpora.
The poor performance of the generic model is
somewhat surprising, as it is trained with far more
data than any of the corpus-specific models. In
many tasks in natural language processing, in-
creasing the amount of training data improves the
resulting model, although this is not necessarily
the case if the additional data is noisy or out-of-
domain. This suggests two possibilities: 1) the
language in the corpora varies substantially, per-
haps due to the speakers? ages or the activity that
was transcribed; and 2) the maze annotations are
inconsistent between corpora.
4.5 Multi-Corpus Models
It is possible that poor performance of the generic
model relative to the baseline corpus-specific
models can be attributed to systematic differences
between the SALT corpora. We may be able to
train a model for a set of corpora that share particu-
lar characteristics that can outperform the baseline
models because such a model could leverage more
training data. We first evaluate a model for corpora
that contain transcripts collected from children of
similar ages. We also evaluate task-specific mod-
els, specifically a maze-detection model for story
retellings, and another for conversations. These
two types of models could perform well if chil-
dren of similar ages or performing similar tasks
produce mazes in a similar manner. Finally, we
train models for each group of annotators to see
whether systematic variation in annotation stan-
dards between research groups could be respon-
sible for the generic model?s poor performance.
We train all of these models similarly to the
generic model: we pool the training sections of
the selected corpora, train the model, then test on
the development section of each selected corpus.
We use the combined development sections of the
selected corpora to tune the penalty matrix to bal-
ance precision and recall.
Again, we only report F1 scores that are higher
than the baseline model?s, and we test whether
the improvement is statistically significant. We
do not report results where just the precision or
just the recall exceeds the baseline model perfor-
mance, but not F1, because these are typically the
result of model imbalance, favoring precision at
the expense of recall or vice versa. Bear in mind
that we roughly balance precision and recall on the
combined development sets, not each corpus?s de-
velopment set individually.
4.5.1 Age-Specific Model
We train a single model on the following cor-
pora: ENNI, GILLAMNT, NARRATIVESSS, and
NARRATIVESTORYRETELL. As shown in Ta-
ble 1, these corpora contain transcripts collected
from children roughly aged 4-12. In three of the
four corpora, the age-based model performs worse
than the baseline. The only exception is NAR-
74
RATIVESTORYRETELL, for which the age-based
model outperforms the baseline in terms of both
tagging (F1=0.794, p ? 0.0673) and bracketing
(F1=0.679, p ? 0.0062).
4.5.2 Task-Specific Models
We construct two task-specific models for maze
detection: one for conversations, and the other
for narrative tasks. A conversational model
trained on the CONVERSATION and NZCON-
VERSATION corpora does not improve perfor-
mance on either corpus relative to the base-
line. A model for narrative tasks trained on the
ENNI, GILLAMNT, NARRATIVESSS, NARRA-
TIVESTORYRETELL, NZPERSONALNARRATIVE
and NZSTORYRETELL corpora only improves
performance on one of these, relative to the base-
line. Specifically, the narrative task model im-
proves performance on the NARRATIVESSS cor-
pus both in terms of tagging (F1=0.797, p ?
0.0005) and bracketing (F1=0.693, p ? 0.0002).
4.5.3 Research Group-Specific Models
There are two groups of researchers that have
annotated multiple corpora: a group in New
Zealand, which annotated the NZCONVERSA-
TION, NZPERSONALNARRATIVE, and NZSTO-
RYRETELL corpora; and another group in Wis-
consin, which annotated the CONVERSATION,
EXPOSITORY, NARRATIVESSS, and NARRA-
TIVESTORYRETELL corpora. We trained re-
search group-specific models, one for each of
these groups.
Overall, these models do not improve perfor-
mance. The New Zealand research group model
does not significantly improve performance on any
of the corpora they annotated, relative to the base-
line. The Wisconsin research group model yields
significant improvement on the NARRATIVESSS
corpus, both in terms of tagging (F1=0.803, p ?
0.0001) and bracketing (F1=0.699, p ? 0.0001)
performance. Performance on the CONVERSA-
TION and EXPOSITORY corpora is lower with
the Wisconsin research group model than with
the corpus-specific baseline models, while perfor-
mance on NARRATIVESTORYRETELL is essen-
tially the same with the two models.
5 Discussion
We compared corpus-specific models for maze de-
tection to more generic models applicable to mul-
tiple corpora, and found that the generic models
performed worse than the corpus-specific ones.
This was surprising because the more generic
models were able to leverage more training data
than the corpus specific ones, and more training
data typically improves the performance of data-
driven models such as our maze detector. These
results strongly suggest that there are substantial
differences between the nine SALT corpora.
We suspect there are many areas in which the
SALT corpora diverge from one another. One
such area may be the nature of the language: per-
haps the language differs so much between each
of the corpora that it is difficult to learn a model
appropriate for one corpus from any of the oth-
ers. Another potential source of divegence is in
transcription, which does not always follow the
SALT guidelines (Miller et al., 2011). Two of the
idiosyncracies we have observed are: more than
three X?s (or a consonant followed by multiple
X?s) to indicate unintelligble language, instead of
the conventional X, XX, and XXX for unintelligi-
ble words, phrases, and utterances, respectively;
and non-canonical transcriptions of what appear
to be filled pauses, including ?uhm? and ?umhm?.
These idiosyncracies could be straightforward to
normalize using automated methods, but doing so
requires that they be identified to begin with. Fur-
thermore, although these idiosyncracies may ap-
pear to be minor, taken together they may actually
be substantial.
Another potential source of variation between
corpora is likely in the maze annotations them-
selves. SALT?s definition of mazes, ?filled pauses,
false starts, and repetitions and revisions of words,
morphemes and phrases? (Miller et al., 2011, p.
48), is very short, and none of the components
is defined in the SALT manual. In contrast, the
Disfluency Annotation Stylebook for Switchboard
Corpus (Meteer et al., 1995) describes a system
of disfluency annotations over approximately 25
pages, devoting two pages to filled pauses and five
to restarts. The Switchboard disfluency annota-
tions are much richer than SALT maze annota-
tions, and we are not suggesting that they are ap-
propriate for a clinical setting. However, between
the stark contrast in detail of the two annotation
systems? guidelines, and our finding that cross-
corpus models for maze detection perform poorly,
we recommend that SALT?s definition of mazes
and their components be elaborated and clarified.
This would be of benefit not just to those trying to
75
automate the application of SALT annotations, but
also to clinicians who use SALT and depend upon
consistently annotated transcripts.
There are two clear tasks for future research that
build upon these results. First, maze detection per-
formance can surely be improved. We note, how-
ever, that evaluating maze detectors in terms of F1
score may not always be appropriate if such a de-
tector is used in a pipeline. For example, there
may be a minimum acceptable level of precision
for a maze detector used in a preprocessing step
to applying SALT error codes so that maze exci-
sion does not create additional errors. In such a
scenario, the goal would be to maximize recall at
a given level of precision.
The second task suggested by this paper is to ex-
plore the hypothesized differences within and be-
tween corpora. Such exploration could ultimately
result in more rigorous, communicable guidelines
for maze annotations, as well as other annotations
and conventions in SALT. If there are systematic
differences in maze annotations across the SALT
corpora, such exploration could suggest ways of
making the annotations consistent without com-
pletely redoing them.
Acknowledgments
We would like to thank members of the ASD re-
search group at the Center for Spoken Language
Understanding at OHSU, for useful input into this
study: Jan van Santen, Alison Presmanes Hill,
Steven Bedrick, Emily Prud?hommeaux, Kyle
Gorman and Masoud Rouhizadeh. This research
was supported in part by NIH NIDCD award
R01DC012033 and NSF award #0826654. Any
opinions, findings, conclusions or recommenda-
tions expressed in this publication are those of the
authors and do not reflect the views of the NIH or
NSF.
References
Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In Proceed-
ings of the second meeting of the North American
Chapter of the Association for Computational Lin-
guistics on Language technologies, pages 1?9. As-
sociation for Computational Linguistics.
Michelle Dunn, Judith Flax, Martin Sliwinski, and
Dorothy Aram. 1996. The use of spontaneous lan-
guage measures as criteria for identifying children
with specific language impairment: An attempt to
reconcile clinical and research incongruence. Jour-
nal of Speech and Hearing research, 39(3):643.
Ling-yu Guo, J Bruce Tomblin, and Vicki Samel-
son. 2008. Speech disruptions in the narratives
of english-speaking children with specific language
impairment. Journal of Speech, Language, and
Hearing Research, 51(3):722?738.
John J Heilmann. 2010. Myths and realities of lan-
guage sample analysis. SIG 1 Perspectives on Lan-
guage Learning and Education, 17(1):4?8.
Mark Johnson and Eugene Charniak. 2004. A tag-
based noisy-channel model of speech repairs. In
Proceedings of the 42nd Meeting of the Association
for Computational Linguistics (ACL?04), Main Vol-
ume, pages 33?39, Barcelona, Spain, July.
Judith R Johnston. 2006. Thinking about child lan-
guage: Research to practice. Thinking Publications.
Willem JM Levelt. 1993. Speaking: From intention to
articulation, volume 1. MIT press, Cambridge, MA.
Barbara G MacLachlan and Robin S Chapman. 1988.
Communication breakdowns in normal and lan-
guage learning-disabled children?s conversation and
narration. Journal of Speech and Hearing Disor-
ders, 53(1):2.
Marie W Meteer, Ann A Taylor, Robert MacIntyre,
and Rukmini Iyer. 1995. Dysfluency annotation
stylebook for the switchboard corpus. University of
Pennsylvania.
Jon Miller and Robin Chapman. 1985. Systematic
analysis of language transcripts. Madison, WI: Lan-
guage Analysis Laboratory.
Jon F Miller, Karen Andriacchi, and Ann Nockerts.
2011. Assessing language production using SALT
software: A clinician?s guide to language sample
analysis. SALT Software, LLC.
Christine Nakatani and Julia Hirschberg. 1993. A
speech-first model for repair detection and correc-
tion. In Proceedings of the 31st Annual Meeting
of the Association for Computational Linguistics,
pages 46?53, Columbus, Ohio, USA, June. Associ-
ation for Computational Linguistics.
Marilyn A Nippold, Tracy C Mansfield, Jesse L Billow,
and J Bruce Tomblin. 2008. Expository discourse
in adolescents with language impairments: Exam-
ining syntactic development. American Journal of
Speech-Language Pathology, 17(4):356?366.
Eric W Noreen. 1989. Computer intensive methods
for testing hypotheses. an introduction. 1989. John
Wiley & Sons, 2(5):33.
Albert Postma and Herman Kolk. 1993. The covert
repair hypothesis: Prearticulatory repair processes in
normal and stuttered disfluencies. Journal of Speech
and Hearing Research, 36(3):472.
76
Xian Qian and Yang Liu. 2013. Disfluency detection
using multi-step stacked learning. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 820?825, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Christina Reuterski?old Wagner, Ulrika Nettelbladt, Bir-
gitta Sahl?en, and Claes Nilholm. 2000. Conver-
sation versus narration in pre-school children with
language impairment. International Journal of Lan-
guage & Communication Disorders, 35(1):83?93.
Matthew Rispoli, Pamela Hadley, and Janet Holt.
2008. Stalls and revisions: A developmental per-
spective on sentence production. Journal of Speech,
Language, and Hearing Research, 51(4):953?966.
Cheryl M Scott and Jennifer Windsor. 2000. General
language performance measures in spoken and writ-
ten narrative and expository discourse of school-age
children with language learning disabilities. Journal
of Speech, Language & Hearing Research, 43(2).
Eleanor Messing Semel, Elisabeth Hemmersam Wiig,
and Wayne Secord. 2003. Clinical evaluation of
language fundamentals. The Psychological Corpo-
ration, A Harcourt Assessment Company, Toronto,
Canada, fourth edition.
Helen Tager-Flusberg, Sally Rogers, Judith Cooper,
Rebecca Landa, Catherine Lord, Rhea Paul, Ma-
bel Rice, Carol Stoel-Gammon, Amy Wetherby, and
Paul Yoder. 2009. Defining spoken language bench-
marks and selecting measures of expressive lan-
guage development for young children with autism
spectrum disorders. Journal of Speech, Language
and Hearing Research, 52(3):643.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003.
Maximum-margin markov networks. In Neural In-
formation Processing Systems (NIPS).
Elin T Thordardottir and Susan Ellis Weismer. 2001.
Content mazes and filled pauses in narrative lan-
guage samples of children with specific language
impairment. Brain and cognition, 48(2-3):587?592.
Danielle Wetherell, Nicola Botting, and Gina Conti-
Ramsden. 2007. Narrative in adolescent specific
language impairment (sli): A comparison with peers
across two different narrative genres. International
Journal of Language & Communication Disorders,
42(5):583?605.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Pro-
ceedings of the 18th conference on Computational
linguistics-Volume 2, pages 947?953. Association
for Computational Linguistics.
Simon Zwarts and Mark Johnson. 2011. The impact
of language models and loss functions on repair dis-
fluency detection. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
703?711, Portland, Oregon, USA, June. Association
for Computational Linguistics.
77
