Proceedings of the ACL 2007 Demo and Poster Sessions, pages 105?108,
Prague, June 2007. c?2007 Association for Computational Linguistics
Disambiguating Between Generic and Referential ?You? in Dialog?
Surabhi Gupta
Department of Computer Science
Stanford University
Stanford, CA 94305, US
surabhi@cs.stanford.edu
Matthew Purver
Center for the Study
of Language and Information
Stanford University
Stanford, CA 94305, US
mpurver@stanford.edu
Dan Jurafsky
Department of Linguistics
Stanford University
Stanford, CA 94305, US
jurafsky@stanford.edu
Abstract
We describe an algorithm for a novel task: disam-
biguating the pronoun you in conversation. You can
be generic or referential; finding referential you is im-
portant for tasks such as addressee identification or
extracting ?owners? of action items. Our classifier
achieves 84% accuracy in two-person conversations;
an initial study shows promising performance even on
more complex multi-party meetings.
1 Introduction and Background
This paper describes an algorithm for disambiguat-
ing the generic and referential senses of the pronoun
you.
Our overall aim is the extraction of action items
from multi-party human-human conversations, con-
crete decisions in which one (or more) individuals
take on a group commitment to perform a given task
(Purver et al, 2006). Besides identifying the task it-
self, it is crucial to determine the owner, or person
responsible. Occasionally, the name of the responsi-
ble party is mentioned explicitly. More usually, the
owner is addressed directly and therefore referred to
using a second-person pronoun, as in example (1).1
(1)
A: and um if you can get that binding point also
maybe with a nice example that would be helpful
for Johno and me.
B: Oh yeah uh O K.
It can also be important to distinguish between
singular and plural reference, as in example (2)
where the task is assigned to more than one person:
(2)
A: So y- so you guys will send to the rest of us um a
version of um, this, and - the - uh, description -
B: With sugge- yeah, suggested improvements and -
Use of ?you? might therefore help us both in de-
?This work was supported by the CALO project
(DARPA grant NBCH-D-03-0010) and ONR (MURI award
N000140510388). The authors also thank John Niekrasz for
annotating our test data.
1(1,2) are taken from the ICSI Meeting Corpus (Shriberg et
al., 2004); (3,4) from Switchboard (Godfrey et al, 1992).
tecting the fact that a task is being assigned, and in
identifying the owner. While there is an increas-
ing body of work concerning addressee identifica-
tion (Katzenmaier et al, 2004; Jovanovic et al,
2006), there is very little investigating the problem
of second-person pronoun resolution, and it is this
that we address here. Most cases of ?you? do not in
fact refer to the addressee but are generic, as in ex-
ample (3); automatic referentiality classification is
therefore very important.
(3)
B: Well, usually what you do is just wait until you
think it?s stopped,
and then you patch them up.
2 Related Work
Previous linguistic work has recognized that ?you?
is not always addressee-referring, differentiating be-
tween generic and referential uses (Holmes, 1998;
Meyers, 1990) as well as idiomatic cases of ?you
know?. For example, (Jurafsky et al, 2002) found
that ?you know? covered 47% of cases, the referen-
tial class 22%, and the generic class 27%, with no
significant differences in surface form (duration or
vowel reduction) between the different cases.
While there seems to be no previous work investi-
gating automatic classification, there is related work
on classifying ?it?, which also takes various referen-
tial and non-referential readings: (Mu?ller, 2006) use
lexical and syntactic features in a rule-based clas-
sifier to detect non-referential uses, achieving raw
accuracies around 74-80% and F-scores 63-69%.
3 Data
We used the Switchboard corpus of two-party tele-
phone conversations (Godfrey et al, 1992), and an-
notated the data with four classes: generic, referen-
tial singular, referential plural and a reported refer-
ential class, for mention in reported speech of an
105
Training Testing
Generic 360 79
Referential singular 287 92
Referential plural 17 3
Reported referential 5 1
Ambiguous 4 1
Total 673 176
Table 1: Number of cases found.
originally referential use (as the original addressee
may not be the current addressee ? see example (4)).
We allowed a separate class for genuinely ambigu-
ous cases. Switchboard explicitly tags ?you know?
when used as a discourse marker; as this (generic)
case is common and seems trivial we removed it
from our data.
(4)
B: Well, uh, I guess probably the last one I went to I
met so many people that I had not seen in proba-
bly ten, over ten years.
It was like, don?t you remember me.
And I am like no.
A: Am I related to you?
To test inter-annotator agreement, two people an-
notated 4 conversations, yielding 85 utterances con-
taining ?you?; the task was reported to be easy, and
the kappa was 100%.
We then annotated a total of 42 conversations for
training and 13 for testing. Different labelers an-
notated the training and test sets; none of the au-
thors were involved in labeling the test set. Table 1
presents information about the number of instances
of each of these classes found.
4 Features
All features used for classifier experiments were
extracted from the Switchboard LDC Treebank 3
release, which includes transcripts, part of speech
information using the Penn tagset (Marcus et al,
1994) and dialog act tags (Jurafsky et al, 1997).
Features fell into four main categories:2 senten-
tial features which capture lexical features of the
utterance itself; part-of-speech features which cap-
ture shallow syntactic patterns; dialog act features
capturing the discourse function of the current ut-
terance and surrounding context; and context fea-
tures which give oracle information (i.e., the cor-
rect generic/referential label) about preceding uses
2Currently, features are all based on perfect transcriptions.
of ?you?. We also investigated using the presence
of a question mark in the transcription as a feature,
as a possible replacement for some dialog act fea-
tures. Table 2 presents our features in detail.
N Features
Sentential Features (Sent)
2 you, you know, you guys
N number of you, your, yourself
2 you (say|said|tell|told|mention(ed)|mean(t)|sound(ed))
2 you (hear|heard)
2 (do|does|did|have|has|had|are|could|should|n?t) you
2 ?if you?
2 (which|what|where|when|how) you
Part of Speech Features (POS)
2 Comparative JJR tag
2 you (VB*)
2 (I|we) (VB*)
2 (PRP*) you
Dialog Act Features (DA)
46 DA tag of current utterance i
46 DA tag of previous utterance i ? 1
46 DA tag of utterance i ? 2
2 Presence of any question DA tag (Q DA)
2 Presence of elaboration DA tag
Oracle Context Features (Ctxt)
3 Class of utterance i ? 1
3 Class of utterance i ? 2
3 Class of previous utterance by same speaker
3 Class of previous labeled utterance
Other Features (QM)
2 Question mark
Table 2: Features investigated. N indicates the num-
ber of possible values (there are 46 DA tags; context
features can be generic, referential or N/A).
5 Experiments and Results
As Table 1 shows, there are very few occurrences
of the referential plural, reported referential and am-
biguous classes. We therefore decided to model our
problem as a two way classification task, predicting
generic versus referential (collapsing referential sin-
gular and plural as one category). Note that we ex-
pect this to be the major useful distinction for our
overall action-item detection task.
Baseline A simple baseline involves predicting the
dominant class (in the test set, referential). This
gives 54.59% accuracy (see Table 1).3
SVM Results We used LIBSVM (Chang and Lin,
2001), a support vector machine classifier trained
using an RBF kernel. Table 3 presents results for
3Precision and recall are of course 54.59% and 100%.
106
Features Accuracy F-Score
Ctxt 45.66% 0%
Baseline 54.59% 70.63%
Sent 67.05% 57.14%
Sent + Ctxt + POS 67.05% 57.14%
Sent + Ctxt + POS + QM 76.30% 72.84%
Sent + Ctxt + POS + Q DA 79.19% 77.50%
DA 80.92% 79.75%
Sent + Ctxt + POS +
QM + DA 84.39% 84.21%
Table 3: SVM results: generic versus referential
various selected sets of features. The best set of fea-
tures gave accuracy of 84.39% and f-score 84.21%.
Discussion Overall performance is respectable;
precision was consistently high (94% for the
highest-accuracy result). Perhaps surprisingly, none
of the context or part-of-speech features were found
to be useful; however, dialog act features proved
very useful ? using these features alone give us
an accuracy of 80.92% ? with the referential class
strongly associated with question dialog acts.
We used manually produced dialog act tags, and
automatic labeling accuracy with this fine-grained
tagset will be low; we would therefore prefer to
use more robust features if possible. We found that
one such heuristic feature, the presence of ques-
tion mark, cannot entirely substitute: accuracy is
reduced to 76.3%. However, using only the binary
Q DA feature (which clusters together all the dif-
ferent kinds of question DAs) does better (79.19%).
Although worse than performance with a full tagset,
this gives hope that using a coarse-grained set of
tags might allow reasonable results. As (Stolcke et
al., 2000) report good accuracy (87%) for statement
vs. question classification on manual Switchboard
transcripts, such coarse-grained information might
be reliably available.
Surprisingly, using the oracle context features (the
correct classification for the previous you) alone per-
forms worse than the baseline; and adding these fea-
tures to sentential features gives no improvement.
This suggests that the generic/referential status of
each you may be independent of previous yous.
Features Accuracy F-Score
Prosodic only 46.66% 44.31%
Baseline 54.59% 70.63%
Sent + Ctxt + POS +
QM + DA + Prosodic 84.39% 84.21%
Table 4: SVM results: prosodic features
Category Referential Generic
Count 294 340
Pitch (Hz) 156.18 143.98
Intensity (dB) 60.06 59.41
Duration (msec) 139.50 136.84
Table 5: Prosodic feature analysis
6 Prosodic Features
We next checked a set of prosodic features, test-
ing the hypothesis that generics are prosodically re-
duced. Mean pitch, intensity and duration were ex-
tracted using Praat, both averaged over the entire
utterance and just for the word ?you?. Classifi-
cation results are shown in Table 4. Using only
prosodic features performs below the baseline; in-
cluding prosodic features with the best-performing
feature set from Table 3 gives identical performance
to that with lexical and contextual features alone.
To see why the prosodic features did not help, we
examined the difference between the average pitch,
intensity and duration for referential versus generic
cases (Table 5). A one-sided t-test shows no signif-
icant differences between the average intensity and
duration (confirming the results of (Jurafsky et al,
2002), who found no significant change in duration).
The difference in the average pitch was found to be
significant (p=0.2) ? but not enough for this feature
alone to cause an increase in overall accuracy.
7 Error Analysis
We performed an error analysis on our best classi-
fier output on the training set; accuracy was 94.53%,
giving a total of 36 errors.
Half of the errors (18 of 36) were ambiguous even
for humans (the authors), if looking at the sentence
alone without the neighboring context from the ac-
tual conversation ? see (5a). Treating these exam-
ples thus needs a detailed model of dialog context.
The other major class of errors requires detailed
107
knowledge about sentential semantics and/or the
world ? see e.g. (5b,c), which we can tell are ref-
erential because they predicate inter-personal com-
parison or communication.
In addition, as questions are such a useful feature
(see above), the classifier tends to label all question
cases as referential. However, generic uses do occur
within questions (5d), especially if rhetorical (5e):
(5) a. so uh and if you don?t have the money then use a
credit card
b. I?m probably older than you
c. although uh I will personally tell you I used to work
at a bank
d. Do they survive longer if you plant them in the winter
time?
e. my question I guess are they really your peers?
8 Initial Multi-Party Experiments
The experiments above used two-person dialog data:
we expect that multi-party data is more complex. We
performed an initial exploratory study, applying the
same classes and features to multi-party meetings.
Two annotators labeled one meeting from the
AMI corpus (Carletta et al, 2006), giving a total of
52 utterances containing ?you? on which to assess
agreement: kappa was 87.18% for two way clas-
sification of generic versus referential. One of the
authors then labeled a testing set of 203 utterances;
104 are generic and 99 referential, giving a baseline
accuracy of 51.23% (and F-score of 67.65%).
We performed experiments for the same task: de-
tecting generic versus referential uses. Due to the
small amount of data, we trained the classifier on the
Switchboard training set from section 3 (i.e. on two-
party rather than multi-party data). Lacking part-of-
speech or dialog act features (since the dialog act
tagset differs from the Switchboard tagset), we used
only the sentential, context and question mark fea-
tures described in Table 2.
However, the classifier still achieves an accuracy
of 73.89% and F-score of 74.15%, comparable to the
results on Switchboard without dialog act features
(accuracy 76.30%). Precision is lower, though (both
precision and recall are 73-75%).
9 Conclusions
We have presented results on two person and multi-
party data for the task of generic versus referential
?you? detection. We have seen that the problem is
a real one: in both datasets the distribution of the
classes is approximately 50/50, and baseline accu-
racy is low. Classifier accuracy on two-party data is
reasonable, and we see promising results on multi-
party data with a basic set of features. We expect the
accuracy to go up once we train and test on same-
genre data and also add features that are more spe-
cific to multi-party data.
References
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot,
T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal,
G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post,
D. Reidsma, and P. Wellner. 2006. The AMI meeting cor-
pus. In MLMI 2005, Revised Selected Papers.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: a library for
Support Vector Machines. Software available at http:
//www.csie.ntu.edu.tw/?cjlin/libsvm.
J. J. Godfrey, E. Holliman, and J. McDaniel. 1992. SWITCH-
BOARD: Telephone speech corpus for research and devel-
opment. In Proceedings of IEEE ICASSP-92.
J. Holmes. 1998. Generic pronouns in the Wellington corpus
of spoken New Zealand English. Ko?tare, 1(1).
N. Jovanovic, R. op den Akker, and A. Nijholt. 2006. Ad-
dressee identification in face-to-face meetings. In Proceed-
ings of the 11th Conference of the EACL.
D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board SWBD-DAMSL shallow-discourse-function annota-
tion coders manual, draft 13. Technical Report 97-02, Uni-
versity of Colorado, Boulder.
D. Jurafsky, A. Bell, and C. Girand. 2002. The role of the
lemma in form variation. In C. Gussenhoven and N. Warner,
editors, Papers in Laboratory Phonology VII, pages 1?34.
M. Katzenmaier, R. Stiefelhagen, and T. Schultz. 2004. Iden-
tifying the addressee in human-human-robot interactions
based on head pose and speech. In Proceedings of the 6th
International Conference on Multimodal Interfaces.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre, A. Bies,
M. Ferguson, K. Katz, and B. Schasberger. 1994. The Penn
treebank: Annotating predicate argument structure. In ARPA
Human Language Technology Workshop.
M. W. Meyers. 1990. Current generic pronoun usage. Ameri-
can Speech, 65(3):228?237.
C. Mu?ller. 2006. Automatic detection of nonreferential It in
spoken multi-party dialog. In Proceedings of the 11th Con-
ference of the EACL.
M. Purver, P. Ehlen, and J. Niekrasz. 2006. Detecting action
items in multi-party meetings: Annotation and initial exper-
iments. In MLMI 2006, Revised Selected Papers.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey. 2004.
The ICSI Meeting Recorder Dialog Act (MRDA) Corpus. In
Proceedings of the 5th SIGdial Workshop.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D. Juraf-
sky, P. Taylor, C. V. Ess-Dykema, R. Martin, and M. Meteer.
2000. Dialogue act modeling for automatic tagging and
recognition of conversational speech. Computational Lin-
guistics, 26(3):339?373.
108
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 193?196,
Prague, June 2007. c?2007 Association for Computational Linguistics
Measuring Importance and Query Relevance in Topic-focused
Multi-document Summarization
Surabhi Gupta and Ani Nenkova and Dan Jurafsky
Stanford University
Stanford, CA 94305
surabhi@cs.stanford.edu, {anenkova,jurafsky}@stanford.edu
Abstract
The increasing complexity of summarization systems
makes it difficult to analyze exactly which mod-
ules make a difference in performance. We carried
out a principled comparison between the two most
commonly used schemes for assigning importance to
words in the context of query focused multi-document
summarization: raw frequency (word probability) and
log-likelihood ratio. We demonstrate that the advan-
tages of log-likelihood ratio come from its known dis-
tributional properties which allow for the identifica-
tion of a set of words that in its entirety defines the
aboutness of the input. We also find that LLR is more
suitable for query-focused summarization since, un-
like raw frequency, it is more sensitive to the integra-
tion of the information need defined by the user.
1 Introduction
Recently the task of multi-document summarization
in response to a complex user query has received
considerable attention. In generic summarization,
the summary is meant to give an overview of the
information in the documents. By contrast, when
the summary is produced in response to a user query
or topic (query-focused, topic-focused, or generally
focused summary), the topic/query determines what
information is appropriate for inclusion in the sum-
mary, making the task potentially more challenging.
In this paper we present an analytical study of two
questions regarding aspects of the topic-focused sce-
nario. First, two estimates of importance on words
have been used very successfully both in generic and
query-focused summarization: frequency (Luhn,
1958; Nenkova et al, 2006; Vanderwende et al,
2006) and loglikelihood ratio (Lin and Hovy, 2000;
Conroy et al, 2006; Lacatusu et al, 2006). While
both schemes have proved to be suitable for sum-
marization, with generally better results from log-
likelihood ratio, no study has investigated in what
respects and by how much they differ. Second, there
are many little-understood aspects of the differences
between generic and query-focused summarization.
For example, we?d like to know if a particular word
weighting scheme is more suitable for focused sum-
marization than others. More significantly, previous
studies show that generic and focused systems per-
form very similarly to each other in query-focused
summarization (Nenkova, 2005) and it is of interest
to find out why.
To address these questions we examine the two
weighting schemes: raw frequency (or word proba-
bility estimated from the input), and log-likelihood
ratio (LLR) and two of its variants. These metrics
are used to assign importance to individual content
words in the input, as we discuss below.
Word probability R(w) = nN , where n is the num-
ber of times the word w appeared in the input and N
is the total number of words in the input.
Log-likelihood ratio (LLR) The likelihood ratio ?
(Manning and Schutze, 1999) uses a background
corpus to estimate the importance of a word and it
is proportional to the mutual information between
a word w and the input to be summarized; ?(w) is
defined as the ratio between the probability (under
a binomial distribution) of observing w in the input
and the background corpus assuming equal proba-
bility of occurrence of w in both and the probability
of the data assuming different probabilities for w in
the input and the background corpus.
LLR with cut-off (LLR(C)) A useful property
of the log-likelihood ratio is that the quantity
193
?2 log(?) is asymptotically well approximated by
?2 distribution. A word appears in the input sig-
nificantly more often than in the background corpus
when ?2 log(?) > 10. Such words are called signa-
ture terms in Lin and Hovy (2000) who were the first
to introduce the log-likelihood weighting scheme for
summarization. Each descriptive word is assigned
an equal weight and the rest of the words have a
weight of zero:
R(w) = 1 if (?2 log(?(w)) > 10), 0 otherwise.
This weighting scheme has been adopted in several
recent generic and topic-focused summarizers (Con-
roy et al, 2006; Lacatusu et al, 2006).
LLR(CQ) The above three weighting schemes as-
sign a weight to words regardless of the user query
and are most appropriate for generic summarization.
When a user query is available, it should inform
the summarizer to make the summary more focused.
In Conroy et al (2006) such query sensititivity is
achieved by augmenting LLR(C) with all content
words from the user query, each assigned a weight
of 1 equal to the weight of words defined by LLR(C)
as topic words from the input to the summarizer.
2 Data
We used the data from the 2005 Document Under-
standing Conference (DUC) for our experiments.
The task is to produce a 250-word summary in re-
sponse to a topic defined by a user for a total of 50
topics with approximately 25 documents for each
marked as relevant by the topic creator. In com-
puting LLR, the remaining 49 topics were used as a
background corpus as is often done by DUC partic-
ipants. A sample topic (d301) shows the complexity
of the queries:
Identify and describe types of organized crime that
crosses borders or involves more than one country. Name
the countries involved. Also identify the perpetrators in-
volved with each type of crime, including both individuals
and organizations if possible.
3 The Experiment
In the summarizers we compare here, the various
weighting methods we describe above are used to
assign importance to individual content words in the
input. The weight or importance of a sentence S in
GENERIC FOCUSED
Frequency 0.11972 0.11795
(0.11168?0.12735) (0.11010?0.12521)
LLR 0.11223 0.11600
(0.10627?0.11873) (0.10915?0.12281)
LLR(C) 0.11949 0.12201
(0.11249?0.12724) (0.11507?0.12950)
LLR(CQ) not app 0.12546
(.11884?.13247)
Table 1: SU4 ROUGE recall (and 95% confidence
intervals) for runs on the entire input (GENERIC) and
on relevant sentences (FOCUSED).
the input is defined as
WeightR(S) =
?
w?S
R(w) (1)
where R(w) assigns a weight for each word w.
For GENERIC summarization, the top scoring sen-
tences in the input are taken to form a generic extrac-
tive summary. In the computation of sentence im-
portance, only nouns, verbs, adjectives and adverbs
are considered and a short list of light verbs are ex-
cluded: ?has, was, have, are, will, were, do, been,
say, said, says?. For FOCUSED summarization, we
modify this algorithm merely by running the sen-
tence selection algorithm on only those sentences
in the input that are relevent to the user query. In
some previous DUC evaluations, relevant sentences
are explicitly marked by annotators and given to sys-
tems. In our version here, a sentence in the input is
considered relevant if it contains at least one word
from the user query.
For evaluation we use ROUGE (Lin, 2004) SU4
recall metric1, which was among the official auto-
matic evaluation metrics for DUC.
4 Results
The results are shown in Table 1. The focused sum-
marizer using LLR(CQ) is the best, and it signif-
icantly outperforms the focused summarizer based
on frequency. Also, LLR (using log-likelihood ra-
tio to assign weights to all words) perfroms signif-
icantly worse than LLR(C). We can observe some
trends even from the results for which there is no
significance. Both LLR and LLR(C) are sensitive to
the introduction of topic relevance, producing some-
what better summaries in the FOCUSED scenario
1
-n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -d
194
compared to the GENERIC scenario. This is not the
case for the frequency summarizer, where using only
the relevant sentences has a negative impact.
4.1 Focused summarization: do we need query
expansion?
In the FOCUSED condition there was little (for LLR
weighting) or no (for frequency) improvement over
GENERIC. One possible explanation for the lack of
clear improvement in the FOCUSED setting is that
there are not enough relevant sentences, making it
impossible to get stable estimates of word impor-
tance. Alternatively, it could be the case that many
of the sentences are relevant, so estimates from the
relevant portion of the input are about the same as
those from the entire input.
To distinguish between these two hypotheses, we
conducted an oracle experiment. We modified the
FOCUSED condition by expanding the topic words
from the user query with all content words from any
of the human-written summaries for the topic. This
increases the number of relevant sentences for each
topic. No automatic method for query expansion can
be expected to give more accurate results, since the
content of the human summaries is a direct indica-
tion of what information in the input was important
and relevant and, moreover, the ROUGE evaluation
metric is based on direct n-gram comparison with
these human summaries.
Even under these conditions there was no signif-
icant improvement for the summarizers, each get-
ting better by 0.002: the frequency summarizer gets
R-SU4 of 0.12048 and the LLR(CQ) summarizer
achieves R-SU4 of 0.12717.
These results seem to suggest that considering the
content words in the user topic results in enough rel-
evant sentences. Indeed, Table 2 shows the mini-
mum, maximum and average percentage of relevant
sentences in the input (containing at least one con-
tent words from the user the query), both as defined
by the original query and by the oracle query ex-
pansion. It is clear from the table that, on aver-
age, over half of the input comprises sentences that
are relevant to the user topic. Oracle query expan-
sion makes the number of relevant sentences almost
equivalent to the input size and it is thus not sur-
prising that the corresponding results for content se-
lection are nearly identical to the query independent
Original query Oracle query expansion
Min 13% 52%
Average 57% 86%
Max 82% 98%
Table 2: Percentage of relevant sentences (contain-
ing words from the user query) in the input. The
oracle query expansion considers all content words
form human summaries of the input as query words.
runs of generic summaries for the entire input.
These numbers indictate that rather than finding
ways for query expansion, it might instead be more
important to find techniques for constraining the
query, determining which parts of the input are di-
rectly related to the user questions. Such techniques
have been described in the recent multi-strategy ap-
proach of Lacatusu et al (2006) for example, where
one of the strategies breaks down the user topic
into smaller questions that are answered using ro-
bust question-answering techniques.
4.2 Why is log-likelihood ratio better than
frequency?
Frequency and log-likelihood ratio weighting for
content words produce similar results when applied
to rank all words in the input, while the cut-off
for topicality in LLR(C) does have a positive im-
pact on content selection. A closer look at the
two weighting schemes confirms that when cut-off
is not used, similar weighting of content words is
produced. The Spearman correlation coefficient be-
tween the weights for words assigned by the two
schemes is on average 0.64. At the same time, it is
likely that the weights of sentences are dominated
by only the top most highly weighted words. In
order to see to what extent the two schemes iden-
tify the same or different words as the most impor-
tant ones, we computed the overlap between the 250
most highly weighted words according to LLR and
frequency. The average overlap across the 50 sets
was quite large, 70%.
To illustrate the degree of overlap, we list below
are the most highly weighted words according to
each weighting scheme for our sample topic con-
cerning crimes across borders.
LLR drug, cocaine, traffickers, cartel, police, crime, en-
forcement, u.s., smuggling, trafficking, arrested, government,
seized, year, drugs, organised, heroin, criminal, cartels, last,
195
official, country, law, border, kilos, arrest, more, mexican, laun-
dering, officials, money, accounts, charges, authorities, cor-
ruption, anti-drug, international, banks, operations, seizures,
federal, italian, smugglers, dealers, narcotics, criminals, tons,
most, planes, customs
Frequency drug, cocaine, officials, police, more, last, gov-
ernment, year, cartel, traffickers, u.s., other, drugs, enforce-
ment, crime, money, country, arrested, federal, most, now, traf-
ficking, seized, law, years, new, charges, smuggling, being, of-
ficial, organised, international, former, authorities, only, crimi-
nal, border, people, countries, state, world, trade, first, mexican,
many, accounts, according, bank, heroin, cartels
It becomes clear that the advantage of likelihood
ratio as a weighting scheme does not come from
major differences in overall weights it assigns to
words compared to frequency. It is the signifi-
cance cut-off for the likelihood ratio that leads to
noticeable improvement (see Table 1). When this
weighting scheme is augmented by adding a score
of 1 for content words that appear in the user topic,
the summaries improve even further (LLR(CQ)).
Half of the improvement can be attributed to the
cut-off (LLR(C)), and the other half to focusing
the summary using the information from the user
query (LLR(CQ)). The advantage of likelihood ra-
tio comes from its providing a principled criterion
for deciding which words are truly descriptive of the
input and which are not. Raw frequency provides no
such cut-off.
5 Conclusions
In this paper we examined two weighting schemes
for estimating word importance that have been suc-
cessfully used in current systems but have not to-
date been directly compared. Our analysis con-
firmed that log-likelihood ratio leads to better re-
sults, but not because it defines a more accurate as-
signment of importance than raw frequency. Rather,
its power comes from the use of a known distribution
that makes it possible to determine which words are
truly descriptive of the input. Only when such words
are viewed as equally important in defining the topic
does this weighting scheme show improved perfor-
mance. Using the significance cut-off and consider-
ing all words above it equally important is key.
Log-likelihood ratio summarizer is more sensitive
to topicality or relevance and produces summaries
that are better when it take the user request into ac-
count than when it does not. This is not the case for
a summarizer based on frequency.
At the same time it is noteworthy that the generic
summarizers perform about as well as their focused
counterparts. This may be related to our discovery
that on average 57% of the sentences in the doc-
ument are relevant and that ideal query expansion
leads to a situation in which almost all sentences
in the input become relevant. These facts could
be an unplanned side-effect from the way the test
topics were produced: annotators might have been
influenced by information in the input to be sum-
marizied when defining their topic. Such observa-
tions also suggest that a competitive generic summa-
rizer would be an appropriate baseline for the topic-
focused task in future DUCs. In addition, including
some irrelavant documents in the input might make
the task more challenging and allow more room for
advances in query expansion and other summary fo-
cusing techniques.
References
J. Conroy, J. Schlesinger, and D. O?Leary. 2006. Topic-focused
multi-document summarization using an approximate oracle
score. In Proceedings of the COLING/ACL?06 (Poster Ses-
sion).
F. Lacatusu, A. Hickl, K. Roberts, Y. Shi, J. Bensley, B. Rink,
P. Wang, and L. Taylor. 2006. Lcc?s gistexter at duc 2006:
Multi-strategy multi-document summarization. In Proceed-
ings of DUC?06.
C. Lin and E. Hovy. 2000. The automated acquisition of topic
signatures for text summarization. In Proceedings of COL-
ING?00.
C. Lin. 2004. Rouge: a package for automatic evaluation of
summaries. In Proceedings of the Workshop on Text Sum-
marization Branches Out (WAS 2004).
H. P. Luhn. 1958. The automatic creation of literature abstracts.
IBM Journal of Research and Development, 2(2):159?165.
C. Manning and H. Schutze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press.
A. Nenkova, L. Vanderwende, and K. McKeown. 2006.
A compositional context sensitive multi-document summa-
rizer: Exploring the factors that influence summarization. In
Proceedings of ACM SIGIR?06.
A. Nenkova. 2005. Automatic text summarization of newswire:
lessons learned from the document understanding confer-
ence. In Proceedings of AAAI?05.
L. Vanderwende, H. Suzuki, and C. Brockett. 2006. Microsoft
research at duc 2006: Task-focused summarization with sen-
tence simplification and lexical expansion. In Proceedings of
DUC?06.
196
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 96?103,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatically Detecting Action Items in Audio Meeting Recordings
William Morgan Pi-Chuan Chang Surabhi Gupta
Department of Computer Science
Stanford University
353 Serra Mall
Stanford, CA 94305-9205
ruby@cs.stanford.edu
pcchang@cs.stanford.edu
surabhi@cs.stanford.edu
Jason M. Brenier
Department of Linguistics
Center for Spoken Language Research
Institute of Cognitive Science
University of Colorado at Boulder
594 UCB
Boulder, Colorado 80309-0594
jbrenier@colorado.edu
Abstract
Identification of action items in meeting
recordings can provide immediate access
to salient information in a medium noto-
riously difficult to search and summarize.
To this end, we use a maximum entropy
model to automatically detect action item-
related utterances from multi-party audio
meeting recordings. We compare the ef-
fect of lexical, temporal, syntactic, seman-
tic, and prosodic features on system per-
formance. We show that on a corpus of ac-
tion item annotations on the ICSI meeting
recordings, characterized by high imbal-
ance and low inter-annotator agreement,
the system performs at an F measure of
31.92%. While this is low compared to
better-studied tasks on more mature cor-
pora, the relative usefulness of the features
towards this task is indicative of their use-
fulness on more consistent annotations, as
well as to related tasks.
1 Introduction
Meetings are a ubiquitous feature of workplace
environments, and recordings of meetings pro-
vide obvious benefit in that they can be replayed
or searched through at a later date. As record-
ing technology becomes more easily available and
storage space becomes less costly, the feasibil-
ity of producing and storing these recordings in-
creases. This is particularly true for audio record-
ings, which are cheaper to produce and store than
full audio-video recordings.
However, audio recordings are notoriously diffi-
cult to search or to summarize. This is doubly true
of multi-party recordings, which, in addition to the
difficulties presented by single-party recordings,
typically contain backchannels, elaborations, and
side topics, all of which further confound search
and summarization processes. Making efficient
use of large meeting corpora thus requires intel-
ligent summary and review techniques.
One possible user goal given a corpus of meet-
ing recordings is to discover the action items de-
cided within the meetings. Action items are deci-
sions made within the meeting that require post-
meeting attention or labor. Rapid identification
of action items can provide immediate access to
salient portions of the meetings. A review of ac-
tion items can also function as (part of) a summary
of the meeting content.
To this end, we explore the task of applying
maximum entropy classifiers to the task of auto-
matically detecting action item utterances in au-
dio recordings of multi-party meetings. Although
available corpora for action items are not ideal, it
is hoped that the feature analysis presented here
will be of use to later work on other corpora.
2 Related work
Multi-party meetings have attracted a significant
amount of recent research attention. The creation
of the ICSI corpus (Janin et al, 2003), comprised
of 72 hours of meeting recordings with an average
of 6 speakers per meeting, with associated tran-
scripts, has spurred further annotations for var-
ious types of information, including dialog acts
(Shriberg et al, 2004), topic hierarchies and action
items (Gruenstein et al, 2005), and ?hot spots?
(Wrede and Shriberg, 2003).
The classification of individual utterances based
on their role in the dialog, i.e. as opposed to their
semantic payload, has a long history, especially
in the context of dialog act (DA) classification.
96
Research on DA classification initially focused
on two-party conversational speech (Mast et al,
1996; Stolcke et al, 1998; Shriberg et al, 1998)
and, more recently, has extended to multi-party
audio recordings like the ICSI corpus (Shriberg
et al, 2004). Machine learning techniques such
as graphical models (Ji and Bilmes, 2005), maxi-
mum entropy models (Ang et al, 2005), and hid-
den Markov models (Zimmermann et al, 2005)
have been used to classify utterances from multi-
party conversations.
It is only more recently that work focused
specifically on action items themselves has been
developed. SVMs have been successfully applied
to the task of extracting action items from email
messages (Bennett and Carbonell, 2005; Corston-
Oliver et al, 2004). Bennett and Carbonell, in par-
ticular, distinguish the task of action item detec-
tion in email from the more well-studied task of
text classification, noting the finer granularity of
the action item task and the difference of seman-
tics vs. intent. (Although recent work has begun to
blur this latter division, e.g. Cohen et al (2004).)
In the audio domain, annotations for action item
utterances on several recorded meeting corpora,
including the ICSI corpus, have recently become
available (Gruenstein et al, 2005), enabling work
on this topic.
3 Data
We use action item annotations produced by Gru-
enstein et al (2005). This corpus provides topic
hierarchy and action item annotations for the ICSI
meeting corpus as well as other corpora of meet-
ings; due to the ready availability of other types of
annotations for the ICSI corpus, we focus solely
on the annotations for these meetings. Figure 1
gives an example of the annotations.
The corpus covers 54 ICSI meetings annotated
by two human annotators, and several other meet-
ings annotated by one annotator. Of the 54 meet-
ings with dual annotations, 6 contain no action
items. For this study we consider only those meet-
ings which contain action items and which are an-
notated by both annotators.
As the annotations were produced by a small
number of untrained annotators, an immediate
question is the degree of consistency and reliabil-
ity. Inter-annotator agreement is typically mea-
sured by the kappa statistic (Carletta, 1996), de-
kappa
fre
qu
en
cy
0.0 0.2 0.4 0.6 0.8 1.0
0
2
4
6
8
Figure 2: Distribution of ? (inter-annotator agree-
ment) across the 54 ICSI meetings tagged by two
annotators. Of the two meetings with ? = 1.0, one
has only two action items and the other only four.
fined as:
? = P (O) ? P (E)1 ? P (E)
where P (O) is the probability of the observed
agreement, and P (E) the probability of the ?ex-
pected agreement? (i.e., under the assumption the
two sets of annotations are independent). The
kappa statistic ranges from ?1 to 1, indicating per-
fect disagreement and perfect agreement, respec-
tively.
Overall inter-annotator agreement as measured
by ? on the action item corpus is poor, as noted in
Purver et al (2006), with an overall ? of 0.364 and
values for individual meetings ranging from 1.0 to
less than zero. Figure 2 shows the distribution of
? across all 54 annotated ICSI meetings.
To reduce the effect of poor inter-annotator
agreement, we focus on the top 15 meetings as
ranked by ?; the minimum ? in this set is 0.435.
Although this reduces the total amount of data
available, our intention is that this subset of the
most consistent annotations will form a higher-
quality corpus.
While the corpus classifies related action item
utterances into action item ?groups,? in this study
we wish to treat the annotations as simply binary
attributes. Visual analysis of annotations for sev-
eral meetings outside the set of chosen 15 suggests
that the union of the two sets of annotations yields
the most consistent resulting annotation; thus, for
this study, we consider an utterance to be an action
item if at least one of the annotators marked it as
such.
The 15-meeting subset contains 24,250 utter-
97
A1 A2
X X So that will be sort of the assignment for next week, is to?
X X to?for slides and whatever net you picked and what it can do and?and how far
you?ve gotten. Pppt!
X - Well, I?d like to also,
X X though, uh, ha- have a first cut at what the
X X belief-net looks like.
- X Even if it?s really crude.
- - OK? So, you know,
- - here a- here are?
- X So we?re supposed to @@ about features and whatnot, and?
Figure 1: Example transcript and action item annotations (marked ?X?) from annotators A1 and A2.
?@@? signifies an unintelligible word. This transcript is from an ICSI meeting recording and has ? =
0.373, ranking it 16th out of 54 meetings in annotator agreement.
0 500 1000 1500 2000 2500
Figure 3: Number of total and action item utter-
ances across the 15 selected meetings. There are
24,250 utterances total, 590 of which (2.4%) are
action item utterances.
ances total; under the union strategy above, 590 of
these are action item utterances. Figure 3 shows
the number of action item utterances and the num-
ber of total utterances in the 15 selected meetings.
One noteworthy feature of the ICSI corpus un-
derlying the action item annotations is the ?digit
reading task,? in which the participants of meet-
ings take turns reading aloud strings of digits.
This task was designed to provide a constrained-
vocabulary training set of speech recognition de-
velopers interested in multi-party speech. In this
study we did not remove these sections; the net
effect is that some portions of the data consist of
these fairly atypical utterances.
4 Experimental methodology
We formulate the action item detection task as one
of binary classification of utterances. We apply a
maximum entropy (maxent) model (Berger et al,
1996) to this task.
Maxent models seek to maximize the condi-
tional probability of a class c given the observa-
tions X using the exponential form
P (c|X) = 1Z(X) exp
[
?
i
?i,c fi,c(X)
]
where fi,c(X) is the ith feature of the data X
in class c, ?i,c is the corresponding weight, and
Z(X) is a normalization term. Maxent models
choose the weights ?i,c so as to maximize the en-
tropy of the induced distribution while remaining
consistent with the data and labels; the intuition is
that such a distribution makes the fewest assump-
tions about the underlying data.
Our maxent model is regularized by a quadratic
prior and uses quasi-Newton parameter optimiza-
tion. Due to the limited amount of training data
(see Section 3) and to avoid overfitting, we em-
ploy 10-fold cross validation in each experiment.
To evaluate system performance, we calculate
the F measure (F ) of precision (P ) and recall (R),
defined as:
P = |A ? C||A|
R = |A ? C||C|
F = 2PRP + R
where A is the set of utterances marked as action
items by the system, and C is the set of (all) cor-
rect action item utterances.
98
The use of precision and recall is motivated by
the fact that the large imbalance between posi-
tive and negative examples in the corpus (Sec-
tion 3) means that simpler metrics like accuracy
are insufficient?a system that simply classifies
every utterance as negative will achieve an accu-
racy of 97.5%, which clearly is not a good reflec-
tion of desired behavior. Recall and F measure for
such a system, however, will be zero.
Likewise, a system that flips a coin weighted in
proportion to the number of positive examples in
the entire corpus will have an accuracy of 95.25%,
but will only achieve P = R = F = 2.4%.
5 Features
As noted in Section 3, we treat the task of produc-
ing action item annotations as a binary classifica-
tion task. To this end, we consider the following
sets of features. (Note that all real-valued features
were range-normalized so as to lie in [0, 1] and that
no binning was employed.)
5.1 Immediate lexical features
We extract word unigram and bigram features
from the transcript for each utterance. We nor-
malize for case and for certain contractions; for
example, ?I?ll? is transformed into ?I will?.
Note that these are oracle features, as the tran-
scripts are human-produced and not the product
of automatic speech recognizer (ASR) system out-
put.
5.2 Contextual lexical features
We extract word unigram and bigram features
from the transcript for the previous and next ut-
terances across all speakers in the meeting.
5.3 Syntactic features
Under the hypothesis that action item utterances
will exhibit particular syntactic patterns, we use
a conditional Markov model part-of-speech (POS)
tagger (Toutanova and Manning, 2000) trained on
the Switchboard corpus (Godfrey et al, 1992) to
tag utterance words for part of speech. We use the
following binary POS features:
? Presence of UH tag, denoting the presence of
an ?interjection? (including filled pauses, un-
filled pauses, and discourse markers).
? Presence of MD tag, denoting presence of a
modal verb.
? Number of NN* tags, denoting the number of
nouns.
? Number of VB* tags, denoting the number of
verbs.
? Presence of VBD tag, denoting the presence
of a past-tense verb.
5.4 Prosodic features
Under the hypothesis that action item utterances
will exhibit particular prosodic behavior?for ex-
ample, that they are emphasized, or are pitched a
certain way?we performed pitch extraction using
an auto-correlation method within the sound anal-
ysis package Praat (Boersma and Weenink, 2005).
From the meeting audio files we extract the fol-
lowing prosodic features, on a per-utterance basis:
(pitch measures are in Hz; intensity in energy; nor-
malization in all cases is z-normalization)
? Pitch and intensity range, minimum, and
maximum.
? Pitch and intensity mean.
? Pitch and intensity median (0.5 quantile).
? Pitch and intensity standard deviation.
? Pitch slope, processed to eliminate halv-
ing/doubling.
? Number of voiced frames.
? Duration-normalized pitch and intensity
ranges and voiced frame count.
? Speaker-normalized pitch and intensity
means.
5.5 Temporal features
Under the hypothesis that the length of an utter-
ance or its location within the meeting as a whole
will determine its likelihood of being an action
item?for example, shorter statements near the
end of the meeting might be more likely to be ac-
tion items?we extract the duration of each utter-
ance and the time from its occurrence until the end
of the meeting. (Note that the use of this feature
precludes operating in an online setting, where the
end of the meeting may not be known in advance.)
5.6 General semantic features
Under the hypothesis that action item utterances
will frequently involve temporal expressions?e.g.
?Let?s have the paper written by next Tuesday??
we use Identifinder (Bikel et al, 1997) to mark
temporal expressions (?TIMEX? tags) in utterance
transcripts, and create a binary feature denoting
99
the existence of a temporal expression in each ut-
terance.
Note that as Identifinder was trained on broad-
cast news corpora, applying it to the very different
domain of multi-party meeting transcripts may not
result in optimal behavior.
5.7 Dialog-specific semantic features
Under the hypothesis that action item utterances
may be closely correlated with specific dialog
act tags, we use the dialog act annotations from
the ICSI Meeting Recorder Dialog Act Corpus.
(Shriberg et al, 2004) As these DA annotations
do not correspond one-to-one with utterances in
the ICSI corpus, we align them in the most liberal
way possible, i.e., if at least one word in an utter-
ance is annotated for a particular DA, we mark the
entirety of that utterance as exhibiting that DA.
We consider both fine-grained and coarse-
grained dialog acts.1 The former yields 56 fea-
tures, indicating occurrence of DA tags such
as ?appreciation,? ?rhetorical question,? and
?task management?; the latter consists of only
7 classes??disruption,? ?backchannel,? ?filler,?
?statement,? ?question,? ?unlabeled,? and ?un-
known.?
6 Results
The final performance for the maxent model
across different feature sets is given in Table 1.
F measures scores range from 13.81 to 31.92.
Figure 4 shows the interpolated precision-recall
curves for several of these feature sets; these
graphs display the level of precision that can be
achieved if one is willing to sacrifice some recall,
and vice versa.
Although ideally, all combinations of features
should be evaluated separately, the large number
of features in this precludes this strategy. The
combination of features explored here was cho-
sen so as to start from simpler features and suc-
cessively add more complex ones. We start with
transcript features that are immediate and context-
independent (?unigram?, ?bigram?, ?TIMEX?);
then add transcript features that require context
(?temporal?, ?context?), then non-transcript (i.e.
audio signal) features (?prosodic?), and finally add
features that require both the transcript and the au-
dio signal (?DA?).
1We use the map 01 grouping defined in the MRDA cor-
pus to collapse the tags.
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
recall
pr
ec
isi
on
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
pr
ec
isi
on
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
pr
ec
isi
on
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
pr
ec
isi
on
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
pr
ec
isi
on
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
pr
ec
isi
on
unigram
bigram
temporal
context+prosodic
fine?grained DAs
Figure 4: Interpolated precision-recall curve for
several (cumulative) feature sets. This graph sug-
gests the level of precision that can be achieved
if one is willing to sacrifice some recall, and vice
versa.
In total, nine combinations of features were
considered. In every case except that of syn-
tactic and coarse-grained dialog act features, the
additional features improved system performance
and these features were used in succeeding exper-
iments. Syntactic and coarse-grained DA features
resulted in a drop in performance and were dis-
carded from succeeding systems.
7 Analysis
The unigram and bigram features provide signif-
icant discriminative power. Tables 2 and 3 give
the top features, as determined by weight, for the
models trained only on these features. It is clear
from Table 3 that the detailed end-of-utterance
punctuation in the human-generated transcripts
provide valuable discriminative power.
The performance gain from adding TIMEX tag-
ging features is small and likely not statistically
significant. Post-hoc analysis of the TIMEX tag-
ging (Section 5.6) suggests that Identifinder tag-
ging accuracy is quite plausible in general, but ex-
hibits an unfortunate tendency to mark the digit-
reading (see Section 3) portion of the meetings as
temporal expressions. It is plausible that remov-
ing these utterances from the meetings would al-
low this feature a higher accuracy.
Based on the low feature weight assigned, utter-
ance length appears to provide no significant value
to the model. However, the time until the meet-
ing is over ranks as the highest-weighted feature
in the unigram+bigram+TIMEX+temporal feature
set. This feature is thus responsible for the 39.25%
100
features number F % imp.
unigram 6844 13.81
unigram+bigram 61281 16.72 21.07
unigram+bigram+TIMEX 61284 16.84 0.72
unigram+bigram+TIMEX+temporal 61286 23.45 39.25
unigram+bigram+TIMEX+temporal+syntactic 61291 21.94 -6.44
unigram+bigram+TIMEX+temporal+context 183833 25.62 9.25
unigram+bigram+TIMEX+temporal+context+prosodic 183871 27.44 7.10
unigram+bigram+TIMEX+temporal+context+prosodic+coarse DAs 183878 26.47 -3.53
unigram+bigram+TIMEX+temporal+context+prosodic+fine DAs 183927 31.92 16.33
Table 1: Performance of the maxent classifier as measured by F measure, the relative improvement from
the preceding feature set, and the number of features, across all feature sets tried. Italicized lines denote
the addition of features which do not improve performance; these are omitted from succeeding systems.
feature +/- ?
?pull? + 2.2100
?email? + 1.7883
?needs? + 1.7212
?added? + 1.6613
?mm-hmm? - 1.5937
?present? + 1.5740
?nine? - 1.5019
?!? - 1.5001
?five? - 1.4944
?together? + 1.4882
Table 2: Features, evidence type (positive denotes
action item), and weight for the top ten features
in the unigram-only model. ?Nine? and ?five? are
common words in the digit-reading task (see Sec-
tion 3).
feature +/- ?
?- $? - 1.4308
?i will? + 1.4128
?, $? - 1.3115
?uh $? - 1.2752
?w- $? - 1.2419
?. $? - 1.2247
?email? + 1.2062
?six $? - 1.1874
?* in? - 1.1833
?so $? - 1.1819
Table 3: Features, evidence type and weight for
the top ten features in the unigram+bigram model.
The symbol * denotes the beginning of an utter-
ance and $ the end. All of the top ten features are
bigrams except for the unigrams ?email?.
feature +/- ?
mean intensity (norm.) - 1.4288
mean pitch (norm.) - 1.0661
intensity range + 1.0510
?i will? + 0.8657
?email? + 0.8113
reformulate/summarize (DA) + 0.7946
?just go? (next) + 0.7190
?i will? (prev.) + 0.7074
?the paper? + 0.6788
understanding check (DA) + 0.6547
Table 4: Features, evidence type and weight for
the top ten features on the best-performing model.
Bigrams labeled ?prev.? and ?next? correspond to
the lexemes from previous and next utterances, re-
spectively. Prosodic features labeled as ?norm.?
have been normalized on a per-speaker basis.
boost in F measure in row 3 of Table 1.
The addition of part-of-speech tags actually de-
creases system performance. It is unclear why this
is the case. It may be that the unigram and bi-
gram features already adequately capture any dis-
tinctions these features make, or simply that these
features are generally not useful for distinguishing
action items.
Contextual features, on the other hand, im-
prove system performance significantly. A post-
hoc analysis of the action item annotations makes
clear why: action items are often split across mul-
tiple utterances (e.g. as in Figure 1), only a portion
of which contain lexical cues sufficient to distin-
guish them as such. Contextual features thus allow
utterances immediately surrounding these ?obvi-
ous? action items to be tagged as well.
101
Prosodic features yield a 7.10% increase in
F measure, and analysis shows that speaker-
normalized intensity and pitch, and the range in
intensity of an utterance, are valuable discrimina-
tive features. The subsequent addition of coarse-
grained dialog act tags does not further improve
system performance. It is likely this is due to rea-
sons similar to those for POS tags?either the cat-
egories are insufficient to distinguish action item
utterances, or whatever usefulness they provide is
subsumed by other features.
Table 4 shows the feature weights for the top-
ranked features on the best-scoring system. The
addition of the fine-grained DA tags results in a
significant increase in performance.The F measure
of this best feature set is 31.92%.
8 Conclusions
We have shown that several classes of features are
useful for the task of action item annotation from
multi-party meeting corpora. Simple lexical fea-
tures, their contextual versions, the time until the
end of the meeting, prosodic features, and fine-
grained dialog acts each contribute significant in-
creases in system performance.
While the raw system performance numbers of
Table 1 are low relative to other, better-studied
tasks on other, more mature corpora, we believe
the relative usefulness of the features towards this
task is indicative of their usefulness on more con-
sistent annotations, as well as to related tasks.
The Gruenstein et al (2005) corpus provides
a valuable and necessary resource for research in
this area, but several factors raise the question of
annotation quality. The low ? scores in Section 3
are indicative of annotation problems. Post-hoc
error analysis yields many examples of utterances
which are somewhat difficult to imagine as pos-
sible, never mind desirable, to tag. The fact that
the extremely useful oracular information present
in the fine-grained DA annotation does not raise
performance to the high levels that one might ex-
pect further suggests that the annotations are not
ideal?or, at the least, that they are inconsistent
with the DA annotations.2
This analysis is consistent with the findings of
Purver et al (2006), who achieve an F measure of
2Which is not to say they are devoid of significant value?
training and testing our best system on the corpus with the
590 positive classifications randomly shuffled across all ut-
terances yields an F measure of only 4.82.
less than 25% when applying SVMs to the classi-
fication task to the same corpus, and motivate the
development of a new corpus of action item anno-
tations.
9 Future work
In Section 6 we showed that contextual lexical
features are useful for the task of action item de-
tection, at least in the fairly limited manner em-
ployed in our implementation, which simply looks
at immediate previous and immediate next utter-
ances. It seems likely that applying a sequence
model such as an HMM or conditional random
field (CRFs) will act as a generalization of this fea-
ture and may further improve performance.
Addition of features such as speaker change and
?hot spots? (Wrede and Shriberg, 2003) may also
aid classification. Conversely, it is possible that
feature selection techniques may improve perfor-
mance by helping to eliminate poor-quality fea-
tures. In this work we have followed an ?ev-
erything but the kitchen sink? approach, in part
because we were curious about which features
would prove useful. The effect of adding POS and
coarse-grained DA features illustrates that this is
not necessarily the ideal strategy in terms of ulti-
mate system performance.
In general, the features evaluated in this
work are an indiscriminate mix of human- and
automatically-generated features; of the human-
generated features, some are plausible to generate
automatically, at some loss of quality (e.g. tran-
scripts) while others are unlikely to be automati-
cally generated in the foreseeable future (e.g. fine-
grained dialog acts). Future work may focus on
the effects that automatic generation of the former
has on overall system performance (although this
may require higher-quality annotations to be use-
ful.) For example, the detailed end-of-utterance
punctuation present in the human transcripts pro-
vides valuable discriminative power (Table 3), but
current ASR systems are not likely to be able to
provide this level of detail. Switching to ASR out-
put will have a negative effect on performance.
One final issue is that of utterance segmenta-
tion. The scheme used in the ICSI meeting corpus
does not necessarily correspond to the ideal seg-
mentation for other tasks. The action item annota-
tions were performed on these segmentations, and
in this study we did not attempt resegmentation,
but in the future it may prove valuable to collapse,
102
for example, successive un-interrupted utterances
from the same speaker into a single utterance.
In conclusion, while overall system perfor-
mance does not approach levels typical of better-
studied classification tasks such as named-entity
recognition, we believe that this is a largely a prod-
uct of the current action item annotation quality.
We believe that the feature analysis presented here
is useful, for this task and for other related tasks,
and that, provided with a set of more consistent
action item annotations, the current system can be
used as is to achieve better performance.
Acknowledgments
The authors wish to thank Dan Jurafsky, Chris
Manning, Stanley Peters, Matthew Purver, and
several anonymous reviewers for valuable advice
and comments.
References
Jeremy Ang, Yang Liu, and Elizabeth Shriberg. 2005.
Automatic dialog act segmentation and classifica-
tion in multiparty meetings. In Proceedings of the
ICASSP.
Paul N. Bennett and Jaime Carbonell. 2005. Detecting
action-items in e-mail. In Proceedings of SIGIR.
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguis-
tics, 22(1):39?71.
D. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: a high-performance learning name-
finder. In Proceedings of the Conference on Applied
NLP.
Paul Boersma and David Weenink. 2005. Praat: doing
phonetics by computer v4.4.12 (computer program).
J. Carletta. 1996. Assessing agreement on classifica-
tion tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249?254.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
?speech acts?. In Proceedings of EMNLP.
Simon Corston-Oliver, Eric Ringger, Michael Ga-
mon, and Richard Campbell. 2004. Task-focused
summarization of email. In Text Summarization
Branches Out: Proceedings of the ACL Workshop.
J. Godfrey, E. Holliman, and J.McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for
research and development. In Proceedings of
ICAASP.
Alexander Gruenstein, John Niekrasz, and Matthew
Purver. 2005. Meeting structure annotation: Data
and tools. In Proceedings of the 6th SIGDIAL Work-
shop on Discourse and Dialogue.
Adam Janin, Don Baron, Jane Edwards, Dan Ellis,
David Gelbart, Nelson Morgan, Barbara Peskin,
Thilo Pfau, Elizabeth Shriberg, Andreas Stolcke,
and Chuck Wooters. 2003. The ICSI meeting cor-
pus. In Proceedings of the ICASSP.
Gang Ji and Jeff Bilmes. 2005. Dialog act tag-
ging using graphical models. In Proceedings of the
ICASSP.
Marion Mast, R. Kompe, S. Harbeck, A. Kie?ling,
H. Niemann, E. No?th, E.G. Schukat-Talamazzini,
and V. Warnke. 1996. Dialog act classification with
the help of prosody. In Proceedings of the ICSLP.
Matthew Purver, Patrick Ehlen, and John Niekrasz.
2006. Detecting action items in multi-party meet-
ings: Annotation and initial experiments. In Pro-
ceedings of the 3rd Joint Workshop on MLMI.
Elizabeth Shriberg, Rebecca Bates, Andreas Stolcke,
Paul Taylor, Daniel Jurafsky, Klaus Ries, Noah Coc-
caro, Rachel Martin, Marie Meteer, and Carol Van
EssDykema. 1998. Can prosody aid the auto-
matic classification of dialog acts in conversational
speech? Language and Speech, 41(3?4):439?487.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The ICSI meeting
recorder dialog act (MRDA) corpus. In Proceedings
of the 5th SIGDIAL Workshop on Discourse and Di-
alogue.
Andreas Stolcke, Elizabeth Shriberg, Rebecca Bates,
Noah Coccaro, Daniel Jurafsky, Rachel Mar-
tin, Marie Meteer, Klaus Ries, Paul Taylor, and
Carol Van EssDykema. 1998. Dialog act model-
ing for conversational speech. In Proceedings of
the AAAI Spring Symposium on Applying Machine
Learning to Discourse Processing.
Kristina Toutanova and Christopher D. Manning.
2000. Enriching the knowledge sources used in a
maximum entropy part-of-speech tagger. In Pro-
ceedings of EMNLP.
Britta Wrede and Elizabeth Shriberg. 2003. Spot-
ting ?hot spots? in meetings: Human judgments and
prosodic cues. In Proceedings of the European Con-
ference on Speech Communication and Technology.
Matthias Zimmermann, Yang Liu, Elizabeth Shriberg,
and Andreas Stolcke. 2005. Toward joint segmen-
tation and classification of dialog acts in multiparty
meetings. In Proceedings of the 2nd Joint Workshop
on MLMI.
103
