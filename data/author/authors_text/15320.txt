Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1660?1669, Dublin, Ireland, August 23-29 2014.
Query-Focused Opinion Summarization for User-Generated Content
Lu Wang
1
Hema Raghavan
2
Claire Cardie
1
Vittorio Castelli
3
1
Department of Computer Science, Cornell University, Ithaca, NY 14853, USA
{luwang, cardie}@cs.cornell.edu
2
LinkedIn, CA, USA
hraghavan@linkedin.com
3
IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA
vittorio@us.ibm.com
Abstract
We present a submodular function-based framework for query-focused opinion summarization. Within our
framework, relevance ordering produced by a statistical ranker, and information coverage with respect to
topic distribution and diverse viewpoints are both encoded as submodular functions. Dispersion functions
are utilized to minimize the redundancy. We are the first to evaluate different metrics of text similarity for
submodularity-based summarization methods. By experimenting on community QA and blog summariza-
tion, we show that our system outperforms state-of-the-art approaches in both automatic evaluation and
human evaluation. A human evaluation task is conducted on Amazon Mechanical Turk with scale, and
shows that our systems are able to generate summaries of high overall quality and information diversity.
1 Introduction
Social media forums, such as social networks, blogs, newsgroups, and community question answering
(QA), offer avenues for people to express their opinions as well collect other people?s thoughts on topics
as diverse as health, politics and software (Liu et al., 2008). However, digesting the large amount of
information in long threads on newsgroups, or even knowing which threads to pay attention to, can be
overwhelming. A text-based summary that highlights the diversity of opinions on a given topic can
lighten this information overload. In this work, we design a submodular function-based framework for
opinion summarization on community question answering and blog data.
Question: What is the long term effect of piracy on the music and film industry?
Best Answer: Rising costs for movies and music. ... If they sell less, they need to raise the price to make up for what they lost. The
other thing will be music and movies with less quality. ...
Other Answers:
Ans1: Its bad... really bad. (Just watch this movie and you will find out ... Piracy causes rappers to appear on your computer).
Ans2: By removing the profitability of music & film companies, piracy takes away their motivation to produce new music & movies.
If they can?t protect their copyrights, they can?t continue to do business. ...
Ans4: It is forcing them to rework their business model, which is a good thing. In short, I don?t think the music industry in particular
will ever enjoy the huge profits of the 90?s. ...
Ans6: Please-People in those businesses make millions of dollars as it is!! I don?t think piracy hurts them at all!!!
Figure 1: Example discussion on Yahoo! Answers. Besides the best answer, other answers also contain
relevant information (in italics). For example, the sentence in blue has a contrasting viewpoint compared
to the other answers.
Opinion summarization has previously been applied to restricted domains, such as product reviews (Hu
and Liu, 2004; Lerman et al., 2009) and news (Stoyanov and Cardie, 2006), where the output summary
is either presented in a structured way with respect to each aspect of the product or organized along
contrastive viewpoints. Unlike those works, we address user generated online data: community QA and
blogs. These forums use a substantially less formal language than news articles, and at the same time
address a much broader spectrum of topics than product reviews. As a result, they present new challenges
for automatic summarization. For example, Figure 1 illustrates a sample question from Yahoo! Answers
1
along with the answers from different users. The question receives more than one answer, and one of
them is selected as the ?best answer? by the asker or other participants. In general, answers from other
users also provide relevant information. While community QA successfully pools rich knowledge from
the wisdom of the crowd, users might need to seine through numerous posts to extract the information
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
http://answers.yahoo.com/
1660
they need. Hence, it would be beneficial to summarize answers automatically and present the summaries
to users who ask similar questions in the future. In this work, we aim to return a summary that encapsu-
lates different perspectives for a given opinion question and a set of relevant answers or documents.
In our work we assume that there is a central topic (or query) on which a user is seeking diverse opin-
ions. We predict query-relevance through automatically learned statistical rankers. Our ranking function
not only aims to find sentences that are on the topic of the query but also ones that are ?opinionated?
through the use of several features that indicate subjectivity and sentiment. The relevance score is en-
coded in a submodular function. Diversity is accounted for by a dispersion function that maximizes the
pairwise distance between the pairs of sentences selected.
Our chief contributions are:
(1) We develop a submodular function-based framework for query-focused opinion summarization. To
the best of our knowledge, this is the first time that submodular functions have been used to support
opinion summarization. We test our framework on two tasks: summarizing opinionated sentences in
community QA (Yahoo! Answers) and blogs (TAC-2008 corpus). Human evaluation using Amazon Me-
chanical Turk shows that our system generates the best summary 57.1% of the time. On the other hand,
the best answer picked by Yahoo! users is chosen only 31.9% of the time. We also obtain significant
higher Pyramid F1 score on the blog task as compared to the system of Lin and Bilmes (2011).
(2) Within our summarization framework, the statistically learned sentence relevance is included as part
of our objective function, whereas previous work on submodular summarization (Lin and Bilmes, 2011)
only uses ngram overlap for query relevance. Additionally, we use Latent Dirichlet Allocation (Blei et
al., 2003) to model the topic structure of the sentences, and induce clusterings according to the learned
topics. Therefore, our system is capable of generating summaries with broader topic coverage.
(3) Furthermore, we are the first to study how different metrics for computing text similarity or dis-
similarity affect the quality of submodularity-based summarization methods. We show empirically that
lexical representation-based similarity, such as TFIDF scores, uniformly outperforms semantic similar-
ity computed with WordNet. Moreover, when measuring the summary diversity, topical representation
is marginally better than lexical representation, and both of them beats semantic representation.
2 Related Work
Our work falls in the realm of query-focused summarization, where a user asks a question and the sys-
tem generates a summary of the answers containing pertinent and diverse information. A wide range
of methods have been investigated, where relevance is often estimated through TF-IDF similarity (Car-
bonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian model
over queries and documents (Daum?e and Marcu, 2006). Most work only implicitly penalizes summary
redundancy, e.g. by downweighting the importance of words that are already selected.
Encouraging diversity of a summary has recently been addressed through submodular functions, which
have been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al.,
2012), and comments summarization (Dasgupta et al., 2013). However, these works either ignore the
query information (when available) or else use simple ngram matching between the query and sentences.
In contrast, we propose to optimize an objective function that addresses both relevance and diversity.
Previous work on generating opinion summaries mainly considers product reviews (Hu and Liu, 2004;
Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editori-
als (Paul et al., 2010). Mostly, there is no query information, and summaries are formulated in a struc-
tured way based on product features or contrastive standpoints. Our work is more related to opinion
summarization on user-generated content, such as community QA. Liu et al. (2008) manually construct
taxonomies for questions in community QA. Summaries are generated by clustering sentences according
to their polarity based on a small dictionary. Tomasoni and Huang (2010) introduce coverage and quality
constraints on the sentences, and utilize an integer linear programming framework to select sentences.
3 Submodular Opinion Summarization
In this section, we describe how query-focused opinion summarization can be addressed by submodular
functions combined with dispersion functions. We first define our problem. Then we introduce the
1661
Basic Features Sentiment Features
- answer position in all answers/sentence position in blog - number/portion of sentiment words from a lexicon (Section 3.2)
- length of the answer/sentence - if contains sentiment words with the same polarity as
- length is less than 5 words sentiment words in query
Query-Sentence Overlap Features Query-Independent Features
- unigram/bigram TF/TFIDF similarity with query - unigram/bigram TFIDF similarity with cluster centroid
- number of key phrases in the query that appear in the - sumBasic score (Nenkova and Vanderwende, 2005)
sentence. A model similar to that described in - number of topic signature words (Lin and Hovy, 2000)
(Luo et al., 2013) was applied to detect key phrases. - JS divergence with cluster
Table 1: Features used for candidate ranking. We use them for ranking answers in both community QA
and blogs.
components of our objective function (Sections 3.1?3.3). The full objective function is presented in
Section 3.4. Lastly, we describe a greedy algorithm with constant factor approximation to the optimal
solution for generating summaries (Section 3.5).
A set of documents or answers to be summarized are first split into a set of individual sentences
V = {s
1
, ? ? ? , s
n
}. Our problem is to select a subset S ? V that maximizes a given objective function
f : 2
V
? R within a length constraint: S
?
= argmax
S?V
f(S), subject to | S |? c. | S | is the length of
the summary S, and c is the length limit.
Definition 1 A function f : 2
V
? R is submodular iff for all s ? V and every S ? S
?
? V , it satisfies
f(S ? {s})? f(S) ? f(S
?
? {s})? f(S
?
).
Previous submodularity-based summarization work assumes this diminishing return property makes
submodular functions a natural fit for summarization and achieves state-of-the-art results on various
datasets. In this paper, we follow the same assumption and work with non-decreasing submodular func-
tions. Nevertheless, they have limitations, one of which is that functions well suited to modeling diversity
are not submodular. Recently, Dasgupta et al. (2013) proved that diversity can nonetheless be encoded
in well-designed dispersion functions which still maintain a constant factor approximation when solved
by a greedy algorithm.
Based on these considerations, we propose an objective function f(S) mainly considering three as-
pects: relevance (Section 3.1), coverage (Section 3.2), and non-redundancy (Section 3.3). Relevance
and coverage are encoded in a non-decreasing submodular function, and non-redundancy is enforced by
maximizing the dispersion function.
3.1 Relevance Function
We first utilize statistical rankers to produce a preference ordering of the candidate answers or sentences.
We choose ListNet (Cao et al., 2007), which has been shown to be effective in many information retrieval
tasks, as our ranker. We use the implementation from Ranklib (Dang, 2011).
Features used in the ranking algorithm are summarized in Table 1. All features are normalized by
standardization. Due to the length limit, we cannot provide the full results on feature evaluation. Never-
theless, we find that ranking candidates by TFIDF similarity or key phrases overlapping with the query
can produce comparable results with using the full feature set (see Section 5).
We take the ranks output by the ranker, and define the relevance of the current summary S as: r(S) =
?
|S|
i
?
rank
?1
i
, where rank
i
is the rank of sentence s
i
in V . For QA answer ranking, sentences from the
same answer have the same ranking. The function r(S) is our first submodular function.
3.2 Coverage Functions
Topic Coverage. This function is designed to capture the idea that a comprehensive opinion sum-
mary should provide thoughts on distinct aspects. Topic models such as Latent Dirichlet Allocation
(LDA) (Blei et al., 2003) and its variants are able to discover hidden topics or aspects of document col-
lections, and thus afford a natural way to cluster texts according to their topics. Recent work (Xie and
Xing, 2013) shows the effectiveness of utilizing topic models for newsgroup document clustering. We
first learn an LDA model from the data, and treat each topic as a cluster. We estimate a sentence-topic
distribution
~
? for each sentence, and assign the sentence to the cluster k corresponding to the mode of the
distribution (i.e., k = argmax
i
?
i
). This naive approach produces comparable clustering performance to
the state-of-the-art according to (Xie and Xing, 2013). T is defined as the clustering induced by our algo-
rithm on the set V . The topic coverage of the current summary S is defined as t(S) =
?
T?T
?
|S ? T |.
1662
From the concavity of the square root it follows that sets S with uniform coverages of topics are preferred
to sets with skewed coverage.
Authorship Coverage. This term encourages the summarization algorithm to select sentences from
different authors. Let A be the clustering induced by the sentence to author relation. In community
QA, sentences from the answers given by the same user belong to the same cluster. Similarly, sentences
from blogs with the same author are in the same cluster. The authorship score is defined as a(S) =
?
A?A
?
|S ?A|.
Polarity Coverage. The polarity score encourages the selection of summaries that cover both positive
and negative opinions. We categorize each sentence simply by counting the number of polarized words
given by our lexicon. A sentence belongs to a positive cluster if it has more positive words than negative
ones, and vice versa. If any negator co-occurs with a sentiment word (e.g. within a window of size 5),
the sentiment is reversed.
2
The polarity clustering P thus have two clusters corresponding to positive
and negative opinions. The score is defined as p(S) =
?
P?P
?
| S ? P |. Our lexicon consists of
MPQA lexicon (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and
Sebastiani, 2006). Words with conflicting sentiments from different lexicons are removed.
Content Coverage. Similarly to Lin and Bilmes (2011) and Dasgupta et al. (2013), we use the following
function to measure content coverage of the current summary S: c(S) =
?
v?V
min(cov(v, S), ? ?
cov(v, V )), where cov(v, S) =
?
u?S
sim(v, u). We experiment with two types of similarity functions.
One is a Cosine TFIDF similarity score. The other is a WordNet-based semantic similarity score between
pairwise dependency relations from two sentences (Dasgupta et al., 2013). Specifically, sim
Sem
(v, u) =
?
rel
i
?v,rel
j
?u
WN(a
i
, a
j
) ?WN(b
i
, b
j
), where rel
i
= (a
i
, b
i
), rel
j
= (a
j
, b
j
), WN(w
i
, w
j
) is the
shortest path length. All scores are scaled onto [0, 1].
3.3 Dispersion Function
Summaries should contain as little redundant information as possible. We achieve this by adding an
additional term to the objective function, encoded by a dispersion function. Given a set of sentences
S, a complete graph is constructed with each sentence in S as a node. The weight of each edge (u, v)
is their dissimilarity d
?
(u, v). Then the distance between any pair of u and v, d(u, v), is defined as the
total weight of the shortest path connecting u and v.
3
We experiment with two forms of dispersion
function (Dasgupta et al., 2013): (1) h
sum
=
?
u,v?V,u6=v
d(u, v), and (2) h
min
= min
u,v?V,u6=v
d(u, v).
Then we need to define the dissimilarity function d
?
(?, ?). There are different ways to measure the
dissimilarity between sentences (Mihalcea et al., 2006; Agirre et al., 2012). In this work, we experiment
with three types of dissimilarity functions.
Lexical Dissimilarity. This function is based on the well-known Cosine similarity score using TFIDF
weights. Let sim
tfidf
(u, v) be the Cosine similarity between u and v, then we have d
?
Lex
(u, v) =
1? sim
tfidf
(u, v).
Semantic Dissimilarity. This function is based on the semantic meaning embedded in the dependency
relations. d
?
Sem
(u, v) = 1 ? sim
Sem
(v, u), where sim
Sem
(v, u) is the semantic similarity used in
content coverage measurement in Section 3.2.
Topical Dissimilarity. We propose a novel dissimilarity measure based on topic models. Celikyilmaz
et al. (2010) show that estimating the similarity between query and passages by using topic structures
can help improve the retrieval performance. As discussed in the topic coverage in Section 3.2, each
sentence is represented by its sentence-topic distributions estimated by LDA. For candidate sentence u
and v, let their topic distributions be P
u
and P
v
. Then the dissimilarity between u and v can be defined
as: d
?
Topic
(u, v) = JSD(P
u
||P
v
) =
1
2
(
?
i
P
u
(i) log
2
P
u
(i)
P
a
(i)
+
?
i
P
v
(i) log
2
P
v
(i)
P
a
(i)
) where P
a
(i) =
1
2
(P
u
(i) + P
v
(i)).
3.4 Full Objective Function
The objective function takes the interpolation of the submodular functions and dispersion function:
F(S) = r(S) + ?t(S) + ?a(S) + ?p(S) + ?c(S) + ?h(S). (1)
2
There exists a large amount of work on determining the polarity of a sentence (Pang and Lee, 2008) which can be employed
for polarity clustering in this work. We decide to focus on summarization, and estimate sentence polarity through sentiment
word summation (Yu and Hatzivassiloglou, 2003), though we do not distinguish different sentiment words.
3
This definition of distance is used to produce theoretical guarantees for the greedy algorithm described in Section 3.5.
1663
The coefficients ?, ?, ?, ?, ? are non-negative real numbers and can be tuned on a development set.
4
Notice that each summand except h(S) is a non-decreasing, non-negative, and submodular function,
and summation preserves monotonicity, non-negativity, and submodularity. Dispersion function h(s) is
either h
sum
or h
min
as introduced previously.
3.5 Summary Generation via Greedy Algorithm
Generating the summary that maximizes our objective function in Equation 1 is NP-hard (Chandra and
Halld?orsson, 1996). We choose to use a greedy algorithm that guarantees to obtain a constant factor ap-
proximation to the optimal solution (Nemhauser et al., 1978; Dasgupta et al., 2013). Concretely, starting
with an empty set, for each iteration, we add a new sentence so that the current summary achieves the
maximum value of the objective function. In addition to the theoretical guarantee, existing work (Mc-
Donald, 2007) has empirically shown that classical greedy algorithms usually works near-optimally.
4 Experimental Setup
4.1 Opinion Question Identification
We first build a classifier to automatically detect opinion oriented questions in Community QA; questions
in the blog dataset are all opinionated. Our opinion question classifier is trained on two opinion question
datasets: (1) the first, from Li et al. (2008a), contains 646 opinionated and 332 objective questions; (2)
the second dataset, from Amiri et al. (2013), consists of 317 implicit opinion questions, such as ?What
can you do to help environment??, and 317 objective questions. We train a RBF kernel based SVM
classifier to identify opinion questions, which achieves F1 scores of 0.79 and 0.80 on the two datasets
when evaluated using 10-fold cross-validation (the best F1 scores reported are 0.75 and 0.79).
4.2 Datasets
Community QA Summarization: Yahoo! Answers. We use the Yahoo! Answers dataset from Yahoo!
Webscope
TM
program,
5
which contains 3,895,407 questions. We first run the opinion question classifier
to identify the opinion questions. For summarization purpose, we require each question having at least 5
answers, with the average length of answers larger than 20 words. This results in 130,609 questions.
To make a compelling task, we reserve questions with an average length of answers larger than 50
words as our test set for both ranking and summarization; all the other questions are used for training. As
a result, we have 92,109 questions in the training set for learning the statistical ranker, and 38,500 in the
test set. The category distribution of training and test questions (Yahoo! Answers organizes the questions
into predefined categories) are similar. 10,000 questions from the training set are further reserved as the
development set. Each question in the Yahoo! Answers dataset has a user-voted best answer. These best
answers are used to train the statistical ranker that predicts relevance. Separate topic models are learned
for each category, where the category tag is provided by Yahoo! Answer.
Blog Summarization: TAC 2008. We use the TAC 2008 corpus (Dang, 2008), which consists of 25
topics. 23 of them are provided with human labeled nuggets, which TAC used in human evaluation. TAC
also provides snippets (i.e., sentences) that are frequently retrieved by participant systems or identified
as relevant by human annotators. We do not assume those snippets are known to any of our systems.
4.3 Comparisons
For both opinion summarization tasks, we compare with (1) the approach by Dasgupta et al. (2013), and
(2) the systems from Lin and Bilmes (2011) with and without query information. The sentence clustering
process in Lin and Bilmes (2011) is done by using CLUTO (Karypis, 2003). For the implementation of
systems in Lin and Bilmes (2011) and Dasgupta et al. (2013), we always use the parameters reported to
have the best performance in their work.
For cQA summarization, we use the best answer voted by the user as a baseline. Note that this is a
strong baseline since all the other systems are unaware of which answer is the best. For blog summa-
rization, we have three additional baselines ? the best systems in TAC 2008 (Kim et al., 2008; Li et al.,
2008b), top sentences returned by our ranker, a baseline produced by TFIDF similarity and a lexicon
4
The values for the coefficients are 5.0, 1.0, 10.0, 5.0, 10.0 for ?, ?, ?, ?, ?, respectively, as tuned on the development set.
5
http://sandbox.yahoo.com/
1664
(henceforth called TFIDF+Lexicon). In TFIDF+Lexicon, sentences are ranked by the TFIDF similar-
ity with the query, and then sentences with sentiment words are selected in sequence. This baseline aims
to show the performance when we only have access to lexicons without using a learning algorithm.
5 Results
5.1 Evaluating the Ranker
We evaluate our ranker (described in Section 3.1) on the task of best answer prediction. Table 2 compares
the average precision and mean reciprocal rank (MRR) of our method to those of three baselines, (1)
where answers are ranked randomly (Baseline (Random)), (2) by length (Baseline (Length)), and (3)
by Jensen Shannon Divergence (JSD) with all answers. We expect that the best answer is the one that
covers the most information, which is likely to have a smaller JSD. Therefore, we use JSD to rank
answers in the ascending order. Table 2 manifests that our ranker outperforms all the other methods.
Baseline (Random) Baseline (Length) JSD Ranker (ListNet)
Avg Precision 0.1305 0.2834 0.4000 0.5336
MRR 0.3403 0.4889 0.5909 0.6496
Table 2: Performance for best answer prediction. Our ranker outperforms the three baselines.
5.2 Community QA Summarization
Automatic Evaluation. Since human written abstracts are not available for the Yahoo! Answers dataset,
we adopt the Jensen-Shannon divergence (JSD) to measure the summary quality. Intuitively, a smaller
JSD implies that the summary covers more of the content in the answer set. Louis and Nenkova (2013)
report that JSD has a strong negative correlation (Spearman correlation = ?0.737) with the overall
summary quality for multi-document summarization (MDS) on news articles and blogs. Our task is
similar to MDS. Meanwhile, the average JSD of the best answers in our test set is smaller than that of
the other answers (0.39 vs. 0.49), with an average length of 103 words compared with 67 words for the
other answers. Also, on the blog task (Section 5.3), the top two systems by JSD also have the top two
ROUGE scores (a common metric for summarization evaluation when human-constructed summaries
are available). Thus, we conjecture that JSD is a good metric for community QA summaries.
Table 3 (left) shows that our system using a content coverage function based on Cosine using TFIDF
weights, and a dispersion function (h
sum
) based on lexicon dissimilarity and 100 topics, outperforms all
of the compared approaches (paired-t test, p < 0.05). The topic number is tuned on the development set,
and we find that varying the number of topics does not impact performance too much. Meanwhile, both
our system and Dasgupta et al. (2013) produce better JSD scores than the two variants of the Lin and
Bilmes (2011) system, which implies the effectiveness of the dispersion function. We further examine the
effectiveness of each component that contributes to the objective function (Section 3.4), and the results
are shown in Table 3 (right).
Length
100 200
Best answer 0.3858 -
Lin and Bilmes (2011) 0.3398 0.2008
Lin and Bilmes (2011) + q 0.3379 0.1988
Dasgupta et al. (2013) 0.3316 0.1939
Our system 0.3017 0.1758
JSD
100
JSD
200
Rel(evance) 0.3424 0.2053
Rel + Aut(hor) 0.3375 0.2040
Rel + Aut + TM (Topic Models) 0.3366 0.2033
Rel + Aut + TM + Pol(arity) 0.3309 0.1983
Rel + Aut + TM + Pol + Cont(ent Coverage) 0.3102 0.1851
Rel + Aut + TM + Pol + Cont + Disp(ersion) 0.3017 0.1758
Table 3: [Left] Summaries evaluated by Jensen-Shannon divergence (JSD) on Yahoo Answer for sum-
maries of 100 words and 200 words. The average length of the best answer is 102.70. [Right] Value
addition of each component in the objective function. The JSD on each line is statistically significantly
lower than the JSD on the previous (? = 0.05).
Human Evaluation. Human evaluation for Yahoo! Answers is carried out on Amazon Mechanical Turk
6
with carefully designed tasks (or ?HITs?). Turkers are presented summaries from different systems in a
random order, and asked to provide two rankings, one for overall quality and the other for information
diversity. We indicate that informativeness and non-redundancy are desirable for quality; however, Turk-
ers are allowed to consider other desiderata, such as coherence or responsiveness, and write down those
when they submit the answers. Here we believe that ranking the summaries is easier than evaluating each
summary in isolation (Lerman et al., 2009).
6
https://www.mturk.com/mturk/
1665
We randomly select 100 questions from our test set, each of which is evaluated by 4 distinct Turkers
located in United States. 40 HITs are thus created, each containing 10 different questions. Four system
summaries (best answer, Dasgupta et al. (2013), and our system with 100 and 200 words respectively) are
displayed along with one noisy summary (i.e. irrelevant to the question) per question in random order.
7
We reject Turkers? HITs if they rank the noisy summary higher than any other. Two duplicate questions
are added to test intra-annotator agreement. We reject HITs if Turkers produced inconsistent rankings
for both duplicate questions. A total of 137 submissions of which 40 HITs pass the above quality filters.
Turkers of all accepted submissions report themselves as native English speakers. An inter-rater agree-
ment of Fleiss? ? of 0.28 (fair agreement (Landis and Koch, 1977)) is computed for quality ranking and
? is 0.43 (moderate agreement) for diversity ranking. Table 4 shows the percentage of times a particular
method is picked as the best summary, and the macro-/micro-average rank of a method, for both overall
quality and information diversity. Macro-average is computed by first averaging the ranks per question
and then averaging across all questions.
For overall quality, our system with a 200 word limit is selected as the best in 44.6% of the evaluations.
It outperforms the best answer (31.9%) significantly, which suggests that our system summary covers rel-
evant information that is not contained in the best answer. Our system with a length constraint of 100
words is chosen as the best for quality 12.5% times while that of Dasgupta et al. (2013) is chosen 11.0%
of the time. Our system is also voted as the best summary for diversity in 78.7% of the evaluations. More
interestingly, both of our systems, with 100 words and 200 words, outperform the best answer and Das-
gupta et al. (2013) for average ranking (both overall quality and information diversity) significantly by
using Wilcoxon signed-rank test (p < 0.05). When we check the reasons given by Turkers, we found that
people usually prefer our summaries due to ?helpful suggestions that covered many options? or being
?balanced with different opinions?. When Turks prefer the best answers, they mostly stress on coherence
and responsiveness. Sample summaries from all the systems are displayed in Figure 2.
Length of Summary Overall Quality Information Diversity
% Average Rank % Average Rank
Best Macro Micro Best Macro Micro
Best answer 102.70 31.9% 2.68 2.69 9.6% 3.27 3.29
Dasgupta et al. (2013)
100
11.0% 2.84 2.83 5.0% 2.95 2.94
Our system 12.5% 2.50
?
2.50
?
6.7% 2.43
?
2.43
?
Our system 200 44.6% 1.98
?
1.98
?
78.7% 1.35
?
1.34
?
Table 4: Human evaluation on Yahoo! Answer Data. Boldface implies statistically significance com-
pared to other results in the same columns using paired-t test. Both of our systems are ranked higher
(i.e. numbers in bold with
?
) than the best answers voted by Yahoo! users and system summaries from
Dasgupta et al. (2013).
Question: What is the long term effect of piracy on the music and film industry?
Dasgupta et al. (2013) (Qty Rank=2.75 Div. Rank=2.5):
?In short, I don?t think the music industry in particular will ever enjoy the huge profits of the 90?s.
?Please-People in those businesses make millions of dollars as it is !! I don?t think piracy hurts them at all !!!
?The other thing will be music and movies with less quality.
?Its a big gray area, I dont see anything wrong with burning a mix cd or a cd for a friend so long as youre not selling them for profit.
?By removing the profitability of music & film companies, piracy takes away their motivation to produce new music & movies.
Our system (100 words) (Qty Rank=2.25 Div. Rank=2.25):
?Rising costs for movies and music. The other thing will be music and movies with less quality.
?Now, with piracy, there isn?t the willingness to take chances.
?But it?s also like the person put the effort into it and they aren?t getting paid. It?s a big gray area, I don?t see anything wrong with burning a mix cd
or a cd for a friend so long as you?re not selling them for profit.
?It is forcing them to rework their business model, which is a good thing.
Our system (200 words) (Qty. Rank=2.25, Div Rank=1.25):
?Rising costs for movies and music. The other thing will be music and movies with less quality.
?Now, with piracy, there isn?t the willingness to take chances. American Idol is the result of this. .... The real problem here is that the mainstream
music will become even tighter. Record labels will not won?t to go far from what is currently like by the majority.
?I hate when people who have billions of dollars whine about not having more money. But it?s also like the person put the effort into it and they
aren?t getting paid ... I don?t see anything wrong with burning a mix cd or a cd for a friend ....
?It is forcing them to rework their business model, which is a good thing.
?By removing the profitability of music & film companies, piracy takes away their motivation to produce new music & movies.
Figure 2: Sample summaries from Dasgupta et al. (2013), and our systems (100 words and 200 words).
Sentences from separate bullets (?) are partial answers from different users.
7
Note that we aim to compare results with the gold-standard best answers of about 100 words. The evaluation of the
200-word summaries is provided only as an additional data-point.
1666
5.3 Blog Summarization
Automatic Evaluation. We use the ROUGE (Lin and Hovy, 2003) software with standard options to
automatically evaluate summaries with reference to the human labeled nuggets as those are available
for this task. ROUGE-2 measures bigram overlap and ROUGE-SU4 measures the overlap of unigram
and skip-bigram separated by up to four words. We use the ranker trained on Yahoo! data to produce
relevance ordering, and adopt the system parameters from Section 5.2. Table 5 (left) shows that our
system outperforms the best system in TAC?08 with highest ROUGE-2 score (Kim et al., 2008), the two
baselines (TFIDF+Lexicon, and our ranker), Lin and Bilmes (2011), and Dasgupta et al. (2013).
ROUGE-2 ROUGE-SU4 JSD
Best system in TAC?08 0.2923 0.3766 0.3286
TFIDF + Lexicon 0.3069 0.3876 0.2429
Ranker (ListNet) 0.3200 0.3960 0.2293
Lin and Bilmes (2011) 0.2732 0.3582 0.2330
Lin and Bilmes (2011) + q 0.2852 0.3700 0.2349
Dasgupta et al. (2013) 0.2618 0.3500 0.2370
Our system 0.3234 0.3978 0.2258
Pyramid F-score
Best system in TAC?08 0.2225
Lin and Bilmes (2011) 0.2790
Our system 0.3620
Table 5: Results on TAC?08 dataset. [Left] Our system has significant better ROUGE scores than all
the other systems except our ranker (paired-t test, p < 0.05). We also achieve the best JS divergence.
[Right] Human evaluation with Pyramid F-score. Our system significantly outperforms the others.
Human Evaluation. For human evaluation, we use the standard Pyramid F-score used in the TAC?08
opinion summarization track with ? = 3 (Dang, 2008). In the TAC task, systems are allowed to return up
to 7,000 non-white characters for each question. Since the TAC metric favors recall we do not produce
summaries shorter than 7,000 characters. We ask two human judges to evaluate our system along with
the one that got the highest Pyramid F-score in the TAC?08 and Lin and Bilmes (2011). Cohen?s ? for
inter-annotator agreement is 0.68 (substantial). While we did not explicitly evaluate non-redundancy,
both of our judges report that our system summaries contain less redundant information.
5.4 Further Discussion
Yahoo! Answer
DISPERSION
sum
DISPERSION
min
DISSIMI Cont
tfidf
Cont
sem
Cont
tfidf
Cont
sem
Semantic 0.3143 0.324 3 0.3129 0.3232
Topical 0.3101 0.3202 0.3106 0.3209
Lexical 0.3017 0.3147 0.3071 0.3172
TAC 2008
DISPERSION
sum
DISPERSION
min
DISSIMI Cont
tfidf
Cont
sem
Cont
tfidf
Cont
sem
Semantic 0.2216 0.2169 0.2772 0.2579
Topical 0.2128 0.2090 0.3234 0.3056
Lexical 0.2167 0.2129 0.3117 0.3160
Table 6: Effect of different dispersion functions, content coverage, and dissimilarity metrics on our
system. [Left] JSD values for different combinations on Yahoo! data, using LDA with 100 topics.
All systems are significantly different from each other at significance level ? = 0.05. Systems using
summation of distances for dispersion function (h
sum
) uniformly outperform the ones using minimum
distance (h
min
). [Right] ROUGE scores of different choices for TAC 2008 data. All systems use LDA
with 40 topics. The parameters of our systems are adopted from the ones tuned on Yahoo! Answers.
Given that the text similarity metrics and dispersion functions play important roles in the framework,
we further study the effectiveness of different content coverage functions (Cosine using TFIDF vs. Se-
mantic), dispersion functions (h
sum
vs. h
min
), and dissimilarity metrics used in dispersion functions
(Semantic vs. Topical vs. Lexical). Results on Yahoo! Answer (Table 6 (left)) show that systems using
summation of distances for dispersion functions (h
sum
) uniformly outperform the ones using minimum
distance (h
min
). Meanwhile, Cosine using TFIDF is better at measuring content coverage than WordNet-
based semantic measurement, and this may due to the limited coverage of WordNet on verbs. This is also
true for dissimilarity metrics. Results on blog data (Table 6 (right)), however, show that using minimum
distance for dispersion produces better results. This indicates that optimal dispersion function varies by
genre. Topical-based dissimilarity also marginally outperforms the other two metrics in blog data.
6 Conclusion
We propose a submodular function-based opinion summarization framework. Tested on community QA
and blog summarization, our approach outperforms state-of-the-art methods that are also based on sub-
modularity in both automatic evaluation and human evaluation. Our framework is capable of including
statistically learned sentence relevance and encouraging the summary to cover diverse topics. We also
study different metrics on text similarity estimation and their effect on summarization.
1667
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on seman-
tic textual similarity. In Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation
(SemEval 2012), pages 385?393, Montr?eal, Canada, 7-8 June. Association for Computational Linguistics.
Hadi Amiri, Zheng-Jun Zha, and Tat-Seng Chua. 2013. A pattern matching based model for implicit opinion
question identification. In AAAI. AAAI Press.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res.,
3:993?1022, March.
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to rank: From pairwise approach
to listwise approach. In Proceedings of the 24th International Conference on Machine Learning, ICML ?07,
pages 129?136, New York, NY, USA. ACM.
Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents
and producing summaries. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR ?98, pages 335?336, New York, NY, USA. ACM.
Asli Celikyilmaz, Dilek Hakkani-Tur, and Gokhan Tur. 2010. Lda based similarity modeling for question answer-
ing. In Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, SS ?10, pages 1?9, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Barun Chandra and Magn?us M. Halld?orsson. 1996. Facility dispersion and remote subgraphs. In Proceedings
of the 5th Scandinavian Workshop on Algorithm Theory, SWAT ?96, pages 53?65, London, UK, UK. Springer-
Verlag.
Hoa Tran Dang. 2008. Overview of the tac 2008 opinion question answering and summarization tasks. In Proc.
TAC 2008.
Van Dang. 2011. RankLib. http://www.cs.umass.edu/?vdang/ranklib.html.
Anirban Dasgupta, Ravi Kumar, and Sujith Ravi. 2013. Summarization through submodularity and dispersion.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 1014?1022, Sofia, Bulgaria, August. Association for Computational Linguistics.
Hal Daum?e, III and Daniel Marcu. 2006. Bayesian query-focused summarization. In Proceedings of the 21st
International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for
Computational Linguistics, ACL-44, pages 305?312, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiwordnet: A publicly available lexical resource for opinion
mining. In In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC06, pages
417?422.
Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the Tenth ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ?04, pages 168?177, New
York, NY, USA. ACM.
George Karypis. 2003. CLUTO - a clustering toolkit. Technical Report #02-017, November.
Hyun Duk Kim, Dae Hoon Park, V.G.Vinod Vydiswaran, and ChengXiang Zhai. 2008. Opinion summarization
using entity features and probabilistic sentence coherence optimization: Uiuc at tac 2008 opinion summarization
pilot. In Proc. TAC 2008.
J R Landis and G G Koch. 1977. The measurement of observer agreement for categorical data. Biometrics,
33(1):159?174.
Kevin Lerman, Sasha Blair-Goldensohn, and Ryan McDonald. 2009. Sentiment summarization: Evaluating and
learning user preferences. In Proceedings of the 12th Conference of the European Chapter of the Association for
Computational Linguistics, EACL ?09, pages 514?522, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Baoli Li, Yandong Liu, and Eugene Agichtein. 2008a. Cocqa: Co-training over questions and answers with an
application to predicting question subjectivity orientation. In EMNLP, pages 937?946.
Wenjie Li, You Ouyang, Yi Hu, and Furu Wei. 2008b. Polyu at tac 2008. In Proc. TAC 2008.
1668
Hui Lin and Jeff Bilmes. 2011. A class of submodular functions for document summarization. In Proceedings
of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies -
Volume 1, HLT ?11, pages 510?520, Stroudsburg, PA, USA. Association for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summarization.
COLING ?00, pages 495?501, Stroudsburg, PA, USA. Association for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational
Linguistics on Human Language Technology - Volume 1, pages 71?78.
Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin, Dingyi Han, and Yong Yu. 2008. Understanding and sum-
marizing answers in community-based question answering services. In Proceedings of the 22Nd International
Conference on Computational Linguistics - Volume 1, COLING ?08, pages 497?504, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Annie Louis and Ani Nenkova. 2013. Automatically assessing machine summary content without a gold standard.
Comput. Linguist., 39(2):267?300, June.
Xiaoqiang Luo, Hema Raghavan, Vittorio Castelli, Sameer Maskey, and Radu Florian. 2013. Finding what matters
in questions. In HLT-NAACL, pages 878?887.
Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. ECIR?07,
pages 557?564, Berlin, Heidelberg. Springer-Verlag.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of
text semantic similarity. In Proceedings of the 21st National Conference on Artificial Intelligence - Volume 1,
AAAI?06, pages 775?780. AAAI Press.
G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. 1978. An analysis of approximations for maximizing submod-
ular set functionsI. Mathematical Programming, 14(1):265?294, December.
Ani Nenkova and Lucy Vanderwende. 2005. The impact of frequency on summarization. Microsoft Research,
Redmond, Washington, Tech. Rep. MSR-TR-2005-101.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?135,
January.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju. 2010. Summarizing contrastive viewpoints in opinionated
text. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP
?10, pages 66?76, Stroudsburg, PA, USA. Association for Computational Linguistics.
Ruben Sipos, Pannaga Shivaswamy, and Thorsten Joachims. 2012. Large-margin learning of submodular summa-
rization models. EACL ?12, pages 224?233, Stroudsburg, PA, USA. Association for Computational Linguistics.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and Daniel M. Ogilvie. 1966. The General Inquirer: A
Computer Approach to Content Analysis. MIT Press, Cambridge, MA.
Veselin Stoyanov and Claire Cardie. 2006. Partially supervised coreference resolution for opinion summarization
through structured rule learning. In Proceedings of the 2006 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?06, pages 336?344, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Mattia Tomasoni and Minlie Huang. 2010. Metadata-aware measures for answer summarization in community
question answering. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguis-
tics, ACL ?10, pages 760?769, Stroudsburg, PA, USA. Association for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level senti-
ment analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in
Natural Language Processing, HLT ?05, pages 347?354, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Pengtao Xie and Eric Xing. 2013. Integrating document clustering and topic modeling. In Proceedings of the
Twenty-Ninth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-13), pages 694?
703, Corvallis, Oregon. AUAI Press.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sentences. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP).
1669
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1384?1394,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Sentence Compression Based Framework to Query-Focused
Multi-Document Summarization
Lu Wang1 Hema Raghavan2 Vittorio Castelli2 Radu Florian2 Claire Cardie1
1Department of Computer Science, Cornell University, Ithaca, NY 14853, USA
{luwang, cardie}@cs.cornell.edu
2IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA
{hraghav, vittorio, raduf}@us.ibm.com
Abstract
We consider the problem of using sentence
compression techniques to facilitate query-
focused multi-document summarization. We
present a sentence-compression-based frame-
work for the task, and design a series of
learning-based compression models built on
parse trees. An innovative beam search de-
coder is proposed to efficiently find highly
probable compressions. Under this frame-
work, we show how to integrate various in-
dicative metrics such as linguistic motivation
and query relevance into the compression pro-
cess by deriving a novel formulation of a com-
pression scoring function. Our best model
achieves statistically significant improvement
over the state-of-the-art systems on several
metrics (e.g. 8.0% and 5.4% improvements in
ROUGE-2 respectively) for the DUC 2006 and
2007 summarization task.
1 Introduction
The explosion of the Internet clearly warrants
the development of techniques for organizing and
presenting information to users in an effective
way. Query-focused multi-document summariza-
tion (MDS) methods have been proposed as one
such technique and have attracted significant at-
tention in recent years. The goal of query-focused
MDS is to synthesize a brief (often fixed-length)
and well-organized summary from a set of topic-
related documents that answer a complex ques-
tion or address a topic statement. The result-
ing summaries, in turn, can support a number of
information analysis applications including open-
ended question answering, recommender systems,
and summarization of search engine results. As
further evidence of its importance, the Document
Understanding Conference (DUC) has used query-
focused MDS as its main task since 2004 to foster
new research on automatic summarization in the
context of users? needs.
To date, most top-performing systems for
multi-document summarization?whether query-
specific or not?remain largely extractive: their
summaries are comprised exclusively of sen-
tences selected directly from the documents
to be summarized (Erkan and Radev, 2004;
Haghighi and Vanderwende, 2009; Celikyilmaz
and Hakkani-Tu?r, 2011). Despite their simplicity,
extractive approaches have some disadvantages.
First, lengthy sentences that are partly relevant
are either excluded from the summary or (if se-
lected) can block the selection of other important
sentences, due to summary length constraints.
In addition, when people write summaries, they
tend to abstract the content and seldom use
entire sentences taken verbatim from the original
documents. In news articles, for example, most
sentences are lengthy and contain both potentially
useful information for a summary as well as un-
necessary details that are better omitted. Consider
the following DUC query as input for a MDS
system:1 ?In what ways have stolen artworks
been recovered? How often are suspects arrested
or prosecuted for the thefts?? One manually gen-
erated summary includes the following sentence
but removes the bracketed words in gray:
A man suspected of stealing a million-dollar collection
of [hundreds of ancient] Nepalese and Tibetan art objects in
New York [11 years ago] was arrested [Thursday at his South
Los Angeles home, where he had been hiding the antiquities,
police said].
In this example, the compressed sentence is rela-
1From DUC 2005, query for topic d422g.
1384
tively more succinct and readable than the origi-
nal (e.g. in terms of Flesch-Kincaid Reading Ease
Score (Kincaid et al, 1975)). Likewise, removing
information irrelevant to the query (e.g. ?11 years
ago?, ?police said?) is crucial for query-focused
MDS.
Sentence compression techniques (Knight and
Marcu, 2000; Clarke and Lapata, 2008) are the
standard for producing a compact and grammat-
ical version of a sentence while preserving rel-
evance, and prior research (e.g. Lin (2003)) has
demonstrated their potential usefulness for generic
document summarization. Similarly, strides have
been made to incorporate sentence compression
into query-focused MDS systems (Zajic et al,
2006). Most attempts, however, fail to produce
better results than those of the best systems built
on pure extraction-based approaches that use no
sentence compression.
In this paper we investigate the role of sentence
compression techniques for query-focused MDS.
We extend existing work in the area first by inves-
tigating the role of learning-based sentence com-
pression techniques. In addition, we design three
types of approaches to sentence-compression?
rule-based, sequence-based and tree-based?and
examine them within our compression-based
framework for query-specific MDS. Our top-
performing sentence compression algorithm in-
corporates measures of query relevance, con-
tent importance, redundancy and language qual-
ity, among others. Our tree-based methods rely on
a scoring function that allows for easy and flexi-
ble tailoring of sentence compression to the sum-
marization task, ultimately resulting in significant
improvements for MDS, while at the same time
remaining competitive with existing methods in
terms of sentence compression, as discussed next.
We evaluate the summarization models on
the standard Document Understanding Confer-
ence (DUC) 2006 and 2007 corpora 2 for query-
focused MDS and find that all of our compression-
based summarization models achieve statistically
significantly better performance than the best
DUC 2006 systems. Our best-performing sys-
tem yields an 11.02 ROUGE-2 score (Lin and
Hovy, 2003), a 8.0% improvement over the best
reported score (10.2 (Davis et al, 2012)) on the
2We believe that we can easily adapt our system for tasks
(e.g. TAC-08?s opinion summarization or TAC-09?s update
summarization) or domains (e.g. web pages or wikipedia
pages). We reserve that for future work.
DUC 2006 dataset, and an 13.49 ROUGE-2, a
5.4% improvement over the best score in DUC
2007 (12.8 (Davis et al, 2012)). We also ob-
serve substantial improvements over previous sys-
tems w.r.t. the manual Pyramid (Nenkova and
Passonneau, 2004) evaluation measure (26.4 vs.
22.9 (Jagarlamudi et al, 2006)); human annota-
tors furthermore rate our system-generated sum-
maries as having less redundancy and compara-
ble quality w.r.t. other linguistic quality metrics.
With these results we believe we are the first
to successfully show that sentence compression
can provide statistically significant improvements
over pure extraction-based approaches for query-
focused MDS.
2 Related Work
Existing research on query-focused multi-
document summarization (MDS) largely relies
on extractive approaches, where systems usually
take as input a set of documents and select
the top relevant sentences for inclusion in the
final summary. A wide range of methods have
been employed for this task. For unsupervised
methods, sentence importance can be estimated
by calculating topic signature words (Lin and
Hovy, 2000; Conroy et al, 2006), combining
query similarity and document centrality within
a graph-based model (Otterbacher et al, 2005),
or using a Bayesian model with sophisticated
inference (Daume? and Marcu, 2006). Davis et
al. (2012) first learn the term weights by Latent
Semantic Analysis, and then greedily select
sentences that cover the maximum combined
weights. Supervised approaches have mainly
focused on applying discriminative learning for
ranking sentences (Fuentes et al, 2007). Lin and
Bilmes (2011) use a class of carefully designed
submodular functions to reward the diversity of
the summaries and select sentences greedily.
Our work is more related to the less studied
area of sentence compression as applied to (sin-
gle) document summarization. Zajic et al (2006)
tackle the query-focused MDS problem using a
compress-first strategy: they develop heuristics to
generate multiple alternative compressions of all
sentences in the original document; these then be-
come the candidates for extraction. This approach,
however, does not outperform some extraction-
based approaches. A similar idea has been stud-
ied for MDS (Lin, 2003; Gillick and Favre, 2009),
1385
but limited improvement is observed over extrac-
tive baselines with simple compression rules. Fi-
nally, although learning-based compression meth-
ods are promising (Martins and Smith, 2009;
Berg-Kirkpatrick et al, 2011), it is unclear how
well they handle issues of redundancy.
Our research is also inspired by probabilis-
tic sentence-compression approaches, such as the
noisy-channel model (Knight and Marcu, 2000;
Turner and Charniak, 2005), and its extension via
synchronous context-free grammars (SCFG) (Aho
and Ullman, 1969; Lewis and Stearns, 1968) for
robust probability estimation (Galley and McKe-
own, 2007). Rather than attempt to derive a new
parse tree like Knight and Marcu (2000) and Gal-
ley and McKeown (2007), we learn to safely re-
move a set of constituents in our parse tree-based
compression model while preserving grammati-
cal structure and essential content. Sentence-level
compression has also been examined via a dis-
criminative model McDonald (2006), and Clarke
and Lapata (2008) also incorporate discourse in-
formation by using integer linear programming.
3 The Framework
We now present our query-focused MDS frame-
work consisting of three steps: Sentence Rank-
ing, Sentence Compression and Post-processing.
First, sentence ranking determines the importance
of each sentence given the query. Then, a sen-
tence compressor iteratively generates the most
likely succinct versions of the ranked sentences,
which are cumulatively added to the summary, un-
til a length limit is reached. Finally, the post-
processing stage applies coreference resolution
and sentence reordering to build the summary.
Sentence Ranking. This stage aims to rank sen-
tences in order of relevance to the query. Un-
surprisingly, ranking algorithms have been suc-
cessfully applied to this task. We experimented
with two of them ? Support Vector Regres-
sion (SVR) (Mozer et al, 1997) and Lamb-
daMART (Burges et al, 2007). The former
has been used previously for MDS (Ouyang et
al., 2011). LambdaMart on the other hand has
shown considerable success in information re-
trieval tasks (Burges, 2010); we are the first to
apply it to summarization. For training, we use
40 topics (i.e. queries) from the DUC 2005 cor-
pus (Dang, 2005) along with their manually gener-
ated abstracts. As in previous work (Shen and Li,
Basic Features
relative/absolute position
is among the first 1/3/5 sentences?
number of words (with/without stopwords)
number of words more than 5/10 (with/without stopwords)
Query-Relevant Features
unigram/bigram/skip bigram (at most four words apart) overlap
unigram/bigram TF/TF-IDF similarity
mention overlap
subject/object/indirect object overlap
semantic role overlap
relation overlap
Query-Independent Features
average/total unigram/bigram IDF/TF-IDF
unigram/bigram TF/TF-IDF similarity with the centroid of the cluster
average/sum of sumBasic/SumFocus (Toutanova et al, 2007)
average/sum of mutual information
average/sum of number of topic signature words (Lin and Hovy, 2000)
basic/improved sentence scorers from Conroy et al (2006)
Content Features
contains verb/web link/phone number?
contains/portion of words between parentheses
Table 1: Sentence-level features for sentence ranking.
2011; Ouyang et al, 2011), we use the ROUGE-
2 score, which measures bigram overlap between
a sentence and the abstracts, as the objective for
regression.
While space limitations preclude a longer dis-
cussion of the full feature set (ref. Table 1), we
describe next the query-relevant features used for
sentence ranking as these are the most impor-
tant for our summarization setting. The goal of
this feature subset is to determine the similarity
between the query and each candidate sentence.
When computing similarity, we remove stopwords
as well as the words ?discuss, describe, specify,
explain, identify, include, involve, note? that are
adopted and extended from Conroy et al (2006).
Then we conduct simple query expansion based
on the title of the topic and cross-document coref-
erence resolution. Specifically, we first add the
words from the topic title to the query. And for
each mention in the query, we add other mentions
within the set of documents that corefer with this
mention. Finally, we compute two versions of the
features?one based on the original query and an-
other on the expanded one. We also derive the
semantic role overlap and relation instance over-
lap between the query and each sentence. Cross-
document coreference resolution, semantic role la-
beling and relation extraction are accomplished
via the methods described in Section 5.
Sentence Compression. As the main focus of
this paper, we propose three types of compression
methods, described in detail in Section 4 below.
Post-processing. Post-processing performs
coreference resolution and sentence ordering.
1386
Basic Features Syntactic Tree Features
first 1/3/5 tokens (toks)? POS tag
last 1/3/5 toks? parent/grandparent label
first letter/all letters capitalized? leftmost child of parent?
is negation? second leftmost child of parent?
is stopword? is headword?
Dependency Tree Features in NP/VP/ADVP/ADJP chunk?
dependency relation (dep rel) Semantic Features
parent/grandparent dep rel is a predicate?
is the root? semantic role label
has a depth larger than 3/5?
Rule-Based Features
For each rule in Table 2 , we construct a corresponding feature to
indicate whether the token is identified by the rule.
Table 3: Token-level features for sequence-based com-
pression.
We replace each pronoun with its referent unless
they appear in the same sentence. For sentence
ordering, each compressed sentence is assigned
to the most similar (tf-idf) query sentence. Then
a Chronological Ordering algorithm (Barzilay et
al., 2002) sorts the sentences for each query based
first on the time stamp, and then the position in
the source document.
4 Sentence Compression
Sentence compression is typically formulated as
the problem of removing secondary information
from a sentence while maintaining its grammati-
cality and semantic structure (Knight and Marcu,
2000; McDonald, 2006; Galley and McKeown,
2007; Clarke and Lapata, 2008). We leave other
rewrite operations, such as paraphrasing and re-
ordering, for future work. Below we describe
the sentence compression approaches developed
in this research: RULE-BASED COMPRESSION,
SEQUENCE-BASED COMPRESSION, and TREE-
BASED COMPRESSION.
4.1 Rule-based Compression
Turner and Charniak (2005) have shown that ap-
plying hand-crafted rules for trimming sentences
can improve both content and linguistic qual-
ity. Our rule-based approach extends existing
work (Conroy et al, 2006; Toutanova et al, 2007)
to create the linguistically-motivated compression
rules of Table 2. To avoid ill-formed output, we
disallow compressions of more than 10 words by
each rule.
4.2 Sequence-based Compression
As in McDonald (2006) and Clarke and Lapata
(2008), our sequence-based compression model
makes a binary ?keep-or-delete? decision for each
word in the sentence. In contrast, however, we
Figure 1: Diagram of tree-based compression. The
nodes to be dropped are grayed out. In this example,
the root of the gray subtree (a ?PP?) would be labeled
REMOVE. Its siblings and parent are labeled RETAIN
and PARTIAL, respectively. The trimmed tree is real-
ized as ?Malaria causes millions of deaths.?
view compression as a sequential tagging problem
and make use of linear-chain Conditional Ran-
dom Fields (CRFs) (Lafferty et al, 2001) to se-
lect the most likely compression. We represent
each sentence as a sequence of tokens, X =
x0x1 . . . xn, and generate a sequence of labels,
Y = y0y1 . . . yn, that encode which tokens are
kept, using a BIO label format: {B-RETAIN de-
notes the beginning of a retained sequence, I-
RETAIN indicates tokens ?inside? the retained se-
quence, O marks tokens to be removed}.
The CRF model is built using the features
shown in Table 3. ?Dependency Tree Features?
encode the grammatical relations in which each
word is involved as a dependent. For the ?Syntac-
tic Tree?, ?Dependency Tree? and ?Rule-Based?
features, we also include features for the two
words that precede and the two that follow the cur-
rent word. Detailed descriptions of the training
data and experimental setup are in Section 5.
During inference, we find the maximally likely
sequence Y according to a CRF with parameter
? (Y = argmaxY ? P (Y ?|X; ?)), while simulta-
neously enforcing the rules of Table 2 to reduce
the hypothesis space and encourage grammatical
compression. To do this, we encode these rules as
features for each token, and whenever these fea-
ture functions fire, we restrict the possible label
for that token to ?O?.
4.3 Tree-based Compression
Our tree-based compression methods are in line
with syntax-driven approaches (Galley and McK-
eown, 2007), where operations are carried out
on parse tree constituents. Unlike previous
work (Knight and Marcu, 2000; Galley and McK-
eown, 2007), we do not produce a new parse tree,
1387
Rule Example
Header [MOSCOW , October 19 ( Xinhua ) ?] Russian federal troops Tuesday continued...
Relative dates ...Centers for Disease Control confirmed [Tuesday] that there was...
Intra-sentential attribution ...fueling the La Nina weather phenomenon, [the U.N. weather agency said].
Lead adverbials [Interestingly], while the Democrats tend to talk about...
Noun appositives Wayne County Prosecutor [John O?Hara] wanted to send a message...
Nonrestrictive relative clause Putin, [who was born on October 7, 1952 in Leningrad], was elected in the presidential election...
Adverbial clausal modifiers [Starting in 1998], California will require 2 per cent of a manufacturer...
(Lead sentence) [Given the short time], car makers see electric vehicles as...
Within Parentheses ...to Christian home schoolers in the early 1990s [(www.homecomputermarket.com)].
Table 2: Linguistically-motivated rules for sentence compression. The grayed-out words in brackets are removed.
but focus on learning to identify the proper set of
constituents to be removed. In particular, when a
node is dropped from the tree, all words it sub-
sumes will be deleted from the sentence.
Formally, given a parse tree T of the sentence
to be compressed and a tree traversal algorithm,
T can be presented as a list of ordered constituent
nodes, T = t0t1 . . . tm. Our objective is to find a
set of labels, L = l0l1 . . . lm, where li ? {RETAIN,
REMOVE, PARTIAL}. RETAIN (RET) and RE-
MOVE (REM) denote whether the node ti is re-
tained or removed. PARTIAL (PAR) means ti is
partly removed, i.e. at least one child subtree of ti
is dropped.
Labels are identified, in order, according to the
tree traversal algorithm. Every node label needs
to be compatible with the labeling history: given
a node ti, and a set of labels l0 . . . li?1 predicted
for nodes t0 . . . ti?1, li =RET or li =REM is com-
patible with the history when all children of ti are
labeled as RET or REM, respectively; li =PAR is
compatible when ti has at least two descendents
tj and tk (j < i and k < i), one of which is
RETained and the other, REMoved. As such, the
root of the gray subtree in Figure 1 is labeled as
REM; its left siblings as RET; its parent as PAR.
As the space of possible compressions is expo-
nential in the number of leaves in the parse tree,
instead of looking for the globally optimal solu-
tion, we use beam search to find a set of highly
likely compressions and employ a language model
trained on a large corpus for evaluation.
A Beam Search Decoder. The beam search de-
coder (see Algorithm 1) takes as input the sen-
tence?s parse tree T = t0t1 . . . tm, an order-
ing O for traversing T (e.g. postorder) as a se-
quence of nodes in T , the set L of possible
node labels, a scoring function S for evaluat-
ing each sentence compression hypothesis, and
a beam size N . Specifically, O is a permuta-
tion on the set {0, 1, . . . ,m}?each element an
index onto T . Following O, T is re-ordered as
tO0tO1 . . . tOm , and the decoder considers each or-
dered constituent tOi in turn. In iteration i, all
existing sentence compression hypotheses are ex-
panded by one node, tOi , labeling it with all com-
patible labels. The new hypotheses (usually sub-
sentences) are ranked by the scorer S and the top
N are preserved to be extended in the next itera-
tion. See Figure 2 for an example.
Input : parse tree T , ordering O = O0O1 . . . Om,
L ={RET, REM, PAR}, hypothesis scorer S,
beam size N
Output: N best compressions
stack? ? (empty set);
foreach node tOi in T = tO0 . . . tOm doif i == 0 (first node visited) then
foreach label lO0 in L donewHypothesis h? ? [lO0 ];put h? into Stack;
end
else
newStack? ? (empty set);
foreach hypothesis h in stack do
foreach label lOi in L doif lOi is compatible thennewHypothesis h? ? h + [lOi ];put h? into newStack;
end
end
end
stack? newStack;
end
Apply S to sort hypotheses in stack in descending
order;
Keep the N best hypotheses in stack;
end
Algorithm 1: Beam search decoder.
Our BASIC Tree-based Compression in-
stantiates the beam search decoder with
postorder traversal and a hypothesis scorer
that takes a possible sentence compression?
a sequence of nodes (e.g. tO0 . . . tOk ) and
their labels (e.g. lO0 . . . lOk )?and returns?k
j=1 logP (lOj |tOj ) (denoted later as
ScoreBasic). The probability is estimated by
a Maximum Entropy classifier (Berger et al,
1388
Figure 2: Example of beam search decoding. For
postorder traversal, the three nodes are visited in a
bottom-up order. The associated compression hypothe-
ses (boxed) are ranked based on the scores in parenthe-
ses. Beam scores for other nodes are omitted.
Basic Features Syntactic Tree Features
projection falls w/in first 1/3/5 toks?? constituent label
projection falls w/in last 1/3/5 toks?? parent left/right sibling label
subsumes first 1/3/5 toks?? grandparent left/right sibling label
subsumes last 1/3/5 toks?? is leftmost child of parent?
number of words larger than 5/10?? is second leftmost child of parent?
is leaf node?? is head node of parent?
is root of parsing tree?? label of its head node
has word with first letter capitalized? has a depth greater than 3/5/10?
has word with all letters capitalized? Dependency Tree Features
has negation? dep rel of head node?
has stopwords? dep rel of parent?s head node?
Semantic Features dep rel of grandparent?s head node?
the head node has predicate? contain root of dep tree??
semantic roles of head node has a depth larger than 3/5??
Rule-Based Features
For each rule in Table 2 , we construct a corresponding feature to indicate
whether the token is identified by the rule.
Table 4: Constituent-level features for tree-based com-
pression. ? or ? denote features that are concatenated
with every Syntactic Tree feature to compose a new
one.
1996) trained at the constituent level using the
features in Table 4. We also apply the rules of
Table 2 during the decoding process. Concretely,
if the words subsumed by a node are identified
by any rule, we only consider REM as the node?s
label.
Given the N -best compressions from the de-
coder, we evaluate the yield of the trimmed trees
using a language model trained on the Giga-
word (Graff, 2003) corpus and return the compres-
sion with the highest probability. Thus, the de-
coder is quite flexible ? its learned scoring func-
tion allows us to incorporate features salient for
sentence compression while its language model
guarantees the linguistic quality of the compressed
string. In the sections below we consider addi-
tional improvements.
4.3.1 Improving Beam Search
CONTEXT-aware search is based on the intu-
ition that predictions on preceding context can
be leveraged to facilitate the prediction of the
current node. For example, parent nodes with
children that have all been removed (retained)
should have a label of REM (RET). In light of
this, we encode these contextual predictions as
additional features of S, that is, ALL-CHILDREN-
REMOVED/RETAINED, ANY-LEFTSIBLING-
REMOVED/RETAINED/PARTLY REMOVED,
LABEL-OF-LEFT-SIBLING/HEAD-NODE.
HEAD-driven search modifies the BASIC pos-
torder tree traversal by visiting the head node first
at each level, leaving other orders unchanged. In
a nutshell, if the head node is dropped, then its
modifiers need not be preserved. We adopt the
same features as CONTEXT-aware search, but re-
move those involving left siblings. We also add
one more feature: LABEL-OF-THE-HEAD-NODE-
IT-MODIFIES.
4.3.2 Task-Specific Sentence Compression
The current scorer ScoreBasic is still fairly naive
in that it focuses only on features of the sen-
tence to be compressed. However extra-sentential
knowledge can also be important for query-
focused MDS. For example, information regard-
ing relevance to the query might lead the de-
coder to produce compressions better suited for
the summary. Towards this goal, we construct
a compression scoring function?the multi-scorer
(MULTI)?that allows the incorporation of mul-
tiple task-specific scorers. Given a hypothesis at
any stage of decoding, which yields a sequence of
words W = w0w1...wj , we propose the following
component scorers.
Query Relevance. Query information ought to
guide the compressor to identify the relevant con-
tent. The query Q is expanded as described in
Section 3. Let |W ? Q| denote the number of
unique overlapping words betweenW andQ, then
scoreq = |W ?Q|/|W |.
Importance. A query-independent impor-
tance score is defined as the average Sum-
Basic (Toutanova et al, 2007) value in W ,
i.e. scoreim =?ji=1 SumBasic(wi)/|W |.
Language Model. We let scorelm be the proba-
bility of W computed by a language model.
Cross-Sentence Redundancy. To encourage di-
versified content, we define a redundancy score to
discount replicated content: scorered = 1? |W ?
C|/|W |, whereC is the words already selected for
the summary.
1389
The multi-scorer is defined as a linear
combination of the component scorers: Let
~? = (?0, . . . , ?4), 0 ? ?i ? 1, ????score =
(scoreBasic, scoreq, scoreim, scorelm, scorered),
S = scoremulti = ~? ? ????score (1)
The parameters ~? are tuned on a held-out tuning
set by grid search. We linearly normalize the score
of each metric, where the minimum and maximum
values are estimated from the tuning data.
5 Experimental Setup
We evaluate our methods on the DUC 2005, 2006
and 2007 datasets (Dang, 2005; Dang, 2006;
Dang, 2007), each of which is a collection of
newswire articles. 50 complex queries (topics) are
provided for DUC 2005 and 2006, 35 are collected
for DUC 2007 main task. Relevant documents for
each query are provided along with 4 to 9 human
MDS abstracts. The task is to generate a summary
within 250 words to address the query. We split
DUC 2005 into two parts: 40 topics to train the
sentence ranking models, and 10 for ranking algo-
rithm selection and parameter tuning for the multi-
scorer. DUC 2006 and DUC 2007 are reserved as
held out test sets.
Sentence Compression. The dataset
from Clarke and Lapata (2008) is used to
train the CRF and MaxEnt classifiers (Section 4).
It includes 82 newswire articles with one manually
produced compression aligned to each sentence.
Preprocessing. Documents are processed by a
full NLP pipeline, including token and sentence
segmentation, parsing, semantic role labeling,
and an information extraction pipeline consist-
ing of mention detection, NP coreference, cross-
document resolution, and relation detection (Flo-
rian et al, 2004; Luo et al, 2004; Luo and Zitouni,
2005).
Learning for Sentence Ranking and Compres-
sion. We use Weka (Hall et al, 2009) to train a
support vector regressor and experiment with var-
ious rankers in RankLib (Dang, 2011)3. As Lamb-
daMART has an edge over other rankers on the
held-out dataset, we selected it to produce ranked
sentences for further processing. For sequence-
based compression using CRFs, we employ Mal-
let (McCallum, 2002) and integrate the Table 2
rules during inference. NLTK (Bird et al, 2009)
3Default parameters are used. If an algorithm needs a val-
idation set, we use 10 out of 40 topics.
MaxEnt classifiers are used for tree-based com-
pression. Beam size is fixed at 2000.4 Sen-
tence compressions are evaluated by a 5-gram lan-
guage model trained on Gigaword (Graff, 2003)
by SRILM (Stolcke, 2002).
6 Results
The results in Table 5 use the official ROUGE soft-
ware with standard options5 and report ROUGE-
2 (R-2) (measures bigram overlap) and ROUGE-
SU4 (R-SU4) (measures unigram and skip-bigram
separated by up to four words). We compare our
sentence-compression-based methods to the best
performing systems based on ROUGE in DUC
2006 and 2007 (Jagarlamudi et al, 2006; Pingali
et al, 2007), system by Davis et al (2012) that
report the best R-2 score on DUC 2006 and 2007
thus far, and to the purely extractive methods of
SVR and LambdaMART.
Our sentence-compression-based systems
(marked with ?) show statistically significant
improvements over pure extractive summarization
for both R-2 and R-SU4 (paired t-test, p < 0.01).
This means our systems can effectively remove
redundancy within the summary through compres-
sion. Furthermore, our HEAD-driven beam search
method with MULTI-scorer beats all systems on
DUC 20066 and all systems on DUC 2007 except
the best system in terms of R-2 (p < 0.01). Its
R-SU4 score is also significantly (p < 0.01)
better than extractive methods, rule-based and
sequence-based compression methods on both
DUC 2006 and 2007. Moreover, our systems with
learning-based compression have considerable
compression rates, indicating their capability to
remove superfluous words as well as improve
summary quality.
Human Evaluation. The Pyramid (Nenkova
and Passonneau, 2004) evaluation was developed
to manually assess how many relevant facts or
Summarization Content Units (SCUs) are cap-
tured by system summaries. We ask a professional
annotator (who is not one of the authors, is highly
experienced in annotating for various NLP tasks,
and is fluent in English) to carry out a Pyramid
evaluation on 10 randomly selected topics from
4We looked at various beam sizes on the heldout data, and
observed that the performance peaks around this value.
5ROUGE-1.5.5.pl -n 4 -w 1.2 -m -2 4 -u -c 95 -r 1000 -f
A -p 0.5 -t 0 -a -d
6The system output from Davis et al (2012) is not avail-
able, so significance tests are not conducted on it.
1390
DUC 2006 DUC 2007
System C Rate R-2 R-SU4 C Rate R-2 R-SU4
Best DUC system ? 9.56 15.53 ? 12.62 17.90
Davis et al (2012) ? 10.2 15.2 ? 12.8 17.5
SVR 100% 7.78 13.02 100% 9.53 14.69
LambdaMART 100% 9.84 14.63 100% 12.34 15.62
Rule-based 78.99% 10.62 ?? 15.73 ? 78.11% 13.18? 18.15?
Sequence 76.34% 10.49 ? 15.60 ? 77.20% 13.25? 18.23?
Tree (BASIC + ScoreBasic) 70.48% 10.49 ? 15.86 ? 69.27% 13.00? 18.29?
Tree (CONTEXT + ScoreBasic) 65.21% 10.55 ?? 16.10 ? 63.44% 12.75 18.07?
Tree (HEAD + ScoreBasic) 66.70% 10.66 ?? 16.18 ? 65.05% 12.93 18.15?
Tree (HEAD + MULTI) 70.20% 11.02 ?? 16.25 ? 73.40% 13.49? 18.46?
Table 5: Query-focused MDS performance comparison: C Rate or compression rate is the proportion of words
preserved. R-2 (ROUGE-2) and R-SU4 (ROUGE-SU4) scores are multiplied by 100. ??? indicates that data is
unavailable. BASIC, CONTEXT and HEAD represent the basic beam search decoder, context-aware and head-driven
search extensions respectively. ScoreBasic and MULTI refer to the type of scorer used. Statistically significant
improvements (p < 0.01) over the best system in DUC 06 and 07 are marked with ?. ? indicates statistical
significance (p < 0.01) over extractive approaches (SVR or LambdaMART). HEAD + MULTI outperforms all the
other extract- and compression-based systems in R-2.
System Pyr Gra Non-Red Ref Foc Coh
Best DUC system (ROUGE) 22.9?8.2 3.5?0.9 3.5?1.0 3.5?1.1 3.6?1.0 2.9?1.1
Best DUC system (LQ) ? 4.0?0.8 4.2?0.7 3.8?0.7 3.6?0.9 3.4?0.9
Our System 26.4?10.3 3.0?0.9 4.0?1.1 3.6?1.0 3.4?0.9 2.8?1.0
Table 6: Human evaluation on our multi-scorer based system, Jagarlamudi et al (2006) (Best DUC system
(ROUGE)), and Lacatusu et al (2006) (Best DUC system (LQ)). Our system can synthesize more relevant content
according to Pyramid (?100). We also examine linguistic quality (LQ) in Grammaticality (Gra), Non-redundancy
(Non-Red), Referential clarity (Ref), Focus (Foc), and Structure and Coherence (Coh) like Dang (2006), each rated
from 1 (very poor) to 5 (very good). Our system has better non-redundancy than Jagarlamudi et al (2006) and is
comparable to Jagarlamudi et al (2006) and Lacatusu et al (2006) in other metrics except grammaticality.
the DUC 2006 task with gold-standard SCU an-
notation in abstracts. The Pyramid score (see Ta-
ble 6) is re-calculated for the system with best
ROUGE scores in DUC 2006 (Jagarlamudi et al,
2006) along with our system by the same annota-
tor to make a meaningful comparison.
We further evaluate the linguistic quality (LQ)
of the summaries for the same 10 topics in ac-
cordance with the measurement in Dang (2006).
Four native speakers who are undergraduate stu-
dents in computer science (none are authors) per-
formed the task, We compare our system based
on HEAD-driven beam search with MULTI-scorer
to the best systems in DUC 2006 achieving top
ROUGE scores (Jagarlamudi et al, 2006) (Best
DUC system (ROUGE)) and top linguistic quality
scores (Lacatusu et al, 2006) (Best DUC system
(LQ))7. The average score and standard deviation
for each metric is displayed in Table 6. Our sys-
tem achieves a higher Pyramid score, an indication
that it captures more of the salient facts. We also
7Lacatusu et al (2006) obtain the best scores in three lin-
guistic quality metrics (i.e. grammaticality, focus, structure
and coherence), and overall responsiveness on DUC 2006.
attain better non-redundancy than Jagarlamudi et
al. (2006), meaning that human raters perceive
less replicative content in our summaries. Scores
for other metrics are comparable to Jagarlamudi
et al (2006) and Lacatusu et al (2006), which
either uses minimal non-learning-based compres-
sion rules or is a pure extractive system. However,
our compression system sometimes generates less
grammatical sentences, and those are mostly due
to parsing errors. For example, parsing a clause
starting with a past tense verb as an adverbial
clausal modifier can lead to an ill-formed com-
pression. Those issues can be addressed by an-
alyzing k-best parse trees and we leave it in the
future work. A sample summary from our multi-
scorer based system is in Figure 3.
Sentence Compression Evaluation. We
also evaluate sentence compression separately
on (Clarke and Lapata, 2008), adopting the same
partitions as (Martins and Smith, 2009), i.e. 1, 188
sentences for training and 441 for testing. Our
compression models are compared with Hedge
Trimmer (Dorr et al, 2003), a discriminative
model proposed by McDonald (2006) and a
1391
System C Rate Uni-Prec Uni-Rec Uni-F1 Rel-F1
HedgeTrimmer 57.64% 0.72 0.65 0.64 0.50
McDonald (2006) 70.95% 0.77 0.78 0.77 0.55
Martins and Smith (2009) 71.35% 0.77 0.78 0.77 0.56
Rule-based 87.65% 0.74 0.91 0.80 0.63
Sequence 70.79% 0.77 0.80 0.76 0.58
Tree (BASIC) 69.65% 0.77 0.79 0.75 0.56
Tree (CONTEXT) 67.01% 0.79 0.78 0.76 0.57
Tree (HEAD) 68.06% 0.79 0.80 0.77 0.59
Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
all use single-scorer. Our context-aware and head-driven tree-based approaches outperform all the other systems
significantly (p < 0.01) in precision (Uni-Prec) without sacrificing the recalls (i.e. there is no statistically signifi-
cant difference between our models and McDonald (2006) / M & S (2009) with p > 0.05). Italicized numbers for
unigram F1 (Uni-F1) are statistically indistinguishable (p > 0.05). Our head-driven tree-based approach also pro-
duces significantly better grammatical relations F1 scores (Rel-F1) than all the other systems except the rule-based
method (p < 0.01).
Topic D0626H: How were the bombings of the US em-
bassies in Kenya and Tanzania conducted? What terror-
ist groups and individuals were responsible? How and
where were the attacks planned?
WASHINGTON, August 13 (Xinhua) ? President Bill
Clinton Thursday condemned terrorist bomb attacks at
U.S. embassies in Kenya and Tanzania and vowed to find
the bombers and bring them to justice. Clinton met with
his top aides Wednesday in the White House to assess the
situation following the twin bombings at U.S. embassies
in Kenya and Tanzania, which have killed more than 250
people and injured over 5,000, most of them Kenyans and
Tanzanians. Local sources said the plan to bomb U.S. em-
bassies in Kenya and Tanzania took three months to com-
plete and bombers destined for Kenya were dispatched
through Somali and Rwanda. FBI Director Louis Freeh,
Attorney General Janet Reno and other senior U.S. gov-
ernment officials will hold a news conference at 1 p.m.
EDT (1700GMT) at FBI headquarters in Washington ?to
announce developments in the investigation of the bomb-
ings of the U.S. embassies in Kenya and Tanzania,? the
FBI said in a statement. ...
Figure 3: Part of the summary generated by the multi-
scorer based summarizer for topic D0626H (DUC
2006). Grayed out words are removed. Query-
irrelevant phrases, such as temporal information or
source of the news, have been removed.
dependency-tree based compressor (Martins and
Smith, 2009)8. We adopt the metrics in Martins
and Smith (2009) to measure the unigram-level
macro precision, recall, and F1-measure with
respect to human annotated compression. In
addition, we also compute the F1 scores of
grammatical relations which are annotated by
RASP (Briscoe and Carroll, 2002) according
to Clarke and Lapata (2008).
In Table 7, our context-aware and head-driven
tree-based compression systems show statistically
significantly (p < 0.01) higher precisions (Uni-
8Thanks to Andre? F.T. Martins for system outputs.
Prec) than all the other systems, without decreas-
ing the recalls (Uni-Rec) significantly (p > 0.05)
based on a paired t-test. Unigram F1 scores (Uni-
F1) in italics indicate that the corresponding sys-
tems are not statistically distinguishable (p >
0.05). For grammatical relation evaluation, our
head-driven tree-based system obtains statistically
significantly (p < 0.01) better F1 score (Rel-F1
than all the other systems except the rule-based
system).
7 Conclusion
We have presented a framework for query-focused
multi-document summarization based on sentence
compression. We propose three types of com-
pression approaches. Our tree-based compres-
sion method can easily incorporate measures of
query relevance, content importance, redundancy
and language quality into the compression pro-
cess. By testing on a standard dataset using the
automatic metric ROUGE, our models show sub-
stantial improvement over pure extraction-based
methods and state-of-the-art systems. Our best
system also yields better results for human eval-
uation based on Pyramid and achieves comparable
linguistic quality scores.
Acknowledgments
This work was supported in part by National Sci-
ence Foundation Grant IIS-0968450 and a gift
from Boeing. We thank Ding-Jung Han, Young-
Suk Lee, Xiaoqiang Luo, Sameer Maskey, Myle
Ott, Salim Roukos, Yiye Ruan, Ming Tan, Todd
Ward, Bowen Zhou, and the ACL reviewers for
valuable suggestions and advice on various as-
pects of this work.
1392
References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax directed
translations and the pushdown assembler. J. Comput. Syst.
Sci., 3(1):37?56.
Regina Barzilay, Noemie Elhadad, and Kathleen R. McKe-
own. 2002. Inferring strategies for sentence ordering in
multidocument news summarization. J. Artif. Int. Res.,
17(1):35?55, August.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011.
Jointly learning to extract and compress. ACL ?11, pages
481?490, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Comput. Linguist.,
22(1):39?71, March.
Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural
Language Processing with Python. O?Reilly Media.
T. Briscoe and J. Carroll. 2002. Robust accurate statistical
annotation of general text.
Christopher J.C. Burges, Robert Ragno, and Quoc Viet Le.
2007. Learning to rank with nonsmooth cost functions. In
B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Advances
in Neural Information Processing Systems 19, pages 193?
200. MIT Press, Cambridge, MA.
Christopher J. C. Burges. 2010. From RankNet to Lamb-
daRank to LambdaMART: An overview. Technical report,
Microsoft Research.
Asli Celikyilmaz and Dilek Hakkani-Tu?r. 2011. Discovery
of topically coherent sentences for extractive summariza-
tion. ACL ?11, pages 491?499, Stroudsburg, PA, USA.
Association for Computational Linguistics.
James Clarke and Mirella Lapata. 2008. Global inference
for sentence compression an integer linear programming
approach. J. Artif. Int. Res., 31(1):399?429, March.
John M. Conroy, Judith D. Schlesinger, Dianne P. O?Leary,
and Jade Goldstein, 2006. Back to Basics: CLASSY 2006.
U.S. National Inst. of Standards and Technology.
Hoa T. Dang. 2005. Overview of DUC 2005. In Document
Understanding Conference.
Hoa Tran Dang. 2006. Overview of DUC 2006. In
Proc. Document Understanding Workshop, page 10 pages.
NIST.
Hoa T. Dang. 2007. Overview of DUC 2007. In Document
Understanding Conference.
Van Dang. 2011. RankLib. Online.
Hal Daume?, III and Daniel Marcu. 2006. Bayesian
query-focused summarization. ACL ?06, pages 305?312,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Sashka T. Davis, John M. Conroy, and Judith D. Schlesinger.
2012. Occams - an optimal combinatorial covering algo-
rithm for multi-document summarization. In ICDM Work-
shops, pages 454?463.
Bonnie J Dorr, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: a parse-and-trim approach to headline
generation. In Proceedings of the HLT-NAACL 03 on
Text summarization workshop - Volume 5, HLT-NAACL-
DUC ?03, pages 1 ? 8, Stroudsburg, PA, USA. Association
for Computational Linguistics, Association for Computa-
tional Linguistics.
Gu?nes Erkan and Dragomir R. Radev. 2004. Lexrank: graph-
based lexical centrality as salience in text summarization.
J. Artif. Int. Res., 22(1):457?479, December.
Radu Florian, Hany Hassan, Abraham Ittycheriah, Hongyan
Jing, Nanda Kambhatla, Xiaoqiang Luo, Nicolas Nicolov,
and Salim Roukos. 2004. A statistical model for multilin-
gual entity detection and tracking. In HLT-NAACL, pages
1?8.
Maria Fuentes, Enrique Alfonseca, and Horacio Rodr??guez.
2007. Support vector machines for query-focused sum-
marization trained and evaluated on pyramid data. In Pro-
ceedings of the 45th Annual Meeting of the ACL on In-
teractive Poster and Demonstration Sessions, ACL ?07,
pages 57?60, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Michel Galley and Kathleen McKeown. 2007. Lexicalized
Markov grammars for sentence compression. NAACL
?07, pages 180?187, Rochester, New York, April. Asso-
ciation for Computational Linguistics.
Dan Gillick and Benoit Favre. 2009. A scalable global model
for summarization. In Proceedings of the Workshop on In-
teger Linear Programming for Natural Langauge Process-
ing, ILP ?09, pages 10?18, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
David Graff. 2003. English Gigaword.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-document summarization.
NAACL ?09, pages 362?370, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten. 2009.
The weka data mining software: an update. SIGKDD Ex-
plor. Newsl., 11(1):10?18, November.
Jagadeesh Jagarlamudi, Prasad Pingali, and Vasudeva Varma,
2006. Query Independent Sentence Scoring approach to
DUC 2006.
J. Peter Kincaid, Robert P. Fishburne, Richard L. Rogers, and
Brad S. Chissom. 1975. Derivation of New Readability
Formulas (Automated Readability Index, Fog Count and
Flesch Reading Ease Formula) for Navy Enlisted Person-
nel. Technical report, February.
Kevin Knight and Daniel Marcu. 2000. Statistics-based sum-
marization - step one: Sentence compression. AAAI ?00,
pages 703?710. AAAI Press.
Finley Lacatusu, Andrew Hickl, Kirk Roberts, Ying Shi,
Jeremy Bensley, Bryan Rink, Patrick Wang, and Lara Tay-
lor, 2006. LCCs gistexter at duc 2006: Multi-strategy
multi-document summarization.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
1393
Proceedings of the Eighteenth International Conference
on Machine Learning, ICML ?01, pages 282?289, San
Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
P. M. Lewis, II and R. E. Stearns. 1968. Syntax-directed
transduction. J. ACM, 15(3):465?488, July.
Hui Lin and Jeff Bilmes. 2011. A class of submodular func-
tions for document summarization. In Proceedings of the
49th Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume 1,
HLT ?11, pages 510?520, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2000. The automated ac-
quisition of topic signatures for text summarization. In
Proceedings of the 18th conference on Computational
linguistics - Volume 1, COLING ?00, pages 495?501,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic eval-
uation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computational
Linguistics on Human Language Technology - Volume 1,
pages 71?78.
Chin-Yew Lin. 2003. Improving summarization perfor-
mance by sentence compression: a pilot study. In Pro-
ceedings of the sixth international workshop on Informa-
tion retrieval with Asian languages - Volume 11, AsianIR
?03, pages 1?8, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-lingual
coreference resolution with syntactic features. In
HLT/EMNLP.
Xiaoqiang Luo, Abraham Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based on
the bell tree. In ACL, pages 135?142.
Andre? F. T. Martins and Noah A. Smith. 2009. Summariza-
tion with a joint model for sentence extraction and com-
pression. In Proceedings of the Workshop on Integer Lin-
ear Programming for Natural Langauge Processing, ILP
?09, pages 1?9, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit. http://mallet.cs.umass.edu.
Ryan McDonald. 2006. Discriminative Sentence Compres-
sion with Soft Syntactic Constraints. In Proceedings of
the 11th?EACL, Trento, Italy, April.
Michael Mozer, Michael I. Jordan, and Thomas Petsche, ed-
itors. 1997. Advances in Neural Information Processing
Systems 9, NIPS, Denver, CO, USA, December 2-5, 1996.
MIT Press.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluating
content selection in summarization: The pyramid method.
In Daniel Marcu Susan Dumais and Salim Roukos, edi-
tors, HLT-NAACL 2004: Main Proceedings, pages 145?
152, Boston, Massachusetts, USA, May 2 - May 7. Asso-
ciation for Computational Linguistics.
Jahna Otterbacher, Gu?nes? Erkan, and Dragomir R. Radev.
2005. Using random walks for question-focused sentence
retrieval. In Proceedings of the conference on Human
Language Technology and Empirical Methods in Natural
Language Processing, HLT ?05, pages 915?922, Strouds-
burg, PA, USA. Association for Computational Linguis-
tics.
You Ouyang, Wenjie Li, Sujian Li, and Qin Lu. 2011.
Applying regression models to query-focused multi-
document summarization. Inf. Process. Manage.,
47(2):227?237, March.
Prasad Pingali, Rahul K, and Vasudeva Varma, 2007. IIIT
Hyderabad at DUC 2007. U.S. National Inst. of Standards
and Technology.
Chao Shen and Tao Li. 2011. Learning to rank for query-
focused multi-document summarization. In Diane J.
Cook, Jian Pei, Wei Wang 0010, Osmar R. Zaane, and
Xindong Wu, editors, ICDM, pages 626?634. IEEE.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of ICSLP, volume 2,
pages 901?904, Denver, USA.
Kristina Toutanova, Chris Brockett, Michael Gamon, Ja-
gadeesh Jagarlamudi, Hisami Suzuki, and Lucy Vander-
wende. 2007. The PYTHY Summarization System: Mi-
crosoft Research at DUC 2007. In Proc. of DUC.
Jenine Turner and Eugene Charniak. 2005. Supervised and
unsupervised learning for sentence compression. ACL
?05, pages 290?297, Stroudsburg, PA, USA. Association
for Computational Linguistics.
David Zajic, Bonnie J Dorr, Jimmy Lin, and R. Schwartz.
2006. Sentence compression as a component of a multi-
document summarization system. Proceedings of the
2006 Document Understanding Workshop, New York.
1394
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1395?1405,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Domain-Independent Abstract Generation
for Focused Meeting Summarization
Lu Wang
Department of Computer Science
Cornell University
Ithaca, NY 14853
luwang@cs.cornell.edu
Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
cardie@cs.cornell.edu
Abstract
We address the challenge of generating natu-
ral language abstractive summaries for spoken
meetings in a domain-independent fashion.
We apply Multiple-Sequence Alignment to in-
duce abstract generation templates that can be
used for different domains. An Overgenerate-
and-Rank strategy is utilized to produce and
rank candidate abstracts. Experiments us-
ing in-domain and out-of-domain training on
disparate corpora show that our system uni-
formly outperforms state-of-the-art supervised
extract-based approaches. In addition, human
judges rate our system summaries significantly
higher than compared systems in fluency and
overall quality.
1 Introduction
Meetings are a common way to collaborate,
share information and exchange opinions. Con-
sequently, automatically generated meeting sum-
maries could be of great value to people and busi-
nesses alike by providing quick access to the es-
sential content of past meetings. Focused meet-
ing summaries have been proposed as particularly
useful; in contrast to summaries of a meeting as
a whole, they refer to summaries of a specific as-
pect of a meeting, such as the DECISIONS reached,
PROBLEMS discussed, PROGRESS made or AC-
TION ITEMS that emerged (Carenini et al, 2011).
Our goal is to provide an automatic summariza-
tion system that can generate abstract-style fo-
cused meeting summaries to help users digest the
vast amount of meeting content in an easy manner.
Existing meeting summarization systems re-
main largely extractive: their summaries are com-
prised exclusively of patchworks of utterances se-
lected directly from the meetings to be summa-
rized (Riedhammer et al, 2010; Bui et al, 2009;
Xie et al, 2008). Although relatively easy to con-
struct, extractive approaches fall short of produc-
ing concise and readable summaries, largely due
C: Looking at what we?ve got, we we want an LCD dis-
play with a spinning wheel.
B: You have to have some push-buttons, don?t you?
C: Just spinning and not scrolling, I would say.
B: I think the spinning wheel is definitely very now.
A: but since LCDs seems to be uh a definite yes,
C: We?re having push-buttons on the outside
C: and then on the inside an LCD with spinning wheel,
Decision Abstract (Summary):
The remote will have push buttons outside, and an LCD
and spinning wheel inside.
A: and um I?m not sure about the buttons being in the
shape of fruit though.
D: Maybe make it like fruity colours or something.
C: The power button could be like a big apple or some-
thing.
D: Um like I?m just thinking bright colours.
Problem Abstract (Summary):
How to incorporate a fruit and vegetable theme into the
remote.
Figure 1: Clips from the AMI meeting corpus (Mc-
cowan et al, 2005). A, B, C and D refer to distinct
speakers. Also shown is the gold-standard (manual)
abstract (summary) for the decision and the problem.
to the noisy, fragmented, ungrammatical and un-
structured text of meeting transcripts (Murray et
al., 2010b; Liu and Liu, 2009).
In contrast, human-written meeting summaries
are typically in the form of abstracts ? distilla-
tions of the original conversation written in new
language. A user study from Murray et al (2010b)
showed that people demonstrate a strong prefer-
ence for abstractive summaries over extracts when
the text to be summarized is conversational. Con-
sider, for example, the two types of focused sum-
mary along with their associated dialogue snippets
in Figure 1. We can see that extracts are likely to
include unnecessary and noisy information from
the meeting transcripts. On the contrary, the man-
ually composed summaries (abstracts) are more
compact and readable, and are written in a dis-
tinctly non-conversational style.
1395
To address the limitations of extract-based sum-
maries, we propose a complete and fully automatic
domain-independent abstract generation frame-
work for focused meeting summarization. Fol-
lowing existing language generation research (An-
geli et al, 2010; Konstas and Lapata, 2012), we
first perform content selection: given the dia-
logue acts relevant to one element of the meet-
ing (e.g. a single decision or problem), we train
a classifier to identify summary-worthy phrases.
Next, we develop an ?overgenerate-and-rank?
strategy (Walker et al, 2001; Heilman and Smith,
2010) for surface realization, which generates and
ranks candidate sentences for the abstract. Af-
ter redundancy reduction, the full meeting abstract
can thus comprise the focused summary for each
meeting element. As described in subsequent sec-
tions, the generation framework allows us to iden-
tify and reformulate the important information for
the focused summary. Our contributions are as fol-
lows:
? To the best of our knowledge, our system is
the first fully automatic system to generate
natural language abstracts for spoken meet-
ings.
? We present a novel template extraction al-
gorithm, based on Multiple Sequence Align-
ment (MSA) (Durbin et al, 1998), to induce
domain-independent templates that guide ab-
stract generation. MSA is commonly used
in bioinformatics to identify equivalent frag-
ments of DNAs (Durbin et al, 1998) and
has also been employed for learning para-
phrases (Barzilay and Lee, 2003).
? Although our framework requires labeled
training data for each type of focused sum-
mary (decisions, problems, etc.), we also
make initial tries for domain adaptation so
that our summarization method does not need
human-written abstracts for each new meet-
ing domain (e.g. faculty meetings, theater
group meetings, project group meetings).
We instantiate the abstract generation frame-
work on two corpora from disparate domains
? the AMI Meeting Corpus (Mccowan et al,
2005) and ICSI Meeting Corpus (Janin et al,
2003) ? and produce systems to generate fo-
cused summaries with regard to four types of
meeting elements: DECISIONs, PROBLEMs, AC-
TION ITEMSs, and PROGRESS. Automatic eval-
uation (using ROUGE (Lin and Hovy, 2003) and
BLEU (Papineni et al, 2002)) against manually
generated focused summaries shows that our sum-
marizers uniformly and statistically significantly
outperform two baseline systems as well as a
state-of-the-art supervised extraction-based sys-
tem. Human evaluation also indicates that the
abstractive summaries produced by our systems
are more linguistically appealing than those of
the utterance-level extraction-based system, pre-
ferring them over summaries from the extraction-
based system of comparable semantic correctness
(62.3% vs. 37.7%).
Finally, we examine the generality of our model
across domains for two types of focused summa-
rization ? decisions and problems ? by train-
ing the summarizer on out-of-domain data (i.e. the
AMI corpus for use on the ICSI meeting data,
and vice versa). The resulting systems yield re-
sults comparable to those from the same system
trained on in-domain data, and statistically signif-
icantly outperform supervised extractive summa-
rization approaches trained on in-domain data.
2 Related Work
Most research on spoken dialogue summariza-
tion attempts to generate summaries for full dia-
logues (Carenini et al, 2011). Only recently has
the task of focused summarization been studied.
Supervised methods are investigated to identify
key phrases or utterances for inclusion in the de-
cision summary (Ferna?ndez et al, 2008; Bui et
al., 2009). Based on Ferna?ndez et al (2008), a
relation representation is proposed by Wang and
Cardie (2012) to form structured summaries; we
adopt this representation here for content selec-
tion.
Our research is also in line with generating ab-
stractive summaries for conversations. Extrac-
tive approaches (Murray et al, 2005; Xie et al,
2008; Galley, 2006) have been investigated exten-
sively in conversation summarization. Murray et
al. (2010a) present an abstraction system consist-
ing of interpretation and transformation steps. Ut-
terances are mapped to a simple conversation on-
tology in the interpretation step according to their
type, such as a decision or problem. Then an in-
teger linear programming approach is employed
to select the utterances that cover more entities as
1396
Dialogue Acts: 
C: Looking at what we've got, 
we we want [an LCD display 
with a spinning wheel]. 
B: You have to have some 
push-buttons, don't you? 
C: Just spinning and not 
scrolling , I would say . 
B: I think the spinning wheel is 
definitely very now. 
A: but since LCDs seems to be 
uh a definite yes, 
C: We're having push-buttons 
[on the outside] 
C: and then on the inside an 
LCD with spinning wheel, 
Relation Instances: 
<want, an LCD display with a spinning 
wheel> 
<an LCD display, with a spinning 
wheel> 
<have, some push-buttons> 
<having, push-buttons on the outside> 
<push-buttons, on the outside> 
<an LCD, with spinning wheel> 
? (other possibilities) 
<want, an LCD display with a spinning wheel> 
? The team will want an LCD display with a 
spinning wheel. 
? The team with work with an LCD display 
with a spinning wheel. 
? The group decide to use an LCD display with 
a spinning wheel. 
? (other possibilities) 
<push-buttons, on the outside> 
? Push-buttons are going to be on the outside. 
? Push-buttons on the outside will be used. 
? There will be push-buttons on the outside. 
? (other possibilities) 
One-Best 
Abstract: 
The group decide to 
use an LCD display 
with a spinning 
wheel. 
One-Best 
Abstract: 
There will be push-
buttons on the 
outside. 
Final Summary: 
The group decide to 
use an LCD display with 
a spinning wheel. 
There will be push-
buttons on the outside. 
Learned Templates 
? (all possible abstracts per relation 
instance) 
Relation 
Extraction 
Content Selection 
Template 
Filling 
Statistical 
Ranking 
Surface Realization 
? (one-best abstract 
per relation instance) 
Post-
Selection 
Figure 2: The abstract generation framework. It takes as input a cluster of meeting-item-specific dialogue acts,
from which one focused summary is constructed. Sample relation instances are denoted in bold (The indicators
are further italicized and the arguments are in [brackets]). Summary-worthy relation instances are identified by
content selection module (see Section 4) and then filled into the learned templates individually. A statistical ranker
subsequently selects one best abstract per relation instance (see Section 5.2). The post-selection component reduces
the redundancy and outputs the final summary (see Section 5.3).
determined by an external ontology. Liu and Liu
(2009) apply sentence compression on extracted
summary utterances. Though some of the unnec-
essary words are dropped, the resulting compres-
sions can still be ungrammatical and unstructured.
This work is also broadly related to ex-
pert system-based language generation (Reiter
and Dale, 2000) and concept-to-text generation
tasks (Angeli et al, 2010; Konstas and Lapata,
2012), where the generation process is decom-
posed into content selection (or text planning) and
surface realization. For instance, Angeli et al
(2010) learn from structured database records and
parallel textual descriptions. They generate texts
based on a series of decisions made to select the
records, fields, and proper templates for render-
ing. Those techniques that are tailored to specific
domains (e.g. weather forecasts or sportcastings)
cannot be directly applied to the conversational
data, as their input is well-structured and the tem-
plates learned are domain-specific.
3 Framework
Our domain-independent abstract generation
framework produces a summarizer that gener-
ates a grammatical abstract from a cluster of
meeting-element-related dialogue acts (DAs) ?
all utterances associated with a single decision,
problem, action item or progress step of interest.
Note that identifying these DA clusters is a diffi-
cult task in itself (Bui et al, 2009). Accordingly,
our experiments evaluate two conditions ? one
in which we assume that they are perfectly iden-
tified, and one in which we identify the clusters
automatically.
The summarizer consists of two major compo-
nents and is depicted in Figure 2. Given the DA
cluster to be summarized, the Content Selection
module identifies a set of summary-worthy rela-
tion instances represented as indicator-argument
pairs (i.e. these constitute a finer-grained represen-
tation than DAs). The Surface Realization compo-
nent then generates a short summary in three steps.
In the first step, each relation instance is filled into
templates with disparate structures that are learned
automatically from the training set (Template Fill-
ing). A statistical ranker then selects one best ab-
stract per relation instance (Statistical Ranking).
Finally, selected abstracts are processed for redun-
dancy removal in Post-Selection. Detailed descrip-
tions for each individual step are provided in Sec-
tions 4 and 5.
4 Content Selection
Phrase-based content selection approaches have
been shown to support better meeting sum-
maries (Ferna?ndez et al, 2008). Therefore, we
chose a content selection representation of a finer
granularity than an utterance: we identify relation
instances that can both effectively detect the cru-
cial content and incorporate enough syntactic in-
formation to facilitate the downstream surface re-
alization.
More specifically, our relation instances are
based on information extraction methods that
identify a lexical indicator (or trigger) that evokes
a relation of interest and then employ syntac-
tic information, often in conjunction with se-
mantic constraints, to find the argument con-
stituent(or target phrase) to be extracted. Rela-
1397
tion instances, then, are represented by indicator-
argument pairs (Chen et al, 2011). For example,
in the DA cluster of Figure 2, ?want, an LCD dis-
play with a spinning wheel? and ?push-buttons, on
the outside? are two relation instances.
Relation Instance Extraction We adopt and
extend the syntactic constraints from Wang and
Cardie (2012) to identify all relation instances in
the input utterances; the summary-worthy ones
will be selected by a discriminative classifier.
Constituent and dependency parses are obtained
by the Stanford parser (Klein and Manning, 2003).
Both the indicator and argument take the form of
constituents in the parse tree. We restrict the el-
igible indicator to be a noun or verb; the eligi-
ble arguments is a noun phrase (NP), prepositional
phrase (PP) or adjectival phrase (ADJP). A valid
indicator-argument pair should have at least one
content word and satisfy one of the following con-
straints:
? When the indicator is a noun, the argument
has to be a modifier or complement of the in-
dicator.
? When the indicator is a verb, the argument
has to be the subject or the object if it is an
NP, or a modifier or complement of the indi-
cator if it is a PP/ADJP.
We view relation extraction as a binary classifi-
cation problem rather than a clustering task (Chen
et al, 2011). All relation instances can be cate-
gorized as summary-worthy or not, but only the
summary-worthy ones are used for abstract gen-
eration. A discriminative classifier is trained for
this purpose based on Support Vector Machines
(SVMs) (Joachims, 1998) with an RBF kernel.
For training data construction, we consider a re-
lation instance to be a positive example if it shares
any content word with its corresponding abstracts,
and a negative example otherwise. The features
used are shown in Table 1.
5 Surface Realization
In this section, we describe surface realization,
which renders the relation instances into natural
language abstracts. This process begins with tem-
plate extraction (Section 5.1). Once the templates
are learned, the relation instances from Section 4
are filled into the templates to generate an abstract
(see Section 5.2). Redundancy handling is dis-
cussed in Section 5.3.
Basic Features
number of words/content words
portion of content words/stopwords
number of content words in indicator/argument
number of content words that are also in previous DA
indicator/argument only contains stopword?
number of new nouns
Content Features
has capitalized word?
has proper noun?
TF/IDF/TFIDF min/max/average
Discourse Features
main speaker or not?
is in an adjacency pair (AP)?
is in the source/target of the AP?
number of source/target DA in the AP
is the target of the AP a positive/negative/neutral response?
is the source of the AP a question?
Syntax Features
indicator/argument constituent tag
dependency relation of indicator and argument
Table 1: Features for content selection. Most are
adapted from previous work (Galley, 2006; Xie et al,
2008; Wang and Cardie, 2012). Every basic or con-
tent feature is concatenated with the constituent tags of
indicator and argument to compose a new one. Main
speakers include the most talkative speaker (who has
said the most words) and other speakers whose word
count is more than 20% of the most talkative one (Xie
et al, 2008). Adjacency pair (AP) (Galley, 2006) is
an important conversational analysis concept; each AP
consists of a source utterance and a target utterance pro-
duced by different speakers.
5.1 Template Extraction
Sentence Clustering. Template extraction starts
with clustering the sentences that constitute the
manually generated abstracts in the training data
according to their lexical and structural similarity.
From each cluster, multiple-sequence alignment
techniques are employed to capture the recurring
patterns.
Intuitively, desirable templates are those that
can be applied in different domains to generate
the same type of focused summary (e.g. decision
or problem summaries). We do not want sen-
tences to be clustered only because they describe
the same domain-specific details (e.g. they are all
about ?data collection?), which will lead to frag-
mented templates that are not reusable for new do-
mains. We therefore replace all appearances of
dates, numbers, and proper names with generic la-
bels. We also replace words that appear in both
the abstract and supporting dialogue acts by a la-
bel indicating its phrase type. For any noun phrase
with its head word abstracted, the whole phrase is
also replaced with ?NP?.
1398
        
start They 
The group were not sure whether to VP NP 
use 
NP should include end 
how much would cost to make 
1) The group were not sure whether to [include]VP [a recharger for the remote]NP . 2) The group were not sure whether to use [plastic and rubber or titanium for the case]NP . 3) The group were not sure whether [the remote control]NP should include [functions for controlling video]NP . 4) They were not sure how much [a recharger]NP would cost to make . ? (Kther abstracts) 
1) The group were not sure whether to VP NP .  2) The group were not sure whether to use NP .  3) The group were not sure whether NP should include NP .  4) They were not sure how much NP would cost to make . 
Generic Label Replacement + Clustering 
Template Examples:  Fine T1: The group were not sure whether to SLOTVP NP . (1, 2)  Fine T2: The group were not sure whether NP SLOTVP SLOTVP NP . (3)  Fine T3: SLOTNP were not sure SLOTWHADJP SLOTWHADJP NP SLOTVP SLOTVP SLOTVP SLOTVP SLOTVP . (4)  Coarse T1: SLOTNP SLOTNP were not sure SLOTSBAR SLOTVP SLOTVP SLOTNP . (1, 2)  Coarse T2: SLOTNP SLOTNP were not sure SLOTSBAR SLOTNP SLOTVP SLOTVP SLOTNP . (3)  Coarse T3: SLOTNP were not sure SLOTWHADJP SLOTWHADJP SLOTNP SLOTVP SLOTVP SLOTVP SLOTVP . (4) 
Template Induction 
MSA 
Figure 3: Example of template extraction by Multiple-
Sequence Alignment for problem abstracts from AMI.
Backbone nodes shared by at least 50% sentences are
shaded. The grammatical errors exist in the original
abstracts.
Following Barzilay and Lee (2003), we ap-
proach the sentence clustering task by hierarchical
complete-link clustering with a similarity metric
based on word n-gram overlap (n = 1, 2, 3). Clus-
ters with fewer than three abstracts are removed1.
Learning the Templates via MSA. For learn-
ing the structural patterns among the abstracts,
Multiple-Sequence Alignment (MSA) is first com-
puted for each cluster. MSA takes as input multi-
ple sentences and one scoring function to measure
the similarity between any two words. For inser-
tions or deletions, a gap cost is also added. MSA
can thus find the best way to align the sequences
with insertions or deletions in accordance with the
scorer. However, computing an optimal MSA is
NP-complete (Wang and Jiang, 1994), thus we
implement an approximate algorithm (Needleman
and Wunsch, 1970) that iteratively aligns two se-
quences each time and treats the resulting align-
ment as a new sequence2. Figure 3 demonstrates
an MSA computed from a sample cluster of ab-
1Clustering stops when the similarity between any pair-
wise clusters is below 5. This is applied to every type of sum-
marization. We tune the parameter on a small held-out devel-
opment set by manually evaluating the induced templates. No
significant change is observed within a small range.
2We adopt the scoring function for MSA from Barzilay
and Lee (2003), where aligning two identical words scores
1, inserting a gap scores ?0.01, and aligning two different
words scores ?0.5.
stracts. The MSA is represented in the form of
word lattice, from which we can detect the struc-
tural similarities shared by the sentences.
To transform the resulting MSAs into templates,
we need to decide whether a word in the sentence
should be retained to comprise the template or ab-
stracted. The backbone nodes in an MSA are iden-
tified as the ones shared by more than 50%3 of the
cluster?s sentences (shaded in gray in Figure 3).
We then create a FINE template for each sentence
by abstracting the non-backbone words, i.e. re-
placing each of those words with a generic token
(last step in Figure 3). We also create a COARSE
template that only preserves the nodes shared by
all of the cluster?s sentences. By using the op-
erations above, domain-independent patterns are
thus identified and domain-specific details are re-
moved.
Note that we do not explicitly evaluate the qual-
ity of the learned templates, which would require
a significant amount of manual evaluation. In-
stead, they are evaluated extrinsically. We encode
the templates as features (Angeli et al, 2010) that
could be selected or ignored in the succeeding ab-
stract ranking model.
5.2 Template Filling
An Overgenerate-and-Rank Approach. Since
filling the relation instances into templates of dis-
tinct structures may result in abstracts of vary-
ing quality, we rank the abstracts based on the
features of the template, the transformation con-
ducted, and the generated abstract. This is realized
by the Overgenerate-and-Rank strategy (Walker et
al., 2001; Heilman and Smith, 2010). It takes as
input a set of relation instances (from the same
cluster) R = {?indi, argi?}Ni=1 that are produced
by content selection component, a set of templates
T = {tj}Mj=1 that are represented as parsing trees,
a transformation function F (described below),
and a statistical ranker S for ranking the generated
abstracts, for which we defer description later in
this Section.
For each ?indi, argi?, the overgenerate-and-
rank approach fills it into each template in T by
applying F to generate all possible abstracts. Then
the ranker S selects the best abstract absi. Post-
selection is conducted on the abstracts {absi}Ni=1
to form the final summary.
3See Barzilay and Lee (2003) for a detailed discussion
about the choice of 50% according to pigeonhole principle.
1399
The transformation function F models the
constituent-level transformations of relation in-
stances and their mappings to the parse trees of
templates. With the intuition that people will reuse
the relation instances from the transcripts albeit
not necessarily in their original form to write the
abstracts, we consider three major types of map-
ping operations for the indicator or argument in the
source pair, namely, Full-Constituent Mapping,
Sub-Constituent Mapping, and Removal. Full-
Constituent Mapping denotes that a source con-
stituent is mapped directly to a target constituent
of the template parse tree with the same tag. Sub-
Constituent Mapping encodes more complex and
flexible transformations in that a sub-constituent
of the source is mapped to a target constituent
with the same tag. This operation applies when
the source has a tag of PP or ADJP, in which case
its sub-constituent, if any, with a tag of NP, VP or
ADJP can be mapped to the target constituent with
the same tag. For instance, an argument ?with a
spinning wheel? (PP) can be mapped to an NP in a
template because it has a sub-constituent ?a spin-
ning wheel? (NP). Removal means a source is not
mapped to any constituent in the template.
Formally, F is defined as:
F (?indsrc, argsrc?, t) =
{?indtrank , argtrank , indtark , argtark ?}Kk=1
where ?indsrc, argsrc? ? R is a relation in-
stance (source pair); t ? T is a template; indtrank
and argtrank is the transformed pair of indsrc and
argsrc; indtark and argtark are constituents in t, and
they compose one target pair for ?indsrc, argsrc?.
We require that indsrc and argsrc are not removed
at the same time. Moreover, for valid indtark and
argtark , the words subsumed by them should be all
abstracted in the template, and they do not overlap
in the parse tree.
To obtain the realized abstract, we traverse the
parse tree of the filled template in pre-order. The
words subsumed by the leaf nodes are thus col-
lected sequentially.
Learning a Statistical Ranker. We utilize a dis-
criminative ranker based on Support Vector Re-
gression (SVR) (Smola and Scho?lkopf, 2004) to
rank the generated abstracts. Given the train-
ing data that includes clusters of gold-standard
summary-worthy relation instances, associated ab-
stracts they support, and the parallel templates for
each abstract, training samples for the ranker are
Basic Features
number of words in indsrc/argsrc
number of new nouns in indsrc/argsrc
indtrank /argtrank only has stopword?number of new nouns in indtrank /argtrankStructure Features
constituent tag of indsrc/argsrc
constituent tag of indsrc with constituent tag of indtar
constituent tag of argsrc with constituent tag of argtar
transformation of indsrc/argsrc combined with constituent tag
dependency relation of indsrc and argsrc
dependency relation of indtar and argtar
above 2 features have same value?
Template Features
template type (fine/coarse)
realized template (e.g. ?the group decided to?)
number of words in template
the template has verb?
Realization Features
realization has verb?
realization starts with verb?
realization has adjacent verbs/NPs?
indsrc precedes/succeeds argsrc?
indtar precedes/succeeds argtar?
above 2 features have same value?
Language Model Features
log pLM (first word in indtrank |previous 1/2 words)
log pLM (realization)
log pLM (first word in argtrank |previous 1/2 words)
log pLM (realization)/length
log pLM (next word | last 1/2 words in indtrank )
log pLM (next word | last 1/2 words in argtrank )
Table 2: Features for abstracts ranking. The language
model features are based on a 5-gram language model
trained on Gigaword (Graff, 2003) by SRILM (Stolcke,
2002).
constructed according to the transformation func-
tion F mentioned above. Each sample is repre-
sented as:
(?indsrc, argsrc?, ?indtrank , argtrank , indtark , argtark ?, t, a)
where ?indsrc, argsrc? is the source pair,
?indtrank , argtrank ? is the transformed pair,
?indtark , argtark ? is the target pair in template t,
and a is the abstract parallel to t.
We first find ?indtar,absk , argtar,absk ?, whichis the corresponding constituent pair of
?indtark , argtark ? in a. Then we identify
the summary-worthy words subsumed by
?indtrank , argtrank ? that also appear in a. If those
words are all subsumed by ?indtar,absk , argtar,absk ?,then it is considered to be a positive sample, and
a negative sample otherwise. Table 2 displays the
features used in abstract ranking.
5.3 Post-Selection: Redundancy Handling.
Post-selection aims to maximize the information
coverage and minimize the redundancy of the
summary. Given the generated abstracts A =
1400
Input : relation instances R = {?indi, argi?}Ni=1,
generated abstracts A = {absi}Ni=1, objective
function f , cost function C
Output: final abstract G
G? ? (empty set);
U ? A;
while U 6= ? do
abs? arg maxabsi?U f(A,G?absi)?f(A,G)C(absi) ;if f(A,G ? abs)? f(A,G) ? 0 then
G? G ? abs;
end
U ? U \ abs;
end
Algorithm 1: Greedy algorithm for post-
selection to generate the final summary.
{absi}Ni=1, we use a greedy algorithm (Lin and
Bilmes, 2010) to select a subsetA?, whereA? ? A,
to form the final summary. We define wij as
the unigram similarity between abstracts absi and
absj , C(absi) as the number of words in absi. We
employ the following objective function:
f(A,G) =?absi?A\G
?
absj?G wi,j , G ? AAlgorithm 1 sequentially finds an abstract with
the greatest ratio of objective function gain to
length, and add it to the summary if the gain is
non-negative.
6 Experimental Setup
Corpora. Two disparate corpora are used for
evaluation. The AMI meeting corpus (Mccowan
et al, 2005) contains 139 scenario-driven meet-
ings, where groups of four people participate in
a series of four meetings for a fictitious project of
designing remote control. The ICSI meeting cor-
pus (Janin et al, 2003) consists of 75 naturally oc-
curring meetings, each of them has 4 to 10 par-
ticipants. Compared to the fabricated topics in
AMI, the conversations in ICSI tend to be special-
ized and technical, e.g. discussion about speech
and language technology. We use 57 meetings in
ICSI and 139 meetings in AMI that include a short
(usually one-sentence), manually constructed ab-
stract summarizing each important output for ev-
ery meeting. Decision and problem summaries are
annotated for both corpora. AMI has extra ac-
tion item summaries, and ICSI has progress sum-
maries. The set of dialogue acts that support each
abstract are annotated as such.
System Inputs. We consider two system input
settings. In the True Clusterings setting, we
use the annotations to create perfect partitions of
the DAs for input to the system; in the System
Figure 4: Content selection evaluation by using
ROUGE-SU4 (multiplied by 100). SVM-DA and
SVM-TOKEN denotes for supervised extract-based
methods with SVMs on utterance- and token-level.
Summaries for decision, problem, action item, and
progress are generated and evaluated for AMI and ICSI
(with names in parentheses). X-axis shows the number
of meetings used for training.
Clusterings setting, we employ a hierarchical ag-
glomerative clustering algorithm used for this task
in (Wang and Cardie, 2011). DAs are grouped ac-
cording to a classifier trained beforehand.
Baselines and Comparisons. We compare our
system with (1) two unsupervised baselines, (2)
two supervised extractive approaches, and (3) an
oracle derived from the gold standard abstracts.
Baselines. As in Riedhammer et al (2010), the
LONGEST DA in each cluster is selected as the
summary. The second baseline picks the clus-
ter prototype (i.e. the DA with the largest TF-
IDF similarity with the cluster centroid) as the
summary according to Wang and Cardie (2011).
Although it is possible that important content is
spread over multiple DAs, both baselines allow
us to determine summary quality when summaries
are restricted to a single utterance.
Supervised Learning. We also compare our
approach to two supervised extractive sum-
marization methods ? Support Vector Ma-
chines (Joachims, 1998) trained with the same fea-
1401
tures as our system (see Table 1) to identify the im-
portant DAs (no syntax features) (Xie et al, 2008;
Sandu et al, 2010) or tokens (Ferna?ndez et al,
2008) to include into the summary4.
Oracle. We compute an oracle consisting of the
words from the DA cluster that also appear in the
associated abstract to reflect the gap between the
best possible extracts and the human abstracts.
7 Results
Content Selection Evaluation. We first employ
ROUGE (Lin and Hovy, 2003) to evaluate the
content selection component with respect to the
human written abstracts. ROUGE computes the
ngram overlapping between the system summaries
with the reference summaries, and has been used
for both text and speech summarization (Dang,
2005; Xie et al, 2008). We report ROUGE-2 (R-
2) and ROUGE-SU4 (R-SU4) that are shown to
correlate with human evaluation reasonably well.
In AMI, four meetings of different functions are
carried out in each group5. 35 meetings for ?con-
ceptual design? are randomly selected for testing.
For ICSI, we reserve 12 meetings for testing.
The R-SU4 scores for each system are displayed
in Figure 4 and show that our system uniformly
outperforms the baselines and supervised systems.
The learning curve of our system is relatively flat,
which means not many training meetings are re-
quired to reach a usable performance level.
Note that the ROUGE scores are relative low
when the reference summaries are human ab-
stracts, even for evaluation among abstracts pro-
duced by different annotators (Dang, 2005). The
intrinsic difference of styles between dialogue and
human abstract further lowers the scores. But the
trend is still respected among the systems.
Abstract Generation Evaluation. To evaluate
the full abstract generation system, the BLEU
score (Papineni et al, 2002) (the precision of uni-
grams and bigrams with a brevity penalty) is com-
puted with human abstracts as reference. BLEU
has a fairly good agreement with human judge-
ment and has been used to evaluate a variety of
language generation systems (Angeli et al, 2010;
Konstas and Lapata, 2012).
4We use SVMlight (Joachims, 1999) with RBF kernel by
default parameters for SVM-based classifiers and regressor.
5The four types of meetings in AMI are: project kick-off
(35 meetings), functional design (35 meetings), conceptual
design (35 meetings), and detailed design (34 meetings).
Figure 5: Full abstract generation system evaluation
by using BLEU (multiplied by 100). SVM-DA de-
notes for supervised extractive methods with SVMs on
utterance-level.
We are not aware of any existing work gen-
erating abstractive summaries for conversations.
Therefore, we compare our full system against
a supervised utterance-level extractive method
based on SVMs along with the baselines. The
BLEU scores in Figure 5 show that our system im-
proves the scores consistently over the baselines
and the SVM-based approach.
Domain Adaptation Evaluation. We further
examine our system in domain adaptation sce-
narios for decision and problem summarization,
where we train the system on AMI for use on ICSI,
and vice versa. Table 3 indicates that, with both
true clusterings and system clusterings, our sys-
tem trained on out-of-domain data achieves com-
parable performance with the same system trained
on in-domain data. In most experiments, it also
significantly outperforms the baselines and the
extract-based approaches (p < 0.05).
Human Evaluation. We randomly select 15 de-
cision and 15 problem DA clusters (true cluster-
ings). We evaluate fluency (is the text gram-
matical?) and semantic correctness (does the
summary convey the gist of the DAs in the clus-
ter?) for OUR SYSTEM trained on IN-domain data
1402
System (True Clusterings) AMI Decision ICSI Decision AMI Problem ICSI Problem
R-2 R-SU4 BLEU R-2 R-SU4 BLEU R-2 R-SU4 BLEU R-2 R-SU4 BLEU
CENTROID DA 1.3 3.0 7.7 1.8 3.5 3.8 1.0 2.7 4.2 1.0 2.3 2.8
LONGEST DA 1.6 3.3 7.0 2.8 4.7 6.5 1.0 3.0 3.6 1.2 3.4 4.6
SVM-DA (IN) 3.4 4.7 9.7 3.4 4.5 5.7 1.4 2.4 5.0 1.6 3.4 3.4
SVM-DA (OUT) 2.7 4.2 6.6 3.1 4.2 4.6 1.4 2.2 2.5 1.3 3.0 4.6
OUR SYSTEM (IN) 4.5 6.2 11.6 4.9 7.1 10.0 3.1 4.8 7.2 4.0 5.9 6.0
OUR SYSTEM (OUT) 4.6 6.1 10.3 4.8 6.4 7.8 3.5 4.7 6.2 3.0 5.5 5.3
ORACLE 7.5 12.0 22.8 9.9 14.9 20.2 6.6 11.3 18.9 6.4 12.6 13.0
System (System Clusterings) AMI Decision ICSI Decision AMI Problem ICSI Problem
R-2 R-SU4 BLEU R-2 R-SU4 BLEU R-2 R-SU4 BLEU R-2 R-SU4 BLEU
CENTROID DA 1.4 3.3 3.8 1.4 2.1 2.0 0.8 2.8 2.9 0.9 2.3 1.8
LONGEST DA 1.4 3.3 5.7 1.7 3.4 5.5 0.8 3.2 4.1 0.9 3.4 4.4
SVM-DA (IN) 2.6 4.6 10.5 3.5 6.5 7.1 1.8 3.7 4.9 1.8 4.0 4.6
SVM-DA (OUT) 3.4 5.8 10.3 2.7 4.8 6.3 2.1 3.8 4.3 1.5 3.8 3.5
OUR SYSTEM (IN) 3.5 5.4 11.7 4.4 7.4 9.1 3.3 4.6 9.5 2.3 4.2 7.4
OUR SYSTEM (OUT) 3.9 6.4 11.4 4.1 5.1 8.4 3.6 5.6 8.9 1.8 4.0 6.8
ORACLE 6.4 12.0 15.1 8.2 15.2 17.6 6.5 13.0 20.9 5.5 11.9 15.5
Table 3: Domain adaptation evaluation. Systems trained on out-of-domain data are denoted with ?(OUT)?, oth-
erwise with ?(IN)?. ROUGE and BLEU scores are multiplied by 100. Our systems that statistically significantly
outperform all the other approaches (except ORACLE) are in bold (p < 0.05, paired t-test). The numbers in italics
show the significant improvement over the baselines by our systems.
System Fluency Semantic Length
Mean S.D. Mean S.D.
OUR SYSTEM (IN) 3.67 0.85 3.27 1.03 23.65
OUR SYSTEM (OUT) 3.58 0.90 3.25 1.16 24.17
SVM-DA (IN) 3.36 0.84 3.44 1.26 38.83
Table 4: Human evaluation results of Fluency and Se-
mantic correctness for the generated abstracts. The rat-
ings are on 1 (worst) to 5 (best) scale. The average
Length of the abstracts for each system is also listed.
and OUT-of-domain data, and for the utterance-
level extraction system (SVM-DA) trained on in-
domain data. Each cluster of DAs along with three
randomly ordered summaries are presented to the
judges. Five native speaking Ph.D. students (none
are authors) performed the task.
We carry out an one-way Analysis of Variance
which shows significant differences in score as a
function of system (p < 0.05, paired t-test). Re-
sults in Table 4 demonstrate that our system sum-
maries are significantly more compact and fluent
than the extract-based method (p < 0.05) while
semantic correctness is comparable.
The judges also rank the three summaries in
terms of the overall quality in content, concise-
ness and grammaticality. An inter-rater agreement
of Fleiss?s ? = 0.45 (moderate agreement (Landis
and Koch, 1977)) was computed. Judges selected
our system as the best system in 62.3% scenarios
(IN-DOMAIN: 35.6%, OUT-OF-DOMAIN: 26.7%).
Sample summaries are exhibited in Figure 6.
8 Conclusion
We presented a domain-independent abstract gen-
eration framework for focused meeting summa-
rization. Experimental results on two disparate
meeting corpora show that our system can uni-
Decision Summary:
Human: The remote will have push buttons outside, and
an LCD and spinning wheel inside.
Our System (In): The group decide to use an LCD dis-
play with a spinning wheel. There will be push-buttons on
the outside.
Our System (Out): LCD display is going to be with a
spinning wheel. It is necessary having push-buttons on
the outside.
SVM-DA: Looking at what we?ve got, we we want an
LCD display with a spinning wheel. Just spinning and not
scrolling, I would say. I think the spinning wheel is defi-
nitely very now. We?re having push-buttons on the outside
Problem Summary:
Human: How to incorporate a fruit and vegetable theme
into the remote.
Our System (In): Whether to include the shape of fruit.
The team had to thinking bright colors.
Our System (Out): It is unclear that the buttons being in
the shape of fruit.
SVM-DA: and um Im not sure about the buttons being in
the shape of fruit though.
Figure 6: Sample decision and problem sum-
maries generated by various systems for examples
in Figure 1.
formly outperform the state-of-the-art supervised
extraction-based systems in both automatic and
manual evaluation. Our system also exhibits an
ability to train on out-of-domain data to generate
abstracts for a new target domain.
9 Acknowledgments
This work was supported in part by National Sci-
ence Foundation Grant IIS-0968450 and a gift
from Boeing. We thank Moontae Lee, Myle Ott,
Yiye Ruan, Chenhao Tan, and the ACL reviewers
for valuable suggestions and advice on various as-
pects of this work.
1403
References
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach
to generation. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 502?512, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach using
multiple-sequence alignment. In Proceedings of the
2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology - Volume 1, NAACL
?03, pages 16?23, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Trung H. Bui, Matthew Frampton, John Dowding, and
Stanley Peters. 2009. Extracting decisions from
multi-party dialogue using directed graphical mod-
els and semantic similarity. In Proceedings of the
SIGDIAL 2009 Conference: The 10th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue, SIGDIAL ?09, pages 235?243, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Giuseppe Carenini, Gabriel Murray, and Raymond Ng.
2011. Methods for Mining and Summarizing Text
Conversations. Morgan & Claypool Publishers.
Harr Chen, Edward Benson, Tahira Naseem, and
Regina Barzilay. 2011. In-domain relation discov-
ery with meta-constraints via posterior regulariza-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ?11,
pages 530?540, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hoa T. Dang. 2005. Overview of DUC 2005. In Doc-
ument Understanding Conference.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological Sequence
Analysis: Probabilistic Models of Proteins and Nu-
cleic Acids. Cambridge University Press, July.
Raquel Ferna?ndez, Matthew Frampton, John Dowding,
Anish Adukuzhiyil, Patrick Ehlen, and Stanley Pe-
ters. 2008. Identifying relevant phrases to sum-
marize decisions in spoken meetings. In INTER-
SPEECH, pages 78?81.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?06, pages 364?372, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
David Graff. 2003. English Gigaword.
Michael Heilman and Noah A. Smith. 2010. Good
question! statistical ranking for question generation.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
?10, pages 609?617, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The icsi meeting corpus.
volume 1, pages I?364?I?367 vol.1.
Thorsten Joachims. 1998. Text categorization with
suport vector machines: Learning with many rele-
vant features. In Proceedings of the 10th European
Conference onMachine Learning, ECML ?98, pages
137?142, London, UK, UK. Springer-Verlag.
Thorsten Joachims. 1999. Advances in kernel meth-
ods. chapter Making large-scale support vector ma-
chine learning practical, pages 169?184. MIT Press,
Cambridge, MA, USA.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text generation via discriminative reranking. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Long Papers
- Volume 1, ACL ?12, pages 369?378, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
J R Landis and G G Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159?174.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submod-
ular functions. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 912?920, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology - Volume 1, pages 71?78.
Fei Liu and Yang Liu. 2009. From extractive to ab-
stractive meeting summaries: can it be done by sen-
tence compression? In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, ACLShort
?09, pages 261?264, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
1404
I. Mccowan, G. Lathoud, M. Lincoln, A. Lisowska,
W. Post, D. Reidsma, and P. Wellner. 2005. The ami
meeting corpus. In In: Proceedings Measuring Be-
havior 2005, 5th International Conference on Meth-
ods and Techniques in Behavioral Research. L.P.J.J.
Noldus, F. Grieco, L.W.S. Loijens and P.H. Zimmer-
man (Eds.), Wageningen: Noldus Information Tech-
nology.
Gabriel Murray, Steve Renals, and Jean Carletta. 2005.
Extractive summarization of meeting recordings. In
INTERSPEECH, pages 593?596.
Gabriel Murray, Giuseppe Carenini, and Raymond Ng.
2010a. Interpretation and transformation for ab-
stracting conversations. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Com-
putational Linguistics, HLT ?10, pages 894?902,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Gabriel Murray, Giuseppe Carenini, and Raymond T.
Ng. 2010b. Generating and validating abstracts of
meeting conversations: a user study. In INLG.
S. B. Needleman and C. D. Wunsch. 1970. A general
method applicable to the search for similarities in
the amino acid sequence of two proteins. Journal of
molecular biology, 48(3):443?453, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. Cambridge Univer-
sity Press, New York, NY, USA.
Korbinian Riedhammer, Benoit Favre, and Dilek
Hakkani-Tu?r. 2010. Long story short - global unsu-
pervised models for keyphrase based meeting sum-
marization. Speech Commun., 52(10):801?815, Oc-
tober.
Oana Sandu, Giuseppe Carenini, Gabriel Murray, and
Raymond Ng. 2010. Domain adaptation to sum-
marize human conversations. In Proceedings of the
2010 Workshop on Domain Adaptation for Natural
Language Processing, DANLP 2010, pages 16?22,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Alex J. Smola and Bernhard Scho?lkopf. 2004. A tu-
torial on support vector regression. Statistics and
Computing, 14(3):199?222, August.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
volume 2, pages 901?904, Denver, USA.
Marilyn A. Walker, Owen Rambow, and Monica Ro-
gati. 2001. Spot: a trainable sentence planner.
In Proceedings of the second meeting of the North
American Chapter of the Association for Com-
putational Linguistics on Language technologies,
NAACL ?01, pages 1?8, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Lu Wang and Claire Cardie. 2011. Summarizing de-
cisions in spoken meetings. In Proceedings of the
Workshop on Automatic Summarization for Different
Genres, Media, and Languages, WASDGML ?11,
pages 16?24, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Lu Wang and Claire Cardie. 2012. Focused meet-
ing summarization via unsupervised relation extrac-
tion. In Proceedings of the 13th Annual Meeting of
the Special Interest Group on Discourse and Dia-
logue, SIGDIAL ?12, pages 304?313, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Lusheng Wang and Tao Jiang. 1994. On the complex-
ity of multiple sequence alignment. Journal of Com-
putational Biology, 1(4):337?348.
Shasha Xie, Yang Liu, and Hui Lin. 2008. Evaluating
the effectiveness of features and sampling in extrac-
tive meeting summarization. In in Proc. of IEEE
Spoken Language Technology (SLT.
1405
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 693?699,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Piece of My Mind: A Sentiment Analysis Approach
for Online Dispute Detection
Lu Wang
Department of Computer Science
Cornell University
Ithaca, NY 14853
luwang@cs.cornell.edu
Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
cardie@cs.cornell.edu
Abstract
We investigate the novel task of online dis-
pute detection and propose a sentiment analy-
sis solution to the problem: we aim to identify
the sequence of sentence-level sentiments ex-
pressed during a discussion and to use them
as features in a classifier that predicts the
DISPUTE/NON-DISPUTE label for the dis-
cussion as a whole. We evaluate dispute de-
tection approaches on a newly created corpus
of Wikipedia Talk page disputes and find that
classifiers that rely on our sentiment tagging
features outperform those that do not. The best
model achieves a very promising F1 score of
0.78 and an accuracy of 0.80.
1 Introduction
As the web has grown in popularity and scope, so
has the promise of collaborative information en-
vironments for the joint creation and exchange of
knowledge (Jones and Rafaeli, 2000; Sack, 2005).
Wikipedia, a wiki-based online encyclopedia, is
arguably the best example: its distributed edit-
ing environment allows readers to collaborate as
content editors and has facilitated the production
of over four billion articles
1
of surprisingly high
quality (Giles, 2005) in English alone since its de-
but in 2001.
Existing studies of collaborative knowledge
systems have shown, however, that the quality of
the generated content (e.g. an encyclopedia arti-
cle) is highly correlated with the effectiveness of
the online collaboration (Kittur and Kraut, 2008;
Kraut and Resnick, 2012); fruitful collaboration,
in turn, inevitably requires dealing with the dis-
putes and conflicts that arise (Kittur et al, 2007).
Unfortunately, human monitoring of the often
massive social media and collaboration sites to de-
tect, much less mediate, disputes is not feasible.
1
http://en.wikipedia.org
In this work, we investigate the heretofore novel
task of dispute detection in online discussions.
Previous work in this general area has analyzed
dispute-laden content to discover features corre-
lated with conflicts and disputes (Kittur et al,
2007). Research focused primarily on cues de-
rived from the edit history of the jointly created
content (e.g. the number of revisions, their tem-
poral density (Kittur et al, 2007; Yasseri et al,
2012)) and relied on small numbers of manually
selected discussions known to involve disputes. In
contrast, we investigate methods for the automatic
detection, i.e. prediction, of discussions involving
disputes. We are also interested in understanding
whether, and which, linguistic features of the dis-
cussion are important for dispute detection.
Drawing inspiration from studies of human me-
diation of online conflicts (e.g. Billings and Watts
(2010), Kittur et al (2007), Kraut and Resnick
(2012)), we hypothesize that effective methods
for dispute detection should take into account the
sentiment and opinions expressed by participants
in the collaborative endeavor. As a result, we
propose a sentiment analysis approach for online
dispute detection that identifies the sequence of
sentence-level sentiments (i.e. very negative, neg-
ative, neutral, positive, very positive) expressed
during the discussion and uses them as features
in a classifier that predicts the DISPUTE/NON-
DISPUTE label for the discussion as a whole. Con-
sider, for example, the snippet in Figure 1 from the
Wikipedia Talk page for the article on Philadel-
phia; it discusses the choice of a picture for the
article?s ?infobox?. The sequence of almost exclu-
sively negative statements provides evidence of a
dispute in this portion of the discussion.
Unfortunately, sentence-level sentiment tagging
for this domain is challenging in its own right
due to the less formal, often ungrammatical, lan-
guage and the dynamic nature of online conver-
sations. ?Really, grow up? (segment 3) should
693
1-Emy111: I think everyone is forgetting that my previous image was the
lead image for well over a year! ...
> Massimo: I?m sorry to say so, but it is grossly over processed...
2-Emy111: i?m glad you paid more money for a camera than I did. con-
grats... i appreciate your constructive criticism. thank you.
> Massimo: I just want to have the best picture as a lead for the article ...
3-Emy111: Wow, I am really enjoying this photography debate... [so don?t
make assumptions you know nothing about.]
NN
[Really, grow up.]
N
[If you
all want to complain about Photoshop editing, lets all go buy medium for-
mat film cameras, shoot film, and scan it, so no manipulation is possible.]
O
[Sound good?]
NN
> Massimo: ... I do feel it is a pity, that you turned out to be a sore loser...
Figure 1: From the Wikipedia Talk page for the article
?Philadelphia?. Omitted sentences are indicated by ellipsis.
Names of editors are in bold. The start of each set of related
turns is numbered; ?>? is an indicator for the reply structure.
presumably be tagged as a negative sentence as
should the sarcastic sentences ?Sounds good?? (in
the same turn) and ?congrats? and ?thank you?
(in segment 2). We expect that these, and other,
examples will be difficult for the sentence-level
classifier unless the discourse context of each sen-
tence is considered. Previous research on senti-
ment prediction for online discussions, however,
focuses on turn-level predictions (Hahn et al,
2006; Yin et al, 2012).
2
As the first work that
predicts sentence-level sentiment for online dis-
cussions, we investigate isotonic Conditional Ran-
dom Fields (CRFs) (Mao and Lebanon, 2007) for
the sentiment-tagging task as they preserve the ad-
vantages of the popular CRF-based sequential tag-
ging models (Lafferty et al, 2001) while provid-
ing an efficient mechanism for encoding domain
knowledge ? in our case, a sentiment lexicon ?
through isotonic constraints on model parameters.
We evaluate our dispute detection approach us-
ing a newly created corpus of discussions from
Wikipedia Talk pages (3609 disputes, 3609 non-
disputes).
3
We find that classifiers that employ the
learned sentiment features outperform others that
do not. The best model achieves a very promis-
ing F1 score of 0.78 and an accuracy of 0.80 on
the Wikipedia dispute corpus. To the best of our
knowledge, this represents the first computational
approach to automatically identify online disputes
on a dataset of scale.
Additional Related Work. Sentiment analysis
has been utilized as a key enabling technique in
a number of conversation-based applications. Pre-
vious work mainly studies the attitudes in spoken
2
A notable exception is Hassan et al (2010), which identi-
fies sentences containing ?attitudes? (e.g. opinions), but does
not distinguish them w.r.t. sentiment. Context information is
also not considered.
3
The talk page associated with each article records con-
versations among editors about the article content and allows
editors to discuss the writing process, e.g. planning and orga-
nizing the content.
meetings (Galley et al, 2004; Hahn et al, 2006) or
broadcast conversations (Wang et al, 2011) using
variants of Conditional Random Fields (Lafferty et
al., 2001) and predicts sentiment at the turn-level,
while our predictions are made for each sentence.
2 Data Construction: A Dispute Corpus
We construct the first dispute detection corpus to
date; it consists of dispute and non-dispute discus-
sions from Wikipedia Talk pages.
Step 1: Get Talk Pages of Disputed Articles.
Wikipedia articles are edited by different editors.
If an article is observed to have disputes on its
talk page, editors can assign dispute tags to the
article to flag it for attention. In this research, we
are interested in talk pages whose corresponding
articles are labeled with the following tags:
DISPUTED, TOTALLYDISPUTED, DISPUTED-
SECTION, TOTALLYDISPUTED-SECTION, POV.
The tags indicate that an article is disputed, or the
neutrality of the article is disputed (POV).
We use the 2013-03-04 Wikipedia data dump,
and extract talk pages for articles that are labeled
with dispute tags by checking the revision history.
This results in 19,071 talk pages.
Step 2: Get Discussions with Disputes. Dis-
pute tags can also be added to talk pages them-
selves. Therefore, in addition to the tags men-
tioned above, we also consider the ?Request for
Comment? (RFC) tag on talk pages. According to
Wikipedia
4
, RFC is used to request outside opin-
ions concerning the disputes.
3609 discussions are collected with dispute
tags found in the revision history. We further
classify dispute discussions into three subcate-
gories: CONTROVERSY, REQUEST FOR COM-
MENT (RFC), and RESOLVED based on the tags
found in discussions (see Table 1). The numbers
of discussions for the three types are 42, 3484, and
105, respectively. Note that dispute tags only ap-
pear in a small number of articles and talk pages.
There may exist other discussions with disputes.
Dispute Subcategory Wikipedia Tags on Talk pages
Controversy CONTROVERSIAL, TOTALLYDISPUTED,
DISPUTED, CALM TALK, POV
Request for Comment RFC
Resolved Any tag from above + RESOLVED
Table 1: Subcategory for disputes with corresponding tags.
Note that each discussion in the RESOLVED class has more
than one tag.
Step 3: Get Discussions without Disputes. Like-
wise, we collect non-dispute discussions from
4
http://en.wikipedia.org/wiki/Wikipedia:
Requests_for_comment
694
pages that are never tagged with disputes. We con-
sider non-dispute discussions with at least 3 dis-
tinct speakers and 10 turns. 3609 discussions are
randomly selected with this criterion. The average
turn numbers for dispute and non-dispute discus-
sions are 45.03 and 22.95, respectively.
3 Sentence-level Sentiment Prediction
This section describes our sentence-level senti-
ment tagger, from which we construct features for
dispute detection (Section 4).
Consider a discussion comprised of sequential
turns; each turn consists of a sequence of sen-
tences. Our model takes as input the sentences
x = {x
1
, ? ? ? , x
n
} from a single turn, and out-
puts the corresponding sequence of sentiment la-
bels y = {y
1
, ? ? ? , y
n
}, where y
i
? O,O =
{NN,N,O,P,PP}. The labels in O represent
very negative (NN), negative (N), neutral (O), pos-
itive (P), and very positive (PP), respectively.
Given that traditional Conditional Random
Fields (CRFs) (Lafferty et al, 2001) ignore the or-
dinal relations among sentiment labels, we choose
isotonic CRFs (Mao and Lebanon, 2007) for
sentence-level sentiment analysis as they can en-
force monotonicity constraints on the parameters
consistent with the ordinal structure and domain
knowledge (e.g. word-level sentiment conveyed
via a lexicon). Concretely, we take a lexiconM =
M
p
?M
n
, whereM
p
andM
n
are two sets of fea-
tures (usually words) identified as strongly associ-
ated with positive and negative sentiment. Assume
?
??,w?
encodes the weight between label ? and
feature w, for each feature w ? M
p
; then the iso-
tonic CRF enforces ? ? ?
?
? ?
??,w?
? ?
??
?
,w?
.
For example, when ?totally agree? is observed in
training, parameter ?
?PP,totally agree?
is likely to
increase. Similar constraints are defined onM
n
.
Our lexicon is built by combining MPQA (Wil-
son et al, 2005), General Inquirer (Stone et al,
1966), and SentiWordNet (Esuli and Sebastiani,
2006) lexicons. Words with contradictory senti-
ments are removed. We use the features in Table 2
for sentiment prediction.
Syntactic/Semantic Features. We have two ver-
sions of dependency relation features, the origi-
nal form and a form that generalizes a word to its
POS tag, e.g. ?nsubj(wrong, you)? is generalized
to ?nsubj(ADJ, you)? and ?nsubj(wrong, PRP)?.
Discourse Features. We extract the initial uni-
gram, bigram, and trigram of each utterance as dis-
Lexical Features Syntactic/Semantic Features
- unigram/bigram - unigram with POS tag
- number of words all uppercased - dependency relation
- number of words Conversation Features
Discourse Features - quote overlap with target
- initial uni-/bi-/tri-gram - TFIDF similarity with target
- repeated punctuations (remove quote first)
- hedging phrases collected from Sentiment Features
Farkas et al (2010) - connective + sentiment words
- number of negators - sentiment dependency relation
- sentiment words
Table 2: Features used in sentence-level sentiment predic-
tion. Numerical features are first normalized by standardiza-
tion, then binned into 5 categories.
course features (Hirschberg and Litman, 1993).
Sentiment Features. We gather connectives from
the Penn Discourse TreeBank (Rashmi Prasad and
Webber, 2008) and combine them with any senti-
ment word that precedes or follows it as new fea-
tures. Sentiment dependency relations are the de-
pendency relations that include a sentiment word.
We replace those words with their polarity equiv-
alents. For example, relation ?nsubj(wrong, you)?
becomes ?nsubj(SentiWord
neg
, you)?.
4 Online Dispute Detection
4.1 Training A Sentiment Classifier
Dataset. We train the sentiment classifier using
the Authority and Alignment in Wikipedia Discus-
sions (AAWD) corpus (Bender et al, 2011) on a 5-
point scale (i.e. NN, N, O, P, PP). AAWD consists
of 221 English Wikipedia discussions with posi-
tive and negative alignment annotations. Annota-
tors either label each sentence as positive, negative
or neutral, or label the full turn. For instances that
have only a turn-level label, we assume all sen-
tences have the same label as the turn. We further
transform the labels into the five sentiment labels.
Sentences annotated as being a positive alignment
by at least two annotators are treated as very posi-
tive (PP). If a sentence is only selected as positive
by one annotator or obtains the label via turn-level
annotation, it is positive (P). Very negative (NN)
and negative (N) are collected in the same way.
All others are neutral (O). Among all 16,501 sen-
tences in AAWD, 1,930 and 1,102 are labeled as
NN and N. 532 and 99 of them are PP and P. The
other 12,648 are considered neutral.
Evaluation. To evaluate the performance of the
sentiment tagger, we compare to two baselines.
(1) Baseline (Polarity): a sentence is predicted as
positive if it has more positive words than nega-
tive words, or negative if more negative words are
observed. Otherwise, it is neutral. (2) Baseline
(Distance) is extended from (Hassan et al, 2010).
Each sentiment word is associated with the closest
695
Pos Neg Neutral
Baseline (Polarity) 22.53 38.61 66.45
Baseline (Distance) 33.75 55.79 88.97
SVM (3-way) 44.62 52.56 80.84
CRF (3-way) 56.28 56.37 89.41
CRF (5-way) 58.39 56.30 90.10
isotonic CRF 68.18 62.53 88.87
Table 3: F1 scores for positive and negative alignment on
Wikipedia Talk pages (AAWD) using 5-fold cross-validation.
In each column, bold entries (if any) are statistically signif-
icantly higher than all the rest. We also compare with an
SVM and linear CRF trained with three classes (3-way). Our
model based on the isotonic CRF produces significantly bet-
ter results than all the other systems.
second person pronoun, and a surface distance is
computed. An SVM classifier (Joachims, 1999) is
trained using features of the sentiment words and
minimum/maximum/average of the distances.
We also compare with two state-of-the-art
methods that are used in sentiment prediction for
conversations: (1) an SVM (RBF kernel) that is
employed for identifying sentiment-bearing sen-
tences (Hassan et al, 2010), and (dis)agreement
detection (Yin et al, 2012) in online debates; (2)
a Linear CRF for (dis)agreement identification in
broadcast conversations (Wang et al, 2011).
We evaluate the systems using standard F1 on
classes of positive, negative, and neutral, where
samples predicted as PP and P are positive align-
ment, and samples tagged as NN and N are neg-
ative alignment. Table 3 describes the main re-
sults on the AAWD dataset: our isotonic CRF
based system significantly outperforms the alter-
natives for positive and negative alignment detec-
tion (paired-t test, p < 0.05).
4.2 Dispute Detection
We model dispute detection as a standard bi-
nary classification task, and investigate four major
types of features as described below.
Lexical Features. We first collect unigram and
bigram features for each discussion.
Topic Features. Articles on specific topics, such
as politics or religions, tend to arouse more dis-
putes. We thus extract the category informa-
tion of the corresponding article for each talk page.
We further utilize unigrams and bigrams of
the category as topic features.
Discussion Features. This type of feature aims
to capture the structure of the discussion. Intu-
itively, the more turns or the more participants
a discussion has, the more likely there is a
dispute. Meanwhile, participants tend to produce
longer utterances when they make arguments.
We choose number of turns, number
of participants, average number of
words in each turn as features. In addi-
tion, the frequency of revisions made during the
discussion has been shown to be good indicator
for controversial articles (Vuong et al, 2008), that
are presumably prone to have disputes. Therefore,
we encode the number of revisions that
happened during the discussion as a feature.
Sentiment Features. This set of features en-
code the sentiment distribution and transition in
the discussion. We train our sentiment tagging
model on the full AAWD dataset, and run it on
the Wikipedia dispute corpus.
Given that consistent negative senti-
ment flow usually indicates an ongoing
dispute, we first extract features from
sentiment distribution in the form
of number/probability of sentiment
per type. We also estimate the sentiment
transition probability P (S
t
? S
t+1
) from
our predictions, where S
t
and S
t+1
are sentiment
labels for the current sentence and the next. We
then have features as number/portion of
sentiment transitions per type.
Features described above mostly depict the
global sentiment flow in the discussions. We fur-
ther construct a local version of them, since sen-
timent distribution may change as discussion pro-
ceeds. For example, less positive sentiment can be
observed as dispute being escalated. We thus split
each discussion into three equal length stages, and
create sentiment distribution and transition fea-
tures for each stage.
Prec Rec F1 Acc
Baseline (Random) 50.00 50.00 50.00 50.00
Baseline (All dispute) 50.00 100.00 66.67 50.00
Logistic Regression 74.76 72.29 73.50 73.94
SVM
Linear
69.81 71.90 70.84 70.41
SVM
RBF
77.38 79.14 78.25 80.00
Table 4: Dispute detection results on Wikipedia Talk pages.
The numbers are multiplied by 100. The items in bold are sta-
tistically significantly higher than others in the same column
(paired-t test, p < 0.05). SVM with the RBF kernel achieves
the best performance in precision, F1, and accuracy.
Results and Error Analysis. We experiment with
logistic regression, SVM with linear and RBF ker-
nels, which are effective methods in multiple text
categorization tasks (Joachims, 1999; Zhang and
J. Oles, 2001). We normalize the features by stan-
dardization and conduct a 5-fold cross-validation.
Two baselines are listed: (1) labels are randomly
assigned; (2) all discussions have disputes.
Main results for different classifiers are dis-
played in Table 4. All learning based methods
696
T1 T2 T3T4T5 T6 T7 T8 T9 T10 T11 T12 T13T14 T15 T16 T17 T182
10
12Sentimen
t
A B C D EF
Sentiment Flow in Discussion with Unresolved Dispute
Sample sentences (sentiment in parentheses)
A: no, I sincerely plead with you... (N) If not, you are just wasting my
time. (NN)
B: I believe Sweet?s proposal... is quite silly. (NN)
C: Tell you what. (NN) If you can get two other editors to agree... I will
shut up and sit down. (NN)
D: But some idiot forging your signature claimed that doing so would
violate. (NN)... Please go have some morning coffee. (O)
E: And I don?t like coffee. (NN) Good luck to you. (NN)
F: Was that all? (NN)... I think that you are in error... (N)
T1 T2 T3 T4 T5 T6 T7 T8 T9 T10 T11 T12 T13 T142
10
12Sentimen
t
A B
C
D
E FSentiment Flow in Discussion with Resolved Dispute A: So far so confusing. (NN)...B: ... I can not see a rationale for the landrace having its own article...(N) With Turkish Van being a miserable stub, there?s no such rationale for
forking off a new article... (NN)...
C: I?ve also copied your post immediately above to that article?s talk page
since it is a great ?nutshell? summary. (PP)
D: Err.. how can the opposite be true... (N)
E: Thanks for this, though I have to say some of the facts floating around
this discussion are wrong. (P)
F: Great. (PP) Let?s make sure the article is clear on this. (O)
Figure 2: Sentiment flow for a discussion with unresolved dispute about the definition of ?white people? (top) and a dis-
cussion with resolved dispute on merging articles about van cat (bottom). The labels {NN,N,O,P,PP} are mapped to
{?2,?1, 0, 1, 2} in sequence. Sentiment values are convolved by Gaussian smoothing kernel, and cubic-spline interpolation is
then conducted. Different speakers are represented by curves of different colors. Dashed vertical lines delimit turns. Represen-
tative sentences are labeled with letters and their sentiment labels are shown on the right. For unresolved dispute (top), we see
that negative sentiment exists throughout the discussion. Whereas, for the resolved dispute (bottom), less negative sentiment is
observed at the end of the discussion; participants also show appreciation after the problem is solved (e.g. E and F in the plot).
Prec Rec F1 Acc
Lexical (Lex) 75.86 34.66 47.58 61.82
Topic (Top) 68.44 71.46 69.92 69.26
Discussion (Dis) 69.73 76.14 72.79 71.54
Sentiment (Senti
g+l
) 72.54 69.52 71.00 71.60
Top + Dis 68.49 71.79 70.10 69.38
Top + Dis + Senti
g
77.39 78.36 77.87 77.74
Top + Dis + Senti
g+l
77.38 79.14 78.25 80.00
Lex + Top + Dis + Senti
g+l
78.38 75.12 76.71 77.20
Table 5: Dispute detection results with different feature
sets by SVM with RBF kernel. The numbers are multi-
plied by 100. Senti
g
represents global sentiment features, and
Senti
g+l
includes both global and local features. The number
in bold is statistically significantly higher than other numbers
in the same column (paired-t test, p < 0.05), and the italic
entry has the highest absolute value.
outperform the two baselines, and among them,
SVM with the RBF kernel achieves the best F1
score and accuracy (0.78 and 0.80). Experimental
results with various combinations of features sets
are displayed in Table 5. As it can be seen, senti-
ment features obtains the best accuracy among the
four types of features. A combination of topic, dis-
cussion, and sentiment features achieves the best
performance on recall, F1, and accuracy. Specif-
ically, the accuracy is significantly higher than all
the other systems (paired-t test, p < 0.05).
After a closer look at the results, we find two
main reasons for incorrect predictions. Firstly,
sentiment prediction errors get propagated into
dispute detection. Due to the limitation of ex-
isting general-purpose lexicons, some opinionated
dialog-specific terms are hard to catch. For exam-
ple, ?I told you over and over again...? strongly
suggests a negative sentiment, but no single word
shows negative connotation. Constructing a lexi-
con tuned for conversational text may improve the
performance. Secondly, some dispute discussions
are harder to detect than the others due to differ-
ent dialog structures. For instance, the recalls for
dispute discussions of ?controversy?, ?RFC?, and
?resolved? are 0.78, 0.79, and 0.86 respectively.
We intend to design models that are able to cap-
ture dialog structures in the future work.
Sentiment Flow Visualization. We visualize the
sentiment flow of two disputed discussions in Fig-
ure 2. The plots reveal persistent negative sen-
timent in unresolved disputes (top). For the re-
solved dispute (bottom), participants show grati-
tude when the problem is settled.
5 Conclusion
We present a sentiment analysis-based approach
to online dispute detection. We create a large-
scale dispute corpus from Wikipedia Talk pages to
study the problem. A sentiment prediction model
based on isotonic CRFs is proposed to output sen-
timent labels at the sentence-level. Experiments
on our dispute corpus also demonstrate that clas-
sifiers trained with sentiment tagging features out-
perform others that do not.
Acknowledgments We heartily thank the Cornell
NLP Group, the reviewers, and Yiye Ruan for
helpful comments. We also thank Emily Ben-
der and Mari Ostendorf for providing the AAWD
dataset. This work was supported in part by NSF
grants IIS-0968450 and IIS-1314778, and DARPA
DEFT Grant FA8750-13-2-0015. The views and
conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily
representing the official policies or endorsements,
either expressed or implied, of NSF, DARPA or
the U.S. Government.
697
References
Emily M. Bender, Jonathan T. Morgan, Meghan Ox-
ley, Mark Zachry, Brian Hutchinson, Alex Marin,
Bin Zhang, and Mari Ostendorf. 2011. Anno-
tating social acts: Authority claims and alignment
moves in wikipedia talk pages. In Proceedings of
the Workshop on Languages in Social Media, LSM
?11, pages 48?57, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Matt Billings and Leon Adam Watts. 2010. Under-
standing dispute resolution online: using text to re-
flect personal and substantive issues in conflict. In
Elizabeth D. Mynatt, Don Schoner, Geraldine Fitz-
patrick, Scott E. Hudson, W. Keith Edwards, and
Tom Rodden, editors, CHI, pages 1447?1456. ACM.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiwordnet: A publicly available lexical resource
for opinion mining. In In Proceedings of the 5th
Conference on Language Resources and Evaluation
(LREC06), pages 417?422.
Rich?ard Farkas, Veronika Vincze, Gy?orgy M?ora, J?anos
Csirik, and Gy?orgy Szarvas. 2010. The conll-2010
shared task: Learning to detect hedges and their
scope in natural language text. In Proceedings of
the Fourteenth Conference on Computational Natu-
ral Language Learning ? Shared Task, CoNLL ?10:
Shared Task, pages 1?12, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech:
use of Bayesian networks to model pragmatic de-
pendencies. In ACL ?04: Proceedings of the 42nd
Annual Meeting on Association for Computational
Linguistics, pages 669+, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Jim Giles. 2005. Internet encyclopaedias go head to
head. Nature, 438(7070):900?901.
Sangyun Hahn, Richard Ladner, and Mari Ostendorf.
2006. Agreement/disagreement classification: Ex-
ploiting unlabeled data using contrast classifiers.
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Companion Volume:
Short Papers, pages 53?56, New York City, USA,
June. Association for Computational Linguistics.
Ahmed Hassan, Vahed Qazvinian, and Dragomir
Radev. 2010. What?s with the attitude?: Identify-
ing sentences with attitude in online discussions. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 1245?1255, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Julia Hirschberg and Diane Litman. 1993. Empirical
studies on the disambiguation of cue phrases. Com-
put. Linguist., 19(3):501?530, September.
Thorsten Joachims. 1999. Advances in kernel meth-
ods. chapter Making Large-scale Support Vector
Machine Learning Practical, pages 169?184. MIT
Press, Cambridge, MA, USA.
Quentin Jones and Sheizaf Rafaeli. 2000. Time to
split, virtually: discourse architecture and commu-
nity building create vibrant virtual publics. Elec-
tronic Markets, 10:214?223.
Aniket Kittur and Robert E. Kraut. 2008. Harness-
ing the wisdom of crowds in wikipedia: Quality
through coordination. In Proceedings of the 2008
ACM Conference on Computer Supported Coopera-
tive Work, CSCW ?08, pages 37?46, New York, NY,
USA. ACM.
Aniket Kittur, Bongwon Suh, Bryan A. Pendleton, and
Ed H. Chi. 2007. He says, she says: Conflict and
coordination in wikipedia. In Proceedings of the
SIGCHI Conference on Human Factors in Comput-
ing Systems, CHI ?07, pages 453?462, New York,
NY, USA. ACM.
Robert E. Kraut and Paul Resnick. 2012. Building suc-
cessful online communities: Evidence-based social
design. MIT Press, Cambridge, MA.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Yi Mao and Guy Lebanon. 2007. Isotonic conditional
random fields and local sentiment flow. In Advances
in Neural Information Processing Systems.
Alan Lee Eleni Miltsakaki Livio Robaldo Ar-
avind Joshi Rashmi Prasad, Nikhil Dinesh and Bon-
nie Webber. 2008. The penn discourse tree-
bank 2.0. In Proceedings of the Sixth Interna-
tional Conference on Language Resources and Eval-
uation (LREC?08), Marrakech, Morocco, may. Eu-
ropean Language Resources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.
Warren Sack. 2005. Digital formations: It and new
architectures in the global realm. chapter Discourse
architecture and very large-scale conversation, pages
242?282. Princeton University Press, Princeton, NJ
USA.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General In-
quirer: A Computer Approach to Content Analysis.
MIT Press, Cambridge, MA.
Ba-Quy Vuong, Ee-Peng Lim, Aixin Sun, Minh-Tam
Le, Hady Wirawan Lauw, and Kuiyu Chang. 2008.
On ranking controversies in wikipedia: Models and
evaluation. In Proceedings of the 2008 Interna-
tional Conference on Web Search and Data Mining,
WSDM ?08, pages 171?182, New York, NY, USA.
ACM.
698
Wen Wang, Sibel Yaman, Kristin Precoda, Colleen
Richey, and Geoffrey Raymond. 2011. Detection of
agreement and disagreement in broadcast conversa-
tions. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies: Short Papers - Volume
2, HLT ?11, pages 374?378, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ?05, pages 347?354, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Taha Yasseri, R?obert Sumi, Andr?as Rung, Andr?as Kor-
nai, and J?anos Kert?esz. 2012. Dynamics of conflicts
in wikipedia. CoRR, abs/1202.3643.
Jie Yin, Paul Thomas, Nalin Narang, and Cecile Paris.
2012. Unifying local and global agreement and
disagreement classification in online debates. In
Proceedings of the 3rd Workshop in Computational
Approaches to Subjectivity and Sentiment Analysis,
WASSA ?12, pages 61?69, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Tong Zhang and Frank J. Oles. 2001. Text categoriza-
tion based on regularized linear classification meth-
ods. Inf. Retr., 4(1):5?31, April.
699
Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 16?24,
Portland, Oregon, June 23, 2011. c?2011 Association for Computational Linguistics
Summarizing Decisions in Spoken Meetings
Lu Wang
Department of Computer Science
Cornell University
Ithaca, NY 14853
luwang@cs.cornell.edu
Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
cardie@cs.cornell.edu
Abstract
This paper addresses the problem of summa-
rizing decisions in spoken meetings: our goal
is to produce a concise decision abstract for
each meeting decision. We explore and com-
pare token-level and dialogue act-level au-
tomatic summarization methods using both
unsupervised and supervised learning frame-
works. In the supervised summarization set-
ting, and given true clusterings of decision-
related utterances, we find that token-level
summaries that employ discourse context can
approach an upper bound for decision ab-
stracts derived directly from dialogue acts.
In the unsupervised summarization setting,we
find that summaries based on unsupervised
partitioning of decision-related utterances per-
form comparably to those based on partitions
generated using supervised techniques (0.22
ROUGE-F1 using LDA-based topic models
vs. 0.23 using SVMs).
1 Introduction
Meetings are a common way for people to share in-
formation and discuss problems. And an effective
meeting always leads to concrete decisions. As a re-
sult, it would be useful to develop automatic meth-
ods that summarize not the entire meeting dialogue,
but just the important decisions made. In particular,
decision summaries would allow participants to re-
view decisions from previous meetings as they pre-
pare for an upcoming meeting. For those who did
not participate in the earlier meetings, decision sum-
maries might provide one type of efficient overview
of the meeting contents. For managers, decision
summaries could act as a concise record of the idea
generation process.
While there has been some previous work in
summarizing meetings and conversations, very lit-
tle work has focused on decision summarization:
Ferna?ndez et al (2008a) and Bui et al (2009) in-
vestigate the use of a semantic parser and machine
learning methods for phrase- and token-level deci-
sion summarization. We believe our work is the first
to explore and compare token-level and dialogue
act-level approaches ? using both unsupervised and
supervised learning methods ? for summarizing de-
cisions in meetings.
C: Just spinning and not scrolling , I would say . (1)
C: But if you?ve got a [disfmarker] if if you?ve got a flipped
thing , effectively it?s something that?s curved on one side
and flat on the other side , but you folded it in half . (2)
D: the case would be rubber and the the buttons , (3)
B: I think the spinning wheel is definitely very now . (1)
B: and then make the colour of the main remote [vocal-
sound] the colour like vegetable colours , do you know ? (4)
B: I mean I suppose vegetable colours would be orange
and green and some reds and um maybe purple (4)
A: but since LCDs seems to be uh a definite yes , (1)
A: Flat on the top . (2)
Decision Abstracts (Summary)
DECISION 1: The remote will have an LCD and spinning
wheel inside.
DECISION 2: The case will be flat on top and curved on
the bottom.
DECISION 3: The remote control and its buttons will be
made of rubber.
DECISION 4: The remote will resemble a vegetable and
be in bright vegetable colors.
Table 1: A clip of a meeting from the AMI meeting cor-
pus (Carletta et al, 2005). A, B, C and D refer to distinct
speakers; the numbers in parentheses indicate the asso-
ciated meeting decision: DECISION 1, 2, 3 or 4. Also
shown is the gold-standard (manual) abstract (summary)
for each decision.
16
Consider the sample dialogue snippet in Table 1,
which is part of the AMI meeting corpus (Carletta et
al., 2005). The Table lists only decision-related di-
alogue acts (DRDAs) ? utterances associated with
at least one decision made in the meeting.1 The DR-
DAs are ordered by time; intervening utterances are
not shown. DRDAs are important because they con-
tain critical information for decision summary con-
struction.
Table 1 clearly shows some challenges for deci-
sion summarization for spoken meetings beyond the
disfluencies, high word error rates, absence of punc-
tuation, interruptions and hesitations due to speech.
First, different decisions can be discussed more or
less concurrently; as a result, the utterances asso-
ciated with a single decision are not contiguous in
the dialogue. In Table 1, the dialogue acts (hence-
forth, DAs) concerning DECISION 1, for exam-
ple, are interleaved with DAs for other decisions.
Second, some decision-related DAs contribute more
than others to the associated decision. In compos-
ing the summary for DECISION 1, for example, we
might safely ignore the first DA for DECISION 1. Fi-
nally, more so than for standard text summarization,
purely extract-based summaries are not likely to be
easily interpretable: DRDAs often contain text that
is irrelevant to the decision and many will only be
understandable if analyzed in the context of the sur-
rounding utterances.
In this paper, we study methods for decision sum-
marization for spoken meetings. We assume that
all decision-related DAs have been identified and
aim to produce a summary for the meeting in the
form of concise decision abstracts (see Table 1), one
for each decision made. In response to the chal-
lenges described above, we propose a summariza-
tion framework that includes:
Clustering of decision-related DAs. Here we aim to
partition the decision-related utterances (DRDAs)
according to the decisions each supports. This step
is similar in spirit to many standard text summariza-
tion techniques (Salton et al, 1997) that begin by
grouping sentences according to semantic similar-
ity.
Summarization at the DA-level. We select just the im-
portant DRDAs in each cluster. Our goal is to elimi-
nate redundant and less informative utterances. The
1These are similar, but not completely equivalent, to the de-
cision dialogue acts (DDAs) of Bui et al (2009), Ferna?ndez et
al. (2008a), Frampton et al (2009). The latter refer to all DAs
that appear in a decision discussion even if they do NOT support
any particular decision.
selected DRDAs are then concatenated to form the
decision summary.
Optional token-level summarization of the selected
DRDAs. Methods are employed to capture con-
cisely the gist of each decision, discarding any
distracting text.
Incorporation of the discourse context as needed.
We hypothesize that this will produce more
interpretable summaries.
More specifically, we compare both unsupervised
(TFIDF (Salton et al, 1997) and LDA topic mod-
eling (Blei et al, 2003)) and (pairwise) supervised
clustering procedures (using SVMs and MaxEnt) for
partitioning DRDAs according to the decision each
supports. We also investigate unsupervised methods
and supervised learning for decision summarization
at both the DA and token level, with and without the
incorporation of discourse context. During training,
the supervised decision summarizers are told which
DRDAs for each decision are the most informative
for constructing the decision abstract.
Our experiments employ the aforementioned
AMI meeting corpus: we compare our decision
summaries to the manually generated decision ab-
stracts for each meeting and evaluate performance
using the ROUGE-1 (Lin and Hovy, 2003) text sum-
marization evaluation metric.
In the supervised summarization setting, our ex-
periments demonstrate that with true clusterings of
decision-related DAs, token-level summaries that
employ limited discourse context can approach an
upper bound for summaries extracted directly from
DRDAs2 ? 0.4387 ROUGE-F1 vs. 0.5333. When
using system-generated DRDA clusterings, the DA-
level summaries always dominate token-level meth-
ods in terms of performance.
For the unsupervised summarization setting, we
investigate the use of both unsupervised and su-
pervised methods for the initial DRDA clustering
step. We find that summaries based on unsupervised
clusterings perform comparably to those generated
using supervised techniques (0.2214 ROUGE-F1
using LDA-based topic models vs. 0.2349 using
SVMs). As in the supervised summarization setting,
we observe that including additional discourse con-
text boosts performance only for token-level sum-
maries.
2The upper bound measures the vocabulary overlap of each
gold-standard decision summary with the complete text of all of
its associated DRDAs.
17
2 Related Work
There exists much previous research on automatic
text summarization using corpus-based, knowledge-
based or statistical methods (Mani, 1999; Marcu,
2000). Dialogue summarization methods, how-
ever, generally try to account for the special char-
acteristics of speech. Among early work in
this subarea, Zechner (2002) investigates speech
summarization based on maximal marginal rele-
vance (MMR) and cross-speaker linking of infor-
mation. Popular supervised methods for summa-
rizing speech ? including maximum entropy, con-
ditional random fields (CRFs), and support vector
machines (SVMs) ? are investigated in Buist et al
(2004), Xie et al (2008) and Galley (2006). Tech-
niques for determining semantic similarity are used
for selecting relevant utterances in Gurevych and
Strube (2004).
Studies in Banerjee et al (2005) show that de-
cisions are considered to be one of the most im-
portant outputs of meetings. And in recent years,
there has been much research on detecting decision-
related DAs. Hsueh and Moore (2008), for exam-
ple, propose maximum entropy classification tech-
niques to identify DRDAs in meetings; Ferna?ndez
et al (2008b) develop a model of decision-making
dialogue structure and detect decision DAs based on
it; and Frampton et al (2009) implement a real-time
decision detection system.
Ferna?ndez et al (2008a) and Bui et al (2009),
however, might be the most relevant previous work
to ours. The systems in both papers run an open-
domain semantic parser on meeting transcriptions
to produce multiple short fragments, and then em-
ploy machine learning methods to select the phrases
or words that comprise the decision summary. Al-
though their task is also decision summarization,
their gold-standard summaries consist of manually
annotated words from the meeting while we judge
performance using manually constructed decision
abstracts as the gold standard. The latter are more
readable, but often use a vocabulary different from
that of the associated decision-related utterances in
the meeting.
Our work differs from all of the above in that we
(1) incorporate a clustering step to partition DRDAs
according to the decision each supports; (2) generate
decision summaries at both the DA- and token-level;
and (3) investigate the role of discourse context for
decision summarization.
In the following sections, we investigate methods
for clustering DRDAs (Section 3) and generating
DA-level and token-level decision summaries (Sec-
tion 4). In each case, we evaluate the methods using
the AMI meeting corpus.
3 Clustering Decision-Related Dialogue
Acts
We design a preprocessing step that facilitates deci-
sion summarization by clustering all of the decision-
related dialogue acts according to the decision(s) it
supports. Because it is not clear how many deci-
sions are made in a meeting, we use a hierarchi-
cal agglomerative clustering algorithm (rather than
techniques that require a priori knowledge of the
number of clusters) and choose the proper stopping
conditions. In particular, we employ average-link
methods: at each iteration, we merge the two clus-
ters with the maximum average pairwise similarity
among their DRDAs. In the following subsections,
we introduce unsupervised and supervised methods
for measuring the pairwise DRDA similarity.
3.1 DRDA Similarity: Unsupervised Methods
We consider two unsupervised similarity measures
? one based on the TF-IDF score from the Infor-
mation Retrieval research community, and a second
based on Latent Dirichlet Allocation topic models.
TF-IDF similarity. TF-IDF similarity metrics
have worked well as a measure of document simi-
larity. As a result, we employ it as one metric for
measuring the similarity of two DRDAs. Suppose
there are L distinct word types in the corpus. We
treat each decision-related dialgue act DAi as a
document, and represent it as an L-dimensional
feature vector
???
FVi = (xi1, xi2, ..., xiL), where xik
is word wk?s tf ? idf score for DAi. Then the
(average-link) similarity of cluster Cm and cluster
Cn, Sim TFIDF (Cm, Cn), is defined as :
1
| Cm | ? | Cn |
?
DAi?Cm
DAj?Cn
???
FVi ?
???
FVj
?
???
FVi ??
???
FVj ?
LDA topic models. In recent years, topic models
have become a popular technique for discovering the
latent structure of ?topics? or ?concepts? in a cor-
pus. Here we use the Latent Dirichlet Allocation
(LDA) topic models of Blei et al (2003) ? unsuper-
18
Features
number of overlapping words
proportion of the number of overlapping words to the le-
ngth of shorter DA
TF-IDF similarity
whether the DAs are in an adjacency pair (see 4.3)
time difference of pairwise DAs
relative dialogue position of pairwise DAs
whether the two DAs have the same DA type
number of overlapping words in the contexts (see 4.2)
Table 2: Features for Pairwise Supervised Clustering
vised probabilistic generative models that estimate
the properties of multinomial observations. In our
setting, LDA-based topic models provide a soft clus-
tering of the DRDAs according to the topics they
discuss.3 To determine the similarity of two DR-
DAs, we effectively measure the similarity of their
term-based topic distributions.
To train an LDA-based topic model for our task4,
we treat each DRDA as an individual document.
After training, each DRDA, DAi, is assigned a
topic distribution
??
?i according to the learned model.
Thus, we can define the similarity of cluster Cm and
cluster Cn, Sim LDA(Cm, Cn), as :
1
| Cm | ? | Cn |
?
DAi?Cm
DAj?Cn
??
?i ?
??
?j
3.2 DRDA Similarity: Supervised Techniques
In addition to unsupervised methods for clustering
DRDAs, we also explore an approach based on Pair-
wise Supervised Learning: we develop a classifier
that determines whether or not a pair of DRDAs sup-
ports the same decision. So each training and test
example is a feature vector that is a function of two
DRDAs: for DAi and DAj , the feature vector is
???
FVij = f(DAi, DAj) = {fv1ij , fv
2
ij , ..., fv
k
ij}. Ta-
ble 2 gives a full list of features that are used. Be-
cause the annotations for the time information and
dialogue type of DAs are available from the cor-
pus, we employ features including time difference
of pairwise DAs, relative position5 and whether they
3We cannot easily associate each topic with a decision be-
cause the number of decisions is not known a priori.
4Parameter estimation and inference done by GibbsLDA++.
5Here is the definition for the relative position of pairwise
DAs. Suppose there are N DAs in one meeting ordered by time,
have the same DA type.
We employ Support Vector Machines (SVMs)
and Maximum Entropy (MaxEnt) as our learning
methods, because SVMs are shown to be effective
in text categorization (Joachims, 1998) and Max-
Ent has been applied in many natural language
processing tasks (Berger et al, 1996). Given an
???
FVij , for SVMs, we utilize the decision value of
wT ?
???
FVij + b as the similarity, where w is the
weight vector and b is the bias. For MaxEnt, we
make use of the probability of P (SameDecision |
???
FVij) as the similarity value.
3.3 Experiments
Corpus. We use the AMI meeting Corpus (Car-
letta et al, 2005), a freely available corpus of multi-
party meetings that contains a wide range of anno-
tations. The 129 scenario-driven meetings involve
four participants playing different roles on a de-
sign team. A short (usually one-sentence) abstract
is included that describes each decision, action, or
problem discussed in the meeting; and each DA is
linked to the abstracts it supports. We use the manu-
ally constructed decision abstracts as gold-standard
summaries and assume that all decision-related DAs
have been identified (but not linked to the decision(s)
it supports).
Baselines. Two clustering baselines are utilized
for comparison. One baseline places all decision-
related DAs for the meeting into a single partition
(ALLINONEGROUP). The second uses the text seg-
mentation software of Choi (2000) to partition the
decision-related DAs (ordered according to time)
into several topic-based groups (CHOISEGMENT).
Experimental Setup and Evaluation. Results for
pairwise supervised clustering were obtained using
3-fold cross-validation. In the current work, stop-
ping conditions for hierarchical agglomerative clus-
tering are selected manually: For the TF-IDF and
topic model approaches, we stop when the similar-
ity measure reaches 0.035 and 0.015, respectively;
For the SVM and MaxEnt versions, we use 0 and
0.45, respectively. We use the Mallet implementa-
tion for MaxEnt and the SVMlight implementation
of SVMs.
Our evaluation metrics include b3 (also called B-
cubed) (Bagga and Baldwin, 1998), which is a com-
DAi is the ith DA and DAj is positioned at j. So the relative
position of DAi and DAj is
|i?j|
N .
19
B-cubed Pairwise VOI
PRECISION RECALL F1 PRECISION RECALL F1
Baselines
AllInOneGroup 0.2854 1.0000 0.4441 0.1823 1.0000 0.3083 2.2279
ChoiSegment 0.4235 0.9657 0.5888 0.2390 0.8493 0.3730 1.8061
Unsupervised Methods
TFIDF 0.6840 0.6686 0.6762 0.3281 0.3004 0.3137 1.6604
LDA topic models 0.8265 0.6432 0.7235 0.4588 0.2980 0.3613 1.4203
Pairwise Supervised Methods
SVM 0.7593 0.7466 0.7529 0.5474 0.4821 0.5127 1.2239
MaxEnt 0.6999 0.7948 0.7443 0.4858 0.5704 0.5247 1.2726
Table 3: Results for Clustering Decision-Related DAs According to the Decision Each Supports
mon measure employed in noun phrase coreference
resolution research; a pairwise scorer that measures
correctness for every pair of DRDAs; and a variation
of information (VOI) scorer (Meila?, 2007), which
measures the difference between the distributions of
the true clustering and system generated clustering.
As space is limited, we refer the readers to the orig-
inal papers for more details. For b3 scorer and pair-
wise scorer, higher results represent better perfor-
mance; for VOI, lower is better.6
Results. The results in Table 3 show first that all
of the proposed clustering methods outperform the
baselines. Among the unsupervised methods, the
LDA topic modeling is preferred to TFIDF. For the
supervised methods, SVMs and MaxEnt produce
comparable results.
4 Decision Summarization
In this section, we turn to decision summarization ?
extracting a short description of each decision based
on the decision-related DAs in each cluster. We in-
vestigate options for constructing an extract-based
summary that consists of a single DRDA and an
abstract-based summary comprised of keywords that
describe the decision. For both types of summary,
we employ standard techniques from text summa-
rization, but also explore the use of dialogue-specific
features and the use of discourse context.
4.1 DA-Level Summarization Based on Unsu-
pervised Methods
We make use of two unsupervised methods to sum-
marize the DRDAs in each ?decision cluster?. The
first method simply returns the longest DRDA in the
6The MUC scorer is popular in coreference evaluation, but it
is flawed in measuring the singleton clusters which is prevalent
in the AMI corpus. So we do not use it in this work.
Lexical Features
unigram/bigram
length of the DA
contain digits?
has overlapping words with next DA?
next DA is a positive feedback?
Structural Features
relative position in the meeting?(beginning, ending, or else)
in an AP?
if in an AP, AP type
if in an AP, the other part is decision-related?
if in an AP, is the source part or target part?
if in an AP and is source part, target is positive feedback?
if in an AP and is target part, source is a question?
Discourse Features
relative position to ?WRAP UP? or ?RECAP?
Other Features
DA type
speaker role
topic
Table 4: Features Used in DA-Level Summarization
cluster as the summary (LONGEST DA). The sec-
ond approach returns the decision cluster prototype,
i.e., the DRDA with the largest TF-IDF similar-
ity with the cluster centroid (PROTOTYPE DA). Al-
though important decision-related information may
be spread over multiple DRDAs, both unsupervised
methods allow us to determine summary quality
when summaries are restricted to a single utterance.
4.2 DA-Level and Token-Level Summarization
Using Supervised Learning
Because the AMI corpus contains a decision abstract
for each decision made in the meeting, we can use
this supervisory information to train classifiers that
can identify informative DRDAs (for DA-level sum-
maries) or informative tokens (for token-level sum-
maries).
20
Lexical Features
current token/current token and next token
length of the DA
is digit?
appearing in next DA?
next DA is a positive feedback?
Structural Features
see Table 3
Grammatical Features
part-of-speech
phrase type (VP/NP/PP)
dependency relations
Other Features
speaker role
topic
Table 5: Features Used in Token-Level Summarization
PREC REC F1
True Clusterings
Longest DA 0.3655 0.4077 0.3545
Prototype DA 0.3626 0.4140 0.3539
System Clusterings
using LDA
Longest DA 0.3623 0.1892 0.2214
Prototype DA 0.3669 0.1887 0.2212
using SVMs
Longest DA 0.3719 0.1261 0.1682
Prototype DA 0.3816 0.1264 0.1700
No Clustering
Longest DA 0.1039 0.1382 0.1080
Prototype DA 0.1350 0.1209 0.1138
Upper Bound 0.8970 0.4089 0.5333
Table 6: Results for ROUGE-1: Decision Summary Gen-
eration Using Unsupervised Methods
Dialogue Act-based Summarization. Previous
research (e.g., Murray et al (2005), Galley
(2006), Gurevych and Strube (2004)) has shown
that DRDA-level extractive summarization can be
effective when viewed as a binary classification task.
To implement this approach, we assume that the
DRDA to be extracted for the summary is the one
with the largest vocabulary overlap with the cluster?s
gold-standard decision abstract. This DA-level sum-
marization method has an advantage that the sum-
mary maintains good readability without a natural
language generation component.
Token-based Summarization. As shown in Table
1, some decision-related DAs contain many useless
words when compared with the gold-standard ab-
stracts. As a result, we propose a method for token-
level decision summarization that focuses on iden-
tifying critical keywords from the cluster?s DRDAs.
We follow the method of Ferna?ndez et al (2008a),
but use a larger set of features and different learning
methods.
Adding Discourse Context. For each of the su-
pervised DA- and token-based summarization meth-
ods, we also investigate the role of the discourse
context. Specifically, we augment the DRDA clus-
terings with additional (not decision-related) DAs
from the meeting dialogue: for each decision par-
tition, we include the DA with the highest TF-IDF
similarity with the centroid of the partition. We
will investigate the possible effects of this additional
context on summary quality.
In the next subsection, we describe the features
used for supervised learning of DA- and token-based
decision summaries.
4.3 Dialogue Cues for Decision Summarization
Different from text, dialogues have some notable
features that we expect to be useful for finding in-
formative, decision-related utterances. This section
describes some of the dialogue-based features em-
ployed in our classifiers. The full lists of features
are shown in Table 4 and Table 5.
Structural Information: Adjacency Pairs. An
Adjacency Pair (AP) is an important conversational
analysis concept; APs are considered the fundamen-
tal unit of conversational organization (Schegloff
and Sacks, 1973). In the AMI corpus, an AP pair
consists of a source utterance and a target utterance,
produced by different speakers. The source pre-
cedes the target but they are not necessarily adja-
cent. We include features to indicate whether or not
two DAs are APs indicating QUESTION+ANSWER
or POSITIVE FEEDBACK. For these features, we use
the gold-standard AP annotations. We also include
one feature that checks membership in a small set
of words to decide whether a DA contains positive
feedback (e.g., ?yeah?, ?yes?).
Discourse Information: Review and Closing In-
dicator. Another pragmatic cue for dialogue dis-
cussion is terms like ?wrap up? or ?recap?, indicat-
ing that speakers will review the key meeting con-
tent. We include the distance between these indica-
tors and DAs as a feature.
Grammatical Information: Dependency Relation
Between Words. For token-level summarization,
we make use of the grammatical relationships in
the DAs. As in Bui et al (2009) and Ferna?ndez
21
CRFs SVMs
PRECISION RECALL F1 PRECISION RECALL F1
True Clusterings
DA 0.3922 0.4449 0.3789 0.3661 0.4695 0.3727
Token 0.5055 0.2453 0.3033 0.4953 0.3788 0.3963
DA+Context 0.3753 0.4372 0.3678 0.3595 0.4449 0.3640
Token+Context 0.5682 0.2825 0.3454 0.6213 0.3868 0.4387
System Clusterings
using LDA
DA 0.3087 0.1663 0.1935 0.3391 0.2097 0.2349
Token 0.3379 0.0911 0.1307 0.3760 0.1427 0.1843
DA+Context 0.3305 0.1748 0.2041 0.2903 0.1869 0.2068
Token+Context 0.4557 0.1198 0.1727 0.4882 0.1486 0.2056
using SVMs
DA 0.3508 0.1884 0.2197 0.3592 0.2026 0.2348
Token 0.2807 0.04968 0.0777 0.3607 0.0885 0.1246
DA+Context 0.3583 0.1891 0.2221 0.3418 0.1892 0.2213
Token+Context 0.4891 0.0822 0.1288 0.4873 0.0914 0.1393
No Clustering
DA 0.08673 0.1957 0.0993 0.0707 0.1979 0.0916
Token 0.1906 0.0625 0.0868 0.1890 0.3068 0.2057
Table 7: Results for ROUGE-1: Summary Generation Using Supervised Learning
et al (2008a), we design features that encode (a)
basic predicate-argument structures involving major
phrase types (S, VP, NP, and PP) and (b) additional
typed dependencies from Marneffe et al (2006). We
use the Stanford Parser.
5 Experiments
Experiments based on supervised learning are per-
formed using 3-fold cross-validation. We train two
different types of classifiers for identifying infor-
mative DAs or tokens: Conditional Random Fields
(CRFs) (via Mallet) and Support Vector Machines
(SVMs) (via SVMlight).
We remove function words from DAs before us-
ing them as the input of our systems. The AMI deci-
sion abstracts are the gold-standard summaries. We
use the ROUGE (Lin and Hovy, 2003) evaluation
measure. ROUGE is a recall-based method that can
identify systems producing succinct and descriptive
summaries.7
Results and Analysis. Results for the unsuper-
vised and supervised summarization methods are
shown in Tables 6 and 7, respectively. In the tables,
TRUE CLUSTERINGS means that we apply our meth-
ods on the gold-standard DRDA clusterings. SYS-
TEM CLUSTERINGS use clusterings obtained from
the methods introduced in Section 4; we show re-
7We use the stemming option of the ROUGE software at
http://berouge.com/.
sults only using the best unsupervised (USING LDA)
and supervised (USING SVMS) DRDA clustering
techniques.
Both Table 6 and 7 show that some attempt to
cluster DRDAs improves the summarization results
vs. NO CLUSTERING. In Table 6, there is no signif-
icant difference between the results obtained from
the LONGEST DA and PROTOTYPE DA for any ex-
periment setting. This is because the longest DA is
often selected as the prototype. An UPPER BOUND
result is listed for comparison: for each decision
cluster, this system selects all words from the DR-
DAs that are part of the decision abstract (discarding
duplicates).
Table 7 presents the results for supervised sum-
marization. Rows starting with DA or TOKEN indi-
cate results at the DA- or token-level. The +CON-
TEXT rows show results when discourse context is
included.8 We see that: (1) SVMs have a superior or
comparable summarization performance vs. CRFs
on every task. (2) Token-level summaries perform
better than DA-level summaries only using TRUE
CLUSTERINGS and the SVM-based summarizer. (3)
Discourse context generally improves token-level
summaries but not DA-level summaries.9 (4) DRDA
8In our experiments, we choose the top 20 relevant DAs as
context.
9We do not extract words from the discourse context and
experiments where we tried this were unsuccessful.
22
clusterings produced by (unsupervised) LDA lead to
summaries that are quite comparable in quality to
those generated from DRDA clusterings produced
by SVMs (supervised). From Table 6, we see that
F1 is 0.2214 when choosing longest DAs from LDA-
generated clusterings, which is comparable with the
F1s of 0.1935 and 0.2349, attained when employing
CRF and SVMs on the same clusterings.
The results in Table 7 are achieved by compar-
ing abstracts having function words with system-
generated summaries without function words. To re-
duce the vocabulary difference as much as possible,
we also ran experiments that remove function words
from the gold-standard abstracts, but no significant
difference is observed.10
Finally, we considered comparing our systems to
the earlier similar work of (Ferna?ndez et al, 2008a)
and (Bui et al, 2009), but found that it would
be quite difficult because they employ a different
notion from DRDAs which is Decision Dialogue
Acts(DDAs). In addition, they manually annotate
words from their DDAs as the gold-standard sum-
mary, guaranteeing that their decision summaries
employ the same vocabulary as the DDAs. We in-
stead use the actual decision abstracts from the AMI
corpus.
5.1 Sample Decision Summaries
Here we show sample summaries produced using
our methods (Table 8). We pick one of the clus-
terings generated by LDA consisting of four DAs
which support two decisions and take SVMs as
the supervised summarization method. We remove
function words and special markers like ?[disf-
marker]? from the DAs.
The outputs indicate that either the longest DA or
prototype DA contains part of the decisions in this
?mixed? cluster. Adding discourse context refines
the summaries at both the DA- and token-levels.
6 Conclusion
In this work, we explore methods for producing de-
cision summaries from spoken meetings at both the
DA-level and the token-level. We show that clus-
10Given abstracts without function words, and using the clus-
terings generated by LDA and employ CRF on DA- and token-
level summarization, we get F1s of 0.1954 and 0.1329, which
is marginally better than the corresponding 0.1935 and 0.1307
in Table 7. Similarly, if SVMs are employed in the same cases,
we get F1s of 0.2367 and 0.1861 instead of 0.2349 and 0.1843.
All of the other results obtain negligible minor increases in F1.
DA (1): um of course , as [disfmarker] we , we?ve already
talked about the personal face plates in this meeting , (a)
DA (2): and I?d like to stick to that . (a)
DA (3): Well , I guess plastic and coated in rubber . (b)
DA (4): So the actual remote would be hard plastic and
the casings rubber . (b)
Decision (a): Will use personal face plates.
Decision (b): Case will be plastic and coated in rubber.
Longest DA:
talked about personal face plates in meeting
Prototype DA:
actual remote hard plastic casings rubber
DA-level:
talked about personal face plates in meeting, like to
stick to, guess plastic and coated in rubber,
actual remote hard plastic casings rubber
Token-level:
actual remote plastic casings rubber
DA-level and Discourse Context:
talked about personal face plates in meeting, guess plastic
and coated in rubber, actual remote hard plastic casings
rubber
Token-level and Discourse Context:
remote plastic rubber
Table 8: Sample system outputs by different methods are
in the third cell (methods? names are in bold). First cell
contains four DAs. (a) or (b) refers to the decision that
DA supports, which is listed in the second cell.
tering DRDAs before identifying informative con-
tent to extract can improve summarization quality.
We also find that unsupervised clustering of DR-
DAs (using LDA-based topic models) can produce
summaries of comparable quality to those gener-
ated from supervised DRDA clustering. Token-level
summarization methods can be boosted by adding
discourse context and outperform DA-level summa-
rization when true DRDA clusterings are available;
otherwise, DA-level summarization methods offer
better performance.
Acknowledgments. This work was supported in part
by National Science Foundation Grants IIS-0535099 and
IIS-0968450, and by a gift from Google.
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563?566.
Satanjeev Banerjee, Carolyn Penstein Rose?, and Alexan-
der I. Rudnicky. 2005. The necessity of a meet-
23
ing recording and playback system, and the benefit of
topic-level annotations to meeting browsing. In IN-
TERACT, pages 643?656.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Comput. Linguist.,
22:39?71, March.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Trung H. Bui, Matthew Frampton, John Dowding, and
Stanley Peters. 2009. Extracting decisions from
multi-party dialogue using directed graphical models
and semantic similarity. In Proceedings of the SIG-
DIAL 2009 Conference, pages 235?243.
Anne Hendrik Buist, Wessel Kraaij, and Stephan Raaij-
makers. 2004. Automatic summarization of meeting
data: A feasibility study. In in Proc. Meeting of Com-
putational Linguistics in the Netherlands (CLIN.
Jean Carletta, Simone Ashby, Sebastien Bourban,
Mike Flynn, Thomas Hain, Jaroslav Kadlec, Vasilis
Karaiskos, Wessel Kraaij, Melissa Kronenthal, Guil-
laume Lathoud, Mike Lincoln, Agnes Lisowska, and
Mccowan Wilfried Post Dennis Reidsma. 2005. The
ami meeting corpus: A pre-announcement. In In Proc.
MLMI, pages 28?39.
Freddy Y. Y. Choi. 2000. Advances in domain inde-
pendent linear text segmentation. In Proceedings of
the 1st North American chapter of the Association for
Computational Linguistics conference, pages 26?33.
Raquel Ferna?ndez, Matthew Frampton, John Dowding,
Anish Adukuzhiyil, Patrick Ehlen, and Stanley Peters.
2008a. Identifying relevant phrases to summarize de-
cisions in spoken meetings. INTERSPEECH-2008,
pages 78?81.
Raquel Ferna?ndez, Matthew Frampton, Patrick Ehlen,
Matthew Purver, and Stanley Peters. 2008b. Mod-
elling and detecting decisions in multi-party dialogue.
In Proceedings of the 9th SIGdial Workshop on Dis-
course and Dialogue, pages 156?163.
Matthew Frampton, Jia Huang, Trung Huu Bui, and Stan-
ley Peters. 2009. Real-time decision detection in
multi-party dialogue. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 3 - Volume 3, pages 1133?1141.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 364?
372.
Iryna Gurevych and Michael Strube. 2004. Semantic
similarity applied to spoken dialogue summarization.
In Proceedings of the 20th international conference on
Computational Linguistics.
Pei-Yun Hsueh and Johanna D. Moore. 2008. Automatic
decision detection in meeting speech. In Proceedings
of the 4th international conference on Machine learn-
ing for multimodal interaction, pages 168?179.
Thorsten Joachims. 1998. Text categorization with Sup-
port Vector Machines: Learning with many relevant
features. In Claire Ne?dellec and Ce?line Rouveirol,
editors, Machine Learning: ECML-98, volume 1398,
chapter 19, pages 137?142. Berlin/Heidelberg.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy - Volume 1, pages 71?78.
Inderjeet Mani. 1999. Advances in Automatic Text Sum-
marization. MIT Press, Cambridge, MA, USA.
Daniel Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. MIT Press, Cam-
bridge, MA, USA.
M. Marneffe, B. Maccartney, and C. Manning. 2006.
Generating Typed Dependency Parses from Phrase
Structure Parses. In Proceedings of LREC-06, pages
449?454.
Marina Meila?. 2007. Comparing clusterings?an infor-
mation based distance. J. Multivar. Anal., 98:873?895,
May.
Gabriel Murray, Steve Renals, and Jean Carletta. 2005.
Extractive summarization of meeting recordings. In
in Proceedings of the 9th European Conference on
Speech Communication and Technology, pages 593?
596.
Gerard Salton, Amit Singhal, Mandar Mitra, and Chris
Buckley. 1997. Automatic text structuring and
summarization. Inf. Process. Manage., 33:193?207,
March.
E. A. Schegloff and H. Sacks. 1973. Opening up clos-
ings. Semiotica, 8(4):289?327.
Shasha Xie, Yang Liu, and Hui Lin. 2008. Evaluating
the effectiveness of features and sampling in extractive
meeting summarization. In in Proc. of IEEE Spoken
Language Technology (SLT.
Klaus Zechner. 2002. Automatic summarization of
open-domain multiparty dialogues in diverse genres.
Comput. Linguist., 28:447?485, December.
24
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 40?49,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Unsupervised Topic Modeling Approaches to Decision Summarization in
Spoken Meetings
Lu Wang
Department of Computer Science
Cornell University
Ithaca, NY 14853
luwang@cs.cornell.edu
Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
cardie@cs.cornell.edu
Abstract
We present a token-level decision summariza-
tion framework that utilizes the latent topic
structures of utterances to identify ?summary-
worthy? words. Concretely, a series of
unsupervised topic models is explored and
experimental results show that fine-grained
topic models, which discover topics at the
utterance-level rather than the document-level,
can better identify the gist of the decision-
making process. Moreover, our proposed
token-level summarization approach, which
is able to remove redundancies within utter-
ances, outperforms existing utterance ranking
based summarization methods. Finally, con-
text information is also investigated to add ad-
ditional relevant information to the summary.
1 Introduction
Meetings are an important way for information shar-
ing and collaboration, where people can discuss
problems and make concrete decisions. Not sur-
prisingly, there is an increasing interest in develop-
ing methods for extractive summarization for meet-
ings and conversations (Zechner, 2002; Maskey and
Hirschberg, 2005; Galley, 2006; Lin and Chen,
2010; Murray et al, 2010a). Carenini et al (2011)
describe the specific need for focused summaries of
meetings, i.e., summaries of a particular aspect of a
meeting rather than of the meeting as a whole. For
example, the decisions made, the action items that
emerged and the problems arised are all important
outcomes of meetings. In particular, decision sum-
maries would allow participants to review decisions
from previous meetings and understand the related
topics quickly, which facilitates preparation for the
upcoming meetings.
A:We decided our target group is the focus on who can
afford it , (1)
B:Uh I?m kinda liking the idea of latex , if if spongy is
the in thing . (2)
B:what I?ve seen , just not related to this , but of latex
cases before , is that [vocalsound] there?s uh like a hard
plastic inside , and it?s just covered with the latex . (2)
C:Um [disfmarker] And I think if we wanna keep our costs
down , we should just go for pushbuttons , (3)
D:but if it?s gonna be in a latex type thing and that?s
gonna look cool , then that?s probably gonna have a
bigger impact than the scroll wheel . (2)
A:we?re gonna go with um type pushbuttons , (3)
A:So we?re gonna have like a menu button , (4)
C:uh volume , favourite channels , uh and menu . (4)
A:Pre-set channels (4)
Decision Abstracts (Summary)
DECISION 1: The target group comprises of individuals
who can afford the product.
DECISION 2: The remote will have a latex case.
DECISION 3: The remote will have pushbuttons.
DECISION 4: The remote will have a power button, volume
buttons, channel preset buttons, and a menu button.
Figure 1: A clip of a meeting from the AMI meeting cor-
pus (Carletta et al, 2005). A, B, C and D refer to distinct
speakers; the numbers in parentheses indicate the associated
meeting decision: DECISION 1, 2, 3 or 4. Also shown is the
gold-standard (manual) abstract (summary) for each decision.
Meeting conversation is intrinsically different
from well-written text, as meetings may not be well
organized and most utterances have low density of
salient content. Therefore, multiple problems need
to be addressed for speech summarization. Consider
the sample dialogue snippet in Figure 1 from the
AMI meeting corpus (Carletta et al, 2005). Only
decision-related dialogue acts (DRDAs) ? utter-
40
ances at least one decision made in the meeting1 ?
are listed and ordered by time. Each DRDA is la-
beled numerically according to the decision it sup-
ports; so the second and third utterances (in bold)
support DECISION 2, as do the fifth utterance in the
snippet. Manually constructed decision abstracts for
each decision are shown at the bottom of the figure.
Besides the prevalent dialogue phenomena (such
as ?Uh I?m kinda liking? in Figure 1), disfluencies
and off-topic expressions, we notice that single ut-
terance is usually not informative enough to form
a decision. For instance, no single DRDA associ-
ated with DECISION 4 corresponds all that well with
its decision abstract: ?pushbuttons?, ?menu button?
and ?Pre-set channels? are mentioned in separate
DAs. As a result, extractive summarization methods
that select individual utterance to form the summary
will perform poorly.
Furthermore, it is difficult to identify the core
topic when multiple topics are discussed in one ut-
terance. For example, all of the bold DRDAs sup-
porting DECISION 2 contain the word ?latex?. How-
ever, the last DA in bold also mentions ?bigger im-
pact? and ?the scroll wheel?, which are not specifi-
cally relevant for DECISION 2. Though this problem
can be approached by training a classifier to identify
the relevant phrases and ignore the irrelevant ones
or dialogue phenomena, it needs expensive human
annotation and is limited to the specific domain.
Note also that for DECISION 4, the ?power but-
ton? is not specified in any of the listed DRDAs
supporting it. By looking at the transcript, we find
?power button? mentioned in one of the preceding,
but not decision-related DAs. Consequently another
challenge would be to add complementary knowl-
edge when the DRDAs cannot provide complete in-
formation.
Therefore, we need a summarization approach
that is tolerant of dialogue phenomena, can deter-
mine the key semantic content and is easily trans-
ferable between domains. Recently, topic model-
ing approaches have been investigated and achieved
state-of-the-art results in multi-document summa-
rization (Haghighi and Vanderwende, 2009; Celiky-
1These DRDAs are annotated in the AMI corpus and usually
contain the decision content. They are similar, but not com-
pletely equivalent, to the decision dialogue acts (DDAs) of Bui
et al (2009), Ferna?ndez et al (2008), Frampton et al (2009).
ilmaz and Hakkani-Tur, 2010). Thus, topic mod-
els appear to be a better ref for document simi-
larity w.r.t. semantic concepts than simple literal
word matching. However, very little work has in-
vestigated its role in spoken document summariza-
tion (Chen and Chen, 2008; Hazen, 2011), and much
less conducted comparisons among topic modeling
approaches for focused summarization in meetings.
In contrast to previous work, we study the un-
supervised token-level decision summarization in
meetings by identifying a concise set of key words
or phrases, which can either be output as a com-
pact summary or be a starting point to generate ab-
stractive summaries. This paper addresses problems
mentioned above and make contributions as follows:
? As a step towards creating the abstractive sum-
maries that people prefer when dealing with spo-
ken language (Murray et al, 2010b), we propose a
token-level rather than sentence-level framework
for identifying components of the summary. Ex-
perimental results show that, compared to the sen-
tence ranking based summarization algorithms,
our token-level summarization framework can bet-
ter identify the summary-worthy words and re-
move the redundancies.
? Rather than employing supervised learning meth-
ods that rely on costly manual annotation, we ex-
plore and evaluate topic modeling approaches of
different granularities for the unsupervised deci-
sion summarization at both the token-level and di-
alogue act-level. We investigate three topic mod-
els ? Local LDA (LocalLDA) (Brody and El-
hadad, 2010), Multi-grain LDA (MG-LDA) (Titov
and McDonald, 2008) and Segmented Topic
Model (STM) (Du et al, 2010) ? which can uti-
lize the latent topic structure on utterance level
instead of document level. Under our proposed
token-level summarization framework, three fine-
grained models outperform the basic LDA model
and two extractive baselines that select the longest
and the most representative utterance for each de-
cision, respectively. (ROUGE-SU4 F score of
14.82% for STM vs. 13.58% and 13.46% for
the baselines, given the perfect clusterings of DR-
DAs.)
? In line with prior research that explore the role of
context for utterance-based extractive summariza-
41
tion (Murray and Renals, 2007), we investigate the
role of context in our token-level summarization
framework. For the given clusters of DRDAs, We
study two types of context information ? the DAs
preceding and succeeding a DRDA and DAs of
high TF-IDF similarity with a DRDA. We also in-
vestigate two ways to select relevant words from
the context DA. Experimental results show that
two types of context have comparable effect, but
selecting words from the dominant topic of the
center DRDA performs better than from the dom-
inant topic of the context DA. Moreover, by lever-
aging context, the recall exceeds the provided up-
perbound?s recall (ROUGE-1 recall: 48.10% vs.
45.05% for upperbound by using DRDA only) al-
though the F scores decrease after adding context
information. Finally, we show that when the true
DRDA clusterings are not available, adding con-
text can improve both the recall and F score.
2 Related Work
Speech and dialogue summarization has become im-
portant in recent years as the number of multime-
dia resources containing speech has grown. A pri-
mary goal for most speech summarization systems
is to account for the special characteristics of di-
alogue. Early work in this area investigated su-
pervised learning methods, including maximum en-
tropy, conditional random fields (CRFs), and sup-
port vector machines (SVMs) (Buist et al, 2004;
Galley, 2006; Xie et al, 2008). For unsupervised
methods, maximal marginal relevance (MMR) is in-
vestigated in (Zechner, 2002) and (Xie and Liu,
2010). Gillick et al (2009) introduce a concept-
based global optimization framework by using in-
teger linear programming (ILP).
Only in very recent works has decision sum-
marization been addressed in (Ferna?ndez et al,
2008), (Bui et al, 2009) and (Wang and Cardie,
2011). (Ferna?ndez et al, 2008) and (Bui et al, 2009)
utilize semantic parser to identify candidate phrases
for decision summaries and employ SVM to rank
those phrases. They also train HMM and SVM
directly on a set of decision-related dialogue acts
on token level and use the classifiers to identify
summary-worthy words. Wang and Cardie (2011)
provide an exploration on supervised and unsuper-
vised learning for decision summarization on both
utterance- and token- level.
Our work also arises out of applying topic mod-
els to text summarization (Bhandari et al, 2008;
Haghighi and Vanderwende, 2009; Celikyilmaz and
Hakkani-Tur, 2010; Celikyilmaz and Hakkani-Tur,
2010). Mostly, the sentences are ranked according to
importance based on latent topic structures, and top
ones are selected as the summary. There are some
works for applying document-level topic models to
speech summarization (Kong and shan Leek, 2006;
Chen and Chen, 2008; Hazen, 2011). Different from
their work, we further investigate the topic models of
fine granularity on sentence level and leverage con-
text information for decision summarization task.
Most existing approaches for speech summariza-
tion result in a selection of utterances from the dia-
logue, which cannot remove the redundancy within
utterances. To eliminate the superfluous words, our
work is also inspired by keyphrase extraction of
meetings (Liu et al, 2009; Liu et al, 2011) and
keyphrase based summarization (Riedhammer et al,
2010). However, a small set of keyphrases are not
enough to concretely display the content. Instead of
only picking up keyphrases, our work identifies all
of the summary-worthy words and phrases, and re-
moves redundancies within utterances.
3 Summarization Frameworks
In this section, we first present our proposed token-
level decision summarization framework ? Dom-
Sum ? which utilizes latent topic structure in ut-
terances to extract words from Dominant Topic (see
details in Section 3.1) to form Summaries. In Sec-
tion 3.2, we describe four existing sentence scor-
ing metrics denoted as OneTopic, MultiTopic, TMM-
Sum and KLSum which are also based on latent topic
distributions. We adopt them to the utterance-level
summarization for comparison in Section 6.
3.1 Token-level Summarization Framework
Domsum takes as input the clusters of DRDAs (with
or without additional context DAs), the topic distri-
bution for each DA and the word distribution for
each topic. The output is a set of topic-coherent
summary-worthy words which can be used directly
as the summary or to further generate abstractive
summary. We introduce DomSum in two steps ac-
cording to its input: taking clusters of DRDAs as the
input and with additional context information.
42
DRDAs Only. Given clusters of DRDAs, we use
Algorithm 1 to produce the token-level summary for
each cluster. Generally, Algorithm 1 chooses the
topic with the highest probability as the dominant
topic given the dialogue act (DA). Then it collects
the words with a high joint probability with the dom-
inant topic from that DA.
Input : Cluster C = {DAi}, P (Tj |DAi), P (wk|Tj)
Output: Summary
Summary? ? (empty set)
foreach DAi in C do
DomTopic? maxTj P (Tj |DAi) (*)Candidate? ?
foreach word wk inDAi do
SampleTopic? maxTj P (wk|Tj)P (Tj |DAi)
if DomTopic == SampleTopic then
Candidate? Union(Candidate, wk)
end
end
Summary? Union(Summary, Candidate)
end
Algorithm 1: DomSum ? The token-level sum-
marization framework. DomSum takes as input the
clusters of DRDAs and related probability distribu-
tions.
Leveraging Context. For each DRDA (denoted as
?center DA?), we study two types of context infor-
mation (denoted as ?context DAs?). One is adjacent
DAs, i.e., immediately preceding and succeeding
DAs, the other is the DAs having top TF-IDF simi-
larities with the center DA. Context DAs are added
into the cluster the corresponding center DA in.
We also study two criteria of word selection from
the context DAs. For each context DA, we can take
the words appearing in the dominant topic of ei-
ther this context DA or its center DRDA. We will
show in Section 6.1 that the latter performs better
as it produces more topic-coherent summaries. Al-
gorithm 1 can be easily modified to leverage context
DAs by updating the input clusters and assigning the
proper dominant topic for each DA accordingly ?
this changes the step (?) in Algorithm 1.
3.2 Utterance-level Summarization Metrics
We also adopt four sentence scoring metrics based
on the latent topic structure for extractive summa-
rization. Though they are developed on different
topic models, given the desired topic distributions as
input, they can rank the utterances according to their
importance and provide utterance-level summaries
for comparison.
OneTopic and MultiTopic. In (Bhandari et al,
2008), several sentence scoring functions are intro-
duced based on Probabilistic Latent Semantic Index-
ing. We adopt two metrics, which are OneTopic
and MultiTopic. For OneTopic, topic T with high-
est probability P (T ) is picked as the central topic
per cluster C. The score for DA in C is:
P (DA|T ) =
?
w?DA P (T |DA,w)?
DA??C,w?DA? P (T |DA?, w)
,
MultiTopic modifies OneTopic by taking all of the
topics into consideration. Given a cluster C, DA in
C is scored as:
?
T
P (DA|T )P (T ) =
?
T
?
w?DA P (T |DA,w)?
DA??C,w?DA? P (T |DA?, w)
P (T )
TMMSum. Chen and Chen (2008) propose a Top-
ical Mixture Model (TMM) for speech summariza-
tion, where each dialogue act is modeled as a TMM
for generating the document. TMM is shown to
provide better utterance-level extractive summaries
for spoken documents than other conventional unsu-
pervised approaches, such as Vector Space Model
(VSM) (Gong and Liu, 2001), Latent Semantic
Analysis (LSA) (Gong and Liu, 2001) and Max-
imum Marginal Relevance (MMR) (Murray et al,
2005). The importance of a sentence S can be mea-
sured by its generative probability P (D|S), where
D is the document S belongs to. In our experiments,
one decision is made per cluster of DAs. So we
adopt their scoring metric to compute the generative
probability of the cluster C for each DA:
P (C|DA) =
?
wi?C
?
Tj
P (wi|Tj)P (Tj |DA),
KLSum. Kullback-Lieber (KL) divergence is ex-
plored for summarization in (Haghighi and Vander-
wende, 2009) and (Lin et al, 2010), where it is used
to measure the distance of distributions between the
document and the summary. For a cluster C of DAs,
given a length limit ?, a set of DAs S is selected as:
S? = arg min
S:|S|<?
KL(PC ||PS) = arg min
S:|S|<?
?
Ti
P (Ti|C)log
P (Ti|C)
P (Ti|S)
4 Topic Models
In this section, we briefly describe the three fine-
grained topic models employed to compute the la-
tent topic distributions on utterance level in the
43
meetings. According to the input of Algorithm 1,
we are interested in estimating the topic distribution
for each DA P (T |DA) and the word distribution
for each topic P (w|T ). For MG-LDA, P (T |DA)
is computed as the expectation of local topic distri-
butions with respect to the window distribution.
4.1 Local LDA
Local LDA (LocalLDA) (Brody and Elhadad, 2010)
uses almost the same probabilistic generative model
as Latent Dirichlet Allocation (LDA) (Blei et al,
2003), except that it treats each sentence as a sepa-
rate document2. Each DA d is generated as follows:
1. For each topic k:
(a) Choose word distribution: ?k ? Dir(?)
2. For each DA d:
(a) Choose topic distribution: ?d ? Dir(?)
(b) For each word w in DA d:
i. Choose topic: zd,w ? ?d
ii. choose word: w ? ?zd,w
4.2 Multi-grain LDA
Multi-grain LDA (MG-LDA) (Titov and McDonald,
2008) can model both the meeting specific topics
(e.g. the design of a remote control) and various con-
crete aspects (e.g. the cost or the functionality). The
generative process is:
1. Choose a global topic distribution: ?glm ? Dir(?gl)
2. For each sliding window v of size T :
(a) Choose local topic distribution: ?locm,v ? Dir(?loc)
(b) Choose granularity mixture: pim,v ? Beta(?mix)
3. For each DA d:
(a) choose window distribution: ?m,d ? Dir(?)
4. For each word w in DA d of meeting m:
(a) Choose sliding window: vm,w ? ?m,d
(b) Choose granularity: rm,w ? pim,vm,w
(c) If rm,w = gl, choose global topic: zm,w ? ?glm
(d) If rm,w = loc, choose local topic: zm,w ? ?locm,vm,w
(e) Choose word w from the word distribution: ?rm,wzm,w
4.3 Segmented Topic Model
The last model we utilize is Segmented Topic Model
(STM) (Du et al, 2010), which jointly models
document- and sentence-level latent topics using
a two-parameter Poisson Dirichlet Process (PDP).
Given parameters ?, ?,? and PDP parameters a, b,
the generative process is:
1. Choose distribution of topics: ?m ? Dir(?)
2. For each dialogue act d:
2For the generative process of LDA, the DAs in the same
meeting make up the document, so ?each DA? is changed to
?each meeting? in LocalLDA?s generative process.
(a) Choose distribution of topics: ?d ? PDP (?m, a, b)
3. For each word w in dialogue act d:
(a) Choose topic: zm,w ? ?d
(b) Choose word: w ? ?zm,w
5 Experimental Setup
The Corpus. We evaluate our approach on the
AMI meeting corpus (Carletta et al, 2005) that con-
sists of 140 multi-party meetings. The 129 scenario-
driven meetings involve four participants playing
different roles on a design team. A short (usually
one-sentence) abstract is manually constructed to
summarize each decision discussed in the meeting
and used as gold-standard summaries in our experi-
ments.
System Inputs. Our summarization system re-
quires as input a partitioning of the DRDAs accord-
ing to the decision(s) that each supports (i.e., one
cluster of DRDAs per decision). As mentioned ear-
lier, we assume for all experiments that the DRDAs
for each meeting have been identified. For evalua-
tion we consider two system input settings. In the
True Clusterings setting, we use the AMI annota-
tions to create perfect partitionings of the DRDAs
as the input; in the System Clusterings setting, we
employ a hierarchical agglomerative clustering algo-
rithm used for this task in previous work (Wang and
Cardie, 2011). The Wang and Cardie (2011) cluster-
ing method groups DRDAs according to their LDA
topic distribution similarity. As better approaches
for DRDA clustering become available, they could
be employed instead.
Evaluation Metric. To evaluate the performance
of various summarization approaches, we use the
widely accepted ROUGE (Lin and Hovy, 2003) met-
rics. We use the stemming option of the ROUGE
software at http://berouge.com/ and remove
stopwords from both the system and gold-standard
summaries, same as Riedhammer et al (2010) do.
Inference and Hyperparameters We use the im-
plementation from (Lu et al, 2011) for the three
topic models in Section 4. The collapsed Gibbs
Sampling approach (Griffiths and Steyvers, 2004) is
exploited for inference. Hyperparameters are cho-
sen according to (Brody and Elhadad, 2010), (Titov
and McDonald, 2008) and (Du et al, 2010). In LDA
and LocalLDA, ? and ? are both set to 0.1 . For
MG-LDA, ?gl, ?loc and ?mix are set to 0.1; ? is 0.1
44
and the window size T is 3. And the number of lo-
cal topic is set as the same number of global topic as
discussed in (Titov and McDonald, 2008). In STM,
?, a and b are set to 0.5, 0.1 and 1, respectively.
5.1 Baselines and Comparisons
We compare our token-level summarization frame-
work based on the fine-grained topic models to (1)
two unsupervised baselines, (2) token-level summa-
rization by LDA, (3) utterance-level summarization
by Topical Mixture Model (TMM) (Chen and Chen,
2008), (4) utterance-level summarization based on
the fine-grained topic models using existing metrics
(Section 3.2), (5) two supervised methods, and (6)
an upperbound derived from the AMI gold standard
decision abstracts. (1) and (6) are described below,
others will be discussed in Section 6.
The LONGEST DA Baseline. As in (Riedhammer
et al, 2010) and (Wang and Cardie, 2011), this base-
line simply selects the longest DRDA in each cluster
as the summary. Thus, it performs utterance-level
decision summarization. This baseline and the next
allow us to determine summary quality when sum-
maries are restricted to a single utterance.
The PROTOTYPE DA Baseline. Following Wang
and Cardie (2011), the second baseline selects the
decision cluster prototype (i.e., the DRDA with the
largest TF-IDF similarity with the cluster centroid)
as the summary.
Upperbound. We also compute an upperbound
that reflects the gap between the best possible ex-
tractive summaries and the human-written abstracts
according to the ROUGE score: for each cluster of
DRDAs, we select the words that also appear in the
associated decision abstract.
6 Results and Discussion
6.1 True Clusterings
How do fine-grained topic models compare to ba-
sic topic models or baselines? Figure 2 demon-
strates that by using the DomSum token-level sum-
marization framework, the three fine-grained topic
models uniformly outperform the two non-trivial
baselines and TMM (Chen and Chen, 2008) (reim-
plemented by us) that generates utterance-level sum-
maries. Moreover, the fine-grained models also beat
basic LDA under the same DomSum token-level
summarization framework. This shows the fine-
2 3 4 5 6 7 8 9 106
7
8
9
10
11
12
13
14
15
#Topics
ROU
GE?S
U4 F
 (%)
Comparison with Baselines, TMM and LDA
 
 
LocalLDAMG?LDASTMLDATMMBASELINE 1BASELINE 2
Figure 2: With true clusterings of DRDAs as the input, we use
DomSum to compare the performance of LocalLDA, MGLDA
and STM against two baselines, LDA and TMM. ?# topic? in-
dicates the number of topics for the model. For MGLDA, ?#
topic? is the number of local topics.
2 3 4 5 6 7 8 9 1013
13.2
13.4
13.6
13.8
14
14.2
14.4
14.6
14.8
15
#Topic    
ROU
GE?S
U4 F 
(%)
Summarization from DRDAs by Different Metrics Based on STM (DRDA only)
 
 
DomSumOneTopicMultiTopicTMMSumKLSum
Figure 3: With true clusterings of DRDAs as the input, Dom-
Sum is compared with four DA-level summarization metrics us-
ing topic distributions from STM. Results from LocalLDA and
MGLDA are similar so they are not displayed.
grained topic models that discover topic structures
on utterance-level better identify gist information.
Can the proposed token-level summarization
framework better identify important words and
remove redundancies than utterance selection
methods? Figure 3 demonstrates the comparison
results for our DomSum token-level summarization
framework with four existing utterance scoring met-
rics discussed in Section 3.2, namely OneTopic,
MultiTopic, TMMSum and KLSum. The utterance
with highest score is extracted to form the summary.
LocalLDA and STM are utilized to compute the in-
put distributions, i.e., P (T |DA) and P (w|T ). From
Figure 3, DomSum yields the best F scores which
45
2 3 4 5 6 7 8 9 107.5
8
8.5
9
9.5
10
10.5
11
#Topic             
ROU
GE?
SU4
 F (%
)
Leveraging Context by STM
 
 Adj+DomSum(Multi)
Adj+DomSum(One)
TFIDF+DomSum(Multi)
TFIDF+DomSum(One)
Figure 4: Under DomSum framework, two types of context
information are added: Adjacent DA (?Adj?) and DAs with high
TFIDF similarities (?TFIDF?). For each context DA, selecting
words from the dominant topic of center DA (?One?) or the
current context DA (?Multi?) are investigated.
2 3 4 5 6 7 8 9 107.5
8
8.5
9
9.5
10
10.5
11
11.5
12
12.5
#Topic              
ROU
GE?
SU4
 F (%
)
Summarization by Different Metrics (adding Context)
 
 LocalLDA+DomSum(One)STM+DomSum(One)LocalLDA+OneTopicSTM+OneTopicLocalLDA+MultiTopicSTM+MultiTopic
Figure 5: By using adjacent DAs as context, DomSum is com-
pared with two DA-level summarization metrics: OneTopic and
MultiTopic. For DomSum, the words of context DA from dom-
inant topic of the center DA (?One?) is selected; For OneTopic
and MultiTopic, three top ranked DAs are selected.
shows that the token-level summarization approach
is more effective than utterance-level methods.
Which way is better for leveraging context infor-
mation? We explore two types of context infor-
mation. For adjacent content (Adj in Figure 4), 5
DAs immediately preceding and 5 DAs succeeding
the center DRDA are selected. For TF-IDF context
(TFIDF in Figure 4), 10 DAs of highest TF-IDF sim-
ilarity with the center DRDA are taken. We also
explore two ways to extract summary-worthy words
from the context DA ? selecting words from the
dominant topic of either the center DA (denoted as
?One? in parentheses in Figure 4) or the current con-
text DA (denoted as ?multi? in parentheses in Fig-
True Clusterings
R-1 R-2 R-SU4
PREC REC F1 F1 F1
Baselines
Longest DA 34.06 31.28 32.61 12.03 13.58
Prototype DA 40.72 28.21 33.32 12.18 13.46
Supervised
Methods
CRF 52.89 26.77 35.53 11.48 14.03
SVM 43.24 37.92 40.39 12.78 16.24
Our Approach
5 topics
LocalLDA 35.18 38.92 36.95 12.33 14.74
+ context 17.26 45.34 25.00 8.40 11.05
STM 34.06 41.30 37.32 12.42 14.82
+ context 15.60 48.10 23.56 8.16 9.98
10 topics
LocalLDA 36.20 36.81 36.50 12.04 14.34
+ context 21.82 41.57 28.62 9.61 12.24
STM 34.15 40.83 37.19 12.40 14.56
+ context 17.87 46.57 25.82 8.89 10.97
Upperbound 100.00 45.05 62.12 33.27 34.89
Table 1: ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-SU4
(R-SU4) scores for our proposed token-level summarization ap-
proaches along with two baselines, supervised methods and the
Upperbound (only using DRDAs). ? all use True Clusterings
ure 4). Figure 4 indicates that the two types of con-
text information do not have significant difference,
while selecting the words from the dominant topic
of the center DA results in better ROUGE-SU4 F
scores. Notice that compared with Figure 3, the re-
sults in Figure 4 have lower F scores when using the
true clusterings of DRDAs. This is because context
DAs bring in relevant words as well as noisy infor-
mation. We will show in Section 6.2 that when true
clusterings are not available, the context information
can boost both recall and F score.
How do the token-level summarization frame-
work compared to utterance selection methods
for leveraging context? We also compare the
ability of leveraging context of DomSum to utter-
ance scoring metrics, i.e., OneTopic and MultiTopic.
5 DAs preceding and 5 DAs succeeding the center
DA are added as context information. For context
DA under DomSum, we select words from the dom-
inant topic of the center DA (denoted as ?One? in
parentheses in Figure 5). For OneTopic and Mul-
tiTopic, the top 3 DAs are extracted as the sum-
mary. Figure 5 demonstrates the combination of Lo-
calLDA and STM with each of the metrics. Dom-
Sum, as a token-level summarization metrics, domi-
nates other two metrics in leveraging context.
46
System Clusterings
R-1 R-2 R-SU4
PREC REC F1 F1 F1
Baselines
Longest DA 17.06 11.64 13.84 2.76 3.34
Prototype DA 18.14 10.11 12.98 2.84 3.09
Supervised
Methods
CRF 46.97 15.25 23.02 6.09 9.11
SVM 39.05 18.45 25.06 6.11 9.82
Our Approach
5 topics
LocalLDA 25.57 16.57 20.11 4.03 5.87
+ context 20.68 25.96 23.02 3.09 4.48
STM 24.15 17.82 20.51 4.03 5.69
+ context 20.64 30.03 24.47 3.59 4.76
10 topics
LocalLDA 25.98 15.94 19.76 3.59 4.41
+ context 23.98 21.92 22.90 3.45 4.10
STM 26.32 19.14 22.16 4.07 5.88
+ context 22.50 28.40 25.11 3.43 4.15
Table 2: ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-SU4
(R-SU4) scores for our proposed token-level summarization ap-
proaches, compared with two baselines and supervised meth-
ods. ? all use System Clusterings
How do our approach perform when compared
with supervised learning approaches? For a bet-
ter comparison, we also provide summarization
results by using supervised systems along with
an upperbound. We use Support Vector Ma-
chines (Joachims, 1998) with RBF kernel and order-
1 Conditional Random Fields (Lafferty et al, 2001)
? trained with the same features as (Wang and
Cardie, 2011) to identify the summary-worthy to-
kens to include in the abstract. A three-fold cross
validation is conducted for both methods. ROUGE-
1, ROUGE-2 and ROUGE-SU4 scores are listed in
Table 1. From Table 1, our token-level summa-
rization approaches based on LocalLDA and STM
are shown to outperform the baselines and even the
CRF. Meanwhile, by adding context information,
both LocalLDA and STM can get better ROUGE-1
recall than the supervised methods, even higher than
the provided upperbound which is computed by only
using DRDAs. This shows the DomSum framework
can leverage context to compensate the summaries.
6.2 System Clusterings
Results using the System Clusterings (Table 2)
present similar findings, though all of the system and
baseline scores are lower. By adding context infor-
mation, the token-level summarization approaches
based on fine-grained topic models compare favor-
DRDA (1): I think if we can if we can include them at not too
much extra cost, then I?d put them in,
DRDA (2): Uh um we we?re definitely going in for voice
recognition as well as LCDs, mm.
DRDA (3): So we?ve basically worked out that we?re going
with a simple battery,
context DA (1):So it?s advanced integrated circuits?
context DA (2):the advanced chip
context DA (3): and a curved on one side case which is folded
in on itself , um made out of rubber
Decision Abstract: It will have voice recognition, use a simple
battery, and contain an advanced chip.
Longest DA & Prototype DA: Uh um we we?re definitely going
in for voice recognition as well as LCDs, mm.
TMM: I think if we can if we can include them at not too much
extra cost, then I?d put them in,
SVM: cost voice recognition simple battery
CRF: voice recognition battery
STM: extra cost, definitely going voice recognition LCDs,
simple battery
STM + context: cost, company, advanced integrated circuits, going
voice recognition, simple battery, advanced chip, curved case rubber
Table 3: Sample system outputs by different methods are in the
third cell (methods? names are in bold). First cell contains three
DRDAs supporting the decision in the second cell and three ad-
jacent DAs of them.
ably to the supervised methods in F scores, and also
get the best ROUGE-1 recalls.
6.3 Sample System Summaries
To better exemplify the summaries generated by
different systems, sample output for each method
is shown in Table 3. We see from the table that
utterance-level extractive summaries (Longest DA,
Prototype DA, TMM) make more coherent but still
far from concise and compact abstracts. On the other
hand, the supervised methods (SVM, CRF) that pro-
duce token-level extracts better identify the overall
content of the decision abstract. Unfortunately, they
require human annotation in the training phase. In
comparison, the output of fine-grained topic models
can cover the most useful information.
7 Conclusion
We propose a token-level summarization framework
based on topic models and show that modeling topic
structure at the utterance-level is better at identify-
ing relevant words and phrases than document-level
models. The role of context is also studied and
shown to be able to identify additional summary-
worthy words.
Acknowledgments This work was supported in part by
National Science Foundation Grants IIS-0968450 and
IIS-1111176, and by a gift from Google.
47
References
Harendra Bhandari, Takahiko Ito, Masashi Shimbo, and
Yuji Matsumoto. 2008. Generic text summarization
using probabilistic latent semantic indexing. In Pro-
ceedings of IJCNLP, pages 133?140.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, HLT ?10,
pages 804?812, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Trung H. Bui, Matthew Frampton, John Dowding, and
Stanley Peters. 2009. Extracting decisions from
multi-party dialogue using directed graphical models
and semantic similarity. In Proceedings of the SIG-
DIAL 2009 Conference, pages 235?243.
Anne Hendrik Buist, Wessel Kraaij, and Stephan Raaij-
makers. 2004. Automatic summarization of meeting
data: A feasibility study. In Proc. Meeting of Compu-
tational Linguistics in the Netherlands (CLIN).
Giuseppe Carenini, Gabriel Murray, and Raymond Ng.
2011. Methods for Mining and Summarizing Text Con-
versations. Morgan & Claypool Publishers.
Jean Carletta, Simone Ashby, Sebastien Bourban,
Mike Flynn, Thomas Hain, Jaroslav Kadlec, Vasilis
Karaiskos, Wessel Kraaij, Melissa Kronenthal, Guil-
laume Lathoud, Mike Lincoln, Agnes Lisowska, and
Mccowan Wilfried Post Dennis Reidsma. 2005. The
ami meeting corpus: A pre-announcement. In In Proc.
MLMI, pages 28?39.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A hy-
brid hierarchical model for multi-document summa-
rization. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, ACL
?10, pages 815?824, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Berlin Chen and Yi-Ting Chen. 2008. Extractive
spoken document summarization for information re-
trieval. Pattern Recogn. Lett., 29:426?437, March.
Lan Du, Wray Buntine, and Huidong Jin. 2010. A
segmented topic model based on the two-parameter
poisson-dirichlet process. Mach. Learn., 81:5?19, Oc-
tober.
Raquel Ferna?ndez, Matthew Frampton, John Dowding,
Anish Adukuzhiyil, Patrick Ehlen, and Stanley Peters.
2008. Identifying relevant phrases to summarize de-
cisions in spoken meetings. INTERSPEECH-2008,
pages 78?81.
Matthew Frampton, Jia Huang, Trung Huu Bui, and Stan-
ley Peters. 2009. Real-time decision detection in
multi-party dialogue. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 3 - Volume 3, pages 1133?1141.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 364?
372.
Dan Gillick, Korbinian Riedhammer, Benoit Favre, and
Dilek Hakkani-Tur. 2009. A global optimization
framework for meeting summarization. In Proceed-
ings of the 2009 IEEE International Conference on
Acoustics, Speech and Signal Processing, ICASSP
?09, pages 4769?4772. IEEE Computer Society.
Yihong Gong and Xin Liu. 2001. Generic text summa-
rization using relevance measure and latent semantic
analysis. In Proceedings of the 24th annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, SIGIR ?01, pages 19?
25, New York, NY, USA. ACM.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101(Suppl. 1):5228?5235, April.
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL ?09, pages 362?370. Association for Compu-
tational Linguistics.
Timothy J. Hazen. 2011. Latent topic modeling for au-
dio corpus summarization. In INTERSPEECH, pages
913?916.
Thorsten Joachims. 1998. Text categorization with Sup-
port Vector Machines: Learning with many relevant
features. In Claire Ne?dellec and Ce?line Rouveirol,
editors, Machine Learning: ECML-98, volume 1398,
chapter 19, pages 137?142. Berlin/Heidelberg.
Sheng-Yi Kong and Lin shan Leek. 2006. Improved spo-
ken document summarization using probabilistic latent
semantic analysis (plsa). In Proceedings of the 2006
IEEE International Conference on Acoustics, Speech
and Signal Processing, ICASSP ?06.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
48
Shih-Hsiang Lin and Berlin Chen. 2010. A risk mini-
mization framework for extractive speech summariza-
tion. In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, ACL ?10,
pages 79?87. Association for Computational Linguis-
tics.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy - Volume 1, pages 71?78.
S.-H. Lin, Y.-M. Yeh, and B. Chen. 2010. Leveraging
kullback-leibler divergence measures and information-
rich cues for speech summarization.
Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu. 2009.
Unsupervised approaches for automatic keyword ex-
traction using meeting transcripts. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, NAACL ?09,
pages 620?628, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Fei Liu, Feifan Liu, and Yang Liu. 2011. A supervised
framework for keyword extraction from meeting tran-
scripts. IEEE Transactions on Audio, Speech & Lan-
guage Processing, 19(3):538?548.
Bin Lu, Myle Ott, Claire Cardie, and Benjamin Tsou.
2011. Multi-aspect sentiment analysis with topic mod-
els. In Workshop on Sentiment Elicitation from Natu-
ral Text for Information Retrieval and Extraction.
Sameer Maskey and Julia Hirschberg. 2005. Comparing
Lexical, Acoustic/Prosodic, Structural and Discourse
Features for Speech Summarization. In Proc. Euro-
pean Conference on Speech Communication and Tech-
nology (Eurospeech).
Gabriel Murray and Steve Renals. 2007. Towards on-
line speech summarization. In INTERSPEECH, pages
2785?2788.
Gabriel Murray, Steve Renals, and Jean Carletta. 2005.
Extractive summarization of meeting recordings. In
in Proceedings of the 9th European Conference on
Speech Communication and Technology, pages 593?
596.
Gabriel Murray, Giuseppe Carenini, and Raymond Ng.
2010a. Interpretation and transformation for abstract-
ing conversations. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, HLT ?10, pages 894?902, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Gabriel Murray, Giuseppe Carenini, and Raymond T. Ng.
2010b. Generating and validating abstracts of meeting
conversations: a user study. In INLG?10.
Korbinian Riedhammer, Benoit Favre, and Dilek
Hakkani-Tu?r. 2010. Long story short - global unsu-
pervised models for keyphrase based meeting summa-
rization. Speech Commun., 52(10):801?815, October.
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceed-
ing of the 17th international conference on World Wide
Web, WWW ?08, pages 111?120. ACM.
Lu Wang and Claire Cardie. 2011. Summarizing deci-
sions in spoken meetings. In Proceedings of the Work-
shop on Automatic Summarization for Different Gen-
res, Media, and Languages, pages 16?24, Portland,
Oregon, June. Association for Computational Linguis-
tics.
Shasha Xie and Yang Liu. 2010. Using confusion net-
works for speech summarization. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, HLT ?10, pages 46?54. Associ-
ation for Computational Linguistics.
Shasha Xie, Yang Liu, and Hui Lin. 2008. Evaluating
the effectiveness of features and sampling in extrac-
tive meeting summarization. In Proc. of IEEE Spoken
Language Technology (SLT).
Klaus Zechner. 2002. Automatic summarization of
open-domain multiparty dialogues in diverse genres.
Comput. Linguist., 28:447?485, December.
49
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 304?313,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Focused Meeting Summarization via Unsupervised Relation Extraction
Lu Wang
Department of Computer Science
Cornell University
Ithaca, NY 14853
luwang@cs.cornell.edu
Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
cardie@cs.cornell.edu
Abstract
We present a novel unsupervised framework
for focused meeting summarization that views
the problem as an instance of relation extrac-
tion. We adapt an existing in-domain rela-
tion learner (Chen et al, 2011) by exploit-
ing a set of task-specific constraints and fea-
tures. We evaluate the approach on a decision
summarization task and show that it outper-
forms unsupervised utterance-level extractive
summarization baselines as well as an exist-
ing generic relation-extraction-based summa-
rization method. Moreover, our approach pro-
duces summaries competitive with those gen-
erated by supervised methods in terms of the
standard ROUGE score.
1 Introduction
For better or worse, meetings play an integral role
in most of our daily lives ? they let us share infor-
mation and collaborate with others to solve a prob-
lem, to generate ideas, and to weigh options. Not
surprisingly then, there is growing interest in devel-
oping automatic methods for meeting summariza-
tion (e.g., Zechner (2002), Maskey and Hirschberg
(2005), Galley (2006), Lin and Chen (2010), Mur-
ray et al (2010a)). This paper tackles the task of fo-
cused meeting summarization , i.e., generating sum-
maries of a particular aspect of a meeting rather than
of the meeting as a whole (Carenini et al, 2011).
For example, one might want a summary of just the
DECISIONS made during the meeting, the ACTION
ITEMS that emerged, the IDEAS discussed, or the
HYPOTHESES put forth, etc.
Consider, for example, the task of summarizing
the decisions in the dialogue snippet in Figure 1. The
figure shows only the decision-related dialogue acts
(DRDAs) ? utterances associated with one or more
decisions.1 Each DRDA is labeled numerically ac-
cording to the decision it supports; so the first two
utterances support DECISION 1 as do the final two
utterances in the snippet. Manually constructed de-
cision abstracts for each decision are shown at the
bottom of the figure.2 These constitute the decision-
focused summary for the snippet.
Notice that many portions of the DRDAs are not
relevant to the decision itself: they often begin with
phrases that identify the utterance within the dis-
course as potentially introducing a decision (e.g.,
?Maybe that could be?, ?It seems like you?re gonna
have?), but do not themselves describe the decision.
We will refer to this portion of a DRDA (underlined
in Figure 1) as the Decision Cue.
Moreover, the decision cue is generally directly
followed by the actual Decision Content (e.g., ?be a
little apple?, ?have rubber cases?). Decision Content
phrases are denoted in Figure 1 via italics and square
brackets. Importantly, it is just the decision content
portion of the utterance that should be considered for
incorporation into the focused summary.
1These are similar, but not completely equivalent, to the de-
cision dialogue acts (DDAs) of (Bui et al, 2009), (Ferna?ndez et
al., 2008), (Frampton et al, 2009).
2Murray et al (2010b) show that users much prefer abstrac-
tive summaries over extracts when the text to be summarized
is a conversation. In particular, extractive summaries drawn
from group conversations can be confusing to the reader with-
out additional context; and the noisy, error-prone, disfluent text
of speech transcripts is likely to result in extractive summaries
with low readability.
304
C: Say the standby button is quite kinda separate from all the
other functions. (1)
C: Maybe that could be [a little apple]. (1)
C: It seems like you?re gonna have [rubber cases], as well as
[buttons]. (2)
A: [Rubber buttons] require [rubber case]. (2)
A: You could have [your company badge] and [logo]. (3)
A: I mean a lot of um computers for instance like like on the one
you?ve got there, it actually has a sort of um [stick on badge]. (3)
C: Shall we go [for single curve], just to compromise? (2)
B: We?ll go [for single curve], yeah. (2)
C: And the rubber push buttons, rubber case. (2)
D: And then are we going for sort of [one button] shaped
[like a fruit]. <vocalsound> Or veg. (1)
D: Could be [a red apple], yeah. (1)
Decision Abstracts (Summary)
DECISION 1: The group decided to make the standby button
in the shape of an apple.
DECISION 2: The remote will also feature a rubber case and
rubber buttons, and a single-curved design.
DECISION 3: The remote will feature the company logo,
possibly in a sticker form.
Figure 1: Clip from the AMI meeting corpus (Carletta et al,
2005). A, B, C and D refer to distinct speakers; the numbers
in parentheses indicate the associated meeting decision: DECI-
SION 1, 2 or 3. Also shown is the gold-standard (manual) ab-
stract (summary) for each decision. Colors indicate overlapping
vocabulary between utterances and the summary. Underlining,
italics, and [bracketing] are decscribed in the running text.
This paper presents an unsupervised framework
for focused meeting summarization that supports the
generation of abstractive summaries. (Note that we
do not currently generate actual abstracts, but rather
aim to identify those Content phrases that should
comprise the abstract.) In contrast to existing ap-
proaches to focused meeting summarization (e.g.,
Purver et al (2007), Ferna?ndez et al (2008), Bui et
al. (2009)), we view the problem as an information
extraction task and hypothesize that existing meth-
ods for domain-specific relation extraction can be
modified to identify salient phrases for use in gener-
ating abstractive summaries.
Very generally, information extraction methods
identify a lexical ?trigger? or ?indicator? that evokes
a relation of interest and then employ syntactic in-
formation, often in conjunction with semantic con-
straints, to find the ?target phrase? or ?argument
constituent? to be extracted. Relation instances,
then, are represented by indicator-argument pairs
(Chen et al, 2011).
Figure 1 shows some possible indicator-argument
pairs for identifying the Decision Content phrases
in the dialogue sample. Content indicator words
are shown in italics; the Decision Content target
phrases are the arguments. For example, in the
fourth DRDA, ?require? is the indicator, and ?rub-
ber buttons? and ?rubber case? are both arguments.
Although not shown in Figure 1, it is also possible
to identify relations that correspond to the Decision
Cue phrases.3
Specifically, we focus on the task of decision sum-
marization and, as in previous work in meeting sum-
marization (e.g., Ferna?ndez et al (2008), Wang and
Cardie (2011)), assume that all decision-related ut-
terances (DRDAs) have been identified. We adapt
the unsupervised relation learning approach of Chen
et al (2011) to separately identify relations asso-
ciated with decision cues vs. the decision content
within DRDAs by defining a new set of task-specific
constraints and features to take the place of the
domain-specific constraints and features of the orig-
inal model. Output of the system is a set of extracted
indicator-argument decision content relations (see
the ?OUR METHOD? sample summary of Table 6)
that can be used as the basis of the decision abstract.
We evaluate the approach (using the AMI cor-
pus (Carletta et al, 2005)) under two input set-
tings ? in the True Clusterings setting, we assume
that the DRDAs for each meeting have been per-
fectly grouped according to the decision(s) each sup-
ports; in the System Clusterings setting, an auto-
mated system performs the DRDA-decision pairing.
The results show that the relation-based summariza-
tion approach outperforms two extractive summa-
rization baselines that select the longest and the most
representative utterance for each decision, respec-
tively. (ROUGE-1 F score of 37.47% vs. 32.61%
and 33.32% for the baselines given the True Cluster-
ings of DRDAs.) Moreover, our approach performs
admirably in comparison to two supervised learning
alternatives (scores of 35.61% and 40.87%) that aim
to identify the important tokens to include in the de-
cision abstract given the DRDA clusterings. In con-
trast to our approach which is transferable to differ-
ent domains or tasks, these methods would require
labeled data for retraining for each new meeting cor-
pus.
3Consider, for example, the phrases underlined in the sixth
and seventh DRDAs. ?I mean? and ?shall we? are two typical
Decision Cue phrases where ?mean? and ?shall? are possible
indicators with ?I? and ?we? as their arguments, respectively.
305
Finally, in order to compare our approach to an-
other relation-based summarization technique, we
modify the multi-document summarization system
of Hachey (2009) to the single-document meeting
scenario. Here again, our proposed approach per-
forms better (37.47% vs. 34.69%). Experiments un-
der the System Clusterings setting produce the same
overall results, albeit with lower scores for all of the
systems and baselines.
In the remainder of the paper, we review related
work in Section 2 and give a high-level description
of the relation-based approach to focused summa-
rization in Section 3. Sections 4, 5 and 6 present the
modifications to the Chen et al (2011) relation ex-
traction model required for its instantiation for the
meeting summarization task. Sections 7 and 8 pro-
vide our experimental setup and results.
2 Related Work
Most research on spoken dialogue summariza-
tion attempts to generate summaries for full dia-
logues (Carenini et al, 2011). Only recently, how-
ever, has the task of focused summarization, and de-
cision summarization, in particular, been addressed.
Ferna?ndez et al (2008) and Bui et al (2009) em-
ploy supervised learning methods to rank phrases
or words for inclusion in the decision summary.
In comparison, Ferna?ndez et al (2008) find that
the phrase-based approach yields better recall than
token-based methods, concluding that phrases have
the potential to support better summaries. Input to
their system, however, is narrowed down (manually)
from the full set of DRDAs to the subset that is use-
ful for summarization. In addition, they evaluate
their system w.r.t. informative phrases or words that
have been manually annotated within this DRDA
subset. We are instead interested in comparing our
extracted relations to the abstractive summaries.
In contrast to our phrase-based approach, we pre-
viously explored a collection of supervised and un-
supervised learning methods for utterance-level (i.e.,
dialogue act) and token-level decision summariza-
tion (Wang and Cardie, 2011). We adopt here the
two unsupervised baselines (utterance-level sum-
maries) from that work for use in our evaluation.
We further employ their supervised summarization
methods as comparison points for token-level sum-
marization, adding additional features for consis-
tency with the other approaches in the evaluation.
Murray et al (2010a) develop an integer linear pro-
gramming approach for focused summarization at
the utterance-level, selecting sentences that cover
more of the entities mentioned in the meeting as de-
termined through the use of an external ontology.
The most relevant previous work is Hachey
(2009), which uses relational representations to fa-
cilitate sentence-ranking for multi-document sum-
marization. The method utilizes generic relation ex-
traction to represent the concepts in the documents
as relation instances; summaries are generated based
on a set cover algorithm that selects a subset of
the sentences that best cover the weighted concepts.
Thus, the goal of Hachey?s approach is sentence ex-
traction rather than phrase extraction. Although his
relation extraction method, like ours (see Section
4), is probabilistic and unsupervised (he uses Latent
Dirichelt Allocation (Blei et al, 2003)), the relations
are limited to pairs of named-entities, which is not
appropriate for our decision summarization setting.
Nevertheless, we will adapt his approach for com-
parison with our relation-based summarization tech-
nique and include it for evaluation.
3 Focused Summarization as Relation Ex-
traction
Given the DRDAs for each meeting grouped (not
necessarily correctly) according to the decisions
they support, we put each cluster of DRDAs (or-
dered according to time within the cluster) into one
?decision document?. The goal will be to pro-
duce one decision abstract for each such decision
document. We obtain constituent and dependency
parses using the Stanford parser (Klein and Man-
ning, 2003; de Marneffe et al, 2006). With the cor-
pus of constituent-parsed decision documents as the
input, we will use and modify Chen et al (2011)?s
system to identify decision cue relations and deci-
sion content relations for each cluster.4 (Section 6
will make clear how the learned decision cue rela-
tions will be used to identify decision content re-
lations.) The salient decision content relation in-
stances will be returned as decision summary com-
4Other unsupervised relation learning methods might also
be appropriate (e.g., Open IE (Banko et al, 2007)), but they
generally model relations between pairs of entities and group
relations only according to lexical similarity.
306
ponents.
Designed for in-domain relation discovery from
standard written texts (e.g., newswire), however, the
Chen et al (2011) system cannot be applied to our
task directly. In our setting, for example, neither the
number of relations nor the relation types is known
in advance.
In the following sections, we describe the modi-
fications needed for the spoken meeting genre and
decision-focused summarization task. In particular,
Chen et al (2011) provide two mechanisms that al-
low for this type of tailoring: the feature set used to
cluster potential relation instances into groups/types,
and a set of global constraints that characterize the
general qualities (e.g., syntactic form, prevalence,
discourse behavior) of a good relation for the task.
4 Model
In this section, we describe the Chen et al (2011)
probabilistic relation learning model used for both
Decision Cue and Decision Content relation extrac-
tion. The parameter estimation and constraint en-
coding through posterior inference are presented in
Section 5.
The relation learning model takes as input clus-
ters of DRDAs, sorted according to utterance time
and concatenated into one decision document. We
assume one decision will be made per document.
The goal for the model is to explain how the de-
cision documents are generated from the latent re-
lation variables. The posterior regularization tech-
nique (Section 5) biases inference to adhere to the
declarative constraints on relation instances. In gen-
eral, instead of extracting relation instances strictly
satisfying a set of human-written rules, features and
constraints are designed to allow the model to reveal
diverse relation types and to ensure that the identi-
fied relation instances are coherent and meaningful.
For each decision document, we select the relation
instance with highest probability for each relation
type and concatenate them to form the decision sum-
mary.
We restrict the eligible indicators to be a noun or
verb, and eligible arguments to be a noun phrase
(NP), prepositional phrase (PP) or clause introduced
by ?to? (S). Given a pre-specified number of relation
types K, the model employs a set of features ?i(w)
and ?a(x) (see Section 6) to describe the indicator
?0?k,? |?i|i i ?k,?bi i ?k,? |?a|a a ?k,?ba a ?k K
?0
?i(w)|?i| W ?a(x)|?a|X
sd,k
zd,kad,kid,k K D
Figure 2: Graphical model representation for the relation
learning model. D is the number of decision documents (each
decision document consists of a cluster of DRDAs). K is the
number of relation types. W and X represent the number of in-
dicators and arguments in the decision document. |?i| and |?a|
are the number of features for indicator and argument.
word w and argument constituent x. Each relation
type k is associated with a set of feature distributions
?k and a location distribution ?k. ?k include four pa-
rameter vectors: ?ik for indicator words, ?bik for non-
indicator words, ?ak for argument constituents, and
?bak for non-argument constituents. Each decision
document is divided into L equal-length segments
and the location parameter vector ?k describes the
probability of relation k arising from each segment.
The plate diagram for the model is shown in Fig-
ure 2. The generative process and likelihood of the
model are shown in Appendix A.
5 Parameter Estimation and Inference via
Posterior Regularization
In order to specify global preferences for the rela-
tion instances (e.g. the syntactic structure of the ex-
pressions), we impose inequality constraints on ex-
pectations of the posterior distributions during infer-
ence (Graca et al, 2008).
5.1 Variational inference with Constraints
Suppose we are interested in estimating the posterior
distribution p(?, z|x) of a model in general, where
?, z and x are parameters to estimate, latent vari-
ables and observations, respectively. We aim to find
a distribution q(?, z) ? Q that minimizes the KL-
divergence to the true posterior
KL(q(?, z)?p(?, z|x)) (1)
307
A mean-field assumption is made for variational
inference, where q(?, z) = q(?)q(z). Then we can
minimize Equation 1 by performing coordinate de-
scent on q(?) and q(z). Now we intend to have fine-
level control on the posteriors to induce meaningful
semantic parts. For instance, we would like most of
the extracted relation instances to satisfy a set of pre-
defined syntactic patterns. As presented in (Graca et
al., 2008), a general way to put constraints on pos-
terior q is through bounding expectations of given
functions: Eq[f(z)] ? b, where f(z) is a determin-
istic function of z, and b is a pre-specified threshold.
For instance, define f(z) as a function to count the
number of generated relation instances that meet the
pre-defined syntactic patterns, then most of the ex-
tracted relation instances will have the desired syn-
tactic structures.
By using the mean-field assumption, the model in
Section 4 is factorized as
q(?, ?, z, i, a) =
K?
k=1
q(?k; ??k)q(?ik; ??ik)q(?bik ; ??bik )q(?ak ??ak)q(?bak ; ??bak )
?
D?
d=1
q(zd,k, id,k, ad,k; c?d,k) (2)
The constraints are encoded in the inequalities
Eq[f(z, i, a)] ? b or Eq[f(z, i, a)] ? b, and affect
the inference as described above. Updates for the
parameters are discussed in Appendix B.
5.2 Task-Specific Constraints.
We define four types of constraints for the decision
relation extraction model.
Syntactic Constraints. Syntactic constraints are
widely used for information extraction (IE) systems
(Snow et al, 2005; Banko and Etzioni, 2008), as it
has been shown that most relations are expressed via
a small number of common syntactic patterns. For
each relation type, we require at least 80%5 of the
induced relation instances in expectation to match
one of the following syntactic patterns:
? The indicator is a verb and the argument is a noun
phrase. The headword of the argument is the direct
object of the indicator or the nominal subject of the
indicator.
5Experiments show that this threshold is suitable for deci-
sion relation extraction, so we adopt it from (Chen et al, 2011).
? The indicator is a verb and the argument is a prepo-
sitional phrase or a clause starting with ?to?. The
indicator and the argument have the same parent in
the constituent parsing tree.
? The indicator is a noun and is the headword of a
noun phrase, and the argument is a prepositional
phrase. The noun phrase with the indicator as its
headword and the argument have the same parent in
the constituent parsing tree.
For relation k, let f(zk, ik, ak) count the number
of induced indicator ik and argument ak pairs that
match one of the patterns above, and b is set to 0.8D,
whereD is the number of decision documents. Then
the syntactic constraint is encoded in the inequality
Eq[f(zk, ik, ak)] ? b.
Prevalence Constraints. The prevalence con-
straint is enforced on the number of times a relation
is instantiated, in order to guarantee that every rela-
tion has enough instantiations across the corpus and
is task-relevant. Again, we require each relation to
have induced instances in at least 80% of decision
documents.
Occurrence Constraints. Diversity of relation
types is enforced through occurrence constraints. In
particular, for each decision document, we restrict
each word to trigger at most two relation types as in-
dicator and occur at most twice as part of a relation?s
argument in expectation. An entire span of argument
constituent can appear in at most one relation type.
Discourse Constraints. The discourse constraint
captures the insight that the final decision on an is-
sue is generally made, or at least restated, at the end
of the decision-related discussion. As each decision
document is divided into four equal parts, we re-
strict 50% of the relation instances to be from the
last quarter of the decision documents.
6 Features
Table 1 lists the features we use for discovering
both the decision cue relations and decision con-
tent relations. We start with a collection of domain-
independent BASIC FEATURES shown to be use-
ful in relation extraction (Banko and Etzioni, 2008;
Chen et al, 2011). Then we add MEETING FEA-
TURES, STRUCTURAL FEATURES and SEMANTIC
FEATURES that have been found to be good pre-
dictors for decision detection (Hsueh and Moore,
2007) or meeting and decision summarization (Gal-
308
Basic Features
unigram (stemmed)
part-of-speech (POS)
constituent label (NP, VP, S/SBAR (start with ?to?))
dependency label
Meeting Features
Dialogue Act (DA) type
speaker role
topic
Structural Features (Galley, 2006) (Wang and Cardie, 2011)
in an Adjacency Pair (AP)?
if in an AP, AP type
if in an AP, the other part is decision-related?
if in an AP, the source part or target part?
if in an AP and is source part, is the target positive feedback?
if in an AP and is target part, is the source a question?
Semantic Features (from WordNet) (Miller, 1995)
first Synset of head word with the given POS
first hypernym path for the first synset of head word
Other Features (only for Argument)
number of words (without stopwords)
has capitalized word or not
has proper noun or not
Table 1: Features for Decision Cue and Decision Content re-
lation extraction. All features, except the last type of features,
are used for both the indicator and argument. (An Adjacency
Pair (AP) is an important conversational analysis concept (Schegloff
and Sacks, 1973). In the AMI corpus, an AP pair consists of a source
utterance and a target utterance, produced by different speakers.)
ley, 2006; Murray and Carenini, 2008; Ferna?ndez et
al., 2008; Wang and Cardie, 2011). Features em-
ployed only for argument?s are listed in the last cat-
egory in Table 1.
After applying the features in Table 1 and the
global constraints from Section 5 in preliminary ex-
periments, we found that the extracted relation in-
stances are mostly derived from decision cue rela-
tions. Sample decision cue relations and instances
are displayed in Table 2 and are not necessarily sur-
prising: previous research (Hsueh and Moore, 2007)
has observed the important role of personal pro-
nouns, such as ?we? and ?I?, in decision-making ex-
pressions. Notably, the decision cue is always fol-
lowed by the decision content. As a result, we in-
clude two additional features (see Table 3) that rely
on the cues to identify the decision content. Finally,
we disallow content relation instances with an argu-
ment containing just a personal pronoun.
7 Experiment Setup
The Corpus. We evaluate our approach on the
AMI meeting corpus (Carletta et al, 2005) that con-
sists of 140 multi-party meetings with a wide range
Decision Cue Relations Relation Instances
Group Wrap-up / Recap we have, we are, we say, we want
Personal Explanation I mean, I think, I guess, I (would) say
Suggestion do we, we (could/should) do
Final Decision it is (gonna), it will, we will
Table 2: Sample Decision Cue relation instances. The words
in parentheses are filled for illustration purposes, while they are
not part of the relation instances.
Discourse Features
clause position (first, second, other)
position to the first decision cue relation if any (before, after)
Table 3: Additional features for Decision Content relation ex-
traction, inspired by Decision Cue relations. Both indicator and
argument use those features.
of annotations. The 129 scenario-driven meetings
involve four participants playing different roles on
a design team. Importantly, the corpus includes a
short (usually one-sentence), manually constructed
abstract summarizing each decision discussed in the
meeting. In addition, all of the dialogue acts that
support (i.e., are relevant to) each decision are an-
notated as such. We use the manually constructed
decision abstracts as gold-standard summaries.
System Inputs. We consider two system input set-
tings. In the True Clusterings setting, we use
the AMI annotations to create perfect partitionings
of the DRDAs for input to the summarization sys-
tem; in the System Clusterings setting, we em-
ploy a hierarchical agglomerative clustering algo-
rithm used for this task in previous work (Wang and
Cardie, 2011). The Wang and Cardie (2011) cluster-
ing method groups DRDAs according to their LDA
topic distribution similarity. As better approaches
for DRDA clustering become available, they could
be employed instead.
Evaluation Metrics. We use the widely accepted
ROUGE (Lin and Hovy, 2003) evaluation measure.
We adopt the ROUGE-1 and ROUGE-SU4 met-
rics from (Hachey, 2009), and also use ROUGE-
2. We choose the stemming option of the ROUGE
software at http://berouge.com/ and remove
stopwords from both the system and gold-standard
summaries.
Training and Parameters. The Dirichlet hyper-
parameters are set to 0.1 for the priors. When train-
ing the model, ten random restarts are performed
and each run stops when reaching a convergence
threshold (10?5). Then we select the posterior with
309
the lowest final free energy. For the parameters
used in posterior constraints, we either adopt them
from (Chen et al, 2011) or choose them arbitrarily
without tuning in the spirit of making the approach
domain-independent.
We compare our decision summarization ap-
proach with (1) two unsupervised baselines, (2)
the unsupervised relation-based approach of Hachey
(2009), (3) two supervised methods, and (4) an up-
perbound derived from the gold standard decision
abstracts.
The LONGEST DA Baseline. As in Riedhammer
et al (2010) and Wang and Cardie (2011), this base-
line simply selects the longest DRDA in each clus-
ter as the summary. Thus, this baseline performs
utterance-level decision summarization. Although
it?s possible that decision content is spread over mul-
tiple DRDAs in the cluster, this baseline and the next
allow us to determine summary quality when sum-
maries are restricted to a single utterance.
The PROTOTYPE DA Baseline. Following Wang
and Cardie (2011), the second baseline selects the
decision cluster prototype (i.e., the DRDA with the
largest TF-IDF similarity with the cluster centroid)
as the summary.
The Generic Relation Extraction (GRE) Method
of Hachey (2009). Hachey (2009) presents
a generic relation extraction (GRE) for multi-
document summarization. Informative sentences
are extracted to form summaries instead of relation
instances. Relation types are discovered by Latent
Dirichlet Allocation, such that a probability is
output for each relation instance given a topic
(equivalent to relation). Their relation instances are
named entity(NE)-mention pairs conforming to a
set of pre-specified rules. For comparison, we use
these same rules to select noun-mention pairs rather
than NE-mention pairs, which is better suited to
meetings, which do not contain many NEs.6
6Because an approximate set cover algorithm is used in
GRE, one decision-related dialogue act (DRDA) is extracted
each time until the summary reaches the desired length. We run
two sets of experiments using this GRE system with different
output summaries ? one selects one entire DRDA as the final
summary (as Hachey (2009) does), and another one outputs the
relation instances with highest probability conditional on each
relation type. We find that the first set of experiments gets better
True Clusterings
R-1 R-2 R-SU4
PREC REC F1 F1 F1
Baselines
Longest DA 34.06 31.28 32.61 12.03 13.58
Prototype DA 40.72 28.21 33.32 12.18 13.46
GRE
5 topics 38.51 30.66 34.13 11.44 13.54
10 topics 39.39 31.01 34.69 11.28 13.42
15 topics 38.00 29.83 33.41 11.40 12.80
20 topics 37.24 30.13 33.30 10.89 12.95
Supervised Methods
CRF 53.95 26.57 35.61 11.52 14.07
SVM 42.30 41.49 40.87 12.91 16.29
Our Method
5 Relations 39.33 35.12 37.10 12.05 14.29
10 Relations 37.94 37.03 37.47 12.20 14.59
15 Relations 37.36 37.43 37.39 11.47 14.00
20 Relations 37.27 37.64 37.45 11.40 13.90
Upperbound 100.00 45.05 62.12 33.27 34.89
Table 4: ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-
SU4 (R-SU4) scores for summaries produced by the baselines,
GRE (Hachey, 2009)?s best results, the supervised methods, our
method and an upperbound ? all with perfect/true DRDA clus-
terings.
Supervised Learning (SVMs and CRFs). We
also compare our approach to two supervised learn-
ing methods ? Support Vector Machines (Joachims,
1998) with RBF kernel and order-1 Conditional
Random Fields (Lafferty et al, 2001) ? trained us-
ing the same features as our system (see Tables 1
and 3) to identify the important tokens to include in
the decision abstract. Three-fold cross validation is
conducted for both methods.
Upperbound. We also compute an upperbound
that reflects the gap between the best possible ex-
tractive summaries and the human-written abstracts
according to the ROUGE score: for each cluster of
DRDAs, we select the words that also appear in the
associated decision abstract.
8 Results and Discussion
Table 4 illustrates that, using True (DRDA) Clus-
terings our method outperforms the two baselines
and the generic relation extraction (GRE) based sys-
tem in terms of F score in ROUGE-1 and ROUGE-
SU4 with varied numbers of relations. Note that for
GRE based approach, we only list out their best re-
sults for utterance-level summarization. If using the
salient relation instances identified by GRE as the
summaries, the ROUGE results will be significantly
performance than the second, so we only report the best results
for their system in this paper.
310
System Clusterings
R-1 R-2 R-SU4
PREC REC F1 F1 F1
Baselines
Longest DA 17.06 11.64 13.84 2.76 3.34
Prototype DA 18.14 10.11 12.98 2.84 3.09
GRE
5 topics 17.10 9.76 12.40 3.03 3.41
10 topics 16.28 10.03 12.35 3.00 3.36
15 topics 16.54 10.90 13.04 2.84 3.28
20 topics 17.25 8.99 11.80 2.90 3.23
Supervised Methods
CRF 47.36 15.34 23.18 6.12 9.21
SVM 39.50 18.49 25.19 6.15 9.86
Our Method
5 Relations 16.12 18.93 17.41 3.31 5.56
10 Relations 16.27 18.93 17.50 3.32 5.69
15 Relations 16.42 19.14 17.68 3.47 5.75
20 Relations 16.75 18.25 17.47 3.33 5.64
Table 5: ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-
SU4 (R-SU4) scores for summaries produced by the baselines,
GRE (Hachey, 2009)?s best results, the supervised methods and
our method ? all with system clusterings.
lower. When measured by ROUGE-2, our method
still have better or comparable performances than
other unsupervised methods. Moreover, our sys-
tem achieves F scores in between those of the su-
pervised learning methods, performing better than
the CRF in both recall and F score. The recall score
for the upperbound in ROUGE-1, on the other hand,
indicates that there is still a wide gap between the
extractive summaries and human-written abstracts:
without additional lexical information (e.g., seman-
tic class information, ontologies) or a real language
generation component, recall appears to be a bottle-
neck for extractive summarization methods that se-
lect content only from decision-related dialogue acts
(DRDAs).
Results using the System Clusterings (Table 5)
are comparable, although all of the system and base-
line scores are much lower. Supervised methods get
the best F scores largely due to their high precision;
but our method attains the best recall in ROUGE-1.
Discussion. To better exemplify the summaries
generated by different systems, sample output for
each method is shown in Table 6. The GRE system
uses an approximate algorithm for set cover extrac-
tion, we list the first three selected DRDA in order.
We see from the table that utterance-level extractive
summaries (Longest DA, Prototype DA, GRE) make
more coherent but still far from concise and compact
DRDA (1): Uh the batteries, uh we also thought about that already,
DRDA (2): uh will be chargeable with uh uh an option for a
mount station
DRDA (3): Maybe it?s better to to include rechargeable batteries
DRDA (4): We already decided that on the previous meeting.
DRDA (5): which you can recharge through the docking station.
DRDA (6): normal plain batteries you can buy at the supermarket
or retail shop. Yeah.
Decision Abstract: The remote will use rechargeable batteries
which recharge in a docking station.
Longest DA & Prototype DA: normal plain batteries you can
buy at the supermarket or retail shop. Yeah.
GRE: 1st: normal plain batteries you can buy at the supermarket
or retail shop. Yeah.
2nd: which you can recharge through the docking station.
3rd: uh will be chargeable with uh uh an option for a mount station
SVM: batteries include rechargeable batteries decided recharge
docking station
CRF: chargeable station rechargeable batteries
Our Method: <option, for a mount station>,
<include, rechargeable batteries>,
<decided, that on the previous meeting>,
<recharge, through the docking station>,
<buy, normal plain batteries>
Table 6: Sample system outputs by different methods are in
the third cell (methods? names are in bold). First cell contains
the six DRDAs supporting the decision abstracted in the second
cell.
abstracts. On the other hand, the supervised methods
(SVM, CRF) that produce token-level extracts better
identify the overall content of the decision abstract.
Unfortunately, they require human annotation in the
training phase; in addition, the output is ungrammat-
ical and lacks coherence. In comparison, our sys-
tem presents the decision summary in the form of
phrase-based relations that provide a relatively com-
prehensive expression.
9 Conclusions
We present a novel framework for focused meet-
ing summarization based on unsupervised relation
extraction. Our approach is shown to outperform
unsupervised utterance-level extractive summariza-
tion baselines as well as an existing generic relation-
extraction-based summarization method. Our ap-
proach also produces summaries competitive with
those generated by supervised methods in terms of
the standard ROUGE score. Overall, we find that
relation-based methods for focused summarization
have potential as a technique for supporting the gen-
eration of abstractive decision summaries.
Acknowledgments This work was supported in part by
National Science Foundation Grants IIS-0968450 and
IIS-1111176, and by a gift from Google.
311
References
Michele Banko and Oren Etzioni. 2008. The Tradeoffs
Between Open and Traditional Relation Extraction. In
Proceedings of ACL-08: HLT, Columbus, Ohio.
Michele Banko, Michael J Cafarella, Stephen Soderl,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In In IJCAI, pages
2670?2676.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Trung H. Bui, Matthew Frampton, John Dowding, and
Stanley Peters. 2009. Extracting decisions from
multi-party dialogue using directed graphical models
and semantic similarity. In Proceedings of the SIG-
DIAL 2009 Conference, pages 235?243.
Giuseppe Carenini, Gabriel Murray, and Raymond Ng.
2011. Methods for Mining and Summarizing Text Con-
versations. Morgan & Claypool Publishers.
Jean Carletta, Simone Ashby, Sebastien Bourban,
Mike Flynn, Thomas Hain, Jaroslav Kadlec, Vasilis
Karaiskos, Wessel Kraaij, Melissa Kronenthal, Guil-
laume Lathoud, Mike Lincoln, Agnes Lisowska, and
Mccowan Wilfried Post Dennis Reidsma. 2005. The
ami meeting corpus: A pre-announcement. In In Proc.
MLMI, pages 28?39.
Harr Chen, Edward Benson, Tahira Naseem, and Regina
Barzilay. 2011. In-domain relation discovery with
meta-constraints via posterior regularization. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ?11, pages 530?540,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure trees. In LREC.
Raquel Ferna?ndez, Matthew Frampton, John Dowding,
Anish Adukuzhiyil, Patrick Ehlen, and Stanley Peters.
2008. Identifying relevant phrases to summarize de-
cisions in spoken meetings. INTERSPEECH-2008,
pages 78?81.
Matthew Frampton, Jia Huang, Trung Huu Bui, and Stan-
ley Peters. 2009. Real-time decision detection in
multi-party dialogue. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 3 - Volume 3, pages 1133?1141.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 364?
372.
Joao Graca, Kuzman Ganchev, and Ben Taskar. 2008.
Expectation maximization and posterior constraints.
In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, edi-
tors, Advances in Neural Information Processing Sys-
tems 20, pages 569?576. MIT Press, Cambridge, MA.
Ben Hachey. 2009. Multi-document summarisation us-
ing generic relation extraction. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 1 - Volume 1, EMNLP
?09, pages 420?429, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Pei-yun Hsueh and Johanna Moore. 2007. What deci-
sions have you made: Automatic decision detection in
conversational speech. In In NAACL/HLT 2007.
Thorsten Joachims. 1998. Text categorization with Sup-
port Vector Machines: Learning with many relevant
features. In Claire Ne?dellec and Ce?line Rouveirol,
editors, Machine Learning: ECML-98, volume 1398,
chapter 19, pages 137?142. Berlin/Heidelberg.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, ACL ?03, pages 423?430, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Shih-Hsiang Lin and Berlin Chen. 2010. A risk mini-
mization framework for extractive speech summariza-
tion. In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, ACL ?10,
pages 79?87, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy - Volume 1, pages 71?78.
Sameer Maskey and Julia Hirschberg. 2005. Comparing
Lexical, Acoustic/Prosodic, Structural and Discourse
Features for Speech Summarization. In Proc. Euro-
pean Conference on Speech Communication and Tech-
nology (Eurospeech).
George A. Miller. 1995. Wordnet: a lexical database for
english. Commun. ACM, 38:39?41, November.
Gabriel Murray and Giuseppe Carenini. 2008. Summa-
rizing spoken and written conversations. In Proceed-
312
ings of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 773?782.
Gabriel Murray, Giuseppe Carenini, and Raymond Ng.
2010a. Interpretation and transformation for abstract-
ing conversations. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, HLT ?10, pages 894?902, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Gabriel Murray, Giuseppe Carenini, and Raymond T. Ng.
2010b. Generating and validating abstracts of meeting
conversations: a user study. In INLG?10.
Matthew Purver, John Dowding, John Niekrasz, Patrick
Ehlen, Sharareh Noorbaloochi, and Stanley Peters.
2007. Detecting and summarizing action items in
multi-party dialogue. In in Proceedings of the 8th SIG-
dial Workshop on Discourse and Dialogue.
Korbinian Riedhammer, Benoit Favre, and Dilek
Hakkani-Tu?r. 2010. Long story short - global unsu-
pervised models for keyphrase based meeting summa-
rization. Speech Commun., 52(10):801?815, October.
E. A. Schegloff and H. Sacks. 1973. Opening up clos-
ings. Semiotica, 8(4):289?327.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning Syntactic Patterns for Automatic Hypernym
Discovery. In Lawrence K. Saul, Yair Weiss, and
Le?on Bottou, editors, Advances in Neural Information
Processing Systems 17, pages 1297?1304. MIT Press,
Cambridge, MA.
Lu Wang and Claire Cardie. 2011. Summarizing deci-
sions in spoken meetings. In Proceedings of the Work-
shop on Automatic Summarization for Different Gen-
res, Media, and Languages, pages 16?24, Portland,
Oregon, June. Association for Computational Linguis-
tics.
Klaus Zechner. 2002. Automatic summarization of
open-domain multiparty dialogues in diverse genres.
Comput. Linguist., 28:447?485, December.
Appendix A Generative Process
The entire generative process is as follows (?Dir?
and ?Mult? refer to the Dirichlet distribution and
multinomial distribution):
1. For each relation type k:
(a) For each indicator feature ?i, draw feature distribu-
tions ?ik,?i , ?bik,?i ? Dir(?0)
(b) For each argument feature ?a, draw feature distri-
butions ?ak,?a , ?bak,?a ? Dir(?0)
(c) Draw location distribution ?k ? Dir(?0)
2. For each relation type k and decision document d:
(a) Select decision document segment sd,k ?
Mult(?k)
(b) Select DRDA zd,k uniformly from segment sd,k,
and indicator id,k and argument constituent ad,k
uniformly from DRDA zd,k
3. For each indicator word w in every decision document d:
(a) For each indicator feature ?i(w) ?
Mult( 1Z?Kk=1?k,?i), where ?k,?i is ?ik,?i if
id,k = w and ?bik,?i otherwise. Z is thenormalization factor.
4. For each argument constituent x in every decision docu-
ment d:
(a) For each indicator feature ?a(x) ?
Mult( 1Z?Kk=1?k,?a), where ?k,?a is ?ak,?a
if ad,k = x and ?bak,?a otherwise. Z is the
normalization factor.
Given ?0 and ?0, The joint distribution of a set of
feature parameters ?, the location distributions ?, a
set of DRDAs z, and the selected indicators i and
arguments a is:
P (?, ?, z, i, a; ?0, ?0) =
K?
k=1
P (?ik; ?0)P (?bik ; ?0)P (?ak |?0)P (?bak ; ?0)P (?k;?0)
? (
D?
d=1
P (id,k; zd,k)P (ad,k; zd,k)P (zd,k; sd,k)P (sd,k;?k)
? (P (w = id,k; ?ik)
?
w 6=id,k
P (w; ?bik ))
? (P (x = ad,k; ?ak)
?
x 6=ad,k
P (x; ?bak )))
Appendix B Updates for the Parameters
The constraints put on the posterior will only affect
the update for q(z). For q(?), the update is
q(?) = argmin
q(?)
KL(q(?)?q?(?)), (3)
where q?(?) ? expEq(z)[log p(?, z, x)], and q(?)
is updated to q?(?). For q(z), the update is
q(z) = argmin
q(z)
KL(q(z)?q?(z))
s.t. Eq(z)[fc(z)] ? bc, ?c ? C (4)
where q?(z) ? expEq(?)[log p(?, z, x)]. Equa-
tion 4 is easily solved via the dual (Graca et al,
2008) (Chen et al, 2011).
313
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 97?106,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
Improving Agreement and Disagreement Identification
in Online Discussions with A Socially-Tuned Sentiment Lexicon
Lu Wang
Department of Computer Science
Cornell University
Ithaca, NY 14853
luwang@cs.cornell.edu
Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
cardie@cs.cornell.edu
Abstract
We study the problem of agreement and
disagreement detection in online discus-
sions. An isotonic Conditional Random
Fields (isotonic CRF) based sequential
model is proposed to make predictions
on sentence- or segment-level. We auto-
matically construct a socially-tuned lex-
icon that is bootstrapped from existing
general-purpose sentiment lexicons to fur-
ther improve the performance. We eval-
uate our agreement and disagreement tag-
ging model on two disparate online discus-
sion corpora ? Wikipedia Talk pages and
online debates. Our model is shown to
outperform the state-of-the-art approaches
in both datasets. For example, the iso-
tonic CRF model achieves F1 scores of
0.74 and 0.67 for agreement and disagree-
ment detection, when a linear chain CRF
obtains 0.58 and 0.56 for the discussions
on Wikipedia Talk pages.
1 Introduction
We are in an era where people can easily voice and
exchange their opinions on the internet through
forums or social media. Mining public opinion
and the social interactions from online discus-
sions is an important task, which has a wide range
of applications. For example, by analyzing the
users? attitude in forum posts on social and po-
litical problems, it is able to identify ideological
stance (Somasundaran and Wiebe, 2009) and user
relations (Qiu et al., 2013), and thus further dis-
cover subgroups (Hassan et al., 2012; Abu-Jbara
et al., 2012) with similar ideological viewpoint.
Meanwhile, catching the sentiment in the conver-
sation can help detect online disputes, reveal popu-
lar or controversial topics, and potentially disclose
the public opinion formation process.
In this work, we study the problem of agreement
and disagreement identification in online discus-
sions. Sentence-level agreement and disagreement
detection for this domain is challenging in its own
right due to the dynamic nature of online conversa-
tions, and the less formal, and usually very emo-
tional language used. As an example, consider a
snippet of discussion from Wikipedia Talk page
for article ?Iraq War? where editors argue on the
correctness of the information in the opening para-
graph (Figure 1). ?So what?? should presumably
be tagged as a negative sentence as should the sen-
tence ?If you?re going to troll, do us all a favor
and stick to the guidelines.?. We hypothesize that
these, and other, examples will be difficult for the
tagger unless the context surrounding each sen-
tence is considered and in the absence of a sen-
timent lexicon tuned for conversational text (Ding
et al., 2008; Choi and Cardie, 2009).
As a result, we investigate isotonic Condi-
tional Random Fields (isotonic CRF) (Mao and
Lebanon, 2007) for the sentiment tagging task
since they preserve the advantages of the popu-
lar CRF sequential tagging models (Lafferty et
al., 2001) while providing an efficient mechanism
to encode domain knowledge ? in our case, a
sentiment lexicon ? through isotonic constraints
on the model parameters. In particular, we boot-
strap the construction of a sentiment lexicon from
Wikipedia talk pages using the lexical items in ex-
isting general-purpose sentiment lexicons as seeds
and in conjunction with an existing label propaga-
tion algorithm (Zhu and Ghahramani, 2002).
1
To summarize, our chief contributions include:
(1) We propose an agreement and disagree-
ment identification model based on isotonic Con-
ditional Random Fields (Mao and Lebanon, 2007)
to identify users? attitude in online discussion.
Our predictions that are made on the sentence-
1
Our online discussion lexicon (Section 4) will be made
publicly available.
97
Zer0faults: So questions comments feedback welcome.
Other views etc. I just hope we can remove the assertations
that WMD?s were in fact the sole reason for the US invasion,
considering that HJ Res 114 covers many many reasons.
>Mr. Tibbs: So basically what you want to do is remove all
mention of the cassus belli of the Iraq War and try to create
the false impression that this military action was as inevitable
as the sunrise.
[NN ]
No. Just because things didn?t turn out the
way the Bush administration wanted doesn?t give you license
to rewrite history.
[NN ]
...
>>MONGO: Regardless, the article is an antiwar propa-
ganda tool.
[NN ]
...
>>>Mr. Tibbs: So what?
[NN ]
That wasn?t the cassus
belli and trying to give that impression After the Fact is
Untrue.
[NN ]
Hell, the reason it wasn?t the cassus belli is be-
cause there are dictators in Africa that make Saddam look like
a pussycat...
>>Haizum: Start using the proper format or it?s over for your
comments.
[N ]
If you?re going to troll, do us all a favor and
stick to the guidelines.
[N ]
...
Tmorton166: Hi, I wonder if, as an outsider to this debate I
can put my word in here. I considered mediating this discus-
sion however I?d prefer just to comment and leave it at that :).
I agree mostly with what Zer0faults is saying
[PP ]
. ...
>Mr. Tibbs: Here?s the problem with that.
[NN ]
It?s not about
publicity or press coverage. It?s about the fact that the Iraq
disarmament crisis set off the 2003 Invasion of Iraq. ... And
theres a huge problem with rewriting the intro as if the Iraq
disarmament crisis never happened.
[NN ]
>>Tmorton166: ... To suggest in the opening paragraph that
the ONLY reason for the war was WMD?s is wrong - because
it simply isn?t.
[NN ]
However I agree that the emphasis needs
to be on the armaments crisis because it was the reason sold
to the public and the major one used to justify the invasion but
it needs to acknowledge that there was at least 12 reasons for
the war as well.
[PP ]
...
Figure 1: Example discussion from wikipedia talk page
for article ?Iraq War?, where editors discuss about the cor-
rectness of the information in the opening paragraph. We
only show some sentences that are relevant for demonstra-
tion. Other sentences are omitted by ellipsis. Names of ed-
itors are in bold. ?>? is an indicator for the reply structure,
where turns starting with > are response for most previous
turn that with one less >. We use ?NN?, ?N?, and ?PP? to in-
dicate ?strongly disagree?, ?disagree?, and ?strongly agree?.
Sentences in blue are examples whose sentiment is hard to
detect by an existing lexicon.
or segment-level, are able to discover fine-grained
sentiment flow within each turn, which can be fur-
ther applied in other applications, such as dispute
detection or argumentation structure analysis. We
employ two existing online discussion data sets:
the Authority and Alignment in Wikipedia Dis-
cussions (AAWD) corpus of Bender et al. (2011)
(Wikipedia talk pages) and the Internet Argu-
ment Corpus (IAC) of Walker et al. (2012a). Ex-
perimental results show that our model signifi-
cantly outperforms state-of-the-art methods on the
AAWD data (our F1 scores are 0.74 and 0.67 for
agreement and disagreement, vs. 0.58 and 0.56 for
the linear chain CRF approach) and IAC data (our
F1 scores are 0.61 and 0.78 for agreement and dis-
agreement, vs. 0.28 and 0.73 for SVM).
(2) Furthermore, we construct a new senti-
ment lexicon for online discussion. We show
that the learned lexicon significantly improves per-
formance over systems that use existing general-
purpose lexicons (i.e. MPQA lexicon (Wilson et
al., 2005), General Inquirer (Stone et al., 1966),
and SentiWordNet (Esuli and Sebastiani, 2006)).
Our lexicon is constructed from a very large-scale
discussion corpus based on Wikipedia talk page,
where previous work (Somasundaran and Wiebe,
2010) for constructing online discussion lexicon
relies on human annotations derived from limited
number of conversations.
In the remainder of the paper, we describe first
the related work (Section 2). Then we intro-
duce the sentence-level agreement and disagree-
ment identification model (Section 3) as well as
the label propagation algorithm for lexicon con-
struction (Section 4). After explain the experimen-
tal setup, we display the results and provide further
analysis in Section 6.
2 Related Work
Sentiment analysis has been utilized as a key en-
abling technique in a number of conversation-
based applications. Previous work mainly stud-
ies the attitudes in spoken meetings (Galley et al.,
2004; Hahn et al., 2006) or broadcast conversa-
tions (Wang et al., 2011) using Conditional Ran-
dom Fields (CRF) (Lafferty et al., 2001). Galley
et al. (2004) employ Conditional Markov models
to detect if discussants reach at an agreement in
spoken meetings. Each state in their model is an
individual turn and prediction is made on the turn-
level. In the same spirit, Wang et al. (2011) also
propose a sequential model based on CRF for de-
tecting agreements and disagreements in broadcast
conversations, where they primarily show the ef-
ficiency of prosodic features. While we also ex-
ploit a sequential model extended from CRFs, our
predictions are made for each sentence or segment
rather than at the turn-level. Moreover, we experi-
ment with online discussion datasets that exhibit
a more realistic distribution of disagreement vs.
agreement, where much more disagreement is ob-
served due to its function and the relation between
the participants. This renders the detection prob-
lem more challenging.
Only recently, agreement and disagreement de-
tection is studied for online discussion, especially
98
for online debate. Abbott et al. (2011) investi-
gate different types of features based on depen-
dency relations as well as manually-labeled fea-
tures, such as if the participants are nice, nasty,
or sarcastic, and respect or insult the target par-
ticipants. Automatically inducing those features
from human annotation are challenging itself, so
it would be difficult to reproduce their work on
new datasets. We use only automatically gener-
ated features. Using the same dataset, Misra and
Walker (2013) study the effectiveness of topic-
independent features, e.g. discourse cues indicat-
ing agreement or negative opinion. Those cues,
which serve a similar purpose as a sentiment lex-
icon, are also constructed manually. In our work,
we create an online discussion lexicon automat-
ically and construct sentiment features based on
the lexicon. Also targeting online debate, Yin et
al. (2012) train a logistic regression classifier with
features aggregating posts from the same partici-
pant to predict the sentiment for each individual
post. This approach works only when the speaker
has enough posts on each topic, which is not ap-
plicable to newcomers. Hassan et al. (2010) focus
on predicting the attitude of participants towards
each other. They relate the sentiment words to
the second person pronoun, which produces strong
baselines. We also adopt their baselines in our
work. Although there are available datasets with
(dis)agreement annotated on Wikipedia talk pages,
we are not aware of any published work that uti-
lizes these annotations. Dialogue act recognition
on talk pages (Ferschke et al., 2012) might be the
most related.
While detecting agreement and disagreement in
conversations is useful on its own, it is also a key
component for related tasks, such as stance pre-
diction (Thomas et al., 2006; Somasundaran and
Wiebe, 2009; Walker et al., 2012b) and subgroup
detection (Hassan et al., 2012; Abu-Jbara et al.,
2012). For instance, Thomas et al. (2006) train an
agreement detection classifier with Support Vec-
tor Machines on congressional floor-debate tran-
scripts to determine whether the speeches repre-
sent support of or opposition to the proposed leg-
islation. Somasundaran and Wiebe (2009) design
various sentiment constraints for inclusion in an
integer linear programming framework for stance
classification. For subgroup detection, Abu-Jbara
et al. (2012) uses the polarity of the expressions in
the discussions and partition discussants into sub-
groups based on the intuition that people in the
same group should mostly agree with each other.
Though those work highly relies on the compo-
nent of agreement and disagreement detection, the
evaluation is always performed on the ultimate ap-
plication only.
3 The Model
We first give a brief overview on isotonic Con-
ditional Random Fields (isotonic CRF) (Mao and
Lebanon, 2007), which is used as the backbone
approach for our sentence- or segment-level agree-
ment and disagreement detection model. We defer
the explanation of online discussion lexicon con-
struction in Section 4.
3.1 Problem Description
Consider a discussion comprised of sequential
turns uttered by the participants; each turn con-
sists of a sequence of text units, where each unit
can be a sentence or a segment of several sen-
tences. Our model takes as input the text units
x = {x
1
, ? ? ? , x
n
} in the same turn, and outputs
a sequence of sentiment labels y = {y
1
, ? ? ? , y
n
},
where y
i
? O,O = {NN,N,O,P,PP}. The la-
bels in O represent strongly disagree (NN), dis-
agree (N), neutral (O), agree (P), strongly agree
(PP), respectively. In addition, elements in the
partially ordered set O possess an ordinal relation
?. Here, we differentiate agreement and disagree-
ment with different intensity, because the output
of our classifier can be used for other applications,
such as dispute detection, where ?strongly dis-
agree? (e.g. NN) plays an important role. Mean-
while, fine-grained sentiment labels potentially
provide richer context information for the sequen-
tial model employed for this task.
3.2 Isotonic Conditional Random Fields
Conditional Random Fields (CRF) have been suc-
cessfully applied in numerous sequential labeling
tasks (Lafferty et al., 2001). Given a sequence
of utterances or segments x = {x
1
, ? ? ? , x
n
}, ac-
cording to linear-chain CRF, the probability of the
labels y for x is given by:
p(y|x) =
1
Z(x)
exp(
?
i
?
?,?
?
??,??
f
??,??
(y
i?1
, y
i
)
+
?
i
?
?,w
?
??,w?
g
??,w?
(y
i
, x
i
))
(1)
99
f??,??
(y
i?1
, y
i
) and g
??,w?
(y
i
, x
i
) are feature
functions. Given that y
i?1
, y
i
, x
i
take values of
?, ?, w, the functions are indexed by pairs ??, ??
and ??,w?. ?
??,??
, ?
??,w?
are the parameters.
CRF, as defined above, is not appropriate for or-
dinal data like sentiment, because it ignores the
ordinal relation among sentiment labels. Isotonic
Conditional Random Fields (isotonic CRF) are
proposed by Mao and Lebanon (2007) to enforce a
set of monotonicity constraints on the parameters
that are consistent with the ordinal structure and
domain knowledge (in our case, a sentiment lexi-
con automatically constructed from online discus-
sions).
Given a lexiconM = M
p
?M
n
, whereM
p
and M
n
are two sets of features (usually words)
identified as strongly associated with positive sen-
timent and negative sentiment. The constraints are
encoded as below. For each feature w ? M
p
, iso-
tonic CRF enforces ? ? ?
?
? ?
??,w?
? ?
??
?
,w?
.
Intuitively, the parameters ?
??,w?
are intimately
tied to the model probabilities. When a feature
such as ?totally agree? is observed in the training
data, the feature parameter for ?
?PP,totally agree?
is
likely to increase. Similar constraints are also de-
fined onM
n
. In this work, we boostrap the con-
struction of an online discussion sentiment lexicon
used asM in the isotonic CRF (see Section 4).
The parameters can be found by maximizing the
likelihood subject to the monotonicity constraints.
We adopt the re-parameterization from Mao and
Lebanon (2007) for a simpler optimization prob-
lem, and refer the readers to Mao and Lebanon
(2007) for more details.
2
3.3 Features
The features used in sentiment prediction are listed
in Table 1. Features with numerical values are first
normalized by standardization, then binned into 5
categories.
Syntactic/Semantic Features. Dependency re-
lations have been shown to be effective for various
sentiment prediction tasks (Joshi and Penstein-
Ros?e, 2009; Somasundaran and Wiebe, 2009;
Hassan et al., 2010; Abu-Jbara et al., 2012). We
have two versions of dependency relation as fea-
tures, one being the original form, another gen-
2
The full implementation is based on MALLET (McCal-
lum, 2002). We thank Yi Mao for sharing the implementation
of the core learning algorithm.
Lexical Features
- unigram/bigram
- num of words all uppercased
- num of words
Discourse Features
- initial uni-/bi-/trigram
- repeated punctuations
- hedging (Farkas et al., 2010)
- number of negators
Syntactic/Semantic Features
- unigram with POS tag
- dependency relation
Conversation Features
- quote overlap with target
- TFIDF similarity with target (remove quote first)
Sentiment Features
- connective + sentiment words
- sentiment dependency relation
- sentiment words
Table 1: Features used in sentiment prediction.
eralizing a word to its POS tag in turn. For in-
stance, ?nsubj(wrong, you)? is generlized as the
?nsubj(ADJ, you)? and ?nsubj(wrong, PRP)?. We
use Stanford parser (de Marneffe et al., 2006) to
obtain parse trees and dependency relations.
Discourse Features. Previous work (Hirschberg
and Litman, 1993; Abbott et al., 2011) suggests
that discourse markers, such as what?, actually,
may have their use for expressing opinions. We
extract the initial unigram, bigram, and trigram of
each utterance as discourse features (Hirschberg
and Litman, 1993). Hedge words are collected
from the CoNLL-2012 shared task (Farkas et al.,
2010).
Conversation Features. Conversation features
encode some useful information regarding the
similarity between the current utterance(s) and the
sentences uttered by the target participant. TFIDF
similarity is computed. We also check if the cur-
rent utterance(s) quotes target sentences and com-
pute its length.
Sentiment Features. We gather connectives
from Penn Discourse TreeBank (Rashmi Prasad
and Webber, 2008) and combine them with any
sentiment word that precedes or follows it as
new features. Sentiment dependency relations are
the subset of dependency relations with sentiment
words. We replace those words with their polarity
equivalents. For example, relation ?nsubj(wrong,
you)? becomes ?nsubj(SentiWord
neg
, you)?.
100
POSITIVE
please elaborate, nod, await response, from experiences, anti-war, profits, promises of, is undisputed,
royalty, sunlight, conclusively, badges, prophecies, in vivo, tesla, pioneer, published material, from god,
plea for, lend itself, geek, intuition, morning, anti SentiWord
neg
, connected closely, Rel(undertake,
to), intelligibility, Rel(articles, detailed), of noting, for brevity, Rel(believer, am), endorsements, testable,
source carefully
NEGATIVE
: (, TOT, ?!!, in contrast, ought to, whatever, Rel(nothing, you), anyway, Rel(crap, your), by facts, pur-
porting, disproven, Rel(judgement, our), Rel(demonstrating, you), opt for, subdue to, disinformation,
tornado, heroin, Rel(newbies, the), Rel (intentional, is), pretext, watergate, folly, perjury, Rel(lock, ar-
ticle), contrast with, poke to, censoring information, partisanship, insurrection, bigot, Rel(informative,
less), clowns, Rel(feeling, mixed), never-ending
Table 2: Example terms and relations from our online discussion lexicon. We choose for display terms
that do not contain any seed word.
4 Online Discussion Sentiment Lexicon
Construction
So far as we know, there is no lexicon available
for online discussions. Thus, we create from a
large-scale corpus via label propagation. The la-
bel propagation algorithm, proposed by Zhu and
Ghahramani (2002), is a semi-supervised learning
method. In general, it takes as input a set of seed
samples (e.g. sentiment words in our case), and
the similarity between pairwise samples, then it-
eratively assigns values to the unlabeled samples
(see Algorithm 1). The construction of graph G is
discussed in Section 4.1. Sample sentiment words
in the new lexicon are listed in Table 2.
Input : G = (V,E), w
ij
? [0, 1], positive
seed words P , negative seed words
N , number of iterations T
Output: {y
i
}
|V |?1
i=0
y
i
= 1.0, ?v
i
? P
y
i
= ?1.0, ?v
i
? N
y
i
= 0.0, ?v
i
/? P ?N
for t = 1 ? ? ?T do
y
i
=
?
(v
i
,v
j
)?E
w
ij
?y
j
?
(v
i
,v
j
)?E
w
ij
, ?v
i
? V
y
i
= 1.0, ?v
i
? P
y
i
= ?1.0, ?v
i
? N
end
Algorithm 1: The label propagation algo-
rithm (Zhu and Ghahramani, 2002) used for
constructing online discussion lexicon.
4.1 Graph Construction
Node Set V . Traditional lexicons, like General
Inquirer (Stone et al., 1966), usually consist of po-
larized unigrams. As we mentioned in Section 1,
unigrams lack the capability of capturing the sen-
timent conveyed in online discussions. Instead,
bigrams, dependency relations, and even punctu-
ation can serve as supplement to the unigrams.
Therefore, we consider four types of text units as
nodes in the graph: unigrams, bigrams, depen-
dency relations, sentiment dependency relations.
Sentiment dependency relations are described in
Section 3.3. We replace all relation names with a
general label. Text units that appear in at least 10
discussions are retained as nodes to reduce noise.
Edge Set E. As Velikovich et al. (2010) and
Feng et al. (2013) notice, a dense graph with a
large number of nodes is susceptible to propagat-
ing noise, and will not scale well. We thus adopt
the algorithm in Feng et al. (2013) to construct
a sparsely connected graph. For each text unit t,
we first compute its representation vector ~a using
Pairwise Mutual Information scores with respect
to the top 50 co-occuring text units. We define
?co-occur? as text units appearing in the same sen-
tence. An edge is created between two text units
t
0
and t
1
only if they ever co-occur. The similar-
ity between t
0
and t
1
is calculated as the Cosine
similarity between ~a
0
and ~a
1
.
Seed Words. The seed sentiment are collected
from three existing lexicons: MPQA lexicon, Gen-
eral Inquirer, and SentiWordNet. Each word in
SentiWordNet is associated with a positive score
and a negative score; words with a polarity score
101
larger than 0.7 are retained. We remove words
with conflicting sentiments.
4.2 Data
The graph is constructed based on Wikipedia talk
pages. We download the 2013-03-04 Wikipedia
data dump, which contains 4,412,582 talk pages.
Since we are interested in conversational lan-
guages, we filter out talk pages with fewer than
5 participants. This results in a dataset of 20,884
talk pages, from which the graph is constructed.
5 Experimental Setup
5.1 Datasets
Wikipedia Talk pages. The first dataset we use
is Authority and Alignment in Wikipedia Dis-
cussions (AAWD) corpus (Bender et al., 2011).
AAWD consists of 221 English Wikipedia discus-
sions with agreement and disagreement annota-
tions.
3
The annotation of AAWD is made at utterance-
or turn-level, where a turn is defined as continu-
ous body of text uttered by the same participant.
Annotators either label each utterance as agree-
ment, disagreement or neutral, and select the cor-
responding spans of text, or label the full turn.
Each turn is annotated by two or three people. To
induce an utterance-level label for instances that
have only a turn-level label, we assume they have
the same label as the turn.
To train our sentiment model, we further trans-
form agreement and disagreement labels (i.e. 3-
way) into the 5-way labels. For utterances that
are annotated as agreement and have the text
span specified by at least two annotators, they are
treated as ?strongly agree? (PP). If an utterance is
only selected as agreement by one annotator or it
gets the label by turn-level annotation, it is ?agree?
(P). ?Strongly disagree? (NN) and ?disagree? (N)
are collected in the same way from disagreement
label. All others are neutral (O). In total, we have
16,501 utterances. 1,930 and 1,102 utterances are
labeled as ?NN? and ?N?. 532 and 99 of them are
?PP? and ?P?. All other 12,648 are neutral sam-
ples.
4
3
Bender et al. (2011) originally use positive alignment
and negative alignment to indicate two types of social moves.
They define those alignment moves as ?agreeing or disagree-
ing? with the target. We thus use agreement and disagreement
instead of positive and negative alignment in this work.
4
345 samples with both positive and negative labels are
treated as neutral.
Online Debate. The second dataset is the Inter-
net Argument Corpus (IAC) (Walker et al., 2012a)
collected from an online debate forum. Each dis-
cussion in IAC consists of multiple posts, where
we treat each post as a turn. Most posts (72.3%)
contain quoted content from the posts they target
at or other resources. A post can have more than
one quote, which naturally break the post into mul-
tiple segments. 1,806 discussions are annotated
with agreement and disagreement on the segment-
level from -5 to 5, with -5 as strongly disagree and
5 as strongly agree. We first compute the average
score for each segment among different annotators
and transform the score into sentiment label in the
following way. We treat [?5,?3] as NN (1595
segments), (?3,?1] as N (4548 segments), [1, 3)
as P (911 samples), [3, 5] as PP (199), all others as
O (290 segments).
In the test phase, utterances or segments pre-
dicted with NN or N are treated as disagreement;
the ones predicted as PP or P are agreement; O is
neutral.
5.2 Comparison
We compare with two baselines. (1) Baseline (Po-
larity) is based on counting the sentiment words
from our lexicon. An utterance or segment is
predicted as agreement if it contains more posi-
tive words than negative words, or disagreement
if more negative words are observed. Other-
wise, it is neutral. (2) Baseline (Distance) is ex-
tended from (Hassan et al., 2010). Each sentiment
word is associated with the closest second per-
son pronoun, and a surface distance can be com-
puted between them. A classifier based on Sup-
port Vector Machines (Joachims, 1999) (SVM) is
trained with the features of sentiment words, min-
imum/maximum/average of the distances.
We also compare with two state-of-the-art
methods that are widely used in sentiment predic-
tion for conversations. The first one is an RBF
kernel SVM based approach, which has been used
for sentiment prediction (Hassan et al., 2010), and
(dis)agreement detection (Yin et al., 2012) in on-
line debates. The second is linear chain CRF,
which has been utilized for (dis)agreement iden-
tification in broadcast conversations (Wang et al.,
2011).
102
Strict F1 Soft F1
Agree Disagree Neutral Agree Disagree Neutral
Baseline (Polarity) 14.56 25.70 64.04 22.53 38.61 66.45
Baseline (Distance) 8.08 20.68 84.87 33.75 55.79 88.97
SVM (3-way) 26.76 35.79 77.39 44.62 52.56 80.84
+ downsampling 21.60 36.32 72.11 31.86 49.58 74.92
CRF (3-way) 20.99 23.85 85.28 56.28 56.37 89.41
CRF (5-way) 20.47 19.42 85.86 58.39 56.30 90.10
+ downsampling 24.26 31.28 77.12 47.30 46.24 80.18
isotonic CRF 24.32 21.95 86.26 68.18 62.53 88.87
+ downsampling 29.62 34.17 80.97 55.38 53.00 84.56
+ new lexicon 46.01 51.49 87.40 74.47 67.02 90.56
+ new lexicon + downsampling 47.90 49.61 81.60 64.97 58.97 84.04
Table 3: Strict and soft F1 scores for agreement and disagreement detection on Wikipedia talk pages
(AAWD). All the numbers are multiplied by 100. In each column, bold entries (if any) are statistically
significantly higher than all the rest, and the italic entry has the highest absolute value. Our model based
on the isotonic CRF with the new lexicon produces significantly better results than all the other systems
for agreement and disagreement detection. Downsampling, however, is not always helpful.
6 Results
In this section, we first show the experimental re-
sults on sentence- and segment-level agreement
and disagreement detection in two types of online
discussions ? Wikipedia Talk pages and online de-
bates. Then we provide more detailed analysis for
the features used in our model. Furthermore, we
discuss several types of errors made in the model.
6.1 Wikipedia Talk Pages
We evaluate the systems by standard F1 score on
each of the three categories: agreement, disagree-
ment, and neutral. For AAWD, we compute two
versions of F1 scores. Strict F1 is computed
against the true labels. For soft F1, if a sentence
is never labeled by any annotator on the sentence-
level and adopts its agreement/disagreement label
from the turn-level annotation, then it is treated as
a true positive when predicted as neutral.
Table 3 demonstrates our main results on the
Wikipedia Talk pages (AAWD dataset). With-
out downsampling, our isotonic CRF based sys-
tems with the new lexicon significantly outper-
form the compared approaches for agreement and
disagreement detection according to the paired-
t test (p < 0.05). We also perform downsam-
pling by removing the turns only containing neu-
tral utterances. However, it does not always help
with performance. We suspect that, with less neu-
tral samples in the training data, the classifier is
less likely to make neutral predictions, which thus
decreases true positive predictions. For strict F-
scores on agreement/disagreement, downsampling
Agree Disagree Neu
Baseline (Polarity) 3.33 5.96 65.61
Baseline (Distance) 1.65 5.07 85.41
SVM (3-way) 25.62 69.10 31.47
+ new lexicon features 28.35 72.58 34.53
CRF (3-way) 29.46 74.81 31.93
CRF (5-way) 24.54 69.31 39.60
+ new lexicon features 28.85 71.81 39.14
isotonic CRF 53.40 76.77 44.10
+ new lexicon 61.49 77.80 51.43
Table 4: F1 scores for agreement and disagree-
ment detection on online debate (IAC). All the
numbers are multiplied by 100. In each column,
bold entries (if any) are statistically significantly
higher than all the rest, and the italic entry has the
highest absolute value except baselines. We have
two main observations: 1) Both of our models
based on isotonic CRF significantly outperform
other systems for agreement and disagreement de-
tection. 2) By adding the new lexicon, either as
features or constraints in isotonic CRF, all systems
achieve better F1 scores.
has mixed effect, but mostly we get slightly better
performance.
6.2 Online Debates
Similarly, F1 scores for agreement, disagreement
and neutral for online debates (IAC dataset) are
displayed in Table 4. Both of our systems based
on isotonic CRF achieve significantly better F1
scores than the comparison. Especially, our sys-
tem with the new lexicon produces the best results.
For SVM and linear-chain CRF based systems, we
also add new sentiment features constructed from
the new lexicon as described in Section 3.3. We
103
can see that those sentiment features also boost the
performance for both of the compared approaches.
6.3 Feature Evaluation
Moreover, we evaluate the effectiveness of fea-
tures by adding one type of features each time.
The results are listed in Table 5. As it can be seen,
the performance gets improved incrementally with
every new set of features.
We also utilize ?
2
-test to highlight some of
the salient features on the two datasets. We can
see from Table 6 that, for online debates (IAC),
some features are highly topic related, such as ?the
male? or ?the scientist?. This observation concurs
with the conclusion in Misra and Walker (2013)
that features with topic information are indicative
for agreement and disagreement detection.
AAWD Agree Disagree Neu
Lex 40.77 52.90 79.65
Lex + Syn 68.18 63.91 88.87
Lex + Syn + Disc 70.93 63.69 89.32
Lex + Syn + Disc + Con 71.27 63.72 89.60
Lex + Syn + Disc + Con + Sent 74.47 67.02 90.56
IAC Agree Disagree Neu
Lex 56.65 75.35 45.72
Lex + Syn 54.16 75.13 46.12
Lex + Syn + Disc 54.27 76.41 47.60
Lex + Syn + Disc + Con 55.31 77.25 48.87
Lex + Syn + Disc + Con + Sent 61.49 77.80 51.43
Table 5: Results on Wikipedia talk page
(AAWD) (with soft F1 score) and online de-
bate (IAC) with different feature sets (i.e Lexical,
Syntacitc/Semantic, Discourse, Conversation, and
Sentiment features) by using isotonic CRF. The
numbers in bold are statistically significantly
higher than the numbers above it (paired-t test,
p < 0.05).
6.4 Error Analysis
After a closer look at the data, we found two ma-
jor types of errors. Firstly, people express dis-
agreement not only by using opinionated words,
but also by providing contradictory example. This
needs a deeper understanding of the semantic in-
formation embedded in the text. Techniques like
textual entailment can be used in the further work.
Secondly, a sequence of sentences with sarcasm is
hard to detect. For instance, ?Bravo, my friends!
Bravo! Goebbles would be proud of your abilities
to whitewash information.? We observe terms like
?Bravo?, ?friends?, and ?be proud of? that are in-
dicators for positive sentiment; however, they are
AAWD
POSITIVE: agree, nsubj (agree, I), nsubj (right,
you), Rel (Sentiment
pos
, I), thanks, amod (idea,
good), nsubj(glad, I), good point, concur, happy
with, advmod (good, pretty), suggestion
Hedge
NEGATIVE: you, your, nsubj (negative, you),
numberOfNegator, don?t, nsubj (disagree, I),
actually
SentInitial
, please stop
SentInitial
, what
?
SentInitial
, should
Hedge
IAC
POSITIVE: amod (conclusion, logical), Rel (agree,
on), Rel (have, justified), Rel (work, out), one
might
SentInitial
, to confirm
Hedge
, women
NEGATIVE: their kind, the male, the female, the
scientist, according to, is stated, poss (understand-
ing, my), hell
SentInitial
, whatever
SentInitial
Table 6: Relevant features by ?
2
test on AAWD
and IAC.
in sarcastic tone. We believe a model that is able
to detect sarcasm would further improve the per-
formance.
7 Conclusion
We present an agreement and disagreement detec-
tion model based on isotonic CRFs that outputs
labels at the sentence- or segment-level. We boot-
strap the construction of a sentiment lexicon for
online discussions, encoding it in the form of do-
main knowledge for the isotonic CRF learner. Our
sentiment-tagging model is shown to outperform
the state-of-the-art approaches on both Wikipedia
Talk pages and online debates.
Acknowledgments We heartily thank the Cornell
NLP Group and the reviewers for helpful com-
ments. This work was supported in part by NSF
grants IIS-0968450 and IIS-1314778, and DARPA
DEFT Grant FA8750-13-2-0015. The views and
conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily
representing the official policies or endorsements,
either expressed or implied, of NSF, DARPA or
the U.S. Government.
References
Rob Abbott, Marilyn Walker, Pranav Anand, Jean E.
Fox Tree, Robeson Bowmani, and Joseph King. 2011.
How can you say such things?!?: Recognizing disagree-
ment in informal political argument. In Proceedings of
the Workshop on Languages in Social Media, LSM ?11,
pages 2?11, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi, and
104
Dragomir Radev. 2012. Subgroup detection in ideo-
logical discussions. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics: Long Papers - Volume 1, ACL ?12, pages 399?409,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Emily M. Bender, Jonathan T. Morgan, Meghan Oxley, Mark
Zachry, Brian Hutchinson, Alex Marin, Bin Zhang, and
Mari Ostendorf. 2011. Annotating social acts: Author-
ity claims and alignment moves in wikipedia talk pages.
In Proceedings of the Workshop on Languages in Social
Media, LSM ?11, pages 48?57, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Yejin Choi and Claire Cardie. 2009. Adapting a polarity lex-
icon using integer linear programming for domain-specific
sentiment classification. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language Pro-
cessing: Volume 2 - Volume 2, EMNLP ?09, pages 590?
598, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006. Generating typed dependency
parses from phrase structure trees. In LREC.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A holistic
lexicon-based approach to opinion mining. In Proceed-
ings of the 2008 International Conference on Web Search
and Data Mining, WSDM ?08, pages 231?240, New York,
NY, USA. ACM.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiwordnet:
A publicly available lexical resource for opinion mining.
In In Proceedings of the 5th Conference on Language Re-
sources and Evaluation (LREC06, pages 417?422.
Rich?ard Farkas, Veronika Vincze, Gy?orgy M?ora, J?anos
Csirik, and Gy?orgy Szarvas. 2010. The conll-2010 shared
task: Learning to detect hedges and their scope in nat-
ural language text. In Proceedings of the Fourteenth
Conference on Computational Natural Language Learn-
ing ? Shared Task, CoNLL ?10: Shared Task, pages 1?
12, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Song Feng, Jun Seok Kang, Polina Kuznetsova, and Yejin
Choi. 2013. Connotation lexicon: A dash of sentiment
beneath the surface meaning. In ACL, pages 1774?1784.
The Association for Computer Linguistics.
Oliver Ferschke, Iryna Gurevych, and Yevgen Chebotar.
2012. Behind the article: Recognizing dialog acts in
wikipedia talk pages. In Proceedings of the 13th Con-
ference of the European Chapter of the Association for
Computational Linguistics, EACL ?12, pages 777?786,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Michel Galley, Kathleen McKeown, Julia Hirschberg, and
Elizabeth Shriberg. 2004. Identifying agreement and dis-
agreement in conversational speech: use of Bayesian net-
works to model pragmatic dependencies. In ACL ?04:
Proceedings of the 42nd Annual Meeting on Association
for Computational Linguistics, pages 669+, Morristown,
NJ, USA. Association for Computational Linguistics.
Sangyun Hahn, Richard Ladner, and Mari Ostendorf. 2006.
Agreement/disagreement classification: Exploiting unla-
beled data using contrast classifiers. In Proceedings of the
Human Language Technology Conference of the NAACL,
Companion Volume: Short Papers, pages 53?56, New
York City, USA, June. Association for Computational Lin-
guistics.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude?: Identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?10, pages 1245?1255,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ahmed Hassan, Amjad Abu-Jbara, and Dragomir Radev.
2012. Detecting subgroups in online discussions by mod-
eling positive and negative relations among participants.
In Proceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-CoNLL
?12, pages 59?70, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Julia Hirschberg and Diane Litman. 1993. Empirical studies
on the disambiguation of cue phrases. Comput. Linguist.,
19(3):501?530, September.
Thorsten Joachims. 1999. Advances in kernel meth-
ods. chapter Making Large-scale Support Vector Machine
Learning Practical, pages 169?184. MIT Press, Cam-
bridge, MA, USA.
Mahesh Joshi and Carolyn Penstein-Ros?e. 2009. Generaliz-
ing dependency features for opinion mining. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Papers,
ACLShort ?09, pages 313?316, Stroudsburg, PA, USA.
Association for Computational Linguistics.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
Proceedings of the Eighteenth International Conference
on Machine Learning, ICML ?01, pages 282?289, San
Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Yi Mao and Guy Lebanon. 2007. Isotonic conditional ran-
dom fields and local sentiment flow. In Advances in Neu-
ral Information Processing Systems.
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit. http://mallet.cs.umass.edu.
Amita Misra and Marilyn Walker. 2013. Topic independent
identification of agreement and disagreement in social me-
dia dialogue. In Proceedings of the SIGDIAL 2013 Con-
ference, pages 41?50, Metz, France, August. Association
for Computational Linguistics.
Minghui Qiu, Liu Yang, and Jing Jiang. 2013. Mining user
relations from online discussions using sentiment analy-
sis and probabilistic matrix factorization. In Proceedings
of the 2013 Conference of the North American Chapter
of the Association for Computational Linguistics: Human
Language Technologies, pages 401?410, Atlanta, Georgia,
June. Association for Computational Linguistics.
Alan Lee Eleni Miltsakaki Livio Robaldo Aravind Joshi
Rashmi Prasad, Nikhil Dinesh and Bonnie Webber.
2008. The penn discourse treebank 2.0. In Bente
Maegaard Joseph Mariani Jan Odijk Stelios Piperidis
Daniel Tapias Nicoletta Calzolari (Conference Chair),
105
Khalid Choukri, editor, Proceedings of the Sixth Interna-
tional Conference on Language Resources and Evaluation
(LREC?08), Marrakech, Morocco, may. European Lan-
guage Resources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
Swapna Somasundaran and Janyce Wiebe. 2009. Recogniz-
ing stances in online debates. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Language
Processing of the AFNLP: Volume 1 - Volume 1, ACL ?09,
pages 226?234, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Swapna Somasundaran and Janyce Wiebe. 2010. Recogniz-
ing stances in ideological on-line debates. In Proceedings
of the NAACL HLT 2010 Workshop on Computational Ap-
proaches to Analysis and Generation of Emotion in Text,
CAAGET ?10, pages 116?124, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and
Daniel M. Ogilvie. 1966. The General Inquirer: A Com-
puter Approach to Content Analysis. MIT Press, Cam-
bridge, MA.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from con-
gressional floor-debate transcripts. In Proceedings of the
2006 Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?06, pages 327?335, Strouds-
burg, PA, USA. Association for Computational Linguis-
tics.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Hannan,
and Ryan McDonald. 2010. The viability of web-derived
polarity lexicons. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics, HLT
?10, pages 777?785, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Marilyn Walker, Jean Fox Tree, Pranav Anand, Rob Abbott,
and Joseph King. 2012a. A corpus for research on de-
liberation and debate. In Proceedings of the Eight Inter-
national Conference on Language Resources and Evalu-
ation (LREC?12), Istanbul, Turkey, may. European Lan-
guage Resources Association (ELRA).
Marilyn A. Walker, Pranav Anand, Rob Abbott, and Ricky
Grant. 2012b. Stance classification using dialogic prop-
erties of persuasion. In HLT-NAACL, pages 592?596. The
Association for Computational Linguistics.
Wen Wang, Sibel Yaman, Kristin Precoda, Colleen Richey,
and Geoffrey Raymond. 2011. Detection of agreement
and disagreement in broadcast conversations. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technolo-
gies: Short Papers - Volume 2, HLT ?11, pages 374?
378, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level sentiment
analysis. In Proceedings of the Conference on Human
Language Technology and Empirical Methods in Natural
Language Processing, HLT ?05, pages 347?354, Strouds-
burg, PA, USA. Association for Computational Linguis-
tics.
Jie Yin, Paul Thomas, Nalin Narang, and Cecile Paris. 2012.
Unifying local and global agreement and disagreement
classification in online debates. In Proceedings of the
3rd Workshop in Computational Approaches to Subjec-
tivity and Sentiment Analysis, WASSA ?12, pages 61?
69, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning from
labeled and unlabeled data with label propagation. In
Technical Report CMU-CALD-02-107.
106
