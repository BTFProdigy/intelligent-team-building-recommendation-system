Chart-Based Transfer Rule Application in Machine Translation 
Adam Meyers 
New York University 
meyers@cs.nyu.edu 
Mich iko  Kosaka 
Monlnouth University 
kosaka@monmouth.edu 
Ralph GrishInan 
New York University 
gr ishman@cs.nyu.edu 
Abstract 
35"ansfer-based Machine Translation systems re- 
quire a procedure for choosing the set; of transfer 
rules for generating a target language transla- 
tion from a given source language sentence. In 
an MT system with many comI)eting transfer 
rules, choosing t;he best, set of transfer ules for 
translation may involve the evaluation of an ex- 
plosive number of competing wets. We propose a
sohltion t;o this problem l)ased on current best- 
first chart parsing algorithms. 
1 Introduct ion 
ri~'ansfer-based Machine 'Kanslation systenls re- 
quire a procedure for choosing the set of trans- 
tier rules for generating a target  language I;rans- 
lation from a given source language sentence. 
This procedure is trivial tbr a system if, given 
a (:ontext, one transtb.r ule. can l)e selected un- 
~mfl)iguously. O|;herwise, choosing the besl; set; 
of transfer ules may involve the. evaluation of 
mmmrous competing sets. In fact, the number 
of l)ossible transfer ule combinations increas- 
es exponentially with the length of the source, 
language sentence,. This situation mirrors the 
t)roblem of choosing productions in a nondeter- 
ministic parser, in this paI)er, we descril)e a 
system for choosing transfer ules, based on s- 
tatistical chart parsing (Bol)row, 1990; Chitrao 
and Grishman, 1990; Caraballo and Charniak, 
1997; Charniak et al, 1998). 
In our Machine %'anslation system, transfer 
rules are generated automatically from parsed 
parallel text along the lines of (Matsulnoto el; 
al,, 1993; Meyers et al, 1996; Meyers et al, 
1998b). Our system tends to acquire a large 
nmnber of transt~r rules, due lnainly to 3,1terna- 
tive ways of translating the same sequences of 
words, non-literal translations in parallel text 
and parsing e, rrors. It is therefore crucial that 
our system choose the best set of rules efficient- 
ly. While the technique discussed he.re obviously 
applies to similar such systems, it could also ap- 
ply to hand-coded systems in which each word 
or group of words is related to more than one 
transfer ule. D)r example, both Multra (Hein, 
1996) and the Eurotra system described in (Way 
el; al., 1997) require components for deciding 
which combination of transtbr ules to use. The 
proi).osed technique may 1)e used with syst;ems 
like these, t)rovided that all transfer ules are as- 
signed initial scores rating thcqr at)propriateness 
for translation. These al)t)rol)riateness ratings 
couhl be dependent or independent of context. 
2 Previous Work 
The MT literature deserib(;s everal techniques 
tbr deriving the appropriate translation. Statis- 
tical systems l;hal; do not incorporate linguistic 
analysis (Brown el: al., 1993) typically choose 
the most likely translation based on a statis- 
tical mode.l, i.e.., translation probability deter- 
mines the translation. (Hein, 1996) reports a set; 
of (hand-coded) fea|;llre structure based prefi~r- 
ence rules to choose among alternatives in Mu\]- 
tra. There is some discussion about adding 
some transtbr ules automatically acquired flom 
corpora to Multra? Assuming that they over- 
generate rules (as we did), a system like the one 
we propose should 1)e beneficial. In (Way et al, 
1997), many ditDrent criteria are used to dloose 
trmlsi~;r ules to execute including: pretbrmlces 
for specific rules over general ones, and comt)lex 
rule nol, ation that insures that tb.w rules can 21)- 
ply to the same set, of words. 
The Pangloss Mark III system (Nirenburg 
~This translatioll procedm'e would probably comple- 
menI~ not; replace exist, ing procedures in these systelns. 
2http : / / s tp .  l i ng .  uu. se /~corpora /p lug / repor t  s / 
ansk_last/ is a report on this 1)reject; for Multra. 
537 
and Frederking, 1995) uses a chart-walk algo- 
rithm to combine the results of three MT en- 
gines: an example-based ngine, a knowledge- 
based engine, and a lexical-transfer engine. 
Each engine contributes its best edges and tile 
chart-walk algorithm uses dynamic program- 
ruing to find the combination of edges with the 
best overall score that covers the input string. 
Scores of edges are normalized so that the scores 
fi'om the different engines are comparable and 
weighted to favor engines which tend to produce 
better results. Pangloss's algorithm combines 
whole MT systems. In contrast, our algorith- 
m combines output of individual transfer ules 
within a single MT system. Also, we use a best- 
first search that incorporates a probabilistic- 
based figure of merit, whereas Pangloss uses an 
empirically based weighting scheme and what 
appears to be a top-down search. 
Best-first probabilistic chart parsers (Bo- 
brow, 1990; Chitrao and Grishman, 1990; Cara- 
ballo and Charniak, 1997; Charniak et al, 1998) 
strive to find the best parse, without exhaus- 
tively trying a l l  possible productions. A proba- 
bilistic figure of merit (Caraballo and Charniak, 
1997; Charniak et al, 1998) is devised for rank- 
ing edges. The highest ranking edges are pur- 
sued first and the parser halts after it produces 
a complete parse. We propose an algorithm for 
choosing and applying transthr ules based on 
probability. Each final translation is derived 
from a specific set of transfer ules. If the pro- 
cedure immediately selected these transfer rules 
and applied them in tile correct order, we would 
arrive at tile final translation while creating the 
minimum number of edges. Our procedure uses 
about 4 tinms this minimum number of edges. 
With respect o chart parsing, (Charniak et al, 
1998) report that their parser can achieve good 
results while producing about three times tile 
mininmm number of edges required to produce 
the final parse. 
3 Test  Data  
We conducted two experiments. For experimen- 
t1, we parsed a sentence-aligned pair of Span- 
ish and English corpora, each containing 1155 
sentences of Microsoft Excel Help Text. These 
pairs of parsed sentences were divided into dis- 
tinct training and test sets, ninety percent for 
training and ten percent fbr test. The training 
Source Tree Target Tree 
D = vo lvcr  D' = reca lcu late  
s,,I,J  
A = Exce l  E = ca lcu la r  
Obj~en A' = Excel I C' = workbook 
B' = va lues  
/ C = l ibro 
k , 
B =va lores  \ae  
F = t raba jo  
Excel vuelve a calcular Excel recalculates 
valores en libro de trabajo values iu workbook 
Figure 1: Spanish and English I{egularized 
Parse 2?ees 
set was used to acquire transfer ules (Meyers 
et al, 1998b) which were then used to translate 
tile sentences in tile test set. This paper focus- 
es on our technique for applying these transfer 
rules in order to translate the test sentences. 
The test and training sets in experiment1 
were rotated, assigning a different enth of the 
sentences to the test set in each rotation. In this 
ww we tested tile program on the entire corpus. 
Only one test set (one tenth of the corpus) was 
used for tuning the system (luring development. 
~:ansfer rules, 11.09 on average, were acquired 
t'rom each training set and used for translation 
of the corresponding test set. For Experiment 
2, we parsed 2617 pairs of aligned sentences and 
used the same rotation procedure for dividing 
test and training corpora. The Experiment 2
corpus included the experinlentl corpus. An av- 
erage of 2191 transfer ules were acquired from 
a given set of Experinmnt 2 training sentences. 
Experimentl isorchestrated in a carefld man- 
ner that may not be practical for extremely 
large corpora, and Experiment 2 shows how the 
program performs if we scale up and elilniuate 
some of the fine-tuning. Apart from corpus size, 
there are two main difference between the two 
experiments: (1) the experimentl corpus was 
aligned completely by hand, whereas the Exper- 
iment 2 corpus was aligned automatically using 
the system described ill (Meyers et al, 1998a); 
and (2) the parsers were tuned to the experi- 
mentl sentences, but not the Experiment 2 sen- 
tences (that did not overlap with experinmntl). 
538 
1) A = Excel  
2) B =va lores  
C = l ibro 
v 
A' = Excel 
B'  = values 
.~) 
r 
C' = workbook  
F = trabajo 
D = volvcr S.IJ.i~ 
4) 
1 E = ealcular  
O b. \ ]~en l 
2 3 
1)' = recalculate 
1 2 3 
Figure 2: A S('t of %-ansfer Rules 
4 Parses  and  Trans fer  Ru les  
Figure 1 is a pair of "regularized" parses t br a 
corresi)onding pair of Spanish and Fmglish sen- 
tences fi'om Microsoft Excel hell) text. These 
at'(; F-structure-like dependency analyses of sen- 
tences that represent 1)redicate argument struc- 
ture. This representation serves to neutralize 
some ditfbrences between related sentence tyt)es, 
e.g., the regularized parse of related active and 
t)a,~sive senten(:es are identical, except tbr the 
{i'.ature value pair {Mood, Passive}. Nodes (wfl- 
ues) are labeled with head words and arcs (fea- 
tures) are labeled with gramma~;ical thnetions 
(subject, object), 1)repositions (in) and subor- 
dinate conjunctions (beNre). a For demonstra- 
tion purposes, the source tree in Figure 1 is the 
input to our translation system and the target 
tree is the outl)ut. 
The t;ransfer rules in Figure 2 can be 
used to convert the intmt; tree into the out- 
1)at tree. These transtbr rules are pairs of 
corresponding rooted substructures, where a 
substructure (Matsumoto et al, 1993) is a 
connected set of arcs and nodes. A rule 
aMorphologieal features and their values (Gram- 
Number: plural) are also represented as ares and nodes. 
consists of o, ither a pair of "open" substructures 
(rule 4) or a pair of "closed" substructures (rules 
1, 2 and 3). Closed substructures consist of s- 
ingle nodes (A,A',B,B',C') or subtrees (the left 
hand side of rule 3). Open substructures con- 
tain one or more open arcs, arcs without heads 
(both sul)structures in rule 4). 
5 Simplif ied Translat ion with 
Tree-based Transfer Rules 
The rules in Figure 2 could combine by filling 
in the open arcs in rule 4 with the roots of the 
substructures in rules 1, 2 and 3. The result 
would be a closed edge which maps the left; tree 
in l,'igure, 1 into the right tree. Just as edges of a 
chart parser are based on the context free rules 
used by the chart parser, edges of our trans- 
lation system are, based on these trans~L'r ules. 
Initial edges are identical to transtb, r rules. Oth- 
er edges result from combining one closed edge 
with one open edge. Figure 3 lists the sequence 
of edges which wouhl result from combining the 
initial edges based (m Rules 1-4 to replicate, the 
trees in Figure 1. The translation proceeds by 
incrementally matching the left hand sides of 
Rules 1-4 with the intmt tree (and insuring that 
the tree is completely covered by these rules). 
The right-hand sides of these comt)atil)le rules 
are also (:ombined t;o 1)reduce the translal;iolL 
This is an idealized view of our system in which 
each node in the input tree matches the left;- 
hand side of exactly one transfer rule: there is 
no ambiguity and no combinatorial explosion. 
The reality is that more than one transfer ules 
may be activated tbr each node, as suggested 
in Figure 4. 4 If each of the six nodes of the 
source tree corresponded to five transfer rules, 
there are 56 = 15625 possible combinations of 
rules to consider. To produce t lm output  in Fig- 
ure 3, a minimum of seven edges would be re- 
quired: four initial edges derived ti'om the o- 
riginal transfer ules plus three additional edges 
representing the combination of edges (steps 2, 
3 and 4 in Figure 3). The speed of our system is 
measured by the number of actual edges divided 
by this minimuln. 
4The third example listed would actually involve two 
trm~sfer rules, one translating "volver" to "ret)cat" and 
the second translating "calcular" to "calculal;e". 
539 
1) 
2) 
D = vo lver  
S u ~  
1 E = ca lcu la r  
Obj~n 
2 3 
D = vo lver  
A = Exce l  E = ca lcu la r  
Obj~n 
2 3 
v 
v 
D' = reca lcu la te  
I 2 3 
D' = reca lcu la te  
A' = Excel 2 3 
3) 
D = volver 
A = Exce l  E = ca leu la r  
B = valores 3 
D'  = reca lcu la te  
A' = Excel / 3 
g' = values 
4) 
D = volver 
A = Excel E = ca lcu la r  
Ob/~n 
B = va io res  C = l ib ro  
de 
F = t raba jo  
v 
D'  = reca lcu la te  
A' = Excel \ C' = workbook 
B' = va lues  
Figure 3: An Idealized Translation Procedure 
6 Best  F i r s t  T rans la t ion  Procedure  
The following is an outline of our best first 
search procedure for finding a single translation: 
1. For each node N, find TN, the set of com- 
patible transfer ules 
2. Create initial edges for all TN 
3. Repeat until a "finished" edge is tbund or 
an edge limit is reached: 
(a) Find the highest scoring edge E 
(b) If complete, combine E with compati- 
ble incoml)lete dges 
(c) If incomplete, combine E with com- 
patible complete dges 
(d) Incomplete dge + complete edge = 
new edge 
The procedure creates one initial edge 
for each matching transfer rule in the 
database 5 and puts these edges in a 
'~The left-hand side of a matching transfer rule is com- 
patible with a substructure in the input source tree. 
540 
D'  = recalculate 
D = velvet 1 2 3 / %  
Sub, i / ~ a !)' = calculate 
/ \ 
/ E = \ '4 .  '+" 
3 again 
D = repeat 
Sabj ~ b j  
1 E = calculation 
Figure 4: Multiple \[lYansfer Rules for Each Sub- 
structm:e 
queue prioritized by score. The pro- 
cedure iteratively combines the best 
s(:oring edge with some other comt)al;ilfle 
edge to t)roduce a new edge. and inserts the new 
edge in the queu('.. The score for each new edge 
is a function of the scores of the edges used to 
produce it:. The process contimms m~til either 
an edge limit is reache(l (the system looks like 
it; will take too long to terminate) or a complete 
edge is t)roduced whose left-hand side is the 
input tree: we (:all this edge a "finished edge". 
We use the tbllowing technique for calculating 
the score tbr initial edges. 6 The score tbr each 
initial edge E rooted at N, based on rule/~, is 
calculated as follows: 
1. SCO17.F=I(S) " " F,.c.,~(n) = ~'?.q'~D~(~a  ~t N~) 
Where the fl'equency (Freq) of a rule is the 
nmnber of times it matched an exmnple in 
the training corpus, during rule ~cquisition. 
The denominator is the combined fl'equen- 
cies of all rules that match N. 
aThis is somewhat det)cndent on the way these |;rans- 
fer rules are derived. Other systems would t)robably have 
to use some other scoring system. 
Ezperiment 1:1155 sentences 
Norm No Norm 
Total Translations 
Over Edge Limit 
Actual Edges 
Miniature Edges 
Edge Ratio 
Accuracy 
1153 
2 
93,719 
22,125 
3.3 
70.9 
1127 
28 
579,278 
20,125 
1.4.8 
70.9 
Ezpcriment 2:2617 sentences 
Norm No Norm 
Total Translations 
Over Edge Limit 
Actual Edges 
Minimum Edges 
Edge Ratio 
A(:curacy 
2610 
7 
262,172 
48,570 
4.0 
62.6 
2544 
73 
1,398,796 
42,770 
15.5 
61.5 
Figure 5: Result:s 
2, S s ) = s ,o,.(;.l ( S ) - No,., , , ,  
Where the Norm (normalization) t~ctor is 
equal to the highest SCORE1 for any rule 
matching N. 
Since the log.2 of probabilities are necessarily 
negative, this has the effect of setting the E of 
each of the most t)rol)able initial edges to zero. 
The scores tbr non-initial edges are calculated 
by ad(ling u I) the scores of the initial e(tges of 
which they are comt)osed. 7 
Without any normMization (Score(S) = 
SCORE1 (,9)), small trees are favored over large 
trees. This slows down the process of finding the 
final result. The normalization we use insures 
that the most probable set; of transihr ules are 
considered early on. 
7 Resu l ts  
Figure 5 gives our results for both experiments 
1 and 2, both with normalization (Norm) and 
without (No Norm). "Total Translations" refer 
to the number of sen|;ences which were translat- 
ed successfully 1)y the system and "Over Edge 
Limit" refers to the numl)er of sentences which 
caused the system to exceed the edge limit, i.e., 
once the system produces over 10,000 edges, 
trm~slation failure is assmned. The system cur- 
7Scoring for special cases is not; included in this paper. 
These cases include rules for conjunctions and rules ibr 
words that do not match any transfer ules in a given 
context (we currently leave the word untranslated.) 
541 
rently will only fail to produce some transla- 
tion for any input if the edge limit is exceed- 
ed. "Actual Edges" reibrs to the total number 
of edges used tbr attempting to translate very 
sentence in the corpus. "Minimum Edges" refer 
to the total minimum number of edges required 
for successful translations. The "Edge Ratio" 
is a ratio between: (1) "Total Edges" less the 
mnnber of edges used in failed translations; and 
(2) The "Minimum Edges". This ratio, in com- 
l)ination with, the number of "Over Edge Limit" 
measures the efficiency of a given system. "Ac- 
curacy" is an assessment of translation quality 
which we will discuss in the next section. 
Normalization caused significant speed-up for 
both experiments. If you compare the total 
number of edges used with and without nor- 
malization, speed-up is a factor of 6.2 for Ex- 
periment I and 5.3 for Experiment 2. If you 
compare actual edge ratios, speed-up is a factor 
of 4:.5 tbr Experiment 1 and 3.9 tbr Experiment 
2. In addition, the number of failed parses went 
down by a fhctor of 10 for both experiments. As 
should be expected, accuracy was virtually the 
same with and without normalization, although 
normalization <lid cause a slight improvemen- 
t. Normalization should produce the essentially 
the same result in less time. 
These results suggest that we can probably 
count on a speed-up of at least 4 and a signif 
icant decline in failed parses by using normM- 
ization. The ditferences in performance on the 
two corpora are most likely due to the degree of 
hand-tuning for Experiment 1. 
7.1 Our  Accuracy  Measure  
"Accuracy" in Figure 5 is the average of the 
tbllowing score for each translated sentence: 
ITNYu ~ TMSI 
1/2 x (ITNYuI + ITMsl) 
TNZU is the set of words in NYU's translation 
and TMS is the set of words in the original Mi- 
crosoft translation. If TNYU = "A B C D E" 
and TMS = "A B C F", then the intersection 
set "A B C" is length 3 (the numerator) and 
the average length of TNZU and TMS is 4 1/2 
(the denominator). The accuracy score equals 
3 + 4 1/2 = 2/3. This is a Dice coefficient com- 
parison of our translation with the original. It is 
an inexpensive nmthod of measuring the pertbr- 
mance of a new version of our system, hnprove- 
ments in the average accuracy score for our san> 
ple set; of sentences usually reflect an improve- 
ment in overall translation quality. While it is 
significant hat the accuracy scores in Figure 5 
did not go down when we normalized the scores, 
the slight improvement in accuracy should not 
be given nmch weight. Our accuracy score is 
flawed in that it cannot account for the follow- 
ing facts: (1) good paraphrases are perfectly ac- 
ceptable; (2) some diflbrences in word selection 
are more significant han others; and (3) errors 
in syntax are not directly accounted tbr. 
NYU's system translates the Spanish sen- 
tence "1. Selection la celda en la que desea 
introducir una rethrencia" as "1. select the cel- 
l that you want to enter a reference in". Mi- 
crosoft translates this sentence as "1. Select the 
cell in which you want; to enter the reference". 
Our system gives NYU's translation an accu- 
racy score of .75 due to the degree of overlap 
with Microsoft's translation. A truman reviewer 
wouhl probably rate NYU's translation as com- 
pletely acceptable. In contrast, NYU's system 
produced the following unacceptable translation 
which also received a score of .75: the Spanish 
sentence "Elija la funcidn que desea pegar en la 
f6rmula en el cuadro de di~logo Asistente para 
flmciones" is translated as " "Choose the flmc- 
tion that wants to paste Function Wizard in the 
formula in the dialog box", in contr,~st with Mi- 
crosoft's translation "Choose the flmction you 
want to paste into the tbrmula fl'om the Func- 
tion Wizard dialog box". In fact, some good 
translations will get worse scores than some 
bad ones, e.g., an acceptable one word trans- 
lation can even get a score of 0, e.g.,"SUPR" 
was translated as "DEL" by Microsoft and as 
"Delete" by NYU. Nevertheless, by averaging 
this accuracy score over many examples, it has 
proved a valuable measure for comparing differ- 
ent versions of a particular system: better sys- 
tems get better results. Similarly, after tweak- 
ing the system, a better translation of a partic- 
ular sentence will usually yield a better score. 
8 Future  Work  
Fnture work should address two limitations of 
our current system: (1) Bad parses yield bad 
transihr rules; and (2) sparse data limits the size 
of our transfer rule database and our options for 
542 
applying transfer ules selectively. To nttack the 
"bad parse" problem, we are eonsideriug using 
our MT system with less-detailed parsers, since 
these parsers typically produce less error-prone 
output. We will have to conduct exl)erimcnts 
to determine the minimum level of detM1 that 
is needed, a 
Previous to the work reported in this paper, 
we ran our MT system on bilinguM corpora in 
which the sentences were Migned manuMly. The 
cost of manuM aligmnent limited the size of the 
corpora we could use. A lot of our recent MT 
research as bo.en tbcused on solving this sparse 
data prol)lem through our develoi)ment of a sen- 
tence alignment progrmn (Meyers et al, 1998a). 
We now have 300,000 automaticMly aligned sen- 
tences in the Microsoft help text domain tbr fu- 
ture experiineni;s. In addition to provi(ting us 
with many more transfer ules, this shouhl Mlow 
us to colh'.ct transfer rule co-occurrence infor- 
mation which we c~m then use to apply tr;mstbr 
rules more effectively, perhaps improving trans- 
b~tion quality. In a preliminary experime, nt a- 
hmg these lines using the Experiment 1. tort)us, 
co-occurrence information had no noticeable f  
feet. However, we are hot)eflfl that flltm'e ex- 
t)eriments with 300,000 Migned sentences (300 
tinies as nnlch data) will 1)e more successful. 
Re ferences  
Robert J. Bobrow. 1990. S1;~Ltistical agenda 
parsing. In I)ARPA Speech and Lang'uagc 
Workshop, pages 222-224. 
Peter Brown~ Stephen A. Delb~ t)ietra, Vin- 
cent J. Della Pietra, and Robert L. Mer- 
cer. 1993. The Mathematics of Statistical 
M~zchine 'h'anslation: 1)arametcr Estimation. 
Computational Lin.quistics, 19:263-312. 
Sh;~ron A. Caraballo and Eugene Chm'niak. 
1997. New figures of merit tbr best-tirst prot)- 
M)ilistie chart parsing. Computational Lin- 
guistics, 24:275-298. 
Eugene Ctmrniak, Sharon Goldwater, and M~rk 
Johnson. 1998. Edge-Based Best-First Chart 
Parsing. In Proceedings of the Sixth Annual 
Workshop for Very Lawc Corpora, Montreal. 
SOne could set u 1) a contimmm from detailed parser- 
s like Proteus down to shallow verb-group/noun-grouI) 
recognizers, with the Penn treetmnk based parsers ly- 
ing somewhere in the middle. As one travels down t, he 
eonLinlmIn t;o t;he lower detail parsers, tim error rate nat- 
urally decreases. 
Mahesh V. Chitrao and RMph Gris}unan. 1990. 
St;~tisti('al pnrsing of messages. In \])AIIPA 
Speech and La'n,g'uagc Workshop, pages 263 
266. 
Annn Sggvall ltein. 1996. Pretbrence Mecha- 
nisms of the Multra Machine %'ansb~tion Sys- 
tem. In Barbara H. Partee and Petr Sgall, 
editors, Discourse and Meaning: Papers in 
11onor of Eva 11aji~ovd. John Benja.mins Pub- 
lishing Company, Amsterdam. 
Y. Matsumoto, H. Ishimoto, T. Utsuro, and 
M. Nagao. 1993. Structural Matching of 
Parallel Texts. In 31st Annual Meeting of 
the Association for Computational Linguis- 
tics: "Proceedings of the Uo~@rencc". 
Adam Meyers, Roman Ymlgm'ber, a.nd Ralph 
Grishman. 1996. Alignment of Shared 
Forests fi)r BilinguM Corpora. In Proceedings 
of Coliw.I 1996: The 16th International Con- 
fercncc on Computational Linguistics, l)ages 
460 465. 
Adam Meyers, Miehiko Kosak~, and Ralph Gr- 
ishman. 1998m A Multilingual Procedure 
for Dict;ionary-B;~sed Sentence Aligmnent. In 
Proceedings of AMTA '98: Machine Transla- 
tion and th, c ht:fo'rmation Soup, t)~ges 187. 
198. 
Adam Meyers, R,om~m Ym~g~rber, Ralph Gr- 
ishmml, Cntherine Macleod, mM Antonio 
Moreno-S~mdow~l. 1998|). l)eriving ~l~:a.ns- 
fin: Rules from Domimmce-Preserving Align- 
ments. In I)'rocccdim.ls o.f Coling-A CL98: Th.c 
171h International Conference on Computa- 
tional Ling,uistics and the 36th, Meeting of the 
Association for Computational Linguistics. 
Sergei Nirenlmrg mM Robert E. l~:ederking. 
1995. The Pangloss Mark III Machine 'l?nms- 
lt~tion System: Multi-Engine System Archi- 
tecture. Te(:hnical report, NMSU Oil,L, USC 
ISI, ;rod CMU CMT. 
Andrew Way, Ian Crookston, and Jane Shell;on. 
1997. A Typology of ~IYanslation Prol)lems 
for Eurotra Translation Machines. Machine 
\[l}'anslation, 12:323 374. 
543 
Covering Treebanks with GLARF
Adam Meyers
 
and Ralph Grishman   and Michiko Kosaka  and Shubin Zhao  
 
New York University, 719 Broadway, 7th Floor, NY, NY 10003 USA
 Monmouth University, West Long Branch, N.J. 07764, USA
meyers/grishman/shubinz@cs.nyu.edu, kosaka@monmouth.edu
Abstract
This paper introduces GLARF, a frame-
work for predicate argument structure.
We report on converting the Penn Tree-
bank II into GLARF by automatic
methods that achieved about 90% pre-
cision/recall on test sentences from the
Penn Treebank. Plans for a corpus
of hand-corrected output, extensions of
GLARF to Japanese and applications
for MT are also discussed.
1 Introduction
Applications using annotated corpora are often,
by design, limited by the information found in
those corpora. Since most English treebanks pro-
vide limited predicate-argument (PRED-ARG)
information, parsers based on these treebanks do
not produce more detailed predicate argument
structures (PRED-ARG structures). The Penn
Treebank II (Marcus et al, 1994) marks sub-
jects (SBJ), logical objects of passives (LGS),
some reduced relative clauses (RRC), as well as
other grammatical information, but does not mark
each constituent with a grammatical role. In our
view, a full PRED-ARG description of a sen-
tence would do just that: assign each constituent
a grammatical role that relates that constituent to
one or more other constituents in the sentence.
For example, the role HEAD relates a constituent
to its parent and the role OBJ relates a constituent
to the HEAD of its parent. We believe that the
absence of this detail limits the range of appli-
cations for treebank-based parsers. In particu-
lar, they limit the extent to which it is possible
to generalize, e.g., marking IND-OBJ and OBJ
roles allows one to generalize a single pattern to
cover two related examples (?John gave Mary a
book? = ?John gave a book to Mary?). Distin-
guishing complement PPs (COMP) from adjunct
PPs (ADV) is useful because the former is likely
to have an idiosyncratic interpretation, e.g., the
object of ?at? in ?John is angry at Mary? is not
a locative and should be distinguished from the
locative case by many applications.
In an attempt to fill this gap, we have begun
a project to add this information using both au-
tomatic procedures and hand-annotation. We are
implementing automatic procedures for mapping
the Penn Treebank II (PTB) into a PRED-ARG
representation and then we are correcting the out-
put of these procedures manually. In particular,
we are hoping to encode information that will en-
able a greater level of regularization across lin-
guistic structures than is possible with PTB.
This paper introduces GLARF, the Grammati-
cal and Logical Argument Representation Frame-
work. We designed GLARF with four objec-
tives in mind: (1) capturing regularizations ?
noncanonical constructions (e.g., passives, filler-
gap constructions, etc.) are represented in terms
of their canonical counterparts (simple declara-
tive clauses); (2) representing all phenomena us-
ing one simple data structure: the typed feature
structure (3) consistently labeling all arguments
and adjuncts for phrases with clear heads; and (4)
producing clear and consistent PRED-ARGs for
phrases that do not have heads, e.g., conjoined
structures, named entities, etc. ? rather than try-
ing to squeeze these phrases into an X-bar mold,
we customized our representations to reflect their
head-less properties. We believe that a framework
for PRED-ARG needs to satisfy these objectives
to adequately cover a corpus like PTB.
We believe that GLARF, because of its uni-
form treatment of PRED-ARG relations, will be
valuable for many applications, including ques-
tion answering, information extraction, and ma-
chine translation. In particular, for MT, we ex-
pect it will benefit procedures which learn trans-
lation rules from syntactically analyzed parallel
corpora, such as (Matsumoto et al, 1993; Mey-
ers et al, 1996). Much closer alignments will
be possible using GLARF, because of its multi-
ple levels of representation, than would be pos-
sible with surface structure alone (An example is
provided at the end of Section 2). For this reason,
we are currently investigating the extension of our
mapping procedure to treebanks of Japanese (the
Kyoto Corpus) and Spanish (the UAM Treebank
(Moreno et al, 2000)). Ultimately, we intend to
create a parallel trilingual treebank using a com-
bination of automatic methods and human correc-
tion. Such a treebank would be valuable resource
for corpus-trained MT systems.
The primary goal of this paper is to discuss the
considerations for adding PRED-ARG informa-
tion to PTB, and to report on the performance of
our mapping procedure. We intend to wait until
these procedures are mature before beginning an-
notation on a larger scale. We also describe our
initial research on covering the Kyoto Corpus of
Japanese with GLARF.
2 Previous Treebanks
There are several corpora annotated with PRED-
ARG information, but each encode some dis-
tinctions that are different. The Susanne Cor-
pus (Sampson, 1995) consists of about 1/6 of the
Brown Corpus annotated with detailed syntactic
information. Unlike GLARF, the Susanne frame-
work does not guarantee that each constituent be
assigned a grammatical role. Some grammatical
roles (e.g., subject, object) are marked explicitly,
others are implied by phrasetags (Fr corresponds
to the GLARF node label SBAR under a REL-
ATIVE arc label) and other constituents are not
assigned roles (e.g., constituents of NPs). Apart
from this concern, it is reasonable to ask why
we did not adapt this scheme for our use. Su-
sanne?s granularity surpasses PTB-based GLARF
in many areas with about 350 wordtags (part of
speech) and 100 phrasetags (phrase node labels).
However, GLARF would express many of the de-
tails in other ways, using fewer node and part of
speech (POS) labels and more attributes and role
labels. In the feature structure tradition, GLARF
can represent varying levels of detail by adding
or subtracting attributes or defining subsumption
hierarchies. Thus both Susanne?s NP1p word-
tag and Penn?s NNP wordtag would correspond
to GLARF?s NNP POS tag. A GLARF-style
Susanne analysis of ?Ontario, Canada? is (NP
(PROVINCE (NNP Ontario)) (PUNCTUATION
(, ,)) (COUNTRY (NNP Canada)) (PATTERN
NAME) (SEM-FEATURE LOC)). A GLARF-
style PTB analysis uses the roles NAME1 and
NAME2 instead of PROVINCE and COUNTRY,
where name roles (NAME1, NAME2) are more
general than PROVINCE and COUNTRY in a
subsumption hierarchy. In contrast, attempts to
convert PTB into Susanne would fail because de-
tail would be unavailable. Similarly, attempts to
convert Susanne into the PTB framework would
lose information. In summary, GLARF?s ability
to represent varying levels of detail allows dif-
ferent types of treebank formats to be converted
into GLARF, even if they cannot be converted into
each other. Perhaps, GLARF can become a lingua
franca among annotated treebanks.
The Negra Corpus (Brants et al, 1997) pro-
vides PRED-ARG information for German, simi-
lar in granularity to GLARF. The most significant
difference is that GLARF regularizes some phe-
nomena which a Negra version of English would
probably not, e.g., control phenomena. Another
novel feature of GLARF is the ability to represent
paraphrases (in the Harrisian sense) that are not
entirely syntactic, e.g., nominalizations as sen-
tences. Other schemes seem to only regularize
strictly syntactic phenomena.
3 The Structure of GLARF
In GLARF, each sentence is represented by a
typed feature structure. As is standard, we
model feature structures as single-rooted directed
acyclic graphs (DAGs). Each nonterminal is la-
beled with a phrase category, and each leaf is la-
beled with either: (a) a (PTB) POS label and a
word (eat, fish, etc.) or (b) an attribute value (e.g.,
singular, passive, etc.). Types are based on non-
terminal node labels, POSs and other attributes
(Carpenter, 1992). Each arc bears a feature label
which represents either a grammatical role (SBJ,
OBJ, etc.) or some attribute of a word or phrase
(morphological features, tense, semantic features,
etc.).1 For example, the subject of a sentence is
the head of a SBJ arc, an attribute like SINGU-
LAR is the head of a GRAM-NUMBER arc, etc.
A constituent involved in multiple surface or log-
ical relations may be at the head of multiple arcs.
For example, the surface subject (S-SBJ) of a pas-
sive verb is also the logical object (L-OBJ). These
two roles are represented as two arcs which share
the same head. This sort of structure sharing anal-
ysis originates with Relational Grammar and re-
lated frameworks (Perlmutter, 1984; Johnson and
Postal, 1980) and is common in Feature Structure
frameworks (LFG, HPSG, etc.). Following (John-
son et al, 1993)2, arcs are typed. There are five
different types of role labels:
 Attribute roles: Gram-Number (grammati-
cal number), Mood, Tense, Sem-Feature (se-
mantic features like temporal/locative), etc.
 Surface-only relations (prefixed with S-),
e.g., the surface subject (S-SBJ) of a passive.
 Logical-only Roles (prefixed with L-), e.g.,
the logical object (L-OBJ) of a passive.
 Intermediate roles (prefixed with I-) repre-
senting neither surface, nor logical positions.
In ?John seemed to be kidnapped by aliens?,
?John? is the surface subject of ?seem?, the
logical object of ?kidnapped?, and the in-
termediate subject of ?to be?. Intermedi-
ate arcs capture are helpful for modeling the
way sentences conform to constraints. The
intermediate subject arc obeys lexical con-
straints and connect the surface subjects of
?seem? (COMLEX Syntax class TO-INF-
RS (Macleod et al, 1998a)) to the subject
of the infinitive. However, the subject of the
infinitive in this case is not a logical sub-
ject due to the passive. In some cases, in-
termediate arcs are subject to number agree-
ment, e.g., in ?Which aliens did you say
were seen??, the I-SBJ of ?were seen? agrees
with ?were?.
 Combined surface/logical roles (unprefixed
arcs, which we refer to as SL- arcs). For ex-
1A few grammatical roles are nonfunctional, e.g., a con-
stituent can have multiple ADV constituents. We number
these roles (ADV1, ADV2,  ) to preserve functionality.
2That paper uses two arc types: category and relational.
ample, ?John? in ?John ate cheese? would be
the target of a SBJ subject arc.
Logical relations, encoded with SL- and L-
arcs, are defined more broadly in GLARF than
in most frameworks. Any regularization from a
non-canonical linguistic structure to a canonical
one results in logical relations. Following (Harris,
1968) and others, our model of canonical linguis-
tic structure is the tensed active indicative sen-
tence with no missing arguments. The following
argument types will be at the head of logical (L-)
arcs based on counterparts in canonical sentences
which are at the head of SL- arcs: logical argu-
ments of passives, understood subjects of infini-
tives, understood fillers of gaps, and interpreted
arguments of nominalizations (In ?Rome?s de-
struction of Carthage?, ?Rome? is the logical sub-
ject and ?Carthage? is the logical object). While
canonical sentence structure provides one level
of regularization, canonical verb argument struc-
tures provide another. In the case of argument al-
ternations (Levin, 1993), the same role marks an
alternating argument regardless of where it occurs
in a sentence. Thus ?the man? is the indirect ob-
ject (IND-OBJ) and ?a dollar? is the direct object
(OBJ) in both ?She gave the man a dollar? and
?She gave a dollar to the man? (the dative alter-
nation). Similarly, ?the people? is the logical ob-
ject (L-OBJ) of both ?The people evacuated from
the town? and ?The troops evacuated the people
from the town?, when we assume the appropriate
regularization. Encoding this information allows
applications to generalize. For example, a single
Information Extraction pattern that recognizes the
IND-OBJ/OBJ distinction would be able to han-
dle these two examples. Without this distinction,
2 patterns would be needed.
Due to the diverse types of logical roles, we
sub-type roles according to the type of regu-
larization that they reflect. Depending on the
application, one can apply different filters to a
detailed GLARF representation, only looking at
certain types of arcs. For example, one might
choose all logical (L- and SL-) roles for an
application that is trying to acquire selection
restrictions, or all surface (S- and SL-) roles
if one was interested in obtaining a surface
parse. For other applications, one might want to
choose between subtypes of logical arcs. Given
(S (NP-SBJ (PRP they))
(VP (VP (VBD spent)
(NP-2 ($ $)
(CD 325,000)
(-NONE- *U*))
(PP-TMP-3 (IN in)
(NP (CD 1989))))
(CC and)
(VP (NP=2 ($ $)
(CD 340,000)
(-NONE- *U*))
(PP-TMP=3 (IN in)
(NP (CD 1990))))))
Figure 1: Penn representation of gapping
a trilingual treebank, suppose that a Spanish
treebank sentence corresponds to a Japanese
nominalization phrase and an English nominal-
ization phrase, e.g.,
Disney ha comprado Apple Computers
Disney?s acquisition of Apple Computers
Furthermore, suppose that the English treebank
analyzes the nominalization phrase both as an
NP (Disney = possessive, Apple Computers =
object of preposition) and as a paraphrase of a
sentence (Disney = subject, Apple Computers
= object). For an MT system that aligns the
Spanish and English graph representation, it
may be useful to view the nominalization phrase
in terms of the clausal arguments. However,
in a Japanese/English system, we may only
want to look at the structure of the English
nominalization phrase as an NP.
4 GLARF and the Penn Treebank
This section focuses on some characteristics of
English GLARF and how we map PTB into
GLARF, as exemplified by mapping the PTB rep-
resentation in Figure 1 to the GLARF representa-
tion in Figure 2. In the process, we will discuss
how some of the more interesting linguistic phe-
nomena are represented in GLARF.
4.1 Mapping into GLARF
Our procedure for mapping PTB into GLARF
uses a sequence of transformations. The first
transformation applies to PTB, and the out-
put of each 	

 is the input of
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 146?154,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Automatic Recognition of Logical Relations for English, Chinese and
Japanese in the GLARF Framework
Adam Meyers?, Michiko Kosaka?, Nianwen Xue?, Heng Ji?, Ang Sun?, Shasha Liao? and Wei Xu?
? New York University, ?Monmouth University, ?Brandeis University, ? City University of New York
Abstract
We present GLARF, a framework for repre-
senting three linguistic levels and systems for
generating this representation. We focus on a
logical level, like LFG?s F-structure, but com-
patible with Penn Treebanks. While less fine-
grained than typical semantic role labeling ap-
proaches, our logical structure has several ad-
vantages: (1) it includes all words in all sen-
tences, regardless of part of speech or seman-
tic domain; and (2) it is easier to produce ac-
curately. Our systems achieve 90% for En-
glish/Japanese News and 74.5% for Chinese
News ? these F-scores are nearly the same as
those achieved for treebank-based parsing.
1 Introduction
For decades, computational linguists have paired a
surface syntactic analysis with an analysis represent-
ing something ?deeper?. The work of Harris (1968),
Chomsky (1957) and many others showed that one
could use these deeper analyses to regularize differ-
ences between ways of expressing the same idea.
For statistical methods, these regularizations, in ef-
fect, reduce the number of significant differences be-
tween observable patterns in data and raise the fre-
quency of each difference. Patterns are thus easier
to learn from training data and easier to recognize in
test data, thus somewhat compensating for the spare-
ness of data. In addition, deeper analyses are often
considered semantic in nature because conceptually,
two expressions that share the same regularized form
also share some aspects of meaning. The specific de-
tails of this ?deep? analysis have varied quite a bit,
perhaps more than surface syntax.
In the 1970s and 1980s, Lexical Function Gram-
mar?s (LFG) way of dividing C-structure (surface)
and F-structure (deep) led to parsers such as (Hobbs
and Grishman, 1976) which produced these two lev-
els, typically in two stages. However, enthusiasm
for these two-stage parsers was eclipsed by the ad-
vent of one stage parsers with much higher accu-
racy (about 90% vs about 60%), the now-popular
treebank-based parsers including (Charniak, 2001;
Collins, 1999) and many others. Currently, many
different ?deeper? levels are being manually anno-
tated and automatically transduced, typically using
surface parsing and other processors as input. One
of the most popular, semantic role labels (annota-
tion and transducers based on the annotation) char-
acterize relations anchored by select predicate types
like verbs (Palmer et al, 2005), nouns (Meyers et
al., 2004a), discourse connectives (Miltsakaki et al,
2004) or those predicates that are part of particular
semantic frames (Baker et al, 1998). The CONLL
tasks for 2008 and 2009 (Surdeanu et al, 2008;
Hajic? et al, 2009) has focused on unifying many of
these individual efforts to produce a logical structure
for multiple parts of speech and multiple languages.
Like the CONLL shared task, we link surface lev-
els to logical levels for multiple languages. How-
ever, there are several differences: (1) The logical
structures produced automatically by our system can
be expected to be more accurate than the compara-
ble CONLL systems because our task involves pre-
dicting semantic roles with less fine-grained distinc-
tions. Our English and Japanese results were higher
than the CONLL 2009 SRL systems. Our English F-
scores range from 76.3% (spoken) to 89.9% (News):
146
the best CONLL 2009 English scores were 73.31%
(Brown) and 85.63% (WSJ). Our Japanese system
scored 90.6%: the best CONLL 2009 Japanese score
was 78.35%. Our Chinese system 74.5%, 4 points
lower than the best CONLL 2009 system (78.6%),
probably due to our system?s failings, rather than the
complexity of the task; (2) Each of the languages
in our system uses the same linguistic framework,
using the same types of relations, same analyses of
comparable constructions, etc. In one case, this re-
quired a conversion from a different framework to
our own. In contrast, the 2009 CONLL task puts
several different frameworks into one compatible in-
put format. (3) The logical structures produced by
our system typically connect all the words in the sen-
tence. While this is true for some of the CONLL
2009 languages, e.g., Czech, it is not true about
all the languages. In particular, the CONLL 2009
English and Chinese logical structures only include
noun and verb predicates.
In this paper, we will describe the GLARF frame-
work (Grammatical and Logical Representation
Framework) and a system for producing GLARF
output (Meyers et al, 2001; Meyers, 2008). GLARF
provides a logical structure for English, Chinese and
Japanese with an F-score that is within a few per-
centage points of the best parsing results for that
language. Like LFG?s (LFG) F-structure, our log-
ical structure is less fine-grained than many of the
popular semantic role labeling schemes, but also has
two main advantages over these schemes: it is more
reliable and it is more comprehensive in the sense
that it covers all parts of speech and the resulting
logical structure is a connected graph. Our approach
has proved adequate for three genetically unrelated
natural languages: English, Chinese and Japanese.
It is thus a good candidate for additional languages
with accurate parsers.
2 The GLARF framework
Our system creates a multi-tiered representation in
the GLARF framework, combining the theory un-
derlying the Penn Treebank for English (Marcus et
al., 1994) and Chinese (Xue et al, 2005) (Chom-
skian linguistics of the 1970s and 1980s) with: (2)
Relational Grammar?s graph-based way of repre-
senting ?levels? as sequences of relations; (2) Fea-
ture structures in the style of Head-Driven Phrase
Structure Grammar; and (3) The Z. Harris style goal
of attempting to regularize multiple ways of saying
the same thing into a single representation. Our
approach differs from LFG F-structure in several
ways: we have more than two levels; we have a
different set of relational labels; and finally, our ap-
proach is designed to be compatible with the Penn
Treebank framework and therefore, Penn-Treebank-
based parsers. In addition, the expansion of our the-
ory is governed more by available resources than by
the underlying theory. As our main goal is to use
our system to regularize data, we freely incorporate
any analysis that fits this goal. Over time, we have
found ways of incorporating Named Entities, Prop-
Bank, NomBank and the Penn Discourse Treebank.
Our agenda also includes incorporating the results of
other research efforts (Pustejovsky et al, 2005).
For each sentence, we generate a feature structure
(FS) representing our most complete analysis. We
distill a subset of this information into a dependency
structure governed by theoretical assumptions, e.g.,
about identifying functors of phrases. Each GLARF
dependency is between a functor and an argument,
where the functor is the head of a phrase, conjunc-
tion, complementizer, or other function word. We
have built applications that use each of these two
representations, e.g., the dependency representation
is used in (Shinyama, 2007) and the FS represen-
tation is used in (K. Parton and K. R. McKeown
and R. Coyne and M. Diab and R. Grishman and
D. Hakkani-Tu?r and M. Harper and H. Ji and W. Y.
Ma and A. Meyers and S. Stolbach and A. Sun and
G. Tu?r and W. Xu and S. Yarman, 2009).
In the dependency representation, each sentence
is a set of 23 tuples, each 23-tuple characterizing up
to three relations between two words: (1) a SUR-
FACE relation, the relation between a functor and an
argument in the parse of a sentence; (2) a LOGIC1
relation which regularizes for lexical and syntac-
tic phenomena like passive, relative clauses, deleted
subjects; and (3) a LOGIC2 relation corresponding
to relations in PropBank, NomBank, and the Penn
Discourse Treebank (PDTB). While the full output
has all this information, we will limit this paper to
a discussion of the LOGIC1 relations. Figure 1 is
a 5 tuple subset of the 23 tuple GLARF analysis of
the sentence Who was eaten by Grendel? (The full
147
L1 Surf L2 Func Arg
NIL SENT NIL Who was
PRD PRD NIL was eaten
COMP COMP ARG0 eaten by
OBJ NIL ARG1 eaten Who
NIL OBJ NIL by Grendel
SBJ NIL NIL eaten Grendel
Figure 1: 5-tuples: Who was eaten by Grendel
Who
eaten
was
by
PRD
S?OBJ
L?OBJ
ARG1
COMP
ARG0
S?SENT
L?SBJ
Grendel
Figure 2: Graph of Who was eaten by Grendel
23 tuples include unique ids and fine-grained lin-
guistic features). The fields listed are: logic1 label
(L1), surface label (Surf), logic2 label (L2), func-
tor (Func) and argument (Arg). NIL indicates that
there is no relation of that type. Figure 2 repre-
sents this as a graph. For edges with two labels,
the ARG0 or ARG1 label indicates a LOGIC2 re-
lation. Edges with an L- prefix are LOGIC1 la-
bels (the edges are curved); edges with S-prefixes
are SURFACE relations (the edges are dashed); and
other (thick) edges bear unprefixed labels represent-
ing combined SURFACE/LOGIC1 relations. Delet-
ing the dashed edges yields a LOGIC1 representa-
tion; deleting the curved edges yields a SURFACE
representation; and a LOGIC2 consists of the edges
labeled ARGO and ARG1 relations, plus the sur-
face subtrees rooted where the LOGIC2 edges ter-
minate. Taken together, a sentence?s SURFACE re-
lations form a tree; the LOGIC1 relations form a
directed acyclic graph; and the LOGIC2 relations
form directed graphs with some cycles and, due to
PDTB relations, may connect sentences to previous
ones, e.g., adverbs like however, take the previous
sentence as one of their arguments.
LOGIC1 relations (based on Relational Gram-
mar) regularize across grammatical and lexical al-
ternations. For example, subcategorized verbal ar-
guments include: SBJect, OBJect and IND-OBJ (in-
direct Object), COMPlement, PRT (Particle), PRD
(predicative complement). Other verbal modifiers
include AUXilliary, PARENthetical, ADVerbial. In
contrast, FrameNet and PropBank make finer dis-
tinctions. Both PP arguments of consulted in John
consulted with Mary about the project bear COMP
relations with the verb in GLARF, but would have
distinct labels in both PropBank and FrameNet.
Thus Semantic Role Labeling (SRL) should be more
difficult than recognizing LOGIC1 relations.
Beginning with Penn Treebank II, Penn Treebank
annotation includes Function tags, hyphenated addi-
tions to phrasal categories which indicate their func-
tion. There are several types of function tags:
? Argument Tags such as SBJ, OBJ, IO (IND-
OBJ), CLR (COMP) and PRD?These are lim-
ited to verbal relations and not all are used in
all treebanks. For example, OBJ and IO are
used in the Chinese, but not the English tree-
bank. These labels can often be directly trans-
lated into GLARF LOGIC1 relations.
? Adjunct Tags such as ADV, TMP, DIR, LOC,
MNR, PRP?These tags often translate into a
single LOGIC1 tag (ADV). However, some of
these also correspond to LOGIC1 arguments.
In particular, some DIR and MNR tags are re-
alized as LOGIC1 COMP relations (based on
dictionary entries). The fine grained seman-
tic distinctions are maintained in other features
that are part of the GLARF description.
In addition, GLARF treats Penn?s PRN phrasal
category as a relation rather than a phrasal category.
For example, given a sentence like, Banana ketchup,
the agency claims, is very nutritious, the phrase
the agency claims is analyzed as an S(entence) in
GLARF bearing a (surface) PAREN relation to the
main clause. Furthermore, the whole sentence is a
COMP of the verb claims. Since PAREN is a SUR-
FACE relation, not a LOGIC1 relation, there is no
LOGIC1 cycle as shown by the set of 5-tuples in
Figure 3? a cycle only exists if you include both
SURFACE and LOGIC1 relations in a single graph.
Another important feature of the GLARF frame-
work is transparency, a term originating from N.
148
L1 Surf L2 Func Arg
NIL SBJ ARG1 is ketchup
PRD PRD ARG2 is nutritious
SBJ NIL NIL nutritious Ketchup
ADV ADV NIL nutritious very
N-POS N-POS NIL ketchup Banana
NIL PAREN NIL is claims
SBJ SBJ ARG0 claims agency
Q-POS Q-POS NIL agency the
COMP NIL ARG1 claims is
Figure 3: 5-tuples: Banana Ketchup, the agency claims,
is very nutritious
L1 Surf L2 Func Arg
SBJ SBJ ARG0 ate and
OBJ OBJ ARG1 ate box
CONJ CONJ NIL and John
CONJ CONJ NIL and Mary
COMP COMP NIL box of
Q-POS Q-POS NIL box the
OBJ OBJ NIL of cookies
Figure 4: 5-tuples: John and Mary ate the box of cookies
Sager?s unpublished work. A relation between two
words is transparent if: the functor fails to character-
ize the selectional properties of the phrase (or sub-
graph in a Dependency Analysis), but its argument
does. For example, relations between conjunctions
(e.g., and, or, but) and their conjuncts are transparent
CONJ relations. Thus although and links together
John and Mary, it is these dependents that deter-
mine that the resulting phrase is noun-like (an NP
in phrase structure terminology) and sentient (and
thus can occur as the subject of verbs like ate). An-
other common example of transparent relations are
the relations connecting certain nouns and the prepo-
sitional objects under them, e.g., the box of cookies
is edible, because cookies are edible even though
boxes are not. These features are marked in the
NOMLEX-PLUS dictionary (Meyers et al, 2004b).
In Figure 4, we represent transparent relations, by
prefixing the LOGIC1 label with asterisks.
The above description most accurately describes
English GLARF. However, Chinese GLARF has
most of the same properties, the main exception be-
ing that PDTB arguments are not currently marked.
For Japanese, we have only a preliminary represen-
tation of LOGIC2 relations and they are not derived
from PropBank/NomBank/PDTB.
2.1 Scoring the LOGIC1 Structure
For purposes of scoring, we chose to focus on
LOGIC1 relations, our proposed high-performance
level of semantics. We scored with respect to: the
LOGIC1 relational label, the identity of the functor
and the argument, and whether the relation is trans-
parent or not. If the system output differs in any of
these respects, the relation is marked wrong. The
following sections will briefly describe each system
and present an evaluation of its results.
The answer keys for each language were created
by native speakers editing system output, as repre-
sented similarly to the examples in this paper, al-
though part of speech is included for added clar-
ity. In addition, as we attempted to evaluate logi-
cal relation (or dependency) accuracy independent
of sentence splitting. We obtained sentence divi-
sions from data providers and treebank annotation
for all the Japanese and most of the English data, but
used automatic sentence divisions for the English
BLOG data. For the Chinese, we omitted several
sentences from our evaluation set due to incorrect
sentence splits. The English and Japanese answer
keys were annotated by single native speakers ex-
pert in GLARF. The Chinese data was annotated by
several native speakers and may have been subject
to some interannotator agreement difficulties, which
we intend to resolve in future work. Currently, cor-
recting system output is the best way to create an-
swer keys due to certain ambiguities in the frame-
work, some of which we hope to incorporate into fu-
ture scoring procedures. For example, consider the
interpretation of the phrase five acres of land in Eng-
land with respect to PP attachment. The difference
in meaning between attaching the PP in England
to acres or to land is too subtle for these authors?
we have difficulty imagining situations where one
statement would be accurate and the other would
not. This ambiguity is completely predictable be-
cause acres is a transparent noun and similar ambi-
guities hold for all such cases where a transparent
noun takes a complement and is followed by a PP
attachment. We believe that a more complex scor-
ing program could account for most of these cases.
149
Similar complexities arise for coordination and sev-
eral other phenomena.
3 English GLARF
We generate English GLARF output by applying a
procedure that combines:
1. The output of the 2005 version of the Charniak
parser described in (Charniak, 2001), which
label precision and recall scores in the 85%
range. The updated version of the parser seems
to perform closer to 90% on News data and per-
form lower on other genres. That performance
would reflect reports on other versions of the
Charniak parser for which statistics are avail-
able (Foster and van Genabith, 2008).
2. Named entity (NE) tags from the JET NE sys-
tem (Ji and Grishman, 2006), which achieves
F-scores ranging 86%-91% on newswire for
both English and Chinese (depending on
Epoch). The JET system identifies seven
classes of NEs: Person, GPE, Location, Orga-
nization, Facility, Weapon and Vehicle.
3. Machine Readable dictionaries: COMLEX
(Macleod et al, 1998), NOMBANK dictio-
naries (from http://nlp.cs.nyu.edu/
meyers/nombank/) and others.
4. A sequence of hand-written rules (citations
omitted) such that: (1) the first set of rules con-
vert the Penn Treebank into a Feature Structure
representation; and (2) each rule N after the
first rule is applied to an entire Feature Struc-
ture that is the output of rule N ? 1.
For this paper, we evaluated the English output for
several different genres, all of which approximately
track parsing results for that genre. For written
genres, we chose between 40 and 50 sentences.
For speech transcripts, we chose 100 sentences?we
chose this larger number because a lot of so-called
sentences contained text with empty logical de-
scriptions, e.g., single word utterances contain no
relations between pairs of words. Each text comes
from a different genre. For NEWS text, we used 50
sentences from the aligned Japanese-English data
created as part of the JENAAD corpus (Utiyama
Genre Prec Rec F
NEWS 731815 = 89.7%
715
812 = 90.0% 89.9%
BLOG 704844 = 83.4%
704
899 = 78.3% 80.8%
LETT 392434 = 90.3%
392
449 = 87.3% 88.8%
TELE 472604 = 78.1%
472
610 = 77.4% 77.8%
NARR 732959 = 76.3%
732
964 = 75.9% 76.1%
Table 1: English Aggregate Scores
Corpus Prec Rec F Sents
NEWS 90.5% 90.8% 90.6% 50
BLOG 84.1% 79.6% 81.7% 46
LETT 93.9% 89.2% 91.4% 46
TELE 81.4% 83.2% 84.9% 103
NARR 77.1% 78.1% 79.5% 100
Table 2: English Score per Sentence
and Isahara, 2003); the web text (BLOGs) was
taken from some corpora provided by the Linguistic
Data Consortium through the GALE (http:
//projects.ldc.upenn.edu/gale/) pro-
gram; the LETTer genre (a letter from Good Will)
was taken from the ICIC Corpus of Fundraising
Texts (Indiana Center for Intercultural Communi-
cation); Finally, we chose two spoken language
transcripts: a TELEphone conversation from
the Switchboard Corpus (http://www.ldc.
upenn.edu/Catalog/readme_files/
switchboard.readme.html) and one NAR-
Rative from the Charlotte Narrative and Conversa-
tion Collection (http://newsouthvoices.
uncc.edu/cncc.php). In both cases, we
assumed perfect sentence splitting (based on Penn
Treebank annotation). The ICIC, Switchboard
and Charlotte texts that we used are part of the
Open American National Corpus (OANC), in
particular, the SIGANN shared subcorpus of the
OANC (http://nlp.cs.nyu.edu/wiki/
corpuswg/ULA-OANC-1) (Meyers et al, 2007).
Comparable work for English includes: (1) (Gab-
bard et al, 2006), a system which reproduces the
function tags of the Penn Treebank with 89% accu-
racy and empty categories (and their antecedents)
with varying accuracies ranging from 82.2% to
96.3%, excluding null complementizers, as these are
theory-internal and have no value for filling gaps.
(2) Current systems that generate LFG F-structure
150
such as (Wagner et al, 2007) which achieve an F
score of 91.1 on the F-structure PRED relations,
which are similar to our LOGIC1 relations.
4 Chinese GLARF
The Chinese GLARF program takes a Chinese
Treebank-style syntactic parse and the output of a
Chinese PropBanker (Xue, 2008) as input, and at-
tempts to determine the relations between the head
and its dependents within each constituent. It does
this by first exploiting the structural information
and detecting six broad categories of syntactic rela-
tions that hold between the head and its dependents.
These are predication, modification, complementa-
tion, coordination, auxiliary, and flat. Predication
holds at the clause level between the subject and the
predicate, where the predicate is considered to be
the head and the subject is considered to the depen-
dent. Modification can also hold mainly within NPs
and VPs, where the dependents are modifiers of the
NP head or adjuncts to the head verb. Coordination
holds almost for all phrasal categories where each
non-punctuation child within this constituent is ei-
ther conjunction or a conjunct. The head in a co-
ordination structure is underspecified and can be ei-
ther a conjunct or a conjunction depending on the
grammatical framework. Complementation holds
between a head and its complement, with the com-
plement usually being a core argument of the head.
For example, inside a PP, the preposition is the head
and the phrase or clause it takes is the dependent. An
auxiliary structure is one where the auxiliary takes
a VP as its complement. This structure is identi-
fied so that the auxiliary and the verb it modifies can
form a verb group in the GLARF framework. Flat
structures are structures where a constituent has no
meaningful internal structure, which is possible in a
small number of cases. After these six broad cate-
gories of relations are identified, more fine-grained
relation can be detected with additional information.
Figure 5 is a sample 4-tuple for a Chinese translation
of the sentence in figure 3.
For the results reported in Table 3, we used the
Harper and Huang parser described in (Harper and
Huang, Forthcoming) which can achieve F-scores
as high as 85.2%, in combination with informa-
tion about named entities from the output of the
Figure 5: Agency claims, Banana Ketchup is very have
nutrition DE.
JET Named Entity tagger for Chinese (86%-91% F-
measure as per section 3). We used the NE tags to
adjust the parts of speech and the phrasal boundaries
of named entities (we do the same with English).
As shown in Table 3, we tried two versions of the
Harper and Huang parser, one which adds function
tags to the output and one that does not. The Chinese
GLARF system scores significantly (13.9% F-score)
higher given function tagged input, than parser out-
put without function tags. Our current score is about
10 points lower than the parser score. Our initial er-
ror analysis suggests that the most common forms
of errors involve: (1) the processing of long NPs;
(2) segmentation and POS errors; (3) conjunction
scope; and (4) modifier attachment.
5 Japanese GLARF
For Japanese, we process text with the KNP parser
(Kurohashi and Nagao, 1998) and convert the output
into the GLARF framework. The KNP/Kyoto Cor-
pus framework is a Japanese-specific Dependency
framework, very different from the Penn Treebank
framework used for the other systems. Process-
ing in Japanese proceeds as follows: (1) we pro-
cess the Japanese with the Juman segmenter (Kuro-
151
Type Prec Rec F
No Function Tags Version
Aggr 8431374 = 61.4%
843
1352 = 62.4% 61.8%
Aver 62.3% 63.5% 63.6%
Function Tags Version
Aggr 10311415 = 72.9%
1031
1352 = 76.3% 74.5%
Aver 73.0% 75.3% 74.9%
Table 3: 53 Chinese Newswire Sentences: Aggregate and
Average Sentence Scores
hashi et al, 1994) and KNP parser 2.0 (Kurohashi
and Nagao, 1998), which has reported accuracy of
91.32% F score for dependency accuracy, as re-
ported in (Noro et al, 2005). As is standard in
Japanese linguistics, the KNP/Kyoto Corpus (K)
framework uses a dependency analysis that has some
features of a phrase structure analysis. In partic-
ular, the dependency relations are between bun-
setsu, small constituents which include a head word
and some number of modifiers which are typically
function words (particles, auxiliaries, etc.), but can
also be prenominal noun modifiers. Bunsetsu can
also include multiple words in the case of names.
The K framework differentiates types of dependen-
cies into: the normal head-argument variety, coor-
dination (or parallel) and apposition. We convert
the head-argument variety of dependency straight-
forwardly into a phrase consisting of the head and
all the arguments. In a similar way, appositive re-
lations could be represented using an APPOSITIVE
relation (as is currently done with English). In the
case of bunsetsu, the task is to choose a head and
label the other constituents?This is very similar to
our task of labeling and subdividing the flat noun
phrases of the English Penn Treebank. Conjunction
is a little different because the K analysis assumes
that the final conjunct is the functor, rather than a
conjunction. We automatically changed this analy-
sis to be the same as it is for English and Chinese.
When there was no actual conjunction, we created a
theory-internal NULL conjunction. The final stages
include: (1) processing conjunction and apposition,
including recognizing cases that the parser does not
recognize; (2) correcting parts of speech; (3) label-
ing all relations between arguments and heads; (4)
recognizing and labeling special constituent types
Figure 6: It is the state?s duty to protect lives and assets.
Type Prec Rec F
Aggr 764843 = 91.0%
764
840 = 90.6% 90.8%
Aver 90.7% 90.6% 90.6%
Table 4: 40 Japanese Sentences from JENAA Corpus:
Aggregate and Average Sentence Scores
such as Named Entities, double quote constituents
and number phrases (twenty one); (5) handling com-
mon idioms; and (6) processing light verb and cop-
ula constructions.
Figure 6 is a sample 4-tuple for a Japanese
sentence meaning It is the state?s duty to protect
lives and assets. Conjunction is handled as dis-
cussed above, using an invisible NULL conjunction
and transparent (asterisked) logical CONJ relations.
Copulas in all three languages take surface subjects,
which are the LOGIC1 subjects of the PRD argu-
ment of the copula. We have left out glosses for the
particles, which act solely as case markers and help
us identify the grammatical relation.
We scored Japanese GLARF on forty sentences of
the Japanese side of the JENAA data (25 of which
are parallel with the English sentences scored). Like
the English, the F score is very close to the parsing
scores achieved by the parser.
152
6 Concluding Remarks and Future Work
In this paper, we have described three systems
for generating GLARF representations automati-
cally from text, each system combines the out-
put of a parser and possibly some other processor
(segmenter, Named Entity Recognizer, PropBanker,
etc.) and creates a logical representation of the sen-
tence. Dictionaries, word lists, and various other
resources are used, in conjunction with hand writ-
ten rules. In each case, the results are very close to
parsing accuracy. These logical structures are in the
same annotation framework, using the same labeling
scheme and the same analysis for key types of con-
structions. There are several advantages to our ap-
proach over other characterizations of logical struc-
ture: (1) our representation is among the most accu-
rate and reliable; (2) our representation connects all
the words in the sentence; and (3) having the same
representation for multiple languages facilitates run-
ning the same procedures in multiple languages and
creating multilingual applications.
The English system was developed for the News
genre, specifically the Penn Treebank Wall Street
Journal Corpus. We are therefore considering
adding rules to better handle constructions that ap-
pear in other genres, but not news. The experi-
ments describe here should go a long way towards
achieving this goal. We are also considering ex-
periments with parsers tailored to particular genres
and/or parsers that add function tags (Harper et al,
2005). In addition, our current GLARF system uses
internal Propbank/NomBank rules, which have good
precision, but low recall. We expect that we achieve
better results if we incorporate the output of state
of the art SRL systems, although we would have to
conduct experiments as to whether or not we can im-
prove such results with additional rules.
We developed the English system over the course
of eight years or so. In contrast, the Chinese and
Japanese systems are newer and considerably less
time was spent developing them. Thus they cur-
rently do not represent as many regularizations. One
obstacle is that we do not currently use subcate-
gorization dictionaries for either language, while
we have several for English. In particular, these
would be helpful in predicting and filling relative
clause and others gaps. We are considering auto-
matically acquiring simple dictionaries by recording
frequently occurring argument types of verbs over
a larger corpus, e.g., along the lines of (Kawahara
and Kurohashi, 2002). In addition, existing Japanese
dictionaries such as the IPAL (monolingual) dictio-
nary (technology Promotion Agency, 1987) or previ-
ously acquired case information reported in (Kawa-
hara and Kurohashi, 2002).
Finally, we are investigating several avenues for
using this system output for Machine Translation
(MT) including: (1) aiding word alignment for other
MT system (Wang et al, 2007); and (2) aiding the
creation various MT models involving analyzed text,
e.g., (Gildea, 2004; Shen et al, 2008).
Acknowledgments
This work was supported by NSF Grant IIS-
0534700 Structure Alignment-based MT.
References
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The
Berkeley FrameNet Project. In Coling-ACL98, pages
86?90.
E. Charniak. 2001. Immediate-head parsing for language
models. In ACL 2001, pages 116?123.
N. Chomsky. 1957. Syntactic Structures. Mouton, The
Hague.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
J. Foster and J. van Genabith. 2008. Parser Evaluation
and the BNC: 4 Parsers and 3 Evaluation Metrics. In
LREC 2008, Marrakech, Morocco.
R. Gabbard, M. Marcus, and S. Kulick. 2006. Fully pars-
ing the penn treebank. In NAACL/HLT, pages 184?
191.
D. Gildea. 2004. Dependencies vs. Constituents for
Tree-Based Alignment. In EMNLP, Barcelona.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,
M. A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?,
J. ?Ste?pa?nek, P. Stran?a?k, M. Surdeanu, N. Xue, and
Y. Zhang. 2009. The CoNLL-2009 shared task:
Syntactic and semantic dependencies in multiple lan-
guages. In CoNLL-2009, Boulder, Colorado, USA.
M. Harper and Z. Huang. Forthcoming. Chinese Statis-
tical Parsing. In J. Olive, editor, Global Autonomous
Language Exploitation. Publisher to be Announced.
M. Harper, B. Dorr, J. Hale, B. Roark, I. Shafran,
M. Lease, Y. Liu, M. Snover, L. Yung, A. Krasnyan-
skaya, and R. Stewart. 2005. Parsing and Spoken
153
Structural Event. Technical Report, The John-Hopkins
University, 2005 Summer Research Workshop.
Z. Harris. 1968. Mathematical Structures of Language.
Wiley-Interscience, New York.
J. R. Hobbs and R. Grishman. 1976. The Automatic
Transformational Analysis of English Sentences: An
Implementation. International Journal of Computer
Mathematics, 5:267?283.
H. Ji and R. Grishman. 2006. Analysis and Repair of
Name Tagger Errors. In COLING/ACL 2006, Sydney,
Australia.
K. Parton and K. R. McKeown and R. Coyne and M. Diab
and R. Grishman and D. Hakkani-Tu?r and M. Harper
and H. Ji and W. Y. Ma and A. Meyers and S. Stol-
bach and A. Sun and G. Tu?r and W. Xu and S. Yarman.
2009. Who, What, When, Where, Why? Comparing
Multiple Approaches to the Cross-Lingual 5W Task.
In ACL 2009.
D. Kawahara and S. Kurohashi. 2002. Fertilization
of Case Frame Dictionary for Robust Japanese Case
Analysis. In Proc. of COLING 2002.
S. Kurohashi and M. Nagao. 1998. Building a Japanese
parsed corpus while improving the parsing system. In
Proceedings of The 1st International Conference on
Language Resources & Evaluation, pages 719?724.
S. Kurohashi, T. Nakamura, Y. Matsumoto, and M. Na-
gao. 1994. Improvements of Japanese Morpholog-
ical Analyzer JUMAN. In Proc. of International
Workshop on Sharable Natural Language Resources
(SNLR), pages 22?28.
C. Macleod, R. Grishman, and A. Meyers. 1998. COM-
LEX Syntax. Computers and the Humanities, 31:459?
481.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
A. Meyers, M. Kosaka, S. Sekine, R. Grishman, and
S. Zhao. 2001. Parsing and GLARFing. In Proceed-
ings of RANLP-2001, Tzigov Chark, Bulgaria.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004a. The Nom-
Bank Project: An Interim Report. In NAACL/HLT
2004 Workshop Frontiers in Corpus Annotation,
Boston.
A. Meyers, R. Reeves, Catherine Macleod, Rachel Szeke-
ley, Veronkia Zielinska, and Brian Young. 2004b. The
Cross-Breeding of Dictionaries. In Proceedings of
LREC-2004, Lisbon, Portugal.
A. Meyers, N. Ide, L. Denoyer, and Y. Shinyama. 2007.
The shared corpora working group report. In Pro-
ceedings of The Linguistic Annotation Workshop, ACL
2007, pages 184?190, Prague, Czech Republic.
A. Meyers. 2008. Using treebank, dictionaries and
glarf to improve nombank annotation. In Proceedings
of The Linguistic Annotation Workshop, LREC 2008,
Marrakesh, Morocco.
E. Miltsakaki, A. Joshi, R. Prasad, and B. Webber. 2004.
Annotating discourse connectives and their arguments.
In A. Meyers, editor, NAACL/HLT 2004 Workshop:
Frontiers in Corpus Annotation, pages 9?16, Boston,
Massachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
T. Noro, C. Koike, T. Hashimoto, T. Tokunaga, and
Hozumi Tanaka. 2005. Evaluation of a Japanese CFG
Derived from a Syntactically Annotated corpus with
Respect to Dependency Measures. In 2005 Workshop
on Treebanks and Linguistic theories, pages 115?126.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
J. Pustejovsky, A. Meyers, M. Palmer, and M. Poe-
sio. 2005. Merging PropBank, NomBank, TimeBank,
Penn Discourse Treebank and Coreference. In ACL
2005 Workshop: Frontiers in Corpus Annotation II:
Pie in the Sky.
L. Shen, J. Xu, and R. Weischedel. 2008. A New String-
to-Dependency Machine Translation Algorithm with a
Target Dependency Language Model. In ACL 2008.
Y. Shinyama. 2007. Being Lazy and Preemptive at
Learning toward Information Extraction. Ph.D. the-
sis, NYU.
M. Surdeanu, R. Johansson, A. Meyers, Ll. Ma?rquez,
and J. Nivre. 2008. The CoNLL-2008 Shared Task
on Joint Parsing of Syntactic and Semantic Dependen-
cies. In Proceedings of the CoNLL-2008 Shared Task,
Manchester, GB.
Information technology Promotion Agency. 1987. IPA
Lexicon of the Japanese Language for Computers
IPAL (Basic Verbs). (in Japanese).
M. Utiyama and H. Isahara. 2003. Reliable Measures
for Aligning Japanese-English News Articles and Sen-
tences. In ACL-2003, pages 72?79.
J. Wagner, D. Seddah, J. Foster, and J. van Genabith.
2007. C-Structures and F-Structures for the British
National Corpus. In Proceedings of the Twelfth In-
ternational Lexical Functional Grammar Conference,
Stanford. CSLI Publications.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese syn-
tactic reordering for statistical machine translation. In
EMNLP-CoNLL 2007, pages 737?745.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese TreeBank: Phrase Structure Annotation
of a Large Corpus. Natural Language Engineering,
11:207?238.
N. Xue. 2008. Labeling Chinese Predicates with Seman-
tic roles. Computational Linguistics, 34:225?255.
154
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 116?120,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Transducing Logical Relations from Automatic and Manual GLARF
Adam Meyers?, Michiko Kosaka?, Heng Ji?, Nianwen Xue?,
Mary Harper?, Ang Sun?, Wei Xu? and Shasha Liao?
? New York Univ., ?Monmouth Univ., ?Brandeis Univ,, ?City Univ. of New York, ?Johns
Hopkins Human Lang. Tech. Ctr. of Excellence & U. of Maryland, College Park
Abstract
GLARF relations are generated from tree-
bank and parses for English, Chinese and
Japanese. Our evaluation of system out-
put for these input types requires consid-
eration of multiple correct answers.1
1 Introduction
Systems, such as treebank-based parsers (Char-
niak, 2001; Collins, 1999) and semantic role la-
belers (Gildea and Jurafsky, 2002; Xue, 2008), are
trained and tested on hand-annotated data. Evalu-
ation is based on differences between system out-
put and test data. Other systems use these pro-
grams to perform tasks unrelated to the original
annotation. For example, participating systems in
CONLL (Surdeanu et al, 2008; Hajic? et al, 2009),
ACE and GALE tasks merged the results of sev-
eral processors (parsers, named entity recognizers,
etc.) not initially designed for the task at hand.
This paper discusses differences between hand-
annotated data and automatically generated data
with respect to our GLARFers, systems for gen-
erating Grammatical and Logical Representation
Framework (GLARF) for English, Chinese and
Japanese sentences. The paper describes GLARF
(Meyers et al, 2001; Meyers et al, 2009) and
GLARFers and compares GLARF produced from
treebank and parses.
2 GLARF
Figure 1 includes simplified GLARF analyses for
English, Chinese and Japanese sentences. For
each sentence, a GLARFer constructs both a Fea-
ture Structure (FS) representing a constituency
analysis and a set of 31-tuples, each representing
1Support includes: NSF IIS-0534700 & IIS-0534325
Structure Alignment-based MT; DARPA HR0011-06-C-
0023 & HR0011-06-C-0023; CUNY REP & GRTI Program.
This work does not necessarily reflect views of sponsors.
up to three dependency relations between pairs of
words. Due to space limitations, we will focus on
the 6 fields of the 31-tuple represented in Figure 1.
These include: (1) a functor (func); (2) the de-
pending argument (Arg); (3) a surface (Surf) la-
bel based on the position in the parse tree with no
regularizations; (4) a logic1 label (L
?
1) for a re-
lation that reflects grammar-based regularizations
of the surface level. This marks relations for fill-
ing gaps in relative clauses or missing infinitival
subjects, represents passives as paraphrases as ac-
tives, etc. While the general framework supports
many regularizations, the relations actually repre-
sented depends on the implemented grammar, e.g.,
our current grammar of English regularizes across
passives and relative clauses, but our grammars
of Japanese and Chinese do not currently.; (5) a
logic2 label (L2) for Chinese and English, which
represents PropBank, NomBank and Penn Dis-
course Treebank relations; and (6) Asterisks (*)
indicate transparent relations, relations where the
functor inherits semantic properties of certain spe-
cial arguments (*CONJ, *OBJ, *PRD, *COMP).
Figure 1 contains several transparent relations.
The interpretation of the *CONJ relations in the
Japanese example, include not only that the nouns
[zaisan] (assets) and [seimei] (lives) are con-
joined, but also that these two nouns, together
form the object of the Japanese verb [mamoru]
(protect). Thus, for example, semantic selection
patterns should treat these nouns as possible ob-
jects for this verb. Transparent relations may serve
to neutralize some of the problematic cases of at-
tachment ambiguity. For example, in the English
sentence A number of phrases with modifiers are
not ambiguous, there is a transparent *COMP re-
lation between numbers and of and a transpar-
ent *OBJ relation between of and phrases. Thus,
high attachment of the PP with modifiers, would
have the same interpretation as low attachment
since phrases is the underlying head of number of
116
Figure 1: GLARF 5-tuples for 3 languages
phrases. In this same example, the adverb not can
be attached to either the copula are or the pred-
icative adjective, with no discernible difference in
meaning?this factor is indicated by the transparent
designation of the relations where the copula is a
functor. Transparent features also provide us with
a simple way of handling certain function words,
such as the Chinese word De which inherits the
function of its underlying head, connecting a vari-
ety of such modifiers to head nouns (an adjective
in the Chinese example.). For conjunction cases,
the number of underlying relations would multi-
ply, e.g., Mary and John bought and sold stock
would (underlyingly) have four subject relations
derived by pairing each of the underlying subject
nouns Mary and John with each of the underlying
main predicate verbs bought and sold.
3 Automatic vs. Manual Annotation
Apart from accuracy, there are several other ways
that automatic and manual annotation differs. For
Penn-treebank (PTB) parsing, for example, most
parsers (not all) leave out function tags and empty
categories. Consistency is an important goal for
manual annotation for many reasons including: (1)
in the absence of a clear correct answer, consis-
tency helps clarify measures of annotation quality
(inter-annotator agreement scores); and (2) consis-
tent annotation is better training data for machine
learning. Thus, annotation specifications use de-
faults to ensure the consistent handling of spurious
ambiguity. For example, given a sentence like I
bought three acres of land in California, the PP in
California can be attached to either acres or land
with no difference in meaning. While annotation
guidelines may direct a human annotator to prefer,
for example, high attachment, systems output may
have other preferences, e.g., the probability that
land is modified by a PP (headed by in) versus the
probability that acres can be so modified.
Even if the manual annotation for a particular
corpus is consistent when it comes to other factors
such as tokenization or part of speech, developers
of parsers sometimes change these guidelines to
suit their needs. For example, users of the Char-
niak parser (Charniak, 2001) should add the AUX
category to the PTB parts of speech and adjust
their systems to account for the conversion of the
word ain?t into the tokens IS and n?t. Similarly, to-
kenization decisions with respect to hyphens vary
among different versions of the Penn Treebank, as
well as different parsers based on these treebanks.
Thus if a system uses multiple parsers, such differ-
ences must be accounted for. Differences that are
not important for a particular application should
be ignored (e.g., by merging alternative analyses).
For example, in the case of spurious attachment
ambiguity, a system may need to either accept both
as right answers or derive a common representa-
tion for both. Of course, many of the particular
problems that result from spurious ambiguity can
be accounted for in hind sight. Nevertheless, it
is precisely this lack of a controlled environment
which adds elements of spurious ambiguity. Us-
ing new processors or training on new treebanks
can bring new instances of spurious ambiguity.
4 Experiments and Evaluation
We ran GLARFers on both manually created tree-
banks and automatically produced parses for En-
glish, Chinese and Japanese. For each corpus, we
created one or more answer keys by correcting
117
system output. For this paper, we evaluate solely
on the logic1 relations (the second column in fig-
ure 1.) Figure 2 lists our results for all three lan-
guages, based on treebank and parser input.
As in (Meyers et al, 2009), we generated 4-
tuples consisting of the following for each depen-
dency: (A) the logic1 label (SBJ, OBJ, etc.), (B)
its transparency (True or False), (C) The functor (a
single word or a named entity); and (D) the argu-
ment (a single word or a named entity). In the case
of conjunction where there was no lexical con-
junction word, we used either punctuation (com-
mas or semi-colons) or the placeholder *NULL*.
We then corrected these results by hand to produce
the answer key?an answer was correct if all four
members of the tuple were correct and incorrect
otherwise. Table 2 provides the Precision, Recall
and F-scores for our output. The F-T columns
indicates a modified F-score derived by ignoring
the +/-Transparent distinction (resulting changes
in precision, recall and F-score are the same).
For English and Japanese, an expert native
speaking linguist corrected the output. For Chi-
nese, several native speaking computational lin-
guists shared the task. By checking compatibil-
ity of the answer keys with outputs derived from
different sources (parser, treebank), we could de-
tect errors and inconsistencies. We processed the
following corpora. English: 86 sentence article
(wsj 2300) from the Wall Street Journal PTB test
corpus (WSJ); 46 sentence letter from Good Will
(LET), the first 100 sentences of a switchboard
telephone transcript (TEL) and the first 100 sen-
tences of a narrative from the Charlotte Narra-
tive and Conversation (NAR). These samples are
taken from the PTB WSJ Corpus and the SIGANN
shared subcorpus of the OANC. The filenames are:
110CYL067, NapierDianne and sw2014. Chi-
nese: a 20 sentence sample of text from the
Penn Chinese Treebank (CTB) (Xue et al, 2005).
Japanese: 20 sentences from the Kyoto Corpus
(KYO) (Kurohashi and Nagao, 1998)
5 Running the GLARFer Programs
We use Charniak, UMD and KNP parsers (Char-
niak, 2001; Huang and Harper, 2009; Kurohashi
and Nagao, 1998), JET Named Entity tagger (Gr-
ishman et al, 2005; Ji and Grishman, 2006)
and other resources in conjunction with language-
specific GLARFers that incorporate hand-written
rules to convert output of these processors into
a final representation, including logic1 struc-
ture, the focus of this paper. English GLAR-
Fer rules use Comlex (Macleod et al, 1998a)
and the various NomBank lexicons (http://
nlp.cs.nyu.edu/meyers/nombank/) for
lexical lookup. The GLARF rules implemented
vary by language as follows. English: cor-
recting/standardizing phrase boundaries and part
of speech (POS); recognizing multiword expres-
sions; marking subconstituents; labeling rela-
tions; incorporating NEs; regularizing infiniti-
val, passives, relatives, VP deletion, predica-
tive and numerous other constructions. Chi-
nese: correcting/standardizing phrase boundaries
and POS, marking subconstituents, labeling rela-
tions; regularizing copula constructions; incorpo-
rating NEs; recognizing dates and number expres-
sions. Japanese: converting to PTB format; cor-
recting/standardizing phrase boundaries and POS;
labeling relations; processing NEs, double quote
constructions, number phrases, common idioms,
light verbs and copula constructions.
6 Discussion
Naturally, the treebank-based system out-
performed parse-based system. The Charniak
parser for English was trained on the Wall Street
Journal corpus and can achieve about 90% accu-
racy on similar corpora, but lower accuracy on
other genres. Differences between treebank and
parser results for English were higher for LET and
NAR genres than for the TEL because the system
is not currently designed to handle TEL-specific
features like disfluencies. All processors were
trained on or initially designed for news corpora.
Thus corpora out of this domain usually produce
lower results. LET was easier as it consisted
mainly of short simple sentences. In (Meyers et
al., 2009), we evaluated our results on 40 Japanese
sentences from the JENAAD corpus (Utiyama
and Isahara, 2003) and achieved a higher F-score
(90.6%) relative to the Kyoto corpus, as JENAAD
tends to have fewer long complex sentences.
By using our answer key for multiple inputs, we
discovered errors and consequently improved the
quality of the answer keys. However, at times we
were also compelled to fork the answer keys?given
multiple correct answers, we needed to allow dif-
ferent answer keys corresponding to different in-
puts. For English, these items represent approxi-
mately 2% of the answer keys (there were a total
118
Treebank Parser
ID % Prec % Rec F F-T % Prec % Rec F F-T
WSJ 12381491 = 83.0
1238
1471 = 84.2 83.6 87.1
1164
1452 = 80.2
1164
1475 = 78.9 79.5 81.8
LET 419451 = 92.9
419
454 = 92.3 92.6 93.3
390
434 = 89.9
390
454 = 85.9 87.8 87.8
TEL 478627 = 76.2
478
589 = 81.2 78.6 82.2
439
587 = 74.8
439
589 = 74.5 74.7 77.4
NAR 8171013 = 80.7
817
973 =84.0 82.3 84.1
724
957 = 75.7
724
969 = 74.7 75.2 76.1
CTB 351400 = 87.8
351
394 = 89.1 88.4 88.7
352
403 = 87.3
352
438 = 80.4 83.7 83.7
KYO 525575 = 91.3
525
577 = 91.0 91.1 91.1
493
581 = 84.9
493
572 = 86.2 85.5 87.8
Figure 2: Logic1 Scores
Figure 3: Examples of Answer Key Divergences
of 74 4-tuples out of a total of 3487). Figure 3 lists
examples of answer key divergences that we have
found: (1) alternative tokenizations; (2) spurious
differences in attachment and conjunction scope;
and (3) ambiguities specific to our framework.
Examples 1 and 2 reflect different treatments of
hyphenation and contractions in treebank specifi-
cations over time. Parsers trained on different tree-
banks will either keep hyphenated words together
or separate more words at hyphens. The Treebank
treatment of can?t regularizes so that (can need
not be differentiated from ca), whereas the parser
treatment makes maintaining character offsets eas-
ier. In example 3, the Japanese parser recognizes
a single word whereas the treebank divides it into
a prefix plus stem. Example 4 is a case of differ-
ences in character encoding (zero).
Example 5 is a common case of spurious attach-
ment ambiguity for English, where a transparent
noun takes an of PP complement?nouns such as
form, variety and thousands bear the feature trans-
parent in the NOMLEX-PLUS dictionary (a Nom-
Bank dictionary based on NOMLEX (Macleod et
al., 1998b)). The relative clause attaches either
to the noun thousands or people and, therefore,
the subject gap of the relative is filled by either
thousands or people. This ambiguity is spurious
since there is no meaningful distinction between
these two attachments. Example 6 is a case of
attachment ambiguity due to a support construc-
tion (Meyers et al, 2004). The recipient of the
gift will be Goodwill regardless of whether the
PP is attached to give or gift. Thus there is not
much sense in marking one attachment more cor-
rect than the other. Example 7 is a case of conjunc-
tion ambiguity?the context does not make it clear
whether or not the pearls are part of a necklace or
just the beads are. The distinction is of little con-
sequence to the understanding of the narrative.
Example 8 is a case in which our grammar han-
dles a case ambiguously: the prenominal adjective
can be analyzed either as a simple noun plus ad-
jective phrase meaning various businesses or as a
noun plus relative clause meaning businesses that
are varied. Example 9 is a common case in Chi-
nese where the verb/noun distinction, while un-
clear, is not crucial to the meaning of the phrase ?
under either interpretation, 5 billion was exported.
7 Concluding Remarks
We have discussed challenges of automatic an-
notation when transducers of other annotation
schemata are used as input. Models underly-
ing different transducers approximate the origi-
nal annotation in different ways, as do transduc-
ers trained on different corpora. We have found
it necessary to allow for multiple correct answers,
due to such differences, as well as, genuine and
spurious ambiguities. In the future, we intend to
investigate automatic ways of identifying and han-
dling spurious ambiguities which are predictable,
including examples like 5,6 and 7 in figure 3 in-
volving transparent functors.
119
References
E. Charniak. 2001. Immediate-head parsing for lan-
guage models. In ACL 2001, pages 116?123.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
D. Gildea and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics,
28:245?288.
R. Grishman, D. Westbrook, and A. Meyers. 2005.
Nyu?s english ace 2005 system description. In ACE
2005 Evaluation Workshop.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,
M. A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre,
S. Pado?, J. ?Ste?pa?nek, P. Stran?a?k, M. Surdeanu,
N. Xue, and Y. Zhang. 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In CoNLL-2009, Boulder, Col-
orado, USA.
Z. Huang and M. Harper. 2009. Self-training PCFG
Grammars with Latent Annotations across Lan-
guages. In EMNLP 2009.
H. Ji and R. Grishman. 2006. Analysis and Repair of
Name Tagger Errors. In COLING/ACL 2006, Syd-
ney, Australia.
S. Kurohashi and M. Nagao. 1998. Building a
Japanese parsed corpus while improving the pars-
ing system. In Proceedings of The 1st International
Conference on Language Resources & Evaluation,
pages 719?724.
C. Macleod, R. Grishman, and A. Meyers. 1998a.
COMLEX Syntax. Computers and the Humanities,
31:459?481.
C. Macleod, R. Grishman, A. Meyers, L. Barrett, and
R. Reeves. 1998b. Nomlex: A lexicon of nominal-
izations. In Proceedings of Euralex98.
A. Meyers, M. Kosaka, S. Sekine, R. Grishman, and
S. Zhao. 2001. Parsing and GLARFing. In Pro-
ceedings of RANLP-2001, Tzigov Chark, Bulgaria.
A. Meyers, R. Reeves, and Catherine Macleod. 2004.
NP-External Arguments: A Study of Argument
Sharing in English. In The ACL 2004 Workshop
on Multiword Expressions: Integrating Processing,
Barcelona, Spain.
A. Meyers, M. Kosaka, N. Xue, H. Ji, A. Sun, S. Liao,
and W. Xu. 2009. Automatic Recognition of Log-
ical Relations for English, Chinese and Japanese in
the GLARF Framework. In SEW-2009 at NAACL-
HLT-2009.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma?rquez,
and J. Nivre. 2008. The CoNLL-2008 Shared Task
on Joint Parsing of Syntactic and Semantic Depen-
dencies. In Proceedings of the CoNLL-2008 Shared
Task, Manchester, GB.
M. Utiyama and H. Isahara. 2003. Reliable Mea-
sures for Aligning Japanese-English News Articles
and Sentences. In ACL-2003, pages 72?79.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese Treebank: Phrase Structure Annota-
tion of a Large Corpus. Natural Language Engi-
neering.
N. Xue. 2008. Labeling Chinese Predicates with Se-
mantic roles. Computational Linguistics, 34:225?
255.
120
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 88?97,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Improving MT Word Alignment Using Aligned Multi-Stage Parses
Adam Meyers?, Michiko Kosaka?, Shasha Liao? and Nianwen Xue?
? New York University, ?Monmouth University, ?Brandeis University
Abstract
We use hand-coded rules and graph-aligned
logical dependencies to reorder English text
towards Chinese word order. We obtain a
1.5% higher F-score for Giza++ compared to
running with unprocessed text. We describe
this research and its implications for SMT.
1 Introduction
Some statistical machine translation (SMT) systems
use pattern-based rules acquired from linguistically
processed bitexts. They acquire these rules through
the alignment of a parsed structure in one language
with a raw string in the other language (Yamada and
Knight, 2001; Shen et al, 2008) or the alignment
of source/target language parse trees (Zhang et al,
2008; Cowan, 2008). This paper shows that ma-
chine translation (MT) can also benefit by aligning a
?deeper? level of analysis than parsed text, which in-
cludes semantic role labeling, regularization of pas-
sives and wh constructions, etc. We create GLARF
representations (Meyers et al, 2009) for English and
Chinese sentences, in the form of directed acyclic
graphs. We describe two graph-based techniques
for reordering English sentences to be closer to that
of corresponding Chinese sentences. One technique
is based on manually created rules and the other is
based on an automatic alignment of GLARF repre-
sentations of Chinese/English sentences. After re-
ordering, we align words of the reordered English
with the words of the Chinese, using the Giza++
word aligner(Och and Ney, 2003). For both tech-
niques, the resulting alignment has a higher F-score
than Giza++ on raw text (a 0.7% to 1.5% absolute
improvement). In principle, our reordered text can
be used to improve any Chinese/English SMT sys-
tem for which Giza++ (or other word aligners) are
part of the processing pipeline.
These experiments are a first step in using
GLARF-style analyses for MT, potentially improv-
ing systems that already perform well with aligned
text lacking large gaps in surface alignment. We hy-
pothesize that SMT systems are most likely to ben-
efit from deep analysis for structures where source
and target language word order differs the most. We
propose using deep analysis to reorder such struc-
tures in one language to more closely reflect the
word order of the other language. The text would be
reordered at two stages in an SMT system: (1) prior
to acquiring a translation model; and (2) either prior
to translation (if source text is reordered) or after
translation (if target text is reordered). Our system
moves large constituents (e.g., noun post-modifiers)
to bring English word order closer to that of parallel
Chinese sentences. This improves word alignment
and is likely to improve SMT.
For this work we use two English/Chinese bitext
corpora developed by the Linguistic Data Consor-
tium (LDC): the Tides FBIS corpus and the GALE
Y1 Q4 Chinese/English Word-Alignment corpus.
We used 2300 aligned sentences from FBIS for de-
velopment purposes. We divided the GALE corpus
into into a 3407 sentence development subcorpus
(DEV) and a 1505 sentence test subcorpus (TEST).
We used the LDC?s manual alignments of the FBIS
corpus to score these data.
88
2 Related Work in SMT
Four papers stand out as closely related to the
present study. (Collins et al, 2005; Wang et al,
2007) describe experiments which use manually cre-
ated parse-tree-based rules to reorder one side of
a bitext: German/English in (Collins et al, 2005)
and English/Chinese in (Wang et al, 2007). Both
achieve BLEU score improvements for SMT: 25.2%
to 26.8% for (Collins et al, 2005) and 28.52 to 30.86
for (Wang et al, 2007). (Wang et al, 2007) uses
rules very similar to our own as they use the same
language pair, although they reorder the Chinese,
whereas we reorder the English. The most signifi-
cant differences between our research and (Collins
et al, 2005; Wang et al, 2007) are: (1) our manual
rules benefit from a level of representation ?deeper?
than a surface parse; and (2) In addition to the hand-
coded rules, we also use automatic alignment-based
rules. (Wu and Fung, 2009) uses PropBank role la-
bels (Palmer et al, 2005) as the basis of a second
pass filter over an SMT system to improve the BLEU
score from 42.99 to 43.51. The main similarity to
the current study is the use of a level of represen-
tation that is ?deeper? than a surface parse. How-
ever, our application of linguistic structure is more
like that of (Wang et al, 2007) and our ?deep? level
connects all predicates and arguments in the sen-
tence, regardless of part of speech, rather than just
connecting verbs to their arguments. (Bryl and van
Genabith, 2010) describes an open source LFG F-
structure alignment tool with an algorithm similar to
our previous work. They evaluate their alignment
output on 20 manually-aligned German and English
F-structures. They leave the impact of their work on
MT to future research.
In addition to these papers, there has also been
some work on rule-based reordering preprocessors
to word alignment based on shallower linguistic in-
formation. For example (Crego and Marin?o, 2006)
reorders based on patterns of POS tags. We hypoth-
esize that this is similar to the above approaches in
that patterns of POS tags are likely to simulate pars-
ing or chunking.
3 Preparing the Data
The two stage parsers of previous decades (Hobbs
and Grishman, 1976) generated a syntactic repre-
sentation analogous to the (more accurate) output
of current treebank-based parsers (Charniak, 2001)
and an additional second stage output that regular-
ized constructions (passive, active, relative clauses)
to representations similar to active clauses with no
gaps, e.g., The book was read by Mary was given a
representation similar to that of Mary read the book.
Treating the active clause as canonical provides a
way to reduce variation in language and thus, mak-
ing it easier to acquire and apply statistical informa-
tion from corpora?there is more evidence for partic-
ular statistical patterns when applications learn pat-
terns and patterns more readily match data.
Two-stage parsers were influenced by linguistic
theories (Harris, 1968; Chomsky, 1957; Bresnan and
Kaplan, 1982) which distinguish a ?surface? and a
?deep? level. The deep level neutralizes differences
between ways to express the same meaning?a pas-
sive like The cheese was eaten by rats was analyzed
in terms of the active form Rats ate the cheese. Cur-
rently ?semantic parsing? refers to a similar repre-
sentation, e.g., (Wagner et al, 2007) or our own
GLARF (Meyers et al, 2009). However, the term is
also used for semantic role labelers (Gildea and Ju-
rafsky, 2002; Xue, 2008), systems which typically
label semantic relations between verbs and their ar-
guments and rarely cover arguments of other parts
of speech. Second stage semantic parsers like our
own, connect all the tokens in the sentence. Aligned
text processed in this way can (for example) repre-
sent differences in English/Chinese noun modifier
order, including relative clauses. In contrast, few
role labelers handle noun modifiers and none han-
dle relative clauses. Below, we describe the GLARF
framework and our system for generating GLARF
representations of English and Chinese sentences.
For each language, we combine several types of
information which may include: named entity (NE)
tagging, date/number regularization, recognition of
multi-word expressions (the preposition with respect
to, the noun hand me down and the verb ad lib),
role labels for predicates of all parts of speech, regu-
larizing passives and other constructions, error cor-
rection, among other processes into a single typed
feature structure (TFS) representation. This TFS
is converted into a set of 25-tuples representing
dependency-style relations between pairs of words
in the sentence. Three types of dependencies are
89
n1
know
SBJ OBJ
n3
of
n5 OBJ
SBJ OBJ
N?POS
COMP
n4
the
Q?POS
n2? n3?
n6?
n1?
I rules
tennis
n6
n2
 
 
  
Figure 1: Word-Aligned Logic1 Dependencies
represented: surface dependencies (close to the level
of the parser), logic1 dependencies (reflecting var-
ious regularizations) and logic2 dependencies (re-
flecting the output of a PropBanker, NomBanker
and Penn Discourse Treebank transducer).(Palmer
et al, 2005; Xue and Palmer, 2003; Meyers et al,
2004; Miltsakaki et al, 2004) The surface depen-
dency graph is a tree; The logic1 dependency graph
is an directed acyclic graph; and The logic2 depen-
dency graph is a directed graph with cycles, cover-
ing only a subset of the tokens in the sentence. For
these experiments, we focus on the logic1 relations,
but will sometimes use the surface relations as well.
Figure 1 is a simple dependency-based logic1 repre-
sentation of I know the rules of tennis and its Chi-
nese translation. The edge labels name the relations
between heads and dependents, e.g., I is the SBJ of
know and the dashed lines indicate word level corre-
spondences. Each node is labeled with both a word
and a unique node identifier (n1, n1?, etc.)
The English system achieves F-scores for logic1
dependencies on parsed news text in the 80?90%
range and the Chinese system achieves F-scores in
the 74?84% range, depending on the complexity of
the text. The English system has been created over
the course of about 9 years, and consequently is
more extensive than the Chinese system, which has
been created over the past 3 years. The systems are
described in more detail in (Meyers et al, 2009).
The GLARF representations are created in a se-
ries of steps involving several processors. The En-
glish pipeline includes: (1) dividing text into sen-
tences; (2) running the JET NE tagger (Ji and Gr-
ishman, 2006); (3) running scripts that clean up data
(to prevent parser crashes); (4) running a parser (cur-
rently Charniak?s 2005 parser based on (Charniak,
2001)); (5) running filters that: (a) correct com-
mon parsing errors; (b) merge NE information with
the parse, resolving conflicts in constituent bound-
aries by hand-coded rules; (c) regularize numbers,
dates, times and holidays; (d) identify heads and
label relations between constituents; (e) regularize
text grammatically (filling empty subjects, resolv-
ing relative clause and Wh gaps, etc.); (f) mark con-
junction scope; (g) identify transparent constituents
(e.g., recognizing, that A variety of different peo-
ple has the semantic features of people (human), not
those of variety, the syntactic head of the phrase.);
among other aspects. The Chinese pipeline is simi-
lar, except that it includes the LDC word segmenter
and a PropBanker (Xue, 2008). Also, the regulariza-
tion routines are not as completely developed, e.g.,
relative clause gaps and passives are not handled
yet. The Chinese system currently uses the Berke-
ley parser (Petrov and Klein, 2007). Each of these
pipelines derives typed feature structure representa-
tions, which are then converted into the 25 tuple rep-
resentation of 3 types of dependencies between pairs
of tokens: surface, logic1 and logic2.
To insure that the logic1 graphs are acyclic, we as-
sume that certain edges are surface only and that the
resulting directed acyclic graphs can have multiple
roots. It turns out that the multiple rooted cases are
mostly limited to a few constructions, the most com-
mon being parenthetical clauses and relative clauses.
A parenthetical clause takes the main clause as an
argument. For example, in The word ?potato?, he
claimed, is spelled with a final ?e?., the verb claimed,
takes the entire main clause as an argument, we as-
sume that he claimed is a dependent on the main
verb (is) spelled labeled PARENTHETICAL in our
surface dependency structure, but that the main verb
(is) spelled is a dependent of the verb claimed in
our logic1 structure, labeled COMPLEMENT. Thus
the logic1 surface dependency structure have dis-
tinct roots. In a relative clause, such as the book that
I read?, we assume that the clause that I read is a de-
pendent on the noun book in our surface dependency
structure with the label RELATIVE, but book is a de-
pendent on the verb read in our logic1 dependency
structure, with the label OBJ. This, means that our
logic1 dependency graphs for sentences containing
relative clauses are multi-rooted. One of the roots is
the same as the root of the surface tree and the other
root is the root of the relative clause graph (a rela-
90
tive pronoun or a main verb). Furthermore, there is
a surface path connecting the relative clause root to
the rest of the graph. Noncyclic graph traversal is
possible, provide that: (1) we use the surface path to
enter the graph representing the relative clause ? oth-
erwise, the traversal would skip the relative clause;
and (2) we halt the traversal if we reach this path a
second time ? this avoids traversing down an end-
less path. The parenthetical and relative clause are
representative of the handful of cases in which naive
representations would introduce loops. All cases of
which we are aware have the essential properties of
one of these two cases: (1) either introducing a dif-
ferent single root of the clause; or (2) introducing an
additional root that can be bridged by a surface path.
4 Manual Reordering Rules
We derived manual rules for making the English
Word Order more like the Chinese by manually in-
specting the data. We inspected the first 100-200
sentences of the DEV corpus by first transliterating
the Chinese into English ? replaced each Chinese
word with the aligned English counterpart. Several
patterns emerged which were easy to formalize into
rules in the GLARF framework. These patterns were
verified and sometimes generalized through discus-
sions with native Chinese speakers and linguists.
Our rules, similar to those of (Wang et al, 2007) are
as follows (results are discussed in section 6): (1)
Front a post-nominal PP headed by a preposition in
the list {of, in, with, about)}. (2) Front post-nominal
relative clause that begins with that or does not have
any relative pronoun, such that the main predicate is
not a copula plus adjective construction. (3) Front
post-nominal relative clause that begins with that or
has no relative pronoun if the main predicate is a
copula+adjective construction which is not negated
by a word from the set {no neither nor never not
n?t}. (4) Front post-nominal reduced relative in the
form of a passive or adjectival phrase. (5) Move ad-
verbials more than and less than after numbers that
they modify. (6) Move PPs that post-modify adjec-
tives to the position before the adjective. (7) Move
subordinate conjunctions before and after to the end
of the clause that they introduce. (8) Move an ini-
tial one-word-long title (Mr., Ms., Dr., President) to
the end of the name. (9) Move temporal adverbials
(adverb, PP, subordinate clause that is semantically
temporal) to pre-verb position.
5 Automatic Node Alignment and its
Application for Word Alignment
In this experiment, we automatically derive re-
orderings of the English sentences from an align-
ment between nodes in logic1 dependency graphs
for the English (source) and Chinese (target) sen-
tences. Source/Target designations are for conve-
nience, since the direction of MT is irrelevant.
We define an alignment as a partial function from
the nodes in the source graph and the nodes in the
target graph. We, furthermore, assume that this map-
ping is 1 to 1 for most node pairs, but can be n to 1
(or 1 to n). Furthermore, we allow some nodes, in
effect, to represent multiple tokens. These are iden-
tified as part of the GLARF analysis of a particular
sentence string and reflect language-specific rules.
Thus, for our purposes, a mapping between a source
and target node, each representing a multi-word ex-
pression is 1 to 1, rather than N to N.
We identify the following types of multi-word ex-
pressions for this purpose: (a) idiomatic expressions
from our monolingual lexicons, (b) dates, (c) times
(d) numbers and (e) ACE (Grishman, 2000) NEs.
Dates, holidays and times are regularized using ISO-
TimeML, e.g., January 3, 1977 becomes 1977-03-01
and numbers are converted to Arabic numbers.
5.1 ALIGN-ALG1
This work uses a modified version of ALIGN-
ALG1, a graph alignment algorithm we previously
used to align 1990s-style two-stage parser output for
MT experiments. ALIGN-ALG1 is an O(n2) algo-
rithm, n is the maximum number of nodes in the
source and target graphs (Meyers et al, 1996; Mey-
ers et al, 1998). Given Source Tree T and Target
Tree T ?, an alignment(T, T ?) is a partial function
from nodes N in T to nodes N ? in T ?. An exhaus-
tive search of possible alignments would consider all
non-intersecting combinations of the T ?T ? pairs of
source/target nodes ? There are at most T ! such pair-
ings where T >= T ?.1 However, ALIGN-ALG1 as-
sumes that some of these pairings are unlikely, and
1This ignores N to 1 matches, which we allow, although rel-
atively rarely.
91
favors pairings that assume the structure of the trees
correspond more closely. In particular, it is assumed
that ancestor nodes are more likely to match if most
of their descendant nodes match as well.
ALIGN-ALG1 finds the highest scoring align-
ment, where the score of an alignment is the sum
of the scores of the node pairs in the partial func-
tion. The score for each node pair (n, n?) partially
depends on the scores of a mapping from the chil-
dren of n to the children of n?. While the process
of calculating the scores is recursive, it can be made
efficient using dynamic programming.
ALIGN-ALG1 assumes that we align r and r?,
the roots of T and T ?. Calculating the scores for r
and r?, entails calculating the scores of pairs of their
children, and by extension all mappings from N to
N ? that obey the dominance preserving constraint:
Given nodes n1 and n2 in N and nodes n?1 and n?2
in N ?, where all 4 nodes are part of the alignment,
it cannot be the case that: n1 dominates n2, but
n?1 does not dominate n?2. Here, dominates means
is an ancestor in the dependency graph. ALIGN-
ALG1 scores each pair of nodes using the formula:
Score(n, n?) = Lex(n, n?) + ChildV al(n, n?),
where Lex(n, n?) is a score based on matching the
words labeling nodes n and n?, e.g., the score is 1 if
the pair is found in a bilingual dictionary and 0 oth-
erwise. Given n has children c0, . . . , ci and n? has
children c?0, . . . , c?j , to calculate ChildVal: (1) Cre-
ate Child-Matrix, a (i+ 1)? (j + 1) matrix (2) Fill
every position (1 <= x <= i, 1 <= x? <= j)
with Score(x, x?) (3) Fill every position (i+1, 1 <=
x? <= j) with Score(n, x?) minus a penalty (e.g.,
- .1) for collapsing an edge. This treats n? and x?
as a single unit, matched to n.2 (4) Fill every po-
sition (1 <= x <= i, j+1) with Score(x, n?) mi-
nus a penalty for collapsing an edge. Thus n + x is
paired with n?. (5) Set (i+1,j+1) to ??. Collapsing
both source and target edges is not permitted. (6) For
all sets of positions in the matrix such that no node
or column is repeated, select the set with the high-
est aggregate score. The aggregate score is the nu-
meric value of ChildV al(n, n?). If (n,n?) is part of
the alignment that is ultimately chosen, this choice
of node pairs is also part of the alignment. There
2The slight penalty represents that collapsing edges compli-
cate the analysis and is thus disfavored (Occam?s Razor).
are at most max(i + 1, j + 1)! possible pairings.
Rather than calculating them all, a greedy heuristic
can reduce the calculation time with minimal effect
on accuracy: the highest scoring cell in the matrix is
chosen first, conflicting cells are eliminated, the next
highest scoring cell is chosen, etc.
Consider the example in Figure 1, assum-
ing the dashed lines connect lexical matches
(the function LEX returns 1 for these node
pairs). Where n1 and n1? are the roots,
Score(n1, n1?) = 1 + ChildV al(n1, n1?). Cal-
culating ChildV al(n1, n1?) requires a recursive
descent down the pairs of nodes, until the bot-
tom most pair is scored. Score(n6, n6?) = 1.
Score(n5, n6?) = 0 + .9 (derived by collaps-
ing an edge and subtracting a penalty of .1).
Score(n3, n3?) = 1 + .9 = 1.9. Score(n2, n2?) =
1. ChildV al(n1, n1?) = 1 + 1.9 = 2.9. Thus
Score(n1, n1?) = 3.9. The alignment includes:
(n1, n1?), (n2, n2?), (n3, n3?), (n5, n6?), (n6, n6?).
The collapsing of edges helps recognize cases
where multiple predicates form substructures, e.g.,
take a walk, is angry, etc. in one tree can map to sin-
gle verbs in the other tree, allowing outgoing edges
from walk or angry to map to outgoing edges of the
corresponding verb, e.g., the agent and goal of John
walked to the store could map to the agent and goal
of John took a walk to the store.
In practice, ALIGN-ALG1 falls short because:
(1) Our translation dictionary does not have suffi-
cient coverage for the algorithm to perform well; (2)
The assumption that the roots of both graphs should
be aligned is often false. Parallel text often reflects
a dynamic, rather than a literal translation. In one
pair of aligned sentences in the FBIS corpus, the
English phrase the above mentioned requests cor-
responds to: meaning these re-
quests of Chen Shui-bian ? Chen Shui-bian has no
counterpart in the English. Parts of translations can
be omitted due to: (a) the discretion of the trans-
lators, (b) the expected world knowledge of partic-
ular language communities, (c) the cultural impor-
tance of particular information, etc.; (3) Violations
of the dominance-preserving constraint exist. The
most common type that we have observed consists
of sequences of transparent nouns and of (e.g., se-
ries of) in English corresponding to quantifiers in
92
Chinese ( ). Thus the head of the English con-
struction corresponds to the dependent of the Chi-
nese construction and vice versa.
5.2 Lexical Resources
Our primary bilingual Chinese/English dictionary
(LEX1) had insufficient coverage for ALIGN-ALG1
to be effective. LEX1 is a merger between:
The LDC 2002 Chinese-English Dictionary and
HowNet. In addition, we manually added additional
translations of units of measure from English. We
also used NEDICT, a name translation dictionary (Ji
et al, 2009) and AUTODICT, English/Chinese word
to word pairs with high similarity scores taken from
MT phase tables created as part of the (Zhang et al,
2007) system. The NEDICT was used both for pre-
cise matches and partial matches (since, NEs can
often be synonymous with substrings of NEs). In
addition, we used some WordNet (Fellbaum, 1998)
synonyms of English to expand the coverage of all
the dictionaries, allowing English words to match
Chinese word translations of their synonyms. We
allowed additional matches of function words that
served similar functions in the two languages includ-
ing: copulas, pronouns and determiners.
Finally, we use a mutual information (MI) based
approach to find further lexical information. We run
our alignment program over the corpus two times,
the first time, we acquire statistical information
useful for generating a MI-based score. This score
is used as a lexical score on the second pass for
items that do not match any of the dictionaries. On
the first pass, we tally the frequency of each pair
of source/target words s and t, such that neither
s, nor t are matched lexically to any other item
in the sentence. We, furthermore, keep track of
the number of times each word appears in the
corpus and the number of times each word appeared
unaligned in the corpus. We tally MI as follows:
pair?frequency2
1+(source?word?frequency?target?word?frequency)
One is added to the denominator as a variation on
add-one smoothing (Laplace, 1816), intended to
penalize low frequency scores. We calculate this
score in two ways: (a) using the global frequencies
of the source and target words; and (b) using the
frequency these words were unaligned. The larger
of the two scores is the one that is actually use.
Different lexicons are given different weights.
Matches between words in the hand-coded transla-
tion dictionary and NEDICT are given a score of
1.0. Matches in other dictionaries are allotted lower
scores to represent that these are based on automati-
cally acquired information, which we assume is less
reliable than manually coded information.3
5.3 ALIGN-ALG2
With ALIGN-ALG2, we partially address two lim-
itations of ALIGN-ALG1: (1) the assumption that
the roots of source and target graph are aligned;
and (2) the dominance-preserving constraint. Ba-
sically, we assume that structural similarity is fa-
vored, but not necessarily at the global level. Thus
it is likely that many subparts of corresponding trees
correspond closely, but not necessarily the highest
nodes in the trees.
We use ALIGN-ALG1 to align every possible pair
of S source nodes and T target nodes. Then we look
for P , the highest scoring node pair of all SXT
pairs. P and all the pairs of descendants that are
used to derive this score (the highest scoring pairs
of children, grand children, etc.) become the initial
output. Then we find all unmatched source and tar-
get children, and look up the highest scoring pair of
these nodes, and we repeat the process, adding the
resulting node pairs to the output. We continue to
repeat this process until either all the nodes are in-
cluded in the output or there is no remaining pair
with a score above a threshold score (we leave au-
tomatic methods of tuning this score to future work
and preliminarily have set this parameter to .3). This
means that: 1) some parts of the graphs are left un-
aligned (the alignment is a partial mapping); 2) the
alignment is more resilient to misalignment caused
by differences in graph structure, regardless of the
reason; and 3) the alignment may be between pair
of unconnected graphs, each containing subsets of
nodes and edges in the source and target graphs.
While more complex than ALIGN-ALG1, ALIGN-
ALG2 performs relatively quickly. After one itera-
tion using ALIGN-ALG1, scores are looked up, not
recalculated.
3Current informal weights of .2 to .6 may be replaced with
automatically tuned weights (hill-climbing, etc.) in future work.
93
5.4 Treating Multiple Tokens as One
In some cases, parsing and segmentation of text
can be corrected through minor modifications to our
alignment routine. Similarly, we use bilingual lex-
ical information to determine that certain other ad-
jacent tokens should be treated as single words for
purposes of alignment.
Given a language for which segmentation is a
common source of processing error (Chinese), if a
token is unaligned, we check to see whether subdi-
viding the token into two sub-tokens would allow
one or both of these sub-tokens to be alignable with
unaligned tokens in the other language. We iter-
ate through the string one token at a time, trying
all partitions. Given a source token ABC, consist-
ing of segments A, B and C, we test the two pairs of
subsequences {A, BC} and {AB, C}, to see which
of the two partitions (if any) could be aligned with
unaligned target tokens and we compare the scores
of both, selecting the highest score. Unless no par-
tition yields further source/target matches, we then
choose the highest scoring partition and add the re-
sulting node pairings to our alignment. In a similar
way, if there are a pair of aligned names consisting
of source tokens sj . . . sk and target tokens tj . . . tk,
we look for adjacent unaligned source nodes (a se-
quence of nodes ending in sj?1 or beginning with
sk+1) and/or adjacent target language nodes, such
that adding these nodes to the name sequence would
produce at least as high a lexical score. The lexi-
con can also be used to match two adjacent items to
the same word. We use a similar routine that checks
our lexicons for words that are adjacent to matching
words. This is particularly meaningful for the entries
automatically acquired by means of MI, as our cur-
rent method for acquiring MI would not distinguish
between 1 to 1 and N to 1 cases. Thus MI scores
for adjacent items typically does mean that an N to
1 match is appropriate. For example, the Chinese
word had high MI with every word
in the sequence (except and): ambassador extraor-
dinary and plenipotentiary (example is from FBIS).
This routine was able to cause our procedure to treat
this English sequence as a single token.
5.5 Using Node Alignment for Reordering
Given a node alignment, we can attempt to reorder
the source language so that words associated with
aligned nodes reflect the order of the words label-
ing the corresponding target nodes. Specifically,
we reorder our surface phrase structure-based repre-
sentation of the source language (English) and then
print out all the words yielded from the resulting
reordered tree. Reordering takes place in a bottom
up fashion as follows: for each phrase P with chil-
dren c0 . . . cn, reorder the structure beneath the child
nodes first. Then build the new-constituent right
to left, one child at a time from cn . . . c0. Start-
ing with an empty sequence, each item is put in
its proper place among the constituents in the se-
quence so far. At each step, place some ci after some
cj in ci+1 . . . cn, such that cj align precedes ci
and cj is after every ck in ci+1 . . . cn such that
ci align precedes ck. If cj does not exist, ci is
placed at the beginning of the sequence so far.
Definition of X align precedes Y , where X and
Y are nodes sharing the same parent: (1) Let pairsX
be the set of source/target pairs in the alignment such
that some (leaf node) descendant of X is the source
node in the pair; (2) Let pairsY be the set of pairs
in the alignment such that some descendant of Y is
the source node in the pair; (3) let Xtmax be the last
target member of a pair in pairsX , where the or-
der is determined by the word order of the target
words labeling the nodes; (4) let Ytmin be the first
target member of a pair in pairsY , where the order
is determined the same way; (5) let Xsmin be the
first source member of a pair in pairsx, according
to the source sentence word order; (6) let Ysmax be
the last source word in a pair in pairsY ordered the
same way. (7) X align precedes Y if: Xtmax pre-
cedes Ytmin and there is no source/target pair Q,R
in the alignment such that: (A) R precedes, Ytmin;
(B) Xtmax precedes R; (C) Q either precedes Xsmin
or follows Ysmax; (D) If Q precedes Ysmax, then R
does not precede Ytmin.
Essentially, the align precedes operator pro-
vides a conservative way to order the source sub-
trees S1 and S2 by their aligned target sub-tree coun-
terparts T1 and T2. The idea is that if T1 and T2
are ordered in an opposite manner to S1 and S2,
the source subtrees should trade places. However,
94
System DEV TEST
BASELINE 53.1% 49.9%
MANUAL 54.0% 50.6%
(p < .01) (not significant)
ALIGN 53.5% 51.1%
(p < .05) (p < .01)
ALIGN+MI 53.8% 51.4%
(p < .01) (p < .01)
Table 1: F Scores for Reordering Rules
a source/target pair Bs, Bt can block this reorder-
ing if doing so would upset the order of the moved
constituents relative to Bs and Bt e.g., if before the
move, Bs precedes S2 and Bt precedes T2, but af-
ter the move S2 would precede Bs. This reordering
proceeds from right to left, halting after placing c0.
6 Results
The results summarized in table 1, provide F-scores
(the harmonic mean of precision and recall) of the
word alignment resulting from running GIZA++
with and without our reordering rules, using the
LDC?s manually created word alignments for our
DEV and TEST corpora.4 Giza++ is run with En-
glish as source and Chinese as target. Our baseline
is the result of running Giza++ on the raw text. The
statistical significance of differences from the base-
line are provided in parentheses, next to each non-
baseline score(rounded to 2 significant digits). We
divided both corpora into 20 parts and ran all ver-
sions of the program on each section. We compared
the system output for each section against the base-
line and used the sign test to calculate statistical sig-
nificance. All system output except one5 achieved
at least p < .05 and most systems achieved signifi-
cance well below p < .01.
Informally, we observe that the rules reordering
common noun modifiers produce most of the total
4We used F-scores, which (Fraser and Marcu, 2007) show to
correlate well with improvements in BLEU. We weighted pre-
cision and recall evenly since we do not currently have BLEU
scores for MT that use these alignments and therefore cannot
tune the weights. Our results also showed improvements in
alignment error rate (AER) (Och and Ney, 2000), which incor-
porate the ?possible? and ?sure? portions of the manual align-
ment into F-score, but do not seem to correlate well with BLEU.
5When run on the test corpus, the manual system outper-
formed the baseline system on only 13 out of 20 sections.
improvement. However, space limitations prevent a
detailed exploration of these differences. The results
show that for both DEV and TEST corpora, both re-
ordering approaches improve F-scores of GIZA++
over the baseline. The manual rules (MANUAL)
seem to suffer somewhat from overtraining on the
DEV corpus, as they were designed based on DEV
corpus examples, whereas the alignment based ap-
proaches (ALIGN and subsequent entries in the ta-
ble) seem resilient to these effects. The use of Mu-
tual Information (ALIGN+MI) seems to further im-
prove the F-score.
The two approaches worked for many of the same
phenomena, e.g., they fronted many of the same
noun post-modifiers. The advantage of the hand-
coded rules seems to be that they cover reordering
of words which we cannot align. For example, a
rule that fronts post-nominal of phrases operates re-
gardless of dictionary coverage. Thus the rule-based
version fronted the of phrase in the NP the govern-
ment of the Guangxi Zhuangzu Autonomous Region
in our DEV corpus, due to the absolute application
of the rule. However, the alignment-based version
did not front the PP because the name was not found
in NEDICT. On the other hand, exceptions to this
rule were better handled by the alignment-based sys-
tem. For example, if series of aligns with the quan-
tifier , the PP would be incorrectly fronted
by the manual, but not the alignment-based system.
Also, the alignment-based method can handle cases
not covered by our rules with minimal labor. Thus,
the automatic system, but not the manual-rule sys-
tem fronted the locative PP in Guangxi to the po-
sition between been and quite in the sentence: for-
eign businessmen have been quite actively investing
in Guangxi. This is closer to the Chinese, but may
have been difficult to predict with an automatic rule
for several reasons, e.g., it is not clear if all post-
verbal locative phrases should front.
We further analyzed the DEV ALIGN+MI run to
determine both how often nodes were combined to-
gether by our algorithm to produce N to 1 align-
ments and the number of reorderings undertaken. It
turns out that out of the 59,032 pairs of nodes were
aligned for 3076 sentence pairs:6 55,391 alignments
6When sentences were misparsed in one language or the
other they were not reordered by the program.
95
were 1 to 1 (93.8% of the total) , 3443 alignments
were 2 to 1 (5.8% of the total) and 203 alignments
were N to 1, where N is greater than 2 (0.3% of the
total). The reordering program moved 1597 single
tokens; 2140 blocks 2 or 3 tokens long; 1203 blocks
of 4 or 5 tokens; 610 blocks of 6 or 7 tokens, 419
blocks of 8, 9 or 10 tokens, and 383 blocks of more
than 10 tokens.
7 Concluding Remarks
We have demonstrated that deep level linguistic
analysis can be used to improve word alignment re-
sults. It is natural to consider whether or not these
reorderings are likely to improve MT results. Both
the manual and alignment-based systems moved
post-nominal English modifiers to pre-nominal po-
sition, to reflect Chinese word order ? other move-
ments were much less frequent. In principle, these
selective reorderings may help SMT systems iden-
tify phrases of English that correspond to phrases of
Chinese, thus improving the quality of the phrase ta-
bles, especially when large chunks are moved. We
would also expect that the precision of our system to
be more important than the recall, since our system
would not yield an improvement if it produced too
much noise. Further experiments with current MT
systems are needed to assess whether this is actually
the case. We are considering such tests for future re-
search, using the Moses SMT system (Koehn et al,
2007).
Our representation had several possible advan-
tages over pure parse-based methods. We used se-
mantic features such as temporal, locative and trans-
parent (whether a low-content words inherits its se-
mantics) to help guide our alignment. The regu-
larized structure, also, helped identify long-distance
dependency relationships. We are also consider-
ing several improvements for our alignment-based
rules: (1) using additional dictionary resources such
as CATVAR (Habash and Dorr, 2003), so that cross-
part-of speech alignments can be more readily rec-
ognized; (2) finding more optimal orderings for
unaligned source language words. For example,
the alignment-based method reordered a bright star
arising from China?s policy to a bright arising from
China ?s policy star, separating bright from star,
even though bright star function as a unit; (3) incor-
porating and using multi-word bilingual dictionary
entries.; (4) automatic methods for tuning parame-
ters of our system that are currently hand-coded; (5)
training MI on a much larger corpus; (6) investigat-
ing possible ways to merge the manual-rules with
the alignment-based approach; and (7) performing
similar experiments with English/Japanese bitexts.
We would expect both parse-based approaches
and our system to handle mismatches that cover
large distances better than more shallow approaches
to reordering, e.g., (Crego and Marin?o, 2006) in the
same way that a full-parse handles constituent struc-
ture more completely than a chunker. In addition,
we would expect our approach to work best in lan-
guages where there are large differences in word or-
der, as these are exactly the cases that all predicate-
argument structure is designed to handle well (they
reduce apparent variation in structure). Towards this
end we are currently working on a Japanese/English
system. Obviously, the cost of developing GLARF
(or similar) systems are high, require linguistic ex-
pertise and may not be possible for resource-poor
languages. Nevertheless, we maintain that such sys-
tems are useful for many purposes and are there-
fore worth the cost. The GLARF system for En-
glish is available for download at http://nlp.
cs.nyu.edu/meyers/GLARF.html.
Acknowledgments
This work was supported by NSF Grant IIS-
0534700 Structure Alignment-based MT.
References
J. Bresnan and R. M. Kaplan. 1982. Syntactic Represen-
tation: Lexical-Functional Grammar: A Formal The-
ory for Grammatical Representation. In J. Bresnan,
editor, The Mental Representation of Grammatical Re-
lations. The MIT Press, Cambridge.
A. Bryl and J. van Genabith. 2010. f-align: An Open-
Source Alignment Tool for LFG f-Structures. In Pro-
ceedings of AMTA 2010.
E. Charniak. 2001. Immediate-head parsing for language
models. In ACL 2001, pages 116?123.
N. Chomsky. 1957. Syntactic Structures. Mouton, The
Hague.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause
Restructuring for Statistical Machine Translation. In
ACL 2005.
96
B. A. Cowan. 2008. A Tree-to-Tree Model for Statistical
Machine Translation. Ph.D. thesis, MIT.
J. M. Crego and J. B. Marin?o. 2006. Integration of POS-
tag-based source reordering into SMT decoding by an
extended search graph. In AMTA?06.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. The MIT Press, Cambridge.
A. Fraser and D. Marcu. 2007. Measuring Word
Alignment Quality for Statistical Machine Translation.
Computational Linguistics, 33:293?303.
D. Gildea and D. Jurafsky. 2002. Automatic Labeling of
Semantic Roles. Computational Linguistics, 28:245?
288.
R. Grishman. 2000. Entity Annotation Guidelines.
ftp://jaguar.ncsl.nist.gov/ace/phase1/edt phase1 v2.2.pdf.
N. Habash and B. Dorr. 2003. CatVar: A Database of
Categorial Variations for English. In Proceedings of
the MT Summit, pages 471?474, New Orleans.
Z. Harris. 1968. Mathematical Structures of Language.
Wiley-Interscience, New York.
J. R. Hobbs and R. Grishman. 1976. The Automatic
Transformational Analysis of English Sentences: An
Implementation. International Journal of Computer
Mathematics, 5:267?283.
H. Ji and R. Grishman. 2006. Analysis and Repair of
Name Tagger Errors. In COLING/ACL 2006, Sydney,
Australia.
H. Ji, R. Grishman, D. Freitag, M. Blume, J. Wang,
S. Khadivi, R. Zens, and H. Ney. 2009. Name Transla-
tion for Distillation. In Global Autonomous Language
Exploitation. Springer.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In ACL 2007 Demon-
stration Session, Prague.
P. Laplace. 1816. Essai philosophique sur les probabil-
its. Courcier Imprimeur, Paris.
Adam Meyers, Roman Yangarber, and Ralph Grishman.
1996. Alignment of Shared Forests for Bilingual Cor-
pora. In Proceedings of Coling 1996: The 16th In-
ternational Conference on Computational Linguistics,
pages 460?465.
Adam Meyers, Roman Yangarber, Ralph Grishman,
Catherine Macleod, and Antonio Moreno-Sandoval.
1998. Deriving Transfer Rules from Dominance-
Preserving Alignments. In Proceedings of Coling-
ACL98: The 17th International Conference on Com-
putational Linguistics and the 36th Meeting of the As-
sociation for Computational Linguistics.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. Annotating
Noun Argument Structure for NomBank. In Proceed-
ings of LREC-2004, Lisbon, Portugal.
A. Meyers, M. Kosaka, N. Xue, H. Ji, A. Sun, S. Liao,
and W. Xu. 2009. Automatic Recognition of Logi-
cal Relations for English, Chinese and Japanese in the
GLARF Framework. In SEW-2009 at NAACL-HLT-
2009.
E. Miltsakaki, A. Joshi, R. Prasad, and B. Webber. 2004.
Annotating discourse connectives and their arguments.
In A. Meyers, editor, NAACL/HLT 2004 Workshop:
Frontiers in Corpus Annotation, pages 9?16, Boston,
Massachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
F. J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In ACL 2000.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
S. Petrov and D. Klein. 2007. Improved Inference for
Unlexicalized Parsing. In HLT-NAACL 2007.
L. Shen, J. Xu, and R. Weischedel. 2008. A New String-
to-Dependency Machine Translation Algorithm with a
Target Dependency Language Model. In ACL 2008.
J. Wagner, D. Seddah, J. Foster, and J. van Genabith.
2007. C-Structures and F-Structures for the British
National Corpus. In Proceedings of the Twelfth In-
ternational Lexical Functional Grammar Conference,
Stanford. CSLI Publications.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese syn-
tactic reordering for statistical machine translation. In
EMNLP-CoNLL 2007, pages 737?745.
D. Wu and P. Fung. 2009. Semantic roles for smt: A
hybrid two-pass model. In HLT-NAACL-2009, pages
13?16, Boulder, Colorado, June. Association for Com-
putational Linguistics.
N. Xue and M. Palmer. 2003. Annotating the Proposi-
tions in the Penn Chinese Treebank. In The Proceed-
ings of the 2nd SIGHAN Workshop on Chinese Lan-
guage Processing, Sapporo.
N. Xue. 2008. Labeling Chinese Predicates with Seman-
tic roles. Computational Linguistics, 34:225?255.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In ACL, pages 523?530.
Y. Zhang, R. Zens, and H. Ney. 2007. Chunk-Level
Reordering of Source Language Sentences with Auto-
matically Learned Rules for Statistical Machine Trans-
lation. In Proc. of NAACL/HLT 2007.
M. Zhang, H. Jiang, A. Aw, H. Li, C. L. Tan, and S. Li.
2008. A Tree Sequence Alignment-based Tree-to-Tree
Translation Model. In ACL 2008.
97
