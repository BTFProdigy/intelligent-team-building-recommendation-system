Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 182?183,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Self-training and co-training in biomedical word sense disambiguation
Antonio Jimeno-Yepes
National Library of Medicine
8600 Rockville Pike
Bethesda, 20894, MD, USA
antonio.jimeno@gmail.com
Alan R. Aronson
National Library of Medicine
8600 Rockville Pike
Bethesda, 20894, MD, USA
alan@nlm.nih.gov
Abstract
Word sense disambiguation (WSD) is an inter-
mediate task within information retrieval and
information extraction, attempting to select
the proper sense of ambiguous words. Due to
the scarcity of training data, semi-supervised
learning, which profits from seed annotated
examples and a large set of unlabeled data,
are worth researching. We present preliminary
results of two semi-supervised learning algo-
rithms on biomedical word sense disambigua-
tion. Both methods add relevant unlabeled ex-
amples to the training set, and optimal param-
eters are similar for each ambiguous word.
1 Introduction
Word sense disambiguation (WSD) is an interme-
diate task within information retrieval and informa-
tion extraction, attempting to select the proper sense
of ambiguous words. Supervised learning achieves
better performance compared to other WSD ap-
proaches (Jimeno-Yepes et al, 2011). Manual anno-
tation requires a large level of human effort whereas
there is a large quantity of unlabeled data. Our
work follows (Mihalcea, 2004) but is applied to the
biomedical domain; it relies on two semi-supervised
learning algorithms.
We have performed experiments of semi-
supervised learning for word sense disambiguation
in the biomedical domain. In the following section,
we present the evaluated algorithms. Then, we
present preliminary results for self-training and
co-training, which show a modest improvement
with a common set-up of the algorithms for the
evaluated ambiguous words.
2 Methods
For self-training we use the definition by (Clark et
al., 2003): ?a tagger that is retrained on its own
labeled cache on each round?. The classifier is
trained on the available training data which is then
used to label the unlabeled examples from which
the ones with enough prediction confidence are se-
lected and added to the training set. The process
is repeated for a number of predefined iterations.
Co-training (Blum and Mitchell, 1998) uses several
classifiers trained on independent views of the same
instances. These classifiers are then used to label the
unlabeled set, and from this newly annotated data
set the annotations with higher prediction probabil-
ity are selected. These newly labeled examples are
added to the training set and the process is repeated
for a number of iterations. Both bootstrapping algo-
rithms produce an enlarged training data set.
Co-training requires two independent views on
the same data set. As first view, we use the context
around the ambiguous word. As second view, we
use the MEDLINE MeSH indexing available from
PubMed which is obtained by human assignment of
MeSH heading based on their full-text articles.
Methods are evaluated with the accuracy mea-
sure on the MSH WSD set built automatically using
MeSH indexing from MEDLINE (Jimeno-Yepes et
al., 2011) 1 in which senses are denoted by UMLS
concept identifiers. To avoid any bias derived from
1Available from: http://wsd.nlm.nih.gov/collaboration.shtml
182
the indexing of the UMLS concept related to the am-
biguous word, the concept has been removed from
the MeSH indexing of the recovered citations.
10-fold cross validation using Na??ve Bayes (NB)
has been used to compare both views which achieve
similar accuracy (0.9386 context text, 0.9317 MeSH
indexing) while the combined view achieves even
better accuracy (0.9491).
In both algorithms a set of parameters is used: the
number of iterations (1-10), the size of the pool of
unlabeled examples (100, 500, 1000) and the growth
rate or number of unlabeled examples which are se-
lected to be added to the training set (1, 10, 20, 50,
100).
3 Results and discussion
Results shown in Table 1 have been obtained from
21 ambiguous words which achieved lower perfor-
mance in a preliminary cross-validation study. Each
ambiguous word has around 2 candidate senses with
100 examples for each sense. We have split the ex-
amples for each ambiguous word into 2/3 for train-
ing and 1/3 for test.
The baseline is NB trained and tested using this
split. Semi-supervised algorithms use this split, but
the training data is enlarged with selected unlabeled
examples. Self-training and the baseline use the
combined views while co-training relies on two NB
classifiers, each trained on one view of the train-
ing data. Even though we are willing to evalu-
ate other classifiers, NB was selected for this ex-
ploratory work since it is fast and space efficient.
Unlabeled examples are MEDLINE citations which
contain the ambiguous word and MeSH heading
terms. Any mention of MeSH heading related to the
ambiguous word has been removed. Optimal param-
eters were selected, and average accuracy is shown
in Table 1.
Method Accuracy
Baseline 0.8594
Self-training 0.8763 (1.93%)
Co-training 0.8759 (1.88%)
Table 1: Accuracy for the baseline, self-training and co-
training
Both semi-supervised algorithms show a modest
improvement on the baseline which is a bit higher
for self-training. Best results are achieved with a
small number of iterations (< 5), a small growth
rate (1-10) and a pool of unlabeled data over 100 in-
stances. Noise affects the performance with a larger
number of iterations, which after an initial increase,
shows a steep decrease in accuracy. Small growth
rate ensures a smoothed increase in accuracy. A
larger growth rate adds more noise after each iter-
ation. A larger pool of unlabeled data offers a larger
set of candidate unlabeled examples to choose from
at a higher computational cost.
4 Conclusions and Future work
Preliminary results show a modest improvement on
the baseline classifier. This means that the semi-
supervised algorithms have identified relevant dis-
ambiguated instances to be added to the training set.
We plan to evaluate the performance of these al-
gorithms on all the ambiguous words available in the
MSH WSD set. In addition, since the results have
shown that performance decreases rapidly after few
iterations, we would like to further explore smooth-
ing techniques applied to bootstrapping algorithms
and the effect on classifiers other than NB.
Acknowledgments
This work was supported by the Intramural Research
Program of the NIH, National Library of Medicine,
administered by ORISE.
References
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In Proceedings of the
eleventh annual conference on Computational learn-
ing theory, pages 92?100. ACM.
S. Clark, J.R. Curran, and M. Osborne. 2003. Bootstrap-
ping POS taggers using unlabelled data. In Proceed-
ings of the seventh conference on Natural language
learning at HLT-NAACL 2003-Volume 4, pages 49?55.
Association for Computational Linguistics.
A. Jimeno-Yepes, B.T. McInnes, and A.R. Aronson.
2011. Exploiting MeSH indexing in MEDLINE
to generate a data set for word sense disambigua-
tion(accepted). BMC bioinformatics.
R. Mihalcea. 2004. Co-training and self-training
for word sense disambiguation. In Proceedings of
the Conference on Computational Natural Language
Learning (CoNLL-2004).
183
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 102?110,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Using the argumentative structure of scientific literature to improve
information access
Antonio Jimeno Yepes
National ICT Australia
Victoria Research Laboratory
Melbourne, Australia
antonio.jimeno@gmail.com
James G. Mork
National Library of Medicine
8600 Rockville Pike
Bethesda, 20894, MD, USA
mork@nlm.nih.gov
Alan R. Aronson
National Library of Medicine
8600 Rockville Pike
Bethesda, 20894, MD, USA
alan@nlm.nih.gov
Abstract
MEDLINE/PubMed contains structured
abstracts that can provide argumentative
labels. Selection of abstract sentences
based on the argumentative label has
shown to improve the performance of in-
formation retrieval tasks. These abstracts
make up less than one quarter of all the
abstracts in MEDLINE/PubMed, so it is
worthwhile to learn how to automatically
label the non-structured ones.
We have compared several machine learn-
ing algorithms trained on structured ab-
stracts to identify argumentative labels.
We have performed an intrinsic evalua-
tion on predicting argumentative labels for
non-structured abstracts and an extrinsic
evaluation to predict argumentative labels
on abstracts relevant to Gene Reference
Into Function (GeneRIF) indexing.
Intrinsic evaluation shows that argumen-
tative labels can be assigned effectively
to structured abstracts. Algorithms that
model the argumentative structure seem
to perform better than other algorithms.
Extrinsic results show that assigning ar-
gumentative labels to non-structured ab-
stracts improves the performance on
GeneRIF indexing. On the other hand, the
algorithms that model the argumentative
structure of the abstracts obtain lower per-
formance in the extrinsic evaluation.
1 Introduction
MEDLINE R?/PubMed R? is the largest repository
of biomedical abstracts. The large quantity of
unstructured information available from MED-
LINE/PubMed prevents finding information effi-
ciently. Reducing the information that users need
to process could improve information access and
support database curation. It has been suggested
that identifying the argumentative label of the ab-
stract sentences could provide better information
through information retrieval (Ruch et al, 2003;
Jonnalagadda et al, 2012) and/or information ex-
traction (Mizuta et al, 2006).
Some journals indexed in MEDLINE/PubMed
already provide the abstracts in a structured for-
mat (Ripple et al, 2012). A structured abstract1 is
an abstract with distinct labeled sections (e.g., In-
troduction, Background, or Results). In the MED-
LINE/PubMed data, these labels usually appear in
all uppercase letters and are followed by a colon
(e.g., MATERIALS AND METHODS:). Structured
abstracts are becoming an increasingly larger seg-
ment of the MEDLINE/PubMed database with al-
most a quarter of all abstracts added to the MED-
LINE/PubMed database each year being struc-
tured abstracts. A recent PubMed query (April 22,
2013) shows 1,050,748 citations from 2012, and
249,196 (23.72%)2 of these are considered struc-
tured abstracts.
On August 16, 2010, PubMed began display-
ing structured abstracts formatted to highlight the
various sections within the structured abstracts to
help readers identify areas of interest3. The XML
formatted abstract from MEDLINE/PubMed sep-
arates each label in the structured abstract and in-
cludes a mapping to one of five U.S. National Li-
brary of Medicine (NLM) assigned categories as
shown in the example below:
<AbstractText Label=?MATERIALS AND
METHODS? NlmCategory=?METHODS?>
The five NLM categories that all labels
are mapped to are OBJECTIVE, CONCLU-
SIONS, RESULTS, METHODS, and BACK-
GROUND (Ripple et al, 2011). If a label is new
1http://www.nlm.nih.gov/bsd/policy/structured abstracts.html
2hasstructuredabstract AND 2012[pdat]
3http://www.nlm.nih.gov/pubs/techbull/ja10/ja10 structured abstracts.html
102
or not in the list of reviewed structured abstract la-
bels, it will receive a category of UNASSIGNED.
There are multiple criteria for deciding what ab-
stracts are considered structured abstracts or not.
One simple definition would be that an abstract
contains one or more author defined labels. A
more rigid criterion which is followed by NLM4
is that an abstract must contain three or more
unique valid labels (previously identified and cat-
egorized), and one of the labels must be an ending
type label (e.g., CONCLUSIONS). The five NLM
categories are normally manually reviewed and as-
signed once a year to as many new labels as pos-
sible. Currently, NLM has identified 1,949 (Au-
gust 31, 2012) unique labels and categorized them
into one of the five categories. These 1,949 labels
make up approximately 98% of all labels and la-
bel variations found in the structured abstracts in
MEDLINE/PubMed3. An example of structured
abstract is presented in Table 1.
Several studies have shown that the labels of the
structured abstracts can be reassigned effectively
based on a Conditional Random Field (CRF) mod-
els (Hirohata et al, 2008). On the other hand, it
is unclear if these models are as effective on non-
structured abstracts (Agarwal and Yu, 2009).
In this paper, we compare several learning al-
gorithms trained on structured abstract data to as-
sign argumentative labels to non-structured ab-
stracts. We performed comparison tests of the
trained models both intrinsically on a held out set
of the structured abstracts and extrinsically on a
set of non-structured abstracts.
The intrinsic evaluation is performed on a data
set of held out structured abstracts that have had
their label identification removed to model non-
structured abstracts. Argumentative labels are as-
signed to the sentences based on the trained mod-
els and used to identify label categorization.
The extrinsic evaluation is performed on a data
set of non-structured abstracts on the task of iden-
tifying GeneRIF (Gene Into Function) sentences.
Argumentative labels are assigned to the sentences
based on the trained models and used to perform
the selection of relevant GeneRIF sentences.
Intrinsic evaluation shows that argumentative
labels can be assigned effectively to structured ab-
stracts. Algorithms that model the argumentative
structure, like Conditional Random Field (CRF),
seem to perform better than other algorithms. Re-
4http://structuredabstracts.nlm.nih.gov/Implementation.shtml
sults show that using the argumentative labels as-
signed by the learning algorithms improves the
performance in GeneRIF sentence selection. On
the other hand, models like CRF, which better
model the argumentative structure of the struc-
tured abstracts, tend to perform below other learn-
ing algorithms on the extrinsic evaluation. This
shows that non-structured abstracts do not have the
same layout compared to structured ones.
2 Related work
As presented in the introduction, one of the ob-
jectives of our work is to assign structured ab-
stract labels to abstracts without these labels. The
idea is to help in the curation process of exist-
ing databases and to improve the efficiency of
information access. Previous work on MED-
LINE/PubMed abstracts has focused on learning
to identify these labels mainly in the Randomized
Control Trials (RCT) domain. (McKnight and
Srinivasan, 2003) used a Support Vector Machine
(SVM) and a linear classifier and tried to pre-
dict the labels of MEDLINE structured abstracts.
Their work finds that it is possible to learn a model
to label the abstract with modest results. Further
studies have been conducted by (Ruch et al, 2003;
Tbahriti et al, 2005; Ruch et al, 2007) to use
the argumentative model of the abstracts. They
have used this to improve retrieval and indexing of
MEDLINE citations, respectively. In their work,
they have used a multi-class Na??ve Bayes classi-
fier.
(Hirohata et al, 2008) have shown that the la-
bels in structured abstracts follow a certain argu-
mentative structure. Using the current set of labels
used at the NLM, a typical argumentative struc-
ture consists of OBJECTIVE, METHODS, RE-
SULTS and CONCLUSION. This notion is some-
what already explored by (McKnight and Srini-
vasan, 2003) by using the position of the sentence.
More advanced approaches have been used that
train a model that considers the sequence of labels
in the structured abstracts. (Lin et al, 2006) used a
generative model, comparing them to discrimina-
tive ones. More recent work has been dealing with
Conditional Random Fields (Hirohata et al, 2008)
with good performance.
(Agarwal and Yu, 2009) used similar ap-
proaches and evaluated the labeling of full text
articles with the trained model on structured ab-
stracts. Their evaluation included as well a set of
103
<Abstract><AbstractText Label=?PURPOSE? NlmCategory=?OBJECTIVE?>To explore the effects of cervical loop
electrosurgical excision procedure (LEEP) or cold knife conization (CKC) on pregnancy outcomes.</AbstractText>
<AbstractText Label=?MATERIALS AND METHODS? NlmCategory=?METHODS?>Patients with cervical intraep-
ithelial neoplasia (CIN) who wanted to become pregnant and received LEEP or CKC were considered as the treat-
ment groups. Women who wanted to become pregnant and only underwent colposcopic biopsy without any treat-
ments were considered as the control group. The pregnancy outcomes were observed and compared in the three
groups.</AbstractText>
<AbstractText Label=?RESULTS? NlmCategory=?RESULTS?>Premature delivery rate was higher (p = 0.048) in the
CKC group (14/36, 38.88%) than in control group (14/68, 20.5%) with a odds ratio (OR) of 2.455 (1.007 - 5.985);
and premature delivery was related to cone depth, OR was significantly increased when the cone depth was more than
15 mm. There was no significant difference in premature delivery between LEEP (10 / 48, 20.83%) and the control
groups. The average gestational weeks were shorter (p = 0.049) in the CKC group (36.9 +/- 2.4) than in the control
group (37.8 +/- 2.6), but similar in LEEP (38.1 +/- 2.4) and control groups. There were no significant differences
in cesarean sections between the three groups. The ratio of neonatal birth weight less than 2,500 g was significantly
higher (p = 0.005) in the CKC group (15/36) than in the control group (10/68), but similar in the LEEP and control
groups.</AbstractText>
<AbstractText Label=?CONCLUSION? NlmCategory=?CONCLUSIONS?>Compared with CKC, LEEP is relatively
safe. LEEP should be a priority in the treatment of patients with CIN who want to become pregnant.</AbstractText>
</Abstract>
Table 1: XML example for PMID 23590007
abstracts manually annotated. They found that the
performance on full-text was below what was ex-
pected. A similar result was found in the manu-
ally annotated set. They found, as well, that the
abstract sentences are noisy and sometimes the
sentences from structured abstracts did not belong
with the label they were assigned to.
A large number of abstracts in MEDLINE are
not structured; thus intrinsic evaluation of the al-
gorithms trained to predict the argumentative la-
bels on structured abstracts is not completely real-
istic. Extrinsic evaluation has been previously per-
formed by (Ruch et al, 2003; Tbahriti et al, 2005;
Ruch et al, 2007) in information retrieval results
evaluating a Na??ve Bayes classifier. We have ex-
tended this work by evaluating a larger set of al-
gorithms and heuristics on a data set developed
to tune and evaluate a system for GeneRIF index-
ing on a data set containing mostly non-structured
abstracts. The idea is that GeneRIF relevant sen-
tences will be assigned distinctive argumentative
labels.
A Gene Reference Into Function (GeneRIF) de-
scribes novel functionality of genes. The cre-
ation of GeneRIF entries involves the identifica-
tion of the genes mentioned in MEDLINE cita-
tions and the citation sentences describing a novel
function. GeneRIFs are available from the NCBI
(National Center for Biotechnology Information)
Gene database5. An example sentence is shown
below linked to the BRCA1 gene with gene id
672 from the citation with PubMed R? identifier
(PMID) 22093627:
5http://www.ncbi.nlm.nih.gov/sites/entrez?db=gene
FISH-positive EGFR expression is associated
with gender and smoking status, but not
correlated with the expression of ERCC1 and
BRCA1 proteins in non-small cell lung cancer.
There is limited previous work related to
GeneRIF span extraction. Most of the available
publications are related to the TREC Genomics
Track in 2003 (Hersh and Bhupatiraju, 2003).
There were two main tasks in this track, the first
one consisted of identifying relevant citations to
be considered for GeneRIF annotation.
In the second task, the participants had to pro-
vide spans of text that would correspond to rel-
evant GeneRIF annotations for a set of citations.
Considering this second task, the participants were
not provided with a training data set. The Dice
coefficient was used to measure the similarity be-
tween the submitted span of text from the title and
abstract of the citation and the official GeneRIF
text in the test set.
Surprisingly, one of the main conclusions was
that a very competitive system could be obtained
by simply delivering the title of the citation as the
best GeneRIF span of text. Few teams (EMC (Je-
lier et al, 2003) and Berkley (Bhalotia et al, 2003)
being exceptions), achieved results better than that
simple strategy. Another conclusion of the Ge-
nomics Track was that the sentence position in the
citation is a good indicator for GeneRIF sentence
identification: either the title or sentences close to
the end of the citation were found to be the best
candidates.
Subsequent to the 2003 Genomics Track, there
has been some further work related to GeneRIF
104
sentence selection. (Lu et al, 2006; Lu et al,
2007) sought to reproduce the results already
available from Entrez Gene (former name for the
NCBI Gene database). In their approach, a set
of features is identified from the sentences and
used in the algorithm: Gene Ontology (GO) to-
ken matches, cue words and sentence position in
the abstract. (Gobeill et al, 2008) combined argu-
mentative features using discourse-analysis mod-
els (LASt) and an automatic text categorizer to
estimate the density of Gene Ontology categories
(GOEx). The combination of these two feature
sets produced results comparable to the best 2003
Genomics Track system.
3 Methods
As in previous work, we approach the problem
of learning to label sentences in abstracts us-
ing machine learning methods on structured ab-
stracts. We have compared a large range of ma-
chine learning algorithms, including Conditional
Random Field. The evaluation is performed in-
trinsically on a held out set of structured abstracts
and then evaluated extrinsically on a dataset devel-
oped for the evaluation of algorithms for GeneRIF
indexing.
3.1 Structured abstracts data set
This data set is used to train the machine learning
algorithms and to peform the intrinsic evaluation
of structured abstracts. The abstracts have been
collected from PubMed using the query hasstruc-
turedabstract, selecting the top 100k citations sat-
ifying the query.
The abstract defined within the Abstract at-
tribute is split into several AbstractText tags. Each
AbstractText tag has the label Label that shows
the original label as provided by the journal while
the NlmCategory represents the category as added
by the NLM.
From this set, 2/3 of the citations (66,666) are
considered for training the machine learning algo-
rithms while 1/3 of the citations (33,334) are re-
served for testing. The abstract paragraphs have
been split into sentences and the structured ab-
stract label has been transferred to them. For in-
stance, all the sentences in the INTRODUCTION
section are labeled as INTRODUCTION.
An analysis of the abstracts has shown that there
are cases in which the article keywords were in-
cluded as part of the abstract in a BACKGROUND
section. These were easily recognized by the orig-
inal label KEYWORD. We have removed these
paragraphs since they are not typical sentences
in MEDLINE but a list of keywords. We find
that there are sections like OBJECTIVE where the
number of sentences is very low, with less than 2
sentences on average, while RESULTS is the sec-
tion with the largest number of sentences on aver-
age with over 4.5 sentences.
There are five candidate labels identified from
the structured abstracts, presented in Table 2. The
distribution of labels shows that some labels like
CONCLUSIONS, METHODS and RESULTS are
very frequent. CONCLUSIONS and METHODS
are assigned to more than one paragraph since the
number is bigger compared to the number of cita-
tions in each set. This seems to happen when more
than one journal label in the same citation map
to METHODS or CONCLUSION, e.g. PMID:
23538919.
Label Paragraphs Sentences
BACKGROUND 53,348 132,890
CONCLUSIONS 101,830 205,394
METHODS 107,227 304,487
OBJECTIVE 60,846 95,547
RESULTS 95,824 436,653
Table 2: Structured abstracts data set statistics
We have compared the performance of sev-
eral learning algorithms. Among other classi-
fiers, we use Na??ve Bayes and Linear Regression,
which might be seen as a generative learner ver-
sus discriminative (Jordan, 2002) learner. We have
used the implementation available from the Mallet
package (McCallum, 2002).
In addition to these two classifiers, we have
used AdaBoostM1 and SVM. SVM has been
trained using stochastic gradient descent (Zhang,
2004), which is very efficient for linear ker-
nels. Table 2 shows a large imbalance between
the labels, so we have used the modified Huber
Loss (Zhang, 2004), which has already been used
in the context of MeSH indexing (Yeganova et al,
2011). Both algorithms were trained based on the
one-versus-all approach. We have turned the algo-
rithms into multi-class classifiers by selecting the
prediction with the highest confidence by the clas-
sifiers (Tsoumakas and Katakis, 2007). We have
used the implementation of these algorithms avail-
105
able from the MTI ML package6, previously used
in the task of MeSH indexing (Jimeno-Yepes et al,
2012).
The learning algorithms have been trained on
the text of the paragraph or sentences from the
data set presented above. The text is lowercased
and tokenized. In addition to the textual features,
the position of the sentence or paragraph from the
beginning of the abstract is used as well.
As we have seen, argumentative structure of the
abstract labels has been previously modeled using
a linear chain CRF (Lafferty et al, 2001). CRF
is trained using the text features from sentences or
paragraphs in conjunction of the abstract labels to
perform the label assignment. In our experiments,
we have used the implementation available from
the Mallet package, using only an order 1 model.
3.2 GeneRIF data set
We have developed a data set to compare and
evaluate GeneRIF indexing approaches (Jimeno-
Yepes et al, 2013) as part of the Gene Indexing
Assistant project at the NLM7. The current scope
of our work is limited to the human species. The
development is performed in two steps described
below. The first step consists of selecting cita-
tions from journals typically associated with hu-
man species. During the second step, we apply
Index Section rules for citation filtering plus ad-
ditional rules to further focus the set of selected
citations. Since there was no GeneRIF indexing
before 2002, only articles from 2002 through 2011
from the 2011 MEDLINE Baseline 8 (11/19/2010)
were used to build the data set.
A subset of the filtered citations was collected
for annotation. The annotations were performed
by two annotators. Guidelines were prepared and
tested on a small set by the two annotators and re-
fined before annotating the entire set.
The data set has been annotated with GeneRIF
categories of the sentences. The categories are:
Expression, Function, Isolation, Non-GeneRIF,
Other, Reference, and Structure. We assigned the
GeneRIF category to all the categories that did
not belong to Non-GeneRIF. The indexing task is
then to categorize the sentences into GeneRIF sen-
tences and Non-GeneRIF ones. Based on their an-
notation work on the data set, the F-measure for
6http://ii.nlm.nih.gov/MTI ML/index.shtml
7http://www.lhncbc.nlm.nih.gov/project/automated-
indexing-research
8http://mbr.nlm.nih.gov
the annotators is 0.81. We have used this annota-
tion for the extrinsic evaluation of GeneRIF index-
ing.
This data set has been further split into training
and testing subsets. Table 3 shows the distribution
between GeneRIF and Non-GeneRIF sentences.
Set Total GeneRIF Non-GeneRIF
Training 1987 829 (42%) 1158 (58%)
Testing 999 433 (43%) 566 (57%)
Table 3: GeneRIF sentence distribution
In previous work, the indexing of GeneRIF sen-
tences, on our data set, was performed based on
a trained classifier on a set of features that per-
formed well on the GeneRIF testing set (Jimeno-
Yepes et al, 2013). Na??ve Bayes was the learning
algorithm that performed the best compared to the
other methods and has been selected in this work
as the method to be used to combine the features
of the argumentative labeling algorithms.
The set of features in the baseline experiments
include the position of the sentence from the be-
ginning of the abstract, the position of the sentence
counting from the end of the abstract, the sen-
tence text, the annotation of disease terms, based
on MetaMap (Aronson and Lang, 2010), and gene
terms, based on a dictionary approach, and the
Gene Ontology term density (Gobeill et al, 2008).
4 Results
As mentioned before, we have performed the eval-
uation of the algorithms intrinsically, given a set
of structured abstracts, and extrinsically based on
their performance on GeneRIF sentence indexing.
4.1 Intrinsic evaluation (structured
abstracts)
Tables 4 and 5 show the results of the intrinsic
evaluation for paragraph and sentence experiments
respectively. The algorithms are trained to label
the paragraphs or sentences from the structured
abstracts. The precision (P), recall (R) and F1
(F) values are presented for each argumentative la-
bel. The methods evaluated include Na??ve Bayes
(NB), Logistic Regression (LR), SVM based on
modified Huber Loss (Huber) and AdaBoostM1
(ADA). These methods have been trained on the
text of either the sentence or the paragraph, and
might include their position feature, indicated with
the letter P (e.g. NB P for Na??ve Bayes trained
106
Label NB NB P LR LR P ADA ADA P Huber HuberP CRF
BACKGROUND P 0.6047 0.6853 0.6374 0.7369 0.6098 0.7308 0.5862 0.7166 0.7357
R 0.5672 0.7190 0.5868 0.7207 0.3676 0.7337 0.4984 0.6694 0.7093
F 0.5854 0.7017 0.6110 0.7287 0.4587 0.7323 0.5387 0.6922 0.7223
CONCLUSIONS P 0.7532 0.8626 0.8365 0.9413 0.6975 0.8862 0.7578 0.9051 0.9769
R 0.8606 0.9366 0.8675 0.9552 0.8246 0.9404 0.7987 0.9340 0.9784
F 0.8033 0.8981 0.8517 0.9482 0.7557 0.9125 0.7777 0.9193 0.9776
METHODS P 0.9002 0.9278 0.9113 0.9396 0.8256 0.9041 0.8668 0.9116 0.9684
R 0.9040 0.9126 0.9294 0.9493 0.8955 0.9250 0.9012 0.9237 0.9675
F 0.9021 0.9201 0.9203 0.9444 0.8591 0.9144 0.8837 0.9176 0.9680
OBJECTIVE P 0.7294 0.7650 0.7167 0.7531 0.6763 0.7565 0.6788 0.7160 0.7608
R 0.6453 0.7190 0.7255 0.7549 0.6937 0.7228 0.6733 0.7365 0.7759
F 0.6848 0.7413 0.7210 0.7540 0.6849 0.7393 0.6761 0.7261 0.7683
RESULTS P 0.8841 0.9106 0.9086 0.9372 0.8554 0.9157 0.8560 0.9122 0.9692
R 0.8414 0.8542 0.8857 0.9216 0.7842 0.8564 0.8447 0.8846 0.9758
F 0.8622 0.8815 0.8970 0.9294 0.8182 0.8851 0.8503 0.8981 0.9725
Average P 0.7743 0.8303 0.8021 0.8616 0.7329 0.8387 0.7491 0.8323 0.8822
R 0.7637 0.8283 0.7990 0.8604 0.7131 0.8357 0.7433 0.8296 0.8814
F 0.7690 0.8293 0.8005 0.8610 0.7229 0.8372 0.7462 0.8310 0.8818
Table 4: Intrinsic evaluation of paragraph based labeling
Label NB NB P LR LR P ADA ADA P Huber HuberP CRF
BACKGROUND P 0.4983 0.6313 0.5558 0.6862 0.4779 0.6417 0.5153 0.6495 0.6738
R 0.4980 0.6921 0.5084 0.7139 0.3207 0.6993 0.3372 0.6554 0.7104
F 0.4981 0.6603 0.5311 0.6998 0.3838 0.6693 0.4076 0.6524 0.6916
CONCLUSIONS P 0.5876 0.7270 0.6794 0.8431 0.5672 0.7651 0.6153 0.7767 0.8977
R 0.7103 0.8388 0.6788 0.8187 0.4998 0.6816 0.5163 0.7213 0.8671
F 0.6431 0.7789 0.6791 0.8307 0.5314 0.7209 0.5615 0.7480 0.8821
METHODS P 0.7857 0.8206 0.8193 0.8549 0.7224 0.7793 0.7343 0.7894 0.8931
R 0.8084 0.8366 0.8427 0.8696 0.7789 0.8152 0.7828 0.8250 0.8988
F 0.7969 0.8285 0.8308 0.8622 0.7496 0.7968 0.7578 0.8068 0.8960
OBJECTIVE P 0.5522 0.6237 0.6032 0.6696 0.5497 0.6671 0.5525 0.6259 0.6258
R 0.4894 0.5530 0.4995 0.5534 0.4082 0.4518 0.4479 0.5036 0.5779
F 0.5189 0.5862 0.5465 0.6060 0.4685 0.5388 0.4947 0.5581 0.6009
RESULTS P 0.8294 0.8517 0.8071 0.8449 0.6903 0.7665 0.6957 0.7877 0.8892
R 0.7517 0.7743 0.8429 0.8679 0.7998 0.8143 0.6957 0.8208 0.8995
F 0.7886 0.8112 0.8246 0.8563 0.7410 0.7897 0.6957 0.8039 0.8943
Average P 0.6506 0.7309 0.6930 0.7797 0.6015 0.7239 0.6226 0.7258 0.7959
R 0.6516 0.7390 0.6745 0.7647 0.5615 0.6924 0.5560 0.7052 0.7907
F 0.6511 0.7349 0.6836 0.7721 0.5808 0.7078 0.5874 0.7154 0.7933
Table 5: Intrinsic evaluation of sentence based labeling
with the features from text and the position). The
results include those based on CRF trained on the
text of either the sentence or the paragraph taking
into account the labeling sequence.
CRF has the best performance in both tables,
with the differences being more dramatic on the
paragraph results. These results are comparable
to (Hirohata et al, 2008), even though we are
working with a different set of labels. Compar-
ing the remaining learning algorithms, LR per-
forms better than the other classifiers. Both Ad-
aBoostM1 and SVM perform not as well as NB
and LR; this could be due to the noise referred
to by (Agarwal and Yu, 2009) that appears in the
structured abstract sentences. Considering either
the paragraph or the sentence text, the position in-
formation helps improve their performance.
CONCLUSIONS, METHODS and RESULTS
labels have the best performance, which matches
the most frequent labels in the dataset (see Ta-
ble 2). BACKGROUND and OBJECTIVE have
worse performance compared to the other labels.
These two labels have the largest imbalance com-
pared to the other labels, which seems to nega-
tively impact the classifiers performance.
The results based on the paragraphs outperform
the ones based on the sentences. Argumentative
structure of the paragraphs seems to be easier,
probably due to the fact that individual sentences
have been shown to be noisy (Agarwal and Yu,
2009), and this could explain this behaviour.
107
4.2 Extrinsic evaluation (GeneRIFs)
Extrinsic evaluation is performed on the GeneRIF
data set presented in the Methods section. The
idea of the evaluation is to assign one of the ar-
gumentative labels to the sentences, based on the
models trained on structured abstracts, and eval-
uate the impact of this assignment in the selec-
tion of GeneRIF sentences. From the set of ma-
chine learning algorithms intrinsically evaluated,
we have selected the LR models trained with and
without position information (Pos) and the CRF
model. The LR and CRF models are used to la-
bel the GeneRIF training and testing data with the
argumentative labels.
Table 6 shows the results of the extrinsic evalu-
ation. Results obtained with the argumentative la-
bel feature and with or without the set of features
used in the baseline are compared to the baseline
model, i.e. NB and the set of features presented
in the Methods section. In all the cases, precision
(P), recall (R) and F1 using the argumentative fea-
tures improve over the baseline.
The intrinsic evaluation was performed either
on sentences or paragraphs. The sentence mod-
els perform better than the paragraph based mod-
els. We find as well that LR with sentence position
performs slightly better than when combined with
the baseline features, with higher recall but lower
precision. Contrary to the intrinsic results, LR per-
forms better than CRF, even though both outper-
form the baseline. This means that non-structured
sentences do not necessarily follow the same argu-
mentative structure as the structured abstracts.
Label P R F
Baseline 0.6210 0.6605 0.6405
LR Par 0.7235 0.6767 0.6993
LR Par + Base 0.7184 0.8014 0.7576
LR Par Pos 0.5978 0.8891 0.7149
LR Par Pos + Base 0.6883 0.8060 0.7426
LR Sen 0.7039 0.7852 0.7424
LR Sen + Base 0.7325 0.7968 0.7633
LR Sen Pos 0.7014 0.9007 0.7887
LR Sen Pos + Base 0.7222 0.8406 0.7769
CRF Par 0.6682 0.6744 0.6713
CRF Par + Base 0.7036 0.8060 0.7513
CRF Sen 0.6536 0.8499 0.7390
CRF Sen + Base 0.7134 0.7875 0.7486
Table 6: GeneRIF extrinsic evaluation
5 Discussion
Results show that it is possible to automatically
predict the argumentative label of the structured
abstracts and to improve the performance for
GeneRIF annotation. Intrinsic evaluation shows
that paragraph labeling is easier compared to sen-
tence labeling, which might be partly due to the
noise in the sentences as identified by (Agarwal
and Yu, 2009). The excellent performance for
paragraph labeling was already shown by previous
work (Hirohata et al, 2008) while sentence label-
ing issues for structured abstracts was previously
introduced by (Agarwal and Yu, 2009). In both in-
trinsic tasks, adding the position of the paragraph
or sentence improves the performance of the learn-
ing algorithms.
Extrinsic evaluation shows that, compared to
the baseline features for GeneRIF annotation,
adding argumentative labeling using the trained
models improves its performance, which is close
to the human performance reported in the Meth-
ods section. On the other hand, we find that the
CRF models show lower performance compared
to the LR models. From the LR models, the po-
sition of the sentence or paragraph seems to have
better performance.
In addition, the LR model trained on the sen-
tences performs better compared to the model
trained on the paragraphs. This might be partly
due to the fact that sentence based models seem
to be better suited than the paragraph based ones
as might have been expected. The fact that the
CRF models performance is below the LR mod-
els denotes that the structured abstracts seem to
follow a pattern that is different in the case of
non-structured abstracts. Looking closer at the
assigned labels, the LR models tend to assign
more CONCLUSIONS and RESULTS labels to
the GeneRIF sentences compared to the CRF ones.
6 Conclusions and Future Work
We have presented an evaluation of several learn-
ing algorithms to label abstract text in MED-
LINE/PubMed with argumentative labels, based
on MEDLINE/PubMed structured abstracts. The
results show that this task can be achieved with
high performance in the case of labeling the para-
graphs but this is not the same in the case of sen-
tences. This intrinsic evaluation was performed on
structured abstracts, and in this set the CRF mod-
els seem to perform much better compared to the
108
other models that do not use the labeling sequence.
On the other hand, when applying the trained
models to MEDLINE/PubMed non-structured ab-
stracts, we find that the extrinsic evaluation of
these labeling on the GeneRIF task shows lower
performance for the CRF models. This indicates
that the structured abstracts follow a pattern that
non-structured ones do not follow. The extrin-
sic evaluation shows that labeling the sentences
with argumentative labels improves the indexing
of GeneRIF sentences. The argumentative labels
help identifying target sentences for the GeneRIF
indexing, but more refined labels learned from
non-structured abstracts could provide better per-
formance. An idea to extend this research would
be evaluating the latent discovery of section labels
and to apply this labeling to the proposed GeneRIF
task and to other tasks, e.g. MeSH indexing. La-
tent labels might accommodate better the argu-
mentative structure of non-structured abstracts.
As shown in this work, the argumentative lay-
out of non-structured abstracts and structured ab-
stracts is not the same. There is still the open ques-
tion if there is any layout regularity in the non-
structured abstracts that could be exploited to im-
prove information access.
7 Acknowledgements
NICTA is funded by the Australian Government
as represented by the Department of Broadband,
Communications and the Digital Economy and
the Australian Research Council through the ICT
Centre of Excellence program.
This work was also supported in part by the In-
tramural Research Program of the NIH, National
Library of Medicine.
References
S Agarwal and H Yu. 2009. Automatically classifying
sentences in full-text biomedical articles into Intro-
duction, Methods, Results and Discussion. Bioin-
formatics, 25(23):3174?3180.
A R Aronson and F M Lang. 2010. An overview
of MetaMap: historical perspective and recent ad-
vances. Journal of the American Medical Informat-
ics Association, 17(3):229?236.
G. Bhalotia, PI Nakov, A S Schwartz, and M A Hearst.
2003. BioText team report for the TREC 2003 ge-
nomics track. In Proceedings of TREC. Citeseer.
J Gobeill, I Tbahriti, F Ehrler, A Mottaz, A Veuthey,
and P Ruch. 2008. Gene Ontology density estima-
tion and discourse analysis for automatic GeneRiF
extraction. BMC Bioinformatics, 9(Suppl 3):S9.
W Hersh and R T Bhupatiraju. 2003. TREC genomics
track overview. In TREC 2003, pages 14?23.
K Hirohata, Naoaki Okazaki, Sophia Ananiadou, Mit-
suru Ishizuka, and Manchester Interdisciplinary
Biocentre. 2008. Identifying sections in scientific
abstracts using conditional random fields. In Proc.
of 3rd International Joint Conference on Natural
Language Processing, pages 381?388.
R Jelier, M Schuemie, C Eijk, M Weeber, E Mulligen,
B Schijvenaars, B Mons, and J Kors. 2003. Search-
ing for GeneRIFs: concept-based query expansion
and Bayes classification. In Proceedings of TREC,
pages 167?174.
A Jimeno-Yepes, J G Mork, D Demner-Fushman, and
A R Aronson. 2012. A One-Size-Fits-All Indexing
Method Does Not Exist: Automatic Selection Based
on Meta-Learning. Journal of Computing Science
and Engineering, 6(2):151?160.
A Jimeno-Yepes, J C Sticco, J G Mork, and A R Aron-
son. 2013. GeneRIF indexing: sentence selection
based on machine learning. BMC Bioinformatics,
14(1):147.
S Jonnalagadda, G D Fiol, R Medlin, C Weir, M Fisz-
man, J Mostafa, and H Liu. 2012. Automatically
extracting sentences from medline citations to sup-
port clinicians? information needs. In Healthcare
Informatics, Imaging and Systems Biology (HISB),
2012 IEEE Second International Conference on,
pages 72?72. IEEE.
A Jordan. 2002. On discriminative vs. generative
classifiers: A comparison of logistic regression and
naive bayes. Advances in neural information pro-
cessing systems, 14:841.
J D Lafferty, A McCallum, and F Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceed-
ings of the Eighteenth International Conference on
Machine Learning, ICML ?01, pages 282?289, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
J Lin, D Karakos, D Demner-Fushman, and S Khudan-
pur. 2006. Generative content models for structural
analysis of medical abstracts. In Proceedings of the
HLT-NAACL BioNLP Workshop on Linking Natural
Language and Biology, pages 65?72. Association
for Computational Linguistics.
Z Lu, K B Cohen, and L Hunter. 2006. Finding
GeneRIFs via gene ontology annotations. In Pacific
Symposium on Biocomputing. Pacific Symposium on
Biocomputing, page 52. NIH Public Access.
Z Lu, K B Cohen, and L Hunter. 2007. GeneRIF qual-
ity assurance as summary revision. In Pacific Sym-
posium on Biocomputing, page 269. NIH Public Ac-
cess.
109
A McCallum. 2002. Mallet: A machine learning for
language toolkit. URL http://mallet.cs.umass.edu.
L McKnight and P Srinivasan. 2003. Categorization
of sentence types in medical abstracts. In AMIA An-
nual Symposium Proceedings, volume 2003, page
440. American Medical Informatics Association.
Y Mizuta, A Korhonen, T Mullen, and N Collier. 2006.
Zone analysis in biology articles as a basis for infor-
mation extraction. International journal of medical
informatics, 75(6):468?487.
A M Ripple, J G Mork, L S Knecht, and B L
Humphreys. 2011. A retrospective cohort study
of structured abstracts in MEDLINE, 1992?2006.
Journal of the Medical Library Association: JMLA,
99(2):160.
A M Ripple, J G Mork, J M Rozier, and L S Knecht.
2012. Structured Abstracts in MEDLINE: Twenty-
Five Years Later.
P Ruch, C Chichester, G Cohen, G Coray, F Ehrler,
H Ghorbel, and V Mu?ller, Hand Pallotta. 2003. Re-
port on the TREC 2003 experiment: Genomic track.
TREC-03.
P Ruch, A Geissbuhler, J Gobeill, F Lisacek, I Tbahriti,
A Veuthey, and A R Aronson. 2007. Using dis-
course analysis to improve text categorization in
MEDLINE. Studies in health technology and infor-
matics, 129(1):710.
I Tbahriti, C Chichester, F Lisacek, and P Ruch. 2005.
Using argumentation to retrieve articles with similar
citations: An inquiry into improving related articles
search in the MEDLINE digital library. In Interna-
tional Journal of Medical Informatics. Citeseer.
G Tsoumakas and I Katakis. 2007. Multi-label clas-
sification: An overview. International Journal of
Data Warehousing and Mining (IJDWM), 3(3):1?13.
L Yeganova, Donald C Comeau, W Kim, and J Wilbur.
2011. Text mining techniques for leveraging posi-
tively labeled data. In Proceedings of BioNLP 2011
Workshop, pages 155?163. Association for Compu-
tational Linguistics.
T Zhang. 2004. Solving large scale linear predic-
tion problems using stochastic gradient descent al-
gorithms. In Proceedings of the twenty-first inter-
national conference on Machine learning, page 116.
ACM.
110
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 35?44,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Extracting Biomedical Events and Modifications Using Subgraph
Matching with Noisy Training Data
Andrew MacKinlay?, David Martinez?, Antonio Jimeno Yepes?,
Haibin Liu?, W. John Wilbur? and Karin Verspoor?
? NICTA Victoria Research Laboratory, University of Melbourne, Australia
{andrew.mackinlay, david.martinez}@nicta.com.au
{antonio.jimeno, karin.verspoor}@nicta.com.au
? National Center for Biotechnology Information, Bethesda, MD, USA
haibin.liu@nih.gov, wilbur@ncbi.nlm.nih.gov
Abstract
The Genia Event (GE) extraction task of
the BioNLP Shared Task addresses the ex-
traction of biomedical events from the nat-
ural language text of the published litera-
ture. In our submission, we modified an
existing system for learning of event pat-
terns via dependency parse subgraphs to
utilise a more accurate parser and signifi-
cantly more, but noisier, training data. We
explore the impact of these two aspects of
the system and conclude that the change in
parser limits recall to an extent that cannot
be offset by the large quantities of training
data. However, our extensions of the sys-
tem to extract modification events shows
promise.
1 Introduction
In this paper, we describe our submission to the
Genia Event (GE) information extraction subtask
of the BioNLP Shared Task. This task requires the
development of systems that are capable of iden-
tifying bio-molecular events as those events are
expressed in full-text publications. The task rep-
resents an important contribution to the broader
problem of converting unstructured information
captured in the biomedical literature into struc-
tured information that can be used to index and
analyse bio-molecular relationships.
This year?s task builds on previous instantia-
tions of this task (Kim et al, 2009; Kim et al,
2012), with only minor changes in the task defini-
tion introduced for 2011. The task organisers pro-
vided full text publications annotated with men-
tions of biological entities including proteins and
genes, and asked participants to provide annota-
tions of simple events including gene expression,
binding, localization, and protein modification, as
well as higher-order regulation events (e.g., pos-
itive regulation of gene expression). In our sub-
mission, we built on a system originally developed
for the BioNLP-ST 2011 (Liu et al, 2011) and ex-
tended in more recent work (Liu et al, 2013a; Liu
et al, 2013b). This system learns to recognise sub-
graphs of syntactic dependency parse graphs that
express a given bio-molecular event, and matches
those subgraphs to new text using an algorithm
called Approximate Subgraph Matching.
Due to the method?s fundamental dependency
on the syntactic dependency parse of the text, in
this work we set out to explore the impact of
substituting the previously employed dependency
parsers with a different parser which has been
demonstrated to achieve higher performance than
other commonly used parsers for full-text biomed-
ical literature (Verspoor et al, 2012).
In addition, we aimed to address the relatively
lower recall of the method through incorporation
of large quantities of external training data, ac-
quired through integration of previously automat-
ically extracted bio-molecular events available in
a web repository of such extracted events, EVEX
(Van Landeghem et al, 2011; Van Landeghem
et al, 2012), and additional bio-molecular events
generated from a large sample of full text pub-
lications using one of the state-of-the-art event
extraction systems, TEES (Bjo?rne and Salakoski,
2011). Since the performance of the subgraph
matching method, as an instance-based learning
strategy (Alpaydin, 2004), is dependent on having
good training examples that express the events in a
range of syntactic structures, the motivation under-
lying this was to increase the amount of training
data available to the system, even if that data was
derived from a less-than-perfect source. The aug-
mentation of training corpora with external unla-
belled data that is automatically processed to gen-
erate additional labels has been explored for re-
training the same system, in an approach known as
self-training. This approach has been shown to be
35
very effective for improving parsing performance
(McClosky et al, 2006; McClosky and Charniak,
2008). Self-training of the TEES system has been
previously explored (Bjorne et al, 2012), with
somewhat mixed results, but with evidence sug-
gesting it could be useful with an appropriate strat-
egy for selecting training examples. Here, rather
than training our system with its own output over
external data, we explore a semi-supervised learn-
ing approach in which we train our system with the
outputs of a different system (TEES) over external
data.
2 Methodology
2.1 Base Event Extraction System
The event extraction algorithm is essentially the
same as the one used in Liu et al (2013b). A fuller
description can be found there, but we summarise
the most important aspects of it here.
2.1.1 Event Extraction with ASM
The principal method used in event extraction is
Approximate Subgraph Matching, or ASM (Liu et
al., 2013a). Broadly, we learn subgraph patterns
from the event structures in the training data, and
then apply them by looking for matches with the
patterns of the learned rules, using ASM to allow
for non-exact matches of the patterns.
The first stage in this is learning the rules which
link subgraphs to associated patterns. The input
is a set of dependency-parsed articles (the setup
is described in ?2.1.2), and a set of gold-standard
annotations of proteins and events in the shared
task format. Using the standoff annotations in the
training data, every protein and trigger is mapped
to one or more nodes in the corresponding depen-
dency graphs. In addition, the textual content of
every protein is replaced with a generic string en-
abling abstraction over individual protein names.
Then, for each event annotation in the training
data, we retrieve the nodes from the graph corre-
sponding to the associated trigger and protein en-
tities. We determine the shortest path (or paths, in
case of a tie) connecting the graph trigger to each
of the event argument nodes. For arguments which
are themselves events (e.g., for regulatory events),
the node corresponding to the trigger of the event
argument is used instead of a protein node. Where
there are multiple arguments, we take the union of
the shortest paths to each individual argument.
This path is then used as the pattern compo-
nent of an event rule. The rule also consists of an
event type, and a mapping from event arguments
to nodes from the pattern graph, or to an event
type/node pair for nested event arguments. Af-
ter processing all training documents, we get on
the order of a few thousand rules; this can be de-
creased slightly by removing rules with subgraphs
that are isomorphic to those of other rules.
In principle, this set of rules could then be di-
rectly applied to the test documents, by searching
for any matching subgraphs. However, in practice
doing so leads to very low recall, since the pat-
terns are not general enough to get a broad range of
matches on new data. We can alleviate this by re-
laxing the strictness of the subgraph matching pro-
cess. Most basically, we relax node matching. In-
stead of requiring an exact match between both the
token and the part-of-speech of the nodes of the
sentence graph and those from the rule subgraph,
we also allow a match on the basis of the lemma
(according to BioLemmatizer (Liu et al, 2012)),
and a coarse-grained POS-tag (where there is only
one POS-tag for nouns, verbs and adjectives).
More importantly, we also relax the require-
ments on how closely the graphs must match, by
using ASM. ASM defines distances measures be-
tween subgraphs, based on structure, edge labels
and edge directions, and uses a set of specified
weights to combine them into an overall subgraph
distance. We have a pre-configured set of distance
thresholds for each event type, and for each sen-
tence/rule pairing, we extract events for any rules
with subgraphs under the given threshold.
The problem with this approximate matching is
that some rules now match too broadly, and pre-
cision is reduced. This is mitigated by adding
an iterative optimisation phase. In each iteration,
we run the event extraction using the current rule
set over some dataset ? usually the training set,
or a subset of it. We check the contribution of
each rule in terms of postulated events and actual
events which match the gold standard. If the ra-
tio of matched to postulated events is too low (for
the work reported here, the threshold is 0.25), the
rule is discarded. This process is repeated until no
more rules are discarded. This can take multiple
iterations since the rules are interdependent due to
the presence of nested event arguments.
The optimisation step is by far the most time-
consuming step of our process, especially for the
large rule sets produced in some configurations.
36
We were able to improve optimisation times some-
what by parallelising the event extraction, and
temporarily removing documents with long ex-
traction times from the optimisation process un-
til as late as possible, but it remained the primary
bottleneck in our experimentation.
2.1.2 Parsing Pipeline
In our parsing pipeline, we first split sentences
using the JULIE Sentence Boundary Detector, or
JSBD (Tomanek et al, 2007). We then parse
using a version of clearnlp1 (Choi and McCal-
lum, 2013), a successor to ClearParser (Choi and
Palmer, 2011), which was shown to have state-
of-the-art performance over the CRAFT corpus
of full-text biomedical articles (Verspoor et al,
2012). We use dependency and POS-tagging mod-
els trained on the CRAFT corpus (except where
noted); these pre-trained models are provided with
clearnlp. Our fork of clearnlp integrates to-
ken span marking into the parsing process, so the
dependency nodes can easily be matched to the
standoff annotations provided with the shared task
data. This pipeline is not dependent on any pre-
annotated data, so can thus be trivially applied to
extra data not provided as part of the shared task.
In addition the parsing is fast, requiring roughly 46
wall-clock seconds (processing serially) to parse
the 5059 sentences from the training and develop-
ment sets of the 2013 GE task ? an average of 9 ms
per sentence. The ability to apply the same pars-
ing configuration to new text was useful for adding
extra training data, as discussed in ?2.2.
The usage of clearnlp as the parser is the pri-
mary point of difference between our system and
that of Liu et al (2013b), who use the Charniak-
Johnson parser with the McClosky biomedical
model (CJM; McClosky and Charniak (2008)), al-
though there are other minor differences in tokeni-
sation and sentence splitting. We expected that the
higher accuracy of clearnlp over biomedical text
would translate into increased accuracy of event
detection in the shared task; we consider this ques-
tion in some detail below.
2.2 Adding Noisy Training Data
One of the limitations of the ASM approach is that
the high precision comes at the cost of lower re-
call. Our hypothesis is that adding extra training
instances, even if some are errors, will raise re-
call and improve overall performance. We utilised
1https://code.google.com/p/clearnlp/
two sources of automatically-annotated data: the
EVEX database, and running an automatic event
annotator over documents from PubMed Central
(PMC) and MEDLINE.
To test our hypothesis, we utilise one of the
best performing automatic event extractors in pre-
vious BioNLP tasks: TEES (Turku Event Extrac-
tion System)2 (Bjo?rne et al, 2011). We expand our
pool of training examples by adding the highest-
confidence events TEES identifies in unlabelled
text. We explored different approaches to ranking
events based on classifier confidence empirically.
TEES relies on multi-class SVMs both for trig-
ger and event classification, and produces confi-
dence scores for each prediction. We explored
ranking events according to: (i) score of the trig-
ger prediction, (ii) score of the event-type predic-
tion, and (iii) sum of trigger and event type predic-
tions. We also compared the performance when
selecting the top-k events overall, versus choos-
ing the top-k events for each event type. We also
tested adding as many instances per event-type as
there were in the manually-annotated dataset, with
different multiplying factors. Finally, we evalu-
ated the effect of using different splits of the data
for the evaluation and optimisation steps of ASM.
This is the full list of parameters that we tested
over held-out data:
? Original confidence scores: we ranked events
according to the three SVM scores mentioned
above: trigger prediction, event-type predic-
tion, and combined.
? Overall top-k: we selected the top 1,000,
5,000, 10,000, 20,000, 30,000, 40,000, and
50,000 for the different experimental runs.
? Top-k per type: for each event type, we se-
lected the top 400, 1,000, and 2,000.
? Training bias per type: we add as many in-
stances from EVEX per type as there are in
the manually annotated data. We experiment
with adding up to 6 times as many as in man-
ually annotated data.
? Training/optimisation split: we combine
manually and automatically annotated data
for training. For optimisation we tested
different options: manually annotated only,
manual + automatic, manual + top-100
events, and manual + top-1000 events.
2http://jbjorne.github.com/TEES/
37
We did not explore all these settings exhaus-
tively due to time constraints, and we report here
the most promising settings. It is worth mention-
ing that most of the configurations contributed to
improve the baseline performance. We only ob-
served drops when using automatically-annotated
data in the optimisation step.
2.2.1 Data from EVEX
Conveniently, the developers of TEES have re-
leased the output of their tool over the full 2009
collection of MEDLINE, consisting of abstracts of
biomedical articles, in a collection known as the
EVEX dataset. We used the full EVEX dataset as
provided by the University of Turku, and explored
different ways of ranking the full list of events as
described above.
2.2.2 Data from TEES
To augment the training data, we annotated two
data sets with TEES based on MEDLINE and
PubMed Central (PMC). The developers of TEES
released a trained model for the GE 2013 training
data that we utilised.
Due to the long pre-processing time of TEES,
which includes gene named entity recognition,
part-of-speech tagging and parsing, we used the
EVEX pre-processed MEDLINE, which required
some adaptation of the EVEX XML to the XML
format accepted by TEES. Once this adaptation
was finished, the files were processed by TEES.
Then, we have selected articles from PMC us-
ing a query containing specific MeSH headings
related to the GE task and limiting the result to
only the Open Access part of PMC. From the al-
most 600k articles from the PMC Open Access set,
we reduced the total number of articles to around
155k. The PMC query is the following:
(Genetic Phenomena[MH] OR Metabolic
Phenomena[MH] OR Cell Physiological
Phenomena[MH] OR Biochemical
Processes[MH]) AND open access[filter]
Furthermore, the articles were split into sections
and specific sections from the full text like Intro-
duction, Background and Methods were removed
to reduce the quantity of text to be annotated by
TEES. The PMC files produced by this filtering
were processed by TEES on the NICTA cluster.
2.3 Modification Detection
To evaluate the utility of ASM for a diverse range
of tasks, we also applied it to the task of detect-
ing modification (SPECULATION or NEGATION)
NEGATION cues
? Basic: not, no, never, nor, only, neither, fail, cease,
stop, terminate, end, lacking, missing, absent, absence,
failure, negative, unlikely, without, lack, unable
? Data-derived: any, prevention, prevent, disrupt, dis-
ruption
SPECULATION cues:
? Basic: analysis, whether, may, should, can, could, un-
certain, questionable, possible, likely, probable, prob-
ably, possibly, conceivable, conceivably, perhaps, ad-
dress, analyze, analyse, assess, ask, compare, consider,
enquire, evaluate, examine, experiment, explore, inves-
tigate, test, research, study, speculate
? Data-derived: measure, measurement, suggest, sug-
gestion, value, quantify, quantification, determine, de-
termination, detect, detection, calculate, calculation
Table 1: Modification cues
of events. In event detection, triggers are explic-
itly annotated, so the linguistic cue which indi-
cates that an event is occurring is easy to identify.
As described in Section 3.2, these triggers are im-
portant for learning event patterns.
The event extraction method is based on paths
between dependency graph nodes, so it is neces-
sary to have at least two relevant graph nodes be-
fore we can determine a path between them. For
learning modification rules, one graph node is the
trigger of the event which is subjec to modifica-
tion. However here we needed a method to deter-
mine another node in the sentence which provided
evidence that NEGATION or SPECULATION was
occurring, and could thus form an endpoint for a
semantically relevant graph pattern. To achieve
this, we specified a set cue lemmas for NEGATION
and SPECULATION. The basic set of cue lemmas
came from a variety of sources. Some were man-
ually specified and some were derived from previ-
ous work on modification detection (Cohen et al,
2011; MacKinlay et al, 2012). We manually ex-
panded this cue list to include obvious derivational
variants. This gave us a basic set of 34 SPECULA-
TION and 21 NEGATION cues.
We also used a data-driven strategy to find ad-
ditional lemmas indicative of modification. We
adapted the method of Rayson and Garside (2000)
which uses log-likelihood for finding words that
characterise differences between corpora. Here,
the ?corpora? are sentences attached to all events
in the training set, and sentences attached to events
which are subject to NEGATION or SPECULATION
(treated separately). We build a frequency distri-
bution over lemmas in each set of sentences, and
calculate the log-likelihood for all lemmas, us-
38
ing the observed frequency from the modification
events and the expected frequency over all events.
Sorting by decreasing log-likelihood, we get a
list of lemmas which are most strongly associated
with NEGATION or SPECULATION. We manually
examined the highest-ranked lemmas from these
two lists and noted lemmas which may occur,
according to human judgment, in phrases which
would denote the relevant modification type. We
found seven extra SPECULATION cues and three
extra NEGATION cues. Expanding with morpho-
logical variants as described above yielded 47
SPECULATION cues and 26 NEGATION cues to-
tal. These cues are shown, divided into basic and
data-derived, in Table 1.
For every node N with a lemma in the appro-
priate set of cue lemmas, we create a rule based
on the shortest path between the cue lemma node
N and the event trigger node. The trigger lem-
mas are replaced with generic lemmas which only
reflect the POS-tag of the trigger, to broaden the
range of possible matches. Each rule thus consists
of the POS-tag of an event trigger, and a subgraph
pattern including the abstracted event trigger node.
At modification detection time, the rules are ap-
plied in a similar way to the event rules. After
detecting events, we look for matches of each ex-
tracted event with every modification rule. A rule
R is considered to match if the event trigger node
POS tag matches the POS tag of the rule, and the
subgraph pattern of the rule matches the graph of
the sentence, including a node corresponding to
the event trigger node. If R is found to match
for a given event and sentence, any events which
have the trigger defined in the rule are marked as
SPECULATION or NEGATION as appropriate. As
in event extraction, we use ASM to allow a looser
match between graphs, but initial experimentation
showed that increasing the match thresholds be-
yond a relatively small distance was detrimental.
We have not yet added an optimisation phase for
modification, which might allow larger ASM dis-
tance threshold to have more benefit.
3 Results
We present our results over development data,
and the official test. We report the Approximate
Span/Approximate Recursive metric in all our ta-
bles, for easy comparison of scores. We describe
the data split used for development, explain our
event extraction results, and finally describe our
performance in modification detection.
3.1 Data division for development
In the data provided by the task organisers, the
split of data between training and development
sets, with 249 and 222 article sections respec-
tively, was fairly even. If we had used such a split,
we would have had an unfeasibly small amount
of data to train from during development, and
possible unexpected effects when we sharply in-
creased the amount of training data for running
over the held-out test set. We instead used our
own data set split during development, pooling
the provided training and development sets, and
randomly selecting six PMC articles (PMC IDs
2626671, 2674207, 3062687, 3148254, 3333881
and 3359311) for the development set, with the
remainder available for training. We respected ar-
ticle boundaries in the new split to avoid training
and testing on sentences taken from different sec-
tions of the same article. Results over the devel-
opment set reported in this section are over this
data split. We will refer to our training subset as
GE13tr, and to the testing subset as GE13dev.
For our runs over the official test of this chal-
lenge, we merged all the manually annotated data
from 2013 to be used as training. We also per-
formed some experiments with adding the exam-
ples from the 2011 GE task to our training data.
3.2 Event Extraction
For our first experiment, we evaluated the contri-
bution of the automatically annotated data over us-
ing GE13tr data only. We performed a set of ex-
periments to explore the parameters described in
Section 2.2 over two sources of extra examples:
EVEX and TEES.
Using EVEX data in training resulted in clear
improvements in performance when only manu-
ally annotated data was consulted for optimisa-
tion. The increase was mainly due to the better
recall, with small variations in precision over the
baseline for the majority of experiments. Our best
run over the GE13dev data followed this setting:
rank events according to trigger scores, include all
top-30000 events (without considering the types of
the events), and use only manually annotated data
for the optimisation step. Other settings also per-
formed well, as we will see below.
For TEES, we selected noisy examples from
MEDLINE and PMC to be used as additional
39
System Prec. Rec. F-sc.
GE13tr 60.40 27.02 37.34
+TEES 59.27 29.89 39.74
+TEES +EVEX (top5k) 46.93 30.78 37.18
+TEES +EVEX (top20k) 56.32 31.90 40.73
+TEES +EVEX (top30k) 55.34 32.48 40.93
+TEES +EVEX (pt1k) 58.54 30.96 40.50
+TEES +EVEX (trx4) 57.83 31.23 40.56
Table 2: Impact of adding extra training data to the
ASM method. top5k,20k,30k: using the top 5,000,
20,000, and 30,000 events. pt1k: using the top
1,000 events per event-type. trx4: following the
training bias of events, with a multiplying factor
of four. For TEES we always use the top 10,000
events. Evaluated over GE13dev.
training data. Initial results showed that when us-
ing only MEDLINE annotated data in the train-
ing step, the performance decreased compared to
not using any additional data. This might have
been due to differences between the EVEX pre-
processed data that we used and what TEES was
expecting, so the MEDLINE set was not consid-
ered for further experimentation. Using PMC ar-
ticles annotated with TEES in the training step se-
lected by the evidence score of TEES shows an in-
crease of recall while slightly decreasing the pre-
cision, which was expected. We selected the top
10000 events from the PMC set based on the evi-
dence score as additional training data.
Table 2 summarises the results of combin-
ing different settings of EVEX with TEES. We
achieve a considerable boost in recall, at the cost
of precision for most configurations. The only set-
ting where there is a slight drop in F-score is the
experiment with only 5000 events from EVEX; in
the remaining runs we are able to alleviate the drop
in precision, and improve the F-score. Consider-
ing the addition of top-events according to their
type, the increment in recall is slightly lower, but
these runs are able to reach similar F-score to the
best ones, using less training data. Results with
TEES might be slightly overoptimistic since the
PMC annotation is based on a TEES model trained
on the 2013 GE data and our configurations are
evaluated on a subset of this data.
For our next experiment, we tested the contribu-
tion of adding the dataset from the 2011 GE task
to the training dataset. We use this data both in
the training and optimisation steps. The results are
Train Prec. Rec. F-sc.
GE13tr 60.40 27.02 37.34
+GE11 53.41 32.62 40.50
Table 3: Adding GE11 data to the training and op-
timisation steps. Evaluated over GE13dev.
Parser Train Prec. Rec. F-sc.
clearnlp
GE13 60.40 27.02 37.34
+GE11 53.41 32.62 40.50
CJM
GE13 60.96 33.11 42.91
+GE11 64.11 38.93 48.44
Table 4: Performance depending on the applied
parsing pipeline (clearnlp for this work against
the CJM pipeline of Liu et al (2013b)) over
GE13dev. For each run, the available data was
used both in training and optimisation.
given in Table 3, where we can observe a boost in
recall at the cost of precision. Overall, the im-
proved F-score suggests that this dataset would
make a useful contribution to the system.
We also compared our system to that of Liu
et al (2013b), where the primary difference
(although not the only difference, as noted in
?2.1.2) is the use of clearnlp instead of the CJM
(Charniak-Johnson/McClosky) pipeline. It is thus
somewhat surprising to see in Table 4 that the
CJM pipeline outperforms our clearnlp pipeline
by 5.5?8% in F-score, depending on the train-
ing data. For the smaller GE13-only training set,
the gap is smaller, and the precision figures are
in fact comparable. However, the recall is uni-
formly lower, suggesting that the rules learned
from clearnlp parses are for some reason less gen-
erally applicable. Another interesting difference
is that our clearnlp pipeline gets a smaller benefit
from the addition of the GE11 training data. We
consider possible reasons for this in ?4.1.
Table 5 contains the evaluation of different ex-
periments on the official test data. We tested the
baseline system using the training and develop-
ment data from 2011 and 2013 GE tasks and the
addition of TEES and EVEX data. The additional
data improves the recall slightly compared to not
using it, while, as expected, it decreases the pre-
cision. Table 5 also shows the results for our of-
ficial submission (+TEES+EVEX sub), which due
to time constraints was a combination of the opti-
mised rules of different data splits and has a lower
40
Train Prec. Rec. F-sc.
GE11, GE13 65.71 32.57 43.55
+TEES+EVEX 63.67 33.50 43.91
+TEES+EVEX * 50.68 36.99 42.77
Table 5: Test set results, always optimised over
gold data only. * denotes the official submission.
performance compared to the other results.
3.3 Modification Detection
We show results for selected modification detec-
tion experiments in Table 6. In all cases we used
all of the available gold training data from the
GE11 and GE13 datasets. To assess the impact of
modification cues, we show results using the basic
set as well as with the addition of the data-derived
cues. It has often been noted (MacKinlay et al,
2012; Cohen et al, 2011) that modification detec-
tion accuracy is strongly dependent on the quality
of the upstream event annotation, so we provide an
oracle evaluation, using gold-standard event anno-
tations rather than automatic output.
The performance over the automatically-
annotated runs is respectable, given that the recall
is fundamentally limited by the recall of the input
event annotations, which is only around 30% for
the configurations shown. With the oracle event
annotations, the results improve substantially,
with considerable gains in precision, and recall
increasing by a factor of 4?6. This boost in recall
in particular is more than we would naively expect
from the roughly threefold increase in recall over
the events. It seems that many of the modification
rules we learned were even more effective over
events which our pipeline was unable to detect.
The modification rules were learned from oracle
event data, but this does not fully explain the
discrepancy. Regardless, our algorithm for mod-
ification detection showed excellent performance
over the oracle annotations. Over the 2009 version
of the BioNLP shared task data, MacKinlay et al
(2012) report F-scores of 54.6% for NEGATION
and 51.7% for SPECULATION. These are not
directly comparable with those in Table 6, but
running our newer algorithm over the same 2009
data gives F-scores of 84.2% for NEGATION and
69.1% for SPECULATION.
For the official run, which conflates event
extraction and modification detection accuracy,
our system was ranked third for NEGATION and
SPECULATION out of the three competing teams,
although the other teams had event extraction F-
scores of roughly 8% higher than our system. For
SPECULATION, our system had the highest preci-
sion of 34.15%, while the F-score of 20.22% was
close to the best result of 23.92%. Our NEGA-
TION detection was less competitive, with an F-
score of 20.94% ? roughly 6% lower than the other
teams. We cannot extrapolate directly from the or-
acle evaluation in Table 6, but it seems to indicate
that an increase in event extraction accuracy would
have flow-on benefits in modification detection.
4 Discussion
4.1 Detrimental Effects of Parser Choice
The biggest surprise here was that clearnlp, a
more accurate dependency parser for the biomed-
ical domain, as evaluated on the CRAFT tree-
bank, gave a substantially lower event extrac-
tion F-score than the CJM parser. To determine
whether preprocessing caused the differences, we
replaced the existing modules (sentence-splitting
from JSBD and tokenisation/POS-tagging from
clearnlp) with the BioC-derived versions from the
CJM pipeline, but this yielded only an insignifi-
cant decrease in accuracy.
Over the same training data, the optimised rules
from CJM have an average of 2.6 nodes per sub-
graph path, compared to 3.9 nodes per path using
clearnlp. A longer path is less likely to match
than a shorter path, so this may help to explain
the lower generalisability of the clearnlp-derived
rules. While it is possible for a longer subgraph
to match just as generally, if the test sentences
are parsed consistently, in general there are more
nodes and edges which can fail to match due to mi-
nor surface variations. One way to mitigate this is
to raise the ASM distance thresholds to compen-
sate for this; preliminary experiments suggest it
would provide a small (? 1%) boost in F-score but
this would not close the gap between the parsers.
Both parsers produce outputs with Stanford
Dependency labels (de Marneffe and Manning,
2008), so we might naively expect similar graph
topology and subgraph pattern lengths. However,
the CJM pipeline produces graphs in the ?CCpro-
cessed? SD format, which are simpler and denser.
If a node N has a link to a node O with a conjunc-
tion link to another node P (from e.g. and), an ex-
tra link with the same label is added directly from
N to P in the CCprocessed format. This means
41
NEGATION SPECULATION
Eval Events (F-sc) Cues P / R / F P / R / F
Dev
GE13+TEES+EVEX (40.93) Basic 32.69 / 13.71 / 19.32 37.04 / 14.49 / 20.83
GE13+TEES+EVEX (40.93) B + Data 32.69 / 12.88 / 18.48 39.71 / 17.20 / 24.00
Oracle (100.0) B + Data 82.48 / 71.07 / 76.35 78.79 / 67.71 / 72.83
Test
GE11+GE13 (43.55) B + Data 39.53 / 13.99 / 20.66 50.00 / 13.85 / 21.69
GE11+GE13+TEES+EVEX * (42.77) B + Data 32.76 / 15.38 / 20.94 34.15 / 14.36 / 20.22
Table 6: Results for SPECULATION and NEGATION using automatically-annotated events (showing the
F-score of the configuration), as well as using oracle event annotations from the gold standard, over our
development set and the official test set. Rules are learned from GE13+GE11 gold data (excluding any
test data). Cues for learning rules are either the basic manually-specified set (34 SPEC/21 NEG) or the
augmented set with data-driven additions (47 SPEC/26 NEG). * denotes the official submission.
there are more direct links in the graph, match-
ing the semantics more closely. The shortest path
fromN to P is now direct, instead of viaO, which
could enable the CJM pipeline to produce more
general rules.
To evaluate how much this detrimentally af-
fects the clearnlp pipeline, as a post hoc in-
vestigation, we implemented a conversion mod-
ule. Using Stanford Dependency parser code,
we replicated the CCprocessed conversion on the
clearnlp graphs, reducing the average subgraph
pattern length to 2.8, and slightly improving ac-
curacy. Over our development set, compared to
the results in Table 3 it gave a 0.7% absolute F-
score boost over using GE13 training-data only,
and 1.1% over using GE11 and GE13 training data
(in both cases improving recall). Over the test
set, the improvement was greater, with a P/R/F
of 35.66/64.99/46.05, a 2.5% increase in F-score
compared to the results in Table 5 and only 2.9%
less than the official Liu et al (2012) submission.
Clearly some of the inter-parser discrepancies
are due to surface features and post-processing,
and as noted above, we can also achieve small im-
provements by relaxing ASM thresholds, so some
problems may be caused by the default parameters
being suboptimal for the parser. However, the ac-
curacy is still lower where we would expect it to
be higher, and this remaining discrepancy is diffi-
cult to explain without performing a detailed error
analysis, which we leave for future work.
4.2 Effect of additional data
Our initial intuition that using additional noisy
training data during the training of the system
would improve the performance is supported by
the results in Table 2. Table 3 shows that us-
ing a larger set of manually annotated data based
on 2011 GE task data also improves performance.
However, these tables also indicate that adding
manually annotated data produces an increase in
performance comparable to adding the noisy data,
despite its smaller size, and when using this man-
ually annotated set together with the noisy data,
the improvement resulting from the noisy data is
smaller (Table 5). Noisy data was only used dur-
ing training, which limits its effectiveness?any
rule extracted from automatically acquired anno-
tations that are not seen during optimisation of the
rule set will have a lower weight. On the other
hand, we found that using noisy data for optimi-
sation seemed to decrease performance. Together,
these results suggest that studying strategies, pos-
sibly self-training, for selection of events from the
noisy data to be used during rule set optimisation
in the ASM method are warranted.
5 Conclusion
Using additional training data, whether manually
annotated or noisy, improves the performance of
our baseline event extraction system. The gains
that we achieved by adding training data, however,
were outweighed by a loss of performance due to
our parser substitution, with longer dependency
subgraphs limiting rule generalisability the most
likely explanation. Our experiments demonstrate
that while a given parser might be ?better? in one
evaluation context, that advantage may not trans-
late to improved performance in a downstream
task that depends strongly on the parser output.
We presented an extension of the subgraph match-
ing methodology to extract modification events
which, when based on a good core event extrac-
tion system, shows very promising results.
42
Acknowledgments
NICTA is funded by the Australian Government
as represented by the Department of Broadband,
Communications and the Digital Economy and
the Australian Research Council through the ICT
Centre of Excellence program. This research was
supported in part by the Intramural Research Pro-
gram of the NIH, NLM.
References
Ethem Alpaydin. 2004. Introduction to Machine
Learning. MIT Press.
Jari Bjo?rne and T. Salakoski. 2011. Generalizing
biomedical event extraction. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 183?
191.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2011. Ex-
tracting contextualized complex biological events
with rich graph-based features sets. Computational
Intelligence, 27(4):541?557.
Jari Bjorne, Filip Ginter, and Tapio Salakoski. 2012.
University of turku in the bionlp?11 shared task.
BMC Bioinformatics, 13(Suppl 11):S4.
Jinho D. Choi and Andrew McCallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics, Sofia, Bulgaria.
Jinho D. Choi and Martha Palmer. 2011. Getting the
most out of transition-based dependency parsing. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 687?692, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
K.B. Cohen, K. Verspoor, H.L. Johnson, C. Roeder,
P.V. Ogren, W.A. Baumgartner, E. White, H. Tip-
ney, and L. Hunter. 2011. High-precision biological
event extraction: Effects of system and data. Com-
putational Intelligence, 27(4):681701, November.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In CrossParser ?08: Coling 2008: Pro-
ceedings of the workshop on Cross-Framework and
Cross-Domain Parser Evaluation, pages 1?8, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
J.D. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsu-
jii. 2009. Overview of bionlp?09 shared task on
event extraction. Proceedings of Natural Language
Processing in Biomedicine (BioNLP) NAACL 2009
Workshop, pages 1?9.
Jin-Dong Kim, Ngan Nguyen, Yue Wang, Jun?ichi Tsu-
jii, Toshihisa Takagi, and Akinori Yonezawa. 2012.
The genia event and protein coreference tasks of
the bionlp shared task 2011. BMC Bioinformatics,
13(Suppl 11):S1.
H. Liu, R. Komandur, and K. Verspoor. 2011. From
graphs to events: A subgraph matching approach for
information eextraction from biomedical text. ACL
HLT 2011, page 164.
Haibin Liu, Tom Christiansen, William Baumgartner,
and Karin Verspoor. 2012. Biolemmatizer: a
lemmatization tool for morphological processing of
biomedical text. Journal of Biomedical Semantics,
3(1):3.
Haibin Liu, Lawrence Hunter, Vlado Keselj, and Karin
Verspoor. 2013a. Approximate subgraph matching-
based literature mining for biomedical events and re-
lations. PLoS ONE, 8(4):e60954, 04.
Haibin Liu, Karin Verspoor, Don Comeau, Andrew
MacKinlay, and W. John Wilbur. 2013b. General-
izing an approximate subgraph matching-based sys-
tem to extract events in molecular biology and can-
cer genetics. In Proceedings of the 2013 BioNLP
Workshop Companion Volume for the Shared Task.
Andrew MacKinlay, David Martinez, and Timo-
thy Baldwin. 2012. Detecting modification of
biomedical events using a deep parsing approach.
BMC Medical Informatics and Decision Making,
12(Suppl 1):S4.
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of
the Association for Computational Linguistics (ACL
2008, short papers).
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
conference of the North American chapter of the
ACL, pages 152?159.
Paul Rayson and Roger Garside. 2000. Comparing
corpora using frequency profiling. In The Workshop
on Comparing Corpora, pages 1?6, Hong Kong,
China, October. Association for Computational Lin-
guistics.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. Sentence and token splitting based on con-
ditional random fields. In Proceedings of the 10th
Conference of the Pacific Association for Compu-
tational Linguistics, pages 49?57, Melbourne, Aus-
tralia.
S. Van Landeghem, F. Ginter, Y. Van de Peer, and
T. Salakoski. 2011. EVEX: A pubmed-scale re-
source for homology-based generalization of text
mining predictions. In Proceedings of BioNLP 2011
Workshop, pages 28?37.
43
S. Van Landeghem, K. Hakala, S. Ro?nnqvist,
T. Salakoski, Y. Van de Peer, and F. Ginter. 2012.
Exploring biomolecular literature with EVEX: Con-
necting genes through events, homology and indirect
associations. Advances in Bioinformatics, Special
issue Literature-Mining Solutions for Life Science
Research:ID 582765.
Karin Verspoor, K. Bretonnel Cohen, Arrick Lan-
franchi, Colin Warner, Helen L. Johnson, Christophe
Roeder, Jinho D. Choi, Christopher Funk, Yuriy
Malenkiy, Miriam Eckert, Nianwen Xue, William
A. Baumgartner Jr., Michael Bada, Martha Palmer, ,
and Lawrence E. Hunter. 2012. A corpus of full-text
journal articles is a robust evaluation tool for reveal-
ing differences in performance of biomedical natural
language processing tools. BMC Bioinformatics.
44
