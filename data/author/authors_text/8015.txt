Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 520?527,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Bilingual-LSA Based LM Adaptation for Spoken Language Translation
Yik-Cheung Tam and Ian Lane and Tanja Schultz
InterACT, Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
{yct,ian.lane,tanja}@cs.cmu.edu
Abstract
We propose a novel approach to crosslingual
language model (LM) adaptation based on
bilingual Latent Semantic Analysis (bLSA).
A bLSA model is introduced which enables
latent topic distributions to be efficiently
transferred across languages by enforcing
a one-to-one topic correspondence during
training. Using the proposed bLSA frame-
work crosslingual LM adaptation can be per-
formed by, first, inferring the topic poste-
rior distribution of the source text and then
applying the inferred distribution to the tar-
get language N-gram LM via marginal adap-
tation. The proposed framework also en-
ables rapid bootstrapping of LSA models
for new languages based on a source LSA
model from another language. On Chinese
to English speech and text translation the
proposed bLSA framework successfully re-
duced word perplexity of the English LM by
over 27% for a unigram LM and up to 13.6%
for a 4-gram LM. Furthermore, the pro-
posed approach consistently improved ma-
chine translation quality on both speech and
text based adaptation.
1 Introduction
Language model adaptation is crucial to numerous
speech and translation tasks as it enables higher-
level contextual information to be effectively incor-
porated into a background LM improving recogni-
tion or translation performance. One approach is
to employ Latent Semantic Analysis (LSA) to cap-
ture in-domain word unigram distributions which
are then integrated into the background N-gram
LM. This approach has been successfully applied
in automatic speech recognition (ASR) (Tam and
Schultz, 2006) using the Latent Dirichlet Alloca-
tion (LDA) (Blei et al, 2003). The LDA model can
be viewed as a Bayesian topic mixture model with
the topic mixture weights drawn from a Dirichlet
distribution. For LM adaptation, the topic mixture
weights are estimated based on in-domain adapta-
tion text (e.g. ASR hypotheses). The adapted mix-
ture weights are then used to interpolate a topic-
dependent unigram LM, which is finally integrated
into the background N-gram LM using marginal
adaptation (Kneser et al, 1997)
In this paper, we propose a framework to per-
form LM adaptation across languages, enabling the
adaptation of a LM from one language based on the
adaptation text of another language. In statistical
machine translation (SMT), one approach is to ap-
ply LM adaptation on the target language based on
an initial translation of input references (Kim and
Khudanpur, 2003; Paulik et al, 2005). This scheme
is limited by the coverage of the translation model,
and overall by the quality of translation. Since this
approach only allows to apply LM adaptation af-
ter translation, available knowledge cannot be ap-
plied to extend the coverage. We propose a bilingual
LSA model (bLSA) for crosslingual LM adaptation
that can be applied before translation. The bLSA
model consists of two LSA models: one for each
side of the language trained on parallel document
corpora. The key property of the bLSA model is that
520
the latent topic of the source and target LSA mod-
els can be assumed to be a one-to-one correspon-
dence and thus share a common latent topic space
since the training corpora consist of bilingual paral-
lel data. For instance, say topic 10 of the Chinese
LSA model is about politics. Then topic 10 of the
English LSA model is set to also correspond to pol-
itics and so forth. During LM adaptation, we first
infer the topic mixture weights from the source text
using the source LSA model. Then we transfer the
inferred mixture weights to the target LSA model
and thus obtain the target LSA marginals. The chal-
lenge is to enforce the one-to-one topic correspon-
dence. Our proposal is to share common variational
Dirichlet posteriors over the topic mixture weights
of a document pair in the LDA-style model. The
beauty of the bLSA framework is that the model
searches for a common latent topic space in an un-
supervised fashion, rather than to require manual in-
teraction. Since the topic space is language indepen-
dent, our approach supports topic transfer in multi-
ple language pairs in O(N) where N is the number of
languages.
Related work includes the Bilingual Topic Ad-
mixture Model (BiTAM) for word alignment pro-
posed by (Zhao and Xing, 2006). Basically, the
BiTAM model consists of topic-dependent transla-
tion lexicons modeling Pr(c|e, k) where c, e and
k denotes the source Chinese word, target English
word and the topic index respectively. On the
other hand, the bLSA framework models Pr(c|k)
and Pr(e|k) which is different from the BiTAM
model. By their different modeling nature, the bLSA
model usually supports more topics than the BiTAM
model. Another work by (Kim and Khudanpur,
2004) employed crosslingual LSA using singular
value decomposition which concatenates bilingual
documents into a single input supervector before
projection.
We organize the paper as follows: In Section 2,
we introduce the bLSA framework including La-
tent Dirichlet-Tree Allocation (LDTA) (Tam and
Schultz, 2007) as a correlated LSA model, bLSA
training and crosslingual LM adaptation. In Sec-
tion 3, we present the effect of LM adaptation on
word perplexity, followed by SMT experiments re-
ported in BLEU on both speech and text input in
Section 3.3. Section 4 describes conclusions and fu-
ASR hypo
Chinese LSA English LSA
Chinese N?gram LM English N?gram LM
Chinese ASR Chinese?>English SMT
Chinese?English
Adapt Adapt
MT hypoTopic distribution
Parallel document corpus
Chinese text English text
Figure 1: Topic transfer in bilingual LSA model.
ture works.
2 Bilingual Latent Semantic Analysis
The goal of a bLSA model is to enforce a one-
to-one topic correspondence between monolingual
LSA models, each of which can be modeled using
an LDA-style model. The role of the bLSA model
is to transfer the inferred latent topic distribution
from the source language to the target language as-
suming that the topic distributions on both sides are
identical. The assumption is reasonable for parallel
document pairs which are faithful translations. Fig-
ure 1 illustrates the idea of topic transfer between
monolingual LSA models followed by LM adapta-
tion. One observation is that the topic transfer can be
bi-directional meaning that the ?flow? of topic can
be from ASR to SMT or vice versa. In this paper,
we only focus on ASR-to-SMT direction. Our tar-
get is to minimize the word perplexity on the target
language through LM adaptation. Before we intro-
duce the heuristic of enforcing a one-to-one topic
correspondence, we describe the Latent Dirichlet-
Tree Allocation (LDTA) for LSA.
2.1 Latent Dirichlet-Tree Allocation
The LDTA model extends the LDA model in which
correlation among latent topics are captured using a
Dirichlet-Tree prior. Figure 2 illustrates a depth-two
Dirichlet-Tree. A tree of depth one simply falls back
to the LDA model. The LDTA model is a generative
model with the following generative process:
1. Sample a vector of branch probabilities bj ?
521
Dir(.)Dir(.) Dir(.)
Dir(.)
topic 1 topic 4Latent topics topic K
j=1
j=2 j=3
Figure 2: Dirichlet-Tree prior of depth two.
Dir(?j) for each node j = 1...J where ?j de-
notes the parameter (aka the pseudo-counts of
its outgoing branches) of the Dirichlet distribu-
tion at node j.
2. Compute the topic proportions as:
?k =
?
jc
b?jc(k)jc (1)
where ?jc(k) is an indicator function which sets
to unity when the c-th branch of the j-th node
leads to the leaf node of topic k and zero other-
wise. The k-th topic proportion ?k is computed
as the product of branch probabilities from the
root node to the leaf node of topic k.
3. Generate a document using the topic multino-
mial for each word wi:
zi ? Mult(?)
wi ? Mult(?.zi)
where ?.zi denotes the topic-dependent uni-
gram LM indexed by zi.
The joint distribution of the latent variables (topic
sequence zn1 and the Dirichlet nodes over child
branches bj) and an observed document wn1 can be
written as follows:
p(wn1 , zn1 , bJ1 ) = p(bJ1 |{?j})
n
?
i
?wizi ? ?zi
where p(bJ1 |{?j}) =
J
?
j
Dir(bj;?j)
?
?
jc
b?jc?1jc
Similar to LDA training, we apply the variational
Bayes approach by optimizing the lower bound of
the marginalized document likelihood:
L(wn1 ; ?,?)=Eq[log
p(wn1 , zn1 , bJ1 ; ?)
q(zn1 , bJ1 ; ?)
]
=Eq[log p(wn1 |zn1 )] + Eq[log
p(zn1 |bJ1 )
q(zn1 )
]
+Eq[log
p(bJ1 ; {?j})
q(bJ1 ; {?j})
]
where q(zn1 , bJ1 ; ?) =
?n
i q(zi) ?
?J
j q(bj) is a fac-
torizable variational posterior distribution over the
latent variables parameterized by ? which are deter-
mined in the E-step. ? is the model parameters for
a Dirichlet-Tree {?j} and the topic-dependent uni-
gram LM {?wk}. The LDTA model has an E-step
similar to the LDA model:
E-Step:
?jc = ?jc +
n
?
i
K
?
k
qik ? ?jc(k) (2)
qik ? ?wik ? eEq[log ?k] (3)
where
Eq[log ?k] =
?
jc
?jc(k)Eq[log bjc]
=
?
jc
?jc(k)
(
?(?jc)??(
?
c
?jc)
)
where qik denotes q(zi = k) meaning the variational
topic posterior of word wi. Eqn 2 and Eqn 3 are
executed iteratively until convergence is reached.
M-Step:
?wk ?
n
?
i
qik ? ?(wi, w) (4)
where ?(wi, w) is a Kronecker Delta function. The
alpha parameters can be estimated with iterative
methods such as Newton-Raphson or simple gradi-
ent ascent procedure.
2.2 Bilingual LSA training
For the following explanations, we assume that our
source and target languages are Chinese and En-
glish respectively. The bLSA model training is a
522
two-stage procedure. At the first stage, we train
a Chinese LSA model using the Chinese docu-
ments in parallel corpora. We applied the varia-
tional EM algorithm (Eqn 2?4) to train a Chinese
LSA model. Then we used the model to compute
the term eEq[log ?k] needed in Eqn 3 for each Chinese
document in parallel corpora. At the second stage,
we apply the same eEq [log ?k] to bootstrap an English
LSA model, which is the key to enforce a one-to-one
topic correspondence. Now the hyper-parameters of
the variational Dirichlet posteriors of each node in
the Dirichlet-Tree are shared among the Chinese and
English model. Precisely, we apply only Eqn 3 with
fixed eEq [log ?k] in the E-step and Eqn 4 in the M-step
on {?wk} to bootstrap an English LSA model. No-
tice that the E-step is non-iterative resulting in rapid
LSA training. In short, given a monolingual LSA
model, we can rapidly bootstrap LSA models of new
languages using parallel document corpora. Notice
that the English and Chinese vocabulary sizes do not
need to be similar. In our setup, the Chinese vo-
cabulary comes from the ASR system while the En-
glish vocabulary comes from the English part of the
parallel corpora. Since the topic transfer can be bi-
directional, we can perform the bLSA training in a
reverse manner, i.e. training an English LSA model
followed by bootstrapping a Chinese LSA model.
2.3 Crosslingual LM adaptation
Given a source text, we apply the E-step to estimate
variational Dirichlet posterior of each node in the
Dirichlet-Tree. We estimate the topic weights on the
source language using the following equation:
??(CH)k ?
?
jc
(
?jc
?
c? ?jc?
)?jc(k)
(5)
Then we apply the topic weights into the target LSA
model to obtain an in-domain LSA marginals:
PrEN (w) =
K
?
k=1
?(EN)wk ? ??
(CH)
k (6)
We integrate the LSA marginal into the target back-
ground LM using marginal adaptation (Kneser et al,
1997) which minimizes the Kullback-Leibler diver-
gence between the adapted LM and the background
LM:
Pra(w|h) ?
(
Prldta(w)
Prbg(w)
)?
? Prbg(w|h) (7)
Likewise, LM adaptation can take place on the
source language as well due to the bi-directional na-
ture of the bLSA framework when target-side adap-
tation text is available. In this paper, we focus on
LM adaptation on the target language for SMT.
3 Experimental Setup
We evaluated our bLSA model using the Chinese?
English parallel document corpora consisting of the
Xinhua news, Hong Kong news and Sina news. The
combined corpora contains 67k parallel documents
with 35M Chinese (CH) words and 43M English
(EN) words. Our spoken language translation sys-
tem translates from Chinese to English. The Chinese
vocabulary comes from the ASR decoder while the
English vocabulary is derived from the English por-
tion of the parallel training corpora. The vocabulary
sizes for Chinese and English are 108k and 69k re-
spectively. Our background English LM is a 4-gram
LM trained with the modified Kneser-Ney smooth-
ing scheme using the SRILM toolkit on the same
training text. We explore the bLSA training in both
directions: EN?CH and CH?EN meaning that an
English LSA model is trained first and a Chinese
LSA model is bootstrapped or vice versa. Exper-
iments explore which bootstrapping direction yield
best results measured in terms of English word per-
plexity. The number of latent topics is set to 200 and
a balanced binary Dirichlet-Tree prior is used.
With an increasing interest in the ASR-SMT cou-
pling for spoken language translation, we also eval-
uated our approach with Chinese ASR hypotheses
and compared with Chinese manual transcriptions.
We are interested to see the impact due to recog-
nition errors on the ASR hypotheses compared to
the manual transcriptions. We employed the CMU-
InterACT ASR system developed for the GALE
2006 evaluation. We trained acoustic models with
over 500 hours of quickly transcribed speech data re-
leased by the GALE program and the LM with over
800M-word Chinese corpora. The character error
rates on the CCTV, RFA and NTDTV shows in the
RT04 test set are 7.4%, 25.5% and 13.1% respec-
tively.
523
Topic index Top words
?CH-40? flying, submarine, aircraft, air, pilot, land, mission, brand-new
?EN-40? air, sea, submarine, aircraft, flight, flying, ship, test
?CH-41? satellite, han-tian, launch, space, china, technology, astronomy
?EN-41? space, satellite, china, technology, satellites, science
?CH-42? fire, airport, services, marine, accident, air
?EN-42? fire, airport, services, department, marine, air, service
Table 1: Parallel topics extracted by the bLSA
model. Top words on the Chinese side are translated
into English for illustration purpose.
-3.05e+08
-3e+08
-2.95e+08
-2.9e+08
-2.85e+08
-2.8e+08
-2.75e+08
-2.7e+08
 2  4  6  8  10  12  14  16  18  20
Tr
ain
ing
 lo
g 
lik
eli
ho
od
# of training iterations
bootstrapped EN LSA
monolingual EN LSA
Figure 3: Comparison of training log likelihood of
English LSA models bootstrapped from a Chinese
LSA and from a flat monolingual English LSA.
3.1 Analysis of the bLSA model
By examining the top-words of the extracted paral-
lel topics, we verify the validity of the heuristic de-
scribed in Section 2.2 which enforces a one-to-one
topic correspondence in the bLSA model. Table 1
shows the latent topics extracted by the CH?EN
bLSA model. We can see that the Chinese-English
topic words have strong correlations. Many of them
are actually translation pairs with similar word rank-
ings. From this viewpoint, we can interpret bLSA as
a crosslingual word trigger model. The result indi-
cates that our heuristic is effective to extract parallel
latent topics. As a sanity check, we also examine the
likelihood of the training data when an English LSA
model is bootstrapped. We can see from Figure 3
that the likelihood increases monotonically with the
number of training iterations. The figure also shows
that by sharing the variational Dirichlet posteriors
from the Chinese LSA model, we can bootstrap an
English LSA model rapidly compared to monolin-
gual English LSA training with both training proce-
dures started from the same flat model.
LM (43M) CCTV RFA NTDTV
BG EN unigram 1065 1220 1549
+CH?EN (CH ref) 755 880 1113
+EN?CH (CH ref) 762 896 1111
+CH?EN (CH hypo) 757 885 1126
+EN?CH (CH hypo) 766 896 1129
+CH?EN (EN ref) 731 838 1075
+EN?CH (EN ref) 747 848 1087
Table 2: English word perplexity (PPL) on the RT04
test set using a unigram LM.
3.2 LM adaptation results
We trained the bLSA models on both CH?EN and
EN?CH directions and compared their LM adapta-
tion performance using the Chinese ASR hypothe-
ses (hypo) and the manual transcriptions (ref) as in-
put. We adapted the English background LM using
the LSA marginals described in Section 2.3 for each
show on the test set.
We first evaluated the English word perplexity us-
ing the EN unigram LM generated by the bLSA
model. Table 2 shows that the bLSA-based LM
adaptation reduces the word perplexity by over 27%
relative compared to an unadapted EN unigram LM.
The results indicate that the bLSA model success-
fully leverages the text from the source language and
improves the word perplexity on the target language.
We observe that there is almost no performance dif-
ference when either the ASR hypotheses or the man-
ual transcriptions are used for adaptation. The result
is encouraging since the bLSA model may be in-
sensitive to moderate recognition errors through the
projection of the input adaptation text into the latent
topic space. We also apply an English translation
reference for adaptation to show an oracle perfor-
mance. The results using the Chinese hypotheses are
not too far off from the oracle performance. Another
observation is that the CH?EN bLSA model seems
to give better performance than the EN?CH bLSA
model. However, their differences are not signifi-
cant. The result may imply that the direction of the
bLSA training is not important since the latent topic
space captured by either language is similar when
parallel training corpora are used. Table 3 shows the
word perplexity when the background 4-gram En-
glish LM is adapted with the tuning parameter ? set
524
LM (43M, ? = 0.7) CCTV RFA NTDTV
BG EN 4-gram 118 212 203
+CH?EN (CH ref) 102 191 179
+EN?CH (CH ref) 102 198 179
+CH?EN (CH hypo) 102 193 180
+EN?CH (CH hypo) 103 198 180
+CH?EN (EN ref) 100 186 176
+EN?CH (EN ref) 101 190 176
Table 3: English word perplexity (PPL) on the RT04
test set using a 4-gram LM.
 100
 105
 110
 115
 120
 125
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
En
gli
sh
 W
or
d 
Pe
rp
lex
ity
Beta
CCTV (CER=7.4%)
BG 4-gram
+bLSA (CH reference)
+bLSA (CH ASR hypo)
+bLSA (EN reference)
Figure 4: Word perplexity with different ? using
manual reference or ASR hypotheses on CCTV.
to 0.7. Figure 4 shows the change of perplexity with
different ?. We see that the adaptation performance
using the ASR hypotheses or the manual transcrip-
tions are almost identical on different ? with an op-
timal value at around 0.7. The results show that the
proposed approach successfully reduces the perplex-
ity in the range of 9?13.6% relative compared to an
unadapted baseline on different shows when ASR
hypotheses are used. Moreover, we observe simi-
lar performance using ASR hypotheses or manual
Chinese transcriptions which is consistent with the
results on Table 2. On the other hand, it is interest-
ing to see that the performance gap from the oracle
adaptation is somewhat related to the degree of mis-
match between the test show and the training condi-
tion. The gap looks wider on the RFA and NTDTV
shows compared to the CCTV show.
3.3 Incorporating bLSA into Spoken Language
Translation
To investigate the effectiveness of bLSA LM adap-
tation for spoken language translation, we incorpo-
rated the proposed approach into our state-of-the-art
phrase-based SMT system. Translation performance
was evaluated on the RT04 broadcast news evalua-
tion set when applied to both the manual transcrip-
tions and 1-best ASR hypotheses. During evalua-
tion two performance metrics, BLEU (Papineni et
al., 2002) and NIST, were computed. In both cases, a
single English reference was used during scoring. In
the transcription case the original English references
were used. For the ASR case, as utterance segmen-
tation was performed automatically, the number of
sentences generated by ASR and SMT differed from
the number of English references. In this case, Lev-
enshtein alignment was used to align the translation
output to the English references before scoring.
3.4 Baseline SMT Setup
The baseline SMT system consisted of a non adap-
tive system trained using the same Chinese-English
parallel document corpora used in the previous ex-
periments (Sections 3.1 and 3.2). For phrase extrac-
tion a cleaned subset of these corpora, consisting of
1M Chinese-English sentence pairs, was used. SMT
decoding parameters were optimized using man-
ual transcriptions and translations of 272 utterances
from the RT04 development set (LDC2006E10).
SMT translation was performed in two stages us-
ing an approach similar to that in (Vogel, 2003).
First, a translation lattice was constructed by match-
ing all possible bilingual phrase-pairs, extracted
from the training corpora, to the input sentence.
Phrase extraction was performed using the ?PESA?
(Phrase Pair Extraction as Sentence Splitting) ap-
proach described in (Vogel, 2005). Next, a search
was performed to find the best path through the lat-
tice, i.e. that with maximum translation-score. Dur-
ing search reordering was allowed on the target lan-
guage side. The final translation result was that
hypothesis with maximum translation-score, which
is a log-linear combination of 10 scores consist-
ing of Target LM probability, Distortion Penalty,
Word-Count Penalty, Phrase-Count and six Phrase-
Alignment scores. Weights for each component
score were optimized to maximize BLEU-score on
the development set using MER optimization as de-
scribed in (Venugopal et al, 2005).
525
Translation Quality - BLEU (NIST)
SMT Target LM CCTV RFA NTDTV ALL
Manual Transcription
Baseline LM: 0.162 (5.212) 0.087 (3.854) 0.140 (4.859) 0.132 (5.146)
bLSA (bLSA-Adapted LM): 0.164 (5.212) 0.087 (3.897) 0.143 (4.864) 0.134 (5.162)
1-best ASR Output
CER (%) 7.4 25.5 13.1 14.9
Baseline LM: 0.129 (4.15) 0.051 (2.77) 0.086 (3.50) 0.095 (3.90)
bLSA (bLSA-Adapted LM): 0.132 (4.16) 0.050 (2.79) 0.089 (3.53) 0.096 (3.91)
Table 4: Translation performance of baseline and bLSA-Adapted Chinese-English SMT systems on manual
transcriptions and 1-best ASR hypotheses
3.5 Performance of Baseline SMT System
First, the baseline system performance was evalu-
ated by applying the system described above to the
reference transcriptions and 1-best ASR hypotheses
generated by our Mandarin speech recognition sys-
tem. The translation accuracy in terms of BLEU and
NIST for each individual show (?CCTV?, ?RFA?,
and ?NTDTV?), and for the complete test-set, are
shown in Table 4 (Baseline LM). When applied to
the reference transcriptions an overall BLEU score
of 0.132 was obtained. BLEU-scores ranged be-
tween 0.087 and 0.162 for the ?RFA?, ?NTDTV? and
?CCTV? shows, respectively. As the ?RFA? show
contained a large segment of conversational speech,
translation quality was considerably lower for this
show due to genre mismatch with the training cor-
pora of newspaper text.
For the 1-best ASR hypotheses, an overall BLEU
score of 0.095 was achieved. For the ASR case,
the relative reduction in BLEU scores for the RFA
and NTDTV shows is large, due to the significantly
lower recognition accuracies for these shows. BLEU
score is also degraded due to poor alignment of ref-
erences during scoring.
3.6 Incorporation of bLSA Adaptation
Next, the effectiveness of bLSA based LM adapta-
tion was evaluated. For each show the target En-
glish LM was adapted using bLSA-adaptation, as
described in Section 2.3. SMT was then applied us-
ing an identical setup to that used in the baseline ex-
periments.
The translation accuracy when bLSA adaptation
was incorporated is shown in Table 4. When ap-
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
CCTV RFA NTDTV All shows
BLE
U
Baseline-LM bLSA Adapted LM
Figure 5: BLEU score for those 25% utterances
which resulted in different translations after bLSA
adaptation (manual transcriptions)
plied to the manual transcriptions, bLSA adaptation
improved the overall BLEU-score by 1.7% relative
(from 0.132 to 0.134). For all three shows bLSA
adaptation gained higher BLEU and NIST metrics.
A similar trend was also observed when the pro-
posed approach was applied to the 1-best ASR out-
put. On the evaluation set a relative improvement in
BLEU score of 1.0% was gained.
The semantic interpretation of the majority of ut-
terances in broadcast news are not affected by topic
context. In the experimental evaluation it was ob-
served that only 25% of utterances produced differ-
ent translation output when bLSA adaptation was
performed compared to the topic-independent base-
line. Although the improvement in translation qual-
ity (BLEU) was small when evaluated over the en-
tire test set, the improvement in BLEU score for
526
these 25% utterances was significant. The trans-
lation quality for the baseline and bLSA-adaptive
system when evaluated only on these utterances is
shown in Figure 5 for the manual transcription case.
On this subset of utterances an overall improvement
in BLEU of 0.007 (5.7% relative) was gained, with
a gain of 0.012 (10.6% relative) points for the ?NT-
DTV? show. A similar trend was observed when ap-
plied to the 1-best ASR output. In this case a rel-
ative improvement in BLEU of 12.6% was gained
for ?NTDTV?, and for ?All shows? 0.007 (3.7%)
was gained. Current evaluation metrics for trans-
lation, such as ?BLEU?, do not consider the rela-
tive importance of specific words or phrases during
translation and thus are unable to highlight the true
effectiveness of the proposed approach. In future
work, we intend to investigate other evaluation met-
rics which consider the relative informational con-
tent of words.
4 Conclusions
We proposed a bilingual latent semantic model
for crosslingual LM adaptation in spoken language
translation. The bLSA model consists of a set of
monolingual LSA models in which a one-to-one
topic correspondence is enforced between the LSA
models through the sharing of variational Dirich-
let posteriors. Bootstrapping a LSA model for a
new language can be performed rapidly with topic
transfer from a well-trained LSA model of another
language. We transfer the inferred topic distribu-
tion from the input source text to the target lan-
guage effectively to obtain an in-domain target LSA
marginals for LM adaptation. Results showed that
our approach significantly reduces the word per-
plexity on the target language in both cases using
ASR hypotheses and manual transcripts. Interest-
ingly, the adaptation performance is not much af-
fected when ASR hypotheses were used. We eval-
uated the adapted LM on SMT and found that the
evaluation metrics are crucial to reflect the actual
improvement in performance. Future directions in-
clude the exploration of story-dependent LM adap-
tation with automatic story segmentation instead of
show-dependent adaptation due to the possibility of
multiple stories within a show. We will investigate
the incorporation of monolingual documents for po-
tentially better bilingual LSA modeling.
Acknowledgment
This work is partly supported by the Defense Ad-
vanced Research Projects Agency (DARPA) under
Contract No. HR0011-06-2-0001. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of DARPA.
References
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet
Allocation. In Journal of Machine Learning Research,
pages 1107?1135.
W. Kim and S. Khudanpur. 2003. LM adaptation using
cross-lingual information. In Proc. of Eurospeech.
W. Kim and S. Khudanpur. 2004. Cross-lingual latent
semantic analysis for LM. In Proc. of ICASSP.
R. Kneser, J. Peters, and D. Klakow. 1997. Language
model adaptation using dynamic marginals. In Proc.
of Eurospeech, pages 1971?1974.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: A method for automatic evaluation of machine
translation. In Proc. of ACL.
M. Paulik, C. Fu?gen, T. Schaaf, T. Schultz, S. Stu?ker, and
A. Waibel. 2005. Document driven machine transla-
tion enhanced automatic speech recognition. In Proc.
of Interspeech.
Y. C. Tam and T. Schultz. 2006. Unsupervised language
model adaptation using latent semantic marginals. In
Proc. of Interspeech.
Y. C. Tam and T. Schultz. 2007. Correlated latent seman-
tic model for unsupervised language model adaptation.
In Proc. of ICASSP.
A. Venugopal, A. Zollmann, and A. Waibel. 2005. Train-
ing and evaluation error minimization rules for statis-
tical machine translation. In Proc. of ACL.
S. Vogel. 2003. SMT decoder dissected: Word reorder-
ing. In Proc. of ICNLPKE.
S. Vogel. 2005. PESA: Phrase pair extraction as sentence
splitting. In Proc. of the Machine Translation Summit.
B. Zhao and E. P. Xing. 2006. BiTAM: Bilingual topic
admixture models for word alignment. In Proc. of
ACL.
527
PLASER: Pronunciation Learning via Automatic Speech Recognition
Brian Mak, Manhung Siu, Mimi Ng, Yik-Cheung Tam?, Yu-Chung Chan?
Kin-Wah Chan, Ka-Yee Leung, Simon Ho, Fong-Ho Chong, Jimmy Wong, and Jacqueline Lo
Hong Kong University of Science and Technology
Department of Computer Science, and
Department of Electronic and Electrical Engineering
{bmak, eemsiu, ssmimi}@ust.hk
Abstract
PLASER is a multimedia tool with instant
feedback designed to teach English pronunci-
ation for high-school students of Hong Kong
whose mother tongue is Cantonese Chinese.
The objective is to teach correct pronunciation
and not to assess a student?s overall pronuncia-
tion quality. Major challenges related to speech
recognition technology include: allowance for
non-native accent, reliable and corrective feed-
backs, and visualization of errors.
PLASER employs hidden Markov models
to represent position-dependent English
phonemes. They are discriminatively trained
using the standard American English TIMIT
corpus together with a set of TIMIT utterances
collected from ?good? local English speakers.
There are two kinds of speaking exercises:
minimal-pair exercises and word exercises.
In the word exercises, PLASER computes
a confidence-based score for each phoneme
of the given word, and paints each vowel or
consonant segment in the word using a novel
3-color scheme to indicate their pronunciation
accuracy. PLASER was used by 900 students
of grade 7 and 8 over a period of 2?3 months.
About 80% of the students said that they pre-
ferred using PLASER over traditional English
classes to learn pronunciation. A pronunciation
test was also conducted before and after they
used PLASER. The result from 210 students
shows that the students? pronunciation skill
was improved. (The statistics is significant at
the 99% confidence level.)
?Mr. Tam is now a graduate student at the Department of
Computer Science at Carnegie Mellon University.
?Mr. Chan is now working at SpeechWorks Inc.
1 Introduction
The phenomenal advances in automatic speech recogni-
tion (ASR) technologies in the last decade led to the re-
cent employment of the technologies in computer-aided
language learning (CALL) 1. One example is the LIS-
TEN project (Mostow et al, 1994). However, one has
to bear in mind that the goal of ASR in most other com-
mon classification applications (such as automated call
centers, dictation, etc.) is orthogonal to that in CALL:
while the former requires ASR in general to be forgiv-
ing to allophonic variations due to speaker idiosyncrasies
or accent, pronunciation learning demands strict distinc-
tion among different sounds though the extent of strict-
ness could be very subjective with a human teacher. As
a result, technologies developed for mainstream ASR ap-
plications may not work satisfactorily for pronunciation
learning.
In the area of pronunciation learning, ASR has been
used in CALL for two different purposes: teaching
correct pronunciation of a foreign language to stu-
dents (Kawai and Hirose, 2000), and assessing the pro-
nunciation quality of a speaker speaking a foreign lan-
guage (Witt and Young, 2000; Neumeyer et al, 2000;
Franco et al, 2000). The former asks for accurate and
precise phoneme recognition while the latter may toler-
ate more recognition noises. The judgment for the for-
mer task is comparatively more objective than that for
the latter which, on the other hand, is usually required to
correlate well with human judges. In this paper, we de-
scribe a multimedia tool we built for high-school students
in Hong Kong to self-learn American English pronuncia-
tion. Their mother tongue is Cantonese Chinese. The ob-
jective is to teach correct pronunciation of basic English
phonemes (possibly with local accent), and not to assess
a student?s overall pronunciation quality. Although there
1CALL applies many different technologies to help lan-
guage learning, but this paper concerns only the one area of
pronunciation learning in CALL.
exist commercial products for the purpose, they have two
major problems: First, they are not built for Cantonese-
speaking Chinese; and, second, the feedback from these
products does not pinpoint precisely which phonemes are
poorly pronounced and which phonemes are well pro-
nounced. As a matter of fact, most of these systems only
provide an overall score for a word or utterance. As the
feedback is not indicative, students would not know how
to improve or correct their mistakes. One reason is the
relatively poor performance of phoneme recognition ?
the best phoneme recognition accuracy is about 75% for
the TIMIT corpus.
We took a pragmatic view and designed a multimedia
learning tool called PLASER ? Pronunciation Learning
via Automatic SpEech Recognition ? according to our
following beliefs and guidelines:
1. It is an illusive goal for average students to learn
to speak a second language without local accent.
Therefore, PLASER should be tolerant to mi-
nor Cantonese accents, lest the students become
too frustrated from continually getting low scores.
For example, there is no ?r? sound in Cantonese
and consequently Cantonese usually speaks the ?r?
phoneme with weak retroflexion.
2. Performance of phoneme recognition over a long
continuous utterance is still far from being satisfac-
tory for pedagogical purpose.
3. PLASER?s performance must be reliable even at the
expense of lower accuracy.
4. To be useful for correcting mistakes, PLASER must
provide meaningful and indicative feedbacks to pin-
point which parts of an utterance are wrongly pro-
nounced and to what extent.
5. The knowledge of IPA symbols is not a pre-requisite
to learning pronunciation.
This paper is organized as follows: in the next Section,
we first present the overall system design of PLASER.
This is followed by a discussion of our acoustic models
in Section 3. Section 4 gives a detailed description of
our confidence-based approach in pronunciation scoring,
and the related feedback visualization is given in Section
5. Both quantitative and qualitative evaluation results are
given in Section 6. Finally, we summarize the lessons we
learned in building PLASER and point out some future
works in Section 7.
Table 1: Phonemes that are taught in PLASER (written
in TIMIT-bet)
Lesson# Phoneme Pair Lesson# Phoneme Pair
1 iy ih 11 k g
2 eh ey 12 s z
3 ae ah 13 sh zh
4 aa ao 14 ch jh
5 ax er 15 f v
6 ow uh 16 th dh
7 uw ay 17 m n
8 oy aw 18 ng h
9 p b 19 l r
10 t d 20 w y
2 PLASER: System Design
PLASER runs under Microsoft Windows (98, NT, 2000)
with an easy-to-use web-like interface requiring only
standard utilities such as the Internet Explorer and Media
Player. PLASER consists of 20 lessons, and each lesson
teaches two American English phonemes as shown in Ta-
ble 1. The two phonemes in a lesson are usually the most
confusable pair among the 40 phonemes. PLASER con-
tains a lot of word examples and for each word there are
its English spelling, its Chinese translation, a picture, and
a pronunciation video-clip (PVC) which a native Amer-
ican English speaker helped record. A user may read
and listen to the materials of each word as many times
as he likes at his own pace. Besides descriptive materials,
PLASER uses four types of exercises to teach pronunci-
ation:
Read-Along Exercise: Basic pronunciation drills with
no assessment.
Minimal-Pair Listening Exercise: This is used to train
users? ear. Words from one minimal pairs are ran-
domly embedded in a sentence that makes perfect
sense with either word in the pair. A user listens to
recordings of such sentences and chooses between
the two words.
Minimal-Pair Speaking Exercise: Similar to the
Minimal-Pair Listening Exercise except that now
only minimal pairs are given and a user is asked to
say them. A student may pick any one of the two
words to say but not to mix up with its counterpart
in the pair. It is a two-class classification problem.
Word-List Speaking Exercise: A student may pick any
word from a list to say, and PLASER has to decide
how well each phoneme in the word is pronounced.
Figure 1: A snapshot of PLASER running its word exer-
cise
Fig. 1 shows a snapshot of PLASER running the Word-
List Speaking Exercise in the lesson teaching the two
phonemes: ?ih? and ?iy?. The user has selected the word
?cheese? to practise. The top left panel tells how to pro-
duce the phoneme ?iy? with the help of an animated GIF
that shows a cross-sectional view of the vocal tract dur-
ing the phoneme?s production. At the bottom right panel
are the word?s spelling, its Chinese translation, its pic-
ture, plus a recording button and a playback button. The
word?s PVC is shown at the top right panel. The mid-
dle panel in the screen is reserved for feedbacks. The
feedback for Word-List Speaking Exercise consists of an
overall score for the practising word (?cheese? here) as
well as a confidence score for each individual phoneme
in the word using a novel 3-color scheme. Confidence
scores are derived from a log-likelihood ratio between the
desired target and some reference. Garbage rejection is
also implemented in a similar manner. Refer Section 4
and 5 for more details.
As a self-learning as well as a teaching aid, the length
of each lesson is designed to take about 25?30 minutes
to complete. Students? performance is recorded for later
reviews by students themselves if PLASER is used as
a learning tool, or by teachers if PLASER is used as a
teaching aid.
3 Acoustic Modelling
For the development of PLASER?s acoustic models, addi-
tional speech data were collected from local high-school
students:
HKTIMIT: A set of TIMIT utterances collected from
a group of 61 local (Cantonese) high-school stu-
dents who spoke ?good? English to the local stan-
dard. There are 29 females and 32 males, and each
recorded 250 TIMIT sentences. The data were di-
vided into a training set of 9,163 utterances from 17
females and 20 males, and a test set of 6,015 utter-
ances from 12 females and 13 males.
MP-DATA: A superset of words used in PLASER?s
minimal-pair exercises recorded by eight high-
school students, 4 males and 4 females, each speak-
ing ?300 words for a total of 2,431 words.
WL-DATA: A superset of words used in PLASER?s
word exercises by the same eight students who
recorded the MP-DATA for a total of 2,265 words.
All data were recorded with the same conditions as those
of TIMIT. In addition, all utterances of MP-DATA and
WL-DATA were phonetically transcribed.
The standard American English TIMIT corpus to-
gether with the HKTIMIT corpus were used to develop
Cantonese-accented English phoneme HMMs. The com-
mon 13 mel-frequency cepstral coefficients and their first
and second order derivatives were used for acoustic repre-
sentation. All phoneme HMMs have three real states, and
there are an additional 3-state silence model and a 1-state
short-pause HMM. Three kinds of modelling techniques
were investigated:
Context-Independent Modelling: Context-
independent HMMs (CIHMM) were trained
for the 40 phonemes taught in PLASER. Including
the silence and short-pause models, there are totally
42 HMMs.
Position-Dependent HMM (PDHMM): Due to con-
cerns of limited computing resources in local pub-
lic schools, a restricted form of context-dependent
modelling was chosen. Since PLASER will only
perform phoneme recognition on isolated words, we
postulate that it may be important to capture the
word-boundary effect of a phoneme. Thus, three
variants of each phoneme are modelled depending
on whether it appears at the beginning, in the mid-
dle, or at the end of a word.
(MCE) Discriminative Training: With the goal of min-
imizing classification errors in a development data-
set which is WL-DATA in our case, word-based
MCE/GPD algorithm (Juang and Katagiri, 1992;
Chou, 2000) was applied to improve the EM-trained
acoustic models.
We started with a baseline system using 40 mono-
phones with 24 mixtures per state. It gives a phoneme
recognition accuracy of 39.9% on the HKTIMIT test set.
The low accuracy perhaps indicates an unexpected lower
Modelling Technique Classification
Acc. of MP-DATA
CIHMM, 24 mixtures 81.50
CIHMM, 24 mixtures + MCE 84.48
PDHMM, 20 mixtures 82.83
PDHMM, 20 mixtures + MCE 85.29
Table 2: Investigation of various modelling techniques on
minimal-pair classification
English proficiency of local students as well as a large
deviation of local English from native American English.
We then investigated PDHMM and MCE training, and
gauged our progress by the classification accuracy of
minimal pairs in the MP-DATA set. The results are tabu-
lated in Table 2.
By using PDHMMs, the inventory of models is only
increased by three times, requiring little additional com-
putational resources. Yet they result in a relative error
reduction of 7.2%. MCE discriminative training gives an
additional relative improvement of about 14?16%.
4 Confidence-based Phoneme Assessment
The assessment of pronunciation accuracy is cast as a
phoneme verification problem. The posterior probabil-
ity of a phoneme is used as the Goodness of Pronunci-
ation measure (GOP), which has been shown in many
works (Witt and Young, 2000; Franco et al, 2000) that it
is a good measure. PLASER computes both a GOP score
and a normalized GOP score for two types of feedback as
will be discussed in Section 5.
When a student runs a PLASER word exercise, s/he
will randomly pick a word from a list and watches its
pronunciation video-clip (PVC). When s/he feels com-
fortable to try s/he records her/his voice speaking the
word. PLASER then computes a confidence-based GOP
for each phoneme in the word as follows.
STEP 1: PLASER consults its dictionary for the stan-
dard phonemic transcription of the word which
should be the same as that of its PVC.
STEP 2: Based on the transcription, forced alignment is
performed on the student?s speech.
STEP 3: For each acoustic segment Xu of phoneme yu
(where u denotes the phoneme index), PLASER
computes its GOP(yu), su, as its posterior probabil-
ity by the following log-likelihood ratio normalized
by its duration Tu:
su = logProb(yu|Xu)
? 1Tu ? log
[
p(Xu|yu)p(yu)?N
k=1 p(Xu|yk)p(yk)
]
(1)
? 1Tu ? log
[ p(Xu|yu)
p(Xu|yjmax)
]
(2)
where N is the number of phonemes, and jmax is
the phoneme model that gives the highest likelihood
of the given segment. This GOP is used with some
thresholds to decide if the phoneme is pronounced
correctly.
In practice, the denominator in Equation 2 is re-
placed by the Viterbi likelihood of the segment given
by a phone loop. Notice that the Viterbi path of
a segment may contain more than one phoneme
model.
STEP 4: Besides the raw GOP score, GOP(yu) = su
computed in STEP 3, a normalized GOP score is
also computed by normalizing the GOP score to the
range [0.0 .. 1.0] using a sigmoid function. That is,
the normalized GOP for the phoneme yu is given by
sigmoid(su) = 11 + exp (??su + ?) (3)
where the parameters ? and ? are empirically found.
The current PLASER implementation has some mod-
ifications due to practical reasons: The phone loop for
computing the denominator of Equation 2 uses only the
middle-position PDHMM of each phoneme plus the si-
lence and short pause models for faster computation. For
greater computation savings, the phone loop may also be
replaced by a single Gaussian Mixture Model (GMM)
trained by all phoneme segments in the training data. In
our experience, a GMM with 32 mixtures suffices with a
slight degradation in performance.
5 Visualization of Recognition Results
Two kinds of feedback of different resolutions are given
for the word exercise:
? an overall phoneme score of the whole word; and,
? a phoneme-by-phoneme assessment by a 3-color
scheme.
5.1 Overall Phoneme Score of a Word
The use of posterior probability as the GOP score for as-
sessing the accuracy of a phoneme segment allows us to
readily define an overall phoneme score (PS) for a word
as a weighted sum of the normalized GOPs of its com-
posing phonemes:
PS(word) =
N?
k=1
wk ? normalized-GOP(phonemek) (4)
where wk is the weighting of the k-th phoneme among
the N phonemes composing the word. In the current
PLASER, all phonemes in a word are equally weighted.
5.2 A 3-Color Feedback Scheme for Phoneme
Confidence
The usefulness of an overall confidence for a word may
be limited as it does not pinpoint the pronunciation accu-
racy of each phoneme in the word, and thus, the user still
does not know how to correct his mistakes when the score
is not good. Any attempt to report phoneme confidence
score has to face the following two problems:
? unless users can read phonemic transcriptions, it
is not clear how to report the confidence scores at
phoneme level; and,
? unless the phoneme confidence scores are highly re-
liable, reporting its precise value may be too risky.
Our solution is a visual feedback that gives a color to
the letters in the word spelling to indicate the pronuncia-
tion accuracy of their associated phonemes. To do that,
STEP 1: We first designed a rule-based algorithm to
map each phoneme in the transcription of a
word to its spelling letters. For example, for
the word ?beat? with the phonemic transcrip-
tion ?/b/ /iy/ /t/?, the three phonemes are
mapped to the letters ?b?, ?ea? and ?t? respec-
tively. On the other hand, for the word ?eve? with
the phonemic transcription ?/iy/ /v/?, the two
phonemes are mapped to the letters ?e? and ?v? re-
spectively while the last letter ?e? is not mapped to
any phoneme.
STEP 2: A novel 3-color scheme was devised to reduce
the preciseness of phoneme confidence scores. Two
thresholds were found for each phoneme to label its
confidence as good, fair, or bad. If the confidence
score of a phoneme is good/fair/bad, its correspond-
ing spelling letter(s) is/are painted in blue/green/red
respectively. Two examples are shown in Fig. 2. The
use of colors is also more appealing to users.
To find the two thresholds in the 3-color scheme, we
treated the problem as a bi-threshold verification prob-
lem. The detailed algorithm is beyond the scope of this
paper and will only be briefly described here. For details,
please refer to (Ho and Mak, 2003).
Firstly, one has to decide how forgiving one wants to
be and specifies the following two figures:
? the false acceptance rate (FA) for an incorrectly pro-
nounced phoneme; and,
? the false rejection rate (FR) for a correctly pro-
nounced phoneme.
If one sets FA very low, it will be hard to get ?blue?
scores; on the other hand, if one sets FR very low, it may
be too forgiving and ?red? scores will rarely show up.
Due to its bi-threshold nature, it turns out that in such cir-
cumstances, simple method to determine the two thresh-
olds will results in dominating ?green? scores with little
?blue? or ?red? scores. The more complicated algorithm
in (Ho and Mak, 2003) tries to avoid that.
Furthermore, due to scarcity of training data in the
development data set, the phonemes were grouped into
9 phoneme classes in PLASER, and class-dependent
thresholds were determined from the development data
set. The 9 phoneme classes are: affricates, diphthongs,
fricatives, nasals, semi-vowels, stops, back vowels, mid
vowels, and front vowels.
b e a t
bad
goodfair
(a) beat
e
unused
goodfair
v e
(b) eve
Figure 2: A three-color scheme for showing phoneme
confidence (The figure has to be read with color print-
outs, or electronically on a color display. The letters
marked with ?bad?, ?fair?, ?good?, and ?unused? are
painted in red, green, blue, and gray respectively.)
6 Evaluation
A beta version of PLASER was tested by 900 students of
Grade Seven and Eight over a period of about 3 months
in twelve high schools. Both quantitative and qualitative
evaluations were conducted to gauge the effectiveness of
using PLASER to learn English pronunciation.
6.1 Quantitative Evaluation
A pronunciation test consisting of speaking 60 words
was conducted once before a student even started to
use PLASER and once after they finished the 3-month
PLASER trial. The recordings from 210 students were
successfully collected. Recordings were not obtained
from the rest of students for various reasons:
? Some schools did not have time to do the evaluation
recordings due to schedule problems.
? Some recordings were poorly administered; e.g.
parts of utterances were missing in the files.
? Some schools accidently erased or lost the recorded
speech files in their computers.
At the end, recordings from 210 students were found to
be good enough for evaluation. Their recordings were
transcribed and compared with the correct transcriptions
to find their pronunciation accuracies. The two his-
tograms in Fig. 3 summarize their pronunciation accura-
cies of the 60 words before and after they practiced with
PLASER. Here are some detailed statistics:
? 73% of the students had their pronunciation accu-
racy improved by an average of (absolute) 4.53%.
? The remaining 27% of the students got worse for
unknown reasons by an average of 2.68%.
? Collectively we observe an obvious improvement:
the mean accuracy after the use of PLASER is
greater than that before using PLASER, and the
mean difference is statistically significant at the 99%
confidence level.
6.2 Qualitative Evaluation
In addition, a questionnaire survey was conducted to get
comments and suggestions from teachers and students af-
ter they finished the study. Some figures are worth men-
tioning:
? 77% of the students believed that their pronuncia-
tion skill was improved after using PLASER while
91% of school teachers believed their students? pro-
nunciation had improved.
? 77% of the students like to use PLASER to learn
English pronunciation.
? 53% of the students preferred using PLASER to the
traditional classroom teaching method to learn En-
glish pronunciation while 73% of the teachers would
prefer their students using PLASER to self-learn
pronunciation.
0
10
20
30
40
50
60
70
80
90
0-0.499 0.5-0.549 0.55-0.599 0.6-0.649 0.65-0.699 0.7-0.749 0.75-0.799 0.8-0.849 0.85-0.99Pronunciation Correctness (min. = 0.0; max = 1.0)
N
u
m
b
e
r
 
o
f
 
S
t
u
d
e
n
t
s
before using PLASER
after using PLASER
Figure 3: Results of the pronunciation evaluation test
? All teachers would recommend their students to use
PLASER to learn English pronunciation.
7 Discussion & Future Works
More work is being planned to further improve
PLASER?s performance. Robustness is the key problem.
In the school environment, one simply cannot expect the
students to use learning tools quietly. In addition, about
forty students use a language laboratory at the same time.
Since the headset microphones available in all schools are
not uni-directional, recordings from neighboring students
are picked up on top of the user?s. This kind of ?bab-
ble noise? hurts PLASER?s performance to a great ex-
tent: not only does it affect the accuracy of our phoneme
recognizer, various thresholds used in our confidence-
based scoring and noise rejection are affected too. Var-
ious well-known robust techniques such as spectral sub-
traction (Boll, 1979), MLLR adaptation (Leggetter and
Woodland, 1995), parallel model combination (Gales and
Young, 1996), and stochastic matching (Sankar and Lee,
1996), etc. are being investigated.
To further improve phoneme discrimination, we are
trying to build statistical models to test the presence of
articulatory features in each phoneme (Leung and Siu,
2003). The outcome of the test will be a posterior prob-
ability of an articulatory feature which will then be com-
bined with the score from the acoustic models with the
hope to give even better accuracy.
Finally, the recognizer has to be optimized for the
slower machines used in many local schools.
8 Acknowledgements
This work is supported by the Hong Kong Quality Edu-
cation Fund under the grant number QEF99/00.EG01.
References
S.F. Boll. 1979. Suppression of Acoustic Noise in
Speech Using Spectral Subtraction. IEEE Transac-
tions on Acoustics, Speech and Signal Processing,
24:113?120.
W. Chou. 2000. Discriminant-Function-Based Mini-
mum Recognition Error Rate Pattern-Recognition Ap-
proach to Speech Recognition. Proceedings of the
IEEE, 88(8):1201?1223, August.
H. Franco, L. Neumeyer, V. Digalakis, and O. Ronen.
2000. Combination of Machine Scores for Automatic
Grading of Pronunciation Quality. Speech Communi-
cations, 30(2?3):121?130, Feb.
M.J.F. Gales and S.J. Young. 1996. Robust Continuous
Speech Recognition Using Parallel Model Combina-
tion. IEEE Transactions on Speech and Audio Pro-
cessing, 4(5):352?359, September.
Simon Ho and Brian Mak. 2003. English Pronuncia-
tion Evaluation as a Bi-threshold Phoneme Verification
Problem. In Proceedings of the European Conference
on Speech Communication and Technology, (submit-
ted).
B.H. Juang and S. Katagiri. 1992. Discriminative Train-
ing for Minimum Error Classification. IEEE Transac-
tion on Signal Processing, 40(12):3043?3054, Dec.
G. Kawai and K. Hirose. 2000. Teaching the Pro-
nunciation of Japanese Double-mora Phonemes using
Speech Recognition Technology. Speech Communica-
tions, 30(2?3):83?93, Feb.
C.J. Leggetter and P.C. Woodland. 1995. Maximum
Likelihood Linear Regression for Speaker Adaptation
of Continuous Density Hidden Markov Models. Jour-
nal of Computer Speech and Language, 9(2):171?185,
April.
K. Y. Leung and M. H. Siu. 2003. Phone Level
Confidence Measure Using Articulatory Features. In
Proceedings of the IEEE International Conference on
Acoustics, Speech, and Signal Processing.
J. Mostow, S. Roth, A. G. Hauptmann, and M. Kane.
1994. A Prototype Reading Coach that Listens. In
Proceedings of the Twelfth National Conference on Ar-
tificial Intelligence (AAAI-94), American Association
for Artificial Intelligence, pages 785?792.
L. Neumeyer, H. Franco, V. Digalakis, and M. Weintraub.
2000. Automatic Scoring of Pronunciation Quality.
Speech Communications, 30(2?3):83?93, Feb.
A. Sankar and C.H. Lee. 1996. A Maximum-Likelihood
Approach to Stochastic Matching for Robust Speech
Recognition. IEEE Transactions on Speech and Audio
Processing, 4(3):190?202.
S.M. Witt and S.J. Young. 2000. Phone-level Pronuncia-
tion Scoring and Assessment for Interactive Language
Learning. Speech Communications, 30(2?3):95?108,
Feb.
