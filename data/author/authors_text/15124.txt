Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 835?842, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Using the Web as an Implicit Training Set:
Application to Structural Ambiguity Resolution
Preslav Nakov and Marti Hearst
EECS and SIMS
University of California at Berkeley
Berkeley, CA 94720
nakov@cs.berkeley.edu, hearst@sims.berkeley.edu
Abstract
Recent work has shown that very large
corpora can act as training data for NLP
algorithms even without explicit labels. In
this paper we show how the use of sur-
face features and paraphrases in queries
against search engines can be used to infer
labels for structural ambiguity resolution
tasks. Using unsupervised algorithms, we
achieve 84% precision on PP-attachment
and 80% on noun compound coordination.
1 Introduction
Resolution of structural ambiguity problems such
as noun compound bracketing, prepositional phrase
(PP) attachment, and noun phrase coordination re-
quires using information about lexical items and
their cooccurrences. This in turn leads to the data
sparseness problem, since algorithms that rely on
making decisions based on individual lexical items
must have statistics about every word that may be
encountered. Past approaches have dealt with the
data sparseness problem by attempting to generalize
from semantic classes, either manually built or auto-
matically derived.
More recently, Banko and Brill (2001) have ad-
vocated for the creative use of very large text col-
lections as an alternative to sophisticated algorithms
and hand-built resources. They demonstrate the idea
on a lexical disambiguation problem for which la-
beled examples are available ?for free?. The prob-
lem is to choose which of 2-3 commonly confused
words (e.g., {principle, principal}) are appropriate
for a given context. The labeled data comes ?for
free? by assuming that in most edited written text,
the words are used correctly, so training can be done
directly from the text. Banko and Brill (2001) show
that even using a very simple algorithm, the results
continue to improve log-linearly with more training
data, even out to a billion words. A potential limita-
tion of this approach is the question of how applica-
ble it is for NLP problems more generally ? how can
we treat a large corpus as a labeled collection for a
wide range of NLP tasks?
In a related strand of work, Lapata and Keller
(2004) show that computing n-gram statistics over
very large corpora yields results that are competi-
tive with if not better than the best supervised and
knowledge-based approaches on a wide range of
NLP tasks. For example, they show that for the
problem of noun compound bracketing, the perfor-
mance of an n-gram based model computed using
search engine statistics was not significantly differ-
ent from the best supervised algorithm whose pa-
rameters were tuned and which used a taxonomy.
They find however that these approaches generally
fail to outperform supervised state-of-the-art models
that are trained on smaller corpora, and so conclude
that web-based n-gram statistics should be the base-
line to beat.
We feel the potential of these ideas is not yet fully
realized. We are interested in finding ways to further
exploit the availability of enormous web corpora as
implicit training data. This is especially important
for structural ambiguity problems in which the de-
cisions must be made on the basis of the behavior
835
of individual lexical items. The trick is to figure out
how to use information that is latent in the web as a
corpus, and web search engines as query interfaces
to that corpus.
In this paper we describe two techniques ? sur-
face features and paraphrases ? that push the ideas
of Banko and Brill (2001) and Lapata and Keller
(2004) farther, enabling the use of statistics gathered
from very large corpora in an unsupervised man-
ner. In recent work (Nakov and Hearst, 2005) we
showed that a variation of the techniques, when ap-
plied to the problem of noun compound bracketing,
produces higher accuracy than Lapata and Keller
(2004) and the best supervised results. In this pa-
per we adapt the techniques to the structural disam-
biguation problems of prepositional phrase attach-
ment and noun compound coordination.
2 Prepositional Phrase Attachment
A long-standing challenge for syntactic parsers is
the attachment decision for prepositional phrases. In
a configuration where a verb takes a noun comple-
ment that is followed by a PP, the problem arises of
whether the PP attaches to the noun or to the verb.
Consider the following contrastive pair of sentences:
(1) Peter spent millions of dollars. (noun)
(2) Peter spent time with his family. (verb)
In the first example, the PP millions of dollars at-
taches to the noun millions, while in the second the
PP with his family attaches to the verb spent.
Past work on PP-attachment has often cast these
associations as the quadruple (v, n1, p, n2), where v
is the verb, n1 is the head of the direct object, p is the
preposition (the head of the PP) and n2 is the head
of the NP inside the PP. For example, the quadruple
for (2) is (spent, time, with, family).
2.1 Related Work
Early work on PP-attachment ambiguity resolu-
tion relied on syntactic (e.g., ?minimal attachment?
and ?right association?) and pragmatic considera-
tions. Most recent work can be divided into su-
pervised and unsupervised approaches. Supervised
approaches tend to make use of semantic classes
or thesauri in order to deal with data sparseness
problems. Brill and Resnik (1994) used the su-
pervised transformation-based learning method and
lexical and conceptual classes derived from Word-
Net, achieving 82% precision on 500 randomly se-
lected examples. Ratnaparkhi et al (1994) cre-
ated a benchmark dataset of 27,937 quadruples
(v, n1, p, n2), extracted from the Wall Street Jour-
nal. They found the human performance on this
task to be 88%1. Using this dataset, they trained a
maximum entropy model and a binary hierarchy of
word classes derived by mutual information, achiev-
ing 81.6% precision. Collins and Brooks (1995)
used a supervised back-off model to achieve 84.5%
precision on the Ratnaparkhi test set. Stetina and
Makoto (1997) use a supervised method with a deci-
sion tree and WordNet classes to achieve 88.1% pre-
cision on the same test set. Toutanova et al (2004)
use a supervised method that makes use of morpho-
logical and syntactic analysis and WordNet synsets,
yielding 87.5% accuracy.
In the unsupervised approaches, the attachment
decision depends largely on co-occurrence statistics
drawn from text collections. The pioneering work
in this area was that of Hindle and Rooth (1993).
Using a partially parsed corpus, they calculate and
compare lexical associations over subsets of the tu-
ple (v, n1, p), ignoring n2, and achieve 80% preci-
sion at 80% recall.
More recently, Ratnaparkhi (1998) developed an
unsupervised method that collects statistics from
text annotated with part-of-speech tags and mor-
phological base forms. An extraction heuristic is
used to identify unambiguous attachment decisions,
for example, the algorithm can assume a noun at-
tachment if there is no verb within k words to the
left of the preposition in a given sentence, among
other conditions. This extraction heuristic uncov-
ered 910K unique tuples of the form (v, p, n2) and
(n, p, n2), although the results are very noisy, sug-
gesting the correct attachment only about 69% of the
time. The tuples are used as training data for clas-
sifiers, the best of which achieves 81.9% precision
on the Ratnaparkhi test set. Pantel and Lin (2000)
describe an unsupervised method that uses a collo-
cation database, a thesaurus, a dependency parser,
and a large corpus (125M words), achieving 84.3%
precision on the Ratnaparkhi test set. Using sim-
1When presented with a whole sentence, average humans
score 93%.
836
ple combinations of web-based n-grams, Lapata and
Keller (2005) achieve lower results, in the low 70?s.
Using a different collection consisting of German
PP-attachment decisions, Volk (2000) uses the web
to obtain n-gram counts. He compared Pr(p|n1) to
Pr(p|v), where Pr(p|x) = #(x, p)/#(x). Here x
can be n1 or v. The bigram frequencies #(x, p)
were obtained using the Altavista NEAR operator.
The method was able to make a decision on 58%
of the examples with a precision of 75% (baseline
63%). Volk (2001) then improved on these results
by comparing Pr(p, n2|n1) to Pr(p, n2|v). Using
inflected forms, he achieved P=75% and R=85%.
Calvo and Gelbukh (2003) experimented with a
variation of this, using exact phrases instead of the
NEAR operator. For example, to disambiguate Veo
al gato con un telescopio, they compared frequen-
cies for phrases such as ?ver con telescopio? and
?gato con telescopio?. They tested this idea on 181
randomly chosen Spanish disambiguation examples,
labelling 89.5% recall with a precision of 91.97%.
2.2 Models and Features
2.2.1 n-gram Models
We computed two co-occurrence models;
(i) Pr(p|n1) vs. Pr(p|v)
(ii) Pr(p, n2|n1) vs. Pr(p, n2|v).
Each of these was computed two different ways:
using Pr (probabilities) and # (frequencies). We es-
timate the n-gram counts using exact phrase queries
(with inflections, derived from WordNet 2.0) using
the MSN Search Engine. We also allow for deter-
miners, where appropriate, e.g., between the prepo-
sition and the noun when querying for #(p, n2). We
add up the frequencies for all possible variations.
Web frequencies were reliable enough and did not
need smoothing for (i), but for (ii), smoothing using
the technique described in Hindle and Rooth (1993)
led to better recall. We also tried back-off from (ii)
to (i), as well as back-off plus smoothing, but did not
find improvements over smoothing alone. We found
n-gram counts to be unreliable when pronouns ap-
pear in the test set rather than nouns, and disabled
them in these cases. Such examples can still be han-
dled by paraphrases or surface features (see below).
2.2.2 Web-Derived Surface Features
Authors sometimes (consciously or not) disam-
biguate the words they write by using surface-level
markers to suggest the correct meaning. We have
found that exploiting these markers, when they oc-
cur, can prove to be very helpful for making dis-
ambiguation decisions. The enormous size of web
search engine indexes facilitates finding such mark-
ers frequently enough to make them useful.
For example, John opened the door with a key is
a difficult verb attachment example because doors,
keys, and opening are all semantically related. To
determine if this should be a verb or a noun attach-
ment, we search for cues that indicate which of these
terms tend to associate most closely. If we see paren-
theses used as follows:
?open the door (with a key)?
this suggests a verb attachment, since the parenthe-
ses signal that ?with a key? acts as its own unit.
Similarly, hyphens, colons, capitalization, and other
punctuation can help signal disambiguation deci-
sions. For Jean ate spaghetti with sauce, if we see
?eat: spaghetti with sauce?
this suggests a noun attachment.
Table 1 illustrates a wide variety of surface fea-
tures, along with the attachment decisions they are
assumed to suggest (events of frequency 1 have been
ignored). The surface features for PP-attachment
have low recall: most of the examples have no sur-
face features extracted.
We gather the statistics needed by issuing queries
to web search engines. Unfortunately, search en-
gines usually ignore punctuation characters, thus
preventing querying directly for terms containing
hyphens, brackets, etc. We collect these numbers
indirectly by issuing queries with exact phrases and
then post-processing the top 1,000 resulting sum-
maries2, looking for the surface features of interest.
We use Google for both the surface feature and para-
phrase extractions (described below).
2.2.3 Paraphrases
The second way we extend the use of web counts
is by paraphrasing the relation of interest and see-
ing if it can be found in its alternative form, which
2We often obtain more than 1,000 summaries per example
because we usually issue multiple queries per surface pattern,
by varying inflections and inclusion of determiners.
837
suggests the correct attachment decision. We use
the following patterns along with their associated at-
tachment predictions:
(1) v n2 n1 (noun)
(2) v p n2 n1 (verb)
(3) p n2 * v n1 (verb)
(4) n1 p n2 v (noun)
(5) v pronoun p n2 (verb)
(6) be n1 p n2 (noun)
The idea behind Pattern (1) is to determine
if ?n1 p n2? can be expressed as a noun com-
pound; if this happens sufficiently often, we can
predict a noun attachment. For example, meet/v
demands/n1 from/p customers/n2 becomes meet/v
the customers/n2 demands/n1.
Note that the pattern could wrongly target ditran-
sitive verbs: e.g., it could turn gave/v an apple/n1
to/p him/n2 into gave/v him/n2 an apple/n1. To pre-
vent this, we do not allow a determiner before n1,
but we do require one before n2. In addition, we
disallow the pattern if the preposition is to and we
require both n1 and n2 to be nouns (as opposed to
numbers, percents, pronouns, determiners etc.).
Pattern (2) predicts a verb attachment. It presup-
poses that ?p n2? is an indirect object of the verb v
and tries to switch it with the direct object n1, e.g.,
had/v a program/n1 in/p place/n2 would be trans-
formed into had/v in/p place/n2 a program/n1. We
require n1 to be preceded by a determiner (to prevent
?n2 n1? forming a noun compound).
Pattern (3) looks for appositions, where the PP has
moved in front of the verb, e.g., to/p him/n2 I gave/v
an apple/n1. The symbol * indicates a wildcard po-
sition where we allow up to three intervening words.
Pattern (4) looks for appositions, where the PP has
moved in front of the verb together with n1. It would
transform shaken/v confidence/n1 in/p markets/n2
into confidence/n1 in/p markets/n2 shaken/v.
Pattern (5) is motivated by the observation that
if n1 is a pronoun, this suggests a verb attach-
ment (Hindle and Rooth, 1993). (A separate feature
checks if n1 is a pronoun.) The pattern substitutes
n1 with a dative pronoun (we allow him and her),
e.g., it will convert put/v a client/n1 at/p odds/n2
into put/v him at/p odds/n2.
Pattern (6) is motivated by the observation that the
verb to be is typically used with a noun attachment.
(A separate feature checks if v is a form of the verb
to be.) The pattern substitutes v with is and are, e.g.
it will turn eat/v spaghetti/n1 with/p sauce/n2 into is
spaghetti/n1 with/p sauce/n2.
These patterns all allow for determiners where ap-
propriate, unless explicitly stated otherwise. For a
given example, a prediction is made if at least one
instance of the pattern has been found.
2.3 Evaluation
For the evaluation, we used the test part (3,097 ex-
amples) of the benchmark dataset by Ratnaparkhi et
al. (1994). We used all 3,097 test examples in order
to make our results directly comparable.
Unfortunately, there are numerous errors in the
test set3. There are 149 examples in which a bare
determiner is labeled as n1 or n2 rather than the ac-
tual head noun. Supervised algorithms can compen-
sate for this problem by learning from the training
set that ?the? can act as a noun in this collection, but
unsupervised algorithms cannot.
In addition, there are also around 230 examples
in which the nouns contain special symbols like: %,
slash, &, ?, which are lost when querying against a
search engine. This poses a problem for our algo-
rithm but is not a problem with the test set itself.
The results are shown in Table 2. Following Rat-
naparkhi (1998), we predict a noun attachment if the
preposition is of (a very reliable heuristic). The table
shows the performance for each feature in isolation
(excluding examples whose preposition is of). The
surface features are represented by a single score in
Table 2: for a given example, we sum up separately
the number of noun- and verb-attachment pattern
matches, and assign the attachment with the larger
number of matches.
We combine the bold rows of Table 2 in a majority
vote (assigning noun attachment to all of instances),
obtaining P=85.01%, R=91.77%. To get 100% re-
call, we assign all undecided cases to verb (since
the majority of the remaining non-of instances at-
tach to the verb, yielding P=83.63%, R=100%. We
show 0.95-level confidence intervals for the preci-
sion, computed by a general method based on con-
stant chi-square boundaries (Fleiss, 1981).
A test for statistical significance reveals that our
results are as strong as those of the leading unsuper-
3Ratnaparkhi (1998) notes that the test set contains errors,
but does not correct them.
838
Example Predicts P(%) R(%)
open Door with a key noun 100.00 0.13
(open) door with a key noun 66.67 0.28
open (door with a key) noun 71.43 0.97
open - door with a key noun 69.70 1.52
open / door with a key noun 60.00 0.46
open, door with a key noun 65.77 5.11
open: door with a key noun 64.71 1.57
open; door with a key noun 60.00 0.23
open. door with a key noun 64.13 4.24
open? door with a key noun 83.33 0.55
open! door with a key noun 66.67 0.14
open door With a Key verb 0.00 0.00
(open door) with a key verb 50.00 0.09
open door (with a key) verb 73.58 2.44
open door - with a key verb 68.18 2.03
open door / with a key verb 100.00 0.14
open door, with a key verb 58.44 7.09
open door: with a key verb 70.59 0.78
open door; with a key verb 75.00 0.18
open door. with a key verb 60.77 5.99
open door! with a key verb 100.00 0.18
Table 1: PP-attachment surface features. Preci-
sion and recall shown are across all examples, not
just the door example shown.
vised approach on this collection (Pantel and Lin,
2000). Unlike that work, we do not require a collo-
cation database, a thesaurus, a dependency parser,
nor a large domain-dependent text corpus, which
makes our approach easier to implement and to ex-
tend to other languages.
3 Coordination
Coordinating conjunctions (and, or, but, etc.) pose
major challenges to parsers and their proper han-
dling is essential for the understanding of the sen-
tence. Consider the following ?cooked? example:
The Department of Chronic Diseases and Health
Promotion leads and strengthens global efforts to
prevent and control chronic diseases or disabilities
and to promote health and quality of life.
Conjunctions can link two words, two con-
stituents (e.g., NPs), two clauses or even two sen-
tences. Thus, the first challenge is to identify the
boundaries of the conjuncts of each coordination.
The next problem comes from the interaction of
the coordinations with other constituents that attach
to its conjuncts (most often prepositional phrases).
In the example above we need to decide between
[health and [quality of life]] and [[health and qual-
Model P(%) R(%)
Baseline (noun attach) 41.82 100.00
#(x, p) 58.91 83.97
Pr(p|x) 66.81 83.97
Pr(p|x) smoothed 66.81 83.97
#(x, p, n2) 65.78 81.02
Pr(p, n2|x) 68.34 81.62
Pr(p, n2|x) smoothed 68.46 83.97
(1) ?v n2 n1? 59.29 22.06
(2) ?p n2 v n1? 57.79 71.58
(3) ?n1 * p n2 v? 65.78 20.73
(4) ?v p n2 n1? 81.05 8.75
(5) ?v pronoun p n2? 75.30 30.40
(6) ?be n1 p n2? 63.65 30.54
n1 is pronoun 98.48 3.04
v is to be 79.23 9.53
Surface features (summed) 73.13 9.26
Maj. vote, of ? noun 85.01?1.21 91.77
Maj. vote, of ? noun, N/A ? verb 83.63?1.30 100.00
Table 2: PP-attachment results, in percentages.
ity] of life]. From a semantic point of view, we
need to determine whether the or in chronic dis-
eases or disabilities really means or or is used as an
and (Agarwal and Boggess, 1992). Finally, we need
to choose between a non-elided and an elided read-
ing: [[chronic diseases] or disabilities] vs. [chronic
[diseases or disabilities]].
Below we focus on a special case of the latter
problem: noun compound (NC) coordination. Con-
sider the NC car and truck production. Its real
meaning is car production and truck production.
However, due to the principle of economy of ex-
pression, the first instance of production has been
compressed out by means of ellipsis. By contrast,
in president and chief executive, president is simply
linked to chief executive. There is also an all-way co-
ordination, where the conjunct is part of the whole,
as in Securities and Exchange Commission.
More formally, we consider configurations of the
kind n1 c n2 h, where n1 and n2 are nouns, c is a
coordination (and or or) and h is the head noun4.
The task is to decide whether there is an ellipsis or
not, independently of the local context. Syntacti-
cally, this can be expressed by the following brack-
etings: [[n1 c n2] h] versus [n1 c [n2 h]]. (Collins?
parser (Collins, 1997) always predicts a flat NP for
such configurations.) In order to make the task more
4The configurations of the kind n h1 c h2 (e.g., company/n
cars/h1 and/c trucks/h2) can be handled in a similar way.
839
realistic (from a parser?s perspective), we ignore the
option of all-way coordination and try to predict the
bracketing in Penn Treebank (Marcus et al, 1994)
for configurations of this kind. The Penn Treebank
brackets NCs with ellipsis as, e.g.,
(NP car/NN and/CC truck/NN production/NN).
and without ellipsis as
(NP (NP president/NN) and/CC (NP chief/NN exec-
utive/NN))
The NPs with ellipsis are flat, while the others con-
tain internal NPs. The all-way coordinations can ap-
pear bracketed either way and make the task harder.
3.1 Related Work
Coordination ambiguity is under-explored, despite
being one of the three major sources of structural
ambiguity (together with prepositional phrase at-
tachment and noun compound bracketing), and be-
longing to the class of ambiguities for which the
number of analyses is the number of binary trees
over the corresponding nodes (Church and Patil,
1982), and despite the fact that conjunctions are
among the most frequent words.
Rus et al (2002) present a deterministic rule-
based approach for bracketing in context of coor-
dinated NCs of the kind n1 c n2 h, as a necessary
step towards logical form derivation. Their algo-
rithm uses POS tagging, syntactic parses, semantic
senses of the nouns (manually annotated), lookups
in a semantic network (WordNet) and the type of the
coordination conjunction to make a 3-way classifi-
cation: ellipsis, no ellipsis and all-way coordination.
Using a back-off sequence of 3 different heuristics,
they achieve 83.52% precision (baseline 61.52%) on
a set of 298 examples. When 3 additional context-
dependent heuristics and 224 additional examples
with local contexts are added, the precision jumps
to 87.42% (baseline 52.35%), with 71.05% recall.
Resnik (1999) disambiguates two kinds of pat-
terns: n1 and n2 n3 and n1 n2 and n3 n4
(e.g., [food/n1 [handling/n2 and/c storage/n3]
procedures/n4]). While there are two options for
the former (all-way coordinations are not allowed),
there are 5 valid bracketings for the latter. Follow-
ing Kurohashi and Nagao (1992), Resnik makes de-
cisions based on similarity of form (i.e., number
agreement: P=53%, R=90.6%), similarity of mean-
ing (P=66%, R=71.2%) and conceptual association
Example Predicts P(%) R(%)
(buy) and sell orders NO ellipsis 33.33 1.40
buy (and sell orders) NO ellipsis 70.00 4.67
buy: and sell orders NO ellipsis 0.00 0.00
buy; and sell orders NO ellipsis 66.67 2.80
buy. and sell orders NO ellipsis 68.57 8.18
buy[...] and sell orders NO ellipsis 49.00 46.73
buy- and sell orders ellipsis 77.27 5.14
buy and sell / orders ellipsis 50.54 21.73
(buy and sell) orders ellipsis 92.31 3.04
buy and sell (orders) ellipsis 90.91 2.57
buy and sell, orders ellipsis 92.86 13.08
buy and sell: orders ellipsis 93.75 3.74
buy and sell; orders ellipsis 100.00 1.87
buy and sell. orders ellipsis 93.33 7.01
buy and sell[...] orders ellipsis 85.19 18.93
Table 3: Coordination surface features. Precision
and recall shown are across all examples, not just the
buy and sell orders shown.
(P=75.0%, R=69.3%). Using a decision tree to com-
bine the three information sources, he achieves 80%
precision (baseline 66%) at 100% recall for the 3-
noun coordinations. For the 4-noun coordinations
the precision is 81.6% (baseline 44.9%), 85.4% re-
call.
Chantree et al (2005) cover a large set of ambi-
guities, not limited to nouns. They allow the head
word to be a noun, a verb or an adjective, and the
modifier to be an adjective, a preposition, an ad-
verb, etc. They extract distributional information
from the British National Corpus and distributional
similarities between words, similarly to (Resnik,
1999). In two different experiments they achieve
P=88.2%, R=38.5% and P=80.8%, R=53.8% (base-
line P=75%).
Goldberg (1999) resolves the attachment of am-
biguous coordinate phrases of the kind n1 p n2 c
n3, e.g., box/n1 of/p chocolates/n2 and/c roses/n3.
Using an adaptation of the algorithm proposed by
Ratnaparkhi (1998) for PP-attachment, she achieves
P=72% (baseline P=64%), R=100.00%.
Agarwal and Boggess (1992) focus on the identi-
fication of the conjuncts of coordinate conjunctions.
Using POS and case labels in a deterministic algo-
rithm, they achieve P=81.6%. Kurohashi and Na-
gao (1992) work on the same problem for Japanese.
Their algorithm looks for similar word sequences
among with sentence simplification, and achieves a
precision of 81.3%.
840
3.2 Models and Features
3.2.1 n-gram Models
We use the following n-gram models:
(i) #(n1, h) vs. #(n2, h)
(ii) #(n1, h) vs. #(n1, c, n2)
Model (i) compares how likely it is that n1 mod-
ifies h, as opposed to n2 modifying h. Model (ii)
checks which association is stronger: between n1
and h, or between n1 and n2. Regardless of whether
the coordination is or or and, we query for both and
we add up the corresponding counts.
3.2.2 Web-Derived Surface Features
The set of surface features is similar to the one we
used for PP-attachment. These are brackets, slash,
comma, colon, semicolon, dot, question mark, ex-
clamation mark, and any character. There are two
additional ellipsis-predicting features: a dash after
n1 and a slash after n2, see Table 3.
3.2.3 Paraphrases
We use the following paraphrase patterns:
(1) n2 c n1 h (ellipsis)
(2) n2 h c n1 (NO ellipsis)
(3) n1 h c n2 h (ellipsis)
(4) n2 h c n1 h (ellipsis)
If matched frequently enough, each of these pat-
terns predicts the coordination decision indicated in
parentheses. If found only infrequently or not found
at all, the opposite decision is made. Pattern (1)
switches the places of n1 and n2 in the coordinated
NC. For example, bar and pie graph can easily be-
come pie and bar graph, which favors ellipsis. Pat-
tern (2) moves n2 and h together to the left of the
coordination conjunction, and places n1 to the right.
If this happens frequently enough, there is no ellip-
sis. Pattern (3) inserts the elided head h after n1 with
the hope that if there is ellipsis, we will find the full
phrase elsewhere in the data. Pattern (4) combines
pattern (1) and pattern (3); it not only inserts h after
n1 but also switches the places of n1 and n2.
As shown in Table 4, we included four of the
heuristics by Rus et al (2002). Heuristic 1 predicts
no coordination when n1 and n2 are the same, e.g.,
milk and milk products. Heuristics 2 and 3 perform a
lookup in WordNet and we did not use them. Heuris-
tics 4, 5 and 6 exploit the local context, namely the
Model P(%) R(%)
Baseline: ellipsis 56.54 100.00
(n1, h) vs. (n2, h) 80.33 28.50
(n1, h) vs. (n1, c, n2) 61.14 45.09
(n2, c, n1, h) 88.33 14.02
(n2, h, c, n1) 76.60 21.96
(n1, h, c, n2, h) 75.00 6.54
(n2, h, c, n1, h) 78.67 17.52
Heuristic 1 75.00 0.93
Heuristic 4 64.29 6.54
Heuristic 5 61.54 12.15
Heuristic 6 87.09 7.24
Number agreement 72.22 46.26
Surface sum 82.80 21.73
Majority vote 83.82 80.84
Majority vote, N/A ? no ellipsis 80.61 100.00
Table 4: Coordination results, in percentages.
adjectives modifying n1 and/or n2. Heuristic 4 pre-
dicts no ellipsis if both n1 and n2 are modified by
adjectives. Heuristic 5 predicts ellipsis if the coor-
dination is or and n1 is modified by an adjective,
but n2 is not. Heuristic 6 predicts no ellipsis if n1
is not modified by an adjective, but n2 is. We used
versions of heuristics 4, 5 and 6 that check for deter-
miners rather than adjectives.
Finally, we included the number agreement fea-
ture (Resnik, 1993): (a) if n1 and n2 match in num-
ber, but n1 and h do not, predict ellipsis; (b) if n1
and n2 do not match in number, but n1 and h do,
predict no ellipsis; (c) otherwise leave undecided.
3.3 Evaluation
We evaluated the algorithms on a collection of 428
examples extracted from the Penn Treebank. On ex-
traction, determiners and non-noun modifiers were
allowed, but the program was only presented with
the quadruple (n1, c, n2, h). As Table 4 shows, our
overall performance of 80.61 is on par with other ap-
proaches, whose best scores fall into the low 80?s for
precision. (Direct comparison is not possible, as the
tasks and datasets all differ.)
As Table 4 shows, n-gram model (i) performs
well, but n-gram model (ii) performs poorly, proba-
bly because the (n1, c, n2) contains three words, as
opposed to two for the alternative (n1, h), and thus
a priori is less likely to be observed.
The surface features are less effective for resolv-
ing coordinations. As Table 3 shows, they are very
good predictors of ellipsis, but are less reliable when
841
predicting NO ellipsis. We combine the bold rows
of Table 4 in a majority vote, obtaining P=83.82%,
R=80.84%. We assign all undecided cases to no el-
lipsis, yielding P=80.61%, R=100%.
4 Conclusions and Future Work
We have shown that simple unsupervised algorithms
that make use of bigrams, surface features and para-
phrases extracted from a very large corpus are ef-
fective for several structural ambiguity resolutions
tasks, yielding results competitive with the best un-
supervised results, and close to supervised results.
The method does not require labeled training data,
nor lexicons nor ontologies. We think this is a
promising direction for a wide range of NLP tasks.
In future work we intend to explore better-motivated
evidence combination algorithms and to apply the
approach to other NLP problems.
Acknowledgements. This research was supported
by NSF DBI-0317510 and a gift from Genentech.
References
Rajeev Agarwal and Lois Boggess. 1992. A simple but useful
approach to conjunct identification. In Proceedings of ACL.
Michele Banko and Eric Brill. 2001. Scaling to very very large
corpora for natural language disambiguation. In Proceed-
ings of ACL.
Eric Brill and Philip Resnik. 1994. A rule-based approach
to prepositional phrase attachment disambiguation. In Pro-
ceedings of COLING.
Hiram Calvo and Alexander Gelbukh. 2003. Improving prepo-
sitional phrase attachment disambiguation using the web as
corpus. In Progress in Pattern Recognition, Speech and
Image Analysis: 8th Iberoamerican Congress on Pattern
Recognition, CIARP 2003.
Francis Chantree, Adam Kilgarriff, Anne De Roeck, and Alis-
tair Willis. 2005. Using a distributional thesaurus to resolve
coordination ambiguities. In Technical Report 2005/02. The
Open University, UK.
Kenneth Church and Ramesh Patil. 1982. Coping with syntac-
tic ambiguity or how to put the block in the box on the table.
Amer. J. of Computational Linguistics, 8(3-4):139?149.
Michael Collins and James Brooks. 1995. Prepositional phrase
attachment through a backed-off model. In Proceedings of
EMNLP, pages 27?38.
M. Collins. 1997. Three generative, lexicalised models for sta-
tistical parsing. In Proceedings of ACL, pages 16?23.
Joseph Fleiss. 1981. Statistical Methods for Rates and Propor-
tions (2nd Ed.). John Wiley & Sons, New York.
Miriam Goldberg. 1999. An unsupervised model for statis-
tically determining coordinate phrase attachment. In Pro-
ceedings of ACL.
Donald Hindle and Mats Rooth. 1993. Structural ambiguity
and lexical relations. Computational Linguistics, 19(1):103?
120.
Sadao Kurohashi and Makoto Nagao. 1992. Dynamic pro-
gramming method for analyzing conjunctive structures in
japanese. In Proceedings of COLING, volume 1.
Mirella Lapata and Frank Keller. 2004. The Web as a base-
line: Evaluating the performance of unsupervised Web-
based models for a range of NLP tasks. In Proceedings of
HLT-NAACL, pages 121?128, Boston.
Mirella Lapata and Frank Keller. 2005. Web-based models for
natural language processing. ACM Transactions on Speech
and Language Processing, 2:1?31.
Mitchell Marcus, Beatrice Santorini, and Mary Marcinkiewicz.
1994. Building a large annotated corpus of English: The
Penn Treebank. Computational Linguistics, 19(2):313?330.
Preslav Nakov and Marti Hearst. 2005. Search engine statistics
beyond the n-gram: Application to noun compound bracket-
ing. In Proceedings of CoNLL 2005.
Patrick Pantel and Dekang Lin. 2000. An unsupervised ap-
proach to prepositional phrase attachment using contextually
similar words. In Proceedings of ACL.
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos. 1994.
A maximum entropy model for prepositional phrase attach-
ment. In Proceedings of the ARPA Workshop on Human Lan-
guage Technology., pages 250?255.
Adwait Ratnaparkhi. 1998. Statistical models for unsuper-
vised prepositional phrase attachment. In Proceedings of
COLING-ACL, volume 2, pages 1079?1085.
Philip Resnik. 1993. Selection and information: a class-based
approach to lexical relationships. Ph.D. thesis, University
of Pennsylvania, UMI Order No. GAX94-13894.
Philip Resnik. 1999. Semantic similarity in a taxonomy: An
information-based measure and its application to problems
of ambiguity in natural language. JAIR, 11:95?130.
Vasile Rus, Dan Moldovan, and Orest Bolohan. 2002. Brack-
eting compound nouns for logic form derivation. In Su-
san M. Haller and Gene Simmons, editors, FLAIRS Confer-
ence. AAAI Press.
Jiri Stetina and Makoto. 1997. Corpus based PP attachment
ambiguity resolution with a semantic dictionary. In Proceed-
ings of WVLC, pages 66?80.
Kristina Toutanova, Christopher D. Manning, and Andrew Y.
Ng. 2004. Learning random walk models for inducing word
dependency distributions. In Proceedings of ICML.
Martin Volk. 2000. Scaling up. using the WWW to resolve PP
attachment ambiguities. In Proceedings of Konvens-2000.
Sprachkommunikation.
Martin Volk. 2001. Exploiting the WWW as a corpus to resolve
PP attachment ambiguities. In Proc. of Corpus Linguistics.
842
Category-Based Pseudowords
Preslav I. Nakov
EECS, UC Berkeley
Berkeley, CA 94720
nakov@cs.berkeley.edu
Marti A. Hearst
SIMS, UC Berkeley
Berkeley, CA 94720
hearst@sims.berkeley.edu
Abstract
A pseudoword is a composite comprised of two
or more words chosen at random; the individual
occurrences of the original words within a text
are replaced by their conflation. Pseudowords
are a useful mechanism for evaluating the im-
pact of word sense ambiguity in many NLP
applications. However, the standard method
for constructing pseudowords has some draw-
backs. Because the constituent words are cho-
sen at random, the word contexts that surround
pseudowords do not necessarily reflect the con-
texts that real ambiguous words occur in. This
in turn leads to an optimistic upper bound on
algorithm performance. To address these draw-
backs, we propose the use of lexical categories
to create more realistic pseudowords, and eval-
uate the results of different variations of this
idea against the standard approach.
1 Introduction
In order to evaluate a word sense disambiguation (WSD)
algorithm in a new language or domain, a sense-tagged
evaluation corpus is needed, but this is expensive to pro-
duce manually. As an alternative, researchers often use
pseudowords. To create a pseudoword, two or more
randomly-chosen words (e.g., banana and door) are se-
lected and their individual occurrences are replaced by
their conflation (e.g., banana-door). Since their introduc-
tion (Gale et al, 1992; Schuetze, 1992), pseudowords
have been accepted as an upper bound of the true accu-
racy of algorithms that assign word sense distinctions.
In most cases, constituent words are chosen entirely
randomly. When used to evaluate a real WSD system on
the SENSEVAL1 corpus, pseudowords were found to be
optimistic in their estimations compared to real ambigu-
ous words with the same distribution (Gaustad, 2001).
Real ambiguous words often have senses that are similar
in meaning, and thus difficult to distinguish (as measured
by low inter-annotator agreement), while pseudowords,
because they are randomly chosen, are highly likely to
combine semantically distinct words. Another drawback
is that the results produced using pseudowords are dif-
ficult to characterize in terms of the types of ambiguity
they model.
To create more plausibly-motivated pseudoword pair-
ings, we introduce the use of lexical category member-
ship for pseudoword generation. The main idea is to take
note of the relative frequencies at which pairs of lexi-
cal categories tend to represent real ambiguous words,
and then use unambiguous words drawn from those cate-
gories to generate pseudowords. In the remainder of this
paper we describe the category-based pseudoword gener-
ation process and evaluate the results against the standard
methods and against a real word sense disambiguation
task.
2 MeSH and Medline
In this paper we use the MeSH (Medical Subject Head-
ings) lexical hierarchy1, but the approach should be
equally applicable to other domains using other thesauri
and ontologies. In MeSH, each concept is assigned one
or more alphanumeric descriptor codes corresponding
to particular positions in the hierarchy. For example,
A (Anatomy), A01 (Body Regions), A01.456 (Head),
A01.456.505 (Face), A01.456.505.420 (Eye). Eye is
ambiguous according to MeSH and has a second code:
A09.371 (A09 represents Sense Organs).
In the studies reported here, we truncate the MeSH
code at the first period. This allows for generalization
over different words; e.g., for eye, we discriminate be-
tween senses represented by A01 and A09. This trun-
cation reduces the average number of senses per token
from 2.12 to 1.39, and the maximum number of ambigu-
ity classes for a given word to 7; 71.18% of the tokens
have a single class and 22.14% have two classes. From
a collection of 180,226 abstracts from Medline 20032,
1http://www.nlm.nih.gov/mesh
2235 MB of plain text, after XML removal, from files med-
Ambig. pair Pair freq. Class 1 freq. Class 2 freq
{A11,A15} 16127 49350 3417
{A12,A15} 13662 7403 3417
{D12,D24} 12608 28805 17064
{E05,H01} 11753 17506 40744
{I01,N03} 6988 7721 11046
{A02,A10} 6834 4936 14083
Table 1: Most frequent ambiguous 2-category pairs.
training was done on 2/3 of the abstracts (120,150) and
testing on the remaining 1/3 (60,076).
3 Pseudoword Generation
For the creation of pseudowrods with two-sense ambigui-
ties, we first determined which ambiguous words fall into
exactly two MeSH categories and built a list L of pairs
(see Table 1). We then generated pseudowords with the
following characteristics:
? The two possible pseudoword categories represent a
pair that is really seen in the testing corpus and thus
needs to be disambiguated;
? The number of pseudowords drawn from a particular
pair is proportional to its frequency;
? Multi-word concepts can be used as pseudoword
elements: e.g., ion-exchange chromatography
and long-term effects can be conflated as ion-
exchange chromatography long-term effects
? Only unambiguous words are used as pseudoword
constituents.
An important aspect of pseudoword creation is the rel-
ative frequencies of the underlying words. Since the stan-
dard baseline for a WSD algorithm is to always choose
the most frequent sense, a baseline that is evaluated on
words whose senses are evenly balanced will be expected
to do more poorly than one tested against words that are
heavily skewed towards one sense (Sanderson & van Ri-
jsbergen, 1999).
In naturally occurring text, the more frequent sense for
the two-sense distinction is reported to occur 92% of the
time on average; this result has been found both on the
CACM collection and on the WordNet SEMCOR sense-
tagged corpus (Sanderson & van Rijsbergen, 1999).
However, the challenge for WSD programs is to work on
the harder cases, and the artificially constructed SENSE-
VAL1 corpus has more evenly distributed senses (Gaus-
tad, 2001).
In these experiments, we explicitly compare pseu-
dowords whose underlying word frequencies are even
line03n0201.xml through med-line03n0209.xml.
w1 w2 pair #w1 #w2
artifact triton {E05,H01} 55 40
humerus mucus memb. {A02,A10} 51 38
lovastatin palmitic acid {D04,D10} 35 54
child abuse Minnesota {I01,Z01} 39 45
thumb pupils {A01,A09} 56 38
haptoglobin hla antigens {D12,D24} 46 53
Table 2: Sample pseudowords.
against those that are skewed. To generate pseudowords
with more uniform underlying distributions, we first cal-
culate the expected testing corpus frequency of those
words wi that have been unambiguously mapped to
MeSH and whose class is used in at least one pair in L. In
this collection the expected frequency was E = 45.21 with
a standard deviation of 451.19. We then built a list W of
all MeSH concepts mapped in the text that have a class
used in a pair in L and whose frequency is in the interval
[E/2;3E/2], i.e. [34;56]. This yields a list of concepts that
could potentially be combined in 64,596 pseudowords for
evaluation of the WSD algorithm performance over the
classes in L.
We then generated a random subset of 1,000 pseu-
dowords (88,758 instances) out of the possible 64,596 by
applying the following importance sampling procedure:
1) Select a category pair c1,c2 from L by sampling
from a multinomial distribution whose parameters are
proportional to the frequencies of the elements of L.
2) Sample uniformly to draw two random distinct
words w1 and w2 from W whose classes correspond to
the classes selected in step 1).
3) If the word pair w1,w2 has been sampled already, go
to step 1) and try again.
Table 2 shows a random selection of pseudowords gen-
erated by the algorithm. Note that the more unusual pair-
ings come from the less frequent category pairs, whereas
those in which word senses are closer in meaning are
drawn from more common category pairs.
4 Results
For the experiments reported below, we trained an un-
supervised Naive Bayes classifier using the categories as
both targets and as context features. For example, an oc-
currence of the word haptoglobin in the context surround-
ing the word to be disambiguated will be replaced by its
category label D12. Only unambiguous context words
were used. The result of the disambiguation step is a cat-
egory name, standing as a proxy for the word sense.
Table 3 reports accuracies for several experiments in
terms of macroaverages (average over the individual ac-
curacies for each pseudoword). Baseline refers to choos-
CW Base. Pess. Real. Abbrev. Opt.
10 53.24 62.93 64.60 70.37 71.35
20 53.24 66.80 68.90 73.83 76.36
40 53.24 69.92 73.28 76.46 80.03
300 53.24 72.79 75.34 77.99 81.88
Table 3: Accuracies (in %?s) of Baseline, Pessimistic, Re-
alistic, Abbreviation, and Optimistic datasets for different
context window (CW) sizes.
AAP: acetaminophen D02
auricular acupuncture E02
GST: general systems theory H01
glutathione s-transferase D08
ED: eating disorders F03
endogenous depression F03
elemental diet J02
Table 4: Sample category mappings for abbreviations.
ing the most frequent sense3. Pessimistic refers to the
evenly distributed category-based pseudowords, gener-
ated by requiring the word frequency to fall in the interval
[E/2;3E/2]. In the column labeled Realistic, the require-
ment for evenly distributed senses is dropped, although
the component words must have a frequency of at least
5. The column labeled Optimistic refers to the results
when the pseudowords are generated the standard way:
the words are selected at random rather than according to
the category sets.
We expected the Realistic pseudowords to produce
a better lower-bound estimate of the performance of a
WSD algorithm on real word senses than Optimistic. To
test this hypothesis we followed a method suggested by
Liu et al (2002) and evaluated the classifier on a set of
217 two-sense abbreviations (see Table 4).
Abbreviations are real ambiguous words, but they are
also artificial in a sense. Many homonyms are similar in
meaning as well as spelling because they derive etymo-
logically from the same root. By contrast, similar spelling
in abbreviations is often simply an accident of shared ini-
tial characters in compound nouns. Thus abbreviations
occupy an intermediate position between entirely random
pseudowords and standard real ambiguous words.
We extracted 98,841 unique abbreviation-expansion
pairs4 using code developed by Schwartz & Hearst
(2003), and retained only those abbreviations whose ex-
pansions could be fully and unambiguously mapped to
a single truncated MeSH category. The different expan-
sions of each abbreviation were required to correspond
3The baseline is dependent on the (pseudo)words used. The
one shown is the baseline for the abbreviations collection.
4From med-line03n0210.xml to med-line03n0229.xml.
to exactly two distinct categories (with overlap allowed
when there were more than two expansions for a given
abbreviation).
The question we wanted to explore is how well does
the classifier do on category-based pseudowords versus
abbreviations. As can be seen from Table 3, the ac-
curacies for the abbreviations (evaluated on 332,020 in-
stances) fall between the Realistic and Optimistic pseu-
dowords, as expected.
5 Conclusions
We have shown that creating pseudowords based on dis-
tributions from lexical category co-occurrence can pro-
duce a more accurate lower-bound for WSD systems
that use pseudowords than the standard approach. This
method allows for the detailed study of a particular sense
ambiguity set since many different pseudowords can be
generated from one category pair. Additionally, this
method provides a better-motivated basis for the grouping
of words into pseudowords, since they more realistically
model the meaning similarity patterns of real ambiguous
words than do randomly paired words.
Acknowledgements Special thanks to Barbara
Rosario for the discussions and valuable suggestions
and to Ariel Schwartz for providing the abbreviation
extraction code. This work was supported by a gift from
Genentech and an ARDA Aquaint contact.
References
William A. Gale, Kenneth W. Church and David
Yarowsky. 1992. Work on statistical methods for word
sense disambiguation., In R. Goldman et al (Eds.),
Working Notes of the AAAI Fall Symposium on Prob-
abilistic Approaches to Natural Language, 54-60.
Tanja Gaustad. 2001. Statistical Corpus-Based Word
Sense Disambiguation: Pseudowords vs. Real Am-
biguous Words., Proc. 39th Annual Meeting of ACL
(ACL/EACL 2001) - Student Research Workshop.
Hongfang Liu, Stephen B. Johnson and Carol Friedman.
2002. Automatic Resolution of Ambiguous Terms
Based on Machine Learning and Conceptual Relations
in the UMLS, JAMIA 2002.
Mark Sanderson and Keith van Rijsbergen. 1999. The
impact on retrieval effectiveness of skewed frequency
distributions., TOIS 17(4): 440-465.
Hinrich Schuetze. 1992. Context space., In R. Goldman
et al (Eds.), Working Notes of the AAAI Fall Sym-
posium on Probabilistic Approaches to Natural Lan-
guage, 54-60.
Ariel Schwartz and Marti Hearst. 2003. A Simple
Algorithm for Identifying Abbreviation Definitions in
Biomedical Text., In Proceedings of the Pacific Sympo-
sium on Biocomputing (PSB 2003) Kauai, Jan 2003.
Supporting Annotation Layers for Natural Language Processing
Preslav Nakov, Ariel Schwartz, Brian Wolf
Computer Science Division
University of California, Berkeley
Berkeley, CA 94720
{nakov,sariel}@cs.berkeley.edu
Marti Hearst
SIMS
University of California, Berkeley
Berkeley, CA 94720
hearst@sims.berkeley.edu
Abstract
We demonstrate a system for flexible
querying against text that has been anno-
tated with the results of NLP processing.
The system supports self-overlapping and
parallel layers, integration of syntactic and
ontological hierarchies, flexibility in the
format of returned results, and tight inte-
gration with SQL. We present a query lan-
guage and its use on examples taken from
the NLP literature.
1 Introduction
Today most natural language processing (NLP)
algorithms make use of the results of previous
processing steps. For example, a word sense disam-
biguation algorithm may combine the output of a to-
kenizer, a part-of-speech tagger, a phrase boundary
recognizer, and a module that classifies noun phrases
into semantic categories. Currently there is no stan-
dard way to represent and store the results of such
processing for efficient retrieval.
We propose a framework for annotating text with
the results of NLP processing and then querying
against those annotations in flexible ways. The
framework includes a query language and an in-
dexing architecture for efficient retrieval, built on
top of a relational database management system
(RDBMS). The model allows for both hierarchical
and overlapping layers of annotation as well as for
querying at multiple levels of description.
In the remainder of the paper we describe related
work, illustrate the annotation model and the query
language and describe the indexing architecture and
the experimental results, thus showing the feasibility
of the approach for a variety of NLP tasks.
2 Related Work
There are several specialized tools for indexing and
querying treebanks. (See Bird et al (2005) for an
overview and critical comparisons.) TGrep21 is a
a grep-like utility for the Penn Treebank corpus of
parsed Wall Street Journal texts. It allows Boolean
expressions over nodes and regular expressions in-
side nodes. Matching uses a binary index and is
performed recursively starting at the top node in the
query. TIGERSearch2 is associated with the German
syntactic corpus TIGER. The tool is more typed than
TGrep2 and allows search over discontinuous con-
stituents that are common in German. TIGERSearch
stores the corpus in a Prolog-like logical form and
searches using unification matching. LPath is an
extension of XPath with three features: immedi-
ate precedence, subtree scoping and edge alignment.
The queries are executed in an SQL database (Lai
and Bird, 2004). Other tree query languages include
CorpusSearch, Gsearch, Linguist?s Search Engine,
Netgraph, TIQL, VIQTORYA etc.
Some tools go beyond the tree model and al-
low multiple intersecting hierarchies. Emu (Cas-
sidy and Harrington, 2001) supports sequential lev-
els of annotations over speech datasets. Hierarchi-
cal relations may exist between tokens in different
levels, but precedence is defined only between el-
ements within the same level. The queries cannot
1http://tedlab.mit.edu/?dr/Tgrep2/
2http://www.ims.uni-stuttgart.de/projekte/TIGER/TIGERSearch/
express immediate precedence and are executed us-
ing a linear search. NiteQL is the query language
for the MATE annotation workbench (McKelvie et
al., 2001). It is highly expressive and, similarly to
TIGERSearch, allows quering of intersecting hier-
archies. However, the system uses XML for stor-
age and retrieval, with an in-memory representation,
which may limit its scalability.
Bird and Liberman (2001) introduce an abstract
general annotation approach, based on annotation
graphs.3 The model is best suited for speech data,
where time constraints are limited within an inter-
val, but it is unnecessarily complex for supporting
annotations on written text.
3 The Layered Query Language
Our framework differs from others by simultane-
ously supporting several key features:
? Multiple overlapping layers (which cannot be
expressed in a single XML file), including self-
overlapping (e.g., a word shared by two phrases
from the same layer), and parallel layers, as
when multiple syntactic parses span the same
text.
? Integration of multiple intersecting hierarchies
(e.g., MeSH, UMLS, WordNet).
? Flexible results format.
? Tight integration with SQL, including applica-
tion of SQL operators over the returned results.
? Scalability to large collections such as MED-
LINE (containing millions of documents).4
While existing systems possess some of these fea-
tures, none offers all of them.
We assume that the underlying text is fairly static.
While we support addition, removal and editing of
annotations via a Java API, we do not optimize for
efficient editing, but instead focus on compact rep-
resentation, easy query formulation, easy addition
and removal of layers, and straightforward trans-
lation into SQL. Below we illustrate our Layered
Query Language (LQL) using examples from bio-
science NLP.5
3http://agtk.sourceforge.net/
4http://www.nlm.nih.gov/pubs/factsheets/medline.html
5See http://biotext.berkeley.edu/lql/ for a formal description
of the language and additional examples.
Figure 1 illustrates the layered annotation of a
sentence from biomedical text. Each annotation rep-
resents an interval spanning a sequence of charac-
ters, using absolute beginning and ending positions.
Each layer corresponds to a conceptually different
kind of annotation (e.g., word, gene/protein6, shal-
low parse). Layers can be sequential, overlapping
(e.g., two concepts sharing the same word), and hi-
erarchical (either spanning, when the intervals are
nested as in a parse tree, or ontologically, when the
token itself is derived from a hierarchical ontology).
Word, POS and shallow parse layers are sequen-
tial (the latter can skip or span multiple words). The
gene/protein layer assigns IDs from the LocusLink
database of gene names.7 For a given gene there are
as many LocusLink IDs as the number of organisms
it is found in (e.g., 4 in the case of the gene Bcl-2).
The MeSH layer contains entities from the hier-
archical medical ontology MeSH (Medical Subject
Headings).8 The MeSH annotations on Figure 1 are
overlapping (share the word cell) and hierarchical
both ways: spanning, since blood cell (with MeSH
id D001773) orthographically spans the word cell
(id A11), and ontologically, since blood cell is a kind
of cell and cell death (id D016923) is a kind of Bio-
logical Phenomena.
Given this annotation, we can extract potential
protein-protein interactions from MEDLINE text.
One simple approach is to follow (Blaschke et al,
1999), who developed a list of verbs (and their de-
rived forms) and scanned for sentences containing
the pattern PROTEIN ... INTERACTION-VERB ...
PROTEIN. This can be expressed in LQL as follows:
FROM
[layer=?sentence? { ALLOW GAPS }
[layer=?protein?] AS prot1
[layer=?pos? && tag_type="verb" &&
content=?activates?]
[layer=?protein?] AS prot2
] SELECT prot1.content, prot2.content
This example extracts sentences containing a pro-
tein name in the gene/protein layer, followed by any
sequence of words (because of ALLOW GAPS), fol-
lowed by the interaction verb activates, followed by
any sequence of words, and finally followed by an-
6Genes and their corresponding proteins often share the
same name and the difference between them is often elided.
7http://www.ncbi.nlm.nih.gov/LocusLink
8http://www.nlm.nih.gov/mesh/meshhome.html
Figure 1: Illustration of the annotation layers. The full parse, sentence and section layers are not shown.
other protein name. All possible protein matches
within the same sentence will be returned. The re-
sults are presented as pairs of protein names.
Each query level specifies a layer (e.g., sentence,
part-of-speech, gene/protein) and optional restric-
tions on the attribute values. A binding statement
is allowed after the layer?s closing bracket. We
can search for more than one verb simultaneously,
e.g., by changing the POS layer of the query above
to [layer=?pos? && (content=?activates?
|| content=?inhibit? || content=?binds?)].
Further, a wildcard like content ? ?activate%?
can match the verb forms activate, activates and
activated. We can also use double quotes " to make
the comparison case insensitive. Finally, since LQL
is automatically translated into SQL, SQL code
can be written to surround the LQL query and to
reference its results, thus allowing the use of SQL
operators such as GROUP BY, COUNT, DISTINCT,
ORDER BY, etc., as well as set operations like UNION.
Now consider the task of extracting interactions
between chemicals and diseases. Given the sen-
tence ?Adherence to statin prevents one coronary
heart disease event for every 429 patients.?, we
want to extract the relation that statin (potentially)
prevents coronary heart disease. The latter is in
the MeSH hierarchy (id D003327) with tree codes
C14.280.647.250 and C14.907.553.470.250, while
the former is listed in the MeSH supplementary con-
cepts (ID C047068). In fact, the whole C subtree
in MeSH contains diseases and all supplementary
MeSH concepts represent chemicals. So we can find
potentially useful sentences (to be further processed
by another algorithm) using the following query:
FROM
[layer=?sentence? {NO ORDER, ALLOW GAPS}
[layer=?shallow_parse? && tag_type=?NP?
[layer=?chemicals?] AS chem $
]
[layer=?shallow_parse? && tag_type=?NP?
[layer=?MeSH? && label BELOW "C"] AS dis $
]
] AS sent
SELECT chem.content,dis.content,sent.content
This looks for sentences containing two NPs in any
order without overlaps (NO ORDER) and separated by
any number of intervening elements. We further re-
quire one of the NPs to end (ensured by the $ sym-
bol) with a chemical, and the other (the disease) to
end with a MeSH term from the C subtree.
4 System Architecture
Our basic model is similar to that of TIPSTER (Gr-
ishman, 1996): each annotation is stored as a record,
which specifies the character-level beginning and
ending positions, the layer and the type. The ba-
sic table9 contains the following columns: (1) an-
notation id; (2) doc id; (3) section: title, abstract
or body; (4) layer id: layer identifier (word, POS,
shallow parse, sentence, etc.); (5) start char pos:
beginning character position, relative to section and
doc id; (6) end char pos: ending character posi-
tion; (7) tag type: a layer-specific token identifier.
After evaluating various different extensions
of the structure above, we have arrived at one
with some additional columns, which improves
cross-layer query performance: (8) sentence id;
(9) word id; (10) first word pos; and (11)
last word pos. Columns (9)-(11) treat the word
layer as atomic and require all annotations to coin-
cide with word boundaries.
Finally, we use two types of composite indexes:
forward, which looks for positions in a given docu-
ment, and inverted, which supports searching based
on annotation values.10 An index lookup can be per-
formed on any column combination that corresponds
to an index prefix. An RDBMS? query optimizer
estimates the optimal access paths (index and table
scans), and join orders based on statistics collected
over the stored records. In complex queries a com-
bination of forward (F) and inverted (I) indexes is
typically used. The particular ones we used are:11
(F) +doc id+section+layer id+sentence
+first word pos+last word pos+tag type
(I) +layer id+tag type+doc id+section+sentence
+first word pos+last word pos
(I) +word id+layer id+tag type+doc id+section
+sentence+first word pos
We have experimented with the system on a col-
lection of 1.4 million MEDLINE abstracts, which
include 10 million sentences annotated with 320
million multi-layered annotations. The current data-
base size is around 70 GB. Annotations are indexed
as they are inserted into the database.
9There are some additional tables mapping token IDs to en-
tities (the string in case of a word, the MeSH label(s) in case of
a MeSH term etc.)
10These inverted indexes can be seen as a direct extension of
the widely used inverted file indexes in traditional IR systems.
11There is also an index on annotation id, which allows for
annotating relations between annotations.
Our initial evaluation shows variation in the exe-
cution time, depending on the kind and complexity
of the query. Response time for simple queries is
usually less than a minute, while for more complex
ones it can be much longer. We are in the process of
further investigating and tuning the system.
5 Conclusions and Future Work
We have provided a mechanism to effectively store
and query layers of textual annotations, focusing
on compact representation, easy query formulation,
easy addition and removal of layers, and straight-
forward translation into SQL. Using a collection of
1.4 MEDLINE abstracts, we have evaluated vari-
ous structures for data storage and have arrived at
a promising one.
We have also designed a concise language (LQL)
to express queries that span multiple levels of anno-
tation structure, allowing users to express queries in
a syntax that closely resembles the underlying anno-
tation structure. We plan to release the software to
the research community for use in their own annota-
tion and querying needs.
Acknowledgements This research was supported
by NSF DBI-0317510 and a gift from Genentech.
References
Steven Bird and Mark Liberman. 2001. A formal framework
for linguistic annotation. Speech Communication, 33(1-
2):23?60.
Steven Bird, Yi Chen, Susan Davidson, Haejoong Lee, and
Yifeng Zheng. 2005. Extending XPath to support linguis-
tic queries. In Proceedings of PLANX, pages 35?46.
Christian Blaschke, Miguel Andrade, Christos Ouzounis, and
Alfonso Valencia. 1999. Automatic extraction of biological
information from scientific text: Protein-protein interactions.
In Proceedings of ISMB, pages 60?67.
Steve Cassidy and Jonathan Harrington. 2001. Multi-level an-
notation in the Emu speech database management system.
Speech Communication, 33(1-2):61?77.
Ralph Grishman. 1996. Building an architecture: a CAWG
saga. Advances in Text Processing: Tipster Program Ph. II.
Catherine Lai and Steven Bird. 2004. Querying and updating
treebanks: A critical survey and requirements analysis. In
Proceedings Australasian Language Technology Workshop,
pages 139?146.
David McKelvie, Amy Isard, Andreas Mengel, Morten Moeller,
Michael Grosse, and Marion Klein. 2001. The MATE work-
bench - an annotation tool for XML coded speech corpora.
Speech Communication, 33(1-2):97?112.
Towards Deeper Understanding and Personalisation in CALL 
Galia Angelova, Albena Strupchanska, Ognyan Kalaydjiev, Milena Yankova 
Institute for Parallel Processing, Bulgarian Academy of Sciences, Sofia, Bulgaria 
{galia, albena, ogi, myankova}@lml.bas.bg 
Svetla Boytcheva, Irena Vitanova 
Sofia University ?St. Kliment Ohridski?, Sofia, Bulgaria 
svetla@fmi.uni-sofia.bg, itv@gmx.co.uk 
Preslav Nakov 
University of California at Berkeley, USA, nakov@eecs.berkeley.edu 
 
Abstract 
We consider in depth the semantic analysis in 
learning systems as well as some information 
retrieval techniques applied for measuring the 
document similarity in eLearning. These 
results are obtained in a CALL project, which 
ended by extensive user evaluation. After 
several years spent in the development of 
CALL modules and prototypes, we think that 
much closer cooperation with real teaching 
experts is necessary, to find the proper 
learning niches and suitable wrappings of the 
language technologies, which could give birth 
to useful eLearning solutions. 
1 Introduction 
The tendency to develop natural interfaces for all 
users implies man-machine interaction in a natural 
way, including natural language too, both as 
speech and as free text. Many recent eLearning 
research prototypes try to cope with the 
unrestricted text input as it is considered old-
fashioned and even obsolete to offer interfaces 
based on menu-buttons and mouse-clicking 
communication only. On the other hand, the 
available eLearning platforms such as WebCT [1], 
CISCO [2], and the freeware HotPotatoes [3], are 
far from the application of advanced language 
technologies that might provide interfaces based on 
speech and language processing. They represent 
complex communication environments and/or 
empty shells where the teacher uploads training 
materials, drills, etc. using specialised authoring 
tools. Recently on-line voice communication 
between teachers and students has been made 
available as well, via fast Internet in virtual 
classrooms, but no speech or language processing 
has been considered. So there is a deep, principle 
gap between the advanced research on tutoring 
systems and the typical market eLearning 
environments addressing primarily the 
communication needs of the mass user. 
In what follows we will concentrate on research 
prototypes integrating language technologies in 
eLearning environments. In general, such 
prototypes might be called Intelligent Tutoring 
Systems (ITS) and we will stick to this notion here. 
Most of the systems discussed below address 
Computer-Aided Language Learning (CALL) but 
language technologies are applied for automatic 
analysis of user utterances in other domains too. A 
review of forty Intelligent CALL systems (Gam-
per, 2002) summarises the current trends to embed 
?intelligence? in CALL. What we developed (and 
report here) might be considered intelligent 
because of the integration of reasoning and the 
orientation to adaptivity and personalisation. 
This paper is structured as follows. In section 2 
we consider the task of semantic analysis of the 
learner's input, which is an obligatory element 
when the student is given the opportunity to type in 
freely in response to ITS's questions and/or drills. 
Section 3 deals with Information Retrieval (IR) 
approaches for measuring document similarity, 
which are integrated in ITS as techniques for e.g. 
assessing the content of student essays or choosing 
the most relevant text to be shown to the learner. 
Section 4 discusses how the language technologies 
in question can provide some adaptivity of the ITS, 
as a step towards personalisation. In section 5 we 
summarise the current results regarding the 
evaluation of our prototypes with real users. 
Section 6 contains the conclusion. 
2 Semantic Analysis in ITS 
Although the automatic analysis of user 
utterances is a hot research topic, it achieved only 
partial success so far. The review (Nerbonne, 
2002) shows that Natural Language Processing 
(NLP) is often integrated in CALL, as the domain 
of language learning is the first ?candidate? for the 
application of computational linguistics tools. 
Different language technologies are applied in 
?programs designed to help people learn foreign 
languages?: morphology and lemmatisation, 
syntax, corpus-based language acquisition, speech 
processing, etc. Attempts to implement automatic 
semantic analysis of free text input are relatively 
rare, due to the sophisticated paradigm and the 
default assumption that it will have a very limited 
success (i.e. will be the next failure). 
The famous collection of papers (Holland, 1995) 
presents several systems, which integrate NLP 
modules in different ways. The most advanced one 
regarding semantic analysis is MILT (Dorr, 1995), 
where the correctness as well as the 
appropriateness of the student?s answer are 
checked by matching them against expectations. 
This is performed in the context of a question-
answering session, where expected answers are 
predefined by the foreign language tutor. The 
language-independent internal semantic 
representation is based on lexical conceptual 
structures, which (following Jackendoff) have 
types, with primitives and propositional 
descriptions along different dimensions and fields 
etc. Consider as an example that the teacher has 
specified that ?John ran to the house? is a correct 
answer. This sentence is processed by the system 
and the following lexical conceptual structure is 
obtained: 
[Event GO Loc  
([Thing JOHN], 
 [Path TO Loc ([Position AT Loc ([Thing JOHN], 
[Property HOUSE])])], 
 [Manner RUNNINGLY])] 
which is stored by the tutoring system and later 
matched against the student?s answer. If the 
student types ?John went to the house?, the system 
must determine whether this matches the teacher-
specified answer. The student?s sentence is 
processed and respresented as: 
[Event GO Loc  
([Thing JOHN], 
 [Path TO Loc ([Position AT Loc ([Thing JOHN],   
[Property HOUSE])])])] 
The matcher compares the two lexical 
conceptual structures and produces the output: 
 Missing: MANNER RUNNINGLY 
 INCORRECT ANSWER 
Put another way, the comparison of internal 
representations helps in the diagnostics of semantic 
errors and appropriateness, which are two 
different notions. For instance ?John loves Marry?
is a semantically correct sentence, but it is not an 
appropriate answer when the system expects ?John 
ran to the house?. Further discussions in (Dorr, 
1995) show that the matching scenario is very 
useful in question-answering lessons, which are 
formulated as sets of free response questions 
associated with a picture or text in the target 
language. In an authoring session, the lesson 
designer enters the texts, the questions and a 
sample appropriate answer to each question. At 
lesson time, the questions are presented to the 
student who answers them. If the predefined 
answers are general enough, the system will 
flexibly recognise a set of possible answers. For 
instance, the student might answer: 
Juan died   or Carlos killed Juan   or 
Carlos murdered Juan 
to the question ?What happened to Juan?, which 
checks the comprehension of a simple newspaper 
article. The matching technique can be extended to 
check whether the translations of sentences into the 
target language are correct etc. Even as an earlier 
implementation at the ?concept demonstration 
stage?, this prototype identifies possible solutions 
for the integration of semantic analysis in CALL. 
A recent system Why2-Atlas (VanLehn, 2002), 
based on deep syntactic analysis and compositional 
semantics, aims at the understanding of student 
essays in the domain of physics. Why2-Atlas is 
developed within the project Why2 where several 
different NL processing techniques are compared 
(Rose, 2002). The sentence-level understander 
converts each sentence of the student's essay into a 
set of propositions. For instance, the sentence 
?Should the arrow have been drawn to point 
down?? 
is to be (roughly speaking) converted to 
eevents, vvectors, sdraw(e,s,v) & tense 
(e, past)&mood(e,interrog)&direction(v,down). 
As the authors note in (VanLehn, 2002), this is 
just an approximation of the real output, which 
illustrates the challenge of converting words into 
the appropriate domain-specific predicates. The 
left-corner parser LCFlex copes with 
ungrammatical input by skipping words, inserting 
missing categories and relaxing grammatical 
constraints as necessary in order to parse the 
sentence. For instance, ?Should the arrow have 
been drawn point down?? would parse. In case of 
too many analyses, the parser uses statistical 
information about the word roots frequency and 
the grammatical analyses in order to determine the 
most likely parse. If no complete analysis can be 
produced, a fragmentary analysis will be passed for 
further processing. The fragments present 
?domain-specific predicates that are looking for 
argument fillers, and domain-specific typed 
variables that are looking for arguments to fill?. If 
the symbolic approach for input analysis via 
logical forms fails, a probabilistic one will be used 
as an alternative. 
What is particularly interesting for us here, is the 
discourse-level understander (VanLehn, 2002) 
which, given logical forms, outputs a proof. 
Topologically, this is a forest of interwoven trees, 
where the leaves are facts from the problem 
statement or assumptions made during the proof 
construction. The roots (conclusions) are student?s 
propositions. Consider the example: 
 Question: Suppose you are in a free-falling 
elevator and you hold your keys motionless in 
front of your face and then let go. What will 
happen to them? Explain. 
 Answer: The keys will fall parallel to the 
person face because of the constant acceleration 
caused by gravity but later the keys may go over 
your head because the mass of the keys is less. 
The essay answer will be translated into four 
propositions, which will be passed to the discourse 
understander. The first one (keys fall parallel to the 
person's face) is correct and becomes the root of 
the proof. The second one (gravitation 
acceleration is constant) corresponds to facts from 
the knowledge base. The third proposition (keys go 
over the person's head) is based on the common 
misconception that heavier objects fall faster, 
which is pre-stored in the knowledge base as well, 
it becomes the root of the proof. The last one (the 
mass of the keys is less) corresponds to a node of 
the interior of the proof of the third proposition. 
Once a proof has been constructed, a tutorial 
strategist performs an analysis in order to find 
flaws and to discuss them. Here the major one is 
the misconception ?heavier objects fall faster?. The 
tutoring goals have priorities as follows: fix 
misconceptions, then fix self-contradictions, errors 
and incorrect assumptions, and lastly elicit missing 
mandatory points. The Why2 project in general, 
and Why2-Atlas in particular, illustrate the recent 
trends in the ITS development: 
(i) mixture of symbolic and stochastic appro-
aches in order to cope with the free NL input; 
(ii) application of shallow and partial analysis as 
an alternative to the deep understanding; 
(iii) integration of AI techniques (esp. reasoning 
and personalisation); 
(iv) organisation of bigger projects with 
considerable duration to attack the whole 
spectre of problems together (incl. 
development of authoring tools, systematic 
user evaluation at all stages, several 
development cycles and so on). 
We are experienced in the application of 
semantic analysis to CALL in two scenarios. The 
first one1, in 1999-2002, deals with deep 
understanding of the correct sentences and proving 
the domain correctness and the appropriateness of 
the logical form of each one. The second one 
focuses on the integration of  shallow analysis and 
partial understanding in CALL (Boytcheva, 2004). 
 
1 In Larflast, a Copernicus Joint Research Project. 
The system described in (Angelova, 2002) is a 
learning environment for teaching English 
financial terminology to adults, foreigners, with 
intermediate level of English proficiency. The 
prototype is a Web-based learning environment 
where the student accomplishes three basic tasks: 
(i) reading teaching materials, (ii) performing test 
exercises and (iii) discussing his/her own learner 
model with the system. The project is oriented to 
learners who need English language competence as 
well as expertise in correct usage of English 
financial terms. This ambitiously formulated 
paradigm required the integration of some formal 
techniques for NL understanding, allowing for 
analysis of the user?s answers to drills where the 
student is given the opportunity to enter free 
natural language text (normally short discourse of 
2-3 sentences). The morphological, syntax and 
semantic analysis is performed by the system 
Parasite (Ramsay, 2000), developed in UMIST. 
After the logical form has been produced for each 
correct sentence, the CALL environment has to 
determine whether the student?s utterance matches 
the expected appropriate answer in the current 
learning situation. A special prover has been 
developed, which checks whether the logical form 
of the answer is ?between? the minimum and 
maximum predefined answers (Angelova, 2002). 
Unlike MILT (Dorr, 1995), we think that the 
correct answer has to be subsumed by the 
maximum expected one, i.e. there is not only a 
lower but also an upper limit on the correctness. 
Table 1 lists examples for all diagnostic cases from 
user?s perspective, by sentences in natural 
language. Please note that nowadays the deductive 
approach can be relatively efficient, as our prover 
(in Sicstus Prolog) works on-line, integrated in a 
Web-based environment, in real time with several 
hundred meaning postulates. Proofs are certainly 
based on a predefined ontology of the domain 
terms, which in this case is a lexical one since the 
terms are treated as words with special lexical 
meaning encoded in the meaning postulates thus 
forming a hidden hierarchy of meanings. The 
conceptual and lexical hierarchy of meanings are 
further discussed in (Angelova, 2004).  
However, we discovered that deep semantic 
analysis is difficult to integrate in CALL. First, this 
requires enormous amount of efforts for the 
meaning postulates acquisition. While hierarchy of 
terms is reusable, as it is in fact the domain model, 
the propositions, which encode the lexical 
semantics are somewhat application and domain 
specific and therefore difficult to reuse or to 
transfer  to another domain (moreover they are 
bound to the domain words). Implementing the 
prover and testing the definitions and the inference 
Table 1: Decisions about erroneous answers according to the configuration of the logical forms of the 
predefined minimal, maximal and the current learner?s answer (see also Angelova, 2002). 
procedures with several hundred predicates 
required approximately one man-year for an AI 
expert who worked closely with domain experts. 
Second, the result is not perfect from the 
perspective of the user who has to answer with 
correct and full sentences (see section 5 for 
details). Thus our recent work (Boytcheva, 2004) 
is directed towards integration of shallow and 
deep semantic techniques in CALL systems. We 
use shallow parsing, which allows for the 
processing of both syntactically incorrect and 
incomplete answers. However, during the user?s 
Case Sample of learner?s utterance Discussion 
Kernel (predefined 
minimum answer) 
Primary market is a financial market that 
operates with newly issued debt instruments 
and securities. 
The logical form is pre-stored in the 
system as a Kernel. 
Cover (predefined 
maximum answer) 
Primary market is a financial market that 
operates with newly issued debt instruments 
and securities and provides new investments 
and its goal is to raise capital. 
The logical form is pre-stored in the 
system as a Cover. 
1.Correct answer Primary market is a financial market that 
operates with newly issued debt instruments 
and securities and provides new investments. 
This logical form is between the 
Kernel and the Cover. 
2a) Incomplete 
answer 
Primary market is a financial market that 
operates with newly issued securities. 
Missing Kernel term: debt 
instruments. 
2b) Specialisation 
of concepts from 
the definition 
Primary market is a financial market that 
operates with newly issued bonds.
Bond is a specialisation of security; 
Missing: debt instruments. 
2c) Paraphrase 
using the concept 
definition 
Primary market is a financial market that 
operates with new emissions of stocks, bonds 
and other financial assets.
New emissions = newly issued; 
stocks, bonds and other financial 
assets = debt instruments and 
securities. 
3a) Partially correct Primary market is a financial market that 
operates with newly issued debt instruments 
and securities for instant delivery.
Wrong: for instant delivery.
3b) Generalisation 
of concepts from 
the definition 
Primary market is a market that operates 
with newly issued financial instruments.
Market is a generalisation of 
financial market; Financial 
instruments are generalisation of debt 
instruments and securities. 
4. Partially correct Primary market is a financial market that 
operates with newly issued securities for 
instant delivery and provides new 
investments. 
Wrong: for instant delivery;
Missing: debt instruments. 
5. Wrong answer Primary market is an organisation in which 
the total worth is divided into commercial 
papers.
Wrong: an organisation in which the 
total worth is divided into 
commercial papers;
Missing: financial market that 
operates with newly issued debt 
instruments and securities. 
6. Wrong answer Primary market provides new investments for 
instant delivery.
Wrong: for instant delivery;
Missing: financial market that 
operates with newly issued debt 
instruments and securities; 
7. Partially correct Primary market is a financial market that 
operates with newly issued securities and 
provides new investments. 
Missing: debt instruments. 
8. Wrong answer Primary market provides new investments. Missing: financial market that 
operates with newly issued debt 
instruments and securities. 
utterances evaluation we use deep semantic 
analysis concerning the concepts and the relations 
that are important for the domain only. Users? 
answers are represented as logical forms, 
convenient for the inference mechanism, which  
takes into account the type hierarchy and is 
elaborated in domain-specific points only. Thus 
the combination of shallow and deep techniques 
gives the users more freedom in answering, i.e. 
various utterances to express themselves without 
impeding the evaluation process. The idea to 
apply the shallow NLP techniques in CALL was 
inspired by their successful application in IE for 
template filling. The assessment of user 
knowledge in a specific domain can be viewed as 
a kind of  template filling, where the templates 
correspond to concepts and relations relevant to 
the tested domain. 
3 Exploiting Document Proximity in ITS 
There is a huge demand for intelligent systems 
that can handle free texts produced by the learners 
in eLearning mode. As most of the courses being 
taught are represented as texts, the challenge is to 
compare one text to another. Since the phrasing 
will not be the same in both texts, the comparison 
needs to be performed at the semantic level. One 
solution is sketched above: translate the student?s 
text to a set of logical forms and then apply 
symbolic approaches for their assessment. 
Unfortunately, there are only few research 
prototypes that address the problem from this 
perspective, which are very expensive and have 
delivered only partially applicable results so far. 
Another option is to try to exploit  the IR 
techniques we have at hand in order to check for 
instance whether the student?s answer contains the 
?right words? (in which case it would be a good 
writing, since it would be similar to the 
expectation). A natural choice for assessing the 
usage of the ?right words? is the so-called Latent 
Semantic Analysis (LSA) as it reveals the latent 
links between the words and phrases, especially 
when it is trained with enough samples. Below we 
briefly overview the application of LSA in 
eLearning and our experiments in this direction. 
The classical LSA method, as proposed in 
(Deerwester, 1990) is a bag-of-words technique, 
which represents the text semantics by assigning 
vectors to words and texts (or text fragments). 
Indeed, knowing how words are combined to 
encode the document knowledge is a kind of 
semantic representation of the word meaning and 
text semantics. The underlying idea is that words 
are semantically similar, if they appear in similar 
texts, and texts are semantically similar, if they 
contain similar words. This mutual word-text 
dependency is investigated by building a word-
text matrix, where each cell contains the number 
of occurrences of word X in document Y, after 
which the original matrix is submitted to Singular 
Value Decomposition ? a transformation that is 
meant to reveal the hidden (latent) similarity 
between words and texts. This produces a vector 
of low dimensionality (the claim is that 300 is 
near optimal) for each word and for each text. The 
similarity between two words, two texts, or a 
word and a text, is given by the cosine of the 
angle between their corresponding vectors (the 
cosine is the most popular similarity measure). 
Therefore, the similarity between two words or 
two sets of words is a number between ?1 (lowest 
similarity) and 1 (highest similarity). Without 
morphology and grammar rules, syntactical 
analysis, and manually encoded implicit 
knowledge, LSA is considered successful in 
various experiments including assessment of 
student essays. 
For the purposes of assessment, usually a high-
dimensional space is computed from texts 
describing the domain (most often the available 
electronic version of the course). Each word from 
the domain as well as the student?s essay are 
juxtaposed a vector, usually a 300-dimensional 
one. The student gets as feedback an assessment 
score and/or an indication of the topics/aspects 
that are not covered well by the essay. The 
Intelligent Essay Assessor (IEA) (Foltz 1999a, 
Foltz 1999b) is based on reference texts (manually 
pre-graded essays) and assigns a holistic score 
and a gold standard score. The former is 
computed by seeking the closest pre-graded essay 
and returning its grade (i.e. the current one is 
scored as the closest pre-graded one), while the 
latter is based on a standard essay written by an 
expert. It returns the proximity between the 
student?s essay and the expert?s one. An 
experiment with 188 student essays showed a 
correlation of 0.80 between the IEA scores and 
teacher?s ones, which is a very high similarity. 
However, IEA outputs no comments or advice 
regarding the student essay. The Apex system 
(Lemaire, 2001) performs a semantic comparison 
between the essay and the parts of the course 
previously marked as relevant by the teacher. The 
whole student essay is to be compared to each of 
these text fragments. For instance, if the student 
has to write an answer to the question ?What were 
the consequences of the financial crash of 1929??, 
the essay is compared to the following sections of 
the teaching course: The political consequences in 
Europe, Unemployment and poverty, The 
economical effects, The consequences until 1940.
An experiment with 31 student essays in the 
domain of Sociology of Education exhibited  a 
correlation of 0.51 between Apex grades and 
teacher?s ones, which is close to the correlation 
agreement between two human graders in this 
literary domain. Select-a-Kibitzer (Wiemer-
Hastings, 2000) aims at the assessment of essay 
composition. Students are required to write on 
topics like: ?If you could change something about 
school, what would you change??. The assessment 
module is based on reference sentences of what 
students usually discuss about school (food, 
teachers, school hours, etc.). Several kinds of 
feedback are delivered to the student, concerning 
the text coherence, the kind of sentences or the 
topic of the composition. For example, the advice 
regarding coherence can be: ?I couldn?t quite 
understand the connection between the first 
sentence and the second one. Could you make it a 
bit clearer? Or maybe make a new paragraph.?
(Here the underlying idea is that totally new 
words in the subsequent sentence normally 
concern another topic, i.e. this fits to a new 
paragraph). A principled criticism of these three 
recent systems is that the bag-of-words model 
does not take into consideration the grammar 
correctness and the discourse structure, i.e. two 
essays with the same sentences structured in a 
different order would be scored identically (which 
is a funny idea from an NLP perspective). A 
further example illustrates attempts to combine 
the strengths of the bag-of-words and the 
symbolic approaches, while trying to avoid some 
of their weaknesses. CarmelTC (Rose, 2002), a 
recent system which analyses essay answers to 
qualitative physics questions, learns to classify 
units of text based on features extracted from a 
syntactic analysis of that text. The system was 
developed inside the Why2-Atlas conceptual 
physics tutoring environment for the purpose of 
grading short essays written in response to 
questions such as ?Suppose you are running in a 
straight line at constant speed. You throw a 
pumpkin straight up. Where will it land? 
Explain?. CarmelTC?s goal is not to assign a letter 
grade to student essays, but to tally which set of 
?correct answer? aspects are present in student 
essays (e.g. a satisfactory answer to the example 
question above should include a detailed 
explanation of how the Newton's 1st law applies to 
this scenario. Then the student should infer that 
the pumpkin and the man will continue at the 
same constant horizontal velocity that they both 
had before the release. Thus, they will always 
have the same displacement from the point of 
release. Therefore, after the pumpkin rises and 
falls, it will land back in the man's hands. The 
?presence? of certain sentences is checked by 
word classification). The evaluation shows that 
the hybrid CarmelTC approach achieves 90% 
precision, 80% recall and 85% F-measure, and 
thus outperforms the pure bag-of-words run of 
LSA, which scores 93% precision, 54% recall  
and 70% F-measure  (Rose, 2002). 
Our experiments with LSA (Angelova, 2002) 
were focused on finding financial texts, which are 
appropriate to be shown as teaching materials in a 
particular learning situation. Given a set of key-
words, agents retrieve texts from well-known 
financial sites and store them to the servers of our 
environment for further assignment of 
appropriateness. We implemented the classical 
LSA scenario and applied it as a filtering 
procedure, which assigns off-line a similarity 
score to each new text. The text archive consisted 
of 800 most relevant readings, which represent 
HTML-pages with textual information (elements 
signaling prevailing technical content, e.g. tables, 
have been excluded). These texts are offered as 
suggested readings but are also used for building 
dynamic concordances, which show samples of 
terms usages to the learner. The latter may be 
displayed in cases of language errors to drills 
where the student makes linguistic mistakes. 
Choosing this option (view samples) is up to the 
student. The dynamic nature of the text collection 
ensures the appearance of new samples, which 
makes the browsing interesting at every run. 
4 Personalisation 
Our learning environment supports 
personalisation as follows: 
 as a step towards instructional as well as content 
planning: a planner (the so-called pedagogical 
agent) plans the next learner?s moves across the 
hypertext pages which, technically, constitute 
the Web-site of our tutoring system; these moves 
are between (i) performing drills and (ii) choices 
for suggestion of readings, which may be either 
texts from Internet or especially generated Web-
pages. The pedagogical agent deals with both 
presentational and educational issues. The local 
planning strategy aims at creating a complete 
view of the learner?s knowledge of the current 
concept. It supports movements between drills 
with increasing complexity, when the student 
answers correctly. The global planning strategy 
determines movements between drills testing 
different concepts, going from the simple and 
general concepts to the more specific and 
complex notions. 
 as a step towards personalised IR: an LSA-filter 
assigns proximity score to constantly updated 
texts, which are stored as suggested readings. 
This allows for constant update of the system?s 
text archive and, following the practice at the 
main financial sites, provides up-to-date news 
and readings, which may be used as texts for 
different teaching purposes. As key words for 
initial collection of texts, the not_known and 
wrongly_known terms from the learner?s 
models are chosen, so the CALL system always 
stores the proper relevant text for each student. 
The adaptivity is provided using an ontology of 
financial terms as a backbone of all system?s 
resources. No matter whether these are conceptual 
(e.g. knowledge base), linguistic (e.g. lexicons, 
meaning postulates, etc) or pedagogical resources 
(e.g. set of preliminary given drills or learner 
model, which is dynamically constructed at run-
time), the ontology always represents the unifying 
skeleton as all chunks of knowledge are organised 
around the terms?labels. In addition to the is-a 
partition, we support in the knowledge base 
explicit declarations of the perspectives or 
viewpoints. E.g., the isa_kind/4 clause: 
isa_kind(security, [bond, hybrid_security, stock], 
[exhaustive, disjoint],  
?status of security holder: creditor or owner?) 
means that the securities are disjoint and 
exhaustively classified into bonds, stocks and 
hybrid securities depending on the status of their 
owner. These comments provide nice visualisation 
(Angelova, 2004). 
5 User Study and User Evaluation  
Larflast started with a user study of how 
foreigners ? adults acquire domain terminology in 
their second language. In fact the acquisition is 
closely related to the elicitation of domain 
knowledge, especially in a relatively new domain 
(students have to learn simultaneously a subject 
with its terminology and its specific language 
utterances). Mistakes are linguistically-motivated 
but wrong domain conceptualisations contribute 
to the erroneous answers as well. Erroneous 
answers appear in terminology learning due to the 
following reasons: 
 Language errors (spelling, morphology, 
syntax); 
 Question misunderstanding, which causes 
wrong answer; 
 Correct question understanding, but
absent knowledge of the correct term,
which implies usage of paraphrases and 
generalisation instead of the expected 
answer; 
 Correct question understanding, but
absent domain knowledge, which implies 
specialisation, partially correct answers,
incomplete answers and wrong answers. 
This classification influenced considerably the 
design of the prover?s algorithms, i.e. the decision 
how to check of the appropriateness of the student 
answer. The diagnostics shown in Table 1 follows 
closely the four reasons above. 
Our learning prototype was tested by (i) two 
groups of university students in finance with 
intermediate knowledge of English, (ii) their 
university lecturers in English, and (iii) a group of 
students of English philology. The system was 
evaluated as a CALL-tool for self-tuition and 
other autonomous classroom activities, i.e. as an 
integral part of a course in ?English for Special 
Purposes?. The learners could test their 
knowledge through the specially designed 
exercises, compare their answers to the correct 
ones using the generated feedback (immediate, 
concrete and time-saving, it comes in summary 
form, which is crucial in order to accomplish the 
system?s use autonomously) and extract additional 
information from the suggested readings and 
concordancers. 
The users liked the feedback after performing 
drills, immediately after they prompted erroneous 
answers to exercises where this term appears. 
They evaluated positively the visualisation of the 
hierarchy as well as the surrounding context of 
texts and terms usages organised in a 
concordancer, which is dynamically built and 
centred on the terms discussed at the particular 
learning situation. The teachers were very pleased 
to have concordancers with contiguously updated 
term usages; they would gladly see such a 
language resource integrated in a further authoring 
tool, because searching suitable texts in Internet is 
a difficult and time-consuming task.  
Unfortunately the learners were not very 
enthusiastic regarding the free NL input, as it 
permits relatively restricted simple answers and 
does not go beyond the human capacity of the 
teacher. The main disappointment of both learners 
and teachers is the system?s inability to answer 
why, i.e. while the formal semantics and reasoning 
tools provide extremely comprehensive diagnostic 
about the error type, they tell nothing about the 
reason. Fortunately, all users liked the fact that 
there were numerous examples of terms usages 
from real texts whenever morphological or syntax 
errors were encountered in the free NL input. 
Thus we conclude with a certain pessimism 
concerning the appropriateness of today?s formal 
semantic approaches in ITS and much optimism 
that data-driven corpus techniques, if properly 
applied, fit quite well to the adaptive ITS. What is 
still desirable regarding the filtering module is to 
restrict the genre of the suggested readings, since 
the current texts are freely collected from the 
Internet and some of them should be used as 
teaching materials (LSA cannot recognise the text 
educational appropriateness since it considers the 
terms? occurrences only; other supervised 
techniques such as text categorisation might 
improve the filtering, if properly integrated). 
As a possible improvement of the current 
paradigm for formal analysis, we turned recently 
to partial analysis, which gives more flexibility to 
the students to enter phrases instead of full 
sentences (Boytcheva, 2004). 
6 Conclusion 
The conclusion is that teachers as well as 
learners like CALL systems that are easy to 
integrate in the typical educational tasks, i.e. the 
area of language learning has well-established 
traditions and the experimental software is well-
accepted, only if it is really useful and facilitates 
the learning process. Our feeling is that all 
attempts to integrate language technologies in 
CALL should be closely related to testing the 
laboratory software with real students. At the 
same time, cooperation with teachers is an 
obligatory condition as the necessary pedagogical 
background is often missing in the research 
environments where normally the NLP 
applications and language resources appear. 
Language technologies have a long way to go, 
until they find the proper wrappings for 
integration of advanced applications and the 
necessary resources into useful CALL systems. 
References  
[1] WebCT, http://www.webct.com/ 
[2] CISCO, http://cisco.netacad.net/ 
[3] HotPotatoes, http://web.uvic.ca/hrd/hotpot/ 
Angelova G., Boytcheva, Sv., Kalaydjiev, O. 
Trausan-Matu, St., Nakov, P. and A. 
Strupchanska. 2002. Adaptivity in Web-based 
CALL In Proc. of ECAI?02, the 15th European 
Conference on AI, IOS Press, pp. 445-449. 
Angelova G., Strupchanska, A., Kalaydjiev, O., 
Boytcheva, Sv. and I. Vitanova. 2004. 
Terminological Grid and Free Text Repositories 
in Computer-Aided Teaching of Foreign 
Language Terminology. Proc. "Language 
Resources: Integration & Development in e-
learning & in Teaching Computational 
Linguistics", Workshop at LREC 2004, 35-40. 
Boytcheva Sv., Vitanova, I., Strupchanska, A., 
Yankova, M. and G. Angelova. 2004. Towards 
the assessment of free learner's utterances in 
CALL. Proc. "NLP and Speech Technologies in 
Advanced Language Learning Systems", 
InSTIL/ICALL Symposium, Venice,17-19 June. 
Deerwester S., Dumais S.T., Furnas G.W., 
Landauer T.K., and Harshman R. 1990. 
Indexing by latent semantic analysis, Journal of 
the American Society for Information Science, 
41(6), pp. 391?407. 
Dorr, B., Hendler, J., Blanksteen, S. and B. 
Migdaloff. 1995. On Beyond Syntax: Use of 
Lexical Conceptual Structure for Intelligent 
Tutoring. In (Holland, 1995), pp. 289-311. 
Foltz P.W., Laham D., and Landauer T.K. 1999. 
Automated essay scoring: Applications to 
educational technology, In Proceedings of the 
ED-MEDIA Conference, Seattle. 
Foltz P.W., Laham D., and Landauer T.K. 1999. 
The intelligent essay assessor: Applications to 
educational technology, Interactive Multimedia 
Electronic Journal of Computer-Enhanced 
Learning, 1(2).  
Gamper, J. and J. Knapp. 2002. Review of 
intelligent CALL systems. Computer Assisted 
Language Learning 15/4, pp. 329-342. 
Holland, M., Kaplan, J. and R. Sams (eds.) 1995. 
Intelligent Language Tutors: Theory Shaping 
Technology. Lawrence Erlbaum Associates, Inc. 
Lemaire B. and Dessus P. 2001. A system to 
assess the semantic content of student essays, J. 
of Educ. Computing Research, 24(3), 305?320. 
Nerbonne, J. 2002. Computer-Assisted Language 
Learning and Natural Language Processing. In: 
R. Mitkov (Ed.) Handbook of Computational 
Linguistics, Oxford Univ. Press, pp. 670-698.  
Ramsay, A. and H. Seville. 2000. What did he 
mean by that? Proc. Int. Conf. AIMSA-2000, 
Springer, LNAI 1904, pp. 199-209.  
Rose, C.P., Bhembe, D., Roque, A., Siler, S., 
Srivastava, R. and K. van Lehn. 2002. A hybrid 
language understanding approach for robust 
selection of tutoring goals. . In Proc. of the Int. 
Conf. Intelligent Tutoring Systems, Springer, 
LNCS, 2363: 552-561 
VanLehn, K., Jordan, P., Rose, C., Bhembe, D., 
Boettner, M., Gaydos, A., Makatchev, M., 
Pappuswamy, U., Rindenberg, M., Roque, A., 
Siler, A. and Srivastava, R. 2002. The 
Architecture of Why2-Atlas: A Coach for 
Qualitative Physics Essay Writing. In Proc. of 
the Int. Conf. Intelligent Tutoring Systems, 
Springer, Lecture Notes in CS, 2363: 158-162. 
Wiemer-Hastings P. and Graesser A. 2000. Select-
a-kibitzer: A computer tool that gives 
meaningful feedback on student compositions,
Interactive Learning Environments, 8(2), pp. 
149?169. 
Robust Ending Guessing Rules with Application to Slavonic Languages 
Preslav NAKOV 
EECS, CS Division,  
University of California, Berkeley  
Berkeley, CA, 94720  
USA 
nakov@cs.berkeley.edu 
Elena PASKALEVA 
Linguistic Modelling Department, IPP 
Bulgarian Academy of Sciences 
25A, Acad. G. Bontchev St 
Sofia, Bulgaria, 1113 
hellen@lml.bas.bg 
 
Abstract 
The paper studies the automatic extraction of 
diagnostic word endings for Slavonic langua-
ges aimed to determine some grammatical, 
morphological and semantic properties of the 
underlying word. In particular, ending gues-
sing rules are being learned from a large mor-
phological dictionary of Bulgarian in order to 
predict POS, gender, number, article and se-
mantics. A simple exact high accuracy algo-
rithm is developed and compared to an appro-
ximate one, which uses a scoring function pre-
viously proposed by Mikheev for POS gues-
sing. It is shown how the number of rules of 
the latter can be reduced by a factor of up to 
35, without sacrificing performance. The eva-
luation demonstrates coverage close to 100%, 
and precision of 97-99% for the approximate 
algorithm. 
1 Introduction 
An important property of the Slavonic languages 
is the rich morphology, which determines the spe-
cifics of their representation and processing in 
NLP applications. This variety is arranged not only 
linearly along the paradigmatic axe, i.e. abundance 
of wordforms for a given lemma (up to 52 forms 
for the Bulgarian verb), but also in the derivational 
tree (up to 30 members per word formation). The 
grammatical system of the Slavonic languages and 
their descriptions differentiate these two mecha-
nisms as word formation and word derivation.
The word formation building blocks define the 
so called inflectional classes, which represent se-
quential letter strings associated with word classes 
as well as with individual words, also known as i-
suffixes in Porter-like stemmers (Porter,1980). The 
derivational building blocks represent derivational 
suffixes listed in grammars (d-suffixes in Porter-
like stemmers). A considerable part of the Slavonic 
d-suffixes change not only the part of speech 
(POS) but also the semantics of the newly formed 
word. When multiple d-suffixes are concatenated, 
the word formation chain yields also a semantic 
derivation. For example, the chain (observe ; ob-
server ; observing ; observability): 
!"#$%&-(=>=?) ;
@=ABCD-"'($ ;
@=ABCD=EFB-(! ;
@=ABCD=EFB@-)*' 
represents the derivation: 
verb ; noun ; adjective ; noun 
but also the following semantic transformation: 
action ; actor ; feature ; abstract feature 
The combination of grammatical and semantic 
functions of the Slavonic d-suffixes, together with 
their frequent usage (at least for some of them) and 
the high productivity, make very attractive the idea 
to study the regularities and the predictive power 
of ending letter combinations in a large text set. 
We believe the results obtained over a representati-
ve collection can be used in a variety of robust 
analysis applications. Linguistically, we interpret 
the last term as operations over a large text set with 
insufficient linguistic support, typically given by a 
lexical database, grammatical rules, parsing rules 
etc. We target applications like POS tagging, text 
categorisation, information extraction, word sense 
disambiguation, question answering etc. 
Below we concentrate on the automatic extracti-
on of a set of diagnostic word endings for Bulgari-
an that can determine the POS as well as some 
grammatical, morphological and semantic properti-
es of the underlying word. This is a two-step pro-
cess including endings identification & learning 
and application & evaluation.  
The paper is organised as follows. Section 2 dis-
cusses the related work on POS guessing and gene-
ral morphology. Section 3 introduces our basic re-
source: the Large Grammatical Dictionary of Bul-
garian. Section 4 describes two algorithms for en-
ding guessing rules induction (an exact and an ap-
proximate one) and how to reduce the number of 
rules by a factor of up to 35. Section 5 contains the 
experimental setup and evaluation trying to predict 
POS, gender, number, article and semantics. Sec-
tion 6 discusses the results and Section 7 points to 
direction for future work. 
2 Related Work 
POS guessing. Kupiec (1992) uses pre-specified 
suffixes and performs statistical learning for POS 
guessing. The XEROX tagger comes with a list of 
built-in ending guessing rules (Cutting et al,1992). 
In addition to the ending, Weischedel et al (1993) 
exploit capitalisation. Thede and Harper (1997) 
consider contextual information, word endings, en-
tropy and open-class smoothing. A similar appro-
ach is presented in (Schmid,1995). Ruch et al 
(2000) combine POS guessing, contextual rules 
and Markov models to build a POS tagger for bio-
medical text. A very influential is the work of Brill 
(1997), who induces more linguistically motivated 
rules exploiting both a tagged corpus and a lexi-
con. He does not look at the affixes only, but also 
checks their POS class in a lexicon. Mikheev 
(1997) proposes a similar approach, but learns the 
rules from raw as opposed to tagged text. Daciuk 
(1999) speeds up the process by means of finite 
state transducers. 
General morphology. Nakov et al (2003) use 
ending guessing rules to predict the morphological 
class of unknown German nouns. Schone and 
Jurafsky (2000) apply latent semantic analysis for 
a knowledge-free morphology induction. DeJean 
(1998), Hafer and Weiss (1974) follow a successor 
variety approach: the word is cut, if the number of 
distinct letters after a pre-specified sequence sur-
passes a threshold. Goldsmith (2001) performs a 
minimum description length analysis of the mor-
phology of several European languages using cor-
pora. Gaussier (1999) induces derivational mor-
phology from a lexicon by means of p-similarity 
based splitting. Jacquemin (1997) focuses on the 
morphological processes. Van den Bosch and Da-
elemans (1999) propose a memory-based appro-
ach, which maps directly from letters in context to 
categories that encode morphological boundaries, 
syntactic class labels and spelling changes. Yarow-
sky and Wicentowski (2000) present a corpus-ba-
sed approach for morphological analysis of both 
regular and irregular forms based on four models 
including: relative corpus frequency, context simi-
larity, weighted string similarity and incremental 
retraining of inflectional transduction probabilities. 
Another interesting work, exploiting capitalisation 
and fixed/variable suffixes, is presented in Cucer-
zan and Yarowsky (2000).  
3 Source Data 
As the related work above shows, a large lexical 
database is often needed for the automatic identifi-
cation of good diagnostic word endings. In particu-
lar, in our experiments we used the Large Gram-
matical Dictionary of Bulgarian (Paskaleva,2003), 
created at the Linguistic Modelling Department of 
the Bulgarian Academy of Sciences (CLPP-BAS) 
and comprising approximately 995,000 wordforms 
(about 65,000 lemmas), encoded in DELAF format 
(Silberztein,1993). The following information is 
listed for each wordform: 1) lemma; 2) lemma pro-
perties (POS, additional grammatical features rela-
ted to the word formation: gender, e.g. for the no-
uns; degree, e.g. for the adjectives; transitivity, for 
verbs; kind for pronouns/numerals, etc.); and 3) 
properties of the wordform as a member of the 
lemma paradigm. The first group of properties re-
present our primary learning resource, as we focus 
on the extraction of ending rules for whole word 
classes and not for individual words. 
4 Ending Guessing Rules Extraction 
Our learning algorithms produce lists of endings 
of various length (up to 8 letters), predicting diffe-
rent kinds of linguistic information (see below for 
details): 
 POS: adjective/adverb/noun/numeral/verb  
 article: definite/indefinite/none 
 gender: feminine/masculine/neutre/none 
 number: singular/plural/none 
 semantics: human/animate/none 
We use two different algorithms, inducing exact 
and approximate ending rules, accordingly. 
4.1 Exact Rules 
A study of the ending letter sequences of the dic-
tionary entries and their properties shows the well 
known inverse correlation between the length of a 
word ending and its ambiguity: the shorter the 
string, the more likely to be ambiguous.  
This raises the idea of a simple algorithm pro-
ducing 100% correct rules1. Suppose we want to 
predict POS and let us consider all wordforms in 
the dictionary that end on ?-=?. There are 203,420 
of them, distributed as follows2: V=128,162(63.00%),
A=42,262(20.78%), N=32,597(16.02%), NU=240(0.12%), 
ADV=99(0.05%), PRO=38(0.02%), INTJ=7(0.00%), 
CONJ=7(0.00%), PC=6(0.00%), PREP=2(0.00%). Let us 
now consider a sequence with an additional star-
ting letter, e.g. ?-E=?. There are 83,375 wordforms 
with this ending, distributed in POS as follows: 
V=42,843(51.39%), A=22,225(26.66%), N=18,092(21.70%), 
NU=157(0.19%), ADV=48(0.06%), PRO=9(0.01%), 
CONJ=1(0.00%). When a further letter is included, 
 
1 As it is 100% precise it risks over fitting and thus a 
low coverage. We will return to this issue later. 
2 We use the following abbreviations for the ten POS: 
A (adjective), ADV (adverb), CONJ (conjunction), 
INTJ (interjunction), N (noun), NU (numeral), PC (par-
ticle), PREP (preposition), PRO (pronoun) and V (verb). 
we obtain e.g. ?-=E=? with a total frequency of 
72,235 and a POS distribution: V=42249(58.49%),
A=21415(29.65%), N=8399(11.63%), NU=119(0.16%), 
ADV=47(0.07%), PRO=6(0.01%). Next, for ?-W=E=? we 
have a frequency of only 799 and an even lower 
ambiguity: N=793(99.25%), A=6(0.75%). Finally, there 
is a single POS tag for ?-XW=E=?: N=726(100.00%).
Note how the most likely tag (shown in italic for 
each ending above) and the degree of certainty 
about it change. At the beginning, the most likely 
tag was V, but later it changed to N. In addition, 
the uncertainty does not necessarily decrease mo-
notonically as the most likely tag changes from 
V(63.00%) to V(51.39%) to V(58.49%) to 
N(99.25%) and to N(100.00%). Generalizing this 
example, we obtain the following  
 
Exact Algorithm: 
1. S = 
E = {all possible endings of dictionary word-
forms, up to k letters long}; 
2. While E [
2.1. Take a random ending e from E of mini-
mum length.
2.2. If all wordforms in the dictionary that 
end on e have the same POS then S  e.
3. Output S.
Wordforms Number of 
Different POS count % 
1 936,409 97.37%
2 24,913 2.59%
3 356 0.03%
4 1 0.00%
Table 1: Dictionary ambiguity with respect to POS. 
While it is clear that this approach produces only 
100% correct rules (and also the shortest possible 
ones), its coverage is not guaranteed to be 100% 
due to homography, i.e. the same graphemic word-
form can be met in the dictionary multiple times 
with different annotations. For example, ?\EA]=@=?
is annotated as3:
\EA]=@=,\EAF]=.V+F+T:Psf 
\EA]=@=,\EA]=@.ADJ:sf 
\EA]=@=,\EA]=@=.N+F:s 
The first one denotes the inflected wordform se-
lected of the finite transitive verb select, the second 
 
3 The format used is as follows ?inflected_form,
lemma . lemma_properties : wordform_properties?
one stands for the feminine adjective selected, and 
the last one, for the feminine noun defence.
In fact, the level of ambiguity is relatively low: 
97.37% of the wordforms in the dictionary are un-
ambiguous, so ignoring the ambiguity on training 
is not unreasonable. See Table 1 for a detailed dic-
tionary ambiguity distribution with respect to POS. 
4.2 Approximate Rules 
Our approximate rules are similar to the ones 
proposed by Mikheev (1997), who uses a dictiona-
ry to build POS prediction rules with four parts: 
deletion (?), addition (+), checking against the dic-
tionary (?) and POS assignment (;). Generally 
speaking, each rule operates either on the begin-
ning or the ending of the target wordform. For 
example, the following rule says that if an unkno-
wn word ends on ?-ied?, this ending should be 
stripped, ?-y? should be appended, a check should 
be performed of whether the newly created word is 
in the dictionary and annotated as (VB VBP) there, 
and if so, (JJ VBD VBN) for the original word 
should be predicted: 
e[?ied +y ?(VP VBP) ; (JJ VBD VBN)] 
All rule elements are optional, except for the 
POS assignment. This means that a rule can just 
add and/or remove letters, without looking in the 
dictionary (although it could potentially benefit 
from doing so). When both removal and addition 
are used, one can account for mutations in the 
word stem. In fact, Mikheev uses the following 
restricted types of rules: Prefix (prefix deletion and 
dictionary lookup), Suffix0 (suffix deletion and dic-
tionary lookup), Suffix1 (suffix deletion with mu-
tation in the last letter and dictionary lookup), En-
ding (suffix deletion). There are separate ending 
guessing rules for hyphenated, capitalised and all 
other words. 
Given a dictionary, a scan through the word-
forms is performed, during which all possible rules 
are collected and scored, and those above some 
threshold are selected. Finally, rule merging is ap-
plied to rules with identical preconditions but dif-
ferent predictions: the new rule predicts the union 
of the predictions of the original rules, which re-
sults in higher ambiguity but possibly allows the 
new rule to pass above the threshold after being 
rescored. 
We do not use the full power of the Mikheev-
like rules and we limit ourselves to ending rules 
without dictionary lookup and single class predicti-
ons. Further, at present we do not treat the hyphe-
nated or capitalised wordforms in any special way.  
The intuition behind the Mikheev?s rule score is 
that a good guessing rule should be unambiguous 
(predicts a particular class without or with only 
very few exceptions), frequent (must be based on a 
large number of occurrences) and long (the longer 
the rule the lower the probability that it will hap-
pen by chance and thus the better its prediction). 
These criteria are combined in the following for-
mula: 
)log(1
)1()1(
2/)1(
l
n
ppt
pscore
n
+

=


where: 
 - l is the rule length; 
 - x is the number of successful rule guesses; 
 - n is the total number of training instances com-
patible with the rule; 
 - p is a smoothed version of the maximum like-
lihood estimation p? , which ensures that neither 
p nor (1?p) could be zero: p = (x+0.5)/(n+1); 
 - n
pp )1(  is an estimation of the dispersion; 
 - )1( 2/)1( nt  is a coefficient of the t-distribution with 
n?1 degrees of freedom and confidence level .
It is important to note that Mikheev weights the 
rule frequencies with the frequencies of the word-
forms they match as estimated from raw text. We 
performed experiments both with and without such 
weighting.  
 
Cleaned Threshold Original 100% only everything
0.00 738,446 115,474 20,846
0.50 597,238 89,324 18,663
0.80 122,439 27,477 4,881
0.90 55,144 15,071 2,459
0.95 22,015 7,673 1,402
Table 2: Mikheev-like rules for POS guessing co-
unt: original and cleaned (100% correct and all). 
Column 2 of Table 2 gives an idea about the 
number of selected ending guessing rules for POS  
prediction when different thresholds are used (and 
when the training was performed on a subset of the 
dictionary, containing 894,915 wordforms, as des-
cribed below). We were unhappy with such a large 
number of rules, especially after we observed that 
they were highly redundant. For example, if the 
threshold is set to 0.95, all the rules listed in Table 
3 (and many more) are selected. In fact, all these 
are covered by the ending ?-dEF?, which is 100% 
correct, and they all predict that the POS should be 
verb. So, all we need is to keep ?-dEF?, while drop-
ping all other longer endings that have additional 
starting letters4. This reduces the number of rules 
by a factor of 3 to 7 (see column 3 of Table 2).  
Thinking again, we can see that we can reduce 
the number of rules even further. For example, the-
re is a rule ?->=d=?, which is scored 0.99967073 
and was met 6,593 times as a verb and only once 
as a noun (i.e. it is 99.98% correct). There is ano-
ther one ?-e>=d=?, which is scored 0.99943267, 
and was met 1,498 times, always as a verb. There 
are also rules like ?-=>=d=?, ?-f>=d=?, etc. Obvio-
usly, all they, and any other ending on ?->=d=?, 
will make the same prediction, so we do not need 
to keep them. Removing the redundancies of this 
kind leads to another dramatic drop in the number 
of rules by a similar factor (see column 4 of Table 
2). In the experiments below we always applied 
this kind of cleaning.5
Ending Score Frequency 
-F@e>=dEF 0.98336703 47 
-@e>=dEF 0.99666399 241 
-e>=dEF 0.99944650 1,489 
->=dEF 0.99987014 6,546 
-=dEF 0.99992176 11,346 
-dEF 0.99995697 22,074 
Table 3: Some redundant selection for ?-dEF?. 
5 Evaluation 
We ran two different general types of experi-
ments: using the dictionary only and using additio-
nal raw text to estimate the frequencies of the dicti-
onary words. We split the dictionary into two parts 
at random: 894,915 wordforms for training (about 
90%) and the remaining 99,624 wordforms for tes-
ting. In the dictionary-only experiments we extrac-
ted the ending guessing rules by observing the en-
dings of all wordforms from the training part of the 
dictionary6. We then applied the rules thus learned 
(each time preferring the longest one that is com-
patible with the target word) to the testing part of 
the dictionary and we measured the precision P
(what % of the cases the predicted class matched 
the hypothesised one) and the coverage C (what % 
of the cases there was at least one rule that was 
compatible with the target wordform). We also 
calculated a kind of F-measure, which is normally 
 
4 Table 3 does not list all of them and there are seve-
ral dozens additional highly scored ones, e.g. ?-FdEF?. 
5 It looks like Mikheev (1997) did not observe that 
kind of redundancy. 
6 For a given word, we extracted all the correspon-
ding endings up to 8 letters long. This can possibly be 
the whole word. 
defined as 2PR/(P+R), where R is the recall (pro-
portion of proposed instances out of all that have to 
be found). Precision, recall and F-measure are de-
fined in the information retrieval community in 
terms of positive and negative documents for a gi-
ven query, i.e. with respect to a single class, but 
here we have multiple of them. While we can defi-
ne both an overall and a class-specific precision, it 
makes sense to talk about recall with respect to a 
particular class, but about coverage, when this is a 
measure for all classes. So, we redefined the F-me-
asure as 2PC/(P+C). 
In the dictionary+text experiments, we use the 
same training and testing parts of the dictionary, 
and in addition, the frequencies for the correspon-
ding words in the training and testing text sets, ac-
cordingly. I.e. a wordform in the text that is not in 
the dictionary is ignored and the rest are treated as 
if they have been repeated in its training/testing 
part the same number of times as they were met in 
the training/testing raw text.  
We used a collection of 23.5 MB of various 
genres of Bulgarian texts as follows: 
 legal: 742 KB 
 poetry: 236 KB 
 prose: 1,032 KB 
 religion: 393 KB 
 newspapers: 21,118 KB 
We used 2,211 KB of the newspaper texts for 
testing (about 10% of the collection) and the rest 
for training. As we already mentioned above, the 
same graphemic wordform can be met in the dicti-
onary multiple times with different annotations. In 
such cases, we treated them as equally likely both 
on training and testing. This resulted in 1,751,963 
wordforms tokens on training and 18,832 on tes-
ting. The huge difference is due to the fact that on 
testing we have both 10 times smaller dictionary 
and 10 times smaller text set to estimate the word-
form frequencies from, which multiply and result 
in 100 fold drop. 
For all experiments, we excluded the wordforms 
from a stoplist composed of the closed class words, 
i.e. the ones with the following POS (counts in pa-
rentheses): auxiliary verbs (91), conjunctions (31), 
interjections (28), particles (41), prepositions (69) 
and pronouns (286). We have been hesitating also 
about the numerals but there were 582 of them in 
the dictionary and one can produce more, so they 
do not represent a closed class and we did not in-
clude them. A potential problem with the stop-
words removal is that many of them can also be 
non-stop ones depending on their POS, e.g.: while 
g\D/preposition (under), EFhX/pronoun (these) and 
AXB/auxiliary (has been) are stop-words, g\D/noun 
(floor), EFhX/noun (theses) and iXB/person (Bill)
are not. We did not try to address this problem 
(which would have required POS tagging and pos-
sibly morphological analysis, which is unacceptab-
le, given our task) and we simply removed all ho-
mographs that matched a stoplist wordform.  
We performed several experiments trying to as-
sess the performance of the ending guessing rules 
as predictors for POS, article, gender, number and 
semantics. The details follow. 
5.1 POS 
We do a major distinction, between the follow-
ing five open POS classes: A (adjective), ADV (ad-
verb), N (noun), NU (numeral) and V (verb). Re-
member that we already excluded the auxiliary 
verbs, conjunctions, interjections, particles, prepo-
sitions and pronouns (and all their homographs). 
Some statistics are shown in Table 4 and the re-
sults of the evaluation are presented on Figure 1. 
Note the differences in the distribution of the dic-
tionary vs. text ending frequency estimations. Note 
also how the results for training and testing using 
raw text lead to consistently lower performance. 
The same observation can be made for the other 
kinds of predictions, see Figures 2-7. 
 
Class Dictionary Text 
A 129,828 17.34% 217,035 17.33%
ADV 661 0.09% 62,996 5.03%
N 84,303 11.26% 646,890 51.65%
NU 408 0.05% 12,112 0.97%
V 533,453 71.26% 313,327 25.02%
Table 4: Prior (training) distribution of POS.
55%
60%
65%
70%
75%
80%
85%
90%
95%
100%
mik-0.50 mik-0.80 mik-0.90 mik-0.95 exact
precision (dictionary)
coverage (dictionary)
F measure (dictionary)
precision (text)
coverage (text)
F measure (text)
 
Figure 1: Results for POS.
5.2 Article 
We learn rules to distinguish between three clas-
ses of articles: definite, indefinite and none. Unlike 
English, the articles in Bulgarian7 appear augmen-
ted at the end of one of the words in a noun phrase, 
typically the first one. The feminine and neutre no-
 
7 Bulgarian and Macedonian are the only Slavonic 
languages with definite articles of this kind. 
uns, adjectives, numerals and some verb forms 
(e.g. participles) have the same form for both defi-
nite and indefinite articles (e.g. defence: \EA]=@= 
; \EA]=@=Ea/(in)def), while for masculine these 
are distinct (e.g. man: j\>Ff ; j\>Ff=/indef, j\-
>FfkE/def). We ran two experiments: with (see 
Table 5 and Figure 2) and without POS (see Table 
6 and Figure 3). Note that we certainly need the 
none class in a real system so we had to include it.  
 
Class Dictionary Text 
def 324,253 39.31% 393,658 28.01%
indef 250,345 30.35% 703,116 50.02%
none 250,345 30.35% 308,802 21.97%
Table 5: Prior (training) distribution for article 
(no POS). 
Class Dictionary Text 
A+def 73,866 10.00% 96,909 7.89%
A+indef 48,327 6.54% 116,282 9.47%
N+def 43,426 5.88% 230,506 18.78%
N+indef 40,343 5.46% 390,609 31.82%
NU+def 229 0.03% 4,927 0.40%
NU+indef 179 0.02% 7,185 0.59%
V+def 142,500 19.28% 5,635 0.46%
V+indef 137,692 18.63% 66,798 5.44%
none 252,407 34.16% 308,802 25.15%
Table 6: Prior (training) distribution of article 
(with POS). 
60%
65%
70%
75%
80%
85%
90%
95%
100%
mik-0.50 mik-0.80 mik-0.90 mik-0.95 exact
precision (dictionary)
coverage (dictionary)
F measure (dictionary)
precision (text)
coverage (text)
F measure (text)
 
Figure 2: Results for article (no POS). 
50%
55%
60%
65%
70%
75%
80%
85%
90%
95%
100%
mik-0.50 mik-0.80 mik-0.90 mik-0.95 exact
precision (dictionary)
coverage (dictionary)
F measure (dictionary)
precision (text)
coverage (text)
F measure (text)
 
Figure 3: Results for article (with POS). 
5.3 Gender 
There are three genders in Bulgarian: masculine,
feminine and neuter. Only some of the word clas-
ses can have gender, namely: adjectives, nouns, 
numerals and some verb forms (e.g. participles). 
The results of the gender guessing experiments are 
shown in Tables 7, 8 and Figures 4, 5. 
 
Class Dictionary Text 
Fem 112,201 13.65% 87,856 5.76%
Mas 150,426 18.30% 110,361 7.24%
Neu 121,134 14.73% 87,608 5.74%
none 438,386 53.32% 1,239,183 81.26%
Table 7: Prior (training) distribution of gender 
(no POS). 
Class Dictionary Text 
A+fem 30,465 3.96% 56,414 3.89%
A+mas 38,490 5.00% 59,306 4.09%
A+neu 25,258 3.28% 30,556 2.11%
N+neu 276 0.04% 4,592 0.32%
NU+fem 68 0.01% 2,862 0.20%
NU+mas 161 0.02% 6,582 0.45%
NU+neu 65 0.01% 1,139 0.08%
V+fem 67,385 8.76% 12,326 0.85%
V+mas 96,529 12.55% 28,660 1.97%
V+neu 72,040 9.37% 9,671 0.67%
none 438,386 57.00% 1,239,183 85.38%
Table 8: Prior (training) distribution of gender 
(with POS). 
80%
85%
90%
95%
100%
mik-0.50 mik-0.80 mik-0.90 mik-0.95 exact
precision (dictionary)
coverage (dictionary)
F measure (dictionary)
precision (text)
coverage (text)
F measure (text)
 
Figure 4: Results for gender (no POS). 
75%
80%
85%
90%
95%
100%
mik-0.50 mik-0.80 mik-0.90 mik-0.95 exact
precision (dictionary)
coverage (dictionary)
F measure (dictionary)
precision (text)
coverage (text)
F measure (text)
 
Figure 5: Results for gender (with POS). 
5.4 Number 
There are two grammatical numbers in today?s 
Bulgarian: singular and plural8. Again, only some 
of the word classes can have number, namely: ad-
jectives, nouns, numerals and some verb forms 
(e.g. participles). Tables 9, 10 and Figures 6, 7 for 
the results of the number guessing experiments. 
Class Dictionary Text 
Plural 146,592 17.49% 299,134 20.84%
Singular 455,186 54.32% 827,131 57.62%
none 236,260 28.19% 309,276 21.54%
Table 9: Prior (training) distribution of number 
(no POS). 
 
Class Dictionary Text 
A+pl 29,281 3.92% 68,144 5.48%
A+sg 100,535 13.44% 148,845 11.97%
N+pl 31,766 4.25% 163,403 13.14%
N+sg 46,317 6.19% 468,577 37.69%
NU+pl 57 0.01% 69 0.01%
NU+sg 197 0.03% 8,218 0.66%
V+pl 67,557 9.03% 25,939 2.09%
V+sg 235,896 31.54% 50,657 4.07%
none 236,260 31.59% 309,276 24.88%
Table 10: Prior (training) distribution of number 
(with POS). 
70%
75%
80%
85%
90%
95%
100%
mik-0.50 mik-0.80 mik-0.90 mik-0.95 exact
precision (dictionary)
coverage (dictionary)
F measure (dictionary)
precision (text)
coverage (text)
F measure (text)
 
Figure 6: Results for number (no POS). 
50%
55%
60%
65%
70%
75%
80%
85%
90%
95%
100%
mik-0.50 mik-0.80 mik-0.90 mik-0.95 exact
precision (dictionary)
coverage (dictionary)
F measure (dictionary)
precision (text)
coverage (text)
F measure (text)
 
Figure 7: Results for number (with POS). 
 
8 The Old Bulgarian language used to have also a 
dual number. The only Slavonic language this gramma-
tical number has been preserved in is Slovenian. 
5.5 Semantics 
The last kind of experiments we performed was 
recognising some kind of semantics. We tried to 
guess whether a wordform is a human, animate or 
neither, as we had such information in our dictio-
nary. These are always limited to nouns (at least in 
our dictionary annotations), so we did not have se-
parate experiments with and without POS (they 
would have produced almost the same result, 
except for some potential problems caused by ho-
mographs with a non-noun POS). The results are 
shown in Table 11 and Figure 8.  
Class Dictionary Text 
Animate 1,765 0.21% 4,536 0.28%
Human 26,918 3.14% 121,299 7.39%
none 828,887 96.66% 1,516,053 92.34%
Table 11: Training (prior) distribution of 
semantics.
80%
85%
90%
95%
100%
mik-0.50 mik-0.80 mik-0.90 mik-0.95 exact
precision (dictionary)
coverage (dictionary)
F measure (dictionary)
precision (text)
coverage (text)
F measure (text)
 
Figure 8: Results for semantics: human/animate.
75%
80%
85%
90%
95%
100%
freq 1 freq 2 freq 3 freq 4 freq 5 freq 10 freq 20
precision (dictionary)
coverage (dictionary)
F measure (dictionary)
 
Figure 9: Results for article: the exact algorithm  
for different thresholds. 
 
Figures 1-8 show that the approximate rules with 
confidence score of 0.50 perform consistently bet-
ter than the exact ones, where we keep every single 
rule, even the ones met only once. So, we are very 
likely to over fit. One way to prevent this is to 
ignore some of the least reliable rules. The simp-
lest criterion for this is the minimum frequency. 
We performed some experiments, setting it to 1, 2, 
3, 4, 5, 10 and 20. The results are shown on Figure 
9, where we can see that while gaining a little bit 
on recall, we lose a lot on precision. Thus, if we 
stick to the exact rules, we apparently cannot gain 
by removing some of the rules based on frequency. 
In fact, this is not necessarily true, as it could be 
possible when using more complex criterion that 
takes into account more than just frequency, e.g. 
rule length.  
6 Discussion 
Table 12 contains summary results for the expe-
riments with the exact and the approximate rules 
(with a threshold of 0.50, since, as Figures 1-8 
show, it had the highest F-measure). The first two 
columns describe the kind of experiment and the 
method, followed by the precision, coverage and 
F-measure. Finally, the last two columns show the 
corresponding number of rules used and the num-
ber of target classes.  
 
Experiment Method P C F # rules # class
article exact 98.61% 94.00% 96.25% 53,216 3
article mik-.50 97.02% 99.97% 98.47% 10,745 3 
article+POS exact 97.01% 83.09% 89.51% 85,061 9
article+POS mik-.50 92.33% 99.84% 95.94% 27,263 9 
gender exact 99.04% 93.88% 96.39% 39,309 4
gender mik-.50 97.43% 100.00% 98.70% 7,263 4 
gender+POS exact 98.35% 87.45% 92.58% 53,385 11 
gender+POS mik-.50 94.79% 99.81% 97.24% 12,473 11 
number exact 99.21% 95.49% 97.31% 40,856 3
number mik-.50 97.90% 100.00% 98.94% 7,493 3 
number+POS exact 97.60% 84.07% 90.33% 81,154 9
number+POS mik-.50 93.08% 99.92% 96.38% 20,144 9 
POS exact 97.70% 84.23% 90.47% 79,609 5
POS mik-.50 93.23% 100.00% 96.50% 18,663 5 
semantics exact 99.10% 97.13% 98.11% 43,902 3
semantics mik-.50 98.33% 99.99% 99.15% 9,971 3 
Table 12: Experiments summary (dictionary). 
There are several interesting observations about 
Table 12 (and Figures 1-8). First, the precision of 
the exact rules is consistently higher than that of 
the approximate ones with a threshold of 0.50. This 
is not surprising as the ending guessing rules pro-
duced by the exact method are guaranteed to be 
100% correct on the training set (but not necessari-
ly on the testing one, as we explained above). Fi-
gures 1-8 show that this observation holds for all 
other score thresholds considered, even for 0.95 
(remember that the score reflects not only the rule 
accuracy but also its length and smoothed frequen-
cy). The situation is reversed with respect to the 
coverage: the exact rules have a lower coverage, 
which more than compensates for their higher pre-
cision. As a result, the F-measure is consistently 
lower for the exact algorithm as compared to the 
approximate one with a threshold of 0.50. Figures 
1-8 show this is not the case for higher thresholds 
(especially 0.95) when the coverage becomes lo-
wer and the F-measure gets worse as compared to 
that of the exact method. 
Comparing article, gender and number to arti-
cle+POS, gender+POS and number+POS, accor-
dingly, where the number of classes is increased by 
a factor of 3, we can see that the exact algorithm 
remains robust with respect to precision: there is a 
decrease of about 1-1.5% only. The precision of 
the approximate rules is decreased by 3-4%. On 
the other hand, the coverage of the approximate ru-
les is virtually unaffected and stays very close to 
100% (decreased by less than 0.2%), while for the 
exact rules it drops significantly: by 6-9%. As a re-
sult, the approximate algorithm has a more robust 
F-measure, which drops by 1-2.5% only, while for 
the exact algorithm this is 4-7%.  
The approximate method is also more robust 
with respect to the number of rules, as it produces 
about five times less of them as compared to the 
exact one. When article, gender and number are 
combined with POS, the number of rules is increa-
sed by a factor of 2 to 3. 
Overall, the approximate rules with a threshold 
of 0.50 exhibit a very high coverage (100% or very 
close) and precision/F-measure of about 97-99%.  
Finally, the tasks are not equally hard. The easi-
est one is semantics, and the hardest one is POS.  
 
Class P R F 
A+fem 91.67% 87.58% 89.58%
A+mas 92.21% 85.83% 88.91%
A+neu 83.78% 85.44% 84.60%
N+neu 16.24% 3.82% 6.18%
NU+fem 44.44% 80.00% 57.14%
NU+mas 85.71% 81.82% 83.72%
NU+neu 85.71% 60.00% 70.59%
V+fem 93.88% 97.73% 95.77%
V+mas 93.10% 96.94% 94.98%
V+neu 88.10% 96.79% 92.24%
None 98.66% 96.49% 97.56%
Table 13: Testing performance per class for 
gender+POS approximate rules 0.50 (dictionary).  
Class P R F 
A+fem 96.79% 96.96% 96.88%
A+mas 97.04% 95.74% 96.39%
A+neu 94.83% 95.03% 94.93%
N+neu 21.74% 18.29% 19.87%
NU+fem 80.00% 100.00% 88.89%
NU+mas 94.74% 81.82% 87.80%
NU+neu 85.71% 66.67% 75.00%
V+fem 98.36% 99.12% 98.74%
V+mas 98.41% 98.49% 98.45%
V+neu 95.92% 97.42% 96.67%
None 99.27% 99.01% 99.14%
Table 14: Testing performance per class for 
gender+POS exact rules (dictionary). 
It is interesting to observe the performance of the 
different classes in a particular experiment, e.g. 
gender+POS. Note that now we can calculate a 
true recall as opposed to coverage, as we can work 
with a particular class. The results for the gen-
der+POS, dictionary trained, experiments are sho-
wn in Tables 13 and 14. We can see that the preci-
sion, the recall and the F-measure of the exact ru-
les are consistently better for each class as compa-
red to the ones obtained using approximate rules 
(with threshold of 0.50). Note however that the 
recall here is calculated only for the part for which 
there was a prediction. The exact rules covered 
84,512 out of all 96,643 wordforms (coverage: 
87.45%) and 83,120 of them were correct (precisi-
on: 98.35%). The per-class P, R and F are calcula-
ted only for those 84,512 wordforms for which a 
prediction has been made. I.e. we did not assign 
the non-covered wordforms the class none by defa-
ult, although probably we should, as it is the most 
frequent one. The approximate rules made pre-
dictions for 96,458 wordforms (coverage: 99.81%) 
91,430 of which were correct (precision: 94.79%). 
Table 15 shows the performance for the approxi-
mate rules as evaluated on the training set9. Out of 
the 867,567 wordforms, 866,786 have been cove-
red (coverage 99.91%), 835,330 of which correctly 
(precision 96.37%). We see that the class N+neu 
was hard to predict not only on testing but also on 
training.  
 
Class P R F 
A+fem 95.60% 91.35% 93.43%
A+mas 96.36% 90.64% 93.41%
A+neu 87.26% 90.41% 88.81%
N+neu 83.67% 8.50% 15.44%
NU+fem 98.53% 83.75% 90.54%
NU+mas 90.96% 89.44% 90.20%
NU+neu 98.48% 89.04% 93.53%
V+fem 95.91% 98.59% 97.23%
V+mas 94.58% 98.49% 96.50%
V+neu 89.21% 99.10% 93.90%
none 99.53% 97.29% 98.40%
Table 15: Training performance per class for 
gender+POS approximate rules 0.50 (dictionary).  
Something that Table 12 does not show, but one 
can see on Figures 1-8, is the consistently worse 
performance of training & testing on the dictionary 
vs. training & testing only on these dictionary 
words that are met in the raw text, using the corres-
ponding frequencies. The major reason for this is 
 
9 We do not show a corresponding training accu-
racy table for the exact rules as every cell there is 
replaced with 100%, i.e. there is a perfect fit. 
the insufficient amount of training text. While the 
number of word tokens is high, the number of 
word types is much less than that of the dictionary. 
So, the significantly lower variability of word-
forms more than compensates any gains of having 
real word frequencies. We believe weighting thro-
ugh real text is important and we plan to re-run 
these experiments with word frequencies estimated 
from orders of magnitude more textual data (it is 
cheap and freely available on the Web). Another, 
less attractive alternative could be to add the dic-
tionary as a text. That way we would have incor-
rect frequency estimations for some of the words, 
but also the learning algorithm would have access 
to the rich word variability of the dictionary words. 
7 Future Work 
There are several possible extensions to the work 
presented above. First, the exact algorithm can be 
extended with non-exact rules. Second, the Mikhe-
ev-like ending guessing rules construction could be 
augmented with a merging phase as originally pro-
posed. It would be interesting to consider using the 
dictionary not only during rules generation but also 
during their application: e.g. try to add/remove suf-
fixes/prefixes and check whether the newly obtai-
ned word is listed in the dictionary (e.g. we might 
have the word @=ABCD=EFB/noun (observer) but 
not @=ABCD=EFB(!/adj (observing), generated fol-
lowing a standard derivational rule). There are pre-
fixes, mostly foreign, that can attach to any open 
class word, but the resulting words are unlikely to 
be in our dictionary: ?=@EX-? (anti-), ?lBE]=-? (ul-
tra-), ?mlgF]-? (super-), ?f\@E]=-? (contra-). Fur-
thermore, there are some important prefixes, speci-
fic to Bulgarian, that can limit the possible POS: 
e.g. ?g\-? and ?@=n-? (?-? is part of the prefix) are 
used to construct a comparative and a superlative 
form, accordingly, and can be used only with ad-
jectives, adverbs and some verb forms (e.g. partici-
ple). We believe in the potential of the combined 
evidence from both prefixes and suffixes. In additi-
on, it seems important to allow for mutations in the 
word stem as these are common in Slavonic langu-
ages. Finally, it might be beneficial to learn separa-
te rules for capitalised and dashed words (but may-
be it is not that important as their usage is less fre-
quent, especially the capitalisation). 
We would like to try other scoring and smoo-
thing approaches. We did not address the problem 
of selecting the best threshold (although it is clear 
that it should be low, maybe around 0.50). One 
way to do this is to split the training set into rules-
training and threshold-training sets. Next, it looks 
promising to try to estimate the dictionary word 
frequencies using a search engine instead of text 
corpus, as proposed by Lapata and Keller (2004). 
While the exact algorithm performed worse due 
to insufficient coverage10, we believe it has a po-
tential, e.g. if extended with some approximate ru-
les. Note that the way the exact rules were built is 
very similar to the standard algorithm for decision 
tree construction. Thus the corresponding tree cut-
ting criteria used to prevent over fitting can help 
decide when to go further and look at longer en-
dings and when to stop.  
It is interesting to see how the proposed rules 
perform for other Slavonic languages. In particular, 
we plan similar experiments for Russian as a com-
parable morphological dictionary with the same 
kind of linguistic annotations is already available.  
Finally, we would like to explore the machine 
learning potential offered by morphological dictio-
naries with application to other related tasks such 
as stemming (Nakov, 2003), lemmatisation and 
POS tagging. 
References  
E. Brill. 1997. Unsupervised Learning of Disambi-
guation Rules for Part of Speech Tagging. Natu-
ral Language Processing Using Very Large Cor-
pora. Kluwer Academic Press. 1997. 
S. Cucerzan, D. Yarowsky. 2000. Language inde-
pendent minimally supervised induction of lexi-
cal probabilities. ACL, 270-277. 
D. Cutting, J. Kupiec, J. Pedersen, P. Sibun. 1992. 
A practical part-of-speech tagger. ANLP, 133-140. 
J. Daciuk. 1999. Treatment of Unknown Words. 
Workshop on Implementing Automata. IX-1?IX-9. 
H. DeJean. 1998. Morphemes as necessary con-
cepts for structures: Discovery from untagged 
corpora. Workshop on Paradigms and Groun-
ding in Natural Language Learning, 295-299. 
E. Gaussier. 1999. Unsuppervised learning of deri-
vational morphology from inflectional lexicons. 
Workshop on Unsupervised Learning in Natural 
Language Processing (ACL).
R. Goldsmith. 1998. Automatic collection and ana-
lysis of German compounds. Workshop on Com-
putational Treatment of Nominals (COLING-
ACL), 61-69. 
M. Hafer, S. Weiss. 1974. Word segmentation by 
letter successor varieties. Information Storage 
and Retrieval, 10:371-385. 
C. Jacquemin. 1997. Guessing morphology from 
terms and corpora. ACM SIGIR, 156?167. 
 
10 Note that we can achieve 100% coverage by assig-
ning the examples that are not covered to a default class 
(e.g. none or the most frequent one). It would be interes-
ting to compare the precision of the exact and the appro-
ximate rules under these conditions. 
J. Kupiec. 1992. Robust part-of-speech tagging us-
ing a hidden Markov model. Computer Speech 
and Language, 6(3):225-242. 
M. Lapata, F. Keller. 2004. The Web as a Baseline: 
Evaluating the Performance of Unsupervised 
Web-based Models for a Range of NLP Tasks. 
ACL, 121-128. 
A. Mikheev. 1997. Automatic Rule Induction for 
Unknown Word Guessing. Computational Lin-
guistics, 23(3):405-423. 
Nakov P. 2003. BulStem: Design and Evaluation 
of Inflectional Stemmer for Bulgarian. Workshop 
on Balkan Language Resources and Tools (Bal-
kan Conference in Informatics).
http://iit.demokritos.gr/skel/bci03_workshop/papers/ 
P. Nakov, Y. Bonev, G. Angelova, E. Gius, W. von 
Hahn. 2003. Guessing Morphological Classes of 
Unknown German Nouns. Recent Advances in 
Natural Language Processing, 319-326. 
E. Paskaleva. 2003. Compilation and validation of 
morphological resources. Workshop on Balkan 
Language Resources and Tools (Balkan Confe-
rence on Informatics).
http://iit.demokritos.gr/skel/bci03_workshop/papers/ 
M. Porter. 1980. An algorithm for suffix stripping. 
Program 14(3):130-137. 
P. Ruch, R. Baud, P. Bouillon, G. Robert. 2000. 
Minimal Commitment and Full Lexical Disambi-
guation: Balancing Rules and Hidden Markov 
Models. CoNLL (ACL-SIGNLL), 111-115. 
H. Schmid. 1995. Improvements in part-of-speech 
tagging with an application to German. Feldweg 
and Hinrichs, eds., Lexikon und Text, 47-50.  
P. Schone, D. Jurafsky. 2000. Knowledge-Free In-
duction of Morphology Using Latent Semantic 
Analysis. CoNLL (ACL-SIGNLL), 67-72. 
Silberztein M. 1993. Dictionnaires electroniques et 
analyse automatique de textes: le systeme 
INTEX. Masson, Paris. 
S. Thede S., M. Harper. 1997. Analysis of Unkno-
wn Lexical Items using Morphological and Syn-
tactic Information with the TIMIT Corpus. 
Workshop on Very Large Corpora, W97-0124. 
A. Van den Bosch, W. Daelemans. 1999. Memory-
based morphological analysis. ACL, 285-292. 
R. Weischedel, R. Schwartz, J. Palmucci, M. Mete-
er, L. Ramshaw. 1993. Coping with Ambiguity 
and Unknown Words through Probabilistic Mo-
dels. Computational Linguistics, 19(2):359-382. 
D. Yarowsky, R. Wicentowski. 2000. Minimally 
supervised morphological analysis by multimo-
dal alignment. ACL, 207-216. 
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 17?24, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Search Engine Statistics Beyond the n-gram:
Application to Noun Compound Bracketing
Preslav Nakov
EECS, Computer Science Division
University of California, Berkeley
Berkeley, CA 94720
nakov@cs.berkeley.edu
Marti Hearst
SIMS
University of California, Berkeley
Berkeley, CA 94720
hearst@sims.berkeley.edu
Abstract
In order to achieve the long-range goal
of semantic interpretation of noun com-
pounds, it is often necessary to first de-
termine their syntactic structure. This pa-
per describes an unsupervised method for
noun compound bracketing which extracts
statistics from Web search engines using a
?2 measure, a new set of surface features,
and paraphrases. On a gold standard, the
system achieves results of 89.34% (base-
line 66.80%), which is a sizable improve-
ment over the state of the art (80.70%).
1 Introduction
An important but understudied language analy-
sis problem is that of noun compound bracketing,
which is generally viewed as a necessary step to-
wards noun compound interpretation. Consider the
following contrastive pair of noun compounds:
(1) liver cell antibody
(2) liver cell line
In example (1) an antibody targets a liver cell, while
(2) refers to a cell line which is derived from the
liver. In order to make these semantic distinctions
accurately, it can be useful to begin with the cor-
rect grouping of terms, since choosing a particular
syntactic structure limits the options left for seman-
tics. Although equivalent at the part of speech (POS)
level, these two noun compounds have different syn-
tactic trees. The distinction can be represented as a
binary tree or, equivalently, as a binary bracketing:
(1b) [ [ liver cell ] antibody ] (left bracketing)
(2b) [ liver [cell line] ] (right bracketing)
In this paper, we describe a highly accurate un-
supervised method for making bracketing decisions
for noun compounds (NCs). We improve on the cur-
rent standard approach of using bigram estimates to
compute adjacency and dependency scores by intro-
ducing the use of the ?2 measure for this problem.
We also introduce a new set of surface features for
querying Web search engines which prove highly ef-
fective. Finally, we experiment with paraphrases for
improving prediction statistics. We have evaluated
the application of combinations of these features to
predict NC bracketing on two distinct collections,
one consisting of terms drawn from encyclopedia
text, and another drawn from bioscience text.
The remainder of this paper describes related
work, the word association models, the surface fea-
tures, the paraphrase features and the results.
2 Related Work
The syntax and semantics of NCs is an active area of
research; the Journal of Computer Speech and Lan-
guage has an upcoming special issue on Multiword
Expressions.
The best known early work on automated un-
supervised NC bracketing is that of Lauer (1995)
who introduces the probabilistic dependency model
for the syntactic disambiguation of NCs and argues
against the adjacency model, proposed by Marcus
(1980), Pustejovsky et al (1993) and Resnik (1993).
Lauer collects n-gram statistics from Grolier?s en-
cyclopedia, containing about 8 million words. To
17
overcome data sparsity problems, he estimates prob-
abilities over conceptual categories in a taxonomy
(Roget?s thesaurus) rather than for individual words.
Lauer evaluated his models on a set of 244 unam-
biguous NCs derived from the same encyclopedia
(inter-annotator agreement 81.50%) and achieved
77.50% for the dependency model above (baseline
66.80%). Adding POS and further tuning allowed
him to achieve the state-of-the-art result of 80.70%.
More recently, Keller and Lapata (2003) evalu-
ate the utility of using Web search engines for ob-
taining frequencies for unseen bigrams. They then
later propose using Web counts as a baseline unsu-
pervised method for many NLP tasks (Lapata and
Keller, 2004). They apply this idea to six NLP tasks,
including the syntactic and semantic disambigua-
tion of NCs following Lauer (1995), and show that
variations on bigram counts perform nearly as well
as more elaborate methods. They do not use tax-
onomies and work with the word n-grams directly,
achieving 78.68% with a much simpler version of
the dependency model.
Girju et al (2005) propose a supervised model
(decision tree) for NC bracketing in context, based
on five semantic features (requiring the correct
WordNet sense to be given): the top three Word-
Net semantic classes for each noun, derivationally
related forms and whether the noun is a nominaliza-
tion. The algorithm achieves accuracy of 83.10%.
3 Models and Features
3.1 Adjacency and Dependency Models
In related work, a distinction is often made between
what is called the dependency model and the adja-
cency model. The main idea is as follows. For a
given 3-word NC w1w2w3, there are two reasons it
may take on right bracketing, [w1[w2w3]]. Either (a)
w2w3 is a compound (modified byw1), or (b)w1 and
w2 independently modify w3. This distinction can
be seen in the examples home health care (health
care is a compound modified by home) versus adult
male rat (adult and male independently modify rat).
The adjacency model checks (a), whether w2w3
is a compound (i.e., how strongly w2 modifies w3
as opposed to w1w2 being a compound) to decide
whether or not to predict a right bracketing. The
dependency model checks (b), does w1 modify w3
(as opposed to w1 modifying w2).
Left bracketing is a bit different since there is only
modificational choice for a 3-word NC. If w1 modi-
fies w2, this implies that w1w2 is a compound which
in turn modifies w3, as in law enforcement agent.
Thus the usefulness of the adjacency model vs.
the dependency model can depend in part on the mix
of left and right bracketing. Below we show that the
dependency model works better than the adjaceny
model, confirming other results in the literature. The
next subsections describe several different ways to
compute these measures.
3.2 Using Frequencies
The most straightforward way to compute adjacency
and dependency scores is to simply count the cor-
responding frequencies. Lapata and Keller (2004)
achieved their best accuracy (78.68%) with the de-
pendency model and the simple symmetric score
#(wi, wj).1
3.3 Computing Probabilities
Lauer (1995) assumes that adjacency and depen-
dency should be computed via probabilities. Since
they are relatively simple to compute, we investigate
them in our experiments.
Consider the dependency model, as introduced
above, and the NC w1w2w3. Let Pr(wi ? wj |wj)
be the probability that the word wi precedes a
given fixed word wj . Assuming that the distinct
head-modifier relations are independent, we obtain
Pr(right) = Pr(w1 ? w3|w3)Pr(w2 ? w3|w3)
and Pr(left) = Pr(w1 ? w2|w2)Pr(w2 ? w3|w3).
To choose the more likely structure, we can drop
the shared factor and compare Pr(w1 ? w3|w3) to
Pr(w1 ? w2|w2).
The alternative adjacency model compares
Pr(w2 ? w3|w3) to Pr(w1 ? w2|w2), i.e. the
association strength between the last two words vs.
that between the first two. If the first probability is
larger than the second, the model predicts right.
The probability Pr(w1 ? w2|w2) can be esti-
mated as #(w1, w2)/#(w2), where #(w1, w2) and
#(w2) are the corresponding bigram and unigram
1This score worked best on training, when Keller&Lapata
were doing model selection. On testing, Pr (with the depen-
dency model) worked better and achieved accuracy of 80.32%,
but this result was ignored, as Pr did worse on training.
18
frequencies. They can be approximated as the num-
ber of pages returned by a search engine in response
to queries for the exact phrase ?w1 w2? and for the
word w2. In our experiments below we smoothed2
each of these frequencies by adding 0.5 to avoid
problems caused by nonexistent n-grams.
Unless some particular probabilistic interpreta-
tion is needed,3 there is no reason why for a given
ordered pair of words (wi, wj), we should use
Pr(wi ? wj |wj) rather than Pr(wj ? wi|wi),
i < j. This is confirmed by the adjacency model
experiments in (Lapata and Keller, 2004) on Lauer?s
NC set. Their results show that both ways of
computing the probabilities make sense: using Al-
tavista queries, the former achieves a higher accu-
racy (70.49% vs. 68.85%), but the latter is better on
the British National Corpus (65.57% vs. 63.11%).
3.4 Other Measures of Association
In both models, the probability Pr(wi ? wj |wj)
can be replaced by some (possibly symmetric) mea-
sure of association between wi and wj , such as Chi
squared (?2). To calculate ?2(wi, wj), we need:
(A) #(wi, wj);
(B) #(wi, wj), the number of bigrams in which the
first word is wi, followed by a word other than
wj ;
(C) #(wi, wj), the number of bigrams, ending in
wj , whose first word is other than wi;
(D) #(wi, wj), the number of bigrams in which the
first word is not wi and the second is not wj .
They are combined in the following formula:
?2 =
N(AD ?BC)2
(A+ C)(B +D)(A+B)(C +D)
(1)
Here N = A + B + C + D is the total num-
ber of bigrams, B = #(wi) ? #(wi, wj) and C =
#(wj) ?#(wi, wj). While it is hard to estimate D
2Zero counts sometimes happen for #(w1, w3), but are rare
for unigrams and bigrams on the Web, and there is no need for
a more sophisticated smoothing.
3For example, as used by Lauer to introduce a prior for left-
right bracketing preference. The best Lauer model does not
work with words directly, but uses a taxonomy and further needs
a probabilistic interpretation so that the hidden taxonomy vari-
ables can be summed out. Because of that summation, the term
Pr(w2 ? w3|w3) does not cancel in his dependency model.
directly, we can calculate it asD = N?A?B?C.
Finally, we estimate N as the total number of in-
dexed bigrams on the Web. They are estimated as 8
trillion, since Google indexes about 8 billion pages
and each contains about 1,000 words on average.
Other measures of word association are possible,
such as mutual information (MI), which we can use
with the dependency and the adjacency models, sim-
ilarly to #, ?2 or Pr. However, in our experiments,
?2 worked better than other methods; this is not sur-
prising, as ?2 is known to outperform MI as a mea-
sure of association (Yang and Pedersen, 1997).
3.5 Web-Derived Surface Features
Authors sometimes (consciously or not) disam-
biguate the words they write by using surface-level
markers to suggest the correct meaning. We have
found that exploiting these markers, when they oc-
cur, can prove to be very helpful for making brack-
eting predictions. The enormous size of Web search
engine indexes facilitates finding such markers fre-
quently enough to make them useful.
One very productive feature is the dash (hyphen).
Starting with the term cell cycle analysis, if we can
find a version of it in which a dash occurs between
the first two words: cell-cycle, this suggests a left
bracketing for the full NC. Similarly, the dash in
donor T-cell favors a right bracketing. The right-
hand dashes are less reliable though, as their scope
is ambiguous. In fiber optics-system, the hyphen in-
dicates that the noun compound fiber optics modifies
system. There are also cases with multiple hyphens,
as in t-cell-depletion, which preclude their use.
The genitive ending, or possessive marker is an-
other useful indicator. The phrase brain?s stem
cells suggests a right bracketing for brain stem cells,
while brain stem?s cells favors a left bracketing.4
Another highly reliable source is related to inter-
nal capitalization. For example Plasmodium vivax
Malaria suggests left bracketing, while brain Stem
cells would favor a right one. (We disable this fea-
ture on Roman digits and single-letter words to pre-
vent problems with terms like vitamin D deficiency,
where the capitalization is just a convention as op-
posed to a special mark to make the reader think that
the last two terms should go together.)
4Features can also occur combined, e.g. brain?s stem-cells.
19
We can also make use of embedded slashes. For
example in leukemia/lymphoma cell, the slash pre-
dicts a right bracketing since the first word is an al-
ternative and cannot be a modifier of the second one.
In some cases we can find instances of the NC
in which one or more words are enclosed in paren-
theses, e.g., growth factor (beta) or (growth fac-
tor) beta, both of which indicate a left structure, or
(brain) stem cells, which suggests a right bracketing.
Even a comma, a dot or a colon (or any spe-
cial character) can act as indicators. For example,
?health care, provider? or ?lung cancer: patients?
are weak predictors of a left bracketing, showing
that the author chose to keep two of the words to-
gether, separating out the third one.
We can also exploit dashes to words external to
the target NC, as in mouse-brain stem cells, which
is a weak indicator of right bracketing.
Unfortunately, Web search engines ignore punc-
tuation characters, thus preventing querying directly
for terms containing hyphens, brackets, apostrophes,
etc. We collect them indirectly by issuing queries
with the NC as an exact phrase and then post-
processing the resulting summaries, looking for the
surface features of interest. Search engines typically
allow the user to explore up to 1000 results. We col-
lect all results and summary texts that are available
for the target NC and then search for the surface pat-
terns using regular expressions over the text. Each
match increases the score for left or right bracket-
ing, depending on which the pattern favors.
While some of the above features are clearly
more reliable than others, we do not try to weight
them. For a given NC, we post-process the returned
Web summaries, then we find the number of left-
predicting surface feature instances (regardless of
their type) and compare it to the number of right-
predicting ones to make a bracketing decision.5
3.6 Other Web-Derived Features
Some features can be obtained by using the over-
all counts returned by the search engine. As these
counts are derived from the entire Web, as opposed
to a set of up to 1,000 summaries, they are of differ-
ent magnitude, and we did not want to simply add
them to the surface features above. They appear as
5This appears as Surface features (sum) in Tables 1 and 2.
independent models in Tables 1 and 2.
First, in some cases, we can query for possessive
markers directly: although search engines drop the
apostrophe, they keep the s, so we can query for
?brain?s? (but not for ?brains? ?). We then com-
pare the number of times the possessive marker ap-
peared on the second vs. the first word, to make a
bracketing decision.
Abbreviations are another important feature. For
example, ?tumor necrosis factor (NF)? suggests a
right bracketing, while ?tumor necrosis (TN) fac-
tor? would favor left. We would like to issue exact
phrase queries for the two patterns and see which
one is more frequent. Unfortunately, the search en-
gines drop the brackets and ignore the capitalization,
so we issue queries with the parentheses removed, as
in ?tumor necrosis factor nf?. This produces highly
accurate results, although errors occur when the ab-
breviation is an existing word (e.g., me), a Roman
digit (e.g., IV), a state (e.g., CA), etc.
Another reliable feature is concatenation. Con-
sider the NC health care reform, which is left-
bracketed. Now, consider the bigram ?health care?.
At the time of writing, Google estimates 80,900,000
pages for it as an exact term. Now, if we try the
word healthcare we get 80,500,000 hits. At the
same time, carereform returns just 109. This sug-
gests that authors sometimes concatenate words that
act as compounds. We find below that comparing
the frequency of the concatenation of the left bigram
to that of the right (adjacency model for concatena-
tions) often yields accurate results. We also tried the
dependency model for concatenations, as well as the
concatenations of two words in the context of the
third one (i.e., compare frequencies of ?healthcare
reform? and ?health carereform?).
We also used Google?s support for ?*?, which al-
lows a single word wildcard, to see how often two of
the words are present but separated from the third by
some other word(s). This implicitly tries to capture
paraphrases involving the two sub-concepts making
up the whole. For example, we compared the fre-
quency of ?health care * reform? to that of ?health
* care reform?. We also used 2 and 3 stars and
switched the word group order (indicated with rev.
in Tables 1 and 2), e.g., ?care reform * * health?.
We also tried a simple reorder without inserting
stars, i.e., compare the frequency of ?reform health
20
care? to that of ?care reform health?. For exam-
ple, when analyzing myosin heavy chain we see that
heavy chain myosin is very frequent, which provides
evidence against grouping heavy and chain together
as they can commute.
Further, we tried to look inside the internal inflec-
tion variability. The idea is that if ?tyrosine kinase
activation? is left-bracketed, then the first two words
probably make a whole and thus the second word
can be found inflected elsewhere but the first word
cannot, e.g., ?tyrosine kinases activation?. Alterna-
tively, if we find different internal inflections of the
first word, this would favor a right bracketing.
Finally, we tried switching the word order of the
first two words. If they independently modify the
third one (which implies a right bracketing), then we
could expect to see also a form with the first two
words switched, e.g., if we are given ?adult male
rat?, we would also expect ?male adult rat?.
3.7 Paraphrases
Warren (1978) proposes that the semantics of the re-
lations between words in a noun compound are of-
ten made overt by paraphrase. As an example of
prepositional paraphrase, an author describing the
concept of brain stem cells may choose to write it
in a more expanded manner, such as stem cells in
the brain. This contrast can be helpful for syntactic
bracketing, suggesting that the full NC takes on right
bracketing, since stem and cells are kept together in
the expanded version. However, this NC is ambigu-
ous, and can also be paraphrased as cells from the
brain stem, implying a left bracketing.
Some NCs? meaning cannot be readily expressed
with a prepositional paraphrase (Warren, 1978). An
alternative is the copula paraphrase, as in office
building that/which is a skyscraper (right bracket-
ing), or a verbal paraphrase such as pain associated
with arthritis migraine (left).
Other researchers have used prepositional para-
phrases as a proxy for determining the semantic rela-
tions that hold between nouns in a compound (Lauer,
1995; Keller and Lapata, 2003; Girju et al, 2005).
Since most NCs have a prepositional paraphrase,
Lauer builds a model trying to choose between the
most likely candidate prepositions: of, for, in, at,
on, from, with and about (excluding like which is
mentioned by Warren). This could be problematic
though, since as a study by Downing (1977) shows,
when no context is provided, people often come up
with incompatible interpretations.
In contrast, we use paraphrases in order to make
syntactic bracketing assignments. Instead of trying
to manually decide the correct paraphrases, we can
issue queries using paraphrase patterns and find out
how often each occurs in the corpus. We then add
up the number of hits predicting a left versus a right
bracketing and compare the counts.
Unfortunately, search engines lack linguistic an-
notations, making general verbal paraphrases too ex-
pensive. Instead we used a small set of hand-chosen
paraphrases: associated with, caused by, contained
in, derived from, focusing on, found in, involved in,
located at/in, made of, performed by, preventing,
related to and used by/in/for. It is however feasi-
ble to generate queries predicting left/right brack-
eting with/without a determiner for every preposi-
tion.6 For the copula paraphrases we combine two
verb forms is and was, and three complementizers
that, which and who. These are optionally combined
with a preposition or a verb form, e.g. themes that
are used in science fiction.
4 Evaluation
4.1 Lauer?s Dataset
We experimented with the dataset from (Lauer,
1995), in order to produce results comparable to
those of Lauer and Keller & Lapata. The set consists
of 244 unambiguous 3-noun NCs extracted from
Grolier?s encyclopedia; however, only 216 of these
NCs are unique.
Lauer (1995) derived n-gram frequencies from
the Grolier?s corpus and tested the dependency and
the adjacency models using this text. To help combat
data sparseness issues he also incorporated a taxon-
omy and some additional information (see Related
Work section above). Lapata and Keller (2004) de-
rived their statistics from the Web and achieved re-
sults close to Lauer?s using simple lexical models.
4.2 Biomedical Dataset
We constructed a new set of noun compounds from
the biomedical literature. Using the Open NLP
6In addition to the articles (a, an, the), we also used quanti-
fiers (e.g. some, every) and pronouns (e.g. this, his).
21
tools,7 we sentence splitted, tokenized, POS tagged
and shallow parsed a set of 1.4 million MEDLINE
abstracts (citations between 1994 and 2003). Then
we extracted all 3-noun sequences falling in the last
three positions of noun phrases (NPs) found in the
shallow parse. If the NP contained other nouns, the
sequence was discarded. This allows for NCs which
are modified by adjectives, determiners, and so on,
but prevents extracting 3-noun NCs that are part of
longer NCs. For details, see (Nakov et al, 2005).
This procedure resulted in 418,678 different NC
types. We manually investigated the most frequent
ones, removing those that had errors in tokeniza-
tion (e.g., containing words like transplan or tation),
POS tagging (e.g., acute lung injury, where acute
was wrongly tagged as a noun) or shallow parsing
(e.g., situ hybridization, that misses in). We had to
consider the first 843 examples in order to obtain
500 good ones, which suggests an extraction accu-
racy of 59%. This number is low mainly because the
tokenizer handles dash-connected words as a single
token (e.g. factor-alpha) and many tokens contained
other special characters (e.g. cd4+), which cannot
be used in a query against a search engine and had
to be discarded.
The 500 NCs were annotated independently by
two judges, one of which has a biomedical back-
ground; the other one was one of the authors. The
problematic cases were reconsidered by the two
judges and after agreement was reached, the set con-
tained: 361 left bracketed, 69 right bracketed and
70 ambiguous NCs. The latter group was excluded
from the experiments.8
We calculated the inter-annotator agreement on
the 430 cases that were marked as unambiguous
after agreement. Using the original annotator?s
choices, we obtained an agreement of 88% or 82%,
depending on whether we consider the annotations,
that were initially marked as ambiguous by one of
the judges to be correct. The corresponding values
for the kappa statistics were .606 (substantial agree-
ment) and .442 (moderate agreement).
7http://opennlp.sourceforge.net/
8Two NCs can appear more than once but with a different
inflection or with a different word variant, e.g,. colon cancer
cells and colon carcinoma cells.
4.3 Experiments
The n-grams, surface features, and paraphrase
counts were collected by issuing exact phrase
queries, limiting the pages to English and request-
ing filtering of similar results.9 For each NC, we
generated all possible word inflections (e.g., tumor
and tumors) and alternative word variants (e.g., tu-
mor and tumour). For the biomedical dataset they
were automatically obtained from the UMLS Spe-
cialist lexicon.10 For Lauer?s set we used Carroll?s
morphological tools.11 For bigrams, we inflect only
the second word. Similarly, for a prepositional para-
phrase we generate all possible inflected forms for
the two parts, before and after the preposition.
4.4 Results and Discussion
The results are shown in Tables 1 and 2. As NCs
are left-bracketed at least 2/3rds of the time (Lauer,
1995), a straightforward baseline is to always as-
sign a left bracketing. Tables 1 and 2 suggest that
the surface features perform best. The paraphrases
are equally good on the biomedical dataset, but on
Lauer?s set their performance is lower and is compa-
rable to that of the dependency model.
The dependency model clearly outperforms the
adjacency one (as other researchers have found) on
Lauer?s set, but not on the biomedical set, where it
is equally good. ?2 barely outperforms #, but on the
biomedical set ?2 is a clear winner (by about 1.5%)
on both dependency and adjacency models.
The frequencies (#) outperform or at least rival the
probabilities on both sets and for both models. This
is not surprising, given the previous results by Lap-
ata and Keller (2004). Frequencies also outperform
Pr on the biomedical set. This may be due to the
abundance of single-letter words in that set (because
of terms like T cell, B cell, vitamin D etc.; similar
problems are caused by Roman digits like ii, iii etc.),
whose Web frequencies are rather unreliable, as they
are used by Pr but not by frequencies. Single-letter
words cause potential problems for the paraphrases
9In our experiments we used MSN Search statistics for the
n-grams and the paraphrases (unless the pattern contained a
?*?), and Google for the surface features. MSN always re-
turned exact numbers, while Google and Yahoo rounded their
page hits, which generally leads to lower accuracy (Yahoo was
better than Google for these estimates).
10http://www.nlm.nih.gov/pubs/factsheets/umlslex.html
11http://www.cogs.susx.ac.uk/lab/nlp/carroll/morph.html
22
Model
?
? ? P(%) C(%)
# adjacency 183 61 0 75.00 100.00
Pr adjacency 180 64 0 73.77 100.00
MI adjacency 182 62 0 74.59 100.00
?2 adjacency 184 60 0 75.41 100.00
# dependency 193 50 1 79.42 99.59
Pr dependency 194 50 0 79.51 100.00
MI dependency 194 50 0 79.51 100.00
?2 dependency 195 50 0 79.92 100.00
# adjacency (*) 152 41 51 78.76 79.10
# adjacency (**) 162 43 39 79.02 84.02
# adjacency (***) 150 51 43 74.63 82.38
# adjacency (*, rev.) 163 48 33 77.25 86.47
# adjacency (**, rev.) 165 51 28 76.39 88.52
# adjacency (***, rev.) 156 57 31 73.24 87.30
Concatenation adj. 175 48 21 78.48 91.39
Concatenation dep. 167 41 36 80.29 85.25
Concatenation triples 76 3 165 96.20 32.38
Inflection Variability 69 36 139 65.71 43.03
Swap first two words 66 38 140 63.46 42.62
Reorder 112 40 92 73.68 62.30
Abbreviations 21 3 220 87.50 9.84
Possessives 32 4 208 88.89 14.75
Paraphrases 174 38 32 82.08 86.89
Surface features (sum) 183 31 30 85.51 87.70
Majority vote 210 22 12 90.52 95.08
Majority vote? left 218 26 0 89.34 100.00
Baseline (choose left) 163 81 0 66.80 100.00
Table 1: Lauer Set. Shown are the numbers for cor-
rect (?), incorrect (?), and no prediction (?), fol-
lowed by precision (P, calculated over? and? only)
and coverage (C, % examples with prediction). We
use ??? for back-off to another model in case of ?.
as well, by returning too many false positives, but
they work very well with concatenations and dashes:
e.g., T cell is often written as Tcell.
As Table 4 shows, most of the surface features
that we predicted to be right-bracketing actually in-
dicated left. Overall, the surface features were very
good at predicting left bracketing, but unreliable for
right-bracketed examples. This is probably in part
due to the fact that they look for adjacent words, i.e.,
they act as a kind of adjacency model.
We obtained our best overall results by combining
the most reliable models, marked in bold in Tables
1, 2 and 4. As they have independent errors, we used
a majority vote combination.
Table 3 compares our results to those of Lauer
(1995) and of Lapata and Keller (2004). It is impor-
tant to note though, that our results are directly com-
parable to those of Lauer, while the Keller&Lapata?s
are not, since they used half of the Lauer set for de-
Model
?
? ? P(%) C(%)
# adjacency 374 56 0 86.98 100.00
Pr adjacency 353 77 0 82.09 100.00
MI adjacency 372 58 0 86.51 100.00
?2 adjacency 379 51 0 88.14 100.00
# dependency 374 56 0 86.98 100.00
Pr dependency 369 61 0 85.81 100.00
MI dependency 369 61 0 85.81 100.00
?2 dependency 380 50 0 88.37 100.00
# adjacency (*) 373 57 0 86.74 100.00
# adjacency (**) 358 72 0 83.26 100.00
# adjacency (***) 334 88 8 79.15 98.14
# adjacency (*, rev.) 370 59 1 86.25 99.77
# adjacency (**, rev.) 367 62 1 85.55 99.77
# adjacency (***, rev.) 351 79 0 81.63 100.00
Concatenation adj. 370 47 13 88.73 96.98
Concatenation dep. 366 43 21 89.49 95.12
Concatenation triple 238 37 155 86.55 63.95
Inflection Variability 198 49 183 80.16 57.44
Swap first two words 90 18 322 83.33 25.12
Reorder 320 78 32 80.40 92.56
Abbreviations 133 23 274 85.25 36.27
Possessives 48 7 375 87.27 12.79
Paraphrases 383 44 3 89.70 99.30
Surface features (sum) 382 48 0 88.84 100.00
Majority vote 403 17 10 95.95 97.67
Majority vote? right 410 20 0 95.35 100.00
Baseline (choose left) 361 69 0 83.95 100.00
Table 2: Biomedical Set.
velopment and the other half for testing.12 We, fol-
lowing Lauer, used everything for testing. Lapata &
Keller also used the AltaVista search engine, which
no longer exists in its earlier form. The table does
not contain the results of Girju et al (2005), who
achieved 83.10% accuracy, but used a supervised al-
gorithm and targeted bracketing in context. They
further ?shuffled? the Lauer?s set, mixing it with ad-
ditional data, thus making their results even harder
to compare to these in the table.
Note that using page hits as a proxy for n-gram
frequencies can produce some counter-intuitive re-
sults. Consider the bigrams w1w4, w2w4 and w3w4
and a page that contains each bigram exactly once.
A search engine will contribute a page count of 1 for
w4 instead of a frequency of 3; thus the page hits
for w4 can be smaller than the page hits for the sum
of the individual bigrams. See Keller and Lapata
(2003) for more issues.
12In fact, the differences are negligible; their system achieves
pretty much the same result on the half split as well as on the
whole set (personal communication).
23
Model Acc. %
LEFT (baseline) 66.80
Lauer adjacency 68.90
Lauer dependency 77.50
Our ?2 dependency 79.92
Lauer tuned 80.70
?Upper bound? (humans - Lauer) 81.50
Our majority vote? left 89.34
Keller&Lapata: LEFT (baseline) 63.93
Keller&Lapata: best BNC 68.03
Keller&Lapata: best AltaVista 78.68
Table 3: Comparison to previous unsupervised
results on Lauer?s set. The results of Keller & La-
pata are on half of Lauer?s set and thus are only in-
directly comparable (note the different baseline).
5 Conclusions and Future Work
We have extended and improved upon the state-of-
the-art approaches to NC bracketing using an un-
supervised method that is more robust than Lauer
(1995) and more accurate than Lapata and Keller
(2004). Future work will include testing on NCs
consisting of more than 3 nouns, recognizing the
ambiguous cases, and bracketing NPs that include
determiners and modifiers. We plan to test this ap-
proach on other important NLP problems.
As mentioned above, NC bracketing should be
helpful for semantic interpretation. Another possi-
ble application is the refinement of parser output.
Currently, NPs in the Penn TreeBank are flat, with-
out internal structure. Absent any other information,
probabilistic parsers typically assume right bracket-
ing, which is incorrect about 2/3rds of the time for
3-noun NCs. It may be useful to augment the Penn
TreeBank with dependencies inside the currently flat
NPs, which may improve their performance overall.
Acknowledgements We thank Dan Klein, Frank
Keller and Mirella Lapata for valuable comments,
Janice Hamer for the annotations, and Mark Lauer
for his dataset. This research was supported by NSF
DBI-0317510, and a gift from Genentech.
References
Pamela Downing. 1977. On the creation and use of english
compound nouns. Language, (53):810?842.
R. Girju, D. Moldovan, M. Tatu, and D. Antohe. 2005. On the
semantics of noun compounds. Journal of Computer Speech
and Language - Special Issue on Multiword Expressions.
Example Predicts Accuracy Coverage
brain-stem cells left 88.22 92.79
brain stem?s cells left 91.43 16.28
(brain stem) cells left 96.55 6.74
brain stem (cells) left 100.00 1.63
brain stem, cells left 96.13 42.09
brain stem: cells left 97.53 18.84
brain stem cells-death left 80.69 60.23
brain stem cells/tissues left 83.59 45.35
brain stem Cells left 90.32 36.04
brain stem/cells left 100.00 7.21
brain. stem cells left 97.58 38.37
brain stem-cells right 25.35 50.47
brain?s stem cells right 55.88 7.90
(brain) stem cells right 46.67 3.49
brain (stem cells) right 0.00 0.23
brain, stem cells right 54.84 14.42
brain: stem cells right 44.44 6.28
rat-brain stem cells right 17.97 68.60
neural/brain stem cells right 16.36 51.16
brain Stem cells right 24.69 18.84
brain/stem cells right 53.33 3.49
brain stem. cells right 39.34 14.19
Table 4: Surface features analysis (%s), run over
the biomedical set.
Frank Keller and Mirella Lapata. 2003. Using the Web to
obtain frequencies for unseen bigrams. Computational Lin-
guistics, 29:459?484.
Mirella Lapata and Frank Keller. 2004. The Web as a base-
line: Evaluating the performance of unsupervised Web-
based models for a range of NLP tasks. In Proceedings of
HLT-NAACL, pages 121?128, Boston.
Mark Lauer. 1995. Designing Statistical Language Learners:
Experiments on Noun Compounds. Ph.D. thesis, Department
of Computing Macquarie University NSW 2109 Australia.
Mitchell Marcus. 1980. A Theory of Syntactic Recognition for
Natural Language. MIT Press.
Preslav Nakov, Ariel Schwartz, Brian Wolf, and Marti Hearst.
2005. Scaling up BioNLP: Application of a text annotation
architecture to noun compound bracketing. In Proceedings
of SIG BioLINK.
James Pustejovsky, Peter Anick, and Sabine Bergler. 1993.
Lexical semantic techniques for corpus analysis. Compu-
tational Linguistics, 19(2):331?358.
Philip Resnik. 1993. Selection and information: a class-based
approach to lexical relationships. Ph.D. thesis, University
of Pennsylvania, UMI Order No. GAX94-13894.
Beatrice Warren. 1978. Semantic patterns of noun-noun com-
pounds. In Gothenburg Studies in English 41, Goteburg,
Acta Universtatis Gothoburgensis.
Y. Yang and J. Pedersen. 1997. A comparative study on feature
selection in text categorization. In Proceedings of ICML?97),
pages 412?420.
24
Proceedings of the Second Workshop on Statistical Machine Translation, pages 212?215,
Prague, June 2007. c?2007 Association for Computational Linguistics
UCB System Description for the WMT 2007 Shared Task
Preslav Nakov
EECS, CS division
University of California at Berkeley
Berkeley, CA 94720
nakov@cs.berkeley.edu
Marti Hearst
School of Information
University of California at Berkeley
Berkeley, CA 94720
hearst@ischool.berkeley.edu
Abstract
For the WMT 2007 shared task, the UC
Berkeley team employed three techniques of
interest. First, we used monolingual syntac-
tic paraphrases to provide syntactic variety
to the source training set sentences. Sec-
ond, we trained two language models: a
small in-domain model and a large out-of-
domain model. Finally, we made use of re-
sults from prior research that shows that cog-
nate pairs can improve word alignments. We
contributed runs translating English to Span-
ish, French, and German using various com-
binations of these techniques.
1 Introduction
Modern Statistical Machine Translation (SMT) sys-
tems are trained on aligned sentences of bilingual
corpora, typically from one domain. When tested on
text from that same domain, such systems demon-
strate state-of-the art performance; however, on
out-of-domain text the results can get significantly
worse. For example, on the WMT 2006 Shared
Task evaluation, the French to English translation
BLEU scores dropped from about 30 to about 20 for
nearly all systems, when tested on News Commen-
tary rather than Europarl (Koehn and Monz, 2006).
Therefore, this year the shared task organizers
have provided 1M words of bilingual News Com-
mentary training data in addition to the Europarl
data (about 30M words), thus challenging the par-
ticipants to experiment with domain adaptation.
Below we describe our domain adaptation exper-
iments, trying to achieve better results on the News
Commentary data. In addition to training on both
data sets, we make use of monolingual syntactic
paraphrases of the English side of the data.
2 Monolingual Syntactic Paraphrasing
In many cases, the testing text contains ?phrases?
that are equivalent, but syntactically different from
the phrases learned on training, and the potential for
a high-quality translation is missed. We address this
problem by using nearly equivalent syntactic para-
phrases of the original sentences. Each paraphrased
sentence is paired with the foreign translation that is
associated with the original sentence in the training
data. This augmented training corpus can then be
used to train an SMT system. Alternatively, we can
paraphrase the test sentences making them closer to
the target language syntax.
Given an English sentence, we parse it with the
Stanford parser (Klein and Manning, 2003) and then
generate paraphrases using the following syntactic
transformations:
1. [NP NP1 P NP2]? [NP NP2 NP1].
inequality in income? income inequality.
2. [NP NP1 of NP2]? [NP NP2 poss NP1].
inequality of income? income?s inequality.
3. NPposs ? NP.
income?s inequality? income inequality.
4. NPposs ? NPPPof .
income?s inequality? inequality of income.
5. NPNC ? NPposs.
income inequality? income?s inequality.
6. NPNC ? NPPP .
income inequality? inequality in incomes.
212
Sharply rising income inequality has raised the stakes of the economic game .
Sharply rising income inequality has raised the economic game ?s stakes .
Sharply rising income inequality has raised the economic game stakes .
Sharply rising inequality of income has raised the stakes of the economic game .
Sharply rising inequality of income has raised the economic game ?s stakes .
Sharply rising inequality of income has raised the economic game stakes .
Sharply rising inequality of incomes has raised the stakes of the economic game .
Sharply rising inequality of incomes has raised the economic game ?s stakes .
Sharply rising inequality of incomes has raised the economic game stakes .
Sharply rising inequality in income has raised the stakes of the economic game .
Sharply rising inequality in income has raised the economic game ?s stakes .
Sharply rising inequality in income has raised the economic game stakes .
Sharply rising inequality in incomes has raised the stakes of the economic game .
Sharply rising inequality in incomes has raised the economic game ?s stakes .
Sharply rising inequality in incomes has raised the economic game stakes .
Table 1: Sample sentence and automatically generated paraphrases. Paraphrased NCs are in italics.
7. remove that where optional
I think that he is right? I think he is right.
8. add that where optional
I think he is right? I think that he is right.
where:
poss possessive marker: ? or ?s;
P preposition;
NPPP NP with internal PP-attachment;
NPPPof NP with internal PP headed by of;
NPposs NP with internal possessive marker;
NPNC NP that is a Noun Compound.
While the first four and the last two transfor-
mations are purely syntactic, (5) and (6) are not.
The algorithm must determine whether a possessive
marker is feasible for (5) and must choose the cor-
rect preposition for (6). In either case, for noun com-
pounds (NCs) of length 3 or more, it also needs to
choose the position to modify, e.g., inquiry?s com-
mittee chairman vs. inquiry committee?s chairman.
In order to ensure accuracy of the paraphrases,
we use statistics gathered from the Web, using a
variation of the approaches presented in Lapata and
Keller (2004) and Nakov and Hearst (2005). We use
patterns to generate possible prepositional or copula
paraphrases in the context of the preceding and the
following word in the sentence, First we split the
NC into two parts N1 and N2 in all possible ways,
e.g., beef import ban lifting would be split as: (a)
N1=?beef?, N2=?import ban lifting?, (b) N1=?beef
import?, N2=?ban lifting?, and (c) N1=?beef import
ban?, N2=?lifting?. For every split, we issue exact
phrase queries to the Google search engine using
the following patterns:
"lt N1 poss N2 rt"
"lt N2 prep det N ?1 rt"
"lt N2 that be det N ?1 rt"
"lt N2 that be prep det N ?1 rt"
where: lt is the word preceding N1 in the original
sentence or empty if none, rt is the word following
N2 in the original sentence or empty if none, poss
is a possessive marker (?s or ?), that is that, which
or who, be is is or are, det is a determiner (the, a,
an, or none), prep is one of the 8 prepositions used
by Lauer (1995) for semantic interpretation of NCs:
about, at, for, from, in, of, on, and with, and N ?1 can
be either N1, or N1 with the number of its last word
changed from singular/plural to plural/singular.
For all splits, we collect the number of page hits
for each instantiation of each pattern, filtering out
the paraphrases whose page hit count is less than 10.
We then calculate the total number of page hitsH for
all paraphrases (for all splits and all patterns), and
retain those ones whose page hits count is at least
10% of H . Note that this allows for multiple para-
phrases of an NC. If no paraphrases are retained, we
213
repeat the above procedure with lt set to the empty
string. If there are still no good paraphrases, we set
the rt to the empty string. If this does not help ei-
ther, we make a final attempt, by setting both lt and
rt to the empty string.
Table 1 shows the paraphrases for a sample sen-
tence. We can see that income inequality is para-
phrased as inequality of income, inequality of in-
comes, inequality in income and inequality in in-
comes; also economic game?s stakes becomes eco-
nomic game stakes and stakes of the economic game.
3 Experiments
Table 2 shows a summary of our submissions: the
official runs are marked with a ?. For our experi-
ments, we used the baseline system, provided by the
organizers, which we modified in different ways, as
described below.
3.1 Domain Adaptation
All our systems were trained on both corpora.
? Language models. We used two language
models (LM) ? a small in-domain one (trained
onNews Commentary) and a big out-of-domain
one (trained on Europarl). For example, for EN
? ES (from English to Spanish), on the low-
ercased tuning data set, using in-domain LM
only achieved a BLEU of 0.332910, while us-
ing both LMs yielded 0.354927, a significant
effect.
? Cognates. Previous research has found that
using cognates can help get better word align-
ments (and ultimately better MT results), espe-
cially in case of a small training set. We used
the method described in (Kondrak et al, 2003)
in order to extract cognates from the two data
sets. We then added them as sentence pairs to
the News Commentary corpus before training
the word alignment models1 for ucb3, ucb4 and
ucb5.
1Following (Kondrak et al, 2003), we considered words of
length 4 or more, we required the length ratio to be between
7
10 and
10
7 , and we accepted as potential cognates all pairs for
which the longest common subsequence ratio (LCSR) was 0.58
or more. We repeated 3 times the cognate pairs extracted from
the Europarl, and 4 times the ones from News Commentary.
? Phrases. The ucb5 system uses the Europarl
data in order to learn an additional phrase ta-
ble and an additional lexicalized re-ordering
model.
3.2 Paraphrasing the Training Set
In two of our experiments (ucb3, ucb4 and ucb5),
we used a paraphrased version of the training News
Commentary data, using all rules (1)-(8). We trained
two separate MT systems: one on the original cor-
pus, and another one on the paraphrased version.
We then used both resulting lexicalized re-ordering
models and a merged phrase table with extra para-
meters: if a phrase appeared in both phrase tables,
it now had 9 instead of 5 parameters (4 from each
table, plus a phrase penalty), and if it was in one
of the phrase tables only, the 4 missing parameters
were filled with 1e-40.
The ucb5 system is also trained on Europarl,
yielding a third lexicalized re-ordering model and
adding 4 new parameters to the phrase table entries.
Unfortunately, longer sentences (up to 100 to-
kens, rather than 40), longer phrases (up to 10 to-
kens, rather than 7), two LMs (rather than just
one), higher-order LMs (order 7, rather than 3),
multiple higher-order lexicalized re-ordering mod-
els (up to 3), etc. all contributed to increased sys-
tem?s complexity, and, as a result, time limitations
prevented us from performing minimum-error-rate
training (MERT) (Och, 2003) for ucb3, ucb4 and
ucb5. Therefore, we used the MERT parameter val-
ues from ucb1 instead, e.g. the first 4 phrase weights
of ucb1 were divided by two, copied twice and used
in ucb3 as the first 8 phrase-table parameters. The
extra 4 parameters of ucb5 came from training a sep-
arate MT system on the Europarl data (scaled ac-
cordingly).
3.3 Paraphrasing the Test Set
In some of our experiments (ucb2 and ucb4), given
a test sentence, we generated the single most-likely
paraphrase, which makes it syntactically closer to
Spanish and French. Unlike English, which makes
extensive use of noun compounds, these languages
strongly prefer connecting the nouns with a preposi-
tion (and less often turning a noun into an adjective).
Therefore, we paraphrased all NCs using preposi-
tions, by applying rules (4) and (6). In addition, we
214
Languages System LM size Paraphrasing Cognates? Extra phrases MERT
News Europarl train? test? Europarl finished?
EN? ES ucb1? 3 5 +
ucb2 3 5 + +
ucb3 5 7 + +
ucb4 5 7 + + +
ucb5 5 7 + + +
EN? FR ucb3 5 7 + +
ucb4? 5 7 + + +
EN? DE ucb1? 5 7 + +
ucb2 5 7 + + +
Table 2: Summary of our submissions. All runs are for the News Commentary test data. The official
submissions are marked with a star.
applied rule (8), since its Spanish/French equivalent
que (as well as the German da?) is always obliga-
tory. These transformations affected 927 out of the
2007 test sentences. We also used this transformed
data set when translating to German (however, Ger-
man uses NCs as much as English does).
3.4 Other Non-standard Settings
Below we discuss some non-standard settings that
differ from the ones suggested by the organizers in
their baseline system. First, following Birch et al
(2006), who found that higher-order LMs give bet-
ter results2, we used a 5-gram LM for News Com-
mentary, and 7-gram LM for Europarl (as opposed
to 3-gram, as done normally). Second, for all runs
we trained our systems on all sentences of length up
to 100 (rather than 40, as suggested in the baseline
system). Third, we used a maximum phrase length
limit of 10 (rather than 7, as typically done). Fourth,
we used both a lexicalized and distance-based re-
ordering models (as opposed to lexicalized only, as
in the baseline system). Finally, while we did not
use any resources other than the ones provided by
the shared task organizers, we made use of Web fre-
quencies when paraphrasing the training corpus, as
explained above.
4 Conclusions and Future Work
We have presented various approaches to domain
adaptation and their combinations. Unfortunately,
2They used a 5-gram LM trained on Europarl, but we
pushed the idea further, using a 7-gram LM with a Kneser-Ney
smoothing.
computational complexity and time limitations pre-
vented us from doing proper MERT for the interest-
ing more complex systems. We plan to do a proper
MERT training and to study the impact of the indi-
vidual components in isolation.
Acknowledgements: This work supported in part
by NSF DBI-0317510.
References
Alexandra Birch, Chris Callison-Burch, Miles Osborne, and
Philipp Koehn. 2006. Constraining the phrase-based, joint
probability statistical translation model. In Proc. of Work-
shop on Statistical Machine Translation, pages 154?157.
Dan Klein and Christopher Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of ACL ?03.
Philipp Koehn and Christof Monz. 2006. Manual and auto-
matic evaluation of machine translation between european
languages. In Proceedings on the Workshop on Statistical
Machine Translation, pages 102?121.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight. 2003.
Cognates can improve statistical translation models. In Pro-
ceedings of NAACL, pages 46?48.
Mirella Lapata and Frank Keller. 2004. The web as a base-
line: Evaluating the performance of unsupervised web-based
models for a range of nlp tasks. In Proceedings of HLT-
NAACL ?04.
Mark Lauer. 1995. Corpus statistics meet the noun compound:
some empirical results. In Proceedings of ACL ?95.
Preslav Nakov and Marti Hearst. 2005. Search engine statistics
beyond the n-gram: Application to noun compound bracket-
ing. In Proceedings of CoNLL ?05.
Franz Josef Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL, pages
160?167.
215
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 13?18,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 04:
Classification of Semantic Relations between Nominals
Roxana Girju
Univ. of Illinois
at Urbana-Champaign
Urbana, IL 61801
girju@uiuc.edu
Preslav Nakov
Univ. of California at Berkeley
Berkeley, CA 94720
nakov@cs.berkeley.edu
Vivi Nastase
EML Research gGmbH
Heidelberg, Germany 69118
nastase@eml-research.de
Stan Szpakowicz
University of Ottawa
Ottawa, ON K1N 6N5
szpak@site.uottawa.ca
Peter Turney
National Research Council of Canada
Ottawa, ON K1A 0R6
peter.turney@nrc-cnrc.gc.ca
Deniz Yuret
Koc? University
Istanbul, Turkey 34450
dyuret@ku.edu.tr
Abstract
The NLP community has shown a renewed
interest in deeper semantic analyses, among
them automatic recognition of relations be-
tween pairs of words in a text. We present an
evaluation task designed to provide a frame-
work for comparing different approaches to
classifying semantic relations between nom-
inals in a sentence. This is part of SemEval,
the 4th edition of the semantic evaluation
event previously known as SensEval. We de-
fine the task, describe the training/test data
and their creation, list the participating sys-
tems and discuss their results. There were
14 teams who submitted 15 systems.
1 Task Description and Related Work
The theme of Task 4 is the classification of semantic
relations between simple nominals (nouns or base
noun phrases) other than named entities ? honey
bee, for example, shows an instance of the Product-
Producer relation. The classification occurs in the
context of a sentence in a written English text. Al-
gorithms for classifying semantic relations can be
applied in information retrieval, information extrac-
tion, text summarization, question answering and so
on. The recognition of textual entailment (Tatu and
Moldovan, 2005) is an example of successful use of
this type of deeper analysis in high-end NLP appli-
cations.
The literature shows a wide variety of methods
of nominal relation classification. They depend as
much on the training data as on the domain of ap-
plication and the available resources. Rosario and
Hearst (2001) classify noun compounds from the
domain of medicine, using 13 classes that describe
the semantic relation between the head noun and
the modifier in a given noun compound. Rosario
et al (2002) classify noun compounds using the
MeSH hierarchy and a multi-level hierarchy of se-
mantic relations, with 15 classes at the top level.
Nastase and Szpakowicz (2003) present a two-level
hierarchy for classifying noun-modifier relations in
base noun phrases from general text, with 5 classes
at the top and 30 classes at the bottom; other re-
searchers (Turney and Littman, 2005; Turney, 2005;
Nastase et al, 2006) have used their class scheme
and data set. Moldovan et al (2004) propose a 35-
class scheme to classify relations in various phrases;
the same scheme has been applied to noun com-
pounds and other noun phrases (Girju et al, 2005).
Chklovski and Pantel (2004) introduce a 5-class set,
designed specifically for characterizing verb-verb
semantic relations. Stephens et al (2001) propose
17 classes targeted to relations between genes. La-
pata (2002) presents a binary classification of rela-
tions in nominalizations.
There is little consensus on the relation sets and
algorithms for analyzing semantic relations, and it
seems unlikely that any single scheme could work
for all applications. For example, the gene-gene re-
lation scheme of Stephens et al (2001), with rela-
tions like X phosphorylates Y, is unlikely to be trans-
ferred easily to general text.
We have created a benchmark data set to allow the
evaluation of different semantic relation classifica-
tion algorithms. We do not presume to propose a sin-
gle classification scheme, however alluring it would
13
Relation Training data Test data Agreement Example
positive set size positive set size (independent tagging)
Cause-Effect 52.1% 140 51.3% 80 86.1% laugh (cause) wrinkles (effect)
Instrument-Agency 50.7% 140 48.7% 78 69.6% laser (instrument) printer (agency)
Product-Producer 60.7% 140 66.7% 93 68.5% honey (product) bee (producer)
Origin-Entity 38.6% 140 44.4% 81 77.8% message (entity) from outer-space (origin)
Theme-Tool 41.4% 140 40.8% 71 47.8% news (theme) conference(tool)
Part-Whole 46.4% 140 36.1% 72 73.2% the door (part) of the car (whole)
Content-Container 46.4% 140 51.4% 74 69.1% the apples (content) in the basket (container)
Table 1: Data set statistics
be to try to design a unified standard ? it would be
likely to have shortcomings just as any of the others
we have just reviewed. Instead, we have decided to
focus on separate semantic relations that many re-
searchers list in their relation sets. We have built an-
notated data sets for seven such relations. Every data
set supports a separate binary classification task.
2 Building the Annotated Data Sets
Ours is a new evaluation task, so we began with data
set creation and annotation guidelines. The data set
that Nastase and Szpakowicz (2003) created had re-
lation labels and part-of-speech and WordNet sense
annotations, to facilitate classification. (Moldovan
et al, 2004; Girju et al, 2005) gave the annotators
an example of each phrase in a sentence along with
WordNet senses and position of arguments. Our
annotations include all these, to support a variety
of methods (since we work with relations between
nominals, the part of speech is always noun). We
have used WordNet 3.0 on the Web and sense index
tags.
We chose the following semantic relations:
Cause-Effect, Content-Container, Instrument-
Agency, Origin-Entity, Part-Whole, Product-
Producer and Theme-Tool. We wrote seven detailed
definitions, including restrictions and conventions,
plus prototypical positive and near-miss negative
examples. For each relation separately, we based
data collection on wild-card search patterns that
Google allows. We built the patterns manually,
following Hearst (1992) and Nakov and Hearst
(2006). Instances of the relation Content-Container,
for example, come up in response to queries such as
?* contains *?, ?* holds *?, ?the * in the *?. Fol-
lowing the model of the Senseval-3 English Lexical
Sample Task, we set out to collect 140 training and
at least 70 test examples per relation, so we had a
number of different patterns to ensure variety. We
also aimed to collect a balanced number of positive
and negative examples. The use of heuristic patterns
to search for both positive and negative examples
should naturally result in negative examples that
are near misses. We believe that near misses are
more useful for supervised learning than negative
examples that are generated randomly.
?Among the contents of the <e1>vessel</e1>
were a set of carpenter?s <e2>tools</e2>, sev-
eral large storage jars, ceramic utensils, ropes and
remnants of food, as well as a heavy load of ballast
stones.?
WordNet(e1) = ?vessel%1:06:00::?,
WordNet(e2) = ?tool%1:06:00::?,
Content-Container(e2, e1) = ?true?,
Query = ?contents of the * were a?
Figure 1: Annotations illustrated
Figure 1 illustrates the annotations. We tag the
nominals, so parsing or chunking is not necessary.
For Task 4, we define a nominal as a noun or base
noun phrase, excluding names entities. A base noun
phrase, e.g., lawn or lawn mower, is a noun with pre-
modifiers. We also exclude complex noun phrases
(e.g., with attached prepositional phrases ? the en-
gine of the lawn mower).
The procedure was the same for each relation.
One person gathered the sample sentences (aim-
ing approximately for a similar number of positive
and negative examples) and tagged the entities; two
other people annotated the sentences with WordNet
senses and classified the relations. The detailed re-
lation definitions and the preliminary discussions of
positive and negative examples served to maximize
the agreement between the annotators. They first
classified the data independently, then discussed ev-
ery disagreement and looked for consensus. Only
the agreed-upon examples went into the data sets.
Next, we split each data set into 140 training and
no fewer than 70 test examples. (We published the
training set for the Content-Container relation as de-
velopment data two months before the test set.) Ta-
ble 1 shows the number of positive and negative ex-
14
amples for each relation.1
The average inter-annotator agreement on rela-
tions (true/false) after the independent annotation
step was 70.3%, and the average agreement on
WordNet sense labels was 71.9%. In the process of
arriving at a consensus between annotators, the def-
inition of each relation was revised to cover explic-
itly cases where there had been disagreement. We
expect that these revised definitions would lead to
much higher levels of agreement than the original
definitions did.
3 The Participants
The task of classifying semantic relations between
nominals has attracted the participation of 14 teams
who submitted 15 systems. Table 4 lists the sys-
tems, the authors and their affiliations, and brief de-
scriptions. The systems? performance information
in terms of precision, recall, F -measure and accu-
racy, macroaveraged over all relations, appears in
Table 3. We computed these measures as described
in Lewis (1991).
We distinguish four categories of systems based
on the type of information used ? WordNet senses
and/or Google queries:
A ? WordNet = NO & Query = NO;
B ? WordNet = YES & Query = NO;
C ? WordNet = NO & Query = YES;
D ? WordNet = YES & Query = YES.
WordNet = ?YES? or WordNet = ?NO? tells us
only whether a system uses the WordNet sense la-
bels in the data sets. A system may use WordNet
internally for varied purposes, but ignore our sense
labels; such a system would be in category A or C .
Based on the input variation, each submitted system
may have up to 4 variations ? A,B,C,D.
Table 2 presents three baselines for a relation.
Majority always guesses either ?true? or ?false?,
whichever is the majority in the test set (maximizes
accuracy). Alltrue always guesses ?true? (maxi-
mizes recall). Probmatch randomly guesses ?true?
(?false?) with the probability matching the distribu-
tion of ?true? (?false?) in the test dataset (balances
precision and recall).
We present the results in Table 3 grouped by cat-
egory, to facilitate system comparison.
1As this paper serves also as a documentation of the data set,
the order of relations in the table is the same as in the data set.
Type P R F Acc
majority 81.3 42.9 30.8 57.0
alltrue 48.5 100.0 64.8 48.5
probmatch 48.5 48.5 48.5 51.7
Table 2: Baselines: precision, recall, F -measure and
accuracy averaged over the 7 binary classifications.
Team P R F Acc
A ? WordNet = NO & Query = NO
UCD-FC 66.1 66.7 64.8 66.0
ILK 60.5 69.5 63.8 63.5
UCB? 62.7 63.0 62.7 65.4
UMELB-B 61.5 55.7 57.8 62.7
UTH 56.1 57.1 55.9 58.8
UC3M 48.2 40.3 43.1 49.9
avg?stdev 59.2?6.3 58.7?10.5 58.0?8.1 61.1?6.0
B ? WordNet = YES & Query = NO
UIUC? 79.7 69.8 72.4 76.3
FBK-IRST 70.9 73.4 71.8 72.9
ILK 72.8 70.6 71.5 73.2
UCD-S1 69.9 64.6 66.8 71.4
UCD-PN 62.0 71.7 65.4 67.0
UC3M 66.7 62.8 64.3 67.2
CMU-AT 55.7 66.7 60.4 59.1
UCD-FC 66.4 58.1 60.3 63.6
UMELB-A 61.7 56.8 58.7 62.5
UVAVU 56.8 56.3 56.1 57.7
LCC-SRN 55.9 57.8 51.4 53.7
avg ? stdev 65.3?7.7 64.4?6.5 63.6?6.9 65.9?7.2
C ? WordNet = NO & Query = YES
UCB? 64.2 66.5 65.1 67.0
UCD-FC 66.1 66.7 64.8 66.0
UC3M 49.4 43.9 45.3 50.1
avg?stdev 59.9?9.1 59.0?13.1 58.4?11.3 61.0?9.5
D ? WordNet = YES & Query = YES
UTD-HLT-CG 67.3 65.3 62.6 67.2
UCD-FC 66.4 58.1 60.3 63.6
UC3M 60.9 57.8 58.8 62.3
avg?stdev 64.9?3.5 60.4?4.2 60.6?1.9 64.4?2.5
Systems tagged with ? have a Task 4 organizer as part of the team.
Table 3: System performance grouped by category.
Precision, recall, F -measure and accuracy macro-
averaged over each system?s performance on all 7
relations.
4 Discussion
The highest average accuracy on Task 4 was 76.3%.
Therefore, the average initial agreement between an-
notators (70.3%), before revising the definitions, is
not an upper bound on the accuracy that can be
achieved. That the initial agreement between anno-
tators is not a good indicator of the accuracy that can
be achieved is also supported by the low correlation
15
System Institution Team Description System Type
UVAVU Univ. of Amsterdam
TNO Science & Industry
Free Univ. Amsterdam
Sophia Katrenko
Willem Robert van
Hage
similarity measures in WordNet; syn-
tactic dependencies; lexical patterns;
logical combination of attributes
B
CMU -AT Carnegie Mellon Univ. Alicia Tribble
Scott E. Fahlman
WordNet; manually-built ontologies;
Scone Knowledge Representation Lan-
guage; semantic distance
B
ILK Tilburg University Caroline Sporleder
Roser Morante
Antal van den Bosch
semantic clusters based on noun simi-
larity; WordNet supersenses; grammat-
ical relation between entities; head of
sentence; WEKA
A, B
FBK-IRST Fondazione Bruno
Kessler - IRST
Claudio Giuliano
Alberto Lavelli
Daniele Pighin
Lorenza Romano
shallow and deep syntactic information;
WordNet synsets and hypernyms; ker-
nel methods; SVM
B
LCC-SRN Language Computer
Corp.
Adriana Badulescu named entity recognition; lexical, se-
mantic, syntactic features; decision tree
and semantic scattering
B
UMELB-A Univ. of Melbourne Su Kim
Timothy Baldwin
sense collocations; similarity of con-
stituents; extending training and testing
data using similar words
B
UMELB-B Univ. of Melbourne Su Kim
Timothy Baldwin
similarity of nearest-neighbor matching
over the union of senses for the two
nominals; cascaded tagging with de-
creasing thresholds
A
UCB? Univ. of California at
Berkeley
Preslav Nakov
Marti Hearst
VSM; joining terms; KNN-1 A, C
UC3M Univ. Carlos III of Madrid Isabel Segura Bedmar
Doaa Sammy
Jose? Luis Mart??nez
Ferna?ndez
WordNet path; syntactic features; SVM A, B, C, D
UCD-S1 Univ. College Dublin Cristina Butnariu
Tony Veale
lexical-semantic categories from Word-
Net; syntactic patterns from corpora,
SVM
B
UCD-FC Univ. College Dublin Fintan Costello WordNet; additional noun compounds
tagged corpus; Naive Bayes
A, B, C, D
UCD-PN Univ. College Dublin Paul Nulty WordNet supersenses; web-based fre-
quency counts for specific joining
terms; WEKA (SMO)
B
UIUC? Univ. of Illinois at Urbana
Champaign
Roxana Girju
Brandon Beamer
Suma Bhat
Brant Chee
Andrew Fister
Alla Rozovskaya
features based on WordNet, NomLex-
PLUS, grammatical roles, lexico-
syntactic patterns, semantic parses
B
UTD-HLT-CG Univ. of Texas at Dallas Cristina Nicolae
Garbiel Nicolae
Sanda Harabagiu
lexico-semantic features from Word-
Net, VerbNet; semantic features from a
PropBank parser; dependency features
D
UTH Univ. of Tokio Eiji Aramaki
Takeshi Imai
Kengo Miyo
Kazuhiko Ohe
joining phrases; physical size for enti-
ties; web-mining; SVM
A
Systems tagged with ? have a Task 4 organizer as part of the team.
Table 4: Short description of the teams and the participating systems.
16
Relation Team Type P R F Acc Test size Base-F Base-Acc Avg. rank
Cause-Effect UIUC B4 69.5 100.0 82.0 77.5 80 67.8 51.2 3.4
Instrument-Agency FBK-IRST B4 76.9 78.9 77.9 78.2 78 65.5 51.3 3.4
Product-Producer UCD-S1 B4 80.6 87.1 83.7 77.4 93 80.0 66.7 1.7
Origin-Entity ILK B3 70.6 66.7 68.6 72.8 81 61.5 55.6 6.0
Theme-Tool ILK B4 69.0 69.0 69.0 74.6 71 58.0 59.2 6.0
Part-Whole UC3M B4 72.4 80.8 76.4 81.9 72 53.1 63.9 4.5
Content-Container UIUC B4 93.1 71.1 80.6 82.4 74 67.9 51.4 3.1
Table 5: The best results per relation. Precision, recall, F -measure and accuracy macro-averaged over each
system?s performance on all 7 relations. Base-F shows the baseline F -measure (alltrue), Base-Acc ? the
baseline accuracy score (majority). The last column shows the average rank for each relation.
of 0.15 between the Acc column in Table 5 and the
Agreement column in Table 1.
We performed various analyses of the results,
which we summarize here in four questions. We
write Xi to refer to four possible system categories
(Ai, Bi, Ci, and Di) with four possible amounts of
training data (X1 for training examples 1 to 35, X2
for 1 to 70, X3 for 1 to 105, and X4 for 1 to 140).
Does more training data help?
Overall, the results suggest that more training data
improves the performance. There were 17 cases in
which we had results for all four possible amounts
of training data. All average F -measure differences,
F (X4)?F (Xi) where X = A to D, i = 1 to 3, for
these 17 sets of results are statistically significant:
F (X4)?F (X1): N = 17, avg = 8.3, std = 5.8, min =
1.1, max = 19.6, t-value = ?5.9, p-value = 0.00001.
F (X4)?F (X2): N = 17, avg = 4.0, std = 3.7, min =
?3.5, max = 10.5, t-value = 4.5, p-value = 0.0002.
F (X4)?F (X3): N = 17, avg = 0.9, std = 1.7, min =
?2.6, max = 4.7, t-value = 2.1, p-value = 0.03.
Does WordNet help?
The statistics show that WordNet is important, al-
though the contribution varies across systems. Three
teams submitted altogether 12 results both for A1?
A4 and B1?B4. The average F -measure difference,
F (Bi)?F (Ai), i = 1 to 4, is significant:
F (Bi)?F (Ai): N = 12, avg = 6.1, std = 8.4, min =
?4.5, max = 21.2, t-value = ?2.5, p-value = 0.01.
The results of the UCD-FC system actually went
down when WordNet was used. The statistics for the
remaining two teams, however, are a bit better:
F (Bi)?F (Ai): N = 8, avg = 10.4, std = 6.7, min =
?1.0, max = 21.2, t-value = ?4.4, p-value = 0.002.
Does knowing the query help?
Overall, knowing the query did not seem to improve
the results. Three teams submitted 12 results both
for A1?A4 and C1?C4. The average F -measure dif-
ference, F (Ci)?F (Ai) , i = 1 to 4, is not significant:
F (Ci)?F (Ai): N = 12, avg = 0.9, std = 1.8, min =
?2.0, max = 5.0, t-value = ?1.6, p-value = 0.06.
Again, the UCD-FC system differed from the
other systems in that the A and C scores were iden-
tical, but even averaging over the remaining two sys-
tems and 8 cases does not show a statistically signif-
icant advantage:
F (Ci)?F (Ai): N = 8, avg = 1.3, std = 2.2, min =
?2.0, max = 5.0, t-value = ?1.7, p-value = 0.07.
Are some relations harder to classify?
Table 5 shows the best results for each relation in
terms of precision, recall, and F -measure, per team
and system category. Column Base-F presents the
baseline F -measure (alltrue), while Base-Acc the
baseline accuracy score (majority). For all seven re-
lations, the best team significantly outperforms the
baseline. The category of the best-scoring system
in almost every case is B4 (only the ILK B4 system
scored second on the Origin-Entity relation).
Table 5 suggests that some relations are more dif-
ficult to classify than others. The best F -measure
ranges from 83.7 for Product?Producer to 68.6 for
Origin?Entity. The difference between the best F -
measure and the baseline F -measure ranges from
23.3 for Part-Whole to 3.7 for Product-Producer.
The difference between the best accuracy and the
baseline accuracy ranges from 31.0 for Content-
Container to 10.7 for Product-Producer.
The F column shows the best result for each rela-
tion, but similar differences among the relations may
be observed when all results are pooled. The Avg.
rank column computes the average rank of each re-
lation in the ordered list of relations generated by
each system. For example, Product?Producer is of-
ten listed as the first or the second easiest relation
(with an average rank of 1.7), while Origin?Entity
and Theme?Tool are identified as the most difficult
17
relations to classify (with average ranks of 6.0).
5 Conclusion
This paper describes a new semantic evaluation task,
Classification of Semantic Relations between Nom-
inals. We have accomplished our goal of providing
a framework and a benchmark data set to allow for
comparisons of methods for this task. The data in-
cluded different types of information ? lexical se-
mantic information, context, query used ? meant to
facilitate the analysis of useful sources of informa-
tion for determining the semantic relation between
nominals. The results that the participating systems
have reported show successful approaches to this
difficult task, and the advantages of using lexical se-
mantic information.
The success of the task ? measured in the inter-
est of the community and the results of the partici-
pating systems ? shows that the framework and the
data are useful resources. By making this collection
freely accessible, we encourage further research into
this domain and integration of semantic relation al-
gorithms in high-end applications.
Acknowledgments
We thank Eneko Agirre, Llu??s Ma`rquez and Richard
Wicentowski, the organizers of SemEval 2007, for
their guidance and prompt support in all organiza-
tional matters. We thank Marti Hearst for valu-
able advice throughout the task description and de-
bates on semantic relation definitions. We thank the
anonymous reviewers for their helpful comments.
References
T. Chklovski and P. Pantel. 2004. Verbocean: Mining the
web for fine-grained semantic verb relations. In Proc.
Conf. on Empirical Methods in Natural Language Pro-
cessing, EMNLP-04, pages 33?40, Barcelona, Spain.
R. Girju, D. Moldovan, M. Tatu, and D. Antohe. 2005.
On the semantics of noun compounds. Computer
Speech and Language, 19:479?496.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. 14th International
Conf. on Computational Linguistics (COLING-92),
pages 539?545.
M. Lapata. 2002. The disambiguation of nominaliza-
tions. Computational Linguistics, 28(3):357?388.
D.D. Lewis. 1991. Evaluating text categorization.
In Proceedings of the Speech and Natural Language
Workshop, pages 312?318, Asilomar.
D. Moldovan, A. Badulescu, M. Tatu, D. Antohe, and
R. Girju. 2004. Models for the semantic classification
of noun phrases. In Proc. Computational Lexical Se-
mantics Workshop at HLT-NAACL 2004, pages 60?67,
Boston, MA.
P. Nakov and M. Hearst. 2006. Using verbs to char-
acterize noun-noun relations. In Proc. Twelfth Inter-
national Conf. in Artificial Intelligence (AIMSA-06),
pages 233?244, Varna,Bulgaria.
V. Nastase and S. Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5), pages 285?301, Tilburg, The Netherlands.
V. Nastase, J. Sayyad-Shirabad, M. Sokolova, and S. Sz-
pakowicz. 2006. Learning noun-modifier semantic
relations with corpus-based and WordNet-based fea-
tures. In Proc. 21st National Conf. on Artificial Intel-
ligence (AAAI 2006), pages 781?787, Boston, MA.
B. Rosario and M. Hearst. 2001. Classifying the seman-
tic relations in noun-compounds via domain-specific
lexical hierarchy. In Proc. 2001 Conf. on Empirical
Methods in Natural Language Processing (EMNLP-
01), pages 82?90.
B. Rosario, M. Hearst, and C. Fillmore. 2002. The de-
scent of hierarchy, and selection in relational seman-
tics. In Proc. 40th Annual Meeting of the Association
for Computational Linguistics (ACL-02), pages 417?
424, Philadelphia, PA.
M. Stephens, M. Palakal, S. Mukhopadhyay, and R. Raje.
2001. Detecting gene relations from MEDLINE ab-
stracts. In Proc. Sixth Annual Pacific Symposium on
Biocomputing, pages 483?496.
M. Tatu and D. Moldovan. 2005. A semantic approach to
recognizing textual entailment. In Proc. Human Lan-
guage Technology Conf. and Conf. on Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP
2005), pages 371?378, Vancouver, Canada.
P.D. Turney and M.L. Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning, 60(1-3):251?278.
P.D. Turney. 2005. Measuring semantic similarity by
latent relational analysis. In Proc. Nineteenth Interna-
tional Joint Conf. on Artificial Intelligence (IJCAI-05),
pages 1136?1141, Edinburgh, Scotland.
18
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 366?369,
Prague, June 2007. c?2007 Association for Computational Linguistics
UCB: System Description for SemEval Task #4
Preslav I. Nakov
EECS, CS division
University of California at Berkeley
Berkeley, CA 94720
nakov@cs.berkeley.edu
Marti A. Hearst
School of Information
University of California at Berkeley
Berkeley, CA 94720
hearst@ischool.berkeley.edu
Abstract
The UC Berkeley team participated in the
SemEval 2007 Task #4, with an approach
that leverages the vast size of the Web in or-
der to build lexically-specific features. The
idea is to determine which verbs, preposi-
tions, and conjunctions are used in sentences
containing a target word pair, and to com-
pare those to features extracted for other
word pairs in order to determine which are
most similar. By combining these Web fea-
tures with words from the sentence context,
our team was able to achieve the best results
for systems of category C and third best for
systems of category A.
1 Introduction
Semantic relation classification is an important but
understudied language problem arising in many
NLP applications, including question answering, in-
formation retrieval, machine translation, word sense
disambiguation, information extraction, etc. This
year?s SemEval (previously SensEval) competition
has included a task targeting the important special
case of Classification of Semantic Relations between
Nominals. In the present paper we describe the UCB
system which took part in that competition.
The SemEval dataset contains a total of 7 se-
mantic relations (not exhaustive and possibly over-
lapping), with 140 training and about 70 testing
sentences per relation. Sentence classes are ap-
proximately 50% negative and 50% positive (?near
misses?). Table 1 lists the 7 relations together with
some examples.
# Relation Name Examples
1 Cause-Effect hormone-growth, laugh-wrinkle
2 Instrument-Agency laser-printer, ax-murderer
3 Product-Producer honey-bee, philosopher-theory
4 Origin-Entity grain-alcohol, desert-storm
5 Theme-Tool work-force, copyright-law
6 Part-Whole leg-table, door-car
7 Content-Container apple-basket, plane-cargo
Table 1: SemEval dataset: Relations with examples
(context sentences are not shown).
Each example consists of a sentence, two nomi-
nals to be judged on whether they are in the target
semantic relation, manually annotated WordNet 3.0
sense keys for these nominals, and the Web query
used to obtain that example:
"Among the contents of the <e1>vessel</e1>
were a set of carpenters <e2>tools</e2>,
several large storage jars, ceramic
utensils, ropes and remnants of food, as
well as a heavy load of ballast stones."
WordNet(e1) = "vessel%1:06:00::",
WordNet(e2) = "tool%1:06:00::",
Content-Container(e2, e1) = "true",
Query = "contents of the * were a"
2 Related Work
Lauer (1995) proposes that eight prepositions are
enough to characterize the relation between nouns
in a noun-noun compound: of, for, in, at, on, from,
with or about. Lapata and Keller (2005) improve
on his results by using Web statistics. Rosario et al
(2002) use a ?descent of hierarchy?, which charac-
terizes the relation based on the semantic category of
the two nouns. Girju et al (2005) apply SVM, deci-
sion trees, semantic scattering and iterative seman-
366
tic specialization, using WordNet, word sense dis-
ambiguation, and linguistic features. Barker and Sz-
pakowicz (1998) propose a two-level hierarchy with
5 classes at the upper level and 30 at the lower level.
Turney (2005) introduces latent relational analysis,
which uses the Web, synonyms, patterns like ?X for
Y ?, ?X such as Y ?, etc., and singular value decom-
position to smooth the frequencies. Turney (2006)
induces patterns from the Web, e.g. CAUSE is best
characterized by ?Y * causes X?, and ?Y in * early
X? is the best pattern for TEMPORAL. Kim and Bald-
win (2006) propose to use a predefined set of seed
verbs and multiple resources: WordNet, CoreLex,
and Moby?s thesaurus. Finally, in a previous publi-
cation (Nakov and Hearst, 2006), we make the claim
that the relation between the nouns in a noun-noun
compound can be characterized by the set of inter-
vening verbs extracted from the Web.
3 Method
Given an entity-annotated example sentence, we re-
duce the target entities e1 and e2 to single nouns
noun1 and noun2, by keeping their last nouns
only, which we assume to be the heads. We then
mine the Web for sentences containing both noun1
and noun2, from which we extract features, con-
sisting of word(s), part of speech (verb, preposi-
tion, verb+preposition, coordinating conjunction),
and whether noun1 precedes noun2. Table 2 shows
some example features and their frequencies.
We start with a set of exact phrase queries
against Google: ?infl1 THAT * infl2?, ?infl2
THAT * infl1?, ?infl1 * infl2?, and ?infl2 *
infl1?, where infl1 and infl2 are inflectional vari-
ants of noun1 and noun2, generated using WordNet
(Fellbaum, 1998); THAT can be that, which, or who;
and * stands for 0 or more (up to 8) stars separated
by spaces, representing the Google * single-word
wildcard match operator. For each query, we collect
the text snippets from the result set (up to 1000 per
query), split them into sentences, assign POS tags
using the OpenNLP tagger1, and extract features:
Verb: If one of the nouns is the subject, and the
other one is a direct or indirect object of that verb,
we extract it and we lemmatize it using WordNet
(Fellbaum, 1998). We ignore modals and auxil-
1OpenNLP: http://opennlp.sourceforge.net
Freq. Feature POS Direction
2205 of P 2? 1
1923 be V 1? 2
771 include V 1? 2
382 serve on V 2? 1
189 chair V 2? 1
189 have V 1? 2
169 consist of V 1? 2
148 comprise V 1? 2
106 sit on V 2? 1
81 be chaired by V 1? 2
78 appoint V 1? 2
77 on P 2? 1
66 and C 1? 2
. . . . . . . . . . . .
Table 2: Most frequent features for committee
member. V stands for verb, P for preposition, and
C for coordinating conjunction.
iaries, but retain the passive be, verb particles and
prepositions (in case of indirect object).
Preposition: If one of the nouns is the head of
an NP which contains a PP, inside which there is an
NP headed by the other noun (or an inflectional form
thereof), we extract the preposition heading that PP.
Coordination: If the two nouns are the heads of
two coordinated NPs, we extract the coordinating
conjunction.
In addition, we include some non-Web features2:
Sentence word: We use as features the words
from the context sentence, after stop words removal
and stemming with the Porter stemmer.
Entity word: We also use the lemmas of the
words that are part of ei (i = 1, 2).
Query word: Finally, we use the individual
words that are part of the query string. This feature
is used for category C runs only (see below).
Once extracted, the features are used to calculate
the similarity between two noun pairs. Each feature
triplet is assigned a weight. We wish to downweight
very common features, such as ?of? used as a prepo-
sition in the 2 ? 1 direction, so we apply tf.idf
weighting to each feature. We then use the following
variant of the Dice coefficient to compare the weight
vectors A = (a1, . . . , an) and B = (b1, . . . , bn):
Dice(A,B) =
2?
?n
i=1 min(ai, bi)
?n
i=1 ai +
?n
i=1 bi
(1)
This vector representation is similar to that of
2Features have type prefix to prevent them from mixing.
367
System Relation P R F Acc
UCB-A1 Cause-Effect 58.2 78.0 66.7 60.0
Instrument-Agency 62.5 78.9 69.8 66.7
Product-Producer 77.3 54.8 64.2 59.1
Origin-Entity 67.9 52.8 59.4 67.9
Theme-Tool 50.0 31.0 38.3 59.2
Part-Whole 51.9 53.8 52.8 65.3
Content-Container 62.2 60.5 61.3 60.8
average 61.4 58.6 58.9 62.7
UCB-A2 Cause-Effect 58.0 70.7 63.7 58.8
Instrument-Agency 65.9 71.1 68.4 67.9
Product-Producer 80.0 77.4 78.7 72.0
Origin-Entity 60.6 55.6 58.0 64.2
Theme-Tool 45.0 31.0 36.7 56.3
Part-Whole 41.7 38.5 40.0 58.3
Content-Container 56.4 57.9 57.1 55.4
average 58.2 57.5 57.5 61.9
UCB-A3 Cause-Effect 62.5 73.2 67.4 63.8
Instrument-Agency 65.9 76.3 70.7 69.2
Product-Producer 75.0 67.7 71.2 63.4
Origin-Entity 48.4 41.7 44.8 54.3
Theme-Tool 62.5 51.7 56.6 67.6
Part-Whole 50.0 46.2 48.0 63.9
Content-Container 64.9 63.2 64.0 63.5
average 61.3 60.0 60.4 63.7
UCB-A4 Cause-Effect 63.5 80.5 71.0 66.2
Instrument-Agency 70.0 73.7 71.8 71.8
Product-Producer 76.3 72.6 74.4 66.7
Origin-Entity 50.0 47.2 48.6 55.6
Theme-Tool 61.5 55.2 58.2 67.6
Part-Whole 52.2 46.2 49.0 65.3
Content-Container 65.8 65.8 65.8 64.9
average 62.7 63.0 62.7 65.4
Baseline (majority) 81.3 42.9 30.8 57.0
Table 3: Task 4 results. UCB systems A1-A4.
Lin (1998), who measures word similarity by using
triples extracted from a dependency parser. In par-
ticular, given a noun, he finds all verbs that have it
as a subject or object, and all adjectives that modify
it, together with the corresponding frequencies.
4 Experiments and Results
Participants were asked to classify their systems
into categories depending on whether they used the
WordNet sense (WN) and/or the Google query (GC).
Our team submitted runs for categories A (WN=no,
QC=no) and C (WN=no, QC=yes) only, since we
believe that having the target entities annotated with
the correct WordNet senses is an unrealistic assump-
tion for a real-world application.
Following Turney and Littman (2005) and Barker
and Szpakowicz (1998), we used a 1-nearest-
neighbor classifier. Given a test example, we calcu-
lated the Dice coefficient between its feature vector
System Relation P R F Acc
UCB-C1 Cause-Effect 58.5 75.6 66.0 60.0
Instrument-Agency 65.2 78.9 71.4 69.2
Product-Producer 81.4 56.5 66.7 62.4
Origin-Entity 67.9 52.8 59.4 67.9
Theme-Tool 50.0 31.0 38.3 59.2
Part-Whole 51.9 53.8 52.8 65.3
Content-Container 62.2 60.5 61.3 60.8
Average 62.4 58.5 59.4 63.5
UCB-C2 Cause-Effect 58.0 70.7 63.7 58.8
Instrument-Agency 67.5 71.1 69.2 69.2
Product-Producer 80.3 79.0 79.7 73.1
Origin-Entity 60.6 55.6 58.0 64.2
Theme-Tool 50.0 37.9 43.1 59.2
Part-Whole 43.5 38.5 40.8 59.7
Content-Container 56.4 57.9 57.1 55.4
Average 59.5 58.7 58.8 62.8
UCB-C3 Cause-Effect 62.5 73.2 67.4 63.8
Instrument-Agency 68.2 78.9 73.2 71.8
Product-Producer 74.1 69.4 71.7 63.4
Origin-Entity 56.8 58.3 57.5 61.7
Theme-Tool 62.5 51.7 56.6 67.6
Part-Whole 50.0 42.3 45.8 63.9
Content-Container 64.9 63.2 64.0 63.5
Average 62.7 62.4 62.3 65.1
UCB-C4 Cause-Effect 63.5 80.5 71.0 66.2
Instrument-Agency 70.7 76.3 73.4 73.1
Product-Producer 76.7 74.2 75.4 67.7
Origin-Entity 59.0 63.9 61.3 64.2
Theme-Tool 63.0 58.6 60.7 69.0
Part-Whole 52.2 46.2 49.0 65.3
Content-Container 64.1 65.8 64.9 63.5
Average 64.2 66.5 65.1 67.0
Baseline (majority) 81.3 42.9 30.8 57.0
Table 4: Task 4 results. UCB systems C1-C4.
and the vector of each of the training examples. If
there was a single highest-scoring training example,
we predicted its class for that test example. Oth-
erwise, if there were ties for first, we assumed the
class predicted by the majority of the tied examples.
If there was no majority, we predicted the class that
was most likely on the training data. Regardless of
the classifier?s prediction, if the head words of the
two entities e1 and e2 had the same lemma, we clas-
sified that example as negative.
Table 3 and 4 show the results for our A and C
runs for different amounts of training data: 45 (A1,
C1), 90 (A2, C2), 105 (A3, C3) and 140 (A4, C4).
All results are above the baseline: always propose
the majority label (?true?/?false?) in the test set. In
fact, our category C system is the best-performing
(in terms of F and Acc) among the participating
systems, and we achieved the third best results for
category A. Our category C results are slightly but
368
consistently better than forA for all measures (P ,R,
F , Acc), which suggests that knowing the query is
helpful. Interestingly, systems UCB-A2 and UCB-
C2 performed worse than UCB-A1 and UCB-C1,
which means that having more training data does not
necessarily help with a 1NN classifier.
Table 5 shows additional analysis for A4 and C4.
We study the effect of adding extra Google contexts
(using up to 10 stars, rather than 8), and using differ-
ent subsets of features. We show the results for: (a)
leave-one-out cross-validation on the training data,
(b) on the test data, and (c) our official UCB runs.
Acknowledgements: This work is supported in
part by NSF DBI-0317510.
References
Ken Barker and Stan Szpakowicz. 1998. Semi-automatic
recognition of noun modifier relationships. In Proceedings
of COLING-ACL?98, pages 96?102.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel Antohe.
2005. On the semantics of noun compounds. Computer
Speech and Language, 19(4):479?496.
Su Nam Kim and Timothy Baldwin. 2006. Interpreting seman-
tic relations in noun compounds via verb semantics. In Pro-
ceedings of COLING/ACL 2006. (poster), pages 491?498.
Mirella Lapata and Frank Keller. 2005. Web-based models for
natural language processing. ACM Transactions on Speech
and Language Processing, 2:1?31.
Mark Lauer. 1995. Designing Statistical Language Learners:
Experiments on Noun Compounds. Ph.D. thesis, Department
of Computing Macquarie University NSW 2109 Australia.
Dekang Lin. 1998. An information-theoretic definition of sim-
ilarity. In Proceedings of International Conference on Ma-
chine Learning, pages 296?304.
Preslav Nakov and Marti Hearst. 2006. Using verbs to charac-
terize noun-noun relations. In Proceedings of AIMSA, pages
233?244.
Barbara Rosario, Marti Hearst, and Charles Fillmore. 2002.
The descent of hierarchy, and selection in relational seman-
tics. In ACL, pages 247?254.
Peter Turney and Michael Littman. 2005. Corpus-based learn-
ing of analogies and semantic relations. Machine Learning
Journal, 60(1-3):251?278.
Peter Turney. 2005. Measuring semantic similarity by latent
relational analysis. In Proceedings IJCAI, pages 1136?1141.
Peter Turney. 2006. Expressing implicit semantic relations
without supervision. In Proceedings of COLING-ACL,
pages 313?320.
Features Used Leave-1-out Test UCB
Cause-Effect
sent 45.7 50.0
p 55.0 53.8
v 59.3 68.8
v + p 57.1 63.7
v + p + c 70.5 67.5
v + p + c + sent 58.5 66.2 66.2
v + p + c + sent + query 59.3 66.2 66.2
Instrument-Agency
sent 63.6 59.0
p 62.1 70.5
v 71.4 69.2
v + p 70.7 70.5
v + p + c 70.0 70.5
v + p + c + sent 68.6 71.8 71.8
v + p + c + sent + query 70.0 73.1 73.1
Product-Producer
sent 47.9 59.1
p 55.7 58.1
v 70.0 61.3
v + p 66.4 65.6
v + p + c 67.1 65.6
v + p + c + sent 66.4 69.9 66.7
v + p + c + sent + query 67.9 69.9 67.7
Origin-Entity
sent 64.3 72.8
p 63.6 56.8
v 69.3 71.6
v + p 67.9 69.1
v + p + c 66.4 70.4
v + p + c + sent 68.6 72.8 55.6
v + p + c + sent + query 67.9 72.8 64.2
Theme-Tool
sent 66.4 69.0
p 56.4 56.3
v 61.4 70.4
v + p 56.4 67.6
v + p + c 57.1 69.0
v + p + c + sent 52.1 62.0 67.6
v + p + c + sent + query 52.9 62.0 69.0
Part-Whole
sent 47.1 51.4
p 57.1 54.1
v 60.0 66.7
v + p 62.1 63.9
v + p + c 61.4 63.9
v + p + c + sent 60.0 61.1 65.3
v + p + c + sent + query 60.0 61.1 65.3
Content-Container
sent 56.4 54.1
p 57.9 59.5
v 71.4 67.6
v + p 72.1 67.6
v + p + c 72.9 67.6
v + p + c + sent 69.3 67.6 64.9
v + p + c + sent + query 71.4 71.6 63.5
Average A4 67.3 65.4
Average C4 68.1 67.0
Table 5: Accuracy for different features and extra
Web contexts: on leave-one-out cross-validation,
on testing data, and in the official UCB runs.
369
Proceedings of the Third Workshop on Statistical Machine Translation, pages 147?150,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Improving English-Spanish Statistical Machine Translation: Experiments in
Domain Adaptation, Sentence Paraphrasing, Tokenization, and Recasing
Preslav Nakov?
EECS, CS division
University of California at Berkeley
Berkeley, CA 94720
nakov@cs.berkeley.edu
Abstract
We describe the experiments of the UC Berke-
ley team on improving English-Spanish ma-
chine translation of news text, as part of the
WMT?08 Shared Translation Task. We ex-
periment with domain adaptation, combin-
ing a small in-domain news bi-text and a
large out-of-domain one from the Europarl
corpus, building two separate phrase transla-
tion models and two separate language mod-
els. We further add a third phrase transla-
tion model trained on a version of the news
bi-text augmented with monolingual sentence-
level syntactic paraphrases on the source-
language side, and we combine all models in
a log-linear model using minimum error rate
training. Finally, we experiment with differ-
ent tokenization and recasing rules, achieving
35.09% Bleu score on the WMT?07 news test
data when translating from English to Span-
ish, which is a sizable improvement over the
highest Bleu score achieved on that dataset
at WMT?07: 33.10% (in fact, by our sys-
tem). On the WMT?08 English to Spanish
news translation, we achieve 21.92%, which
makes our team the second best on Bleu score.
1 Introduction
Modern Statistical Machine Translation (SMT) sys-
tems are trained on sentence-aligned bilingual cor-
pora, typically from a single domain. When tested
on text from that same domain, they demonstrate
?After January 2008 at the Linguistic Modeling Depart-
ment, Institute for Parallel Processing, Bulgarian Academy of
Sciences, nakov@lml.bas.bg
state-of-the art performance, but on out-of-domain
test data the results can get significantly worse. For
example, on the WMT?06 Shared Translation Task,
the scores for French to English translation dropped
from about 30 to about 20 Bleu points for nearly all
systems when tested on News Commentary rather
than Europarl text, which was used on training
(Koehn and Monz, 2006).
Therefore, in 2007 the Shared Task organizers
provided 1M words of bilingual News Commentary
training data in addition to the 30M Europarl data,
thus inviting interest in domain adaptation experi-
ments. Given the success of the idea, the same task
was offered this year with slightly larger training bi-
texts: 1.3M and 32M words, respectively.
2 System Parameters
The team of the University of California at Berkeley
(ucb) participated in the WMT?08 Shared Transla-
tion Task with two systems, English?Spanish and
Spanish?English, applied to translatingNews Com-
mentary text, for which a very limited amount of
training data was provided. We experimented with
domain adaptation, combining the provided small
in-domain bi-text and the large out-of-domain one
from the Europarl corpus, building two phrase trans-
lation models and two language models. We further
added a third phrase translation model trained on a
version of the news bi-text augmented with mono-
lingual sentence-level syntactic paraphrases on the
source-language side, and we combined all models
in one big log-linear model using minimum error
rate training. We also experimented with different
tokenization and recasing ideas.
147
2.1 Sentence-Level Syntactic Paraphrases
The idea of using paraphrases is motivated by the
observation that, in many cases, the testing text
contains pieces that are equivalent, but syntacti-
cally different from the phrases learned on train-
ing, which might result in missing the opportu-
nity for a high-quality translation. For example, an
English?Spanish SMT system could have an entry
in its phrase table for inequality of income, but not
for income inequality. Note that the latter phrase
is hard to translate into Spanish where noun com-
pounds are rare: the correct translation in this case
requires a suitable Spanish preposition and a re-
ordering, which are hard for the system to realize
and do properly. We address this problem by gen-
erating nearly-equivalent syntactic paraphrases of
the source-side training sentences, targeted at noun
compounds. We then pair each paraphrased sen-
tence with the foreign translation associated with the
original sentence in the training data. The resulting
augmented bi-text is used to train an SMT system,
which learns many useful new phrases. The idea
was introduced in (Nakov and Hearst, 2007), and is
described in more detail in (Nakov, 2007).
Unfortunately, using multiple paraphrased ver-
sions of the same sentence changes the word fre-
quencies in the training bi-text, thus causing worse
maximum likelihood estimates, which results in bad
system performance. However, real improvements
can still be achieved by merging the phrase tables of
the two systems, giving priority to the original.
2.2 Domain Adaptation
In our previous findings (Nakov and Hearst, 2007),
we found that using in-domain and out-of-domain
language models is the best way to perform do-
main adaptation. Following (Koehn and Schroeder,
2007), we further used two phrase tables.
2.3 Improving the Recaser
One problem we noticed with the default recasing
is that unknown words are left in lowercase. How-
ever, many unknown words are in fact named en-
tities (persons, organization, or locations), which
should be spelled capitalized. Therefore, we pre-
pared a new recasing script, which makes sure that
all unknown words keep their original case.
2.4 Changing Tokenization/Detokenization
We found the default tokenizer problematic: it
keeps complex adjectives as one word, e.g., well-
rehearsed, self-assured, Arab-Israeli. While lin-
guistically correct, this is problematic for machine
translation due to data sparsity. For example, the
SMT system might know how to translate into Span-
ish both well and rehearsed, but not well-rehearsed,
and thus at translation time it would be forced to
handle it as an unknown word. A similar problem
is related to double dashes ?--?, as illustrated by the
following training sentence: ?So the question now
is what can China do to freeze--and, if possible, to
reverse--North Korea?s nuclear program.?
Therefore, we changed the tokenizer, so that it
puts a space around ?-? and ?--?. We also changed the
detokenizer accordingly, adding some rules for fix-
ing erroneous output, e.g., making sure that in Span-
ish text ? and ?, ? and ! match. We also added some
rules for numbers, e.g., the English 1,185.32 should
be spelled as 1.185,32 in Spanish.
3 The UCB System
As Table 1 shows, we performed many experiments
varying different parameters of the system. Due to
space limitations, here we will only describe our best
system, news10?euro10?par10.
To build the system, we trained three separate
phrase-based SMT systems (max phrase lengths 10):
on the original News Commentary corpus (news),
on the paraphrased version of News Commentary
(par), and on the Europarl dataset (euro). As a re-
sult, we obtained three phrase tables, Tnews, Tpar,
and Teuro, and three lexicalized reordering models,
Rnews, Rpar, and Reuro, which we had to merge.
First, we kept all phrase pairs from Tnews. Then
we added those phrase pairs from Teuro which were
not present in Tnews. Finally, we added to them
those from Tpar which were not in Tnews nor in
Teuro. For each phrase pair added, we retained its as-
sociated features: forward phrase translation proba-
bility, reverse phrase translation probability, forward
lexical translation probability, reverse lexical trans-
lation probability, and phrase penalty. We further
added three new features ? Pnews, Peuro, and Ppar ?
each of them was 1 if the phrase pair came from that
system, and 0.5 otherwise.
148
BLEU Toke- News Comm. Europarl Tuning
Model DR IR nizer slen plen LM slen plen LM #iter score
1 2 3 4 5 6 7 8 9 10 11 12
I. Original Tokenizer
news7 (baseline) 32.04 32.30 def. 40 7 3 ? ? ? 8 33.51
news7 31.98 32.21 def. 100 7 3 ? ? ? 19 33.95
news10 32.43 32.67 def. 100 10 3 ? ? ? 13 34.50
II. New Tokenizer
- II.1. Europarl only
euro7 29.92 30.19 new ? ? ? 40 7 5 10 33.02
euro10 30.14 30.36 new ? ? ? 40 10 5 10 32.86
- II.2. News Commentary only
par10 31.17 31.44 new 100 10 3 ? ? ? 8 33.91
news10 32.27 32.53 new 100 10 3 ? ? ? 12 34.49
news10?par10 32.09 32.34 new 100 10 3 ? ? ? 24 34.63
- II.3. News Commentary + Europarl
-- II.3.1. using Europarl LM
par10 32.88 33.16 new 100 10 3 ? ? 5 11 35.54
news10 33.99 34.26 new 100 10 3 ? ? 5 8 36.16
news10?par10 34.42 34.71 new 100 10 3 ? ? 5 17 36.41
-- II.3.2. using Europarl LM & Phrase Table (max phrase length 7)
?news10+euro7+par10 32.75 32.96 new 100 10 3 40 7 5 27 35.28
?news10+euro7 34.06 34.32 new 100 10 3 40 7 5 28 36.82
news10?euro7 34.05 34.31 new 100 10 3 40 7 5 9 36.71
news10?par10?euro7 34.25 34.52 new 100 10 3 40 7 5 14 36.88
news10?euro7?par10 34.69 34.97 new 100 10 3 40 7 5 10 37.01
-- II.3.3. using Europarl LM & Phrase Table (max phrase length 10)
?news10+euro10+par10 32.74 33.02 new 100 10 3 40 10 5 36 35.60
news10?euro10?par10 34.85 35.09 new 100 10 3 40 10 5 12 37.13
Table 1: English?Spanish translation experiments with the WMT?07 data: training on News Commentary and
Europarl, and evaluating on News Commentary. Column 1 provides a brief description of the model used. Here
we use euro, news and par to refer to using phrase tables extracted from the Europarl, the News Commentary, or the
Paraphrased News Commentary training bi-text; the index indicates the maximum phrase length allowed. The? oper-
ation means the phrase tables are merged, giving priority to the left one and using additional features indicating where
each phrase pair came from, while the+ operation indicates the phrase tables are used together without priorities. The
models using the + operation are marked with a ? as a reminder that the involved phrase tables are used together, as
opposed to being priority-merged. Note also that the models from II.3.1. only use the Spanish part of the Europarl
training data to build an out-of-domain language model; this is not indicated in column 1, but can be seen in column
10. Columns 2 and 3 show the testing Bleu score after applying the Default Recaser (DR) and the Improved Recaser
(IR), respectively. Column 4 shows whether the default or the new tokenizer was used. Columns 5, 6 and 7 contain the
parameters of the News Commentary training data: maximum length of the training sentences used (slen), maximum
length of the extracted phrases (plen), and order of the language model (LM), respectively. Columns 8, 9 and 10 con-
tain the same parameters for the Europarl training data. Column 11 shows the number of iterations the MERT tuning
took, and column 12 gives the corresponding tuning Bleu score achieved. Finally, for the WMT?08 competition, we
used the system marked in bold.
149
We further merged Rnews, Reuro, and Rpar in
a similar manner: we first kept all phrases from
Rnews, then we added those from Reuro which were
not present in Rnews, and finally those from Rpar
which were not in Rnews nor in Reuro.
We used two language models with Kneser-Ney
smoothing: a 3-gram model trained on News Com-
mentary, and a 5-gram model trained on Europarl.
We then trained a log-linear model using the fol-
lowing feature functions: language model proba-
bilities, word penalty, distortion cost, and the pa-
rameters from the phrase table. We set the feature
weights by optimizing the Bleu score directly using
minimum error rate training (Och, 2003) on the de-
velopment set. We used these weights in a beam
search decoder to produce translations for the test
sentences, which we compared to the WMT?07 gold
standard using Bleu (Papineni et al, 2002).
4 Results and Discussion
Table 1 shows the evaluation results using the
WMT?07 News Commentary test data. Our best
English?Spanish system news10?euro10?par10
(see the table caption for explanation of the nota-
tion), which is also our submission, achieved 35.09
Bleu score with the improved recaser; with the de-
fault recaser, the score drops to 34.85.
Due to space limitations, our Spanish?English
results are not in Table 1. This time, we did not use
paraphrases, and our best system news10?euro10
achieved 35.78 and 35.17 Bleu score with the im-
proved and the default recaser, respectively.
As the table shows, using the improved recaser
yields consistent improvements by about 0.3 Bleu
points. Using an out-of-domain language model
adds about 2 additional Bleu points, e.g., news10
improves from 32.53 to 34.26, and news10?par10
improves from 32.34 to 34.71. The impact of
also adding an out-of-domain phrase table is tiny:
news10?euro7 improves on news10 by 0.05 only.
Adding paraphrases however can yield an absolute
improvement of about 0.6, e.g., 34.31 vs. 34.97
for news10?euro7 and news10?euro7?par10. Inter-
estingly, using an out-of-domain phrase table has a
bigger impact when paraphrases are used, e.g., for
news10?par10 and news10?euro7?par10 we have
34.71 and 34.97, respectively. Finally, we were sur-
prised to find out that using the new tokenizer does
not help: for news10 the default tokenizer yields
32.67, while the new one only achieves 32.53. This
is surprising for us, since the new tokenizer used to
help consistent on the WMT?06 data.
5 Conclusions and Future Work
We described the UCB system for the WMT?08
Shared Translation Task. By combining in-domain
and out-of-domain data, and by using sentence-
level syntactic paraphrases and a better recaser, we
achieved an improvement of almost 2 Bleu points1
over the best result on the WMT?07 test data2,
and the second best Bleu score for this year?s
English?Spanish translation of news text.
In future work, we plan a deeper analysis of the
obtained results. First, we would like to experiment
with new ways to combine data from different do-
mains. We also plan to further improve the recaser,
and to investigate why the new tokenizer did not help
for the WMT?07 data.
References
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
european languages. In Proceedings on the Workshop
on Statistical Machine Translation, pages 102?121.
Philipp Koehn and Josh Schroeder. 2007. Experiments in
domain adaptation for statistical machine translation.
In Proceedings of the Second Workshop on Statistical
Machine Translation, pages 224?227. Association for
Computational Linguistics.
Preslav Nakov and Marti Hearst. 2007. UCB system de-
scription for the WMT 2007 shared task. In Workshop
on Statistical Machine Translation, pages 212?215.
Preslav Nakov. 2007. Using the Web as an Implicit
Training Set: Application to Noun Compound Syntax
and Semantics. Ph.D. thesis, EECS Department, Uni-
versity of California, Berkeley, UCB/EECS-2007-173.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
1Note however that this year we had more training data com-
pared to last year: 1.3M vs. 1M words for News Commentary,
and 32M vs. 30M words for Europarl.
2In fact, achieved by our system at WMT?07.
150
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1358?1367,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Improved Statistical Machine Translation for Resource-Poor Languages
Using Related Resource-Rich Languages
Preslav Nakov
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
nakov@comp.nus.edu.sg
Hwee Tou Ng
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
nght@comp.nus.edu.sg
Abstract
We propose a novel language-independent
approach for improving statistical ma-
chine translation for resource-poor lan-
guages by exploiting their similarity to
resource-rich ones. More precisely, we
improve the translation from a resource-
poor source language X
1
into a resource-
rich language Y given a bi-text contain-
ing a limited number of parallel sentences
for X
1
-Y and a larger bi-text for X
2
-Y
for some resource-rich language X
2
that
is closely related to X
1
. The evaluation
for Indonesian?English (using Malay)
and Spanish?English (using Portuguese
and pretending Spanish is resource-poor)
shows an absolute gain of up to 1.35 and
3.37 Bleu points, respectively, which is an
improvement over the rivaling approaches,
while using much less additional data.
1 Introduction
Recent developments in statistical machine trans-
lation (SMT), e.g., the availability of efficient im-
plementations of integrated open-source toolkits
like Moses (Koehn et al, 2007), have made it pos-
sible to build a prototype system with decent trans-
lation quality for any language pair in a few days
or even hours. In theory. In practice, doing so
requires having a large set of parallel sentence-
aligned bi-lingual texts (a bi-text) for that lan-
guage pair, which is often unavailable. Large high-
quality bi-texts are rare; except for Arabic, Chi-
nese, and some official languages of the European
Union (EU), most of the 6,500+ world languages
remain resource-poor from an SMT viewpoint.
While manually creating a small bi-text could
be relatively easy, building a large one is hard,
e.g., because of copyright. Most bi-texts for SMT
come from parliament debates and legislation of
multi-lingual countries (e.g., French-English from
Canada, and Chinese-English from Hong Kong),
or from international organizations like the United
Nations and the European Union. For exam-
ple, the Europarl corpus of parliament proceed-
ings consists of about 1.3M parallel sentences (up
to 44M words) per language for 11 languages
(Koehn, 2005), and the JRC-Acquis corpus pro-
vides a comparable amount of European legisla-
tion in 22 languages (Steinberger et al, 2006).
The official languages of the EU are especially
lucky in that respect; while this includes such
?classic SMT languages? like English and French,
and some important international ones like Span-
ish and Portuguese, most of the rest have a limited
number of speakers and were resource-poor until
recently; this is changing quickly because of the
increasing volume of EU parliament debates and
the ever-growing European legislation. Thus, be-
coming an official language of the EU has turned
out to be an easy recipe for getting resource-rich in
bi-texts quickly. Of course, not all languages are
that ?lucky?, but many can still benefit.
In this paper, we propose using bi-texts for
resource-rich language pairs to build better SMT
systems for resource-poor ones by exploiting the
similarity between a resource-poor language and a
resource-rich one.
The proposed method allows non-EU languages
to benefit from being closely related to one or
more official languages of the EU, the most
obvious candidates being Norwegian (related to
Swedish), Moldavian
1
(related to Romanian), and
Macedonian
2
(related to Bulgarian). After Croa-
tia joins the EU, Serbian, Bosnian and Montene-
grin will be able to benefit from Croatian gradually
turning resource-rich (all four split from Serbo-
Croatian after the breakup of Yugoslavia). The
newly-made EU-official (and thus not as resource-
1
Not recognized by Romania.
2
Not recognized by Bulgaria and Greece.
1358
rich) Czech and Slovak are another possible pair
of candidates. As we will see below, even such
resource-rich languages like Spanish and Por-
tuguese can benefit from the proposed method. Of
course, many pairs of closely related languages
can be also found outside of Europe, Malay and
Indonesian being just one such example we will
experiment with.
The remainder of the present paper is organized
as follows: Section 2 presents our method, Sec-
tion 3 describes the experiments, and Section 4
discusses the results and the general applicability
of the approach. Section 5 provides an overview
of the related work. Finally, Section 6 concludes
and suggests possible directions for future work.
2 Method
We propose a novel language-independent ap-
proach for improving statistical machine trans-
lation for resource-poor languages by exploiting
their similarity to resource-rich ones. More pre-
cisely, we improve the translation from a resource-
poor source language X
1
into a resource-rich tar-
get language Y given a bi-text containing a limited
number of parallel sentences forX
1
-Y and a much
larger bi-text forX
2
-Y for some resource-rich lan-
guage X
2
that is closely related to X
1
.
Our method exploits the similarity between re-
lated languages with respect to word order, syntax,
and, most importantly, vocabulary overlap ? re-
lated languages share a large number of cognates.
Before we present the method, we will describe
two simple strategies for integrating the bi-text for
X
2
-Y into a phrase-based SMT system for X
1
-Y .
2.1 Merging Bi-texts
We can simply concatenate the bi-texts for X
1
-Y
and X
2
-Y into one large bi-text and use it to train
an SMT system.
This offers several advantages. First, it can
yield improved word alignments for the sentences
that came from the X
1
-Y bi-text, e.g., since the
additional sentences can provide new contexts for
the rare words in that bi-text; rare words are
hard to align, which could have a disastrous ef-
fect on the subsequent phrase extraction stage.
Second, it can provide new source-language side
translation options, thus increasing the lexical
coverage and reducing the number of unknown
words at translation time; it can also provide new
useful non-compositional phrases on the source-
language side, thus yielding more fluent transla-
tion output. Third, it can offer new target-language
side phrases for known source phrases, which
could improve fluency by providing more trans-
lation options for the language model (LM) to
choose from. Fourth, bad phrases including words
from X
2
that do not exist in X
1
will be effectively
ignored at translation time since they could never
possibly match the input, while bad new target-
language translations still have the chance to be
filtered out by the language model.
However, simple concatenation can be problem-
atic. First, when concatenating the small bi-text
for X
1
-Y with the much larger one for X
2
-Y , the
latter will dominate during word alignment and
phrase extraction, thus hugely influencing both
lexical and phrase translation probabilities, which
can yield poor performance. This can be counter-
acted by repeating the small bi-text several times
so that the large one does not dominate. Sec-
ond, since the bi-texts are merged mechanically,
there is no way to distinguish between phrases ex-
tracted from the bi-text for X
1
-Y (which should
be good), from those coming from the bi-text for
X
2
-Y (whose quality might be questionable).
2.2 Combining Phrase Tables
An alternative way of making use of the additional
bi-text for X
2
-Y to train an improved SMT sys-
tem for X
1
? Y is to build separate phrase ta-
bles from X
1
-Y and X
2
-Y , which can then be
(a) used together, e.g., as alternative decoding
paths, (b) merged, e.g., using one or more extra
features to indicate the bi-text each phrase came
from, or (c) interpolated, e.g., using simple linear
interpolation.
Building two separate phrase tables offers sev-
eral advantages. First, the good phrases from the
bi-text forX
1
-Y are clearly distinguished from (or
given a higher weight in the linear interpolation
compared to) the potentially bad ones from the
X
2
-Y bi-text. Second, the lexical and the phrase
translation probabilities are combined in a princi-
pled manner. Third, using an X
2
-Y bi-text that is
much larger than that for X
1
-Y is not problematic
any more. Fourth, as with bi-text merging, there
are many additional source- and target-language
phrases, which offer new translation options.
On the negative side, the opportunity is lost
to obtain improved word alignments for the sen-
tences in the X
1
-Y bi-text.
1359
2.3 Proposed Method
Taking into account the potential advantages and
disadvantages of the above strategies, we pro-
pose a method that tries to get the best of both:
(i) increased lexical coverage by using additional
phrase pairs independently extracted from X
2
-Y ,
and (ii) improved word alignments for X
1
-Y by
biasing the word alignment process with addi-
tional sentence pairs from X
2
-Y (possibly also re-
peating X
1
-Y several times). A detailed descrip-
tion of the method follows:
1. Build a bi-text B
cat
that is a concatenation
of the bi-texts for X
1
-Y and X
2
-Y . Gener-
ate word alignments forB
cat
, extract phrases,
and build a phrase table T
cat
.
2. Build a bi-text B
rep
from the X
1
-Y bi-text
repeated k times followed by one copy of the
X
2
-Y bi-text. Generate word alignments for
B
rep
, then truncate them, only keeping word
alignments for one copy of the X
1
-Y bi-text.
Use these word alignments to extract phrases,
and build a phrase table T
rep trunc
.
3. Produce a phrase table T
merged
by combin-
ing T
cat
and T
rep trunc
, giving priority to the
latter, and use it in an X
1
? Y SMT system.
2.4 Transliteration
As we mentioned above, our method relies on the
existence of a large number of cognates between
related languages. While linguists define cognates
as words derived from a common root
3
(Bickford
and Tuggy, 2002), computational linguists typi-
cally ignore origin, defining them as words in dif-
ferent languages that are mutual translations and
have a similar orthography (Bergsma and Kon-
drak, 2007; Mann and Yarowsky, 2001; Melamed,
1999). In this paper, we adopt the latter definition.
Cognates between related languages often ex-
hibit minor spelling variations, which can be sim-
ply due to different rules of orthography, (e.g.,
senhor vs. se?nor in Portuguese and Spanish), but
often stem from real phonological differences. For
example, the Portuguese suffix -c??ao corresponds
to the Spanish suffix -ci?on, e.g., evoluc??ao vs.
evoluci?on. Such correspondences can be quite fre-
quent and thus easy to learn automatically
4
. Even
3
E.g., Latin tu, Old English thou, Spanish t?u, Greek s?u and
German du are all cognates meaning ?2
nd
person singular?.
4
Not all such differences are systematic; many apply to a
particular word only, e.g., kerana vs. karena in Malay and
Indonesian, or dizer vs. decir in Portuguese and Spanish.
more frequent can be the inflectional variations.
For example, in Portuguese and Spanish respec-
tively, verb endings like -ou vs. -?o (for 3rd person
singular, simple past tense), e.g., visitou vs. visit?o,
or -ei vs. -?e (for 1st person singular, simple past
tense), e.g., visitei vs. visit?e.
If such systematic differences exist between the
languages X
1
and X
2
, it might be useful to learn
and to use them as a pre-processing step in order
to transliterate the X
2
side of the X
2
-Y bi-text
and thus increase its vocabulary overlap with the
source language X
1
.
We will describe our approach to automatic
transliteration in more detail in Section 3.4 below.
3 Experiments
3.1 Language Pairs
We experimented with two language pairs: the
closely relatedMalay and Indonesian and the more
dissimilar Spanish and Portuguese.
Malay and Indonesian are mutually intelligible,
but differ in pronunciation and vocabulary. An ex-
ample follows
5
:
? Malay: Semua manusia dilahirkan bebas
dan samarata dari segi kemuliaan dan hak-
hak.
? Indonesian: Semua orang dilahirkan
merdeka dan mempunyai martabat dan
hak-hak yang sama.
Spanish and Portuguese also exhibit a notice-
able degree of mutual intelligibility, but differ in
pronunciation, spelling, and vocabulary. Unlike
Malay and Indonesian, however, they also differ
syntactically and have a high degree of spelling
differences as demonstrated by the following ex-
amples
6
:
? Spanish: Se?nora Presidenta, estimados cole-
gas, lo que est?a sucediendo en Oriente Medio
es una tragedia.
? Portuguese: Senhora Presidente, caros cole-
gas, o que est?a a acontecer no Medio Oriente
?e uma trag?edia.
5
In English: All human beings are born free and equal in
dignity and rights. (from Article 1 of the Universal Declara-
tion of Human Rights)
6
In English: Madam President, ladies and gentlemen, the
events in the Middle East are a real tragedy.
1360
3.2 Datasets
In our experiments, we used the following number
of training sentence pairs (number of words shown
in parentheses) for English (en), Indonesian (in),
Malay (ml), Portuguese(pt), and Spanish (es):
? Indonesian-English (in-en):
? 28,383 pairs (0.8M, 0.9M words);
? monolingual English en
in
: 5.1M words.
? Malay-English (ml-en):
? 190,503 pairs (5.4M, 5.8M words);
? monoling. English en
ml
: 27.9M words.
? Spanish-English (es-en):
? 1,240,518 pairs (35.7M, 34.6M words);
? monolingual English en
es:pt
: 45.3M
words (the same as for pt-en).
? Portuguese-English (pt-en):
? 1,230,038 pairs (35.9M, 34.6M words).
? monolingual English en
es:pt
: 45.3M
words (the same as for es-en).
All of the above datasets contain sentences with
up to 100 tokens. In addition, for each of the
four language pairs, we have a development and
a testing bi-text, each with 2,000 parallel sentence
pairs. We made sure the development and the test-
ing bi-texts shared no sentences with the training
bi-texts; we further excluded from the monolin-
gual English data all sentences from the English
sides of the training and the development bi-texts.
The training bi-text datasets for es-en and pt-en
were built from release v.3 of the Europarl corpus,
excluding the Q4/2000 portion out of which we
created our testing and development datasets.
We built the in-en bi-texts from texts that we
downloaded from the Web. We translated the In-
donesian texts to English using Google Translate,
and we matched
7
them against the English texts
using a cosine similarity measure and heuristic
constraints based on document length in words
and in sentences, overlap of numbers, words in
uppercase, and words in the title. Next, we ex-
tracted pairs of sentences from the matched doc-
ument pairs using competitive linking (Melamed,
2000), and we retained the ones whose similarity
was above a pre-specified threshold. The ml-en
was built in a similar manner.
7
Note that the automatic translations were used for match-
ing only; the final bi-text contained no automatic translations.
3.3 Baseline SMT System
In the baseline, we used the following setup: We
first tokenized and lowercased both sides of the
training bi-text. We then built separate directed
word alignments for English?X andX?English
(X?{Indonesian, Spanish}) using IBM model 4
(Brown et al, 1993), combined them using the in-
tersect+grow heuristic (Och and Ney, 2003), and
extracted phrase-level translation pairs of maxi-
mum length seven using the alignment template
approach (Och and Ney, 2004). We thus obtained
a phrase table where each pair is associated with
five parameters: forward and reverse phrase trans-
lation probabilities, forward and reverse lexical
translation probabilities, and phrase penalty.
We then trained a log-linear model using stan-
dard SMT feature functions: trigram language
model probability, word penalty, distance-based
8
distortion cost, and the parameters from the phrase
table. We set al weights by optimizing Bleu (Pap-
ineni et al, 2002) using minimum error rate train-
ing (MERT) (Och, 2003) on a separate develop-
ment set of 2,000 sentences (Indonesian or Span-
ish), and we used them in a beam search decoder
(Koehn et al, 2007) to translate 2,000 test sen-
tences (Indonesian or Spanish) into English. Fi-
nally, we detokenized the output, and we evaluated
it against a lowercased gold standard using Bleu
9
.
3.4 Transliteration
As was mentioned in Section 2, transliteration can
be helpful for languages with regular spelling dif-
ferences. Thus, we built a system for translitera-
tion from Portuguese into Spanish that was trained
on a list of automatically extracted likely cognates.
The system was applied on the Portuguese side of
the pt-en training bi-text.
Classic approaches to automatic cognate extrac-
tion look for non-stopwords with similar spelling
that appear in parallel sentences in a bi-text (Kon-
drak et al, 2003). In our case, however, we need to
extract cognates between Spanish and Portuguese
given pt-en and es-en bi-texts only, i.e., without
having a pt-es bi-text. Although it is easy to con-
struct a pt-es bi-text from the Europarl corpus,
we chose not to do so since, in general, synthe-
8
We also tried lexicalized reordering (Koehn et al, 2005).
While it yielded higher absolute Bleu scores, the relative im-
provement for a sample of our experiments was very similar
to that achieved with distance-based re-ordering.
9
We used version 11b of the NIST scoring tool:
http://www.nist.gov/speech/tools/
1361
sizing a bi-text for X
1
-X
2
would be impossible:
e.g., it cannot be done for ml-in given our training
datasets for in-en andml-en since the English sides
of these bi-texts have no sentences in common.
Thus, we extracted the list of likely cognates be-
tween Portuguese and Spanish from the training
pt-en and es-en bi-texts using English as a pivot as
follows: We started with IBM model 4 word align-
ments, from which we extracted four conditional
lexical translation probabilities: Pr(p
j
|e
i
) and
Pr(e
i
|p
j
) for Portuguese-English, and Pr(s
k
|e
i
)
and Pr(e
i
|s
k
) for Spanish-English, where p
j
, e
i
and s
k
stand for a Portuguese, an English and
a Spanish word respectively. Following Wu and
Wang (2007), we then induced conditional lexical
translation probabilities Pr(p
j
|s
k
) and Pr(s
k
|p
j
)
for Portuguese-Spanish as follows:
Pr(p
j
|s
k
) =
?
i
Pr(p
j
|e
i
, s
k
)Pr(e
i
|s
k
)
Assuming p
j
is conditionally independent of s
k
given e
i
, we can simplify the above expression:
Pr(p
j
|s
k
) =
?
i
Pr(p
j
|e
i
)Pr(e
i
|s
k
)
Similarly, for Pr(s
k
|p
j
), we obtain
Pr(s
k
|p
j
) =
?
i
Pr(s
k
|e
i
)Pr(e
i
|p
j
)
We excluded all stopwords, words of length less
than three, and those containing digits. We further
calculated Prod(p
j
, s
k
) = Pr(p
j
|s
k
)Pr(s
k
|p
j
),
and we excluded all Portuguese-Spanish word
pairs (p
j
, s
k
) for which Prod(p
j
, s
k
) < 0.01.
From the remaining pairs, we extracted likely cog-
nates based on Prod(p
j
, s
k
) and on the ortho-
graphic similarity between p
j
and s
k
.
Following Melamed (1995), we measured the
orthographic similarity using the longest common
subsequence ratio (LCSR), defined as follows:
LCSR(s
1
, s
2
) =
|LCS(s
1
,s
2
)|
max(|s
1
|,|s
2
|)
where LCS(s
1
, s
2
) is the longest common subse-
quence of s
1
and s
2
, and |s| is the length of s.
We retained as likely cognates all pairs for
which LCSR was 0.58 or higher; that value was
found by Kondrak et al (2003) to be optimal for a
number of language pairs in the Europarl corpus.
Finally, we performed competitive linking
(Melamed, 2000), assuming that each Portuguese
wordform had at most one Spanish best cognate
match. Thus, using the values of Prod(p
j
, s
k
),
we induced a fully-connected weighted bipartite
graph. Then, we performed a greedy approxima-
tion to the maximum weighted bipartite match-
ing in that graph (i.e., competitive linking) as fol-
lows: First, we accepted as cognates the cross-
lingual pair (p
j
, s
k
) with the highest Prod(p
j
, s
k
)
in the graph, and we discarded p
j
and s
k
from fur-
ther consideration. Then, we accepted the next
highest-scored pair, and we discarded the involved
wordforms and so forth. The process was repeated
until there were no matchable pairs left.
As a result of the above procedure, we ended
up with 28,725 Portuguese-Spanish cognate pairs,
9,201 (or 32%) of which had spelling differences.
For each pair in the list of cognate pairs, we added
spaces between any two adjacent letters for both
wordforms, and we further appended the start and
the end characters ? and $. For example, the cog-
nate pair evoluc??ao ? evoluci?on became
? e v o l u c? ?a o $ ? ? e v o l u c i ?o n $
We randomly split the resulting list into a train-
ing (26,725 pairs) and a development dataset
(2,000 pairs), and trained and tuned a character-
level phrase-based monotone SMT system similar
to (Finch and Sumita, 2008) to transliterate a Por-
tuguese wordform into a Spanish wordform. We
used a Spanish language model trained on 14M
word tokens (obtained from the above-mentioned
45.3M-token monolingual English corpus after ex-
cluding punctuation, stopwords, words of length
less than three, and those containing digits): one
per line and character-separated with added start
and end characters as in the above example. We set
both the maximum phrase length and the language
model order to ten; this value was found by tun-
ing on the development dataset. The system was
tuned using MERT, and the feature weights were
saved. The tuning Bleu was 95.22%, while the
baseline Bleu, for leaving the Portuguese words
intact, was 87.63%. Finally, the training and the
tuning datasets were merged, and a new training
round was performed. The resulting system was
used with the saved feature weights to transliterate
the Portuguese side of the training pt-en bi-text,
which yielded a new pt
es
-en training bi-text.
We did the same for Malay into Indonesian. We
extracted 5,847 cognate pairs, 844 (or 14.4%) of
which had spelling differences, and we trained a
transliteration system. The highest tuning Bleu
was 95.18% (for maximum phrase size and LM
order of 10), but the baseline was 93.15%. We
then re-trained the system on the combination of
the training and the development datasets, and we
transliterated the Malay side of the training ml-en
bi-text, obtaining a new ml
in
-en training bi-text.
1362
# Train LM Dev Test 10K 20K 40K 80K 160K 320K 640K 1230K
1 ml-en en
ml
ml-en ml-en 44.93 46.98 47.15 48.04 49.01 ? ? ?
2 ml
in
-en en
ml
ml-en ml-en 38.99 40.96 41.02 41.88 42.81 ? ? ?
3 ml-en en
ml
ml-en in-en 13.69 14.58 14.76 15.12 15.84 ? ? ?
4 ml-en en
ml
in-en in-en 13.98 14.75 14.91 15.51 16.27 ? ? ?
5 ml-en en
in
in-en in-en 15.56 16.38 16.52 17.04 17.90 ? ? ?
6 ml
in
-en en
in
in-en in-en 16.44 17.36 17.62 18.14 19.15 ? ? ?
7 pt-en en
es:pt
pt-en pt-en 21.28 23.11 24.43 25.72 26.43 27.10 27.78 27.96
8 pt
es
-en en
es:pt
pt-en pt-en 10.91 11.56 12.16 12.50 12.83 13.27 13.48 13.71
9 pt-en en
es:pt
pt-en es-en 4.40 4.77 4.57 5.02 4.99 5.32 5.08 5.34
10 pt-en en
es:pt
es-en es-en 4.91 5.12 5.64 5.82 6.35 6.87 6.44 7.10
11 pt
es
-en en
es:pt
es-en es-en 8.18 9.03 9.97 10.66 11.35 12.26 12.69 13.79
Table 1: Cross-lingual SMT experiments (shown in bold). Columns 2-5 present the bi-texts used for
training, development and testing, and the monolingual data used to train the English language model.
The following columns show the resulting Bleu (in %s) for different numbers of training sentence pairs.
3.5 Cross-lingual Translation
In this subsection, we study the similarity between
the original and the additional source languages.
First, we measured the vocabulary overlap be-
tween Spanish and Portuguese, which was fea-
sible since our training pt-en and es-en bi-texts
are from the same time span in the Europarl cor-
pus and their English sides largely overlap. We
found 110,053 Portuguese and 121,444 Spanish
word types, and 44,461 (or 36.6%) of them were
identical. Unfortunately, we could not do the same
for Malay and Indonesian since the English sides
of the in-en and ml-en bi-texts do not overlap.
Second, following the setup of the baseline sys-
tem, we performed cross-lingual experiments. The
results are shown in Table 1. As we can see, this
yielded a huge decrease in Bleu compared to the
baseline ? three to five times ? even for very large
training datasets, and even when a proper English
LM and development dataset were used: compare
line 1 to lines 3-6, and line 7 to lines 9-11.
Third, we tried transliteration. Bleu doubled for
Spanish (see lines 10-11), but improved far less for
Indonesian (lines 5-6). Training on the translit-
erated data and testing on Malay and Portuguese
yielded about 10-12% relative decrease for Malay
(lines 1-2) but 50% for Portuguese (lines 7-
8).
10
Thus, unlike Spanish and Portuguese, there
were far less systematic spelling variations be-
tween Malay and Indonesian. A closer inspec-
tion confirmed this: many extracted likely Malay-
Indonesian cognate pairs with spelling differences
were in fact forms of a word existing in both lan-
guages, e.g., kata and berkata (?to say?).
10
However, as lines 8 and 11 show, a system trained on
1.23M pt
es
-en sentence pairs, performs equally well when
translating Portuguese and Spanish text: 13.71% vs. 13.79%.
3.6 Using an Additional Language
We performed various experiments combining the
original and an additional training bi-text:
Two-tables: We built two separate phrase tables
for the two bi-texts, and we used them in the alter-
native decoding path model of Birch et al (2007).
Interpolation: We built two separate phrase
tables for the original and for the additional bi-
text, and we used linear interpolation to com-
bine the corresponding conditional probabilities:
Pr(e|s) = ?Pr
orig
(e|s) + (1 ? ?)Pr
extra
(e|s).
We optimized the value of ? on the development
dataset, trying .5, .6, .7, .8 and .9; we used the
same ? for all four conditional probabilities.
Merge: We built separate phrase tables, T
orig
and T
extra
, for the original and for the additional
training bi-text. We then concatenated them giv-
ing priority to T
orig
: We kept all phrase pairs from
T
orig
, adding to them those ones from T
extra
that
were not present in T
orig
. For each phrase pair
added, we retained its associated conditional prob-
abilities and the phrase penalty. We further added
three additional features to each entry in the new
table: F
1
, F
2
and F
3
. The value of F
1
was 1 if
the phrase pair came from T
orig
, and 0.5 other-
wise. Similarly, F
2
=1 if the phrase pair came from
T
extra
, and F
2
=0.5 otherwise. The value of F
3
was 1 if the pair came from both T
orig
and T
extra
,
and 0.5 otherwise. We experimented using (1)
F
1
only, (2) F
1
and F
2
, (3) F
1
, F
2
, and F
3
. We set
all feature weights using MERT, and we optimized
the number of features on the development set.
11
11
In theory, we should have also re-normalized the proba-
bilities since they may not sum to one. In practice, this was
not that important since the log-linear SMT model does not
require that the features be probabilities at all (e.g., the phrase
penalty), and we had extra features whose impact was bigger.
1363
Concat?k: We concatenated k copies of the
original and one copy of the additional training bi-
text; we then trained and tuned an SMT system as
for the baseline. The value for k was optimized on
the development dataset.
Concat?k:align: We concatenated k copies of
the original and one copy of the additional train-
ing bi-text. We then generated IBM model 4 word
alignments, and we truncated them, only keeping
them for one copy of the original training bi-text.
Next, we extracted phrase pairs, thus buildng a
phrase table, and we tuned an SMT system as for
the baseline.
Our Method: Our method was described in
Section 2. We used merge to combine the phrase
tables for concat?k:align and concat?1, consid-
ering the former as T
orig
and the latter as T
extra
.
We had two parameters to tune: k and the number
of extra features in the merged phrase table.
Figure 1: Impact of k on Bleu for concat?k for
different number of extra ml-en sentence pairs
in Indonesian?English SMT.
4 Results and Discussion
First, we studied the impact of k on concat?k
for Indonesian?English SMT using Malay as an
additional language. We tried all values of k
such that 1?k?16 with 10000n extra ml-en sen-
tence pairs, n?{1,2,4,8,16}. As we can see in
Figure 1, the highest Bleu scores are achieved
for (n; k)?{(1;2),(2;2),(4;4),(8;7),(16;16)}, i.e.,
when k ? n. In order to limit the search space,
we used this relationship between k and n in our
experiments (also for Portuguese and Spanish).
Table 2 shows the results for experiments on
improving Indonesian?English SMT using 10K,
20K, . . ., 160K additional ml-en pairs of paral-
lel sentences. Several observations can be made.
First, using more additional sentences yields bet-
ter results. Second, with one exception, all ex-
periments yield improvements over the baseline.
Third, the improvements are always statistically
significant for our method, according to (Collins
et al, 2005)?s sign test. Overall, among the dif-
ferent bi-text combination strategies, our method
performs best, followed by concat?k, merge, and
interpolate, which are very close in performance;
these three strategies are the only ones to consis-
tently yield higher Bleu as the number of addi-
tional ml-en sentence pairs grows. Methods like
concat?1, concat?k:align and two-tables are
somewhat inconsistent in that respect. The latter
method performs worst and is the only one to go
below the baseline (for 10K ml-en pairs).
Table 3 shows the results when using pt-en data
to improve Spanish?English SMT. Overall, the
results and the conclusions that can be made are
consistent with those for Table 2. We can further
observe that, as the size of the original bi-text in-
creases, the gain in Bleu decreases, which is to be
expected. Note also that here transliteration is very
important: it doubles the absolute gain in Bleu.
Finally, Table 4 shows a comparison to the piv-
oting technique of Callison-Burch et al (2006).
for English?Spanish SMT. Despite using just
Portuguese, we achieve an improvement that is, in
five out of six cases, much better than what they
achieve with eight pivot languages (which include
not only Portuguese, but also two other Romance
languages, French and Italian, which are closely
related to Spanish). Moreover, our method yields
improvements for very large original datasets ?
1.2M pairs, while theirs stops improving at 160K.
However, our improvements are only statistically
significant for 160K original pairs or less. Finally,
note that our translation direction is reversed.
Based on the experimental results, we can make
several conclusions. First, we have shown that us-
ing bi-text data from related languages improves
SMT: we achieved up to 1.35 and 3.37 improve-
ment in Bleu for in-en (+ml-en) and es-en (+pt-
en) respectively. Second, while simple concate-
nation can help, it is problematic when the ad-
ditional sentences out-number the ones from the
original bi-text. Third, concatenation can work
very well if the original bi-text is repeated enough
times so that the additional bi-text does not dom-
inate. Fourth, merging phrase tables giving prior-
ity to the original bi-text and using additional fea-
1364
in-en ml-en Baseline Two tables Interpol. Merge concat?1 concat?k concat?k:align Our method
28.4K 10K 23.80
< ?
23.79
<
23.89
<
(.9)
23.97
<
(3)
24.29
<
24.29
<
(1)
24.01
<
(1)
<
24.51
(2;1)
(+0.72)
28.4K 20K 23.80
<
24.24
<
24.22
<
(.8)
?
24.46
<
(3)
24.37
< ?
24.48
(2)
<
24.35
<
(2)
<
24.70
(2;2)
(+0.90)
28.4K 40K 23.80
<
24.27
<
24.27
<
(.8)
24.43
?
(3)
24.38
? ?
24.54
(4)
<
24.39
<
(4)
<
24.73
(4;2)
(+0.93)
28.4K 80K 23.80
<
24.11
< ?
24.46
<
(.8)
<
24.67
(3)
24.17
< ?
24.65
<
(8)
24.18
<
(8)
<
24.97
(8;3)
(+1.17)
28.4K 160K 23.80
< <
24.58
< <
24.58
<
(.8)
<
24.79
?
(3)
?
24.43
< <
25.00
(16)
?
24.27
<
(16)
<
25.15
(16;3)
(+1.35)
Table 2: Improving Indonesian?English SMT using ml-en data. Shown are the Bleu scores (in %s)
for different methods. A subscript shows the best parameter value(s) found on the development set and
used on the test set to produce the given result. Bleu scores that are statistically significantly better than
the baseline/our method are marked on the left/right side by
<
(for p < 0.01) or
?
(for p < 0.05).
es-en pt-en Transl. Baseline Two tables Interpol. Merge concat?1 concat?k concat?k:align Our method
10K 160K no 22.87
< <
23.81
<
23.73
(.5)
<
23.60
(2)
<
23.54
< <
23.83
<
(16)
22.93
<
(16)
<
23.98
(16;3)
(+1.11)
yes 22.87
< <
25.29
? <
25.22
<
(.5)
<
25.16
<
(2)
<
25.26
<
25.42
(16)
<
23.31
<
(16)
<
25.73
(16;3)
(+2.86)
20K 160K no 24.71
< <
25.22
?
25.02
<
(.5)
<
25.32
?
(3)
<
25.19
< <
25.29
<
(8)
24.91
<
(8)
<
25.65
(8;2)
(+0.94)
yes 24.71
< <
26.07
? <
26.07
(.7)
<
26.04
<
(3)
<
26.16
? <
26.18
?
(8)
24.88
<
(8)
<
26.36
(8;3)
(+1.65)
40K 160K no 25.80
<
25.96
<
26.15
<
(.6)
25.99
<
(3)
26.24
<
25.92
<
(4)
25.99
<
(4)
<
26.49
(4;2)
(+0.69)
yes 25.80
< <
26.68
<
26.43
(.7)
<
26.64
(3)
<
26.78
<
26.93
(4)
25.88
<
(4)
<
26.95
(4;3)
(+1.15)
80K 160K no 27.08
? ?
26.89
<
27.04
<
(.8)
27.02
<
(3)
27.23 27.09
<
(2)
27.01
<
(2)
?
27.30
(2;2)
(+0.22)
yes 27.08
<
27.20
<
27.42
(.5)
27.29
?
(3)
27.26
< ?
27.53
(2)
27.09
<
(2)
<
27.49
(2;3)
(+0.41)
160K 160K no 27.90 27.99 27.72
(.5)
27.95
(2)
27.83
<
27.83
<
(1)
27.94
(1)
28.05
(1;3)
(+0.15)
yes 27.90 28.11
?
28.13
(.6)
?
28.17
(2)
?
28.14
?
28.14
(1)
28.06
(1)
28.16
(1;2)
(+0.26)
Table 3: Improving Spanish?English SMT using 160K additional pt-en sentence pairs. Column
three shows whether transliteration was used; the following columns list the Bleu scores (in %s) for
different methods. A small subscript shows the best parameter value(s) found on the development set
and used on the test set to produce the given result. Bleu scores that are statistically significantly better
than the baseline/our method are marked on the left/right side by
<
(for p < 0.01) or
?
(for p < 0.05).
tures is a good strategy. Fifth, part of the improve-
ment when combining bi-texts is due to increased
vocabulary coverage because of cognates, but an-
other part comes from improved word alignments.
Sixth, the best results are achieved when the latter
two sources are first isolated and then combined
(our method). Finally, transliteration can help a lot
in case of systematic spelling variations between
the original and the additional source languages.
5 Related Work
In this section, we describe two general lines of
related previous research: using cognates between
the source and the target language, and source-
language side paraphrasing with a pivot language.
5.1 Cognates
Many researchers have used likely cognates to
obtain improved word alignments and thus build
better SMT systems. Al-Onaizan et al (1999)
extracted such likely cognates for Czech-English
using one of the variations of LCSR (Melamed,
1995) described in (Tiedemann, 1999) as a simi-
larity measure. They used these cognates to im-
prove word alignments with IBM models 1-4 in
three different ways: (1) by seeding the parameters
of IBM model 1, (2) by constraining the word co-
occurrences when training IBM models 1-4, and
(3) by adding the cognate pairs to the bi-text as
additional ?sentence pairs?. The last approach per-
formed best and was later used by Kondrak et al
(2003) who demonstrated improved SMT for nine
European languages.
Unlike these approaches, which extract cog-
nates between the source and the target language,
we use cognates between the source and some
other related language that is different from the
target. Moreover, we only implicitly rely on the
existence of such cognates; we do not try to ex-
tract them at all, and we leave them in their origi-
nal sentence contexts.
12
12
However, in some of our experiments, we extract cog-
nates for training a transliteration system from the resource-
rich source language X
2
into the resource-poor one X
1
.
1365
Direction System 10K 20K 40K 80K 160K 320K 1,230K
en?es baseline 22.6 25.0 26.5 26.5 28.7 30.0 ?
pivoting (+8 languages ? ?1.3M pairs) 23.3 26.0 27.2 28.0 29.0 30.0 ?
improvement +0.7 +1.0 +0.7 +1.5 +0.3 +0.0 ?
es?en baseline 22.87 24.71 25.80 27.08 27.90 28.46 29.90
our method (+1 language ? 160K pairs) 23.98
?
25.65
?
26.49
?
27.30

28.05 28.52 29.87
improvement +1.11
?
+0.94
?
+0.69
?
+0.22

+0.15 +0.06 -0.03
our method (translit., +1 lang. ? 160K) 25.73
?
26.36
?
26.95
?
27.49
?
28.16 28.43 29.94
improvement +2.86
?
+1.65
?
+1.15
?
+0.41
?
+0.26 -0.03 +0.04
our method (+1 language ? 1.23M pairs) 24.23
?
25.70
?
26.78
?
27.49 28.22

28.58 29.84
improvement +1.36
?
+0.99
?
+0.98
?
+0.41 +0.32

+0.12 -0.06
our method (translit., +1 lang. ? 1.23M) 26.24
?
26.82
?
27.47
?
27.85
?
28.50
?
28.70 29.99
improvement +3.37
?
+2.11
?
+1.67
?
+0.77
?
+0.60
?
+0.24 +0.09
Table 4: Comparison to the pivoting technique of Callison-Burch et al (2006) for English?Spanish.
Shown are Bleu scores (in %s) and absolute improvement over the baseline for training bi-texts with
different numbers of parallel sentences (10K, 20K, . . ., 1230K) and fixed amount of additional data:
(1) about 1.3M sentence pairs for each of eight additional languages in Callison-Burch et al (2006)?s
pivoting, and (2) 160K and 1,230K pairs for one language (Portuguese) for our method. Statistically
significant improvements over the baseline are marked with a
?
(for p < 0.01) and with a

(for p < 0.05).
5.2 Paraphrasing with a Pivot-Language
Another relevant line of research is on using multi-
lingual parallel corpora to improve SMT using ad-
ditional languages as pivots.
Callison-Burch et al (2006) improved
English?Spanish and English?French SMT
using source-language paraphrases extracted with
the pivoting technique of Bannard and Callison-
Burch (2005) and eight additional languages from
the Europarl corpus (Koehn, 2005). For example,
using German as a pivot, they extracted English
paraphrases from a parallel English-German
corpus by looking for English phrases that were
aligned to the same German phrase: e.g., if under
control and in check were aligned to unter con-
trolle, they were hypothesized to be paraphrases
with some probability. Such (English) paraphrases
were added as additional entries in the phrase
table of an English?Spanish/English?French
phrase-based SMT system and paired with the
foreign (Spanish/French) translation of the origi-
nal (English) phrase. The system was then tuned
with MERT using an extra feature penalizing
low-probability paraphrases; this yielded up to
1.8% absolute improvement in Bleu.
Other important publications about pivoting ap-
proaches for machine translation include (Wu and
Wang, 2007), (Utiyama and Isahara, 2007), (Haji?c
et al, 2000) and (Habash and Hu, 2009).
Unlike pivoting, which can only improve
source-language lexical coverage, we augment
both the source- and the target-language sides.
Second, while pivoting ignores context when ex-
tracting paraphrases, we take it into account.
Third, by using as an additional language one that
is related to the source, we are able to get increase
in Bleu that is comparable and even better than
what pivoting achieves with eight pivot languages.
On the negative side, our approach is limited in
that it requires that X
2
be related to X
1
, while the
pivoting language Z does not need to be related to
X
1
nor to Y . However, we only need one addi-
tional parallel corpus (for X
2
-Y ), while pivoting
needs two: one for X
1
-Z and one for Z-Y . Fi-
nally, note that our approach is orthogonal to piv-
oting, and thus the two can be combined.
6 Conclusion and Future Work
We have proposed a novel method for improving
SMT for resource-poor languages by exploiting
their similarity to resource-rich ones.
In future work, we would like to extend that ap-
proach in several interesting directions. First, we
want to make better use of multi-lingual parallel
corpora, e.g., while we had access to a Spanish-
Portuguese-English corpus, we used it as two
separate bi-texts Spanish-English and Portuguese-
English. Second, we would like to exploit multi-
ple auxiliary resource-rich languages the resource-
poor source language is related to. Third, we could
also experiment with using auxiliary languages
that are related to the target language.
Acknowledgments
This research was supported by research grant
POD0713875.
1366
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz Joseph
Och, David Purdy, Noah Smith, and David
Yarowsky. 1999. Statistical machine translation.
Technical report, CLSP, Johns Hopkins University,
Baltimore, MD.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of ACL?05, pages 597?604.
Shane Bergsma and Grzegorz Kondrak. 2007.
Alignment-based discriminative string similarity. In
Proceedings of ACL?07, pages 656?663.
Albert Bickford and David Tuggy. 2002.
Electronic glossary of linguistic terms.
http://www.sil.org/mexico/ling/glosario/E005ai-
Glossary.htm.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of WMT?2007, pages 9?
16.
Peter Brown, Vincent Della Pietra, Stephen Della
Pietra, and Robert Mercer. 1993. The mathematics
of statistical machine translation: parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine trans-
lation using paraphrases. In Proceedings of HLT-
NAACL?06, pages 17?24.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL?05, pages 531?
540.
Andrew Finch and Eiichiro Sumita. 2008. Phrase-
based machine transliteration. In Proceedings of
WTCAST?08, pages 13?18.
Nizar Habash and Jun Hu. 2009. Improving Arabic-
Chinese statistical machine translation using English
as pivot language. In Proceedings of the WMT?09,
pages 173?181.
Jan Haji?c, Jan Hric, and Vladislav Kubo?n. 2000. Ma-
chine translation of very close languages. In Pro-
ceedings of ANLP?00, pages 7?12.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation.
In Proceedings of IWSLT?05.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of ACL?07. Demonstration ses-
sion, pages 177?180.
Philipp Koehn. 2005. Europarl: A parallel corpus for
evaluation of machine translation. In Proceedings of
MT Summit, pages 79?86.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can improve statistical translation
models. In Proceedings of NAACL?03, pages 46?48.
Gideon Mann and David Yarowsky. 2001. Multipath
translation lexicon induction via bridge languages.
In Proceedings of NAACL?01, pages 1?8.
Dan Melamed. 1995. Automatic evaluation and uni-
form filter cascades for inducing N-best translation
lexicons. In Proceedings of WVLC?95, pages 184?
198.
Dan Melamed. 1999. Bitext maps and alignment
via pattern recognition. Computational Linguistics,
25(1):107?130.
Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL?03, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL?02, pages 311?318.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and
Daniel Varga. 2006. The JRC-Acquis: A multilin-
gual aligned parallel corpus with 20+ languages. In
Proceedings of LREC?2006, pages 2142?2147.
Jorg Tiedemann. 1999. Automatic construction of
weighted string similarity measures. In Proceedings
of EMNLP-VLC?99, pages 213?219.
Masao Utiyama and Hitoshi Isahara. 2007. A com-
parison of pivot methods for phrase-based statisti-
cal machine translation. In Proceedings of NAACL-
HLT?07, pages 484?491.
Hua Wu and Haifeng Wang. 2007. Pivot language ap-
proach for phrase-based statistical machine transla-
tion. Machine Translation, 21(3):165?181.
1367
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 75?79,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
NUS at WMT09: Domain Adaptation Experiments for English-Spanish
Machine Translation of News Commentary Text
Preslav Nakov
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
nakov@comp.nus.edu.sg
Hwee Tou Ng
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
nght@comp.nus.edu.sg
Abstract
We describe the system developed by the
team of the National University of Singa-
pore for English to Spanish machine trans-
lation of News Commentary text for the
WMT09 Shared Translation Task. Our
approach is based on domain adaptation,
combining a small in-domain News Com-
mentary bi-text and a large out-of-domain
one from the Europarl corpus, from which
we built and combined two separate phrase
tables. We further combined two language
models (in-domain and out-of-domain),
and we experimented with cognates, im-
proved tokenization and recasing, achiev-
ing the highest lowercased NIST score of
6.963 and the second best lowercased Bleu
score of 24.91% for training without us-
ing additional external data for English-to-
Spanish translation at the shared task.
1 Introduction
Modern Statistical Machine Translation (SMT)
systems are typically trained on sentence-aligned
parallel texts (bi-texts) from a particular domain.
When tested on text from that domain, they
demonstrate state-of-the art performance, but on
out-of-domain test data the results can deteriorate
significantly. For example, on the WMT06 Shared
Translation Task, the scores for French-to-English
translation dropped from about 30 to about 20
Bleu points for nearly all systems when tested on
News Commentary instead of the Europarl1 text,
which was used for training (Koehn and Monz,
2006).
1See (Koehn, 2005) for details about the Europarl corpus.
Subsequently, in 2007 and 2008, the WMT
Shared Translation Task organizers provided a
limited amount of bilingual News Commentary
training data (1-1.3M words) in addition to the
large amount of Europarl data (30-32M words),
and set up separate evaluations on News Commen-
tary and on Europarl data, thus inviting interest in
domain adaptation experiments for the News do-
main (Callison-Burch et al, 2007; Callison-Burch
et al, 2008). This year, the evaluation is on News
Commentary only, which makes domain adapta-
tion the central focus of the Shared Translation
Task.
The team of the National University of Singa-
pore (NUS) participated in the WMT09 Shared
Translation Task with an English-to-Spanish sys-
tem.2 Our approach is based on domain adapta-
tion, combining the small in-domain News Com-
mentary bi-text (1.8M words) and the large out-
of-domain one from the Europarl corpus (40M
words), from which we built and combined two
separate phrase tables. We further used two
language models (in-domain and out-of-domain),
cognates, improved tokenization, and additional
smart recasing as a post-processing step.
2 The NUS System
Below we describe separately the standard and the
nonstandard settings of our system.
2.1 Standard Settings
In our baseline experiments, we used the follow-
ing general setup: First, we tokenized the par-
2The task organizers invited submissions translating for-
ward and/or backward between English and five other Euro-
pean languages (French, Spanish, German, Czech and Hun-
garian), but we only participated in English?Spanish, due to
time limitations.
75
allel bi-text, converted it to lowercase, and fil-
tered out the overly-long training sentences, which
complicate word alignments (we tried maximum
length limits of 40 and 100). We then built sep-
arate English-to-Spanish and Spanish-to-English
directed word alignments using IBM model 4
(Brown et al, 1993), combined them using the in-
tersect+grow heuristic (Och and Ney, 2003), and
extracted phrase-level translation pairs of maxi-
mum length 7 using the alignment template ap-
proach (Och and Ney, 2004). We thus obtained
a phrase table where each phrase translation pair
is associated with the following five standard pa-
rameters: forward and reverse phrase translation
probabilities, forward and reverse lexical transla-
tion probabilities, and phrase penalty.
We then trained a log-linear model using the
standard feature functions: language model proba-
bility, word penalty, distortion costs (we tried dis-
tance based and lexicalized reordering models),
and the parameters from the phrase table. We
set al feature weights by optimizing Bleu (Pap-
ineni et al, 2002) directly using minimum error
rate training (MERT) (Och, 2003) on the tuning
part of the development set (dev-test2009a).
We used these weights in a beam search decoder
(Koehn et al, 2007) to translate the test sentences
(the English part of dev-test2009b, tokenized
and lowercased). We then recased the output us-
ing a monotone model that translates from low-
ercase to uppercase Spanish, we post-cased it us-
ing a simple heuristic, de-tokenized the result, and
compared it to the gold standard (the Spanish part
of dev-test2009b) using Bleu and NIST.
2.2 Nonstandard Settings
The nonstandard features of our system can be
summarized as follows:
Two Language Models. Following Nakov
and Hearst (2007), we used two language mod-
els (LM) ? an in-domain one (trained on a con-
catenation of the provided monolingual Spanish
News Commentary data and the Spanish side of the
training News Commentary bi-text) and an out-of-
domain one (trained on the provided monolingual
Spanish Europarl data). For both LMs, we used
5-gram models with Kneser-Ney smoothing.
Merging Two Phrase Tables. Following
Nakov (2008), we trained and merged two phrase-
based SMT systems: a small in-domain one using
the News Commentary bi-text, and a large out-of-
domain one using the Europarl bi-text. As a result,
we obtained two phrase tables, Tnews and Teuro,
and two lexicalized reordering models, Rnews and
Reuro. We merged the phrase table as follows.
First, we kept all phrase pairs from Tnews. Then
we added those phrase pairs from Teuro which
were not present in Tnews. For each phrase pair
added, we retained its associated features: forward
and reverse phrase translation probabilities, for-
ward and reverse lexical translation probabilities,
and phrase penalty. We further added two new fea-
tures, Fnews and Feuro, which show the source of
each phrase. Their values are 1 and 0.5 when the
phrase was extracted from the News Commentary
bi-text, 0.5 and 1 when it was extracted from the
Europarl bi-text, and 1 and 1 when it was extracted
from both. As a result, we ended up with seven pa-
rameters for each entry in the merged phrase table.
Merging Two Lexicalized Reordering Tables.
When building the two phrase tables, we also
built two lexicalized reordering tables (Koehn et
al., 2005) for them, Rnews and Reuro, which we
merged as follows: We first kept all phrases from
Rnews, then we added those from Reuro which
were not present in Rnews. This resulting lexical-
ized reordering table was used together with the
above-described merged phrase table.
Cognates. Previous research has shown that us-
ing cognates can yield better word alignments (Al-
Onaizan et al, 1999; Kondrak et al, 2003), which
in turn often means higher-quality phrase pairs and
better SMT systems. Linguists define cognates
as words derived from a common root (Bickford
and Tuggy, 2002). Following previous researchers
in computational linguistics (Bergsma and Kon-
drak, 2007; Mann and Yarowsky, 2001; Melamed,
1999), however, we adopted a simplified definition
which ignores origin, defining cognates as words
in different languages that are mutual translations
and have a similar orthography. We extracted and
used such potential cognates in order to bias the
training of the IBM word alignment models. Fol-
lowing Melamed (1995), we measured the ortho-
graphic similarity using longest common subse-
quence ratio (LCSR), which is defined as follows:
LCSR(s1, s2) = |LCS(s1,s2)|max(|s1|,|s2|)
where LCS(s1, s2) is the longest common subse-
quence of s1 and s2, and |s| is the length of s.
Following Nakov et al (2007), we combined the
LCSR similarity measure with competitive linking
(Melamed, 2000) in order to extract potential cog-
76
nates from the training bi-text. Competitive link-
ing assumes that, given a source English sentence
and its Spanish translation, a source word is ei-
ther translated with a single target word or is not
translated at all. Given an English-Spanish sen-
tence pair, we calculated LCSR for all cross-lingual
word pairs (excluding stopwords and words of
length 3 or less), which induced a fully-connected
weighted bipartite graph. Then, we performed a
greedy approximation to the maximum weighted
bipartite matching in that graph (competitive link-
ing) as follows: First, we aligned the most sim-
ilar pair of unaligned words and we discarded
these words from further consideration. Then, we
aligned the next most similar pair of unaligned
words, and so forth. The process was repeated un-
til there were no words left or the maximal word
pair similarity fell below a pre-specified threshold
? (0 ? ? ? 1), which typically left some words
unaligned.3 As a result we ended up with a list C
of potential cognate pairs. Following (Al-Onaizan
et al, 1999; Kondrak et al, 2003; Nakov et al,
2007) we filtered out the duplicates in C , and we
added the remaining cognate pairs as additional
?sentence? pairs to the bi-text in order to bias the
subsequent training of the IBM word alignment
models.
Improved (De-)tokenization. The default to-
kenizer does not split on hyphenated compound
words like nation-building, well-rehearsed, self-
assured, Arab-Israeli, domestically-oriented, etc.
While linguistically correct, this can be problem-
atic for machine translation since it can cause data
sparsity issues. For example, the system might
know how to translate into Spanish both well and
rehearsed, but not well-rehearsed, and thus at
translation time it would be forced to handle it as
an unknown word, i.e., copy it to the output un-
translated. A similar problem is related to double
dashes, as illustrated by the following training sen-
tence: ?So the question now is what can China do
to freeze--and, if possible, to reverse--North Ko-
rea?s nuclear program.? We changed the tokenizer
so that it splits on ?-? and ?--?; we altered the de-
tokenizer accordingly.
Improved Recaser. The default recaser sug-
gested by the WMT09 organizers was based on a
monotone translation model. We trained such a
recaser on the Spanish side of the News Commen-
3For News Commentary, we used ? = 0.4, which was
found by optimizing on the development set; for Europarl,
we set ? = 0.58 as suggested by Kondrak et al (2003).
tary bi-text that translates from lowercase to up-
percase Spanish. While being good overall, it had
a problem with unknown words, leaving them in
lowercase. In a News Commentary text, however,
most unknown words are named entities ? persons,
organization, locations ? which are spelled with a
capitalized initial in Spanish. Therefore, we used
an additional recasing script, which runs over the
output of the default recaser and sets the casing of
the unknown words to the original casing they had
in the English input. It also makes sure all sen-
tences start with a capitalized initial.
Rule-based Post-editing. We did a quick study
of the system errors on the development set, and
we designed some heuristic post-editing rules,
e.g.,
? ? or ! without ? or ? to the left: if so, we
insert ?/? at the sentence beginning;
? numbers: we change English numbers like
1,185.32 to Spanish-style 1.185,32;
? duplicate punctuation: we remove dupli-
cate sentence end markers, quotes, commas,
parentheses, etc.
3 Experiments and Evaluation
Table 1 shows the performance of a simple
baseline system and the impact of different
cumulative modifications to that system when
tuning on dev-test2009a and testing on
dev-test2009b. The table report the Bleu and
NIST scores measured on the detokenized out-
put under three conditions: (1) without recasing
(?Lowercased?), 2) using the default recaser (?Re-
cased (default)?), and (3) using an improved re-
caser and post-editing rules Post-cased & Post-
edited?). In the following discussion, we will dis-
cuss the Bleu results under condition (3).
System 1 uses sentences of length up to 40
tokens from the News Commentary bi-text, the
default (de-)tokenizer, distance reordering, and a
3-gram language model trained on the Spanish
side of the bi-text. Its performance is quite mod-
est: 15.32% of Bleu with the default recaser, and
16.92% when the improved recaser and the post-
editing rules are used.
System 2 increases to 100 the maximum length
of the sentences in the bi-text, which yields 0.55%
absolute improvement in Bleu.
System 3 uses the new (de-)tokenizer, but this
turns out to make almost no difference.
77
Recased Post-cased &
Lowercased (default) Post-edited
# Bitext System Bleu NIST Bleu NIST Bleu NIST
1 news News Commentary baseline 18.38 5.7837 15.32 5.2266 16.92 5.5091
2 news + max sentence length 100 18.91 5.8540 15.93 5.3119 17.47 5.5874
3 news + improved (de-)tokenizer 18.96 5.8706 15.97 5.3254 17.48 5.6020
4 news + lexicalized reordering 19.81 5.9422 16.64 5.3793 18.28 5.6696
5 news + LM: old+monol. News, 5-gram 22.29 6.2791 18.91 5.6901 20.55 5.9924
6 news + LM2: Europarl, 5-gram 22.46 6.2438 19.10 5.6606 20.75 5.9570
7 news + cognates 23.14 6.3504 19.64 5.7478 21.32 6.0478
8 euro Europarl (? system 6) 23.73 6.4673 20.23 5.8707 21.89 6.1577
9 euro + cognates (? system 7) 23.95 6.4709 20.44 5.8742 22.10 6.1607
10 both Combining 7 & 9 24.40 6.5723 20.74 5.9575 22.37 6.2506
Table 1: Impact of the combined modifications for English-to-Spanish machine translation on
dev-test2009b. We report the Bleu and NIST scores measured on the detokenized output under
three conditions: (1) without recasing (?Lowercased?), (2) using the default recaser (?Recased (default)?),
and (3) using an improved recaser and post-editing rules (?Post-cased & Post-edited?). The News Com-
mentary baseline system uses sentences of length up to 40 tokens from the News Commentary bi-text,
the default tokenizer and de-tokenizer, a distance-based reordering model, and a trigram language model
trained on the Spanish side of the bi-text. The Europarl system is the same as system 6, except that it
uses the Europarl bi-text instead of the News Commentary bi-text.
System 4 adds a lexicalized re-ordering model,
which yields 0.8% absolute improvement.
System 5 improves the language model. It adds
the additional monolingual Spanish News Com-
mentary data provided by the organizers to the
Spanish side of the bi-text, and uses a 5-gram lan-
guage model instead of the 3-gram LM used by
Systems 1-4. This yields a sizable absolute gain in
Bleu: 2.27%.
System 6 adds a second 5-gram LM trained on
the monolingual Europarl data, gaining 0.2%.
System 7 augments the training bi-text with
cognate pairs, gaining another 0.57%.
System 8 is the same as System 6, except that
it is trained on the out-of-domain Europarl bi-
text instead of the in-domain News Commentary
bi-text. Surprisingly, this turns out to work bet-
ter than the in-domain System 6 by 1.14% of
Bleu. This is a quite surprising result since in
both WMT07 and WMT08, for which compara-
ble kinds and size of training data was provided,
training on the out-of-domain Europarl was al-
ways worse than training on the in-domain News
Commentary. We are not sure why it is different
this year, but it could be due to the way the dev-
train and dev-test was created for the 2009 data ?
by extracting alternating sentences from the origi-
nal development set.
System 9 augments the Europarl bi-text with
cognate pairs, gaining another 0.21%.
System 10 merges the phrase tables of systems
7 and 9, and is otherwise the same as them. This
adds another 0.27%.
Our official submission to WMT09 is the post-
edited System 10, re-tuned on the full development
set: dev-test2009a + dev-test2009b (in
order to produce more stable results with MERT).
4 Conclusion and Future Work
As we can see in Table 1, we have achieved not
only a huge ?vertical? absolute improvement of
5.5-6% in Bleu from System 1 to System 10, but
also a significant ?horizontal? one: our recased and
post-edited result for System 10 is better than that
of the default recaser by 1.63% in Bleu (22.37%
vs. 20.74%). Still, the lowercased Bleu of 24.40%
suggests that there may be a lot of room for fur-
ther improvement in recasing ? we are still about
2% below it. While this is probably due primarily
to the system choosing a different sentence-initial
word, it certainly deserves further investigation in
future work.
Acknowledgments
This research was supported by research grant
POD0713875.
78
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz Joseph
Och, David Purdy, Noah Smith, and David
Yarowsky. 1999. Statistical machine translation.
Technical report, CLSP, Johns Hopkins University,
Baltimore, MD.
Shane Bergsma and Grzegorz Kondrak. 2007.
Alignment-based discriminative string similarity. In
Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?07), pages
656?663, Prague, Czech Republic.
Albert Bickford and David Tuggy. 2002.
Electronic glossary of linguistic terms.
http://www.sil.org/mexico/ling/glosario/E005ai-
Glossary.htm.
Peter Brown, Vincent Della Pietra, Stephen Della
Pietra, and Robert Mercer. 1993. The mathematics
of statistical machine translation: parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 136?158, Prague, Czech
Republic.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 70?106, Columbus,
OH, USA.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of the First
Workshop on Statistical Machine Translation, pages
102?121, New York, NY, USA.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation (IWSLT?05), Pitts-
burgh, PA, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine trans-
lation. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics
(ACL?07). Demonstration session, pages 177?180,
Prague, Czech Republic.
P. Koehn. 2005. Europarl: A parallel corpus for eval-
uation of machine translation. In Proceedings of the
X MT Summit, pages 79?86, Phuket, Thailand.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can improve statistical translation
models. In Proceedings of the Annual Meeting of the
North American Association for Computational Lin-
guistics (NAACL?03), pages 46?48, Sapporo, Japan.
Gideon Mann and David Yarowsky. 2001. Multipath
translation lexicon induction via bridge languages.
In Proceedings of the Annual Meeting of the North
American Association for Computational Linguis-
tics (NAACL?01), pages 1?8, Pittsburgh, PA, USA.
Dan Melamed. 1995. Automatic evaluation and uni-
form filter cascades for inducing N-best translation
lexicons. In Proceedings of the Third Workshop on
Very Large Corpora, pages 184?198, Cambridge,
MA, USA.
Dan Melamed. 1999. Bitext maps and alignment
via pattern recognition. Computational Linguistics,
25(1):107?130.
Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Preslav Nakov and Marti Hearst. 2007. UCB system
description for the WMT 2007 shared task. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 212?215, Prague, Czech
Republic.
Preslav Nakov, Svetlin Nakov, and Elena Paskaleva.
2007. Improved word alignments using the Web as
a corpus. In Proceedigs of Recent Advances in Nat-
ural Language Processing (RANLP?07), pages 400?
405, Borovets, Bulgaria.
Preslav Nakov. 2008. Improving English-Spanish sta-
tistical machine translation: Experiments in domain
adaptation, sentence paraphrasing, tokenization, and
recasing. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 147?150,
Columbus, OH, USA.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics (ACL?03), pages 160?167, Sap-
poro, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics (ACL?02), pages 311?318,
Philadelphia, PA, USA.
79
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 94?99,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 8: Multi-Way Classification
of Semantic Relations Between Pairs of Nominals
Iris Hendrickx? , Su Nam Kim? , Zornitsa Kozareva? , Preslav Nakov? ,
Diarmuid O? Se?aghdha?, Sebastian Pado?? , Marco Pennacchiotti??,
Lorenza Romano??, Stan Szpakowicz??
Abstract
We present a brief overview of the main
challenges in the extraction of semantic
relations from English text, and discuss the
shortcomings of previous data sets and shared
tasks. This leads us to introduce a new
task, which will be part of SemEval-2010:
multi-way classification of mutually exclusive
semantic relations between pairs of common
nominals. The task is designed to compare
different approaches to the problem and to
provide a standard testbed for future research,
which can benefit many applications in
Natural Language Processing.
1 Introduction
The computational linguistics community has a con-
siderable interest in robust knowledge extraction,
both as an end in itself and as an intermediate step
in a variety of Natural Language Processing (NLP)
applications. Semantic relations between pairs of
words are an interesting case of such semantic
knowledge. It can guide the recovery of useful facts
about the world, the interpretation of a sentence, or
even discourse processing. For example, pears and
bowl are connected in a CONTENT-CONTAINER re-
lation in the sentence ?The bowl contained apples,
?University of Antwerp, iris.hendrickx@ua.ac.be
?University of Melbourne, snkim@csse.unimelb.edu.au
?University of Alicante, zkozareva@dlsi.ua.es
?National University of Singapore, nakov@comp.nus.edu.sg
?University of Cambridge, do242@cl.cam.ac.uk
?University of Stuttgart, pado@stanford.edu
??Yahoo! Inc., pennacc@yahoo-inc.com
??Fondazione Bruno Kessler, romano@fbk.eu
??University of Ottawa and Polish Academy of Sciences,
szpak@site.uottawa.ca
pears, and oranges.?, while ginseng and taste are in
an ENTITY-ORIGIN relation in ?The taste is not from
alcohol, but from the ginseng.?.
The automatic recognition of semantic relations
can have many applications, such as information
extraction (IE), document summarization, machine
translation, or construction of thesauri and seman-
tic networks. It can also facilitate auxiliary tasks
such as word sense disambiguation, language mod-
eling, paraphrasing or recognizing textual entail-
ment. For example, semantic network construction
can benefit from detecting a FUNCTION relation be-
tween airplane and transportation in ?the airplane
is used for transportation? or a PART-WHOLE rela-
tion in ?the car has an engine?. Similarly, all do-
mains that require deep understanding of text rela-
tions can benefit from knowing the relations that de-
scribe events like ACQUISITION between named en-
tities in ?Yahoo has made a definitive agreement to
acquire Flickr?.
In this paper, we focus on the recognition of se-
mantic relations between pairs of common nomi-
nals. We present a task which will be part of the
SemEval-2010 evaluation exercise and for which we
are developing a new benchmark data set. This data
set and the associated task address three significant
problems encountered in previous work: (1) the def-
inition of a suitable set of relations; (2) the incorpo-
ration of context; (3) the desire for a realistic exper-
imental design. We outline these issues in Section
2. Section 3 describes the inventory of relations we
adopted for the task. The annotation process, the
design of the task itself and the evaluation method-
ology are presented in Sections 4-6.
94
2 Semantic Relation Classification: Issues
2.1 Defining the Relation Inventory
A wide variety of relation classification schemes ex-
ist in the literature, reflecting the needs and granular-
ities of various applications. Some researchers only
investigate relations between named entities or in-
ternal to noun-noun compounds, while others have a
more general focus. Some schemes are specific to a
domain such as biomedical text.
Rosario and Hearst (2001) classify noun com-
pounds from the domain of medicine into 13 classes
that describe the semantic relation between the head
noun and the modifier. Rosario et al (2002) classify
noun compounds using the MeSH hierarchy and a
multi-level hierarchy of semantic relations, with 15
classes at the top level. Stephens et al (2001) pro-
pose 17 very specific classes targeting relations be-
tween genes. Nastase and Szpakowicz (2003) ad-
dress the problem of classifying noun-modifier rela-
tions in general text. They propose a two-level hier-
archy, with 5 classes at the first level and 30 classes
at the second one; other researchers (Kim and Bald-
win, 2005; Nakov and Hearst, 2008; Nastase et al,
2006; Turney, 2005; Turney and Littman, 2005)
have used their class scheme and data set. Moldovan
et al (2004) propose a 35-class scheme to classify
relations in various phrases; the same scheme has
been applied to noun compounds and other noun
phrases (Girju et al, 2005). Lapata (2002) presents a
binary classification of relations in nominalizations.
Pantel and Pennacchiotti (2006) concentrate on five
relations in an IE-style setting. In short, there is little
agreement on relation inventories.
2.2 The Role of Context
A fundamental question in relation classification is
whether the relations between nominals should be
considered out of context or in context. When one
looks at real data, it becomes clear that context does
indeed play a role. Consider, for example, the noun
compound wood shed : it may refer either to a shed
made of wood, or to a shed of any material used to
store wood. This ambiguity is likely to be resolved
in particular contexts. In fact, most NLP applica-
tions will want to determine not all possible relations
between two words, but rather the relation between
two instances in a particular context. While the in-
tegration of context is common in the field of IE (cf.
work in the context of ACE1), much of the exist-
ing literature on relation extraction considers word
pairs out of context (thus, types rather than tokens).
A notable exception is SemEval-2007 Task 4 Clas-
sification of Semantic Relations between Nominals
(Girju et al, 2007; Girju et al, 2008), the first to of-
fer a standard benchmark data set for seven semantic
relations between common nouns in context.
2.3 Style of Classification
The design of SemEval-2007 Task 4 had an im-
portant limitation. The data set avoided the chal-
lenge of defining a single unified standard classifi-
cation scheme by creating seven separate training
and test sets, one for each semantic relation. That
made the relation recognition task on each data set
a simple binary (positive / negative) classification
task.2 Clearly, this does not easily transfer to prac-
tical NLP settings, where any relation can hold be-
tween a pair of nominals which occur in a sentence
or a discourse.
2.4 Summary
While there is a substantial amount of work on re-
lation extraction, the lack of standardization makes
it difficult to compare different approaches. It is
known from other fields that the availability of stan-
dard benchmark data sets can provide a boost to the
advancement of a field. As a first step, SemEval-
2007 Task 4 offered many useful insights into the
performance of different approaches to semantic re-
lation classification; it has also motivated follow-
up research (Davidov and Rappoport, 2008; Ka-
trenko and Adriaans, 2008; Nakov and Hearst, 2008;
O? Se?aghdha and Copestake, 2008).
Our objective is to build on the achievements of
SemEval-2007 Task 4 while addressing its short-
comings. In particular, we consider a larger set of
semantic relations (9 instead of 7), we assume a
proper multi-class classification setting, we emulate
the effect of an ?open? relation inventory by means
of a tenth class OTHER, and we will release to the
research community a data set with a considerably
1http://www.itl.nist.gov/iad/mig/tests/
ace/
2Although it was not designed for a multi-class set-up, some
subsequent publications tried to use the data sets in that manner.
95
larger number of examples than SemEval-2007 Task
4 or other comparable data sets. The last point is cru-
cial for ensuring the robustness of the performance
estimates for competing systems.
3 Designing an Inventory of Semantic Re-
lations Between Nominals
We begin by considering the first of the problems
listed above: defining of an inventory of semantic
relations. Ideally, it should be exhaustive (should al-
low the description of relations between any pair of
nominals) and mutually exclusive (each pair of nom-
inals in context should map onto only one relation).
The literature, however, suggests no such inventory
that could satisfy all needs. In practice, one always
must decide on a trade-off between these two prop-
erties. For example, the gene-gene relation inven-
tory of Stephens et al (2001), with relations like X
phosphorylates Y, arguably allows no overlaps, but
is too specific for applications to general text.
On the other hand, schemes aimed at exhaus-
tiveness tend to run into overlap issues, due
to such fundamental linguistic phenomena as
metaphor (Lakoff, 1987). For example, in the sen-
tence Dark clouds gather over Nepal., the relation
between dark clouds and Nepal is literally a type of
ENTITY-DESTINATION, but in fact it refers to the
ethnic unrest in Nepal.
We seek a pragmatic compromise between the
two extremes. We have selected nine relations with
sufficiently broad coverage to be of general and
practical interest. We aim at avoiding ?real? overlap
to the extent that this is possible, but we include two
sets of similar relations (ENTITY-ORIGIN/ENTITY-
DESTINATION and CONTENT-CONTAINER/COM-
PONENT-WHOLE/MEMBER-COLLECTION), which
can help assess the models? ability to make such
fine-grained distinctions.3
As in Semeval-2007 Task 4, we give ordered two-
word names to the relations, where each word de-
scribes the role of the corresponding argument. The
full list of our nine relations follows4 (the definitions
we show here are intended to be indicative rather
than complete):
3COMPONENT-WHOLE and MEMBER-COLLECTION are
proper subsets of PART-WHOLE, one of the relations in
SemEval-2007 Task 4.
4We have taken the first five from SemEval-2007 Task 4.
Cause-Effect. An event or object leads to an effect.
Example: Smoking causes cancer.
Instrument-Agency. An agent uses an instrument.
Example: laser printer
Product-Producer. A producer causes a product to
exist. Example: The farmer grows apples.
Content-Container. An object is physically stored
in a delineated area of space, the container. Ex-
ample: Earth is located in the Milky Way.
Entity-Origin. An entity is coming or is derived
from an origin (e.g., position or material). Ex-
ample: letters from foreign countries
Entity-Destination. An entity is moving towards a
destination. Example: The boy went to bed.
Component-Whole. An object is a component of a
larger whole. Example: My apartment has a
large kitchen.
Member-Collection. A member forms a nonfunc-
tional part of a collection. Example: There are
many trees in the forest.
Communication-Topic. An act of communication,
whether written or spoken, is about a topic. Ex-
ample: The lecture was about semantics.
We add a tenth element to this set, the pseudo-
relation OTHER. It stands for any relation which
is not one of the nine explicitly annotated relations.
This is motivated by modelling considerations. Pre-
sumably, the data for OTHER will be very nonho-
mogeneous. By including it, we force any model of
the complete data set to correctly identify the deci-
sion boundaries between the individual relations and
?everything else?. This encourages good generaliza-
tion behaviour to larger, noisier data sets commonly
seen in real-world applications.
3.1 Semantic Relations versus Semantic Roles
There are three main differences between our task
(classification of semantic relations between nomi-
nals) and the related task of automatic labeling of
semantic roles (Gildea and Jurafsky, 2002).
The first difference is to do with the linguistic
phenomena described. Lexical resources for theo-
ries of semantic roles such as FrameNet (Fillmore et
96
al., 2003) and PropBank (Palmer et al, 2005) have
been developed to describe the linguistic realization
patterns of events and states. Thus, they target pri-
marily verbs (or event nominalizations) and their de-
pendents, which are typically nouns. In contrast,
semantic relations may occur between all parts of
speech, although we limit our attention to nominals
in this task. Also, semantic role descriptions typi-
cally relate an event to a set of multiple participants
and props, while semantic relations are in practice
(although not necessarily) binary.
The second major difference is the syntactic con-
text. Theories of semantic roles usually developed
out of syntactic descriptions of verb valencies, and
thus they focus on describing the linking patterns of
verbs and their direct dependents, phenomena like
raising and noninstantiations notwithstanding (Fill-
more, 2002). Semantic relations are not tied to
predicate-argument structures. They can also be es-
tablished within noun phrases, noun compounds, or
sentences more generally (cf. the examples above).
The third difference is that of the level of gen-
eralization. FrameNet currently contains more than
825 different frames (event classes). Since the se-
mantic roles are designed to be interpreted at the
frame level, there is a priori a very large number
of unrelated semantic roles. There is a rudimen-
tary frame hierarchy that defines mappings between
roles of individual frames,5 but it is far from com-
plete. The situation is similar in PropBank. Prop-
Bank does use a small number of semantic roles, but
these are again to be interpreted at the level of in-
dividual predicates, with little cross-predicate gen-
eralization. In contrast, all of the semantic relation
inventories discussed in Section 1 contain fewer than
50 types of semantic relations. More generally, se-
mantic relation inventories attempt to generalize re-
lations across wide groups of verbs (Chklovski and
Pantel, 2004) and include relations that are not verb-
centered (Nastase and Szpakowicz, 2003; Moldovan
et al, 2004). Using the same labels for similar se-
mantic relations facilitates supervised learning. For
example, a model trained with examples of sell re-
lations should be able to transfer what it has learned
to give relations. This has the potential of adding
5For example, it relates the BUYER role of the COM-
MERCE SELL frame (verb sell ) to the RECIPIENT role of the
GIVING frame (verb give).
1. People in Hawaii might be feeling
<e1>aftershocks</e1> from that power-
ful <e2>earthquake</e2> for weeks.
2. My new <e1>apartment</e1> has a
<e2>large kitchen</e2>.
Figure 1: Two example sentences with annotation
crucial robustness and coverage to analysis tools in
NLP applications based on semantic relations.
4 Annotation
The next step in our study will be the actual annota-
tion of relations between nominals. For the purpose
of annotation, we define a nominal as a noun or a
base noun phrase. A base noun phrase is a noun and
its pre-modifiers (e.g., nouns, adjectives, determin-
ers). We do not include complex noun phrases (e.g.,
noun phrases with attached prepositional phrases or
relative clauses). For example, lawn is a noun, lawn
mower is a base noun phrase, and the engine of the
lawn mower is a complex noun phrase.
We focus on heads that are common nouns. This
emphasis distinguishes our task from much work in
IE, which focuses on named entities and on consid-
erably more fine-grained relations than we do. For
example, Patwardhan and Riloff (2007) identify cat-
egories like Terrorist organization as participants in
terror-related semantic relations, which consists pre-
dominantly of named entities. We feel that named
entities are a specific category of nominal expres-
sions best dealt with using techniques which do not
apply to common nouns; for example, they do not
lend themselves well to semantic generalization.
Figure 1 shows two examples of annotated sen-
tences. The XML tags <e1> and <e2> mark the
target nominals. Since all nine proper semantic re-
lations in this task are asymmetric, the ordering of
the two nominals must be taken into account. In
example 1, CAUSE-EFFECT(e1, e2) does not hold,
although CAUSE-EFFECT(e2, e1) would. In exam-
ple 2, COMPONENT-WHOLE(e2, e1) holds.
We are currently developing annotation guide-
lines for each of the relations. They will give a pre-
cise definition for each relation and some prototypi-
cal examples, similarly to SemEval-2007 Task 4.
The annotation will take place in two rounds. In
the first round, we will do a coarse-grained search
97
for positive examples for each relation. We will
collect data from the Web using a semi-automatic,
pattern-based search procedure. In order to ensure
a wide variety of example sentences, we will use
several dozen patterns per relation. We will also
ensure that patterns retrieve both positive and nega-
tive example sentences; the latter will help populate
the OTHER relation with realistic near-miss negative
examples of the other relations. The patterns will
be manually constructed following the approach of
Hearst (1992) and Nakov and Hearst (2008).6
The example collection for each relation R will
be passed to two independent annotators. In order to
maintain exclusivity of relations, only examples that
are negative for all relations but R will be included
as positive and only examples that are negative for
all nine relations will be included as OTHER. Next,
the annotators will compare their decisions and as-
sess inter-annotator agreement. Consensus will be
sought; if the annotators cannot agree on an exam-
ple it will not be included in the data set, but it will
be recorded for future analysis.
Finally, two other task organizers will look for
overlap across all relations. They will discard any
example marked as positive in two or more relations,
as well as examples in OTHER marked as positive in
any of the other classes. The OTHER relation will,
then, consist of examples that are negatives for all
other relations and near-misses for any relation.
Data sets. The annotated data will be divided into
a training set, a development set and a test set. There
will be 1000 annotated examples for each of the
ten relations: 700 for training, 100 for development
and 200 for testing. All data will be released under
the Creative Commons Attribution 3.0 Unported Li-
cense7. The annotation guidelines will be included
in the distribution.
5 The Classification Task
The actual task that we will run at SemEval-2010
will be a multi-way classification task. Not all pairs
of nominals in each sentence will be labeled, so the
gold-standard boundaries of the nominals to be clas-
sified will be provided as part of the test data.
6Note that, unlike in Semeval 2007 Task 4, we will not re-
lease the patterns to the participants.
7http://creativecommons.org/licenses/by/
3.0/
In contrast with Semeval 2007 Task 4, in which
the ordering of the entities was provided with each
example, we aim at a more realistic scenario in
which the ordering of the labels is not given. Par-
ticipants in the task will be asked to discover both
the relation and the order of the arguments. Thus,
the more challenging task is to identify the most
informative ordering and relation between a pair
of nominals. The stipulation ?most informative?
is necessary since with our current set of asym-
metrical relations that includes OTHER, each pair
of nominals that instantiates a relation in one di-
rection (e.g., REL(e1, e2)), instantiates OTHER in
the inverse direction (OTHER (e2, e1)). Thus, the
correct answers for the two examples in Figure 1
are CAUSE-EFFECT (earthquake, aftershocks) and
COMPONENT-WHOLE (large kitchen, apartment).
Note that unlike in SemEval-2007 Task 4, we will
not provide manually annotated WordNet senses,
thus making the task more realistic. WordNet senses
did, however, serve for disambiguation purposes in
SemEval-2007 Task 4. We will therefore have to
assess the effect of this change on inter-annotator
agreement.
6 Evaluation Methodology
The official ranking of the participating systems will
be based on their macro-averaged F-scores for the
nine proper relations. We will also compute and re-
port their accuracy over all ten relations, including
OTHER. We will further analyze the results quan-
titatively and qualitatively to gauge which relations
are most difficult to classify.
Similarly to SemEval-2007 Task 4, in order to
assess the effect of varying quantities of training
data, we will ask the teams to submit several sets of
guesses for the labels for the test data, using varying
fractions of the training data. We may, for example,
request test results when training on the first 50, 100,
200, 400 and all 700 examples from each relation.
We will provide a Perl-based automatic evalua-
tion tool that the participants can use when train-
ing/tuning/testing their systems. We will use the
same tool for the official evaluation.
7 Conclusion
We have introduced a new task, which will be part of
SemEval-2010: multi-way classification of semantic
98
relations between pairs of common nominals. The
task will compare different approaches to the prob-
lem and provide a standard testbed for future re-
search, which can benefit many NLP applications.
The description we have presented here should
be considered preliminary. We invite the in-
terested reader to visit the official task web-
site http://semeval2.fbk.eu/semeval2.
php?location=tasks\#T11, where up-to-
date information will be published; there is also a
discussion group and a mailing list.
References
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In Proc. EMNLP 2004, pages 33?40.
Dmitry Davidov and Ari Rappoport. 2008. Classifica-
tion of semantic relationships between nominals using
pattern clusters. In Proc. ACL-08: HLT, pages 227?
235.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16:235?250.
Charles J. Fillmore. 2002. FrameNet and the linking be-
tween semantic and syntactic relations. In Proc. COL-
ING 2002, pages 28?36.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Roxana Girju, Dan Moldovan, Marta Tatu, , and Dan An-
tohe. 2005. On the semantics of noun compounds.
Computer Speech and Language, 19:479?496.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 task 04: Classification of semantic re-
lations between nominals. In Proc. 4th Semantic Eval-
uation Workshop (SemEval-2007).
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2008.
Classification of semantic relations between nominals.
Language Resources and Evaluation. In print.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. COLING
92, pages 539?545.
Sophia Katrenko and Pieter Adriaans. 2008. Semantic
types of some generic relation arguments: Detection
and evaluation. In Proc. ACL-08: HLT, Short Papers,
pages 185?188.
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using WordNet sim-
ilarity. In Proc. IJCAI, pages 945?956.
George Lakoff. 1987. Women, fire, and dangerous
things. University of Chicago Press, Chicago, IL.
Maria Lapata. 2002. The disambiguation of nominalisa-
tions. Computational Linguistics, 28:357?388.
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the se-
mantic classification of noun phrases. In HLT-NAACL
2004: Workshop on Computational Lexical Semantics,
pages 60?67.
Preslav Nakov and Marti A. Hearst. 2008. Solving rela-
tional similarity problems using the web as a corpus.
In Proc. ACL-08: HLT, pages 452?460.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5), pages 285?301.
Vivi Nastase, Jelber Sayyad-Shirabad, Marina Sokolova,
and Stan Szpakowicz. 2006. Learning noun-modifier
semantic relations with corpus-based and WordNet-
based features. In Proc. AAAI, pages 781?787.
Diarmuid O? Se?aghdha and Ann Copestake. 2008. Se-
mantic classification with distributional kernels. In
Proc. COLING 2008, pages 649?656.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1):71?106.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proc. COLING/ACL, pages
113?120.
Siddharth Patwardhan and Ellen Riloff. 2007. Effective
information extraction with semantic affinity patterns
and relevant regions. In Proc. EMNLP-CoNLL), pages
717?727.
Barbara Rosario and Marti Hearst. 2001. Classifying the
semantic relations in noun compounds via a domain-
specific lexical hierarchy. In Proc. EMNLP 2001,
pages 82?90.
Barbara Rosario, Marti Hearst, and Charles Fillmore.
2002. The descent of hierarchy, and selection in re-
lational semantics. In Proc. ACL-02, pages 247?254.
Matthew Stephens, Mathew Palakal, Snehasis
Mukhopadhyay, Rajeev Raje, and Javed Mostafa.
2001. Detecting gene relations from Medline ab-
stracts. In Pacific Symposium on Biocomputing, pages
483?495.
Peter D. Turney and Michael L. Littman. 2005. Corpus-
based learning of analogies and semantic relations.
Machine Learning, 60(1-3):251?278.
Peter D. Turney. 2005. Measuring semantic similarity by
latent relational analysis. In Proc. IJCAI, pages 1136?
1141.
99
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 100?105,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 9: The Interpretation of Noun Compounds
Using Paraphrasing Verbs and Prepositions
Cristina Butnariu
University College Dublin
ioana.butnariu@ucd.ie
Su Nam Kim
University of Melbourne
nkim@csse.unimelb.edu.au
Preslav Nakov
National University of Singapore
nakov@comp.nus.edu.sg
Diarmuid O? Se?aghdha
University of Cambridge
do242@cam.ac.uk
Stan Szpakowicz
University of Ottawa
Polish Academy of Sciences
szpak@site.uottawa.ca
Tony Veale
University College Dublin
tony.veale@ucd.ie
Abstract
We present a brief overview of the main
challenges in understanding the semantics of
noun compounds and consider some known
methods. We introduce a new task to be
part of SemEval-2010: the interpretation of
noun compounds using paraphrasing verbs
and prepositions. The task is meant to provide
a standard testbed for future research on noun
compound semantics. It should also promote
paraphrase-based approaches to the problem,
which can benefit many NLP applications.
1 Introduction
Noun compounds (NCs) ? sequences of two or more
nouns acting as a single noun,1 e.g., colon cancer
tumor suppressor protein ? are abundant in English
and pose a major challenge to the automatic anal-
ysis of written text. Baldwin and Tanaka (2004)
calculated that 3.9% and 2.6% of the tokens in
the Reuters corpus and the British National Corpus
(BNC), respectively, are part of a noun compound.
Compounding is also an extremely productive pro-
cess in English. The frequency spectrum of com-
pound types follows a Zipfian or power-law distribu-
tion (O? Se?aghdha, 2008), so in practice many com-
pound tokens encountered belong to a ?long tail?
of low-frequency types. For example, over half of
the two-noun NC types in the BNC occur just once
(Lapata and Lascarides, 2003). Even for relatively
frequent NCs that occur ten or more times in the
BNC, static English dictionaries give only 27% cov-
erage (Tanaka and Baldwin, 2003). Taken together,
1We follow the definition in (Downing, 1977).
the factors of high frequency and high productiv-
ity mean that achieving robust NC interpretation is
an important goal for broad-coverage semantic pro-
cessing. NCs provide a concise means of evoking a
relationship between two or more nouns, and natu-
ral language processing (NLP) systems that do not
try to recover these implicit relations from NCs are
effectively discarding valuable semantic informa-
tion. Broad coverage should therefore be achieved
by post-hoc interpretation rather than pre-hoc enu-
meration, since it is impossible to build a lexicon of
all NCs likely to be encountered.
The challenges presented by NCs and their se-
mantics have generated significant ongoing interest
in NC interpretation in the NLP community. Repre-
sentative publications include (Butnariu and Veale,
2008; Girju, 2007; Kim and Baldwin, 2006; Nakov,
2008b; Nastase and Szpakowicz, 2003; O? Se?aghdha
and Copestake, 2007). Applications that have been
suggested include Question Answering, Machine
Translation, Information Retrieval and Information
Extraction. For example, a question-answering sys-
tem may need to determine whether headaches in-
duced by caffeine withdrawal is a good paraphrase
for caffeine headaches when answering questions
about the causes of headaches, while an information
extraction system may need to decide whether caf-
feine withdrawal headache and caffeine headache
refer to the same concept when used in the same
document. Similarly, a machine translation system
facing the unknown NC WTO Geneva headquarters
might benefit from the ability to paraphrase it as
Geneva headquarters of the WTO or as WTO head-
quarters located in Geneva. Given a query like can-
100
cer treatment, an information retrieval system could
use suitable paraphrasing verbs like relieve and pre-
vent for page ranking and query refinement.
In this paper, we introduce a new task, which will
be part of the SemEval-2010 competition: NC inter-
pretation using paraphrasing verbs and prepositions.
The task is intended to provide a standard testbed
for future research on noun compound semantics.
We also hope that it will promote paraphrase-based
approaches to the problem, which can benefit many
NLP applications.
The remainder of the paper is organized as fol-
lows: Section 2 presents a brief overview of the
existing approaches to NC semantic interpretation
and introduces the one we will adopt for SemEval-
2010 Task 9; Section 3 provides a general descrip-
tion of the task, the data collection, and the evalua-
tion methodology; Section 4 offers a conclusion.
2 Models of Relational Semantics in NCs
2.1 Inventory-Based Semantics
The prevalent view in theoretical and computational
linguistics holds that the semantic relations that im-
plicitly link the nouns of an NC can be adequately
enumerated via a small inventory of abstract re-
lational categories. In this view, mountain hut,
field mouse and village feast all express ?location
in space?, while the relation implicit in history book
and nativity play can be characterized as ?topicality?
or ?aboutness?. A sample of some of the most influ-
ential relation inventories appears in Table 1.
Levi (1978) proposes that complex nominals ?
a general concept grouping together nominal com-
pounds (e.g., peanut butter), nominalizations (e.g.,
dream analysis) and non-predicative noun phrases
(e.g., electric shock) ? are derived through the com-
plementary processes of recoverable predicate dele-
tion and nominalization; each process is associated
with its own inventory of semantic categories. Table
1 lists the categories for the former.
Warren (1978) posits a hierarchical classifica-
tion scheme derived from a large-scale corpus study
of NCs. The top-level relations in her hierar-
chy are listed in Table 1, while the next level
subdivides CONSTITUTE into SOURCE-RESULT,
RESULT-SOURCE and COPULA; COPULA is then
further subdivided at two additional levels.
In computational linguistics, popular invento-
ries of semantic relations have been proposed by
Nastase and Szpakowicz (2003) and Girju et al
(2005), among others. The former groups 30 fine-
grained relations into five coarse-grained super-
categories, while the latter is a flat list of 21 re-
lations. Both schemes are intended to be suit-
able for broad-coverage analysis of text. For spe-
cialized applications, however, it is often useful
to use domain-specific relations. For example,
Rosario and Hearst (2001) propose 18 abstract rela-
tions for interpreting NCs in biomedical text, e.g.,
DEFECT, MATERIAL, PERSON AFFILIATED,
ATTRIBUTE OF CLINICAL STUDY.
Inventory-based analyses offer significant advan-
tages. Abstract relations such as ?location? and ?pos-
session? capture valuable generalizations about NC
semantics in a parsimonious framework. Unlike
paraphrase-based analyses (Section 2.2), they are
not tied to specific lexical items, which may them-
selves be semantically ambiguous. They also lend
themselves particularly well to automatic interpreta-
tion methods based on multi-class classification.
On the other hand, relation inventories have been
criticized on a number of fronts, most influentially
by Downing (1977). She argues that the great vari-
ety of NC relations makes listing them all impos-
sible; creative NCs like plate length (?what your
hair is when it drags in your food?) are intuitively
compositional, but cannot be assigned to any stan-
dard inventory category. A second criticism is that
restricted inventories are too impoverished a repre-
sentation scheme for NC semantics, e.g., headache
pills and sleeping pills would both be analyzed as
FOR in Levi?s classification, but express very differ-
ent (indeed, contrary) relationships. Downing writes
(p. 826): ?These interpretations are at best reducible
to underlying relationships. . . , but only with the loss
of much of the semantic material considered by sub-
jects to be relevant or essential to the definitions.?
A further drawback associated with sets of abstract
relations is that it is difficult to identify the ?correct?
inventory or to decide whether one proposed classi-
fication scheme should be favored over another.
2.2 Interpretation Using Verbal Paraphrases
An alternative approach to NC interpretation asso-
ciates each compound with an explanatory para-
101
Author(s) Relation Inventory
Levi (1978) CAUSE, HAVE, MAKE, USE, BE, IN, FOR, FROM, ABOUT
Warren (1978) POSSESSION, LOCATION, PURPOSE, ACTIVITY-ACTOR, RESEMBLANCE, CONSTITUTE
Nastase and CAUSALITY (cause, effect, detraction, purpose),
Szpakowicz PARTICIPANT (agent, beneficiary, instrument, object property,
(2003) object, part, possessor, property, product, source, whole, stative),
QUALITY (container, content, equative, material, measure, topic, type),
SPATIAL (direction, location at, location from, location),
TEMPORALITY (frequency, time at, time through)
Girju et al (2005) POSSESSION, ATTRIBUTE-HOLDER, AGENT, TEMPORAL, PART-WHOLE, IS-A, CAUSE,
MAKE/PRODUCE, INSTRUMENT, LOCATION/SPACE, PURPOSE, SOURCE, TOPIC, MANNER,
MEANS, THEME, ACCOMPANIMENT, EXPERIENCER, RECIPIENT, MEASURE, RESULT
Lauer (1995) OF, FOR, IN, AT, ON, FROM, WITH, ABOUT
Table 1: Previously proposed inventories of semantic relations for noun compound interpretation. The first two come
from linguistic theories; the rest have been proposed in computational linguistics.
phrase. Thus, cheese knife and kitchen knife can be
expanded as a knife for cutting cheese and a knife
used in a kitchen, respectively. In the paraphrase-
based paradigm, semantic relations need not come
from a small set; it is possible to have many sub-
tle distinctions afforded by the vocabulary of the
paraphrasing language (in our case, English). This
paradigm avoids the problems of coverage and rep-
resentational poverty, which Downing (1977) ob-
served in inventory-based approaches. It also re-
flects cognitive-linguistic theories of NC semantics,
in which compounds are held to express underlying
event frames and whose constituents are held to de-
note event participants (Ryder, 1994).
Lauer (1995) associates NC semantics with
prepositional paraphrases. As Lauer only consid-
ers a handful of prepositions (about, at, for,
from, in, of, on, with), his model is es-
sentially inventory-based. On the other hand, noun-
preposition co-occurrences can easily be identified
in a corpus, so an automatic interpretation can be
implemented through simple unsupervised methods.
The disadvantage of this approach is the absence of a
one-to-one mapping from prepositions to meanings;
prepositions can be ambiguous (of indicates many
different relations) or synonymous (at, in and on
all express ?location?). This concern arises with all
paraphrasing models, but it is exacerbated by the re-
stricted nature of prepositions. Furthermore, many
NCs cannot be paraphrased adequately with prepo-
sitions, e.g., woman driver, honey bee.
A richer, more flexible paraphrasing model is af-
forded by the use of verbs. In such a model, a honey
bee is a bee that produces honey, a sleeping pill
is a pill that induces sleeping and a headache pill
is a pill that relieves headaches. In some previous
computational work on NC interpretation, manually
constructed dictionaries provided typical activities
or functions associated with nouns (Finin, 1980; Is-
abelle, 1984; Johnston and Busa, 1996). It is, how-
ever, impractical to build large structured lexicons
for broad-coverage systems; these methods can only
be applied to specialized domains. On the other
hand, we expect that the ready availability of large
text corpora should facilitate the automatic mining
of rich paraphrase information.
The SemEval-2010 task we present here builds on
the work of Nakov (Nakov and Hearst, 2006; Nakov,
2007; Nakov, 2008b), where NCs are paraphrased
by combinations of verbs and prepositions. Given
the problem of synonymy, we do not provide a sin-
gle correct paraphrase for a given NC but a prob-
ability distribution over a range of candidates. For
example, highly probable paraphrases for chocolate
bar are bar made of chocolate and bar that tastes
like chocolate, while bar that eats chocolate is very
unlikely. As described in Section 3.3, a set of gold-
standard paraphrase distributions can be constructed
by collating responses from a large number of hu-
man subjects.
In this framework, the task of interpretation be-
comes one of identifying the most likely paraphrases
for an NC. Nakov (2008b) and Butnariu and Veale
(2008) have demonstrated that paraphrasing infor-
mation can be collected from corpora in an un-
supervised fashion; we expect that participants in
102
SemEval-2010 Task 9 will further develop suitable
techniques for this problem. Paraphrases of this kind
have been shown to be useful in applications such as
machine translation (Nakov, 2008a) and as an inter-
mediate step in inventory-based classification of ab-
stract relations (Kim and Baldwin, 2006; Nakov and
Hearst, 2008). Progress in paraphrasing is therefore
likely to have follow-on benefits in many areas.
3 Task Description
The description of the task we present below is pre-
liminary. We invite the interested reader to visit the
official Website of SemEval-2010 Task 9, where up-
to-date information will be published; there is also a
discussion group and a mailing list.2
3.1 Preliminary Study
In a preliminary study, we asked 25-30 human sub-
jects to paraphrase 250 noun-noun compounds us-
ing suitable paraphrasing verbs. This is the Levi-
250 dataset (Levi, 1978); see (Nakov, 2008b) for de-
tails.3 The most popular paraphrases tend to be quite
apt, while some less frequent choices are question-
able. For example, for chocolate bar we obtained
the following paraphrases (the number of subjects
who proposed each one is shown in parentheses):
contain (17); be made of (16); be made
from (10); taste like (7); be composed
of (7); consist of (5); be (3); have (2);
smell of (2); be manufactured from (2);
be formed from (2); melt into (2); serve
(1); sell (1); incorporate (1); be made with
(1); be comprised of (1); be constituted
by (1); be solidified from (1); be flavored
with (1); store (1); be flavored with (1); be
created from (1); taste of (1)
3.2 Objective
We propose a task in which participating systems
must estimate the quality of paraphrases for a test
set of NCs. A list of verb/preposition paraphrases
will be provided for each NC, and for each list a
participating system will be asked to provide aptness
2Please follow the Task #9 link at the SemEval-2010 home-
page http://semeval2.fbk.eu
3This dataset is available from http://sourceforge.
net/projects/multiword/
scores that correlate well (in terms of frequency dis-
tribution) with the human judgments collated from
our test subjects.
3.3 Datasets
Trial/Development Data. As trial/development
data, we will release the previously collected para-
phrase sets for the Levi-250 dataset (after further
review and cleaning). This dataset consists of 250
noun-noun compounds, each paraphrased by 25-30
human subjects (Nakov, 2008b).
Test Data. The test data will consist of approx-
imately 300 NCs, each accompanied by a set of
paraphrasing verbs and prepositions. Following the
methodology of Nakov (2008b), we will use the
Amazon Mechanical Turk Web service4 to recruit
human subjects. This service offers an inexpensive
way to recruit subjects for tasks that require human
intelligence, and provides an API which allows a
computer program to easily run tasks and collate
the responses from human subjects. The Mechanical
Turk is becoming a popular means to elicit and col-
lect linguistic intuitions for NLP research; see Snow
et al (2008) for an overview and a discussion of is-
sues that arise.
We intend to recruit 100 annotators for each NC,
and we will require each annotator to paraphrase
at least five NCs. Annotators will be given clear
instructions and will be asked to produce one or
more paraphrases for a given NC. To help us filter
out subjects with an insufficient grasp of English or
an insufficient interest in the task, annotators will
be asked to complete a short and simple multiple-
choice pretest on NC comprehension before pro-
ceeding to the paraphrasing step.
Post-processing. We will manually check the
trial/development data and the test data. Depending
on the quality of the paraphrases, we may decide to
drop the least frequent verbs.
License. All data will be released under the Cre-
ative Commons Attribution 3.0 Unported license5.
3.4 Evaluation
Single-NC Scores. For each NC, we will compare
human scores (our gold standard) with those pro-
posed by each participating system. We have con-
4http://www.mturk.com
5http://creativecommons.org/licenses/by/3.0/
103
sidered three scores: (1) Pearson?s correlation, (2)
cosine similarity, and (3) Spearman?s rank correla-
tion.
Pearson?s correlation coefficient is a standard
measure of the correlation strength between two dis-
tributions; it can be calculated as follows:
? = E(XY ) ? E(X)E(Y )?
E(X2) ? [E(X)]2?E(Y 2) ? [E(Y )]2
(1)
where X = (x1, . . . , xn) and Y = (y1, . . . , yn) are
vectors of numerical scores for each paraphrase pro-
vided by the humans and the competing systems, re-
spectively, n is the number of paraphrases to score,
and E(X) is the expectation of X .
Cosine correlation coefficient is another popu-
lar alternative and was used by Nakov and Hearst
(2008); it can be seen as an uncentered version of
Pearson?s correlation coefficient:
? = X.Y?X??Y ? (2)
Spearman?s rank correlation coefficient is suit-
able for comparing rankings of sets of items; it is
a special case of Pearson?s correlation, derived by
considering rank indices (1,2,. . . ) as item scores . It
is defined as follows:
? = n
?xiyi ? (?xi)(? yi)?
n?x2i ? (
?xi)2
?
n? y2i ? (
? yi)2
(3)
One problem with using Spearman?s rank coef-
ficient for the current task is the assumption that
swapping any two ranks has the same effect. The
often-skewed nature of paraphrase frequency distri-
butions means that swapping some ranks is intu-
itively less ?wrong? than swapping others. Consider,
for example, the following list of human-proposed
paraphrasing verbs for child actor, which is given in
Nakov (2007):
be (22); look like (4); portray (3); start as
(1); include (1); play (1); have (1); involve
(1); act like (1); star as (1); work as (1);
mimic (1); pass as (1); resemble (1); be
classified as (1); substitute for (1); qualify
as (1); act as (1)
Clearly, a system that swaps the positions for
be (22) and look like (4) for child actor will
have made a significant error, while swapping con-
tain (17) and be made of (16) for chocolate bar (see
Section 3.1) would be less inappropriate. However,
Spearman?s coefficient treats both alterations iden-
tically since it only looks at ranks; thus, we do not
plan to use it for official evaluation, though it may
be useful for post-hoc analysis.
Final Score. A participating system?s final score
will be the average of the scores it achieves over all
test examples.
Scoring Tool. We will provide an automatic eval-
uation tool that participants can use when train-
ing/tuning/testing their systems. We will use the
same tool for the official evaluation.
4 Conclusion
We have presented a noun compound paraphrasing
task that will run as part of SemEval-2010. The goal
of the task is to promote and explore the feasibility
of paraphrase-based methods for compound inter-
pretation. We believe paraphrasing holds some key
advantages over more traditional inventory-based
approaches, such as the ability of paraphrases to rep-
resent fine-grained and overlapping meanings, and
the utility of the resulting paraphrases for other ap-
plications such as Question Answering, Information
Extraction/Retrieval and Machine Translation.
The proposed paraphrasing task is predicated on
two important assumptions: first, that paraphrasing
via a combination of verbs and prepositions pro-
vides a powerful framework for representing and in-
terpreting the meaning of compositional nonlexical-
ized noun compounds; and second, that humans can
agree amongst themselves about what constitutes a
good paraphrase for any given NC. As researchers in
this area and as proponents of this task, we believe
that both assumptions are valid, but if the analysis
of the task were to raise doubts about either assump-
tion (e.g., by showing poor agreement amongst hu-
man annotators), then this in itself would be a mean-
ingful and successful output of the task. As such,
we anticipate that the task and its associated dataset
will inspire further research, both on the theory and
development of paraphrase-based compound inter-
pretation and on its practical applications.
104
References
Timothy Baldwin and Takaaki Tanaka. 2004. Transla-
tion by machine of compound nominals: Getting it
right. In Proceedings of the ACL 2004 Workshop on
Multiword Expressions: Integrating Processing, pages
24?31.
Cristina Butnariu and Tony Veale. 2008. A concept-
centered approach to noun-compound interpretation.
In Proceedings of the 22nd International Conference
on Computational Linguistics (COLING 2008), pages
81?88.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53(4):810?842.
Timothy Finin. 1980. The Semantic Interpretation of
Compound Nominals. Ph.D. Dissertation, University
of Illinois, Urbana, Illinois.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the semantics of noun compounds.
Journal of Computer Speech and Language - Special
Issue on Multiword Expressions, 4(19):479?496.
Roxana Girju. 2007. Improving the interpretation of
noun phrases with cross-linguistic information. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL 2007), pages
568?575.
Pierre Isabelle. 1984. Another look at nominal com-
pounds. In Proceedings of the 10th International Con-
ference on Computational Linguistics, pages 509?516.
Michael Johnston and Frederica Busa. 1996. Qualia
structure and the compositional interpretation of com-
pounds. In Proceedings of the ACL 1996 Workshop on
Breadth and Depth of Semantic Lexicons, pages 77?
88.
Su Nam Kim and Timothy Baldwin. 2006. Interpret-
ing semantic relations in noun compounds via verb
semantics. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics
(COLING/ACL 2006) Main Conference Poster Ses-
sions, pages 491?498.
Mirella Lapata and Alex Lascarides. 2003. Detecting
novel compounds: the role of distributional evidence.
In Proceedings of the 10th conference of the European
chapter of the Association for Computational Linguis-
tics (EACL 2003), pages 235?242.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds. Ph.D.
thesis, Dept. of Computing, Macquarie University,
Australia.
Judith Levi. 1978. The Syntax and Semantics of Complex
Nominals. Academic Press, New York.
Preslav Nakov andMarti A. Hearst. 2006. Using verbs to
characterize noun-noun relations. In LNCS vol. 4183:
Proceedings of the 12th international conference on
Artificial Intelligence: Methodology, Systems and Ap-
plications (AIMSA 2006), pages 233?244. Springer.
Preslav Nakov and Marti A. Hearst. 2008. Solving re-
lational similarity problems using the web as a cor-
pus. In Proceedings of the 46th Annual Meeting of the
Association of Computational Linguistics (ACL 2008),
pages 452?460.
Preslav Nakov. 2007. Using the Web as an Implicit
Training Set: Application to Noun Compound Syntax
and Semantics. Ph.D. thesis, EECS Department, Uni-
versity of California, Berkeley, UCB/EECS-2007-173.
Preslav Nakov. 2008a. Improved statistical machine
translation using monolingual paraphrases. In Pro-
ceedings of the 18th European Conference on Artificial
Intelligence (ECAI?2008), pages 338?342.
Preslav Nakov. 2008b. Noun compound interpretation
using paraphrasing verbs: Feasibility study. In LNAI
vol. 5253: Proceedings of the 13th international con-
ference on Artificial Intelligence: Methodology, Sys-
tems and Applications (AIMSA 2008), pages 103?117.
Springer.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings of
the 5th International Workshop on Computational Se-
mantics, pages 285?301.
Diarmuid O? Se?aghdha and Ann Copestake. 2007. Co-
occurrence contexts for noun compound interpreta-
tion. In Proceedings of the Workshop on A Broader
Perspective on Multiword Expressions, pages 57?64.
Diarmuid O? Se?aghdha. 2008. Learning Compound Noun
Semantics. Ph.D. thesis, University of Cambridge.
Barbara Rosario and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via a
domain-specific lexical hierarchy. In Proceedings of
the 2001 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2005), pages 82?90.
Mary Ellen Ryder. 1994. Ordered Chaos: The Interpre-
tation of English Noun-Noun Compounds. University
of California Press, Berkeley, CA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2008), pages 254?263.
Takaaki Tanaka and Timothy Baldwin. 2003. Noun-
noun compound machine translation: a feasibility
study on shallow processing. In Proceedings of the
ACL 2003 workshop on Multiword expressions, pages
17?24.
Beatrice Warren. 1978. Semantic patterns of noun-noun
compounds. In Gothenburg Studies in English 41,
Goteburg, Acta Universtatis Gothoburgensis.
105
Proceedings of the Workshop on BioNLP: Shared Task, pages 95?98,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Tunable Domain-Independent Event Extraction in the MIRA Framework
Georgi Georgiev1 Kuzman Ganchev1 Vassil Momtchev1
georgi.georgiev@ontotext.com kuzman.ganchev@ontotext.com vassil.momtchev@ontotext.com
Deyan Peychev1 Preslav Nakov1 Angus Roberts2
deyan.peychev@ontotext.com preslav.nakov@ontotext.com a.roberts@dcs.shef.ac.uk
1 Ontotext AD, 135 Tsarigradsko Chaussee, Sofia 1784, Bulgaria
2 The Department of Computer Science, Regent Court 211 Portobello, Sheffield, S1 4DP. UK.
Abstract
We describe the system of the PIKB team
for BioNLP?09 Shared Task 1, which targets
tunable domain-independent event extraction.
Our approach is based on a three-stage clas-
sification: (1) trigger word tagging, (2) sim-
ple event extraction, and (3) complex event
extraction. We use the MIRA framework for
all three stages, which allows us to trade pre-
cision for increased recall by appropriately
changing the loss function during training. We
report results for three systems focusing on re-
call (R = 28.88%), precision (P = 65.58%),
and F1-measure (F1 = 33.57%), respectively.
1 Introduction
Molecular interactions have been the focus of inten-
sive research in the development of in-silico biology.
Recent developments like the Pathway and Interac-
tion Knowledge Base (PIKB) aim to make available
to the user the large semantics of the existing molec-
ular interactions data using massive knowledge syn-
dication. PIKB is part of LinkedLifeData1, a plat-
form for semantic data integration based on RDF2
syndication and lightweight reasoning.
Our system is based on the MIRA framework
where, by appropriately changing the loss function
on training, we can achieve any desirable balance
between precision and recall. For example, low pre-
cision with high recall would be appropriate in a
search that aims to identify as many potential candi-
dates as possible to be further examined by the user,
1http://www.linkedlifedata.com
2http://www.w3.org/RDF/
while high precision might be essential when adding
relations to a knowledge base. Such a tunable sys-
tem is practical for a variety of important tasks, in-
cluding but not limited to, populating extracted facts
in PIKB and reasoning on top of new and old data.
Our system is based on a three-stage classification
process: (1) trigger word tagging using a linear se-
quence model, (2) simple event extraction, and (3)
complex event extraction. In stage (2), we generate
relations between a trigger word and one or more
proteins, while in stage (3), we look for complex in-
teractions between simple events, trigger words and
proteins. We use MIRA for all three stages with a
loss function tuned for high recall.
2 One-best MIRA and Loss Functions
In what follows, xi will denote a generic input sen-
tence, and yi will be the ?gold? labeling of xi. For
each pair of a sentence xi and a labeling y, we com-
pute a vector-valued feature representation f(xi, y).
Given a weight vector w, the dot-product w ? f(x, y)
ranks the possible labelings y of x; we will denote
the top scoring labeling as yw(x). As with hidden
Markov models (Rabiner, 1989), yw(x) can be com-
puted efficiently for suitable feature functions using
dynamic programming.
The learning portion of our method requires find-
ing a weight vector w that scores the correct labeling
of the training data higher than any incorrect label-
ing. We used a one-best version of MIRA (Cram-
mer, 2004; McDonald et al, 2005) to choose w.
MIRA is an online learning algorithm that updates
the weight vector w for each training sentence xi
according to the following rule:
95
wnew = argmin
w
?w ? wold?
s.t. w ? f(xi, yi) ? w ? f(x, y?) ? L(yi, y?)
where L(yi, y) is a measure of the loss of using y in-
stead of the correct labeling yi, and y? is a shorthand
for ywold(xi). In case of a single constraint, this pro-
gram has a closed-form solution. The most straight-
forward and the most commonly used loss function
is the Hamming loss, which sets the loss of labeling
y with respect to the gold labeling yi as the number
of training examples where the two labelings dis-
agree. Since Hamming loss is not flexible enough
for targeted training towards recall or precision, we
use a number of task-specific loss functions (see
Sections 3 and 5 for details). We implemented one-
best MIRA and the corresponding loss functions in
an in-house toolkit called Edlin. Edlin provides gen-
eral machine learning architecture for linear models
and a framework with implementations of popular
learning algorithms including Naive Bayes, percep-
tron, maximum entropy, one-best MIRA, and condi-
tional random fields (CRF) among others.
3 Trigger Word Tagging
The training and the development abstracts were
first tokenized and split into sentences using maxi-
mum entropy models trained on the Genia3 corpora.
Subsequently, we trained several sequence taggers
in order to identify the trigger words in text. All
our experiments used the standard BIO encoding
(Ramshaw and Marcus, 1995) with different feature
sets and learning procedures. We focused on recall
since it determines the upper bound on the perfor-
mance of our final system. In our experiments, we
found that simultaneously identifying trigger words
and the event types they trigger yielded low recall;
thus, we settled on identifying trigger words in text
as one kind of entity, regardless of event types.
In our initial experiments, we used a CRF-
based sequence tagger (Lafferty et al, 2001), which
yielded R=43.51%. We further tried feature induc-
tion (McCallum, 2003) and second-order Markov
assumptions for the CRF, achieving 44.72% and
49.64% recall, respectively.
3http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/wiki.cgi
Feature Set R P F1
Baseline (current word) 44.82 2.86 05.38
+ POS & char 3-gram 77.41 27.96 41.09
+ previous POS tag 79.77 29.32 42.88
+ lexicon (final tagger) 80.44 29.65 43.33
Table 1: Recall (R), precision (P), and F1-measure for the
trigger words tagger (in %s) on the development dataset
for different feature sets using MIRA training with false
negatives as a loss function.
Feature Sets
entity type of e1 and e2
words in e1 and e2
word bigrams in e1 and e2
POS of e1 and e2
words between e1 and e2
word bigrams between e1 and e2
POS between e1 and e2
distance between e1 and e2
distance between e1 and e2 in the dependency graph
steps in parse tree to get e1 and e2 in the same phrase
various combinations of the above features
Table 2: Our feature set for the MIRA classifier that pre-
dicts binary relations. Here e1 and e2 can be proteins
and/or trigger words.
Subsequently, we settled on using MIRA so that
we can trade-off precision for recall. In order to
boost recall, we defined the loss function as the num-
ber of false negative trigger chunks. Thus, a larger
loss update was made whenever the model failed to
discover a trigger word, while discovering spurious
trigger words was penalized less severely. We ex-
perimented with popular feature sets previously used
for named entity (McCallum and Li, 2003) and gene
(McDonald and Pereira, 2005) recognition including
orthographic, part-of-speech (POS), shallow parsing
and gazetteers. However, we found that only a small
number of them was really helpful; a summary is
presented in Table 1. In order to boost recall even
further, we prepared a gazetteer of trigger chunks
derived from the training data, and we extended it
with the corresponding WordNet synsets; we thus
achieved 80.44% recall for our final tagger.
4 Event Extraction
The input to our event extraction algorithm is a list
of trigger words and a list of genes or gene prod-
96
ucts (e.g., proteins); the output is a set of relations
as defined for Task 1. Our algorithm works in two
stages. First, we generate events corresponding to
relations between a trigger word and one or more
proteins (simple events); then we generate events for
relations between trigger words, proteins and simple
events (complex events). The two stages differ only
in the input data; thus, below we will describe our
system for the first stage only.
For each sentence, we considered all pairs of en-
tities (trigger words and proteins), and we used an
unstructured classifier to determine the relationship
for a given pair. These relationships encoded both
the type of event (e.g., binding, regulation) and enti-
ties? roles in that event (e.g., theme, cause); there
was also a special relationship for unrelated enti-
ties. We constructed labeled examples to train a
MIRA classifier using the training data provided by
the task organizers; n-ary relations were then recon-
structed from classifier?s predictions. The features
we used are summarized in Table 2: they are over
the words separating the two entities and their part-
of-speech tags. We further used some simple fea-
tures from syntactic phrases (OpenNLP4 parser) and
dependency parse trees (McDonald et al, 2005), ex-
tracted using parsers trained on Genia corpora.
After some initial experiments, we found that our
features were not sufficiently rich to allow us to learn
the relationships between proteins that are part of the
same event: we achieved a very low recall of about
20%. Consequently, we focused on the relationships
between a trigger word and a protein. Since the com-
petition stipulated that each trigger could be associ-
ated with only one type of event, we first chose the
event type for each trigger by selecting the protein-
label pair with the highest score. We then fixed the
event type for this trigger word, and we discarded all
proteins for which our classifier assigned a different
event type to the target trigger-protein pair. Finally,
we added to our output list all binary relations where
the role of the protein was theme.
For some event classes ? binding, regulation, pos-
itive regulation and negative regulation ? the output
of the binary classifier was further transformed so
that n-ary relations can be formed. However, the
way we did this was somewhat ad-hoc. For bind-
4http://opennlp.sourceforge.net
Event Class R P F1
Localization 10.92 82.61 19.29
Binding 7.20 39.68 12.20
Gene expression 30.47 74.58 43.26
Transcription 10.95 39.47 17.14
Protein catabolism 28.57 57.14 38.10
Phosphorylation 34.07 86.79 48.94
Event Total 21.52 68.68 32.77
Regulation 1.37 26.67 2.61
Positive regulation 1.12 25.58 2.14
Negative regulation 0.26 100.00 0.53
Regulation Total 0.97 27.12 1.87
Overall 10.84 64.13 18.55
Table 3: Our official results: for an erroneous submission.
ing events, we added a 3-ary relation between the
trigger, the highest scoring protein, and the second
highest scoring protein. For regulation events, we
added a 3-ary relation between the trigger and every
pair of proteins where one was a theme and the other
one was a cause. This aggressive addition of poten-
tial matches slightly reduced the overall precision,
but helped improve the recall for the final system.
5 Results and Discussion
Unfortunately, we made an error when making our
official submission, which resulted in low scores;
Table 3 shows the results for that submission.
The rest of this section describes the results and
the implementation for the system we intended to
submit. All reported results are for exact span
matches and were obtained using the online tool pro-
vided by the task organizers.
As stated in Section 4, we used a linear model
trained using one-best MIRA with ten runs over
the data for the event extraction system. We over-
sampled the unstructured training instances that cor-
responded to a relation so that they become roughly
equal in number to those that do not correspond to a
relation. Finally, we performed parameter averaging
as described in (Freund and Schapire, 1999). These
details turned out to be very important for the system
performance.
Table 4 shows the results for three different loss
functions that gave the best results in our experi-
ments. In describing the loss functions, we define
three different types of errors: (1) if the system cor-
rectly predicted that a relation should be present,
97
0-1 Loss High Recall High Precision
Event Class R P F1 R P F1 R P F1
Localization 33.33 69.05 44.96 39.08 48.23 43.17 25.86 86.54 39.82
Binding 38.33 32.60 35.23 46.97 24.51 32.21 24.50 37.95 29.77
Gene expression 57.89 65.72 61.56 64.82 53.49 58.61 47.65 76.27 58.65
Transcription 30.66 33.87 32.18 33.58 22.12 26.67 21.17 47.54 29.29
Protein catabolism 42.86 85.71 57.14 42.86 60.00 50.00 42.86 85.71 57.14
Phosphorylation 75.56 77.86 76.69 77.78 65.22 70.95 52.59 82.56 64.25
Event total 49.64 54.60 52.00 55.98 41.55 47.70 37.93 65.83 48.13
Regulation 0.00 0.00 0.00 2.41 22.58 4.35 0.00 0.00 0.00
Positive regulation 1.73 30.91 3.28 5.29 25.24 8.75 0.20 28.57 0.40
Negative regulation 0.53 40.00 1.04 1.06 23.53 2.02 0.26 100.00 0.53
Regulation Total 1.15 30.16 2.21 3.81 24.80 6.61 0.18 37.50 0.36
Overall 24.45 53.54 33.57 28.88 39.71 33.44 18.32 65.58 28.64
Table 4: Results (in %s) for one-best MIRA with different loss functions.
but guessed the wrong type, we call this a cross-
labeling; (2) a false positive occurs when the learner
guessed some relation while there should have been
none; (3) the reverse is a false negative. All loss
functions we considered had a cross-labeling loss of
1. The 0-1 loss also has a loss of 1 for false positives
and false negatives. The high-recall loss function
penalizes false positives with 0.1 and false negatives
with 5. The high-precision loss function penalizes
false negatives with 0.1 and false positives with 5.
The values 0.1 and 5 were chosen on the develop-
ment data, but were not optimized aggressively.
In conclusion, we have built three domain-
independent event extraction systems based on the
MIRA framework, each using a different loss func-
tion. Overall, they perform quite well and would
have been ranked second on precision5, and 6th on
recall, and 7th on F1-measure.
6 Future Work
After integrating domain knowledge, which should
improve the recall for complex events and should
boost the overall precision, we intend to transform
the system output into RDF and add it to the PIKB
repository. The required efforts discouraged us from
building a middle ontology between the BioNLP and
the PIKB data models, especially given the time lim-
itations for the present task competition. However,
we believe this is a promising direction, which we
plan to pursue in future work.
5Our official submission is second on precision as well.
Acknowledgments
The work reported in this paper was partially sup-
ported by the EU FP7 - 215535 LarKC.
References
Koby Crammer. 2004. Online Learning of Complex Cat-
egorial Problems. Ph.D. thesis, Hebrew University of
Jerusalem.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. In
Machine Learning, pages 277?296.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML. Morgan Kaufmann.
Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In Proceedings of CoNLL.
Andrew McCallum. 2003. Efficiently inducing features
of conditional random fields. In Proceedings of UAI.
Ryan McDonald and Fernando Pereira. 2005. Identify-
ing gene and protein mentions in text using conditional
random fields. BMC Bioinformatics, (Suppl 1):S6(6).
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL. ACL.
Lawrence Rabiner. 1989. A tutorial on hidden Markov
models and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2).
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In David
Yarovsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora. ACL.
98
Proceedings of ACL-08: HLT, pages 452?460,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Solving Relational Similarity Problems Using the Web as a Corpus
Preslav Nakov?
EECS, CS division
University of California at Berkeley
Berkeley, CA 94720, USA
nakov@cs.berkeley.edu
Marti A. Hearst
School of Information
University of California at Berkeley
Berkeley, CA 94720, USA
hearst@ischool.berkeley.edu
Abstract
We present a simple linguistically-motivated
method for characterizing the semantic rela-
tions that hold between two nouns. The ap-
proach leverages the vast size of the Web
in order to build lexically-specific features.
The main idea is to look for verbs, preposi-
tions, and coordinating conjunctions that can
help make explicit the hidden relations be-
tween the target nouns. Using these fea-
tures in instance-based classifiers, we demon-
strate state-of-the-art results on various rela-
tional similarity problems, including mapping
noun-modifier pairs to abstract relations like
TIME, LOCATION and CONTAINER, charac-
terizing noun-noun compounds in terms of ab-
stract linguistic predicates like CAUSE, USE,
and FROM, classifying the relations between
nominals in context, and solving SAT verbal
analogy problems. In essence, the approach
puts together some existing ideas, showing
that they apply generally to various semantic
tasks, finding that verbs are especially useful
features.
1 Introduction
Despite the tremendous amount of work on word
similarity (see (Budanitsky and Hirst, 2006) for an
overview), there is surprisingly little research on the
important related problem of relational similarity ?
semantic similarity between pairs of words. Stu-
dents who took the SAT test before 2005 or who
?After January 2008 at the Linguistic Modeling Depart-
ment, Institute for Parallel Processing, Bulgarian Academy of
Sciences, nakov@lml.bas.bg
are taking the GRE test nowadays are familiar with
an instance of this problem ? verbal analogy ques-
tions, which ask whether, e.g., the relationship be-
tween ostrich and bird is more similar to that be-
tween lion and cat, or rather between primate and
monkey. These analogies are difficult, and the aver-
age test taker gives a correct answer 57% of the time
(Turney and Littman, 2005).
Many NLP applications could benefit from solv-
ing relational similarity problems, including but
not limited to question answering, information re-
trieval, machine translation, word sense disambigua-
tion, and information extraction. For example, a
relational search engine like TextRunner, which
serves queries like ?find all X such that X causes
wrinkles?, asking for all entities that are in a par-
ticular relation with a given entity (Cafarella et al,
2006), needs to recognize that laugh wrinkles is
an instance of CAUSE-EFFECT. While there are
not many success stories so far, measuring seman-
tic similarity has proven its advantages for textual
entailment (Tatu and Moldovan, 2005).
In this paper, we introduce a novel linguistically-
motivated Web-based approach to relational simi-
larity, which, despite its simplicity, achieves state-
of-the-art performance on a number of problems.
Following Turney (2006b), we test our approach
on SAT verbal analogy questions and on mapping
noun-modifier pairs to abstract relations like TIME,
LOCATION and CONTAINER. We further apply it
to (1) characterizing noun-noun compounds using
abstract linguistic predicates like CAUSE, USE, and
FROM, and (2) classifying the relation between pairs
of nominals in context.
452
2 Related Work
2.1 Characterizing Semantic Relations
Turney and Littman (2005) characterize the relation-
ship between two words as a vector with coordinates
corresponding to the Web frequencies of 128 fixed
phrases like ?X for Y ? and ?Y for X? instantiated
from a fixed set of 64 joining terms like for, such
as, not the, is *, etc. These vectors are used in a
nearest-neighbor classifier to solve SAT verbal anal-
ogy problems, yielding 47% accuracy. The same ap-
proach is applied to classifying noun-modifier pairs:
using the Diverse dataset of Nastase and Szpakow-
icz (2003), Turney&Littman achieve F-measures of
26.5% with 30 fine-grained relations, and 43.2%
with 5 course-grained relations.
Turney (2005) extends the above approach by in-
troducing the latent relational analysis (LRA), which
uses automatically generated synonyms, learns suit-
able patterns, and performs singular value decom-
position in order to smooth the frequencies. The full
algorithm consists of 12 steps described in detail in
(Turney, 2006b). When applied to SAT questions,
it achieves the state-of-the-art accuracy of 56%. On
the Diverse dataset, it yields an F-measure of 39.8%
with 30 classes, and 58% with 5 classes.
Turney (2006a) presents an unsupervised algo-
rithm for mining the Web for patterns expressing
implicit semantic relations. For example, CAUSE
(e.g., cold virus) is best characterized by ?Y * causes
X?, and ?Y in * early X? is the best pattern for
TEMPORAL (e.g., morning frost). With 5 classes,
he achieves F-measure=50.2%.
2.2 Noun-Noun Compound Semantics
Lauer (1995) reduces the problem of noun com-
pound interpretation to choosing the best paraphras-
ing preposition from the following set: of, for, in,
at, on, from, with or about. He achieved 40% accu-
racy using corpus frequencies. This result was im-
proved to 55.7% by Lapata and Keller (2005) who
used Web-derived n-gram frequencies.
Barker and Szpakowicz (1998) use syntactic clues
and the identity of the nouns in a nearest-neighbor
classifier, achieving 60-70% accuracy.
Rosario and Hearst (2001) used a discriminative
classifier to assign 18 relations for noun compounds
from biomedical text, achieving 60% accuracy.
Rosario et al (2002) reported 90% accuracy with
a ?descent of hierarchy? approach which character-
izes the relationship between the nouns in a bio-
science noun-noun compound based on the MeSH
categories the nouns belong to.
Girju et al (2005) apply both classic (SVM and
decision trees) and novel supervised models (seman-
tic scattering and iterative semantic specialization),
using WordNet, word sense disambiguation, and a
set of linguistic features. They test their system
against both Lauer?s 8 prepositional paraphrases and
another set of 21 semantic relations, achieving up to
54% accuracy on the latter.
In a previous work (Nakov and Hearst, 2006), we
have shown that the relationship between the nouns
in a noun-noun compound can be characterized us-
ing verbs extracted from the Web, but we provided
no formal evaluation.
Kim and Baldwin (2006) characterized the se-
mantic relationship in a noun-noun compound us-
ing the verbs connecting the two nouns by compar-
ing them to predefined seed verbs. Their approach
is highly resource intensive (uses WordNet, CoreLex
and Moby?s thesaurus), and is quite sensitive to the
seed set of verbs: on a collection of 453 examples
and 19 relations, they achieved 52.6% accuracy with
84 seed verbs, but only 46.7% with 57 seed verbs.
2.3 Paraphrase Acquisition
Our method of extraction of paraphrasing verbs and
prepositions is similar to previous paraphrase ac-
quisition approaches. Lin and Pantel (2001) ex-
tract paraphrases from dependency tree paths whose
ends contain semantically similar sets of words by
generalizing over these ends. For example, given
?X solves Y?, they extract paraphrases like ?X finds
a solution to Y?, ?X tries to solve Y?, ?X resolves
Y?, ?Y is resolved by X?, etc. The approach is ex-
tended by Shinyama et al (2002), who use named
entity recognizers and look for anchors belong-
ing to matching semantic classes, e.g., LOCATION,
ORGANIZATION. The idea is further extended by
Nakov et al (2004), who apply it in the biomedical
domain, imposing the additional restriction that the
sentences from which the paraphrases are extracted
cite the same target paper.
453
2.4 Word Similarity
Another important group of related work is on us-
ing syntactic dependency features in a vector-space
model for measuring word similarity, e.g., (Alshawi
and Carter, 1994), (Grishman and Sterling, 1994),
(Ruge, 1992), and (Lin, 1998). For example, given a
noun, Lin (1998) extracts verbs that have that noun
as a subject or object, and adjectives that modify it.
3 Method
Given a pair of nouns, we try to characterize the
semantic relation between them by leveraging the
vast size of the Web to build linguistically-motivated
lexically-specific features. We mine the Web for
sentences containing the target nouns, and we ex-
tract the connecting verbs, prepositions, and coordi-
nating conjunctions, which we use in a vector-space
model to measure relational similarity.
The process of extraction starts with exact phrase
queries issued against a Web search engine (Google)
using the following patterns:
?infl1 THAT * infl2?
?infl2 THAT * infl1?
?infl1 * infl2?
?infl2 * infl1?
where: infl1 and infl2 are inflected variants of
noun1 and noun2 generated using the Java Word-
Net Library1; THAT is a complementizer and can be
that, which, or who; and * stands for 0 or more (up
to 8) instances of Google?s star operator.
The first two patterns are subsumed by the last
two and are used to obtain more sentences from the
search engine since including e.g. that in the query
changes the set of returned results and their ranking.
For each query, we collect the text snippets from
the result set (up to 1,000 per query). We split them
into sentences, and we filter out all incomplete ones
and those that do not contain the target nouns. We
further make sure that the word sequence follow-
ing the second mentioned target noun is nonempty
and contains at least one nonnoun, thus ensuring
the snippet includes the entire noun phrase: snippets
representing incomplete sentences often end with a
period anyway. We then perform POS tagging us-
ing the Stanford POS tagger (Toutanova et al, 2003)
1JWNL: http://jwordnet.sourceforge.net
Freq. Feature POS Direction
2205 of P 2 ? 1
1923 be V 1 ? 2
771 include V 1 ? 2
382 serve on V 2 ? 1
189 chair V 2 ? 1
189 have V 1 ? 2
169 consist of V 1 ? 2
148 comprise V 1 ? 2
106 sit on V 2 ? 1
81 be chaired by V 1 ? 2
78 appoint V 1 ? 2
77 on P 2 ? 1
66 and C 1 ? 2
66 be elected V 1 ? 2
58 replace V 1 ? 2
48 lead V 2 ? 1
47 be intended for V 1 ? 2
45 join V 2 ? 1
. . . . . . . . . . . .
4 be signed up for V 2 ? 1
Table 1: The most frequent Web-derived features for
committee member. Here V stands for verb (possibly
+preposition and/or +particle), P for preposition and C
for coordinating conjunction; 1 ? 2 means committee
precedes the feature and member follows it; 2 ? 1means
member precedes the feature and committee follows it.
and shallow parsing with the OpenNLP tools2, and
we extract the following types of features:
Verb: We extract a verb if the subject NP of that
verb is headed by one of the target nouns (or an in-
flected form), and its direct object NP is headed by
the other target noun (or an inflected form). For ex-
ample, the verb include will be extracted from ?The
committee includes many members.? We also ex-
tract verbs from relative clauses, e.g., ?This is a com-
mittee which includes many members.? Verb parti-
cles are also recognized, e.g., ?The committee must
rotate off 1/3 of its members.? We ignore modals
and auxiliaries, but retain the passive be. Finally, we
lemmatize the main verb using WordNet?s morpho-
logical analyzer Morphy (Fellbaum, 1998).
Verb+Preposition: If the subject NP of a verb is
headed by one of the target nouns (or an inflected
form), and its indirect object is a PP containing an
NP which is headed by the other target noun (or an
inflected form), we extract the verb and the preposi-
2OpenNLP: http://opennlp.sourceforge.net
454
tion heading that PP, e.g., ?The thesis advisory com-
mittee consists of three qualified members.? As in
the verb case, we extract verb+preposition from rel-
ative clauses, we include particles, we ignore modals
and auxiliaries, and we lemmatize the verbs.
Preposition: If one of the target nouns is the head
of an NP containing a PP with an internal NP headed
by the other target noun (or an inflected form), we
extract the preposition heading that PP, e.g., ?The
members of the committee held a meeting.?
Coordinating conjunction: If the two target
nouns are the heads of coordinated NPs, we extract
the coordinating conjunction.
In addition to the lexical part, for each extracted
feature, we keep a direction. Therefore the preposi-
tion of represents two different features in the fol-
lowing examples ?member of the committee? and
?committee of members?. See Table 1 for examples.
We use the above-described features to calculate
relational similarity, i.e., similarity between pairs of
nouns. In order to downweight very common fea-
tures like of, we use TF.IDF-weighting:
w(x) = TF (x)? log
(
N
DF (x)
)
(1)
In the above formula, TF (x) is the number of
times the feature x has been extracted for the tar-
get noun pair, DF (x) is the total number of training
noun pairs that have that feature, and N is the total
number of training noun pairs.
Given two nouns and their TF.IDF-weighted fre-
quency vectors A and B, we calculate the similarity
between them using the following generalized vari-
ant of the Dice coefficient:
Dice(A,B) =
2?
?n
i=1 min(ai, bi)?n
i=1 ai +
?n
i=1 bi
(2)
Other variants are also possible, e.g., Lin (1998).
4 Relational Similarity Experiments
4.1 SAT Verbal Analogy
Following Turney (2006b), we use SAT verbal anal-
ogy as a benchmark problem for relational similar-
ity. We experiment with the 374 SAT questions
collected by Turney and Littman (2005). Table 2
shows two sample questions: the top word pairs
ostrich:bird palatable:toothsome
(a) lion:cat (a) rancid:fragrant
(b) goose:flock (b) chewy:textured
(c) ewe:sheep (c) coarse:rough
(d) cub:bear (d) solitude:company
(e) primate:monkey (e) no choice
Table 2: SAT verbal analogy: sample questions. The
stem is in bold, the correct answer is in italic, and the
distractors are in plain text.
are called stems, the ones in italic are the solu-
tions, and the remaining ones are distractors. Tur-
ney (2006b) achieves 56% accuracy on this dataset,
which matches the average human performance of
57%, and represents a significant improvement over
the 20% random-guessing baseline.
Note that the righthand side example in Table
2 is missing one distractor; so do 21 questions.
The dataset alo mixes different parts of speech:
while solitude and company are nouns, all remaining
words are adjectives. Other examples contain verbs
and adverbs, and even relate pairs of different POS.
This is problematic for our approach, which requires
that both words be nouns3. After having filtered all
examples containing nonnouns, we ended up with
184 questions, which we used in the evaluation.
Given a verbal analogy example, we build six fea-
ture vectors ? one for each of the six word pairs. We
then calculate the relational similarity between the
stem of the analogy and each of the five candidates,
and we choose the pair with the highest score; we
make no prediction in case of a tie.
The evaluation results for a leave-one-out cross-
validation are shown in Table 3. We also show 95%-
confidence intervals for the accuracy. The last line
in the table shows the performance of Turney?s LRA
when limited to the 184 noun-only examples. Our
best model v + p + c performs a bit better, 71.3%
vs. 67.4%, but the difference is not statistically sig-
nificant. However, this ?inferred? accuracy could be
misleading, and the LRA could have performed bet-
ter if it was restricted to solve noun-only analogies,
which seem easier than the general ones, as demon-
strated by the significant increase in accuracy for
LRA when limited to nouns: 67.4% vs. 56%.
3It can be extended to handle adjective-noun pairs as well,
as demonstrated in section 4.2 below.
455
Model X ? ? Accuracy Cover.
v + p + c 129 52 3 71.3?7.0 98.4
v 122 56 6 68.5?7.2 96.7
v + p 119 61 4 66.1?7.2 97.8
v + c 117 62 5 65.4?7.2 97.3
p + c 90 90 4 50.0?7.2 97.8
p 84 94 6 47.2?7.2 96.7
baseline 37 147 0 20.0?5.2 100.0
LRA 122 59 3 67.4?7.1 98.4
Table 3: SAT verbal analogy: 184 noun-only examples.
v stands for verb, p for preposition, and c for coordinating
conjunction. For each model, the number of correct (X),
wrong (?), and nonclassified examples (?) is shown, fol-
lowed by accuracy and coverage (in %s).
Model X ? ? Accuracy Cover.
v + p 240 352 8 40.5?3.9 98.7
v + p + c 238 354 8 40.2?3.9 98.7
v 234 350 16 40.1?3.9 97.3
v + c 230 362 8 38.9?3.8 98.7
p + c 114 471 15 19.5?3.0 97.5
p 110 475 15 19.1?3.0 97.5
baseline 49 551 0 8.2?1.9 100.0
LRA 239 361 0 39.8?3.8 100.0
Table 4: Head-modifier relations, 30 classes: evaluation
on the Diverse dataset, micro-averaged (in %s).
4.2 Head-Modifier Relations
Next, we experiment with the Diverse dataset of
Barker and Szpakowicz (1998), which consists of
600 head-modifier pairs: noun-noun, adjective-noun
and adverb-noun. Each example is annotated with
one of 30 fine-grained relations, which are fur-
ther grouped into the following 5 coarse-grained
classes (the fine-grained relations are shown in
parentheses): CAUSALITY (cause, effect, purpose,
detraction), TEMPORALITY (frequency, time at,
time through), SPATIAL (direction, location, lo-
cation at, location from), PARTICIPANT (agent,
beneficiary, instrument, object, object property,
part, possessor, property, product, source, stative,
whole) and QUALITY (container, content, equa-
tive, material, measure, topic, type). For example,
exam anxiety is classified as effect and therefore as
CAUSALITY, and blue book is property and there-
fore also PARTICIPANT.
Some examples in the dataset are problematic for
our method. First, in three cases, there are two mod-
ifiers, e.g., infectious disease agent, and we had to
ignore the first one. Second, seven examples have
an adverb modifier, e.g., daily exercise, and 262 ex-
amples have an adjective modifier, e.g., tiny cloud.
We treat them as if the modifier was a noun, which
works in many cases, since many adjectives and ad-
verbs can be used predicatively, e.g., ?This exercise
is performed daily.? or ?This cloud looks very tiny.?
For the evaluation, we created a feature vector for
each head-modifier pair, and we performed a leave-
one-out cross-validation: we left one example for
testing and we trained on the remaining 599 ones,
repeating this procedure 600 times so that each ex-
ample be used for testing. Following Turney and
Littman (2005) we used a 1-nearest-neighbor classi-
fier. We calculated the similarity between the feature
vector of the testing example and each of the train-
ing examples? vectors. If there was a unique most
similar training example, we predicted its class, and
if there were ties, we chose the class predicted by the
majority of tied examples, if there was a majority.
The results for the 30-class Diverse dataset are
shown in Table 4. Our best model achieves 40.5%
accuracy, which is slightly better than LRA?s 39.8%,
but the difference is not statistically significant.
Table 4 shows that the verbs are the most impor-
tant features, yielding about 40% accuracy regard-
less of whether used alone or in combination with
prepositions and/or coordinating conjunctions; not
using them results in 50% drop in accuracy.
The reason coordinating conjunctions do not help
is that head-modifier relations are typically ex-
pressed with verbal or prepositional paraphrases.
Therefore, coordinating conjunctions only help with
some infrequent relations like equative, e.g., finding
player and coach on the Web suggests an equative
relation for player coach (and for coach player).
As Table 3 shows, this is different for SAT ver-
bal analogy, where verbs are still the most important
feature type and the only whose presence/absence
makes a statistical difference. However, this time
coordinating conjunctions (with prepositions) do
help a bit (the difference is not statistically signifi-
cant) since SAT verbal analogy questions ask for a
broader range of relations, e.g., antonymy, for which
coordinating conjunctions like but are helpful.
456
Model Accuracy
v + p + c + sent + query (type C) 68.1?4.0
v 67.9?4.0
v + p + c 67.8?4.0
v + p + c + sent (type A) 67.3?4.0
v + p 66.9?4.0
sent (sentence words only) 59.3?4.2
p 58.4?4.2
Baseline (majority class) 57.0?4.2
v + p + c + sent + query (C), 8 stars 67.0?4.0
v + p + c + sent (A), 8 stars 65.4?4.1
Best type C on SemEval 67.0?4.0
Best type A on SemEval 66.0?4.1
Table 5: Relations between nominals: evaluation on the
SemEval dataset. Accuracy is macro-averaged (in %s),
up to 10 Google stars are used unless otherwise stated.
4.3 Relations Between Nominals
We further experimented with the SemEval?07 task
4 dataset (Girju et al, 2007), where each example
consists of a sentence, a target semantic relation, two
nominals to be judged on whether they are in that re-
lation, manually annotated WordNet senses, and the
Web query used to obtain the sentence:
"Among the contents of the
<e1>vessel</e1> were a set of
carpenter?s <e2>tools</e2>, several
large storage jars, ceramic utensils,
ropes and remnants of food, as well
as a heavy load of ballast stones."
WordNet(e1) = "vessel%1:06:00::",
WordNet(e2) = "tool%1:06:00::",
Content-Container(e2, e1) = "true",
Query = "contents of the * were a"
The following nonexhaustive and possibly over-
lapping relations are possible: Cause-Effect
(e.g., hormone-growth), Instrument-Agency
(e.g., laser-printer), Theme-Tool (e.g., work-
force), Origin-Entity (e.g., grain-alcohol),
Content-Container (e.g., bananas-basket),
Product-Producer (e.g., honey-bee), and
Part-Whole (e.g., leg-table). Each relation is
considered in isolation; there are 140 training and at
least 70 test examples per relation.
Given an example, we reduced the target entities
e1 and e2 to single nouns by retaining their heads
only. We then mined the Web for sentences con-
taining these nouns, and we extracted the above-
described feature types: verbs, prepositions and co-
ordinating conjunctions. We further used the follow-
ing problem-specific contextual feature types:
Sentence words: after stop words removal and
stemming with the Porter (1980) stemmer;
Entity words: lemmata of the words in e1 and e2;
Query words: words part of the query string.
Each feature type has a specific prefix which pre-
vents it from mixing with other feature types; the
last feature type is used for type C only (see below).
The SemEval competition defines four types of
systems, depending on whether the manually anno-
tatedWordNet senses and theGoogle query are used:
A (WordNet=no, Query=no), B (WordNet=yes,
Query=no), C (WordNet=no, Query=yes), and D
(WordNet=yes, Query=yes). We experimented with
types A and C only since we believe that having the
manually annotated WordNet sense keys is an unre-
alistic assumption for a real-world application.
As before, we used a 1-nearest-neighbor classifier
with TF.IDF-weighting, breaking ties by predicting
the majority class on the training data. The evalu-
ation results are shown in Table 5. We studied the
effect of different subsets of features and of more
Google star operators. As the table shows, using
up to ten Google stars instead of up to eight (see
section 3) yields a slight improvement in accuracy
for systems of both type A (65.4% vs. 67.3%) and
type C (67.0% vs. 68.1%). Both results represent
a statistically significant improvement over the ma-
jority class baseline and over using sentence words
only, and a slight improvement over the best type A
and type C systems on SemEval?07, which achieved
66% and 67% accuracy, respectively.4
4.4 Noun-Noun Compound Relations
The last dataset we experimented with is a subset
of the 387 examples listed in the appendix of (Levi,
1978). Levi?s theory is one of the most impor-
tant linguistic theories of the syntax and semantics
of complex nominals ? a general concept grouping
4The best type B system on SemEval achieved 76.3% ac-
curacy using the manually-annotated WordNet senses in context
for each example, which constitutes an additional data source,
as opposed to an additional resource. The systems that used
WordNet as a resource only, i.e., ignoring the manually anno-
tated senses, were classified as type A or C. (Girju et al, 2007)
457
USING THAT NOT USING THAT
Model Accuracy Cover. ANF ASF Accuracy Cover. ANF ASF
Human: all v 78.4?6.0 99.5 34.3 70.9 ? ? ?
Human: first v from each worker 72.3?6.4 99.5 11.6 25.5 ? ? ? ?
v + p + c 50.0?6.7 99.1 216.6 1716.0 49.1?6.7 99.1 206.6 1647.6
v + p 50.0?6.7 99.1 208.9 1427.9 47.6?6.6 99.1 198.9 1359.5
v + c 46.7?6.6 99.1 187.8 1107.2 43.9?6.5 99.1 177.8 1038.8
v 45.8?6.6 99.1 180.0 819.1 42.9?6.5 99.1 170.0 750.7
p 33.0?6.0 99.1 28.9 608.8 33.0?6.0 99.1 28.9 608.8
p + c 32.1?5.9 99.1 36.6 896.9 32.1?5.9 99.1 36.6 896.9
Baseline 19.6?4.8 100.0 ? ? ? ? ? ?
Table 6: Noun-noun compound relations, 12 classes: evaluation on Levi-214 dataset. Shown are micro-averaged
accuracy and coverage in %s, followed by average number of features (ANF) and average sum of feature frequencies
(ASF) per example. The righthand side reports the results when the query patterns involving THAT were not used. For
comparison purposes, the top rows show the performance with the human-proposed verbs used as features.
together the partially overlapping classes of nom-
inal compounds (e.g., peanut butter), nominaliza-
tions (e.g., dream analysis), and nonpredicate noun
phrases (e.g., electric shock).
In Levi?s theory, complex nominals can be derived
from relative clauses by removing one of the fol-
lowing 12 abstract predicates: CAUSE1 (e.g., tear
gas), CAUSE2 (e.g., drug deaths), HAVE1 (e.g., ap-
ple cake), HAVE2 (e.g., lemon peel), MAKE1 (e.g.,
silkworm), MAKE2 (e.g., snowball), USE (e.g., steam
iron), BE (e.g., soldier ant), IN (e.g., field mouse),
FOR (e.g., horse doctor), FROM (e.g., olive oil), and
ABOUT (e.g., price war). In the resulting nominals,
the modifier is typically the object of the predicate;
when it is the subject, the predicate is marked with
the index 2. The second derivational mechanism in
the theory is nominalization; it produces nominals
whose head is a nominalized verb.
Since we are interested in noun compounds only,
we manually cleansed the set of 387 examples. We
first excluded all concatenations (e.g., silkworm) and
examples with adjectival modifiers (e.g., electric
shock), thus obtaining 250 noun-noun compounds
(Levi-250 dataset). We further filtered out all nom-
inalizations for which the dataset provides no ab-
stract predicate (e.g., city planner), thus ending up
with 214 examples (Levi-214 dataset).
As in the previous experiments, for each of the
214 noun-noun compounds, we mined the Web
for sentences containing both target nouns, from
which we extracted paraphrasing verbs, prepositions
and coordinating conjunctions. We then performed
leave-one-out cross-validation experiments with a
1-nearest-neighbor classifier, trying to predict the
correct predicate for the testing example. The re-
sults are shown in Table 6. As we can see, us-
ing prepositions alone yields about 33% accuracy,
which is a statistically significant improvement over
the majority-class baseline. Overall, the most impor-
tant features are the verbs: they yield 45.8% accu-
racy when used alone, and 50% together with prepo-
sitions. Adding coordinating conjunctions helps a
bit with verbs, but not with prepositions. Note how-
ever that none of the differences between the differ-
ent feature combinations involving verbs are statis-
tically significant.
The righthand side of the table reports the results
when the query patterns involving THAT (see section
3) were not used. We can observe a small 1-3% drop
in accuracy for all models involving verbs, but it is
not statistically significant.
We also show the average number of distinct fea-
tures and sum of feature counts per example: as we
can see, there is a strong positive correlation be-
tween number of features and accuracy.
5 Comparison to Human Judgments
Since in all above tasks the most important fea-
tures were the verbs, we decided to compare our
Web-derived verbs to human-proposed ones for all
noun-noun compounds in the Levi-250 dataset. We
asked human subjects to produce verbs, possibly
458
followed by prepositions, that could be used in a
paraphrase involving that. For example, olive oil
can be paraphrased as ?oil that comes from olives?,
?oil that is obtained from olives? or ?oil that is from
olives?. Note that this implicitly allows for prepo-
sitional paraphrases ? when the verb is to be and is
followed by a preposition, as in the last paraphrase.
We used the Amazon Mechanical Turk Web ser-
vice5 to recruit human subjects, and we instructed
them to propose at least three paraphrasing verbs
per noun-noun compound, if possible. We randomly
distributed the noun-noun compounds into groups of
5 and we requested 25 different human subjects per
group. Each human subject was allowed to work
on any number of groups, but not on the same one
twice. A total of 174 different human subjects pro-
duced 19,018 verbs. After filtering the bad submis-
sions and normalizing the verbs, we ended up with
17,821 verbs. See (Nakov, 2007) for further de-
tails on the process of extraction and cleansing. The
dataset itself is freely available (Nakov, 2008).
We compared the human-proposed and the Web-
derived verbs for Levi-214, aggregated by relation.
Given a relation, we collected all verbs belong-
ing to noun-noun compounds from that relation to-
gether with their frequencies. From a vector-space
model point of view, we summed their correspond-
ing frequency vectors. We did this separately for
the human- and the program-generated verbs, and
we compared the resulting vectors using Dice co-
efficient with TF.IDF, calculated as before. Figure
1 shows the cosine correlations using all human-
proposed verbs and the first verb from each judge.
We can see a very-high correlation (mid-70% to
mid-90%) for relations like CAUSE1, MAKE1, BE,
but low correlations of 11-30% for reverse relations
like HAVE2 and MAKE2. Interestingly, using the first
verb only improves the results for highly-correlated
relations, but negatively affects low-correlated ones.
Finally, we repeated the cross-validation exper-
iment with the Levi-214 dataset, this time using
the human-proposed verbs6 as features. As Table
6 shows, we achieved 78.4% accuracy using all
verbs (and and 72.3% with the first verb from each
worker), which is a statistically significant improve-
5http://www.mturk.com
6Note that the human subjects proposed their verbs without
any context and independently of our Web-derived sentences.
Figure 1: Cosine correlation (in %s) between the
human- and the program- generated verbs by rela-
tion: using all human-proposed verbs vs. the first verb.
ment over the 50% of our best Web-based model.
This result is strong for a 12-way classification prob-
lem, and confirms our observation that verbs and
prepositions are among the most important features
for relational similarity problems. It further suggests
that the human-proposed verbs might be an upper
bound on the accuracy that could be achieved with
automatically extracted features.
6 Conclusions and Future Work
We have presented a simple approach for character-
izing the relation between a pair of nouns in terms
of linguistically-motivated features which could be
useful for many NLP tasks. We found that verbs
were especially useful features for this task. An im-
portant advantage of the approach is that it does not
require knowledge about the semantics of the indi-
vidual nouns. A potential drawback is that it might
not work well for low-frequency words.
The evaluation on several relational similarity
problems, including SAT verbal analogy, head-
modifier relations, and relations between complex
nominals has shown state-of-the-art performance.
The presented approach can be further extended to
other combinations of parts of speech: not just noun-
noun and adjective-noun. Using a parser with a
richer set of syntactic dependency features, e.g., as
proposed by Pado? and Lapata (2007), is another
promising direction for future work.
Acknowledgments
This research was supported in part by NSF DBI-
0317510.
459
References
Hiyan Alshawi and David Carter. 1994. Training
and scaling preference functions for disambiguation.
Computational Linguistics, 20(4):635?648.
Ken Barker and Stan Szpakowicz. 1998. Semi-automatic
recognition of noun modifier relationships. In Proc. of
Computational linguistics, pages 96?102.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating wordnet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32(1):13?47.
Michael Cafarella, Michele Banko, and Oren Etzioni.
2006. Relational Web search. Technical Report 2006-
04-02, University of Washington, Department of Com-
puter Science and Engineering.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the semantics of noun compounds.
Journal of Computer Speech and Language - Special
Issue on Multiword Expressions, 4(19):479?496.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic rela-
tions between nominals. In Proceedings of SemEval,
pages 13?18, Prague, Czech Republic.
Ralph Grishman and John Sterling. 1994. Generalizing
automatically generated selectional patterns. In Pro-
ceedings of the 15th conference on Computational lin-
guistics, pages 742?747.
Su Nam Kim and Timothy Baldwin. 2006. Interpret-
ing semantic relations in noun compounds via verb se-
mantics. In Proceedings of the COLING/ACL on Main
conference poster sessions, pages 491?498.
Mirella Lapata and Frank Keller. 2005. Web-based
models for natural language processing. ACM Trans.
Speech Lang. Process., 2(1):3.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds. Ph.D.
thesis, Dept. of Computing, Macquarie University,
Australia.
Judith Levi. 1978. The Syntax and Semantics of Complex
Nominals. Academic Press, New York.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question-answering. Natural Language
Engineering, 7(4):343?360.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of ICML, pages 296?304.
Preslav Nakov and Marti Hearst. 2006. Using verbs to
characterize noun-noun relations. In AIMSA, volume
4183 of LNCS, pages 233?244. Springer.
Preslav Nakov, Ariel Schwartz, and Marti Hearst. 2004.
Citances: Citation sentences for semantic analysis of
bioscience text. In Proceedings of SIGIR?04Workshop
on Search and Discovery in Bioinformatics, pages 81?
88, Sheffield, UK.
Preslav Nakov. 2007. Using the Web as an Implicit
Training Set: Application to Noun Compound Syntax
and Semantics. Ph.D. thesis, EECS Department, Uni-
versity of California, Berkeley, UCB/EECS-2007-173.
Preslav Nakov. 2008. Paraphrasing verbs for noun com-
pound interpretation. In Proceedings of the LREC?08
Workshop: Towards a Shared Task for Multiword Ex-
pressions (MWE?08), Marrakech, Morocco.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5), pages 285?301, Tilburg, The Netherlands.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161?199.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Barbara Rosario and Marti Hearst. 2001. Classifying the
semantic relations in noun compounds via a domain-
specific lexical hierarchy. In Proceedings of EMNLP,
pages 82?90.
Barbara Rosario, Marti Hearst, and Charles Fillmore.
2002. The descent of hierarchy, and selection in rela-
tional semantics. In Proceedings of ACL, pages 247?
254.
Gerda Ruge. 1992. Experiment on linguistically-based
term associations. Inf. Process. Manage., 28(3):317?
332.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proceedings of HLT, pages 313?318.
Marta Tatu and Dan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. In Proceed-
ings of HLT, pages 371?378.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL, pages 252?259.
Peter Turney and Michael Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning Journal, 60(1-3):251?278.
Peter Turney. 2005. Measuring semantic similarity by
latent relational analysis. In Proceedings of IJCAI,
pages 1136?1141.
Peter Turney. 2006a. Expressing implicit semantic re-
lations without supervision. In Proceedings of ACL,
pages 313?320.
Peter Turney. 2006b. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
460
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 193?202, Dublin, Ireland, August 23-29 2014.
A Study of using Syntactic and Semantic Structures
for Concept Segmentation and Labeling
Iman Saleh
?
, Shafiq Joty, Llu??s M
`
arquez,
Alessandro Moschitti, Preslav Nakov
ALT Research Group
Qatar Computing Research Institute
{sjoty,lmarquez,amoschitti,pnakov}
@qf.org.qa
Scott Cyphers, Jim Glass
MIT CSAIL
Cambridge, Massachusetts 02139
USA
{cyphers,glass}@mit.edu
Abstract
This paper presents an empirical study on using syntactic and semantic information for Concept
Segmentation and Labeling (CSL), a well-known component in spoken language understand-
ing. Our approach is based on reranking N -best outputs from a state-of-the-art CSL parser. We
perform extensive experimentation by comparing different tree-based kernels with a variety of
representations of the available linguistic information, including semantic concepts, words, POS
tags, shallow and full syntax, and discourse trees. The results show that the structured representa-
tion with the semantic concepts yields significant improvement over the base CSL parser, much
larger compared to learning with an explicit feature vector representation. We also show that
shallow syntax helps improve the results and that discourse relations can be partially beneficial.
1 Introduction
Spoken Language Understanding aims to interpret user utterances and to convert them to logical forms,
or, equivalently, database queries, which can then be used to satisfy the user?s information needs. This
process is known as Concept Segmentation and Labeling (CSL): it maps utterances into meaning repre-
sentations based on semantic constituents. The latter are basically sequences of semantic entities, often
referred to as concepts, attributes or semantic tags. Traditionally, grammar-based methods have been
used for CSL, but more recently machine learning approaches to semantic structure computation have
been shown to yield higher accuracy. However, most previous work did not exploit syntactic/semantic
structures of the utterances, and the state-of-the-art is represented by conditional models for sequence la-
beling, such as Conditional Random Fields (Lafferty et al., 2001) trained with simple morphological and
lexical features. In our study, we measure the impact of syntactic and discourse structures by also com-
bining them with innovative features. In the following subsections, we present the application context
for our CSL task and then we outline the challenges and the findings of our research.
1.1 Semantic parsing for the ?restaurant? domain
We experiment with the dataset of McGraw et al. (2012), containing spoken and typed questions about
restaurants, which are to be answered using a database of free text such as reviews, categorical data such
as names and locations, and semi-categorical data such as user-reported cuisines and amenities.
Semantic parsing, in the form of sequential segmentation and labeling, makes it easy to convert spoken
and typed questions such as ?cheap lebanese restaurants in doha with take out? into database queries.
First, a language-specific semantic parser tokenizes, segments and labels the question:
[
Price
cheap] [
Cuisine
lebanese] [
Other
restaurants in] [
City
doha] [
Other
with] [
Amenity
take out]
Then, label-specific normalizers are applied to the segments, with the option to possibly relabel mis-
labeled segments; at this point, discourse history may be incorporated as well.
[
Price
low] [
Cuisine
lebanese] [
City
doha] [
Amenity
carry out]
?
Iman Saleh (iman.saleh@fci-cu.edu.eg) is affiliated to Faculty of Computers and Information, Cairo University.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
193
Finally, a database query is formed from the list of labels and values, and is then executed against the
database, e.g., MongoDB; a backoff mechanism may be used if the query does not succeed.
{$and [{cuisine:"lebanese"}, {city:"doha"}, {price:"low"}, {amenity:"carry out"}]}
1.2 Related work on CSL
Pieraccini et al. (1991) used Hidden Markov Models (HMMs) for CSL, where the observations were
word sequences and the hidden states were meaning units, i.e, concepts. In subsequent work (Rubinstein
and Hastie, 1997; Santaf?e et al., 2007; Raymond and Riccardi, 2007; De Mori et al., 2008), other genera-
tive models were applied, which model the joint probability of a word sequence and a concept sequence,
as well as discriminative models, which directly model a conditional probability over the concepts in the
input text.
Seneff (1989) and Miller et al. (1994) used stochastic grammars for CSL. In particular, they applied
stochastic Finite State Transducers (FST) for recognizing constituent annotations. FSTs describe local
syntactic structures with a sequence of words, e.g., noun phrases or even constituents. Papineni et al.
(1998) proposed and evaluated exponential models, but, nowadays, Conditional Random Fields (Lafferty
et al., 2001) are considered to be the state-of-the-art. More recently, Wang et al. (2009) illustrated an
approach for CSL that is specific to query understanding for web applications. A general survey of CSL
approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on
shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview.
Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels
for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used
explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins,
2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al.,
2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL
hypotheses using structures built on top of concepts, words and features that are simpler than those
studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar
to ours, as it models the extraction of semantics as a reranking task using string kernels.
1.3 Syntactic and semantic structures for CSL
The related work has highlighted that automatic CSL is mostly based on powerful machine learning al-
gorithms and simple feature representations based on word and tag n-grams. In this paper, we study the
impact of more advanced linguistic processing on CSL, such as shallow and full syntactic parsing and
discourse structure. We use a reranking approach to select the best hypothesis annotated with concepts
derived by a local model, where the hypotheses are represented as trees enriched with semantic con-
cepts similarly to (Dinarelli et al., 2011). These tree-based structures can capture dependencies between
sentence constituents and concepts. However, extracting features from them is rather difficult as their
number is exponentially large. Thus, we rely on structural kernels (e.g., see (Moschitti, 2006)) for au-
tomatically encoding tree fragments, which represent syntactic and semantic dependencies from words
and concepts, and we train the reranking functions with Support Vector Machines (e.g., see (Joachims,
1999)). Additionally, we experiment with several types of kernels and newly designed feature vectors.
We test our models on the above-mentioned Restaurant domain. The results show that (i) the basic
CRF model, in fact semi-CRF (see below), is very accurate, achieving more than 83% in F
1
-score, which
indicates that improving over the semi-CRF approach is very hard; (ii) the upper-bound performance
of the reranking approach is very high as well, i.e., the correct annotation is generated in the first 100
hypotheses in 98.72% of the cases; (iii) our feature vectors show improvement only when all feature
groups are used together; otherwise, we only observe marginal improvement; (iv) structural kernels yield
a 10% relative error reduction from the semi-CRF baseline, which is more than double the feature vector
result; (v) syntactic information significantly improves on the best model, but only when using shallow
syntax; and finally, (vi) although, discourse structures provide good improvement over the semi-CRF
model, they perform lower than shallow syntax (thus, a valuable use of discourse features is still an open
problem that we plan to pursue in future work).
194
2 CSL reranking
Reranking is based on a list of N annotation hypotheses, which are generated and sorted by probability
using local classifiers. Then a reranker, typically a meta-classifier, tries to select the best hypothesis from
the list. The reranker can exploit global information, and, specifically, the dependencies between the
different concepts that are made available by the local model. We use semi-CRF as our local model since
it yields the highest accuracy in CSL (when using a single model), and preference reranking with kernel
machines to rerank the N hypotheses generated by the semi-CRF.
2.1 Basic parser using semi-CRF
We use a semi-Markov CRF (Sarawagi and Cohen, 2004), or semi-CRF, a variation of a linear-chain
CRF (Lafferty et al., 2001), to produce the N -best list of labeled segment hypotheses that serve as the
input to reranking. In a linear-chain CRF, with a sequence of tokens x and labels y, we approximate
p(y|x) as a product of factors of the form p(y
i
|y
i?1
, x), which corresponds to features of the form
f
j
(y
i?1
, y
i
, i, x), where i iterates over the token/label positions. This supports a Viterbi search for the
approximateN best values of y. WithM label values, if for each label y
m
we know the bestN sequences
of labels y
1
, y
2
, . . . , y
i?1
= y
m
, then we can use p(y
i
|y
i?1
, x) to get the probability for extending each
path by each possible label y
i
= y
?
m
. Then for each label y
?
m
, we will have MN paths and scores, one
from each of the paths of length i? 1 ending with y
m
. For each y
?
m
, we pick the N best extended paths.
With semi-CRF, we want a labeled segmentation s rather than a sequence of labels. Each segment
s
i
= (y
i
, t
i
, u
i
) has a label y
i
as well as a starting and ending token position for the segment, t
i
and
u
i
respectively, where u
i
+ 1 = t
i+1
. We approximate p(s|x), with factors of the form p(s
i
|s
i?1
, x),
which we simplify to p(y
i
, u
i
|y
i?1
, t
i
), so features take the form f
j
(y
i?1
, y
i
, t
i
, u
i
), i.e., they can use the
previous segment?s label and the current segment?s label and endpoints. The Viterbi search is extended
to search for a pair of label and segment end. Whereas for M labels we kept track of MN paths, we
must keep track of MLN paths, where L is the maximum segment length.
We use token n-gram features relative to the segment boundaries, n-grams within the segment, token
regular expression and lexicon features within a segment. Each of these features also includes the labels
of the previous and current segment, and the segment length.
2.2 Preference reranking with kernel machines
Preference reranking (PR) uses a classifier C of pairs of hypotheses ?H
i
, H
j
?, which decides if H
i
is
better thanH
j
. Given each training question Q, positive and negative examples are generated for training
the classifier. We adopt the following approach for example generation: the pairs ?H
1
, H
i
? constitute
positive examples, where H
1
has the lowest error rate with respect to the gold standard among the
hypotheses for Q, and vice versa, ?H
i
, H
1
? are considered as negative examples. At testing time, given
a new question Q
?
, C classifies all pairs ?H
i
, H
j
? generated from the annotation hypotheses of Q
?
: a
positive classification is a vote for H
i
, otherwise the vote is for H
j
. Also, the classifier score can be used
as a weighted vote. H
k
are then ranked according to the number (sum) of the (weighted) votes they get.
We build our reranker with kernel machines. The latter, e.g., SVMs, classify an input object o using
the following function: C(o) =
?
i
?
i
y
i
K(o, o
i
), where ?
i
are model parameters estimated from the
training data, o
i
are support objects and y
i
are the labels of the support objects. K(?, ?) is a kernel
function, which computes the scalar product between the two objects in an implicit vector space. In the
case of the reranker, the objects o are ?H
i
, H
j
?, and the kernel is defined as follow:
K(?H
1
, H
2
?, ?H
?
1
, H
?
2
?) = S(H
1
, H
?
1
) + S(H
2
, H
?
2
)? S(H
1
, H
?
2
)? S(H
2
, H
?
1
).
Our reranker also includes traditional feature vectors in addition to the trees. Therefore, we define each
hypothesis H as a tuple ?T,~v? composed of a tree T and a feature vector ~v. We then define a structural
kernel (similarity) between two hypotheses H and H
?
as follows: S(H,H
?
) = S
TK
(T, T
?
) + S
v
(~v,~v
?
),
where S
TK
is one of the tree kernel functions defined in Section 3.1, and S
v
is a kernel over feature
vectors (see Section 3.3), e.g., linear, polynomial, gaussian, etc.
195
(a) Basic Tree (BT). (b) Discourse Tree (DT).
(c) Shallow Syntactic Tree (ShT).
(d) Syntactic Tree (ST).
(e) BT with POS (BTP).
Figure 1: Syntactic/semantic trees. The numeric semantic tagset is defined in Table 7.
3 Structural kernels for semantic parsing
In this section, we briefly describe the kernels we use in S(H,H
?
) for preference reranking. We engineer
them by combining three aspects: (i) different types of existing tree kernels, (ii) new syntactic/semantic
structures for representing CSL, and (iii) new feature vectors.
3.1 Tree kernels
Structural kernels, e.g., tree and sequence kernels, measure the similarity between two structures in terms
of their shared substructures. One interesting aspect is that these kernels correspond to a scalar product
in the fragment space, where each substructure is a feature. Therefore, they can be used in the training
and testing algorithms of kernel machines (see Section 2.2). Below, we briefly describe different types of
kernels we tested in our study, which are made available in the SVM-Light-TK toolkit (Moschitti, 2006).
Subtree Kernel (K0) is one of the simplest tree kernels, as it only generates complete subtrees, i.e., tree
fragments that, given any arbitrary starting node, necessarily include all its descendants.
Syntactic Tree Kernel (K1), also known as a subset tree kernel (Collins and Duffy, 2002), maps ob-
jects in the space of all possible tree fragments constrained by the rule that the sibling nodes cannot
be separated from their parents. In other words, substructures are composed of atomic building blocks
corresponding to nodes, along with all of their direct children. In the case of a syntactic parse tree, these
are complete production rules for the associated parser grammar.
Syntactic Tree Kernel + BOW (K2) extends ST by allowing leaf nodes to be part of the feature space.
The leaves of the trees correspond to words, i.e., we allow bag-of-words (BOW).
Partial Tree Kernel (K3) can be effectively applied to both constituency and dependency parse trees.
It generates all possible connected tree fragments, e.g., sibling nodes can be also separated and be part
of different tree fragments. In other words, a fragment is any possible tree path from whose nodes other
tree paths can depart. Thus, it can generate a very rich feature space.
Sequence Kernel (K4) is the traditional string kernel applied to the words of a sentence. In our case, we
apply it to the sequence of concepts.
3.2 Semantic/syntactic structures
As mentioned before, tree kernels allow us to compute structural similarities between two trees without
explicitly representing them as feature vectors. For the CSL task, we experimented with a number of tree
representations that incorporate different levels of syntactic and semantic information.
To capture the structural dependencies between the semantic tags, we use a basic tree (Figure 1a)
where the words of a sentence are tagged with their semantic tags. More specifically, the words in the
sentence constitute the leaves of the tree, which are in turn connected to the pre-terminals containing the
semantic tags in BIO notation (?B?=begin, ?I?=inside, ?O?=outside). The BIO tags are then generalized
in the upper level, and so on. The basic tree does not include any syntactic information.
196
However, part-of-speech (POS) and phrasal information could be informative for both segmentation
and labeling in semantic parsing. To incorporate this information, we use two extensions of the basic
tree: one that includes the POS tags of the words (Figure 1e), and another one that includes both POS
tags and syntactic chunks (Figure 1c). The POS tags are children of the semantic tags, whereas the
chunks (i.e., phrasal information) are included as parents of the semantic tags.
We also experiment with full syntactic trees (Figure 1d) to see the impact of deep syntactic informa-
tion. The semantic tags are attached to the pre-terminals (i.e., POS tags) in the syntactic tree. We use the
Stanford POS tagger and syntactic parser and the Twitter NLP tool
1
for the shallow trees.
A sentence containing multiple clauses exhibits a coherence structure. For instance, in our example,
the first clause ?along my route tell me the next steak house? is elaborated by the second clause ?that is
within a mile?. The relations by which clauses in a text are linked are called coherence relations (e.g.,
Elaboration, Contrast). Discourse structures capture this coherence structure of text and provide addi-
tional semantic information that could be useful for the CSL task (Stede, 2011). To build the discourse
structure of a sentence, we use a state-of-the-art discourse parser (Joty et al., 2012) which generates
discourse trees in accordance with the Rhetorical Structure Theory of discourse (Mann and Thompson,
1988), as exemplified in Figure 1b. Notice that a text span linked by a coherence relation can be either a
nucleus (i.e., the core part) or a satellite (i.e., a supportive one) depending on how central the claim is.
3.3 New features
In order to compare to the structured representation, we also devoted significant effort towards engineer-
ing a set of features to be used in a flat feature-vector representation; they can be used in isolation or in
combination with the kernel-based approach (as a composite kernel using a linear combination):
CRF-based: these include the basic features used to train the initial semi-CRF model (cf. Section 2.1).
n-gram based: we collected 3- and 4-grams of the output label sequence at the level of concepts, with
artificial tags inserted to identify the start (?S?) and end (?E?) of the sequence.
2
Probability-based: two features computing the probability of the label sequence as an average of the
probabilities at the word level p(l
i
|w
i
) (i.e., assuming independence between words). The unigram prob-
abilities are estimated by frequency counts using maximum likelihood in two ways: (i) from the complete
100-best list of hypotheses; (ii) from the training set (according to the gold standard annotation).
DB-based: a single feature encoding the number of results returned from the database when constructing
a query using the conjunction of all semantic segments in the hypothesis. Three possible values are
considered by using a threshold t: 0 (if the query result is void), 1 (if the number of results is in [1, t]),
and 2 (if the number of results is greater than t). In our case, t is empirically set to 10,000.
4 Experiments
The experiments aim at investigating which structures, and thus which linguistic models and combination
with other models, are the most appropriate for our reranker. We first calculate the oracle accuracy in
order to compute an upper bound of the reranker. Then we present experiments with the feature vectors,
tree kernels, and representations of linguistic information introduced in the previous sections.
4.1 Experimental setup
In our experiments, we use questions annotated with semantic tags in the restaurant domain,
3
which were
collected by McGraw et al. (2012) through crowdsourcing on Amazon Mechanical Turk.
4
We split the
dataset into training, development and test sets. Table 1 shows statistics about the dataset and about the
size of the parts we used for training, development and testing (see the semi-CRF line).
We subsequently split the training data randomly into ten folds. We generated the N -best lists on
the training set in a cross-validation fashion, i.e., iteratively training on nine folds and annotating the
remaining fold. We computed the 100-best hypotheses for each example.
1
Available from http://nlp.stanford.edu/software/index.shtml and https://github.com/aritter/twitter nlp, respectively.
2
For instance, if the output sequence is Other-Rating-Other-Amenity the 3-gram patterns would be: S-Other-Rating, Other-
Rating-Other, Rating-Other-Amenity, and Other-Amenity-E.
3
http://www.sls.csail.mit.edu/downloads/restaurant
4
We could not use the datasets used by Dinarelli et al. (2011), because they use French and Italian corpora for which there
are no reliable syntactic and discourse parsers.
197
Train Devel. Test Total
semi-CRF 6,922 739 1,521 9,182
Reranker 28,482 3,695 7,605 39,782
Table 1: Number of instances and pairs used to
train the semi-CRF and rerankers, respectively.
N 1 2 5 10 100
F
1
83.03 87.76 92.63 95.23 98.72
Table 2: Oracle F
1
-score for N -best lists
of different lengths.
We used the development set to experiment and tune the hyper-parameters of the reranking model. The
results on the development set presented in Section 4.2 were obtained by semi-CRF and reranking models
learned on the training set. The results on the test set were obtained by models trained on the training
plus development sets. Similarly, the N -best lists for the development and test sets were generated using
a single semi-CRF model trained on the training set and the training+development sets, respectively.
Each generated hypothesis is represented using a semantic tree and a feature vector (explained in
Section 3) and two extra features accounting for (i) the semi-CRF probability of the hypothesis, and
(ii) the hypothesis reciprocal rank in the N -best list. SVM-Light-TK
5
is used to train the reranker with
a combination of tree kernels and feature vectors (Moschitti, 2006; Joachims, 1999). Although we
tried several parameters on the validation set, we observed that the default values yielded the highest
results. Thus, we used the default c (trade-off) and tree kernel parameters and a linear kernel for the
feature vectors. Table 1 shows the sizes of the train, the development and the test sets used for the
semi-CRF as well as the number of pairs generated for the reranker. As a baseline, we picked the best-
scored hypothesis in the list, according to the semi-CRF tagger. The evaluation measure used in all
the experiments is the harmonic mean of precision and recall, i.e., the F
1
-score (van Rijsbergen, 1979),
computed at the token level and micro-averaged over the different semantic types.
6
We used paired t-test
to measure the statistical significance of the improvements: we split the test set into 31 equally-sized
samples and performed t-tests based on the F
1
-scores of different models on the resulting samples.
4.2 Results
Oracle accuracy. Table 2 shows the oracle F
1
-score for N -best lists of different lengths, i.e., which
can be achieved by picking the best candidate of the N -best list for various values of N . We can see that
going to 5-best increases the oracle F
1
-score by almost ten points absolute. Going down to 10-best only
adds 2.5 extra F
1
points absolute, and a 100-best list adds 3.5 F
1
points more to yield a respectable F
1
-
score of 98.72. This high result can be explained considering that the size of the complete hypothesis set
is smaller than 100 for most questions. Thus, we can conclude that theN -best lists do include many good
options and do offer quite a large space for potential improvement. We can further observe that going to
5-best lists offers a good balance between the length of the list and the possibility to improve F
1
-score:
generally, we do not want too long N -best lists since they slow down computation and also introduce
more opportunities to make the wrong choice for a reranker (since there are just more candidates to
choose from). In our experiments with larger N , we observed improvements only for 10 and only on the
development set; thus, we will focus on 5-best lists in our experiments below.
K0 K1 K2 K3 K4
Dev 84.21 82.92 83.07 85.07 83.78
Test 84.08 83.19 83.20 84.61 82.93
Table 3: Results for using different tree kernels on the basic tree (BT) representation.
Choosing the best tree kernel. We first select the most appropriate tree kernel to limit the number
of experiment variables. Table 3 shows the results of different tree kernels using the basic tree (BT)
representation (see Figure 1a). We can observe that for both the development set and the test set, kernel
K3 (see Section 3.1) yields the highest F
1
-score.
Impact of feature vectors. Table 4 presents the results for the feature vector experiments in terms
of F
1
-scores and relative error reductions (row RER). The first column shows the baseline, when no
reranking is used; the following four columns contain the results when using vectors including different
5
http://disi.unitn.it/moschitti/Tree-Kernel.htm
6
?Other? is not considered a semantic type, thus ?Other? tokens are not included in the F
1
calculation.
198
Baseline n-grams CRF features Count DB ProbBased AllFeat
Dev 83.86 83.79 83.96 83.80 83.86 83.87 84.49
RER -0.4 0.6 -0.4 0.0 0.0 3.9
Test 83.03 82.90 83.44 82.90 83.01 83.09 83.86
RER -0.7 2.4 -0.7 -0.1 0.3 4.8
Table 4: Feature vector experiments: F
1
score and relative error reduction (in %).
Combining AllFeat and
Baseline BT BTP ShT ST AllFeat +BT +ShT +ShT +BT
Dev 83.86 85.07 85.41 85.06 84.30 84.49 85.57 85.58 85.33
RER 7.5 9.6 7.4 2.8 3.9 10.6 10.7 9.1
Test 83.03 84.61 84.63 84.07 83.81 83.86 84.67 84.79 84.76
RER 9.3 9.4 6.1 4.5 4.8 9.6 10.2 10.2
p.v. 0.00049 0.0002 0.012 0.032 0.00018 0.00028 0.00004 0.000023
Table 5: Tree kernel experiments: F
1
-score, relative error reduction (in %), and p-values.
kinds of features: (i) n-gram features, (ii) all features used by the semi-CRF, (iii) count features, and
(iv) database (DB) features. In each case, we include two additional features: the semi-CRF score
(i.e., the probability) and the reciprocal rank of the hypothesis in the N -best list. Among (i)?(iv), only
the semi-CRF features seem to help; the rest either show no improvements or degrade the performance.
However, putting all these features together (AllFeat) yields sizable gains in terms of F
1
-score and a
relative error reduction of 4-5%; the improvement is statistically significant, and it is slightly larger on
the test dataset compared to the development dataset.
Impact of structural kernels and combinations. Table 5 shows the results when experimenting with
various tree structures (see columns 2-5): (i) the basic tree (BT), (ii) the basic tree augmented with
part-of-speech information (BTP), (iii) shallow syntactic tree (ShT), and (iv) syntactic tree (ST). We
can see that the basic tree works rather well, yielding +1.6 F
1
-score on the test dataset, but adding POS
information can help a bit more, especially for the tuning dataset. Interestingly, the syntactic tree kernels,
ShT and ST, perform worse than BT and BTP, especially on the test dataset. The last three columns in the
table show the results when we combine the AllFeat feature vector (see Table 4) with BT and ShT. We can
see that combining AllFeat with ShT works better, on both development and test sets, than combining it
with BT or with both ShT and BT. Also note the big jump in performance from AllFeat to AllFeat+ShT.
Overall, we can conclude that shallow syntax has a lot to offer over AllFeat, and it is preferable over BT
in the combination with AllFeat. The improvements reported in Tables 5 and 6 are statistically significant
when compared to the semi-CRF baseline as shown by the p.v. (value) row. Moreover, the improvement
of AllFeat + ShT over BT is also statistically significant (p.v.<0.05).
Combining AllFeat and
Baseline DS +DS +DS +BT +DS +ShT
Dev 83.86 84.61 85.14 85.43 85.46
RER 4.7 7.9 9.7 9.9
Test 83.03 84.38 84.55 84.63 84.67
RER 7.9 8.9 9.4 9.6
p.v. 0.0005 0.0001 0.00066 0.00015
Table 6: Experiments with discourse kernels: F
1
score, relative error reduction (in %), and p-values.
Discourse structure. Finally, Table 6 shows the results for the discourse tree kernel (DS), which we
designed and experimented with for the first time in this paper. We see that DS yields sizable improve-
ments over the baseline. We also see that further gains can be achieved by combining DS with AllFeat,
and also with BT and ShT, the best combination being AllFeat+DS+ShT (see last column). However,
comparing to Table 5, we see that it is better to use just AllFeat+ShT and leave DS out. We would like
to note though that the discourse parser produced non-trivial trees for only 30% of the hypotheses (due
to the short, simple nature of the questions); in the remaining cases, it probably hurt rather than helped.
We conclude that discourse structure has clear potential, but how to make best use of it, especially in the
case of short simple questions, remains an open question that deserves further investigation.
199
Tag ID Other Rating Restaurant Amenity Cuisine Dish Hours Location Price
0 Other 8260 35 43 110 15 19 55 113 9
1 Rating 29 266 0 14 3 6 0 0 8
2 Restaurant 72 6 657 20 19 15 0 5 0
3 Amenity 117 9 10 841 27 27 7 12 7
4 Cuisine 36 2 12 26 543 44 3 1 0
5 Dish 23 0 4 20 33 324 1 4 0
6 Hours 61 0 1 2 6 1 426 9 1
7 Location 104 1 14 20 2 1 1 1457 0
8 Price 22 1 0 7 0 2 0 1 204
Table 7: Confusion matrix for the output of the best performing system.
4.3 Error analysis and discussion
Table 7 shows the confusion matrix for our best-performing model AllFeat+ShT (rows = gold standard
tags; columns = system predicted tags). Given the good results of the semantic parser, the numbers in the
diagonal are clearly dominating the weight of the matrix. The largest errors correspond to missed (first
column) and over-generated (first row) entity tokens. Among the proper confusions between semantic
types, Dish and Cuisine tend to mislead each other most. This is due to the fact that these two tags
are semantically similar, thus making them hard to distinguish. We can also notice that it is difficult to
identify Amenity correctly, and the model mistakenly tags many other tags as Amenity. We looked into
some examples to further investigate the errors. Our findings are as follow:
Inaccuracies and inconsistencies in human annotations. Since the annotations were done in Me-
chanical Turk, they have many inaccuracies and inconsistencies. For example, the word good with
exactly the same sense was tagged as both Other and Rating by the Turkers in the following examples:
Gold: [
Other
any good] [
Price
cheap] [
Cuisine
german] [
Other
restaurants] [
Location
nearby]
Model: [
Other
any] [
Rating
good] [
Price
cheap] [
Cuisine
german] [
Other
restaurants] [
Location
nearby]
Gold: [
Other
any place] [
Location
along the road] [
Other
has a] [
Rating
good] [
Dish
beer] [
Other
selection that also serves] ...
Requires lexical semantics and more coverage. In some cases our model fails to generalize well. For
instance, it fails to correctly tag establishments and tameles for the following examples. This suggests
that we need to consider other forms of semantic information, e.g., distributional and compositional
semantics computed from large corpora and/or using Web resources such as Wikipedia.
Gold: [
Other
any] [
Location
dancing establishments] [
Other
with] [
Price
reasonable] [
Other
pricing]
Model: [
Other
any] [
Amenity
dancing] [
Other
establishments] [
Other
with] [
Price
reasonable] [
Other
pricing]
Gold: [
Other
any] [
Cuisine
mexican] [
Other
places have a] [
Dish
tameles] [
Amenity
special today]
Model: [
Other
any] [
Cuisine
mexican] [
Other
places have a] [
Amenity
tameles] [
Other
special] [
Hours
today]
5 Conclusions
We have presented a study on the usage of syntactic and semantic structured information for improved
Concept Segmentation and Labeling (CSL). Our approach is based on reranking a set of N -best se-
quences generated by a state-of-the-art semi-CRF model for CSL. The syntactic and semantic informa-
tion was encoded in tree-based structures, which we used to train a reranker with kernel-based Support
Vector Machines. We empirically compared several variants of syntactic/semantic structured representa-
tions and kernels, including also a vector of manually engineered features.
The first and foremost conclusion from our study is that structural kernels yield significant improve-
ment over the strong baseline system, with a relative error reduction of ?10%. This more than doubles
the improvement when using the explicit feature vector. Second, we observed that shallow syntactic
information also improves results significantly over the best model. Unfortunately, the results obtained
using full syntax and discourse trees are not so clear. This is probably explained by the fact that user
queries are rather short and linguistically not very complex. We also observed that the upper bound per-
formance for the reranker still leaves large room for improvement. Thus, it remains to be seen whether
some alternative kernel representations can be devised to make better use of discourse and other syntac-
tic/semantic information. Also, we think that some innovative features based on analyzing the results
obtained from our database (or the Web) when querying with the segments represented in each hypothe-
ses have the potential to improve the results. All these concerns will be addressed in future work.
200
Acknowledgments
This research is developed by the Arabic Language Technologies (ALT) group at Qatar Computing Re-
search Institute (QCRI) within the Qatar Foundation in collaboration with MIT. It is part of the Interactive
sYstems for Answer Search (Iyas) project.
References
Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete
structures, and the voted perceptron. In Proceedings of the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 263?270, Philadelphia, PA, USA.
Renato De Mori, Dilek Hakkani-T?ur, Michael McTear, Giuseppe Riccardi, and Gokhan Tur. 2008. Spoken
language understanding: a survey. IEEE Signal Processing Magazine, 25:50?58.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe Riccardi. 2011. Discriminative reranking for spoken lan-
guage understanding. IEEE Transactions on Audio, Speech and Language Processing, 20(2):526?539.
Ruifang Ge and Raymond Mooney. 2006. Discriminative reranking for semantic parsing. In Proceedings of the
21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association
for Computational Linguistics, COLING-ACL?06, pages 263?270, Sydney, Australia.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics,
28(3):245?288.
Thorsten Joachims. 1999. Advances in kernel methods. In Bernhard Sch?olkopf, Christopher J. C. Burges, and
Alexander J. Smola, editors, Making Large-scale Support Vector Machine Learning Practical, pages 169?184,
Cambridge, MA, USA. MIT Press.
Shafiq Joty, Giuseppe Carenini, and Raymond Ng. 2012. A novel discriminative framework for sentence-level dis-
course analysis. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing
and Computational Natural Language Learning, EMNLP-CoNLL ?12, pages 904?915, Jeju Island, Korea.
Rohit Kate and Raymond Mooney. 2006. Using string-kernels for learning semantic parsers. In Proceedings of
the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association
for Computational Linguistics, COLING-ACL ?06, pages 913?920, Sydney, Australia.
Terry Koo and Michael Collins. 2005. Hidden-variable models for discriminative reranking. In Proceedings
of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing,
HLT-EMNLP ?05, pages 507?514, Vancouver, British Columbia, Canada.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting-based parse reranking with subtree features. In
Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ?05, pages 189?
196, Ann Arbor, MI, USA.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine
Learning, ICML ?01, pages 282?289, Williamstown, MA, USA.
William Mann and Sandra Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Llu??s M`arquez, Xavier Carreras, Kenneth Litkowski, and Suzanne Stevenson. 2008. Semantic Role Labeling: An
Introduction to the Special Issue. Computational Linguistics, 34(2):145?159.
Ian McGraw, Scott Cyphers, Panupong Pasupat, Jingjing Liu, and Jim Glass. 2012. Automating crowd-supervised
learning for spoken language systems. In Proceedings of 13th Annual Conference of the International Speech
Communication Association, INTERSPEECH ?12, Portland, OR, USA.
Scott Miller, Richard Schwartz, Robert Bobrow, and Robert Ingria. 1994. Statistical language processing using
hidden understanding models. In Proceedings of the workshop on Human Language Technology, HLT ?94,
pages 278?282, Morristown, NJ, USA.
Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2006. Semantic role labeling via tree kernel joint
inference. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X),
pages 61?68, New York City, June.
201
Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In
Proceedings of the 17th European Conference on Machine Learning, ECML?06, pages 318?329, Berlin, Hei-
delberg. Springer-Verlag.
Truc-Vien T. Nguyen and Alessandro Moschitti. 2012. Structural reranking models for named entity recognition.
Intelligenza Artificiale, 6(2):177?190.
Kishore Papineni, Salim Roukos, and Todd Ward. 1998. Maximum likelihood and discriminative training of
direct translation models. In Proceedings of the IEEE International Conference on Acoustics, Speech and
Signal Processing, volume 1, pages 189?192, Seattle, WA, USA.
Roberto Pieraccini, Esther Levin, and Chin-Hui Lee. 1991. Stochastic representation of conceptual structure in
the ATIS task. In Proceedings of the Workshop on Speech and Natural Language, HLT ?91, pages 121?124,
Pacific Grove, CA, USA.
Christian Raymond and Giuseppe Riccardi. 2007. Generative and discriminative algorithms for spoken language
understanding. In Proceedings of 8th Annual Conference of the International Speech Communication Associa-
tion, INTERSPEECH ?07, pages 1605?1608, Antwerp, Belgium.
Yigal Dan Rubinstein and Trevor Hastie. 1997. Discriminative vs informative learning. In Proceedings of the
Third International Conference on Knowledge Discovery and Data Mining, KDD ?97, pages 49?53, Newport
Beach, CA, USA.
Guzm?an Santaf?e, Jose Lozano, and Pedro Larra?naga. 2007. Discriminative vs. generative learning of Bayesian
network classifiers. Lecture Notes in Computer Science, 4724:453?464.
Sunita Sarawagi and William Cohen. 2004. Semi-Markov conditional random fields for information extraction.
In Proceedings of the 18th Annual Conference on Neural Information Processing Systems, NIPS ?04, pages
1185?1192, Vancouver, British Columbia, Canada.
Stephanie Seneff. 1989. TINA: A probabilistic syntactic parser for speech understanding systems. In Proceedings
of the Workshop on Speech and Natural Language, HLT ?89, pages 168?178, Philadelphia, PA, USA.
Manfred Stede. 2011. Discourse Processing. Synthesis Lectures on Human Language Technologies. Morgan and
Claypool Publishers.
Cornelis Joost van Rijsbergen. 1979. Information Retrieval. Butterworth.
Ye-Yi Wang, Raphael Hoffmann, Xiao Li, and Jakub Szymanski. 2009. Semi-supervised learning of semantic
classes for query understanding: from the web and for the web. In Proceedings of the 18th ACM Conference on
Information and Knowledge Management, CIKM ?09, pages 37?46, New York, NY, USA.
202
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 148?157,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Hybrid Morpheme-Word Representation
for Machine Translation of Morphologically Rich Languages?
Minh-Thang Luong Preslav Nakov Min-Yen Kan
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
{luongmin,nakov,kanmy}@comp.nus.edu.sg
Abstract
We propose a language-independent approach
for improving statistical machine translation
for morphologically rich languages using a
hybrid morpheme-word representation where
the basic unit of translation is the morpheme,
but word boundaries are respected at all stages
of the translation process. Our model extends
the classic phrase-based model by means
of (1) word boundary-aware morpheme-level
phrase extraction, (2) minimum error-rate
training for a morpheme-level translation
model using word-level BLEU, and (3) joint
scoring with morpheme- and word-level lan-
guage models. Further improvements are
achieved by combining our model with the
classic one. The evaluation on English to
Finnish using Europarl (714K sentence pairs;
15.5M English words) shows statistically sig-
nificant improvements over the classic model
based on BLEU and human judgments.
1 Introduction
The fast progress of statistical machine translation
(SMT) has boosted translation quality significantly.
While research keeps diversifying, the word remains
the atomic token-unit of translation. This is fine for
languages with limited morphology like English and
French, or no morphology at all like Chinese, but
it is inadequate for morphologically rich languages
like Arabic, Czech or Finnish (Lee, 2004; Goldwater
and McClosky, 2005; Yang and Kirchhoff, 2006).
?This research was sponsored in part by CSIDM (grant #
200805) and by a National Research Foundation grant entitled
?Interactive Media Search? (grant # R-252-000-325-279).
There has been a line of recent SMT research
that incorporates morphological analysis as part of
the translation process, thus providing access to the
information within the individual words. Unfortu-
nately, most of this work either relies on language-
specific tools, or only works for very small datasets.
Below we propose a language-independent ap-
proach to SMT of morphologically rich lan-
guages using a hybrid morpheme-word representa-
tion where the basic unit of translation is the mor-
pheme, but word boundaries are respected at all
stages of the translation process. We use unsuper-
vised morphological analysis and we incorporate its
output into the process of translation, as opposed to
relying on pre-processing and post-processing only
as has been done in previous work.
The remainder of the paper is organized as fol-
lows. Section 2 reviews related work. Sections 3
and 4 present our morphological and phrase merging
enhancements. Section 5 describes our experiments,
and Section 6 analyzes the results. Finally, Section 7
concludes and suggests directions for future work.
2 Related Work
Most previous work on morphology-aware ap-
proaches relies heavily on language-specific tools,
e.g., the TreeTagger (Schmid, 1994) or the Buck-
walter Arabic Morphological Analyzer (Buckwal-
ter, 2004), which hampers their portability to
other languages. Moreover, the prevalent method
for incorporating morphological information is by
heuristically-driven pre- or post-processing. For
example, Sadat and Habash (2006) use different
combinations of Arabic pre-processing schemes
148
for Arabic-English SMT, whereas Oflazer and El-
Kahlout (2007) post-processes Turkish morpheme-
level translations by re-scoring n-best lists with a
word-based language model. These systems, how-
ever, do not attempt to incorporate their analysis as
part of the decoding process, but rather rely on mod-
els designed for word-token translation.
We should also note the importance of the trans-
lation direction: it is much harder to translate from a
morphologically poor to a morphologically rich lan-
guage, where morphological distinctions not present
in the source need to be generated in the target lan-
guage. Research in translating into morphologically
rich languages, has attracted interest for languages
like Arabic (Badr et al, 2008), Greek (Avramidis
and Koehn, 2008), Hungarian (Nova?k, 2009; Koehn
and Haddow, 2009), Russian (Toutanova et al,
2008), and Turkish (Oflazer and El-Kahlout, 2007).
These approaches, however, either only succeed in
enhancing the performance for small bi-texts (Badr
et al, 2008; Oflazer and El-Kahlout, 2007), or im-
prove only modestly for large bi-texts1.
3 Morphological Enhancements
We present a morphologically-enhanced version of
the classic phrase-based SMT model (Koehn et al,
2003). We use a hybrid morpheme-word representa-
tion where the basic unit of translation is the mor-
pheme, but word boundaries are respected at all
stages of the translation process. This is in con-
trast with previous work, where morphological en-
hancements are typically performed as pre-/post-
processing steps only.
In addition to changing the basic translation token
unit from a word to a morpheme, our model extends
the phrase-based SMT model with the following:
1. word boundary-aware morpheme-level phrase
extraction;
2. minimum error-rate training for a morpheme-
level model using word-level BLEU;
3. joint scoring with morpheme- and word-level
language models.
We first introduce our morpheme-level represen-
tation, and then describe our enhancements.
1Avramidis and Koehn (2008) improved by 0.15 BLEU over
a 18.05 English-Greek baseline; Toutanova et al (2008) im-
proved by 0.72 BLEU over a 36.00 English-Russian baseline.
3.1 Morphological Representation
Our morphological representation is based on the
output of an unsupervised morphological analyzer.
Following Virpioja et al (2007), we use Morfessor,
which is trained on raw tokenized text (Creutz and
Lagus, 2007). The tool segments words into mor-
phemes annotated with the following labels: PRE
(prefix), STM (stem), SUF (suffix). Multiple prefixes
and suffixes can be proposed for each word; word
compounding is allowed as well. The output can be
described by the following regular expression:
WORD = ( PRE* STM SUF* )+
For example, uncarefully is analyzed as
un/PRE+ care/STM+ ful/SUF+ ly/SUF
The above token sequence forms the input to our
system. We keep the PRE/STM/SUF tags as part
of the tokens, and distinguish between care/STM+
and care/STM. Note also that the ?+? sign is ap-
pended to each nonfinal tag so that we can distin-
guish word-internal from word-final morphemes.
3.2 Word Boundary-aware Phrase Extraction
The core translation structure of a phrase-based
SMT model is the phrase table, which is learned
from a bilingual parallel sentence-aligned corpus,
typically using the alignment template approach
(Och and Ney, 2004). It contains a set of bilingual
phrase pairs, each associated with five scores: for-
ward and backward phrase translation probabilities,
forward and backward lexicalized translation proba-
bilities, and a constant phrase penalty.
The maximum phrase length n is normally limited
to seven words; higher values of n increase the table
size exponentially without actually yielding perfor-
mance benefit (Koehn et al, 2003). However, things
are different when translating with morphemes, for
two reasons: (1) morpheme-token phrases of length
n can span less than n words; and (2) morpheme-
token phrases may only partially span words.
The first point means that morpheme-token
phrase pairs span fewer word tokens, and thus cover
a smaller context, which may result in fewer total
extracted pairs compared to a word-level approach.
Figure 1 shows a case where three Finnish words
consist of nine morphemes. Previously, this issue
was addressed by simply increasing the value of n
when using morphemes, which is of limited help.
149
SRC = theSTM newSTM , unPRE+ democraticSTM immigrationSTM policySTM
TGT = uusiSTM , ep?PRE+ demokraatSTM+ tSUF+ iSUF+ sSUF+ enSUF maahanmuuttoPRE+ politiikanSTM
(uusi=new  ,  ep?demokraattisen=undemocratic    maahanmuuttopolitiikan=immigration policy)
Figure 1: Example of English-Finnish bilingual fragments morphologically segmented by Morfessor. Solid links
represent IBM Model 4 alignments at the morpheme-token level. Translation glosses for Finnish are given below.
The second point is more interesting: morpheme-
level phrases may span words partially, making them
potentially usable in translating unknown inflected
forms of known source language words, but also
creates the danger of generating sequences of mor-
phemes that are not legal target language words.
For example, let us consider the phrase in Fig-
ure 1: unPRE+ democraticSTM. The original
algorithm will extract the spurious phrase epa?PRE+
demokraatSTM+ tSUF+ iSUF+ sSUF+, beside
the correct one that has enSUF appended at the
end. Such a spurious phrase does not generally help
in translating unknown inflected forms, especially
for morphologically-rich languages that feature mul-
tiple affixes, but negatively affects the translation
model in terms of complexity and quality.
We solve both problems by modifying the phrase-
pair extraction algorithm so that morpheme-token
phrases can extend longer than n, as long as they
span n words or less. We further require that
word boundaries be respected2, i.e., morpheme-
token phrases span a sequence of whole words. This
is a fair extension of the morpheme-token system
with respect to a word-token one since both are re-
stricted to span up to n word-tokens.
3.3 Morpheme-Token MERT Optimizing
Word-Token BLEU
Modern phrase-based SMT systems use a log-linear
model with the following typical feature functions:
language model probabilities, word penalty, distor-
tion cost, and the five parameters from the phrase ta-
ble. Their weights are set by optimizing BLEU score
(Papineni et al, 2001) directly using minimum error
rate training (MERT), as suggested by Och (2003).
In previous work, phrase-based SMT systems
using morpheme-token input/output naturally per-
2This means that we miss the opportunity to generate new
wordforms for known baseforms, but removes the problem of
proposing nonwords in the target language.
formed MERT at the morpheme-token level as well.
This is not optimal since the final expected system
output is a sequence of words, not morphemes. The
main danger is that optimizing a morpheme-token
BLEU score could lead to a suboptimal weight for
the word penalty feature function: this is because
the brevity penalty of BLEU is calculated with re-
spect to the number of morphemes, which may vary
for sentences with an identical number of words.
This motivates us to perform MERT at the word-
token level, although our input consists of mor-
phemes. In particular, for each iteration of MERT,
as soon as the decoder generates a morpheme-token
translation for a sentence, we convert it into a word-
token sequence, which is used to calculate BLEU.
We thus achieve MERT optimization at the word-
token level while translating a morpheme-token in-
put and generating a morpheme-token output.
3.4 Scoring with Twin Language Models
An SMT system that takes morpheme-token input
and generates morpheme-token output should natu-
rally use a morpheme-token language model (LM).
This has the advantage of alleviating the problem of
data sparseness, especially when translating into a
morphologically rich language, since the LM would
be able to handle some new unseen inflected forms
of known words. On the negative side, a morpheme-
token LM spans fewer word-tokens and thus has a
more limited word ?horizon? compared to one op-
erating at the word level. As with the maximum
phrase length, mechanically increasing the order of
the morpheme-token LM has a limited impact.
In order to address the issue in a more princi-
pled manner, we enhance our model with a second
LM that works at the word-token level. This LM is
used together with the morpheme-token LM, which
is achieved by using two separate feature functions
in the log-linear SMT model: one for each LM. We
further had to modify the Moses decoder so that
150
uusiSTM  , ep?PRE+ demokraatSTM+ tSUF+ iSUF+ sSUF+ enSUF maahanmuuttoPRE+ politiikanSTM 
? Score: ?sSUF+ enSUF maahanmuuttoPRE+?  ;  ?enSUF maahanmuuttoPRE+ politiikanSTM ?
? Concatenate: uusi , ep?demokraattisen maahanmuuttopolitiikan
? Score: ?, ep?demokraattisen maahanmuuttopolitiikan?
Previous hypotheses Current hypothesis
(i)
(ii)
(iii)
Figure 2: Scoring with twin LMs. Shown are: (i) The current state of the decoding process with the target phrases
covered by the current partial hypotheses. (ii, iii) Scoring with 3-gram morpheme-token and 3-gram word-token LMs,
respectively. For the word-token LM, the morpheme-token sequence is concatenated into word-tokens before scoring.
it can be enhanced with an appropriate word-token
?view? on the partial morpheme-level hypotheses3.
The interaction of the twin LMs is illustrated in
Figure 2. The word-token LM can capture much
longer phrases and more complete contexts such
as ?, epa?demokraattisen maahanmuuttopolitiikan?
compared to the morpheme-token LM.
Note that scoring with two LMs that see the out-
put sequence as different numbers of tokens is not
readily offered by the existing SMT decoders. For
example, the phrase-based model in Moses (Koehn
et al, 2007) allows scoring with multiple LMs, but
assumes they use the same token granularity, which
is useful for LMs trained on different monolingual
corpora, but cannot handle our case. While the fac-
tored translation model (Koehn and Hoang, 2007) in
Moses does allow scoring with models of different
granularity, e.g., lemma-token and word-token LMs,
it requires a 1:1 correspondence between the tokens
in the different factors, which clearly is not our case.
Note that scoring with twin LMs is conceptu-
ally superior to n-best re-scoring with a word-token
LM, e.g., (Oflazer and El-Kahlout, 2007), since it is
tightly integrated into decoding: it scores partial hy-
potheses and influenced the search process directly.
4 Enriching the Translation Model
Another general strategy for combining evidence
from the word-token and the morpheme-token rep-
resentations is to build two separate SMT systems
and then combine them. This can be done as a
post-processing system combination step; see (Chen
et al, 2009a) for an overview of such approaches.
3We use the term ?hypothesis? to collectively refer to the
following (Koehn, 2003): the source phrase covered, the cor-
responding target phrase, and most importantly, a reference to
the previous hypothesis that it extends.
However, for phrase-based SMT systems, it is theo-
retically more appealing to combine their phrase ta-
bles since this allows the translation models of both
systems to influence the hypothesis search directly.
We now describe our phrase table combination
approach. Note that it is orthogonal to the work pre-
sented in the previous section, which suggests com-
bining the two (which we will do in Section 5).
4.1 Building a Twin Translation Model
Figure 3 shows a general scheme of our twin trans-
lation model. First, we tokenize the input at differ-
ent granularities: (1) morpheme-token and (2) word-
token. We then build separate phrase tables (PT) for
the two inputs: a word-token PTw and a morpheme-
token PTm. Second, we re-tokenize PTw at the
morpheme level, thus obtaining a new phrase table
PTw?m, which is of the same granularity as PTm.
Finally, we merge PTw?m and PTm, and we input
the resulting phrase table to the decoder.
GIZA++
Decoding
Word alignment Morpheme alignment 
Word Morpheme
PTm
PTw?m
PTw
Morphological 
segmenta"on 
Phrase Extrac"on
PT merging
Phrase Extrac"on
GIZA++
Figure 3: Building a twin phrase table (PT). First, sep-
arate PTs are generated for different input granularities:
word-token and morpheme-token. Second, the word-
token PT is retokenized at the morpheme-token level. Fi-
nally, the two PTs are merged and used by the decoder.
151
4.2 Merging and Normalizing Phrase Tables
Below we first describe the two general phrase ta-
ble combination strategies used in previous work:
(1) direct merging using additional feature func-
tions, and (2) phrase table interpolation. We then
introduce our approach.
Add-feature methods. The first line of research
on phrase table merging is exemplified by (Niehues
et al, 2009; Chen et al, 2009b; Do et al, 2009;
Nakov and Ng, 2009). The idea is to select one of
the phrase tables as primary and to add to it all non-
duplicating phrase pairs from the second table to-
gether with their associated scores. For each entry,
features can be added to indicate its origin (whether
from the primary or from the secondary table). Later
in our experiments, we will refer to these baseline
methods as add-1 and add-2, depending on how
many additional features have been added. The val-
ues we used for these features in the baseline are
given in Section 5.4; their weights in the log-linear
model were set in the standard way using MERT.
Interpolation-based methods. A problem with
the above method is that the scores in the merged
phrase table that correspond to forward and back-
ward phrase translation probabilities, and forward
and backward lexicalized translation probabilities
can no longer be interpreted as probabilities since
they are not normalized any more. Theoretically,
this is not necessarily a problem since the log-linear
model used by the decoder does not assume that the
scores for the feature functions come from a normal-
ized probability distribution. While it is possible to
re-normalize the scores to convert them into prob-
abilities, this is rarely done; it also does not solve
the problem with the dropped scores for the dupli-
cated phrases. Instead, the conditional probabilities
in the two phrase tables are often interpolated di-
rectly, e.g., using linear interpolation. Representa-
tive work adopting this approach is (Wu and Wang,
2007). We refer to this method as interpolation.
Our method. The above phrase merging ap-
proaches have been proposed for phrase tables de-
rived from different sources. This is in contrast with
our twin translation scenario, where the morpheme-
token phrase tables are built from the same training
dataset; the main difference being that word align-
ments and phrase extraction were performed at the
word-token level for PTw?m and at the morpheme-
token level for PTm. Thus, we propose different
merging approaches for the phrase translation prob-
abilities and for the lexicalized probabilities.
In phrase-based SMT, phrase translation probabil-
ities are computed using maximum likelihood (ML)
estimation ?(f? |e?) = #(f? ,e?)?
f? #(f? ,e?)
, where #(f? , e?) is
the number of times the pair (f? , e?) is extracted from
the training dataset (Koehn et al, 2003). In order to
preserve the normalized ML estimations as much as
possible, we refrain from interpolation. Instead, we
use the raw counts for the two models #m(f? , e?) and
#w?m(f? , e?) directly as follows:
?(f? , e?) = #m(f? , e?) + #w?m(f? , e?)?
f? #m(f? , e?) +
?
f? #w?m(f? , e?)
For lexicalized translation probabilities, we would
like to use simple interpolation. However, we notice
that when a phrase pair belongs to only one of the
phrase tables, the corresponding lexicalized score
for the other table would be zero. This might cause
some good phrases to be penalized just because they
were not extracted in both tables, which we want to
prevent. We thus perform interpolation from PTm
and PTw according to the following formula:
lex(f? |e?) = ? ? lexm(f?m|e?m)
+ (1? ?)? lexw(f?w|e?w)
where the concatenation of f?m and e?m into word-
token sequences yields f?w and e?w, respectively.
If both (f?m, e?m) and (f?w, e?w) are present in PTm
and PTw, respectively, we have a simple interpola-
tion of their corresponding lexicalized scores lexm
and lexw. However, if one of them is missing, we
do not use a zero for its corresponding lexicalized
score, but use an estimate as follows.
For example, if only the entry (f?m, e?m) is present
in PTm, we first convert (f?m,e?m) into a word-token
pair (f?m?w,e?m?w), and then induce a correspond-
ing word alignment from the morpheme-token align-
ment of (f?m,e?m). We then estimate a lexicalized
phrase score using the original formula given in
(Koehn et al, 2003), where we plug this induced
word alignment and word-token lexical translation
probabilities estimated from the word-token dataset
The case when (f?w, e?w) is present in PTw, but
(f?m, e?m) is not, is solved similarly.
152
5 Experiments and Evaluation
5.1 Datasets
In our experiments, we use the English-Finnish data
from the 2005 shared task (Koehn and Monz, 2005),
which is split into training, development, and test
portions; see Table 1 for details. We further split
the training dataset into four subsets T1, T2, T3, and
T4 of sizes 40K, 80K, 160K, and 320K parallel sen-
tence pairs, which we use for studying the impact of
training data size on translation performance.
Sent. Avg. words Avg. morph.
en fi en fi
Train 714K 21.62 15.80 24.68 26.15
Dev 2K 29.33 20.99 33.40 34.94
Test 2K 28.98 20.72 33.10 34.47
Table 1: Dataset statistics. Shown are the number of
parallel sentences, and the average number of words and
Morfessor morphemes on the English and Finnish sides
of the training, development and test datasets.
5.2 Baseline Systems
We build two phrase-based baseline SMT systems,
both using Moses (Koehn et al, 2007):
w-system: works at the word-token level, extracts
phrases of up to seven words, and uses a 4-gram
word-token LM (as typical for phrase-based SMT);
m-system: works at the morpheme level, tok-
enized using Morfessor4 and augmented with ?+? as
described in Section 3.1.
Following Oflazer and El-Kahlout (2007) and Vir-
pioja et al (2007), we use phrases of up to 10
morpheme-tokens and a 5-gram morpheme-token
LM. None of the enhancements described previ-
ously is applied yet. After decoding, morphemes are
concatenated back to words using the ?+? markers.
To evaluate the translation quality, we compute
BLEU (Papineni et al, 2001) at the word-token
level. We further introduce a morpheme-token ver-
sion of BLEU, which we call m-BLEU: it first seg-
ments the system output and the reference trans-
lation into morpheme-tokens and then calculates a
BLEU score as usual. Table 2 shows the baseline re-
sults. We can see that the m-system achieves much
4We retrained Morfessor for Finnish/English on the
Finnish/English side of the training dataset.
w-system m-system
BLEU m-BLEU BLEU m-BLEU
T1 11.56 45.57 11.07 49.15
T2 12.95 48.63 12.68 53.78
T3 13.64 50.30 13.32 54.40
T4 14.20 50.85 13.57 54.70
Full 14.58 53.05 14.08 55.26
Table 2: Baseline system performance (on the test
dataset). Shown are word BLEU and morpheme m-
BLEU scores for the w-system and m-system.
higher m-BLEU scores, indicating that it may have
better morpheme coverage5. However, the m-system
is outperformed by the w-system on the classic word-
token BLEU, which means that it either does not
perform as well as the w-system or that word-token
BLEU is not capable of measuring the morpheme-
level improvements. We return to this question later.
5.3 Adding Morphological Enhancements
We now add our three morphological enhancements
from Section 3 to the baseline m-system:
phr (training) allow morpheme-token phrases to
get potentially longer than seven morpheme-tokens
as long as they cover no more than seven words;
tune (tuning) MERT for morpheme-token trans-
lations while optimizing word-token BLEU;
lm (decoding) scoring morpheme-token transla-
tion hypotheses with a 5-gram morpheme-token and
a 4-gram word-token LM.
The results are shown in Table 3 (ii). As we can
see, each of the three enhancements yields improve-
ments in BLEU score over the m-system, both for
small and for large training corpora. In terms of per-
formance ranking, tune achieves the best absolute
improvement of 0.66 BLEU points on T1 and of 0.47
points on the full dataset, followed by lm and phr.
Table 3 (iii) further shows that using phr and
lm together yields absolute improvements of 0.70
BLEU points on T1 and 0.50 points on the full train-
ing dataset. Further incorporating tune, however,
only helps when training on T1.
Overall, the morphological enhancements are on
par with the w-system baseline, and yield sizable im-
5Note that these morphemes were generated automatically
and thus many of them are erroneous.
153
System T1 (40K) Full (714K)
(i) w-system (w) 11.56 14.58
m-system (m) 11.07 14.08
(ii)
m+phr 11.44+0.37 14.43+0.35
m+tune 11.73+0.66 14.55+0.47
m+lm 11.58+0.51 14.53+0.45
(iii) m+phr+lm 11.77
+0.70 14.58+0.50
m+phr+lm+tune 11.90+0.83 14.39+0.31
Table 3: Impact of the morphological enhancements
(on test dataset). Shown are BLEU scores (in %) for
training on T1 and on the full dataset for (i) baselines,
(ii) enhancements individually, and (iii) combined. Su-
perscripts indicate absolute improvements w.r.t m-system.
provements over the m-system baseline: 0.83 BLEU
points on T1 and 0.50 on the full training dataset.
5.4 Combining Translation Tables
Finally, we investigate the effect of combining
phrase tables derived from a word-token and a
morpheme-token input, as described in Section 4.
We experiment with the following merging methods:
add-1: phrase table merging using one table as
primary and adding one extra feature6;
add-2: phrase table merging using one table as
primary and adding two extra features7;
interpolation: simple linear interpolation with
one parameter ?;
ourMethod: our interpolation-like merging
method described in Section 4.2.
Parameter tuning. We tune the parameters of the
above methods on the development dataset.
T1 (40K) Full (714K)
PTm is primary 11.99 13.45
PTw?m is primary 12.26 14.19
Table 4: Effect of selection of primary phrase table for
add-1 (on dev dataset): PTw?m, derived from a word-
token input, vs. PTm, from a morpheme-token input.
Shown is BLEU (in %) on T1 and the full training dataset.
For add-1 and add-2, we need to decide which
(PTw?m or PTm) phrase table should be consid-
6The feature values are e1, e2/3 or e1/3 (e=2.71828...);
when the phrase pair comes from both tables, from the primary
table only, and from the secondary table only, respectively.
7The feature values are (e1, e1), (e1, e0) or (e0, e1) when
the phrase pair comes from both tables, from the primary table
only, and from the secondary table only, respectively.
ered the primary table. Table 4 shows the results
when trying both strategies on add-1. As we can see,
using PTw?m as primary performs better on T1 and
on the full training dataset; thus, we will use it as
primary on the test dataset for add-1 and add-2.
For interpolation-based methods, we need to
choose a value for the interpolation parameters. Due
to time constraints, we use the same value for the
phrase translation probabilities and for the lexical-
ized probabilities, and we perform grid search for
? ? {0.3, 0.4, 0.5, 0.6, 0.7} using interpolate on the
full training dataset. As Table 5 shows, ? = 0.6
turns out to work best on the development dataset;
we will use this value in our experiments on the test
dataset both for interpolate and for ourMethod8.
? 0.3 0.4 0.5 0.6 0.7
BLEU 14.17 14.49 14.6 14.73 14.52
Table 5: Trying different values for interpolate (on dev
dataset). BLEU (in %) is for the full training dataset.
Evaluation on the test dataset. We integrate the
morphologically enhanced system m+phr+lm and
the word-token based w-system using the four merg-
ing methods above. The results for the full train-
ing dataset are shown in Table 6. As we can see,
add-1 and add-2 make little difference compared to
the m-system baseline. In contrast, interpolation and
ourMethod yield sizable absolute improvements of
0.55 and 0.74 BLEU points, respectively, over the
m-system; moreover, they outperform the w-system.
Merging methods Full (714K)
(i) m-system 14.08
w-system 14.58
(ii) add-1 14.25
+0.17
add-2 13.89?0.19
(iii) interpolation 14.63
+0.55
ourMethod 14.82+0.74
Table 6: Merging m+phr+lm and w-system (on test
dataset). BLEU (in %) is for the full training dataset. Su-
perscripts indicate performance gain/loss w.r.t m-system.
6 Discussion
Below we assess the significance of our results based
on micro-analysis and human judgments.
8Note that this might put ourMethod at disadvantage.
154
6.1 Translation Model Comparison
We first compare the following three phrase ta-
bles: PTm of m-system, maximum phrase length of
10 morpheme-tokens; PTw?m of w-system, maxi-
mum phrase length of 7 word-tokens, re-segmented
into morpheme-tokens; and PTm+phr ? morpheme-
token input using word boundary-aware phrase ex-
traction, maximum phrase length of 7 word-tokens.
Full (714K)
(i)
PTm 43.5M
PTw?m 28.9M
PTm+phr 22.5M
(ii) PTm+phr
?
PTm 21.4M
PTm+phr
?
PTw?m 10.7M
Table 7: Phrase table statistics. The number of phrase
pairs in (i) individual PTs and (ii) PT overlap, is shown.
PTm+phr versus PTm. Table 7 shows that
PTm+phr is about half the size of PTm. Still, as
Table 3 shows, m+phr outperforms the m-system.
Moreover, 95.07% (21.4M/22.5M) of the phrase
pairs in PTm+phr are also in PTm, which confirms
that boundary-aware phrase extraction selects good
phrase pairs from PTm to be retained in PTm+phr.
PTm+phr versus PTw?m. These two tables
are comparable in size: 22.5M and 28.9M pairs,
but their overlap is only 47.67% (10.7M/22.5M) of
PTm+phr. Thus, enriching the translation model
with PTw?m helps improve coverage.
6.2 Significance of the Results
Table 8 shows the performance of our system com-
pared to the two baselines: m-system and w-system.
We achieve an absolute improvement of 0.74 BLEU
points over the m-system, from which our system
evolved. This might look modest, but note that
the baseline BLEU is only 14.08, and thus the rel-
ative improvement is 5.6%, which is not trivial.
Furthermore, we outperform the w-system by 0.24
points (1.56% relative). Both improvements are sta-
tistically significant with p < 0.01, according to
Collins? sign test (Collins et al, 2005).
In terms of m-BLEU, we achieve an improvement
of 2.59 points over the w-system, which suggest our
system might be performing better than what stan-
dard BLEU suggests. Below we test this hypothesis
BLEU m-BLEU
ourSystem 14.82 55.64
m-system 14.08 55.26
w-system 14.58 53.05
Table 8: Our system vs. the two baselines (on the test
dataset): BLEU and m-BLEU scores (in %).
by means of micro-analysis and human evaluation.
Translation Proximity Match. We performed
automatic comparison based on corresponding
phrases between the translation output (out) and the
reference (ref), using the source (src) test dataset as
a pivot. The decoding log gave us the phrases used
to translate src to out, and we only needed to find
correspondences between src and ref, which we ac-
complished by appending the test dataset to training
and performing IBM Model 4 word alignments.
We then looked for phrase triples (src, out, ref ),
where there was a high character-level similarity be-
tween out and ref, measured using longest common
subsequence ratio with a threshold of 0.7, set ex-
perimentally. We extracted 16,262 triples: for 6,758
of them, the translations matched the references ex-
actly, while in the remaining triples, they were close
wordforms9. These numbers support the hypothesis
that our approach yields translations close to the ref-
erence wordforms but unjustly penalized by BLEU,
which only gives credit for exact word matches10.
Human Evaluation. We asked four native
Finnish speakers to evaluate 50 random test sen-
tences. Following (Callison-Burch et al, 2009), we
provided them with the source sentence, its refer-
ence translation, and the outputs of three SMT sys-
tems (m-system, w-system, and ourSystem), which
were shown in different order for each example and
were named sys1, sys2 and sys3 (by order of ap-
pearance). We asked for three pairwise judgments:
(i) sys1 vs. sys2, (ii) sys1 vs. sys3, and (iii) sys2 vs.
sys3. For each pair, a winner had to be designated;
ties were allowed. The results are shown in Table 10.
We can see that the judges consistently preferred
9Examples of such triples are (constitutional
structure, perustuslaillinen rakenne, perustuslaillisempi
rakenne) and (economic and social, taloudellisia ja
sosiaalisia, taloudellisten ja sosiaalisten)
10As a reference, the w-system yielded 15,673 triples, and
6,392 of them were exact matches. Compared to our system,
this means 589 triples and 366 exact matches less.
155
src: as a conservative , i am incredibly thrifty with taxpayers ? money .
ref: maltillisen kokoomuspuolueen edustajana suhtaudun erittain saastavaisesti veronmaksajien rahoihin .
our: konservatiivinen , olen erittain saastavaisesti veronmaksajien rahoja .
w : konservatiivinen , olen aarettoman tarkeaa kanssa veronmaksajien rahoja .
m : kuten konservatiivinen , olen erittain saastavaisesti veronmaksajien rahoja .
Comment: our  m  w. our uses better paraphrases, from which the correct meaning could be inferred. The part
?aarettoman tarkeaa kanssa? in w does not mention the ?thriftiness? and replaces it with ?important? (tarkeaa), which
is wrong. m introduces ?kuten?, which slightly alters the meaning towards ?like a conservative, ...?.
src: we were very constructive and we negotiated until the last minute of these talks in the hague .
ref: olimme erittain rakentavia ja neuvottelimme haagissa viime hetkeen saakka .
our: olemme olleet hyvin rakentavia ja olemme neuvotelleet viime hetkeen saakka naiden neuvottelujen haagissa .
w : olemme olleet hyvin rakentavia ja olemme neuvotelleet viime tippaan niin naiden neuvottelujen haagissa .
m : olimme erittain rakentavan ja neuvottelimme viime hetkeen saakka naiden neuvotteluiden haagissa .
Comment: our  m  w. In our, the meaning is very close to ref with only a minor difference in tense at the
beginning. m only gets the case wrong in ?rakentavan?, and the correct case is easily guessable. For w, the ?viime
tippaan? is in principle correct but somewhat colloquial, and the ?niin? is extra and somewhat confusing.
src: it would be a very dangerous situation if the europeans were to become logistically reliant on russia .
ref: olisi eritta?in vaarallinen tilanne , jos eurooppalaiset tulisivat logistisesti riippuvaisiksi vena?ja?sta? .
our: olisi eritta?in vaarallinen tilanne , jos eurooppalaiset tulee logistisesti riippuvaisia vena?ja?n .
w : se olisi eritta?in vaarallinen tilanne , jos eurooppalaisten tulisi logistically riippuvaisia vena?ja?n .
m : se olisi hyvin vaarallinen tilanne , jos eurooppalaiset haluavat tulla logistisesti riippuvaisia vena?ja?n .
Comment: our  w  m. our is almost correct except for the wrong inflections at the end. w is inferior since it
failed to translate ?logistically?. ?haluavat tulla? in m suggests that the Europeans would ?want to become logistically
dependent?, which is not the case. The ?se? (it), and ?hyvin? (a synonym of ?eritta?in?) are minor mistakes/differences.
Table 9: English-Finnish translation examples. Shown are the source (src), the reference (ref), and the transla-
tions of three systems (our, w, m). Text in bold indicates matches with respect to the ref, while italics show where a
system was judged inferior to the rest, as judged by native Finnish speakers.
(1) ourSystem to the m-system, (2) ourSystem to the
w-system, (3) w-system to the m-system. These pref-
erences are statistically significant, as found by the
sign test. Comparing to Table 8, we can see that
BLEU correlates with human judgments better than
m-BLEU; we plan to investigate this in future work.
our vs. m our vs. w w vs. m
Judge 1 25 18 19 12 21 19
Judge 2 24 16 19 15 25 14
Judge 3 27? 12 17 11 27? 15
Judge 4 25 20 26? 12 22 22
Total 101? 66 81? 50 95? 70
Table 10: Human judgments: ourSystem (our) vs. m-
system (m) vs. w-system (w). For each pair, we show
the number of times each system was judged better than
the other one, ignoring ties. Statistically significant dif-
ferences are marked with ? (p < 0.05) and ? (p < 0.01).
Finally, Table 9 shows some examples demon-
strating how our system improves over the w-system
and the m-system.
7 Conclusion and Future Work
In the quest towards a morphology-aware SMT that
only uses unannotated data, there are two key chal-
lenges: (1) to bring the performance of morpheme-
token systems to a level rivaling the standard word-
token ones, and (2) to incorporate morphological
analysis directly into the translation process.
This work satisfies the first challenge: we have
achieved statistically significant improvements in
BLEU for a large training dataset of 714K sentence
pairs and this was confirmed by human evaluation.
We think we have built a solid framework for the
second challenge, and we plan to extend it further.
Acknowledgements
We thank Joanna Bergstro?m-Lehtovirta (Helsinki
Institute for Information Technology), Katri Haveri-
nen (University of Turku and Turku Centre for Com-
puter Science), Veronika Laippala (University of
Turku), and Sampo Pyysalo (University of Tokyo)
for judging the Finnish translations.
156
References
Eleftherios Avramidis and Philipp Koehn. 2008. Enrich-
ing morphologically poor languages for statistical ma-
chine translation. In ACL-HLT.
Ibrahim Badr, Rabih Zbib, and James Glass. 2008. Seg-
mentation for English-to-Arabic statistical machine
translation. In ACL-HLT.
Tim Buckwalter. 2004. Buckwalter Arabic Morphologi-
cal Analyzer Version 2.0. Linguistic Data Consortium,
Philadelphia?.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
EACL.
Boxing Chen, Min Zhang, Haizhou Li, and Aiti Aw.
2009a. A comparative study of hypothesis alignment
and its improvement for machine translation system
combination. In ACL-IJCNLP.
Yu Chen, Michael Jellinghaus, Andreas Eisele, Yi Zhang,
Sabine Hunsicker, Silke Theison, Christian Feder-
mann, and Hans Uszkoreit. 2009b. Combining multi-
engine translations with Moses. In EACL.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In ACL.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Trans. Speech Lang. Process., 4(1):3.
Thi Ngoc Diep Do, Viet Bac Le, Brigitte Bigi, Laurent
Besacier, and Eric Castelli. 2009. Mining a compa-
rable text corpus for a Vietnamese-French statistical
machine translation system. In EACL.
Sharon Goldwater and David McClosky. 2005. Improv-
ing statistical MT through morphological analysis. In
HLT.
Philipp Koehn and Barry Haddow. 2009. Edinburgh?s
submission to all tracks of the WMT2009 shared task
with reordering and speed improvements to Moses. In
EACL.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In EMNLP-CoNLL.
Philipp Koehn and Christof Monz. 2005. Shared task:
Statistical machine translation between European lan-
guages. In WPT.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In ACL, Demonstration Session.
Philipp Koehn. 2003. Noun phrase translation. Ph.D.
thesis, University of Southern California, Los Angeles,
CA, USA.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In HLT-NAACL.
Preslav Nakov and Hwee Tou Ng. 2009. Improved statis-
tical machine translation for resource-poor languages
using related resource-rich languages. In EMNLP.
Jan Niehues, Teresa Herrmann, Muntsin Kolss, and Alex
Waibel. 2009. The Universita?t Karlsruhe translation
system for the EACL-WMT 2009. In EACL.
Attila Nova?k. 2009. MorphoLogic?s submission for the
WMT 2009 shared task. In EACL.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Kemal Oflazer and Ilknur El-Kahlout. 2007. Exploring
different representational units in English-to-Turkish
statistical machine translation. In StatMT.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. In ACL.
Fatiha Sadat and Nizar Habash. 2006. Combination of
Arabic preprocessing schemes for statistical machine
translation. In ACL.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on New Methods in Language Processing.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In ACL-HLT.
Sami Virpioja, Jaakko J. Vyrynen, Mathias Creutz, and
Markus Sadeniemi. 2007. Morphology-aware statisti-
cal machine translation based on morphs induced in an
unsupervised manner. In Machine Translation Summit
XI.
Hua Wu and Haifeng Wang. 2007. Pivot language
approach for phrase-based statistical machine transla-
tion. Machine Translation, 21(3):165?181.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly in-
flected languages. In EACL.
157
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 648?658,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Large-Scale Noun Compound Interpretation
Using Bootstrapping and the Web as a Corpus
Su Nam Kim
Computer Science & Software Engineering
University of Melbourne
Melbourne, VIC 3010
Australia
snkim@csse.unimelb.edu.au
Preslav Nakov
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
nakov@comp.nus.edu.sg
Abstract
Responding to the need for semantic lexical
resources in natural language processing ap-
plications, we examine methods to acquire
noun compounds (NCs), e.g., orange juice, to-
gether with suitable fine-grained semantic in-
terpretations, e.g., squeezed from, which are
directly usable as paraphrases. We employ
bootstrapping and web statistics, and utilize
the relationship between NCs and paraphras-
ing patterns to jointly extract NCs and such
patterns in multiple alternating iterations. In
evaluation, we found that having one com-
pound noun fixed yields both a higher number
of semantically interpreted NCs and improved
accuracy due to stronger semantic restrictions.
1 Introduction
Noun compounds (NCs) such as malaria mosquito
and colon cancer tumor suppressor protein are chal-
lenging for text processing since the relationship
between the nouns they are composed of is im-
plicit. NCs are abundant in English and understand-
ing their semantics is important in many natural lan-
guage processing (NLP) applications. For example,
a question answering system might need to know
whether protein acting as a tumor suppressor is a
good paraphrase for tumor suppressor protein. Sim-
ilarly, a machine translation system facing the un-
known noun compound Geneva headquarters might
translate it better if it could first paraphrase it as
Geneva headquarters of the WTO. Given a query
for ?migraine treatment?, an information retrieval
system could use paraphrasing verbs like relieve and
prevent for query expansion and result ranking.
Most work on noun compound interpretation has
focused on two-word NCs. There have been two
general lines of research: the first one derives the NC
semantics from the semantics of the nouns it is made
of (Rosario and Hearst, 2002; Moldovan et al, 2004;
Kim and Baldwin, 2005; Girju, 2007; Se?aghdha,
2009; Tratz and Hovy, 2010), while the second one
models the relationship between the nouns directly
(Vanderwende, 1994; Lapata, 2002; Kim and Bald-
win, 2006; Nakov and Hearst, 2006; Nakov and
Hearst, 2008; Butnariu and Veale, 2008).
In either case, the semantics of an NC is typi-
cally expressed by an abstract relation like CAUSE
(e.g., malaria mosquito), SOURCE (e.g., olive oil),
or PURPOSE (e.g., migraine drug), coming from a
small fixed inventory. Some researchers however,
have argued for a more fine-grained, even infinite,
inventory (Finin, 1980). Verbs are particularly use-
ful in this respect and can capture elements of the
semantics that the abstract relations cannot. For ex-
ample, while most NCs expressing MAKE, can be
paraphrased by common patterns like be made of
and be composed of, some NCs allow more specific
patterns, e.g., be squeezed from for orange juice, and
be topped with for bacon pizza.
Recently, the idea of using fine-grained para-
phrasing verbs for NC semantics has been gain-
ing popularity (Butnariu and Veale, 2008; Nakov,
2008b); there has also been a related shared task at
SemEval-2010 (Butnariu et al, 2010). This interest
is partly driven by practicality: verbs are directly us-
able as paraphrases. Still, abstract relations remain
dominant since they offer a more natural generaliza-
tion, which is useful for many NLP applications.
648
One good contribution to this debate would be a
direct study of the relationship between fine-grained
and coarse-grained relations for NC interpretation.
Unfortunately, the existing datasets do not allow
this since they are tied to one particular granular-
ity; moreover, they only contain a few hundred NCs.
Thus, our objective is to build a large-scale dataset
of hundreds of thousands of NCs, each interpreted
(1) by an abstract semantic relation and (2) by a set
of paraphrasing verbs. Having such a large dataset
would also help the overall advancement of the field.
Since there is no universally accepted abstract re-
lation inventory in NLP, and since we are interested
in NC semantics from both a theoretical and a prac-
tical viewpoint, we chose the set of abstract relations
proposed in the theory of Levi (1978), which is dom-
inant in theoretical linguistics and has been also used
in NLP (Nakov and Hearst, 2008).
We use a two-step algorithm to jointly harvest
NCs and patterns (verbs and prepositions) that in-
terpret them for a given abstract relation. First,
we extract NCs using a small number of seed pat-
terns from a given abstract relation. Then, using
the extracted NCs, we harvest more patterns. This
is repeated until no new NCs and patterns can be
extracted or for a pre-specified number of itera-
tions. Our approach combines pattern-based extrac-
tion and bootstrapping, which is novel for NC in-
terpretation; however, such combinations have been
used in other areas, e.g., named entity recognition
(Riloff and Jones, 1999; Thelen and Riloff, 2002;
Curran et al, 2007; McIntosh and Curran, 2009).
The remainder of the paper is organized as fol-
lows: Section 2 gives an overview of related work,
Section 3 motivates our semantic representation,
Sections 4, 5, and 6 explain our method, dataset and
experiments, respectively, Section 7 discusses the
results, Section 8 provides error analysis, and Sec-
tion 9 concludes with suggestions for future work.
2 Related Work
As we mentioned above, the implicit relation be-
tween the two nouns forming a noun compound can
often be expressed overtly using verbal and prepo-
sitional paraphrases. For example, student loan is
?loan given to a student?, while morning tea can be
paraphrased as ?tea in the morning?.
Thus, many NLP approaches to NC semantics
have used verbs and prepositions as a fine-grained
semantic representation or as features when pre-
dicting coarse-grained abstract relations. For ex-
ample, Vanderwende (1994) associated verbs ex-
tracted from definitions in an online dictionary with
abstract relations. Lauer (1995) expressed NC se-
mantics using eight prepositions. Kim and Baldwin
(2006) predicted abstract relations using verbs as
features. Nakov and Hearst (2008) proposed a fine-
grained NC interpretation using a distribution over
Web-derived verbs, prepositions and coordinating
conjunctions; they also used this distribution to pre-
dict coarse-grained abstract relations. Butnariu and
Veale (2008) adopted a similar fine-grained verb-
centered approach to NC semantics. Using a dis-
tribution over verbs as a semantic interpretation was
also carried out in a recent challenge: SemEval-2010
Task 9 (Butnariu et al, 2009; Butnariu et al, 2010).
In noun compound interpretation, verbs and
prepositions can be seen as patterns connecting the
two nouns in a paraphrase. Similar pattern-based ap-
proaches have been popular in information extrac-
tion and ontology learning. For example, Hearst
(1992) extracted hyponyms using patterns such as
X, Y, and/or other Zs, where Z is a hypernym of
X and Y. Berland and Charniak (1999) used sim-
ilar patterns to extract meronymy (part-whole) re-
lations, e.g., parts/NNS of/IN wholes/NNS matches
basements of buildings. Unfortunately, matches are
rare, which makes it difficult to build large semantic
inventories. In order to overcome data sparseness,
pattern-based approaches are often combined with
bootstrapping. For example, Riloff and Jones (1999)
used a multi-level bootstrapping algorithm to learn
both a semantic lexicon and extraction patterns, e.g.,
owned by X extracts COMPANY and facilities in X
extracts LOCATION. That is, they learned seman-
tic lexicons using extraction patterns, and then, al-
ternatively, they extracted new patterns using these
lexicons. They also introduced a second level of
bootstrapping to retain the most reliable examples
only. While the method enables the extraction of
large lexicons, its quality degrades rapidly, which
makes it impossible to run for too many iterations.
Recently, Curran et al (2007) and McIntosh and
Curran (2009) proposed ways to control degradation
using simultaneous learning and weighting.
649
Bootstrapping has been applied to noun com-
pound extraction as well. For example, Kim and
Baldwin (2007) used it to produce a large number
of semantically interpreted noun compounds from
a small number of seeds. In each iteration, the
method replaced one component of an NC with its
synonyms, hypernyms and hyponyms to generate a
new NC. These new NCs were further filtered based
on their semantic similarity with the original NC.
While the method acquired a large number of noun
compounds without significant semantic drifting, its
accuracy degraded rapidly after each iteration. More
importantly, the variation of the sense pairs was lim-
ited since new NCs had to be semantically similar to
the original NCs.
Recently, Kozareva and Hovy (2010) combined
patterns and bootstrapping to learn the selectional
restrictions for various semantic relations. They
used patterns involving the coordinating conjunction
and, e.g., ?* and John fly to *?, and learned argu-
ments such as Mary/Tom and France/New York. Un-
like in NC interpretation, it is not necessary for their
arguments to form an NC, e.g., Mary France and
France Mary are not NCs. Rather, they were in-
terested in building a semantic ontology with a pre-
defined set of semantic relations, similar to YAGO
(Suchanek et al, 2007), where the pattern work for
would have arguments like a company/UNICEF.
3 Semantic Representation
Inspired by (Finin, 1980), Nakov and Hearst (2006)
and (Nakov, 2008b) proposed that NC semantics is
best expressible using paraphrases involving verbs
and/or prepositions. For example, bronze statue is
a statue that is made of, is composed of, consists of,
contains, is of, is, is handcrafted from, is dipped in,
looks like bronze. They further proposed that se-
lecting one such paraphrase is not enough and that
multiple paraphrases are needed for a fine-grained
representation. Finally, they observed that not all
paraphrases are equally good (e.g., is made of is
arguably better than looks like or is dipped in for
MAKE), and thus proposed that the semantics of a
noun compound should be expressed as a distribu-
tion over multiple possible paraphrases. This line of
research was later adopted by SemEval-2010 Task 9
(Butnariu et al, 2010).
It easily follows that the semantics of abstract re-
lations such as MAKE that can hold between the
nouns in an NC can be represented in the same way:
as a distribution over paraphrasing verbs and prepo-
sitions. Note, however, that some NCs are para-
phrasable by more specific verbs that do not nec-
essarily support the target abstract relation. For ex-
ample, malaria mosquito, which expresses CAUSE,
can be paraphrased using verbs like carry, which do
not imply direct causation. Thus, while we will be
focusing on extracting NCs for a particular abstract
relation, we are interested in building semantic rep-
resentations that are specific for these NCs and do
not necessarily apply to all instances of that relation.
Traditionally, the semantics of a noun compound
have been represented as an abstract relation drawn
from a small closed set. Unfortunately, no such set is
universally accepted, and mapping between sets has
proven challenging (Girju et al, 2005). Moreover,
being both abstract and limited, such sets capture
only part of the semantics; often multiple meanings
are possible, and sometimes none of the pre-defined
ones suits a given example. Finally, it is unclear
how useful these sets are since researchers have of-
ten fallen short of demonstrating practical uses.
Arguably, verbs have more expressive power and
are more suitable for semantic representation: there
is an infinite number of them (Downing, 1977), and
they can capture fine-grained aspects of the mean-
ing. For example, while both wrinkle treatment and
migraine treatment express the same abstract rela-
tion TREATMENT-FOR-DISEASE, fine-grained dif-
ferences can be revealed using verbs, e.g., smooth
can paraphrase the former, but not the latter.
In many theories, verbs play an important role in
NC derivation (Levi, 1978). Moreover, speakers of-
ten use verbs to make the hidden relation between
the noun in a noun compound overt. This allows for
simple extraction and for straightforward use in NLP
tasks like textual entailment (Tatu and Moldovan,
2005) and machine translation (Nakov, 2008a).
Finally, a single verb is often not enough, and
the meaning is better approximated by a collection
of verbs. For example, while malaria mosquito ex-
presses CAUSE (and is paraphrasable using cause),
further aspects of the meaning can be captured with
more verbs, e.g., carry, spread, be responsible for,
be infected with, transmit, pass on, etc.
650
4 Method
We harvest noun compounds expressing some target
abstract semantic relation (in the experiments below,
this is Levi?s MAKE2), starting from a small number
of initial seed patterns: paraphrasing verbs and/or
prepositions. Optionally, we might also be given
a small number of noun compounds that instanti-
ate the target abstract relation. We then learn more
noun compounds and patterns for the relation by al-
ternating between the following two bootstrapping
steps, using the Web as a corpus. First, we extract
more noun compounds that are paraphrasable with
the available patterns (see Section 4.1). We then
look for new patterns that can paraphrase the newly-
extracted noun compounds (see Section 4.2). These
two steps are repeated until no new noun compounds
can be extracted or until a pre-determined number of
iterations has been reached. A schematic description
of the algorithm is shown in Figure 1.
(+ H/M of NCs)Patterns
Query Generation
NC Extraction
Pattern
Filtering
Rules
Filtering
NC
Rules
repeat
collected NCs^
Query Generation
w/ NCs^
collected Patterns
stop
if newNCs = 0
or
Iteration limit exceeded
Snippet by Yahoo!
Pattern Extraction
Snippet by Yahoo!
Figure 1: Our bootstrapping algorithm.
4.1 Bootstrapping Step 1: Noun Compound
Extraction
Given a list of patterns (verbs and/or prepositions),
we mine the Web to extract noun compounds that
match these patterns. We experiment with the fol-
lowing three bootstrapping strategies for this step:
? Loose bootstrapping uses the available pat-
terns and imposes no further restrictions.
? Strict bootstrapping requires that, in addition
to the patterns themselves, some noun com-
pounds matching each pattern be made avail-
able as well. A pattern is only instantiated in
the context of either the head or the modifier of
a noun compound that is known to match it.
? NC-only strict bootstrapping is a stricter ver-
sion of strict bootstrapping, where the list of
patterns is limited to the initial seeds.
Below we describe each of the sub-steps of the NC
extraction process: query generation, snippet har-
vesting, and noun compound acquisition & filtering.
4.1.1 Query Generation
We generate generalized exact-phrase queries to
be used in a Web search engine (we use Yahoo!):
"* that PATTERN *" (loose)
"HEAD that PATTERN *" (strict)
"* that PATTERN MOD" (strict)
where PATTERN is an inflected form of a verb, MOD
and HEAD are inflected forms the modifier and the
head of a noun compound that is paraphrasable by
the pattern, that is the word that, and * is the
search engine?s star operator.
We use the first pattern for loose bootstrapping
and the other two for both strict bootstrapping and
NC-only strict bootstrapping.
Note that the above queries are generalizations of
the actual queries we use against the search engine.
In order to instantiate these generalizations, we fur-
ther generate the possible inflections for the verbs
and the nouns involved. For nouns, we produce sin-
gular and plural forms, while for verbs, we vary not
only the number (singular and plural), but also the
tense (we allow present, past, and present perfect).
When inflecting verbs, we distinguish between ac-
tive verb forms like consist of and passive ones like
be made from and we treat them accordingly. Over-
all, in the case of loose bootstrapping, we generate
about 14 and 20 queries per pattern for active and
passive patterns, respectively, while for strict boot-
strapping and NC-only strict bootstrapping, the in-
stantiations yield about 28 and 40 queries for active
and passive patterns, respectively.
651
For example, given the seed be made of, we could
generate "* that were made of *". If we
are further given the NC orange juice, we could also
produce "juice that was made of *" and
"* that is made of oranges".
4.1.2 Snippet Extraction
We execute the above-described instantiations of
the generalized queries against a search engine as
exact phrase queries, and, for each one, we collect
the snippets for the top 1,000 returned results.
4.1.3 NC Extraction and Filtering
Next, we process the snippets returned by the
search engine and we acquire potential noun com-
pounds from them. Then, in each snippet, we look
for an instantiation of the pattern used in the query
and we try to extract suitable noun(s) that occupy the
position(s) of the *.
For loose bootstrapping, we extract two nouns,
one from each end of the matched pattern, while
for strict bootstrapping and for NC-only strict boot-
strapping, we only extract one noun, either preced-
ing or following the pattern, since the other noun
is already fixed. We then lemmatize the extracted
noun(s) and we form NC candidates from the two
arguments of the instantiated pattern, taking into ac-
count whether the pattern is active or passive.
Due to the vast number of snippets we have to
process, we decided not to use a syntactic parser or a
part-of-speech (POS) tagger1; thus, we use heuristic
rules instead. We extract ?phrases? using simple in-
dicators such as punctuation (e.g., comma, period),
coordinating conjunctions2 (e.g., and, or), preposi-
tions (e.g., at, of, from), subordinating conjunctions
(e.g., because, since, although), and relative pro-
nouns (e.g., that, which, who). We then extract the
nouns from these phrases, we lemmatize them using
WordNet, and we form a list of NC candidates.
While the above heuristics work reasonably well
in practice, we perform some further filtering, re-
moving all NC candidates for which one or more of
the following conditions are met:
1In fact, POS taggers and parsers are unreliable for Web-
derived snippets, which often represent parts of sentences and
contain errors in spelling, capitalization and punctuation.
2Note that filtering the arguments using such indicators indi-
rectly subsumes the pattern "X PATTERN Y and" proposed
in (Kozareva and Hovy, 2010).
1. the candidate NC is one of the seed examples
or has been extracted on a previous iteration;
2. the head and the modifier are the same;
3. the head or the modifier are not both listed as
nouns in WordNet (Fellbaum, 1998);
4. the candidate NC occurs less than 100 times in
the Google Web 1T 5-gram corpus3;
5. the NC is extracted less than N times (we tried
5 and 10) in the context of the pattern for all
instantiations of the pattern.
4.2 Bootstrapping Step 2: Pattern Extraction
This is the second step of our bootstrapping algo-
rithm as shown on Figure 1. Given a list of noun
compounds, we mine the Web to extract patterns:
verbs and/or prepositions that can paraphrase each
NC. The idea is to turn the NC?s pre-modifier into
a post-modifying relative clause and to collect the
verbs and prepositions that are used in such clauses.
Below we describe each of the sub-steps of the NC
extraction process: query generation, snippet har-
vesting, and NC extraction & filtering.
4.2.1 Query Generation
The process of extraction starts with exact-phrase
queries issued against a Web search engine (again
Yahoo!) using the following generalized pattern:
"HEAD THAT? * MOD"
where MOD and HEAD are inflected forms of NC?s
modifier and head, respectively, THAT? stands for
that, which, who or the empty string, and * stands
for 1-6 instances of search engine?s star operator.
For example, given orange juice, we could gen-
erate queries like "juice that * oranges",
"juices which * * * * * * oranges",
and "juices * * * orange".
4.2.2 Snippet Extraction
The same as in Section 4.1.2 above.
3http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2006T13
652
4.2.3 Pattern Extraction and Filtering
We split the extracted snippets into sentences, and
filter out all incomplete ones and those that do not
contain (a possibly inflected version of) the target
nouns. We further make sure that the word sequence
following the second mentioned target noun is non-
empty and contains at least one non-noun, thus en-
suring the snippet includes the entire noun phrase.
We then perform shallow parsing, and we extract all
verb forms, and the following preposition, between
the target nouns. We allow for adjectives and partici-
ples to fall between the verb and the preposition but
not nouns; we further ignore modal verbs and aux-
iliaries, but we retain the passive be, and we make
sure there is exactly one verb phrase between the tar-
get nouns. Finally, we lemmatize the verbs to form
the patterns candidates, and we apply the following
pattern selection rules:
1. we filter out all patterns that were provided as
initial seeds or were extracted previously;
2. we select the top 20 most frequent patterns;
3. we filter out all patterns that were extracted less
than N times (we tried 5 and 10) and with less
thanM NCs per pattern (we tried 20 and 50).
5 Target Relation and Seed Examples
As we mentioned above, we use the inventory of
abstract relations proposed in the popular theoreti-
cal linguistics theory of Levi (1978). In this theory,
noun compounds are derived from underlying rel-
ative clauses or noun phrase complement construc-
tions by means of two general processes: predicate
deletion and predicate nominalization. Given a two-
argument predicate, predicate deletion removes that
predicate, but retains its arguments to form an NC,
e.g., pie made of apples ? apple pie. In contrast,
predicate nominalization creates an NC whose head
is a nominalization of the underlying predicate and
whose modifier is either the subject or the object of
that predicate, e.g., The President refused General
MacArthur?s request. ? presidential refusal.
According to Levi, predicate deletion can be ap-
plied to abstract predicates, whose semantics can be
roughly approximated using five paraphrasing verbs
(CAUSE, HAVE, MAKE, USE, and BE) and four
prepositions (IN, FOR, FROM, and ABOUT).
Typically, in predicate deletion, the modifier is
derived from the object of the underlying relative
clause; however, the first three verbs also allow for
it to be derived from the subject. Levi expresses the
distinction using indexes. For example, music box is
MAKE1 (object-derived), i.e., the box makes music,
while chocolate bar is MAKE2 (subject-derived),
i.e., the bar is made of chocolate (note the passive).
Due to time constraints, we focused on one re-
lation of Levi?s, MAKE2, which is among the most
frequent relations an NC can express and is present
in some form in many relation inventories (Warren,
1978; Barker and Szpakowicz, 1998; Rosario and
Hearst, 2001; Nastase and Szpakowicz, 2003; Girju
et al, 2005; Girju et al, 2007; Girju et al, 2009;
Hendrickx et al, 2010; Tratz and Hovy, 2010).
In Levi?s theory, MAKE2 means that the head of
the noun compound is made up of or is a product of
its modifier. There are three subtypes of this relation
(we do not attempt to distinguish between them):
(a) the modifier is a unit and the head is a configu-
ration, e.g., root system;
(b) the modifier represents a material and the head
is a mass or an artefact, e.g., chocolate bar;
(c) the head represents human collectives and
the modifier specifies their membership, e.g.,
worker teams.
There are 20 instances of MAKE2 in the appendix
of (Levi, 1978), and we use them all as seed NCs.
As seed patterns, we use a subset of the human-
proposed paraphrasing verbs and prepositions cor-
responding to these 20 NCs in the dataset in (Nakov,
2008b), where each NC is paraphrased by 25-30 an-
notators. For example, for chocolate bar, we find
the following list of verbs (the number of annotators
who proposed each verb is shown in parentheses):
be made of (16), contain (16), be made from
(10), be composed of (7), taste like (7), con-
sist of (5), be (3), have (2), melt into (2), be
manufactured from (2), be formed from (2),
smell of (2), be flavored with (1), sell (1), taste
of (1), be constituted by (1), incorporate (1),
serve (1), contain (1), store (1), be made with
(1), be solidified from (1), be created from (1),
be flavoured with (1), be comprised of (1).
653
Seed NCs: bronze statue, cable network, candy cigarette, chocolate bar, concrete desert, copper coin, daisy chain, glass eye,
immigrant minority, mountain range, paper money, plastic toy, sand dune, steel helmet, stone tool, student committee,
sugar cube, warrior castle, water drop, worker team
Seed patterns: be composed of, be comprised of, be inhabited by, be lived in by, be made from, be made of, be made up of,
be manufactured from, be printed on, consist of, contain, have, house, include, involve, look like, resemble, taste like
Table 1: Our seed examples: 20 noun compounds and 18 verb patterns.
As we can see, the most frequent patterns are of
highest quality, e.g., be made of (16), while the less
frequent ones can be wrong, e.g., serve (1). There-
fore, we filtered out all verbs that were proposed less
than five times with the 20 seed NCs. We further re-
moved the verb be, which is too general, thus ending
up with 18 seed patterns. Note that some patterns
can paraphrase multiple NCs: the total number of
seed NC-pattern pairs is 84.
The seed NCs and patterns are shown in Table 1.
While some patterns, e.g., taste like do not express
the target relation MAKE2, we kept them anyway
since they were proposed by several human anno-
tators and since they do express the fine-grained se-
mantics of some particular instances of that relation;
thus, we thought they might be useful, even for the
general relation. For example, taste like has been
proposed 8 times for candy cigarette, 7 times for
chocolate bar, and 2 times for sugar cube, and thus
it clearly correlates well with some seed examples,
even if it does not express MAKE2 in general.
6 Experiments and Evaluation
Using the NCs and patterns in Table 1 as initial
seeds, we ran our algorithm for three iterations of
loose bootstrapping and strict bootstrapping, and
for two iterations of NC-only strict bootstrapping.
We only performed up to three iterations because
of the huge number of noun compounds extracted
for NC-only strict bootstrapping (which we only ran
for two iterations) and because of the low number of
new NCs extracted by loose bootstrapping on itera-
tion 3. While we could have run strict bootstrapping
for more iterations, we opted for a comparable num-
ber of iterations for all three methods.
Examples of noun compounds that we have ex-
tracted are bronze bell (be made of, be made from)
and child team (be composed of, include). Exam-
ple patterns are be filled with (cotton bag, water cup)
and use (water sculpture, wood statue).
Limits Extracted & Retained
(see 4.2.3) NCs Patterns Patt.+NC
Loose Bootstrapping
N=5,M=50 1,662 / 61.67 12 / 65.83 1,337
N=10,M=20 590 / 61.52 9 / 65.56 316
Strict Bootstrapping
N=5,M=50 25,375 / 67.42 16 / 71.43 9,760
N=10,M=20 16,090 / 68.27 16 / 78.98 5,026
NC-only Strict Bootstrapping
N=5 205,459 / 69.59 ? ?
N=10 100,550 / 70.43 ? ?
Table 2: Total number and accuracy in % for NCs, pat-
terns and NC-pattern pairs extracted and retained for each
of the three methods over all iterations.
Tables 2 and 3 show the overall results. As we
mentioned in section 4.2.3, at each iteration, we fil-
tered out all patterns that were extracted less thanN
times or with less than M NCs. Note that we only
used the 10 most frequent NCs per pattern as NC
seeds for NC extraction in the next iteration of strict
bootstrapping and NC-only strict bootstrapping. Ta-
ble 3 shows the results for two value combinations
of (N ;M ): (5;50) and (10;20). Note also that if
some NC was extracted by several different patterns,
it was only counted once. Patterns are subject to
particular NCs, and thus we show (1) the number
of patterns extracted with all NCs, i.e., unique NC-
pattern pairs, (2) the accuracy of these pairs,4 and
(3) the number of unique patterns retained after fil-
tering, which will be used to extract new noun com-
pounds on the second step of the current iteration.
4One of the reviewers suggested that evaluating the accuracy
of NC-pattern pairs could potentially conceal some of the drift
of our algorithm. For example, while water cup / be filled with
is a correct NC-pattern pair, water cup is incorrect for MAKE2;
it is probably an instance of Levi?s FOR. Thus, the same boot-
strapping technique evaluated against a fixed set of semantic re-
lations (which is the more traditional approach) could arguably
show bootstrapping going ?off the rails? more quickly than what
we observe here. However, our goal, as stated in Section 3, is to
find NC-specific paraphrases, and our evaluation methodology
is more adequate with respect to this goal.
654
Limits Seeds Iteration 1 Iteration 2 Iteration 3
(see 4.2.3) Patt. NCs Patt. NCs Patterns NCs Patterns NCs
Loose Bootstrapping
N=5,M=50 ? 18 ? 1,144 / 63.11 1,136 / 64.44 / 9 390 / 58.72 201 / 70.00 / 3 128 / 57.03
N=10,M=20 ? 18 ? 502 / 61.55 294 / 62.50 / 8 78 / 60.26 22 / 90.00 / 1 10 / 70.00
Strict Bootstrapping
N=5,M=50 20 18 ? 7,011 / 70.65 5,312 / 74.00 / 10 11,214 / 67.15 4,448 / 60.00 / 6 7,150 / 64.69
N=10,M=20 20 18 ? 4,826 / 71.26 2,838 / 79.38 / 10 7,371 / 67.26 2,188 / 78.33 / 6 3,893 / 66.48
NC-only Strict Bootstrapping
N=5 20 18 ? 7,011 / 70.65 ? 198,448 / 69.55 ? ?
N=10 20 18 ? 4,826 / 71.26 ? 95,524 / 70.59 ? ?
Table 3: Evaluation results for up to three iterations. For NCs, we show the number of unique NCs extracted and
their accuracy in %. For patterns, we show the number of unique NC-pattern pairs extracted, their accuracy in %, and
the number of unique patterns retained and used to extract NCs on the second step of the current iteration. The first
column shows the pattern filtering thresholds used (see Section 4.2.3 for details).
The above accuracies were calculated based on
human judgments by an experienced, well-trained
annotator. We also hired a second annotator for a
small subset of the examples.
For NCs, the first annotator judged whether each
NC is an instance of MAKE2. All NCs were judged,
except for iteration 2 of NC-only strict bootstrap-
ping, where their number was prohibitively high and
only the most frequent noun compounds extracted
for each modifier and for each head were checked:
9,004 NCs for N=5 and 4,262 NCs for N=10.
For patterns, our first annotator judged the cor-
rectness of the unique NC-pattern pairs, i.e., whether
the NC is paraphrasable with the target pattern.
Given the large number of NC-pattern pairs, the an-
notator only judged patterns with their top 10 most
frequent NCs. For example, if there were 5 patterns
extracted, then the NC-pattern pairs to be judged
would be no more than 5 ? 10 = 50.
Our second annotator judged 340 random exam-
ples: 100 NCs and 20 patterns with their top 10 NCs
for each iteration. The Cohen?s kappa (Cohen, 1960)
between the two annotators is .66 (85% initial agree-
ment), which corresponds to substantial agreement
(Landis and Koch, 1977).
7 Discussion
Tables 2 and 3 show that fixing one of the two nouns
in the pattern, as in strict bootstrapping and NC-only
strict bootstrapping, yields significantly higher ac-
curacy (?2 test) for both NC and NC-pattern pair
extraction compared to loose bootstrapping.
The accuracy for NC-only strict bootstrapping is
a bit higher than for strict bootstrapping, but the ac-
tual differences are probably smaller since the eval-
uation of the former on iteration 2 was done for the
most frequent NCs, which are more accurate.
Note that the number of extracted NCs is much
higher with the strict methods because of the higher
number of possible instantiations of the generalized
query patterns. For NC-only strict bootstrapping,
the number of extracted NCs grows exponentially
since the number of patterns does not diminish as
in the other two methods. The number of extracted
patterns is similar for the different methods since we
select no more than 20 of them per iteration.
Overall, the accuracy for all methods decreases
from one iteration to the next since errors accumu-
late; still, the degradation is slow. Note also the ex-
ception of loose bootstrapping on iteration 3.
Comparing the results for N=5 and N=10, we
can see that, for all three methods, using the latter
yields a sizable drop in the number of extracted NCs
and NC-pattern pairs; it also tends to yield a slightly
improved accuracy. Note, however, the exception
of loose bootstrapping for the first two iterations,
where the less restrictive N=5 is more accurate.
As a comparison, we implemented the method
of Kim and Baldwin (2007), which generates new
semantically interpreted NCs by replacing either
the head or the modifier of a seed NC with suit-
able synonyms, hypernyms and sister words from
WordNet, followed by similarity filtering using
WordNet::Similarity (Pedersen et al, 2004).
655
Rep. Iter. 1 Iter. 2 Iter. 3 All
Syn. 11/81.81 3/66.67 0 14/78.57
Hyp. 27/85.19 35/77.14 33/66.67 95/75.79
Sis. 381/82.05 1,736/69.33 17/52.94 2,134/75.12
All 419/82.58 1,774/71.68 50/62.00 2,243/75.47
Table 4: Number of extracted noun compounds and ac-
curacy in % for the method of Kim and Baldwin (2007).
The abbreviations Syn., Hyp., and Sis. indicate using syn-
onyms, hypernyms, and sister words, respectively.
The results for three bootstrapping iterations us-
ing the same list of 20 initial seed NCs as in our pre-
vious experiments, are shown in Table 4. We can see
that the overall accuracy of their method is slightly
better than ours. Note, however, that our method ac-
quired a much larger number of NCs, while allow-
ing more variety in the NC semantics. Moreover, for
each extracted noun compound, we also generated a
list of fine-grained paraphrasing verbs.
8 Error Analysis
Below we analyze the errors of our method.
Many problems were due to wrong POS assign-
ment. For example, on Step 2, because of the omis-
sion of that in ?the statue has such high quality gold
(that) demand is ...?, demand was tagged as a noun
and thus extracted as an NCmodifier instead of gold.
The problem also arose on Step 1, where we used
WordNet to check whether the NC candidates were
composed of two nouns. Since words like clear,
friendly, and single are listed in WordNet as nouns
(which is possible in some contexts), we extracted
wrong NCs such as clear cube, friendly team, and
single chain. There were similar issues with verb-
particle constructions since some particles can be
used as nouns as well, e.g., give back, break down.
Some errors were due to semantic transparency
issues, where the syntactic and the semantic head of
a target NP were mismatched (Fillmore et al, 2002;
Fontenelle, 1999). For example, from the sentence
?This wine is made from a range of white grapes.?,
we would extract range rather than grapes as the po-
tential modifier of wine.
In some cases, the NC-pattern pair was correct,
but the NC did not express the target relation, e.g.,
while contain is a good paraphrase for toy box, the
noun compound itself is not an instance of MAKE2.
There were also cases where the pair of extracted
nouns did not make a good NC, e.g., worker work or
year toy. Note that this is despite our checking that
the candidate NC occurred at least 100 times in the
Google Web 1T 5-gram corpus (see Section 4.1.3).
We hypothesized that such bad NCs would tend to
have a low collocation strength. We tested this hy-
pothesis using the Dice coefficient, calculated using
the Google Web 1T 5-gram corpus. Figure 2 shows a
plot of the NC accuracy vs. collocation strength for
strict bootstrapping with N=5, M=50 for all three
iterations (the results for the other experiments show
a similar trend). We can see that the accuracy im-
proves slightly as the collocation strength increases:
compare the left and the right ends of the graph (the
results are mixed in the middle though).
?Acc.i1?
?Acc.i2?
?Acc.i3?
 40
 50
 60
 70
 80
 90
 100
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Figure 2: NC accuracy vs. collocation strength.
9 Conclusion and Future Work
We have presented a framework for building a very
large dataset of noun compounds expressing a given
target abstract semantic relation. For each extracted
noun compound, we generated a corresponding fine-
grained semantic interpretation: a frequency distri-
bution over suitable paraphrasing verbs.
In future work, we plan to apply our frame-
work to the remaining relations in the inventory of
Levi (1978), and to release the resulting dataset to
the research community. We believe that having a
large-scale dataset of noun compounds interpreted
with both fine- and coarse-grained semantic rela-
tions would be an important contribution to the de-
bate about which representation is preferable for dif-
ferent tasks. It should also help the overall advance-
ment of the field of noun compound interpretation.
656
Acknowledgments
This research is partially supported (for the sec-
ond author) by the SmartBook project, funded by
the Bulgarian National Science Fund under Grant
D002-111/15.12.2008.
We would like to thank the anonymous reviewers
for their detailed and constructive comments, which
have helped us improve the paper.
References
Ken Barker and Stan Szpakowicz. 1998. Semi-automatic
recognition of noun modifier relationships. In Pro-
ceedings of the 17th International Conference on
Computational Linguistics, pages 96?102.
Matthew Berland and Eugene Charniak. 1999. Finding
parts in very large corpora. In Proceedings of the 37th
Annual Meeting of the Association for Computational
Linguistics, ACL ?99, pages 57?64.
Cristina Butnariu and Tony Veale. 2008. A concept-
centered approach to noun-compound interpretation.
In Proceedings of the 22nd International Conference
on Computational Linguistics, COLING ?08, pages
81?88.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-
muid O? Se?aghdha, Stan Szpakowicz, and Tony Veale.
2009. Semeval-2010 task 9: The interpretation of
noun compounds using paraphrasing verbs and prepo-
sitions. In Proceedings of the Workshop on Semantic
Evaluations: Recent Achievements and Future Direc-
tions, SEW ?09, pages 100?105.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-
muid O? Se?aghdha, Stan Szpakowicz, and Tony Veale.
2010. SemEval-2010 task 9: The interpretation of
noun compounds using paraphrasing verbs and prepo-
sitions. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, SemEval-2, pages 39?
44.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 1(20):37?46.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with mutual exclu-
sion bootstrapping. In Proceedings of the Conference
of the Pacific Association for Computational Linguis-
tics, PACLING ?07, pages 172?180.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53:810?842.
Christiane Fellbaum, editor. 1998. WordNet, An Elec-
tronic Lexical Database. MIT Press, Cambridge, Mas-
sachusetts, USA.
Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato.
2002. Seeing arguments through transparent struc-
tures. In Proceedings of the Third International Con-
ference on Language Resources and Evaluation, vol-
ume III of LREC ?02, pages 787?791.
Timothy Wilking Finin. 1980. The semantic interpre-
tation of compound nominals. Ph.D. thesis, Univer-
sity of Illinois at Urbana-Champaign, Champaign, IL,
USA. AAI8026491.
Thierry Fontenelle. 1999. Semantic resources for word
sense disambiguation: a sine qua non. Linguistica e
Filologia, 9:25?41.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the semantics of noun compounds.
Computer Speech and Language, 19(44):479?496.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic rela-
tions between nominals. In Proceedings of the 4th Se-
mantic Evaluation Workshop, SemEval-1, pages 13?
18.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2009.
Classification of semantic relations between nominals.
Language Resources and Evaluation, 43(2):105?121.
Roxana Girju. 2007. Improving the interpretation of
noun phrases with cross-linguistic information. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, ACL ?07, pages
568?575.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the Fourteenth International Conference on Computa-
tional Linguistics, COLING ?92, pages 539?545.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid O? Se?aghdha, Sebastian Pado?, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakow-
icz. 2010. SemEval-2010 task 8: Multi-way classifi-
cation of semantic relations between pairs of nominals.
In Proceedings of the 5th International Workshop on
Semantic Evaluation, SemEval-2, pages 33?38.
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of compound nouns using WordNet sim-
ilarity. In Proceedings of 2nd International Joint Con-
ference on Natural Language Processing, IJCNLP ?05,
pages 945?956.
Su Nam Kim and Timothy Baldwin. 2006. Interpret-
ing semantic relations in noun compounds via verb se-
mantics. In Proceedings of the 44th Annual Meeting
of the Association for Computational Linguistics and
21st International Conference on Computational Lin-
guistics, ACL-COLING ?06, pages 491?498.
657
Su Nam Kim and Timothy Baldwin. 2007. Interpreting
noun compounds using bootstrapping and sense col-
location. In Proceedings of Conference of the Pacific
Association for Computational Linguistics, PACLING
?07, pages 129?136.
Zornitsa Kozareva and Eduard Hovy. 2010. Learning
arguments and supertypes of semantic relations using
recursive patterns. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, ACL ?10, pages 1482?1491.
Richard J. Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159?174.
Maria Lapata. 2002. The disambiguation of nominaliza-
tions. Computational Linguistics, 28(3):357?388.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds. Ph.D.
thesis, Dept. of Computing, Macquarie University,
Australia.
Judith Levi. 1978. The Syntax and Semantics of Complex
Nominals. Academic Press, New York, USA.
Tara McIntosh and James Curran. 2009. Reducing se-
mantic drift with bagging and distributional similar-
ity. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, ACL-IJCNLP ?09, pages 396?404.
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the se-
mantic classification of noun phrases. In Proceedings
of the HLT-NAACL?04 Workshop on Computational
Lexical Semantics, pages 60?67.
Preslav Nakov and Marti A. Hearst. 2006. Using verbs
to characterize noun-noun relations. In Proceedings
of the 12th International Conference on Artificial In-
telligence: Methodology, Systems, and Applications,
AIMSA ?06, pages 233?244.
Preslav Nakov and Marti Hearst. 2008. Solving rela-
tional similarity problems using the web as a corpus.
In Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics, ACL ?08, pages
452?460.
Preslav Nakov. 2008a. Improved Statistical Machine
Translation Using Monolingual Paraphrases. In Pro-
ceedings of the 18th European Conference on Artificial
Intelligence, ECAI ?08, pages 338?342.
Preslav Nakov. 2008b. Noun compound interpretation
using paraphrasing verbs: Feasibility study. In Pro-
ceedings of the 13th international conference on Arti-
ficial Intelligence: Methodology, Systems, and Appli-
cations, AIMSA ?08, pages 103?117.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings of
the 5th International Workshop on Computational Se-
mantics, pages 285?301.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity - measuring the relat-
edness of concepts. In Proceedings of the Nineteenth
National Conference on Artificial Intelligence, AAAI
?04, pages 1024?1025.
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level boot-
strapping. In Proceedings of the Sixteenth National
Conference on Artificial Intelligence, AAAI ?99, pages
474?479.
Barbara Rosario and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via a
domain-specific lexical hierarchy. In Proceedings of
the 6th Conference on Empirical Methods in Natural
Language Processing, EMNLP ?01, pages 82?90.
Barbara Rosario and Marti Hearst. 2002. The descent
of hierarchy, and selection in relational semantics. In
Proceedings of Annual Meeting of the Association for
Computational Linguistics, ACL ?02, pages 247?254.
Diarmuid O? Se?aghdha. 2009. Semantic classification
with WordNet kernels. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, Companion Volume: Short
Papers, NAACL ?09, pages 237?240.
Fabian Suchanek, Gjergji Kasneci, and Gerhard Weikum.
2007. YAGO: A core of semantic knowledge - unify-
ing WordNet and Wikipedia. In Proceedings of 16th
International World Wide Web Conference, WWW
?07, pages 697?706.
Marta Tatu and Dan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. In Proceed-
ings of Human Language Technology Conference and
Conference on Empirical Methods in Natural Lan-
guage Processing, HLT-EMNLP ?05, pages 371?378.
Michael Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extraction
pattern contexts. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?02, pages 214?221.
Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, ACL ?10, pages 678?687.
Lucy Vanderwende. 1994. Algorithm for automatic in-
terpretation of noun sequences. In Proceedings of the
15th Conference on Computational linguistics, pages
782?788.
Beatrice Warren. 1978. Semantic patterns of noun-noun
compounds. In Gothenburg Studies in English 41,
Goteburg, Acta Universtatis Gothoburgensis.
658
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 286?296, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Source Language Adaptation for Resource-Poor Machine Translation
Pidong Wang
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
wangpd@comp.nus.edu.sg
Preslav Nakov
QCRI
Qatar Foundation
Tornado Tower, P.O. 5825
Doha, Qatar
pnakov@qf.org.qa
Hwee Tou Ng
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
nght@comp.nus.edu.sg
Abstract
We propose a novel, language-independent
approach for improving machine translation
from a resource-poor language to X by adapt-
ing a large bi-text for a related resource-rich
language and X (the same target language).
We assume a small bi-text for the resource-
poor language to X pair, which we use to
learn word-level and phrase-level paraphrases
and cross-lingual morphological variants be-
tween the resource-rich and the resource-poor
language; we then adapt the former to get
closer to the latter. Our experiments for
Indonesian/Malay?English translation show
that using the large adapted resource-rich bi-
text yields 6.7 BLEU points of improvement
over the unadapted one and 2.6 BLEU points
over the original small bi-text. Moreover,
combining the small bi-text with the adapted
bi-text outperforms the corresponding com-
binations with the unadapted bi-text by 1.5?
3 BLEU points. We also demonstrate applica-
bility to other languages and domains.
1 Introduction
Statistical machine translation (SMT) systems learn
how to translate from large sentence-aligned bilin-
gual corpora of human-generated translations, called
bi-texts. Unfortunately, collecting sufficiently large,
high-quality bi-texts is hard, and thus most of the
6,500+ world languages remain resource-poor. For-
tunately, many of these resource-poor languages
are related to some resource-rich language, with
whom they overlap in vocabulary and share cog-
nates, which offers opportunities for bi-text reuse.
Example pairs of such resource rich?poor lan-
guages include Spanish?Catalan, Finnish?Estonian,
Swedish?Norwegian, Russian?Ukrainian, Irish?
Gaelic Scottish, Standard German?Swiss Ger-
man, Modern Standard Arabic?Dialectical Arabic
(e.g., Gulf, Egyptian), Turkish?Azerbaijani, etc.
Previous work has already demonstrated the ben-
efits of using a bi-text for a related resource-rich
language to X (e.g., X=English) to improve ma-
chine translation from a resource-poor language to
X (Nakov and Ng, 2009; Nakov and Ng, 2012).
Here we take a different, orthogonal approach: we
adapt the resource-rich language to get closer to the
resource-poor one.
We assume a small bi-text for the resource-poor
language, which we use to learn word-level and
phrase-level paraphrases and cross-lingual morpho-
logical variants between the two languages. Assum-
ing translation into the same target language X , we
adapt (the source side of) a large training bi-text for
a related resource-rich language and X .
Training on the adapted large bi-text yields very
significant improvements in translation quality com-
pared to both (a) training on the unadapted version,
and (b) training on the small bi-text for the resource-
poor language. We further achieve very sizable im-
provements when combining the small bi-text with
the large adapted bi-text, compared to combining the
former with the unadapted bi-text.
While we focus on adapting Malay to look like
Indonesian in our experiments, we also demonstrate
the applicability of our approach to another language
pair, Bulgarian?Macedonian, which is also from a
different domain.
286
2 Related Work
One relevant line of research is on machine trans-
lation between closely related languages, which is
arguably simpler than general SMT, and thus can
be handled using word-for-word translation, man-
ual language-specific rules that take care of the nec-
essary morphological and syntactic transformations,
or character-level translation/transliteration. This
has been tried for a number of language pairs in-
cluding Czech?Slovak (Hajic? et al2000), Turkish?
Crimean Tatar (Altintas and Cicekli, 2002), Irish?
Scottish Gaelic (Scannell, 2006), and Bulgarian?
Macedonian (Nakov and Tiedemann, 2012). In con-
trast, we have a different objective ? we do not carry
out full translation but rather adaptation since our
ultimate goal is to translate into a third language X .
A special case of this same line of research is the
translation between dialects of the same language,
e.g., between Cantonese and Mandarin (Zhang,
1998), or between a dialect of a language and a stan-
dard version of that language, e.g., between some
Arabic dialect (e.g., Egyptian) and Modern Standard
Arabic (Bakr et al2008; Sawaf, 2010; Salloum and
Habash, 2011). Here again, manual rules and/or
language-specific tools are typically used. In the
case of Arabic dialects, a further complication arises
by the informal status of the dialects, which are not
standardized and not used in formal contexts but
rather only in informal online communities1 such as
social networks, chats, Twitter and SMS messages.
This causes further mismatch in domain and genre.
Thus, translating from Arabic dialects to Modern
Standard Arabic requires, among other things, nor-
malizing informal text to a formal form. In fact,
this is a more general problem, which arises with
informal sources like SMS messages and Tweets for
just any language (Aw et al2006; Han and Bald-
win, 2011). Here the main focus is on coping with
spelling errors, abbreviations, and slang, which are
typically addressed using string edit distance, while
also taking pronunciation into account. This is dif-
ferent from our task, where we try to adapt good,
formal text from one language into another.
A second relevant line of research is on language
adaptation and normalization, when done specifi-
cally for improving SMT into another language.
1The Egyptian Wikipedia is one notable exception.
For example, Marujo et al2011) described a
rule-based system for adapting Brazilian Portuguese
(BP) to European Portuguese (EP), which they used
to adapt BP?English bi-texts to EP?English. They
report small improvements in BLEU for EP?English
translation when training on the adapted ?EP??En
bi-text compared to using the unadapted BP?En
(38.55 vs. 38.29), or when an EP?English bi-text is
used in addition to the adapted/unadapted one (41.07
vs. 40.91 BLEU). Unlike this work, which heav-
ily relied on language-specific rules, our approach is
statistical, and largely language-independent; more-
over, our improvements are much more sizable.
A third relevant line of research is on reusing bi-
texts between related languages without or with very
little adaptation, which works well for very closely
related languages. For example, our previous work
(Nakov and Ng, 2009; Nakov and Ng, 2012) ex-
perimented with various techniques for combining
a small bi-text for a resource-poor language (In-
donesian or Spanish, pretending that Spanish is
resource-poor) with a much larger bi-text for a re-
lated resource-rich language (Malay or Portuguese);
the target language of all bi-texts was English. How-
ever, our previous work did not attempt language
adaptation, except for very simple transliteration for
Portuguese?Spanish that ignored context entirely;
since it could not substitute one word for a com-
pletely different word, it did not help much for
Malay?Indonesian, which use unified spelling. Still,
once we have language-adapted the large bi-text, it
makes sense to try to combine it further with the
small bi-text; thus, below we will directly compare
and combine these two approaches.
Another alternative, which we do not explore in
this work, is to use cascaded translation using a
pivot language (Utiyama and Isahara, 2007; Cohn
and Lapata, 2007; Wu and Wang, 2009). Unfortu-
nately, using the resource-rich language as a pivot
(poor?rich?X) would require an additional paral-
lel poor?rich bi-text, which we do not have. Pivoting
over the target X (rich?X?poor) for the purpose
of language adaptation, on the other hand, would
miss the opportunity to exploit the relationship be-
tween the resource-poor and the resource-rich lan-
guage; this would also be circular since the first step
would ask an SMT system to translate its own train-
ing data (we only have one rich?X bi-text).
287
3 Malay and Indonesian
Malay and Indonesian are closely related, mutually
intelligible Austronesian languages with 180 million
speakers combined. They have a unified spelling,
with occasional differences, e.g., kerana vs. karena
(?because?), Inggeris vs. Inggris (?English?), and
wang vs. uang (?money?).
They differ more substantially in vocabulary,
mostly because of loan words, where Malay typi-
cally follows the English pronunciation, while In-
donesian tends to follow Dutch, e.g., televisyen vs.
televisi, Julai vs. Juli, and Jordan vs. Yordania.
While there are many cognates between the two
languages, there are also a lot of false friends, e.g.,
polisi means policy in Malay but police in Indone-
sian. There are also many partial cognates, e.g.,
nanti means both will (future tense marker) and later
in Malay but only later in Indonesian.
Thus, fluent Malay and fluent Indonesian can dif-
fer substantially. Consider, for example, Article 1 of
the Universal Declaration of Human Rights:2
? Semua manusia dilahirkan bebas dan samarata dari segi kemu-
liaan dan hak-hak. Mereka mempunyai pemikiran dan perasaan
hati dan hendaklah bertindak di antara satu sama lain dengan
semangat persaudaraan. (Malay)
? Semua orang dilahirkan merdeka dan mempunyai marta-
bat dan hak-hak yang sama. Mereka dikaruniai akal dan
hati nurani dan hendaknya bergaul satu sama lain dalam
semangat persaudaraan. (Indonesian)
There is only 50% overlap at the word level, but
the actual vocabulary overlap is much higher, e.g.,
there is only one word in the Malay text that does
not exist in Indonesian: samarata (?equal?). Other
differences are due to the use of different morpho-
logical forms, e.g., hendaklah vs. hendaknya (?con-
science?), derivational variants of hendak (?want?).
Of course, word choice in translation is often a
matter of taste. Thus, we asked a native speaker of
Indonesian to adapt the Malay version to Indonesian
while preserving as many words as possible:
? Semua manusia dilahirkan bebas dan mempunyai martabat
dan hak-hak yang sama. Mereka mempunyai pemikiran dan
perasaan dan hendaklah bergaul satu sama lain dalam
semangat persaudaraan. (Indonesian)
2English: All human beings are born free and equal in dig-
nity and rights. They are endowed with reason and conscience
and should act towards one another in a spirit of brotherhood.
Obtaining this latter version from the original
Malay text requires three word-level operations:
(1) deletion of dari, segi, (2) insertion of yang, sama,
and (3) substitution of samarata with mempunyai.
Unfortunately, we do not have parallel Malay-
Indonesian text, which complicates the process of
learning when to apply these operations. Thus, be-
low we restrict our attention to the simplest and most
common operation of word substitution only, leav-
ing the other two3 operations for future work.
Note that word substitution is enough in many
cases, e.g., it is all that is needed for the following
Malay-Indonesian sentence pair:4
? KDNK Malaysia dijangka cecah 8 peratus pada tahun 2010.
? PDB Malaysia akan mencapai 8 persen pada tahun 2010.
4 Method
We improve machine translation from a resource-
poor language (Indonesian) to English by adapting a
bi-text for a related resource-rich language (Malay)
and English, using word-level and phrase-level para-
phrases and cross-lingual morphological variants.
4.1 Word-Level Paraphrasing
Given a Malay sentence, we generate a confusion
network containing multiple Indonesian word-level
paraphrase options for each Malay word. Each such
Indonesian option is associated with a correspond-
ing weight in the network, which is defined as the
probability of this option being a translation of the
original Malay word (see Eq. 1 below). We decode
this confusion network using a large Indonesian lan-
guage model, thus generating a ranked list of n cor-
responding adapted ?Indonesian? sentences.
Then, we pair each such adapted ?Indonesian?
sentence with the English counter-part for the
Malay sentence it was derived from, thus obtain-
ing a synthetic ?Indonesian??English bi-text. Fi-
nally, we combine this synthetic bi-text with the
original Indonesian?English one to train the final
Indonesian?English SMT system.
Below we first describe how we generate word-
level Indonesian options and corresponding weights
for the Malay words. Then, we explain how we
build, decode, and improve the confusion network.
3There are other potentially useful operations, e.g., a correct
translation for the Malay samarata can be obtained by splitting
it into the Indonesian sequence sama rata.
4Malaysia?s GDP is expected to reach 8 percent in 2010.
288
4.1.1 Inducing Word-Level Paraphrases
We use pivoting over English to induce potential
Indonesian translations for a given Malay word.
First, we generate separate word-level alignments
for the Indonesian?English and the Malay?English
bi-texts. Then, we induce Indonesian-Malay word
translation pairs assuming that if an Indonesian word
i and a Malay word m are aligned to the same
English word e, they could be mutual translations.
Each translation pair is associated with a conditional
probability, estimated by pivoting over English:
Pr(i|m) =
?
e
Pr(i|e)Pr(e|m) (1)
Pr(i|e) and Pr(e|m) are estimated using maxi-
mum likelihood from the word alignments. Follow-
ing (Callison-Burch et al2006), we further assume
that i is conditionally independent of m given e.
4.1.2 Confusion Network Construction
Given a Malay sentence, we construct an Indone-
sian confusion network, where each Malay word is
augmented with a set of network transitions: pos-
sible Indonesian word translations. The weight
of such a transition is the conditional Indonesian-
Malay translation probability as calculated by Eq. 1;
the original Malay word is assigned a weight of 1.
Note that we paraphrase each word in the in-
put Malay sentence as opposed to only those Malay
words that we believe not to exist in Indonesian, e.g.,
because they do not appear in our Indonesian mono-
lingual text. This is necessary because of the large
number of false friends and partial cognates between
Malay and Indonesian (see Section 3).
Finally, we decode the confusion network for a
Malay sentence using a large Indonesian language
model, and we extract an n-best list.5 Table 1
shows the 10-best adapted ?Indonesian? sentences6
we generated for the confusion network in Figure 1.
4.1.3 Further Refinements
Many of our paraphrases are bad: some have very
low probabilities, while others involve rare words
for which the probability estimates are unreliable.
5For balance, in case of less than n adaptations for a Malay
sentence, we randomly repeat some of the available ones.
6According to a native Indonesian speaker, options 1 and 3
in Table 1 are perfect adaptations, options 2 and 5 have a wrong
word order, and the rest are grammatical though not perfect.
Moreover, the options we propose for a Malay
word are inherently restricted to the small Indone-
sian vocabulary of the Indonesian?English bi-text.
Below we describe how we address these issues.
Score-based filtering. We filter out translation
pairs whose probabilities (Eq. 1) are lower than
some threshold (tuned on the dev dataset), e.g., 0.01.
Improved estimations for Pr(i|e). We concate-
nate k copies of the Indonesian?English bi-text and
one copy of the Malay?English bi-text, where the
value of k is selected so that we have roughly the
same number of Indonesian and Malay sentences.
Then, we generate word-level alignments for the
resulting bi-text. Finally, we truncate these align-
ments keeping them for one copy of the original
Indonesian?English bi-text only. Thus, we end up
with improved word alignments for the Indonesian?
English bi-text, and with better estimations for Eq. 1.
Since Malay and Indonesian share many cognates,
this improves word alignments for Indonesian words
that occur rarely in the small Indonesian?English bi-
text but are relatively frequent in the larger Malay?
English one; it also helps for some frequent words.
Cross-lingual morphological variants. We in-
crease the Indonesian options for a Malay word us-
ing morphology. Since the set of Indonesian op-
tions for a Malay word in pivoting is restricted to
the Indonesian vocabulary of the small Indonesian?
English bi-text, this is a severe limitation of pivot-
ing. Thus, assuming a large monolingual Indone-
sian text, we first build a lexicon of the words in the
text. Then, we lemmatize these words using two dif-
ferent lemmatizers: the Malay lemmatizer of Bald-
win and Awab (2006), and a similar Indonesian lem-
matizer. Since these two analyzers have different
strengths and weaknesses, we combine their outputs
to increase recall. Next, we group all Indonesian
words that share the same lemma, e.g., for minum,
we obtain {diminum, diminumkan, diminumnya, makan-minum,
makananminuman, meminum, meminumkan, meminumnya, meminum-
minuman, minum, minum-minum, minum-minuman, minuman, minu-
manku, minumannya, peminum, peminumnya, perminum, terminum}.
Since Malay and Indonesian are subject to the same
morphological processes and share many lemmata,
we use such groups to propose Indonesian transla-
tion options for a Malay word. We first lemmatize
the target Malay word, and then we find all groups
of Indonesian words the Malay lemmata belong to.
289
0 1
pdb|0.576172
sebesar|0.052080
maka|0.026044
perkiraan|0.026035
panggar|0.026035
rkp|0.026035
gdp|0.026034
2malaysia|1.0 3
akan|0.079793
untuk|0.050155
diharapkan|0.044511
diperkirakan|0.039131
ke|0.018960
dapat|0.018436
adalah|0.017422
menjadi|0.011655
ini|0.011158
4
remaja|0.047619
mencapai|0.042930
hit|0.030612
sr|0.030482
guncang|0.023810
di|0.022778
untuk|0.018425
hits|0.013605
diguncang|0.010074
58|1.0 6persen|0.473588per|0.148886 7
pada|1.0 8tahun|1.0 92010|1.0 10.|1.0
Figure 1: Indonesian confusion network for the Malay sentence ?KDNK Malaysia dijangka cecah 8 peratus pada tahun 2010.?
Arcs with scores below 0.01 are omitted, and words that exist in Indonesian are not paraphrased (for better readability).
Rank ?Indonesian? Sentence
1 pdb malaysia akan mencapai 8 persen pada tahun 2010 .
2 pdb malaysia untuk mencapai 8 persen pada tahun 2010 .
3 pdb malaysia diperkirakan mencapai 8 persen pada tahun 2010 .
4 maka malaysia akan mencapai 8 persen pada tahun 2010 .
5 maka malaysia untuk mencapai 8 persen pada tahun 2010 .
6 pdb malaysia dapat mencapai 8 persen pada tahun 2010 .
7 maka malaysia diperkirakan mencapai 8 persen pada tahun 2010 .
8 sebesar malaysia akan mencapai 8 persen pada tahun 2010 .
9 pdb malaysia diharapkan mencapai 8 persen pada tahun 2010 .
10 pdb malaysia ini mencapai 8 persen pada tahun 2010 .
Table 1: The 10-best ?Indonesian? sentences extracted from the confusion network in Figure 1.
The union of these groups is the set of morpholog-
ical variants that we will add to the confusion net-
work as additional options for the Malay word.7 For
example, given seperminuman (?drinking?) in the
Malay input, we first find its stem minum, and then
we get the above example set of Indonesian words,
which contains some reasonable substitutes such as
minuman (?drink?). In the confusion network, the
weight of the original Malay word is set to 1, while
the weight of a morphological option is one minus
the minimum edit distance ratio (Ristad and Yian-
ilos, 1998) between it and the Malay word, multi-
plied by the highest probability for all pivoting vari-
ants for the Malay word.
4.2 Phrase-Level Paraphrasing
Word-level paraphrasing ignores context when gen-
erating Indonesian variants, relying on the Indone-
sian language model to make the right contextual
choice. We also try to model context more directly
by generating adaptation options at the phrase level.
7While the different morphological forms typically have dif-
ferent meanings, e.g., minum (?drink?) vs. peminum (?drinker?),
in some cases the forms could have the same translation in En-
glish, e.g., minum (?drink?, verb) vs. minuman (?drink?, noun).
This is our motivation for trying morphological variants, even
though they are almost exclusively derivational, and thus quite
risky as translational variants; see also (Nakov and Ng, 2011).
Phrase-level paraphrase induction. We use
standard phrase-based SMT techniques to build sep-
arate phrase tables for the Indonesian?English and
the Malay?English bi-texts, where we have four
conditional probabilities: forward/reverse phrase
translation probability, and forward/reverse lexical-
ized phrase translation probability. We pivot over
English to generate Indonesian-Malay phrase pairs,
whose probabilities are derived from the corre-
sponding ones in the two phrase tables using Eq. 1.
Cross-lingual morphological variants. While
phrase-level paraphrasing models context better, it
remains limited in the size of its Indonesian vocab-
ulary by the small Indonesian?English bi-text, just
like word-level paraphrasing was. We address this
by transforming the sentences in the development
and the test Indonesian?English bi-texts into confu-
sion networks, where we add Malay morphological
variants for the Indonesian words, weighting them as
before. Note that we do not alter the training bi-text.
4.3 Combining Bi-texts
We combine the Indonesian?English and the syn-
thetic ?Indonesian??English bi-texts as follows:
Simple concatenation. Assuming the two bi-
texts are of comparable quality, we simply train an
SMT system on their concatenation.
290
Balanced concatenation with repetitions. How-
ever, the two bi-texts are not directly comparable and
are clearly not equally good as a source of training
data for an Indonesian-English SMT system. For
one thing, the ?Indonesian??English bi-text is ob-
tained from n-best lists, i.e., it has exactly n very
similar variants for each Malay sentence. Moreover,
the original Malay?English bi-text is much larger
in size than the Indonesian?English one, and now
it has been further expanded n times in order to be-
come an ?Indonesian??English bi-text, which means
that it will dominate the concatenation due to its
size. In order to counter-balance this, we repeat the
smaller Indonesian?English bi-text enough times so
that we can make the number of sentences it contains
roughly the same as for the ?Indonesian??English
bi-text; then we concatenate the two bi-texts and we
train an SMT system on the resulting bi-text.
Sophisticated phrase table combination. Fi-
nally, we experiment with a method for combining
phrase tables proposed in (Nakov and Ng, 2009;
Nakov and Ng, 2012). The first phrase table is
extracted from word alignments for the balanced
concatenation with repetitions, which are then trun-
cated so that they are kept for only one copy of the
Indonesian?English bi-text. The second table is built
from the simple concatenation. The two tables are
then merged as follows: all phrase pairs from the
first one are retained, and to them are added those
phrase pairs from the second one that are not present
in the first one. Each phrase pair retains its orig-
inal scores, which are further augmented with 1?3
additional feature scores indicating its origin: the
first/second/third feature is 1 if the pair came from
the first/second/both table(s), and 0 otherwise. We
experiment using all three, the first two, or the first
feature only; we also try setting the features to 0.5
instead of 0. This makes the following six combina-
tions (0, 00, 000, .5, .5.5, .5.5.5); on testing, we use
the one that achieves the highest BLEU score on the
development set.
Other possibilities for combining the phrase ta-
bles include using alternative decoding paths (Birch
et al2007), simple linear interpolation, and direct
phrase table merging with extra features (Callison-
Burch et al2006); they were previously found in-
ferior to the last two approaches above (Nakov and
Ng, 2009; Nakov and Ng, 2012).
5 Experiments
We run two kinds of experiments: (a) isolated,
where we train on the synthetic ?Indonesian??
English bi-text only, and (b) combined, where we
combine it with the Indonesian?English bi-text.
5.1 Datasets
In our experiments, we use the following datasets,
normally required for Indonesian?English SMT:
? Indonesian?English train bi-text (IN2EN):
28,383 sentence pairs; 915,192 English tokens;
796,787 Indonesian tokens;
? Indon.?English dev bi-text (IN2EN-dev):
2,000 sentence pairs; 36,584 English tokens;
35,708 Indonesian tokens;
? Indon.?English test bi-text (IN2EN-test):
2,018 sentence pairs; 37,101 English tokens;
35,509 Indonesian tokens;
? Monolingual English text (EN-LM): 174,443
sentences; 5,071,988 English tokens.
We also use a Malay?English set (to be turned
into ?Indonesian??English), and monolingual In-
donesian text (for decoding the confusion network):
? Malay?English train bi-text (ML2EN):
290,000 sentence pairs; 8,638,780 English
tokens; 8,061,729 Malay tokens;
? Monolingual Indonesian text (IN-LM):
1,132,082 sentences; 20,452,064 Indonesian
tokens.
5.2 Baseline Systems
We build five baseline systems ? two using a sin-
gle bi-text, ML2EN or IN2EN, and three combin-
ing ML2EN and IN2EN, using simple concatenation,
balanced concatenation, and sophisticated phrase ta-
ble combination. The last combination is a very
strong baseline and the most relevant one we need
to improve upon.
5.3 Isolated Experiments
The isolated experiments only use the adapted
?Indonesian??English bi-text, which allows for a di-
rect comparison to using ML2EN / IN2EN only.
5.3.1 Word-Level Paraphrasing
In our word-level paraphrasing experiments, we
adapt Malay to Indonesian using three kinds of con-
fusion networks (see Section 4.1.3 for details):
291
? CN:pivot ? using word-level pivoting only;
? CN:pivot? ? using word-level pivoting, with
probabilities from word alignments for IN2EN
that were improved using ML2EN;
? CN:pivot?+morph ? CN:pivot? augmented with
cross-lingual morphological variants.
There are two parameter values to be tuned
on IN2EN-dev for the above confusion networks:
(1) the minimum pivoting probability threshold for
the Malay-Indonesian word-level paraphrases, and
(2) the number of n-best Indonesian-adapted sen-
tences that are to be generated for each input Malay
sentence. We try {0.001, 0.005, 0.01, 0.05} for the
threshold and {1, 5, 10} for n.
5.3.2 Phrase-Level Paraphrasing
In our phrase-level paraphrasing experiments, we
use pivoted phrase tables (PPT) with the following
features for each phrase table entry (in addition to
the phrase penalty; see Section 4.2 for more details):
? PPT:1 ? only uses the forward conditional
translation probability;
? PPT:4 ? uses all four conditional probabilities;
? PPT:4::CN:morph ? PPT:4 but used with a
cross-lingual morphological confusion network
for the dev/test Indonesian sentences.
Here we tune one parameter only: the number of
n-best Indonesian-adapted sentences to be generated
for each input Malay sentence; we try {1, 5, 10}.
5.4 Combined Experiments
These experiments assess the impact of our adap-
tation approach when combined with the original
Indonesian?English bi-text IN2EN as opposed to
combining ML2EN with IN2EN (as was in the last
three baselines). We experiment with the same three
combinations: simple concatenation, balanced con-
catenation, and sophisticated phrase table combina-
tion. We tune the parameters as before; for the last
combination, we further tune the six extra feature
combinations (see Section 4.3 for details).
6 Results and Discussion
For all tables, statistically significant improvements
(p < 0.01), according to Collins et al2005)?s sign
test, over the baseline are in bold; in case of two
baselines, underline is used for the second baseline.
System BLEU
ML2EN 14.50
IN2EN 18.67
Simple concatenation 18.49
Balanced concatenation 19.79
Sophisticated phrase table combination 20.10(.5.5)
Table 2: The five baselines. The subscript indicates the
parameters found on IN2EN-dev and used for IN2EN-test.
The scores that are statistically significantly better than
ML2EN and IN2EN (p < 0.01, Collins? sign test) are
shown in bold and are underlined, respectively.
6.1 Baseline Experiments
The results for the baseline systems are shown in Ta-
ble 2. We can see that training on ML2EN instead of
IN2EN yields over 4 points absolute drop in BLEU
(Papineni et al2002) score, even though ML2EN is
about 10 times larger than IN2EN and both bi-texts
are from the same domain. This confirms the exis-
tence of important differences between Malay and
Indonesian. While simple concatenation does not
help, balanced concatenation with repetitions im-
proves by 1.12 BLEU points over IN2EN, which
shows the importance of giving IN2EN a proper
weight in the combined bi-text. This is further re-
confirmed by the sophisticated phrase table combi-
nation, which yields an additional absolute gain of
0.31 BLEU points.
6.2 Isolated Experiments
Table 3 shows the results for the isolated experi-
ments. We can see that word-level paraphrasing
improves by up to 5.56 and 1.39 BLEU points
over the two baselines (both statistically signifi-
cant). Compared to ML2EN, CN:pivot yields an ab-
solute improvement of 4.41 BLEU points, CN:pivot?
adds another 0.59, and CN:pivot?+morph adds 0.56
more. The scores for TER (v. 0.7.25) and METEOR
(v. 1.3) are on par with those for BLEU (NIST v. 13).
Table 3 further shows that the optimal parameters
for the word-level SMT systems (CN:*) involve a
very low probability cutoff, and a high number of
n-best sentences. This shows that they are robust to
noise, probably because bad source-side phrases are
unlikely to match the test-time input. Note also the
effect of repetitions: good word choices are shared
by many n-best sentences, and thus they would have
higher probabilities compared to bad word choices.
292
n-gram precision
System 1-gr. 2-gr. 3-gr. 4-gr. BLEU TER METEOR
ML2EN (baseline) 48.34 19.22 9.54 4.98 14.50 67.14 43.28
IN2EN (baseline) 55.04 23.90 12.87 7.18 18.67 61.99 54.34
CN:pivot 54.50 24.41 13.09 7.35 18.91(+4.41,+0.24)(0.005,10best) 61.94 51.07
CN:pivot? 55.05 25.09 13.60 7.69 19.50(+5.00,+0.83)(0.001,10best) 61.25 51.97
(i) CN:pivot?+morph 55.97 25.73 14.06 7.99 20.06(+5.56,+1.39)(0.005,10best) 60.31 55.65
PPT:1 55.11 25.04 13.66 7.80 19.58(+5.08,+0.91)(10best) 60.92 51.93
PPT:4 56.64 26.20 14.53 8.40 20.63(+6.13,+1.96)(10best) 59.33 54.23
(ii) PPT:4::CN:morph 56.91 26.53 14.76 8.55 20.89(+6.39,+2.22)(10best) 59.30 57.19
System combination: (i) + (ii) 57.73 27.00 15.03 8.71 21.24(+6.74,+2.57) 58.19 54.63
Table 3: Isolated experiments. The subscript shows the best tuning parameters, and the superscript shows the absolute
test improvement over the ML2EN and the IN2EN baselines. The last line shows system combination results.
Combining IN2EN with an adapted version of ML2EN
Combination with Simple Concatenation Balanced Concatenation Sophisticated Combination
(i) + ML2EN (unadapted; baseline) 18.49 19.79 20.10(.5.5)
+ CN:pivot 19.99(+1.50)(0.001,1best) 20.16
(+0.37)
(0.001,10best) 20.32
(+0.22)
(0.01,10best,.5.5)
+ CN:pivot? 20.03(+1.54)(0.05,1best) 20.80
(+1.01)
(0.05,10best) 20.55
(+0.45)
(0.05,10best,.5.5)
(ii) + CN:pivot?+morph 20.60(+2.11)(0.01,10best) 21.15
(+1.36)
(0.01,10best) 21.05
(+0.95)
(0.01,5best,00)
+ PPT:1 20.61(+2.12)(1best) 20.71
(+0.92)
(10best) 20.32
(+0.22)
(1best,000)
+ PPT:4 20.75(+2.26)(1best) 21.08
(+1.29)
(5best) 20.76
(+0.66)
(10best,.5.5.5)
(iii) + PPT:4::CN:morph 21.01(+2.52)(1best) 21.31
(+1.52)
(5best) 20.98
(+0.88)
(10best,.5)
System combination: (i) + (ii) + (iii) 21.55(+3.06) 21.64(+1.85) 21.62(+1.52)
Table 4: Combined experiments: BLEU. The best tuning parameter values are in subscript, and the absolute test
improvement over the corresponding baseline (on top of each column) is in superscript.
The gap between ML2EN and IN2EN for unigram
precision could be explained by vocabulary differ-
ences between Malay and Indonesian. Compared
to IN2EN, all CN:* models have higher 2/3/4-gram
precision. However, CN:pivot has lower unigram
precision, which could be due to bad word align-
ments, as the results for CN:pivot? show.
When morphological variants are further added,
the unigram precision improves by almost 1% ab-
solute over CN:pivot?. This shows the importance
of morphology for overcoming the limitations of the
small Indonesian vocabulary of the IN2EN bi-text.
The lower part of Table 3 shows that phrase-level
paraphrasing performs a bit better. This confirms the
importance of modeling context for closely-related
languages like Malay and Indonesian, which are rich
in false friends and partial cognates. We further
see that using more scores in the phrase table is
better. Extending the Indonesian vocabulary with
cross-lingual morphological variants is still helpful,
though not as much as at the word-level.
Finally, the combination of the output of
the best PPT and the best CN systems using
MEMT (Heafield and Lavie, 2010) yields even fur-
ther improvements, which shows that the two kinds
of paraphrases are complementary. The best overall
BLEU score for our isolated experiments is 21.24,
which is better than the results for all five baselines
in Table 2, including the three bi-text combination
baselines, which only achieve up to 20.10 BLEU.
6.3 Combined Experiments
Table 4 shows the performance of the three bi-
text combination strategies (see Section 4.3 for ad-
ditional details) when applied to combine IN2EN
(1) with the original ML2EN and (2) with various
adapted versions of it.
We can see that for the word-level paraphras-
ing experiments (CN:*), all combinations except
for CN:pivot perform significantly better than their
corresponding baselines, but the improvements are
most sizeable for the simple concatenation.
293
Note that while there is a difference of 0.31 BLEU
points between the balanced concatenation and the
sophisticated combination for the original ML2EN,
they differ little for the adapted versions. This is
probably due to the sophisticated combination as-
suming that the second bi-text is worse than the first
one, which is not really the case for the adapted ver-
sions: as Table 3 shows, they all outperform IN2EN.
Overall, phrase-level paraphrasing performs a bit
better than word-level paraphrasing, and system
combination with MEMT improves even further.
This is consistent with the isolated experiments.
7 Further Analysis
Paraphrasing non-Indonesian words only. In
CN:* above, we paraphrased each word in the Malay
input, because of false friends like polisi and partial
cognates like nanti. This risks proposing worse al-
ternatives, e.g., changing beliau (?he?, respectful) to
ia (?he?, casual), which confusion network weights
and LM would not always handle. Thus, we tried
paraphrasing non-Indonesian words only, i.e., those
not in IN-LM. Since IN-LM occasionally contains
some Malay-specific words, we also tried paraphras-
ing words that occur at most t times in IN-LM. Ta-
ble 5 shows that this hurts by up to 1 BLEU point
for t = 0; 10, and a bit less for t = 20; 40.
System BLEU
CN:pivot, t = 0 17.88(0.01,5best)
CN:pivot, t = 10 17.88(0.05,10best)
CN:pivot, t = 20 18.14(0.01,5best)
CN:pivot, t = 40 18.34(0.01,5best)
CN:pivot (i.e., paraphrase all) 18.91(0.005,10best)
Table 5: Paraphrasing non-Indonesian words only:
those appearing at most t times in IN-LM.
Manual evaluation. We asked a native Indone-
sian speaker who does not speak Malay to judge
whether our ?Indonesian? adaptations are more un-
derstandable to him than the original Malay in-
put for 100 random sentences. We presented him
with two extreme systems: (a) the conservative
CN:pivot,t=0 vs. (b) CN:pivot?+morph. Since the
latter is noisy, the top 3 choices were judged for
it. Table 6 shows that CN:pivot,t=0 is better/equal
to the original 53%/31% of the time. In contrast,
CN:pivot?+morph is typically worse than the orig-
inal; even compared to the best in top 3, the bet-
ter:worse ratio is 45%:43%.
Still, this latter model works better, which means
that phrase-based SMT systems are robust to noise
and prefer more variety. Note also that the judg-
ments were at the sentence level, while phrases are
sub-sentential, i.e., there can be many good phrases
in a ?bad? sentence.
System Better Equal Worse
CN:pivot, t = 0(Rank1) 53% 31% 16%
CN:pivot?+morph(Rank1) 38% 8% 54%
CN:pivot?+morph(Rank2) 41% 9% 50%
CN:pivot?+morph(Rank3) 32% 11% 57%
CN:pivot?+morph(Ranks:1?3) 45% 12% 43%
Table 6: Human judgments: Malay vs. ?Indonesian?.
The parameter values are those from Tables 3 and 5.
Reversed Adaptation. In all experiments above,
we were adapting the Malay sentences to look like
Indonesian. Here we try to reverse the direction of
adaptation, i.e., to adapt Indonesian to Malay: we
thus build a ?Malay? confusion network for each
dev/test Indonesian sentence to be used as an in-
put to a Malay?English SMT system trained on the
ML2EN dataset. We tried two variations of this idea:
? lattice: Use Indonesian-to-Malay confusion
networks directly as input to the ML2EN SMT
system, i.e., tune a log-linear model using con-
fusion networks for the source side of the
IN2EN-dev dataset, and then evaluate the tuned
system using confusion networks for the source
side of the IN2EN-test dataset.
? 1-best: Use the 1-best output from the
Indonesian-to-Malay confusion network for
each sentence of IN2EN-dev and IN2EN-test.
Then pair each 1-best output with the corre-
sponding English sentence. Finally, get an
adapted ?Malay??English development set and
an adapted ?Malay??English test set, and use
them to tune and evaluate the ML2EN SMT
system.
Table 7 shows that both variations perform worse
than CN:pivot. We believe this is because lattice en-
codes many options, but does not use a Malay LM,
while 1-best uses a Malay LM, but has to commit
to 1-best. In contrast, CN:pivot uses both n-best
outputs and an Indonesian LM; designing a similar
setup for reversed adaptation is a research direction
we would like to pursue in future work.
294
System BLEU
CN:pivot (Malay?Indonesian) 18.91(0.005,10best)
CN:pivot (Indonesian?Malay) ? lattice 17.22(0.05)
CN:pivot (Indonesian?Malay) ? 1-best 17.77(0.001)
Table 7: Reversed adaptation: Indonesian to Malay.
Adapting Macedonian to Bulgarian. We ex-
perimented with another pair of closely-related lan-
guages,8 Macedonian (MK) and Bulgarian (BG), us-
ing data from a different, non-newswire domain: the
OPUS corpus of movie subtitles (Tiedemann, 2009).
We used datasets of sizes that are comparable to
those in the previous experiments: 160K MK2EN
and 1.5M BG2EN sentence pairs (1.2M and 11.5M
EN words). Since the sentences were short, we used
10K MK2EN sentence pairs for tuning and testing
(77K and 72K English words). For the LM, we used
9.2M Macedonian and 433M English words.
Table 8 shows that both CN:* and PPT:* yield
statistically significant improvements over balanced
concatenation with unadapted BG2EN; system com-
bination with MEMT improves even further. This
indicates that our approach can work for other pairs
of related languages and even for other domains.
We should note though that the improvements
here are less sizeable than for Indonesian/Malay.
This may be due to our monolingual MK dataset be-
ing smaller (10M MK vs. 20M IN words), and too
noisy, containing many OCR errors, typos, concate-
nated words, and even some Bulgarian text. More-
over, Macedonian and Bulgarian are arguably some-
what more dissimilar than Malay and Indonesian.
System BLEU TER METEOR
BG2EN (baseline) 24.57 57.64 41.60
MK2EN (baseline) 26.46 54.55 46.15
Balanced concatenation of MK2EN with an adapted BG2EN
+ BG2EN (unadapted) 27.33 54.61 48.16
+ CN:pivot?+morph 27.97(+0.64,+1.51) 54.08 49.65
+ PPT:4::CN:morph 28.38(+1.05,+1.92) 53.35 48.21
Combining last three 29.05(+1.72,+2.59) 52.31 50.96
Table 8: Improving Macedonian?English SMT by
adapting Bulgarian to Macedonian.
8There is a heated political and linguistic debate about
whether Macedonian represents a separate language or is a re-
gional literary form of Bulgarian. Since there are no clear cri-
teria for distinguishing a dialect from a language, linguists are
divided on this issue. Politically, the Macedonian remains un-
recognized as a language by Bulgaria and Greece.
8 Conclusion and Future Work
We have presented a novel approach for improving
machine translation for a resource-poor language by
adapting a bi-text for a related resource-rich lan-
guage, using confusion networks, word/phrase-level
paraphrasing, and morphological analysis.
We have achieved very significant improvements
over several baselines (6.7 BLEU points over an un-
adapted version of ML2EN, 2.6 BLEU points over
IN2EN, and 1.5?3 BLEU points over three bi-text
combinations of ML2EN and IN2EN), thus proving
the potential of the idea. We have further demon-
strated the applicability of the general approach to
other languages and domains.
In future work, we would like to add word dele-
tion, insertion, splitting, and concatenation as al-
lowed editing operations. We further want to ex-
plore tighter integration of word-based and phrase-
based paraphrasing. Finally, we plan experiments
with other language pairs and application to other
linguistic problems.
Acknowledgments
We would like to give special thanks to Harta Wijaya
and Aldrian Obaja Muis, native speakers of Indone-
sian, for their help in the linguistic analysis of the
input and output of our system. We would also like
to thank the anonymous reviewers for their construc-
tive comments and suggestions, which have helped
us improve the quality of this paper.
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
References
Kemal Altintas and Ilyas Cicekli. 2002. A machine
translation system between a pair of closely related
languages. In Proceedings of the 17th International
Symposium on Computer and Information Sciences,
ISCIS ?02, pages 192?196.
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normal-
ization. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, ACL-COLING ?06.
295
Hitham Abo Bakr, Khaled Shaalan, and Ibrahim Ziedan.
2008. A hybrid approach for converting written Egyp-
tian colloquial dialect into diacritized Arabic. In Pro-
ceedings of the 6th International Conference on Infor-
matics and Systems, INFOS ?08.
Timothy Baldwin and Su?ad Awab. 2006. Open source
corpus analysis tools for Malay. In Proceedings of the
5th International Conference on Language Resources
and Evaluation, LREC ?06, pages 2212?2215.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, WMT ?07, pages
9?16.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In Proceedings of the Human
Language Technology Conference of NAACL, HLT-
NAACL ?06, pages 17?24.
Trevor Cohn and Mirella Lapata. 2007. Machine trans-
lation by triangulation: Making effective use of multi-
parallel corpora. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics, ACL ?07, pages 728?735.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics,
ACL ?05, pages 531?540.
Jan Hajic?, Jan Hric, and Vladislav Kubon?. 2000. Ma-
chine translation of very close languages. In Proceed-
ings of the Sixth Conference on Applied Natural Lan-
guage Processing, ANLP ?00, pages 7?12.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: Makn sens a #twitter.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, ACL-HLT ?11, pages 368?378.
Kenneth Heafield and Alon Lavie. 2010. Combin-
ing machine translation output with open source:
The Carnegie Mellon multi-engine machine transla-
tion scheme. The Prague Bulletin of Mathematical
Linguistics, 93(1):27?36.
Lu??s Marujo, Nuno Grazina, Tiago Lu??s, Wang Ling,
Lu??sa Coheur, and Isabel Trancoso. 2011. BP2EP -
adaptation of Brazilian Portuguese texts to European
Portuguese. In Proceedings of the 15th Conference
of the European Association for Machine Translation,
EAMT ?11, pages 129?136.
Preslav Nakov and Hwee Tou Ng. 2009. Improved statis-
tical machine translation for resource-poor languages
using related resource-rich languages. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?09, pages 1358?1367.
Preslav Nakov and Hwee Tou Ng. 2011. Trans-
lating from morphologically complex languages: A
paraphrase-based approach. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
ACL-HLT ?11, pages 1298?1307.
Preslav Nakov and Hwee Tou Ng. 2012. Improving
statistical machine translation for a resource-poor lan-
guage using related resource-rich languages. Journal
of Artificial Intelligence Research, 44:179?222.
Preslav Nakov and Jo?rg Tiedemann. 2012. Combin-
ing word-level and character-level models for machine
translation between closely-related languages. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics, ACL-Short ?12.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, ACL ?02, pages 311?318.
Eric Ristad and Peter Yianilos. 1998. Learning string-
edit distance. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 20(5):522?532.
Wael Salloum and Nizar Habash. 2011. Dialectal
to Standard Arabic paraphrasing to improve Arabic-
English statistical machine translation. In Proc. of the
Workshop on Algorithms and Resources for Modelling
of Dialects and Language Varieties, pages 10?21.
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the 9th Confer-
ence of the Association for Machine Translation in the
Americas, AMTA ?09.
Kevin P. Scannell. 2006. Machine translation for closely
related language pairs. In Proceedings of the LREC
2006 Workshop on Strategies for Developing Machine
Translation for Minority Languages.
Jo?rg Tiedemann. 2009. News from OPUS - a collection
of multilingual parallel corpora with tools and inter-
faces. In Recent Advances in Natural Language Pro-
cessing, volume V, pages 237?248.
Masao Utiyama and Hitoshi Isahara. 2007. A com-
parison of pivot methods for phrase-based statistical
machine translation. In Proceedings of the Human
Language Technology Conference of NAACL, HLT-
NAACL ?07, pages 484?491.
Hua Wu and Haifeng Wang. 2009. Revisiting pivot lan-
guage approach for machine translation. In Proceed-
ings of the Joint Conference of the 47th Annual Meet-
ing of the ACL, ACL ?09, pages 154?162.
Xiaoheng Zhang. 1998. Dialect MT: a case study be-
tween Cantonese and Mandarin. In Proceedings of the
17th International Conference on Computational Lin-
guistics, COLING ?98, pages 1460?1464.
296
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 214?220,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Learning to Differentiate Better from Worse Translations
Francisco Guzm
?
an Shafiq Joty Llu??s M
`
arquez
Alessandro Moschitti Preslav Nakov Massimo Nicosia
ALT Research Group
Qatar Computing Research Institute ? Qatar Foundation
{fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qa
Abstract
We present a pairwise learning-to-rank
approach to machine translation evalua-
tion that learns to differentiate better from
worse translations in the context of a given
reference. We integrate several layers
of linguistic information encapsulated in
tree-based structures, making use of both
the reference and the system output simul-
taneously, thus bringing our ranking closer
to how humans evaluate translations. Most
importantly, instead of deciding upfront
which types of features are important, we
use the learning framework of preference
re-ranking kernels to learn the features au-
tomatically. The evaluation results show
that learning in the proposed framework
yields better correlation with humans than
computing the direct similarity over the
same type of structures. Also, we show
our structural kernel learning (SKL) can
be a general framework for MT evaluation,
in which syntactic and semantic informa-
tion can be naturally incorporated.
1 Introduction
We have seen in recent years fast improvement
in the overall quality of machine translation (MT)
systems. This was only possible because of the
use of automatic metrics for MT evaluation, such
as BLEU (Papineni et al., 2002), which is the de-
facto standard; and more recently: TER (Snover et
al., 2006) and METEOR (Lavie and Denkowski,
2009), among other emerging MT evaluation met-
rics. These automatic metrics provide fast and in-
expensive means to compare the output of differ-
ent MT systems, without the need to ask for hu-
man judgments each time the MT system has been
changed.
As a result, this has enabled rapid develop-
ment in the field of statistical machine translation
(SMT), by allowing to train and tune systems as
well as to track progress in a way that highly cor-
relates with human judgments.
Today, MT evaluation is an active field of re-
search, and modern metrics perform analysis at
various levels, e.g., lexical (Papineni et al., 2002;
Snover et al., 2006), including synonymy and
paraphrasing (Lavie and Denkowski, 2009); syn-
tactic (Gim?enez and M`arquez, 2007; Popovi?c
and Ney, 2007; Liu and Gildea, 2005); semantic
(Gim?enez and M`arquez, 2007; Lo et al., 2012);
and discourse (Comelles et al., 2010; Wong and
Kit, 2012; Guzm?an et al., 2014; Joty et al., 2014).
Automatic MT evaluation metrics compare the
output of a system to one or more human ref-
erences in order to produce a similarity score.
The quality of such a metric is typically judged
in terms of correlation of the scores it produces
with scores given by human judges. As a result,
some evaluation metrics have been trained to re-
produce the scores assigned by humans as closely
as possible (Albrecht and Hwa, 2008). Unfortu-
nately, humans have a hard time assigning an ab-
solute score to a translation. Hence, direct hu-
man evaluation scores such as adequacy and flu-
ency, which were widely used in the past, are
now discontinued in favor of ranking-based eval-
uations, where judges are asked to rank the out-
put of 2 to 5 systems instead. It has been shown
that using such ranking-based assessments yields
much higher inter-annotator agreement (Callison-
Burch et al., 2007).
While evaluation metrics still produce numeri-
cal scores, in part because MT evaluation shared
tasks at NIST and WMT ask for it, there has also
been work on a ranking formulation of the MT
evaluation task for a given set of outputs. This
was shown to yield higher correlation with human
judgments (Duh, 2008; Song and Cohn, 2011).
214
Learning automatic metrics in a pairwise set-
ting, i.e., learning to distinguish between two al-
ternative translations and to decide which of the
two is better (which is arguably one of the easiest
ways to produce a ranking), emulates closely how
human judges perform evaluation assessments in
reality. Instead of learning a similarity function
between a translation and the reference, they learn
how to differentiate a better from a worse trans-
lation given a corresponding reference. While the
pairwise setting does not provide an absolute qual-
ity scoring metric, it is useful for most evaluation
and MT development scenarios.
In this paper, we propose a pairwise learning
setting similar to that of Duh (2008), but we extend
it to a new level, both in terms of feature represen-
tation and learning framework. First, we integrate
several layers of linguistic information encapsu-
lated in tree-based structures; Duh (2008) only
used lexical and POS matches as features. Second,
we use information about both the reference and
two alternative translations simultaneously, thus
bringing our ranking closer to how humans rank
translations. Finally, instead of deciding upfront
which types of features between hypotheses and
references are important, we use a our structural
kernel learning (SKL) framework to generate and
select them automatically.
The structural kernel learning (SKL) framework
we propose consists in: (i) designing a struc-
tural representation, e.g., using syntactic and dis-
course trees of translation hypotheses and a refer-
ences; and (ii) applying structural kernels (Mos-
chitti, 2006; Moschitti, 2008), to such representa-
tions in order to automatically inject structural fea-
tures in the preference re-ranking algorithm. We
use this method with translation-reference pairs
to directly learn the features themselves, instead
of learning the importance of a predetermined set
of features. A similar learning framework has
been proven to be effective for question answer-
ing (Moschitti et al., 2007), and textual entailment
recognition (Zanzotto and Moschitti, 2006).
Our goals are twofold: (i) in the short term, to
demonstrate that structural kernel learning is suit-
able for this task, and can effectively learn to rank
hypotheses at the segment-level; and (ii) in the
long term, to show that this approach provides a
unified framework that allows to integrate several
layers of linguistic analysis and information and to
improve over the state-of-the-art.
Below we report the results of some initial ex-
periments using syntactic and discourse structures.
We show that learning in the proposed framework
yields better correlation with humans than apply-
ing the traditional translation?reference similarity
metrics using the same type of structures. We
also show that the contributions of syntax and dis-
course information are cumulative. Finally, de-
spite the limited information we use, we achieve
correlation at the segment level that outperforms
BLEU and other metrics at WMT12, e.g., our met-
ric would have been ranked higher in terms of cor-
relation with human judgments compared to TER,
NIST, and BLEU in the WMT12 Metrics shared
task (Callison-Burch et al., 2012).
2 Kernel-based Learning from Linguistic
Structures
In our pairwise setting, each sentence s in
the source language is represented by a tuple
?t
1
, t
2
, r?, where t
1
and t
2
are two alternative
translations and r is a reference translation. Our
goal is to develop a classifier of such tuples that
decides whether t
1
is a better translation than t
2
given the reference r.
Engineering features for deciding whether t
1
is
a better translation than t
2
is a difficult task. Thus,
we rely on the automatic feature extraction en-
abled by the SKL framework, and our task is re-
duced to choosing: (i) a meaningful structural rep-
resentation for ?t
1
, t
2
, r?, and (ii) a feature func-
tion ?
mt
that maps such structures to substruc-
tures, i.e., our feature space. Since the design
of ?
mt
is complex, we use tree kernels applied
to two simpler structural mappings ?
M
(t
1
, r) and
?
M
(t
2
, r). The latter generate the tree representa-
tions for the translation-reference pairs (t
1
, r) and
(t
2
, r). The next section shows such mappings.
2.1 Representations
To represent a translation-reference pair (t, r), we
adopt shallow syntactic trees combined with RST-
style discourse trees. Shallow trees have been
successfully used for question answering (Severyn
and Moschitti, 2012) and semantic textual sim-
ilarity (Severyn et al., 2013b); while discourse
information has proved useful in MT evaluation
(Guzm?an et al., 2014; Joty et al., 2014). Com-
bined shallow syntax and discourse trees worked
well for concept segmentation and labeling (Saleh
et al., 2014a).
215
DIS:ELABORATION
EDU:NUCLEUS EDU:SATELLITE-REL
VP NP-REL NP VP-REL o-REL o-REL
RB TO-REL VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL ''-REL
not to give them the time to think . "
VP NP-REL NP VP-REL o-REL o-REL
TO-REL `` VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL ''-REL
to " give them no time to think . "
a) Hypothesis
b) Reference DIS:ELABORATION
EDU:NUCLEUS EDU:SATELLITE-REL
Bag-of-words relations 
rela
tion
 pro
pag
atio
n di
rect
ion
Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS.
Figure 1 shows two example trees combining
discourse, shallow syntax and POS: one for a
translation hypothesis (top) and the other one for
the reference (bottom). To build such structures,
we used the Stanford POS tagger (Toutanova et
al., 2003), the Illinois chunker (Punyakanok and
Roth, 2001), and the discourse parser
1
of (Joty et
al., 2012; Joty et al., 2013).
The lexical items constitute the leaves of the
tree. The words are connected to their respec-
tive POS tags, which are in turn grouped into
chunks. Then, the chunks are grouped into el-
ementary discourse units (EDU), to which the
nuclearity status is attached (i.e., NUCLEUS or
SATELLITE). Finally, EDUs and higher-order dis-
course units are connected by discourse relations
(e.g., DIS:ELABORATION).
2.2 Kernels-based modeling
In the SKL framework, the learning objects are
pairs of translations ?t
1
, t
2
?. Our objective is to
automatically learn which pair features are impor-
tant, independently of the source sentence. We
achieve this by using kernel machines (KMs) over
two learning objects ?t
1
, t
2
?, ?t
?
1
, t
?
2
?, along with
an explicit and structural representation of the
pairs (see Fig. 1).
1
The discourse parser can be downloaded from
http://alt.qcri.org/tools/
More specifically, KMs carry out learning using
the scalar product
K
mt
(?t
1
, t
2
?, ?t
?
1
, t
?
2
?) = ?
mt
(t
1
, t
2
) ??
mt
(t
?
1
, t
?
2
),
where ?
mt
maps pairs into the feature space.
Considering that our task is to decide whether
t
1
is better than t
2
, we can conveniently rep-
resent the vector for the pair in terms of the
difference between the two translation vectors,
i.e., ?
mt
(t
1
, t
2
) = ?
K
(t
1
) ? ?
K
(t
2
). We can
approximate K
mt
with a preference kernel PK to
compute this difference in the kernel space K:
PK(?t
1
, t
2
?, ?t
?
1
, t
?
2
?) (1)
= K(t
1
)? ?
K
(t
2
)) ? (?
K
(t
?
1
)? ?
K
(t
?
2
))
= K(t
1
, t
?
1
) +K(t
2
, t
?
2
)?K(t
1
, t
?
2
)?K(t
2
, t
?
1
)
The advantage of this is that now K(t
i
, t
?
j
) =
?
K
(t
i
) ? ?
K
(t
?
j
) is defined between two transla-
tions only, and not between two pairs of transla-
tions. This simplification enables us to map trans-
lations into simple trees, e.g., those in Figure 1,
and then to apply them tree kernels, e.g., the Par-
tial Tree Kernel (Moschitti, 2006), which carry out
a scalar product in the subtree space.
We can further enrich the representation ?
K
, if
we consider all the information available to the
human judges when they are ranking translations.
That is, the two alternative translations along with
their corresponding reference.
216
In particular, let r and r
?
be the references for
the pairs ?t
1
, t
2
? and ?t
?
1
, t
?
2
?, we can redefine all
the members of Eq. 1, e.g., K(t
1
, t
?
1
) becomes
K(?t
1
, r?, ?t
?
1
, r
?
?) = PTK(?
M
(t
1
, r), ?
M
(t
?
1
, r
?
))
+ PTK(?
M
(r, t
1
), ?
M
(r
?
, t
?
1
)),
where ?
M
maps a pair of texts to a single tree.
There are several options to produce the bitext-
to-tree mapping for ?
M
. A simple approach is
to only use the tree corresponding to the first ar-
gument of ?
M
. This leads to the basic model
K(?t
1
, r?, ?t
?
1
, r
?
?) = PTK(?
M
(t
1
), ?
M
(t
?
1
)) +
PTK(?
M
(r), ?
M
(r
?
)), i.e., the sum of two tree
kernels applied to the trees constructed by ?
M
(we
previously informally mentioned it).
However, this simple mapping may be ineffec-
tive since the trees within a pair, e.g., (t
1
, r), are
treated independently, and no meaningful features
connecting t
1
and r can be derived from their
tree fragments. Therefore, we model ?
M
(r, t
1
) by
using word-matching relations between t
1
and r,
such that connections between words and con-
stituents of the two trees are established using
position-independent word matching. For exam-
ple, in Figure 1, the thin dashed arrows show the
links connecting the matching words between t
1
and r. The propagation of these relations works
from the bottom up. Thus, if all children in a con-
stituent have a link, their parent is also linked.
The use of such connections is essential as it en-
ables the comparison of the structural properties
and relations between two translation-reference
pairs. For example, the tree fragment [ELABORA-
TION [SATELLITE]] from the translation is con-
nected to [ELABORATION [SATELLITE]] in the
reference, indicating a link between two entire dis-
course units (drawn with a thicker arrow), and pro-
viding some reliability to the translation
2
.
Note that the use of connections yields a graph
representation instead of a tree. This is problem-
atic as effective models for graph kernels, which
would be a natural fit to this problem, are not cur-
rently available for exploiting linguistic informa-
tion. Thus, we simply use K, as defined above,
where the mapping ?
M
(t
1
, r) only produces a tree
for t
1
annotated with the marker REL represent-
ing the connections to r. This marker is placed on
all node labels of the tree generated from t
1
that
match labels from the tree generated from r.
2
Note that a non-pairwise model, i.e., K(t
1
, r), could
also be used to match the structural information above, but
it would not learn to compare it to a second pair (t
2
, r).
In other words, we only consider the trees en-
riched by markers separately, and ignore the edges
connecting both trees.
3 Experiments and Discussion
We experimented with datasets of segment-level
human rankings of system outputs from the
WMT11 and the WMT12 Metrics shared tasks
(Callison-Burch et al., 2011; Callison-Burch et al.,
2012): we used the WMT11 dataset for training
and the WMT12 dataset for testing. We focused
on translating into English only, for which the
datasets can be split by source language: Czech
(cs), German (de), Spanish (es), and French (fr).
There were about 10,000 non-tied human judg-
ments per language pair per dataset. We scored
our pairwise system predictions with respect to
the WMT12 human judgments using the Kendall?s
Tau (? ), which was official at WMT12.
Table 1 presents the ? scores for all metric vari-
ants introduced in this paper: for the individual
language pairs and overall. The left-hand side of
the table shows the results when using as sim-
ilarity the direct kernel calculation between the
corresponding structures of the candidate transla-
tion and the reference
3
, e.g., as in (Guzm?an et al.,
2014; Joty et al., 2014). The right-hand side con-
tains the results for structured kernel learning.
We can make the following observations:
(i) The overall results for all SKL-trained metrics
are higher than the ones when applying direct sim-
ilarity, showing that learning tree structures is bet-
ter than just calculating similarity.
(ii) Regarding the linguistic representation, we see
that, when learning tree structures, syntactic and
discourse-based trees yield similar improvements
with a slight advantage for the former. More in-
terestingly, when both structures are put together
in a combined tree, the improvement is cumula-
tive and yields the best results by a sizable margin.
This provides positive evidence towards our goal
of a unified tree-based representation with multi-
ple layers of linguistic information.
(iii) Comparing to the best evaluation metrics
that participated in the WMT12 Metrics shared
task, we find that our approach is competitive and
would have been ranked among the top 3 partici-
pants.
3
Applying tree kernels between the members of a pair to
generate one feature (for each different kernel function) has
become a standard practice in text similarity tasks (Severyn et
al., 2013b) and in question answering (Severyn et al., 2013a).
217
Similarity Structured Kernel Learning
Structure cs-en de-en es-en fr-en all cs-en de-en es-en fr-en all
1 SYN 0.169 0.188 0.203 0.222 0.195 0.190 0.244 0.198 0.158 0.198
2 DIS 0.130 0.174 0.188 0.169 0.165 0.176 0.235 0.166 0.160 0.184
3 DIS+POS 0.135 0.186 0.190 0.178 0.172 0.167 0.232 0.202 0.133 0.183
4 DIS+SYN 0.156 0.205 0.206 0.203 0.192 0.210 0.251 0.240 0.223 0.231
Table 1: Kendall?s (? ) correlation with human judgements on WMT12 for each language pair.
Furthermore, our result (0.237) is ahead of the
correlation obtained by popular metrics such as
TER (0.217), NIST (0.214) and BLEU (0.185) at
WMT12. This is very encouraging and shows the
potential of our new proposal.
In this paper, we have presented only the first
exploratory results. Our approach can be easily
extended with richer linguistic structures and fur-
ther combined with some of the already existing
strong evaluation metrics.
Testing
Train cs-en de-en es-en fr-en all
1 cs-en 0.210 0.204 0.217 0.204 0.209
2 de-en 0.196 0.251 0.203 0.202 0.213
3 es-en 0.218 0.204 0.240 0.223 0.221
4 fr-en 0.203 0.218 0.224 0.223 0.217
5 all 0.231 0.258 0.226 0.232 0.237
Table 2: Kendall?s (? ) on WMT12 for cross-
language training with DIS+SYN.
Note that the results in Table 1 were for train-
ing on WMT11 and testing on WMT12 for each
language pair in isolation. Next, we study the im-
pact of the choice of training language pair. Ta-
ble 2 shows cross-language evaluation results for
DIS+SYN: lines 1-4 show results when training on
WMT11 for one language pair, and then testing for
each language pair of WMT12.
We can see that the overall differences in perfor-
mance (see the last column: all) when training on
different source languages are rather small, rang-
ing from 0.209 to 0.221, which suggests that our
approach is quite independent of the source lan-
guage used for training. Still, looking at individ-
ual test languages, we can see that for de-en and
es-en, it is best to train on the same language; this
also holds for fr-en, but there it is equally good
to train on es-en. Interestingly, training on es-en
improves a bit for cs-en.
These somewhat mixed results have motivated
us to try tuning on the full WMT11 dataset; as line
5 shows, this yielded improvements for all lan-
guage pairs except for es-en. Comparing to line
4 in Table 1, we see that the overall Tau improved
from 0.231 to 0.237.
4 Conclusions and Future Work
We have presented a pairwise learning-to-rank ap-
proach to MT evaluation, which learns to differen-
tiate good from bad translations in the context of
a given reference. We have integrated several lay-
ers of linguistic information (lexical, syntactic and
discourse) in tree-based structures, and we have
used the structured kernel learning to identify rel-
evant features and learn pairwise rankers.
The evaluation results have shown that learning
in the proposed SKL framework is possible, yield-
ing better correlation (Kendall?s ? ) with human
judgments than computing the direct kernel sim-
ilarity between translation and reference, over the
same type of structures. We have also shown that
the contributions of syntax and discourse informa-
tion are cumulative, indicating that this learning
framework can be appropriate for the combination
of different sources of information. Finally, de-
spite the limited information we used, we achieved
better correlation at the segment level than BLEU
and other metrics in the WMT12 Metrics task.
In the future, we plan to work towards our long-
term goal, i.e., including more linguistic informa-
tion in the SKL framework and showing that this
can help. This would also include more semantic
information, e.g., in the form of Brown clusters or
using semantic similarity between the words com-
posing the structure calculated with latent seman-
tic analysis (Saleh et al., 2014b).
We further want to show that the proposed
framework is flexible and can include information
in the form of quality scores predicted by other
evaluation metrics, for which a vector of features
would be combined with the structured kernel.
Acknowledgments
This research is part of the Interactive sYstems
for Answer Search (Iyas) project, conducted by
the Arabic Language Technologies (ALT) group
at Qatar Computing Research Institute (QCRI)
within the Qatar Foundation.
218
References
Joshua Albrecht and Rebecca Hwa. 2008. Regression
for machine translation evaluation at the sentence
level. Machine Translation, 22(1-2):1?27.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical
Machine Translation, WMT ?07, pages 136?158,
Prague, Czech Republic.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, WMT ?11, pages 22?64, Edin-
burgh, Scotland, UK.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, WMT
?12, pages 10?51, Montr?eal, Canada.
Elisabet Comelles, Jes?us Gim?enez, Llu??s M`arquez,
Irene Castell?on, and Victoria Arranz. 2010.
Document-level automatic MT evaluation based on
discourse representations. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, WMT ?10, pages 333?
338, Uppsala, Sweden.
Kevin Duh. 2008. Ranking vs. regression in machine
translation evaluation. In Proceedings of the Third
Workshop on Statistical Machine Translation, WMT
?08, pages 191?194, Columbus, Ohio, USA.
Jes?us Gim?enez and Llu??s M`arquez. 2007. Linguis-
tic features for automatic evaluation of heterogenous
MT systems. In Proceedings of the Second Work-
shop on Statistical Machine Translation, WMT ?07,
pages 256?264, Prague, Czech Republic.
Francisco Guzm?an, Shafiq Joty, Llu??s M`arquez, and
Preslav Nakov. 2014. Using discourse structure
improves machine translation evaluation. In Pro-
ceedings of 52nd Annual Meeting of the Association
for Computational Linguistics, ACL ?14, pages 687?
698, Baltimore, Maryland, USA.
Shafiq Joty, Giuseppe Carenini, and Raymond Ng.
2012. A Novel Discriminative Framework for
Sentence-Level Discourse Analysis. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 904?915, Jeju Island, Korea.
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining Intra- and
Multi-sentential Rhetorical Parsing for Document-
level Discourse Analysis. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, ACL ?13, pages 486?496, Sofia,
Bulgaria.
Shafiq Joty, Francisco Guzm?an, Llu??s M`arquez, and
Preslav Nakov. 2014. DiscoTK: Using discourse
structure for machine translation evaluation. In Pro-
ceedings of the Ninth Workshop on Statistical Ma-
chine Translation, WMT ?14, pages 402?408, Balti-
more, Maryland, USA.
Alon Lavie and Michael Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2?3):105?115.
Ding Liu and Daniel Gildea. 2005. Syntactic fea-
tures for evaluation of machine translation. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 25?32, Ann Ar-
bor, Michigan, USA.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
2012. Fully automatic semantic MT evaluation. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, WMT ?12, pages 243?252,
Montr?eal, Canada.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for ques-
tion answer classification. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, ACL ?07, pages 776?783, Prague,
Czech Republic.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of 17th European Conference on Ma-
chine Learning and the 10th European Conference
on Principles and Practice of Knowledge Discovery
in Databases, ECML/PKDD ?06, pages 318?329,
Berlin, Germany.
Alessandro Moschitti. 2008. Kernel methods, syn-
tax and semantics for relational text categorization.
In Proceedings of the 17th ACM Conference on In-
formation and Knowledge Management, CIKM ?08,
pages 253?262, Napa Valley, California, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meting of the Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Philadelphia, Pennsylvania, USA.
Maja Popovi?c and Hermann Ney. 2007. Word error
rates: Decomposition over POS classes and applica-
tions for error analysis. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
WMT ?07, pages 48?55, Prague, Czech Republic.
Vasin Punyakanok and Dan Roth. 2001. The use of
classifiers in sequential inference. In Advances in
Neural Information Processing Systems 14, NIPS
?01, pages 995?1001, Vancouver, Canada.
219
Iman Saleh, Scott Cyphers, Jim Glass, Shafiq Joty,
Llu??s M`arquez, Alessandro Moschitti, and Preslav
Nakov. 2014a. A study of using syntactic and se-
mantic structures for concept segmentation and la-
beling. In Proceedings of the 25th International
Conference on Computational Linguistics, COLING
?14, pages 193?202, Dublin, Ireland.
Iman Saleh, Alessandro Moschitti, Preslav Nakov,
Llu??s M`arquez, and Shafiq Joty. 2014b. Semantic
kernels for semantic parsing. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?14, Doha, Qatar.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In Proceedings of the 35th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ?12,
pages 741?750, Portland, Oregon, USA.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013a. Learning adaptable patterns for
passage reranking. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ?13, pages 75?83, Sofia,
Bulgaria.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013b. Learning semantic textual sim-
ilarity with structural representations. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), ACL ?13, pages 714?718, Sofia, Bulgaria.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference of the
Association for Machine Translation in the Ameri-
cas, AMTA ?06, Cambridge, Massachusetts, USA.
Xingyi Song and Trevor Cohn. 2011. Regression and
ranking based optimisation for sentence-level MT
evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, WMT ?11, pages
123?129, Edinburgh, Scotland, UK.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, HLT-NAACL ?03, pages 173?180, Ed-
monton, Canada.
Billy Wong and Chunyu Kit. 2012. Extending ma-
chine translation evaluation metrics with lexical co-
hesion to document level. In Proceedings of the
2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 1060?1068, Jeju Island, Korea.
Fabio Massimo Zanzotto and Alessandro Moschitti.
2006. Automatic learning of textual entailments
with cross-pair similarities. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics, COLING-ACL
?06, pages 401?408, Sydney, Australia.
220
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 436?442,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Semantic Kernels for Semantic Parsing
Iman Saleh
Faculty of Computers and Information
Cairo University
iman.saleh@fci-cu.edu.eg
Alessandro Moschitti, Preslav Nakov,
Llu??s M
`
arquez, Shafiq Joty
ALT Research Group
Qatar Computing Research Institute
{amoschitti,pnakov,lmarquez,sjoty}@qf.org.qa
Abstract
We present an empirical study on the use
of semantic information for Concept Seg-
mentation and Labeling (CSL), which is
an important step for semantic parsing.
We represent the alternative analyses out-
put by a state-of-the-art CSL parser with
tree structures, which we rerank with a
classifier trained on two types of seman-
tic tree kernels: one processing structures
built with words, concepts and Brown
clusters, and another one using semantic
similarity among the words composing the
structure. The results on a corpus from the
restaurant domain show that our semantic
kernels exploiting similarity measures out-
perform state-of-the-art rerankers.
1 Introduction
Spoken Language Understanding aims to inter-
pret user utterances and to convert them to logical
forms or, equivalently, to database queries, which
can then be used to satisfy the user?s information
needs. This process is known as Concept Segmen-
tation and Labeling (CSL), also called semantic
parsing in the speech community: it maps utter-
ances into meaning representations based on se-
mantic constituents. The latter are basically word
sequences, often referred to as concepts, attributes
or semantic tags. CSL makes it easy to convert
spoken questions such as ?cheap lebanese restau-
rants in doha with take out? into database queries.
First, a language-specific semantic parser tok-
enizes, segments and labels the question:
[
Price
cheap] [
Cuisine
lebanese] [
Other
restaurants in]
[
City
doha] [
Other
with] [
Amenity
take out]
Then, label-specific normalizers are applied to
the segments, with the option to possibly relabel
mislabeled segments:
[
Price
low] [
Cuisine
lebanese] [
City
doha] [
Amenity
carry out]
Finally, a database query is formed from the list
of labels and values, and is then executed against
the database, e.g., MongoDB; a backoff mecha-
nism may be used if the query has not succeeded.
{$and [{cuisine:"lebanese"},{city:"doha"},
{price:"low"},{amenity:"carry out"}]}
The state-of-the-art of CSL is represented by
conditional models for sequence labeling such as
Conditional Random Fields (CRFs) (Lafferty et
al., 2001) trained with simple morphological and
lexical features. The basic CRF model was im-
proved by means of reranking (Moschitti et al.,
2006; Dinarelli et al., 2012) using structural ker-
nels (Moschitti, 2006). Although these meth-
ods exploited sentence structure, they did not use
syntax at all. More recently, we applied shal-
low syntactic structures and discourse parsing with
slightly better results (Saleh et al., 2014). How-
ever, the most obvious models for semantic pars-
ing, i.e., rerankers based on semantic structural
kernels (Bloehdorn and Moschitti, 2007b), had not
been applied to semantic structures yet.
In this paper, we study the impact of semantic
information conveyed by Brown Clusters (BCs)
(Brown et al., 1992) and semantic similarity, while
also combining them with innovative features. We
use reranking, similarly to (Saleh et al., 2014),
to select the best hypothesis annotated with con-
cepts predicted by a local model. The competing
hypotheses are represented as innovative trees en-
riched with the semantic concepts and BC labels.
The trees can capture dependencies between sen-
tence constituents, concepts and BCs. However,
extracting explicit features from them is rather
difficult as their number is exponentially large.
Thus, we rely on (i) Support Vector Machines
(Joachims, 1999) to train the reranking functions
and on (ii) structural kernels (Moschitti, 2010;
Moschitti, 2012; Moschitti, 2013) to automatically
encode tree fragments that represent syntactic and
semantic dependencies from words and concepts.
436
(a) Semantic Kernel Structure (SKS)
(b) SKS with Brown Clusters
Figure 1: CSL structures: standard and with Brown Clusters.
We further apply a semantic kernel (SK),
namely the Smoothed Partial Tree Kernel (Croce
et al., 2011), which uses the lexical similarity be-
tween the tree nodes, while computing the sub-
structure space. This is the first time that SKs are
applied to reranking hypotheses. This (i) makes
the global sentence structure along with concepts
available to the learning algorithm, and (ii) enables
computing the similarity between lexicals in syn-
tactic patterns that are enriched by concepts.
We tested our models on the Restaurant do-
main. Our results show that: (i) The basic CRF
parser, which uses semi-Markov CRF, or semi-
CRF (Sarawagi and Cohen, 2004), is already very
accurate; it achieves F
1
scores over 83%, mak-
ing any further improvement very hard. (ii) The
upper-bound performance of the reranker is very
high as well, i.e., the correct annotation is gen-
erated in the list of the first 100 hypotheses in
98.72% of the cases. (iii) SKs significantly im-
prove over the semi-CRF baseline and our pre-
vious state-of-the-art reranker exploiting shallow
syntactic patterns (Saleh et al., 2014), as shown
by extensive comparisons using several systems.
(iv) Making BCs effective requires a deeper study.
2 Related Work
One of the early approaches to CSL was that
of Pieraccini et al. (1991), where the word se-
quences and concepts were modeled using Hid-
den Markov Models (HMMs) as observations and
hidden states, respectively. Generative models
were exploited by Seneff (1989) and Miller et
al. (1994), who used stochastic grammars for
CSL. Other discriminative models followed such
preliminary work, e.g., (Rubinstein and Hastie,
1997; Santaf?e et al., 2007; Raymond and Riccardi,
2007). CRF-based models are considered to be the
state of the art in CSL (De Mori et al., 2008).
Another relevant line of research are the seman-
tic kernels, i.e., kernels that use lexical similarity
between features. One of the first that applyed
LSA was (Cristianini et al., 2002), whereas (Bloe-
hdorn et al., 2006; Basili et al., 2006) used Word-
Net. Semantic structural kernels of the type we
use in this paper were first introduced in (Bloe-
hdorn and Moschitti, 2007a; Bloehdorn and Mos-
chitti, 2007b). The most advanced model based on
tree kernels, which we also use in this paper, is the
Smoothed PTK (Croce et al., 2011).
3 Reranking for CSL
Reranking is applied to a list of N annotation hy-
potheses, which are generated and sorted by the
probability to be globally correct as estimated us-
ing local classifiers or global classifiers that only
use local features. Then, a reranker, typically a
meta-classifier, tries to select the best hypothe-
sis from the list. The reranker can exploit global
information, and specifically, the dependencies
between the different concepts, which are made
available by the local model. We use semi-CRFs
for the local model as they yield the highest ac-
curacy in CSL (when using a single model) and
preference reranking for the global reranker.
3.1 Preference Reranking (PR)
PR uses a classifier C, which takes a pair of hy-
potheses ?H
i
, H
j
? and decides whether H
i
is bet-
ter than H
j
. Given a training question Q, posi-
tive and negative examples are built for training
the classifier. Let H
1
be the hypothesis with the
lowest error rate with respect to the gold standard
among all hypotheses generated for question Q.
We adopt the following approach for example gen-
eration: the pairs ?H
1
, H
i
? (i = 2, 3, . . . , N ) are
positive examples, while ?H
i
, H
1
? are considered
negative.
437
At testing time, given a new question Q
?
, C clas-
sifies all pairs ?H
i
, H
j
? generated from the anno-
tation hypotheses of Q
?
: a positive classification is
a vote for H
i
, otherwise the vote is for H
j
, where
the classifier score can be used as a weighted vote.
H
k
are then ranked according to the number (sum)
of the votes (weighted by score) they receive.
We build our reranker with SVMs using the
following kernel: K(?H
1
, H
2
?, ?H
?
1
, H
?
2
?) =
?(?H
1
, H
2
?) ? ?(?H
?
1
, H
?
2
?) ,
(
?(H
1
) ?
?(H
2
)
)
?
(
?(H
?
1
) ? ?(H
?
2
)
)
= ?(H
1
)?(H
?
1
) +
?(H
2
)?(H
?
2
) ? ?(H
1
)?(H
?
2
) ? ?(H
2
)?(H
?
1
) =
S(H
1
, H
?
1
) + S(H
2
, H
?
2
) ? S(H
1
, H
?
2
) ?
S(H
2
, H
?
1
). We consider H as a tuple ?T,~v? com-
posed of a tree T and a feature vector ~v. Then, we
define S(H,H
?
) = S
TK
(T, T
?
)+S
v
(~v,~v
?
), where
S
TK
computes one of the tree kernel functions
defined in 3.2 and 3.3; and S
v
is a kernel (see 3.4),
e.g., linear, polynomial, Gaussian, etc.
3.2 Tree kernels (TKs)
TKs measure the similarity between two structures
in terms of the number of substructures they share.
We use two types of tree kernels: (i) Partial Tree
Kernel (PTK), which can be effectively applied
to both constituency and dependency parse trees
(Moschitti, 2006). It generates all possible con-
nected tree fragments, e.g., sibling nodes can be
also separated and can be part of different tree
fragments: a fragment is any possible tree path,
and other tree paths are allowed to depart from its
nodes. Thus, it can generate a very rich feature
space. (ii) The smoothed PTK or semantic kernel
(SK) (Croce et al., 2011), which extends PTK by
allowing soft matching (i.e., via similarity compu-
tation) between nodes associated with different but
related lexical items. The node similarity can be
derived from manually annotated resources, e.g.,
WordNet or Wikipedia, as well as using corpus-
based clustering approaches, e.g., latent semantic
analysis (LSA), as we do in this paper.
3.3 Semantic structures
Tree kernels allow us to compute structural simi-
larities between two trees; thus, we engineered a
special structure for the CSL task. In order to cap-
ture the structural dependencies between the se-
mantic tags,
1
we use a basic tree (see for exam-
ple Figure 1a), where the words of a sentence are
tagged with their semantic tags.
1
They are associated with the following IDs: 0-Other,
1-Rating, 2-Restaurant, 3-Amenity, 4-Cuisine, 5-Dish, 6-
Hours, 7-Location, and 8-Price.
More specifically, the words in the sentence
constitute the leaves of the tree, which are in
turn connected to the pre-terminals containing
the semantic tags in BIO notation (?B?=begin,
?I?=inside, ?O?=outside). The BIO tags are then
generalized in the upper level, and joined to the
Root node. Additionally, part-of-speech (POS)
tags
2
are added to each word by concatenating
it with the string ?::L?, where L is the first let-
ter of the POS-tags of the words, e.g., along, my
and route, receive i, p and n, which are the first
letters of the POS-tags IN, PRN and NN, respec-
tively. SK applied to the above structure can gen-
erate powerful semantic patterns such as [Root
[4-Cuisine [similar to(stake house)]][7-Loc [simi-
lar to(within a mile)]]], e.g., for correctly labeling
new clauses like Pizza Parlor in three kilometers.
The BC labels, represented as cluster IDs, are sim-
ply added as siblings of words as shown in Fig. 1b.
3.4 Feature Vectors
For the sake of comparison, we also devoted
some effort towards engineering a set of features
to be used in a flat feature-vector representation.
These features can be used in isolation to learn
the reranking function, or in combination with the
kernel-based approach (as a composite kernel us-
ing a linear combination). They belong to the fol-
lowing four categories: (i) CRF-based: these in-
clude the basic features used to train the initial
semi-CRF model; (ii) n-gram based: we collected
3- and 4-grams of the output label sequence at
the level of concepts, with artificial tags inserted
to identify the start (?S?) and end (?E?) of the se-
quence.
3
(iii) Probability-based, computing the
probability of the label sequence as an average of
the probabilities at the word level in the N -best
list; and (iv) DB-based: a single feature encoding
the number of results returned from the database
when constructing a query using the conjunction
of all semantic segments in the hypothesis.
4 Experiments
The experiments aim at investigating the role of
feature vectors, PTK, SK and BCs in reranking.
We first describe the experimental setting and then
we move into the analysis of the results.
2
We use the Stanford tagger (Toutanova et al., 2003).
3
For instance, if the output sequence is Other-Rating-
Other-Amenity the 3-gram patterns would be: S-Other-
Rating, Other-Rating-Other, Rating-Other-Amenity, and
Other-Amenity-E.
438
Train Devel. Test Total
semi-CRF 6,922 739 1,521 9,182
Reranker 7,000 3,695 7,605 39,782
Table 1: Number of instances and pairs used to
train the semi-CRF and rerankers, respectively.
4.1 Experimental setup
Dataset. In our experiments, we used questions
annotated with semantic tags, which were col-
lected through crowdsourcing on Amazon Me-
chanical Turk and made available
4
by McGraw et
al. (2012). We split the dataset into training, de-
velopment and test sets. Table 1 shows the num-
ber of examples and example pairs we used for
the semi-CRF and the reranker, respectively. We
subsequently split the training data randomly into
10 folds. We used cross-validation, i.e., iteratively
training with 9 folds and annotating the remaining
fold, in order to generate the N -best lists of hy-
potheses for the entire training dataset. We com-
puted the 100-best hypotheses for each example.
We then used the development dataset to test and
tune the hyper-parameters of our reranking model.
The results on the development set, which we will
present in Section 4.2 below, were obtained us-
ing semi-CRF and reranking models trained on the
training set.
Data representation. Each hypothesis is repre-
sented by a semantic tree, a feature vector (ex-
plained in Section 3), and two extra features:
(i) the semi-CRF probability of the hypothesis,
and (ii) its reciprocal rank in the N -best list.
Learning algorithm. We used the SVM-Light-
TK
5
to train the reranker with a combination of
tree kernels and feature vectors (Moschitti, 2006;
Joachims, 1999). We used the default parameters
and a linear kernel for the feature vectors. As a
baseline, we picked the best-scoring hypothesis in
the list, i.e., the output by the regular semi-CRF
parser. The setting is exactly the same as that de-
scribed in (Saleh et al., 2014).
Evaluation measure. In all experiments, we used
the harmonic mean of precision and recall (F
1
)
(van Rijsbergen, 1979), computed at the token
level and micro-averaged across the different se-
mantic types.
6
4
http://groups.csail.mit.edu/sls/downloads/restaurant/
5
http://disi.unitn.it/moschitti/Tree-Kernel.htm
6
We do not consider ?Other? to be a semantic type; thus,
we did not include it in the F
1
calculation.
N 1 2 5 10 100
F
1
83.03 87.76 92.63 95.23 98.72
Table 2: Oracle F
1
score for N -best lists.
Brown Clusters. Clustering groups of similar
words together provides a way of generalizing
them. In this work, we explore the use of Brown
clusters (Brown et al., 1992) in both feature vec-
tors and tree kernels. The Brown clustering al-
gorithm uses an n-gram class model. It first as-
signs each word to a distinct cluster, and then it
merges different clusters in a bottom-up fashion.
The merge step is done in a way that minimizes the
loss in average mutual information between clus-
ters. The outcome is hierarchical clustering, which
we use in our reranking algorithm. To create the
Brown clusters, we used the Yelp dataset of re-
views.
7
It contains 335,022 reviews about 15,585
businesses; 5,575 of the businesses and 233,839 of
the reviews are restaurant-related. This dataset is
very similar to the dataset of queries about restau-
rants we use in our experiments.
Similarity matrix for SK. We compute the lexi-
cal similarity for SK by applying LSA (Furnas et
al., 1988) to Tripadvisor data. The dataset and the
exact procedure for creating the LSA matrix are
described in (Castellucci et al., 2013; Croce and
Previtali, 2010).
4.2 Results
Oracle accuracy. Table 2 shows the oracle F
1
score for N -best lists of different lengths, i.e., the
F
1
that is achieved by picking the best candidate
in the N -best list for various values of N . Con-
sidering 5-best lists yields an increase in oracle F
1
of almost ten absolute points. Going up to 10-best
lists only adds 2.5 extra F
1
points. The complete
100-best lists add 3.5 extra F
1
points, for a total
of 98.72. This very high value is explained by the
fact that often the total number of different anno-
tations for a given question is smaller than 100. In
our experiments, we will focus on 5-best lists.
Baseline accuracy. We computed F
1
for the semi-
CRF model on both the development and the test
sets, obtaining 83.86 and 83.03, respectively.
Learning Curves. The semantic information in
terms of BCs or semantic similarity derived by
LSA can have a major impact in case of data
scarcity. Therefore, we trained our reranking mod-
els with increasing sizes of training data.
7
http://www.yelp.com/dataset challenge/
439
Development set
79	 ?
80	 ?
81	 ?
82	 ?
83	 ?
84	 ?
85	 ?
86	 ?
0	 ? 1000	 ? 2000	 ? 3000	 ? 4000	 ? 5000	 ? 6000	 ? 7000	 ?
F1-??m
easu
re	 ?
Training	 ?data	 ?size	 ?
PTK	 ? SK	 ?PTK+BC	 ? PTK+all	 ?PTK+BC+all	 ? Baseline	 ?
79	 ?
80	 ?
81	 ?
82	 ?
83	 ?
84	 ?
85	 ?
86	 ?
0	 ? 1000	 ? 2000	 ? 3000	 ? 4000	 ? 5000	 ? 6000	 ? 7000	 ?
F1-??m
easu
re	 ?
Training	 ?data	 ?size	 ?
PTK	 ? SK	 ?SK+BC	 ? PTK+all	 ?SK+all	 ? SK+BC+all	 ?Baseline	 ?
Test set
74	 ?
76	 ?
78	 ?
80	 ?
82	 ?
84	 ?
86	 ?
0	 ? 1000	 ? 2000	 ? 3000	 ? 4000	 ? 5000	 ? 6000	 ? 7000	 ?
F1-??m
easu
re	 ?
Training	 ?data	 ?size	 ?
PTK	 ? SK	 ?
PTK+BC	 ? PTK+all	 ?
PTK+BC+all	 ? Baseline	 ?
79	 ?
80	 ?
81	 ?
82	 ?
83	 ?
84	 ?
85	 ?
0	 ? 1000	 ? 2000	 ? 3000	 ? 4000	 ? 5000	 ? 6000	 ? 7000	 ?
F1-??m
easu
re	 ?
Training	 ?data	 ?size	 ?
PTK	 ? SK	 ?SK+BC	 ? PTK+all	 ?SK+all	 ? SK+BC+all	 ?Baseline	 ?
Figure 2: Learning curves for different reranking models on the development and on the testing sets.
The first two graphs in Fig. 2 show the plots
on the development set whereas the last two are
computed on the test set. The reranking models
reported are Baseline, PTK, PTK+BC, PTK+all
(features), PTK+BC+all, SK, SK+BC, SK+all and
SK+BC+all.
8
We can see that: (i) PTK alone, i.e.,
without semantic information, has the lowest ac-
curacy; (ii) BCs do not improve significantly any
model; (iii) SK almost always achieves the high-
est accuracy; (iv) PTK+all (i.e., the model also us-
ing features) improves on PTK, but its accuracy
is lower than for any model using SK, i.e., us-
ing semantic similarity; and (v) all features pro-
vide an initial boost to SK, but as soon as the data
increases, their impact decreases.
5 Conclusion and Future Work
In summary, the learning curves clearly show the
good generalization ability of SK, which improve
the CRF baseline using little data (?3,000). The
semantic kernel significantly improves over the
semi-CRF baseline and our previous state-of-the-
art reranker exploiting shallow syntactic patterns
(Saleh et al., 2014), which corresponds to PTK+all
in the above comparison.
8
Models are split between 2 plots in order to ease reading.
The improvement falls between 1-2 absolute
percent points. This is remarkable as (i) it corre-
sponds to ?10% relative error reduction, and (ii)
the state-of-the-art baseline system is very difficult
to beat, as confirmed by the low impact of tradi-
tional features and BCs. Although the latter can
generalize over concepts and words, their use is
not straightforward, resulting in no improvement.
In the future, we plan to investigate the use of
semantic similarity from distributional and other
sources (Mihalcea et al., 2006; Pad?o and Lapata,
2007), e.g., Wikipedia (Strube and Ponzetto, 2006;
Mihalcea and Csomai, 2007), Wiktionary (Zesch
et al., 2008), WordNet (Pedersen et al., 2004;
Agirre et al., 2009), FrameNet, VerbNet (Shi and
Mihalcea, 2005), BabelNet (Navigli and Ponzetto,
2010), and LSA, and for different domains.
Acknowledgments
This research is part of the Interactive sYstems
for Answer Search (Iyas) project, conducted by
the Arabic Language Technologies (ALT) group
at Qatar Computing Research Institute (QCRI)
within the Qatar Foundation. We would like to
thank Danilo Croce, Roberto Basili and Giuseppe
Castellucci for helping and providing us with the
similarity matrix for the semantic kernels.
440
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19?27, Boulder, Colorado, June.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2006. A semantic kernel to classify texts with
very few training examples. Informatica (Slovenia),
30(2):163?172.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text
classification. In Advances in Information Retrieval
- Proceedings of the 29th European Conference on
Information Retrieval (ECIR 2007), pages 307?318,
Rome, Italy.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels.
In Proceedings of the 16th ACM Conference on
Information and Knowledge Management (CIKM
2007), pages 861?864, Lisbon, Portugal.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa,
and Alessandro Moschitti. 2006. Semantic kernels
for text classification based on topological measures
of feature similarity. In Proceedings of the 6th IEEE
International Conference on Data Mining (ICDM
06), pages 808?812, Hong Kong.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479.
Giuseppe Castellucci, Simone Filice, Danilo Croce,
and Roberto Basili. 2013. UNITOR: Combining
Syntactic and Semantic Kernels for Twitter Senti-
ment Analysis. In Second Joint Conference on Lex-
ical and Computational Semantics (*SEM), Volume
2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
369?374, Atlanta, Georgia, USA.
Nello Cristianini, John Shawe-Taylor, and Huma
Lodhi. 2002. Latent Semantic Kernels. Journal
of Intelligent Information Systems, 18(2):127?152.
Danilo Croce and Daniele Previtali. 2010. Mani-
fold learning for the semi-supervised induction of
framenet predicates: An empirical investigation. In
Proceedings of the 2010 Workshop on GEometrical
Models of Natural Language Semantics, pages 7?16,
Uppsala, Sweden.
Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Structured lexical similarity via con-
volution kernels on dependency trees. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing, pages 1034?1046,
Edinburgh, Scotland, UK.
Renato De Mori, Frederic B?echet, Dilek Hakkani-T?ur,
Michael McTear, Giuseppe Riccardi, and Gokhan
Tur. 2008. Spoken Language Understanding. IEEE
Signal Processing Magazine, 25:50?58.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe
Riccardi. 2012. Discriminative reranking for
spoken language understanding. IEEE Transac-
tions on Audio, Speech and Language Processing,
20(2):526?539.
G. W. Furnas, S. Deerwester, S. T. Dumais, T. K. Lan-
dauer, R. A. Harshman, L. A. Streeter, and K. E.
Lochbaum. 1988. Information retrieval using a sin-
gular value decomposition model of latent semantic
structure. In Proceedings of the 11th annual inter-
national ACM SIGIR conference on Research and
development in information retrieval (SIGIR ?88),
pages 465?480, New York, USA.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Schlkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT Press, Cambridge,
MA, USA.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning (ICML
2001), pages 282?289, Williamstown, MA, USA.
Ian McGraw, Scott Cyphers, Panupong Pasupat,
Jingjing Liu, and Jim Glass. 2012. Automating
crowd-supervised learning for spoken language sys-
tems. In Proceedings of the 13th Annual Conference
of the International Speech Communication Asso-
ciation (INTERSPEECH 2012), pages 2473?2476,
Portland, OR, USA.
Rada Mihalcea and Andras Csomai. 2007. Wikify!
linking documents to encyclopedic knowledge. In
Proceedings of the sixteenth ACM conference on
Conference on information and knowledge manage-
ment (CIKM 2007), pages 233?242, Lisbon, Portu-
gal.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceed-
ings of the 21st National Conference on Artificial In-
telligence - Volume 1 (AAAI 2006), pages 775?780,
Boston, MA, USA.
Scott Miller, Richard Schwartz, Robert Bobrow, and
Robert Ingria. 1994. Statistical Language Process-
ing using Hidden Understanding Models. In Pro-
ceedings of the workshop on Human Language Tech-
nology (HLT 1994), pages 278?282, Plainsboro, NJ,
USA.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Semantic role labeling via tree kernel
441
joint inference. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning
(CoNLL-X), pages 61?68, New York City, USA.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of the 17th European Conference on
Machine Learning (ECML 2006), pages 318?329,
Berlin, Germany.
Alessandro Moschitti. 2010. Kernel engineering
for fast and easy design of natural language ap-
plications. In Coling 2010: Kernel Engineering
for Fast and Easy Design of Natural Language
Applications?Tutorial notes, pages 1?91, Beijing,
China.
Alessandro Moschitti. 2012. State-of-the-art kernels
for natural language processing. In Tutorial Ab-
stracts of ACL 2012, page 2, Jeju Island, Korea.
Alessandro Moschitti. 2013. Kernel-based learning to
rank with syntactic and semantic structures. In Tu-
torial abstracts of the 36th Annual ACM SIGIR Con-
ference, page 1128, Dublin, Ireland.
Roberto Navigli and Simone Paolo Ponzetto. 2010.
Babelnet: Building a very large multilingual seman-
tic network. In Proceedings of the 48th annual meet-
ing of the association for computational linguistics
(ACL 2010), pages 216?225, Uppsala, Sweden.
Sebastian Pad?o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity - measuring the
relatedness of concepts. In HLT-NAACL 2004:
Demonstration Papers, pages 38?41, Boston, Mas-
sachusetts, USA.
Roberto Pieraccini, Esther Levin, and Chin-Hui Lee.
1991. Stochastic Representation of Conceptual
Structure in the ATIS Task. In Proceedings of the
Fourth Joint DARPA Speech and Natural Language
Workshop, pages 121?124, Los Altos, CA, USA.
Christian Raymond and Giuseppe Riccardi. 2007.
Generative and Discriminative Algorithms for Spo-
ken Language Understanding. In Proceedings
of the 8th Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH 2007), pages 1605?1608, Antwerp, Bel-
gium, August.
Y. Dan Rubinstein and Trevor Hastie. 1997. Discrimi-
native vs Informative Learning. In Proceedings of
the Third International Conference on Knowledge
Discovery and Data Mining (KDD-1997), pages 49?
53, Newport Beach, CA, USA.
Iman Saleh, Scott Cyphers, Jim Glass, Shafiq Joty,
Llu??s M`arquez, Alessandro Moschitti, and Preslav
Nakov. 2014. A study of using syntactic and seman-
tic structures for concept segmentation and labeling.
In Proceedings of the 25th International Conference
on Computational Linguistics, COLING ?14, pages
193?202, Dublin, Ireland.
G. Santaf?e, J.A. Lozano, and P. Larra?naga. 2007.
Discriminative vs. Generative Learning of Bayesian
Network Classifiers. In Proceedings of the 9th Euro-
pean Conference on Symbolic and Quantitative Ap-
proaches to Reasoning with Uncertainty (ECSQARU
2007), pages 453?546, Hammamet, Tunisia.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Advances in Neural Information Pro-
cessing Systems 17 (NIPS 2004), Vancouver, British
Columbia, Canada.
Stephanie Seneff. 1989. TINA: A Probabilistic Syn-
tactic Parser for Speech Understanding Systems.
In Proceedings of the International Conference on
Acoustics, Speech, and Signal Processing (ICASSP-
89), pages 711?714, Glasgow, UK.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining framenet, verbnet and wordnet
for robust semantic parsing. In Computational Lin-
guistics and Intelligent Text Processing, pages 100?
111. Springer Berlin Heidelberg.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In Proceedings of the 21st National Con-
ference on Artificial Intelligence (AAAI?06), pages
1419?1424, Boston, Massachusetts, USA.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Human Language Tech-
nology Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics (HLT-NAACL 2003), pages 173?180, Edmon-
ton, Canada.
Cornelis J. van Rijsbergen. 1979. Information
Retrieval. Butterworth-Heinemann Newton, MA,
USA.
Torsten Zesch, Christof M?uller, and Iryna Gurevych.
2008. Using wiktionary for computing semantic re-
latedness. In Proceedings of the 23rd National Con-
ference on Artificial Intelligence (AAAI?08), pages
861?866, Chicago, Illinois,USA.
442
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1391?1395,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Predicting Dialect Variation in Immigrant Contexts
Using Light Verb Constructions
A. Seza Do
?
gru
?
oz
Netherlands Institute for Advanced Study
Wassenaar, Netherlands
a.s.dogruoz@gmail.com
Preslav Nakov
Qatar Computing Research Institute
Tornado Tower
floor 10, P.O. Box 5825, Doha, Qatar
pnakov@qf.org.qa
Abstract
Languages spoken by immigrants change
due to contact with the local languages.
Capturing these changes is problematic for
current language technologies, which are
typically developed for speakers of the
standard dialect only. Even when dialec-
tal variants are available for such technolo-
gies, we still need to predict which di-
alect is being used. In this study, we dis-
tinguish between the immigrant and the
standard dialect of Turkish by focusing on
Light Verb Constructions. We experiment
with a number of grammatical and contex-
tual features, achieving over 84% accuracy
(56% baseline).
1 Introduction
Human languages are in constant evolution, driven
in part by contact with other languages (Uriel,
1953; Thomason, 2008). In immigrant contexts,
bilingual and multilingual speakers act as agents
of change by transmitting borrowed words and ex-
pressions across languages (Grosjean, 2014). De-
pending on social factors such as duration and in-
tensity of contact with the local languages, large-
scale spread of borrowed elements could lead to
differences between the contact and non-contact
dialects of the same language (Winford, 2005).
For example, Spanish spoken by immigrants in
USA sounds different in comparison to Spanish
spoken in South America (Corval?an, 2003).
In this study, we focus on the immigrant di-
alect of Turkish as spoken in the Netherlands
(NL-Turkish), which differs from Turkish spo-
ken in Turkey (TR-Turkish). In contact situa-
tions, it is common for verbs to be borrowed
across languages and integrated as nominal com-
plements of Light Verb Constructions (LVCs) (Ed-
wards and Gardner-Chloros, 2007; Butt, 2010).
NL-Turkish LVCs are changing due to Dutch in-
fluence (Do?gru?oz and Backus, 2007; Do?gru?oz and
Backus, 2009; Do?gru?oz and Gries, 2012). How-
ever, assessing Dutch influence is not always easy
since NL-Turkish LVCs still co-exist with the TR-
Turkish LVCs. This study aims to automatically
identify the features that can distinguish between
NL-Turkish and TR-Turkish LVCs.
Our study would benefit Machine Translation
systems targeting dialectal variation. It differs
from studies concerning the well-established di-
alectal variations of Arabic, e.g., Levantine, Gulf,
Egyptian, Maghrebi (Salloum and Habash, 2012),
EU vs. Brazilian Portuguese (Marujo et al., 2011)
or Turkish vs. Tatar (Altintas and Cicekli, 2002).
In contrast, we are interested in developing lan-
guage technologies for immigrant dialects, which
are often understudied and lack written resources
due to their unofficial status. When immigrant
speakers face communication difficulties (e.g., bu-
reaucratic affairs with the local officials, teacher-
parent meetings, doctor-patient conversations) in
the local languages (e.g., Dutch) of the host coun-
try, they are often provided with translation equiv-
alents in the standard dialect (e.g., TR-Turkish)
of their native languages. However, these trans-
lations ignore the evolution of the immigrant di-
alect.
1
By identifying the differences between two
dialects of the same variety, we aim to improve
Machine Translation systems targeting immigrant
speakers. Our contributions are the following:
? We are the first to predict on-going dialect
variation in immigrant contexts as opposed to
studying established dialect variations.
? We are also the first to compare bilingual
LVCs with the monolingual ones across two
dialects of the same language.
1
One of the authors failed the driving test in the Nether-
lands due to the dialect variation in the Turkish translation.
1391
? Our comparison of grammatical versus con-
textual features reveals context to be much
more important.
? We experiment with LVCs extracted from
natural spoken data rather than relying on iso-
lated occurences, out of context.
2 Method
We follow Baldwin and Kim (2010) and Butt
(2010) in their definitions of LVCs, which state
that there is a unity between the nominal and the
verbal complements, but the meaning of the verb
is somewhat bleached. In this study, we focus
on Turkish LVCs with the verbal complements of
yapmak/etmek, which both can be translated as
?make/do?. LVCs with these verbal complements
are undergoing change in NL-Turkish (Do?gru?oz
and Backus, 2009).
We experiment with the following features to
predict NL-Turkish vs. TR-Turkish LVCs.
2.1 Nominal Features
In addition to traditional LVCs (e.g. [?ut?u yap-
mak] ?iron do? (to iron) with both complements
of Turkish origins), there is also foreign influ-
ence on Turkish LVCs. Section 2.1.1 describes
the foreign influence on both NL-Turkish and TR-
Turkish nominal complements based on their ety-
mological origins.
2.1.1 Influence on Nominal Complements
Dutch Influence In example (1), the Dutch verb
overplaats is nominalized through the infinitive
marker (-en) and precedes the Turkish verb yap-
mak to form a Turkish-Dutch bilingual LVC.
Example 1:
O arkadas? [overplaats-en yap-?l-acak-t?.]
That friend [replace-inf
2
do-pass-fut-past].
That friend would have been replaced.
In addition to borrowing nominalized Dutch
verbs to form bilingual LVCs, Dutch LVCs are
also translated as a chunk into NL-Turkish. These
translated LVCs sound unconventional to TR-
Turkish speakers (Do?gru?oz and Gries, 2012). In
example (2), the LVC [s?nav yapmak] ?exam do?
is a literal translation of the Dutch [examen doen]
?exam-pl do?, which is used to describe how stu-
dents take high school exams to graduate.
2
acc: accusative, fut:future, inf:infinitive, past:past tense,
part: participle, pres: present tense, pl: plural, poss: poss-
esive, prog:progressive tense, sg: singular
In a similar context, TR-Turkish speakers would
have used [s?nav-a girmek] ?exam enter? instead.
These LVCs are also classified as having their ori-
gins in another language.
Example 2:
?
Uc? g?und?ur [s?nav yap-?yor-uz].
Three day [exam do-prog-1pl].
We are having exams for the last three days.
Other Foreign Influences Although Dutch in-
fluence is clearly present in NL-Turkish LVCs,
TR-Turkish LVCs are also not free of foreign in-
fluence. We have come across Arabic, Persian,
French and English influences on Turkish LVCs
with nominalized foreign verbs or literally trans-
lated LVCs as chunks. Example (3) illustrates how
a borrowed Arabic verb (hitap, ?address?) is in-
tegrated as a nominal complement into a Turkish
LVC [hitap etmek] ?address do?.
Example 3:
Hoca-m diye [hitap edi-yo-z] biz.
Teacher-poss.1sg like [address do-prog-1pl]
we.
We address (him) as the teacher.
Example (4) illustrates how an English LVC [do
sports] is borrowed into Turkish as a chunk [spor
yapmak] ?sports do?.
Example 4:
Yaz?n [spor yap-?yo-z].
summer spor do-prog-1pl
We do sports in summer.
We have identified the etymological origins of
LVCs in both corpora using an online etymolog-
ical dictionary.
3
Although LVCs of Dutch origin
only occur in NL-Turkish, LVCs borrowed from
other languages (e.g., Arabic, English, French) oc-
cur both in NL-Turkish and in TR-Turkish.
2.1.2 Case Marking
We also came across Turkish [N V] constructions
with ?yapmak? and ?etmek? where the nominal
complement acts as the object of the verb.
Turkish marks the direct objects with accusative
case marking if they are definite (Enc?, 1991). In
example (5), the nominal element is the object of
the verb, and thus it has the accusative marker.
Example 5:
Ben kendi [is?-im-i yap-?yor-um.]
I own [work-poss.1sg-acc do-prog-1sg].
I do my own work.
3
http://www.nisanyansozluk.com/
1392
However, indefinite objects of the verb are left
unmarked for case. In example (6), yapmak takes
an indefinite object (food) as the complement. The
boundary between [N V] constructions with in-
definite nominal objects and LVCs are somewhat
blurry. In both cases, the meaning of the verbal
complement is bleached out and the nominal com-
plement weighs heavier than the verbal one. We
will not dwell further on this subtle distinction, but
we plan future work on this topic following Cook
et al. (2007) and Vincze et al. (2013).
Example 6:
Bazen [yemek yap-ar-d?-m]
Sometimes [food do-pres-past-1sg]
I used to sometimes prepare food.
Since Dutch does not mark objects of the verb
morphologically, NL-Turkish speakers have diffi-
culty (e.g., unnecessary addition or omission of
case markers) in determining the definiteness of
the nominal complements in [N V] constructions
(Do?gru?oz and Backus, 2009). Therefore, we ex-
pect this feature to differentiate well between NL-
Turkish and TR-Turkish [N V] constructions and
LVCs with yapmak/etmek as verbal complements.
2.2 Verbal Complements
2.2.1 Finiteness
The verbs in LVCs are assumed to be flexible for
inflection (Baldwin and Kim, 2010). However, we
know little about how fineteness contributes to the
formation of LVCs. To the best of our knowledge,
finiteness has not been tested as a feature for iden-
tifying LVCs earlier. Therefore, we encoded the
finiteness on yapmak/etmek as a binary (yes/no)
feature in both data sets. Example (7) illustrates a
non-finite LVC where the verb stem (et) is accom-
panied with an infinitive marker (-mek).
Example 7:
Misafir-ler-e [ikram et-mek] ic?in al-d?-k
Guest-pl-dat [serve do-inf.] for buy-past-1pl
We bought (it) to serve the guests.
2.2.2 Type
NL-Turkish speakers could use other light verbs
than TR-Turkish speakers for the same LVC con-
struction. In example (8), the NL-Turkish speaker
uses [do?gum etmek] ?birth do? instead of [do?gum
yapmak] ?birth do?, which is commonly preferred
by TR-Turkish speakers. To capture this differ-
ence between the two dialects, we include the verb
type as a feature as well.
Example 8:
Orda kad?n [do
?
gum et-ti].
There lady [birth do-past].
The lady gave birth there.
2.3 Word Order in LVCs
To the best of our knowledge, the influence of
word order in LVCs has not been investigated as
a feature. Although Turkish has a relatively flexi-
ble constituent order, object-verb (OV) is the most
frequent word order for both NL-Turkish and TR-
Turkish (Do?gru?oz and Backus, 2007). NL-Turkish
speakers have adopted Dutch word order verb-
object (VO) for some syntactic constructions, but
we know little about the word order variation for
LVCs. Encoding the word order of LVCs as a
binary feature (OV vs. VO) could give us clues
about differences or similarities of LVC use in NL-
Turkish and in TR-Turkish. In example (9), the
nominal complement (one thing) follows the ver-
bal complement instead of preceding it as seen in
earlier examples.
Example 9:
[Yap-acak bir s?ey] yok.
[Do-part. one thing] exist.not
There is nothing to do.
2.4 Context
So far, most studies were carried out ignoring
the context of LVCs but focusing on their inher-
ent grammatical features (e.g., lexical, syntactic,
semantic or morphological). However, the con-
text of an utterance could potentially provide addi-
tional useful cues. Since our data comes from nat-
ural conversations, we also experimented with the
contextual information (words surrounding LVCs)
as a feature for both data sets.
3 Data
Our data comes from spoken NL-Turkish (46
speakers from the Netherlands, 74,461 words)
and TR-Turkish (22 speakers from Turkey, 28,731
words) corpora collected by one of the authors.
LVC?s are automatically extracted from the data
using their stem forms (?yap-?, ?et-? without the
infinitive -mEk). Table 1 illustrates the frequency
of [N V] constructions with etmek and yapmak in
both data sets.
# etmek # yapmak # Total
NL-Turkish 449 543 992
TR-Turkish 527 755 1282
Total 976 1298
Table 1: Distribution of etmek and yapmak.
1393
4 Experiments
Our aim is to build a classifier that can determine
whether a particular utterance containing an LVC
(with the verbs yapmak or etmek) is uttered by an
NL-Turkish or a TR-Turkish speaker.
We make use the following features in our
classifier: (1) words from the context of the
LVCs, (2) type of the light verb (yapmak or
etmek), (3) the nominal complements, (4) finite-
ness of the verb (finite/non-finite), (5) case
marking on the nominal complement (yes/no),
(6) word order (VO/OV), and (7) etymolog-
ical origins of the nominal complement (Ara-
bic/Dutch/French/English/Persian/Turkish/mixed).
For the contextual features, we experiment with
two models: (a) we distinguish between a word
extracted from the context to the left or to the right
of the verb (yapmak or etmek) in the feature space,
and (b) we do not make a distinction in terms of
context. The reason to experiment with option
(a) is due to the potential importance of the word
order. While the word order variation is already
modeled through feature (6), we also include the
context as an additional feature to test its effect.
On the down side, adding context doubles the fea-
ture space size and could lead to data sparseness
issues. For the context words, we did not filter out
stopwords since they are part of natural speech.
For our experiments, we used an SVM classifier
as implemented in LibSVM. We used a linear ker-
nel; more complex kernels did not help. We report
results for a 5-fold cross-validation.
5 Results
Table 2 illustrates the results of our experiments.
All models outperform the majority class base-
line of always predicting TR-Turkish (which is
56.38% accuracy) by a sizable margin. Further-
more, splitting the context into left/right yields ap-
proximately 1.5% absolute drop in accuracy.
Split the Context?
Features Left vs. Right No Split
Baseline 56.38
Full model 82.81 84.30
no context 70.67
no nominal complements 82.19 83.64
no info about etymol. origin 82.10 83.99
no finiteness 83.03 84.35
no case marking info 82.76 84.43
no word order info 82.89 84.43
no verb type 82.94 84.39
Table 2: Cross-validation accuracy (5 folds).
The lower part of the table shows the results
when turning off each of the feature types. The
context seems to be the most important feature
since its exclusion leads to a drop from low-to-
mid eighties to about 70% accuracy. Except the
nominal complements and the information about
etymological origins, most other features seem to
have marginal impact on accuracy. Excluding the
two features (nominal complements and etymo-
logical origins) lead to approximately 0.5% ab-
solute drop in accuracy. The impact of the last
four features in the table is tiny; excluding some
of them even leads to a tiny improvement.
Overall, we can conclude that by far the most
important features are the context features (with-
out the left/right context split). The other use-
ful features are the nominal complements and the
information about the etymological origin of the
borrowed LVCs. The remaining four linguistic
features seem to be largely irrelevant.
6 Conclusion and Future Work
Language technologies are usually developed for
standard dialects, ignoring the linguistic differ-
ences in other dialects such as those in immigrant
contexts. One of the reasons for this is the dif-
ficulty of assessing and predicting linguistic dif-
ferences across dialects. This is similar to ef-
forts to translate well-established Arabic dialects
(Bakr et al., 2008; Sawaf, 2010), or to adapt be-
tween Brazilian and European Portuguese (Marujo
et al., 2011), Czech?Slovak (Haji?c et al., 2000),
Spanish?Portuguese (Nakov and Ng, 2009; Nakov
and Ng, 2012), Turkish?Crimean Tatar (Altintas
and Cicekli, 2002), Irish?Scottish Gaelic (Scan-
nell, 2006), Bulgarian?Macedonian (Nakov and
Tiedemann, 2012), Malay?Indonesian (Wang et
al., 2012) or Mandarin?Cantonese (Zhang, 1998).
In this work, we have built a classifier that uses
LVCs to differentiate between two different Turk-
ish dialects: standard and immigrant. The results
indicate that contextual features are most useful
for this task. Although this requires further inves-
tigation, we can explain it by the thousands of fea-
tures context generates: each contextual word is a
feature. Thus, it is very hard for our grammatical
features to compete against contextual features but
they do have an impact.
We are planning to extend our study to dialects
in other immigrant settings (e.g., Turkish in Ger-
many) and to other types of multiword expressions
(e.g., [N N] compounds).
1394
References
Kemal Altintas and Ilyas Cicekli. 2002. A machine
translation system between a pair of closely related
languages. In Proceedings of the 17th International
Symposium on Computer and Information Sciences,
ISCIS ?02, pages 192?196, Orlando, FL, USA.
Hitham Abo Bakr, Khaled Shaalan, and Ibrahim
Ziedan. 2008. A hybrid approach for converting
written Egyptian colloquial dialect into diacritized
Arabic. In Proceedings of the 6th International
Conference on Informatics and Systems, INFOS ?08,
Cairo, Egypt.
Timothy Baldwin and Su Nam Kim, 2010. In Nitin
Indurkhya and Fred J. Damerau (eds.), Handbook
of Natural Language Processing, chapter Multiword
expressions, pages 267?292. CRC Press, Boca Ra-
ton, USA, second edition.
Miriam Butt, 2010. In Mengistu Amberber, Brett
Baker, and Mark Harvey (eds.), Complex predi-
cates: cross-linguistic perspectives on event struc-
ture, chapter The light verb jungle: still hacking
away, pages 48?78. Cambridge University Press.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2007. Pulling their weight: Exploiting syntactic
forms for the automatic identification of idiomatic
expressions in context. In Proceedings of MWE ?07,
pages 41?48, Prague, Czech Republic.
Carmen Silva Corval?an. 2003. Otra mirada a la ex-
presi?on del sujeto como variable sint?actica. Lengua,
variaci?on y contexto: Estudios dedicados a Hum-
berto L?opez Morales, 2:849?860.
A. Seza Do?gru?oz and Ad Backus. 2007. Postverbal el-
ements in immigrant Turkish: Evidence of change?
International Journal of Bilingualism, 11(2):185?
220.
A. Seza Do?gru?oz and Ad Backus. 2009. Innova-
tive constructions in Dutch Turkish: An assessment
of ongoing contact-induced change. Bilingualism:
Language and Cognition, 12(01):41?63.
A. Seza Do?gru?oz and Stefan Gries. 2012. Spread of
on-going changes in an immigrant language: Turk-
ish in the Netherlands. Review of Cognitive Linguis-
tics, 10(2).
Malcolm Edwards and Penelope Gardner-Chloros.
2007. Compound verbs in codeswitching: Bilin-
guals making do? International Journal of Bilin-
gualism, 11(1):73?91.
M?urvet Enc?. 1991. The semantics of specificity. Lin-
guistic Inquiry, 22(1):1?25.
Franc?ois Grosjean. 2014. Bicultural bilinguals. Inter-
national Journal of Bilingualism, pages 1?15.
Jan Haji?c, Jan Hric, and Vladislav Kubo?n. 2000. Ma-
chine translation of very close languages. In Pro-
ceedings of ANLP ?00, pages 7?12, Seattle, WA,
USA.
Lu??s Marujo, Nuno Grazina, Tiago Lu??s, Wang Ling,
Lu??sa Coheur, and Isabel Trancoso. 2011. BP2EP -
adaptation of Brazilian Portuguese texts to European
Portuguese. In Proceedings of EAMT ?11, pages
129?136, Leuven, Belgium.
Preslav Nakov and Hwee Tou Ng. 2009. Improved
statistical machine translation for resource-poor lan-
guages using related resource-rich languages. In
Proceedings of EMNLP ?09, pages 1358?1367, Sin-
gapore.
Preslav Nakov and Hwee Tou Ng. 2012. Improving
statistical machine translation for a resource-poor
language using related resource-rich languages. J.
Artif. Intell. Res. (JAIR), 44:179?222.
Preslav Nakov and J?org Tiedemann. 2012. Combin-
ing word-level and character-level models for ma-
chine translation between closely-related languages.
In Proceedings of ACL ?12, Jeju Island, Korea.
Wael Salloum and Nizar Habash. 2012. Elissa: A di-
alectal to standard Arabic machine translation sys-
tem. In Proceedings of COLING ?12, pages 385?
392, Mumbai, India.
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of AMTA ?10,
Denver, Colorado.
Kevin Scannell. 2006. Machine translation for
closely related language pairs. In Proceedings of the
LREC 2006 Workshop on Strategies for developing
machine translation for minority languages, pages
103?107, Genoa, Italy.
Sarah Thomason. 2008. Social and linguistic factors
as predictors of contact-induced change. Journal of
language contact, 2(1):42?56.
Weinreich Uriel. 1953. Languages in contact: Find-
ings and problems. Publications of the Linguistic
Circle of New York, vol. 1.
Veronika Vincze, Istv?an Nagy, and Rich?ard Farkas.
2013. Identifying English and Hungarian light verb
constructions: A contrastive approach. In Proceed-
ings of ACL ?13, pages 255?261, Sofia, Bulgaria.
Pidong Wang, Preslav Nakov, and Hwee Tou Ng.
2012. Source language adaptation for resource-poor
machine translation. In Proceedings of EMNLP-
CoNLL ?12, pages 286?296, Jeju Island, Korea.
Donald Winford. 2005. Contact-induced changes:
Classification and processes. Diachronica,
22(2):373?427.
Xiaoheng Zhang. 1998. Dialect MT: a case study be-
tween Cantonese and Mandarin. In Proceedings of
the COLING ?98, pages 1460?1464, Montreal, Que-
bec, Canada.
1395
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 492?502,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Feature-Rich Part-of-speech Tagging
for Morphologically Complex Languages: Application to Bulgarian
Georgi Georgiev and Valentin Zhikov
Ontotext AD
135 Tsarigradsko Sh., Sofia, Bulgaria
{georgi.georgiev,valentin.zhikov}@ontotext.com
Petya Osenova and Kiril Simov
IICT, Bulgarian Academy of Sciences
25A Acad. G. Bonchev, Sofia, Bulgaria
{petya,kivs}@bultreebank.org
Preslav Nakov
Qatar Computing Research Institute, Qatar Foundation
Tornado Tower, floor 10, P.O. Box 5825, Doha, Qatar
pnakov@qf.org.qa
Abstract
We present experiments with part-of-
speech tagging for Bulgarian, a Slavic lan-
guage with rich inflectional and deriva-
tional morphology. Unlike most previous
work, which has used a small number of
grammatical categories, we work with 680
morpho-syntactic tags. We combine a large
morphological lexicon with prior linguis-
tic knowledge and guided learning from a
POS-annotated corpus, achieving accuracy
of 97.98%, which is a significant improve-
ment over the state-of-the-art for Bulgarian.
1 Introduction
Part-of-speech (POS) tagging is the task of as-
signing each of the words in a given piece of text a
contextually suitable grammatical category. This
is not trivial since words can play different syn-
tactic roles in different contexts, e.g., can is a
noun in ?I opened a can of coke.? but a verb in
?I can write.? Traditionally, linguists have classi-
fied English words into the following eight basic
POS categories: noun, pronoun, adjective, verb,
adverb, preposition, conjunction, and interjection;
this list is often extended a bit, e.g., with deter-
miners, particles, participles, etc., but the number
of categories considered is rarely more than 15.
Computational linguistics works with a larger
inventory of POS tags, e.g., the Penn Treebank
(Marcus et al 1993) uses 48 tags: 36 for part-
of-speech, and 12 for punctuation and currency
symbols. This increase in the number of tags
is partially due to finer granularity, e.g., there
are special tags for determiners, particles, modal
verbs, cardinal numbers, foreign words, existen-
tial there, etc., but also to the desire to encode
morphological information as part of the tags.
For example, there are six tags for verbs in the
Penn Treebank: VB (verb, base form; e.g., sing),
VBD (verb, past tense; e.g., sang), VBG (verb,
gerund or present participle; e.g., singing), VBN
(verb, past participle; e.g., sung) VBP (verb, non-
3rd person singular present; e.g., sing), and VBZ
(verb, 3rd person singular present; e.g., sings);
these tags are morpho-syntactic in nature. Other
corpora have used even larger tagsets, e.g., the
Brown corpus (Kuc?era and Francis, 1967) and the
Lancaster-Oslo/Bergen (LOB) corpus (Johansson
et al 1986) use 87 and 135 tags, respectively.
POS tagging poses major challenges for mor-
phologically complex languages, whose tagsets
encode a lot of additional morpho-syntactic fea-
tures (for most of the basic POS categories), e.g.,
gender, number, person, etc. For example, the
BulTreeBank (Simov et al 2004) for Bulgarian
uses 680 tags, while the Prague Dependency Tree-
bank (Hajic?, 1998) for Czech has over 1,400 tags.
Below we present experiments with POS tag-
ging for Bulgarian, which is an inflectional lan-
guage with rich morphology. Unlike most previ-
ous work, which has used a reduced set of POS
tags, we use all 680 tags in the BulTreeBank. We
combine prior linguistic knowledge and statistical
learning, achieving accuracy comparable to that
reported for state-of-the-art systems for English.
The remainder of the paper is organized as fol-
lows: Section 2 provides an overview of related
work, Section 3 describes Bulgarian morphology,
Section 4 introduces our approach, Section 5 de-
scribes the datasets, Section 6 presents our exper-
iments in detail, Section 7 discusses the results,
Section 8 offers application-specific error analy-
sis, and Section 9 concludes and points to some
promising directions for future work.
492
2 Related Work
Most research on part-of-speech tagging has fo-
cused on English, and has relied on the Penn Tree-
bank (Marcus et al 1993) and its tagset for train-
ing and evaluation. The task is typically addressed
as a sequential tagging problem; one notable ex-
ception is the work of Brill (1995), who proposed
non-sequential transformation-based learning.
A number of different sequential learning
frameworks have been tried, yielding 96-97%
accuracy: Lafferty et al(2001) experimented
with conditional random fields (CRFs) (95.7%
accuracy), Ratnaparkhi (1996) used a maximum
entropy sequence classifier (96.6% accuracy),
Brants (2000) employed a hidden Markov model
(96.6% accuracy), Collins (2002) adopted an av-
eraged perception discriminative sequence model
(97.1% accuracy). All these models fix the order
of inference from left to right.
Toutanova et al(2003) introduced a cyclic de-
pendency network (97.2% accuracy), where the
search is bi-directional. Shen et al(2007) have
further shown that better results (97.3% accu-
racy) can be obtained using guided learning, a
framework for bidirectional sequence classifica-
tion, which integrates token classification and in-
ference order selection into a single learning task
and uses a perceptron-like (Collins and Roark,
2004) passive-aggressive classifier to make the
easiest decisions first. Recently, Tsuruoka et al
(2011), proposed a simple perceptron-based clas-
sifier applied from left to right but augmented
with a lookahead mechanism that searches the
space of future actions, yielding 97.3% accuracy.
For morphologically complex languages, the
problem of POS tagging typically includes mor-
phological disambiguation, which yields a much
larger number of tags. For example, for Arabic,
Habash and Rambow (2005) used support vector
machines (SVM), achieving 97.6% accuracy with
139 tags from the Arabic Treebank (Maamouri et
al., 2003). For Czech, Hajic? et al(2001) com-
bined a hidden Markov model (HMM) with lin-
guistic rules, which yielded 95.2% accuracy using
an inventory of over 1,400 tags from the Prague
Dependency Treebank (Hajic?, 1998). For Ice-
landic, Dredze and Wallenberg (2008) reported
92.1% accuracy with 639 tags developed for the
Icelandic frequency lexicon (Pind et al 1991),
they used guided learning and tag decomposition:
First, a coarse POS class is assigned (e.g., noun,
verb, adjective), then, additional fine-grained
morphological features like case, number and
gender are added, and finally, the proposed tags
are further reconsidered using non-local features.
Similarly, Smith et al(2005) decomposed the
complex tags into factors, where models for pre-
dicting part-of-speech, gender, number, case, and
lemma are estimated separately, and then com-
posed into a single CRF model; this yielded com-
petitive results for Arabic, Korean, and Czech.
Most previous work on Bulgarian POS tagging
has started with large tagsets, which were then
reduced. For example, Dojchinova and Mihov
(2004) mapped their initial tagset of 946 tags to
just 40, which allowed them to achieve 95.5%
accuracy using the transformation-based learning
of Brill (1995), and 98.4% accuracy using manu-
ally crafted linguistic rules. Similarly, Georgiev
et al(2009), who used maximum entropy and
the BulTreeBank (Simov et al 2004), grouped
its 680 fine-grained POS tags into 95 coarse-
grained ones, and thus improved their accuracy
from 90.34% to 94.4%. Simov and Osenova
(2001) used a recurrent neural network to predict
(a) 160 morpho-syntactic tags (92.9% accuracy)
and (b) 15 POS tags (95.2% accuracy).
Some researchers did not reduce the tagset:
Savkov et al(2011) used 680 tags (94.7% ac-
curacy), and Tanev and Mitkov (2002) used 303
tags and the BULMORPH morphological ana-
lyzer (Krushkov, 1997), achieving P=R=95%.
3 Bulgarian Morphology
Bulgarian is an Indo-European language from the
Slavic language group, written with the Cyrillic
alphabet and spoken by about 9-12 million peo-
ple. It is also a member of the Balkan Sprachbund
and thus differs frommost other Slavic languages:
it has no case declensions, uses a suffixed definite
article (which has a short and a long form for sin-
gular masculine), and lacks verb infinitive forms.
It further uses special evidential verb forms to ex-
press unwitnessed, retold, and doubtful activities.
Bulgarian is an inflective language with very
rich morphology. For example, Bulgarian verbs
have 52 synthetic wordforms on average, while
pronouns have altogether more than ten grammat-
ical features (not necessarily shared by all pro-
nouns), including case, gender, person, number,
definiteness, etc.
493
This rich morphology inevitably leads to ambi-
guity proliferation; our analysis of BulTreeBank
shows four major types of ambiguity:
1. Between the wordforms of the same lexeme,
i.e., in the paradigm. For example, divana,
an inflected form of divan (?sofa?, mascu-
line), can mean (a) ?the sofa? (definite, singu-
lar, short definite article) or (b) a count form,
e.g., as in dva divana (?two sofas?).
2. Between two or more lexemes, i.e., conver-
sion. For example, kato can be (a) a subor-
dinator meaning ?as, when?, or (b) a preposi-
tion meaning ?like, such as?.
3. Between a lexeme and an inflected wordform
of another lexeme, i.e., across-paradigms.
For example, politika can mean (a) ?the
politician? (masculine, singular, definite,
short definite article) or (b) ?politics? (fem-
inine, singular, indefinite).
4. Between the wordforms of two or more
lexemes, i.e., across-paradigms and quasi-
conversion. For example, vrvi can mean
(a) ?walks? (verb, 2nd or 3rd person, present
tense) or (b) ?strings, laces? (feminine, plu-
ral, indefinite).
Some morpho-syntactic ambiguities in Bulgar-
ian are occasional, but many are systematic, e.g.,
neuter singular adjectives have the same forms
as adverbs. Overall, most ambiguities are local,
and thus arguably resolvable using n-grams, e.g.,
compare hubavo dete (?beautiful child?), where
hubavo is a neuter adjective, and ?Pe hubavo.?
(?I sing beautifully.?), where it is an adverb of
manner. Other ambiguities, however, are non-
local and may require discourse-level analysis,
e.g., ?Vidh go.? can mean ?I saw him.?, where
go is a masculine pronoun, or ?I saw it.?, where
it is a neuter pronoun. Finally, there are ambi-
guities that are very hard or even impossible1 to
resolve, e.g., ?Deteto vleze veselo.? can mean
both ?The child came in happy.? (veselo is an ad-
jective) and ?The child came in happily.? (it is an
adverb); however, the latter is much more likely.
1The problem also exists for English, e.g., the annotators
of the Penn Treebank were allowed to use tag combinations
for inherently ambiguous cases: JJ|NN (adjective or noun as
prenominal modifier), JJ|VBG (adjective or gerund/present
participle), JJ|VBN (adjective or past participle), NN|VBG
(noun or gerund), and RB|RP (adverb or particle).
In many cases, strong domain preferences exist
about how various systematic ambiguities should
be resolved. We made a study for the newswire
domain, analyzing a corpus of 546,029 words,
and we found that ambiguity type 2 (lexeme-
lexeme) prevailed for functional parts-of-speech,
while the other types were more frequent for in-
flecting parts-of-speech. Below we show the most
frequent types of morpho-syntactic ambiguities
and their frequency in our corpus:
? na: preposition (?of?) vs. emphatic particle,
with a ratio of 28,554 to 38;
? da: auxiliary particle (?to?) vs. affirmative
particle, with a ratio of 12,035 to 543;
? e: 3rd person present auxiliary verb (?to be?)
vs. particle (?well?) vs. interjection (?wow?),
with a ratio of 9,136 to 21 to 5;
? singular masculine noun with a short definite
article vs. count form of a masculine noun,
with a ratio of 6,437 to 1,592;
? adverb vs. neuter singular adjective, with a
ratio of 3,858 to 1,753.
Overall, the following factors should be taken
into account when modeling Bulgarian morpho-
syntax: (1) locality vs. non-locality of grammat-
ical features, (2) interdependence of grammatical
features, and (3) domain-specific preferences.
4 Method
We used the guided learning framework described
in (Shen et al 2007), which has yielded state-of-
the-art results for English and has been success-
fully applied to other morphologically complex
languages such as Icelandic (Dredze and Wallen-
berg, 2008); we found it quite suitable for Bul-
garian as well. We used the feature set defined in
(Shen et al 2007), which includes the following:
1. The feature set of Ratnaparkhi (1996), in-
cluding prefix, suffix and lexical, as well as
some bigram and trigram context features;
2. Feature templates as in (Ratnaparkhi, 1996),
which have been shown helpful in bidirec-
tional search;
3. More bigram and trigram features and bi-
lexical features as in (Shen et al 2007).
Note that we allowed prefixes and suffixes of
length up to 9, as in (Toutanova et al 2003) and
(Tsuruoka and Tsujii, 2005).
494
We further extended the set of features with
the tags proposed for the current word token by a
morphological lexicon, which maps words to pos-
sible tags; it is exhaustive, i.e., the correct tag is
always among the suggested ones for each token.
We also used 70 linguistically-motivated, high-
precision rules in order to further reduce the num-
ber of possible tags suggested by the lexicon.
The rules are similar to those proposed by Hin-
richs and Trushkina (2004) for German; we im-
plemented them as constraints in the CLaRK sys-
tem (Simov et al 2003).
Here is an example of a rule: If a wordform
is ambiguous between a masculine count noun
(Ncmt) and a singular short definite masculine
noun (Ncmsh), the Ncmt tag should be chosen if
the previous token is a numeral or a number.
The 70 rules were developed by linguists based
on observations over the training dataset only.
They target primarily the most frequent cases of
ambiguity, and to a lesser extent some infrequent
but very problematic cases. Some rules operate
over classes of words, while other refer to partic-
ular wordforms. The rules were designed to be
100% accurate on our training dataset; our exper-
iments show that they are also 100% accurate on
the test and on the development dataset.
Note that some of the rules are dependent on
others, and thus the order of their cascaded appli-
cation is important. For example, the wordform 
is ambiguous between an accusative feminine sin-
gular short form of a personal pronoun (?her?) and
an interjection (?wow?). To handle this properly,
the rule for interjection, which targets sentence
initial positions, followed by a comma, needs to
be executed first. The rule for personal pronouns
is only applied afterwards.
Word Tags
To$i Ppe-os3m
obaqe Cc; Dd
nma Afsi; Vnitf-o3s; Vnitf-r3s;
Vpitf-o2s; Vpitf-o3s; Vpitf-r3s
vzmonost Ncfsi
da Ta;Tx
sledi Ncfpi; Vpitf-o2s; Vpitf-o3s; Vpitf-r3s;
Vpitz?2s
. . . . . .
Table 1: Sample fragment showing the possible tags
suggested by the lexicon. The tags that are further
filtered by the rules are in italic; the correct tag is bold.
The rules are quite efficient at reducing the POS
ambiguity. On the test dataset, before the rule ap-
plication, 34.2% of the tokens (excluding punctu-
ation) had more than one tag in our morphological
lexicon. This number is reduced to 18.5% after
the cascaded application of the 70 linguistic rules.
Table 1 illustrates the effect of the rules on a small
sentence fragment. In this example, the rules have
left only one tag (the correct one) for three of the
ambiguous words. Since the rules in essence de-
crease the average number of tags per token, we
calculated that the lexicon suggests 1.6 tags per
token on average, and after the application of the
rules this number decreases to 1.44 per token.
5 Datasets
5.1 BulTreeBank
We used the latest version of the BulTree-
Bank (Simov and Osenova, 2004), which contains
20,556 sentences and 321,542 word tokens (four
times less than the English Penn Treebank), anno-
tated using a total of 680 unique morpho-syntactic
tags. See (Simov et al 2004) for a detailed de-
scription of the BulTreeBank tagset.
We split the data into training/development/test
as shown in Table 2. Note that only 552 of all 680
tag types were used in the training dataset, and
the development and the test datasets combined
contain a total of 128 new tag types that were not
seen in the training dataset. Moreover, 32% of the
word types in the development dataset and 31%
of those in the testing dataset do not occur in the
training dataset. Thus, data sparseness is an issue
at two levels: word-level and tag-level.
Dataset Sentences Tokens Types Tags
Train 16,532 253,526 38,659 552
Dev 2,007 32,995 9,635 425
Test 2,017 35,021 9,627 435
Table 2: Statistics about our datasets.
5.2 Morphological Lexicon
In order to alleviate the data sparseness issues,
we further used a large morphological lexicon for
Bulgarian, which is an extended version of the
dictionary described in (Popov et al 1998) and
(Popov et al 2003). It contains over 1.5M in-
flected wordforms (for 110K lemmata and 40K
proper names), each mapped to a set of possible
morpho-syntactic tags.
495
6 Experiments and Evaluation
State-of-the-art POS taggers for English typically
build a lexicon containing all tags a word type has
taken in the training dataset; this lexicon is then
used to limit the set of possible tags that an input
token can be assigned, i.e., it imposes a hard con-
straint on the possibilities explored by the POS
tagger. For example, if can has only been tagged
as a verb and as a noun in the training dataset,
it will be only assigned those two tags at test
time; other tags such as adjective, adverb and pro-
noun will not be considered. Out-of-vocabulary
words, i.e., those that were not seen in the train-
ing dataset, are constrained as well, e.g., to a small
set of frequent open-class tags.
In our experiments, we used a morphological
lexicon that is much larger than what could be
built from the training corpus only: building a
lexicon from the training corpus only is of lim-
ited utility since one can hardly expect to see in
the training corpus all 52 synthetic forms a verb
can possibly have. Moreover, we did not use the
tags listed in the lexicon as hard constraints (ex-
cept in one of our baselines); instead, we experi-
mented with a different, non-restrictive approach:
we used the lexicon?s predictions as features or
soft constraints, i.e., as suggestions only, thus al-
lowing each token to take any possible tag. Note
that for both known and out-of-vocabulary words
we used all 680 tags rather than the 552 tags ob-
served in the training dataset; we could afford to
explore this huge search space thanks to the effi-
ciency of the guided learning framework. Allow-
ing all 680 tags on training helped the model by
exposing it to a larger set of negative examples.
We combined these lexicon features with stan-
dard features extracted from the training corpus.
We further experimented with the 70 contextual
linguistic rules, using them (a) as soft and (b) as
hard constraints. Finally, we set four baselines:
three that do not use the lexicon and one that does.
Accuracy (%)
# Baselines (token-level)
1 MFT + unknowns are wrong 78.10
2 MFT + unknowns are Ncmsi 78.52
3 MFT + guesser for unknowns 79.49
4 MFT + lexicon tag-classes 94.40
Table 3: Most-frequent-tag (MFT) baselines.
6.1 Baselines
First, we experimented with the most-frequent-
tag baseline, which is standard for POS tagging.
This baseline ignores context altogether and as-
signs each word type the POS tag it was most
frequently seen with in the training dataset; ties
are broken randomly. We coped with word types
not seen in the training dataset using three sim-
ple strategies: (a) we considered them all wrong,
(b) we assigned them Ncmsi, which is the most
frequent open-class tag in the training dataset, or
(c) we used a very simple guesser, which assigned
Ncfsi, Ncnsi, Ncfsi, and Ncmsf, if the target word
ended by -a, -o, -i, and -t, respectively, other-
wise, it assigned Ncmsi. The results are shown
in lines 1-3 of Table 3: we can see that the token-
level accuracy ranges in 78-80% for (a)-(c), which
is relatively high, given that we use a large inven-
tory of 680 morpho-syntactic tags.
We further tried a baseline that uses the above-
described morphological lexicon, in addition to
the training dataset. We first built two frequency
lists, containing respectively (1) the most frequent
tag in the training dataset for each word type, as
before, and (2) the most frequent tag in the train-
ing dataset for each class of tags that can be as-
signed to some word type, according to the lexi-
con. For example, the most frequent tag for poli-
tika is Ncfsi, and the most frequent tag for the
tag-class {Ncmt;Ncmsi} is Ncmt.
Given a target word type, this new baseline first
tries to assign it the most frequent tag from the
first list. If this is not possible, which happens
(i) in case of ties or (ii) when the word type was
not seen on training, it extracts the tag-class from
the lexicon and consults the second list. If there
is a single most frequent tag in the corpus for this
tag-class, it is assigned; otherwise a random tag
from this tag-class is selected.
Line 4 of Table 3 shows that this latter baseline
achieves a very high accuracy of 94.40%. Note,
however, that this is over-optimistic: the lexicon
contains a tag-class for each word type in our test-
ing dataset, i.e., while there can be word types
not seen in the training dataset, there are no word
types that are not listed in the lexicon. Thus, this
high accuracy is probably due to a large extent
to the scale and quality of our morphological lexi-
con, and it might not be as strong with smaller lex-
icons; we plan to investigate this in future work.
496
6.2 Lexicon Tags as Soft Constraints
We experimented with three types of features:
1. Word-related features only;
2. Word-related features + the tags suggested
by the lexicon;
3. Word-related features + the tags suggested
by the lexicon but then further filtered using
the 70 contextual linguistic rules.
Table 4 shows the sentence-level and the token-
level accuracy on the test dataset for the three
kinds of features: shown on lines 1, 3 and 4, re-
spectively. We can see that using the tags pro-
posed by the lexicon as features (lines 3 and 4)
has a major positive impact, yielding up to 49%
error reduction at the token-level and up to 37%
at the sentence-level, as compared to using word-
related features alone (line 1).
Interestingly, filtering the tags proposed by the
lexicon using the 70 contextual linguistic rules
yields a minor decrease in accuracy both at the
word token-level and at the sentence-level (com-
pare line 4 to line 2). This is surprising since
the linguistic rules are extremely reliable: they
were designed to be 100% accurate on the train-
ing dataset, and we found them experimentally to
be 100% correct on the development and on the
testing dataset as well.
One possible explanation is that by limiting the
set of available tags for a given token at training
time, we prevent the model from observing some
potentially useful negative examples. We tested
this hypothesis by using the unfiltered lexicon
predictions at training time but then making use
of the filtered ones at testing time; the results are
shown on line 5. We can observe a small increase
in accuracy compared to line 4: from 97.80% to
97.84% at the token-level, and from 70.30% to
70.40% at the sentence-level. Although these dif-
ferences are tiny, they suggest that having more
negative examples at training is helpful.
We can conclude that using the lexicon as a
source of soft constraints has a major positive im-
pact, e.g., because it provides access to impor-
tant external knowledge that is complementary
to what can be learned from the training corpus
alone; the improvements when using linguistic
rules as soft constraints are more limited.
6.3 Linguistic Rules as Hard Constraints
Next, we experimented with using the suggestions
of the linguistic rules as hard constraints. Table 4
shows that this is a very good idea. Comparing
line 1 to line 2, which do not use the morpholog-
ical lexicon, we can see very significant improve-
ments: from 95.72% to 97.20% at the token-level
and from 52.95% to 64.50% at the sentence-level.
The improvements are smaller but still consistent
when the morphological lexicon is used: compar-
ing lines 3 and 4 to lines 6 and 7, respectively, we
see an improvement from 97.83% to 97.91% and
from 97.80% to 97.93% at the token-level, and
about 1% absolute at the sentence-level.
6.4 Increasing the Beam Size
Finally, we increased the beam size of guided
learning from 1 to 3 as in (Shen et al 2007).
Comparing line 7 to line 8 in Table 4, we can see
that this yields further token-level improvement:
from 97.93% to 97.98%.
7 Discussion
Table 5 compares our results to previously re-
ported evaluation results for Bulgarian. The
first four lines show the token-level accuracy for
standard POS tagging tools trained and evalu-
ated on the BulTreeBank:2 TreeTagger (Schmid,
1994), which uses decision trees, TnT (Brants,
2000), which uses a hidden Markov model,
SVMtool (Gime?nez and Ma`rquez, 2004), which
is based on support vector machines, and
ACOPOST (Schro?der, 2002), implementing the
memory-based model of Daelemans et al(1996).
The following lines report the token-level accu-
racy reported in previous work, as compared to
our own experiments using guided learning.
We can see that we outperform by a very large
margin (92.53% vs. 97.98%, which represents
73% error reduction) the systems from the first
four lines, which are directly comparable to our
experiments: they are trained and evaluated on the
BulTreeBank using the full inventory of 680 tags.
We further achieved statistically significant im-
provement (p < 0.0001; Pearson?s chi-squared
test (Plackett, 1983)) over the best pervious result
on 680 tags: from 94.65% to 97.98%, which rep-
resents 62.24% error reduction at the token-level.
2We used the pre-trained TreeTagger; for the rest, we re-
port the accuracy given on the Webpage of the BulTreeBank:
www.bultreebank.org/taggers/taggers.html
497
Lexicon Linguistic Rules (applied to filter): Beam Accuracy (%)
# (source of) (a) the lexicon features (b) the output tags size Sentence-level Token-level
1 ? ? ? 1 52.95 95.72
2 ? ? yes 1 64.50 97.20
3 features ? ? 1 70.40 97.83
4 features yes ? 1 70.30 97.80
5 features yes, for test only ? 1 70.40 97.84
6 features ? yes 1 71.34 97.91
7 features yes yes 1 71.69 97.93
8 features yes yes 3 71.94 97.98
Table 4: Evaluation results on the test dataset. Line 1 shows the evaluation results when using features derived
from the text corpus only; these features are used by all systems in the table. Line 2 further uses the contextual
linguistic rules to limit the set of possible POS tags that can be predicted. Note that these rules (1) consult the
lexicon, and (2) always predict a single POS tag. Line 3 uses the POS tags listed in the lexicon as features, i.e.,
as soft suggestions only. Line 4 is like line 3, but the list of feature-tags proposed by the lexicon is filtered by
the contextual linguistic rules. Line 5 is like line 4, but the linguistic rules filtering is only applied at test time;
it is not done on training. Lines 6 and 7 are similar to lines 3 and 4, respectively, but here the linguistic rules
are further applied to limit the set of possible POS tags that can be predicted, i.e., the rules are used as hard
constraints. Finally, line 8 is like line 7, but here the beam size is increased to 3.
Overall, we improved over almost all previ-
ously published results. Our accuracy is sec-
ond only to the manual rules approach of Do-
jchinova and Mihov (2004). Note, however, that
they used 40 tags only, i.e., their inventory is 17
times smaller than ours. Moreover, they have op-
timized their tagset specifically to achieve very
high POS tagging accuracy by choosing not to at-
tempt to resolve some inherently hard systematic
ambiguities, e.g., they do not try to choose be-
tween second and third person past singular verbs,
whose inflected forms are identical in Bulgarian
and hard to distinguish when the subject is not
present (Bulgarian is a pro-drop language).
In order to compare our results more closely
to the smaller tagsets in Table 5, we evaluated
our best model with respect to (a) the first letter
of the tag only (which is part-of-speech only, no
morphological information; 13 tags), e.g., Ncmsf
becomes N, and (b) the first two letters of the
tag (POS + limited morphological information;
49 tags), e.g., Ncmsf becomes Nc. This yielded
99.30% accuracy for (a) and 98.85% for (b).
The latter improves over (Dojchinova and Mihov,
2004), while using a bit larger number of tags.
Our best token-level accuracy of 97.98% is
comparable and even slightly better than the state-
of-the-art results for English: 97.33% when using
Penn Treebank data only (Shen et al 2007), and
97.50% for Penn Treebank plus some additional
unlabeled data (S?gaard, 2011). Of course, our
results are only indirectly comparable to English.
Still, our performance is impressive because
(1) our model is trained on 253,526 tokens only
while the standard training sections 0-18 of the
Penn Treebank contain a total of 912,344 tokens,
i.e., almost four times more, and (2) we predict
680 rather than just 48 tags as for the Penn Tree-
bank, which is 14 times more.
Note, however, that (1) we used a large exter-
nal morphological lexicon for Bulgarian, which
yielded about 50% error reduction (without it,
our accuracy was 95.72% only), and (2) our
train/dev/test sentences are generally shorter, and
thus arguably simpler for a POS tagger to analyze:
we have 17.4 words per test sentence in the Bul-
TreeBank vs. 23.7 in the Penn Treebank.
Our results also compare favorably to the state-
of-the-art results for other morphologically com-
plex languages that use large tagsets, e.g., 95.2%
for Czech with 1,400+ tags (Hajic? et al 2001),
92.1% for Icelandic with 639 tags (Dredze and
Wallenberg, 2008), 97.6% for Arabic with 139
tags (Habash and Rambow, 2005).
8 Error Analysis
In this section, we present error analysis with re-
spect to the impact of the POS tagger?s perfor-
mance on other processing steps in a natural lan-
guage processing pipeline, such as lemmatization
and syntactic dependency parsing.
First, we explore the most frequently confused
pairs of tags for our best-performing POS tagging
system; these are shown in Table 6.
498
Accuracy
Tool/Authors Method # Tags (token-level, %)
*TreeTagger Decision Trees 680 89.21
*ACOPOST Memory-based Learning 680 89.91
*SVMtool Support Vector Machines 680 92.22
*TnT Hidden Markov Model 680 92.53
(Georgiev et al 2009) Maximum Entropy 680 90.34
(Simov and Osenova, 2001) Recurrent Neural Network 160 92.87
(Georgiev et al 2009) Maximum Entropy 95 94.43
(Savkov et al 2011) SVM + Lexicon + Rules 680 94.65
(Tanev and Mitkov, 2002) Manual Rules 303 95.00(=P=R)
(Simov and Osenova, 2001) Recurrent Neural Network 15 95.17
(Dojchinova and Mihov, 2004) Transformation-based Learning 40 95.50
(Dojchinova and Mihov, 2004) Manual Rules + Lexicon 40 98.40
Guided Learning 680 95.72
Guided Learning + Lexicon 680 97.83
This work Guided Learning + Lexicon + Rules 680 97.98
Guided Learning + Lexicon + Rules 49 98.85
Guided Learning + Lexicon + Rules 13 99.30
Table 5: Comparison to previous work for Bulgarian. The first four lines report evaluation results for various
standard POS tagging tools, which were retrained and evaluated on the BulTreeBank. The following lines report
token-level accuracy for previously published work, as compared to our own experiments using guided learning.
We can see that most of the wrong tags share
the same part-of-speech (indicated by the initial
uppercase letter), such as V for verb, N for noun,
etc. This means that most errors refer to the mor-
phosyntactic features. For example, personal or
impersonal verb; definite or indefinite feminine
noun; singular or plural masculine adjective, etc.
At the same time, there are also cases, where the
error has to do with the part-of-speech label itself.
For example, between an adjective and an adverb,
or between a numeral and an indefinite pronoun.
We want to use the above tagger to develop
(1) a rule-based lemmatizer, using the morpholog-
ical lexicon, e.g., as in (Plisson et al 2004), and
(2) a dependency parser like MaltParser (Nivre et
al., 2007), trained on the dependency part of the
BulTreeBank. We thus study the potential impact
of wrong tags on the performance of these tools.
The lemmatizer relies on the lexicon and uses
string transformation functions defined via two
operations ? remove and concatenate:
if tag = Tag then
{remove OldEnd; concatenate NewEnd}
where Tag is the tag of the wordform, OldEnd is
the string that has to be removed from the end of
the wordform, and NewEnd is the string that has
to be concatenated to the beginning of the word-
form in order to produce the lemma.
Here is an example of such a rule:
if tag = Vpitf-o1s then
{remove oh; concatenate a}
The application of the above rule to the past
simple verb form qetoh (?I read?) would remove
oh, and then concatenate a. The result would be
the correct lemma qeta (?to read?).
Such rules are generated for each wordform in
the morphological lexicon; the above functional
representation allows for compact representation
in a finite state automaton. Similar rules are ap-
plied to the unknown words, where the lemma-
tizer tries to guess the correct lemma.
Obviously, the applicability of each rule cru-
cially depends on the output of the POS tagger.
If the tagger suggests the correct tag, then the
wordform would be lemmatized correctly. Note
that, in some cases of wrongly assigned POS tags
in a given context, we might still get the correct
lemma. This is possible in the majority of the
erroneous cases in which the part-of-speech has
been assigned correctly, but the wrong grammat-
ical alternative has been selected. In such cases,
the error does not influence lemmatization.
In order to calculate the proportion of such
cases, we divided each tag into two parts:
(a) grammatical features that are common for all
wordforms of a given lemma, and (b) features that
are specific to the wordform.
499
Freq. Gold Tag Proposed Tag
43 Ansi Dm
23 Vpitf-r3s Vnitf-r3s
16 Npmsh Npmsi
14 Vpiif-r3s Vniif-r3s
13 Npfsd Npfsi
12 Dm Ansi
12 Vpitcam-smi Vpitcao-smi
12 Vpptf-r3p Vpitf-r3p
11 Vpptf-r3s Vpptf-o3s
10 Mcmsi Pfe-os-mi
10 Ppetas3n Ppetas3m
10 Ppetds3f Psot?3?f
9 Npnsi Npnsd
9 Vpptf-o3s Vpptf-r3s
8 Dm A-pi
8 Ppxts Ppxtd
7 Mcfsi Pfe-os-fi
7 Npfsi Npfsd
7 Ppetas3m Ppetas3n
7 Vnitf-r3s Vpitf-r3s
7 Vpitcam-p-i Vpitcao-p-i
Table 6: Most frequently confused pairs of tags.
The part-of-speech features are always deter-
mined by the lemma. For example, Bulgarian
verbs have the lemma features aspect and tran-
sitivity. If they are correct, then the lemma is pre-
dicted also correctly, regardless of whether cor-
rect or wrong on the grammatical features. For
example, if the verb participle form (aorist or
imperfect) has its correct aspect and transitivity,
then it is lemmatized also correctly, regardless
of whether the imperfect or aorist features were
guessed correctly; similarly, for other error types.
We evaluated these cases for the 711 errors in our
experiment, and we found that 206 of them (about
29%) were non-problematic for lemmatization.
For the MaltParser, we encode most of the
grammatical features of the wordforms as spe-
cific features for the parser. Hence, it is much
harder to evaluate the problematic cases due to
the tagger. Still, we were able to make an es-
timation of some cases. Our strategy was to ig-
nore the grammatical features that do not always
contribute to the syntactic behavior of the word-
forms. Such grammatical features for the verbs
are aspect and tense. Thus, proposing perfective
instead of imperfective for a verb or present in-
stead of past tense would not cause problems for
the MaltParser. Among our 711 errors, 190 cases
(or about 27%) were not problematic for parsing.
Finally, we should note that there are two spe-
cial classes of tokens for which it is generally
hard to predict some of the grammatical features:
(1) abbreviations and (2) numerals written with
digits. In sentences, they participate in agreement
relations only if they are pronounced as whole
phrases; unfortunately, it is very hard for the tag-
ger to guess such relations since it does not have
at its disposal enough features, such as the inflec-
tion of the numeral form, that might help detect
and use the agreement pattern.
9 Conclusion and Future Work
We have presented experiments with part-of-
speech tagging for Bulgarian, a Slavic language
with rich inflectional and derivational morphol-
ogy. Unlike most previous work for this language,
which has limited the number of possible tags, we
used a very rich tagset of 680 morpho-syntactic
tags as defined in the BulTreeBank. By com-
bining a large morphological lexicon with prior
linguistic knowledge and guided learning from a
POS-annotated corpus, we achieved accuracy of
97.98%, which is a significant improvement over
the state-of-the-art for Bulgarian. Our token-level
accuracy is also comparable to the best results re-
ported for English.
In future work, we want to experiment with a
richer set of features, e.g., derived from unlabeled
data (S?gaard, 2011) or from the Web (Umansky-
Pesin et al 2010; Bansal and Klein, 2011). We
further plan to explore ways to decompose the
complex Bulgarian morpho-syntactic tags, e.g., as
proposed in (Simov and Osenova, 2001) and
(Smith et al 2005). Modeling long-distance
syntactic dependencies (Dredze and Wallenberg,
2008) is another promising direction; we believe
this can be implemented efficiently using poste-
rior regularization (Graca et al 2009) or expecta-
tion constraints (Bellare et al 2009).
Acknowledgments
We would like to thank the anonymous reviewers
for their useful comments, which have helped us
improve the paper.
The research presented above has been par-
tially supported by the EU FP7 project 231720
EuroMatrixPlus, and by the SmartBook project,
funded by the Bulgarian National Science Fund
under grant D002-111/15.12.2008.
500
References
Mohit Bansal and Dan Klein. 2011. Web-scale fea-
tures for full-scale parsing. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, ACL-HLT ?10, pages 693?702, Portland, Ore-
gon, USA.
Kedar Bellare, Gregory Druck, and Andrew McCal-
lum. 2009. Alternating projections for learning
with expectation constraints. In Proceedings of the
25th Conference on Uncertainty in Artificial Intel-
ligence, UAI ?09, pages 43?50, Montreal, Quebec,
Canada.
Thorsten Brants. 2000. TnT ? a statistical part-of-
speech tagger. In Proceedings of the Sixth Applied
Natural Language Processing, ANLP ?00, pages
224?231, Seattle, Washington, USA.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case
study in part-of-speech tagging. Comput. Linguist.,
21:543?565.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics, Main Volume, ACL ?04,
pages 111?118, Barcelona, Spain.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP ?02, pages 1?8,
Philadelphia, PA, USA.
Walter Daelemans, Jakub Zavrel, Peter Berck, and
Steven Gillis. 1996. MBT: A memory-based part
of speech tagger generator. In Eva Ejerhed and
Ido Dagan, editors, Fourth Workshop on Very Large
Corpora, pages 14?27, Copenhagen, Denmark.
Veselka Dojchinova and Stoyan Mihov. 2004. High
performance part-of-speech tagging of Bulgarian.
In Christoph Bussler and Dieter Fensel, editors,
AIMSA, volume 3192 of Lecture Notes in Computer
Science, pages 246?255. Springer.
Mark Dredze and Joel Wallenberg. 2008. Icelandic
data driven part of speech tagging. In Proceedings
of the 44th Annual Meeting of the Association of
Computational Linguistics: Short Papers, ACL ?08,
pages 33?36, Columbus, Ohio, USA.
Georgi Georgiev, Preslav Nakov, Petya Osenova, and
Kiril Simov. 2009. Cross-lingual adaptation as
a baseline: adapting maximum entropy models to
Bulgarian. In Proceedings of the RANLP?09 Work-
shop on Adaptation of Language Resources and
Technology to New Domains, AdaptLRTtoND ?09,
pages 35?38, Borovets, Bulgaria.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool:
A general POS tagger generator based on support
vector machines. In Proceedings of the 4th Inter-
national Conference on Language Resources and
Evaluation, LREC ?04, Lisbon, Portugal.
Joao Graca, Kuzman Ganchev, Ben Taskar, and Fer-
nando Pereira. 2009. Posterior vs parameter spar-
sity in latent variable models. In Yoshua Bengio,
Dale Schuurmans, John D. Lafferty, Christopher
K. I. Williams, and Aron Culotta, editors, Advances
in Neural Information Processing Systems 22, NIPS
?09, pages 664?672. Curran Associates, Inc., Van-
couver, British Columbia, Canada.
Nizar Habash and Owen Rambow. 2005. Arabic to-
kenization, part-of-speech tagging and morpholog-
ical disambiguation in one fell swoop. In Proceed-
ings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, ACL ?05, pages
573?580, Ann Arbor, Michigan.
Jan Hajic?, Pavel Krbec, Pavel Kve?ton?, Karel Oliva,
and Vladim??r Petkevic?. 2001. Serial combination
of rules and statistics: A case study in Czech tag-
ging. In Proceedings of the 39th Annual Meeting
of the Association for Computational Linguistics,
ACL ?01, pages 268?275, Toulouse, France.
Jan Hajic?. 1998. Building a Syntactically Annotated
Corpus: The Prague Dependency Treebank. In Eva
Hajic?ova?, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevova?, pages 12?
19. Prague Karolinum, Charles University Press.
Erhard W. Hinrichs and Julia S. Trushkina. 2004.
Forging agreement: Morphological disambiguation
of noun phrases. Research on Language & Compu-
tation, 2:621?648.
Stig Johansson, Eric Atwell, Roger Garside, and Geof-
frey Leech, 1986. The Tagged LOB Corpus: Users?
manual. ICAME, The Norwegian Computing Cen-
tre for the Humanities, Bergen University, Norway.
Hristo Krushkov. 1997. Modelling and building ma-
chine dictionaries and morphological processors
(in Bulgarian). Ph.D. thesis, University of Plov-
div, Faculty of Mathematics and Informatics, Plov-
div, Bulgaria.
Henry Kuc?era and Winthrop Nelson Francis. 1967.
Computational analysis of present-day American
English. Brown University Press, Providence, RI.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the 18th Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289, San Francisco, CA, USA.
Mohamed Maamouri, Ann Bies, Hubert Jin, and Tim
Buckwalter. 2003. Arabic Treebank: Part 1 v 2.0.
LDC2003T06.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn Treebank. Com-
put. Linguist., 19:313?330.
501
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Jo?rgen Pind, Fridrik Magnu?sson, and Stefa?n Briem.
1991. The Icelandic frequency dictionary. Techni-
cal report, The Institute of Lexicography, University
of Iceland, Reykjavik, Iceland.
Robin L. Plackett. 1983. Karl Pearson and the Chi-
Squared Test. International Statistical Review / Re-
vue Internationale de Statistique, 51(1):59?72.
Joe?l Plisson, Nada Lavrac?, and Dunja Mladenic?. 2004.
A rule based approach to word lemmatization. In
Proceedings of the 7th International Multiconfer-
ence: Information Society, IS ?2004, pages 83?86,
Ljubljana, Slovenia.
Dimitar Popov, Kiril Simov, and Svetlomira Vidinska.
1998. Dictionary of Writing, Pronunciation and
Punctuation of Bulgarian Language (in Bulgarian).
Atlantis KL, Sofia, Bulgaria.
Dimityr Popov, Kiril Simov, Svetlomira Vidinska, and
Petya Osenova. 2003. Spelling Dictionary of Bul-
garian. Nauka i izkustvo, Sofia, Bulgaria.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Eva Ejerhed
and Ido Dagan, editors, Fourth Workshop on Very
Large Corpora, pages 133?142, Copenhagen, Den-
mark.
Aleksandar Savkov, Laska Laskova, Petya Osenova,
Kiril Simov, and Stanislava Kancheva. 2011.
A web-based morphological tagger for Bulgarian.
In Daniela Majchra?kova? and Radovan Garab??k,
editors, Slovko 2011. Sixth International Confer-
ence. Natural Language Processing, Multilingual-
ity, pages 126?137, Modra/Bratislava, Slovakia.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Ingo Schro?der. 2002. A case study in part-of-speech-
tagging using the ICOPOST toolkit. Technical Re-
port FBI-HH-M-314/02, Department of Computer
Science, University of Hamburg.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
ACL ?07, pages 760?767, Prague, Czech Republic.
Kiril Simov and Petya Osenova. 2001. A hybrid
system for morphosyntactic disambiguation in Bul-
garian. In Proceedings of the EuroConference on
Recent Advances in Natural Language Processing,
RANLP ?01, pages 5?7, Tzigov chark, Bulgaria.
Kiril Simov and Petya Osenova. 2004. BTB-TR04:
BulTreeBank morphosyntactic annotation of Bul-
garian texts. Technical Report BTB-TR04, Bulgar-
ian Academy of Sciences.
Kiril Ivanov Simov, Alexander Simov, Milen
Kouylekov, Krasimira Ivanova, Ilko Grigorov, and
Hristo Ganev. 2003. Development of corpora
within the CLaRK system: The BulTreeBank
project experience. In Proceedings of the 10th con-
ference of the European chapter of the Association
for Computational Linguistics, EACL ?03, pages
243?246, Budapest, Hungary.
Kiril Simov, Petya Osenova, and Milena Slavcheva.
2004. BTB-TR03: BulTreeBank morphosyntac-
tic tagset. Technical Report BTB-TR03, Bulgarian
Academy of Sciences.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambigua-
tion with random fields. In Proceedings of Hu-
man Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language
Processing, pages 475?482, Vancouver, British
Columbia, Canada.
Anders S?gaard. 2011. Semi-supervised condensed
nearest neighbor for part-of-speech tagging. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL-HLT ?10,
pages 48?52, Portland, Oregon, USA.
Hristo Tanev and Ruslan Mitkov. 2002. Shallow
language processing architecture for Bulgarian. In
Proceedings of the 19th International Conference
on Computational Linguistics, COLING ?02, pages
1?7, Taipei, Taiwan.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich
part-of-speech tagging with a cyclic dependency
network. In Proceedings of the Conference of
the North American Chapter of the Association
for Computational Linguistics, NAACL ?03, pages
173?180, Edmonton, Canada.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy
for tagging sequence data. In Proceedings of the
Conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, HLT-EMNLP ?05, pages 467?474, Vancouver,
British Columbia, Canada.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi
Kazama. 2011. Learning with lookahead: Can
history-based models rival globally optimized mod-
els? In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, ACL-HLT ?10, pages
238?246, Portland, Oregon, USA.
Shulamit Umansky-Pesin, Roi Reichart, and Ari Rap-
poport. 2010. A multi-domain web-based algo-
rithm for POS tagging of unknown words. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics: Posters, COLING ?10,
pages 1274?1282, Beijing, China.
502
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1298?1307,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Translating from Morphologically Complex Languages:
A Paraphrase-Based Approach
Preslav Nakov
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
nakov@comp.nus.edu.sg
Hwee Tou Ng
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
nght@comp.nus.edu.sg
Abstract
We propose a novel approach to translating
from a morphologically complex language.
Unlike previous research, which has targeted
word inflections and concatenations, we fo-
cus on the pairwise relationship between mor-
phologically related words, which we treat as
potential paraphrases and handle using para-
phrasing techniques at the word, phrase, and
sentence level. An important advantage of
this framework is that it can cope with deriva-
tional morphology, which has so far remained
largely beyond the capabilities of statistical
machine translation systems. Our experiments
translating from Malay, whose morphology is
mostly derivational, into English show signif-
icant improvements over rivaling approaches
based on five automatic evaluation measures
(for 320,000 sentence pairs; 9.5 million En-
glish word tokens).
1 Introduction
Traditionally, statistical machine translation (SMT)
models have assumed that the word should be the ba-
sic token-unit of translation, thus ignoring any word-
internal morphological structure. This assumption
can be traced back to the first word-based models of
IBM (Brown et al, 1993), which were initially pro-
posed for two languages with limited morphology:
French and English. While several significantly im-
proved models have been developed since then, in-
cluding phrase-based (Koehn et al, 2003), hierarchi-
cal (Chiang, 2005), treelet (Quirk et al, 2005), and
syntactic (Galley et al, 2004) models, they all pre-
served the assumption that words should be atomic.
Ignoring morphology was fine as long as the main
research interest remained focused on languages
with limited (e.g., English, French, Spanish) or min-
imal (e.g., Chinese) morphology. Since the attention
shifted to languages like Arabic, however, the im-
portance of morphology became obvious and sev-
eral approaches to handle it have been proposed.
Depending on the particular language of interest,
researchers have paid attention to word inflections
and clitics, e.g., for Arabic, Finnish, and Turkish,
or to noun compounds, e.g., for German. However,
derivational morphology has not been specifically
targeted so far.
In this paper, we propose a paraphrase-based ap-
proach to translating from a morphologically com-
plex language. Unlike previous research, we focus
on the pairwise relationship between morphologi-
cally related wordforms, which we treat as poten-
tial paraphrases, and which we handle using para-
phrasing techniques at various levels: word, phrase,
and sentence level. An important advantage of this
framework is that it can cope with various kinds
of morphological wordforms, including derivational
ones. We demonstrate its potential on Malay, whose
morphology is mostly derivational.
The remainder of the paper is organized as fol-
lows: Section 2 gives an overview of Malay mor-
phology, Section 3 introduces our paraphrase-based
approach to translating from morphologically com-
plex languages, Section 4 describes our dataset and
our experimental setup, Section 5 presents and anal-
yses the results, and Section 6 compares our work to
previous research. Finally, Section 7 concludes the
paper and suggests directions for future work.
1298
2 Malay Morphology and SMT
Malay is an Astronesian language, spoken by about
180 million people. It is official in Malaysia, In-
donesia, Singapore, and Brunei, and has two major
dialects, sometimes regarded as separate languages,
which are mutually intelligible, but occasionally dif-
fer in orthography/pronunciation and vocabulary:
Bahasa Malaysia (lit. ?language of Malaysia?) and
Bahasa Indonesia (lit. ?language of Indonesia?).
Malay is an agglutinative language with very rich
morphology. Unlike other agglutinative languages
such as Finnish, Hungarian, and Turkish, which
are rich in both inflectional and derivational forms,
Malay morphology is mostly derivational. Inflec-
tionally,1 Malay is very similar to Chinese: there is
no grammatical gender, number, or tense, verbs are
not marked for person, etc.
In Malay, new words can be formed by the fol-
lowing three morphological processes:
? Affixation, i.e., attaching affixes, which are not
words themselves, to a word. These can be pre-
fixes (e.g., ajar/?teach? ? pelajar/?student?),
suffixes (e.g., ajar ? ajaran/?teachings?), cir-
cumfixes (e.g., ajar ? pengajaran/?lesson?),
and infixes (e.g., gigi/?teeth? ? gerigi/?toothed
blade?). Infixes only apply to a small number
of words and are not productive.
? Compounding, i.e., forming a new word by
putting two or more existing words together.
For example, kereta/?car? + api/?fire? make
kereta api and keretapi in Bahasa Indonesia and
Bahasa Malaysia, respectively, both meaning
?train?. As in English, Malay compounds are
written separately, but some stable ones like
kerjasama/?collaboration? (from kerja/?work?
and sama/?same?) are concatenated. Concate-
nation is also required when a circumfix is
applied to a compound, e.g., ambil alih/?take
over? (ambil/?take? + alih/?move?) is con-
catenated to form pengambilalihan/?takeover?
when targeted by the circumfix peng-. . .-an.
1Inflection is variation in the form of a word that is oblig-
atory in some given grammatical context. For example, plays,
playing, played are all inflected forms of the verb play. It does
not yield a new word and cannot change the part of speech.
? Reduplication, i.e., word repetition. In
Malay, reduplication requires using a dash. It
can be full (e.g., pelajar-pelajar/?students?),
partial (e.g., adik-beradik/?siblings?, from
adik/?younger brother/sister?), and rhythmic
(e.g., gunung-ganang/?mountains?, from the
word gunung/?mountain?).
Malay has very little inflectional morphology, It
also has some clitics2, which are not very frequent
and are typically spelled concatenated to the preced-
ing word. For example, the politeness marker lah
can be added to the command duduk/?sit down? to
yield duduklah/?please, sit down?, and the pronoun
nya can attach to kereta to form keretanya/?his car?.
Note that clitics are not affixes, and clitic attachment
is not a word derivation or a word inflection process.
Taken together, affixation, compounding, redu-
plication, and clitic attachment yield a rich vari-
ety of wordforms, which cause data sparseness is-
sues. Moreover, the predominantly derivational na-
ture of Malay morphology limits the applicabil-
ity of standard techniques such as (1) removing
some/all of the source-language inflections, (2) seg-
menting affixes from the root, and (3) clustering
words with the same target translation. For example,
if pelajar/?student? is an unknown word and lemma-
tization/stemming reduces it to ajar/?teach?, would
this enable a good translation? Similarly, would seg-
menting3 pelajar as peN+ ajar, i.e., as ?person do-
ing the action? + ?teach?, make it possible to gener-
ate ?student? (e.g., as opposed to ?teacher?)? Finally,
if affixes tend to change semantics so much, how
likely are we to find morphologically related word-
forms that share the same translation? Still, there
are many good reasons to believe that morphologi-
cal processing should help SMT for Malay.
Consider affixation, which can yield words with
similar semantics that can use each other?s trans-
lation options, e.g., diajar/?be taught (intransitive)?
and diajarkan/?be taught (transitive)?. However, this
cannot be predicted from the affix, e.g., compare
minum/?drink (verb)? ? minuman/?drink (noun)? and
makan/?eat? ? makanan/?food?.
2A clitic is a morpheme that has the syntactic characteristics
of a word, but is phonologically bound to another word. For
example, ?s is a clitic in The Queen of England?s crown.
3The prefix peN suffers a nasal replacement of the
archiphoneme N to become pel in pelajar.
1299
Looking at compounding, it is often the case that
the semantics of a compound is a specialization of
the semantics of its head, and thus the target lan-
guage translations available for the head could be us-
able to translate the whole compound, e.g., compare
kerjasama/?collaboration? and kerja/?work?. Alter-
natively, it might be useful to consider a segmented
version of the compound, e.g., kerja sama.
Reduplication, among other functions, expresses
plural, e.g., pelajar-pelajar/?students?. Note, how-
ever, that it is not used when a quantity or a num-
ber word is present, e.g., dua pelajar/?two students?
and banyak pelajar/?many students?. Thus, if we do
not know how to translate pelajar-pelajar, it would
be reasonable to consider the translation options for
pelajar since it could potentially contain among its
translation options the plural ?students?.
Finally, consider clitics. In some cases, a clitic
could express a fine-grained distinction such as po-
liteness, which might not be expressible in the target
language; thus, it might be feasible to simply remove
it. In other cases, e.g., when it is a pronoun, it might
be better to segment it out as a separate word.
3 Method
We propose a paraphrase-based approach to Malay
morphology, where we use paraphrases at three dif-
ferent levels: word, phrase, and sentence level.
First, we transform each development/testing
Malay sentence into a word lattice, where we add
simplified word-level paraphrasing alternatives for
each morphologically complex word. In the lattice,
each alternative w? of an original word w is assigned
the weight of Pr(w?|w), which is estimated using
pivoting over the English side of the training bi-
text. Then, we generate sentence-level paraphrases
of the training Malay sentences, in which exactly
one morphologically complex word is substituted by
a simpler alternative. Finally, we extract additional
Malay phrases from these sentences, which we use
to augment the phrase table with additional transla-
tion options to match the alternative wordforms in
the lattice. We assign each such additional phrase
p? a probability maxp Pr(p?|p), where p is a Malay
phrase that is found in the original training Malay
text. The probability is calculated using phrase-level
pivoting over the English side of the training bi-text.
3.1 Morphological Analysis
Given a Malay word, we build a list of morpholog-
ically simpler words that could be derived from it;
we also generate alternative word segmentations:
(a) words obtainable by affix stripping
e.g., pelajaran ? pelajar, ajaran, ajar
(b) words that are part of a compound word
e.g., kerjasama ? kerja
(c) words appearing on either side of a dash
e.g., adik-beradik ? adik, beradik
(d) words without clitics
e.g., keretanya ? kereta
(e) clitic-segmented word sequences
e.g., keretanya ? kereta nya
(f) dash-segmented wordforms
e.g., aceh-nias ? aceh - nias
(g) combinations of the above.
The list is built by reversing the basic morpho-
logical processes in Malay: (a) addresses affixation,
(b) handles compounding, (c) takes care of redu-
plication, and (d) and (e) deal with clitics. Strictly
speaking, (f) does not necessarily model a morpho-
logical process: it proposes an alternative tokeniza-
tion, but this could make morphological sense too.
Note that (g) could cause potential problems when
interacting with (f), e.g., adik-beradik would be-
come adik - beradik and then by (a) it would turn
into adik - adik, which could cause the SMT sys-
tem to generate two separate translations for the two
instances of adik. To prevent this, we forbid the
application of (f) to reduplications. Taking into ac-
count that reduplications can be partial, we only al-
low (f) if |LCS(l,r)|min(|l|,|r|) < 0.5, where l and r are the
strings to the left and to the right of the dash, re-
spectively, LCS(x, y) is the longest common char-
acter subsequence, not necessarily consecutive, of
the strings x and y, and |x| is the length of the string
x. For example, LCS(adik,beradik)=adik, and thus,
the ratio is 1 (? 0.5) for adik-beradik. Similarly,
LCS(gunung,ganang)=gnng, and thus, the ratio is
4/6=0.67 (? 0.5) for gunung-ganang. However, for
aceh-nias, it is 1/4=0.25, and thus (f) is applicable.
1300
As an illustration, here are the wordforms we
generate for adik-beradiknya/?his siblings?: adik,
adik-beradiknya, adik-beradik nya, adik-beradik,
beradiknya, beradik nya, adik nya, and beradik.
And for berpelajaran/?is educated?, we build the list:
berpelajaran, pelajaran, pelajar, ajaran, and ajar.
Note that the lists do include the original word.
To generate the above wordforms, we used two
morphological analyzers: a freely available Malay
lemmatizer (Baldwin and Awab, 2006), and an in-
house re-implementation of the Indonesian stemmer
described in (Adriani et al, 2007). Note that these
tools? objective is to return a single lemma/stem,
e.g., they would return adik for adik-beradiknya, and
ajar for berpelajaran. However, it was straightfor-
ward to modify them to also output the above in-
termediary wordforms, which the tools were gener-
ating internally anyway when looking for the final
lemma/stem. Finally, since the two modified ana-
lyzers had different strengths and weaknesses, we
combined their outputs to increase recall.
3.2 Word-Level Paraphrasing
We perform word-level paraphrasing of the Malay
sides of the development and the testing bi-texts.
First, for each Malay word, we generate the
above-described list of morphologically simpler
words and alternative word segmentations; we think
of the words in this list as word-level paraphrases.
Then, for each development/testing Malay sentence,
we generate a lattice encoding all possible para-
phrasing options for each individual word.
We further specify a weight for each arc. We as-
sign 1 to the original Malay word w, and Pr(w?|w)
to each paraphrase w? of w, where Pr(w?|w) is the
probability that w? is a good paraphrase of w. Note
that multi-word paraphrases, e.g., resulting from
clitic segmentation, are encoded using a sequence of
arcs; in such cases, we assign Pr(w?|w) to the first
arc, and 1 to each subsequent arc.
We calculate the probability Pr(w?|w) using the
training Malay-English bi-text, which we align at
the word level using IBM model 4 (Brown et al,
1993), and we observe which English words w and
w? are aligned to. More precisely, we use pivoting to
estimate the probability Pr(w?|w) as follows:
Pr(w?|w) = ?i Pr(w?|w, ei)Pr(ei|w)
Then, following (Callison-Burch et al, 2006; Wu
and Wang, 2007), we make the simplifying assump-
tion that w? is conditionally independent of w given
ei, thus obtaining the following expression:
Pr(w?|w) =
?
i Pr(w
?|ei)Pr(ei|w)
We estimate the probability Pr(ei|w) directly
from the word-aligned training bi-text as follows:
Pr(ei|w) = #(w,ei)P
j #(w,ej)
where #(x, e) is the number of times the Malay
word x is aligned to the English word e.
Estimating Pr(w?|ei) cannot be done directly
since w? might not be present on the Malay side of
the training bi-text, e.g., because it is a multi-token
sequence generated by clitic segmentation. Thus, we
think of w? as a pseudoword that stands for the union
of all Malay words in the training bi-text that are re-
ducible to w? by our morphological analysis proce-
dure. So, we estimate Pr(w?|ei) as follows:
Pr(w?|ei) = Pr({v : w? ? forms(v)}|ei)
where forms(x) is the set of the word-level para-
phrases4 for the Malay word x.
Since the training bi-text occurrences of the words
that are reducible to w? are distinct, we can rewrite
the above as follows:
Pr(w?|ei) =
?
v:w??forms(v) Pr(v|ei)
Finally, the probability Pr(v|ei) can be estimated
using maximum likelihood:
Pr(v|ei) = #(v,ei)P
u #(u,ei)
3.3 Sentence-Level Paraphrasing
In order for the word-level paraphrases to work,
there should be phrases in the phrase table that could
potentially match them. For some of the words, e.g.,
the lemmata, there could already be such phrases,
but for other transformations, e.g., clitic segmenta-
tion, this is unlikely. Thus, we need to augment the
phrase table with additional translation options.
One approach would be to modify the phrase ta-
ble directly, e.g., by adding additional entries, where
one or more Malay words are replaced by their para-
phrases. This would be problematic since the phrase
translation probabilities associated with these new
4Note that our paraphrasing process is directed: the para-
phrases are morphologically simpler than the original word.
1301
entries would be hard to estimate. For example, the
clitics, and even many of the intermediate morpho-
logical forms, would not exist as individual words in
the training bi-text, which means that there would be
no word alignments or lexical probabilities available
for them.
Another option would be to generate separate
word alignments for the original training bi-text and
for a version of it where the source (Malay) side
has been paraphrased. Then, the two bi-texts and
their word alignments would be concatenated and
used to build a phrase table (Dyer, 2007; Dyer et
al., 2008; Dyer, 2009). This would solve the prob-
lems with the word alignments and the phrase pair
probabilities estimations in a principled manner, but
it would require choosing for each word only one of
the paraphrases available to it, while we would pre-
fer to have a way to allow all options. Moreover, the
paraphrased and the original versions of the corpus
would be given equal weights, which might not be
desirable. Finally, since the two versions of the bi-
text would be word-aligned separately, there would
be no interaction between them, which might lead
to missed opportunities for improved alignments in
both parts of the bi-text (Nakov and Ng, 2009).
We avoid the above issues by adopting a sentence-
level paraphrasing approach. Following the gen-
eral framework proposed in (Nakov, 2008), we first
create multiple paraphrased versions of the source-
side sentences of the training bi-text. Then, each
paraphrased source sentence is paired with its orig-
inal translation. This augmented bi-text is word-
aligned and a phrase table T ? is built from it, which
is merged with a phrase table T for the original bi-
text. The merged table contains all phrase entries
from T , and the entries for the phrase pairs from T ?
that are not in T . Following Nakov and Ng (2009),
we add up to three additional indicator features (tak-
ing the values 0.5 and 1) to each entry in the merged
phrase table, showing whether the entry came from
(1) T only, (2) T ? only, or (3) both T and T ?. We also
try using the first one or two features only. We set
all feature weights using minimum error rate train-
ing (Och, 2003), and we optimize their number (one,
two, or three) on the development dataset.5
5In theory, we should re-normalize the probabilities; in prac-
tice, this is not strictly required by the log-linear SMT model.
Each of our paraphrased sentences differs from its
original sentence by a single word, which prevents
combinatorial explosions: on average, we generate
14 paraphrased versions per input sentence. It fur-
ther ensures that the paraphrased parts of the sen-
tences will not dominate the word alignments or the
phrase pairs, and that there would be sufficient inter-
action at word alignment time between the original
sentences and their paraphrased versions.
3.4 Phrase-Level Paraphrasing
While our sentence-level paraphrasing informs the
decoder about the origin of each phrase pair (orig-
inal or paraphrased bi-text), it provides no indica-
tion about how good the phrase pairs from the para-
phrased bi-text are likely to be.
Following Callison-Burch et al (2006), we fur-
ther augment the phrase table with one additional
feature whose value is 1 for the phrase pairs com-
ing from the original bi-text, and maxp Pr(p?|p) for
the phrase pairs extracted from the paraphrased bi-
text. Here p is a Malay phrase from T , and p? is a
Malay phrase from T ? that does not exist in T but is
obtainable from p by substituting one or more words
in p with their derivationally related forms generated
by morphological analysis. The probability Pr(p?|p)
is calculated using phrase-level pivoting through En-
glish in the original phrase table T as follows (unlike
word-level pivoting, here ei is an English phrase):
Pr(p?|p) = ?i Pr(p?|ei)Pr(ei|p)
We estimate the probabilities Pr(ei|p) and
Pr(p?|ei) as we did for word-level pivoting, except
that this time we use the list of the phrase pairs ex-
tracted from the original training bi-text, while be-
fore we used IBM model 4 word alignments. When
calculating Pr(p?|ei), we think of p? as the set of all
possible Malay phrases q in T that are reducible to
p? by morphological analysis of the words they con-
tain. This can be rewritten as follows:
Pr(p?|ei) =
?
q:p??par(q) Pr(q|ei)
where par(q) is the set of all possible phrase-level
paraphrases for the Malay phrase q.
The probability Pr(q|ei) is estimated using maxi-
mum likelihood from the list of phrase pairs. There
is no combinatorial explosion here, since the phrases
are short and contain very few paraphrasable words.
1302
Number of sentence pairs 1K 2K 5K 10K 20K 40K 80K 160K 320K
Number of English words 30K 60K 151K 301K 602K 1.2M 2.4M 4.7M 9.5M
baseline 23.81 27.43 31.53 33.69 36.68 38.49 40.53 41.80 43.02
lemmatize all 22.67 26.20 29.68 31.53 33.91 35.64 37.17 38.58 39.68
-1.14 -1.23 -1.85 -2.16 -2.77 -2.85 -3.36 -3.22 -3.34
?noisier? channel model (Dyer, 2007) 23.27 28.42 32.66 33.69 37.16 38.14 39.79 41.76 42.77
-0.54 +0.99 +1.13 +0.00 +0.48 -0.35 -0.74 -0.04 -0.25
lattice + sent-par (orig+lemma) 24.71 28.65 32.42 34.95 37.32 38.40 39.82 41.97 43.36
+0.90 +1.22 +0.89 +1.26 +0.64 -0.09 -0.71 +0.17 +0.34
lattice + sent-par 24.97 29.11 33.03 35.12 37.39 38.73 41.04 42.24 43.52
+1.16 +1.68 +1.50 +1.43 +0.71 +0.24 +0.51 +0.44 +0.50
lattice + sent-par + word-par 25.14 29.17 33.00 35.09 37.39 38.76 40.75 42.23 43.58
+1.33 +1.74 +1.47 +1.40 +0.71 +0.27 +0.22 +0.43 +0.56
lattice + sent-par + word-par + phrase-par 25.27 29.19 33.35 35.23 37.46 39.00 40.95 42.30 43.73
+1.46 +1.76 +1.82 +1.54 +0.78 +0.51 +0.42 +0.50 +0.71
Table 1: Evaluation results. Shown are BLEU scores and improvements over the baseline (in %) for different numbers
of training sentences. Statistically significant improvements are in bold for p < 0.01 and in italic for p < 0.05.
4 Experiments
4.1 Data
We created our Malay-English training and develop-
ment datasets from data that we downloaded from
the Web and then sentence-aligned using various
heuristics. Thus, we ended up with 350,003 training
sentence pairs, including 10.4M English and 9.7M
Malay word tokens. We further downloaded 49.8M
word tokens of monolingual English text, which we
used for language modeling.
For testing, we used 1,420 sentences with 28.8K
Malay word tokens, which were translated by three
human translators, yielding translations of 32.8K,
32.4K, and 32.9K English word tokens, respectively.
For development, we used 2,000 sentence pairs of
63.4K English and 58.5K Malay word tokens.
4.2 General Experimental Setup
First, we tokenized and lowercased all datasets:
training, development, and testing. We then built
directed word-level alignments for the training bi-
text for English?Malay and for Malay?English
using IBM model 4 (Brown et al, 1993), which
we symmetrized using the intersect+grow heuristic
(Och and Ney, 2003). Next, we extracted phrase-
level translation pairs of maximum length seven,
which we scored and used to build a phrase table
where each phrase pair is associated with the fol-
lowing five standard feature functions: forward and
reverse phrase translation probabilities, forward and
reverse lexicalized phrase translation probabilities,
and phrase penalty.
We trained a log-linear model using the following
standard SMT feature functions: trigram language
model probability, word penalty, distance-based dis-
tortion cost, and the five feature functions from the
phrase table. We set al weights on the development
dataset by optimizing BLEU (Papineni et al, 2002)
using minimum error rate training (Och, 2003), and
we plugged them in a beam search decoder (Koehn
et al, 2007) to translate the Malay test sentences to
English. Finally, we detokenized the output, and we
evaluated it against the three reference translations.
4.3 Systems
Using the above general experimental setup, we im-
plemented the following baseline systems:
? baseline. This is the default system, which uses
no morphological processing.
? lemmatize all. This is the second baseline that
uses lemmatized versions of the Malay side of
the training, development and testing datasets.
? ?noisier? channel model.6 This is the model of
Dyer (2007). It uses 0-1 weights in the lattice
and only allows lemmata as alternative word-
forms; it uses no sentence-level or phrase-level
paraphrases.
6We also tried the word segmentation model of Dyer (2009)
as implemented in the cdec decoder (Dyer et al, 2010), which
learns word segmentation lattices from raw text in an unsu-
pervised manner. Unfortunately, it could not learn meaning-
ful word segmentations for Malay, and thus we do not compare
against it. We believe this may be due to its focus on word seg-
mentation, which is of limited use for Malay.
1303
sent. system 1-gram 2-gram 3-gram 4-gram
1k baseline 59.78 29.60 17.36 10.46
paraphrases 62.23 31.19 18.53 11.35
2k baseline 64.20 33.46 20.41 12.92
paraphrases 66.38 35.42 21.97 14.06
5k baseline 68.12 38.12 24.20 15.72
paraphrases 70.41 40.13 25.71 17.02
10k baseline 70.13 40.67 26.15 17.27
paraphrases 72.04 42.28 27.55 18.36
20k baseline 73.19 44.12 29.14 19.50
paraphrases 73.28 44.43 29.77 20.31
40k baseline 74.66 45.97 30.70 20.83
paraphrases 75.47 46.54 31.09 21.17
80k baseline 75.72 48.08 32.80 22.59
paraphrases 76.03 48.47 33.20 23.00
160k baseline 76.55 49.21 34.09 23.78
paraphrases 77.14 49.89 34.57 24.06
320k baseline 77.72 50.54 35.19 24.78
paraphrases 78.03 51.24 35.99 25.42
Table 2: Detailed BLEU n-gram precision scores: in
%, for different numbers of training sentence pairs, for
baseline and lattice + sent-par + word-par + phrase-par.
Our full morphological paraphrasing system is
lattice + sent-par + word-par + phrase-par. We
also experimented with some of its components
turned off. lattice + sent-par + word-par excludes
the additional feature from phrase-level paraphras-
ing. lattice + sent-par has all the morphologically
simpler derived forms in the lattice during decod-
ing, but their weights are uniformly set to 0 rather
than obtained using pivoting from word alignments.
Finally, in order to compare closely to the ?noisier?
channel model, we further limited the morpholog-
ical variants of lattice + sent-par in the lattice to
lemmata only in lattice + sent-par (orig+lemma).
5 Results and Discussion
The experimental results are shown in Table 1.
First, we can see that lemmatize all has a consis-
tently disastrous effect on BLEU, which shows that
Malay morphology does indeed contain information
that is important when translating to English.
Second, Dyer (2007)?s ?noisier? channel model
helps for small datasets only. It performs worse than
lattice + sent-par (orig+lemma), from which it dif-
fers in the phrase table only; this confirms the im-
portance of our sentence-level paraphrasing.
Moving down to lattice + sent-par, we can see
that using multiple morphological wordforms in-
stead of just lemmata has a consistently positive im-
pact on BLEU for datasets of all sizes.
Sent. System BLEU NIST TER METEOR TESLA
1k baseline 23.81 6.7013 64.50 49.26 1.6794
paraphrases 25.27 6.9974 63.03 52.32 1.7579
2k baseline 27.43 7.3790 61.03 54.29 1.8718
paraphrases 29.19 7.7306 59.37 57.32 2.0031
5k baseline 31.53 8.0992 57.12 59.09 2.1172
paraphrases 33.35 8.4127 55.41 61.67 2.2240
10k baseline 33.69 8.5314 55.24 62.26 2.2656
paraphrases 35.23 8.7564 53.60 63.97 2.3634
20k baseline 36.68 8.9604 52.56 64.67 2.3961
paraphrases 37.46 9.0941 52.16 66.42 2.4621
40k baseline 38.49 9.3016 51.20 66.68 2.5166
paraphrases 39.00 9.4184 50.68 67.60 2.5604
80k baseline 40.53 9.6047 49.88 68.77 2.6331
paraphrases 40.95 9.6289 49.09 69.10 2.6628
160k baseline 41.80 9.7479 48.97 69.59 2.6887
paraphrases 42.30 9.8062 48.29 69.62 2.7049
320k baseline 43.02 9.8974 47.44 70.23 2.7398
paraphrases 43.73 9.9945 47.07 70.87 2.7856
Table 3: Results for different evaluation measures: for
baseline and lattice + sent-par + word-par + phrase-par
(in % for all measures except for NIST).
Adding weights obtained using word-level piv-
oting in lattice + sent-par + word-par helps a
bit more, and also using phrase-level paraphrasing
weights yields even bigger further improvements for
lattice + sent-par + word-par + phrase-par.
Overall, our morphological paraphrases yield sta-
tistically significant improvements (p < 0.01) in
BLEU, according to Collins et al (2005)?s sign test,
for bi-texts as large as 320,000 sentence pairs.
A closer look at BLEU. Table 2 shows detailed
n-gram BLEU precision scores for n=1,2,3,4. Our
system outperforms the baseline on all precision
scores and for all numbers of training sentences.
Other evaluation measures. Table 3 reports
the results for five evaluation measures: BLEU
and NIST 11b, TER 0.7.25 (Snover et al, 2006),
METEOR 1.0 (Lavie and Denkowski, 2009), and
TESLA (Liu et al, 2010). Our system consistently
outperforms the baseline for all measures.
Example translations. Table 4 shows two trans-
lation examples. In the first example, the redupli-
cation bekalan-bekalan (?supplies?) is an unknown
word, and was left untranslated by the baseline sys-
tem. It was not a problem for our system though,
which first paraphrased it as bekalan and then trans-
lated it as supply. Even though this is still wrong (we
need the plural supplies), it is arguably preferable to
passing the word untranslated; it also allowed for a
better translation of the surrounding context.
1304
src : Mercy Relief telah menghantar 17 khemah khas bernilai $5,000 setiap satu yang boleh menampung kelas seramai 30
pelajar, selain bekalan-bekalan lain seperti 500 khemah biasa, barang makanan dan ubat-ubatan untuk mangsa gempa Sichuan.
ref1: Mercy Relief has sent 17 special tents valued at $5,000 each, that can accommodate a class of 30 students, including
other aid supplies such as 500 normal tents, food and medicine for the victims of Sichuan quake.
base: mercy relief has sent 17 special tents worth $5,000 each class could accommodate a total of 30 students, besides other
bekalan-bekalan 500 tents as usual, foodstuff and medicines for sichuan quake relief.
para: mercy relief has sent 17 special tents worth $5,000 each class could accommodate a total of 30 students, besides other
supply such as 500 tents, food and medicines for sichuan quake relief.
src : Walaupun hidup susah, kami tetap berusaha untuk menjalani kehidupan seperti biasa.
ref1: Even though life is difficult, we are still trying to go through life as usual.
base: despite the hard life, we will always strive to undergo training as usual.
para: despite the hard life, we will always strive to live normal.
Table 4: Example translations. For each example, we show a source sentence (src), one of the three reference
translations (ref1), and the outputs of baseline (base) and of lattice + sent-par + word-par + phrase-par (para).
In the second example, the baseline system trans-
lated menjalani kehidupan (lit. ?go through life?)
as undergo training, because of a bad phrase pair,
which was extracted from wrong word alignments.
Note that the words menjalani (?go through?) and
kehidupan (?life/existence?) are derivational forms
of jalan (?go?) and hidup (?life/living?), respectively.
Thus, in the paraphrasing system, they were in-
volved in sentence-level paraphrasing, where the
alignments were improved. While the wrong phrase
pair was still available, the system chose a better one
from the paraphrased training bi-text.
6 Related Work
Most research in SMT for a morphologically rich
source language has focused on inflected forms of
the same word. The assumption is that they would
have similar semantics and thus could have the same
translation. Researchers have used stemming (Yang
and Kirchhoff, 2006), lemmatization (Al-Onaizan et
al., 1999; Goldwater and McClosky, 2005; Dyer,
2007), or direct clustering (Talbot and Osborne,
2006) to identify such groups of words and use them
as equivalence classes or as possible alternatives in
translation. Frameworks for the simultaneous use of
different word-level representations have been pro-
posed as well (Koehn and Hoang, 2007).
A second important line of research has focused
on word segmentation, which is useful for languages
like German, which are rich in compound words that
are spelled concatenated (Koehn and Knight, 2003;
Yang and Kirchhoff, 2006), or like Arabic, Turk-
ish, Finnish, and, to a lesser extent, Spanish and
Italian, where clitics often attach to the preceding
word (Habash and Sadat, 2006). For languages with
more or less regular inflectional morphology like
Arabic or Turkish, another good idea is to segment
words into morpheme sequences, e.g., prefix(es)-
stem-suffix(es), which can be used instead of the
original words (Lee, 2004) or in addition to them.
This can be achieved using a lattice input to the
translation system (Dyer et al, 2008; Dyer, 2009).
Unfortunately, none of these general lines of re-
search suits Malay well, whose compounds are
rarely concatenated, clitics are not so frequent, and
morphology is mostly derivational, and thus likely
to generate words whose semantics substantially dif-
fers from the semantics of the original word. There-
fore, we cannot expect the existence of equivalence
classes: it is only occasionally that two derivation-
ally related wordforms would share the same tar-
get language translation. Thus, instead of look-
ing for equivalence classes, we have focused on the
pairwise relationship between derivationally related
wordforms, which we treat as potential paraphrases.
Our approach is an extension of the ?noisier?
channel model of Dyer (2007). He starts by generat-
ing separate word alignments for the original train-
ing bi-text and for a version of it where the source
side has been lemmatized. Then, the two bi-texts
and their word alignments are concatenated and used
to build a phrase table. Finally, the source sides of
the development and the test datasets are converted
into confusion networks where additional arcs are
added for word lemmata. The arc weights are set to
1 for the original wordforms and to 0 for the lem-
mata. In contrast, we provide multiple paraphras-
ing alternatives for each morphologically complex
word, including derivational forms that occupy in-
termediary positions between the original wordform
1305
and its lemma. Note that some of those paraphrasing
alternatives are multi-word, and thus we use a lattice
instead of a confusion network. Moreover, we give
different weights to the different alternatives rather
then assigning them all 0.
Second, our work is related to that of Dyer et
al. (2008), who use a lattice to add a single alter-
native clitic-segmented version of the original word
for Arabic. However, we provide multiple alterna-
tives. We also include derivational forms in addi-
tion to clitic-segmented ones, and we give different
weights to the different alternatives (instead of 0).
Third, our work is also related to that of Dyer
(2009), who uses a lattice to add multiple alterna-
tive segmented versions of the original word for Ger-
man, Hungarian, and Turkish. However, we focus
on derivational morphology rather than on clitics
and inflections, add derivational forms in addition
to clitic-segmented ones, and use cross-lingual word
pivoting to estimate paraphrase probabilities.
Finally, our work is related to that of Callison-
Burch et al (2006), who use cross-lingual pivot-
ing to generate phrase-level paraphrases with corre-
sponding probabilities. However, our paraphrases
are derived through morphological analysis; thus,
we do not need corpora in additional languages.
7 Conclusion and Future Work
We have presented a novel approach to trans-
lating from a morphologically complex language,
which uses paraphrases and paraphrasing tech-
niques at three different levels of translation: word-
level, phrase-level, and sentence-level. Our experi-
ments translating from Malay, whose morphology is
mostly derivational, into English have shown signif-
icant improvements over rivaling approaches based
on several automatic evaluation measures.
In future work, we want to improve the proba-
bility estimations for our paraphrasing models. We
also want to experiment with other morphologically
complex languages and other SMT models.
Acknowledgments
This work was supported by research grant
POD0713875. We would like to thank the anony-
mous reviewers for their detailed and constructive
comments, which have helped us improve the paper.
References
Mirna Adriani, Jelita Asian, Bobby Nazief, S. M.M.
Tahaghoghi, and Hugh E. Williams. 2007. Stemming
Indonesian: A confix-stripping approach. ACM Trans-
actions on Asian Language Information Processing,
6:1?33.
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-Josef
Och, David Purdy, Noah A. Smith, and David
Yarowsky. 1999. Statistical machine translation.
Technical report, JHU Summer Workshop.
Timothy Baldwin and Su?ad Awab. 2006. Open source
corpus analysis tools for Malay. In Proceedings of the
5th International Conference on Language Resources
and Evaluation, LREC ?06, pages 2212?2215.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics, HLT-NAACL ?06, pages 17?24.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, ACL ?05, pages 263?270.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics,
ACL ?05, pages 531?540.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of the 46th Annual Meeting of the Association
for Computational Linguistics, ACL ?08, pages 1012?
1020.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of the ACL 2010 System Demonstrations, ACL
?10, pages 7?12.
Christopher Dyer. 2007. The ?noisier channel?: trans-
lation from morphologically complex languages. In
Proceedings of the Second Workshop on Statistical
Machine Translation, WMT ?07, pages 207?211.
Chris Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for MT. In Proceedings
of Human Language Technologies: The 2009 Annual
1306
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, NAACL ?09,
pages 406?414.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of the Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, HLT-NAACL ?04,
pages 273?280.
Sharon Goldwater and David McClosky. 2005. Improv-
ing statistical MT through morphological analysis. In
Proceedings of the Conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT-EMNLP ?05, pages 676?683.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
Proceedings of the Human Language Technology Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, Companion Vol-
ume: Short Papers, HLT-NAACL ?06, pages 49?52.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, EMNLP-CoNLL ?07, pages 868?876.
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of the 10th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, EACL ?03, pages
187?193.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology, NAACL ?03,
pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics Companion Volume on
Demo and Poster Sessions, ACL ?07, pages 177?180.
Alon Lavie and Michael J. Denkowski. 2009. The me-
teor metric for automatic evaluation of machine trans-
lation. Machine Translation, 23:105?115.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT-NAACL ?04, pages 57?60.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. TESLA: Translation evaluation of sentences
with linear-programming-based analysis. In Proceed-
ings of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, WMT ?10, pages 354?
359.
Preslav Nakov and Hwee Tou Ng. 2009. Improved statis-
tical machine translation for resource-poor languages
using related resource-rich languages. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?09, pages 1358?
1367.
Preslav Nakov. 2008. Improved statistical machine
translation using monolingual paraphrases. In Pro-
ceedings of the 18th European Conference on Artificial
Intelligence, ECAI ?08, pages 338?342.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, ACL ?03, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, ACL ?02, pages 311?318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, ACL ?05, pages 271?279.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the Association for Machine Trans-
lation in the Americas, AMTA ?06, pages 223?231.
David Talbot and Miles Osborne. 2006. Modelling lex-
ical redundancy for machine translation. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, COLING-
ACL ?06, pages 969?976.
Hua Wu and Haifeng Wang. 2007. Pivot language
approach for phrase-based statistical machine transla-
tion. Machine Translation, 21(3):165?181.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly in-
flected languages. In Proceedings of the European
Chapter of the Association for Computational Linguis-
tics, EACL ?06, pages 41?48.
1307
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 301?305,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Combining Word-Level and Character-Level Models
for Machine Translation Between Closely-Related Languages
Preslav Nakov
Qatar Computing Research Institute
Qatar Foundation, P.O. box 5825
Doha, Qatar
pnakov@qf.org.qa
Jo?rg Tiedemann
Department of Linguistics and Philology
Uppsala University
Uppsala, Sweden
jorg.tiedemann@lingfil.uu.se
Abstract
We propose several techniques for improv-
ing statistical machine translation between
closely-related languages with scarce re-
sources. We use character-level translation
trained on n-gram-character-aligned bitexts
and tuned using word-level BLEU, which we
further augment with character-based translit-
eration at the word level and combine with
a word-level translation model. The evalua-
tion on Macedonian-Bulgarian movie subtitles
shows an improvement of 2.84 BLEU points
over a phrase-based word-level baseline.
1 Introduction
Statistical machine translation (SMT) systems, re-
quire parallel corpora of sentences and their transla-
tions, called bitexts, which are often not sufficiently
large. However, for many closely-related languages,
SMT can be carried out even with small bitexts by
exploring relations below the word level.
Closely-related languages such as Macedonian
and Bulgarian exhibit a large overlap in their vo-
cabulary and strong syntactic and lexical similari-
ties. Spelling conventions in such related languages
can still be different, and they may diverge more
substantially at the level of morphology. However,
the differences often constitute consistent regulari-
ties that can be generalized when translating.
The language similarities and the regularities in
morphological variation and spelling motivate the
use of character-level translation models, which
were applied to translation (Vilar et al, 2007; Tiede-
mann, 2009a) and transliteration (Matthews, 2007).
Macedonian Bulgarian
a v m e a h m e
a v m e d a a h m e d a
v e r u v a m v  r v a m
d e k a t o j , q e t o $i
Table 1: Examples from a character-level phrase table
(without scores): mappings can cover words and phrases.
Certainly, translation cannot be adequately mod-
eled as simple transliteration, even for closely-
related languages. However, the strength of phrase-
based SMT (Koehn et al, 2003) is that it can support
rather large sequences (phrases) that capture transla-
tions of entire chunks. This makes it possible to in-
clude mappings that go far beyond the edit-distance-
based string operations usually modeled in translit-
eration. Table 1 shows how character-level phrase
tables can cover mappings spanning over multi-word
units. Thus, character-level phrase-based SMT mod-
els combine the generality of character-by-character
transliteration and lexical mappings of larger units
that could possibly refer to morphemes, words or
phrases, as well as to various combinations thereof.
2 Training Character-level SMT Models
We treat sentences as sequences of characters in-
stead of words, as shown in Figure 1. Due to the
reduced vocabulary, we can use higher-order mod-
els, which is necessary in order to avoid the genera-
tion of non-word sequences. In our case, we opted
for a 10-character language model and a maximum
phrase length of 10 (based on initial experiments).
However, word alignment models are not fit for
character-level SMT, where the vocabulary shrinks.
301
original:
MK: navistina ?
BG: naistina ?
characters:
MK: n a v i s t i n a ?
BG: n a i s t i n a ?
character bigrams:
MK: na av vi is st ti in na a ? ?
BG: na ai is st ti in na a ? ?
Figure 1: Preparing the training corpus for alignment.
Statistical word alignment models heavily rely on
context-independent lexical translation parameters
and, therefore, are unable to properly distinguish
character mapping differences in various contexts.
The alignment models used in the transliteration lit-
erature have the same problem as they are usually
based on edit distance operations and finite-state au-
tomata without contextual history (Jiampojamarn et
al., 2007; Damper et al, 2005; Ristad and Yiani-
los, 1998). We, thus, transformed the input to se-
quences of character n-grams as suggested by Tiede-
mann (2012); examples are shown in Figure 1. This
artificially increases the vocabulary as shown in Ta-
ble 2, making standard alignment models and their
lexical translation parameters more expressive.
Macedonian Bulgarian
single characters 99 101
character bigrams 1,851 1,893
character trigrams 13,794 14,305
words 41,816 30,927
Table 2: Vocabulary size of character-level alignment
models and the corresponding word-level model.
It turns out that bigrams constitute a good com-
promise between generality and contextual speci-
ficity, which yields useful character alignments with
good performance in terms of phrase-based transla-
tion. In our experiments, we used GIZA++ (Och
and Ney, 2003) with standard settings and the grow-
diagonal-final-and heuristics to symmetrize the fi-
nal IBM-model-4-based Viterbi alignments (Brown
et al, 1993). The phrases were extracted and scored
using the Moses training tools (Koehn et al, 2007).1
We tuned the parameters of the log-linear SMT
model using minimum error rate training (Och,
2003), optimizing BLEU (Papineni et al, 2002).
1Note that the extracted phrase table does not include se-
quences of character n-grams. We map character n-gram align-
ments to links between single characters before extraction.
Since BLEU over matching character sequences
does not make much sense, especially if the k-gram
size is limited to small values of k (usually, 4 or
less), we post-processed n-best lists in each tuning
step to calculate the usual word-based BLEU score.
3 Transliteration
We also built a character-level SMT system for
word-level transliteration, which we trained on a list
of automatically extracted pairs of likely cognates.
3.1 Cognate Extraction
Classic NLP approaches to cognate extraction look
for words with similar spelling that co-occur in par-
allel sentences (Kondrak et al, 2003). Since our
Macedonian-Bulgarian bitext (MK?BG) was small,
we further used a MK?EN and an EN?BG bitext.
First, we induced IBM-model-4 word alignments
for MK?EN and EN?BG, from which we extracted
four conditional lexical translation probabilities:
Pr(m|e) and Pr(e|m) for MK?EN, and Pr(b|e) and
Pr(e|b) for EN?BG, where m, e, and b stand for a
Macedonian, an English, and a Bulgarian word.
Then, following (Callison-Burch et al, 2006; Wu
and Wang, 2007; Utiyama and Isahara, 2007), we
induced conditional lexical translation probabilities
as Pr(m|b) =
?
e Pr(m|e) Pr(e|b), where Pr(m|e)
and Pr(e|b) are estimated using maximum likeli-
hood from MK?EN and EN?BG word alignments.
Then, we induced translation probability estima-
tions for the reverse direction Pr(b|m) and we cal-
culated the quantity Piv(m, b) = Pr(m|b) Pr(b|m).
We calculated a similar quantity Dir(m, b), where
the probabilities Pr(m|b) and Pr(b|m) are estimated
using maximum likelihood from the MK?BG bitext
directly. Finally, we calculated the similarity score
S(m, b) = Piv(m, b)+Dir(m, b)+2?LCSR(m, b),
where LCSR is the longest common subsequence of
two strings, divided by the length of the longer one.
The score S(m, b) is high for words that are likely
to be cognates, i.e., that (i) have high probability of
being mutual translations, which is expressed by the
first two terms in the summation, and (ii) have sim-
ilar spelling, as expressed by the last term. Here we
give equal weight to Dir(m, b) and Piv(m, b); we
also give equal weights to the translational similar-
ity (the sum of the first two terms) and to the spelling
similarity (twice LCSR).
302
We excluded all words of length less than three, as
well as all Macedonian-Bulgarian word pairs (m, b)
for which Piv(m, b) + Dir(m, b) < 0.01, and those
for which LCSR(m, b) was below 0.58, a value
found by Kondrak et al (2003) to work well for a
number of European language pairs.
Finally, using S(m, b), we induced a weighted bi-
partite graph, and we performed a greedy approxi-
mation to the maximum weighted bipartite matching
in that graph using competitive linking (Melamed,
2000), to produce the final list of cognate pairs.
Note that the above-described cognate extraction
algorithm has three important components: (1) or-
thographic, based on LCSR, (2) semantic, based
on word alignments and pivoting over English, and
(3) competitive linking. The orthographic compo-
nent is essential when looking for cognates since
they must have similar spelling by definition, while
the semantic component prevents the extraction of
false friends like vreden, which means ?valuable?
in Macedonian but ?harmful? in Bulgarian. Finally,
competitive linking helps prevent issues related to
word inflection that cannot be handled using the se-
mantic component alone.
3.2 Transliteration Training
For each pair in the list of cognate pairs, we added
spaces between any two adjacent letters for both
words, and we further appended special start and
end characters. We split the resulting list into
training, development and testing parts and we
trained and tuned a character-level Macedonian-
Bulgarian phrase-based monotone SMT system sim-
ilar to that in (Finch and Sumita, 2008; Tiedemann
and Nabende, 2009; Nakov and Ng, 2009; Nakov
and Ng, 2012). The system used a character-level
Bulgarian language model trained on words. We set
the maximum phrase length and the language model
order to 10, and we tuned the system using MERT.
3.3 Transliteration Lattice Generation
Given a Macedonian sentence, we generated a lat-
tice where each input Macedonian word of length
three or longer was augmented with Bulgarian al-
ternatives: n-best transliterations generated by the
above character-level Macedonian-Bulgarian SMT
system (after the characters were concatenated to
form a word and the special symbols were removed).
In the lattice, we assigned the original Macedo-
nian word the weight of 1; for the alternatives, we
assigned scores between 0 and 1 that were the sum
of the translation model probabilities of generating
each alternative (the sum was needed since some op-
tions appeared multiple times in the n-best list).
4 Experiments and Evaluation
For our experiments, we used translated movie sub-
titles from the OPUS corpus (Tiedemann, 2009b).
For Macedonian-Bulgarian there were only about
102,000 aligned sentences containing approximately
1.3 million tokens altogether. There was substan-
tially more monolingual data available for Bulgar-
ian: about 16 million sentences containing ca. 136
million tokens.
However, this data was noisy. Thus, we realigned
the corpus using hunalign and we removed some
Bulgarian files that were misclassified as Macedo-
nian and vice versa, using a BLEU-filter. Fur-
thermore, we also removed sentence pairs contain-
ing language-specific characters on the wrong side.
From the remaining data we selected 10,000 sen-
tence pairs (roughly 128,000 words) for develop-
ment and another 10,000 (ca. 125,000 words) for
testing; we used the rest for training.
The evaluation results are summarized in Table 3.
MK?BG BLEU % NIST TER METEOR
Transliteration
no translit. 10.74 3.33 67.92 60.30
t1 letter-based 12.07 3.61 66.42 61.87
t2 cogn.+lattice 22.74 5.51 55.99 66.42
Word-level SMT
w0 Apertium 21.28 5.27 56.92 66.35
w1 SMT baseline 31.10 6.56 50.72 70.53
w2 w1 + t1-lattice 32.19(+1.19) 6.76 49.68 71.18
Character-level SMT
c1 char-aligned 32.28(+1.18) 6.70 49.70 71.35
c2 bigram-aligned 32.71(+1.61) 6.77 49.23 71.65
trigram-aligned 32.07(+0.97) 6.68 49.82 71.21
System combination
w2 + c2 32.92(+1.82) 6.90 48.73 71.71
w1 + c2 33.31(+2.21) 6.91 48.60 71.81
Merged phrase tables
m1 w1 + c2 33.33(+2.13) 6.86 48.86 71.73
m2 w2 + c2 33.94(+2.84) 6.89 48.99 71.76
Table 3: Macedonian-Bulgarian translation and
transliteration. Superscripts show the absolute improve-
ment in BLEU compared to the word-level baseline (w1).
303
Transliteration. The top rows of Table 3 show
the results for Macedonian-Bulgarian transliteration.
First, we can see that the BLEU score for the original
Macedonian testset evaluated against the Bulgarian
reference is 10.74, which is quite high and reflects
the similarity between the two languages. The next
line (t1) shows that many differences between Mace-
donian and Bulgarian stem from mere differences in
orthography: we mapped the six letters in the Mace-
donian alphabet that do not exist in the Bulgarian al-
phabet to corresponding Bulgarian letters and letter
sequences, gaining over 1.3 BLEU points. The fol-
lowing line (t2) shows the results using the sophis-
ticated transliteration described in Section 3, which
takes two kinds of context into account: (1) word-
internal letter context, and (2) sentence-level word
context. We generated a lattice for each Macedonian
test sentence, which included the original Mace-
donian words and the 1-best2 Bulgarian transliter-
ation option from the character-level transliteration
model. We then decoded the lattice using a Bulgar-
ian language model; this increased BLEU to 22.74.
Word-level translation. Naturally, lattice-based
transliteration cannot really compete against stan-
dard word-level translation (w1), which is better
by 8 BLEU points. Still, as line (w2) shows,
using the 1-best transliteration lattice as an input
to (w1) yields3 consistent improvement over (w1)
for four evaluation metrics: BLEU (Papineni et
al., 2002), NIST v. 13, TER (Snover et al, 2006)
v. 0.7.25, and METEOR (Lavie and Denkowski,
2009) v. 1.3. The baseline system is also signifi-
cantly better than the on-line version of Apertium
(http://www.apertium.org/), a shallow transfer-rule-
based MT system that is optimized for closely-
related languages (accessed on 2012/05/02). Here,
Apertium suffers badly from a large number of un-
known words in our testset (ca. 15%).
Character-level translation. Moving down to
the next group of experiments in Table 3, we can
see that standard character-level SMT (c1), i.e.,
simply treating characters as separate words, per-
forms significantly better than word-level SMT. Us-
ing bigram-based character alignments yields fur-
ther improvement of +0.43 BLEU.
2Using 3/5/10/100-best made very little difference.
3The decoder can choose between (a) translating a Macedo-
nian word and (b) using its 1-best Bulgarian transliteration.
System combination. Since word-level and
character-level models have different strengths and
weaknesses, we further tried to combine them.
We used MEMT, a state-of-the-art Multi-Engine
Machine Translation system (Heafield and Lavie,
2010), to combine the outputs of (c3) with the out-
put of (w1) and of (w2). Both combinations im-
proved over the individual systems, but (w1)+(c2)
performed better, by +0.6 BLEU points over (c2).
Combining word-level and phrase-level SMT.
Finally, we also combined (w1) with (c3) in a more
direct way: by merging their phrase tables. First,
we split the phrases in the word-level phrase tables
of (w1) to characters as in character-level models.
Then, we generated four versions of each phrase
pair: with/without ? ? at the beginning/end of the
phrase. Finally, we merged these phrase pairs with
those in the phrase table of (c3), adding two ex-
tra features indicating each phrase pair?s origin: the
first/second feature is 1 if the pair came from the
first/second table, and 0.5 otherwise. This combina-
tion outperformed MEMT, probably because it ex-
pands the search space of the SMT system more di-
rectly. We further tried scoring with two language
models in the process of translation, character-based
and word-based, but we did not get consistent im-
provements. Finally, we experimented with a 1-best
character-level lattice input that encodes the same
options and weights as for (w2). This yielded our
best overall BLEU score of 33.94, which is +2.84
BLEU points of absolute improvement over the (w1)
baseline, and +1.23 BLEU points over (c2).4
5 Conclusion and Future Work
We have explored several combinations of character-
and word-level translation models for translating
between closely-related languages with scarce re-
sources. In future work, we want to use such a model
for pivot-based translations from the resource-poor
language (Macedonian) to other languages (such as
English) via the related language (Bulgarian).
Acknowledgments
The research is partially supported by the EU ICT
PSP project LetsMT!, grant number 250456.
4All improvements over (w1) in Table 3 that are greater or
equal to 0.97 BLEU points are statistically significant according
to Collins? sign test (Collins et al, 2005).
304
References
Peter Brown, Vincent Della Pietra, Stephen Della Pietra,
and Robert Mercer. 1993. The mathematics of statis-
tical machine translation: parameter estimation. Com-
putational Linguistics, 19(2):263?311.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of HLT-NAACL
?06, pages 17?24, New York, NY.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL ?05, pages 531?
540, Ann Arbor, MI.
Robert Damper, Yannick Marchand, John-David
Marsters, and Alex Bazin. 2005. Aligning text and
phonemes for speech technology applications using an
EM-like algorithm. International Journal of Speech
Technology, 8(2):149?162.
Andrew Finch and Eiichiro Sumita. 2008. Phrase-based
machine transliteration. In Proceedings of the Work-
shop on Technologies and Corpora for Asia-Pacific
Speech Translation, pages 13?18, Hyderabad, India.
Kenneth Heafield and Alon Lavie. 2010. Combin-
ing machine translation output with open source:
The Carnegie Mellon multi-engine machine transla-
tion scheme. The Prague Bulletin of Mathematical
Linguistics, 93(1):27?36.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden Markov models to letter-to-phoneme con-
version. In Proceedings of NAACL-HLT ?07, pages
372?379, Rochester, New York.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL ?03, pages 48?54, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL ?07, pages 177?180, Prague, Czech Re-
public.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can improve statistical translation
models. In Proceedings of NAACL ?03, pages 46?48,
Edmonton, Canada.
Alon Lavie and Michael Denkowski. 2009. The Meteor
metric for automatic evaluation of machine translation.
Machine Translation, 23:105?115.
David Matthews. 2007. Machine transliteration of
proper names. Master?s thesis, School of Informatics,
University of Edinburgh, Edinburgh, UK.
Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Preslav Nakov and Hwee Tou Ng. 2009. Improved statis-
tical machine translation for resource-poor languages
using related resource-rich languages. In Proceedings
of EMNLP ?09, pages 1358?1367, Singapore.
Preslav Nakov and Hwee Tou Ng. 2012. Improving
statistical machine translation for a resource-poor lan-
guage using related resource-rich languages. Journal
of cial Intelligence Research, 44.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
?03, pages 160?167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL
?02, pages 311?318, Philadelphia, PA.
Eric Ristad and Peter Yianilos. 1998. Learning string
edit distance. IEEE Transactions on Pattern Recogni-
tion and Machine Intelligence, 20(5):522?532.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA ?06, pages 223?231.
Jo?rg Tiedemann and Peter Nabende. 2009. Translating
transliterations. International Journal of Computing
and ICT Research, 3(1):33?41.
Jo?rg Tiedemann. 2009a. Character-based PSMT for
closely related languages. In Proceedings of EAMT
?09, pages 12?19, Barcelona, Spain.
Jo?rg Tiedemann. 2009b. News from OPUS - A collection
of multilingual parallel corpora with tools and inter-
faces. In Recent Advances in Natural Language Pro-
cessing, volume V, pages 237?248. John Benjamins.
Jo?rg Tiedemann. 2012. Character-based pivot transla-
tion for under-resourced languages and domains. In
Proceedings of EACL ?12, pages 141?151, Avignon,
France.
Masao Utiyama and Hitoshi Isahara. 2007. A compar-
ison of pivot methods for phrase-based statistical ma-
chine translation. In Proceedings of NAACL-HLT ?07,
pages 484?491, Rochester, NY.
David Vilar, Jan-Thorsten Peter, and Hermann Ney.
2007. Can we translate letters? In Proceedings of
WMT ?07, pages 33?39, Prague, Czech Republic.
Hua Wu and Haifeng Wang. 2007. Pivot language
approach for phrase-based statistical machine transla-
tion. Machine Translation, 21(3):165?181.
305
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 12?17,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Tale about PRO and Monsters
Preslav Nakov, Francisco Guzma?n and Stephan Vogel
Qatar Computing Research Institute, Qatar Foundation
Tornado Tower, floor 10, PO box 5825
Doha, Qatar
{pnakov,fherrera,svogel}@qf.org.qa
Abstract
While experimenting with tuning on long
sentences, we made an unexpected discov-
ery: that PRO falls victim to monsters ?
overly long negative examples with very
low BLEU+1 scores, which are unsuitable
for learning and can cause testing BLEU
to drop by several points absolute. We
propose several effective ways to address
the problem, using length- and BLEU+1-
based cut-offs, outlier filters, stochastic
sampling, and random acceptance. The
best of these fixes not only slay and pro-
tect against monsters, but also yield higher
stability for PRO as well as improved test-
time BLEU scores. Thus, we recommend
them to anybody using PRO, monster-
believer or not.
1 Once Upon a Time...
For years, the standard way to do statistical ma-
chine translation parameter tuning has been to
use minimum error-rate training, or MERT (Och,
2003). However, as researchers started using mod-
els with thousands of parameters, new scalable op-
timization algorithms such as MIRA (Watanabe et
al., 2007; Chiang et al, 2008) and PRO (Hopkins
and May, 2011) have emerged. As these algo-
rithms are relatively new, they are still not quite
well understood, and studying their properties is
an active area of research.
For example, Nakov et al (2012) have pointed
out that PRO tends to generate translations that
are consistently shorter than desired. They
have blamed this on inadequate smoothing in
PRO?s optimization objective, namely sentence-
level BLEU+1, and they have addressed the prob-
lem using more sensible smoothing. We wondered
whether the issue could be partially relieved sim-
ply by tuning on longer sentences, for which the
effect of smoothing would naturally be smaller.
To our surprise, tuning on the longer 50% of the
tuning sentences had a disastrous effect on PRO,
causing an absolute drop of three BLEU points
on testing; at the same time, MERT and MIRA
did not have such a problem. While investigating
the reasons, we discovered hundreds of monsters
creeping under PRO?s surface...
Our tale continues as follows. We first explain
what monsters are in Section 2, then we present a
theory about how they can be slayed in Section 3,
we put this theory to test in practice in Section 4,
and we discuss some related efforts in Section 5.
Finally, we present the moral of our tale, and we
hint at some planned future battles in Section 6.
2 Monsters, Inc.
PRO uses pairwise ranking optimization, where
the learning task is to classify pairs of hypotheses
into correctly or incorrectly ordered (Hopkins and
May, 2011). It searches for a vector of weights
w such that higher evaluation metric scores cor-
respond to higher model scores and vice versa.
More formally, PRO looks for weights w such that
g(i, j) > g(i, j?) ? hw(i, j) > hw(i, j?), where
g is a local scoring function (typically, sentence-
level BLEU+1) and hw are the model scores for
a given input sentence i and two candidate hy-
potheses j and j? that were obtained using w. If
g(i, j) > g(i, j?), we will refer to j and j? as the
positive and the negative example in the pair.
Learning good parameter values requires nega-
tive examples that are comparable to the positive
ones. Instead, tuning on long sentences quickly
introduces monsters, i.e., corrupted negative ex-
amples that are unsuitable for learning: they are
(i) much longer than the respective positive ex-
amples and the references, and (ii) have very low
BLEU+1 scores compared to the positive exam-
ples and in absolute terms. The low BLEU+1
means that PRO effectively has to learn from pos-
itive examples only.
12
Avg. Lengths Avg. BLEU+1
iter. pos neg ref. pos neg
1 45.2 44.6 46.5 52.5 37.6
2 46.4 70.5 53.2 52.8 14.5
3 46.4 261.0 53.4 52.4 2.19
4 46.4 250.0 53.0 52.0 2.30
5 46.3 248.0 53.0 52.1 2.34
. . . . . . . . . . . . . . . . . .
25 47.9 229.0 52.5 52.2 2.81
Table 1: PRO iterations, tuning on long sentences.
Table 1 shows an optimization run of PRO when
tuning on long sentences. We can see monsters
after iterations in which positive examples are on
average longer than negative ones (e.g., iter. 1).
As a result, PRO learns to generate longer sen-
tences, but it overshoots too much (iter. 2), which
gives rise to monsters. Ideally, the learning algo-
rithm should be able to recover from overshoot-
ing. However, once monsters are encountered,
they quickly start dominating, with no chance for
PRO to recover since it accumulates n-best lists,
and thus also monsters, over iterations. As a result,
PRO keeps jumping up and down and converges to
random values, as Figure 1 shows.
By default, PRO?s parameters are averaged
over iterations, and thus the final result is quite
mediocre, but selecting the highest tuning score
does not solve the problem either: for example,
on Figure 1, PRO never achieves a BLEU better
than that for the default initialization parameters.
?
?
? ?
?
?
?
? ?
?
? ? ? ? ? ? ? ? ? ? ? ?
?
? ?
5 10 15 20 250
10
20
30
40
iteration
BLE
U sc
ore
?
?
?
?
? ?
?
? ?
?
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 1
2
3
4
5
Leng
th ra
tio
Figure 1: PRO tuning results on long sentences
across iterations. The dark-gray line shows the
tuning BLEU (left axis), the light-gray one is the
hypothesis/reference length ratio (right axis).
Figure 2 shows the translations after iterations
1, 3 and 4; the last two are monsters. The monster
at iteration 3 is potentially useful, but that at itera-
tion 4 is clearly unsuitable as a negative example.
Optimizer Objective BLEU
PRO sent-BLEU+1 44.57
MERT corpus-BLEU 47.53
MIRA pseudo-doc-BLEU 47.80
PRO (6= objective) pseudo-doc-BLEU 21.35
MIRA (6= objective) sent-BLEU+1 47.59
PRO, PC-smooth, ground fixed sent-BLEU+1 45.71
Table 2: PRO vs. MERT vs. MIRA.
We also checked whether other popular opti-
mizers yield very low BLEU scores at test time
when tuned on long sentences. Lines 2-3 in Ta-
ble 2 show that this is not the case for MERT and
MIRA. Since they optimize objectives that are dif-
ferent from PRO?s,1 we further experimented with
plugging MIRA?s objective into PRO and PRO?s
objective into MIRA. The resulting MIRA scores
were not much different from before, while PRO?s
score dropped even further; we also found mon-
sters. Next, we applied the length fix for PRO
proposed in (Nakov et al, 2012); this helped a
bit, but still left PRO two BLEU points behind
MERT2 and MIRA, and the monsters did not go
away. We can conclude that the monster problem
is PRO-specific, cannot be blamed on the objective
function, and is different from the length bias.
Note also that monsters are not specific to a
dataset or language pair. We found them when
tuning on the top-50% of WMT10 and testing on
WMT11 for Spanish-English; this yielded a drop
in BLEU from 29.63 (MERT) to 27.12 (PRO).
From run 110 /home/guzmanhe/NIST12/ems/preslav-mada-atb/tuning/tmp.110
**REF**: but we have to close ranks with each other and realize that in 
unity there is strength while in division there is weakness . 
-----------------------------------------------------
**IT1**: but we are that we add our ranks to some of us and that we know 
that in the strength and weakness in
**IT3**:, we are the but of the that that the , and , of ranks the the on 
the the our the our the some of we can include , and , of to the of we know 
the the our in of the of some people , force of the that that the in of the 
that that the the weakness Union the the , and
**IT4**: namely Dr Heba Handossah and Dr Mona been pushed aside because a 
larger story EU Ambassador to Egypt Ian Burg highlighted 've dragged us 
backwards and dragged our speaking , never balme your defaulting a December 
7th 1941 in Pearl Harbor ) we can include ranks will be joined by all 've 
dragged us backwards and dragged our $ 3.8 billion in tourism income 
proceeds Chamber are divided among themselves : some 've dragged us 
backwards and dragged our were exaggerated . Al @-@ Hakim namely Dr Heba 
Handossah and Dr Mona December 7th 1941 in Pearl Harbor ) cases might be 
known to us December 7th 1941 in Pearl Harbor ) platform depends on 
combating all liberal policies Track and Field Federation shortened strength 
as well face several challenges , namely Dr Heba Handossah and Dr Mona 
platform depends on combating all liberal policies the report forecast that 
the weak structure
**IT7**: , the sakes of our on and the , the we can include however , the Al 
ranks the the on the , to the = last of we , the long of the part of some of 
to the affect that the of some is the with ] us our to the affect that the 
with ] us our of the in baker , the cook , the on and the , the we know , 
has are in the heaven of to the affect that the of weakness of @-@ Ittihad 
@-@ Al the force , to 
Figure 2: Example reference translation and hy-
pothesis translations after iterations 1, 3 and 4.
The last two hypotheses are monsters.
1See (Cherry and Foster, 2012) for details on objectives.
2Also, using PRO to initialize MERT, as implemented in
Moses, yields 46.52 BLEU and monsters, but using MERT to
initialize PRO yields 47.55 and no monsters.
13
3 Slaying Monsters: Theory
Below we explain what monsters are and where
they come from. Then, we propose various mon-
ster slaying techniques to be applied during PRO?s
selection and acceptance steps.
3.1 What is PRO?
PRO is a batch optimizer that iterates between
(i) translation: using the current parameter values,
generate k-best translations, and (ii) optimization:
using the translations from all previous iterations,
find new parameter values. The optimization step
has four substeps:
1. Sampling: For each sentence, sample uni-
formly at random ? = 5000 pairs from the
set of all candidate translations for that sen-
tence from all previous iterations.
2. Selection: From these sampled pairs, select
those for which the absolute difference be-
tween their BLEU+1 scores is higher than
? = 0.05 (note: this is 5 BLEU+1 points).
3. Acceptance: For each sentence, accept the
? = 50 selected pairs with the highest abso-
lute difference in their BLEU+1 scores.
4. Learning: Assemble the accepted pairs for
all sentences into a single set and use it to
train a ranker to prefer the higher-scoring
sentence in each pair.
We believe that monsters are nurtured by PRO?s
selection and acceptance policies. PRO?s selec-
tion step filters pairs involving hypotheses that dif-
fer by less than five BLEU+1 points, but it does
not cut-off ones that differ too much based on
BLEU+1 or length. PRO?s acceptance step selects
? = 50 pairs with the highest BLEU+1 differ-
entials, which creates breeding ground for mon-
sters since these pairs are very likely to include
one monster and one good hypothesis.
Below we discuss monster slaying geared to-
wards the selection and acceptance steps of PRO.
3.2 Slaying at Selection
In the selection step, PRO filters pairs for which
the difference in BLEU+1 is less than five points,
but it has no cut-off on the maximum BLEU+1 dif-
ferentials nor cut-offs based on absolute length or
difference in length. Here, we propose several se-
lection filters, both deterministic and probabilistic.
Cut-offs. A cut-off is a deterministic rule that
filters out pairs that do not comply with some cri-
teria. We experiment with a maximal cut-off on
(a) the difference in BLEU+1 scores and (b) the
difference in lengths. These are relative cut-offs
because they refer to the pair, but absolute cut-offs
that apply to each of the elements in the pair are
also possible (not explored here). Cut-offs (a) and
(b) slay monsters by not allowing the negative ex-
amples to get much worse in BLEU+1 or in length
than the positive example in the pair.
Filtering outliers. Outliers are rare or extreme
observations in a sample. We assume normal dis-
tribution of the BLEU+1 scores (or of the lengths)
of the translation hypotheses for the same source
sentence, and we define as outliers hypotheses
whose BLEU+1 (or length) is more than ? stan-
dard deviations away from the sample average.
We apply the outlier filter to both the positive and
the negative example in a pair, but it is more im-
portant for the latter. We experiment with values
of ? like 2 and 3. This filtering slays monsters be-
cause they are likely outliers. However, it will not
work if the population gets riddled with monsters,
in which case they would become the norm.
Stochastic sampling. Instead of filtering ex-
treme examples, we can randomly sample pairs
according to their probability of being typical. Let
us assume that the values of the local scoring func-
tions, i.e., the BLEU+1 scores, are distributed nor-
mally: g(i, j) ? N(?, ?2). Given a sample of hy-
pothesis translations {j} of the same source sen-
tence i, we can estimate ? empirically. Then,
the difference ? = g(i, j) ? g(i, j?) would be
distributed normally with mean zero and variance
2?2. Now, given a pair of examples, we can calcu-
late their ?, and we can choose to select the pair
with some probability, according to N(0, 2?2).
3.3 Slaying at Acceptance
Another problem is caused by the acceptance
mechanism of PRO: among all selected pairs, it
accepts the top-? with the highest BLEU+1 dif-
ferentials. It is easy to see that these differentials
are highest for nonmonster?monster pairs if such
pairs exist. One way to avoid focusing primarily
on such pairs is to accept a random set of ? pairs,
among the ones that survived the selection step.
One possible caveat is that we can lose some of
the discriminative power of PRO by focusing on
examples that are not different enough.
14
TESTING TUNING (run 1, it. 25, avg.) TEST(tune:full)
Avg. for 3 reruns Lengths BLEU+1 Avg. for 3 reruns
PRO fix BLEU StdDev Pos Neg Ref Pos Neg BLEU StdDev
PRO (baseline) 44.70 0.266 47.9 229.0 52.5 52.2 2.8 47.80 0.052
Max diff. cut-off BLEU+1 max=10 ? 47.94 0.165 47.9 49.6 49.4 49.4 39.9 47.77 0.035
BLEU+1 max=20 ? 47.73 0.136 47.7 55.5 51.1 49.8 32.7 47.85 0.049
LEN max=5 ? 48.09 0.021 46.8 47.0 47.9 52.9 37.8 47.73 0.051
LEN max=10 ? 47.99 0.025 47.3 48.5 48.7 52.5 35.6 47.80 0.056
Outliers BLEU+1 ?=2.0 ? 48.05 0.119 46.8 47.2 47.7 52.2 39.5 47.47 0.090
BLEU+1 ?=3.0 47.12 1.348 47.6 168.0 53.0 51.7 3.9 47.53 0.038
LEN ?=2.0 46.68 2.005 49.3 82.7 53.1 52.3 5.3 47.49 0.085
LEN ?=3.0 47.02 0.727 48.2 163.0 51.4 51.4 4.2 47.65 0.096
Stoch. sampl. ? BLEU+1 46.33 1.000 46.8 216.0 53.3 53.1 2.4 47.74 0.035
? LEN 46.36 1.281 47.4 201.0 52.9 53.4 2.9 47.78 0.081
Table 3: Some fixes to PRO (select pairs with highest BLEU+1 differential, also require at least 5
BLEU+1 points difference). A dagger (?) indicates selection fixes that successfully get rid of monsters.
4 Attacking Monsters: Practice
Below, we first present our general experimental
setup. Then, we present the results for the var-
ious selection alternatives, both with the original
acceptance strategy and with random acceptance.
4.1 Experimental Setup
We used a phrase-based SMT model (Koehn et al,
2003) as implemented in the Moses toolkit (Koehn
et al, 2007). We trained on all Arabic-English
data for NIST 2012 except for UN, we tuned on
(the longest-50% of) the MT06 sentences, and we
tested on MT09. We used the MADA ATB seg-
mentation for Arabic (Roth et al, 2008) and true-
casing for English, phrases of maximal length 7,
Kneser-Ney smoothing, and lexicalized reorder-
ing (Koehn et al, 2005), and a 5-gram language
model, trained on GigaWord v.5 using KenLM
(Heafield, 2011). We dropped unknown words
both at tuning and testing, and we used minimum
Bayes risk decoding at testing (Kumar and Byrne,
2004). We evaluated the output with NIST?s scor-
ing tool v.13a, cased.
We used the Moses implementations of MERT,
PRO and batch MIRA, with the ?return-best-dev
parameter for the latter. We ran these optimizers
for up to 25 iterations and we used 1000-best lists.
For stability (Foster and Kuhn, 2009), we per-
formed three reruns of each experiment (tuning +
evaluation), and we report averaged scores.
4.2 Selection Alternatives
Table 3 presents the results for different selection
alternatives. The first two columns show the test-
ing results: average BLEU and standard deviation
over three reruns.
The following five columns show statistics
about the last iteration (it. 25) of PRO?s tuning
for the worst rerun: average lengths of the positive
and the negative examples and average effective
reference length, followed by average BLEU+1
scores for the positive and the negative examples
in the pairs. The last two columns present the re-
sults when tuning on the full tuning set. These are
included to verify the behavior of PRO in a non-
monster prone environment.
We can see in Table 3 that all selection mech-
anisms considerably improve BLEU compared to
the baseline PRO, by 2-3 BLEU points. However,
not every selection alternative gets rid of monsters,
which can be seen by the large lengths and low
BLEU+1 for the negative examples (in bold).
The max cut-offs for BLEU+1 and for lengths
both slay the monsters, but the latter yields much
lower standard deviation (thirteen times lower than
for the baseline PRO!), thus considerably increas-
ing PRO?s stability. On the full dataset, BLEU
scores are about the same as for the original PRO
(with small improvement for BLEU+1 max=20),
but the standard deviations are slightly better.
Rejecting outliers using BLEU+1 and ? = 3 is
not strong enough to filter out monsters, but mak-
ing this criterion more strict by setting ? = 2,
yields competitive BLEU and kills the monsters.
Rejecting outliers based on length does not
work as effectively though. We can think of two
possible reasons: (i) lengths are not normally dis-
tributed, they are more Poisson-like, and (ii) the
acceptance criterion is based on the top-? differ-
entials based on BLEU+1, not based on length.
On the full dataset, rejecting outliers, BLEU+1
and length, yields lower BLEU and less stability.
15
TESTING TUNING (run 1, it. 25, avg.) TEST(tune:full)
Avg. for 3 reruns Lengths BLEU+1 Avg. for 3 reruns
PRO fix BLEU StdDev Pos Neg Ref Pos Neg BLEU StdDev
PRO (baseline) 44.70 0.266 47.9 229.0 52.5 52.2 2.8 47.80 0.052
Rand. accept PRO, rand ?? 47.87 0.147 47.7 48.5 48.70 47.7 42.9 47.59 0.114
Outliers BLEU+1 ?=2.0, rand? 47.85 0.078 48.2 48.4 48.9 47.5 43.6 47.62 0.091
BLEU+1 ?=3.0, rand 47.97 0.168 47.6 47.6 48.4 47.8 43.6 47.44 0.070
LEN ?=2.0, rand? 47.69 0.114 47.8 47.8 48.6 47.9 43.6 47.48 0.046
LEN ?=3.0, rand 47.89 0.235 47.8 48.0 48.7 47.7 43.1 47.64 0.090
Stoch. sampl. ? BLEU+1, rand? 47.99 0.087 47.9 48.0 48.7 47.8 43.5 47.67 0.096
? LEN, rand? 47.94 0.060 47.8 47.9 48.6 47.8 43.6 47.65 0.097
Table 4: More fixes to PRO (with random acceptance, no minimum BLEU+1). The (??) indicates that
random acceptance kills monsters. The asterisk (?) indicates improved stability over random acceptance.
Reasons (i) and (ii) arguably also apply to
stochastic sampling of differentials (for BLEU+1
or for length), which fails to kill the monsters,
maybe because it gives them some probability of
being selected by design. To alleviate this, we test
the above settings with random acceptance.
4.3 Random Acceptance
Table 4 shows the results for accepting training
pairs for PRO uniformly at random. To eliminate
possible biases, we also removed the min=0.05
BLEU+1 selection criterion. Surprisingly, this
setup effectively eliminated the monster problem.
Further coupling this with the distributional cri-
teria can also yield increased stability, and even
small further increase in test BLEU. For instance,
rejecting BLEU outliers with ? = 2 yields com-
parable average test BLEU, but with only half the
standard deviation.
On the other hand, using the stochastic sam-
pling of differentials based on either BLEU+1 or
lengths improves the test BLEU score while in-
creasing the stability across runs. The random
acceptance has a caveat though: it generally de-
creases the discriminative power of PRO, yielding
worse results when tuning on the full, nonmonster
prone tuning dataset. Stochastic selection does
help to alleviate this problem. Yet, the results are
not as good as when using a max cut-off for the
length. Therefore, we recommend using the latter
as a default setting.
5 Related Work
We are not aware of previous work that discusses
the issue of monsters, but there has been work on
a different, length problem with PRO (Nakov et
al., 2012). We have seen that its solution, fix the
smoothing in BLEU+1, did not work for us.
The stability of MERT has been improved using
regularization (Cer et al, 2008), random restarts
(Moore and Quirk, 2008), multiple replications
(Clark et al, 2011), and parameter aggregation
(Cettolo et al, 2011).
With the emergence of new optimization tech-
niques, there have been studies that compare sta-
bility between MIRA?MERT (Chiang et al, 2008;
Chiang et al, 2009; Cherry and Foster, 2012),
PRO?MERT (Hopkins and May, 2011), MIRA?
PRO?MERT (Cherry and Foster, 2012; Gimpel
and Smith, 2012; Nakov et al, 2012).
Pathological verbosity can be an issue when
tuning MERT on recall-oriented metrics such
as METEOR (Lavie and Denkowski, 2009;
Denkowski and Lavie, 2011). Large variance be-
tween the results obtained with MIRA has also
been reported (Simianer et al, 2012). However,
none of this work has focused on monsters.
6 Tale?s Moral and Future Battles
We have studied a problem with PRO, namely that
it can fall victim to monsters, overly long negative
examples with very low BLEU+1 scores, which
are unsuitable for learning. We have proposed sev-
eral effective ways to address this problem, based
on length- and BLEU+1-based cut-offs, outlier fil-
ters and stochastic sampling. The best of these
fixes have not only slayed the monsters, but have
also brought much higher stability to PRO as well
as improved test-time BLEU scores. These bene-
fits are less visible on the full dataset, but we still
recommend them to everybody who uses PRO as
protection against monsters. Monsters are inher-
ent in PRO; they just do not always take over.
In future work, we plan a deeper look at the
mechanism of monster creation in PRO and its
possible connection to PRO?s length bias.
16
References
Daniel Cer, Daniel Jurafsky, and Christopher Manning.
2008. Regularization and search for minimum error
rate training. In Proc. of Workshop on Statistical
Machine Translation, WMT ?08, pages 26?34.
Mauro Cettolo, Nicola Bertoldi, and Marcello Fed-
erico. 2011. Methods for smoothing the optimizer
instability in SMT. MT Summit XIII: the Machine
Translation Summit, pages 32?39.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL-HLT ?12, pages 427?436.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 224?233.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proc. of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, NAACL-HLT ?09, pages 218?226.
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah
Smith. 2011. Better hypothesis testing for statis-
tical machine translation: Controlling for optimizer
instability. In Proceedings of the Meeting of the As-
sociation for Computational Linguistics, ACL ?11,
pages 176?181.
Michael Denkowski and Alon Lavie. 2011. Meteor-
tuned phrase-based SMT: CMU French-English and
Haitian-English systems for WMT 2011. Techni-
cal report, CMU-LTI-11-011, Language Technolo-
gies Institute, Carnegie Mellon University.
George Foster and Roland Kuhn. 2009. Stabiliz-
ing minimum error rate training. In Proceedings
of the Workshop on Statistical Machine Translation,
StatMT ?09, pages 242?249.
Kevin Gimpel and Noah Smith. 2012. Structured ramp
loss minimization for machine translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL-HLT ?12, pages 221?231.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Workshop on Statistical
Machine Translation, WMT ?11, pages 187?197.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 1352?1362.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics on Human Language Technology, HLT-
NAACL ?03, pages 48?54.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation, IWSLT ?05.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of the Meeting of the Association for Compu-
tational Linguistics, ACL ?07, pages 177?180.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
HLT-NAACL ?04, pages 169?176.
Alon Lavie and Michael Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine Translation, 23:105?115.
Robert Moore and Chris Quirk. 2008. Random
restarts in minimum error rate training for statisti-
cal machine translation. In Proceedings of the Inter-
national Conference on Computational Linguistics,
COLING ?08, pages 585?592.
Preslav Nakov, Francisco Guzma?n, and Stephan Vo-
gel. 2012. Optimizing for sentence-level BLEU+1
yields short translations. In Proceedings of the Inter-
national Conference on Computational Linguistics,
COLING ?12, pages 1979?1994.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
Meeting of the Association for Computational Lin-
guistics, ACL ?03, pages 160?167.
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,
and Cynthia Rudin. 2008. Arabic morphological
tagging, diacritization, and lemmatization using lex-
eme models and feature ranking. In Proceedings
of the Meeting of the Association for Computational
Linguistics, ACL ?08, pages 117?120.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in smt. In
Proceedings of the Meeting of the Association for
Computational Linguistics, ACL ?12, pages 11?21.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin training
for statistical machine translation. In Proceedings of
the Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, EMNLP-CoNLL ?07, pages
764?773.
17
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687?698,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Using Discourse Structure Improves Machine Translation Evaluation
Francisco Guzm
?
an Shafiq Joty Llu??s M
`
arquez and Preslav Nakov
ALT Research Group
Qatar Computing Research Institute ? Qatar Foundation
{fguzman,sjoty,lmarquez,pnakov}@qf.org.qa
Abstract
We present experiments in using dis-
course structure for improving machine
translation evaluation. We first design
two discourse-aware similarity measures,
which use all-subtree kernels to compare
discourse parse trees in accordance with
the Rhetorical Structure Theory. Then,
we show that these measures can help
improve a number of existing machine
translation evaluation metrics both at the
segment- and at the system-level. Rather
than proposing a single new metric, we
show that discourse information is com-
plementary to the state-of-the-art evalu-
ation metrics, and thus should be taken
into account in the development of future
richer evaluation metrics.
1 Introduction
From its foundations, Statistical Machine Transla-
tion (SMT) had two defining characteristics: first,
translation was modeled as a generative process at
the sentence-level. Second, it was purely statisti-
cal over words or word sequences and made lit-
tle to no use of linguistic information. Although
modern SMT systems have switched to a discrim-
inative log-linear framework, which allows for ad-
ditional sources as features, it is generally hard to
incorporate dependencies beyond a small window
of adjacent words, thus making it difficult to use
linguistically-rich models.
Recently, there have been two promising re-
search directions for improving SMT and its eval-
uation: (a) by using more structured linguistic
information, such as syntax (Galley et al, 2004;
Quirk et al, 2005), hierarchical structures (Chi-
ang, 2005), and semantic roles (Wu and Fung,
2009; Lo et al, 2012), and (b) by going beyond
the sentence-level, e.g., translating at the docu-
ment level (Hardmeier et al, 2012).
Going beyond the sentence-level is important
since sentences rarely stand on their own in a
well-written text. Rather, each sentence follows
smoothly from the ones before it, and leads into
the ones that come afterwards. The logical rela-
tionship between sentences carries important in-
formation that allows the text to express a meaning
as a whole beyond the sum of its separate parts.
Note that sentences can be made of several
clauses, which in turn can be interrelated through
the same logical relations. Thus, in a coherent text,
discourse units (sentences or clauses) are logically
connected: the meaning of a unit relates to that of
the previous and the following units.
Discourse analysis seeks to uncover this coher-
ence structure underneath the text. Several formal
theories of discourse have been proposed to de-
scribe the coherence structure (Mann and Thomp-
son, 1988; Asher and Lascarides, 2003; Webber,
2004). For example, the Rhetorical Structure The-
ory (Mann and Thompson, 1988), or RST, repre-
sents text by labeled hierarchical structures called
Discourse Trees (DTs), which can incorporate sev-
eral layers of other linguistic information, e.g.,
syntax, predicate-argument structure, etc.
Modeling discourse brings together the above
research directions (a) and (b), which makes it an
attractive goal for MT. This is demonstrated by the
establishment of a recent workshop dedicated to
Discourse in Machine Translation (Webber et al,
2013), collocated with the 2013 annual meeting of
the Association of Computational Linguistics.
The area of discourse analysis for SMT is still
nascent and, to the best of our knowledge, no
previous research has attempted to use rhetorical
structure for SMT or machine translation evalua-
tion. One possible reason could be the unavailabil-
ity of accurate discourse parsers. However, this
situation is likely to change given the most recent
advances in automatic discourse analysis (Joty et
al., 2012; Joty et al, 2013).
687
We believe that the semantic and pragmatic in-
formation captured in the form of DTs (i) can help
develop discourse-aware SMT systems that pro-
duce coherent translations, and (ii) can yield bet-
ter MT evaluation metrics. While in this work we
focus on the latter, we think that the former is also
within reach, and that SMT systems would bene-
fit from preserving the coherence relations in the
source language when generating target-language
translations.
In this paper, rather than proposing yet another
MT evaluation metric, we show that discourse
information is complementary to many existing
evaluation metrics, and thus should not be ignored.
We first design two discourse-aware similarity
measures, which use DTs generated by a publicly-
available discourse parser (Joty et al, 2012); then,
we show that they can help improve a number of
MT evaluation metrics at the segment- and at the
system-level in the context of the WMT11 and the
WMT12 metrics shared tasks (Callison-Burch et
al., 2011; Callison-Burch et al, 2012).
These metrics tasks are based on sentence-level
evaluation, which arguably can limit the benefits
of using global discourse properties. Fortunately,
several sentences are long and complex enough to
present rich discourse structures connecting their
basic clauses. Thus, although limited, this setting
is able to demonstrate the potential of discourse-
level information for MT evaluation. Furthermore,
sentence-level scoring (i) is compatible with most
translation systems, which work on a sentence-by-
sentence basis, (ii) could be beneficial to mod-
ern MT tuning mechanisms such as PRO (Hop-
kins and May, 2011) and MIRA (Watanabe et al,
2007; Chiang et al, 2008), which also work at
the sentence-level, and (iii) could be used for re-
ranking n-best lists of translation hypotheses.
2 Related Work
Addressing discourse-level phenomena in ma-
chine translation is relatively new as a research di-
rection. Some recent work has looked at anaphora
resolution (Hardmeier and Federico, 2010) and
discourse connectives (Cartoni et al, 2011; Meyer,
2011), to mention two examples.
1
However, so
far the attempts to incorporate discourse-related
knowledge in MT have been only moderately suc-
cessful, at best.
1
We refer the reader to (Hardmeier, 2012) for an in-depth
overview of discourse-related research for MT.
A common argument, is that current automatic
evaluation metrics such as BLEU are inadequate
to capture discourse-related aspects of translation
quality (Hardmeier and Federico, 2010; Meyer et
al., 2012). Thus, there is consensus that discourse-
informed MT evaluation metrics are needed in or-
der to advance research in this direction. Here we
suggest some simple ways to create such metrics,
and we also show that they yield better correlation
with human judgments.
The field of automatic evaluation metrics for
MT is very active, and new metrics are contin-
uously being proposed, especially in the context
of the evaluation campaigns that run as part of
the Workshops on Statistical Machine Transla-
tion (WMT 2008-2012), and NIST Metrics for
Machine Translation Challenge (MetricsMATR),
among others. For example, at WMT12, 12 met-
rics were compared (Callison-Burch et al, 2012),
most of them new.
There have been several attempts to incorpo-
rate syntactic and semantic linguistic knowledge
into MT evaluation. For instance, at the syn-
tactic level, we find metrics that measure the
structural similarity between shallow syntactic se-
quences (Gim?enez and M`arquez, 2007; Popovic
and Ney, 2007) or between constituency trees (Liu
and Gildea, 2005). In the semantic case, there are
metrics that exploit the similarity over named en-
tities and predicate-argument structures (Gim?enez
and M`arquez, 2007; Lo et al, 2012).
In this work, instead of proposing a new metric,
we focus on enriching current MT evaluation met-
rics with discourse information. Our experiments
show that many existing metrics can benefit from
additional knowledge about discourse structure.
In comparison to the syntactic and semantic ex-
tensions of MT metrics, there have been very few
attempts to incorporate discourse information so
far. One example are the semantics-aware metrics
of Gim?enez and M`arquez (2009) and Comelles et
al. (2010), which use the Discourse Representa-
tion Theory (Kamp and Reyle, 1993) and tree-
based discourse representation structures (DRS)
produced by a semantic parser. They calculate the
similarity between the MT output and references
based on DRS subtree matching, as defined in (Liu
and Gildea, 2005), DRS lexical overlap, and DRS
morpho-syntactic overlap. However, they could
not improve correlation with human judgments, as
evaluated on the MetricsMATR dataset.
688
Compared to the previous work, (i) we use a
different discourse representation (RST), (ii) we
compare discourse parses using all-subtree ker-
nels (Collins and Duffy, 2001), (iii) we evaluate
on much larger datasets, for several language pairs
and for multiple metrics, and (iv) we do demon-
strate better correlation with human judgments.
Wong and Kit (2012) recently proposed an
extension of MT metrics with a measure of
document-level lexical cohesion (Halliday and
Hasan, 1976). Lexical cohesion is achieved using
word repetitions and semantically similar words
such as synonyms, hypernyms, and hyponyms.
For BLEU and TER, they observed improved
correlation with human judgments on the MTC4
dataset when linearly interpolating these metrics
with their lexical cohesion score. Unlike their
work, which measures lexical cohesion at the
document-level, here we are concerned with co-
herence (rhetorical) structure, primarily at the
sentence-level.
3 Our Discourse-Based Measures
Our working hypothesis is that the similarity be-
tween the discourse structures of an automatic and
of a reference translation provides additional in-
formation that can be valuable for evaluating MT
systems. In particular, we believe that good trans-
lations should tend to preserve discourse relations.
As an example, consider the three discourse
trees (DTs) shown in Figure 1: (a) for a reference
(human) translation, and (b) and (c) for transla-
tions of two different systems on the WMT12 test
dataset. The leaves of a DT correspond to con-
tiguous atomic text spans, called Elementary Dis-
course Units or EDUs (three in Figure 1a). Ad-
jacent spans are connected by certain coherence
relations (e.g., Elaboration, Attribution), forming
larger discourse units, which in turn are also sub-
ject to this relation linking. Discourse units linked
by a relation are further distinguished based on
their relative importance in the text: nuclei are
the core parts of the relation while satellites are
supportive ones. Note that the nuclearity and re-
lation labels in the reference translation are also
realized in the system translation in (b), but not
in (c), which makes (b) a better translation com-
pared to (c), according to our hypothesis. We ar-
gue that existing metrics that only use lexical and
syntactic information cannot distinguish well be-
tween (b) and (c).
In order to develop a discourse-aware evalua-
tion metric, we first generate discourse trees for
the reference and the system-translated sentences
using a discourse parser, and then we measure the
similarity between the two discourse trees. We de-
scribe these two steps below.
3.1 Generating Discourse Trees
In Rhetorical Structure Theory, discourse analysis
involves two subtasks: (i) discourse segmentation,
or breaking the text into a sequence of EDUs, and
(ii) discourse parsing, or the task of linking the
units (EDUs and larger discourse units) into la-
beled discourse trees. Recently, Joty et al (2012)
proposed discriminative models for both discourse
segmentation and discourse parsing at the sen-
tence level. The segmenter uses a maximum en-
tropy model that achieves state-of-the-art accuracy
on this task, having an F
1
-score of 90.5%, while
human agreement is 98.3%.
The discourse parser uses a dynamic Condi-
tional Random Field (Sutton et al, 2007) as a pars-
ing model in order to infer the probability of all
possible discourse tree constituents. The inferred
(posterior) probabilities are then used in a proba-
bilistic CKY-like bottom-up parsing algorithm to
find the most likely DT. Using the standard set
of 18 coarse-grained relations defined in (Carlson
and Marcu, 2001), the parser achieved an F
1
-score
of 79.8%, which is very close to the human agree-
ment of 83%. These high scores allowed us to de-
velop successful discourse similarity metrics.
2
3.2 Measuring Similarity
A number of metrics have been proposed to mea-
sure the similarity between two labeled trees, e.g.,
Tree Edit Distance (Tai, 1979) and Tree Kernels
(Collins and Duffy, 2001; Moschitti and Basili,
2006). Tree kernels (TKs) provide an effective
way to integrate arbitrary tree structures in kernel-
based machine learning algorithms like SVMs.
In the present work, we use the convolution TK
defined in (Collins and Duffy, 2001), which effi-
ciently calculates the number of common subtrees
in two trees. Note that this kernel was originally
designed for syntactic parsing, where the subtrees
are subject to the constraint that their nodes are
taken with either all or none of the children. This
constraint of the TK imposes some limitations on
the type of substructures that can be compared.
2
The discourse parser is freely available from
http://alt.qcri.org/tools/
689
ElaborationROOT
SPAN NucleusAttributionSatellite
Voices are coming from Germany , SPANSatellite SPANNucleus
suggesting that ECB be the last resort creditor .
(a) A reference (human) translation.
	

 	
		Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 33?38,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 8: Multi-Way Classification
of Semantic Relations Between Pairs of Nominals
Iris Hendrickx
?
, Su Nam Kim
?
, Zornitsa Kozareva
?
, Preslav Nakov
?
,
Diarmuid
?
O S
?
eaghdha
?
, Sebastian Pad
?
o
?
, Marco Pennacchiotti
??
,
Lorenza Romano
??
, Stan Szpakowicz
??
Abstract
SemEval-2 Task 8 focuses on Multi-way
classification of semantic relations between
pairs of nominals. The task was designed
to compare different approaches to seman-
tic relation classification and to provide a
standard testbed for future research. This
paper defines the task, describes the train-
ing and test data and the process of their
creation, lists the participating systems (10
teams, 28 runs), and discusses their results.
1 Introduction
SemEval-2010 Task 8 focused on semantic rela-
tions between pairs of nominals. For example, tea
and ginseng are in an ENTITY-ORIGIN relation in
?The cup contained tea from dried ginseng.?. The
automatic recognition of semantic relations has
many applications, such as information extraction,
document summarization, machine translation, or
construction of thesauri and semantic networks.
It can also facilitate auxiliary tasks such as word
sense disambiguation, language modeling, para-
phrasing, and recognizing textual entailment.
Our goal was to create a testbed for automatic
classification of semantic relations. In developing
the task we met several challenges: selecting a
suitable set of relations, specifying the annotation
procedure, and deciding on the details of the task
itself. They are discussed briefly in Section 2; see
also Hendrickx et al (2009), which includes a sur-
vey of related work. The direct predecessor of Task
8 was Classification of semantic relations between
nominals, Task 4 at SemEval-1 (Girju et al, 2009),
?
University of Lisbon, iris@clul.ul.pt
?
University of Melbourne, snkim@csse.unimelb.edu.au
?
Information Sciences Institute/University of Southern
California, kozareva@isi.edu
?
National University of Singapore, nakov@comp.nus.edu.sg
?
University of Cambridge, do242@cl.cam.ac.uk
?
University of Stuttgart, pado@ims.uni-stuttgart.de
??
Yahoo! Inc., pennacc@yahoo-inc.com
??
Fondazione Bruno Kessler, romano@fbk.eu
??
University of Ottawa and Polish Academy of Sciences,
szpak@site.uottawa.ca
which had a separate binary-labeled dataset for
each of seven relations. We have defined SemEval-
2010 Task 8 as a multi-way classification task in
which the label for each example must be chosen
from the complete set of ten relations and the map-
ping from nouns to argument slots is not provided
in advance. We also provide more data: 10,717 an-
notated examples, compared to 1,529 in SemEval-1
Task 4.
2 Dataset Creation
2.1 The Inventory of Semantic Relations
We first decided on an inventory of semantic rela-
tions. Ideally, it should be exhaustive (enable the
description of relations between any pair of nomi-
nals) and mutually exclusive (each pair of nominals
in context should map onto only one relation). The
literature, however, suggests that no relation inven-
tory satisfies both needs, and, in practice, some
trade-off between them must be accepted.
As a pragmatic compromise, we selected nine
relations with coverage sufficiently broad to be of
general and practical interest. We aimed at avoid-
ing semantic overlap as much as possible. We
included, however, two groups of strongly related
relations (ENTITY-ORIGIN / ENTITY-DESTINA-
TION and CONTENT-CONTAINER / COMPONENT-
WHOLE / MEMBER-COLLECTION) to assess mod-
els? ability to make such fine-grained distinctions.
Our inventory is given below. The first four were
also used in SemEval-1 Task 4, but the annotation
guidelines have been revised, and thus no complete
continuity should be assumed.
Cause-Effect (CE). An event or object leads to an
effect. Example: those cancers were caused
by radiation exposures
Instrument-Agency (IA). An agent uses an in-
strument. Example: phone operator
Product-Producer (PP). A producer causes a
product to exist. Example: a factory manu-
factures suits
33
Content-Container (CC). An object is physically
stored in a delineated area of space. Example:
a bottle full of honey was weighed
Entity-Origin (EO). An entity is coming or is de-
rived from an origin (e.g., position or mate-
rial). Example: letters from foreign countries
Entity-Destination (ED). An entity is moving to-
wards a destination. Example: the boy went
to bed
Component-Whole (CW). An object is a com-
ponent of a larger whole. Example: my
apartment has a large kitchen
Member-Collection (MC). A member forms a
nonfunctional part of a collection. Example:
there are many trees in the forest
Message-Topic (MT). A message, written or spo-
ken, is about a topic. Example: the lecture
was about semantics
2.2 Annotation Guidelines
We defined a set of general annotation guidelines
as well as detailed guidelines for each semantic
relation. Here, we describe the general guidelines,
which delineate the scope of the data to be col-
lected and state general principles relevant to the
annotation of all relations.
1
Our objective is to annotate instances of seman-
tic relations which are true in the sense of hold-
ing in the most plausible truth-conditional inter-
pretation of the sentence. This is in the tradition
of the Textual Entailment or Information Valida-
tion paradigm (Dagan et al, 2009), and in con-
trast to ?aboutness? annotation such as semantic
roles (Carreras and M`arquez, 2004) or the BioNLP
2009 task (Kim et al, 2009) where negated rela-
tions are also labelled as positive. Similarly, we
exclude instances of semantic relations which hold
only in speculative or counterfactural scenarios. In
practice, this means disallowing annotations within
the scope of modals or negations, e.g., ?Smoking
may/may not have caused cancer in this case.?
We accept as relation arguments only noun
phrases with common-noun heads. This distin-
guishes our task from much work in Information
Extraction, which tends to focus on specific classes
of named entities and on considerably more fine-
grained relations than we do. Named entities are a
specific category of nominal expressions best dealt
1
The full task guidelines are available at http://docs.
google.com/View?id=dfhkmm46_0f63mfvf7
with using techniques which do not apply to com-
mon nouns. We only mark up the semantic heads of
nominals, which usually span a single word, except
for lexicalized terms such as science fiction.
We also impose a syntactic locality requirement
on example candidates, thus excluding instances
where the relation arguments occur in separate sen-
tential clauses. Permissible syntactic patterns in-
clude simple and relative clauses, compounds, and
pre- and post-nominal modification. In addition,
we did not annotate examples whose interpretation
relied on discourse knowledge, which led to the
exclusion of pronouns as arguments. Please see
the guidelines for details on other issues, includ-
ing noun compounds, aspectual phenomena and
temporal relations.
2.3 The Annotation Process
The annotation took place in three rounds. First,
we manually collected around 1,200 sentences for
each relation through pattern-based Web search. In
order to ensure a wide variety of example sentences,
we used a substantial number of patterns for each
relation, typically between one hundred and several
hundred. Importantly, in the first round, the relation
itself was not annotated: the goal was merely to
collect positive and near-miss candidate instances.
A rough aim was to have 90% of candidates which
instantiate the target relation (?positive instances?).
In the second round, the collected candidates for
each relation went to two independent annotators
for labeling. Since we have a multi-way classifi-
cation task, the annotators used the full inventory
of nine relations plus OTHER. The annotation was
made easier by the fact that the cases of overlap
were largely systematic, arising from general phe-
nomena like metaphorical use and situations where
more than one relation holds. For example, there is
a systematic potential overlap between CONTENT-
CONTAINER and ENTITY-DESTINATION depend-
ing on whether the situation described in the sen-
tence is static or dynamic, e.g., ?When I came,
the <e1>apples</e1> were already put in the
<e2>basket</e2>.? is CC(e1, e2), while ?Then,
the <e1>apples</e1> were quickly put in the
<e2>basket</e2>.? is ED(e1, e2).
In the third round, the remaining disagreements
were resolved, and, if no consensus could be
achieved, the examples were removed. Finally, we
merged all nine datasets to create a set of 10,717
instances. We released 8,000 for training and kept
34
the rest for testing.
2
Table 1 shows some statistics about the dataset.
The first column (Freq) shows the absolute and rel-
ative frequencies of each relation. The second col-
umn (Pos) shows that the average share of positive
instances was closer to 75% than to 90%, indicating
that the patterns catch a substantial amount of ?near-
miss? cases. However, this effect varies a lot across
relations, causing the non-uniform relation distribu-
tion in the dataset (first column).
3
After the second
round, we also computed inter-annotator agreement
(third column, IAA). Inter-annotator agreement
was computed on the sentence level, as the per-
centage of sentences for which the two annotations
were identical. That is, these figures can be inter-
preted as exact-match accuracies. We do not report
Kappa, since chance agreement on preselected can-
didates is difficult to estimate.
4
IAA is between
60% and 95%, again with large relation-dependent
variation. Some of the relations were particularly
easy to annotate, notably CONTENT-CONTAINER,
which can be resolved through relatively clear cri-
teria, despite the systematic ambiguity mentioned
above. ENTITY-ORIGIN was the hardest relation to
annotate. We encountered ontological difficulties
in defining both Entity (e.g., in contrast to Effect)
and Origin (as opposed to Cause). Our numbers
are on average around 10% higher than those re-
ported by Girju et al (2009). This may be a side
effect of our data collection method. To gather
1,200 examples in realistic time, we had to seek
productive search query patterns, which invited
certain homogeneity. For example, many queries
for CONTENT-CONTAINER centered on ?usual sus-
pect? such as box or suitcase. Many instances of
MEMBER-COLLECTION were collected on the ba-
sis of from available lists of collective names.
3 The Task
The participating systems had to solve the follow-
ing task: given a sentence and two tagged nominals,
predict the relation between those nominals and the
direction of the relation.
We released a detailed scorer which outputs (1) a
confusion matrix, (2) accuracy and coverage, (3)
2
This set includes 891 examples from SemEval-1 Task 4.
We re-annotated them and assigned them as the last examples
of our training dataset to ensure that the test set was unseen.
3
To what extent our candidate selection produces a biased
sample is a question that we cannot address within this paper.
4
We do not report Pos or IAA for OTHER, since OTHER is
a pseudo-relation that was not annotated in its own right. The
numbers would therefore not be comparable to other relations.
Relation Freq Pos IAA
Cause-Effect 1331 (12.4%) 91.2% 79.0%
Component-Whole 1253 (11.7%) 84.3% 70.0%
Entity-Destination 1137 (10.6%) 80.1% 75.2%
Entity-Origin 974 (9.1%) 69.2% 58.2%
Product-Producer 948 (8.8%) 66.3% 84.8%
Member-Collection 923 (8.6%) 74.7% 68.2%
Message-Topic 895 (8.4%) 74.4% 72.4%
Content-Container 732 (6.8%) 59.3% 95.8%
Instrument-Agency 660 (6.2%) 60.8% 65.0%
Other 1864 (17.4%) N/A
4
N/A
4
Total 10717 (100%)
Table 1: Annotation Statistics. Freq: Absolute and
relative frequency in the dataset; Pos: percentage
of ?positive? relation instances in the candidate set;
IAA: inter-annotator agreement
precision (P), recall (R), and F
1
-Score for each
relation, (4) micro-averaged P, R, F
1
, (5) macro-
averaged P, R, F
1
. For (4) and (5), the calculations
ignored the OTHER relation. Our official scoring
metric is macro-averaged F
1
-Score for (9+1)-way
classification, taking directionality into account.
The teams were asked to submit test data pre-
dictions for varying fractions of the training data.
Specifically, we requested results for the first 1000,
2000, 4000, and 8000 training instances, called
TD1 through TD4. TD4 was the full training set.
4 Participants and Results
Table 2 lists the participants and provides a rough
overview of the system features. Table 3 shows the
results. Unless noted otherwise, all quoted numbers
are F
1
-Scores.
Overall Ranking and Training Data. We rank
the teams by the performance of their best system
on TD4, since a per-system ranking would favor
teams with many submitted runs. UTD submit-
ted the best system, with a performance of over
82%, more than 4% better than the second-best
system. FBK IRST places second, with 77.62%,
a tiny margin ahead of ISI (77.57%). Notably, the
ISI system outperforms the FBK IRST system for
TD1 to TD3, where it was second-best. The accu-
racy numbers for TD4 (Acc TD4) lead to the same
overall ranking: micro- versus macro-averaging
does not appear to make much difference either.
A random baseline gives an uninteresting score of
6%. Our competitive baseline system is a simple
Naive Bayes classifier which relies on words in the
sentential context only; two systems scored below
this baseline.
35
System Institution Team Description Res. Class.
Baseline Task organizers local context of 2 words only BN
ECNU-SR-1 East China Normal
University
Man Lan, Yuan
Chen, Zhimin
Zhou, Yu Xu
stem, POS, syntactic patterns S SVM
(multi)
ECNU-SR-2,3 features like ECNU-SR-1, dif-
ferent prob. thresholds
SVM
(binary)
ECNU-SR-4 stem, POS, syntactic patterns,
hyponymy and meronymy rela-
tions
WN,
S
SVM
(multi)
ECNU-SR-5,6 features like ECNU-SR-4, dif-
ferent prob. thresholds
SVM
(binary)
ECNU-SR-7 majority vote of ECNU-1,2,4,5
FBK IRST-6C32 Fondazione Bruno
Kessler
Claudio Giu-
liano, Kateryna
Tymoshenko
3-word window context features
(word form, part of speech, or-
thography) + Cyc; parameter
estimation by optimization on
training set
Cyc SVM
FBK IRST-12C32 FBK IRST-6C32 + distance fea-
tures
FBK IRST-12VBC32 FBK IRST-12C32 + verbs
FBK IRST-6CA,
-12CA, -12VBCA
features as above, parameter es-
timation by cross-validation
FBK NK-RES1 Fondazione Bruno
Kessler
Matteo Negri,
Milen Kouylekov
collocations, glosses, semantic
relations of nominals + context
features
WN BN
FBK NK-RES 2,3,4 like FBK NK-RES1 with differ-
ent context windows and collo-
cation cutoffs
ISI Information Sci-
ences Institute,
University of
Southern Califor-
nia
Stephen Tratz features from different re-
sources, a noun compound
relation system, and various
feature related to capitalization,
affixes, closed-class words
WN,
RT, G
ME
ISTI-1,2 Istituto di sci-
enca e tecnologie
dell?informazione
?A. Faedo?
Andrea Esuli,
Diego Marcheg-
giani, Fabrizio
Sebastiani
Boosting-based classification.
Runs differ in their initializa-
tion.
WN 2S
JU Jadavpur Univer-
sity
Santanu Pal, Partha
Pakray, Dipankar
Das, Sivaji Bandy-
opadhyay
Verbs, nouns, and prepositions;
seed lists for semantic relations;
parse features and NEs
WN,
S
CRF
SEKA Hungarian
Academy of
Sciences
Eszter Simon, An-
dras Kornai
Levin and Roget classes, n-
grams; other grammatical and
formal features
RT,
LC
ME
TUD-base Technische Univer-
sit?at Darmstadt
Gy?orgy Szarvas,
Iryna Gurevych
word, POS n-grams, depen-
dency path, distance
S ME
TUD-wp TUD-base + ESA semantic re-
latedness scores
+WP
TUD-comb TUD-base + own semantic relat-
edness scores
+WP,WN
TUD-comb-threshold TUD-comb with higher thresh-
old for OTHER
UNITN University of
Trento
Fabio Celli punctuation, context words,
prepositional patterns, estima-
tion of semantic relation
? DR
UTD University of Texas
at Dallas
Bryan Rink, Sanda
Harabagiu
context wods, hypernyms, POS,
dependencies, distance, seman-
tic roles, Levin classes, para-
phrases
WN,
S, G,
PB/NB,
LC
SVM,
2S
Table 2: Participants of SemEval-2010 Task 8. Res: Resources used (WN: WordNet data; WP:
Wikipedia data; S: syntax; LC: Levin classes; G: Google n-grams, RT: Roget?s Thesaurus, PB/NB:
PropBank/NomBank). Class: Classification style (ME: Maximum Entropy; BN: Bayes Net; DR: Decision
Rules/Trees; CRF: Conditional Random Fields; 2S: two-step classification)
36
System TD1 TD2 TD3 TD4 Acc TD4 Rank Best Cat Worst Cat-9
Baseline 33.04 42.41 50.89 57.52 50.0 - MC (75.1) IA (28.0)
ECNU-SR-1 52.13 56.58 58.16 60.08 57.1
4
CE (79.7) IA (32.2)
ECNU-SR-2 46.24 47.99 69.83 72.59 67.1 CE (84.4) IA (52.2)
ECNU-SR-3 39.89 42.29 65.47 68.50 62.0 CE (83.4) IA (46.5)
ECNU-SR-4 67.95 70.58 72.99 74.82 70.5 CE (84.6) IA (61.4)
ECNU-SR-5 49.32 50.70 72.63 75.43 70.2 CE (85.1) IA (60.7)
ECNU-SR-6 42.88 45.54 68.87 72.19 65.8 CE (85.2) IA (56.7)
ECNU-SR-7 58.67 58.87 72.79 75.21 70.2 CE (86.1) IA (61.8)
FBK IRST-6C32 60.19 67.31 71.78 76.81 72.4
2
ED (82.6) IA (69.4)
FBK IRST-12C32 60.66 67.91 72.04 76.91 72.4 MC (84.2) IA (68.8)
FBK IRST-12VBC32 62.64 69.86 73.19 77.11 72.3 ED (85.9) PP (68.1)
FBK IRST-6CA 60.58 67.14 71.63 76.28 71.4 CE (82.3) IA (67.7)
FBK IRST-12CA 61.33 67.80 71.65 76.39 71.4 ED (81.8) IA (67.5)
FBK IRST-12VBCA 63.61 70.20 73.40 77.62 72.8 ED (86.5) IA (67.3)
FBK NK-RES1 55.71
?
64.06
?
67.80
?
68.02 62.1
7
ED (77.6) IA (52.9)
FBK NK-RES2 54.27
?
63.68
?
67.08
?
67.48 61.4 ED (77.4) PP (55.2)
FBK NK-RES3 54.25
?
62.73
?
66.11
?
66.90 60.5 MC (76.7) IA (56.3)
FBK NK-RES4 44.11
?
58.85
?
63.06
?
65.84 59.4 MC (76.1) IA/PP (58.0)
ISI 66.68 71.01 75.51 77.57 72.7 3 CE (87.6) IA (61.5)
ISTI-1 50.49
?
55.80
?
61.14
?
68.42 63.2
6
ED (80.7) PP (53.8)
ISTI-2 50.69
?
54.29
?
59.77
?
66.65 61.5 ED (80.2) IA (48.9)
JU 41.62
?
44.98
?
47.81
?
52.16 50.2 9 CE (75.6) IA (27.8)
SEKA 51.81 56.34 61.10 66.33 61.9 8 CE (84.0) PP (43.7)
TUD-base 50.81 54.61 56.98 60.50 56.1
5
CE (80.7) IA (31.1)
TUD-wp 55.34 60.90 63.78 68.00 63.5 ED (82.9) IA (44.1)
TUD-comb 57.84 62.52 66.41 68.88 64.6 CE (83.8) IA (46.8)
TUD-comb-? 58.35 62.45 66.86 69.23 65.4 CE (83.4) IA (46.9)
UNITN 16.57
?
18.56
?
22.45
?
26.67 27.4 10 ED (46.4) PP (0)
UTD 73.08 77.02 79.93 82.19 77.9 1 CE (89.6) IA (68.5)
Table 3: F
1
-Score of all submitted systems on the test dataset as a function of training data: TD1=1000,
TD2=2000, TD3=4000, TD4=8000 training examples. Official results are calculated on TD4. The results
marked with
?
were submitted after the deadline. The best-performing run for each participant is italicized.
As for the amount of training data, we see a sub-
stantial improvement for all systems between TD1
and TD4, with diminishing returns for the transi-
tion between TD3 and TD4 for many, but not all,
systems. Overall, the differences between systems
are smaller for TD4 than they are for TD1. The
spread between the top three systems is around 10%
at TD1, but below 5% at TD4. Still, there are clear
differences in the influence of training data size
even among systems with the same overall archi-
tecture. Notably, ECNU-SR-4 is the second-best
system at TD1 (67.95%), but gains only 7% from
the eightfold increase of the size of the training data.
At the same time, ECNU-SR-3 improves from less
than 40% to almost 69%. The difference between
the systems is that ECNU-SR-4 uses a multi-way
classifier including the class OTHER, while ECNU-
SR-3 uses binary classifiers and assigns OTHER
if no other relation was assigned with p>0.5. It
appears that these probability estimates for classes
are only reliable enough for TD3 and TD4.
The Influence of System Architecture. Almost
all systems used either MaxEnt or SVM classifiers,
with no clear advantage for either. Similarly, two
systems, UTD and ISTI (rank 1 and 6) split the task
into two classification steps (relation and direction),
but the 2nd- and 3rd-ranked systems do not. The
use of a sequence model such as a CRF did not
show a benefit either.
The systems use a variety of resources. Gener-
ally, richer feature sets lead to better performance
(although the differences are often small ? compare
the different FBK IRST systems). This improve-
ment can be explained by the need for semantic
generalization from training to test data. This need
can be addressed using WordNet (contrast ECNU-1
to -3 with ECNU-4 to -6), the Google n-gram col-
lection (see ISI and UTD), or a ?deep? semantic
resource (FBK IRST uses Cyc). Yet, most of these
resources are also included in the less successful
systems, so beneficial integration of knowledge
sources into semantic relation classification seems
to be difficult.
System Combination. The differences between
the systems suggest that it might be possible to
achieve improvements by building an ensemble
37
system. When we combine the top three systems
(UTD, FBK IRST-12VBCA, and ISI) by predict-
ing their majority vote, or OTHER if there was none,
we obtain a small improvement over the UTD sys-
tem with an F
1
-Score of 82.79%. A combination of
the top five systems using the same method shows
a worse performance, however (80.42%). This sug-
gests that the best system outperforms the rest by
a margin that cannot be compensated with system
combination, at least not with a crude majority vote.
We see a similar pattern among the ECNU systems,
where the ECNU-SR-7 combination system is out-
performed by ECNU-SR-5, presumably since it
incorporates the inferior ECNU-SR-1 system.
Relation-specific Analysis. We also analyze the
performance on individual relations, especially the
extremes. There are very stable patterns across all
systems. The best relation (presumably the eas-
iest to classify) is CE, far ahead of ED and MC.
Notably, the performance for the best relation is
75% or above for almost all systems, with compar-
atively small differences between the systems. The
hardest relation is generally IA, followed by PP.
5
Here, the spread among the systems is much larger:
the highest-ranking systems outperform others on
the difficult relations. Recall was the main prob-
lem for both IA and PP: many examples of these
two relations are misclassified, most frequently as
OTHER. Even at TD4, these datasets seem to be
less homogeneous than the others. Intriguingly, PP
shows a very high inter-annotator agreement (Ta-
ble 1). Its difficulty may therefore be due not to
questionable annotation, but to genuine variability,
or at least the selection of difficult patterns by the
dataset creator. Conversely, MC, among the easiest
relations to model, shows only a modest IAA.
Difficult Instances. There were 152 examples
that are classified incorrectly by all systems. We
analyze them, looking for sources of errors. In ad-
dition to a handful of annotation errors and some
borderline cases, they are made up of instances
which illustrate the limits of current shallow mod-
eling approaches in that they require more lexical
knowledge and complex reasoning. A case in point:
The bottle carrier converts your <e1>bottle</e1>
into a <e2>canteen</e2>. This instance of
OTHER is misclassified either as CC (due to the
5
The relation OTHER, which we ignore in the overall F
1
-
score, does even worse, often below 40%. This is to be ex-
pected, since the OTHER examples in our datasets are near
misses for other relations, thus making a very incoherent class.
nominals) or as ED (because of the preposition
into). Another example: [...] <e1>Rudders</e1>
are used by <e2>towboats</e2> and other ves-
sels that require a high degree of manoeuvrability.
This is an instance of CW misclassified as IA, prob-
ably on account of the verb use which is a frequent
indicator of an agentive relation.
5 Discussion and Conclusion
There is little doubt that 19-way classification is a
non-trivial challenge. It is even harder when the
domain is lexical semantics, with its idiosyncrasies,
and when the classes are not necessarily disjoint,
despite our best intentions. It speaks to the success
of the exercise that the participating systems? per-
formance was generally high, well over an order
of magnitude above random guessing. This may
be due to the impressive array of tools and lexical-
semantic resources deployed by the participants.
Section 4 suggests a few ways of interpreting
and analyzing the results. Long-term lessons will
undoubtedly emerge from the workshop discussion.
One optimistic-pessimistic conclusion concerns the
size of the training data. The notable gain TD3?
TD4 suggests that even more data would be helpful,
but that is so much easier said than done: it took
the organizers well in excess of 1000 person-hours
to pin down the problem, hone the guidelines and
relation definitions, construct sufficient amounts of
trustworthy training data, and run the task.
References
X. Carreras and L. M`arquez. 2004. Introduction to
the CoNLL-2004 shared task: Semantic role label-
ing. In Proc. CoNLL-04, Boston, MA.
I. Dagan, B. Dolan, B. Magnini, and D. Roth. 2009.
Recognizing textual entailment: Rational, evalua-
tion and approaches. Natural Language Engineer-
ing, 15(4):i?xvii.
R. Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Tur-
ney, and D. Yuret. 2009. Classification of semantic
relations between nominals. Language Resources
and Evaluation, 43(2):105?121.
I. Hendrickx, S. Kim, Z. Kozareva, P. Nakov, D.
?
O
S?eaghdha, S. Pad?o, M. Pennacchiotti, L. Romano,
and S. Szpakowicz. 2009. SemEval-2010 Task
8: Multi-way classification of semantic relations be-
tween pairs of nominals. In Proc. NAACL Workshop
on Semantic Evaluations, Boulder, CO.
J. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsujii.
2009. Overview of BioNLP?09 shared task on event
extraction. In Proc. BioNLP-09, Boulder, CO.
38
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 39?44,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 9: The Interpretation of Noun Compounds
Using Paraphrasing Verbs and Prepositions
Cristina Butnariu
University College Dublin
ioana.butnariu@ucd.ie
Su Nam Kim
University of Melbourne
nkim@csse.unimelb.edu.au
Preslav Nakov
National University of Singapore
nakov@comp.nus.edu.sg
Diarmuid
?
O S
?
eaghdha
University of Cambridge
do242@cam.ac.uk
Stan Szpakowicz
University of Ottawa
Polish Academy of Sciences
szpak@site.uottawa.ca
Tony Veale
University College Dublin
tony.veale@ucd.ie
Abstract
Previous research has shown that the mean-
ing of many noun-noun compounds N
1
N
2
can be approximated reasonably well by
paraphrasing clauses of the form ?N
2
that
. . . N
1
?, where ?. . . ? stands for a verb
with or without a preposition. For exam-
ple, malaria mosquito is a ?mosquito that
carries malaria?. Evaluating the quality of
such paraphrases is the theme of Task 9 at
SemEval-2010. This paper describes some
background, the task definition, the process
of data collection and the task results. We
also venture a few general conclusions be-
fore the participating teams present their
systems at the SemEval-2010 workshop.
There were 5 teams who submitted 7 sys-
tems.
1 Introduction
Noun compounds (NCs) are sequences of two or
more nouns that act as a single noun,1 e.g., stem
cell, stem cell research, stem cell research organi-
zation, etc. Lapata and Lascarides (2003) observe
that NCs pose syntactic and semantic challenges for
three basic reasons: (1) the compounding process
is extremely productive in English; (2) the seman-
tic relation between the head and the modifier is
implicit; (3) the interpretation can be influenced by
contextual and pragmatic factors. Corpus studies
have shown that while NCs are very common in
English, their frequency distribution follows a Zip-
fian or power-law distribution and the majority of
NCs encountered will be rare types (Tanaka and
Baldwin, 2003; Lapata and Lascarides, 2003; Bald-
win and Tanaka, 2004; ?O S?eaghdha, 2008). As a
consequence, Natural Language Processing (NLP)
1
We follow the definition in (Downing, 1977).
applications cannot afford either to ignore NCs or
to assume that they can be handled by relying on a
dictionary or other static resource.
Trouble with lexical resources for NCs notwith-
standing, NC semantics plays a central role in com-
plex knowledge discovery and applications, includ-
ing but not limited to Question Answering (QA),
Machine Translation (MT), and Information Re-
trieval (IR). For example, knowing the (implicit)
semantic relation between the NC components can
help rank and refine queries in QA and IR, or select
promising translation pairs in MT (Nakov, 2008a).
Thus, robust semantic interpretation of NCs should
be of much help in broad-coverage semantic pro-
cessing.
Proposed approaches to modelling NC seman-
tics have used semantic similarity (Nastase and Sz-
pakowicz, 2003; Moldovan et al, 2004; Kim and
Baldwin, 2005; Nastase and Szpakowicz, 2006;
Girju, 2007; ?O S?eaghdha and Copestake, 2007)
and paraphrasing (Vanderwende, 1994; Kim and
Baldwin, 2006; Butnariu and Veale, 2008; Nakov
and Hearst, 2008). The former body of work seeks
to measure the similarity between known and un-
seen NCs by considering various features, usually
context-related. In contrast, the latter group uses
verb semantics to interpret NCs directly, e.g., olive
oil as ?oil that is extracted from olive(s)?, drug
death as ?death that is caused by drug(s)?, flu shot
as a ?shot that prevents flu?.
The growing popularity ? and expected direct
utility ? of paraphrase-based NC semantics has
encouraged us to propose an evaluation exercise
for the 2010 edition of SemEval. This paper gives
a bird?s-eye view of the task. Section 2 presents
its objective, data, data collection, and evaluation
method. Section 3 lists the participating teams.
Section 4 shows the results and our analysis. In
Section 5, we sum up our experience so far.
39
2 Task Description
2.1 The Objective
For the purpose of the task, we focused on two-
word NCs which are modifier-head pairs of nouns,
such as apple pie or malaria mosquito. There are
several ways to ?attack? the paraphrase-based se-
mantics of such NCs.
We have proposed a rather simple problem: as-
sume that many paraphrases can be found ? perhaps
via clever Web search ? but their relevance is up in
the air. Given sufficient training data, we seek to es-
timate the quality of candidate paraphrases in a test
set. Each NC in the training set comes with a long
list of verbs in the infinitive (often with a prepo-
sition) which may paraphrase the NC adequately.
Examples of apt paraphrasing verbs: olive oil ?
be extracted from, drug death ? be caused by, flu
shot ? prevent. These lists have been constructed
from human-proposed paraphrases. For the train-
ing data, we also provide the participants with a
quality score for each paraphrase, which is a simple
count of the number of human subjects who pro-
posed that paraphrase. At test time, given a noun
compound and a list of paraphrasing verbs, a partic-
ipating system needs to produce aptness scores that
correlate well (in terms of relative ranking) with
the held out human judgments. There may be a
diverse range of paraphrases for a given compound,
some of them in fact might be inappropriate, but
it can be expected that the distribution over para-
phrases estimated from a large number of subjects
will indeed be representative of the compound?s
meaning.
2.2 The Datasets
Following Nakov (2008b), we took advantage of
the Amazon Mechanical Turk2 (MTurk) to acquire
paraphrasing verbs from human annotators. The
service offers inexpensive access to subjects for
tasks which require human intelligence. Its API
allows a computer program to run tasks easily and
collate the subjects? responses. MTurk is becoming
a popular means of eliciting and collecting linguis-
tic intuitions for NLP research; see Snow et al
(2008) for an overview and a further discussion.
Even though we recruited human subjects,
whom we required to take a qualification test,3
2
www.mturk.com
3We soon realized that we also had to offer a version of
our assignments without a qualification test (at a lower pay
rate) since very few people were willing to take a test. Overall,
data collection was time-consuming since many
annotators did not follow the instructions. We had
to monitor their progress and to send them timely
messages, pointing out mistakes. Although the
MTurk service allows task owners to accept or re-
ject individual submissions, rejection was the last
resort since it has the triply unpleasant effect of
(1) denying the worker her fee, (2) negatively af-
fecting her rating, and (3) lowering our rating as
a requester. We thus chose to try and educate our
workers ?on the fly?. Even so, we ended up with
many examples which we had to correct manu-
ally by labor-intensive post-processing. The flaws
were not different from those already described by
Nakov (2008b). Post-editing was also necessary to
lemmatize the paraphrasing verbs systematically.
Trial Data. At the end of August 2009, we
released as trial data the previously collected para-
phrase sets (Nakov, 2008b) for the Levi-250 dataset
(after further review and cleaning). This dataset
consisted of 250 noun-noun compounds form (Levi,
1978), each paraphrased by 25-30 MTurk workers
(without a qualification test).
Training Data. The training dataset was an ex-
tension of the trial dataset. It consisted of the same
250 noun-noun compounds, but the number of an-
notators per compound increased significantly. We
aimed to recruit at least 30 additional MTurk work-
ers per compound; for some compounds we man-
aged to get many more. For example, when we
added the paraphrasing verbs from the trial dataset
to the newly collected verbs, we had 131 different
workers for neighborhood bars, compared to just
50 for tear gas. On the average, we had 72.7 work-
ers per compound. Each worker was instructed
to try to produce at least three paraphrasing verbs,
so we ended up with 191.8 paraphrasing verbs per
compound, 84.6 of them being unique. See Table 1
for more details.
Test Data. The test dataset consisted of 388
noun compounds collected from two data sources:
(1) the Nastase and Szpakowicz (2003) dataset;
and (2) the Lauer (1995) dataset. The former
contains 328 noun-noun compounds (there are
also a number of adjective-noun and adverb-noun
pairs), while the latter contains 266 noun-noun
compounds. Since these datasets overlap between
themselves and with the training dataset, we had
to exclude some examples. In the end, we had 388
we found little difference in the quality of work of subjects
recruited with and without the test.
40
Training: 250 NCs Testing: 388 NCs All: 638 NCs
Total Min/Max/Avg Total Min/Max/Avg Total Min/Max/Avg
MTurk workers 28,199 50/131/72.7 17,067 57/96/68.3 45,266 50/131/71.0
Verb types 32,832 25/173/84.6 17,730 41/133/70.9 50,562 25/173/79.3
Verb tokens 74,407 92/462/191.8 46,247 129/291/185.0 120,654 92/462/189.1
Table 1: Statistics about the the training/test datasets. Shown are the total number of verbs proposed as
well as the minimum, maximum and average number of paraphrasing verb types/tokens per compound.
unique noun-noun compounds for testing, distinct
from those used for training. We aimed for 100
human workers per testing NC, but we could only
get 68.3, with a minimum of 57 and a maximum of
96; there were 185.0 paraphrasing verbs per com-
pound, 70.9 of them being unique, which is close
to what we had for the training data.
Data format. We distribute the training data as
a raw text file. Each line has the following tab-
separated format:
NC paraphrase frequency
where NC is a noun-noun compound (e.g., ap-
ple cake, flu virus), paraphrase is a human-
proposed paraphrasing verb optionally followed
by a preposition, and frequency is the number
of annotators who proposed that paraphrase. Here
is an illustrative extract from the training dataset:
flu virus cause 38
flu virus spread 13
flu virus create 6
flu virus give 5
flu virus produce 5
...
flu virus be made up of 1
flu virus be observed in 1
flu virus exacerbate 1
The test file has a similar format, except that the
frequency is not included and the paraphrases for
each noun compound appear in random order:
...
chest pain originate
chest pain start in
chest pain descend in
chest pain be in
...
License. All datasets are released under the Cre-
ative Commons Attribution 3.0 Unported license.4
4
creativecommons.org/licenses/by/3.0
2.3 Evaluation
All evaluation was performed by computing an ap-
propriate measure of similarity/correlation between
system predictions and the compiled judgements of
the human annotators. We did it on a compound-by-
compound basis and averaged over all compounds
in the test dataset. Section 4 shows results for three
measures: Spearman rank correlation, Pearson cor-
relation, and cosine similarity.
Spearman Rank Correlation (?) was adopted
as the official evaluation measure for the competi-
tion. As a rank correlation statistic, it does not use
the numerical values of the predictions or human
judgements, only their relative ordering encoded
as integer ranks. For a sample of n items ranked
by two methods x and y, the rank correlation ? is
calculated as follows:
? =
n
?
x
i
y
i
? (
?
x
i
)(
?
y
i
)
?
n
?
x
2
i
? (
?
x
i
)
2
?
n
?
y
2
i
? (
?
y
i
)
2
(1)
where x
i
, y
i
are the ranks given by x and y to the
ith item, respectively. The value of ? ranges be-
tween -1.0 (total negative correlation) and 1.0 (total
positive correlation).
Pearson Correlation (r) is a standard measure
of correlation strength between real-valued vari-
ables. The formula is the same as (1), but with
x
i
, y
i
taking real values rather than rank values;
just like ?, r?s values fall between -1.0 and 1.0.
Cosine similarity is frequently used in NLP to
compare numerical vectors:
cos =
?
n
i
x
i
y
i
?
?
n
i
x
2
i
?
n
i
y
2
i
(2)
For non-negative data, the cosine similarity takes
values between 0.0 and 1.0. Pearson?s r can be
viewed as a version of the cosine similarity which
performs centering on x and y.
Baseline: To help interpret these evaluation mea-
sures, we implemented a simple baseline. A dis-
tribution over the paraphrases was estimated by
41
System Institution Team Description
NC-INTERP International Institute of
Information Technology,
Hyderabad
Prashant
Mathur
Unsupervised model using verb-argument frequen-
cies from parsed Web snippets and WordNet
smoothing
UCAM University of Cambridge Clemens Hepp-
ner
Unsupervised model using verb-argument frequen-
cies from the British National Corpus
UCD-GOGGLE-I University College
Dublin
Guofu Li Unsupervised probabilistic model using pattern fre-
quencies estimated from the Google N-Gram corpus
UCD-GOGGLE-II Paraphrase ranking model learned from training
data
UCD-GOGGLE-III Combination of UCD-GOGGLE-I and UCD-
GOGGLE-II
UCD-PN University College
Dublin
Paul Nulty Scoring according to the probability of a paraphrase
appearing in the same set as other paraphrases pro-
vided
UVT-MEPHISTO Tilburg University Sander
Wubben
Supervised memory-based ranker using features
from Google N-Gram Corpus and WordNet
Table 2: Teams participating in SemEval-2010 Task 9
summing the frequencies for all compounds in the
training dataset, and the paraphrases for the test ex-
amples were scored according to this distribution.
Note that this baseline entirely ignores the identity
of the nouns in the compound.
3 Participants
The task attracted five teams, one of which (UCD-
GOGGLE) submitted three runs. The participants
are listed in Table 2 along with brief system de-
scriptions; for more details please see the teams?
own description papers.
4 Results and Discussion
The task results appear in Table 3. In an evaluation
by Spearman?s ? (the official ranking measure),
the winning system was UVT-MEPHISTO, which
scored 0.450. UVT also achieved the top Pear-
son?s r score. UCD-PN is the top-scoring system
according to the cosine measure. One participant
submitted part of his results after the official dead-
line, which is marked by an asterisk.
The participants used a variety of information
sources and estimation methods. UVT-MEPHISTO
is a supervised system that uses frequency informa-
tion from the Google N-Gram Corpus and features
from WordNet (Fellbaum, 1998) to rank candidate
paraphrases. On the other hand, UCD-PN uses
no external resources and no supervised training,
yet came within 0.009 of UVT-MEPHISTO in the
official evaluation. The basic idea of UCD-PN ?
that one can predict the plausibility of a paraphrase
simply by knowing which other paraphrases have
been given for that compound regardless of their
frequency ? is clearly a powerful one. Unlike the
other systems, UCD-PN used information about the
test examples (not their ranks, of course) for model
estimation; this has similarities to ?transductive?
methods for semi-supervised learning. However,
post-hoc analysis shows that UCD-PN would have
preserved its rank if it had estimated its model on
the training data only. On the other hand, if the task
had been designed differently ? by asking systems
to propose paraphrases from the set of all possi-
ble verb/preposition combinations ? then we would
not expect UCD-PN?s approach to work as well as
models that use corpus information.
The other systems are comparable to UVT-
MEPHISTO in that they use corpus frequencies
to evaluate paraphrases and apply some kind of
semantic smoothing to handle sparsity. How-
ever, UCD-GOGGLE-I, UCAM and NC-INTERP
are unsupervised systems. UCAM uses the 100-
million word BNC corpus, while the other systems
use Web-scale resources; this has presumably ex-
acerbated sparsity issues and contributed to a rela-
tively poor performance.
The hybrid approach exemplified by UCD-
GOGGLE-III combines the predictions of a sys-
tem that models paraphrase correlations and one
that learns from corpus frequencies and thus at-
tains better performance. Given that the two top-
scoring systems can also be characterized as using
these two distinct information sources, it is natu-
ral to consider combining these systems. Simply
normalizing (to unit sum) and averaging the two
sets of prediction values for each compound does
42
Rank System Supervised? Hybrid? Spearman ? Pearson r Cosine
1 UVT-MEPHISTO yes no 0.450 0.411 0.635
2 UCD-PN no no 0.441 0.361 0.669
3 UCD-GOGGLE-III yes yes 0.432 0.395 0.652
4 UCD-GOGGLE-II yes no 0.418 0.375 0.660
5 UCD-GOGGLE-I no no 0.380 0.252 0.629
6 UCAM no no 0.267 0.219 0.374
7 NC-INTERP* no no 0.186 0.070 0.466
Baseline yes no 0.425 0.344 0.524
Combining UVT and UCD-PN yes yes 0.472 0.431 0.685
Table 3: Evaluation results for SemEval-2010 Task 9 (* denotes a late submission).
indeed give better scores: Spearman ? = 0.472,
r = 0.431, Cosine = 0.685.
The baseline from Section 2.3 turns out to be
very strong. Evaluating with Spearman?s ?, only
three systems outperform it. It is less competitive
on the other evaluation measures though. This
suggests that global paraphrase frequencies may
be useful for telling sensible paraphrases from bad
ones, but will not do for quantifying the plausibility
of a paraphrase for a given noun compound.
5 Conclusion
Given that it is a newly-proposed task, this initial
experiment in paraphrasing noun compounds has
been a moderate success. The participation rate
has been sufficient for the purposes of comparing
and contrasting different approaches to the role
of paraphrases in the interpretation of noun-noun
compounds. We have seen a variety of approaches
applied to the same dataset, and we have been able
to compare the performance of pure approaches to
hybrid approaches, and of supervised approaches
to unsupervised approaches. The results reported
here are also encouraging, though clearly there is
considerable room for improvement.
This task has established a high baseline for sys-
tems to beat. We can take heart from the fact that
the best performance is apparently obtained from a
combination of corpus-derived usage features and
dictionary-derived linguistic knowledge. Although
clever but simple approaches can do quite well on
such a task, it is encouraging to note that the best
results await those who employ the most robust
and the most informed treatments of NCs and their
paraphrases. Despite a good start, this is a chal-
lenge that remains resolutely open. We expect that
the dataset created for the task will be a valuable
resource for future research.
Acknowledgements
This work is partially supported by grants from
Amazon and from the Bulgarian National Science
Foundation (D002-111/15.12.2008 ? SmartBook).
References
Timothy Baldwin and Takaaki Tanaka. 2004. Trans-
lation by Machine of Compound Nominals: Getting
it Right. In Proceedings of the ACL-04 Workshop
on Multiword Expressions: Integrating Processing,
pages 24?31, Barcelona, Spain.
Cristina Butnariu and Tony Veale. 2008. A Concept-
Centered Approach to Noun-Compound Interpreta-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (COLING-
08), pages 81?88, Manchester, UK.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53(4):810?842.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press.
Roxana Girju. 2007. Improving the Interpretation
of Noun Phrases with Cross-linguistic Information.
In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics (ACL-07),
pages 568?575, Prague, Czech Republic.
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using WordNet
similarity. In Proceedings of the 2nd International
Joint Conference on Natural Language Processing
(IJCNLP-05), pages 945?956, Jeju Island, South Ko-
rea.
Su Nam Kim and Timothy Baldwin. 2006. Inter-
preting Semantic Relations in Noun Compounds via
Verb Semantics. In Proceedings of the COLING-
ACL-06 Main Conference Poster Sessions, pages
491?498, Sydney, Australia.
Mirella Lapata and Alex Lascarides. 2003. Detect-
ing novel compounds: The role of distributional evi-
dence. In Proceedings of the 10th Conference of the
43
European Chapter of the Association for Computa-
tional Linguistics (EACL-03), pages 235?242, Bu-
dapest, Hungary.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds. Ph.D.
thesis, Macquarie University.
Judith Levi. 1978. The Syntax and Semantics of Com-
plex Nominals. Academic Press, New York, NY.
DanMoldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the Se-
mantic Classification of Noun Phrases. In Proceed-
ings of the HLT-NAACL-04 Workshop on Computa-
tional Lexical Semantics, pages 60?67, Boston, MA.
Preslav Nakov and Marti A. Hearst. 2008. Solving
Relational Similarity Problems Using the Web as a
Corpus. In Proceedings of the 46th Annual Meet-
ing of the Association of Computational Linguistics
(ACL-08), pages 452?460, Columbus, OH.
Preslav Nakov. 2008a. Improved Statistical Machine
Translation Using Monolingual Paraphrases. In Pro-
ceedings of the 18th European Conference on Artifi-
cial Intelligence (ECAI-08), pages 338?342, Patras,
Greece.
Preslav Nakov. 2008b. Noun Compound Interpreta-
tion Using Paraphrasing Verbs: Feasibility Study.
In Proceedings of the 13th International Confer-
ence on Artificial Intelligence: Methodology, Sys-
tems and Applications (AIMSA-08), pages 103?117,
Varna, Bulgaria.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings
of the 5th International Workshop on Computational
Semantics (IWCS-03), pages 285?301, Tilburg, The
Netherlands.
Vivi Nastase and Stan Szpakowicz. 2006. Matching
syntactic-semantic graphs for semantic relation as-
signment. In Proceedings of the 1st Workshop on
Graph Based Methods for Natural Language Pro-
cessing (TextGraphs-06), pages 81?88, New York,
NY.
Diarmuid
?
O S?eaghdha and Ann Copestake. 2007. Co-
occurrence Contexts for Noun Compound Interpre-
tation. In Proceedings of the ACL-07 Workshop
on A Broader Perspective on Multiword Expressions
(MWE-07), pages 57?64, Prague, Czech Republic.
Diarmuid
?
O S?eaghdha. 2008. Learning Compound
Noun Semantics. Ph.D. thesis, University of Cam-
bridge.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and Fast ? But is it Good?
Evaluating Non-Expert Annotations for Natural Lan-
guage Tasks. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-08), pages 254?263, Honolulu, HI.
Takaaki Tanaka and Timothy Baldwin. 2003. Noun-
noun compound machine translation: A feasibility
study on shallow processing. In Proceedings of the
ACL-03 Workshop on Multiword Expressions (MWE-
03), pages 17?24, Sapporo, Japan.
Lucy Vanderwende. 1994. Algorithm for Automatic
Interpretation of Noun Sequences. In Proceedings
of the 15th International Conference on Compu-
tational Linguistics (COLING-94), pages 782?788,
Kyoto, Japan.
44
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 138?143, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 4: Free Paraphrases of Noun Compounds
Iris Hendrickx
Radboud University Nijmegen &
Universidade de Lisboa
iris@clul.ul.pt
Preslav Nakov
QCRI, Qatar Foundation
pnakov@qf.org.qa
Stan Szpakowicz
University of Ottawa &
Polish Academy of Sciences
szpak@eecs.uottawa.ca
Zornitsa Kozareva
University of Southern California
kozareva@isi.edu
Diarmuid O? Se?aghdha
University of Cambridge
do242@cam.ac.uk
Tony Veale
University College Dublin
tony.veale@ucd.ie
Abstract
In this paper, we describe SemEval-2013 Task
4: the definition, the data, the evaluation and
the results. The task is to capture some of the
meaning of English noun compounds via para-
phrasing. Given a two-word noun compound,
the participating system is asked to produce
an explicitly ranked list of its free-form para-
phrases. The list is automatically compared
and evaluated against a similarly ranked list
of paraphrases proposed by human annota-
tors, recruited and managed through Ama-
zon?s Mechanical Turk. The comparison of
raw paraphrases is sensitive to syntactic and
morphological variation. The ?gold? ranking
is based on the relative popularity of para-
phrases among annotators. To make the rank-
ing more reliable, highly similar paraphrases
are grouped, so as to downplay superficial dif-
ferences in syntax and morphology. Three
systems participated in the task. They all beat
a simple baseline on one of the two evalua-
tion measures, but not on both measures. This
shows that the task is difficult.
1 Introduction
A noun compound (NC) is a sequence of nouns
which act as a single noun (Downing, 1977), as in
these examples: colon cancer, suppressor protein,
tumor suppressor protein, colon cancer tumor sup-
pressor protein, etc. This type of compounding is
highly productive in English. NCs comprise 3.9%
and 2.6% of all tokens in the Reuters corpus and the
British National Corpus (BNC), respectively (Bald-
win and Tanaka, 2004).
The frequency spectrum of compound types fol-
lows a Zipfian distribution (O? Se?aghdha, 2008), so
many NC tokens belong to a ?long tail? of low-
frequency types. More than half of the two-noun
types in the BNC occur exactly once (Kim and Bald-
win, 2006). Their high frequency and high produc-
tivity make robust NC interpretation an important
goal for broad-coverage semantic processing of En-
glish texts. Systems which ignore NCs may give up
on salient information about the semantic relation-
ships implicit in a text. Compositional interpretation
is also the only way to achieve broad NC coverage,
because it is not feasible to list in a lexicon all com-
pounds which one is likely to encounter. Even for
relatively frequent NCs occurring 10 times or more
in the BNC, static English dictionaries provide only
27% coverage (Tanaka and Baldwin, 2003).
In many natural language processing applications
it is important to understand the syntax and seman-
tics of NCs. NCs often are structurally similar,
but have very different meaning. Consider caffeine
headache and ice-cream headache: a lack of caf-
feine causes the former, an excess of ice-cream ? the
latter. Different interpretations can lead to different
inferences, query expansion, paraphrases, transla-
tions, and so on. A question answering system may
have to determine whether protein acting as a tumor
suppressor is an accurate paraphrase for tumor sup-
pressor protein. An information extraction system
might need to decide whether neck vein thrombosis
and neck thrombosis can co-refer in the same doc-
ument. A machine translation system might para-
phrase the unknown compound WTO Geneva head-
quarters as WTO headquarters located in Geneva.
138
Research on the automatic interpretation of NCs
has focused mainly on common two-word NCs. The
usual task is to classify the semantic relation under-
lying a compound with either one of a small number
of predefined relation labels or a paraphrase from an
open vocabulary. Examples of the former take on
classification include (Moldovan et al, 2004; Girju,
2007; O? Se?aghdha and Copestake, 2008; Tratz and
Hovy, 2010). Examples of the latter include (Nakov,
2008b; Nakov, 2008a; Nakov and Hearst, 2008; But-
nariu and Veale, 2008) and a previous NC paraphras-
ing task at SemEval-2010 (Butnariu et al, 2010),
upon which the task described here builds.
The assumption of a small inventory of prede-
fined relations has some advantages ? parsimony and
generalization ? but at the same time there are lim-
itations on expressivity and coverage. For exam-
ple, the NCs headache pills and fertility pills would
be assigned the same semantic relation (PURPOSE)
in most inventories, but their relational semantics
are quite different (Downing, 1977). Furthermore,
the definitions given by human subjects can involve
rich and specific meanings. For example, Down-
ing (1977) reports that a subject defined the NC
oil bowl as ?the bowl into which the oil in the en-
gine is drained during an oil change?, compared to
which a minimal interpretation bowl for oil seems
very reductive. In view of such arguments, linguists
such as Downing (1977), Ryder (1994) and Coulson
(2001) have argued for a fine-grained, essentially
open-ended space of interpretations.
The idea of working with fine-grained para-
phrases for NC semantics has recently grown in pop-
ularity among NLP researchers (Butnariu and Veale,
2008; Nakov and Hearst, 2008; Nakov, 2008a). Task
9 at SemEval-2010 (Butnariu et al, 2010) was de-
voted to this methodology. In that previous work,
the paraphrases provided by human subjects were
required to fit a restrictive template admitting only
verbs and prepositions occurring between the NC?s
constituent nouns. Annotators recruited through
Amazon Mechanical Turk were asked to provide
paraphrases for the dataset of NCs. The gold stan-
dard for each NC was the ranked list of paraphrases
given by the annotators; this reflects the idea that a
compound?s meaning can be described in different
ways, at different levels of granularity and capturing
different interpretations in the case of ambiguity.
For example, a plastic saw could be a saw made
of plastic or a saw for cutting plastic. Systems par-
ticipating in the task were given the set of attested
paraphrases for each NC, and evaluated according to
how well they could reproduce the humans? ranking.
The design of this task, SemEval-2013 Task 4,
is informed by previous work on compound anno-
tation and interpretation. It is also influenced by
similar initiatives, such as the English Lexical Sub-
stitution task at SemEval-2007 (McCarthy and Nav-
igli, 2007), and by various evaluation exercises in
the fields of paraphrasing and machine translation.
We build on SemEval-2010 Task 9, extending the
task?s flexibility in a number of ways. The restric-
tions on the form of annotators? paraphrases was re-
laxed, giving us a rich dataset of close-to-freeform
paraphrases (Section 3). Rather than ranking a set of
attested paraphrases, systems must now both gener-
ate and rank their paraphrases; the task they perform
is essentially the same as what the annotators were
asked to do. This new setup required us to innovate
in terms of evaluation measures (Section 4).
We anticipate that the dataset and task will be of
broad interest among those who study lexical se-
mantics. We believe that the overall progress in the
field will significantly benefit from a public-domain
set of free-style NC paraphrases. That is why our
primary objective is the challenging endeavour of
preparing and releasing such a dataset to the re-
search community. The common evaluation task
which we establish will also enable researchers to
compare their algorithms and their empirical results.
2 Task description
This is an English NC interpretation task, which ex-
plores the idea of interpreting the semantics of NCs
via free paraphrases. Given a noun-noun compound
such as air filter, the participating systems are asked
to produce an explicitly ranked list of free para-
phrases, as in the following example:
1 filter for air
2 filter of air
3 filter that cleans the air
4 filter which makes air healthier
5 a filter that removes impurities from the air
. . .
139
Such a list is then automatically compared and
evaluated against a similarly ranked list of para-
phrases proposed by human annotators, recruited
and managed via Amazon?s Mechanical Turk. The
comparison of raw paraphrases is sensitive to syn-
tactic and morphological variation. The ranking
of paraphrases is based on their relative popular-
ity among different annotators. To make the rank-
ing more reliable, highly similar paraphrases are
grouped so as to downplay superficial differences in
syntax and morphology.
3 Data collection
We used Amazon?s Mechanical Turk service to
collect diverse paraphrases for a range of ?gold-
standard? NCs.1 We paid the workers a small fee
($0.10) per compound, for which they were asked to
provide five paraphrases. Each paraphrase should
contain the two nouns of the compound (in sin-
gular or plural inflectional forms, but not in an-
other derivational form), an intermediate non-empty
linking phrase and optional preceding or following
terms. The paraphrasing terms could have any part
of speech, so long as the resulting paraphrase was a
well-formed noun phrase headed by the NC?s head.
We gave the workers feedback during data col-
lection if they appeared to have misunderstood the
nature of the task. Once raw paraphrases had been
collected from all workers, we collated them into a
spreadsheet, and we merged identical paraphrases
in order to calculate their overall frequencies. Ill-
formed paraphrases ? those violating the syntactic
restrictions described above ? were manually re-
moved following a consensus decision-making pro-
cedure; every paraphrase was checked by at least
two task organizers. We did not require that the
paraphrases be semantically felicitous, but we per-
formed minor edits on the remaining paraphrases if
they contained obvious typos.
The remaining well-formed paraphrases were
sorted by frequency separately for each NC. The
most frequent paraphrases for a compound are as-
signed the highest rank 0, those with the next-
highest frequency are given a rank of 1, and so on.
1Since the annotation on Mechanical Turk was going slowly,
we also recruited four other annotators to do the same work,
following exactly the same instructions.
Total Min / Max / Avg
Trial/Train (174 NCs)
paraphrases 6,069 1 / 287 / 34.9
unique paraphrases 4,255 1 / 105 / 24.5
Test (181 NCs)
paraphrases 9,706 24 / 99 / 53.6
unique paraphrases 8,216 21 / 80 / 45.4
Table 1: Statistics of the trial and test datasets: the total
number of paraphrases with and without duplicates, and
the minimum / maximum / average per noun compound.
Paraphrases with a frequency of 1 ? proposed for
a given NC by only one annotator ? always occupy
the lowest rank on the list for that compound.
We used 174+181 noun-noun compounds from
the NC dataset of O? Se?aghdha (2007). The trial
dataset, which we initially released to the partici-
pants, consisted of 4,255 human paraphrases for 174
noun-noun pairs; this dataset was also the training
dataset. The test dataset comprised paraphrases for
181 noun-noun pairs. The ?gold standard? contained
9,706 paraphrases of which 8,216 were unique for
those 181 NCs. Further statistics on the datasets are
presented in Table 1.
Compared with the data collected for the
SemEval-2010 Task 9 on the interpretation of noun
compounds, the data collected for this new task have
a far greater range of variety and richness. For ex-
ample, the following (selected) paraphrases for work
area vary from parsimonious to expansive:
? area for work
? area of work
? area where work is done
? area where work is performed
? . . .
? an area cordoned off for persons responsible for
work
? an area where construction work is carried out
? an area where work is accomplished and done
? area where work is conducted
? office area assigned as a work space
? . . .
140
4 Scoring
Noun compounding is a generative aspect of lan-
guage, but so too is the process of NC interpretation:
human speakers typically generate a range of possi-
ble interpretations for a given compound, each em-
phasizing a different aspect of the relationship be-
tween the nouns. Our evaluation framework reflects
the belief that there is rarely a single right answer
for a given noun-noun pairing. Participating systems
are thus expected to demonstrate some generativity
of their own, and are scored not just on the accu-
racy of individual interpretations, but on the overall
breadth of their output.
For evaluation, we provided a scorer imple-
mented, for good portability, as a Java class. For
each noun compound to be evaluated, the scorer
compares a list of system-suggested paraphrases
against a ?gold-standard? reference list, compiled
and rank-ordered from the paraphrases suggested
by our human annotators. The score assigned to
each system is the mean of the system?s performance
across all test compounds. Note that the scorer re-
moves all determiners from both the reference and
the test paraphrases, so a system is neither punished
for not reproducing a determiner or rewarded for
producing the same determiners.
The scorer can match words identically or non-
identically. A match of two identical words Wgold
and Wtest earns a score of 1.0. There is a partial
score of (2 |P | / (|PWgold| + |PWtest|))2 for a
match of two words PWgold and PWtest that are
not identical but share a common prefix P , |P | > 2,
e.g., wmatch(cutting, cuts) = (6/11)2 = 0.297.
Two n-grams Ngold = [GW1, . . . , GWn] and
Ntest = [TW1, . . . , TWn] can be matched if
wmatch(GWi, TWi) > 0 for all i in 1..n. The
score assigned to the match of these two n-grams is
then
?
i wmatch(GWi, TWi). For every n-gram
Ntest = [TW1, . . . , TWn] in a system-generated
paraphrase, the scorer finds a matching n-gram
Ngold = [GW1, . . . , GWn] in the reference para-
phrase Paragold which maximizes this sum.
The overall n-gram overlap score for a reference
paraphrase Paragold and a system-generated para-
phrase Paratest is the sum of the score calculated
for all n-grams in Paratest, where n ranges from 1
to the size of Paratest.
This overall score is then normalized by dividing
by the maximum value among the n-gram overlap
score for Paragold compared with itself and the n-
gram overlap score for Paratest compared with it-
self. This normalization step produces a paraphrase
match score in the range [0.0 ? 1.0]. It punishes a
paraphrase Paratest for both over-generating (con-
taining more words than are found in Paragold)
and under-generating (containing fewer words than
are found in Paragold). In other words, Paratest
should ideally reproduce everything in Paragold,
and nothing more or less.
The reference paraphrases in the ?gold standard?
are ordered by rank; the highest rank is assigned to
the paraphrases which human judges suggested most
often. The rank of a reference paraphrase matters
because a good participating system will aim to re-
produce the top-ranked ?gold-standard? paraphrases
as produced by human judges. The scorer assigns
a multiplier of R/(R + n) to reference paraphrases
at rank n; this multiplier asymptotically approaches
0 for the higher values of n of ever lower-ranked
paraphrases. We choose a default setting of R = 8,
so that a reference paraphrase at rank 0 (the highest
rank) has a multiplier of 1, while a reference para-
phrase at rank 5 has a multiplier of 8/13 = 0.615.
When a system-generated paraphrase Paratest is
matched with a reference paraphrase Paragold, their
normalized n-gram overlap score is scaled by the
rank multiplier attaching to the rank of Paragold rel-
ative to the other reference paraphrases provided by
human judges. The scorer automatically chooses the
reference paraphrase Paragold for a test paraphrase
Paratest so as to maximize this product of normal-
ized n-gram overlap score and rank multiplier.
The overall score assigned to each system for
a specific compound is calculated in two differ-
ent ways: using isomorphic matching of suggested
paraphrases to the ?gold-standard?s? reference para-
phrases (on a one-to-one basis); and using non-
isomorphic matching of system?s paraphrases to the
?gold-standard?s? reference paraphrases (in a poten-
tially many-to-one mapping).
Isomorphic matching rewards both precision and
recall. It rewards a system for accurately reproduc-
ing the paraphrases suggested by human judges, and
for reproducing as many of these as it can, and in
much the same order.
141
In isomorphic mode, system?s paraphrases are
matched 1-to-1 with reference paraphrases on a first-
come first-matched basis, so ordering can be crucial.
Non-isomorphic matching rewards only preci-
sion. It rewards a system for accurately reproducing
the top-ranked human paraphrases in the ?gold stan-
dard?. A system will achieve a higher score in a non-
isomorphic match if it reproduces the top-ranked hu-
man paraphrases as opposed to lower-ranked human
paraphrases. The ordering of system?s paraphrases
is thus not important in non-isomorphic matching.
Each system is evaluated using the scorer in both
modes, isomorphic and non-isomorphic. Systems
which aim only for precision should score highly
on non-isomorphic match mode, but poorly in iso-
morphic match mode. Systems which aim for pre-
cision and recall will face a more substantial chal-
lenge, likely reflected in their scores.
A na??ve baseline
We decided to allow preposition-only paraphrases,
which are abundant in the paraphrases suggested
by human judges in the crowdsourcing Mechanical
Turk collection process. This abundance means that
the top-ranked paraphrase for a given compound is
often a preposition-only phrase, or one of a small
number of very popular paraphrases such as used for
or used in. It is thus straightforward to build a na??ve
baseline generator which we can expect to score
reasonably on this task, at least in non-isomorphic
matching mode. For each test compound M H,
the baseline system generates the following para-
phrases, in this precise order: H of M, H in M, H
for M, H with M, H on M, H about M, H has M, H to
M, H used for M, H used in M.
This na??ve baseline is truly unsophisticated. No
attempt is made to order paraphrases by their corpus
frequencies or by their frequencies in the training
data. The same sequence of paraphrases is generated
for each and every test compound.
5 Results
Three teams participated in the challenge, and all
their systems were supervised. The MELODI sys-
tem relied on semantic vector space model built
from the UKWAC corpus (window-based, 5 words).
It used only the features of the right-hand head noun
to train a maximum entropy classifier.
Team isomorphic non-isomorphic
SFS 23.1 17.9
IIITH 23.1 25.8
MELODI-Primary 13.0 54.8
MELODI-Contrast 13.6 53.6
Naive Baseline 13.8 40.6
Table 2: Results for the participating systems; the base-
line outputs the same paraphrases for all compounds.
The IIITH system used the probabilities of the
preposition co-occurring with a relation to identify
the class of the noun compound. To collect statis-
tics, it used Google n-grams, BNC and ANC.
The SFS system extracted templates and fillers
from the training data, which it then combined with
a four-gram language model and a MaxEnt reranker.
To find similar compounds, they used Lin?s Word-
Net similarity. They further used statistics from the
English Gigaword and the Google n-grams.
Table 2 shows the performance of the partici-
pating systems, SFS, IIITH and MELODI, and the
na??ve baseline. The baseline shows that it is rela-
tively easy to achieve a moderately good score in
non-isomorphic match mode by generating a fixed
set of paraphrases which are both common and
generic: two of the three participating systems,
SFS and IIITH, under-perform the na??ve baseline
in non-isomorphic match mode, but outperform it
in isomorphic mode. The only system to surpass
this baseline in non-isomorphic match mode is the
MELODI system; yet, it under-performs against the
same baseline in isomorphic match mode. No par-
ticipating team submitted a system which would out-
perform the na??ve baseline in both modes.
6 Conclusions
The conclusions we draw from the experience of or-
ganizing the task are mixed. Participation was rea-
sonable but not large, suggesting that NC paraphras-
ing remains a niche interest ? though we believe it
deserves more attention among the broader lexical
semantics community and hope that the availabil-
ity of our freeform paraphrase dataset will attract a
wider audience in the future.
142
We also observed a varied response from our an-
notators in terms of embracing their freedom to gen-
erate complex and rich paraphrases; there are many
possible reasons for this including laziness, time
pressure and the fact that short paraphrases are often
very appropriate paraphrases. The results obtained
by our participants were also modest, demonstrating
that compound paraphrasing is both a difficult task
and a novel one that has not yet been ?solved?.
Acknowledgments
This work has partially supported by a small but ef-
fective grant from Amazon; the credit allowed us
to hire sufficiently many Turkers ? thanks! And a
thank-you to our additional annotators Dave Carter,
Chris Fournier and Colette Joubarne for their com-
plete sets of paraphrases of the noun compounds in
the test data.
References
Timothy Baldwin and Takaaki Tanaka. 2004. Transla-
tion by machine of complex nominals: Getting it right.
Proc. ACL04 Workshop on Multiword Expressions: In-
tegrating Processing, Barcelona, Spain, 24-31.
Cristina Butnariu and Tony Veale. 2008. A concept-
centered approach to noun-compound interpretation.
Proc. 22nd International Conference on Computa-
tional Linguistics (COLING-08), Manchester, UK, 81-
88.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-
muid O? Se?aghdha, Stan Szpakowicz, and Tony Veale.
2010. SemEval-2010 Task 9: The interpretation of
noun compounds using paraphrasing verbs and prepo-
sitions. Proc. 5th International ACL Workshop on Se-
mantic Evaluation, Uppsala, Sweden, 39-44.
Seana Coulson. 2001. Semantic Leaps: Frame-Shifting
and Conceptual Blending in Meaning Construction.
Cambridge University Press, Cambridge, UK.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53(4): 810-842.
Roxana Girju. 2007. Improving the interpretation of
noun phrases with cross-linguistic information. Proc.
45th Annual Meeting of the Association of Computa-
tional Linguistics, Prague, Czech Republic, 568-575.
Su Nam Kim and Timothy Baldwin. 2006. Interpreting
semantic relations in noun compounds via verb seman-
tics. Proc. ACL-06 Main Conference Poster Session,
Sydney, Australia, 491-498.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. Proc.
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), Prague, Czech Republic, 48-53.
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the se-
mantic classification of noun phrases. Dan Moldovan
and Roxana Girju, eds., HLT-NAACL 2004: Workshop
on Computational Lexical Semantics, Boston, MA,
USA, 60-67.
Preslav Nakov and Marti Hearst. 2008. Solving rela-
tional similarity problems using the Web as a corpus.
Proc. 46th Annual Meeting of the Association for Com-
putational Linguistics ACL-08, Columbus, OH, USA,
452-460.
Preslav Nakov. 2008a. Improved statistical machine
translation using monolingual paraphrases. Proc. 18th
European Conference on Artificial Intelligence ECAI-
08, Patras, Greece, 338-342.
Preslav Nakov. 2008b. Noun compound interpretation
using paraphrasing verbs: Feasibility study. Proc.
13th International Conference on Artificial Intelli-
gence: Methodology, Systems, Applications AIMSA-
08, Varna, Bulgaria, Lecture Notes in Computer Sci-
ence 5253, Springer, 103-117.
Diarmuid O? Se?aghdha. 2007. Designing and Evaluating
a Semantic Annotation Scheme for Compound Nouns.
In Proceedings of the 4th Corpus Linguistics Confer-
ence, Birmingham, UK.
Diarmuid O? Se?aghdha. 2008. Learning compound
noun semantics. Ph.D. thesis, Computer Laboratory,
University of Cambridge. Published as University
of Cambridge Computer Laboratory Technical Report
735.
Diarmuid O? Se?aghdha and Ann Copestake. 2009. Using
lexical and relational similarity to classify semantic re-
lations. Proc. 12th Conference of the European Chap-
ter of the Association for Computational Linguistics
EACL-09, Athens, Greece, 621-629.
Diarmuid O? Se?aghdha and Ann Copestake. 2008. Se-
mantic classification with distributional kernels. In
Proc. 22nd International Conference on Computa-
tional Linguistics (COLING-08), Manchester, UK.
Mary Ellen Ryder. 1994. Ordered Chaos: The Interpre-
tation of English Noun-Noun Compounds. University
of California Press, Berkeley, CA, USA.
Takaaki Tanaka and Tim Baldwin. 2003. Noun-noun
compound machine translation: A feasibility study
on shallow processing. Proc. ACL-2003 Workshop
on Multiword Expressions: Analysis, Acquisition and
Treatment, Sapporo, Japan, 17-24.
Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. Proc. 48th Annual Meeting of the As-
sociation for Computational Linguistics ACL-10, Up-
psala, Sweden, 678-687.
143
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 312?320, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 2: Sentiment Analysis in Twitter
Preslav Nakov
QCRI, Qatar Foundation
pnakov@qf.org.qa
Zornitsa Kozareva
USC Information Sciences Institute
kozareva@isi.edu
Alan Ritter
University of Washington
aritter@cs.washington.edu
Sara Rosenthal
Columbia University
sara@cs.columbia.edu
Veselin Stoyanov
JHU HLTCOE
ves@cs.jhu.edu
Theresa Wilson
JHU HLTCOE
taw@jhu.edu
Abstract
In recent years, sentiment analysis in social
media has attracted a lot of research interest
and has been used for a number of applica-
tions. Unfortunately, research has been hin-
dered by the lack of suitable datasets, com-
plicating the comparison between approaches.
To address this issue, we have proposed
SemEval-2013 Task 2: Sentiment Analysis in
Twitter, which included two subtasks: A, an
expression-level subtask, and B, a message-
level subtask. We used crowdsourcing on
Amazon Mechanical Turk to label a large
Twitter training dataset alng with additional
test sets of Twitter and SMS messages for both
subtasks. All datasets used in the evaluation
are released to the research community. The
task attracted significant interest and a total
of 149 submissions from 44 teams. The best-
performing team achieved an F1 of 88.9% and
69% for subtasks A and B, respectively.
1 Introduction
In the past decade, new forms of communication,
such as microblogging and text messaging have
emerged and become ubiquitous. Twitter messages
(tweets) and cell phone messages (SMS) are often
used to share opinions and sentiments about the sur-
rounding world, and the availability of social con-
tent generated on sites such as Twitter creates new
opportunities to automatically study public opinion.
Working with these informal text genres presents
new challenges for natural language processing be-
yond those encountered when working with more
traditional text genres such as newswire.
Tweets and SMS messages are short in length: a
sentence or a headline rather than a document. The
language they use is very informal, with creative
spelling and punctuation, misspellings, slang, new
words, URLs, and genre-specific terminology and
abbreviations, e.g., RT for re-tweet and #hashtags.1
How to handle such challenges so as to automati-
cally mine and understand the opinions and senti-
ments that people are communicating has only very
recently been the subject of research (Jansen et al,
2009; Barbosa and Feng, 2010; Bifet et al, 2011;
Davidov et al, 2010; O?Connor et al, 2010; Pak and
Paroubek, 2010; Tumasjan et al, 2010; Kouloumpis
et al, 2011).
Another aspect of social media data, such as Twit-
ter messages, is that they include rich structured in-
formation about the individuals involved in the com-
munication. For example, Twitter maintains infor-
mation about who follows whom. Re-tweets (re-
shares of a tweet) and tags inside of tweets provide
discourse information. Modeling such structured in-
formation is important because it provides means for
empirically studying social interactions where opin-
ion is conveyed, e.g., we can study the properties of
persuasive language or those associated with influ-
ential users.
Several corpora with detailed opinion and senti-
ment annotation have been made freely available,
e.g., the MPQA corpus (Wiebe et al, 2005) of
newswire text. These corpora have proved very
valuable as resources for learning about the lan-
guage of sentiment in general, but they did not focus
on social media.
1Hashtags are a type of tagging for Twitter messages.
312
Twitter RT @tash jade: That?s really sad, Charlie RT ?Until tonight I never realised how fucked up I was? -
Charlie Sheen #sheenroast
SMS Glad to hear you are coping fine in uni... So, wat interview did you go to? How did it go?
Table 1: Examples of sentences from each corpus that contain subjective phrases.
While some Twitter sentiment datasets have al-
ready been created, they were either small and pro-
prietary, such as the i-sieve corpus (Kouloumpis
et al, 2011), or they were created only for Span-
ish like the TASS corpus2 (Villena-Roma?n et al,
2013), or they relied on noisy labels obtained from
emoticons and hashtags. They further focused on
message-level sentiment, and no Twitter or SMS
corpus with expression-level sentiment annotations
has been made available so far.
Thus, the primary goal of our SemEval-2013 task
2 has been to promote research that will lead to a
better understanding of how sentiment is conveyed
in Tweets and SMS messages. Toward that goal,
we created the SemEval Tweet corpus, which con-
tains Tweets (for both training and testing) and SMS
messages (for testing only) with sentiment expres-
sions annotated with contextual phrase-level polar-
ity as well as an overall message-level polarity. We
used this corpus as a testbed for the system evalua-
tion at SemEval-2013 Task 2.
In the remainder of this paper, we first describe
the task, the dataset creation process, and the evalu-
ation methodology. We then summarize the charac-
teristics of the approaches taken by the participating
systems and we discuss their scores.
2 Task Description
We had two subtasks: an expression-level subtask
and a message-level subtask. Participants could
choose to participate in either or both subtasks. Be-
low we provide short descriptions of the objectives
of these two subtasks.
Subtask A: Contextual Polarity Disambiguation
Given a message containing a marked instance
of a word or a phrase, determine whether that
instance is positive, negative or neutral in that
context. The boundaries for the marked in-
stance were provided: this was a classification
task, not an entity recognition task.
2http://www.daedalus.es/TASS/corpus.php
Subtask B: Message Polarity Classification
Given a message, decide whether it is of
positive, negative, or neutral sentiment. For
messages conveying both a positive and a
negative sentiment, whichever is the stronger
one was to be chosen.
Each participating team was allowed to submit re-
sults for two different systems per subtask: one con-
strained, and one unconstrained. A constrained sys-
tem could only use the provided data for training,
but it could also use other resources such as lexi-
cons obtained elsewhere. An unconstrained system
could use any additional data as part of the training
process; this could be done in a supervised, semi-
supervised, or unsupervised fashion.
Note that constrained/unconstrained refers to the
data used to train a classifier. For example, if other
data (excluding the test data) was used to develop
a sentiment lexicon, and the lexicon was used to
generate features, the system would still be con-
strained. However, if other data (excluding the test
data) was used to develop a sentiment lexicon, and
this lexicon was used to automatically label addi-
tional Tweet/SMS messages and then used with the
original data to train the classifier, then such a sys-
tem would be unconstrained.
3 Dataset Creation
In the following sections we describe the collection
and annotation of the Twitter and SMS datasets.
3.1 Data Collection
Twitter is the most common micro-blogging site on
the Web, and we used it to gather tweets that express
sentiment about popular topics. We first extracted
named entities using a Twitter-tuned NER system
(Ritter et al, 2011) from millions of tweets, which
we collected over a one-year period spanning from
January 2012 to January 2013; we used the public
streaming Twitter API to download tweets.
313
Instructions: Subjective words are ones which convey an opinion. Given a sentence, identify whether it is objective,
positive, negative, or neutral. Then, identify each subjective word or phrase in the context of the sentence and mark
the position of its start and end in the text boxes below. The number above each word indicates its position. The
word/phrase will be generated in the adjacent textbox so that you can confirm that you chose the correct range.
Choose the polarity of the word or phrase by selecting one of the radio buttons: positive, negative, or neutral. If a
sentence is not subjective please select the checkbox indicating that ?There are no subjective words/phrases?. Please
read the examples and invalid responses before beginning if this is your first time answering this hit.
Figure 1: Instructions provided to workers on Mechanical Turk followed by a screenshot.
Average # of Total Phrase Count Vocabulary
Corpus Words Characters Positive Negative Neutral Size
Twitter - Training 25.4 120.0 5,895 3,131 471 20,012
Twitter - Dev 25.5 120.0 648 430 57 4,426
Twitter - Test 25.4 121.2 2,734 1,541 160 11,736
SMS - Test 24.5 95.6 1,071 1,104 159 3,562
Table 2: Statistics for Subtask A.
We then identified popular topics as those named
entities that are frequently mentioned in association
with a specific date (Ritter et al, 2012). Given this
set of automatically identified topics, we gathered
tweets from the same time period which mentioned
the named entities. The testing messages had differ-
ent topics from training and spanned later periods.
To identify messages that express sentiment to-
wards these topics, we filtered the tweets us-
ing SentiWordNet (Baccianella et al, 2010). We
removed messages that contained no sentiment-
bearing words, keeping only those with at least one
word with positive or negative sentiment score that
is greater than 0.3 in SentiWordNet for at least one
sense of the words. Without filtering, we found class
imbalance to be too high.3
Twitter messages are rich in social media features,
including out-of-vocabulary (OOV) words, emoti-
cons, and acronyms; see Table 1. A large portion of
the OOV words are hashtags (e.g., #sheenroast)
and mentions (e.g., @tash jade).
3Filtering based on an existing lexicon does bias the dataset
to some degree; however, note that the text still contains senti-
ment expressions outside those in the lexicon.
Corpus Positive Negative Objective
/ Neutral
Twitter - Training 3,662 1,466 4,600
Twitter - Dev 575 340 739
Twitter - Test 1,573 601 1,640
SMS - Test 492 394 1,208
Table 3: Statistics for Subtask B.
We annotated the same Twitter messages with an-
notations for subtask A and subtask B. However,
the final training and testing datasets overlap only
partially between the two subtasks since we had
to throw away messages with low inter-annotator
agreement, and this differed between the subtasks.
For testing, we also annotated SMS messages, taken
from the NUS SMS corpus4 (Chen and Kan, 2012).
Tables 2 and 3 show statistics about the corpora we
created for subtasks A and B.
4http://wing.comp.nus.edu.sg/SMSCorpus/
314
A B
Lower Avg. Upper Avg.
Twitter - Train 64.7 82.4 90.8 82.7
Twitter - Dev 51.2 74.7 87.8 78.4
Twitter - Test 68.8 83.6 90.9 76.9
SMS - Test 66.5 88.5 81.2 77.6
Table 4: Bounds for datasets in subtasks A and B.
3.2 Annotation Guidelines
The instructions provided to the annotators, along
with an example, are shown in Figure 1. We pro-
vided several additional examples to the annotators,
shown in Table 5.
In addition, we filtered spammers by considering
the following kinds of annotations invalid:
? containing overlapping subjective phrases;
? subjective but without a subjective phrase;
? marking every single word as subjective;
? not having the overall sentiment marked.
3.3 Annotation Process
Our datasets were annotated for sentiment on Me-
chanical Turk. Each sentence was annotated by five
Mechanical Turk workers (Turkers). In order to
qualify for the hits, the Turker had to have an ap-
proval rate greater than 95% and have completed 50
approved hits. Each Turker was paid three cents
per hit. The Turker had to mark all the subjec-
tive words/phrases in the sentence by indicating their
start and end positions and say whether each subjec-
tive word/phrase was positive, negative, or neutral
(subtask A). They also had to indicate the overall
polarity of the sentence (subtask B).
Figure 1 shows the instructions and an exam-
ple provided to the Turkers. The first five rows
of Table 6 show an example of the subjective
words/phrases marked by each of the workers.
For subtask A, we combined the annotations of
each of the workers using intersection as indicated
in the last row of Table 6. A word had to appear
in 2/3 of the annotations in order to be considered
subjective. Similarly, a word had to be labeled with
a particular polarity (positive, negative, or neutral)
2/3 of the time in order to receive that label.
We also experimented with combining annota-
tions by computing the union of the sentences, and
taking the sentence of the worker who annotated the
most hits, but we found that these methods were
not as accurate. Table 4 shows the lower, average,
and upper bounds for all the hits by computing the
bounds for each hit and averaging them together.
This gives a good indication about how well we can
expect the systems to perform. For example, even if
we used the best annotator each time, it would still
not be possible to get perfect accuracy.
For subtask B, the polarity of the entire sentence
was determined based on the majority of the labels.
If there was a tie, the sentence was discarded. In
order to reduce the number of sentences lost, we
combined the objective and the neutral labels, which
Turkers tended to mix up. Table 4 shows the aver-
age bound for subtask B by computing the bounds
for each hit and averaging them together. Since the
polarity is chosen based on the majority, the upper
bound is 100%.
4 Scoring
For both subtasks, the participating systems were
required to perform a three-way classification ? a
particular marked phrase (for subtask A) or an en-
tire message (for subtask B) was to be classified as
positive, negative, or objective. For each system,
we computed a score for predicting positive/negative
phrases/messages vs. the other two classes.
For instance, to compute positive precision, Ppos,
we find the number of phrases/messages that a sys-
tem correctly predicted to be positive, and we divide
that number by the total number of messages it pre-
dicted to be positive. To compute recall, for the pos-
itive class, Rpos, we find the number of messages
correctly predicted to be positive and we divide that
number by the total number of positive messages in
the gold standard.
We then calculate F-score for the positive labels,
the harmonic average of precision and recall as fol-
lows Fpos = 2
PposRpos
Ppos+Rpos
. We carry out a similar
computation to calculate Fneg, which is F1 for neg-
ative messages.
The overall score for each system run is then
given by the average of the F1-scores for the posi-
tive and negative classes: F = (Fpos + Fneg)/2.
315
Authorities are only too aware that Kashgar is 4,000 kilometres (2,500 miles) from Beijing but only a tenth of
the distance from the Pakistani border, and are desperate to ensure instability or militancy does not leak over the
frontiers.
Taiwan-made products stood a good chance of becoming even more competitive thanks to wider access to overseas
markets and lower costs for material imports, he said.
?March appears to be a more reasonable estimate while earlier admission cannot be entirely ruled out,? according
to Chen, also Taiwan?s chief WTO negotiator.
friday evening plans were great, but saturday?s plans didnt go as expected ? i went dancing & it was an ok club,
but terribly crowded :-(
WHY THE HELL DO YOU GUYS ALL HAVE MRS. KENNEDY! SHES A FUCKING DOUCHE
AT&T was okay but whenever they do something nice in the name of customer service it seems like a favor, while
T-Mobile makes that a normal everyday thin
obama should be impeached on TREASON charges. Our Nuclear arsenal was TOP Secret. Till HE told our enemies
what we had. #Coward #Traitor
My graduation speech: ?I?d like to thanks Google, Wikipedia and my computer! :D #iThingteens
Table 5: List of example sentences with annotations that were provided to the annotators. All subjective phrases are
italicized. Positive phrases are in green, negative phrases are in red, and neutral phrases are in blue.
Worker 1 I would love to watch Vampire Diaries :) and some Heroes! Great combination 9/13
Worker 2 I would love to watch Vampire Diaries :) and some Heroes! Great combination 11/13
Worker 3 I would love to watch Vampire Diaries :) and some Heroes! Great combination 10/13
Worker 4 I would love to watch Vampire Diaries :) and some Heroes! Great combination 13/13
Worker 5 I would love to watch Vampire Diaries :) and some Heroes! Great combination 11/13
Intersection I would love to watch Vampire Diaries :) and some Heroes! Great combination
Table 6: Example of a sentence annotated for subjectivity on Mechanical Turk. Words and phrases that were marked as
subjective are italicized and highlighted in bold. The first five rows are annotations provided by Turkers, and the final
row shows their intersection. The final column shows the accuracy for each annotation compared to the intersection.
Note that ignoring Fneutral does not reduce the
task to predicting positive vs. negative labels only
(even though some participants have chosen to do
so) since the gold standard still contains neutral
labels which are to be predicted: Fpos and Fneg
would suffer if these examples are labeled as posi-
tive and/or negative instead of neutral.
We provided participants with a scorer. In addi-
tion to outputting the overall F-score, it produced
a confusion matrix for the three prediction classes
(positive, negative, and objective), and it also vali-
dated the data submission format.
5 Participants and Results
The results for subtask A are shown in Tables 7 and
8 for Twitter and for SMS messages, respectively;
those for subtask B are shown in Table 9 for Twit-
ter and in Table 10 for SMS messages. Systems are
ranked by their scores for the constrained runs; the
ranking based on scores for unconstrained runs is
shown as a subindex.
For both subtasks, there were teams that only sub-
mitted results for the Twitter test set. Some teams
submitted both a constrained and an unconstrained
version (e.g., AVAYA and teragram). As one would
expect, the results on the Twitter test set tended to be
better than those on the SMS test set since the SMS
data was out-of-domain with respect to the training
(Twitter) data.
Moreover, the results for subtask A were signifi-
cantly better than those for subtask B, which shows
that it is a much easier task, probably because there
is less ambiguity at the phrase-level.
5.1 Subtask A: Contextual Polarity
Table 7 shows that subtask A, Twitter, attracted 23
teams, who submitted 21 constrained and 7 uncon-
strained systems. Five teams submitted both a con-
strained and an unconstrained system, and two other
teams submitted constrained systems that are on
the boundary between being constrained and uncon-
strained.
316
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
NRC-Canada 88.93 yes yes
AVAYA 86.98 87.38(1) yes yes
BOUNCE 86.79 yes yes
LVIC-LIMSI 85.70 yes yes
FBM 85.50 yes semi
GU-MLT-LT 85.19 yes yes
UNITOR 84.60 yes yes
USNA 81.31 yes yes
Serendio 80.04 yes yes
ECNUCS 79.48 80.15(2) yes yes
TJP 78.16 yes yes
?columbia-nlp 74.94 yes yes
teragram 74.89(3) yes yes
sielers 74.41 yes yes
KLUE 73.74 yes yes
OPTWIMA 69.17 36.91(6) yes yes
swatcs 67.19 63.86(5) no yes
Kea 63.94 yes yes
senti.ue-en 62.79 71.38(4) yes yes
uottawa 60.20 yes yes
IITB 54.80 yes yes
SenselyticTeam 53.88 yes yes
SU-sentilab 34.73(7) no yes
Majority Baseline 38.10 N/A N/A
Table 7: Results for subtask A on the Twitter dataset. The
? marks a team that includes a task coorganizer, and the
 indicates a system submitted as constrained but which
used additional Tweets or additional sentiment-annotated
text to collect statistics that were then used as a feature.
One system was semi-supervised, and the rest
were supervised. The supervised systems used clas-
sifiers such as SVM (8 systems), Naive Bayes (7 sys-
tems), and Maximum Entropy (3 systems). Other
approaches used include an ensemble of classifiers,
manual rules, and a linear classifier. Two of the sys-
tems chose not to predict neutral as a possible clas-
sification label.
The average F1-measure on the Twitter test set
was 74.1% for constrained systems and 60.5% for
unconstrained ones; this does not mean that using
additional data does not help, it just shows that the
best teams only participated with a constrained sys-
tem. NRC-Canada had the best constrained system
with an F1-measure of 88.9%, and AVAYA had the
best unconstrained one with F1=87.4%.
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
GU-MLT-LT 88.37 yes yes
NRC-Canada 88.00 yes yes
?AVAYA 83.94 85.79(1) yes yes
UNITOR 82.49 yes yes
TJP 81.23 yes yes
LVIC-LIMSI 80.16 yes yes
USNA 79.82 yes yes
ECNUCS 76.69 77.34(2) yes yes
sielers 73.48 yes yes
FBM 72.95 no semi
teragram 72.83 72.83(4) yes yes
KLUE 70.54 yes yes
?columbia-nlp 70.30 yes yes
senti.ue-en 66.09 74.13(3) yes yes
swatcs 66.00 67.68(5) no yes
Kea 63.27 yes yes
uottawa 55.89 yes yes
SU-sentilab 55.38(6) no yes
SenselyticTeam 51.13 yes yes
OPTWIMA 37.32 36.38(7) yes yes
Majority Baseline 31.50 N/A N/A
Table 8: Results for subtask A on the SMS dataset. The
? indicates a late submission, the ? marks a team that
includes a task co-organizer, and the  indicates a sys-
tem submitted as constrained but which used additional
Tweets or additional sentiment-annotated text to collect
statistics that were then used as a feature.
Table 8 shows the results for the SMS test set,
where 20 teams submitted 19 constrained and 7 un-
constrained systems (again, this included two teams
that submitted boundary systems, marked accord-
ingly). The average F-measure on this test set
was 70.8% for constrained systems and 65.7% for
unconstrained systems. The best constrained sys-
tem was that of GU-MLT-LT with an F-measure of
88.4%, and AVAYA had the best unconstrained sys-
tem with an F1 of 85.8%.
5.2 Subtask B: Message Polarity
Table 9 shows that subtask B, Twitter, attracted 38
teams, who submitted 36 constrained and 15 uncon-
strained systems (and two boundary ones).
The average F1-measure was 53.7% for the con-
strained and 54.6% for the unconstrained systems.
317
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
NRC-Canada 69.02 yes yes
GU-MLT-LT 65.27 yes yes
teragram 64.86 64.86(1) yes yes
BOUNCE 63.53 yes yes
KLUE 63.06 yes yes
AMI&ERIC 62.55 61.17(3) yes yes/semi
FBM 61.17 yes yes
AVAYA 60.84 64.06(2) yes yes/semi
SAIL 60.14 61.03(4) yes yes
UT-DB 59.87 yes yes
FBK-irst 59.76 yes yes
nlp.cs.aueb.gr 58.91 yes yes
UNITOR 58.27 59.50(5) yes semi
LVIC-LIMSI 57.14 yes yes
Umigon 56.96 yes yes
NILC USP 56.31 yes yes
DataMining 55.52 yes semi
ECNUCS 55.05 58.42(6) yes yes
nlp.cs.aueb.gr 54.73 yes yes
ASVUniOfLeipzig 54.56 yes yes
SZTE-NLP 54.33 53.10(9) yes yes
CodeX 53.89 yes yes
Oasis 53.84 yes yes
NTNU 53.23 50.71(10) yes yes
UoM 51.81 45.07(15) yes yes
SSA-UO 50.17 yes no
SenselyticTeam 50.10 yes yes
UMCC DLSI (SA) 49.27 48.99(12) yes yes
bwbaugh 48.83 54.37(8) yes yes/semi
senti.ue-en 47.24 47.85(13) yes yes
SU-sentilab 45.75(14) yes yes
OPTWIMA 45.40 54.51(7) yes yes
REACTION 45.01 yes yes
uottawa 42.51 yes yes
IITB 39.80 yes yes
IIRG 34.44 yes yes
sinai 16.28 49.26(11) yes yes
Majority Baseline 29.19 N/A N/A
Table 9: Results for subtask B on the Twitter dataset. The
 indicates a system submitted as constrained but which
used additional Tweets or additional sentiment-annotated
text to collect statistics that were then used as a feature.
These averages are much lower than those for sub-
task A, which indicates that subtask B is harder,
probably because a message can contain parts ex-
pressing both positive and negative sentiment.
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
NRC-Canada 68.46 yes yes
GU-MLT-LT 62.15 yes yes
KLUE 62.03 yes yes
AVAYA 60.00 59.47(1) yes yes/semi
teragram 59.10(2) yes yes
NTNU 57.97 54.55(6) yes yes
CodeX 56.70 yes yes
FBK-irst 54.87 yes yes
AMI&ERIC 53.63 52.62(7) yes yes/semi
ECNUCS 53.21 54.77(5) yes yes
UT-DB 52.46 yes yes
SAIL 51.84 51.98(8) yes yes
UNITOR 51.22 48.88(10) yes semi
SZTE-NLP 51.08 55.46(3) yes yes
SenselyticTeam 51.07 yes yes
NILC USP 50.12 yes yes
REACTION 50.11 yes yes
SU-sentilab 49.57(9) no yes
nlp.cs.aueb.gr 49.41 55.28(4) yes yes
LVIC-LIMSI 49.17 yes yes
FBM 47.40 yes yes
ASVUniOfLeipzig 46.50 yes yes
senti.ue-en 44.65 46.72(12) yes yes
SSA UO 44.39 yes no
UMCC DLSI (SA) 43.39 40.67(14) yes yes
UoM 42.22 35.22(15) yes yes
OPTWIMA 40.98 47.15(11) yes yes
uottawa 40.51 yes yes
bwbaugh 39.73 43.43(13) yes yes/semi
IIRG 22.16 yes yes
Majority Baseline 19.03 N/A N/A
Table 10: Results for subtask B on the SMS dataset. The
 indicates a system submitted as constrained but which
used additional Tweets or additional sentiment-annotated
text to collect statistics that were then used as a feature.
Once again, NRC-Canada had the best con-
strained system with an F1-measure of 69%, fol-
lowed by teragram, which had the best uncon-
strained system with an F1-measure of 64.9%.
As Table 10 shows, the average F1-measure on
the SMS test set was 50.2% for constrained and
50.3% for unconstrained systems. NRC-Canada had
the best constrained system with an F1=68.5%, and
AVAYA had the best unconstrained one with F1-
measure of 59.5%.
318
5.3 Overall
Overall, the results achieved by the best teams were
very strong, especially for the simpler subtask A:
? F1=88.93, NRC-Canada on subtask A, Twitter;
? F1=88.37, GU-MLT-LT on subtask A, SMS;
? F1=69.02, NRC-Canada on subtask B, Twitter;
? F1=68.46, NRC-Canada on subtask B, SMS.
We can see that the strongest team overall was that
of NRC-Canada, which was ranked first on three of
the four conditions; and it was second on subtask A,
SMS. There were two other teams that were strong
across both tasks and on both test sets: GU-MLT-LT
and AVAYA. Three other teams, namely teragram,
BOUNCE and KLUE, were ranked in the top-3 in at
least one subtask and test set.
6 Discussion
We have seen that most participants restricted them-
selves to the provided data and submitted con-
strained systems. Indeed, the best systems for each
of the two subtasks and for each of the two testing
datasets were constrained systems; of course, this
does not mean that additional data would not be use-
ful. Curiously, in some cases where a team submit-
ted a constrained and unconstrained run, the uncon-
strained run actually performed worse.
Not surprisingly, most systems were supervised;
there were only five semi-supervised systems, and
there was only one unsupervised system. One ad-
ditional team declared their system as unsupervised
since it was not making use of the training data; we
still classified it as supervised though since it did use
supervision ? in the form of manual rules.
Most participants predicted all three labels (posi-
tive, negative and neutral), even though some partic-
ipants opted for not predicting neutral, which made
some sense since the final F1-score was averaged
over the positive and the negative predictions only.
The most popular classifiers included SVM, Max-
Ent, linear classifier, Naive Bayes; in some cases,
manual rules or ensembles of classifiers were used.
A variety of features were used, including word-
related (e.g., words, stems, n-grams, word clus-
ters), word-shape (e.g., punctuation, capitalization),
syntactic (e.g., POS tags, dependency relations),
Twitter-specific (e.g., repeated characters, emoti-
cons, URLs, hashtags, slang, abbreviations), and
sentiment-related (e.g., negation); one team also
used discourse relations. Almost all participants re-
lied heavily of various sentiment lexicons, the most
popular ones being MPQA and SentiWordNet, as
well as AFINN and Bing Liu?s Opinion Lexicon;
some participants used their own lexicons ? preex-
isting or built from the provided data.
Given that Twitter messages are noisy, most par-
ticipants did some preprocessing, including tok-
enization, stemming, lemmatization, stopword re-
moval, normalization/removal of URLs, hashtags,
users, slang, emoticons, repeated vowels, punctua-
tion; some even did pronoun resolution.
7 Conclusion
We have described a new task that entered SemEval-
2013: task 2 on Sentiment Analysis on Twitter. The
task has attracted a very high number of participants:
149 submissions from 44 teams.
We believe that the datasets that we have created
as part of the task and which we have released to the
community5 under a Creative Commons Attribution
3.0 Unported License,6 will be found useful by re-
searchers beyond SemEval.
Acknowledgments
The authors would like to thank Kathleen McKeown
for her insight in creating the Amazon Mechanical
Turk annotation task.
Funding for the Amazon Mechanical Turk anno-
tations was provided by the JHU Human Language
Technology Center of Excellence and by the Of-
fice of the Director of National Intelligence (ODNI),
Intelligence Advanced Research Projects Activity
(IARPA), through the U.S. Army Research Lab. All
statements of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of IARPA, the ODNI or the U.S. Government.
5http://www.cs.york.ac.uk/semeval-2013/task2/
6http://creativecommons.org/licenses/by/3.0/
319
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Nicoletta Calzolari (chair), Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios
Piperidis, Mike Rosner, and Daniel Tapias, editors,
Proceedings of the Seventh International Conference
on Language Resources and Evaluation, LREC ?10,
pages 2200?2204, Valletta, Malta.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on Twitter from biased and noisy data.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, COLING ?10,
pages 36?44, Beijing, China.
Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer, and
Ricard Gavalda`. 2011. Detecting sentiment change in
Twitter streaming data. Journal of Machine Learning
Research - Proceedings Track, 17:5?11.
Tao Chen and Min-Yen Kan. 2012. Creating a live, pub-
lic short message service corpus: the NUS SMS cor-
pus. Language Resources and Evaluation, pages 1?
37.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences in
Twitter and Amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ?10, pages 107?116, Upp-
sala, Sweden.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Twitter power: Tweets as elec-
tronic word of mouth. J. Am. Soc. Inf. Sci. Technol.,
60(11):2169?2188.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The Good
the Bad and the OMG! In Lada A. Adamic, Ricardo A.
Baeza-Yates, and Scott Counts, editors, Proceedings of
the Fifth International Conference on Weblogs and So-
cial Media, ICWSM? 11, pages 538?541, Barcelona,
Spain.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In William W. Cohen and
Samuel Gosling, editors, Proceedings of the Fourth
International Conference on Weblogs and Social
Media, ICWSM ?10, Washington, DC, USA.
Alexander Pak and Patrick Paroubek. 2010. Twitter
based system: Using Twitter for disambiguating senti-
ment ambiguous adjectives. In Proceedings of the 5th
International Workshop on Semantic Evaluation, Se-
mEval ?10, pages 436?439, Los Angeles, CA, USA.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an exper-
imental study. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?11, pages 1524?1534, Edinburgh, United
Kingdom.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter. In
Proceedings of the 18th ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD ?12, pages 1104?1112, Beijing, China.
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-
ner, and Isabell M. Welpe. 2010. Predicting elections
with Twitter: What 140 characters reveal about po-
litical sentiment. In William W. Cohen and Samuel
Gosling, editors, Proceedings of the Fourth Inter-
national Conference on Weblogs and Social Media,
ICWSM ?10, pages 178?185, Washington, DC, USA.
The AAAI Press.
Julio Villena-Roma?n, Sara Lana-Serrano, Euge-
nio Mart??nez-Ca?mara, and Jose? Carlos Gonza?lez
Cristo?bal. 2013. TASS - Workshop on Sentiment
Analysis at SEPLN. Procesamiento del Lenguaje
Natural, 50:37?44.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
320
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 73?80,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 9: Sentiment Analysis in Twitter
Sara Rosenthal
Columbia University
sara@cs.columbia.edu
Preslav Nakov
Qatar Computing Research Institute
pnakov@qf.org.qa
Alan Ritter
Carnegie Mellon University
rittera@cs.cmu.edu
Veselin Stoyanov
Johns Hopkins University
ves@cs.jhu.edu
Abstract
We describe the Sentiment Analysis in
Twitter task, ran as part of SemEval-2014.
It is a continuation of the last year?s task
that ran successfully as part of SemEval-
2013. As in 2013, this was the most popu-
lar SemEval task; a total of 46 teams con-
tributed 27 submissions for subtask A (21
teams) and 50 submissions for subtask B
(44 teams). This year, we introduced three
new test sets: (i) regular tweets, (ii) sarcas-
tic tweets, and (iii) LiveJournal sentences.
We further tested on (iv) 2013 tweets, and
(v) 2013 SMS messages. The highest F1-
score on (i) was achieved by NRC-Canada
at 86.63 for subtask A and by TeamX at
70.96 for subtask B.
1 Introduction
In the past decade, new forms of communica-
tion have emerged and have become ubiquitous
through social media. Microblogs (e.g., Twitter),
Weblogs (e.g., LiveJournal) and cell phone mes-
sages (SMS) are often used to share opinions and
sentiments about the surrounding world, and the
availability of social content generated on sites
such as Twitter creates new opportunities to au-
tomatically study public opinion.
Working with these informal text genres
presents new challenges for natural language pro-
cessing beyond those encountered when work-
ing with more traditional text genres such as
newswire. The language in social media is very
informal, with creative spelling and punctuation,
misspellings, slang, new words, URLs, and genre-
specific terminology and abbreviations, e.g., RT
for re-tweet and #hashtags
1
.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1
Hashtags are a type of tagging for Twitter messages.
Moreover, tweets and SMS messages are short:
a sentence or a headline rather than a document.
How to handle such challenges so as to automat-
ically mine and understand people?s opinions and
sentiments has only recently been the subject of
research (Jansen et al., 2009; Barbosa and Feng,
2010; Bifet et al., 2011; Davidov et al., 2010;
O?Connor et al., 2010; Pak and Paroubek, 2010;
Tumasjan et al., 2010; Kouloumpis et al., 2011).
Several corpora with detailed opinion and sen-
timent annotation have been made freely avail-
able, e.g., the MPQA newswire corpus (Wiebe et
al., 2005), the movie reviews corpus (Pang et al.,
2002), or the restaurant and laptop reviews cor-
pora that are part of this year?s SemEval Task 4
(Pontiki et al., 2014). These corpora have proved
very valuable as resources for learning about the
language of sentiment in general, but they do not
focus on tweets. While some Twitter sentiment
datasets were created prior to SemEval-2013, they
were either small and proprietary, such as the i-
sieve corpus (Kouloumpis et al., 2011) or focused
solely on message-level sentiment.
Thus, the primary goal of our SemEval task is
to promote research that will lead to better un-
derstanding of how sentiment is conveyed in So-
cial Media. Toward that goal, we created the Se-
mEval Tweet corpus as part of our inaugural Sen-
timent Analysis in Twitter Task, SemEval-2013
Task 2 (Nakov et al., 2013). It contains tweets
and SMS messages with sentiment expressions an-
notated with contextual phrase-level and message-
level polarity. This year, we extended the corpus
by adding new tweets and LiveJournal sentences.
Another interesting phenomenon that has been
studied in Twitter is the use of the #sarcasm hash-
tag to indicate that a tweet should not be taken lit-
erally (Gonz?alez-Ib?a?nez et al., 2011; Liebrecht et
al., 2013). In fact, sarcasm indicates that the mes-
sage polarity should be flipped. With this in mind,
this year, we also evaluate on sarcastic tweets.
73
In the remainder of this paper, we first describe
the task, the dataset creation process and the eval-
uation methodology. We then summarize the char-
acteristics of the approaches taken by the partici-
pating systems, and we discuss their scores.
2 Task Description
As SemEval-2013 Task 2, we included two sub-
tasks: an expression-level subtask and a message-
level subtask. Participants could choose to partici-
pate in either or both. Below we provide short de-
scriptions of the objectives of these two subtasks.
Subtask A: Contextual Polarity Disambiguation
Given a message containing a marked in-
stance of a word or a phrase, determine
whether that instance is positive, negative or
neutral in that context. The instance bound-
aries were provided: this was a classification
task, not an entity recognition task.
Subtask B: Message Polarity Classification
Given a message, decide whether it is of
positive, negative, or neutral sentiment.
For messages conveying both positive and
negative sentiment, the stronger one is to be
chosen.
Each participating team was allowed to submit
results for two different systems per subtask: one
constrained, and one unconstrained. A constrained
system could only use the provided data for train-
ing, but it could also use other resources such as
lexicons obtained elsewhere. An unconstrained
system could use any additional data as part of
the training process; this could be done in a super-
vised, semi-supervised, or unsupervised fashion.
Note that constrained/unconstrained refers to
the data used to train a classifier. For example,
if other data (excluding the test data) was used to
develop a sentiment lexicon, and the lexicon was
used to generate features, the system would still
be constrained. However, if other data (excluding
the test data) was used to develop a sentiment lexi-
con, and this lexicon was used to automatically la-
bel additional Tweet/SMS messages and then used
with the original data to train the classifier, then
such a system would be considered unconstrained.
3 Datasets
In this section, we describe the process of collect-
ing and annotating the 2014 testing tweets, includ-
ing the sarcastic ones, and LiveJournal sentences.
Corpus Positive Negative Objective
/ Neutral
Twitter2013-train 5,895 3,131 471
Twitter2013-dev 648 430 57
Twitter2013-test 2,734 1,541 160
SMS2013-test 1,071 1,104 159
Twitter2014-test 1,807 578 88
Twitter2014-sarcasm 82 37 5
LiveJournal2014-test 660 511 144
Table 1: Dataset statistics for Subtask A.
3.1 Datasets Used
For training and development, we released the
Twitter train/dev/test datasets from SemEval-2013
task 2, as well as the SMS test set, which uses mes-
sages from the NUS SMS corpus (Chen and Kan,
2013), which we annotated for sentiment in 2013.
We further added a new 2014 Twitter test set,
as well as a small set of tweets that contained
the #sarcasm hashtag to determine how sarcasm
affects the tweet polarity. Finally, we included
sentences from LiveJournal in order to determine
how systems trained on Twitter perform on other
sources. The statistics for each dataset and for
each subtask are shown in Tables 1 and 2.
Corpus Positive Negative Objective
/ Neutral
Twitter2013-train 3,662 1,466 4,600
Twitter2013-dev 575 340 739
Twitter2013-test 1,572 601 1,640
SMS2013-test 492 394 1,207
Twitter2014-test 982 202 669
Twitter2014-sarcasm 33 40 13
LiveJournal2014-test 427 304 411
Table 2: Dataset statistics for Subtask B.
3.2 Annotation
We annotated the new tweets as in 2013: by iden-
tifying tweets from popular topics that contain
sentiment-bearing words by using SentiWordNet
(Baccianella et al., 2010) as a filter. We altered the
annotation task for the sarcastic tweets, displaying
them to the Mechanical Turk annotators without
the #sarcasm hashtag; the Turkers had to deter-
mine whether the tweet is sarcastic on their own.
Moreover, we asked Turkers to indicate the degree
of sarcasm as (a) definitely sarcastic, (b) probably
sarcastic, and (c) not sarcastic.
As in 2013, we combined the annotations using
intersection, where a word had to appear in 2/3
of the annotations to be accepted. An annotated
example from each source is shown in Table 3.
74
Source Example Polarity
Twitter Why would you [still]- wear shorts when it?s this cold?! I [love]+ how Britain see?s a
bit of sun and they?re [like ?OOOH]+ LET?S STRIP!?
positive
SMS [Sorry]- I think tonight [cannot]- and I [not feeling well]- after my rest. negative
LiveJournal [Cool]+ posts , dude ; very [colorful]+ , and [artsy]+ . positive
Twitter Sarcasm [Thanks]+ manager for putting me on the schedule for Sunday negative
Table 3: Example of polarity for each source of messages. The target phrases are marked in [. . .], and
are followed by their polarity; the sentence-level polarity is shown in the last column.
3.3 Tweets Delivery
We did not deliver the annotated tweets to the par-
ticipants directly; instead, we released annotation
indexes, a list of corresponding Twitter IDs, and
a download script that extracts the correspond-
ing tweets via the Twitter API.
2
We provided the
tweets in this manner in order to ensure that Twit-
ter?s terms of service are not violated. Unfor-
tunately, due to this restriction, the task partici-
pants had access to different number of training
tweets depending on when they did the download-
ing. This varied between a minimum of 5,215
tweets and the full set of 10,882 tweets. On av-
erage the teams were able to collect close to 9,000
tweets; for teams that did not participate in 2013,
this was about 8,500. The difference in training
data size did not seem to have had a major impact.
In fact, the top two teams in subtask B (coooolll
and TeamX) trained on less than 8,500 tweets.
4 Scoring
The participating systems were required to per-
form a three-way classification for both subtasks.
A particular marked phrase (for subtask A) or an
entire message (for subtask B) was to be classi-
fied as positive, negative or objective/neutral. We
scored the systems by computing a score for pre-
dicting positive/negative phrases/messages. For
instance, to compute positive precision, p
pos
, we
find the number of phrases/messages that a sys-
tem correctly predicted to be positive, and we di-
vide that number by the total number it predicted
to be positive. To compute positive recall, r
pos
,
we find the number of phrases/messages correctly
predicted to be positive and we divide that number
by the total number of positives in the gold stan-
dard. We then calculate F1-score for the positive
class as follows F
pos
=
2(p
pos
+r
pos
)
p
pos
?r
pos
. We carry
out a similar computation for F
neg
, for the nega-
tive phrases/messages. The overall score is then
F = (F
pos
+ F
neg
)/2.
2
https://dev.twitter.com
We used the two test sets from 2013 and the
three from 2014, which we combined into one test
set and we shuffled to make it hard to guess which
set a sentence came from. This guaranteed that
participants would submit predictions for all five
test sets. It also allowed us to test how well sys-
tems trained on standard tweets generalize to sar-
castic tweets and to LiveJournal sentences, with-
out the participants putting extra efforts into this.
The participants were also not informed about the
source the extra test sets come from.
We provided the participants with a scorer that
outputs the overall score F and a confusion matrix
for each of the five test sets.
5 Participants and Results
The results are shown in Tables 4 and 5, and the
team affiliations are shown in Table 6. Tables 4
and 5 contain results on the two progress test sets
(tweets and SMS messages), which are the official
test sets from the 2013 edition of the task, and on
the three new official 2014 testsets (tweets, tweets
with sarcasm, and LiveJournal). The tables fur-
ther show macro- and micro-averaged results over
the 2014 datasets. There is an index for each re-
sult showing the relative rank of that result within
the respective column. The participating systems
are ranked by their score on the Twitter-2014 test-
set, which is the official ranking for the task; all
remaining rankings are secondary.
As we mentioned above, the participants were
not told that the 2013 test sets would be included
in the big 2014 test set, so that they do not over-
tune their systems on them. However, the 2013
test sets were made available for development, but
it was explicitly forbidden to use them for training.
Still, some participants did not notice this restric-
tion, which resulted in their unusually high scores
on Twitter2013-test; we did our best to identify
all such cases, and we asked the authors to submit
corrected runs. The tables mark such resubmis-
sions accordingly.
75
Most of the submissions were constrained, with
just a few unconstrained: 7 out of 27 for subtask
A, and 8 out of 50 for subtask B. In any case, the
best systems were constrained. Some teams par-
ticipated with both a constrained and an uncon-
strained system, but the unconstrained system was
not always better than the constrained one: some-
times it was worse, sometimes it performed the
same. Thus, we decided to produce a single rank-
ing, including both constrained and unconstrained
systems, where we mark the latter accordingly.
5.1 Subtask A
Table 4 shows the results for subtask A, which at-
tracted 27 submissions from 21 teams. There were
seven unconstrained submissions: five teams sub-
mitted both a constrained and an unconstrained
run, and two teams submitted an unconstrained
run only. The best systems were constrained. All
participating systems outperformed the majority
class baseline by a sizable margin.
5.2 Subtask B
The results for subtask B are shown in Table 5.
The subtask attracted 50 submissions from 44
teams. There were eight unconstrained submis-
sions: six teams submitted both a constrained and
an unconstrained run, and two teams submitted an
unconstrained run only. As for subtask A, the best
systems were constrained. Again, all participating
systems outperformed the majority class baseline;
however, some systems were very close to it.
6 Discussion
Overall, we observed similar trends as in
SemEval-2013 Task 2. Almost all systems used
supervised learning. Most systems were con-
strained, including the best ones in all categories.
As in 2013, we observed several cases of a team
submitting a constrained and an unconstrained run
and the constrained run performing better.
It is unclear why unconstrained systems did not
outperform constrained ones. It could be because
participants did not use enough external data or
because the data they used was too different from
Twitter or from our annotation method. Or it could
be due to our definition of unconstrained, which
labels as unconstrained systems that use additional
tweets directly, but considers unconstrained those
that use additional tweets to build sentiment lexi-
cons and then use these lexicons.
As in 2013, the most popular classifiers were
SVM, MaxEnt, and Naive Bayes. Moreover, two
submissions used deep learning, coooolll (Harbin
Institute of Technology) and ThinkPositive (IBM
Research, Brazil), which were ranked second and
tenth on subtask B, respectively.
The features used were quite varied, includ-
ing word-based (e.g., word and character n-
grams, word shapes, and lemmata), syntactic, and
Twitter-specific such as emoticons and abbrevia-
tions. The participants still relied heavily on lex-
icons of opinion words, the most popular ones
being the same as in 2013: MPQA, SentiWord-
Net and Bing Liu?s opinion lexicon. Popular this
year was also the NRC lexicon (Mohammad et
al., 2013), created by the best-performing team in
2013, which is top-performing this year as well.
Preprocessing of tweets was still a popular tech-
nique. In addition to standard NLP steps such
as tokenization, stemming, lemmatization, stop-
word removal and POS tagging, most teams ap-
plied some kind of Twitter-specific processing
such as substitution/removal of URLs, substitu-
tion of emoticons, word normalization, abbrevi-
ation lookup, and punctuation removal. Finally,
several of the teams used Twitter-tuned NLP tools
such as part of speech and named entity taggers
(Gimpel et al., 2011; Ritter et al., 2011).
The similarity of preprocessing techniques,
NLP tools, classifiers and features used in 2013
and this year is probably partially due to many
teams participating in both years. As Table 6
shows, 18 out of the 46 teams are returning teams.
Comparing the results on the progress Twit-
ter test in 2013 and 2014, we can see that NRC-
Canada, the 2013 winner for subtask A, have
now improved their F1 score from 88.93 to 90.14,
which is the 2014 best score. The best score on the
Progress SMS in 2014 of 89.31 belongs to ECNU;
this is a big jump compared to their 2013 score of
76.69, but it is less compared to the 2013 best of
88.37 achieved by GU-MLT-LT. For subtask B, on
the Twitter progress testset, the 2013 winner NRC-
Canada improves their 2013 result from 69.02 to
70.75, which is the second best in 2014; the win-
ner in 2014, TeamX, achieves 72.12. On the SMS
progress test, the 2013 winner NRC-Canada im-
proves its F1 score from 68.46 to 70.28. Overall,
we see consistent improvements on the progress
testset for both subtasks: 0-1 and 2-3 points abso-
lute for subtasks A and B, respectively.
76
Uncon- 2013: Progress 2014: Official 2014: Average
# System strain.? Tweet SMS Tweet Tweet Live- Macro Micro
sarcasm Journal
1 NRC-Canada 90.14
1
88.03
4
86.63
1
77.13
5
85.49
2
83.08
2
85.61
1
2 SentiKLUE 90.11
2
85.16
8
84.83
2
79.32
3
85.61
1
83.25
1
85.15
2
3 CMUQ-Hybrid
?
88.94
4
87.98
5
84.40
3
76.99
6
84.21
3
81.87
3
84.05
3
4 CMU-Qatar
?
89.85
3
88.08
3
83.45
4
78.07
4
83.89
5
81.80
4
83.56
4
5 ECNU X 87.29
6
89.26
2
82.93
5
73.71
8
81.69
7
79.44
7
81.85
6
6 ECNU 87.28
7
89.31
1
82.67
6
73.71
9
81.67
8
79.35
8
81.75
7
7 Think Positive X 88.06
5
87.65
6
82.05
7
76.74
7
80.90
12
79.90
6
81.15
9
8 Kea
?
84.83
10
84.14
10
81.22
8
65.94
17
81.16
11
76.11
13
80.70
10
9 Lt 3 86.28
8
85.26
7
81.02
9
70.76
13
80.44
13
77.41
11
80.33
13
10 senti.ue 84.05
11
78.72
16
80.54
10
82.75
1
81.90
6
81.73
5
81.47
8
11 LyS 85.69
9
81.44
12
79.92
11
71.67
10
83.95
4
78.51
10
82.21
5
12 UKPDIPF 80.45
15
79.05
14
79.67
12
65.63
18
81.42
9
75.57
14
80.33
11
13 UKPDIPF X 80.45
16
79.05
15
79.67
13
65.63
19
81.42
10
75.57
15
80.33
12
14 TJP 81.13
14
84.41
9
79.30
14
71.20
12
78.27
15
76.26
12
78.39
15
15 SAP-RI 80.32
17
80.26
13
77.26
15
70.64
14
77.68
18
75.19
17
77.32
16
16 senti.ue
?
X 83.80
12
82.93
11
77.07
16
80.02
2
79.70
14
78.93
9
78.83
14
17 SAIL 78.47
18
74.46
20
76.89
17
65.56
20
70.62
22
71.02
21
72.57
21
18 columbia nlp

81.50
13
74.55
19
76.54
18
61.76
22
78.19
16
72.16
19
77.11
18
19 IIT-Patna 76.54
20
75.99
18
76.43
19
71.43
11
77.99
17
75.28
16
77.26
17
20 Citius X 76.59
19
69.31
21
75.21
20
68.40
15
75.82
20
73.14
18
75.38
19
21 Citius 74.71
21
61.44
25
73.03
21
65.18
21
71.64
21
69.95
22
71.90
22
22 IITPatna 70.91
23
77.04
17
72.25
22
66.32
16
76.03
19
71.53
20
74.45
20
23 SU-sentilab 74.34
22
62.58
24
68.26
23
53.31
25
69.53
23
63.70
24
68.59
23
24 Univ. Warwick
?
62.25
26
60.12
26
67.28
24
58.08
24
64.89
25
63.42
25
65.48
25
25 Univ. Warwick
?
X 64.91
25
63.01
23
67.17
25
60.59
23
67.46
24
65.07
23
67.14
24
26 DAEDALUS 67.42
24
63.92
22
60.98
26
45.27
27
61.01
26
55.75
26
60.50
26
27 DAEDALUS X 61.95
27
55.97
27
58.11
27
49.19
26
58.65
27
55.32
27
58.17
27
Majority baseline 38.1 31.5 42.2 39.8 33.4
Table 4: Results for subtask A. The
?
indicates system resubmissions (because they initially trained on
Twitter2013-test), and the

indicates a system that includes a task co-organizer as a team member. The
systems are sorted by their score on the Twitter2014 test dataset; the rankings on the individual datasets
are indicated with a subscript. The last two columns show macro- and micro-averaged results across the
three 2014 test datasets.
Finally, note that for both subtasks, the best sys-
tems on the Twitter-2014 dataset are those that per-
formed best on the 2013 progress Twitter dataset:
NRC-Canada for subtask A, and TeamX (Fuji Xe-
rox Co., Ltd.) for subtask B.
It is interesting to note that the best results
for Twitter2014-test are lower than those for
Twitter2013-test for both subtask A (86.63 vs.
90.14) and subtask B (70.96 vs 72.12). This is
so despite the baselines for Twitter2014-test be-
ing higher than those for Twitter2013-test: 42.2 vs.
38.1 for subtask A, and 34.6 vs. 29.2 for subtask
B. Most likely, having access to Twitter2013-test
at development time, teams have overfitted on it. It
could be also the case that some of the sentiment
dictionaries that were built in 2013 have become
somewhat outdated by 2014.
Finally, note that while some teams such as
NRC-Canada performed well across all test sets,
other such as TeamX, which used a weighting
scheme tuned specifically for class imbalances in
tweets, were only strong on Twitter datasets.
7 Conclusion
We have described the data, the experimental
setup and the results for SemEval-2014 Task 9.
As in 2013, our task was the most popular one at
SemEval-2014, attracting 46 participating teams:
21 in subtask A (27 submissions) and 44 in sub-
task B (50 submissions).
We introduced three new test sets for 2014: an
in-domain Twitter dataset, an out-of-domain Live-
Journal test set, and a dataset of tweets contain-
ing sarcastic content. While the performance on
the LiveJournal test set was mostly comparable
to the in-domain Twitter test set, for most teams
there was a sharp drop in performance for sarcas-
tic tweets, highlighting better handling of sarcas-
tic language as one important direction for future
work in Twitter sentiment analysis.
We plan to run the task again in 2015 with the
inclusion of a new sub-evaluation on detecting sar-
casm with the goal of stimulating research in this
area; we further plan to add one more test domain.
77
Uncon- 2013: Progress 2014: Official 2014: Average
# System strain.? Tweet SMS Tweet Tweet Live- Macro Micro
sarcasm Journal
1 TeamX 72.12
1
57.36
26
70.96
1
56.50
3
69.44
15
65.63
3
69.99
5
2 coooolll 70.40
3
67.68
2
70.14
2
46.66
24
72.90
5
63.23
12
70.51
2
3 RTRGO 69.10
5
67.51
3
69.95
3
47.09
23
72.20
6
63.08
13
70.15
3
4 NRC-Canada 70.75
2
70.28
1
69.85
4
58.16
1
74.84
1
67.62
1
71.37
1
5 TUGAS 65.64
13
62.77
11
69.00
5
52.87
12
69.79
13
63.89
6
68.84
8
6 CISUC KIS
?
67.56
8
65.90
6
67.95
6
55.49
5
74.46
2
65.97
2
70.02
4
7 SAIL 66.80
11
56.98
28
67.77
7
57.26
2
69.34
17
64.79
4
68.06
10
8 SWISS-CHOCOLATE 64.81
18
66.43
5
67.54
8
49.46
16
73.25
4
63.42
10
69.15
6
9 Synalp-Empathic 63.65
23
62.54
12
67.43
9
51.06
15
71.75
9
63.41
11
68.57
9
10 Think Positive X 68.15
7
63.20
9
67.04
10
47.85
21
66.96
24
60.62
18
66.47
15
11 SentiKLUE 69.06
6
67.40
4
67.02
11
43.36
30
73.99
3
61.46
14
68.94
7
12 JOINT FORCES X 66.61
12
62.20
13
66.79
12
45.40
26
70.02
12
60.74
17
67.39
12
13 AMI ERIC 70.09
4
60.29
20
66.55
13
48.19
20
65.32
26
60.02
21
65.58
20
14 AUEB 63.92
21
64.32
8
66.38
14
56.16
4
70.75
11
64.43
5
67.71
11
15 CMU-Qatar
?
65.11
17
62.95
10
65.53
15
40.52
38
65.63
25
57.23
27
64.87
24
16 Lt 3 65.56
14
64.78
7
65.47
16
47.76
22
68.56
20
60.60
19
66.12
17
17 columbia nlp

64.60
19
59.84
21
65.42
17
40.02
40
68.79
19
58.08
25
65.96
19
18 LyS 66.92
10
60.45
19
64.92
18
42.40
33
69.79
14
59.04
22
66.10
18
19 NILC USP 65.39
15
61.35
16
63.94
19
42.06
34
69.02
18
58.34
24
65.21
21
20 senti.ue 67.34
9
59.34
23
63.81
20
55.31
6
71.39
10
63.50
7
66.38
16
21 UKPDIPF 60.65
29
60.56
17
63.77
21
54.59
7
71.92
7
63.43
8
66.53
13
22 UKPDIPF X 60.65
30
60.56
18
63.77
22
54.59
8
71.92
8
63.43
9
66.53
14
23 SU-FMI
?
60.96
28
61.67
15
63.62
23
48.34
19
68.24
21
60.07
20
64.91
23
24 ECNU 62.31
27
59.75
22
63.17
24
51.43
14
69.44
16
61.35
15
65.17
22
25 ECNU X 63.72
22
56.73
29
63.04
25
49.33
17
64.08
31
58.82
23
63.04
27
26 Rapanakis 58.52
32
54.02
35
63.01
26
44.69
27
59.71
37
55.80
31
61.28
32
27 Citius X 63.25
24
58.28
24
62.94
27
46.13
25
64.54
29
57.87
26
63.06
26
28 CMUQ-Hybrid
?
63.22
25
61.75
14
62.71
28
40.95
37
65.14
27
56.27
30
63.00
28
29 Citius 62.53
26
57.69
25
61.92
29
41.00
36
62.40
33
55.11
33
61.51
31
30 KUNLPLab 58.12
33
55.89
31
61.72
30
44.60
28
63.77
32
56.70
29
62.00
29
31 senti.ue
?
X 65.21
16
56.16
30
61.47
31
54.09
9
68.08
22
61.21
16
63.71
25
32 UPV-ELiRF 63.97
20
55.36
33
59.33
32
37.46
42
64.11
30
53.63
37
60.49
33
33 USP Biocom 58.05
34
53.57
36
59.21
33
43.56
29
67.80
23
56.86
28
61.96
30
34 DAEDALUS X 58.94
31
54.96
34
57.64
34
35.26
44
60.99
35
51.30
39
58.26
35
35 IIT-Patna 52.58
40
51.96
37
57.25
35
41.33
35
60.39
36
52.99
38
57.97
36
36 DejaVu 57.43
36
55.57
32
57.02
36
42.46
32
64.69
28
54.72
34
59.46
34
37 GPLSI 57.49
35
46.63
42
56.06
37
53.90
10
57.32
41
55.76
32
56.47
37
38 BUAP 56.85
37
44.27
44
55.76
38
51.52
13
53.94
44
53.74
36
54.97
39
39 SAP-RI 50.18
44
49.00
41
55.47
39
48.64
18
57.86
40
53.99
35
56.17
38
40 UMCC DLSI Sem 51.96
41
50.01
38
55.40
40
42.76
31
53.12
45
50.43
40
54.20
42
41 IBM EG 54.51
38
46.62
43
52.26
41
34.14
46
59.24
38
48.55
43
54.34
41
42 Alberta 53.85
39
49.05
40
52.06
42
40.40
39
52.38
46
48.28
44
51.85
44
43 lsis lif 46.38
46
38.56
47
52.02
43
34.64
45
61.09
34
49.25
41
54.90
40
44 SU-sentilab 50.17
45
49.60
39
49.52
44
31.49
47
55.11
42
45.37
47
51.09
45
45 SINAI 50.59
42
57.34
27
49.50
45
31.15
49
58.33
39
46.33
46
52.26
43
46 IITPatna 50.32
43
40.56
46
48.22
46
36.73
43
54.68
43
46.54
45
50.29
46
47 Univ. Warwick 39.17
48
29.50
49
45.56
47
39.77
41
39.60
49
41.64
48
43.19
48
48 UMCC DLSI Graph 43.24
47
36.66
48
45.49
48
53.15
11
47.81
47
48.82
42
46.56
47
49 Univ. Warwick X 34.23
50
24.63
50
45.11
49
31.40
48
29.34
50
35.28
49
38.88
49
50 DAEDALUS 36.57
49
40.86
45
33.03
50
28.96
50
40.83
48
34.27
50
35.81
50
Majority baseline 29.2 19.0 34.6 27.7 27.2
Table 5: Results for subtask B. The
?
indicates system resubmissions (because they initially trained on
Twitter2013-test), and the

indicates a system that includes a task co-organizer as a team member. The
systems are sorted by their score on the Twitter2014 test dataset; the rankings on the individual datasets
are indicated with a subscript. The last two columns show macro- and micro-averaged results across the
three 2014 test datasets.
In the 2015 edition of the task, we might also
remove the constrained/unconstrained distinction.
Finally, as there are multiple opinions about a
topic in Twitter, we would like to focus on detect-
ing the sentiment trend towards a topic.
Acknowledgements
We would like to thank Kathleen McKeown and
Smaranda Muresan for funding the 2014 Twitter
test sets. We also thank the anonymous reviewers.
78
Subtasks Team Affiliation 2013?
B Alberta University of Alberta
B AMI ERIC AMI Software R&D and Universit?e de Lyon (ERIC LYON 2) yes
B AUEB Athens University of Economics and Business yes
B BUAP Benem?erita Universidad Aut?onoma de Puebla
B CISUC KIS University of Coimbra
A, B Citius University of Santiago de Compostela
A, B CMU-Qatar Carnegie Mellon University, Qatar
A, B CMUQ-Hybrid Carnegie Mellon University, Qatar (different from the above)
A, B columbia nlp Columbia University yes
B cooolll Harbin Institute of Technology
A, B DAEDALUS Daedalus
B DejaVu Indian Institute of Technology, Kanpur
A, B ECNU East China Normal University yes
B GPLSI University of Alicante
B IBM EG IBM Egypt
A, B IITPatna Indian Institute of Technology, Patna
A, B IIT-Patna Indian Institute of Technology, Patna (different from the above)
B JOINT FORCES Zurich University of Applied Sciences
A Kea York University, Toronto yes
B KUNLPLab Koc? University
B lsis lif Aix-Marseille University yes
A, B Lt 3 Ghent University
A, B LyS Universidade da Coru?na
B NILC USP University of S?ao Paulo yes
A, B NRC-Canada National Research Council Canada yes
B Rapanakis Stamatis Rapanakis
B RTRGO Retresco GmbH and University of Gothenburg yes
A, B SAIL Signal Analysis and Interpretation Laboratory yes
A, B SAP-RI SAP Research and Innovation
A, B senti.ue Universidade de
?
Evora yes
A, B SentiKLUE Friedrich-Alexander-Universit?at Erlangen-N?urnberg yes
B SINAI University of Ja?en yes
B SU-FMI Sofia University
A, B SU-sentilab Sabanci University yes
B SWISS-CHOCOLATE ETH Zurich
B Synalp-Empathic University of Lorraine
B TeamX Fuji Xerox Co., Ltd.
A, B Think Positive IBM Research, Brazil
A TJP University of Northumbria at Newcastle Upon Tyne yes
B TUGAS Instituto de Engenharia de Sistemas e Computadores, yes
Investigac??ao e Desenvolvimento em Lisboa
A, B UKPDIPF Ubiquitous Knowledge Processing Lab
B UMCC DLSI Graph Universidad de Matanzas and Univarsidad de Alicante yes
B UMCC DLSI Sem Universidad de Matanzas and Univarsidad de Alicante (different from above) yes
A, B Univ. Warwick University of Warwick
B UPV-ELiRF Universitat Polit`ecnica de Val`encia
B USP Biocom University of S?ao Paulo and Federal University of S?ao Carlos
Table 6: Participating teams, their affiliations, subtasks they have taken part in, and an indication about
whether the team participated in SemEval-2013 Task 2.
79
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. SentiWordNet 3.0: An enhanced
lexical resource for sentiment analysis and opinion
mining. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation,
LREC ?10, Valletta, Malta.
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on Twitter from biased and noisy
data. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 36?44, Beijing, China.
Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer,
and Ricard Gavald`a. 2011. Detecting sentiment
change in Twitter streaming data. Journal of Ma-
chine Learning Research, Proceedings Track, 17:5?
11.
Tao Chen and Min-Yen Kan. 2013. Creating a
live, public short message service corpus: the NUS
SMS corpus. Language Resources and Evaluation,
47(2):299?335.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcasm in Twitter
and Amazon. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing, CoNLL ?10, pages 107?116, Uppsala, Sweden.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: Annotation, features, and experiments.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, ACL-HLT ?11, pages 42?
47, Portland, Oregon, USA.
Roberto Gonz?alez-Ib?a?nez, Smaranda Muresan, and
Nina Wacholder. 2011. Identifying sarcasm in Twit-
ter: a closer look. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Short Pa-
pers, ACL-HLT ?11, pages 581?586, Portland, Ore-
gon, USA.
Bernard Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Twitter power: Tweets as elec-
tronic word of mouth. J. Am. Soc. Inf. Sci. Technol.,
60(11):2169?2188.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The
good the bad and the OMG! In Proceedings of
the Fifth International Conference on Weblogs and
Social Media, ICWSM ?11, Barcelona, Catalonia,
Spain.
Christine Liebrecht, Florian Kunneman, and Antal
Van den Bosch. 2013. The perfect solution for de-
tecting sarcasm in tweets #not. In Proceedings of
the 4th Workshop on Computational Approaches to
Subjectivity, Sentiment and Social Media Analysis,
pages 29?37, Atlanta, Georgia, USA.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-
the-art in sentiment analysis of tweets. In Proceed-
ings of the Seventh international workshop on Se-
mantic Evaluation Exercises, SemEval-2013, pages
321?327, Atlanta, Georgia, USA.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 task 2: Sentiment analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation, SemEval ?13, pages 312?320,
Atlanta, Georgia, USA.
Brendan O?Connor, Ramnath Balasubramanyan, Bryan
Routledge, and Noah Smith. 2010. From tweets
to polls: Linking text sentiment to public opinion
time series. In Proceedings of the Fourth Inter-
national Conference on Weblogs and Social Media,
ICWSM ?10, Washington, DC, USA.
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter based system: Using Twitter for disambiguating
sentiment ambiguous adjectives. In Proceedings of
the 5th International Workshop on Semantic Evalu-
ation, SemEval ?10, pages 436?439, Uppsala, Swe-
den.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing - Volume 10, EMNLP ?02, pages
79?86.
Maria Pontiki, Harris Papageorgiou, Dimitrios Gala-
nis, Ion Androutsopoulos, John Pavlopoulos, and
Suresh Manandhar. 2014. SemEval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation, SemEval ?14, Dublin, Ireland.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?11, pages 1524?1534, Edinburgh,
Scotland, UK.
Andranik Tumasjan, Timm Sprenger, Philipp Sandner,
and Isabell Welpe. 2010. Predicting elections with
Twitter: What 140 characters reveal about politi-
cal sentiment. In Proceedings of the Fourth Inter-
national Conference on Weblogs and Social Media,
ICWSM ?10, Washington, DC, USA.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165?210.
80
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 590?595,
Dublin, Ireland, August 23-24, 2014.
SU-FMI: System Description for SemEval-2014 Task 9
on Sentiment Analysis in Twitter
Boris Velichkov
?
, Borislav Kapukaranov
?
, Ivan Grozev
?
, Jeni Karanesheva
?
, Todor Mihaylov
?
,
Yasen Kiprov
?
, Georgi Georgiev
??
, Ivan Koychev
??
, Preslav Nakov
??
Abstract
We describe the submission of the team
of the Sofia University to SemEval-2014
Task 9 on Sentiment Analysis in Twit-
ter. We participated in subtask B, where
the participating systems had to predict
whether a Twitter message expresses pos-
itive, negative, or neutral sentiment. We
trained an SVM classifier with a linear
kernel using a variety of features. We
used publicly available resources only, and
thus our results should be easily replicable.
Overall, our system is ranked 20th out of
50 submissions (by 44 teams) based on the
average of the three 2014 evaluation data
scores, with an F1-score of 63.62 on gen-
eral tweets, 48.37 on sarcastic tweets, and
68.24 on LiveJournal messages.
1 Introduction
We describe the submission of the team of the
Sofia University, Faculty of Mathematics and In-
formatics (SU-FMI) to SemEval-2014 Task 9 on
Sentiment Analysis in Twitter (Rosenthal et al.,
2014).
?
Sofia University, bobby.velichkov@gmail.com
?
Sofia University, b.kapukaranov@gmail.com
?
Sofia University, iigrozev@gmail.com
?
Sofia University, j.karanesheva@gmail.com
?
Sofia University, tbmihailov@gmail.com
?
Sofia University, yasen.kiprov@gmail.com
??
Ontotext, g.d.georgiev@gmail.com
??
Sofia University, koychev@fmi.uni-sofia.bg
??
Qatar Computing Research Institute,
pnakov@qf.org.qa
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
This SemEval challenge had two subtasks:
? subtask A (term-level) asks to predict the sen-
timent of a phrase inside a tweet;
? subtask B (message-level) asks to predict the
overall sentiment of a tweet message.
In both subtasks, the sentiment can be positive,
negative, or neutral. Here are some examples:
? positive: Gas by my house hit $3.39!!!! I?m
going to Chapel Hill on Sat. :)
? neutral: New York Giants: Game-by-Game
Predictions for the 2nd Half of the Season
http://t.co/yK9VTjcs
? negative: Why the hell does Selma have
school tomorrow but Parlier clovis & others
don?t?
? negative (sarcastic): @MetroNorth wall to
wall people on the platform at South Nor-
walk waiting for the 8:08. Thanks for the Sat.
Sched. Great sense
Below we first describe our preprocessing, fea-
tures and classifier in Section 2. Then, we discuss
our experiments, results and analysis in Section 3.
Finally, we conclude with possible directions for
future work in Section 4.
2 Method
Our approach is inspired by the highest scoring
team in 2013, NRC Canada (Mohammad et al.,
2013). We reused many of their resources.
1
Our system consists of two main submodules,
(i) feature extraction in the framework of GATE
(Cunningham et al., 2011), and (ii) machine learn-
ing using SVM with linear kernels as implemented
in LIBLINEAR
2
(Fan et al., 2008).
1
http://www.umiacs.umd.edu/
?
saif/WebPages/Abstracts/
NRC-SentimentAnalysis.htm
2
http://www.csie.ntu.edu.tw/
?
cjlin/
liblinear/
590
2.1 Preprocessing
We integrated a pipeline of various resources for
tweet analysis that are already available in GATE
(Bontcheva et al., 2013) such as a Twitter tok-
enizer, a sentence splitter, a hashtag tokenizer, a
Twitter POS tagger, a morphological analyzer, and
the Snowball
3
stemmer.
We further implemented in GATE some shal-
low text processing components in order to handle
negation contexts, emoticons, elongated words,
all-caps words and punctuation. We also added
components to find words and phrases contained
in sentiment lexicons, as well as to annotate words
with word cluster IDs using the lexicon built at
CMU,
4
which uses the Brown clusters (Brown et
al., 1992) as implemented
5
by (Liang, 2005).
2.2 Features
2.2.1 Sentiment lexicon features
We used several preexisting lexicons, both manu-
ally designed and automatically generated:
? Minqing Hu and Bing Liu opinion lexicon
(Hu and Liu, 2004): 4,783 positive and 2,006
negative terms;
? MPQA Subjectivity Cues Lexicon (Wilson et
al., 2005): 8,222 terms;
? Macquarie Semantic Orientation Lexicon
(MSOL) (Mohammad et al., 2009): 30,458
positive and 45,942 negative terms;
? NRC Emotion Lexicon (Mohammad et al.,
2013): 14,181 terms with specified emotion.
For each lexicon, we find in the tweet the terms
that are listed in it, and then we calculate the fol-
lowing features:
? Negative terms count;
? Positive terms count;
? Positive negated terms count;
? Positive/negative terms count ratio;
? Sentiment of the last token;
? Overall sentiment terms count.
3
http://snowball.tartarus.org/
4
http://www.ark.cs.cmu.edu/TweetNLP/
cluster_viewer.html
5
http://github.com/percyliang/
brown-cluster
We further used the following lexicons:
? NRC Hashtag Sentiment Lexicon: list of
words and their associations with positive
and negative sentiment (Mohammad et al.,
2013): 54,129 unigrams, 316,531 bigrams,
480,010 pairs, and 78 high-quality positive
and negative hashtag terms;
? Sentiment140 Lexicon: list of words with as-
sociations to positive and negative sentiments
(Mohammad et al., 2013): 62,468 unigrams,
677,698 bigrams, 480,010 pairs;
? Stanford Sentiment Treebank: contains
239,231 evaluated words and phrases. If a
word or a phrase was found in the tweet, we
took the given sentiment label.
For the NRC Hashtag Sentiment Lexicon and
the Sentiment140 Lexicon, we calculated the fol-
lowing features for unigrams, bigrams and pairs:
? Sum of positive terms? sentiment;
? Sum of negative terms? sentiment;
? Sum of the sentiment for all terms in the
tweet;
? Sum of negated positive terms? sentiment;
? Negative/positive terms ratio;
? Max positive sentiment;
? Min negative sentiment;
? Max sentiment of a term.
We used different features for the two lexicon
groups because their contents differ. The first four
lexicons provide a discrete sentiment value for
each word. In contrast, the following two lexicons
offer numeric sentiment scores, which allows for
different feature types such as sums and min/max
scores.
Finally, we manually built a new lexicon with
all emoticons we could find, where we assigned to
each emoticon a positive or a negative label. We
then calculated four features: number of positive
and negative emoticons in the tweet, and whether
the last token is a positive or a negative emoticon.
591
2.2.2 Tweet-level features
We use the following tweet-level features:
? All caps: the number of words with all char-
acters in upper case;
? Hashtags: the number ot hashtags in the
tweet;
? Elongated words: the number of words with
character repetitions.
2.2.3 Term-level features
We used the following term-level features:
? Word n-grams: presence or absence of 1-
grams, 2-grams, 3-grams, 4-grams, and 5-
grams. We add an NGRAM prefix to each n-
gram. Unfortunately, the n-grams increase
the feature space greatly and contribute to
higher sparseness. They also slow down
training dramatically. That is why our final
submission only includes 1-grams.
? Character n-grams: presence or absence of
one, two, three, four and five-character pre-
fixes and suffixes of all words. We add a PRE
or SUF prefix to each character n-gram.
? Negations: the number of negated contexts.
We define a negated context as a segment
of a tweet that starts with a negation word
(e.g., no, shouldnt) from our custom gazetteer
and ends with one of the punctuation marks:
,, ., :, ;, !, ?. A negated context affects the
n-gram and the lexicon features: we add a
NEG suffix to each word following the nega-
tion word, e.g., perfect becomes perfect NEG.
? Punctuation: the number of contiguous se-
quences of exclamation marks, of question
marks, of either exclamation or question
marks, and of both exclamation and question
marks. Also, whether the last token contains
an exclamation or a question mark (excluding
URLs).
? Stemmer: the stem of each word, excluding
URLs. We add a STEM prefix to each stem.
? Lemmatizer: the lemma of each word, ex-
cluding URLs. We add a LEMMA prefix to
each lemma. We use the built-in GATE Mor-
phological analyser as our lemmatizer.
? Word and word bigram clusters: word
clusters have been shown to improve the per-
formance of supervised NLP models (Turian
et al., 2010). We use the word clusters built
by CMU?s NLP toolkit, which were produced
over a collection of 56 million English tweets
(Owoputi et al., 2012) and built using the
Percy Liang?s HMM-based implementation
6
of Brown clustering (Liang, 2005; Brown et
al., 1992), which group the words into 1,000
hierarchical clusters. We use two features
based on these clusters:
? presence/absence of a word in a word
cluster;
? presence/absence of a bigram in a bi-
gram cluster.
? POS tagging: Social media are generally
hard to process using standard NLP tools,
which are typically developed with newswire
text in mind. Such standard tools are not
a good fit for Twitter messages, which are
too brief, contain typos and special word-
forms. Thus, we used a specialized POS
tagger, TwitIE, which is available in GATE
(Bontcheva et al., 2013), and which we in-
tegrated in our pipeline. It provides (i) a
tokenizer specifically trained to handle smi-
lies, user names, URLs, etc., (ii) a normal-
izer to correct slang and misspellings, and
(iii) a POS tagger that uses the Penn Treebank
tagset, but is optimized for tweets. Using the
TwitIE toolkit, we performed POS tagging
and we extracted all POS tag types that we
can find in the tweet together with their fre-
quencies as features.
2.3 Classifier
For classification, we used the above features and
a support vector machine (SVM) classifier as im-
plemented in LIBLINEAR. This is a very scal-
able implementation of SVM that does not support
kernels, and is suitable for classification on large
datasets with a large number of features. This is
particularly useful for text classification, where the
number of features is very large, which means that
the data is likely to be linearly separable, and thus
using kernels is not really necessary. We scaled the
SVM input and we used L2-regularization during
training.
6
https://github.com/percyliang/
brown-cluster
592
3 Experiments, Results, Analysis
3.1 Experimental setup
At development time, we trained on train-2013,
tuned the C value of SVM on dev-2013, and eval-
uated on test-2013 (Nakov et al., 2013). For our
submission, we trained on train-2013+dev-2013,
and we evaluated on the 2014 test dataset pro-
vided by the organizers. This dataset contains two
parts and a total of five datasets: (a) progress test
(the Twitter and SMS test datasets for 2013), and
(b) new test datasets (from Twitter, from Twitter
with sarcasm, and from LiveJournal). We used
C=0.012, which was best on development.
3.2 Official results
Due to our very late entering in the competition,
we have only managed to perform a small num-
ber of experiments, and we only participated in
subtask B. We were ranked 20th out of 50 sub-
missions; our official results are shown in Table 1.
The numbers after our score are the delta to the
best solution. We have also included a ranking
among 2014 participant systems on the 2013 data
sets, released by the organizers.
Data Category F1-score (best) Ranking
tweets2014 63.62 (6.23) 23
sarcasm2014 48.34 (9.82) 19
LiveJournal2014 68.23 (6.60) 21
tweets2013 60.96 (9.79) 29
SMS2013 61.67 (8.61) 16
2014 mean 60.07 (7.55) 20
Table 1: Our submitted system for subtask B.
3.3 Analysis
Tables 2 and 3 analyze the impact of the individual
features. They show the F1-scores and the loss
when a feature or a group of features is removed;
we show the impact on all test datasets, both from
2013 and from 2014. The exception here is the all
+ ngrams row, which contains our scores if we had
used the n-grams feature group.
The features are sorted by their impact on
the Twitter2014 test set. We can see that the
three most important feature groups are POS tags,
word/bigram clusters, and lexicons.
We can further see that although the overall lex-
icon feature group is beneficial, some of the lex-
icons actually hurt the 2014 score and we would
have been better off without them.
These are the Sentiment140 lexicon, the Stan-
ford Sentiment Treebank and the NRC Emotion
lexicon. The highest gain we get is from the lex-
icons of Minqing Hu and Bing Liu. It must be
noted that using lexicons with good results ap-
parently depends on the context, e.g., the Senti-
ment140 lexicon seems to be helping a lot with
the LiveJournal test dataset, but it hurts the Sar-
casm score by a sizeable margin.
Another interesting observation is that even
though including the n-gram feature group is per-
forming notably better on the Twitter2013 test
dataset, it actually worsens performance on all
2014 test sets. Had we included it in our results,
we would have scored lower.
The negation context feature brings little in re-
gards to regular tweets or LiveJournal text, but it
heavily improves our score on the Sarcasm tweets.
It is unclear why our results differ so much from
those of the NRC-Canada team in 2013 since our
features are quite similar. We attribute the differ-
ence to the fact that some of the lexicons we use
actually hurt our score as we mentioned above.
Another difference could be that last year?s NRC
system uses n-grams, which we have disabled as
they lowered our scores. Last but not least, there
could be bugs lurking in our feature representation
that additionally lower our results.
3.4 Post-submission improvements
First, we did more extensive experiments to val-
idate our classifier?s C value. We found that the
best value for C is actually 0.08 instead of our
original proposal 0.012.
Then, we experimented further with our lexi-
con features and we removed the following ones,
which resulted in significant improvement over
our submitted version:
? Sentiment of the last token for NRC Emotion,
MSOL, MPQA, and Bing Liu lexicons;
? Max term positive, negative and sentiment
scores for unigrams of Sentiment140 and
NRC Sentiment lexicons;
? Max term positive, negative and sentiment
scores for bigrams of Sentiment140 and NRC
Sentiment lexicons;
? Max term positive, negative and sentiment
scores for hashtags of Sentiment140 and
NRC Sentiment lexicons.
593
Feature Diff SMS2013 SMS2013 delta Twitter2013 Twitter2013 delta
submitted features 61.67 60.96
no POS tags 54.73 -6.94 52.32 -8.64
no word clusters 58.06 -3.61 55.44 -5.52
all lex removed 59.94 -1.73 58.35 -2.61
no Hu-Liu lex 60.56 -1.11 60.10 -0.86
all + ngrams 61.37 -0.30 62.22 1.26
no NRC #lex 61.35 -0.32 60.66 -0.30
no MSOL lex 61.88 0.21 61.35 0.39
no Stanford lex 61.84 0.17 61.02 0.06
no negation cntx 61.94 0.27 60.88 -0.08
no encodings 61.74 0.07 60.92 -0.04
no NRC emo lex 61.67 0.00 60.96 0.00
no Sent140 lex 61.61 -0.06 60.32 -0.64
Table 2: Ablation experiments on the 2013 test sets.
Feature Diff LiveJournal LJ delta Twitter Twitter delta Sarcasm Sarcasm delta
submitted features 68.23 63.62 48.34
no POS tags 62.28 -5.95 59.00 -4.62 43.70 -4.64
no word clusters 65.08 -3.15 59.82 -3.80 43.96 -4.38
all lex removed 66.16 -2.07 60.73 -2.89 49.59 1.25
no Hu-Liu lex 66.44 -1.79 62.15 -1.47 46.72 -1.62
all + ngrams 67.79 -0.44 62.96 -0.66 47.82 -0.52
no NRC #lex 66.81 -1.42 63.25 -0.37 47.54 -0.80
no MSOL lex 68.50 0.27 63.54 -0.08 48.34 0.00
no Stanford lex 67.86 -0.37 63.70 0.08 48.34 0.00
no negation cntx 68.09 -0.14 63.62 0.00 46.37 -1.97
no encodings 68.23 0.00 63.64 0.02 47.54 -0.80
no NRC emo lex 68.24 0.01 63.62 0.00 48.34 0.00
no Sent140 lex 67.32 -0.91 63.94 0.32 49.47 1.13
Table 3: Ablation experiments on the 2014 test sets.
The improved scores are shown in Table 4, with
the submitted and the best system results.
Test Set New F1 Old F1 Best
tweets2014 66.23 63.62 69.85
sarcasm2014 50.00 48.34 58.16
LiveJournal2014 69.41 68.24 74.84
tweets2013 63.08 60.96 70.75
SMS2013 62.28 61.67 70.28
2014 mean 62.20 60.07 67.62
Table 4: Our post-submission results.
4 Conclusion and Future Work
We have described the system built by the team of
SU-FMI for SemEval-2014 task 9. Due to our late
entering in the competition, we were only ranked
20th out of 50 submissions (from 44 teams).
We have made some interesting observations
about the impact of the different features. Among
the best-performing feature groups were POS-tag
counts, word cluster presence and bigrams, the
Hu-Liu lexicon and the NRC Hashtag Sentiment
lexicon. These had the most sustainable perfor-
mance over the 2013 and the 2014 test datasets.
Others we did not use, seemingly more context
dependent, seem to have been more suited for the
2013 test sets like the n-grams feature group.
Even though we made some improvements af-
ter submitting our initial version, we feel there is
more to gain and optimize. There seem to be sev-
eral low-hanging fruits based on our experiments
data, which could add few points to our F1-scores.
Going forward, our goal is to extend our experi-
ments with more feature sub- and super-sets and to
turn our classifier into a state-of-the-art performer.
594
Acknowledgments
This work is partially supported by the FP7-
ICT Strategic Targeted Research Project PHEME
(No. 611233), and by the European Social Fund
through the Human Resource Development Oper-
ational Programme under contract BG051PO001-
3.3.06-0052 (2012/2014).
References
Kalina Bontcheva, Leon Derczynski, Adam Funk,
Mark Greenwood, Diana Maynard, and Niraj
Aswani. 2013. TwitIE: An open-source infor-
mation extraction pipeline for microblog text. In
Proceedings of the International Conference on Re-
cent Advances in Natural Language Processing,
RANLP ?13, pages 83?90, Hissar, Bulgaria.
Peter Brown, Peter deSouza, Robert Mercer, Vin-
cent Della Pietra, and Jenifer Lai. 1992. Class-
based n-gram models of natural language. Compu-
tational Linguistics, 18:467?479.
Hamish Cunningham, Diana Maynard, and Kalina
Bontcheva. 2011. Text Processing with GATE.
Gateway Press CA.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. J. Mach.
Learn. Res., 9:1871?1874.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177, New York, NY, USA.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Master?s thesis, Massachusetts Insti-
tute of Technology.
Saif Mohammad, Cody Dunne, and Bonnie Dorr.
2009. Generating high-coverage semantic orienta-
tion lexicons from overtly marked words and a the-
saurus. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing: Vol-
ume 2, EMNLP ?09, pages 599?608, Singapore.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the Seventh International Workshop on Semantic
Evaluation Exercises, SemEval ?13, Atlanta, Geor-
gia, USA.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 task 2: Sentiment analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation, SemEval ?13, pages 312?320,
Atlanta, Georgia, USA.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, and Nathan Schneider. 2012. Part-
of-speech tagging for Twitter: Word clusters and
other advances. Technical Report CMU-ML-12-
107, Carnegie Mellon University.
Sara Rosenthal, Alan Ritter, Veselin Stoyanov, and
Preslav Nakov. 2014. SemEval-2014 task 9: Sen-
timent analysis in Twitter. In Proceedings of the
Eighth International Workshop on Semantic Evalu-
ation, SemEval ?14, Dublin, Ireland.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, ACL ?10, pages 384?394, Upp-
sala, Sweden.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the
Conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, HLT ?05, pages 347?354, Vancouver, British
Columbia, Canada.
595
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 298?303,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
QCRI at WMT12: Experiments in Spanish-English and German-English
Machine Translation of News Text
Francisco Guzma?n, Preslav Nakov, Ahmed Thabet, Stephan Vogel
Qatar Computing Research Institute
Qatar Foundation
Tornado Tower, floor 10, PO box 5825
Doha, Qatar
{fguzman,pnakov,ahawad,svogel}@qf.org.qa
Abstract
We describe the systems developed by the
team of the Qatar Computing Research Insti-
tute for the WMT12 Shared Translation Task.
We used a phrase-based statistical machine
translation model with several non-standard
settings, most notably tuning data selection
and phrase table combination. The evaluation
results show that we rank second in BLEU and
TER for Spanish-English, and in the top tier
for German-English.
1 Introduction
The team of the Qatar Computing Research Insti-
tute (QCRI) participated in the Shared Translation
Task of WMT12 for two language pairs:1 Spanish-
English and German-English. We used the state-of-
the-art phrase-based model (Koehn et al, 2003) for
statistical machine translation (SMT) with several
non-standard settings, e.g., data selection and phrase
table combination. The evaluation results show that
we rank second in BLEU (Papineni et al, 2002) and
TER (Snover et al, 2006) for Spanish-English, and
in the top tier for German-English.
In Section 2, we describe the parameters of our
baseline system and the non-standard settings we
experimented with. In Section 3, we discuss our
primary and secondary submissions for the two lan-
guage pairs. Finally, in Section 4, we provide a short
summary.
1The WMT12 organizers invited systems translating be-
tween English and four other European languages, in both di-
rections: French, Spanish, German, and Czech. However, we
only participated in Spanish?English and German?English.
2 System Description
Below, in Section 2.1, we first describe our initial
configuration; then, we discuss our incremental im-
provements. We explored several non-standard set-
tings and extensions and we evaluated their impact
with respect to different baselines. These baselines
are denoted in the tables below by a #number that
corresponds to systems in Figures 1 for Spanish-
English and in Figure 2 for German-English.
We report case insensitive BLEU calculated on
the news2011 testing data using the NIST scoring
tool v.11b.
2.1 Initial Configuration
Our baseline system can be summarized as follows:
? Training: News Commentary + Europarl train-
ing bi-texts;
? Tuning: news2010;
? Testing: news2011;
? Tokenization: splitting words containing a
dash, e.g., first-order becomes first @-@ order;
? Maximum sentence length: 100 tokens;
? Truecasing: convert sentence-initial words to
their most frequent case in the training dataset;
? Word alignments: directed IBM model 4
(Brown et al, 1993) alignments in both direc-
tions, then grow-diag-final-and heuristics;
? Maximum phrase length: 7 tokens;
? Phrase table scores: forward & reverse phrase
translation probabilities, forward & reverse lex-
ical translation probabilities, phrase penalty;
298
? Language model: 5-gram, trained on the target
side of the two training bi-texts;
? Reordering: lexicalized, msd-bidirectional-fe;
? Detokenization: reconnecting words that were
split around dashes;
? Model parameter optimization: minimum error
rate training (MERT), optimizing BLEU.
2.2 Phrase Tables
We experimented with two non-standard settings:
Smoothing. The four standard scores associated
with each phrase pair in the phrase table (forward
& reverse phrase translation probabilities, forward
& reverse lexical translation probabilities) are nor-
mally used unsmoothed. We also experimented with
Good-Turing and Kneser-Ney smoothing (Chen and
Goodman, 1999). As Table 1 shows, the latter works
a bit better for both Spanish-English and German-
English.
es-en de-en
Baseline (es:#3,de:#4) 29.98 22.03
Good Turing 29.98 22.07
Kneser-Ney 30.16 22.30
Table 1: Phrase table smoothing.
Phrase table combination. We built two phrase
tables, one for News Commentary + Europarl and an
additional one for the UN bi-text. We then merged
them,2 adding additional features to each entry in
the merged phrase table: F1, F2, and F3. The
value of F1/F2 is 1 if the phrase pair came from the
first/second phrase table, and 0.5 otherwise, while
F3 is 1 if the phrase pair was in both tables, and 0.5
otherwise. We optimized the weights for all features,
including the additional ones, using MERT.3 Table 2
shows that this improves by +0.42 BLEU points.
2In theory, we should also re-normalize the conditional
probabilities (forward/reverse phrase translation probability,
and forward/reverse lexicalized phrase translation probability)
since they may not sum to one anymore. In practice, this is
not that important since the log-linear phrase-based SMT model
does not require that the phrase table features be probabilities
(e.g., F1, F2, F3, and the phrase penalty are not probabilities);
moreover, we have extra features whose impact is bigger.
3This is similar but different from (Nakov, 2008): when a
phrase pair appeared in both tables, they only kept the entry
from the first table, while we keep the entries from both tables.
es-en
Baseline (es:#7) 30.94
Merging (1) News+EP with (2) UN 31.36
Table 2: Phrase table merging.
2.3 Language Models
We built the language models (LM) for our systems
using a probabilistic 5-gram model with Kneser-
Ney (KN) smoothing. We experimented with LMs
trained on different training datasets. We used the
SRILM toolkit (Stolcke, 2002) for training the lan-
guage models, and the KenLM toolkit (Heafield
and Lavie, 2010) for binarizing the resulting ARPA
models for faster loading with the Moses decoder
(Koehn et al, 2007).
2.3.1 Using WMT12 Corpora Only
We trained 5-gram LMs on datasets provided by
the task organizers. The results are presented in
Table 3. The first line reports the baseline BLEU
scores using a language model trained on the target
side of the News Commentary + Europarl training
bi-texts. The second line shows the results when us-
ing an interpolation (minimizing the perplexity on
the news2010 tuning dataset) of different language
models, trained on the following corpora:
? the monolingual News Commentary corpus
plus the English sides of all training News
Commentary v.7 bi-texts (for French-English,
Spanish-English, German-English, and Czech-
English), with duplicate sentences removed
(5.5M word tokens; one LM);
? the News Crawl 2007-2011 corpora, (1213M
word tokens; separate LM for each of these five
years);
? the Europarl v.7 monolingual corpus (60M
word tokens; one LM);
? the English side of the Spanish-English UN bi-
text (360M word tokens; one LM).
The last line in Table 3 shows the results when
using an additional 5-gram LM in the interpolation,
one trained on the English side of the 109 French-
English bi-text (662M word tokens).
299
We can see that using these interpolations yields
very sizable improvements of 1.7-2.5 BLEU points
over the baseline. However, while the impact of
adding the 109 bi-text to the interpolation is clearly
visible for Spanish-English (+0.47 BLEU), it is al-
most negligible for German-English (+0.06 BLEU).
Corpora es-en de-en
Baseline (es:#1, de:#2) 27.34 20.01
News + EP + UN (interp.) 29.36 21.66
News + EP + UN + 109 (interp.) 29.83 21.72
Table 3: LMs using the provided corpora only.
2.3.2 Using Gigaword
In addition to the WMT12 data, we used the LDC
Gigaword v.5 corpus. We divided the corpus into
reasonably-sized chunks of text of about 2GB per
chunk, and we built a separate intermediate language
model for each chunk. Then, we interpolated these
language models, minimizing the perplexity on the
news2010 development set as with the previous
LMs. We experimented with two different strate-
gies for creating the chunks by segmenting the cor-
pus according to (a) data source, e.g., AFP, Xinhua,
etc., and (b) year of release. We thus compared the
advantages of interpolating epoch-consistent LMs
vs. source-coherent LMs. We trained individual
LMs for each of the segments and we added them
to a pool. Finally, we selected the ten most relevant
ones from this pool based on their perplexity on the
news2010 devset, and we interpolated them.
The results are shown in Table 4. The first line
shows the baseline, which uses an interpolation of
the nine LMs from the previous subsection. The
following two lines show the results when using an
LM trained on Gigaword only. We can see that for
Spanish-English, interpolation by year performs bet-
ter, while for German-English, it is better to use the
by-source chunks. However, the following two lines
show that when we translate with two LMs, one built
from the WMT12 data only and one built using Gi-
gaword data only, interpolation by year is preferable
for Gigaword for both language pairs. For our sub-
mitted systems, we used the LMs shown in bold in
Table 4: we used a single LM for Spanish-English
and two LMs for German-English.
Language Models es-en de-en
Baseline (es:#5, de:#6) 30.31 22.48
GW by year 30.68 22.32
GW by source 30.52 22.56
News-etc + GW by year 30.60 22.71
News-etc + GW by source 30.55 22.54
Table 4: LMs using Gigaword.
2.4 Parameter Tuning and Data Selection
Parameter tuning is a very important step in SMT.
The standard procedure consists of performing a se-
ries of iterations of MERT to choose the model pa-
rameters that maximize the translation quality on a
development set, e.g., as measured by BLEU. While
the procedure is widely adopted, it is also recognized
that the selection of an appropriate development set
is important since it biases the parameters towards
specific types of translations. This is illustrated in
Table 5, which shows BLEU on the news2011 testset
when using different development sets for MERT.
Devset es-en
news2008 29.47
news2009 29.14
news2010 29.61
Table 5: Using different tuning sets for MERT.
To address this problem, we performed a selection
of development data using an n-gram-based similar-
ity ranking. The selection was performed over a pool
of candidate sentences drawn from the news2008,
news2009, and news2010 tuning datasets. The sim-
ilarity metric was defined as follows:
sim(f, g) = 2match(f, g) ? lenpen(f, g) (1)
where 2match represents the number of bi-gram
matches between sentences f and g, and lenpen is
a length penalty to discourage unbalanced matches.
We penalized the length difference using an
inverted-squared sigmoid function:
lenpen(f, g) = 3? 4 ? sig
([
|f | ? |g|
?
]2
)
(2)
300
where |.| denotes the length of a sentence in num-
ber of words, ? controls the maximal tolerance to
differences, and sig is the sigmoid function.
To generate a suitable development set, we av-
eraged the similarity scores of candidate sentences
w.r.t. to the target testset. For instance:
sf =
1
|G|
?
g?G
sim(f, g) (3)
where G is the set of the test sentences.
Finally, we selected a pool of candidates f from
news2008, news2009 and news2011 to generate a
2000-best tuning set. The results when using each of
the above penalty functions are presented on Table 6.
devset es-en
baseline (es:#6) 30.68
selection (? = 5) 30.94
selection (? = 10) 30.90
Table 6: Selecting sentences for MERT.
The average length of the source-side sentences
in our selected sentence pairs was smaller than in
our baseline, the news2011 development dataset.
This means that our selected source-side sentences
tended to be shorter than in the baseline. Moreover,
the standard deviation of the sentence lengths was
smaller for our samples as well, which means that
there were fewer long sentences; this is good since
long sentences can take very long to translate. As
a result, we observed sizable speedup in parameter
tuning when running MERT on our selected tuning
datasets.
2.5 Decoding and Hypothesis Reranking
We experimented with two decoding settings:
(1) monotone at punctuation reordering (Tillmann
and Ney, 2003), and (2) minimum Bayes risk decod-
ing (Kumar and Byrne, 2004). The results are shown
in Table 7. We can see that both yield improvements
in BLEU, even if small.
2.6 System Combination
As the final step in our translation system, we per-
formed hypothesis re-combination of the output of
several of our systems using the Multi-Engine MT
system (MEMT) (Heafield and Lavie, 2010).
es-en de-en
Baseline (es:#2,de:#3) 29.83 21.72
+MP 29.98 22.03
Baseline (es:#4,de:#5) 30.16 22.30
+MBR 30.31 22.48
Table 7: Decoding parameters. Experiments with
monotone at punctuation (MP) reordering, and minimum
Bayes risk (MBR) decoding.
The results for the actual news2012 testset are
shown in Table 8: the system combination results
are our primary submission. We can see that system
combination yielded 0.4 BLEU points of improve-
ment for Spanish-English and 0.2-0.3 BLEU points
for German-English.
3 Our Submissions
Here we briefly describe the cumulative improve-
ments when applying the above modifications to our
baseline system, leading to our official submissions
for the WMT12 Shared Translation Task.
3.1 Spanish-English
The development of our final Spanish-English sys-
tem involved several incremental improvements,
which have been described above and which are
summarized in Figure 1. We started with a base-
line system (see Section 2.1), which scored 27.34
BLEU points. From there, using a large inter-
polated language model trained on the provided
data (see Section 2.3.1) yielded +2.49 BLEU points
of improvement. Monotone-at-punctuation de-
coding contributed an additional improvement of
+0.15, smoothing the phrase table using Kneser-Ney
boosted the score by +0.18, and using minimum
Bayes risk decoding added another +0.15 BLEU
points. Changing the language model to one trained
on Gigaword v.5 and interpolated by year yielded
+0.37 additional points of improvement. Another
+0.26 points came from tuning data selection. Fi-
nally, using the UN data in a merged phrase ta-
ble (see Section 2.2) yielded another +0.42 BLEU
points. Overall, we achieve a total improvement
over our initial baseline of about 4 BLEU points.
301
27.34 
29.83 29.98 30.16 30.31 
30.68 
30.94 
31.36 
25 
26 
27 
28 
29 
30 
31 
32 
1:BA
SELI
NE 
2:+W
MT-L
M 
3:+M
P 
4:+K
N 
5:+M
BR 
6:*G
IGA 
V5-L
M 
7:+T
UNE
-SEL
 
8:+P
T-ME
RGE
 
!
BL
EU
 v12
  sc
ore
  (n
ew
s-2
011
) 
Figure 1: Incremental improvements for the Spanish-English system.
3.2 German-English
Figure 2 shows a similar sequence of improvements
for our German-English system. We started with a
baseline (see Section 2.1) that scored 19.79 BLEU
points. Next, we performed compound splitting for
the German side of the training, the development
and the testing bi-texts, which yielded +0.22 BLEU
points of improvement. Using a large interpolated
language model trained on the provided corpora (see
Section 2.3.1) added another +1.71. Monotone-at-
punctuation decoding contributed +0.31, smoothing
the phrase table using Kneser-Ney boosted the score
by +0.27, and using minimum Bayes risk decoding
added another +0.18 BLEU points. Finally, adding a
second language model trained on the Gigaword v.5
corpus interpolated by year yielded +0.23 additional
BLEU points. Overall, we achieved about 3 BLEU
points of total improvement over our initial baseline.
3.3 Final Submissions
For both language pairs, our primary submission
was a combination of the output of several of our
best systems shown in Figures 1 and 2, which use
different experimental settings; our secondary sub-
mission was our best individual system, i.e., the
right-most one in Figures 1 and 2.
The official BLEU scores, both cased and lower-
cased, for our primary and secondary submissions,
as evaluated on the news2012 dataset, are shown
in Table 8. For Spanish-English, we achieved the
second highest BLEU and TER scores, while for
German-English we were ranked in the top tier.
news2012
lower cased
Spanish-English
Primary 34.0 32.9
Secondary 33.6 32.5
German-English
Primary 23.9 22.6
Secondary 23.6 22.4
Table 8: The official BLEU scores for our submissions
to the WMT12 Shared Translation Task.
4 Conclusion
We have described the primary and the secondary
systems developed by the team of the Qatar Com-
puting Research Institute for Spanish-English and
German-English machine translation of news text
for the WMT12 Shared Translation Task.
We experimented with phrase-based SMT, explor-
ing a number of non-standard settings, most notably
tuning data selection and phrase table combination,
which we described and evaluated in a cumulative
fashion. The automatic evaluation metrics,4 have
ranked our system second for Spanish-English and
in the top tier for German-English.
We plan to continue our work on data selection
for phrase table and the language model training, in
addition to data selection for tuning.
4The evaluation scores for WMT12 are available online:
http://matrix.statmt.org/
302
19.79 20.01 
21.72 
22.03 22.30 
22.48 
22.71 
18 
18.5 
19 
19.5 
20 
20.5 
21 
21.5 
22 
22.5 
23 
1:BA
SELI
NE 
2:+S
PLIT
 
3:WM
T-LM
 
4:+M
P 
5:+K
N 
6:+M
BR 
7:+G
IGA+
WMT
 LM 
BL
EU
 v12
  sc
ore
   (n
ew
s-2
011
) 
Figure 2: Incremental improvements for the German-English system.
Acknowledgments
We would like to thank the anonymous reviewers
for their useful comments, which have helped us im-
prove the text of this paper.
References
Peter Brown, Vincent Della Pietra, Stephen Della Pietra,
and Robert Mercer. 1993. The mathematics of statis-
tical machine translation: parameter estimation. Com-
putational Linguistics, 19(2):263?311.
Stanley Chen and Joshua Goodman. 1999. An empirical
study of smoothing techniques for language modeling.
Computer Speech & Language, 13(4):359?393.
Kenneth Heafield and Alon Lavie. 2010. Combin-
ing machine translation output with open source:
The Carnegie Mellon multi-engine machine transla-
tion scheme. The Prague Bulletin of Mathematical
Linguistics, 93(1):27?36.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 48?54, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL. Demonstration session, ACL ?07, pages
177?180, Prague, Czech Republic.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Susan Dumais, Daniel Marcu, and Salim
Roukos, editors, Proceedings of the Annual Meeting
of the North American chapter of the Association for
Computational Linguistics, HLT-NAACL ?04, pages
169?176, Boston, MA.
Preslav Nakov. 2008. Improving English-Spanish sta-
tistical machine translation: Experiments in domain
adaptation, sentence paraphrasing, tokenization, and
recasing. In Proceedings of the Third Workshop
on Statistical Machine Translation, WMT ?07, pages
147?150, Prague, Czech Republic.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics, ACL ?02, pages 311?318, Philadelphia,
PA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the Annual Meetig of the Associa-
tion for Machine Translation in the Americas, AMTA
?06, pages 223?231.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of Intl. Conf.
on Spoken Language Processing, volume 2 of ICSLP
?02, pages 901?904, Denver, CO.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a dynamic programming beam search al-
gorithm for statistical machine translation. Computa-
tional Linguistics, 29(1):97?133.
303
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402?408,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
DiscoTK: Using Discourse Structure for Machine Translation Evaluation
Shafiq Joty Francisco Guzm
?
an Llu??s M
`
arquez and Preslav Nakov
ALT Research Group
Qatar Computing Research Institute ? Qatar Foundation
{sjoty,fguzman,lmarquez,pnakov}@qf.org.qa
Abstract
We present novel automatic metrics for
machine translation evaluation that use
discourse structure and convolution ker-
nels to compare the discourse tree of an
automatic translation with that of the hu-
man reference. We experiment with five
transformations and augmentations of a
base discourse tree representation based
on the rhetorical structure theory, and we
combine the kernel scores for each of them
into a single score. Finally, we add other
metrics from the ASIYA MT evaluation
toolkit, and we tune the weights of the
combination on actual human judgments.
Experiments on the WMT12 and WMT13
metrics shared task datasets show corre-
lation with human judgments that outper-
forms what the best systems that partici-
pated in these years achieved, both at the
segment and at the system level.
1 Introduction
The rapid development of statistical machine
translation (SMT) that we have seen in recent
years would not have been possible without au-
tomatic metrics for measuring SMT quality. In
particular, the development of BLEU (Papineni
et al., 2002) revolutionized the SMT field, al-
lowing not only to compare two systems in a
way that strongly correlates with human judg-
ments, but it also enabled the rise of discrimina-
tive log-linear models, which use optimizers such
as MERT (Och, 2003), and later MIRA (Watanabe
et al., 2007; Chiang et al., 2008) and PRO (Hop-
kins and May, 2011), to optimize BLEU, or an ap-
proximation thereof, directly. While over the years
other strong metrics such as TER (Snover et al.,
2006) and Meteor (Lavie and Denkowski, 2009)
have emerged, BLEU remains the de-facto stan-
dard, despite its simplicity.
Recently, there has been steady increase in
BLEU scores for well-resourced language pairs
such as Spanish-English and Arabic-English.
However, it was also observed that BLEU-like n-
gram matching metrics are unreliable for high-
quality translation output (Doddington, 2002;
Lavie and Agarwal, 2007). In fact, researchers al-
ready worry that BLEU will soon be unable to dis-
tinguish automatic from human translations.
1
This
is a problem for most present-day metrics, which
cannot tell apart raw machine translation output
from a fully fluent professionally post-edited ver-
sion thereof (Denkowski and Lavie, 2012).
Another concern is that BLEU-like n-gram
matching metrics tend to favor phrase-based SMT
systems over rule-based systems and other SMT
paradigms. In particular, they are unable to cap-
ture the syntactic and semantic structure of sen-
tences, and are thus insensitive to improvement
in these aspects. Furthermore, it has been shown
that lexical similarity is both insufficient and not
strictly necessary for two sentences to convey
the same meaning (Culy and Riehemann, 2003;
Coughlin, 2003; Callison-Burch et al., 2006).
The above issues have motivated a large amount
of work dedicated to design better evaluation met-
rics. The Metrics task at the Workshop on Ma-
chine Translation (WMT) has been instrumental in
this quest. Below we present QCRI?s submission
to the Metrics task of WMT14, which consists of
the DiscoTK family of discourse-based metrics.
In particular, we experiment with five different
transformations and augmentations of a discourse
tree representation, and we combine the kernel
scores for each of them into a single score which
we call DISCOTK
light
. Next, we add to the com-
bination other metrics from the ASIYA MT eval-
uation toolkit (Gim?enez and M`arquez, 2010), to
produce the DISCOTK
party
metric.
1
This would not mean that computers have achieved hu-
man proficiency; it would rather show BLEU?s inadequacy.
402
Finally, we tune the relative weights of the met-
rics in the combination using human judgments
in a learning-to-rank framework. This proved
to be quite beneficial: the tuned version of the
DISCOTK
party
metric was the best performing
metric in the WMT14 Metrics shared task.
The rest of the paper is organized as follows:
Section 2 introduces our basic discourse metrics
and the tree representations they are based on.
Section 3 describes our metric combinations. Sec-
tion 4 presents our experiments and results on
datasets from previous years. Finally, Section 5
concludes and suggests directions for future work.
2 Discourse-Based Metrics
In our recent work (Guzm?an et al., 2014), we used
the information embedded in the discourse-trees
(DTs) to compare the output of an MT system to
a human reference. More specifically, we used
a state-of-the-art sentence-level discourse parser
(Joty et al., 2012) to generate discourse trees for
the sentences in accordance with the Rhetorical
Structure Theory (RST) of discourse (Mann and
Thompson, 1988). Then, we computed the simi-
larity between DTs of the human references and
the system translations using a convolution tree
kernel (Collins and Duffy, 2001), which efficiently
computes the number of common subtrees. Note
that this kernel was originally designed for syntac-
tic parsing, and the subtrees are subject to the con-
straint that their nodes are taken with all or none
of their children, i.e., if we take a direct descen-
dant of a given node, we must also take all siblings
of that descendant. This imposes some limitations
on the type of substructures that can be compared,
and motivates the enriched tree representations ex-
plained in subsections 2.1?2.4.
The motivation to compare discourse trees, is
that translations should preserve the coherence re-
lations. For example, consider the three discourse
trees (DTs) shown in Figure 1. Notice that the
Attribution relation in the reference translation is
also realized in the system translation in (b) but not
in (c), which makes (b) a better translation com-
pared to (c), according to our hypothesis.
In (Guzm?an et al., 2014), we have shown that
discourse structure provides additional informa-
tion for MT evaluation, which is not captured by
existing metrics that use lexical, syntactic and se-
mantic information; thus, discourse should be con-
sidered when developing new rich metrics.
Here, we extend our previous work by devel-
oping metrics that are based on new representa-
tions of the DTs. In the remainder of this section,
we will focus on the individual DT representations
that we will experiment with; then, the following
section will describe the metric combinations and
tuning used to produce the DiscoTK metrics.
2.1 DR-LEX
1
Figure 2a shows our first representation of the DT.
The lexical items, i.e., words, constitute the leaves
of the tree. The words in an Elementary Discourse
Unit (EDU) are grouped under a predefined tag
EDU, to which the nuclearity status of the EDU
is attached: nucleus vs. satellite. Coherence re-
lations, such as Attribution, Elaboration, and En-
ablement, between adjacent text spans constitute
the internal nodes of the tree. Like the EDUs, the
nuclearity statuses of the larger discourse units are
attached to the relation labels. Notice that with
this representation the tree kernel can easily be ex-
tended to find subtree matches at the word level,
i.e., by including an additional layer of dummy
leaves as was done in (Moschitti et al., 2007). We
applied the same solution in our representations.
2.2 DR-NOLEX
Our second representation DR-NOLEX (Figure 2b)
is a simple variation of DR-LEX
1
, where we ex-
clude the lexical items. This allows us to measure
the similarity between two translations in terms of
their discourse structures alone.
2.3 DR-LEX
2
One limitation of DR-LEX
1
and DR-NOLEX is that
they do not separate the structure, i.e., the skele-
ton, of the tree from its labels. Therefore, when
measuring the similarity between two DTs, they
do not allow the tree kernel to give partial credit
to subtrees that differ in labels but match in their
structures. DR-LEX
2
, a variation of DR-LEX
1
, ad-
dresses this limitation as shown in Figure 2c. It
uses predefined tags SPAN and EDU to build the
skeleton of the tree, and considers the nuclearity
and/or relation labels as properties (added as chil-
dren) of these tags. For example, a SPAN has two
properties, namely its nuclearity and its relation,
and an EDU has one property, namely its nucle-
arity. The words of an EDU are placed under the
predefined tag NGRAM.
403
Elaboration ROOT
SPANNucleus AttributionSatellite
Voices are coming from Germany , SPANSatellite SPANNucleus
suggesting that ECB be the last resort creditor .
(a) A reference (human-written) translation.
AttributionROOT
SPANSatellite SPANNucleus
In Germany voices , the ECB should be the lender of last resort .
(b) A higher quality (system-generated) translation.
SPANROOT
In Germany the ECB should be for the creditors of last resort .
(c) A lower quality (system-generated) translation.
Figure 1: Three discourse trees for the translations of a source sentence: (a) the reference, (b) a higher
quality automatic translation, and (c) a lower quality automatic translation.
2.4 DR-LEX
1.1
and DR-LEX
2.1
Although both DR-LEX
1
and DR-LEX
2
allow the
tree kernel to find matches at the word level, the
words are compared in a bag-of-words fashion,
i.e., if the trees share a common word, the ker-
nel will find a match regardless of its position in
the tree. Therefore, a word that has occurred in
an EDU with status Nucleus in one tree could be
matched with the same word under a Satellite in
the other tree. In other words, the kernel based
on these representations is insensitive to the nu-
clearity status and the relation labels under which
the words are matched. DR-LEX
1.1
, an exten-
sion of DR-LEX
1
, and DR-LEX
2.1
, an extension
of DR-LEX
2
, are sensitive to these variations at
the lexical level. DR-LEX
1.1
(Figure 2d) and DR-
LEX
2.1
(Figure 2e) propagate the nuclearity sta-
tuses and/or the relation labels to the lexical items
by including three more subtrees at the EDU level.
3 Metric Combination and Tuning
In this section, we describe our Discourse Tree
Kernel (DiscoTK) metrics. We have two main
versions: DISCOTK
light
, which combines the five
DR-based metrics, and DISCOTK
party
, which fur-
ther adds the Asiya metrics.
3.1 DISCOTK
light
In the previous section, we have presented several
discourse tree representations that can be used to
compare the output of a machine translation sys-
tem to a human reference. Each representation
stresses a different aspect of the discourse tree.
In order to make our estimations more robust,
we propose DISCOTK
light
, a metric that takes ad-
vantage of all the previous discourse representa-
tions by linearly interpolating their scores. Here
are the processing steps needed to compute this
metric:
(i) Parsing: We parsed each sentence in order to
produce discourse trees for the human references
and for the outputs of the systems.
(ii) Tree enrichment/simplification: For each
sentence-level discourse tree, we generated the
five different tree representations: DR-NOLEX,
DR-LEX
1
, DR-LEX
1.1
, DR-LEX
2
, DR-LEX
2.1
.
(iii) Estimation: We calculated the per-sentence
similarity scores between tree representations of
the system hypothesis and the human reference
using the extended convolution tree kernel as de-
scribed in the previous section. To compute the
system-level similarity scores, we calculated the
average sentence-level similarity; note that this en-
sures that our metric is ?the same? at the system
and at the segment level.
(iv) Normalization: In order to make the scores of
the different representations comparable, we per-
formed a min?max normalization
2
for each met-
ric and for each language pair.
(v) Combination: Finally, for each sentence, we
computed DISCOTK
light
as the average of the
normalized similarity scores of the different repre-
sentations. For system-level experiments, we per-
formed linear interpolation of system-level scores.
2
Where x
?
= (x?min)/(max?min).
404
	


 

Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 207?216,
October 25, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Unsupervised Word Segmentation Improves
Dialectal Arabic to English Machine Translation
Kamla Al-Mannai
1
, Hassan Sajjad
1
, Alaa Khader
2
, Fahad Al Obaidli
1
,
Preslav Nakov
1
, Stephan Vogel
1
Qatar Computing Research Institute
1
, Carnegie Mellon University in Qatar
2
{kamlmannai,hsajjad,faalobaidli,pnakov,svogel}@qf.org.qa
1
, akhader@cmu.edu
2
Abstract
We demonstrate the feasibility of using
unsupervised morphological segmentation
for dialects of Arabic, which are poor in
linguistics resources. Our experiments us-
ing a Qatari Arabic to English machine
translation system show that unsupervised
segmentation helps to improve the transla-
tion quality as compared to using no seg-
mentation or to using ATB segmentation,
which was especially designed for Mod-
ern Standard Arabic (MSA). We use MSA
and other dialects to improve Qatari Ara-
bic to English machine translation, and we
show that a uniform segmentation scheme
across them yields an improvement of 1.5
BLEU points over using no segmentation.
1 Introduction
The Arabic language has many varieties, where
the Modern Standard Arabic (MSA) coexists with
various dialects. Dialects differ from MSA and
from each other lexically, phonologically, mor-
phologically and syntactically. MSA has stan-
dard orthography and is used in formal contexts
(e.g., publications, newspaper articles, etc.), while
the dialects are usually limited to daily verbal in-
teractions. However, with the recent rise of social
media, it has become increasingly common to use
dialects in written communication as well, which
has constituted the research in dialectal Arabic
(DA) as a separate field within the broader field
of natural language processing (NLP).
As DA NLP is still in its infancy, there is lack
of basic computational resources and tools, which
are needed in order to apply standard NLP ap-
proaches to the dialects of Arabic. For instance,
statistical approaches need a lot of training data,
which makes it very hard, if not impossible, to
apply them to resource-poor languages; this is
especially true for statistical machine translation
(SMT) of Arabic dialects.
The Arabic language and its dialects are highly
inflectional, and a word can appear in many more
inflected forms compared to English. Consider the
Arabic words

IJ
.
?? ,I
.
??K


,I
.
??

K, and
	
??J
.
??K


: they
all belong to one root word I
.
?? ?playing? /lEb/.
Each morphological variation is derived from a
root word with different affixes addressing differ-
ent functions. This causes data sparseness, and
covering all possible word forms of a root word
may not be always possible. Considering the dif-
ferent variants of Arabic, the problem is exacer-
abated as dialects could use different choices of af-
fixes for the same function. For example, the MSA
word
	
??J
.
??K


/yalEabuwn/, meaning ?they are play-
ing?, could be found as
	
??J
.
??K


/ylEbuwn/ in Gulf,
as @?J
.
??K


?? /Eam yilEabuA/ in Levantine, and as
@?J
.
??J


K
.
/biylEabwA/ in Egyptian Arabic.
One possible solution is to use a morphological
segmenter that segments words into simpler units
such as stems and affixes, which might be covered
in the training set (Zollmann et al., 2006; Tsai et
al., 2010). When applied to dialects, this may re-
duce the lexical gap between dialects and MSA by
matching the common stems. Unfortunately, there
are no standard morphological segmentation tools
for dialects. Due to the difference in morphology,
tools designed for MSA do not work well for di-
alects. Developing rule-based segmenters for each
dialect might appear to be the ideal solution, but,
as the orthography of dialects is not standardized,
crafting linguistic rules for them is very hard.
In this paper, we focus on training an unsuper-
vised model for word segmentation, which we ap-
ply to SMT for a given Arabic dialect. We train a
pre-existing unsupervised segmentation model on
the Arabic side of the training bi-text (and on some
other monolingual data), and then we optimize its
parameters based on the resulting SMT quality.
Similarly, a multi-dialectal word segmenter could
be developed by training on multi-dialectal data.
207
In particular, we develop a Qatari Arabic to En-
glish (QA-EN) SMT system, which we train on a
small pre-existing bi-text. As part of the devel-
opment of the unsupervised segmentation model,
we also collected some additional monolingual
data for Qatari Arabic. Qatari Arabic is a subdi-
alect of the more general Gulf dialect, among with
Saudi, Kuwaiti, Emirati, Bahraini, and Omani; we
collected additional monologual data for each of
these subdialects, and we release this data to the
research community.
We train an unsupervised segmentation tool,
Morphessor, and its MAP model (Creutz and La-
gus, 2007), using different variations of the col-
lected Qatari data. We optimize the single hy-
perparameter of the MAP model by maximizing
the translation quality of the QA-EN SMT sys-
tem in terms of BLEU. Our experimental results
demonstrate that the resulting unsupervised seg-
menter yields improvements in translation quality
when compared to (i) using no segmentation and
(ii) using an MSA-based ATB segmenter.
We further develop a multi-dialectal word seg-
mentation model, which we train on the Arabic
side of the multi-dialectal training data, which
consists of Qatari Arabic, Egyptian Arabic (EGY),
Levantine Arabic (LEV) and MSA to English,
i.e., a scaled combination of all the available par-
allel data. We train a QA-EN SMT system using
the segmented multi-dialectal data, and we show
an absolute gain of 1.5 BLEU points compared to
a baseline that uses no segmentation.
The rest of the paper is organized as follows:
First, we provide an overview of related work on
Dialectal Arabic NLP (Section 2). Next, we dis-
cuss and we illustrate the linguistic differences be-
tween different Arabic dialects in comparison with
and with a focus on Qatari Arabic (Section 3).
Then, we provide statistics about the corpora we
collected and used in our experiments, followed by
an illustration of the orthographic normalization
schemes we applied (Section 4). We next provide
a high-level description of our approach, which
uses morphological segmentation to combine re-
sources for other Arabic dialects in a QA-EN SMT
system effectively (Section 4.3). We also explain
our experimental setup and we present the results
(Section 5). We then discuss translating in the
reverse direction, i.e., into Qatari Arabic (Section
6). Finally, we point to possible directions for fu-
ture work and we conclude the paper (Section 7).
2 Related Work
NLP for DA is still in its early stages of develop-
ment and many challenges need to be overcomed
such as the lack of suitable tools and resources.
Collecting resources for dialectal Arabic:
Several researchers have directed efforts to de-
velop DA computational resources (Maamouri et
al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and
Callison-Burch, 2011; Salama et al., 2014). Zbib
et al. (2012) built two dialectal Arabic-English
parallel corpora for Egyptian and Levantine Ara-
bic using crowdsourcing. Bouamor et al. (2014)
presented a multi-dialectal Arabic parallel corpus,
which covers five Arabic dialects besides MSA
and English. Mubarak and Darwish (2014) col-
lected a multi-dialectal corpus using Twitter. Un-
like previous work, we focus on Gulf subdialects,
particularly Qatari Arabic. The monolingual data
that we collected is a high-quality dialectal re-
source and originates from dialect-specific sources
such as novels and forums.
Adapting SMT resources for other Arabic di-
alects: Many researchers have explored the po-
tential of using MSA as a pivot language for im-
proving SMT of Arabic dialects (Bakr et al., 2008;
Sawaf, 2010; Salloum and Habash, 2011; Sajjad et
al., 2013a; Jeblee et al., 2014). This often involves
DA-MSA conversion schemes as an alternative in
the absence of DA-MSA parallel resources. In
contrast, limited work has been done on lever-
aging available resources for other dialects. Re-
cently, Zbib et al. (2012) have shown that using
a small amount of dialectal data could yield great
improvements for SMT. Here, we investigate the
potential of improving the resource adaptability of
Arabic dialects. Our work is different as we use
an unsupervised segmenter that helps in improv-
ing the lexical overlap between dialects and MSA.
Building morphological segmenters for the
Arabic dialects: Researchers have already fo-
cused efforts on crafting and extending existing
MSA tools to DA by mainly using a set of rules
(Habash et al., 2012). Habash and Rambow
(2006) presented MAGEAD, a knowledge-based
morphological analyzer and generator for Egyp-
tian and Levantine Arabic. Chiang et al. (2006)
developed a Levantine morphological analyzer on
top of an existing MSA analyzer using an explicit
knowledge base.
208
Riesa and Yarowsky (2006) trained a supervised
trie-based model using a small lexicon of dialec-
tal affixes. In our work, we eliminate the need
for linguistic knowledge by training an unsuper-
vised model using available resources. The unsu-
pervised mode of learning allowed us to develop a
multi-dialectal morphological segmenter.
3 Arabic Dialects
In this section, we highlight some of the linguis-
tic differences between Arabic dialects and MSA,
with a focus on the Qatari dialect.
3.1 Phonological Variations
The Gulf dialect often preserves the phonological
representation of MSA, which is not the case with
many other Arabic dialects. For example, in Egyp-
tian (EGY) and in some Levantine (LEV) dialects,
the MSA consonants

H /v/,

? /q/, and
	
X /*/ are
realized as

H /t/, glottal stop /?/, and
	
? /Z/, re-
spectively. While, their MSA pronunciations are
preserved in Gulf Arabic.
In Gulf Arabic, there are some phonological dif-
ferences between countries such as Kuwait (KW),
Saudi Arabia (SA), Bahrain (BH), Qatar (QA),
United Arab Emirates (AE), and Oman (OM).
Here, we focus our discussion on Qatari Arabic,
and we compare it to MSA and other dialects.
The QA dialect borrows two Persian characters
namely h

/J/ and

? /V/. For instance, the MSA
letter h
.
/j/ is converted to /J/ in QA, e.g., ?A?

Jk
.
@

?meeting? is pronounced as /<jtimAE/ in MSA
and /<JtimAE/ in QA. The Persian character h

/J/
is also used in place of ? /k/ in some MSA words
when they are used in QA. For example, ??
?
?
?fish?
/samak/ is pronounced i

?
?
?
/smaJ/ in QA, while
the EGY and the LEV dialects maintain the MSA
pronunciation. The Persian

? /V/ is used to map
the sound of the English letter ?v? in borrowed for-
eign words, e.g., ?K


YJ


	
? ?video? is pronounced as
?K


YJ



? /Viydyw/ as opposed to /fiydywu/; the form
in which it is written in MSA.
The MSA consonant
	
? /D/ is not used in the
QA dialect. It is substituted by
	
? /Z/ in Qatari. For
example, the MSA pronunciation /HaD/ of
	
?k
?to encourage? is transformed to
	
?k /HaZ/ in QA,
but it is maintained in EGY.
Meanwhile, the MSA consonant
	
? /Z/ is re-
alized as /D/ in EGY. For example, the MSA
pronunciation /HaZ/ of
	
?k ?luck? is maintained
in QA and transformed to /HaD/ in EGY. This
change is consistent in all words within each di-
alect. However, such phonological variations be-
tween dialects have the potential to add ambiguity
to dialectal Arabic.
The MSA consonant h
.
/j/ can be used to distin-
guish between different dialects, particularly Gulf
subdialects. h
.
/j/ is pronounced as ?


/y/ in KW,
BH, QA, AE,

? /q/ in OM, much like in EGY,
and h
.
/j/ in SA, much like in LEV. For exam-
ple, the MSA word Yj
.
?? ?mosque? /masjid/ is
pronounced as /masjid/ in MSA, SA, LEV, Y

???
/masqid/ in OM, EGY, YJ


?? /masyid/ in KW,
BH, QA, AE, while the MSA pronunciation is
preserved in SA. This change does not apply to
names. However, we should note that it is not con-
sistent in QA, e.g., the MSA pronunciation of h
.
/j/
in ?J
.
k
.
?mountain? /jabal/ and h
.
QK
.
?tower? /burj/ is
preserved in QA.
3.2 Morphological Variations
In Arabic, a root can produce surface wordforms
by means of inflectional and derivational morpho-
logical processes (Habash, 2010).
An inflectional word form is a variant of a root
word with the same meaning but expressing a dif-
ferent function, e.g., gender, number, case. It is
usually formed by adding a prefix, a suffix, or a
circumfix to a stem word. Note that Arabic di-
alects can make different lexical choices for affix-
ations compared to MSA. For example, the MSA
future prefix ? /s/ is replaced by H
.
/b/ in QA
and by ? /h/ in EGY and LEV. Thus, the MSA
word ??

AJ


? ?he will eat? /say>kul/ becomes ?? AJ


K
.
/biyAkil/ in QA and ?? AJ


? /hayAkul/ in EGY and
LEV.
A derivational word form is formed by applying
a pattern to a root word, e.g., ?player? is derived
from ?play? using the pattern noun + ?er?. An
example of an Arabic derivational form is ??
	
?

K
?do? /tafaE?al/. The root is ??
	
? /faEal/ and it uses
the imperative pattern

H+??
	
?. In EGY, @ /A/ is
added as a prefix; so, it becomes ??
	
?

K@ /AitfaE
?
il/.
209
Meanwhile, the original form is preserved in QA.
Changing the structure of a pattern in a dialect
will result in producing a new dialect-specific or-
thography for every word that is represented by the
structure. For example, the MSA word ??
?

K
?learn?
/taEal?am/ becomes ??
?

K
@ /AitEalim/ in EGY, while
the MSA form is preserved in QA.
3.3 Lexical Variations
Lexical variations are among the most obvious
differences between Arabic dialects. For exam-
ple, the MSA word @
	
XA? ?what? /mA*A/ would be
found as ?

? /$uw/ in LEV, ?K


@

/<yh/ in EGY, and
?
	
J

? /$nuw/ in GLF. We can find lexical variations
in subdialects as well. For example, the MSA
negation word
	
?? /lan/, ?not?, is expressed as I
.
?
/mab/ in QA, as ?? /muw/ in KW, and as I
.
?? /ma-
hab/ in SA.
3.4 Orthographic Variations
Due to the lack of orthographic standardization of
dialectal Arabic, some MSA words can be found
in dialectal text with both MSA and phonologi-
cal spellings. For example, the MSA word

???
g
.
?gathering? /jamEap/ can be also spelled as ???
?

/yamEah/, which is a phonetic variation in QA.
Some dialectal words also vary in spelling due
to variation in their pronunciation, e.g.,
	
??

?

@
/A$uwf/, a QA word meaning ?I see?, can be also
spelled as
	
??k
.
@ /Ajuwf/.
In dialectal Arabic, different orthographic
forms are also possible for entire phrases. For
instance, words followed or preceded by pro-
nouns are commonly reduced to a single word,
e.g., A??

I?

? /glt lahA/ ?I told her? is written as
A??

J?

?. Also, commonly used religious phrases can
be found written as a single unit, e.g., ?<? @ Z A

? A?
/mA $A? A
?
lah/ ?God has willed it? as ?<?A

??.
4 Methodology
In the section, we present some statistics about the
Arabic dialectal data that we have collected. We
processed it to remove orthographic inconsisten-
cies. Then, we used a pre-existing unsupervised
morphological segmenter, Morfessor, in order to
segment the text.
Corpus QCA AVIA
QA
AVIA
O
Sents 14.7 0.9 2
Tokens 115 6.7 15
Table 1: Statistics about the collected parallel cor-
pora (in thousands). AVIA
O
shows the statistics
about the AVIA corpus excluding Qatari data.
4.1 Data Collection
We did an extensive search for available monolin-
gual and bilingual resources for the Gulf dialect,
with a focus on Qatari Arabic. Tables 1 and 2
present some statistics about the corpora we col-
lected. More detailed description follows below.
Bilingual corpora:
? The QCA speech corpus, comprises 14.7k
sentences that are phonetically transcribed from
TV broadcasts in Qatari Arabic and translated to
English; see (Elmahdy et al., 2014) for more de-
tail. The corpus was designed for speech recog-
nition and we faced several normalization-related
issues that we had to resolve before it could be
used for machine translation and language mod-
eling. One example is the usage of five Per-
sian characters to represent some sounds in Ara-
bic words. Moreover, the English side had some
grammatical and spelling errors. We normalized
the Arabic side and corrected the English side of
the corpus as described in Section 4.2. The cor-
pus can be found at http://sprosig.isle.
illinois.edu/corpora/1.
? The AVIA corpus
1
is designed as a refer-
ence source of dialectal Arabic. It consists of 3k
sentences in four Gulf subdialects: Emirati (AE),
Kuwaiti (KW), Qatari (QA), and Hejazi (SA).
2
The data consists of dialectal sentences that con-
tain words commonly used in daily conversation.
Monolingual corpora: We further collected
monolingual corpora consisting of a total of 2.7M
tokens for various Gulf subdialects. The Qatari
part of the data consists of 470K tokens. Most of
the corpus is a collection of novels, belonging to
the romance genre.
3
For the Qatari dialect, we also
collected Qatari forum data.
4
1
http://terpconnect.umd.edu/
?
nlynn/
AVIA/Level3/
2
The website also contains small parallel corpora for
MSA, EGY and LEV to English, but here we focus on Gulf
subdialects only.
3
http://forum.te3p.com/264311-52.html
4
www.qatarshares.com/vb/index.php
210
Corpus Novel Forum
AE BH KW OM QA SA QA
Tokens 573 244 178 372 412 614 69
Types 43 22 27 27 43 71 15
Table 2: Statistics about the collected monolingual
corpora (in thousands of words).
To the best of our knowledge, this is the first
collection of monolingual corpora for Gulf Ara-
bic subdialects. It can be helpful for, e.g., lan-
guage modeling when translating into Arabic, for
learning the similarities and differences between
Gulf subdialects, etc. Table 2 shows some statis-
tics about the data after punctuation tokenization.
4.2 Orthographic Normalization
The inconsistency in the orthographic spelling of
the same word can increase data sparseness. Thus,
we normalize the Arabic text in the collected re-
sources by applying the reduced orthographic nor-
malization scheme, e.g., Tah Marbota is reduced to
Hah. We also normalize extended lines between
letters, e.g., Q?? ?sugar? /sukar/ is changed
to Q??, and we reduce character elongations to
be just two characters long. In order to main-
tain consistency among different resources, we re-
move supplementary diacritics, e.g.,

Y


?

? ?knots?
/Euqad/ is normalized to Y

??, and we map Per-
sian letters to their phonological correspondences
in Qatari Arabic
5
, i.e., ? /G/ to

? /g/,

? /V/ to
	
?
/f/, H

/P/ to H
.
/b/, and

P and h

/J/ to h
.
/j/.
For the English texts, the orthographic varia-
tions were already normalized. However, the En-
glish side of the QCA corpus had some spelling
and grammatical errors, which we corrected man-
ually. On the grammatical side, we only corrected
a subset of the data, which we used for tuning and
testing our SMT system (see Section 5).
4.3 Morphological Decomposition
There is no general Arabic morphological seg-
menter that works for all variations of Arabic. The
most commonly used segmenters for Arabic were
designed for MSA (Habash et al., 2009; Green and
DeNero, 2012). Due to the lexical and morpholog-
ical differences between dialects and MSA, these
MSA-based morphological tools do not work well
for dialects.
5
This issue relates to the QCA corpus.
In this work, we used an unsupervised morpho-
logical segmenter, Morfessor-categories MAP
6
,
an unsupervised model with a single hyper-
parameter (Creutz and Lagus, 2007). We chose
Morfessor because of its superior performance on
Arabic compared to other unsupervised models
(Siivola et al., 2007; Poon et al., 2009).
The model has a single hyperparameter, the per-
plexity threshold parameter B, which controls the
granularity of segmentation. The recommended
value ranges from 1 to 400 where 1 means max-
imum fine-grained segmentation, and 400 restricts
it to the least segmented output. We set the thresh-
old empirically to 70, as shown in Section 5.1.
5 Experimental Setup
We performed an extrinsic evaluation of the varia-
tions in segmentation by building a Qatari Arabic
to English machine translation system on each of
them. We also tested Morfessor on other available
dialects and on MSA, and we will show below how
a uniform segmentation can help to better adapt re-
sources for dialects and MSA for SMT. This sec-
tion describes our experimental setup.
Datasets: We divided the QCA corpus into 1k
sentences each for development and testing, and
we used the remaining 12k for training.
We adapted parallel corpora for Egyptian, Lev-
antine and MSA to English to be used for Qatari
Arabic to English SMT. For MSA, we used par-
allel corpora of TED talks (Cettolo et al., 2012)
and the AMARA corpus (Abdelali et al., 2014),
which consists of educational videos. Since the
QCA corpus is in the speech domain, we believe
that an MSA corpus of spoken domain would be
more helpful than a text domain such as News. For
Egyptian and Levantine, we used the parallel cor-
pus provided by Zbib et al. (2012). There is no
Gulf?English parallel data available in the litera-
ture. The data that we found was a very small col-
lection of subdialects of Gulf Arabic; we did not
use it for MT experiments. However, we used the
Qatari part of the AVIA corpus to train Morfessor.
Machine translation system settings: We used
a phrase-based statistical machine translation
model as implemented in the Moses toolkit
(Koehn et al., 2007) for machine translation.
6
This is an extension of the basic Morfessor method and
is based on a Maximum a Posteriori model.
211
We built separate directed word alignments
for source-to-target and target-to-source using
IBM model 4 (Brown et al., 1993), and we
symmetrized them using the grow-diag-final-and
heuristics (Koehn et al., 2003). We then extracted
phrase pairs with a maximum length of seven, and
we scored them using maximum likelihood esti-
mation with Kneser-Ney smoothing (Kneser and
Ney, 1995). We also built a lexicalized reordering
model, msd-bidirectional-fe. We built a 5-gram
language model on the English side of QCA-train
using KenLM (Heafield, 2011). Finally, we built a
log-linear model using the above features.
We tuned the model weights by optimizing
BLEU (Papineni et al., 2002) on the tuning set, us-
ing PRO (Hopkins and May, 2011) with sentence-
level BLEU+1 optimization (Nakov et al., 2012).
In testing, we used minimum Bayes risk decoding
(Kumar and Byrne, 2004), cube pruning, and the
operation sequence model (Durrani et al., 2011).
Baseline: Our baseline Qatari Arabic to English
MT system is trained on the QCA bitext without
any segmentation of Qatari Arabic. For the exper-
iments described in this paper, we used the English
side of the QCA corpus for language modeling.
5.1 Experimental Results
In this section, we first present our work on using
Morfessor for segmenting Qatari Arabic. We tried
different values of its parameter, and we trained it
using corpora of different sizes to find balanced
settings that improve SMT quality as compared
with no segmentation and with segmentation us-
ing the Stanford ATB segmenter. We further ap-
plied our selected settings to segment MSA, EGY
and LEV and used them for Qatari Arabic to En-
glish machine translation. Our results show that a
uniform segmentation scheme across different di-
alects improves machine translation.
Morfessor training variations: We trained
Morfessor using three corpora: (i) QCA,
(ii) AVIA
QA
plus Qatari Novels, and (iii) a com-
bination thereof. Table 3 shows the results for
our SMT system when trained on the QCA par-
allel corpus, which was segmented using different
training models of Morfessor with B = 40. The
result for segmented Qatari Arabic is always bet-
ter than the baseline, irrespective of the training
model used for segmentation. We can see that the
Morfessor model trained on a large monolingual
corpus, i.e., on (ii) or (iii), yields better results.
Morfessor BLEU OOV%
Baseline 12.2 16.6
QCA 12.5 0.6
AVIA
QA
, Novels 13.5 0.8
QCA, AVIA
QA
, Novels 13.4 0.7
Table 3: Study of the effect of varying the train-
ing datasets for Morfessor on the Qatari to English
SMT. ?Baseline? shows the output of the MT sys-
tem with no segmentation.
B 10 40 70 100 130
BLEU 13.3 13.5 13.8 12.9 12.6
OOV 0.3 0.8 1.4 2.8 2.8
After merging
BLEU 12.5 13.4 13.7 12.8 12.3
OOV 1.5 1.9 3.9 6.5 9.8
Table 4: The effect of varying the perplexity
threshold parameter B of Morfessor on SMT qual-
ity. ?After merging? are the results using the post-
processed Qatari segmented data.
The high reduction in OOV in Table 3 is be-
cause of the fine-grained segmentation. We tried
different values for the perplexity parameter B
in order to find a good balance between better
BLEU scores and linguistically correct segmen-
tations. The first part of Table 4 shows the ef-
fect of different values of B on the quality of the
machine translation system trained on AVIA
QA
,
Qatari Novels. We achieved the best SMT score at
B = 70.
We further analyzed the output of Morfessor
at B = 70 and we noticed that it tends to gener-
ate very small segments of length two and three
characters long. The segmentation produces more
than one stem in a word and does not generate le-
gal word units. For example, the word

??A
	
J??@?
?and the industry? /wAlSinAEp/ is segmented as
PRE/? + PRE/?@ + STM/? + PRE/
	
? + PRE/ @
+ STM/? + SUF/

?. We apply a post-processing
step that merges all stems in a word and affixes
between them to one stem. So, a word can have
only one stem. For example, the word

??A
	
J??@?
would be segmented as PRE/?@? + STM/?A
	
J? +
SUF/

?. This yielded linguistically correct segmen-
tations in many cases. The second part of Table
4 shows the effect of the post-processing on the
BLEU score. We can see that it remains almost
the same with an increase in OOV rate.
212
For rest of the experiments in this paper, we
used a value of 70 for the perplexity threshold
parameter plus the post-processing on segmenta-
tion. We trained Morfessor on the concatenation
of QCA, AVIA
Q
A and Novels.
7
Using other Arabic variations: In this section,
we present experiments using MSA, EGY and
LEV to English bitexts combined with the QCA
bitext for Qatari Arabic to English machine trans-
lation. We explored three segmentation options for
the Arabic side of the data: (i) no segmentation,
(ii) ATB segmentation, and (iii) unsupervised seg-
mentation using Morfessor.
The QCA corpus is of much smaller size com-
pared to other Arabic variants, say MSA. It is pos-
sible that in the training of the machine transla-
tion models, the large corpus dominates the QCA
corpus. In order to avoid that, we balanced the
two corpora by replicating the smaller corpus X
number of times in order to make it approximately
equal to the large corpus (Nakov and Ng, 2009).
8
The complete procedure is described below.
In a nutshell, for building a machine transla-
tion system using the MSA plus Qatari corpus, we
first balanced the Qatari corpus to make it approx-
imately equal to MSA and concatenated them. For
training Morfessor, the Qatari Arabic data con-
sisted of QCA, Novels and AVIA
QA
, while for
SMT, it consisted of QCA only. In both cases,
we balanced it to be approximately equal to MSA.
We then trained Morfessor on the balanced (QCA,
Novels, AVIA
QA
) plus MSA data and we seg-
mented the Arabic side of the balanced QCA plus
MSA training data for machine translation. We
built a machine translation system on the seg-
mented data. We segmented the testing and tuning
data sets similarly. We used the same balancing
when we combined EGY-EN and LEV-EN with
the Qatari Arabic ? English data.
We also tried training multiple unsupervised
models, but this yielded lower SMT quality com-
pared to using a single model trained on multi-
dialects. Using different models could result
in having different segmentation schemes, which
might not help in reducing the vocabulary mis-
match between different variants of Arabic.
7
We did not see a big difference in training Morfessor
with and without the QCA corpus, and we decided to use
the complete data for training.
8
Due to the spoken nature of the QCA corpus, it contains
shorter sentences. Thus, we balanced the corpora based on
the number of tokens rather than on the number of sentences.
Train NONE ATB Morfessor
QCA 12.2 12.9 13.7
?QCA,MSA 12.7 13.3 14.6
?QCA,EGY 13.0 13.5 14.5
?QCA,LEV 13.8 13.7 15.2
Table 5: BLEU scores for Qatari Arabic to English
SMT using three different segmentation settings.
?QCA means the modified QCA corpus with num-
ber of tokens approximately equal to MSA, EGY
and LEV in the respective experiments.
Table 5 shows the results. There are two things
to point here. First, the SMT systems that used
the unsupervised morphological segmenter, Mor-
fessor, outperformed the systems that used no seg-
mentation and those using the ATB segmentation.
The Morfessor-based systems showed consistent
improvements compared to the ATB-based sys-
tems over the no-segmentation systems. This val-
idates our point that unsupervised morphological
segmentation generalizes well for a variety of di-
alects and these SMT results complement that.
The second observation is that adding a bitext for
other dialects and MSA improves machine trans-
lation quality for Qatari?English SMT.
6 Translation into Qatari Arabic
Our monolingual corpora of Gulf subdialects
could be also helpful when translating English into
Qatari Arabic. We conducted a few basic experi-
ments in this direction but without segmentation.
We trained an English to Qatari Arabic SMT
system on the QCA bitext, using the same settings
as described in Section 5. We then normalized the
output of the translation system using the QCRI-
Normalizer (Sajjad et al., 2013b).
9
As a language
model, we used the Arabic side of the QCA cor-
pus, novels and forum data, standalone and to-
gether. Table 6 presents the results of the effect of
varying the language model on the quality of the
SMT system. The best system shows an improve-
ment of 0.22 BLEU points absolute compared to
the baseline system that only uses the Arabic side
of the QCA corpus for LM training.
The SMT system achieved the largest gain when
adding QA forum data to the QCA data. SA and
AE monolingual data also showed good improve-
ments. This might be due to their relatively large
sizes; we need further investigation.
9
http://alt.qcri.org/tools/
213
LM BLEU
QCA 2.78
QCA+QA-Novels 2.64
QCA+QA-Novels+BH-Novels 2.86
QCA+QA-Novels+KW-Novels 2.78
QCA+QA-Novels+AE-Novels 2.92
QCA+QA-Novels+SA-Novels 2.96
QCA+ALL-Novels 2.80
QCA+QA-Novels+QForum 3.00
Table 6: Results for English to Qatari SMT for
varying language models. In all cases, the transla-
tion model is trained on the QCA bitext only.
Note the quite low BLEU scores, especially
compared to the reverse translation direction. One
reason is the morphologically rich nature of Qatari
Arabic, which makes translating into it a hard
problem. The small amount of training data fur-
ther adds to it. We expect to see larger gains com-
pared to Qatari Arabic to English machine transla-
tion when segmentation is used.
7 Conclusion and Future Work
We have demonstrated the feasibility of using
an unsupervised morphological segmenter to in-
crease the resource adaptability of Arabic variants.
We evaluated the segmentation on a Qatari dialect
by building a Qatari Arabic to English machine
translation system. We further adapted MSA,
EGY and LEV in the simplest machine translation
settings and we showed a consistent improvement
of 1.5 BLEU points when compared to the respec-
tive baseline system that uses no segmentation.
In the future, we would like to explore the
impact of segmentation on both the translation
model and the language model when translating
into Qatari Arabic. This involves greater chal-
lenges, as a desegmenter is required for the trans-
lation output with every segmentation scheme.
References
Ahmed Abdelali, Francisco Guzman, Hassan Sajjad,
and Stephan Vogel. 2014. The AMARA corpus:
Building parallel language resources for the educa-
tional domain. In Proceedings of the 9th Interna-
tional Conference on Language Resources and Eval-
uation, Reykjavik, Iceland, May.
Rania Al-Sabbagh and Roxana Girju. 2010. Mining
the web for the induction of a dialectical Arabic lexi-
con. In Proceedings of the 7th International Confer-
ence on Language Resources and Evaluation, Val-
letta, Malta, May.
Hitham Abo Bakr, Khaled Shaalan, and Ibrahim
Ziedan. 2008. A hybrid approach for convert-
ing written Egyptian colloquial dialect into dia-
critized Arabic. In Proceedings of the 6th Inter-
national Conference on Informatics and Systems,
Cairo, Egypt, March.
Houda Bouamor, Nizar Habash, and Kemal Oflazer.
2014. A multidialectal parallel corpus of Arabic. In
Proceedings of the 9th edition of the Language Re-
sources and Evaluation Conference, Reykjavik, Ice-
land, May.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and R. L. Mercer. 1993. The mathe-
matics of statistical machine translation: parameter
estimation. Computational Linguistics, 19(2), June.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit
3
: Web inventory of transcribed
and translated talks. In Proceedings of the 16
th
Con-
ference of the European Association for Machine
Translation, Trento, Italy, May.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic
dialects. In Proceedings of the 11th Conference of
the European Chapter of the Association for Com-
putational Linguistics, Trento, Italy, April.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing, 4(1), January.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, Port-
land, OR, June.
Mohamed Elmahdy, Mark Hasegawa-Johnson, and
Eiman Mustafawi. 2014. Development of a TV
broadcasts speech recognition system for Qatari
Arabic. In Proceedings of the 9th edition of the Lan-
guage Resources and Evaluation Conference, Reyk-
javik, Iceland, May.
Spence Green and John DeNero. 2012. A class-based
agreement model for generating accurately inflected
translations. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics, Jeju Island, Korea, July.
Nizar Habash and Owen Rambow. 2006. MAGEAD:
a morphological analyzer and generator for the Ara-
bic dialects. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, Sydney, Australia, July.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
Mada+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, pos
214
tagging, stemming and lemmatization. In Proceed-
ings of the 2nd International Conference on Ara-
bic Language Resources and Tools (MEDAR), Cairo,
Egypt, April.
Nizar Habash, Ramy Eskander, and Abdelati Hawwari.
2012. A morphological analyzer for Egyptian Ara-
bic. In Proceedings of the 12th Meeting of the Spe-
cial Interest Group on Computational Morphology
and Phonology, Montreal, Canada, June.
Nizar Y Habash. 2010. Introduction to Arabic natural
language processing. Synthesis Lectures on Human
Language Technologies, 3(1), August.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the 6th
Workshop on Statistical Machine Translation, Edin-
burgh, UK, July.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
Scotland, UK, July.
Serena Jeblee, Weston Feely, Houda Bouamor, Alon
Lavie, Nizar Habash, and Kemal Oflazer. 2014.
Domain and Dialect Adaptation for Machine Trans-
lation into Egyptian Arabic. In Proceedings of
the Arabic Natural Language Processing Workshop,
Doha, Qatar, October.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for ngram langauge modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, Detroit,
Michigan, May.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology and North
American Association for Computational Linguis-
tics Conference, Edmonton, Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics, Demonstra-
tion Program, Prague, Czech Republic, June.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics,
Boston, MA, May.
Mohamed Maamouri, Ann Bies, Tim Buckwalter,
Mona Diab, Nizar Habash, Owen Rambow, and
Dalila Tabessi. 2006. Developing and using a pi-
lot dialectal Arabic treebank. In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation, Genova, Italy, May.
Hamdy Mubarak and Kareem Darwish. 2014. Using
Twitter to collect a multi-dialectal corpus of Arabic.
In Proceedings of the Arabic Natural Language Pro-
cessing Workshop, Doha, Qatar, October.
Preslav Nakov and Hwee Tou Ng. 2009. Improved
statistical machine translation for resource-poor lan-
guages using related resource-rich languages. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, Suntec,
Singapore, August.
Preslav Nakov, Francisco Guzman, and Stephan Vo-
gel. 2012. Optimizing for sentence-level BLEU+1
yields short translations. In Proceedings of the
24th International Conference on Computational
Linguistics, Mumbai, India, December.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th annual meeting on association for com-
putational linguistics, Philadelphia, PA, July.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics, Denver, CO,
June.
Jason Riesa and David Yarowsky. 2006. Minimally
supervised morphological segmentation with appli-
cations to machine translation. In Proceedings of
the 7th Conference of the Association for Machine
Translation in the Americas, MA, USA, August.
Hassan Sajjad, Kareem Darwish, and Yonatan Be-
linkov. 2013a. Translating dialectal Arabic to En-
glish. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics,
Sofia, Bulgaria, August.
Hassan Sajjad, Francisco Guzman, Preslav Nakov,
Ahmed Abdelali, Kenton Murray, Fahad Al Obaidli,
and Stephan Vogel. 2013b. QCRI at IWSLT 2013:
Experiments in Arabic-English and English-Arabic
Spoken Language Translation. In Proceedings of the
10th International Workshop on Spoken Language
Translation, Hiedelberg, Germany, December.
Ahmed Salama, Houda Bouamor, Behrang Mohit, and
Kemal Oflazer. 2014. YouDACC: the youtube di-
alectal Arabic commentary corpus. In Proceedings
of the 9th edition of the Language Resources and
Evaluation Conference, Reykjavik, Iceland, May.
Wael Salloum and Nizar Habash. 2011. Dialectal
to standard Arabic paraphrasing to improve Arabic-
English statistical machine translation. In Proceed-
ings of the First Workshop on Algorithms and Re-
sources for Modelling of Dialects and Language Va-
rieties, Edinburgh, Scotland, July.
215
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the 9th Con-
ference of the Association for Machine Translation
in the Americas, Denver, CO, October.
Vesa Siivola, Mathias Creutz, and Mikko Kurimo.
2007. Morfessor and VariKN machine learning
tools for speech and language technology. In
Proceedings of the 8th International Conference
on Speech Communication and Technology (Inter-
speech), Antwerpen, Belgium, August.
Ming-Feng Tsai, Preslav Nakov, and Hwee Tou Ng.
2010. Morphological analysis for resource-poor
machine translation. Technical report, Kent Ridge,
Singapore, December.
Omar F Zaidan and Chris Callison-Burch. 2011. The
Arabic online commentary dataset: an annotated
dataset of informal Arabic with high dialectal con-
tent. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies: short papers-Volume
2, Portland, OR, June.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-
Burch. 2012. Machine translation of Arabic di-
alects. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Montreal, Canada, June.
Andreas Zollmann, Ashish Venugopal, and Stephan
Vogel. 2006. Bridging the inflection morphol-
ogy gap for Arabic statistical machine translation.
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Companion Volume:
Short Papers, New York, NY, June.
216
