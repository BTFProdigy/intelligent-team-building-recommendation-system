A Conversational Interface for Online Shopping
Joyce Chai, Veronika Horvath, Nanda Kambhatla, Nicolas Nicolov & Margo Stys-Budzikowska
Conversational Dialog Systems
IBM T. J. Watson Research Center
30 Saw Mill River Rd, Hawthorne, NY 10532, USA
{jchai, veronika, nanda, nicolas, sm1}@us.ibm.com
ABSTRACT
We present a deployed, conversational dialog system that assists
users in finding computers based on their usage patterns and
constraints on specifications. We discuss findings from a market
survey and two user studies. We compared our system to a directed
dialog system and a menu driven navigation system. We found that
the conversational interface reduced the average number of clicks by
63% and the average interaction time by 33% over a menu driven
search system. The focus of our continuing work includes
developing a dynamic, adaptive dialog management strategy,
robustly handling user input and improving the user interface.
1. INTRODUCTION
Conversational interfaces allow users to interact with automated
systems using speech or typed in text via "conversational dialog".
For the purposes of this paper, a conversational dialog consists of a
sequence of interactions between a user and a system. The user
input is interpreted in the context of previous user inputs in the
current session and from previous sessions.
Conversational interfaces offer greater flexibility to users than
menu-driven (i.e., directed-dialog) interfaces, where users navigate
menus that have a rigid structure [5,4]. Conversational interfaces
permit users to ask queries directly in their own words. Thus, users
do not have to understand the terminology used by system designers
to label hyperlinks on a website or internalize the hierarchical
menus of a telephone system [3] or websites.
Recently, conversational interfaces for executing simple transactions
and for finding information are proliferating [7,6]. In this paper, we
present a conversational dialog system, Natural Language Assistant
(or NLA), that helps users shop for notebook computers and discuss
the results of user studies that we conducted with this system.
2. NATURAL LANGUAGE ASSISTANT
NLA assists users in finding notebooks that satisfy their needs by
engaging them in a dialog. At each turn of the dialog, NLA provides
incremental feedback about its understanding of the user's
constraints and shows products that match these constraints. By
encouraging iterative refinement of the user's query, the system
finds more user constraints and, ultimately, recommends a product
that best matches the user's criteria.
The system consists of three major modules (cf. Figure 1):
Presentation Manager, Dialog Manager, and Action Manager. The
Presentation Manager interprets user input and generates system
responses. It embodies the user interface and contains a shallow
semantic parser and a response generator. The semantic parser
identifies concepts (e.g., MULTIMEDIA) and constraints on
product attributes (e.g., hard disk size more than 20GB) from the
textual user input. The concepts mediate the mapping between user
input and available products through product specifications. They
implement the business logic.
The Dialog Manager uses the current requirements and formulates
action plans for the Action Manager to perform back-end operations
(e.g., database access1). The Dialog Manager constructs a response
to the user based on the results from the Action Manager and the
discourse history and sends the system response to the Presentation
Manager that displays it to the user. The system prompts for features
relevant in the current context. In our mixed initiative dialog
system, the user can always answer the specific question put to
him/her or provide any constraints.
The system has been recently deployed on an external website.
Figure 2 shows the start of a dialog.2
1 See [1] for a survey of natural language interfaces to databases.
2 We are demonstrating the system at HLT?2001 [2].
ManagerPresentation
Manager
telephone
PDA
web
Conversational
Dialog Manager
USER
APIs
speech,
text,..
NLP
Services
History
Action
Manager
Application
Action
Templates
etc...
APIs
Discourse
History
Figure 1. Architecture of the NLA conversational system.
3. USER STUDIES
We conducted a preliminary market survey and two user studies
described in subsections 3.1 and 3.2 respectively.
3.1 Market Survey
For understanding specific user needs and user vocabulary, we
conducted a user survey. Users were given three sets of questions.
The first set, in turn, contained three questions: "What kind of
notebook computer are you looking for?", "What features are
important to you?", and "What do you plan to use this notebook
computer for?". By applying statistical n-gram models and a
shallow noun phrase grammar to the user responses, we extracted
keywords and phrases expressing user's needs and interests. In the
second set of questions, users were asked to rank 10 randomly
selected terms from 90 notebook related terms in order of
familiarity to them. The third set of questions asked for
demographical information about users such as their gender, years
of experience with notebook computers, native language, etc. We
computed correlations between vocabulary/terms and user
demographic information. Over a 30-day period, we received 705
survey responses. From these responses, we learned 195 keywords
and phrases that were included in NLA.
3.2 Usability Testing
3.2.1 Experimental Setup
We conducted two user studies to evaluate usability of the system,
focusing on: dialog flow, ease of use, system responses, and user
vocabulary. The first user study focused on the functionality of
NLA and the second user study compared the functionality of
NLA with that of a directed dialog system and a menu driven
navigation system.
The moderators interviewed 52 users in the user studies: 18 and
34 in the two studies, respectively. All participants were
consumers or small business users with "beginner" or
"intermediate" computer skills. Each participant was asked to find
laptops for a variety of scenarios using three different systems (the
NLA, a directed dialog system and a menu driven navigation
system). Participants were asked to rate each system for each task
on a 1 to 10 scale (10 ? easiest) with respect to the ease of
navigation, clarity of terminology and their confidence in the
system responses. The test subjects were also asked whether the
system had found relevant products and were prompted to share
their impressions as to how well the system understood them and
responded to their requests.
Figure 2. The start of the dialog.
3.2.2 Results
In both studies, participants were very receptive to using natural
language dialog-based search. The users clearly preferred dialog-
based searches to non-dialog based searches3 (79% to 21% users).
Furthermore, they liked the narrowing down of a product list
based on identified constraints as the interaction proceeded. In the
first user study, comparing NLA with a menu driven system, we
found that using NLA reduced the average number of clicks by
63% and the average interaction time by 33%.
In the second user study, we compared NLA with a directed
dialog system and a menu driven search system for finding
computers. One goal of the comparative study was to find out if
there were any statistical differences in confidence, terminology
and navigation ratings across the three systems and whether they
were correlated with different categories of users. The ANOVA
analysis reveals statistical differences in terminology ratings
among the three systems for the category of beginner users only.
There were no statistical differences found in the other ratings of
navigation and confidence across the three sites for different
categories of users. Sandler's A test confirmed that the
terminology rating was significantly different for the categories of
consumers, small business owners, beginners and intermediates.
These comparative results suggest that asking questions relative to
the right level of end user experience is crucial. Asking users
questions about their lifestyle and how they were going to use a
computer accounted for a slight preference of the directed dialog
system over the NLA that uses questions presented on the basis of
understanding features and functions of computer terms.
3.2.3 Lessons from the user studies
Both user studies revealed several dimensions along which NLA
can be improved. The first user study highlighted a definite need
for system acknowledgement and feedback. The users wanted to
know whether the system had understood them. User comments
also revealed that a comparison of features across the whole pool
of products was important for them.
The focus of the second study, incorporating 34 subjects, was to
compare systems of similar functionality and to draw conclusions
about the functionality of NLA. Both the ANOVA and the
Sandler's test point out that terminology was a statistically
significant factor differentiating among the systems. We believe
that using terminology that is not overly technical would
contribute to the success of the dialog search. While the questions
asked by NLA were based on features and functionality of
notebook computers, the users preferred describing usage patterns
and life style issues rather than technical details of computers.
We also found that users' confidence in NLA decreased when the
system responses were inconsistent i.e., were not relevant to their
input. Lack of consistent visual focus on the dialog box was also a
serious drawback since it forced users to scroll in search of the
dialog box on each interaction page.
3 We define a dialog-based search as one comprising of a
sequence of interactions with a system where the system keeps
track of contextual (discourse) information.
3.2.4 Future work
Based on the results of the user studies, we are currently focused
on: developing a dynamic and adaptive dialog management
strategy, improving the robustness of the natural language
processing (NLP), and improving the user interface. Some of
issues mentioned here have been implemented in the next version
of NLA.
We are currently re-designing the questions that NLA asks users
to be simpler, and to focus on usage patterns rather than technical
features. We are also implementing a new dialog management
strategy in NLA that is more adaptive to the user's input, and
implements a mapping from high-level usage patterns to
constraints on low-level technical features.
We are integrating a statistical parser with NLA to more robustly
handle varied user input. The statistical parser should enable NLA
to scale to multiple languages and multiple domains in a more
robust and reliable fashion. We are aiming at an architecture that
separates the NLP processing from the business logic that will
make maintenance of the system easier.4
Improvements to the GUI include better acknowledgement and
feedback mechanisms as well as graphical UI issues. We now
reiterate the user's last query at the beginning of each interaction
page and also convey to the user an explanation of features
incrementally accumulated in the course of the interaction. We
have designed a more uniform, more compact and consistent UI.
In the welcome page, we have abandoned a three-step initiation
(typed input, experience level and preferences for major
specifications) keeping the emphasis on the dialog box. The user
preferences contributed to creating confusion as to the main
means of interaction (many users just clicked on the radial buttons
and did not use the full dialog functionality). We now infer the
technical specifications based the user's stated needs and usage
patterns. Our UI now has a no scrolling policy and we allow for
larger matching set of products to be visualized over a number of
pages.
4. DISCUSSION
In this paper, we have presented a conversational dialog system
for helping users shop for notebook computers. User studies
comparing our conversational dialog system with a menu driven
system have found that the conversational interface reduced the
average number of clicks by 63% and the average interaction time
by 33%. Based on our findings, it appears that for conversational
systems like ours, the sophistication of dialog management and
the actual human computer interface are more important than the
complexity of the natural language processing technique used.
This is especially true for web-based systems where user queries
are often brief and shallow linguistic processing seems to be
adequate. For web-based systems, integrating the conversational
interface with other interfaces (like menu-driven and search-
driven interfaces) for providing a complete and consistent user
experience assumes greater importance.
4 Many systems' fate has been decided not because they cannot
handle complex linguistic constructions but because of the
difficulties in porting such systems out of the research
environments.
The user studies we conducted have highlighted several directions
for further improvements for our system. We plan to modify our
interface to integrate different styles of interaction (e.g., menus,
search, browsing, etc.). We also intend to dynamically classify
each user as belonging to one or more categories of computer
shoppers (e.g., gamers, student users, home business users, etc.)
based on all the user interactions so far. We can then tailor the
whole interface to the perceived category including but not
limited to the actual questions asked, the technical knowledge
assumed by the system and the whole style of interaction.
Another area of potential improvement for the NLA is its inability
to handle any meta-level queries about itself or any deeper
questions about its domain (e.g., NLA currently can not properly
handle the queries, "How can I add memory to this model?" or
"What is DVD?"). Our long-term goal is to integrate different
sources of back-end information (databases, text documents, etc.)
and present users with an integrated, consistent conversational
interface to it.
We believe that conversational interfaces offer the ultimate kind
of personalization. Personalization can be defined as the process
of presenting each user of an automated system with an interface
uniquely tailored to his/her preference of content and style of
interaction. Thus, mixed initiative conversational interfaces are
highly personalized since they allow users to interact with systems
using the words they want, to fetch the content they want in the
style they want. Users can converse with such systems by phrasing
their initial queries at a right level of comfort to them (e.g., "I am
looking for a gift for my wife" or "I am looking for a fast
computer with DVD under 1500 dollars").
5. CONCLUSIONS
Based on our results, we conclude that conversational natural
language dialog interfaces offer powerful personalized alternatives
to traditional menu-driven or search-based interfaces to websites.
For such systems, it is especially important to present users with a
consistent interface integrating different styles of interaction and
to have robust dialog management strategies. The system feedback
and the follow up questions should strike a delicate balance
between exposing the system limitations to users, and making
users aware of the flexibility of the system. In current work we are
focusing on developing dynamic, adaptive dialog management,
robust multi-lingual NLP and improving the user interface.
6. REFERENCES
[1] Androutsopoulos, Ion, and Ritchie, Graeme. Natural
Language Interfaces to Databases ? An Introduction, Natural
Language Engineering 1.1:29-81, 1995.
[2] Budzikowska, M., Chai, J., Govindappa, S., Horvath, V.,
Kambhatla, N., Nicolov, N., and Zadrozny, W.
Conversational Sales Assistant for Online Shopping,
Demonstration at Human Language Technologies
Conference (HLT'2001), San Diego, Calif., 2001.
[3] Carpenter, Bob, and Chu-Carroll, J. Natural Language Call
Routing: A Robust, Self-organizing Approach, Proceedings
of the 5th Int. Conf. on Spoken Language Processing. 1998
[4] Chai, J., Lin, J., Zadrozny, W., Ye, Y., Budzikowska, M.,
Horvath, V., Kambhatla, N., and Wolf, C. Comparative
Evaluation of a Natural Language Dialog Based System and
a Menu-Driven System for Information Access: A Case
Study, Proceedings of RIAO 2000, Paris.
[5] Saito, M., and Ohmura, K. A Cognitive Model for Searching
for Ill-defined Targets on the Web - The Relationship
between Search Strategies and User Satisfaction, 21st Int.
Conference on Research and Development in Information
Retrieval, Australia, 1998.
[6] Walker, M., Fromer, J., and Narayanan, S. Learning Optimal
Dialogue Strategies: A Case Study of a Spoken Dialogue
Agent for Email, 36th Annual Meeting of the ACL, Montreal,
Canada, 1998.
[7] Zadrozny, W., Wolf, C., Kambhatla, N. & Ye, Y.
Conversation Machines for Transaction Processing,
Proceedings of AAAI / IAAI - 1998, Madison, Wisconsin,
U.S.A. 1998.
Conversational Sales Assistant for Online Shopping
Margo Budzikowska, Joyce Chai, Sunil Govindappa, Veronika Horvath, Nanda Kambhatla,
Nicolas Nicolov & Wlodek Zadrozny
Conversational Machines Group
IBM T. J. Watson Research Center
30 Saw Mill River Rd, Hawthorne, NY 10532, U.S.A.
{sm1, jchai, govindap, veronika, nanda, nicolas, wlodz}@us.ibm.com
ABSTRACT
Websites of businesses should accommodate both customer
needs and business requirements. Traditional menu-driven
navigation and key word search do not allow users to describe
their intentions precisely. We have developed a conversational
interface to online shopping that provides convenient,
personalized access to information using natural language
dialog. User studies show significantly reduced length of
interactions in terms of time and number of clicks in finding
products. The core dialog engine is easily adaptable to other
domains.
1. INTRODUCTION
Natural language dialog has been used in many areas, such as
for call-center/routing application (Carpenter & Chu-Carroll
1998), email routing (Walker, Fromer & Narayanan 1998),
information retrieval and database access (Androutsopoulos &
Ritchie 1995), and for telephony banking (Zadrozny et al 1998).
In this demonstration, we present a natural language dialog
interface to online shopping. Our user studies show natural
language dialog to be a very effective means for negotiating
user's requests and intentions in this domain.
2. SYSTEM ARCHITECTURE
In our system, a presentation manager captures queries from
users, employs a parser to transform the user's query into a
logical form, and sends the logical form to a dialog manager.
The presentation manager is also responsible for obtaining the
system's response from the dialog manager and presenting it to
the user using template-based generation. The dialog manager
formulates action plans for an action manager to perform back-
end tasks such as database access, business transactions, etc. The
dialog manager applies information state-based dialog strategies
to formulate responses depending on the current state, discourse
history and the action results from the action manager.
The Data Management Subsystem maintains a ?concept?
repository with common sense ?concepts? and a phrasal lexicon
that lists possible ways for referring to the concepts. Business
Rules map concepts to business specifications by defining
concepts using a propositional logic formula of constraints over
product specifications. Thus, the Business Rules reflect business
goals and decisions. The Extended Database combines product
specifications and precompiled evaluations of the concept
definitions for each product to provide a representation that
guides the natural language dialog. We are investigating
automated tools for helping developers and maintainers extract
relevant concepts and terms on the basis of user descriptions and
queries about products.
3. EVALUATION
We conducted several user studies to evaluate the usability of
NLA (Chai et al 2000). In one study, seventeen test subjects
preferred the dialog-driven navigation of NLA two to one over
menu-driven navigation. Moreover, with NLA, the average
number of clicks was reduced by 63.2% and the average time
was reduced by 33.3%. Analysis of the user queries (average
length = 5.31 words long; standard deviation = 2.62; 85% of
inputs are noun phrases) revealed the brevity and relative
linguistic simplicity of user input. Hence, shallow parsing
techniques were adequate for processing user input. In general,
sophisticated dialog management appears to be more important
than the ability to handle complex natural language sentences.
The user studies also highlighted the need to combine multiple
modalities and styles of interaction.
4. REFERENCES
[1] Androutsopoulos, Ion & Ritchie, Graeme. Natural
Language Interfaces to Databases ? An Introduction,
Natural Language Engineering 1.1:29-81, 1995.
[2] Carpenter, Bob & Chu-Carroll, Jeniffer. Natural Language
Call Routing: A Robust, Self-organizing Approach,
Proceedings of the 5th International Conference on Spoken
Language Processing, 1998.
[3] Chai, J., Lin, J., Zadrozny, W., Ye, Y., Budzikowska, M.,
Horvath, V., Kambhatla, N. & Wolf, C. Comparative
Evaluation of a Natural Language Dialog Based System
and a Menu-Driven System for Information Access: A Case
Study, Proceedings of RIAO 2000, Paris, 2000.
[4] Saito, M. & Ohmura, K. A Cognitive Model for Searching
for Ill-defined Targets on the Web ? The Relationship
between Search Strategies and User Satisfaction. 21st Int.
Conf. on Research and Development in Information
Retrieval, Australia, 1998.
[5] Walker, M., Fromer, J. & Narayanan, S. Learning Optimal
Dialogue Strategies: A Case Study of a Spoken Dialogue
Agent for Email, 36th Annual Meeting of the ACL,
Montreal, Canada, 1998.
[6] Zadrozny, W., Wolf, C., Kambhatla, N. & Ye, Y.
Conversation Machines for Transaction Processing,
Proceedings of AAAI / IAAI - 1998, Madison, Wisconsin,
U.S.A., 1998.
HTML
Application
Server
Client
HTTP
Server
HTML
Servlet
Web Server
Network
(HTTP)
Presentation
Manager
Dialog
Manager
Action
Manager
Quick
Parser
Response
Generator Vector Space Engine
Product
Database
Business Rules
Concepts
Data Management
(Off line)
User Interface
Concept
Interpreter
Explanation
ModelPresentationStrategies
Dialog
Strategies
Action
Strategies
input
output
Communication
Acts
Communication
Acts
Action Specs
Online
Interaction Discourse
Analyzer
Extended
PD
Database
Query
Discourse
History
ActionResults
State
Interpreter



A Statistical Model for Multilingual Entity Detection and Tracking
R. Florian, H. Hassan   , A. Ittycheriah, H. Jing
N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos
I.B.M. T.J. Watson Research Center
Yorktown Heights, NY 10598
{raduf,abei,hjing,nanda,xiaoluo, nicolas,roukos}@us.ibm.com

hanyh@eg.ibm.com
Abstract
Entity detection and tracking is a relatively new
addition to the repertoire of natural language
tasks. In this paper, we present a statistical
language-independent framework for identify-
ing and tracking named, nominal and pronom-
inal references to entities within unrestricted
text documents, and chaining them into clusters
corresponding to each logical entity present in
the text. Both the mention detection model
and the novel entity tracking model can use
arbitrary feature types, being able to integrate
a wide array of lexical, syntactic and seman-
tic features. In addition, the mention detec-
tion model crucially uses feature streams de-
rived from different named entity classifiers.
The proposed framework is evaluated with sev-
eral experiments run in Arabic, Chinese and
English texts; a system based on the approach
described here and submitted to the latest Au-
tomatic Content Extraction (ACE) evaluation
achieved top-tier results in all three evaluation
languages.
1 Introduction
Detecting entities, whether named, nominal or pronom-
inal, in unrestricted text is a crucial step toward under-
standing the text, as it identifies the important concep-
tual objects in a discourse. It is also a necessary step for
identifying the relations present in the text and populating
a knowledge database. This task has applications in in-
formation extraction and summarization, information re-
trieval (one can get al hits for Washington/person and not
the ones for Washington/state or Washington/city), data
mining and question answering.
The Entity Detection and Tracking task (EDT hence-
forth) has close ties to the named entity recognition
(NER) and coreference resolution tasks, which have been
the focus of attention of much investigation in the recent
past (Bikel et al, 1997; Borthwick et al, 1998; Mikheev
et al, 1999; Miller et al, 1998; Aberdeen et al, 1995;
Ng and Cardie, 2002; Soon et al, 2001), and have been
at the center of several evaluations: MUC-6, MUC-7,
CoNLL?02 and CoNLL?03 shared tasks. Usually, in com-
putational linguistic literature, a named entity represents
an instance of a name, either a location, a person, an or-
ganization, and the NER task consists of identifying each
individual occurrence of such an entity. We will instead
adopt the nomenclature of the Automatic Content Extrac-
tion program1 (NIST, 2003a): we will call the instances
of textual references to objects or abstractions mentions,
which can be either named (e.g. John Mayor), nominal
(e.g. the president) or pronominal (e.g. she, it). An entity
consists of all the mentions (of any level) which refer to
one conceptual entity. For instance, in the sentence
President John Smith said he has no comments.
there are two mentions: John Smith and he (in the order
of appearance, their levels are named and pronominal),
but one entity, formed by the set {John Smith, he}.
In this paper, we present a general statistical frame-
work for entity detection and tracking in unrestricted text.
The framework is not language specific, as proved by ap-
plying it to three radically different languages: Arabic,
Chinese and English. We separate the EDT task into a
mention detection part ? the task of finding all mentions
in the text ? and an entity tracking part ? the task of com-
bining the detected mentions into groups of references to
the same object.
The work presented here is motivated by the ACE eval-
uation framework, which has the more general goal of
building multilingual systems which detect not only enti-
ties, but also relations among them and, more recently,
events in which they participate. The EDT task is ar-
guably harder than traditional named entity recognition,
because of the additional complexity involved in extract-
ing non-named mentions (nominals and pronouns) and
the requirement of grouping mentions into entities.
We present and evaluate empirically statistical mod-
els for both mention detection and entity tracking prob-
lems. For mention detection we use approaches based on
Maximum Entropy (MaxEnt henceforth) (Berger et al,
1996) and Robust Risk Minimization (RRM henceforth)
1For a description of the ACE program see
http://www.nist.gov/speech/tests/ace/.
(Zhang et al, 2002). The task is transformed into a se-
quence classification problem. We investigate a wide ar-
ray of lexical, syntactic and semantic features to perform
the mention detection and classification task including,
for all three languages, features based on pre-existing sta-
tistical semantic taggers, even though these taggers have
been trained on different corpora and use different seman-
tic categories. Moreover, the presented approach implic-
itly learns the correlation between these different seman-
tic types and the desired output types.
We propose a novel MaxEnt-based model for predict-
ing whether a mention should or should not be linked to
an existing entity, and show how this model can be used
to build entity chains. The effectiveness of the approach
is tested by applying it on data from the above mentioned
languages ? Arabic, Chinese, English.
The framework presented in this paper is language-
universal ? the classification method does not make any
assumption about the type of input. Most of the fea-
ture types are shared across the languages, but there are a
small number of useful feature types which are language-
specific, especially for the mention detection task.
The paper is organized as follows: Section 2 describes
the algorithms and feature types used for mention detec-
tion. Section 3 presents our approach to entity tracking.
Section 4 describes the experimental framework and the
systems? results for Arabic, Chinese and English on the
data from the latest ACE evaluation (September 2003), an
investigation of the effect of using different feature types,
as well as a discussion of the results.
2 Mention Detection
The mention detection system identifies the named, nom-
inal and pronominal mentions introduced in the previous
section. Similarly to classical NLP tasks such as base
noun phrase chunking (Ramshaw and Marcus, 1994), text
chunking (Ramshaw and Marcus, 1995) or named entity
recognition (Tjong Kim Sang, 2002), we formulate the
mention detection problem as a classification problem,
by assigning to each token in the text a label, indicating
whether it starts a specific mention, is inside a specific
mention, or is outside any mentions.
2.1 The Statistical Classifiers
Good performance in many natural language process-
ing tasks, such as part-of-speech tagging, shallow pars-
ing and named entity recognition, has been shown to de-
pend heavily on integrating many sources of information
(Zhang et al, 2002; Jing et al, 2003; Ittycheriah et al,
2003). Given the stated focus of integrating many feature
types, we are interested in algorithms that can easily in-
tegrate and make effective use of diverse input types. We
selected two methods which satisfy these criteria: a linear
classifier ? the Robust Risk Minimization classifier ? and
a log-linear classifier ? the Maximum Entropy classifier.
Both methods can integrate arbitrary types of informa-
tion and make a classification decision by aggregating all
information available for a given classification.
Before formally describing the methods2, we introduce
some notations: let
 	



be the set of pre-
dicted classes,  be the example space and 

be the feature space. Each example  	
		Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 137?140,
New York, June 2006. c?2006 Association for Computational Linguistics
Weblog Classification for Fast Splog Filtering:
A URL Language Model Segmentation Approach
Franco Salvetti?! Nicolas Nicolov!
franco.salvetti@colorado.edu nicolas@umbrialistens.com
?Dept. of Computer Science, Univ. of Colorado at Boulder, 430 UCB, Boulder, CO 80309-0430
!Umbria, Inc., 1655 Walnut Str, Boulder, CO 80302
Abstract
This paper shows that in the context of
statistical weblog classification for splog
filtering based on n-grams of tokens in
the URL, further segmenting the URLs
beyond the standard punctuation is help-
ful. Many splog URLs contain phrases
in which the words are glued together in
order to avoid splog filtering techniques
based on punctuation segmentation and
unigrams. A technique which segments
long tokens into the words forming the
phrase is proposed and evaluated. The re-
sulting tokens are used as features for a
weblog classifier whose accuracy is sim-
ilar to that of humans (78% vs. 76%) and
reaches 93.3% of precision in identifying
splogs with recall of 50.9%.
1 Introduction
The blogosphere, which is a subset of the web and
is comprised of personal electronic journals (we-
blogs) currently encompasses 27.2 million pages
and doubles in size every 5.5 months (Technorati,
2006). The information contained in the blogo-
sphere has been proven valuable for applications
such as marketing intelligence, trend discovery, and
opinion tracking (Hurst, 2005). Unfortunately in the
last year the blogosphere has been heavily polluted
with spam weblogs (called splogs) which are we-
blogs used for different purposes, including promot-
ing affiliated websites (Wikipedia, 2006). Splogs
can skew the results of applications meant to quan-
titatively analyze the blogosphere. Sophisticated
content-based methods or methods based on link
analysis (Gyo?ngyi et al, 2004), while providing ef-
fective splog filtering, require extra web crawling
and can be slow. While a combination of approaches
is necessary to provide adequate splog filtering, sim-
ilar to (Kan & Thi, 2005), we propose, as a pre-
liminary step in the overall splog filtering, a fast,
lightweight and accurate method merely based on
the analysis of the URL of the weblog without con-
sidering its content.
For quantitative and qualitative analysis of the
content of the blogosphere, it is acceptable to elim-
inate a small fraction of good data from analysis
as long as the remainder of the data is splog-free.
This elimination should be kept to a minimum to
preserve counts needed for reliable analysis. When
using an ensemble of methods for comprehensive
splog filtering it is acceptable for pre-filtering ap-
proaches to lower recall in order to improve preci-
sion allowing more expensive techniques to be ap-
plied on a smaller set of weblogs. The proposed
method reaches 93.3% of precision in classifying a
weblog in terms of spam or good if 49.1% of the
data are left aside (labeled as unknown). If all data
needs to be classified our method achieves 78% ac-
curacy which is comparable to the average accuracy
of humans (76%) on the same classification task.
Sploggers, in creating splogs, aim to increase the
traffic to specific websites. To do so, they frequently
communicate a concept (e.g., a service or a prod-
uct) through a short, sometimes non-grammatical
phrase embedded in the URL of the weblog (e.g.,
http://adult-video-mpegs.blogspot.com ) . We
want to build a statistical classifier which leverages
the language used in these descriptive URLs in order
to classify weblogs as spam or good. We built an
initial language model-based classifier on the tokens
of the URLs after tokenizing on punctuation (., -,
137
, /, ?, =, etc.). We ran the system and got an ac-
curacy of 72.2% which is close to the accuracy of
humans?76% (the baseline is 50% as the training
data is balanced). When we did error analysis on the
misclassified examples we observed that many of the
mistakes were on URLs that contain words glued to-
gether as one token (e.g., dailyfreeipod). Had the
words in these tokens been segmented the initial sys-
tem would have classified the URL correctly. We,
thus, turned our attention to additional segmenting
of the URLs beyond just punctuation and using this
intra-token segmentation in the classification.
Training a segmenter on standard available text
collections (e.g., PTB or BNC) did not seem the way
to procede because the lexical items used and the se-
quence in which they appear differ from the usage
in the URLs. Given that we are interested in unsu-
pervised lightweight approaches for URL segmenta-
tion, one possibility is to use the URLs themselves
after segmenting on punctuation and to try to learn
the segmenting (the majority of URLs are naturally
segmented using punctuation as we shall see later).
We trained a segmenter on the tokens in the URLs,
unfortunately this method did not provide sufficient
improvement over the system which uses tokeniza-
tion on punctuation. We hypothesized that the con-
tent of the splog pages corresponding to the splog
URLs could be used as a corpus to learn the seg-
mentation. We crawled 20K weblogs correspond-
ing to the 20K URLs labeled as spam and good
in the training set, converted them to text, tokenized
and used the token sequences as training data for the
segmenter. This led to a statistically significant im-
provement of 5.8% of the accuracy of the splog filter.
2 Engineering of splogs
Frequently sploggers indicate the semantic con-
tent of the weblogs using descriptive phrases?
often noun groups (non-recursive noun phrases) like
adult-video-mpegs. There are different varieties
of splogs: commercial products (especially electron-
ics), vacations, mortgages, and adult-related.
Users don?t want to see splogs in their results
and marketing intelligence applications are affected
when data contains splogs. Existing approaches
to splog filtering employ statistical classifiers (e.g.,
SVMs) trained on the tokens in a URL after to-
kenization on punctuation (Kolari et al, 2006).
To avoid being identified as a splog by such sys-
tems one of the creative techniques that splog-
gers use is to glue words together into longer to-
kens for which there will not be statistical informa-
tion (e.g., businessopportunitymoneyworkathome
is unlikely to appear in the training data while
business, opportunity, money, work, at and home
are likely to have been seen in training). Another ap-
proach to dealing with splogs is having a list of splog
websites (SURBL, 2006). Such an approach based
on blacklists is now less effective because bloghosts
provide tools which can be used for the automatic
creation of a large quantity of splogs.
3 Splog filtering
The weblog classifier uses a segmenter which splits
the URL in tokens and then the token sequence is
used for supervised learning and classification.
3.1 URL segmentation
The segmenter first tokenizes the URLs on punctua-
tion symbols. Then the current URL tokens are ex-
amined for further possible segmentation. The seg-
menter uses a sliding window of n (e.g., 6) charac-
ters. Going from left to right in a greedy fashion the
segmenter decides whether to split after the current
third character. Figure 1 illustrates the processing of
www.dietthatworks.com when considering the to-
ken dietthatworks. The character ??? indicates that
the left and right tri-grams are kept together while
??? indicates a point where the segmenter decides a
break should occur. The segmentation decisions are
d i e ? t t hatworks
d i e t ? t h aatworks
Figure 1: Workings of the segmenter
based on counts collected during training. For ex-
ample, during the segmentation of dietthatworks
in the case of i e t ? t h a we essentially con-
sider how many times we have seen in the training
data the 6-gram ?iettha? vs. ?iet tha?. Certain
characters (e.g., digits) are generalized both during
training and segmentation.
138
3.2 Classification
For the weblog classification a simple Na??ve Bayes
classifier is used. Given a token sequence T =
?t1, . . . , tn?, representing the segmented URL, the
class c? ? C = {spam,good} is decided as:
c? = arg max
c?C
P (c|T ) = arg max
c?C
P (c) ? P (T |c)
P (T )
= arg max
c?C
P (c) ? P (T |c)
= arg max
c?C
P (c) ?
n?
i=1
P (ti|c)
In the last step we made the conditional indepen-
dence assumption. For calculating P (ti|c) we use
Laplace (add one) smoothing (Jurafsky & Martin,
2000). We have also explored classification via sim-
ple voting techniques such as:
a = sgn
n?
i=1
sgn (P (ti|spam) ? P (ti|good))
c? =
{ spam, if a = 1
good, otherwise
Because we are interested in having control over the
precision/recall of the classifier we introduce a score
meant to be used for deciding whether to label a
URL as unknown.
score(T ) =
????
P (spam|T ) ? P (good|T )
P (spam|T ) + P (good|T )
????
If score(T ) exceeds a certain threshold ? we label
T as spam or good using the greater probability of
P (spam|T ) or P (good|T ). To control the presi-
cion of the classifier we can tune ? . For instance,
when we set ? = 0.75 we achieve 93.3% of preci-
sion which implied a recall of 50.9%. An alternate
commonly used technique to compute a score is to
look at the log likelihood ratio.
4 Experiments and results
First we discuss the segmenter. 10,000 spam and
10,000 good weblog URLs and their corresponding
HTML pages were used for the experiments. The
20,000 weblog HTML pages are used to induce the
segmenter. The first experiment was aimed at find-
ing how common extra segmentation beyond punc-
tuation is as a phenomenon. The segmenter was run
on the actual training URLs. The number of URLs
that are additionally segmented besides the segmen-
tation on punctuation are reported in Table 1.
# of # spam # good
splits URLs URLs
1 2,235 2,274
2 868 459
3 223 46
4 77 7
5 2 1
6 4 1
8 3 ?
Total 3,412 2,788
Table 1: Number of extra segmentations in a URL
The multiple segmentations need not all occur on the
same token in the URL after initial segmentation on
punctuations.
The segmenter was then evaluated on a separate
test set of 1,000 URLs for which the ground truth
for the segmentation was marked. The results are
in Table 2. The evaluation is only on segmentation
events and does not include tokenization decisions
around punctuation.
Precision Recall F-measure
84.31 48.84 61.85
Table 2: Performance of the segmenter
Figure 2 shows long tokens which are correctly split.
The weblog classifier was then run on the test set.
The results are shown in Table 3.
cash ? for ? your ? house
unlimitted ? pet ? supllies
jim ? and ? body ? fat
weight ? loss ? product ? info
kick ? the ? boy ? and ? run
bringing ? back ? the ? past
food ? for ? your ? speakers
Figure 2: Correct segmentations
139
accuracy 78%
prec. spam 82%
rec. spam 71%
f-meas spam 76%
prec. good 74%
rec. good 84%
f-meas good 79%
Table 3: Classification results
The performance of humans on this task was also
evaluated. Eight individuals performed the splog
identification just looking at the unsegmented URLs.
The results for the human annotators are given in Ta-
ble 4. The average accuracy of the humans (76%) is
similar to that of the system (78%).
Mean ?
accuracy 76% 6.71
prec. spam 83% 7.57
rec. spam 65% 6.35
f-meas spam 73% 7.57
prec. good 71% 6.35
rec. good 87% 6.39
f-meas good 78% 6.08
Table 4: Results for the human annotators
From an information retrieval perspective if only
50.9% of the URLs are retrieved (labelled as ei-
ther spam or good and the rest are labelled
as unknown) then of the spam/good decisions
93.3% are correct. This is relevant for cases where
a URL splog filter is in cascade followed by, for ex-
ample, a content-based one.
5 Discussion
The system performs better with the intra-token seg-
mentation because the system is forced to guess un-
seen events on fewer occasions. For instance given
the input URL www.ipodipodipod.com in the sys-
tem which segments solely on punctuation both the
spam and the good model will have to guess the
probability of ipodipodipod and the results depend
merely on the smoothing technique.
Even if we reached the average accuracy of hu-
mans we expect to be able to improve the system
further as the maximum accuracy among the human
annotators is 90%. Among the errors of the seg-
menter the most common are related to plural nouns
(?girl?s? vs. ?girls?) and past tense of verbs
(?dedicate?d? vs. ?dedicated?) .
The proposed approach has ramifications for splog
filtering systems that want to consider the outward
links from a weblog.
6 Conclusions
We have presented a technique for determining
whether a weblog is splog based merely on alalyz-
ing its URL. We proposed an approach where we
initially segment the URL in words and then do the
classification. The technique is simple, yet very
effective?our system reaches an accuracy of 78%
(while humans perform at 76%) and 93.3% of preci-
sion in classifying a weblog with recall of 50.9%.
Acknowledgements. We wish to thank Ted Kre-
mer, Howard Kaushansky, Ash Beits, Allen Bennett,
Susanne Costello, Hillary Gustave, Glenn Meuth,
Micahel Sevilla and Ron Woodward for help with
the experiments and comments on an earlier draft.
References
Gyo?ngyi, Zoltan, Hector Garcia-Molina & Jan Pedersen. 2004.
?Combating Web Spam with TrustRank?. Proceedings of the
30th International Conference on Very Large Data Bases
(VLDB).
Matthew Hurst. 2005. ?Deriving Marketing Intelligence from
Online Discussion?. 11th ACM SIGKDD Int. Conf. on
Knowledge Discovery in Data Mining (KDD05), 419-428.
Chicago, Illinois, USA.
Jurafsky, D. & J.H. Martin. 2000. Speech and Language Pro-
cessing. Upper Saddle River, NJ: Prentice Hall.
Min-Yen Kan & Hoang Oanh Nguyen Thi. 2005. ?Fast Web-
page Classification Using URL Features?. 14th ACM in-
ternational conference on Information and Knowledge Man-
agement, 325-326.
Kolari, Pranam, Tim Finin & Anupam Joshi. 2006. ?SVMs for
the Blogosphere: Blog Identification and Splog Detection?.
AAAI Symposium on Computational Approaches to Analyz-
ing Weblogs, 92-99. Stanford.
SURBL. 2006. SURBL ? Spam URI Realtime Blocklists,
http://www.surbl.org
Technorati. 2006. State of the Blogosphere, Febru-
ary 2006 Part 1: On Blogosphere Growth,
technorati.com/weblog/2006/02/81.html
Wikipedia. 2006. Splog (Spam blog),
http://en.wikipedia.org/wiki/Splog
140
