Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 44?48,
Avignon, France, April 23 - 24 2012. c?2012 Association for Computational Linguistics
Visualising Linguistic Evolution in Academic Discourse
Verena Lyding
European Academy of Bolzano-Bozen
verena.lyding@eurac.edu
Ekaterina Lapshinova-Koltunski
Saarland University
e.lapshinova@mx.uni-saarland.de
Stefania Degaetano-Ortlieb
Saarland University
s.degaetano@mx.uni-saarland.de
Henrik Dittmann
European Academy of Bolzano-Bozen
henrik.dittmann@eurac.edu
Christopher Culy
The University of Tu?bingen
christopher.culy@uni-tuebingen.de
Abstract
The present paper describes procedures to
visualise diachronic language changes in
academic discourse to support analysis.
These changes are reflected in the distri-
bution of different lexico-grammatical fea-
tures according to register. Findings about
register differences are relevant for both lin-
guistic applications (e.g., discourse analysis
and translation studies) and NLP tasks (no-
tably automatic text classification).
1 Introduction
The present paper describes procedures to visu-
alise diachronic language changes in academic
discourse with the aim to facilitate analysis
and interpretation of complex data. Diachronic
changes are reflected by linguistic features of reg-
isters under analysis. Registers are patterns of lan-
guage according to use in context, cf. (Halliday
and Hasan, 1989).
To analyse register change, we extract lexico-
grammatical features from a diachronic corpus of
academic English, and visualise our extraction re-
sults with Structured Parallel Coordinates (SPC),
a tool for the visualisation of structured multidi-
mensional data, cf. (Culy et al, 2011).
Our approach is based on the inspection and
comparison of how different features change over
time and registers. The major aim is to deter-
mine and describe tendencies of features, which
might become rarer, more frequent or cluster in
new ways. The amount and complexity of the in-
terrelated data, which is obtained for nine disci-
plines in two time periods (see section 2) makes
the analysis more difficult.
Structured Parallel Coordinates provide a tool
for the compact visual presentation of complex
data. The visualisation of statistical values for
different linguistic features laid out over time and
register supports data analysis as tendencies be-
come apparent. Furthermore, interactive features
allow for taking different views on the data and
focussing on interesting aspects.
2 Data to Analyse
2.1 Features and theoretical background
When defining lexico-grammatical features, we
refer to Systemic Functional Linguistics (SFL)
and register theory, e.g., (Quirk, 1985), (Halliday
and Hasan, 1989) and (Biber, 1995), which are
concerned with linguistic variation according to
contexts of use, typically distinguishing the three
contextual variables of field, tenor and mode of
discourse. Particular settings of these variables
are associated with the co-occurrences of certain
lexico-grammatical features, creating distinctive
registers (e.g., the language of linguistics in aca-
demic discourse). We also consider investiga-
tions of recent language change, observed, e.g.,
by (Mair, 2006), who analyses changes in prefer-
ences of lexico-grammatical selection in English
in the 1960s vs. the 1990s.
As a case study, we show an analysis of
modal verbs (falling into the contextual variable
of tenor), which we group according to (Biber,
1999) into three categories of meaning that rep-
resent three features: obligation, permission and
volition (see Table 1).
2.2 Resources
The selected features are extracted from SciTex,
cf. (Degaetano et al, 2012) and (Teich and
44
categories of meanings (feature) realisation
obligation/necessity (obligaton) can, could, may, etc.
permission/possibility/ability (permission) must, should, etc.
volition/prediction (volition) will, would, shall, etc.
Table 1: Categories of modal meanings for feature extraction
Fankhauser, 2010), an English corpus which con-
tains full English scientific journal articles from
nine disciplines (see Figure 1). The corpus covers
two time periods: the 1970/early 1980s (SaSci-
Tex) and the early 2000s (DaSciTex), and in-
cludes ca. 34 million tokens. Our focus is espe-
cially on the subcorpora representing contact reg-
isters, i.e. registers emerged out of register con-
tact, in our case with computer science: computa-
tional linguistics (B1), bioinformatics (B2), digi-
tal construction (B3), and microelectronics (B4).
COMPUTER
SCIENCE
(A)
LINGUISTICS
(C1)
CO
M
PU
TA
TI
O
NA
L
LI
NG
UI
ST
IC
S
(B
1)
BIOLOGY
(C2)
B
IO-
INFO
RM
ATICS
(B2)
ELECTRICAL
ENGINEERING
(C4)
M
ICRO-
E
LECTRO
NICS
(B4)
MECHANICAL
ENGINEERING
(C3)
DI
G
IT
AL
CO
NS
TR
UC
TI
O
N
(B
3)
Figure 1: Scientific disciplines in the SciTex corpus
SciTex is annotated1 with information on to-
ken, lemma, part-of-speech and sentence bound-
ary, as well as further information on text bound-
ary, register information, etc., and can be queried
in form of regular expressions by the Corpus
Query Processor (CQP), cf. (Evert, 2005).
2.3 Feature Extraction and Analysis
To extract the above described features for the two
time slices (1970/80s and 2000s) and for all nine
registers of SciTex, we elaborate queries, which
include both lexical (based on token and lemma
information) and grammatical (based on part-of-
speech or sentence boundary information) con-
straints.
1Annotations were obtained by means of a dedicated pro-
cessing pipeline (Kermes, 2011).
Annotations on the register information allow
us to sort the extracted material according to spe-
cific subcorpora. This enables the analysis of fea-
tures possibly involved in creating distinctive reg-
isters. Comparing differences and/or common-
alities in the distribution of features for A-B-C
triples of subcorpora (e.g., A-computer science,
B1-computational linguistics, C1-linguistics, cf.
Figure 1), we analyse whether the contact disci-
plines (B-subcorpora) are more similar to com-
puter science (A-subcorpus), the discipline of ori-
gin (C-subcorpus) or distinct from both (A and C).
The two time periods in SciTex (70/80s vs. 2000s)
enable a diachronic analysis. A more fine-grained
diachronic analysis is also possible with the infor-
mation on the publication year annotated in the
corpus.
3 Analysing language changes with SPC
3.1 SPC visualisation
Structured Parallel Coordinates (Culy et al, 2011)
are a specialisation of the Parallel Coordinates
visualisation (cf. (d?Ocagne, 1885), (Inselberg,
1985), (Inselberg, 2009)) for representing mul-
tidimensional data using a two-dimensional dis-
play. Parallel Coordinates place data on vertical
axes, with the axes lined up horizontally. Each
axis represents a separate data dimension and can
hold either categorical or numerical data. Data
points on different axes are related which is indi-
cated by colored lines connecting all data items
belonging to one record.
Targeted to the application to language data,
SPC additionally provide for ordered characteris-
tics of data within and across data dimensions. In
the n-grams with frequencies/KWIC2 implemen-
tations of SPC, ordered axes represent the linear
ordering of words in text.
In our analysis of language change based on
linguistic features, we are interested in two di-
rections of changes across data sets that can be
represented by ordering: changes over time and
2www.eurac.edu/linfovis
45
changes across registers, e.g., from linguistics and
computer science to computational linguistics.
3.2 Adjustments to SPC
For the analysis of linguistic features with SPC,
we start off with the n-grams with frequencies im-
plementation. In analyzing just two time dimen-
sions the ordered aspect of SPC is not as crucial
and a similar analysis could have been done with
Parallel Coordinates. However, the setup of n-
grams with frequencies conveniently provides us
with the combination of categorical and numerical
data dimensions in one display but separated visu-
ally. For our diachronic register analysis, we cre-
ate a subcorpus comparison application where the
feature under analysis as well as some of the cor-
pus data are placed on the unordered categorical
axes, and frequencies for the two time periods are
placed on ordered axes with numerical scales. As
shown in Figure 2 below, unordered dimensions
are followed by ordered dimensions, the inverse
situation to n-grams with frequencies. To visu-
ally support the categorical nature of data on the
first three axes, SPC was adjusted to display the
connecting lines in discrete colors instead of the
default color scale shading from red to blue. To
improve the comparability of values on numerical
axes, a function for switching between compara-
ble and individual scales was added that applies to
all axes right of the separating red line. Figure 2
and 3 present numerical values as percentages on
comparable scales scaled to 100.
3.3 Interactive features for analysis
SPC provide a number of interactive features that
support data analysis. To highlight and accentuate
selected parts of the data, an axis can be put into
focus and parts of axes can be selected. Lines are
colored according to the axis under focus, and fil-
ters apply to the selected portions of axes, with the
other data rendered in gray. Users can switch be-
tween discrete colors and scaled coloring of con-
necting lines. The scales of numerical axes can be
adjusted interactively, as described above. Hover-
ing over a determined connecting line brings it out
as a slightly wider line and gives a written sum-
mary of the values of that record.
4 Interpreting Visualisation Results
Visualised structures provided by SPC supply us
with information on development tendencies, and
thus, deliver valuable material for further interpre-
tation of language variation across registers and
time.
To analyse the frequencies of modal meanings
(see Table 1) for A-B-C triples of subcorpora, we
use the subcorpus comparison option of SPC. The
interactive functionality of SPC allows us to focus
on different aspects and provides us with dynam-
ically updated versions of the visualisation.
First, by setting focus on the axis of modal
meanings, the visualisation in Figure 2 shows di-
achronic changes of the modal meanings from the
1970/80s to the early 2000s. In both time periods
the permission (blue) meaning is most prominent
and has considerably increased over time. The
volition (green) and obligation (orange) meanings
are less prominent and we can observe a decrease
of volition and a very slight decrease of obliga-
tion.
Second, by setting the axis of the registers into
focus and selecting the disciplines one by one, we
can explore whether there are changes in the use
of modal meanings between the A register, the
contact registers (B), and the respective C regis-
ters. In Figure 3, for example, computer science
and biology have been selected (gray shaded) on
the ?disciplines? axis. For this selection, the struc-
tures starting from the ?registers? axis represent
(1) computer science (blue) being the A regis-
ter, (2) biology (green) from the C registers, and
(3) bioinformatics (orange) from the B registers
as the corresponding contact register. In terms
of register changes, Figure 3 shows that bioin-
formatics differs in the development tendencies
(a) of permission from biology and computer sci-
ence (less increase than the former, more increase
than the latter) and (b) of obligation from biology
(decrease for biology, whereas nearly stable for
bioinformatics and computer science).
5 Conclusion and Future Work
The results described above show that Structured
Parallel Coordinates provides us with a means for
the interactive inspection of complex data sets fa-
cilitating our diachronic register analysis. The vi-
sualisation allows to gain an overview and detect
tendencies by accomodating a complex set of data
in one display (nine registers over two time peri-
ods for three meanings).
The interactive features of SPC give the possi-
bility to put different aspects of the data into fo-
46
Figure 2: Modal meanings in SciTex in the 1970/80s and 2000s
Figure 3: Modal meanings in computer science (A-subcorpus; blue), bioinformatics (from B-subcorpus; orange)
and biology (from C-subcorpus; green)
47
cus, and thus to successively zoom into specific
subsets of the data for detailed analyses. In this
way, we can determine general tendencies (e.g.,
increase of permission over time) or provide de-
tailed analyses for certain linguistic features and
registers by selecting subparts of the data and by
highlighting different data dimensions (e.g., com-
paring changes between different registers).
Future work comprises to use the data obtained
from the corpus to feed several different SPC vi-
sualisations. For example, the data presented in
Figure 2 can also be layed out to place values for
registers instead of values for time periods on the
numerical axes.
Future analyses will focus on inspecting fur-
ther tendencies in the feature development for the
three contextual variables mentioned in 2.1, e.g.,
verb valency patterns for field or conjunctive re-
lations expressing cohesion for mode. We also
aim at analysing several linguistic features at the
same time to possibly detect feature sets involved
in register variation of contact registers. Addition-
ally, a more fine-grained diachronic analysis ac-
cording to the publication years, which are anno-
tated in the corpus, might also prove to be useful.
From a technical point of view, the issue with
fully overlapping lines being displayed in one
color only will be tackled by experimenting with
semi-transparent or stacked lines. Furthermore,
SPC should in the future be expanded by a func-
tion for restructuring the underlying data to cre-
ate different layouts. This could also include the
merging of axes with categorical values (e.g., axes
registers and disciplines in Figure 2 above). Fur-
thermore on each data dimension a ?summary?
category could be introduced that would repre-
sent the sum of all individual values, and would
provide an extra point of reference for the analy-
sis. For interactive data analysis, support could be
provided to select data items based on crossings
or declination of their connecting lines.
References
Douglas Biber. 1995. Dimensions of Register Varia-
tion. A Cross-linguistic Comparison. Cambridge:
Cambridge University Press.
Douglas Biber. 1999. Longman Grammar of Spoken
and Written English. Harlow: Pearson ESL.
Chris Culy, Verena Lyding, and Henrik Dittmann.
2011. Structured Parallel Coordinates: a visualiza-
tion for analyzing structured language data. In Pro-
ceedings of the 3rd International Conference on
Corpus Linguistics, CILC-11, April 6-9, 2011, Va-
lencia, Spain, 485?493.
Stefania Degaetano-Ortlieb, Hannah Kermes, Ekate-
rina Lapshinova-Koltunski and Elke Teich. 2012.
SciTex ? A Diachronic Corpus for Analyzing the
Development of Scientific Registers. In: Paul Ben-
nett, Martin Durrell, Silke Scheible & Richard J.
Whitt (eds.), New Methods in Historical Corpus
Linguistics. CLIP, Vol. 2, Narr: Tu?bingen.
Stefan Evert. 2005. The CQP Query Language Tuto-
rial. IMS, Universita?t Stuttgart.
M.A.K. Halliday and Ruqaiya Hasan. 1989. Language,
context and text: Aspects of language in a social
semiotic perspective. OUP.
Alfred Inselberg. 2009. Parallel Coordinates: VISUAL
Multidimensional Geometry and its Applications.
New York: Springer.
Alfred Inselberg. 1985. The plane with parallel coor-
dinates. The Visual Computer 1(2), pp. 69?91.
Hannah Kermes. 2011. Automatic corpus creation.
Manual. Institute of Applied Linguistics, Transla-
tion and Interpreting, Universita?t des Saarlandes,
Saarbru?cken.
Christian Mair. 2006. Twentieth-Century English: His-
tory, Variation and Standardization. Cambridge:
Cambridge University Press.
Maurice d?Ocagne. 1885. Coordonne?es Paralle`les et
Axiales: Me?thode de transformation ge?ome?trique
et proce?de? nouveau de calcul graphique de?duits de
la conside?ration des coordonne?es paralle`lles. Paris:
Gauthier-Villars.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech
and Jan Svartvik. 1985. A comprehensive grammar
of the English language. Harlow: Longman
Elke Teich and Peter Fankhauser. 2010. Exploring a
corpus of scientific texts using data mining. In:
Gries S., S. Wulff and M. Davies (eds), Corpus-
linguistic applications - Current studies, new direc-
tions. Rodopi, Amsterdam and New York, pp. 233?
247.
48
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 59?68,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Scientific registers and disciplinary diversification:
a comparable corpus approach
Elke Teich
Universita?t des Saarlandes
e.teich@mx.uni-saarland.de
Stefania Degaetano-Ortlieb
Universita?t des Saarlandes
s.degaetano@mx.uni-saarland.de
Hannah Kermes
Universita?t des Saarlandes
h.kermes@mx.uni-saarland.de
Ekaterina Lapshinova-Koltunski
Universita?t des Saarlandes
e.lapshinova@mx.uni-saarland.de
Abstract
We present a study on linguistic con-
trast and commonality in English scien-
tific discourse on the basis of a mono-
lingually comparable corpus. The focus
is on selected scientific disciplines at the
boundaries to computer science (compu-
tational linguistics, bioinformatics, digital
construction, microelectronics). The data
basis is the English Scientific Text Cor-
pus (SCITEX) which covers a time range
of roughly thirty years (1970/80s to early
2000s). In particular, we investigate the
disciplinary diversification/relatedness of
scientific research articles in terms of reg-
ister. Our results are relevant for research
on multilingually comparable corpora as
used in machine translation and related re-
search, since they shed new light on the
notion of ?comparablity?.
1 Introduction: Motivation and Goals
In the context of statistical machine translation,
comparable corpora are typically bilingual, the-
matically similar corpora being utilized to extract
translation equivalents to enrich translation mod-
els. These have proved to be useful, especially for
technically specialized texts or for low resource
languages where parallel corpora are rare (Chiao
and Zweigenbaum (2002); Babych et al (2007)).
The overarching goal of the paper is to provide
evidence that the notion of comparability com-
monly used in that context is rather coarse and
misses important aspects of linguistic variation.
We report on a set of experiments in which a
monolingually comparable corpus is studied. The
corpus contains specialized, technical texts from
nine scientific disciplines, related to each other by
?interdisciplines? (such as computer science - lin-
guistics - computational linguistics) (cf. Section 2
for details). Our study establishes the linguistic
differences and commonalities between the disci-
plines considered on the basis of the concept of
register, i.e., language variation according to situ-
ational context. Situational context is convention-
ally described in terms of field, tenor and mode of
discourse (Quirk et al, 1985). It has been shown
in numerous corpus-linguistic studies that particu-
lar situational settings have specific linguistic cor-
relates at the level of lexico-grammar in the sense
of clusters of lexico-grammatical features that oc-
cur non-randomly (see notably the work by Biber
and colleagues, e.g., Biber (1988, 1993); Biber
et al (1999); Biber (2006, 2012)). Collectively,
the linguistic features associated with field, tenor
and mode then give rise to registers. More specif-
ically, field of discourse relates to the topic of a
discourse and is realized lexico-grammatically in
functional verb classes (e.g., activity, communica-
tion, etc.) with corresponding arguments (e.g., Ac-
tor, Goal, Medium, etc.) and adjunct types (e.g.,
Time, Place, Manner, etc.). Tenor of discourse re-
lates to the roles and attitudes of the participants in
a discourse and is realized lexico-grammatically
in mood, modality as well as stance expressions.
Mode of discourse relates to the presentational
function of language and is realized in Theme-
Rheme and Given-New constellations. A register
is then characterized by particular distributions of
lexico-grammatical features according to a given
contextual configuration.
Apart from exhibiting differences in field, tenor
and mode, scientific texts are associated with par-
ticular discourse ?styles? such as technicality, ab-
stractness or informational density, which may
again be linguistically realized in different ways
and to different degrees across disciplines. Fur-
thermore, in a highly dynamic social domain, such
as the scientific one, both registers and discourse
styles are relatively versatile and subject to change
(cf. Ure (1971, 1982)). This may, for instance,
59
affect conventional phraseology. Finally, register
and stylistic features may be distributed unevenly
across document parts, thus giving rise to varia-
tion according to document structure. In order to
arrive at a comprehensive picture of the linguistic
construal of disciplinarity, we thus need to con-
sider the linguistic encodings according to register
and the linguistic realization of discursive styles as
well as take into account the inherently dynamic
nature of scientific discourse.
Relating this back to the notion of comparabil-
ity, the concept of register may thus provide the
basis for a fine-grained description of comparabil-
ity, as it acknowledges the multi-dimensional na-
ture of linguistic variation.
Our methodology is informed by three sources:
corpus linguistics, linguistic theory and data min-
ing. Standard corpus methods are employed for
the quantification of instances of linguistic fea-
tures that are considered to be relevant indicators
of variation across scientific disciplines and may
be expected to significantly contribute to differ-
ences in language use across disciplines. The the-
oretical basis is provided by Systemic Functional
Linguistics (SFL; Halliday (2004)). The reason
for choosing SFL to inform analysis is its model
of association of contextual variables with lexico-
grammatical domains (cf. above on the notion of
register).
In contrast to other corpus-based studies on reg-
ister, our goal is not to uncover dimensions of vari-
ation or to discover text classes (as e.g. in Biber et
al?s work). The texts in our corpus are taken from
38 journals from nine disciplines (for details see
Section 2) and the text classes are thus extrinsi-
cally defined. We can then think of analysis as a
task of text classification, where we test whether
the extrinsically defined classes have distinctive
linguistic correlates and if so, how well the classes
are distinguished linguistically and which features
contribute most to their distinction. To this end,
we employ data mining techniques, in particular
automatic text classification (see Section 3 for de-
tails). A similar approach to the one developed
here, also working on linguistic variation in the
scientific domain, has been proposed earlier by
Argamon et al (2008). There is related work
in translation studies by Baroni and Bernardini
(2006) and Volansky et al (2011), which uses au-
tomatic text classification to describe the specific
properties of translations (?translationese?). The
earliest work, to our knowledge, combining SFL
with text classification is Whitelaw and Patrick?s
work on spam detection (Whitelaw and Patrick,
2004).
2 Corpus
2.1 Corpus Design and Pre-processing
We have built a corpus composed of English sci-
entific research articles ? the English Scientific
Text Corpus (SCITEX; cf. Teich and Fankhauser
(2010) and Degaetano-Ortlieb et al (forthcom-
ing)) ? that covers nine scientific domains and
amounts to approx. 34 million tokens, drawn from
38 sources. SCITEX contains full journal arti-
cles from two time periods, the 1970s/early 1980s
(SASCITEX) and the early 2000s (DASCITEX). We
selected at least two different journals for each dis-
cipline in both time slices. As our focus is on se-
Figure 1: Scientific disciplines in the SCITEX cor-
pus
lected scientific domains at the boundaries to com-
puter science and some other discipline, SCITEX
has a three-way partition: (1) A-subcorpus: com-
puter science, (2) B-subcorpus: computational lin-
guistics, bioinformatics, digital construction and
microelectronics, and (3) C-subcorpus: linguis-
tics, biology, mechanical engineering and elec-
trical engineering, as shown in Figure 1. In the
present paper, we are mainly interested in the lin-
guistic evolution of the inter-/transdisciplinary do-
mains represented by the B-subcorpus, as these
are the ones that have emerged in the given time
frame (1970s/80s to present). We term these do-
mains contact disciplines, since they have come
about through contact between two existing dis-
60
ciplines (here: computer science and another es-
tablished discipline represented in the A and C
subcorpora, which we term seed disciplines). The
main question we are interested in is whether the
seed and contact disciplines have clearly distin-
guishable linguistic correlates in terms of register.
The text sources for SCITEX are full academic
articles in the form of PDF files. These files were
converted to plain text using an existing commer-
cial software including optical character recogni-
tion (OCR).
In further processing we follow the common
practices in corpus linguistics by (a) accounting
for relevant metadata (e.g., author, title, jour-
nal, year of publications) and document structure
(e.g., abstract, conclusion), and (b) using stan-
dard tools for preprocessing (e.g., tokenization,
tagging, lemmatization, etc.). For corpus query,
we employ the Corpus Query Processor (CQP)
(CWB; Evert, 2004) which works on the basis of
regular expressions. Utilities of CQP allow for the
extraction of distributional information according
to the annotated metadata and document structure.
3 Methods of Analysis
We carry out a diachronic analysis comparing the
two time slices (1970s/80s vs. 2000s) represented
in the SCITEX corpus, aiming to provide answers
to the following questions:
1. How well are the individual disciplines dis-
tinguished?
2. How distinct are the contact disciplines from
their seed disciplines?
Thus, analysis involves comparisons along the
temporal and the disciplinary dimensions.
The hypothesis we have about the outcomes of
our analysis is that disciplines will be better dis-
tinguished from one another over time, including
the contact disciplines, reflecting a process of di-
versification within scientific writing over time.
3.1 Feature Selection
In the first step of analysis we need to determine
which features to investigate. These should be fea-
tures that bring out relevant and significant con-
trasts along the dimensions considered (time, dis-
cipline). For the choice of features potentially
distinguishing individual (scientific) registers, we
draw on SFL?s model of register variation in which
the contextual parameters of field, tenor and mode
are associated with particular lexico-grammatical
domains. Since we want to cover all three con-
textual parameters, we choose at least one fea-
ture for each. For field, we analyze functional
verb classes as well as PoS-patterns that are poten-
tially terminology-forming (e.g. noun-noun struc-
tures); for tenor, we analyze modal verbs and for
mode we analyze theme type as well as conjunc-
tive cohesive relations. As another feature, we an-
alyze n-grams on the basis of PoS combinations
(rather than words), since we have seen in a previ-
ous study that they may be involved in processes
of conventionalization (Kermes and Teich, 2012).
Additionally, on an abstract level, scientific
writing is a highly informational production that is
characterized by technicality, information density
and abstractness (cf. Halliday and Martin (1993)).
Among the linguistic features realizing these prop-
erties are a relatively low type-token ratio (techni-
cality), a relatively high lexical density and low
grammatical intricacy (information density) and
the frequent use of nominal categories (nouns, ad-
jectives) (abstractness).
Table 1 displays the features considered in the
analysis together with their associated contextual
variables and/or abstract discourse properties they
instantiate. Features are extracted from the cor-
pus with CQP. For example, simple queries com-
bine part-of-speech and concrete lemmas (e.g.,
[pos=?MD? & lemma=?must|should?]; for modal
verbs). More complex queries work with posi-
tional attributes, linguistic annotations and lists
(e.g., < s>[conj & lemma!=$modal-adverbs]... as
part of the extraction of textual Theme, which is
realized in English as the first constituent in the
clause).
3.2 Feature Evaluation
We employ statistical and machine learning meth-
ods to measure (a) how much individual features
contribute to a possible distinction and (b) how
well corpora are distinguished by these features.
We employ classification techniques by using fea-
ture ranking (Information Gain) to determine the
relative discriminatory force of features, and su-
pervised machine learning (decision trees and sup-
port vector machines) to distinguish between the
scientific registers in SCITEX. For these steps we
use the WEKA data mining platform (Witten and
Eibe, 2005).
61
contextual parameter/ feature category feature subcategory
abstract discourse property
FIELD
term patterns NN-of-NN, N-N, ADJ-N
verb classes
activity (e.g., make, show)
aspectual (e.g., start, end)
causative (e.g., let, allow)
communication (e.g., note, describe)
existence (e.g., exist, remain)
mental (e.g., see, know)
occurrence (e.g., change, grow)
TENOR modality
obligation/necessity (e.g., must)
permission/possibility/ability (e.g., can)
volition/prediction (e.g., will)
MODE
theme
experiential theme (e.g, The algorithm...)
interpersonal theme (e.g., Interestingly...)
textual theme (e.g., But...)
additive (e.g., and, furthermore)
conjunctive adversative (e.g., nonetheless, however)
cohesive relations causal (e.g., thus, for this reason)
temporal (e.g., then, at this point)
TECHNICALITY type-token ratio STTR
lexical vs. function words no. of lexical PoS categories
INFORMATION DENSITY
lexical density lexical items per clause/sentence
grammatical intricacy
clauses per sentence
wh-words per sentence
sentence length
ABSTRACTNESS PoS distribution no. of nominal vs. verbal categories
CONVENTIONALIZATION
n-grams on PoS basis 2-to-6-grams overall/per section
length of sections tokens per section
Table 1: Features used in analysis
4 Results and Interpretation
Our analysis addresses the question of how dis-
tinctive the subcorpora in SCITEX are comparing
the productions of the 1970/80s with those of the
early 2000s. Considering the diachronic perspec-
tive, we expect to encounter a clearer separation of
individual disciplines overall reflecting a process
of diversification within scientific writing.
The analysis has two parts: First, we calculate
Information Gain of the top twenty features, to see
which features are the most discriminatory ones
across disciplines. Second, we apply automatic
classification, to see how well the subcorpora are
distinguished on the basis of these features.
Table 2 shows the twenty most discriminatory
features for the 70/80s across all subcorpora. The
five highest ranking features are associated with
field (NN: IGain 0.39, LEX: IGain 0.36, commu-
nication verbs: IGain 0.31) and mode (WL: IGain
0.33, LEX/C: IGain 0.32). In the mid range, we
find some tenor features and in the lower range
some other field features as well as document
structure features.
When we compare these results with the ones
for the early 2000s (see Table 3), three main ob-
servations can be made. First, features become
much more pronounced, the IGain values rising
substantially for the top 20 features (1970s/80s
are in the range of 0.23 to 0.39, 2000s are in
the range of 0.31 to 3.1). This includes the nine
features that are identical across SASCITEX and
DASCITEX: existence and communication verbs
as well as adj-n term pattern for field, obliga-
tion modals for tenor, word and sentence length
as well as lexical words per clause for mode, bi-
grams for conventionalization, and length of main
part for document structure, all become more pro-
nounced in DASCITEX (higher IGains) and thus
contribute more to the distinction between disci-
plines. The second observation is that while in
SASCITEX only bi-grams ranges among the top 20
features, in DASCITEX we encounter an increase
in the contribution of gram-based features to the
DASCITEX-internal distinction.1 This may point
to the greater role of conventionalized language in
the distinction between disciplines over time. Ter-
minological studies based on n-grams might indi-
cate a thematic comparability of disciplines. Con-
sider one of the key concepts in computer science,
?algorithm?. The distribution (per million) across
the nine disciplines in DASCITEX varies greatly:
1Note again that in our analysis, n-grams are based on
parts-of-speech, not words.
62
feature IGain contextual parameter discourse property
NN 0.3931 field technicality, abstractness
LEX 0.3647 field technicality
communication 0.3119 field
mental 0.2526 field
existence 0.2372 field
ADV 0.2282 field abstractness
adj-n pattern 0.2253 field technicality
volition 0.3184 tenor
permission 0.2709 tenor
MD 0.2679 tenor
obligation 0.249 tenor
WL 0.3326 mode information density
LEX/C 0.3238 mode information density
SL 0.2974 mode information density
clauses/S 0.287 mode information density
additive 0.2574 mode
WH/S 0.2504 mode information density
bi-grams 0.2382 conventionalization
main 0.2301 document structure
introduction 0.2257 document structure
Table 2: Feature ranking for the 70/80s (SASCITEX): Top 20 features
feature IGain contextual parameter discourse property
existence 0.3987 field
activity 0.3677 field
communication 0.3636 field
STTR 0.3582 field technicality
adj-n pattern 0.3441 field technicality
obligation 0.3548 tenor
LEX/C 3.0803 mode information density
SL 0.5567 mode information density
WL 0.51 mode information density
experiential-theme 0.344 mode
causal 0.3302 mode
main 0.5324 document structure
abstract 0.4981 document structure
n-grams main 0.4925 conventionalization
bi-grams 0.3886 conventionalization
n-grams 0.3706 conventionalization
n-grams abstr 0.3609 conventionalization
n-grams 4 0.3287 conventionalization
n-grams 3 0.3209 conventionalization
n-grams intro 0.3115 conventionalization
Table 3: Feature ranking for the early 2000s (DASCITEX): Top 20 features
computer science (3427), microelectronics (1965),
bioinformatics (1913), digital construction (1735),
computational linguistics (1124), electrical engi-
neering (955), mechanical engineering (129), bi-
ology (59) and linguistics (51). When we look at
the top frequent token n-grams in which algorithm
participates, we find, for example, ?approximation
algorithm? which is mostly shared between com-
puter science, the contact discipines and electrical
engineering, ?learning algorithms? appears prac-
tically everywhere, and ?alignment algorithm? is
almost only mentioned in computational linguis-
tics and bioinformatics (with a few occurrences
in computer science and one in biology). The
stylistics across the disciplines is also notewor-
thy: pure stylistic tri-grams, such as the highly
frequent ?in order to?, ?the number of?, ?based on
the?, ?as shown in?, etc., are also good discrimi-
nators between different disciplines (cf. Kermes
and Teich (2012)). Finally, at the levels of con-
textual and discourse properties, it can be noted
that features associated with information density
become better discriminators between disciplines
in the 2000s having high IGain values, while tenor
features step back decreasing in number, tending
towards greater uniformity (only one tenor feature
(obligation modals) in the top 20 features in the
2000s compared to four in the 70s/80s).
To see how these data are reflected according to
disciplines, we perfom classification for both cor-
63
A B1 B2 B3 B4 C1 C2 C3 C4 total accuracy in %
A 108 2 11 25 1 0 4 6 45 202 53.47
B1 3 22 22 19 7 26 4 9 13 125 17.60
B2 10 21 142 55 30 8 60 60 71 457 31.07
B3 16 24 52 121 32 7 17 37 55 361 33.52
B4 1 4 32 27 91 4 36 45 32 272 33.46
C1 2 24 16 8 1 154 4 6 4 219 70.32
C2 3 6 70 16 22 2 358 30 28 535 66.92
C3 10 10 60 45 44 6 37 137 39 388 35.31
C4 52 25 60 49 39 2 25 24 248 524 47.33
A: Computer Science, B1: Computational Linguistics, B2: Bioinformatics, B3: Digital Construction, B4: Microelectronics,
C1: Linguistics, C2: Biology, C3: Mechanical Engineering, C4: Electrical Engineering
Table 4: Confusion matrix with decision tree for the 70/80s (SASCITEX)
A B1 B2 B3 B4 C1 C2 C3 C4 total accuracy in %
A 156 0 3 4 0 1 1 0 37 202 77.23
B1 1 26 23 11 7 27 3 12 15 125 20.80
B2 2 2 274 47 13 4 32 37 46 457 59.96
B3 8 1 72 156 21 3 16 24 60 361 43.21
B4 0 1 14 8 158 1 49 26 15 272 58.09
C1 2 11 12 0 0 183 0 5 6 219 83.56
C2 2 0 28 4 12 0 463 9 17 535 86.54
C3 3 4 53 18 22 2 40 213 33 388 54.90
C4 30 2 41 25 12 1 24 12 377 524 71.95
A: Computer Science, B1: Computational Linguistics, B2: Bioinformatics, B3: Digital Construction, B4: Microelectronics,
C1: Linguistics, C2: Biology, C3: Mechanical Engineering, C4: Electrical Engineering
Table 5: Confusion matrix with SVM for the 70/80s (SASCITEX)
A B1 B2 B3 B4 C1 C2 C3 C4 total accuracy in %
A 201 1 0 9 7 1 0 2 9 230 87.39
B1 4 97 4 19 1 8 1 0 3 137 70.80
B2 5 0 269 14 6 0 18 6 1 319 84.33
B3 5 3 8 168 8 0 6 30 14 242 69.42
B4 2 2 10 17 156 0 8 9 1 205 76.10
C1 1 11 6 3 0 90 0 0 0 111 81.08
C2 0 0 7 2 2 1 335 3 1 351 95.44
C3 4 1 7 23 6 0 15 229 18 303 75.58
C4 18 2 3 42 7 0 4 34 113 223 50.67
A: Computer Science, B1: Computational Linguistics, B2: Bioinformatics, B3: Digital Construction, B4: Microelectronics,
C1: Linguistics, C2: Biology, C3: Mechanical Engineering, C4: Electrical Engineering
Table 6: Confusion matrix with SVM for the early 2000s (DASCITEX)
pora (SASCITEX and DASCITEX), first, with deci-
sion trees, as they are based on Information Gain,
and second, with support vector machines (SVMs),
as they are used for text categorization tasks with
many relevant features achieving very good results
(cf. Joachims (1998)). Classification is performed
on all features with 10 fold cross-validation. Ta-
ble 4 shows the confusion matrix for all subcor-
pora for the 70/80s and classification accuracy for
each subcorpus achieved by decision tree. The
overall accuracy is 44.79% only, the correctly clas-
sified texts lying on the main diagonal of the ma-
trix.
The confusion matrix produced by SVM is
shown in Table 5, with an overall accuracy of
65.07%. Apart from computational linguistics
(B1), accuracy goes up by about 10% for digi-
tal contruction (B3) and linguistics (C1) and about
25-30% for the other subcorpora compared to de-
cision tree. Accuracy with SVM for the contact
disciplines (B1-B4) ranges from 20-60% and is
much lower than the accuracy achieved for the
seed disciplines (A and C1-C4) with around 54-
86%. Thus, the contact disciplines are not clearly
separated from the seed disciplines. Considering,
for instance the triple A-B1-C1, we can see that
more texts belonging to computational linguistics
(B1) are classified into linguistics (C1) than into
computational linguistics (27 texts in C1 vs. 26 in
B1), i.e., texts in B1 seem to be quite similar to
64
B1 vs A B2 vs A B3 vs A B4 vs A
WL 0.629 WL 0.501 WL 0.399 LEX 0.883
STTR 0.509 LEX 0.355 LEX 0.331 WL 0.763
LEX 0.372 causal 0.334 n-grams 6 0.265 STTR 0.574
ADJ 0.261 n-grams 6 0.306 STTR 0.258 causal 0.560
VV 0.230 STTR 0.303 clauses/S 0.202 NN 0.458
n-grams 6 0.205 n-grams 4 0.284 adj-n-n 0.168 additive 0.440
causal 0.187 temporal 0.283 causal 0.160 temporal 0.433
types 0.174 n-grams 5 0.282 NN 0.13 mental 0.416
adj-c-adj-n 0.145 ADJ 0.273 n-grams 4 0.118 commun. 0.379
introduction 0.129 causative 0.197 ADJ 0.114 n-grams 4 0.364
B1 vs C1 B2 vs C2 B3 vs C3 B4 vs C4
clauses/S 0.230 NN 0.269 LEX/S 0.260 LEX 0.469
ADV 0.204 MD 0.264 main 0.146 VV 0.311
LEX/C 0.196 WH 0.198 n-grams main 0.132 WL 0.309
NN 0.179 permission 0.178 introduction 0.127 main 0.153
WH/S 0.122 volition 0.166 causative 0.114 NN 0.148
LEX 0.120 WL 0.147 exper-theme 0.113 introduction 0.142
occurrence 0.119 SL 0.145 obligation 0.087 LEX/S 0.115
commun. 0.112 WH/S 0.137 n-grams intro 0.086 n-grams main 0.096
MD 0.110 LEX 0.104 aspectual 0.081 causal 0.093
n-grams abstr 0.108 LEX/C 0.098 LEX/C 0.077 n-grams intro 0.088
A: Computer Science, B1: Computational Linguistics, B2: Bioinformatics, B3: Digital Construction, B4: Microelectronics,
C1: Linguistics, C2: Biology, C3: Mechanical Engineering, C4: Electrical Engineering
Table 7: Feature ranking with IGain for the 70/80s (SASCITEX): Top 20 features contact vs seed disci-
plines
B1 vs A B2 vs A B3 vs A B4 vs A
WL 0.694 WL 0.701 WL 0.567 WL 0.791
STTR 0.631 main 0.680 causal 0.488 STTR 0.615
SL 0.441 STTR 0.678 STTR 0.385 VV 0.289
types 0.402 n-grams main 0.634 temporal 0.347 main 0.233
causal 0.237 causal 0.621 n-grams 4 0.345 causal 0.230
n-grams 6 0.217 n-grams 4 0.577 n-grams 0.319 LEX 0.21
n-n 0.192 n-grams 0.552 n-grams 5 0.318 mental 0.196
adj-n 0.171 abstract 0.537 n-grams main 0.282 temporal 0.190
adversative 0.128 bi-grams 0.521 LEX 0.280 n-of-n 0.189
adj-c-adj-n 0.125 introduction 0.487 bi-grams 0.262 aspectual 0.144
B1 vs C1 B2 vs C2 B3 vs C3 B4 vs C4
occurrence 0.264 SL 0.566 WL 0.156 VV 0.436
adj-adj-n 0.193 abstract 0.518 VV 0.139 WL 0.410
ADV 0.189 n-grams abstr 0.505 obligation 0.100 LEX/C 0.329
ADJ 0.137 main 0.412 LEX/C 0.100 ADV 0.243
NN 0.128 introduction 0.353 n-grams 5 0.097 n-grams 3 0.181
types 0.123 n-grams main 0.344 MD 0.088 LEX/S 0.162
LEX/C 0.123 n-grams intro 0.321 ADJ 0.075 activity 0.154
main 0.118 WH 0.204 aspectual 0.064 n-grams 0.147
commun. 0.107 MD 0.202 SL 0.061 STTR 0.135
abstract 0.107 WH/S 0.192 LEX/S 0.059 abstract 0.127
A: Computer Science, B1: Computational Linguistics, B2: Bioinformatics, B3: Digital Construction, B4: Microelectronics,
C1: Linguistics, C2: Biology, C3: Mechanical Engineering, C4: Electrical Engineering
Table 8: Feature ranking with IGain for the early 2000s (DASCITEX): Top 20 features contact vs seed
disciplines
texts in C1 in terms of the features investigated.
In order to check the separation of disciplines
over time, we need to compare classification re-
sults across SASCITEX and DASCITEX. We again
apply SVM, which returns an overall accuracy of
78.17%.2 Comparing the values for the individual
2Decision tree performed poorly again in comparison
subcorpora across SASCITEX and DASCITEX, we
can observe that accuracies are now much higher
for all subcorpora. Considering the contact disci-
plines, they have clearly gained distinctiveness in
the 2000s in comparison to the 1970/80s, as texts
in B1-B4 are classified correctly 69% to 84% of
achieving an accuracy of 57.24% only.
65
the time (instead of 20-60% in the 1970/80s).
In summary, the classification results match the
results obtained by feature ranking, which have
shown that the top 20 features increased discrim-
inatory force over time. This is reflected by a
higher classification accuracy overall and for the
subcorpora.3 The discriminatory force of features
in the 1970s/80s instead, was not strong enough to
clearly separate disciplines.
To see whether there are any particular features
involved in the differentiation of the contact dis-
ciplines in particular vis a` vis computer science
on the one hand and the other seed disciplines
on the other hand, we inspect the confusion ma-
trix as well as the IGains of each B vs. A and
each B vs. the respective C, both for SASCITEX
and DASCITEX. In the comparison to computer
science (A), we can see that the confusion ma-
trixes produced with SVM (cf. Table 5 and 6)
show few texts that are misclassified from the con-
tact disciplines (Bs) into computer science (A) for
both time slices. Thus, the features employed dis-
tinguish Bs from A quite well. Considering the
IGain values (see Table 7 and 8 for the top 10 fea-
tures), besides computational linguistics (B1; rel-
atively low classification accuracy of 20% in the
70/80s), the contact disciplines have the following
features in common: word length (WL), STTR,
causal verbs in the top 10 as well as four-grams,
lexical words (LEX) and temporal conjunctions in
the top 20 features. Except lexical words (LEX),
all features have a higher IGain in the 2000s. In
the comparison to the other seed discipines (Cs),
the confusion matrixes show more misclassifica-
tions of Bs into Cs. Considering the IGain val-
ues there are no tendencies uniformly applying to
the contact disciplines (Bs). They rather show
individual tendencies for each pair (B1 vs. C1,
B2 vs. C2, B3 vs. C3, B4 vs. C4). Features
that contribute to a better classification diachroni-
cally lie in the following parameters: (a) field (oc-
currence, term-patterns, ADV) for computational
linguistics (B1), (b) document structure (abstract,
main, intro), information density (SL) and conven-
tionalization (n-grams abstract) for bioinformatics
(B2), (c) information density (WL) and technical-
ity (VV) for digital construction (B3) and micro-
electronics (B4).
3There are only two exceptions: C1 (linguistics) goes
slightly down (around 2.5%), C4 (electrical engineering)
goes down by over 20% to 50.67% accuracy, i.e., it is not
really distinguishable any more.
5 Summary and Conclusions
We have looked at disciplinary linguistic diversifi-
cation in English scientific writing in terms of reg-
ister, discourse styles and document structure. The
results of our analysis provide evidence of major
motifs of development in scientific writing over
time, showing dynamicity over a time span of only
thirty years. Diversification over time is clearly
borne out for the contact disciplines but is also true
for most of the other disciplines.
Considering the contact disciplines we have
seen that (1) they can be distinguished quite well
from computer science with the same features be-
ing involved in better classification results, (2)
they show individual feature constellations in their
distinction from their seed disciplines. Moreover,
n-grams have gained discriminatory force over
time and are ranked relatively high among our fea-
tures in the 2000s subcorpus. As they are also rel-
evant in terms of terminology, they give an insight
in the relatedness of disciplines.
In terms of methods, we have combined state-
of-the-art corpus processing with techniques of
data analysis as developed in data mining. As such
techniques become more accessible to linguistic,
literary and cultural analysis, the repertoire of
methods for such analysis will be greatly enhanced
in that sounder empirical evidence can be sought
in text-based socio-cultural and historical studies
at large (cf. Jockers (2013)). The crucial factor
in employing such methods is the motivation of
the features to be used in analysis. Here, we have
deliberately not relied on word-based features but
instead mainly employed lexico-grammatical pat-
terns. While bags-of-words are strong discrim-
inators between texts/text classes, they can only
tell us something about lexical variation (e.g., as
an indicator of text topic). However, when reg-
ister or style rather than topicality are in the fo-
cus (such as e.g. the linguistic construal of techni-
cal, dense or abstract discourse or the expression
of field, tenor or mode relations), it will not be suf-
ficient to study lexical word distributions (cf. Co-
hen et al (2010); Teich and Fankhauser (2010) for
some other studies). Instead, one needs to identify
lexico-grammatical patterns that are potential in-
dicators of the more abstract discoursive and con-
textual properties that are in focus.
The insight to be gained from our study for mul-
tilingually comparable corpora is that more elab-
orate definitions of ?comparability? might be re-
66
quired. Our approach offers such a definition of
comparability by being firmly based on an estab-
lished model of linguistic variation, which has also
been widely applied in multilingual contexts, such
as for example, automatic text generation (see
e.g., Matthiessen and Bateman (1991); Bateman
(1997); Kruijff et al (2000)). The parameters of
variation we employ (register: field, tenor, mode;
discourse styles; time) provide a fine-grained grid
of features involved in linguistic variation, which
can be applied to other languages as well. For ex-
ample, we can extract and analyze field features,
such as term patterns (as produced for German by
Weller et al (2011)), tenor features, such as modal
verbs, as well as the other features investigated
using the same tools applied here (part-of-speech
tagger, CQP, R-scripts and WEKA modules) with
only little adaptations (e.g., tag sets, query formu-
lation). Overall, we would expect that applying
the concept of register to the problem of compara-
bility will enable finer-tuned comparable corpora
and thus contribute to their fuller potential for mul-
tilingual language technology.
Acknowledgments
We wish to thank the anonymous reviewers for
their helpful comments. We are especially grate-
ful to Peter Fankhauser for critically assessing our
data and to Noam Ordan for valuable suggestions
regarding the structure of the paper.
References
Shlomo Argamon, Jeff Dodick, and Paul Chase.
Language use reflects scientific methodology:
A corpus-based study of peer-reviewed journal
articles. Scientometrics, 75(2):203?238, 2008.
Bogdan Babych, Anthony Hartley, and Serge
Sharoff. Translating from under-resourced lan-
guages: Comparing direct transfer against pivot
translation. In Proceedings of the MT Sum-
mit XI, pages 412?418, Copenhagen, Denmark,
2007.
Marco Baroni and Silvia Bernardini. A new ap-
proach to the study of translationese: Machine-
learning the difference between original and
translated text. Literary and Linguistic Com-
puting, 21(3):259?274, 2006.
John A. Bateman. Enabling technology for
multilingual natural language generation: The
KPML development environment. Journal
of Natural Language Engineering, 3(1):15?55,
1997.
Douglas Biber. Variation Across Speech and Writ-
ing. Cambridge University Press, Cambridge,
1988.
Douglas Biber. The multi-dimensional approach
to linguistic analyses of genre variation: An
overview of methodology and findings. Com-
puters and the Humanities, 26(5-6):331?345,
1993.
Douglas Biber. University Language: A Corpus-
based Study of Spoken And Written Regis-
ters, volume 23 of Studies in Corpus Lin-
guistics. John Benjamins Publishing, Amster-
dam/Philadelphia, 2006.
Douglas Biber. Register as a predictor of linguis-
tic variation. Corpus Linguistics and Linguistic
Theory, 8(1):9?37, 2012.
Douglas Biber, Stig Johansson, and Geoffrey
Leech. Longman Grammar of Spoken and Writ-
ten English. Longman, Harlow, 1999.
Yun-Chuang Chiao and Pierre Zweigenbaum.
Looking for candidate translational equivalents
in specialized, comparable corpora. In Pro-
ceedings of the 19th international Conference
on Computational Linguistics (COLING), Vol.
2, pages 1?5, Taipei, Taiwan, 2002.
Kevin Bretonnel Cohen, Helen Johnson, Karin
Verspoor, Christophe Roeder, and Lawrence
Hunter. The structural and content aspects of
abstracts versus bodies of full text journal arti-
cles are different. BMC bioinformatics, 11(1):
492, 2010.
CWB. The IMS Open Corpus Workbench, 2010.
http://www.cwb.sourceforge.net.
Stefania Degaetano-Ortlieb, Kermes Hannah,
Ekaterina Lapshinova-Koltunski, and Teich
Elke. SciTex a diachronic corpus for analyz-
ing the development of scientific registers. In
Paul Bennett, Martin Durrell, Silke Scheible,
and Richard J. Whitt, editors, New Methods in
Historical Corpus Linguistics, Corpus Linguis-
tics and Interdisciplinary Perspectives on Lan-
guage (CLIP), Vol. 3. Narr, Tu?bingen, forth-
coming.
Stefan Evert. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. PhD the-
sis, IMS, University of Stuttgart, 2004.
67
M.A.K. Halliday. An Introduction to Functional
Grammar. Arnold, London, 2004.
M.A.K. Halliday and J.R. Martin. Writing sci-
ence: Literacy and discursive power. Falmer
Press, London, 1993.
Thorsten Joachims. Text categorization with sup-
port vector machines: Learning with many rel-
evant features. Machine Learning: ECML-98,
pages 137?142, 1998.
Matthew L. Jockers. Macroanalysis: Digital
Methods and Literary History. University of
Illinois Press, 2013.
Hannah Kermes and Elke Teich. Formulaic ex-
pressions in scientific texts: Corpus design, ex-
traction and exploration. Lexicographica, 28
(1):99?120, 2012.
Geert-Jan Kruijff, Elke Teich, John Bateman,
Ivana Kruijff-Korbayova?, Hana Skoumalova?,
Serge Sharoff, Lena Sokolova, Tony Hartley,
Kamenka Staykova, and Jir??? Hana. Multilin-
guality in a text generation system for three
Slavic languages. In Proceedings of the 18th in-
ternational Conference on Computational Lin-
guistics (COLING), Vol. 1, pages 474?480,
Saarbru?cken, Germany, 2000.
Christian M.I.M. Matthiessen and John A. Bate-
man. Text generation and systemic-functional
linguistics: Experiences from English and
Japanese. Communication in Artificial Intelli-
gence Series. Pinter, 1991.
Randolph Quirk, Sidney Greenbaum, Geoffrey
Leech, and Jan Svartvik. A Comprehensive
Grammar of the English Language. Longman,
London, 1985.
Elke Teich and Peter Fankhauser. Exploring a
corpus of scientific texts using data mining.
In S. Gries, S. Wulff, and M. Davies, editors,
Corpus-linguistic applications: Current stud-
ies, new directions, pages 233?247. Rodopi,
Amsterdam and New York, 2010.
Jean Ure. Lexical density and register differentia-
tion. In G. E. Perren and J. L. M. Trim, editors,
Applications of Linguistics. Selected papers of
the Second International Congress of Applied
Linguistics, Cambridge 1969, pages 443?452.
Cambridge University Press, 1971.
Jean Ure. Introduction: Approaches to the study
of register range. International Journal of the
Sociology of Language, 35:5?23, 1982.
Vered Volansky, Noam Ordan, and Shuly Wintner.
More human or more translated? Original texts
vs. human and machine translations. In Pro-
ceedings of the 11th Bar-Ilan Symposium on the
Foundations of AI with Israeli Seminar on Com-
putational Linguistics (ISCOL), Ramat Gan, Is-
rael, 2011.
Marion Weller, Helena Blancafort, Anita Gojun,
and Ulrich Heid. Terminology extraction and
term variation patterns: a study of French and
German data. In Proceedings of the GSCL:
German Society for Computational Linguistics
and Language Technology, Hamburg, Germany,
2011.
Casey Whitelaw and Jon Patrick. Selecting sys-
temic features for text classification. In Ash
Asudeh, Ce?cile Paris, and Stephen Wan, edi-
tors, Proceedings of the Australasian Language
Technology Workshop, pages 93?100, Sydney,
Australia, 2004.
Ian H. Witten and Frank Eibe. Data Mining: Prac-
tical Machine Learning Tools and Techniques.
Elsevier, Morgan Kaufmann Publishers, Ams-
terdam, Boston, second edition, 2005.
68
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 77?86,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
VARTRA: A Comparable Corpus for Analysis of Translation Variation
Ekaterina Lapshinova-Koltunski
Universita?t des Saarlandes
A 2.2 Universita?t Campus
66123 Saarbru?cken
Germany
e.lapshinova@mx.uni-saarland.de
Abstract
This paper presents a comparable trans-
lation corpus created to investigate trans-
lation variation phenomena in terms of
contrasts between languages, text types
and translation methods (machine vs.
computer-aided vs. human). These phe-
nomena are reflected in linguistic fea-
tures of translated texts belonging to dif-
ferent registers and produced with differ-
ent translation methods. For their analysis,
we combine methods derived from trans-
lation studies, language variation and ma-
chine translation, concentrating especially
on textual and lexico-grammatical varia-
tion. To our knowledge, none of the ex-
isting corpora can provide comparable re-
sources for a comprehensive analysis of
variation across text types and translation
methods. Therefore, the corpus resources
created, as well as our analysis results will
find application in different research areas,
such as translation studies, machine trans-
lation, and others.
1 Introduction: Aims and Motivation
Comparable corpora serve as essential resources
for numerous studies and applications in both
linguistics (contrastive language, text analysis),
translation studies and natural language process-
ing (machine translation, computational lexicog-
raphy, information extraction). Many compara-
ble corpora are available and have been being cre-
ated for different language pairs like (a) English,
German and Italian (Baroni et al, 2009); (b) En-
glish, Norwegian, German and French (Johans-
son, 2002); (c) written or spoken English and Ger-
man (Hansen et al, 2012) or (Lapshinova et al,
2012).
However, comparable corpora may be of the
same language, as the feature of ?comparability?
may relate not only to corpora of different lan-
guages but also to those of the same language.
The main feature that makes them comparable is
that they cover the same text type(s) in the same
proportions, cf. for instance, (Laviosa, 1997) or
(McEnery, 2003), and thus, can be used for a cer-
tain comparison task.
As our research goal is the analysis of trans-
lation variation, we need a corpus which allows
us to compare translations, which differ in the
source/target language, the type of the text trans-
lated (genre or register) and the method of trans-
lation (human with/without CAT1 tools, machine
translation). There are a number of corpus-based
studies dedicated to the analysis of variation phe-
nomena, cf. (Teich, 2003; Steiner, 2004; Neu-
mann, 2011) among others. However, all of
them concentrate on the analysis of human trans-
lations only, comparing translated texts with non-
translated ones. In some works on machine trans-
lation, the focus does lie on comparing differ-
ent translation variants (human vs. machine),
e.g. (White, 1994; Papineni et al, 2002; Babych
and Hartley, 2004; Popovic?, 2011). However, they
all serve the task of automatic machine transla-
tion (MT) systems evaluation and use the human-
produced translations as references or training ma-
terial only. None of them provide analysis of
specifc (linguistic) features of different text types
translated with different translation methods.
The same tendencies are observed in the cor-
pus resources available, as they are mostly built
for certain research goals. Although there exists
a number of translation corpora, none of them
fits our research task: most of them include one
translation method only: EUROPARL (Koehn,
2005) and JRC-Acquis (Steinberger et al, 2006)
? translations produced by humans, or DARPA-
94 (White, 1994) ? machine-translated texts only.
1CAT = computer-aided translation
77
Moreover, they all contain one register only and,
therefore, cannot be applied to a comprehensive
analysis of variation phenomena.
Therefore, we decided to compile our own com-
parable corpus which contains translations from
different languages, of different text types, pro-
duced with different translation methods (human
vs. machine). Furthermore, both human and ma-
chine translations contain further varieties: they
are produced by different translators (both profes-
sional and student), with or without CAT tools or
by different MT systems.
This resource will be valuable not only for our
research goals, or for research purposes of further
translation researchers. It can also find further ap-
plications, e.g. in machine translation or CAT tool
development, as well as translation quality asses-
ment.
The remainder of the paper is structured as fol-
lows. Section 2 presents studies we adopt as the-
oretical background for the selection of features
and requirements for corpus resources. In section
4, we describe the compilation and design of the
comparable translation corpus at hand. In section
5, we demonstrate some examples of corpus ap-
plication, and in section 6, we draw some conclu-
sions and provide more ideas for corpus extension
and its further application.
2 Theoretical Background and Resource
Requirements
To design and annotate a corpus reflecting varia-
tion phenomena, we need to define (linguistic) fea-
tures of translations under analysis. As sources for
these features, we use studies on translation and
translationese, those on language variation, as well
as works on machine translation, for instance MT
evaluation and MT quality assessment.
2.1 Translation analysis and translationese
As already mentioned in section 1 above, trans-
lation studies either analyse differences between
original texts and translations, e.g. (House, 1997;
Matthiessen, 2001; Teich, 2003; Hansen, 2003;
Steiner, 2004), or concentrate on the properties of
translated texts only, e.g. (Baker, 1995). How-
ever, it is important that most of them consider
translations to have their own specific properties
which distinguish them from the originals (both of
the source and target language), and thus, estab-
lish specific language of translations ? the transla-
tionese.
Baker (1995) excludes the influence of the
source language on a translation altogether,
analysing characteristic patterns of translations in-
dependent of the source language. Within this
context, she proposed translation universals ? hy-
potheses on the universal features of translations:
explicitation (tendency to spell things out rather
than leave them implicit), simplification (tendency
to simplify the language used in translation), nor-
malisation (a tendency to exaggerate features of
the target language and to conform to its typi-
cal patterns) and levelling out (individual trans-
lated texts are alike), cf. (Baker, 1996). Addition-
ally, translations can also have features of ?shining
through? defined by Teich (2003) ? in this case we
observe some typical features of the source lan-
guage in the translation. The author analyses this
phenomena comparing different linguistic features
(e.g. passive and passive-like constructions) of
originals and translations in English and German.
In some recent applications of translationese
phenomena, e.g. those for cleaning parallel cor-
pora obtained from the Web, or for the im-
provement of translation and language models in
MT (Baroni and Bernardini, 2005; Kurokawa et
al., 2009; Koppel and Ordan, 2011; Lembersky
et al, 2012), authors succeeded to automatically
identify these features with machine learning tech-
niques.
We aim at employing the knowledge (features
described) from these studies, as well as tech-
niques applied to explore these features in the cor-
pus.
2.2 Language variation
Features of translated texts, as well as those of
their sources are influenced by the text types they
belong to, see (Neumann, 2011). Therefore, we
also refer to studies on language variation which
focus on the analysis of variation across registers
and genres, e.g. (Biber, 1995; Conrad and Biber,
2001; Halliday and Hasan, 1989; Matthiessen,
2006; Neumann, 2011) among others. Register
is described as functional variation, see Quirk et
al. (1985) and Biber et al (1999). For exam-
ple, language may vary according to the activ-
itiy of the involved participants, production va-
rieties (written vs. spoken) of a language or
the relationship between speaker and addressee(s).
These parameters correspond to the variables of
78
field, tenor and mode defined in the framework of
Systemic Functional Linguistics (SFL), which de-
scribes language variation according to situational
contexts, cf. e.g. Halliday and Hasan (1989), and
Halliday (2004).
In SFL, these variables are associated with the
corresponding lexico-grammatical features, e.g.
field of discourse is realised in functional verb
classes (e.g., activity, communication, etc) or term
patterns, tenor is realised in modality (expressed
e.g. by modal verbs) or stance expressions, mode
is realised in information structure and textual co-
hesion (e.g. personal and demonstrative refer-
ence). Thus, differences between registers or text
types can be identified through the analysis of oc-
currence of lexico-grammatical features in these
registers, see Biber?s studies on linguistic varia-
tion, e.g. (Biber, 1988; Biber, 1995) or (Biber et
al., 1999).
Steiner (2001) and Teich (2003) refer to regis-
ters as one of the influencing sources of the prop-
erties of translated text. Thus, we attempt to study
variation in translation variants by analysing dis-
tributions of lexico-grammatical features in our
corpus.
2.3 Machine translation
We also refer to studies on machine translation in
our analysis, as we believe that translation vari-
ation phenomena should not be limited to those
produced by humans. Although most studies com-
paring human and machine translation serve the
task of automatic MT evaluation only, cf. (White,
1994; Papineni et al, 2002; Babych and Hartley,
2004), some of them do use linguistic features for
their analysis.
For instance, Popovic? and Burchardt (2011)
define linguistically influenced categories (inflec-
tions, word order, lexical choices) to automatically
classify errors in the output of MT systems. Spe-
cia (2011) and Specia et al (2011) also utilise lin-
guistic features as indicators for quality estima-
tion in MT. The authors emphasize that most MT
studies ignored the MT system-independent fea-
tures, i.e. those reflecting the properties of the
translation and the original. The authors classify
them into source complexity features (sentence
and word length, type-token-ratio, etc.), target flu-
ency features (e.g. translation sentence length or
coherence of the target sentence) and adequacy
features (e.g. absolute difference between the
number of different phrase types in the source and
target or difference between the depth of their syn-
tactic trees, etc.).
3 Methodology
Consideration of the features described in the
above mentioned frameworks will give us new
insights on variation phenomena in translation.
Thus, we collect these features and extract infor-
mation on their distribution across translation vari-
ants of our corpus to evaluate them later with sta-
tistical methods.
Some of the features described by different
frameworks overlap, e.g. type-token-ratio (TTR)
or sentence length as indicator for simplification
in translationese analysis and as a target fluency
feature in MT quality estimation; modal meanings
and theme-rheme distribution in register analysis
and SFL, or alternation of passive verb construc-
tions in register analysis and translation studies.
Investigating language variation in translation,
we need to compare translations produced by dif-
ferent systems with those produced by humans
(with/without the help of CATs). Furthermore, we
need to compare translated texts either with their
originals in the source or comparable originals in
the target language. Moreover, as we know that
text type has influence on both source and target
text (Neumann, 2011), we need to compare differ-
ent text registers of all translation types.
This requires a certain corpus design: we need
a linguistically-annotated corpus for extraction of
particular features (e.g. morpho-syntactic con-
structions); we need to include meta-information
on (a) translation type (human vs. computer-aided
vs. machine, both rule-based and statistical), (b)
text production type (original vs. translation) and
(c) text type (various registers and domains of dis-
course). This will enable the following analysis
procedures: (1) automatic extraction, (2) statisti-
cal evaluation and (3) classification (clustering) of
lexico-grammatical features.
4 Corpus Resources
4.1 Corpus data collection
Due to the lack of resources required for the anal-
ysis of translation variation, we have compiled our
own translation corpus VARTRA (VARiation in
TRAnslation). In this paper, we present the first
version of the corpus ? VARTRA-SMALL, which
is the small and normalised version used for our
79
first analyses and experiments. The compilation
of the full version of VARTRA is a part of our fu-
ture work, cf. section 6.
VARTRA-SMALL contains English original
texts and variants of their translations (to each
text) into German which were produced by: (1)
human professionals (PT), (2) human student
translators with the help of computer-aided trans-
lation tools (CAT), (3) rule-based MT systems
(RBMT) and (4) statistical MT systems (SMT).
The English originals (EO), as well as the trans-
lations by profesionals (PT) were exported from
the already existing corpus CroCo mentioned in
section 1 above. The CAT variant was pro-
duced by student assistents who used the CAT
tool ACROSS in the translation process2. The
current RBMT variant was translated with SYS-
TRAN (RBMT1)3, although we plan to expand
it with a LINGUATEC-generated version4. For
SMT, we have compiled two versions ? the one
produced with Google Translate5 (SMT1), and the
other one with a Moses system (SMT2).
Each translation variant is saved as a subcor-
pus and covers seven registers of written language:
political essays (ESSAY), fictional texts (FIC-
TION), manuals (INSTR), popular-scientific arti-
cles (POPSCI), letters of share-holders (SHARE),
prepared political speeches (SPEECH), and touris-
tic leaflets (TOU), presented in Table 1. The total
number of tokens in VARTRA-SMALL comprises
795,460 tokens (the full version of VARTRA will
comprise at least ca. 1,7 Mio words).
4.2 Corpus annotation
For the extraction of certain feature types, e.g.
modal verbs, passive and active verb construc-
tions, Theme types, textual cohesion, etc. our cor-
pus should be linguistically annotated. All sub-
corpora of VARTRA-SMALL are tokenised, lem-
matised, tagged with part-of-speech information,
segmented into syntactic chunks and sentences.
The annotations were obtained with Tree Tagger
(Schmid, 1994).
In Table 2, we outline the absolute numbers for
different annotation levels per subcorpus (transla-
tion variant) in VARTRA-SMALL.
VARTRA-SMALL is encoded in CWB and can
be queried with the help of Corpus Query Proces-
2www.my-across.net
3SYSTRAN 6
4www.linguatec.net
5http://translate.google.com/
subc token lemma chunk sent
PT 132609 9137 55319 6525
CAT 139825 10448 58669 6852
RBMT 131330 8376 55714 6195
SMT1 130568 9771 53935 6198
SMT2 127892 7943 51599 6131
Table 2: Annotations in VARTRA-SMALL
sor (CQP) (Evert, 2005). We also encode a part
of the meta-data, such as information on regis-
ter, as well as translation method, tools used and
the source language. A sample output encoded in
CQP format that is subsequently used for corpus
query is shown in Figure 1.
In this way, we have compiled a corpus of dif-
ferent translation variants, which are comparable,
as they contain translations of the same texts pro-
duced with different methods and tools. Thus,
this comparable corpus allows for analysis of con-
trasts in terms of (a) text typology (e.g. fiction
vs. popular-scientific articles); (b) text produc-
tion types (originals vs. translations) and (c) trans-
lation types (human vs. machine and their sub-
types).
Furthermore, examination of some translation
phenomena requires parallel components ? align-
ment between originals and translations. At the
moment, alignment on the sentence level (ex-
ported from CroCo) is available for the EO and
PT subcorpora. We do not provide any alignment
for further translation variants at the moment, al-
though we plan to align all of them with the origi-
nals on word and sentence level.
4.3 Corpus querying
As already mentioned in 4.2, VARTRA-SMALL
can be queried with CQP, which allows definition
of language patterns in form of regular expressions
based on string, part-of-speech and chunk tags, as
well as further constraints. In Table 3, we illus-
trate an example of a query which is built to ex-
tract cases of processual finite passive verb con-
structions in German: lines 1 - 5 are used for pas-
sive from a Verbzweit sentence (construction in
German where the finite verb occupies the posi-
tion after the subject), and lines 6 - 10 are used
for Verbletzt constructions (where the finite verb
occupies the final position in the sentence). In
this example, we make use of part-of-speech (lines
3a, 5, 8 and 9a), lemma (lines 3b and 9b) and
80
EO PT CAT RBMT SMT1 SMT2
ESSAY 15537 15574 15795 15032 15120 14746
FICTION 11249 11257 12566 11048 11028 10528
INSTR 20739 21009 19903 20793 20630 20304
POPSCI 19745 19799 22755 20894 20353 19890
SHARE 24467 24613 24764 22768 22792 22392
SPEECH 23308 23346 24321 23034 22877 22361
TOU 17564 17638 19721 17761 17768 17671
TOTAL 132609 133236 139825 131330 130568 127892
Table 1: Tokens per register in VARTRA-SMALL
chunk type (lines 2b and 6b) information, as well
as chunk (lines 2a, 2c, 6a and 6c) and sentence
(lines 1 and 10) borders.
query block example
1. <s>
2a. <chunk>
2b. [ .chunk type=?NC?]+ Ein Chatfenster
2c. </chunk>
3a. [pos=?VAFIN?&
3b. lemma=?werden?] wird
4. [word!=?.?]* daraufhin
5. [pos=?V.*PP?]; angezeigt
6a. <chunk>
6b. [ .chunk type=?NC?]+ das Transportgut
6c. </chunk>
7. [word!=?.?]* nicht
8. [pos=?V.*PP?] akzeptiert
9a. [pos=?VAFIN?&
9b. lemma=?werden?] wird
10. </s>
Table 3: Example queries to extract processual fi-
nite passive constructions
CQP also allows us to sort the extracted infor-
mation according to the metadata: text registers
and IDs or translation methods and tools. Table
4 shows an example of frequency distribution ac-
cording to the metadata information. In this way,
we can obtain data for our analyses of translation
variation.
5 Preliminary Analyses
5.1 Profile of VARTRA-SMALL in terms of
shallow features
We start our analyses with the comparison of
translation variants only saved in our subcorpora:
PT, CAT, RBMT, SMT1 and SMT2. The structure
method tool register freq
CAT Across POPSCI 101
CAT Across SHARE 90
CAT Across SPEECH 89
CAT Across INSTR 73
RBMT SYSTRAN SHARE 63
RBMT SYSTRAN POPSCI 62
CAT Across TOU 58
Table 4: Example output of V2 processual pas-
sive across translation method, tool and text regis-
ter (absolute frequencies)
of the corpus, as well as the annotations available
already allow us to compare subcorpora (transla-
tion variants) in terms of shallow features, such
as type-token-ration (TTR), lexical density (LD)
and part-of-speech (POS) distributions. These fea-
tures are among the most frequently used variables
which characterise linguistic variation in corpora,
cf. (Biber et al, 1999) among others. They also
deliver the best scores in the identification of trans-
lationese features. We calculate TTR as the per-
centage of different lexical word forms (types)
per subcorpus. LD is calculated as percentage of
content words and the percentages given in the
POS distribution are the percentages of given word
classes per subcorpus, all normalised per cent. The
numerical results for TTR and LD are given in Ta-
ble 5.
subc TTR LD
PT 15.82 48.33
CAT 14.10 44.60
RBMT 15.04 45.08
SMT1 14.32 46.03
SMT2 14.68 47.86
Table 5: TTR and LD in VARTRA-SMALL
81
<translation method=?CAT? tool=?Across? sourceLanguage=?English?>
<text ?CAT ESSAY 001.txt? register=?ESSAY?>
<s>
<chunk type=?NC?>
Die ART d
weltweiten ADJA weltweit
Herausforderungen NN Herausforderung
</chunk>
<chunk type=?PC?>
im APPRART im
Bereich NN Bereich
</chunk>
<chunk type=?NC?>
der ART d
Energiesicherheit NN Energiesicherheit
</chunk>
<chunk type=?VC?>
erfordern VVFIN erfordern
</chunk>
<chunk type=?PC?>
u?ber APPR u?ber
einen ART ein
Zeitraum NN Zeitraum
</chunk>
<chunk type=?PC?>
von APPR von
vielen PIAT viel
Jahrzehnten ADJA jahrzehnte
nachhaltige ADJA nachhaltig
Anstrengungen NN Anstrengung
</chunk>
<chunk type=?PC?>
auf APPR auf
Figure 1: Example of an annotated sample from VARTRA-SMALL
For the analysis of POS distribution, we de-
cide to restrict them to nominal and verbal word
classes. Tables 6 and 7 illustrate distribution of
nominal ? nouns, pronouns (pron), adjectives (adj)
and adpositions (adp), and verbal word classes
? verbs, adverbs (adv) and conjunctions (conj) ?
across different translation variants.
subc noun pron adj adp total
PT 27.18 8.23 9.38 8.31 53.10
CAT 24.80 8.53 8.08 9.52 50.93
RBMT 24.80 8.61 8.91 9.01 51.32
SMT1 27.18 8.04 8.67 9.02 52.89
SMT2 29.78 7.28 10.42 8.64 56.11
Table 6: Nominal word classes in % in VARTRA-
SMALL
5.2 Interpretation of results
According to Biber (1999), high proportion of
variable lexical words in a text is an indicator
of richness and density of experiential meanings.
This characterises the field of discourse (see sec-
subc verb adv conj total
PT 11.80 3.95 5.32 21.06
CAT 13.58 3.69 5.83 23.10
RBMT 12.90 2.74 6.34 21.99
SMT1 11.88 2.81 6.32 21.02
SMT2 9.09 2.52 6.06 17.67
Table 7: Verbal word classes in % in VARTRA-
SMALL
tion 2.2 above), and TTR, thus, indicates infor-
mational density. In terms of translationese (see
section 2.1), TTR reveals simplification features
of translations. Translations always reveal lower
TTR and LD than their originals, cf. (Hansen,
2003).
The highest TTR, thus, the most lexically rich
translation variant in VARTRA is the one pro-
duced by human translators: PT > RBMT >
SMT2 > SMT1 > CAT. It is interesting that the
other human-produced variant demonstrates the
lowest lexical richness which might be explained
by the level of experience of translators (student
82
translators). Another reason could be the strength
of pronominal cohesion and less explicit specifica-
tion of domains. However, the comparison of the
distribution of pronouns (devices for pronominal
cohesion) does not reveal big differences between
PT and CAT, cf. Table 6.
Another simplification feature is LD, which
is also the lowest in CAT-subcorpus of VAR-
TRA: PT > SMT2 > SMT1 > RBMT > CAT.
Steiner (2012) claims that lower lexical density
can indicate increased logical explicitness (in-
creased use of conjunctions and adpositions) in
translations. CAT does demonstrate the highest
number of adpositions in the corpus, although the
difference across subcorpora is not high, see Ta-
ble 6.
The overall variation between the subcorpora in
terms of TTR and LD is not high, which can be in-
terpreted as indicator of levelling out (see section
2.1 above): translations are often more alike in
terms of these features than the individual texts in
a comparable corpus of source or target language.
In terms of nominal vs. verbal word classes,
there seems to be a degree of dominance of nom-
inal classes (56.11% vs. 17.67%) in SMT2 result-
ing in a ratio of 3.18 compared to other subcor-
pora, cf. Table 8.
subc nominal vs. verbal ratio
PT 53.10 : 21.06 2.52
CAT 50.93 : 23.10 2.20
RBMT 51.32 : 21.99 2.33
SMT1 52.89 : 21.02 2.52
SMT2 56.11 : 17.67 3.18
Table 8: Proportionality of nominal vs. verbal op-
position in VARTRA-SMALL
The greatest contributors to this dominance are
nouns and adjectives (Table 6 above). For CAT, we
again observe the lowest numbers (the lowest noun
vs. verb ratio) which means that this translation
variant seems to be the most ?verbal? one. Ac-
cording to Steiner (2012), German translations are
usually more verbal than German originals. Com-
paring German and English in general, the author
claims that German is less ?verbal? than English.
Thus, a higher ?verbality? serves as an indicator
of ?shining though? (see 2.1 above), which we ob-
serve in case of CAT. However, to find this out, we
would need to compare our subcorpora with their
originals, as well as the comparable German orig-
inals.
5.3 First statistical experiments
We use the extracted shallow features for the first
steps in feature evaluation. As our aim is to inves-
tigate the relations between the observed feature
frequencies and the respective translation variants,
we decide for correspondence analysis, a multi-
variate technique, which works on observed fre-
quencies and provides a map of the data usually
plotted in a two dimensional graph, cf. (Baayen,
2008).
As input we use the features described in 5.1
above: TTR, LD, nouns, adjectives (adj), ad-
positions (adp), verbs, adverbs (adv), conjunc-
tions (conj). Additionally, we divide the class
of pronouns into two groups: personal (pers.P)
and demonstrative (dem.P) ? devices to express
pronominal cohesion. We also extract frequency
information on modal verbs which express modal-
ity.
The output of the correspondence analysis is
plotted into a two dimensional graph with arrows
representing the observed feature frequencies and
points representing the translation variants. The
length of the arrows indicates how pronounced a
particular feature is. The position of the points in
relation to the arrows indicates the relative impor-
tance of a feature for a translation variant. The ar-
rows pointing in the direction of an axis indicate a
high contribution to the respective dimension. Fig-
ure 2 shows the graph for our data.
In Table 9, we present the Eigenvalues calcu-
lated for each dimension to assess how well our
data is represented in the graph6. We are able to
obtain a relatively high cumulative value by the
first two dimensions (representing x and y-axis in
Figure 2), as they are the ones used to plot the two-
dimensional graph. The cumulative value for the
first two dimensions is 94,3%, which indicates that
our data is well represented in the graph.
If we consider the y-axis in Figure 2, we see
that there is a separation between human and ma-
chine translation, although SMT2 is on the bor-
derline. CAT is also closer to MT, as it is plotted
much closer to 0 than PT. Conjunctions, personal
pronouns and adverbs seem to be most prominent
contributors to this separation, as their arrows are
6?dim? lists dimensions, ?value? ? Eigenvalues converted
to percentages of explained variation in ?%? and calculated
as cumulative explained variation with the addition of each
dimension in ?cum?.
83
Figure 2: Graph for correspondence analysis on translation variants
dim value % cum% scree plot
1 0.005939 73.0 73.0 *************************
2 0.001726 21.2 94.3 *******
3 0.000352 4.3 98.6 *
4 0.000114 1.4 100.0
??? ??
Total: 0.008131 100.0
Table 9: Contribution of dimensions
the longest ones, and they point in the direction of
the y-axis.
Verbs, adjectives and nouns seem to be most
prominent contributors to the other division (con-
sidering the x-axis). Here, we can observe three
groups of subcorpora: CAT and RBMT share cer-
tain properties which differ them from SMT2. PT
remains on the borderline, whereas SMT1 tend
slightly to SMT2.
6 Conclusion and Future Work
In this paper, we presented a comparable corpus
of translations from English into German, which
contains multiple variants of translation of the
same texts. This corpus is an important resource
for the investigation of variation phenomena re-
flected in linguistic features of translations. The
corpus architecture allows us to extract these fea-
tures automatically. Our preliminary results show
that there are both similarities and differences be-
tween translation variants produced by humans
and machine systems. We expect even more vari-
ation, if we compare the distribution of these fea-
tures across text registers available in all subcor-
pora.
However, there is a need to inspect the reasons
for this variation, as they can be effected by trans-
lator experience, restrictions of the CAT system
applied or the training material used in MT.
We believe that our resources, as well as our re-
search results will find application not only in con-
trastive linguistics or translation studies. On the
one hand, our corpus provides a useful dataset to
investigate translation phenomena and processes,
84
but on the other, it can be used for the develop-
ment, optimisation and evaluation of MT systems,
as well as CAT tools (e.g. translation memories).
In the future, we aim at expanding it with more
data: (1) more texts for the existing registers (each
register should contain around 30,000 words), (2)
further text registers (e.g. academic, web and news
texts). We also plan to produce further human
and machine-generated translations, i.e. (3) ma-
chine translations post-edited by humans, as well
as translation outputs of (4) further MT systems.
Moreover, we aim at adding translations from Ger-
man into English to trace variation influenced by
language typology.
As the automatic tagging of part-of-speech and
chunk information might be erroneous, we plan to
evaluate the output of the TreeTagger and com-
pare it with the output of further tools available,
e.g. MATE dependency parser, cf. (Bohnet,
2010). Furthermore, the originals will be aligned
with their translations on word and sentence level.
This annotation type is particularly important for
the analysis of variation in translation of certain
lexico-grammatical structures.
A part of the corpus (CAT, RBMT and SMT
subcorpora) will be available to a wider academic
public, e.g. via the CLARIN-D repository.
Acknowledgments
The project ?VARTRA: Translation Variation?
was supported by a grant from Forschungsauss-
chu? of the Saarland University. We are espe-
cially grateful to Anne-Katrin Schumann, Elke Te-
ich and Noam Ordan for their comments. Also, we
wish to thank the anonymous reviewers for their
suggestions for improving our paper. All remain-
ing errors remain ours.
References
Across Personal Edition: Free CAT Tool for Freelance
Translators. http://www.my-across.net/
en/translation-workbench.aspx.
Harald Baayen. 2008. Analyzing Linguistic Data. A
Practical Introduction to Statistics Using R. Cam-
bridge University Press.
Bogdan Babych and Anthony Hartley. 2004. Mod-
elling legitimate translation variation for automatic
evaluation of MT quality. Proceedings of LREC-
2004, Vol. 3.
Mona Baker. 1995. Corpora in Translation Studies:
An Overview and Some Suggestions for Future Re-
search. Target, 7(2):223?43.
Mona Baker. 1996. Corpus-based translation studies:
The challenges that lie ahead. Harold Somers (ed.).
Terminology, LSP and Translation. Studies in lan-
guage engineering in honour of Juan C. Sager. Am-
sterdam and Philadelphia: Benjamins: 175?186.
Marco Baroni and Silvia Bernardini. 2005. A New
Approach to the Study of Translationese: Machine-
learning the Difference between Original and Trans-
lated Text. Literary and Linguistic Computing, 21
(3): 259?274.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi and
Eros Zanchetta. 2009. The WaCky Wide Web: A
Collection of Very Large Linguistically Processed
Web-Crawled Corpora. Language Resources and
Evaluation, 43(3): 209?226.
Douglas Biber. 1988. Variation across speech and
writing. Cambridge: Cambridge University Press.
Douglas Biber. 1995. Dimensions of Register Vari-
ation. A Cross-linguistic Comparison. Cambridge:
Cambridge University Press.
Douglas Biber, Stig Johansson, Geoffrey Leech, Su-
san Conrad and Edward Finegan. 1999. Longman
Grammar of Spoken and Written English. Longman,
London.
Bernd Bohnet. 2010. Top Accuracy and Fast Depen-
dency Parsing is not a Contradiction. The 23rd In-
ternational Conference on Computational Linguis-
tics (COLING 2010). Beijing, China.
Susan Conrad and Douglas Biber (eds.). 2001. Varia-
tion in English: Multi-Dimensional studies. Long-
man, London.
The IMS Open Corpus Workbench. 2010.
http://www.cwb.sourceforge.net
Stefan Evert. 2005. The CQP Query Language Tuto-
rial. IMS Stuttgart, CWB version 2.2.b90.
Google Translate. Accessed July 2012.
http://translate.google.com
Michael A.K. Halliday. 1985. Spoken and written lan-
guage. Deakin University Press, Victoria.
Michael A.K. Halliday, and Riquaya Hasan. 1989.
Language, context and text: Aspects of language
in a social semiotic perspective. Oxford University
Press.
Michael A.K. Halliday. 2004. An Introduction to
Functional Grammar, 3. edition. Hodder Education.
Silvia Hansen-Schirra, Stella Neumann, and Erich
Steiner. 2013. Cross-linguistic Corpora for the
Study of Translations. Insights from the Language
Pair English-German. Berlin, New York: de
Gruyter.
85
Silvia Hansen. 2003. The Nature of Translated Text ?
An Interdisciplinary Methodology for the Investiga-
tion of the Specific Properties of Translations. Ph.D.
Theses.
Juliane House. 1997. Translation Quality Assessment:
A Model Revisited. Ph.D. Thesis.
Stig Johansson. Towards a multilingual corpus for con-
trastive analysis and translation studies. Language
and Computers, 43 (1): 47?59.
Adam Kilgariff. 2010. Comparable Corpora Within
and Across Languages, Word Frequency Lists and
the KELLY Project. BUCC, 6th Workshop on
Building and Using Comparable Corpora, Valletta,
Malta.
Phillip Koehn. 2005 Europarl: A Parallel Corpus for
Statistical Machine Translation. MT Summit.
Moshe Koppel and Noam Ordan. 2011. Translationese
and its dialects. Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics (ACL11).
David Kurokawa, Cyril Goutte and Pierre Isabelle.
2009. Automatic Detection of Translated Text and
its Impact on Machine Translation. Proceedings of
MT-Summit-XII.
Ekaterina Lapshinova-Koltunski, Kerstin Kunz and
Marilisa Amoia. 2012. Compiling a Multilingual
Spoken Corpus. Proceedings of the VIIth GSCP
International Conference : Speech and Corpora.
Firenze : Firenze University Press.
Sara Laviosa. 1997. How Comparable Can ?Compara-
ble Corpora? Be? Target, 9(2): 289?319.
Gennady Lembersky, Noam Ordan and Shuly Wint-
ner. 2012. Language models for machine transla-
tion: Original vs. translated texts. Computational
Linguistics.
Linguatec Personal Translator 14.
http://www.linguatec.net/products/tr/pt
Christian M.I.M. Matthiessen. 2001. The environ-
ment of translation. Erich Steiner and Colin Yallop
(eds). Exploring Translation and Multilingual Text
Production: Beyond Content. Berlin and New York:
Mouten de Gruyter.
Christian M.I.M. Matthiessen. 2006. Frequency pro-
files of some basic grammatical systems: an in-
terim report. Geoffrey Thompson and Susan Hun-
ston (eds). System and Corpus: Exploring connec-
tions. Equinox, London.
Tony McEnery. 2003. Oxford Handbook of Computa-
tional Linguistics, chapter Corpus Linguistics: 448?
463. Oxford: Oxford University Press.
Stella Neumann. 2011. Contrastive Register Varia-
tion. A Quantitative Approach to the Comparison
of English and German. Berlin and New York: de
Gruyter.
Kishore Papineni, Salim Roukus, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, 311?318.
Maja Popovic? and Aljoscha Burchardt. 2011. From
Human to Automatic Error Classification for Ma-
chine Translation Output. 15th International Con-
ference of the European Association for Machine
Translation (EAMT 11).
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman, London.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. International Con-
ference on New Methods in Language Processing,
Manchester (UK): 44?49.
Lucia Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. Pro-
ceedings of the 15th Conference of the European As-
sociation for Machine Translation: 73?80.
Lucia Specia, Najeh Hajlaoui, Catalina Hallett and
Wilker Aziz. 2011. Predicting machine translation
adequacy. Machine Translation Summit XIII (2011):
19?23.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis and Daniel
Varga. 2006. The JRC-Acquis: A multilingual
aligned parallel corpus with 20+ languages. Pro-
ceedings of the 5th International Conference on
Language Resources and Evaluation (LREC?2006).
Genoa, Italy, 24-26 May 2006.
Erich Steiner. 2001. Translations English-German:
Investigating the Relative Importance of Sys-
temic Contrasts and of the Text Type translation.
SPRIKreports 7:1?49.
Erich Steiner. 2004. Translated texts: Properties, Vari-
ants, Evaluations. Frankfurt a.Main: Peter Lang.
Erich Steiner. 2012. A characterization of the resource
based on shallow statistics. Hansen-Schirra, Silvia,
Stella Neumann and Erich Steiner (eds). Cross-
linguistic Corpora for the Study of Translations.
Insights from the Language Pair English-German.
Berlin, New York: de Gruyter.
SYSTRAN Enterprise Server 6. Online Tools User
Guide.
Elke Teich. 2003. Cross-linguistic Variation in Sys-
tem and Text. A Methodology for the Investigation
of Translations and Comparable Texts. Berlin and
New York: Mouton de Gruyter.
John S. White. 1994. The ARPA MT Evaluation
Methodologies: Evolution, Lessons, and Further
Approaches. Proceedings of the 1994 Conference
of the Association for Machine Translation in the
Americas, 193?205.
86
