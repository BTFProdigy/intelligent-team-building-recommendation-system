Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 754?765, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Weakly Supervised Training of Semantic Parsers
Jayant Krishnamurthy
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
jayantk@cs.cmu.edu
Tom M. Mitchell
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
tom.mitchell@cmu.edu
Abstract
We present a method for training a semantic
parser using only a knowledge base and an un-
labeled text corpus, without any individually
annotated sentences. Our key observation is
that multiple forms of weak supervision can be
combined to train an accurate semantic parser:
semantic supervision from a knowledge base,
and syntactic supervision from dependency-
parsed sentences. We apply our approach
to train a semantic parser that uses 77 rela-
tions from Freebase in its knowledge repre-
sentation. This semantic parser extracts in-
stances of binary relations with state-of-the-
art accuracy, while simultaneously recovering
much richer semantic structures, such as con-
junctions of multiple relations with partially
shared arguments. We demonstrate recovery
of this richer structure by extracting logical
forms from natural language queries against
Freebase. On this task, the trained semantic
parser achieves 80% precision and 56% recall,
despite never having seen an annotated logical
form.
1 Introduction
Semantic parsing converts natural language state-
ments into logical forms in a meaning repre-
sentation language. For example, the phrase
?town in California? might be represented as
?x.CITY(x) ? LOCATEDIN(x,CALIFORNIA), where
CITY, LOCATEDIN and CALIFORNIA are predicates
and entities from a knowledge base. The expressiv-
ity and utility of semantic parsing is derived from
this meaning representation, which is essentially a
program that is directly executable by a computer.
In this sense, broad coverage semantic parsing is the
goal of natural language understanding.
Unfortunately, due to data annotation constraints,
modern semantic parsers only operate in narrow do-
mains. The best performing semantic parsers are
trained using extensive manual annotation: typi-
cally, a number of sentences must be annotated with
their desired logical form. Although other forms of
supervision exist (Clarke et al2010; Liang et al
2011), these methods similarly require annotations
for individual sentences. More automated training
methods are required to produce semantic parsers
with richer meaning representations.
This paper presents an algorithm for training a se-
mantic parser without per-sentence annotations. In-
stead, our approach exploits two easily-obtainable
sources of supervision: a large knowledge base and
(automatically) dependency-parsed sentences. The
semantic parser is trained to identify relation in-
stances from the knowledge base while simulta-
neously producing parses that syntactically agree
with the dependency parses. Combining these two
sources of supervision allows us to train an accurate
semantic parser for any knowledge base without an-
notated training data.
We demonstrate our approach by training a Com-
binatory Categorial Grammar (CCG) (Steedman,
1996) that parses sentences into logical forms con-
taining any of 77 relations from Freebase. Our
training data consists of relation instances from
Freebase and automatically dependency-parsed sen-
tences from a web corpus. The trained semantic
parser extracts binary relations with state-of-the-art
performance, while recovering considerably richer
semantic structure. We demonstrate recovery of this
semantic structure using natural language queries
754
town
N : ?x.CITY(x)
Lex
in
(N\N)/N : ?f.?g.?x.?y.f(y) ? g(x) ? LOCATEDIN(x, y)
Lex California
N : ?x.x = CALIFORNIA
Lex
N\N : ?g.?x.?y.y = CALIFORNIA ? g(x) ? LOCATEDIN(x, y)
>
N : ?x.?y.y = CALIFORNIA ? CITY(x) ? LOCATEDIN(x, y)
<
Figure 1: An example parse of ?town in California? using the example CCG lexicon. The first stage in parsing
retrieves a category from each word from the lexicon, represented by the ?Lex? entries. The second stage applies CCG
combination rules, in this case both forms of function application, to combine these categories into a semantic parse.
against Freebase. Our weakly-supervised semantic
parser predicts the correct logical form for 56% of
queries, despite never seeing a labeled logical form.
This paper is structured as follows. We first pro-
vide some background information on CCG and the
structure of a knowledge base in Section 2. Section
3 formulates the weakly supervised training prob-
lem for semantic parsers and presents our algorithm.
Section 4 describes how we applied our algorithm to
construct a semantic parser for Freebase, and Sec-
tion 5 presents our results. We conclude with related
work and discussion.
2 Background
2.1 Combinatory Categorial Grammar
Combinatory Categorial grammar (CCG) is a lin-
guistic formalism that represents both the syntax and
semantics of language (Steedman, 1996). CCG is a
lexicalized formalism that encodes all grammatical
information in a lexicon ?. This lexicon contains
syntactic and semantic categories for each word. A
lexicon may include entries such as:
town := N : ?x.CITY(x)
California := N : ?x.x = CALIFORNIA
in := (N\N)/N : ?f.?g.?x.
?y.f(y) ? g(x) ? LOCATEDIN(x, y)
Each entry of the lexicon w := s : l maps a word or
short phrase w to a syntactic category s and a logical
form l. Syntactic categories s may be atomic (N ) or
complex (N\N ). Logical forms l are lambda calcu-
lus expressions constructed using predicates from a
knowledge base. These logical forms combine dur-
ing parsing to form a complete logical form for the
parsed text.
Parses are constructed by combining adjacent cat-
egories using several combination rules, such as for-
ward (>) and backward (<) application:
X/Y : f Y : g =? X : f(g) (>)
Y : g X\Y : f =? X : f(g) (<)
These rules mean that the complex categoryX/Y
(X\Y ) behaves like a function which accepts an ar-
gument of type Y on its right (left) and returns a
value of type X . Parsing amounts to sequentially
applying these two rules, as shown in Figure 1. The
result of parsing is an ordered pair, containing both
a syntactic parse tree and an associated logical form.
We refer to such an ordered pair as a semantic parse,
or by using the letter `.
Given a lexicon, there may be multiple seman-
tic parses ` for a given phrase w. Like context-free
grammars (CFGs), CCGs can be extended to repre-
sent a probability distribution over parses P (`|w; ?)
where ? is a parameter vector.
2.2 Knowledge Base
The main input to our system is a propositional
knowledge base K = (E,R,C,?), containing
entities E, categories C, relations R and relation
instances ?. Categories and relations are pred-
icates which operate on entities and return truth
values; categories c ? C are one-place predi-
cates (CITY(e)) and relations r ? R are two-
place predicates (LOCATEDIN(e1, e2)). Entities e ?
E represent real-world entities and have a set of
known text names. For example, CALIFORNIA
is an entity whose text names include ?Califor-
nia? and ?CA.? Relation instances r(e1, e2) ? ?
are facts asserted by the knowledge base, such
as LOCATEDIN(SACRAMENTO,CALIFORNIA). Ex-
amples of such knowledge bases include Freebase
(Bollacker et al2008), NELL (Carlson et al
2010), and YAGO (Suchanek et al2007).
The knowledge base influences the semantic
parser in two ways. First, CCG logical forms are
constructed by combining categories, relations and
entities from the knowledge base with logical con-
nectives; hence, the predicates in the knowledge
base determine the expressivity of the parser?s se-
mantic representation. Second, the known relation
755
instances r(e1, e2) ? ? are used as weak supervi-
sion to train the semantic parser.
3 Weakly Supervised Semantic Parsing
We define weakly supervised semantic parsing as
the following learning problem.
Input:
1. A knowledge base K = (E,R,C,?), as de-
fined above.
2. A corpus of dependency-parsed sentences S.
3. A CCG lexicon ? that produces logical forms
containing predicates from K. Section 4.1 de-
scribes an approach to generate this lexicon.
4. A procedure for identifying mentions of enti-
ties from K in sentences from S. (e.g., simple
string matching).
Output:
1. Parameters ? for the CCG that produce correct
semantic parses ` for sentences s ? S.
This problem is ill-posed without additional as-
sumptions: since the correct logical form for a sen-
tence is never observed, there is no a priori reason
to prefer one semantic parse to another. Our train-
ing algorithm makes two assumptions about correct
semantic parses, which are encoded as weak super-
vision constraints. These constraints make learning
possible by adding an inductive bias:
1. Every relation instance r(e1, e2) ? ? is ex-
pressed by at least one sentence in S (Riedel
et al2010; Hoffmann et al2011).
2. The correct semantic parse of a sentence s con-
tains a subset of the syntactic dependencies
contained in a dependency parse of s.
Our weakly supervised training uses these con-
straints as a proxy for labeled semantic parses. The
training algorithm has two steps. First, the algo-
rithm constructs a graphical model that contains
both the semantic parser and constant factors en-
coding the above two constraints. This graphical
model is then used to estimate parameters ? for the
semantic parser, essentially optimizing ? to produce
parses that satisfy the weak supervision constraints.
If our assumptions are correct and sufficiently con-
strain the parameter space, then this procedure will
identify parameters for an accurate semantic parser.
3.1 Encoding the Weak Supervision
Constraints
The first step of training constructs a graphical
model containing the semantic parser and two weak
supervision constraints. However, the first weak su-
pervision constraint couples the semantic parses for
every sentence s ? S. Such coupling would result in
an undesirably large graphical model. We therefore
modify this constraint to enforce that every relation
r(e1, e2) is expressed at least once in S(e1,e2) ? S,
the subset of sentences which mention both e1 and
e2. These mentions are detected using the provided
mention-identification procedure.
Figure 2 depicts the graphical model constructed
for training. The semantic constraint couples the ex-
tractions for all sentences S(e1,e2), so the graphical
model is instantiated once per (e1, e2) tuple. The
model has 4 types of random variables and values:
Si = si represents a sentence, Li = `i represents
a semantic parse, Zi = zi represents the satisfac-
tion of the syntactic constraint and Yr = yr repre-
sents the truth value of relation r. Si, Li and Zi are
replicated once for each sentence s ? S(e1,e2), while
Yr is replicated once for each relation type r in the
knowledge base (all r ? R).
For each entity pair (e1, e2), this graphical model
defines a conditional distribution over L,Y,Z given
S. This distribution factorizes as:
p(Y = y,Z = z,L = `|S = s; ?) =
1
Zs
?
r
?(yr, `)
?
i
?(zi, `i, si)?(si, `i; ?)
The factorization contains three replicated fac-
tors. ? represents the semantic parser, which is
parametrized by ? and produces a semantic parse
`i for each sentence si. ? and ? are deterministic
factors representing the two weak supervision con-
straints. We now describe each factor in more detail.
Semantic Parser
The factor ? represents the semantic parser, which
is a log-linear probabilistic CCG using the input lex-
icon ?. Given a sentence s and parameters ?, the
parser defines an unnormalized probability distribu-
tion over semantic parses `, each of which includes
both a syntactic CCG parse tree and logical form.
756
YlocatedIn
?
Y
acquired
?
Y
capitalOf
?
L
1
S
1
?
Z
1
?
L
2
S
2
?
Z
2
?
Figure 2: Factor graph containing the semantic parser
? and weak supervision constraints ? and ?, instanti-
ated for an (e1, e2) tuple occurring in 2 sentences S1 and
S2, with corresponding semantic parses L1 and L2. The
knowledge base contains 3 relations, represented by the
Y variables.
Let f(`, s) represent a feature function mapping se-
mantic parses to vectors of feature values1. The fac-
tor ? is then defined as:
?(s, `; ?) = exp{?T f(`, s)}
If the features f(`, s) factorize according to the
structure of the CCG parse tree, it is possible to
perform exact inference using a CKY-style dynamic
programming algorithm. However, other aspects of
the graphical model preclude exact inference, so we
perform approximate inference using beam search.
Inference is explained in more detail in Section 3.2.
Semantic Constraint
The semantic constraint states that, given an entity
tuple (e1, e2), every relation instance r(e1, e2) ? ?
must be expressed somewhere in S(e1,e2). Further-
more, no semantic parse can express a relation in-
stance which is not in the knowledge base. This con-
straint is identical to the multiple deterministic-OR
constraint used by Hoffmann et al2011) to train a
sentential relation extractor.
The graphical model contains a semantic con-
straint factor ? and one binary variable Yr for each
relation r in the knowledge base. Yr represents
whether r(e1, e2) is expressed by any sentence in
S(e1,e2). The ? factor determines whether each se-
mantic parse in ` extracts a relation between e1 and
e2. It then aggregates these sentence-level extrac-
tions using a deterministic OR: if any sentence ex-
tracts r(e1, e2) then Yr = 1. Otherwise, Yr = 0.
1Section 4.3 describes the features used by our semantic
parser for Freebase.
?(Yr, `) =
1 if Yr = 1 ? ?i.EXTRACTS(`i, r, e1, e2)
1 if Yr = 0 ? 6 ?i.EXTRACTS(`i, r, e1, e2)
0 otherwise
The EXTRACTS function determines the relation
instances that are asserted by a semantic parse `.
EXTRACTS(`, r, e1, e2) is true if ` asserts the rela-
tion r(e1, e2) and false otherwise. This function es-
sentially converts the semantic parser into a senten-
tial relation extractor, and its implementation may
depend on the types of logical connectives included
in the lexicon ?. Logical forms in our Freebase se-
mantic parser consist of conjunctions of predicates
from the knowledge base; we therefore define EX-
TRACTS(`, r, e1, e2) as true if `?s logical form con-
tains the clauses r(x, y), x = e1 and y = e2.
Syntactic Constraint
A problem with the semantic constraint is that it
admits a large number of ungrammatical parses. The
syntactic constraint penalizes ungrammatical parses
by encouraging the semantic parser to produce parse
trees that agree with a dependency parse of the same
sentence. Specifically, the syntactic constraint re-
quires the predicate-argument structure of the CCG
parse to agree with the predicate-argument structure
of the dependency parse.
Agreement is defined as a function of each CCG
rule application in `. In the parse tree `, each rule
application combines two subtrees, `h and `c, into a
single tree spanning a larger portion of the sentence.
A rule application is consistent with a dependency
parse t if the head words of `h and `c have a depen-
dency edge between them in t. AGREE(`, t) is true
if and only if every rule application in ` is consistent
with t. This syntactic constraint is encoded in the
graphical model by the ? factors and Z variables:
?(z, `, s) = 1 if z = AGREE(`,DEPPARSE(s))
0 otherwise
3.2 Parameter Estimation
To train the model, a single training example is con-
structed for every tuple of entities (e1, e2). The in-
put to the model is s = S(e1,e2), the set of sentences
757
containing e1 and e2. The weak supervision vari-
ables, y, z, are the output of the model. y is con-
structed by setting yr = 1 if r(e1, e2) ? ?, and 0
otherwise. This setting trains the semantic parser to
extract every true relation instance between (e1, e2)
from some sentence in S(e1,e2), while simultane-
ously avoiding incorrect instances. Finally, z = 1,
to encourage agreement between the semantic and
dependency parses. The training data for the model
is therefore a collection, {(sj , yj , zj)}nj=1, where j
indexes entity tuples (e1, e2).
Training optimizes the semantic parser parame-
ters ? to predict Y = yj ,Z = zj given S = sj . The
parameters ? are estimated by running the structured
perceptron algorithm (Collins, 2002) on the training
data defined above. The structured perceptron al-
gorithm iteratively applies a simple update rule for
each example (sj , yj , zj) in the training data:
`predicted ? arg max
`
max
y,z
p(`, y, z|sj ; ?t)
`actual ? arg max
`
p(`|yj , zj , sj ; ?t)
?
t+1 ? ?t +
?
i
f(`
actual
i , si)
?
?
i
f(`
predicted
i , si)
Each iteration of training requires solving two
maximization problems. The first maximization,
max`,y,z p(`, y, z|s; ?t), is straightforward because y
and z are deterministic functions of `. Therefore,
it is solved by finding the maximum probability as-
signment `, then choosing values for y and z that
satisfy the weak supervision constraints.
The second maximization, max` p(`|y, z, s; ?t), is
more challenging. When y and z are given, the infer-
ence procedure must restrict its search to the parses
` which satisfy these weak supervision constraints.
The original formulation of the ? factors permitted
tractable inference (Hoffmann et al2011), but the
EXTRACTS function and the ? factors preclude ef-
ficient inference. We approximate this maximiza-
tion using beam search over CCG parses `. For each
sentence s, we perform a beam search to produce
k = 300 possible semantic parses. We then check
the value of ? for each generated parse and elimi-
nate parses which do not satisfy this syntactic con-
straint. Finally, we apply EXTRACTS to each parse,
then use the greedy approximate inference proce-
dure from Hoffmann et al2011) for the ? factors.
4 Building a Grammar for Freebase
We apply the training algorithm from the previous
section to produce a semantic parser for a subset of
Freebase. This section describes details of the gram-
mar we construct for this task, including the con-
struction of the lexicon ?, some extensions to the
CCG parser, and the features used during training.
In this section, we assume access to a knowledge
base K = (E,C,R,?), a corpus of dependency-
parsed sentences S and a procedure for identifying
mentions of entities in sentences.
4.1 Constructing the Lexicon ?
The first step in constructing the semantic parser
is defining a lexicon ?. We construct ? by ap-
plying simple dependency-parse-based heuristics to
sentences in the training corpus. The resulting lex-
icon ? captures a variety of linguistic phenomena,
including verbs, common nouns (?city?), noun com-
pounds (?California city?) and prepositional modi-
fiers (?city in California?).
The first step in lexicon construction is to use the
mention identification procedure to identify all men-
tions of entities in the sentences S. This process
results in (e1, e2, s) triples, consisting of sentences
with two entity mentions. The dependency path be-
tween e1 and e2 in s is then matched against the de-
pendency parse patterns in Table 1. Each matched
pattern adds one or more lexical entries to ?
Each pattern in Table 1 has a corresponding lexi-
cal category template, which is a CCG lexical cate-
gory containing parameters e, c and r that are chosen
at initialization time. Given the triple (e1, e2, s), re-
lations r are chosen such that r(e1, e2) ? ?, and
categories c are chosen such that c(e1) ? ? or
c(e2) ? ?. The template is then instantiated with
every combination of these e, c and r values.
After instantiating lexical categories for each sen-
tence in S, we prune infrequent lexical categories to
improve parser efficiency. This pruning step is re-
quired because the common noun pattern generates
a large number of lexical categories, the majority
of which are incorrect. Therefore, we eliminate all
common noun categories instantiated by fewer than
758
Part of
Dependency Parse Pattern Lexical Category Template
Speech
Proper (name of entity e) w :=N : ?x.x = e
Noun Sacramento Sacramento :=N : ?x.x = SACRAMENTO
Common e1
SBJ
===? [is, are, was, ...]
OBJ
?=== w w :=N : ?x.c(x)
Noun Sacramento is the capital capital :=N : ?x.CITY(x)
Noun e1
NMOD
?===== e2 Type change N : ?x.c(x) to N |N : ?f.?x.?y.c(x) ? f(y) ? r(x, y)
Modifier Sacramento, California N : ?x.CITY(x) to N |N : ?f.?x.?y.CITY(x) ? f(y) ? LOCATEDIN(x, y)
Preposition
e1
NMOD
?===== w
PMOD
?===== e2 w := (N\N)/N : ?f.?g.?x.?y.f(y) ? g(x) ? r(x, y)
Sacramento in California in := (N\N)/N : ?f.?g.?x.?y.f(y) ? g(x) ? LOCATEDIN(x, y)
e1
SBJ
===? VB*
ADV
?=== w
PMOD
?===== e2 w := PP/N : ?f.?x.f(x)
Sacramento is located in California in := PP/N : ?f.?x.f(x)
Verb
e1
SBJ
===? w*
OBJ
?=== e2 w* := (S\N)/N : ?f.?g.?x, y.f(y) ? g(x) ? r(x, y)
Sacramento governs California governs := (S\N)/N : ?f.?g.?x, y.f(y) ? g(x) ? LOCATEDIN(x, y)
e1
SBJ
===? w*
ADV
?=== [IN,TO]
PMOD
?===== e2 w* := (S\N)/PP : ?f.?g.?x, y.f(y) ? g(x) ? r(x, y)
Sacramento is located in California is located := (S\N)/PP : ?f.?g.?x, y.f(y) ? g(x) ? LOCATEDIN(x, y)
e1
NMOD
?===== w*
ADV
?=== [IN,TO]
PMOD
?===== e2 w* := (N\N)/PP : ?f.?g.?y.f(y) ? g(x) ? r(x, y)
Sacramento located in California located := (N\N)/PP : ?f.?g.?y.f(y) ? g(x) ? LOCATEDIN(x, y)
Forms of
(none) w* := (S\N)/N : ?f.?g.?x.g(x) ? f(x)
?to be?
Table 1: Dependency parse patterns used to instantiate lexical categories for the semantic parser lexicon ?. Each
pattern is followed by an example phrase that instantiates it. An * indicates a position that may be filled by multiple
consecutive words in the sentence. e1 and e2 are the entities identified in the sentence, r represents a relation where
r(e1, e2), and c represents a category where c(e1). Each template may be instantiated with multiple values for the
variables e, c, r.
5 sentences in S. The other rules are less fertile, so
we do not need to prune their output.
In addition to these categories, the grammar in-
cludes type-changing rules from N to N |N . These
rules capture noun compounds by allowing nouns to
become functions from nouns to nouns. There are
several such type-changing rules since the resulting
category includes a hidden relation r between the
noun and its modifier (see Table 1). As with lexical
categories, the set of type changing rules included
in the grammar is determined by matching depen-
dency parse patterns to the training data. Similar
rules for noun compounds are used in other CCG
parsers (Clark and Curran, 2007).
The instantiated lexicon represents the semantics
of words and phrases as conjunctions of predicates
from the knowledge base, possibly including exis-
tentially quantified variables and ? expressions. The
syntactic types N and PP are semantically rep-
resented as functions from entities to truth values
(e.g., ?x.CITY(x)), while sentences S are statements
with no ? terms, such as ?x, y.x = CALIFORNIA ?
CITY(y) ? LOCATEDIN(x, y). Variables in the seman-
tic representation (x, y) range over entities from the
knowledge base. Intuitively, the N and PP cate-
gories represent sets of entities, while sentences rep-
resent assertions about the world.
4.2 Extensions to CCG
The semantic parser is trained using sentences from
a web corpus, which contains many out-of-domain
words. As a consequence, many of the words en-
countered during training cannot be represented us-
ing the vocabulary of predicates from the knowl-
edge base. To handle these extraneous words, we
allow the CCG parser to skip words while parsing
a sentence. During parsing, the parser first decides
whether to retrieve a lexical category for each word
in the sentence. The sentence is then parsed as if
only the retrieved lexical categories existed.
4.3 Features
The features f(`, s) for our probabilistic CCG con-
tain two sets of features. The first set contains lexi-
cal features, which count the number of times each
lexical entry is used in `. The second set contains
rule application features, which count the number
of times each combination rule is applied to each
possible set of arguments. An argument is defined
by its syntactic and semantic category, and in some
cases by the lexical entry which created it. We lex-
759
icalize arguments for prepositional phrases PP and
common nouns (initialized by the second rule in Ta-
ble 1). This lexicalization allows the parser to dis-
tinguish between prepositional phrases headed by
different prepositions, as well as between different
common nouns. All other types are distinguished
solely by syntactic and semantic category.
5 Evaluation
In this section, we evaluate the performance of
a semantic parser for Freebase, trained using our
weakly-supervised algorithm. Empirical compari-
son is somewhat difficult because the most compara-
ble previous work ? weakly-supervised relation ex-
traction ? uses a shallower semantic representation.
Our evaluation therefore has two components: (1) a
binary relation extraction task, to demonstrate that
the trained semantic parser extracts instances of bi-
nary relations with performance comparable to other
state-of-the-art systems, and (2) a natural language
database query task, to demonstrate the parser?s abil-
ity to extract more complex logical forms than bi-
nary relation instances, such as logical expressions
involving conjunctions of multiple categories and re-
lations with partially shared arguments.
5.1 Corpus Construction
Our experiments use a subset of 77 relations2 from
Freebase3 as the knowledge base and a corpus of
web sentences. We constructed the sentence corpus
by first sampling sentences from a web crawl and
parsing them with MaltParser (Nivre et al2006).
Long sentences tended to have noisy parses while
also rarely expressing relations, so we discarded
sentences longer than 10 words. Entities were iden-
tified by performing a simple string match between
canonical entity names in Freebase and proper noun
phrases identified by the parser. In cases where a
single noun phrase matched multiple entities, we se-
lected the entity participating in the most relations.
The resulting corpus contains 2.5 million (e1, e2, s)
triples, from which we reserved 10% for validation
and 10% for testing. The validation set was used
to estimate performance during algorithm develop-
2These relations are defined by a set of MQL queries and
potentially traverse multiple relation links.
3http://www.freebase.com
Relation Name
Relation
Sentences
Instances
CITYLOCATEDINSTATE 2951 13422
CITYLOCATEDINCOUNTRY 1696 7904
CITYOFPERSONBIRTH 397 440
COMPANIESHEADQUARTEREDHERE 326 432
MUSICARTISTMUSICIAN 251 291
CITYUNIVERSITIES 239 338
CITYCAPITALOFCOUNTRY 123 2529
HASHUSBAND 103 367
PARENTOFPERSON 85 356
HASSPOUSE 81 461
Table 2: Occurrence statistics for the 10 most frequent
relations in the training data. ?Relation Instances? shows
the number of entity tuples (e1, e2) that appear as positive
examples for each relation, and ?Sentences? shows the
total number of sentences in which these tuples appear.
ment, while the test set was used to generate the fi-
nal experimental results. All triples for each (e1, e2)
tuple were placed in the same set.
Approximately 1% of the resulting (e1, e2, s)
triples are positive examples, meaning there exists
some relation r where r(e1, e2) ? ?4. To improve
training efficiency and prediction performance, we
subsample 5% of the negative examples for training,
producing a training set of 125k sentences with 27k
positive examples. The validation and test sets retain
the original positive/negative ratio. Table 2 shows
some statistics of the most frequent relations in the
test set.
5.2 Relation Extraction
The first experiment measures the semantic parser?s
ability to extract relations from sentences in our web
corpus. We compare our semantic parser to MUL-
TIR (Hoffmann et al2011), which is a state-of-
the-art weakly supervised relation extractor. This
method uses the same weak supervision constraint
and parameter estimation procedure, but replaces the
semantic parser by a linear classifier. The features
for this classifier include the dependency path be-
tween the entity mentions, the type of each mention,
and the intervening context (Mintz et al2009).
Both the semantic parser and MULTIR were
trained by running 5 iterations of the structured per-
4Note that the positive/negative ratio was much lower with-
out the length filter or entity disambiguation, which is partly
why filtering was performed.
760
MULTIR
PARSE+DEP
PARSE
PARSE-DEP
0 0.2 0.4 0.6 0.8 1.0
0
0.2
0.4
0.6
0.8
1.0
Figure 3: Aggregate precision as a function of recall, for
MULTIR (Hoffman et al2011) and our three semantic
parser variants.
MULTIR
PARSE+DEP
PARSE
PARSE-DEP
0 600 1200 1800 2400 3000
0
0.2
0.4
0.6
0.8
1.0
Figure 4: Sentential precision as a function of the ex-
pected number of correct extractions for MULTIR (Hoff-
man et al2011) and our three semantic parser variants.
ceptron algorithm5. At test time, both models pre-
dicted a relation r ? R or NONE for each (e1, e2, s)
triple in the test set. The parser parses the sen-
tence without considering the entities marked in the
sentence, then applies the EXTRACTS function de-
fined in Section 3.1 to identify a relation between e1
and e2. We compare three versions of the semantic
parser: PARSE, which is the basic semantic parser,
PARSE+DEP which additionally observes the cor-
rect dependency parse at test time, and PARSE-DEP
which is trained without the syntactic constraint.
Note that MULTIR uses the sentence?s dependency
parse to construct its feature vector.
Our evaluation considers two performance mea-
sures: aggregate and sentential precision/recall. Ag-
gregate precision takes the union of all extracted re-
lation instances r(e1, e2) from the test corpus and
compares these instances to Freebase. To pro-
5The structured perceptron algorithm does not converge to a
parameter estimate, and we empirically found that performance
did not improve beyond 5 iterations.
MULTIR
PARSE+DEP
PARSE
0 0.2 0.4 0.6 0.8 1.0
0
0.2
0.4
0.6
0.8
1.0
Figure 5: Aggregate precision as a function of recall,
ignoring the two most frequent relations, CITYLOCATE-
DINSTATE and CITYLOCATEDINCOUNTRY.
duce a precision/recall curve, each extracted in-
stance r(e1, e2) is assigned the maximum score over
all sentences which extracted it. This metric is easy
to compute, but may be inaccurate due to inaccura-
cies and missing relations in Freebase.
Sentential precision computes the precision of ex-
tractions on individual (e1, e2, s) tuples. This met-
ric is evaluated by manually sampling and evaluat-
ing 100 test sentences from which a relation was ex-
tracted per model. Unfortunately, it is difficult to
compute recall for this metric, since the true number
of sentences expressing relations is unknown. We
instead report precision as a function of the expected
number of correct extractions, which is directly pro-
portional to recall.
Figure 3 displays aggregate precision/recall and
Figure 4 displays sentential precision/recall for all
4 models. Generally, PARSE behaves like MUL-
TIR with somewhat lower recall. In the sentential
evaluation, PARSE+DEP outperforms both PARSE
and MULTIR. The difference between PARSE+DEP?s
aggregate and sentential precision stems from the
fact that PARSE+DEP extracts each relation instance
from more sentences than either MULTIR or PARSE.
PARSE-DEP has the worst performance in both eval-
uations, suggesting the importance of syntactic su-
pervision. Precision in the aggregate experiment is
low partially due to examples with incorrect entity
disambiguation.
We found that the skewed distribution of relation
types hides interesting differences between the mod-
els. Therefore, we include Figure 5 comparing our
syntactically-supervised parsers to MULTIR, ignor-
ing the two most frequent relations (which together
761
make up over half of all relation instances). Both
PARSE and PARSE+DEP are considerably more pre-
cise than MULTIR on these less frequent relations
because their compositional meaning representation
shares parameter strength between relations. For
example, the semantic parsers learn that ?in? often
combines with a city to form a prepositional phrase;
the parsers can apply this knowledge to identify city
arguments of any relation. However, MULTIR is ca-
pable of higher recall, since its dependency parse
features can represent syntactic dependencies that
cannot be represented by our semantic parsers. This
limitation is a consequence of our heuristic lexicon
initialization procedure, and could be rectified by a
more flexible initialization procedure.
5.3 Natural Language Database Queries
The second experiment measures our trained
parser?s ability to correctly translate natural lan-
guage queries into logical queries against Freebase.
To avoid biasing the evaluation, we constructed
a test corpus of natural language queries in a data-
driven fashion. We searched the test data for sen-
tences with two related entities separated by an ?is
a? expression. The portion of the sentence before the
?is a? expression was discarded and the remainder
retained as a candidate query. For example ?Jesse is
an author from Austin, Texas,? was converted into
the candidate query ?author from Austin, Texas.?
Each candidate query was then annotated with a log-
ical form using categories and relations from the
knowledge base; candidate queries without satisfac-
tory logical forms were discarded. We annotated 50
validation and 50 test queries in this fashion. The
validation set was used to estimate performance dur-
ing algorithm development and the test set was used
to generate the final results. Example queries with
their annotated logical forms are shown in Table 3.
Table 4 displays the results of the query evalua-
tion. For this evaluation, we forced the parser to in-
clude every word of the query in the parse. Precision
is the percentage of successfully parsed queries for
which the correct logical form was predicted. Re-
call is the percentage of all queries for which the
correct logical form was predicted. This evalua-
tion demonstrates that the semantic parser success-
fully interprets common nouns and identifies mul-
tiple relations with shared arguments. The perfor-
Example Query Logical Form
capital of Russia ?x.CITYCAPITALOFCOUNTRY(x, RUSSIA)
wife of Abraham ?x.HASHUSBAND(x,ABRAHAM)
vocalist from ?x.MUSICIAN(x)?
London, England PERSONBORNIN(x, LONDON)?
CITYINCOUNTRY(LONDON, ENGLAND)
home of ?x.HEADQUARTERS(CONOCOPHILLIPS, x)
ConocoPhillips ?CITYINCOUNTRY(x, CANADA)
in Canada
Table 3: Example natural language queries and their cor-
rect annotated logical form.
Precision Recall
PARSE 0.80 0.56
PARSE-DEP 0.45 0.32
Table 4: Precision and recall for predicting logical forms
of natural language queries against Freebase. The table
compares PARSE, trained with syntactic supervision to
PARSE-DEP, trained without syntactic supervision.
mance difference between PARSE and PARSE-DEP
also demonstrates the benefit of including syntactic
supervision.
Examining the system output, we find two ma-
jor sources of error. The first is missing lexical cat-
egories for uncommon words (e.g., ?ex-guitarist?),
which negatively impact recall by making some
queries unparsable. The second is difficulty distin-
guishing between relations with similar type signa-
tures, such as CITYLOCATEDINCOUNTRY and CITY-
CAPITALOFCOUNTRY.
6 Related Work
There are many approaches to supervised seman-
tic parsing, including inductive logic programming
(Zelle and Mooney, 1996), probabilistic and syn-
chronous grammars (Ge and Mooney, 2005; Wong
and Mooney, 2006; Wong and Mooney, 2007; Lu et
al., 2008), and automatically learned transformation
rules (Kate et al2005). This work most closely
follows the work on semantic parsing using CCG
(Zettlemoyer and Collins, 2005; Zettlemoyer and
Collins, 2007; Kwiatkowski et al2010). These su-
pervised systems are all trained with annotated sen-
tence/logical form pairs; hence these approaches are
labor intensive and do not scale to broad domains
with large numbers of predicates.
Several recent papers have attempted to reduce
the amount of human supervision required to train
762
a semantic parser. One line of work eliminates the
need for an annotated logical form, instead using
only the correct answer for a database query (Liang
et al2011) or even a binary correct/incorrect sig-
nal (Clarke et al2010). This type of feedback may
be easier to obtain than full logical forms, but still
requires individually annotated sentences. Other ap-
proaches are completely unsupervised, but do not tie
the language to an existing meaning representation
(Poon and Domingos, 2009). It is also possible to
self-train a semantic parser without any labeled data
(Goldwasser et al2011). However, this approach
does not perform as well as more supervised ap-
proaches, since the parser?s self-training predictions
are not constrained by the correct logical form.
Recent research has produced several weakly su-
pervised relation extractors (Craven and Kumlien,
1999; Mintz et al2009; Wu and Weld, 2010; Riedel
et al2010; Hoffmann et al2011). These sys-
tems scale up to hundreds of predicates, but have
much shallower semantic representations than se-
mantic parsers. For example, these systems can-
not be directly used to respond to natural language
queries. This work extends weakly supervised rela-
tion extraction to produce richer semantic structure,
using only slightly more supervision in the form of
dependency parses.
7 Discussion
This paper presents a method for training a seman-
tic parser using only a knowledge base and a cor-
pus of unlabeled sentences. Our key observation is
that multiple forms of weak supervision can be com-
bined to train an accurate semantic parser: semantic
supervision from a knowledge base of facts, and syn-
tactic supervision in the form of a standard depen-
dency parser. We presented an algorithm for train-
ing a semantic parser in the form of a probabilistic
Combinatory Categorial Grammar, using these two
types of weak supervision. We used this algorithm
to train a semantic parser for an ontology of 77 Free-
base predicates, using Freebase itself as the weak se-
mantic supervision.
Experimental results show that our trained se-
mantic parser extracts binary relations as well as
a state-of-the-art weakly supervised relation extrac-
tor (Hoffmann et al2011). Further experiments
tested our trained parser?s ability to extract more
complex meanings from sentences, including logi-
cal forms involving conjunctions of multiple relation
and category predicates with shared arguments (e.g.,
?x.MUSICIAN(x) ? PERSONBORNIN(x, LONDON) ?
CITYINCOUNTRY(LONDON, ENGLAND)). To test this
capability, we applied the trained parser to natural
language queries against Freebase. The semantic
parser correctly interpreted 56% of these queries,
despite the broad domain and never having seen an
annotated logical form. Together, these two experi-
mental analyses suggest that the combination of syn-
tactic and semantic weak supervision is indeed a suf-
ficient basis for training semantic parsers for a di-
verse range of corpora and predicate ontologies.
One limitation of our method is the reliance on
hand-built dependency parse patterns for lexicon ini-
tialization. Although these patterns capture a va-
riety of linguistic phenomena, they require manual
engineering and may miss important relations. An
area for future work is developing an automated
way to produce this lexicon, perhaps by extend-
ing the recent work on automatic lexicon generation
(Kwiatkowski et al2010) to the weakly supervised
setting. Such an algorithm seems especially impor-
tant if one wishes to model phenomena such as ad-
jectives, which are difficult to initialize heuristically
without generating large numbers of lexical entries.
An elegant aspect of semantic parsing is that it is
easily extensible to include more complex linguis-
tic phenomena, such as quantification and events
(multi-argument relations). In the future, we plan
to increase the expressivity of our parser?s mean-
ing representation to capture more linguistic and se-
mantic phenomena. In this fashion, we can make
progress toward broad coverage semantic parsing,
and thus natural language understanding.
Acknowledgments
This research has been supported in part by DARPA
under contract number FA8750-09-C-0179, and by a
grant from Google. Additionally, we thank Yahoo!
for use of their M45 cluster. We also gratefully ac-
knowledge the contributions of our colleagues on the
NELL project, Justin Betteridge for collecting the
Freebase relations, Jamie Callan and colleagues for
the web crawl, and Thomas Kollar and Matt Gardner
for helpful comments on earlier drafts of this paper.
763
References
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management of
Data, pages 1247?1250.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth AAAI Conference on Artificial Intelli-
gence.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493?552.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
the world?s response. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh Inter-
national Conference on Intelligent Systems for Molec-
ular Biology.
Ruifang Ge and Raymond J. Mooney. 2005. A statistical
semantic parser that integrates syntax and semantics.
In Proceedings of the Ninth Conference on Computa-
tional Natural Language Learning.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised semantic
parsing. Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke S.
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In The 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies.
Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney.
2005. Learning to transform natural to formal lan-
guages. In Proceedings, The Twentieth National Con-
ference on Artificial Intelligence and the Seventeenth
Innovative Applications of Artificial Intelligence Con-
ference.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilistic
CCG grammars from logical form with higher-order
unification. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional semantics.
In Proceedings of the Association for Computational
Linguistics, Portland, Oregon. Association for Com-
putational Linguistics.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-
moyer. 2008. A generative model for parsing natural
language to meaning representations. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of the 2010 European
conference on Machine learning and Knowledge Dis-
covery in Databases.
Mark Steedman. 1996. Surface Structure and Interpre-
tation. The MIT Press.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th international confer-
ence on World Wide Web, WWW ?07, pages 697?706,
New York, NY, USA. ACM.
Yuk Wah Wong and Raymond J. Mooney. 2006. Learn-
ing for semantic parsing with statistical machine trans-
lation. In Proceedings of the Human Language Tech-
nology Conference of the NAACL.
Yuk Wah Wong and Raymond J. Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing with
lambda calculus. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using Wikipedia. In Proceedings of the 48th
764
Annual Meeting of the Association for Computational
Linguistics.
John M. Zelle and Raymond J. Mooney. 1996. Learning
to parse database queries using inductive logic pro-
gramming. In Proceedings of the thirteenth national
conference on Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: structured clas-
sification with probabilistic categorial grammars. In
UAI ?05, Proceedings of the 21st Conference in Un-
certainty in Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed ccg grammars for parsing to logical
form. In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning.
765
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 397?406,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Incorporating Vector Space Similarity in Random Walk Inference over
Knowledge Bases
Matt Gardner
Carnegie Mellon University
mg1@cs.cmu.edu
Partha Talukdar
?
Indian Institute of Science
ppt@serc.iisc.in
Jayant Krishnamurthy
Carnegie Mellon University
jayantk@cs.cmu.edu
Tom Mitchell
Carnegie Mellon University
tom@cs.cmu.edu
Abstract
Much work in recent years has gone into
the construction of large knowledge bases
(KBs), such as Freebase, DBPedia, NELL,
and YAGO. While these KBs are very
large, they are still very incomplete, ne-
cessitating the use of inference to fill in
gaps. Prior work has shown how to make
use of a large text corpus to augment ran-
dom walk inference over KBs. We present
two improvements to the use of such large
corpora to augment KB inference. First,
we present a new technique for combin-
ing KB relations and surface text into a
single graph representation that is much
more compact than graphs used in prior
work. Second, we describe how to incor-
porate vector space similarity into random
walk inference over KBs, reducing the fea-
ture sparsity inherent in using surface text.
This allows us to combine distributional
similarity with symbolic logical inference
in novel and effective ways. With exper-
iments on many relations from two sepa-
rate KBs, we show that our methods sig-
nificantly outperform prior work on KB
inference, both in the size of problem our
methods can handle and in the quality of
predictions made.
1 Introduction
Much work in recent years has gone into the
construction of large knowledge bases, either
by collecting contributions from many users,
as with Freebase (Bollacker et al., 2008) and
?
Research carried out while at the Machine Learning
Department, Carnegie Mellon University.
DBPedia (Mendes et al., 2012), or automat-
ically from web text or other resources, as
done by NELL (Carlson et al., 2010) and
YAGO (Suchanek et al., 2007). These knowl-
edge bases contain millions of real-world enti-
ties and relationships between them. However,
even though they are very large, they are still
very incomplete, missing large fractions of possi-
ble relationships between common entities (West
et al., 2014). Thus the task of inference over
these knowledge bases, predicting new relation-
ships simply by examining the knowledge base it-
self, has become increasingly important.
A promising technique for inferring new re-
lation instances in a knowledge base is random
walk inference, first proposed by Lao and Cohen
(2010). In this method, called the Path Ranking
Algorithm (PRA), the knowledge base is encoded
as a graph, and random walks are used to find
paths that connect the source and target nodes of
relation instances. These paths are used as features
in a logistic regression classifier that predicts new
instances of the given relation. Each path can be
viewed as a horn clause using knowledge base re-
lations as predicates, and so PRA can be thought
of as a kind of discriminatively trained logical in-
ference.
One major deficiency of random walk inference
is the connectivity of the knowledge base graph?
if there is no path connecting two nodes in the
graph, PRA cannot predict any relation instance
between them. Thus prior work has introduced the
use of a text corpus to increase the connectivity of
the graph used as input to PRA (Lao et al., 2012;
Gardner et al., 2013). This approach is not without
its own problems, however. Whereas knowledge
base relations are semantically coherent and dif-
ferent relations have distinct meanings, this is not
397
true of surface text. For example, ?The Nile flows
through Cairo? and ?The Nile runs through Cairo?
have very similar if not identical meaning. Adding
a text corpus to the inference graph increases con-
nectivity, but it also dramatically increases feature
sparsity.
We introduce two new techniques for making
better use of a text corpus for knowledge base in-
ference. First, we describe a new way of incor-
porating the text corpus into the knowledge base
graph that enables much more efficient process-
ing than prior techniques, allowing us to approach
problems that prior work could not feasibly solve.
Second, we introduce the use of vector space sim-
ilarity in random walk inference in order to reduce
the sparsity of surface forms. That is, when fol-
lowing a sequence of edge types in a random walk
on a graph, we allow the walk to follow edges that
are semantically similar to the given edge types,
as defined by some vector space embedding of the
edge types. If a path calls for an edge of type
?flows through?, for example, we accept other
edge types (such as ?runs through?) with probabil-
ity proportional to the vector space similarity be-
tween the two edge types. This lets us combine
notions of distributional similarity with symbolic
logical inference, with the result of decreasing the
sparsity of the feature space considered by PRA.
We show with experiments using both the NELL
and Freebase knowledge bases that this method
gives significantly better performance than prior
approaches to incorporating text data into random
walk inference.
2 Graph Construction
Our method for knowledge base inference, de-
scribed in Section 3, performs random walks over
a graph to obtain features for a logistic regression
classifier. Prior to detailing that technique, we first
describe how we produce a graph G = (N , E ,R)
from a set of knowledge base (KB) relation in-
stances and a set of surface relation instances ex-
tracted from a corpus. Producing a graph from
a knowledge base is straightforward: the set of
nodes N is made up of the entities in the KB; the
set of edge types R is the set of relation types in
the KB, and the typed edges E correspond to re-
lation instances from the KB, with one edge of
type r connecting entity nodes for each (n
1
, r, n
2
)
triple in the KB. Less straightforward is how to
construct a graph from a corpus, and how to con-
nect that graph to the KB graph. We describe our
methods for each of those below.
To create a graph from a corpus, we first prepro-
cess the corpus to obtain a collection of surface
relations, such as those extracted by open infor-
mation extraction systems like OLLIE (Mausam et
al., 2012). These surface relations consist of a pair
of noun phrases in the corpus, and the verb-like
connection between them (either an actual verb,
as done by Talukdar et al. (2012), a dependency
path, as done by Riedel et al. (2013), or OpenIE
relations (Mausam et al., 2012)). The verb-like
connections are naturally represented as edges in
the graph, as they have a similar semantics to the
knowledge base relations that are already repre-
sented as edges. We thus create a graph from these
triples exactly as we do from a KB, with nodes cor-
responding to noun phrase types and edges corre-
sponding to surface relation triples.
So far these two subgraphs we have created
are entirely disconnected, with the KB graph con-
taining nodes representing entities, and the sur-
face relation graph containing nodes representing
noun phrases, with no edges between these noun
phrases and entities. We connect these two graphs
by making use of the ALIAS relation in the KB,
which links entities to potential noun phrase ref-
erents. Each noun phrase in the surface relation
graph is connected to those entity nodes which the
noun phrase can possibly refer to according to the
KB. These edges are not the output of an entity
linking system, as done by Lao et al. (2012), but
express instead the notion that the noun phrase can
refer to the KB entity. The use of an entity linking
system would certainly allow a stronger connec-
tion between noun phrase nodes and entity nodes,
but it would require much more preprocessing and
a much larger graph representation, as each men-
tion of each noun phrase would need its own node,
as opposed to letting every mention of the same
noun phrase share the same node. This graph rep-
resentation allows us to add tens of millions of sur-
face relations to a graph of tens of millions of KB
relations, and perform all of the processing on a
single machine.
As will be discussed in more detail in Section 4,
we also allow edge types to optionally have an as-
sociated vector that ideally captures something of
the semantics of the edge type.
Figure 1 shows the graph constructions used in
our experiments on a subset of KB and surface re-
398
KB Relations:
(Monongahela, RIVERFLOWSTHROUGHCITY, Pittsburgh)
(Pittsburgh, ALIAS, ?Pittsburgh?)
(Pittsburgh, ALIAS, ?Steel City?)
(Monongahela, ALIAS, ?Monongahela River?)
(Monongahela, ALIAS, ?The Mon?)
Surface Relations:
(?The Mon?, ?flows through?, ?Steel City?)
(?Monongahela River?, ?runs through?, ?Pittsburgh?)
Embeddings:
?flows through?: [.2, -.1, .9]
?runs through?: [.1, -.3, .8]
(a) An example data set.
(c) An example graph that replaces surface relations with a
cluster label, as done by Gardner et al. (2013). Note, how-
ever, that the graph structure differs from that prior work;
see Section 5.
(b) An example graph that combines a KB and surface rela-
tions.
(d) An example graph that uses vector space representations
of surface edges, as introduced in this paper.
Figure 1: Example graph construction as used in the experiments in this paper. A graph using only KB
edges is simply a subset of these graphs containing only the RIVERFLOWSTHROUGHCITY edge, and is
not shown.
lations. Note that Figures 1b and 1c are shown as
rough analogues of graphs used in prior work (de-
scribed in more detail in Section 5), and we use
them for comparison in our experiments.
3 The Path Ranking Algorithm
We perform knowledge base inference using the
Path Ranking Algorithm (PRA) (Lao and Cohen,
2010). We begin this section with a brief overview
of PRA, then we present our modification to the
PRA algorithm that allows us to incorporate vector
space similarity into random walk inference.
PRA can be thought of as a method for exploit-
ing local graph structure to generate non-linear
feature combinations for a prediction model. PRA
generates a feature matrix over pairs of nodes in
a graph, then uses logistic regression to classify
those node pairs as belonging to a particular rela-
tion.
More formally, given a graph G with nodes N ,
edges E , and edge labelsR, and a set of node pairs
(s
i
, t
i
) ? D, one can create a connectivity matrix
where rows correspond to node pairs and columns
correspond to edge lables. PRA augments this
matrix with additional columns corresponding to
sequences of edge labels, called path types, and
changes the cell values from representing the pres-
ence of an edge to representing the specificity of
the connection that the path type makes between
the node pair.
Because the feature space considered by PRA
is so large (the set of all possible edge label se-
quences, with cardinality
?
l
i=1
|R|
i
, assuming a
bound l on the maximum path length), the first
step PRA must perform is feature selection, which
is done using random walks over the graph. The
second step of PRA is feature computation, where
each cell in the feature matrix is computed using
a constrained random walk that follows the path
type corresponding to the feature. We now explain
each of these steps in more detail.
Feature selection finds path types pi that are
likely to be useful in predicting new instances of
the relation represented by the input node pairs .
These path types are found by performing random
walks on the graph G starting at the source and
target nodes in D, recording which paths connect
some source node with its target. The edge se-
quences are ranked by frequency of connecting a
source node to a corresponding target node, and
the top k are kept.
Feature computation. Once a set of path types
399
is selected as features, the next step of the PRA
algorithm is to compute a value for each cell in the
feature matrix, corresponding to a node pair and a
path type. The value computed is the probability
of arriving at the target node of a node pair, given
that a random walk began at the source node and
was constrained to follow the path type: p(t|s, pi).
Once these steps have been completed, the re-
sulting feature matrix can be used with whatever
model or learning algorithm is desired; in this and
prior work, simple logistic regression has been
used as the prediction algorithm.
4 Vector space random walks
Our modifications to PRA are confined entirely to
the feature computation step described above; fea-
ture selection (finding potentially useful sequences
of edge types) proceeds as normal, using the sym-
bolic edge types. When computing feature val-
ues, however, we allow a walk to follow an edge
that is semantically similar to the edge type in the
path, as defined by Euclidean distance in the vec-
tor space.
More formally, consider a path type pi. Re-
call that pi is a sequence of edge types <
e
1
, e
2
, . . . , e
l
>, where l is the length of the path;
we will use pi
i
to denote the i
th
edge type in the
sequence. To compute feature values, PRA begins
at some node and follows edges of type pi
i
until
the sequence is finished and a target node has been
reached. Specifically, if a random walk is at a node
n with m outgoing edge types {e
1
, e
2
, . . . , e
m
},
PRA selects the edge type from that set which
matches pi
i
, then selects uniformally at random
from all outgoing edges of that type. If there is
no match in the set, the random walk restarts from
the original start node.
We modify the selection of which edge type to
follow. When a random walk is at a node n with
m outgoing edge types {e
1
, e
2
, . . . , e
m
}, instead
of selecting only the edge type that matches pi
i
,
we allow the walk to select instead an edge that
is close to pi
i
in vector space. For each edge type
at node n, we select the edge with the following
probability:
p(e
j
|pi
i
) ? exp(??v(e
j
) ?v(pi
i
)), ?j, 1 ? j ? m
where v(?) is a function that returns the vector
representation of an edge type, and ? is a spiki-
ness parameter that determines how much weight
to give to the vector space similarity. As ? ap-
proaches infinity, the normalized exponential ap-
proximates a delta function on the closest edge
type to pi
i
, in {e
1
, e
2
, . . . , e
m
}. If pi
i
is in the set
of outgoing edges, this algorithm converges to the
original PRA.
However, if pi
i
is not in the set of outgoing edge
types at a node and all of the edge types are very
dissimilar to pi
i
, this algorithm (with ? not close to
infinity) will lead to a largely uniform distribution
over edge types at that node, and no way for the
random walk to restart. To recover the restart be-
havior of the original PRA, we introduce an addi-
tional restart parameter?, and add another value to
the categorical distribution before normalization:
p(restart|pi
i
) ? exp(? ? ?)
When this restart type is selected, the random
walk begins again, following pi
1
starting at the
source node. With ? set to a value greater than the
maximum similarity between (non-identical) edge
type vectors, and ? set to infinity, this algorithm
exactly replicates the original PRA.
Not all edge types have vector space representa-
tions: the ALIAS relation cannot have a meaning-
ful vector representation, and we do not use vec-
tors to represent KB relations, finding that doing
so was not useful in practice (which makes intu-
itive sense: KB relations are already latent repre-
sentations themselves). While performing random
walks, if pi
i
has no vector representation, we fall
back to the original PRA algorithm for selecting
the next edge.
We note here that when working with vector
spaces it is natural to try clustering the vectors to
reduce the parameter space. Each path type pi is
a feature in our model, and if two path types dif-
fer only in one edge type, and the differing edge
types have very similar vectors, the resultant fea-
ture values will be essentially identical for both
path types. It seems reasonable that running a
simple clustering algorithm over these path types,
to reduce the number of near-duplicate features,
would improve performance. We did not find this
to be the case, however; all attempts we made to
use clustering over these vectors gave performance
indistinguishable from not using clustering. From
this we conclude that the main issue hindering per-
formance when using PRA over these kinds of
graphs is one of limited connectivity, not one of
too many parameters in the model. Though the
400
feature space considered by PRA is very large, the
number of attested features in a real graph is much
smaller, and it is this sparsity which our vector
space methods address.
5 Related Work
Knowledge base inference. Random walk infer-
ence over knowledge bases was first introduced by
Lao and Cohen (2010). This work was improved
upon shortly afterward to also make use of a large
corpus, by representing the corpus as a graph and
connecting it to the knowledge base (Lao et al.,
2012). Gardner et al. (2013) further showed that
replacing surface relation labels with a represen-
tation of a latent embedding of the relation led
to improved prediction performance. This result
is intuitive: the feature space considered by PRA
is exponentially large, and surface relations are
sparse. The relations ?[river] flows through [city]?
and ?[river] runs through [city]? have near iden-
tical meaning, and both should be very predic-
tive for the knowledge base relation RIVERFLOW-
STHROUGHCITY. However, if one of these rela-
tions only appears in the training data and the other
only appears in the test data, neither will be useful
for prediction. Gardner et al. (2013) attempted to
solve this issue by finding a latent symbolic repre-
sentation of the surface relations (such as a cluster-
ing) and replacing the edge labels in the graph with
these latent representations. This makes it more
likely for surface relations seen in training data to
also be seen at test time, and naturally improved
performance.
This representation, however, is still brittle, as
it is still a symbolic representation that is prone to
mismatches between training and test data. If the
clustering algorithm used is too coarse, the fea-
tures will not be useful, and if it is too fine, there
will be more mismatches. Also, verbs that are on
the boundaries of several clusters are problematic
to represent in this manner. We solve these prob-
lems by modifying the PRA algorithm to directly
use vector representations of edge types during the
random walk inference.
These two prior techniques are the most directly
related work to what we present in this paper, and
we compare our work to theirs.
Graph construction. In addition to the incor-
poration of vector space similarity into the PRA
algorithm, the major difference between our work
and the prior approaches mentioned above is in the
construction of the graph used by PRA. We con-
trast our method of graph construction with these
prior approaches in more detail below.
Lao et al. (2012) represent every word of ev-
ery sentence in the corpus as a node in the graph,
with edges between the nodes representing depen-
dency relationships between the words. They then
connect this graph to the KB graph using a simple
entity linking system (combined with coreference
resolution). The resultant graph is enormous, such
that they needed to do complex indexing on the
graph and use a cluster of 500 machines to per-
form the PRA computations. Also, as the edges
represent dependency labels, not words, with this
graph representation the PRA algorithm does not
have access to the verbs or other predicative words
that appear in the corpus, which frequently express
relations. PRA only uses edge types as feature
components, not node types, and so the rich infor-
mation contained in the words is lost. This graph
construction also would not allow the incorpora-
tion of vector space similarity that we introduced,
as dependency labels do not lend themselves well
to vector space representations.
Gardner et al. (2013) take an approach very sim-
ilar to the one presented in Section 2, preprocess-
ing the corpus to obtain surface relations. How-
ever, instead of creating a graph with nodes rep-
resenting noun phrases, they added edges from
the surface relations directly to the entity nodes
in the graph. Using the ALIAS relation, as we do,
they added an edge between every possible con-
cept pair that could be represented by the noun
phrases in a surface relation instance. This leads
to some nonsensical edges added to the graph,
and if the ALIAS relation has high degree (as it
does for many common noun phrases in Freebase),
it quickly becomes unscalable?this method of
graph construction runs out of disk space when
attempting to run on the Freebase experiments in
Section 6. Also, in conflating entity nodes in the
graph with noun phrases, they lose an important
distinction that turns out to be useful for predic-
tion, as we discuss in Section 6.4.
1
1
Recent notions of ?universal schema? (Riedel et al.,
2013) also put KB entities and noun phrases into the same
conceptual space, though they opt for using noun phrases in-
stead of the KB entities used by Gardner et al. In general
this is problematic, as it relies on some kind of entity linking
system as preprocessing, and cannot handle common noun
references of proper entities without losing information. Our
method, and that of Lao et al., skirts this issue entirely by not
trying to merge KB entities with noun phrases.
401
Other related work. Also related to the present
work is recent research on programming lan-
guages for probabilistic logic (Wang et al., 2013).
This work, called ProPPR, uses random walks to
locally ground a query in a small graph before per-
forming propositional inference over the grounded
representation. In some sense this technique is
like a recursive version of PRA, allowing for more
complex inferences than a single iteration of PRA
can make. However, this technique has not yet
been extended to work with large text corpora, and
it does not yet appear to be scalable enough to han-
dle the large graphs that we use in this work. How
best to incorporate the work presented in this pa-
per with ProPPR is an open, and very interesting,
question.
Examples of other systems aimed at reason-
ing over common-sense knowledge are the CYC
project (Lenat, 1995) and ConceptNet (Liu and
Singh, 2004). These common-sense resources
could easily be incorporated into the graphs we
use for performing random walk inference.
Lines of research that seek to incorporate dis-
tributional semantics into traditional natural lan-
guage processing tasks, such as parsing (Socher
et al., 2013a), named entity recognition (Passos et
al., 2014), and sentiment analysis (Socher et al.,
2013b), are also related to what we present in this
paper. While our task is quite different from these
prior works, we also aim to combine distributional
semantics with more traditional methods (in our
case, symbolic logical inference), and we take in-
spiration from these methods.
6 Experiments
We perform both the feature selection step and the
feature computation step of PRA using GraphChi,
an efficient single-machine graph processing li-
brary (Kyrola et al., 2012). We use MAL-
LET?s implementation of logistic regression, with
both L1 and L2 regularization (McCallum, 2002).
To obtain negative evidence, we used a closed
world assumption, treating any (source, target)
pair found during the feature computation step as
a negative example if it was not given as a positive
example. We tuned the parameters to our methods
using a coarse, manual grid search with cross vali-
dation on the training data described below. The
parameters we tuned were the L1 and L2 regu-
larization parameters, how many random walks to
perform in the feature selection and computation
NELL Freebase
Entities 1.2M 20M
Relation instances 3.4M 67M
Total relation types 520 4215
Relation types tested 10 24
Avg. instances/relation 810 200
SVO triples used 404k 28M
Table 1: Statistics of the data used in our experi-
ments.
steps of PRA, and spikiness and restart parameters
for vector space walks. The results presented were
not very sensitive to changes in these parameters.
6.1 Data
We ran experiments on both the NELL and Free-
base knowledge bases. The characteristics of these
knowledge bases are shown in Table 1. The Free-
base KB is very large; to make it slightly more
manageable we filtered out relations that did not
seem applicable to relation extraction, as well as a
few of the largest relations.
2
This still left a very
large, mostly intact KB, as can be seen in the ta-
ble. For our text corpus, we make use of a set of
subject-verb-object triples extracted from depen-
dency parses of ClueWeb documents (Talukdar et
al., 2012). There are 670M such triples in the
data set, most of which are completely irrelevant to
the knowledge base relations we are trying to pre-
dict. For each KB, we filter the SVO triples, keep-
ing only those which can possibly connect training
and test instances of the relations we used in our
experiments. The number of SVO triples kept for
each KB is also shown in Table 1. We obtained
vector space representations of these surface rela-
tions by running PCA on the SVO matrix.
We selected 10 NELL relations and 24 Free-
base relations for testing our methods. The NELL
relations were hand-selected as the relations with
the largest number of known instances that had a
reasonable precision (the NELL KB is automati-
cally created, and some relations have low preci-
sion). We split the known instances of these rela-
tions into 75% training and 25% testing, giving on
average about 650 training instances and 160 test
2
We removed anything under /user, /common, /type (ex-
cept for the relation /type/object/type), /base, and /freebase,
as not applicable to our task. We also removed relations deal-
ing with individual music tracks, book editions, and TV epid-
sodes, as they are very large, very specific, and unlikely to be
useful for predicting the relations in our test set.
402
instances for each relation.
The 24 Freebase relations were semi-randomly
selected. We first filtered the 4215 relations based
on two criteria: the number of relation instances
must be between 1000 and 10000, and there must
be no mediator in the relation.
3
Once we selected
the relations, we kept all instances of each rela-
tion that had some possible connection in the SVO
data.
4
This left on average 200 instances per rela-
tion, which we again split 75%-25% into training
and test sets.
6.2 Methods
The methods we compare correspond to the graphs
shown in Figure 1. The KB method uses the orig-
inal PRA algorithm on just the KB relations, as
presented by Lao and Cohen (2010). KB + SVO
adds surface relations to the graph (Figure 1b). We
present this as roughly analogous to the methods
introduced by Lao et al. (2012), though with some
significant differences in graph representation, as
described in Section 5. KB + Clustered SVO fol-
lows the methods of Gardner et al. (2013), but us-
ing the graph construction introduced in this pa-
per (Figure 1c; their graph construction techniques
would have made graphs too large to be feasible
for the Freebase experiments). KB + Vector SVO
is our method (Figure 1d).
6.3 Evaluation
As evaluation metrics, we use mean average pre-
cision (MAP) and mean reciprocal rank (MRR),
following recent work evaluating relation extrac-
tion performance (West et al., 2014). We test sig-
nificance using a paired permutation test.
The results of these experiments are shown in
Table 2 and Table 3. In Table 4 we show average
precision for every relation tested on the NELL
KB, and we show the same for Freebase in Table 5.
6.4 Discussion
We can see from the tables that KB + Vector SVO
(the method presented in this paper) significantly
outperforms prior approaches in both MAP and
3
A mediator in Freebase is a reified relation in-
stance meant to handle n-ary relations, for instance
/film/performance. PRA in general, and our implementation
of it in particular, needs some modification to be well-suited
to predicting relations with mediators.
4
We first tried randomly selecting instances from these re-
lations, but found that the probability of selecting an instance
that benefited from an SVO connection was negligible. In or-
der to make use of the methods we present, we thus restricted
ourselves to only those that had a possible SVO connection.
Method MAP MRR
KB 0.193 0.635
KB + SVO 0.218 0.763
KB + Clustered SVO 0.276 0.900
KB + Vector SVO 0.301 0.900
Table 2: Results on the NELL knowledge base.
The bolded line is significantly better than all other
results with p < 0.025.
Method MAP MRR
KB 0.278 0.614
KB + SVO 0.294 0.639
KB + Clustered SVO 0.326 0.651
KB + Vector SVO 0.350 0.670
Table 3: Results on the Freebase knowledge base.
The bolded line is significantly better than all other
results with p < 0.0002.
MRR. We believe that this is due to the reduction
in feature sparsity enabled by using vector space
instead of symbolic representations (as that is the
only real difference between KB + Clustered SVO
and KB + Vector SVO), allowing PRA to make
better use of path types found in the training data.
When looking at the results for individual relations
in Table 4 and Table 5, we see that KB + Vector
SVO outperforms other methods on the majority
of relations, and it is a close second when it does
not.
We can also see from the results that mean av-
erage precision seems a little low for all meth-
ods tested. This is because MAP is computed as
the precision of all possible correct predictions in
a ranked list, where precision is counted as 0 if
the correct prediction is not included in the list.
In other words, there are many relation instances
in our randomly selected test set that are not in-
ferrable from the knowledge base, and the low re-
call hurts the MAP metric. MRR, which judges the
precision of the top prediction for each relation,
gives us some confidence that the main issue here
is one of recall, as MRR is reasonably high, es-
pecially on the NELL KB. As further evidence, if
we compute average precision for each query node
(instead of for each relation), excluding queries for
which the system did not return any predictions,
MAP ranges from .29 (KB) to .45 (KB + Vector
SVO) on NELL (with around 30% of queries hav-
ing no prediction), and from .40 (KB) to .49 (KB +
403
Relation KB KB + SVO KB + Clustered SVO KB + Vector SVO
ActorStarredInMovie 0.000 0.032 0.032 0.037
AthletePlaysForTeam 0.200 0.239 0.531 0.589
CityLocatedInCountry 0.126 0.169 0.255 0.347
JournalistWritesForPublication 0.218 0.254 0.291 0.319
RiverFlowsThroughCity 0.000 0.001 0.052 0.076
SportsTeamPositionForSport 0.217 0.217 0.178 0.180
StadiumLocatedInCity 0.090 0.156 0.275 0.321
StateHasLake 0.000 0.000 0.000 0.000
TeamPlaysInLeague 0.934 0.936 0.947 0.939
WriterWroteBook 0.144 0.179 0.195 0.202
Table 4: Average precision for each relation tested on the NELL KB. The best performing method on
each relation is bolded.
Relation KB KB + SVO KB + C-SVO KB + V-SVO
/amusement parks/park/rides 0.000 0.009 0.004 0.013
/architecture/architect/structures designed 0.072 0.199 0.257 0.376
/astronomy/constellation/contains 0.004 0.017 0.000 0.008
/automotive/automotive class/examples 0.003 0.001 0.002 0.006
/automotive/model/automotive class 0.737 0.727 0.742 0.768
/aviation/airline/hubs 0.322 0.286 0.298 0.336
/book/literary series/author s 0.798 0.812 0.818 0.830
/computer/software genre/software in genre 0.000 0.001 0.001 0.001
/education/field of study/journals in this discipline 0.001 0.003 0.003 0.001
/film/film/rating 0.914 0.905 0.914 0.905
/geography/island/body of water 0.569 0.556 0.580 0.602
/geography/lake/basin countries 0.420 0.361 0.409 0.437
/geography/lake/cities 0.111 0.134 0.177 0.175
/geography/river/cities 0.030 0.038 0.045 0.066
/ice hockey/hockey player/hockey position 0.307 0.243 0.222 0.364
/location/administrative division/country 0.989 0.988 0.991 0.989
/medicine/disease/symptoms 0.061 0.078 0.068 0.067
/medicine/drug/drug class 0.169 0.164 0.135 0.157
/people/ethnicity/languages spoken 0.134 0.226 0.188 0.223
/spaceflight/astronaut/missions 0.010 0.186 0.796 0.848
/transportation/bridge/body of water spanned 0.534 0.615 0.681 0.727
/tv/tv program creator/programs created 0.164 0.179 0.163 0.181
/visual art/art period movement/associated artists 0.044 0.040 0.046 0.037
/visual art/visual artist/associated periods or movements 0.276 0.295 0.282 0.290
Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on
each relation is bolded. For space considerations, ?Clustered SVO? is shortened to ?C-SVO? and ?Vector
SVO? is shortened to ?V-SVO? in the table header.
404
Vector SVO) on Freebase, (where 21% of queries
gave no prediction). Our methods thus also im-
prove MAP when calculated in this manner, but it
is not an entirely fair metric,
5
so we use standard
MAP to present our main results.
One interesting phenomenon to note is
a novel use of the ALIAS relation in some
of the relation models. The best exam-
ple of this was found with the relation
/people/ethnicity/languages spoken. A
high-weighted feature when adding surface
relations was the edge sequence <ALIAS, ALIAS
INVERSE>. This edge sequence reflects the
fact that languages frequently share a name
with the group of people that speaks them (e.g.,
Maori, French). And because PRA can gen-
erate compositional features, we also find the
following edge sequence for the same relation:
</people/ethnicity/included in group,
ALIAS, ALIAS INVERSE>. This feature captures
the same notion that languages get their names
from groups of people, but applies it to subgroups
within an ethnicity. These features would be
very difficult, perhaps impossible, to include in
systems that do not distinguish between noun
phrases and knowledge base entities, such as
the graphs constructed by Gardner et al. (2013),
or typical relation extraction systems, which
generally only work with noun phrases after
performing a heuristic entity linking.
7 Conclusion
We have offered two main contributions to the task
of knowledge base inference. First, we have pre-
sented a new technique for combining knowledge
base relations and surface text into a single graph
representation that is much more compact than
graphs used in prior work. This allowed us to ap-
ply methods introduced previously to much larger
problems, running inference on a single machine
over the entire Freebase KB combined with tens of
millions of surface relations. Second, we have de-
scribed how to incorporate vector space similarity
into random walk inference over knowledge bases,
reducing the feature sparsity inherent in using sur-
face text. This allows us to combine distributional
similarity with symbolic logical inference in novel
and effective ways. With experiments on many
5
MAP is intended to include some sense of recall, but ex-
cluding queries with no predictions removes that and opens
the metric to opportunistic behavior.
relations from two separate knowledge bases, we
have shown that our methods significantly outper-
form prior work on knowledge base inference.
The code and data used in the ex-
periments in this paper are available at
http://rtw.ml.cmu.edu/emnlp2014 vector space pra/.
Acknowledgments
This research has been supported in part by
DARPA under contract number FA8750-13-2-
0005, by NSF under grant 31043,18,1121946, and
by generous support from Yahoo! and Google.
References
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of SIGMOD.
Andrew Carlson, Justin Betteridge, Bryan Kisiel,
Burr Settles, Estevam R Hruschka Jr, and Tom M
Mitchell. 2010. Toward an architecture for never-
ending language learning. In AAAI.
Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel,
and Tom Mitchell. 2013. Improving learning and
inference in a large knowledge-base using latent
syntactic cues. In Proceedings of EMNLP. Associa-
tion for Computational Linguistics.
Aapo Kyrola, Guy Blelloch, and Carlos Guestrin.
2012. Graphchi: Large-scale graph computation
on just a pc. In Proceedings of the 10th USENIX
Symposium on Operating Systems Design and Im-
plementation (OSDI), pages 31?46.
Ni Lao and William W Cohen. 2010. Relational re-
trieval using a combination of path-constrained ran-
dom walks. Machine learning, 81(1):53?67.
Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Pro-
ceedings of EMNLP-CoNLL.
Douglas B Lenat. 1995. Cyc: A large-scale investment
in knowledge infrastructure. Communications of the
ACM, 38(11):33?38.
Hugo Liu and Push Singh. 2004. Conceptnet: a practi-
cal commonsense reasoning tool-kit. BT Technology
Journal, 22(4):211?226.
Mausam, Michael Schmitz, Robert Bart, Stephen
Soderland, and Oren Etzioni. 2012. Open language
learning for information extraction. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 523?534. Asso-
ciation for Computational Linguistics.
405
Andrew Kachites McCallum. 2002. Mallet: A ma-
chine learning for language toolkit.
Pablo N. Mendes, Max Jakob, and Christian Bizer.
2012. Dbpedia for nlp: A multilingual cross-domain
knowledge base. In Proceedings of the Eighth In-
ternational Conference on Language Resources and
Evaluation (LREC?12).
Alexandre Passos, Vineet Kumar, and Andrew Mc-
Callum. 2014. Lexicon infused phrase embed-
dings for named entity resolution. arXiv preprint
arXiv:1404.5367.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In Pro-
ceedings of NAACL-HLT.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013a. Parsing with compo-
sitional vector grammars. In In Proceedings of the
ACL conference. Citeseer.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013b. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of WWW.
Partha Pratim Talukdar, Derry Wijaya, and Tom
Mitchell. 2012. Acquiring temporal constraints be-
tween relations. In Proceedings of the 21st ACM
international conference on Information and knowl-
edge management, pages 992?1001. ACM.
William Yang Wang, Kathryn Mazaitis, and
William W. Cohen. 2013. Programming with
personalized pagerank: A locally groundable
first-order probabilistic logic. In Proceedings of the
22Nd ACM International Conference on Conference
on Information &#38; Knowledge Management,
CIKM ?13, pages 2129?2138, New York, NY, USA.
ACM.
Robert West, Evgeniy Gabrilovich, Kevin Murphy,
Shaohua Sun, Rahul Gupta, and Dekang Lin. 2014.
Knowledge base completion via search-based ques-
tion answering. In WWW.
406
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 570?580,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Which Noun Phrases Denote Which Concepts?
Jayant Krishnamurthy
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
jayantk@cs.cmu.edu
Tom M. Mitchell
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
tom.mitchell@cmu.edu
Abstract
Resolving polysemy and synonymy is re-
quired for high-quality information extraction.
We present ConceptResolver, a component for
the Never-Ending Language Learner (NELL)
(Carlson et al, 2010) that handles both phe-
nomena by identifying the latent concepts that
noun phrases refer to. ConceptResolver per-
forms both word sense induction and synonym
resolution on relations extracted from text us-
ing an ontology and a small amount of la-
beled data. Domain knowledge (the ontology)
guides concept creation by defining a set of
possible semantic types for concepts. Word
sense induction is performed by inferring a set
of semantic types for each noun phrase. Syn-
onym detection exploits redundant informa-
tion to train several domain-specific synonym
classifiers in a semi-supervised fashion. When
ConceptResolver is run on NELL?s knowledge
base, 87% of the word senses it creates cor-
respond to real-world concepts, and 85% of
noun phrases that it suggests refer to the same
concept are indeed synonyms.
1 Introduction
Many information extraction systems construct
knowledge bases by extracting structured assertions
from free text (e.g., NELL (Carlson et al, 2010),
TextRunner (Banko et al, 2007)). A major limi-
tation of many of these systems is that they fail to
distinguish between noun phrases and the underly-
ing concepts they refer to. As a result, a polysemous
phrase like ?apple? will refer sometimes to the con-
cept Apple Computer (the company), and other times
to the concept apple (the fruit). Furthermore, two
synonymous noun phrases like ?apple? and ?Apple
?apple?
?apple computer?
apple (the fruit)
Apple Computer
Figure 1: An example mapping from noun phrases (left)
to a set of underlying concepts (right). Arrows indicate
which noun phrases can refer to which concepts.
[eli lilly, lilly]
[kaspersky labs, kaspersky lab, kaspersky]
[careerbuilder, careerbuilder.com]
[l 3 communications, level 3 communications]
[cellular, u.s. cellular]
[jc penney, jc penny]
[nielsen media research, nielsen company]
[universal studios, universal music group, universal]
[amr corporation, amr]
[intel corp, intel corp., intel corporation, intel]
[emmitt smith, chris canty]
[albert pujols, pujols]
[carlos boozer, dennis martinez]
[jason hirsh, taylor buchholz]
[chris snyder, ryan roberts]
[j.p. losman, losman, jp losman]
[san francisco giants, francisco rodriguez]
[andruw jones, andruw]
[aaron heilman, bret boone]
[roberto clemente, clemente]
Figure 2: A random sample of concepts created by Con-
ceptResolver. The first 10 concepts are from company,
while the second 10 are from athlete.
Computer? can refer to the same underlying con-
cept. The result of ignoring this many-to-many map-
ping between noun phrases and underlying concepts
(see Figure 1) is confusion about the meaning of ex-
tracted information. To minimize such confusion, a
system must separately represent noun phrases, the
underlying concepts to which they can refer, and the
many-to-many ?can refer to? relation between them.
The relations extracted by systems like NELL ac-
tually apply to concepts, not to noun phrases. Say
570
the system extracts the relation ceoOf(x1, x2) be-
tween the noun phrases x1 and x2. The correct in-
terpretation of this extracted relation is that there ex-
ist concepts c1 and c2 such that x1 can refer to c1,
x2 can refer to c2, and ceoOf(c1, c2). If the orig-
inal relation were ceoOf(?steve?, ?apple?), then c1
would be Steve Jobs, and c2 would be Apple Com-
puter. A similar interpretation holds for one-place
category predicates like person(x1). We define con-
cept discovery as the problem of (1) identifying con-
cepts like c1 and c2 from extracted predicates like
ceoOf(x1, x2) and (2) mapping noun phrases like
x1, x2 to the concepts they can refer to.
The main input to ConceptResolver is a set of
extracted category and relation instances over noun
phrases, like person(x1) and ceoOf(x1, x2), pro-
duced by running NELL. Here, any individual noun
phrase xi can be labeled with multiple categories
and relations. The output of ConceptResolver is
a set of concepts, {c1, c2, ..., cn}, and a mapping
from each noun phrase in the input to the set of
concepts it can refer to. Like many other systems
(Miller, 1995; Yates and Etzioni, 2007; Lin and Pan-
tel, 2002), ConceptResolver represents each output
concept ci as a set of synonymous noun phrases,
i.e., ci = {xi1, xi2, ..., xim}. For example, Figure 2
shows several concepts output by ConceptResolver;
each concept clearly reveals which noun phrases can
refer to it. Each concept also has a semantic type that
corresponds to a category in ConceptResolver?s on-
tology; for instance, the first 10 concepts in Figure 2
belong to the category company.
Previous approaches to concept discovery use lit-
tle prior knowledge, clustering noun phrases based
on co-occurrence statistics (Pantel and Lin, 2002).
In comparison, ConceptResolver uses a knowledge-
rich approach. In addition to the extracted relations,
ConceptResolver takes as input two other sources of
information: an ontology, and a small number of la-
beled synonyms. The ontology contains a schema
for the relation and category predicates found in
the input instances, including properties of predi-
cates like type restrictions on its domain and range.
The category predicates are used to assign semantic
types to each concept, and the properties of relation
predicates are used to create evidence for synonym
resolution. The labeled synonyms are used as train-
ing data during synonym resolution, where they are
1. Induce Word Senses
i. Use extracted category instances to create
one or more senses per noun phrase.
ii. Use argument type constraints to produce re-
lation evidence for synonym resolution.
2. Cluster Synonymous Senses
For each category C defined in the ontology:
i. Train a semi-supervised classifier to predict
synonymy.
ii. Cluster word senses with semantic type C
using classifier?s predictions.
iii. Output sense clusters as concepts with se-
mantic type C.
Figure 3: High-level outline of ConceptResolver?s algo-
rithm.
used to train a semi-supervised classifier.
ConceptResolver discovers concepts using the
process outlined in Figure 3. It first performs word
sense induction, using the extracted category in-
stances to create one or more unambiguous word
senses for each noun phrase in the knowledge base.
Each word sense is a copy of the original noun
phrase paired with a semantic type (a category) that
restricts the concepts it can refer to. ConceptRe-
solver then performs synonym resolution on these
word senses. This step treats the senses of each se-
mantic type independently, first training a synonym
classifier then clustering the senses based on the
classifier?s decisions. The result of this process is
clusters of synonymous word senses, which are out-
put as concepts. Concepts inherit the semantic type
of the word senses they contain.
We evaluate ConceptResolver using a subset of
NELL?s knowledge base, presenting separate results
for the concepts of each semantic type. The eval-
uation shows that, on average, 87% of the word
senses created by ConceptResolver correspond to
real-world concepts. We additionally find that, on
average, 85% of the noun phrases in each concept
refer to the same real-world entity.
2 Prior Work
Previous work on concept discovery has focused
on the subproblems of word sense induction and
synonym resolution. Word sense induction is typ-
ically performed using unsupervised clustering. In
the SemEval word sense induction and disambigua-
571
tion task (Agirre and Soroa, 2007; Manandhar et al,
2010), all of the submissions in 2007 created senses
by clustering the contexts each word occurs in, and
the 2010 event explicitly disallowed the use of exter-
nal resources like ontologies. Other systems cluster
words to find both word senses and concepts (Pantel
and Lin, 2002; Lin and Pantel, 2002). ConceptRe-
solver?s category-based approach is quite different
from these clustering approaches. Snow et al (2006)
describe a system which adds new word senses to
WordNet. However, Snow et al assume the exis-
tence of an oracle which provides the senses of each
word. In contrast, ConceptResolver automatically
determines the number of senses for each word.
Synonym resolution on relations extracted from
web text has been previously studied by Resolver
(Yates and Etzioni, 2007), which finds synonyms in
relation triples extracted by TextRunner (Banko et
al., 2007). In contrast to our system, Resolver is un-
supervised and does not have a schema for the re-
lations. Due to different inputs, ConceptResolver
and Resolver are not precisely comparable. How-
ever, our evaluation shows that ConceptResolver has
higher synonym resolution precision than Resolver,
which we attribute to our semi-supervised approach
and the known relation schema.
Synonym resolution also arises in record link-
age (Winkler, 1999; Ravikumar and Cohen, 2004)
and citation matching (Bhattacharya and Getoor,
2007; Bhattacharya and Getoor, 2006; Poon and
Domingos, 2007). As with word sense induction,
many approaches to these problems are unsuper-
vised. A problem with these algorithms is that they
require the authors to define domain-specific simi-
larity heuristics to achieve good performance. Other
synonym resolution work is fully supervised (Singla
and Domingos, 2006; McCallum and Wellner, 2004;
Snow et al, 2007), training models using manually
constructed sets of synonyms. These approaches use
large amounts of labeled data, which can be difficult
to create. ConceptResolver?s approach lies between
these two extremes: we label a small number of syn-
onyms (10 pairs), then use semi-supervised training
to learn a similarity function. We think our tech-
nique is a good compromise, as it avoids much of
the manual effort of the other approaches: tuning the
similarity function in one case, and labeling a large
amount of data in the other
ConceptResolver uses a novel algorithm for semi-
supervised clustering which is conceptually similar
to other work in the area. Like other approaches
(Basu et al, 2004; Xing et al, 2003; Klein et al,
2002), we learn a similarity measure for clustering
based on a set of must-link and cannot-link con-
straints. Unlike prior work, our algorithm exploits
multiple views of the data to improve the similar-
ity measure. As far as we know, ConceptResolver
is the first application of semi-supervised cluster-
ing to relational data ? where the items being clus-
tered are connected by relations (Getoor and Diehl,
2005). Interestingly, the relational setting also pro-
vides us with the independent views that are benefi-
cial to semi-supervised training.
Concept discovery is also related to coreference
resolution (Ng, 2008; Poon and Domingos, 2008).
The difference between the two problems is that
coreference resolution finds noun phrases that refer
to the same concept within a specific document. We
think the concepts produced by a system like Con-
ceptResolver could be used to improve coreference
resolution by providing prior knowledge about noun
phrases that can refer to the same concept. This
knowledge could be especially helpful for cross-
document coreference resolution systems (Haghighi
and Klein, 2010), which actually represent concepts
and track mentions of them across documents.
3 Background: Never-Ending Language
Learner
ConceptResolver is designed as a component for the
Never-Ending Language Learner (NELL) (Carlson
et al, 2010). In this section, we provide some per-
tinent background information about NELL that in-
fluenced the design of ConceptResolver 1.
NELL is an information extraction system that
has been running 24x7 for over a year, using coupled
semi-supervised learning to populate an ontology
from unstructured text found on the web. The ontol-
ogy defines two types of predicates: categories (e.g.,
company and CEO) and relations (e.g., ceoOf-
Company). Categories are single-argument pred-
icates, and relations are two-argument predicates.
1More information about NELL, including browsable and
downloadable versions of its knowledge base, is available from
http://rtw.ml.cmu.edu.
572
NELL?s knowledge base contains both definitions
for predicates and extracted instances of each pred-
icate. At present, NELL?s knowledge base defines
approximately 500 predicates and contains over half
a million extracted instances of these predicates with
an accuracy of approximately 0.85.
Relations between predicates are an important
component of NELL?s ontology. For ConceptRe-
solver, the most important relations are domain and
range, which define argument types for each rela-
tion predicate. For example, the first argument of
ceoOfCompany must be a CEO and the second ar-
gument must be a company. Argument type restric-
tions inform ConceptResolver?s word sense induc-
tion process (Section 4.1).
Multiple sources of information are used to popu-
late each predicate with high precision. The system
runs four independent extractors for each predicate:
the first uses web co-occurrence statistics, the sec-
ond uses HTML structures on webpages, the third
uses the morphological structure of the noun phrase
itself, and the fourth exploits empirical regularities
within the knowledge base. These subcomponents
are described in more detail by Carlson et al (2010)
and Wang and Cohen (2007). NELL learns using
a bootstrapping process, iteratively re-training these
extractors using instances in the knowledge base,
then adding some predictions of the learners to the
knowledge base. This iterative learning process can
be viewed as a discrete approximation to EM which
does not explicitly instantiate every latent variable.
As in other information extraction systems, the
category and relation instances extracted by NELL
contain polysemous and synonymous noun phrases.
ConceptResolver was developed to reduce the im-
pact of these phenomena.
4 ConceptResolver
This section describes ConceptResolver, our new
component which creates concepts from NELL?s ex-
tractions. It uses a two-step procedure, first creating
one or more senses for each noun phrase, then clus-
tering synonymous senses to create concepts.
4.1 Word Sense Induction
ConceptResolver induces word senses using a sim-
ple assumption about noun phrases and concepts. If
a noun phrase has multiple senses, the senses should
be distinguishable from context. People can deter-
mine the sense of an ambiguous word given just a
few surrounding words (Kaplan, 1955). We hypoth-
esize that local context enables sense disambigua-
tion by defining the semantic type of the ambiguous
word. ConceptResolver makes the simplifying as-
sumption that all word senses can be distinguished
on the basis of semantic type. As the category pred-
icates in NELL?s ontology define a set of possible
semantic types, this assumption is equivalent to the
one-sense-per-category assumption: a noun phrase
refers to at most one concept in each category of
NELL?s ontology. For example, this means that a
noun phrase can refer to a company and a fruit, but
not multiple companies.
ConceptResolver uses the extracted category as-
sertions to define word senses. Each word sense is
represented as a tuple containing a noun phrase and
a category. In synonym resolution, the category acts
like a type constraint, and only senses with the same
category type can be synonymous. To create senses,
the system interprets each extracted category predi-
cate c(x) as evidence that category c contains a con-
cept denoted by noun phrase x. Because it assumes
that there is at most one such concept, Concept-
Resolver creates one sense of x for each extracted
category predicate. As a concrete example, say
the input assertions contain company(?apple?) and
fruit(?apple?). Sense induction creates two senses
for ?apple?: (?apple?, company) and (?apple?, fruit).
The second step of sense induction produces ev-
idence for synonym resolution by creating relations
between word senses. These relations are created
from input relations and the ontology?s argument
type constraints. Each extracted relation is mapped
to all possible sense relations that satisfy the ar-
gument type constraints. For example, the noun
phrase relation ceoOfCompany(?steve jobs?, ?ap-
ple?) would map to ceoOfCompany((?steve jobs?,
ceo), (?apple?, company)). It would not map to a
similar relation with (?apple?, fruit), however, as
(?apple?, fruit) is not in the range of ceoOfCom-
pany. This process is effective because the relations
in the ontology have restrictive domains and ranges,
so only a small fraction of sense pairs satisfy the ar-
gument type restrictions. It is also not vital that this
mapping be perfect, as the sense relations are only
573
used as evidence for synonym resolution. The final
output of sense induction is a sense-disambiguated
knowledge base, where each noun phrase has been
converted into one or more word senses, and rela-
tions hold between pairs of senses.
4.2 Synonym Resolution
After mapping each noun phrase to one or more
senses (each with a distinct category type), Con-
ceptResolver performs semi-supervised clustering
to find synonymous senses. As only senses with
the same category type can be synonymous, our
synonym resolution algorithm treats senses of each
type independently. For each category, ConceptRe-
solver trains a semi-supervised synonym classifier
then uses its predictions to cluster word senses.
Our key insight is that semantic relations and
string attributes provide independent views of the
data: we can predict that two noun phrases are syn-
onymous either based on the similarity of their text
strings, or based on similarity in the relations NELL
has extracted about them. As a concrete example,
we can decide that (?apple computer?, company)
and (?apple?, company) are synonymous because
the text string ?apple? is similar to ?apple computer,?
or because we have learned that (?steve jobs?, ceo)
is the CEO of both companies. ConceptResolver ex-
ploits these two independent views using co-training
(Blum and Mitchell, 1998) to produce an accurate
synonym classifier using only a handful of labels.
4.2.1 Co-Training the Synonym Classifier
For each category, ConceptResolver co-trains a pair
of synonym classifiers using a handful of labeled
synonymous senses and a large number of automat-
ically created unlabeled sense pairs. Co-training is
a semi-supervised learning algorithm for data sets
where each instance can be classified from two (or
more) independent sets of features. That is, the fea-
tures of each instance xi can be partitioned into two
views, xi = (x1i , x
2
i ), and there exist functions in
each view, f1, f2, such that f1(x1i ) = f
2(x2i ) = yi.
The co-training algorithm uses a bootstrapping pro-
cedure to train f1, f2 using a small set of labeled ex-
amples L and a large pool of unlabeled examples U .
The training process repeatedly trains each classifier
on the labeled examples, then allows each classifier
to label some examples in the unlabeled data pool.
Co-training also has PAC-style theoretical guaran-
tees which show that it can learn classifiers with ar-
bitrarily high accuracy under appropriate conditions
(Blum and Mitchell, 1998).
Figure 4 provides high-level pseudocode for co-
training in the context of ConceptResolver. In Con-
ceptResolver, an instance xi is a pair of senses (e.g.,
<(?apple?, company), (?microsoft?, company)>),
the two views x1i and x
2
i are derived from string
attributes and semantic relations, and the output yi
is whether the senses are synonyms. (The features
of each view are described later in this section.) L
is initialized with a small number of labeled sense
pairs. Ideally, U would contain all pairs of senses
in the category, but this set grows quadratically in
category size. Therefore, ConceptResolver uses the
canopies algorithm (McCallum et al, 2000) to ini-
tialize U with a subset of the sense pairs that are
more likely to be synonymous.
Both the string similarity classifier and the rela-
tion classifier are trained using L2-regularized lo-
gistic regression. The regularization parameter ? is
automatically selected on each iteration by search-
ing for a value which maximizes the loglikelihood
of a validation set, which is constructed by ran-
domly sampling 25% of L on each iteration. ? is
re-selected on each iteration because the initial la-
beled data set is extremely small, so the initial vali-
dation set is not necessarily representative of the ac-
tual data. In our experiments, the initial validation
set contains only 15 instances.
The string similarity classifier bases its decision
on the original noun phrase which mapped to each
sense. We use several string similarity measures as
features, including SoftTFIDF (Cohen et al, 2003),
Level 2 JaroWinkler (Cohen et al, 2003), Fellegi-
Sunter (Fellegi and Sunter, 1969), and Monge-Elkan
(Monge and Elkan, 1996). The first three algorithms
produce similarity scores by matching words in the
two phrases and the fourth is an edit distance. We
also use a heuristic abbreviation detection algorithm
(Schwartz and Hearst, 2003) and convert its output
into a score by dividing the length of the detected
abbreviation by the total length of the string.
The relation classifier?s features capture several
intuitive ways to determine that two items are syn-
onyms from the items they are related to. The re-
lation view contains three features for each relation
574
For each category C:
1. Initialize labeled data L with 10 positive and 50
negative examples (pairs of senses)
2. Initialize unlabeled data U by running canopies
(McCallum et al, 2000) on all senses in C.
3. Repeat 50 times:
i. Train the string similarity classifier on L
ii. Train the relation classifier on L
iii. Label U with each classifier
iv. Add the most confident 5 positive and 25
negative predictions of both classifiers to L
Figure 4: The co-training algorithm for learning synonym
classifiers.
r whose domain is compatible with the current cat-
egory. Consider the sense pair (s, t), and let r(s)
denote s?s values for relation r (i.e., r(s) = {v :
r(s, v)}). For each relation r, we instantiate the fol-
lowing features:
? (Senses which share values are synonyms)
The percent of values of r shared by both s and t,
that is |r(s)?r(t)||r(s)?r(t)| .
? (Senses with different values are not synonyms)
The percent of values of r not shared by s and t, or
1 ? |r(s)?r(t)||r(s)?r(t)| . The feature is set to 0 if either r(s)
or r(t) is empty. This feature is only instantiated if
the ontology specifies that r has at most one value
per sense.
? (Some relations indicate synonymy) A boolean
feature which is true if t ? r(s) or s ? r(t).
The output of co-training is a pair of classifiers for
each category. We combine their predictions using
the assumption that the two views X1, X2 are con-
ditionally independent given Y . As we trained both
classifiers using logistic regression, we have models
for the probabilities P (Y |X1) and P (Y |X2). The
conditional independence assumption implies that
we can combine their predictions using the formula:
P (Y = 1|X1, X2) =
P (Y = 1|X1)P (Y = 1|X2)P (Y = 0)
?
y=0,1 P (Y = y|X
1)P (Y = y|X2)(1? P (Y = y))
The above formula involves a prior term, P (Y ),
because the underlying classifiers are discrimina-
tive. We set P (Y = 1) = .5 in our experi-
ments as this setting reduces our dependence on the
(typically poorly calibrated) probability estimates of
logistic regression. We also limited the probabil-
ity predictions of each classifier to lie in [.01, .99]
to avoid divide-by-zero errors. The probability
P (Y |X1, X2) is the final synonym classifier which
is used for agglomerative clustering.
4.2.2 Agglomerative Clustering
The second step of our algorithm runs agglomera-
tive clustering to enforce transitivity constraints on
the predictions of the co-trained synonym classifier.
As noted in previous works (Snow et al, 2006), syn-
onymy is a transitive relation. If a and b are syn-
onyms, and b and c are synonyms, then a and c must
also be synonyms. Unfortunately, co-training is not
guaranteed to learn a function that satisfies these
transitivity constraints. We enforce the constraints
by running agglomerative clustering, as clusterings
of instances trivially satisfy the transitivity property.
ConceptResolver uses the clustering algorithm
described by Snow et al (2006), which defines a
probabilistic model for clustering and a procedure to
(locally) maximize the likelihood of the final cluster-
ing. The algorithm is essentially bottom-up agglom-
erative clustering of word senses using a similarity
score derived from P (Y |X1, X2). The similarity
score for two senses is defined as:
log
P (Y = 0)P (Y = 1|X1, X2)
P (Y = 1)P (Y = 0|X1, X2)
The similarity score for two clusters is the sum of
the similarity scores for all pairs of senses. The ag-
glomerative clustering algorithm iteratively merges
the two most similar clusters, stopping when the
score of the best possible pair is below 0. The clus-
ters of word senses produced by this process are the
concepts for each category.
5 Evaluation
We perform several experiments to measure Con-
ceptResolver?s performance at each of its respective
tasks. The first experiment evaluates word sense in-
duction using Freebase as a canonical set of con-
cepts. The second experiment evaluates synonym
resolution by comparing ConceptResolver?s sense
clusters to a gold standard clustering.
For both experiments, we used a knowledge base
created by running 140 iterations of NELL. We pre-
processed this knowledge base by removing all noun
575
phrases with zero extracted relations. As Concept-
Resolver treats the instances of each category pred-
icate independently, we chose 7 categories from
NELL?s ontology to use in the evaluation. The cat-
egories were selected on the basis of the number of
extracted relations that ConceptResolver could use
to detect synonyms. The number of noun phrases
in each category is shown in Table 2. We manually
labeled 10 pairs of synonymous senses for each of
these categories. The system automatically synthe-
sized 50 negative examples from the positive exam-
ples by assuming each pair represents a distinct con-
cept, so senses in different pairs are not synonyms.
5.1 Word Sense Induction Evaluation
Our first experiment evaluates the performance of
ConceptResolver?s category-based word sense in-
duction. We estimate two quantities: (1) sense pre-
cision, the fraction of senses created by our system
that correspond to real-world entities, and (2) sense
recall, the fraction of real-world entities that Con-
ceptResolver creates senses for. Sense recall is only
measured over entities which are represented by a
noun phrase in ConceptResolver?s input assertions ?
it is a measure of ConceptResolver?s ability to cre-
ate senses for the noun phrases it is given. Sense
precision is directly determined by how frequently
NELL?s extractors propose correct senses for noun
phrases, while sense recall is related to the correct-
ness of the one-sense-per-category assumption.
Precision and recall were evaluated by comparing
the senses created by ConceptResolver to concepts
in Freebase (Bollacker et al, 2008). We sampled
100 noun phrases from each category and matched
each noun phrase to a set of Freebase concepts. We
interpret each matching Freebase concept as a sense
of the noun phrase. We chose Freebase because it
had good coverage for our evaluation categories.
To align ConceptResolver?s senses with Freebase,
we first matched each of our categories with a set of
similar Freebase categories2. We then used a com-
bination of Freebase?s search API and Mechanical
Turk to align noun phrases with Freebase concepts:
we searched for the noun phrase in Freebase, then
had Mechanical Turk workers label which of the
2In Freebase, concepts are called Topics and categories are
called Types. For clarity, we use our terminology throughout.
Freebase
Category Precision Recall concepts
per Phrase
athlete 0.95 0.56 1.76
city 0.97 0.25 3.86
coach 0.86 0.94 1.06
company 0.85 0.41 2.41
country 0.74 0.56 1.77
sportsteam 0.89 0.30 3.28
stadiumoreventvenue 0.83 0.61 1.63
Table 1: ConceptResolver?s word sense induction perfor-
mance
Figure 5: Empirical distribution of the number of Free-
base concepts per noun phrase in each category
top 10 resulting Freebase concepts the noun phrase
could refer to. After obtaining the list of matching
Freebase concepts for each noun phrase, we com-
puted sense precision as the number of noun phrases
matching ? 1 Freebase concept divided by 100, the
total number of noun phrases. Sense recall is the re-
ciprocal of the average number of Freebase concepts
per noun phrase. Noun phrases matching 0 Freebase
concepts were not included in this computation.
The results of the evaluation in Table 1 show
that ConceptResolver?s word sense induction works
quite well for many categories. Most categories have
high precision, while recall varies by category. Cat-
egories like coach are relatively unambiguous, with
almost exactly 1 sense per noun phrase. Other cate-
gories have almost 4 senses per noun phrase. How-
ever, this average is somewhat misleading. Figure
5 shows the distribution of the number of concepts
per noun phrase in each category. The distribution
shows that most noun phrases are unambiguous, but
a small number of noun phrases have a large num-
ber of senses. In many cases, these noun phrases
576
are generic terms for many items in the category; for
example, ?palace? in stadiumoreventvenue refers
to 10 Freebase concepts. Freebase?s category def-
initions are also overly technical in some cases ?
for example, Freebase?s version of company has a
concept for each registered corporation. This defi-
nition means that some companies like Volkswagen
have more than one concept (in this case, 9 con-
cepts). These results suggest that the one-sense-per-
category assumption holds for most noun phrases.
An important footnote to this evaluation is that the
categories in NELL?s ontology are somewhat arbi-
trary, and that creating subcategories would improve
sense recall. For example, we could define subcat-
egories of sportsteam for various sports (e.g., foot-
ball team); these new categories would allow Con-
ceptResolver to distinguish between teams with the
same name that play different sports. Creating sub-
categories could improve performance in categories
with a high level of polysemy.
5.2 Synonym Resolution Evaluation
Our second experiment evaluates synonym resolu-
tion by comparing the concepts created by Concept-
Resolver to a gold standard set of concepts. Al-
though this experiment is mainly designed to eval-
uate ConceptResolver?s ability to detect synonyms,
it is somewhat affected by the word sense induc-
tion process. Specifically, the gold standard cluster-
ing contains noun phrases that refer to multiple con-
cepts within the same category. (It is unclear how
to create a gold standard clustering without allowing
such mappings.) The word sense induction process
produces only one of these mappings, which limits
maximum possible recall in this experiment.
For this experiment, we report two different mea-
sures of clustering performance. The first measure
is the precision and recall of pairwise synonym de-
cisions, typically known as cluster precision and re-
call. We dub this the clustering metric. We also
adopt the precision/recall measure from Resolver
(Yates and Etzioni, 2007), which we dub the Re-
solver metric. The Resolver metric aligns each pro-
posed cluster containing ? 2 senses with a gold
standard cluster (i.e., a real-world concept) by se-
lecting the cluster that a plurality of the senses in the
proposed cluster refer to. Precision is then the frac-
tion of senses in the proposed cluster which are also
in the gold standard cluster; recall is computed anal-
ogously by swapping the roles of the proposed and
gold standard clusters. Resolver precision can be in-
terpreted as the probability that a randomly sampled
sense (in a cluster with at least 2 senses) is in a clus-
ter representing its true meaning. Incorrect senses
were removed from the data set before evaluating
precision; however, these senses may still affect per-
formance by influencing the clustering process.
Precision was evaluated by sampling 100 random
concepts proposed by ConceptResolver, then manu-
ally scoring each concept using both of the metrics
above. This process mimics aligning each sampled
concept with its best possible match in a gold stan-
dard clustering, then measuring precision with re-
spect to the gold standard.
Recall was evaluated by comparing the system?s
output to a manually constructed set of concepts for
each category. To create this set, we randomly sam-
pled noun phrases from each category and manually
matched each noun phrase to one or more real-world
entities. We then found other noun phrases which re-
ferred to each entity and created a concept for each
entity with at least one unambiguous reference. This
process can create multiple senses for a noun phrase,
depending on the real-world entities represented in
the input assertions. We only included concepts con-
taining at least 2 senses in the test set, as singleton
concepts do not contribute to either recall metric.
The size of each recall test set is listed in Table 2;
we created smaller test sets for categories where syn-
onyms were harder to find. Incorrectly categorized
noun phrases were not included in the gold standard
as they do not correspond to any real-world entities.
Table 2 shows the performance of ConceptRe-
solver on each evaluation category. For each cat-
egory, we also report the baseline recall achieved
by placing each sense in its own cluster. Concept-
Resolver has high precision for several of the cate-
gories. Other categories like athlete and city have
somewhat lower precision. To make this difference
concrete, Figure 2 (first page) shows a random sam-
ple of 10 concepts from both company and athlete.
Recall varies even more widely across categories,
partly because the categories have varying levels of
polysemy, and partly due to differences in average
concept size. The differences in average concept
size are reflected in the baseline recall numbers.
577
Resolver Metric Clustering Metric
Category
# of Recall
Precision Recall F1
Baseline
Precision Recall F1
Baseline
Phrases Set Size Recall Recall
athlete 3886 80 0.69 0.69 0.69 0.46 0.41 0.45 0.43 0.00
city 5710 50 0.66 0.52 0.58 0.42 0.30 0.10 0.15 0.00
coach 889 60 0.90 0.93 0.91 0.43 0.83 0.88 0.85 0.00
company 3553 60 0.93 0.71 0.81 0.39 0.79 0.44 0.57 0.00
country 693 60 0.98 0.50 0.66 0.30 0.94 0.15 0.26 0.00
sportsteam 2085 100 0.95 0.48 0.64 0.29 0.87 0.15 0.26 0.00
stadiumoreventvenue 1662 100 0.84 0.73 0.78 0.39 0.65 0.49 0.56 0.00
Table 2: Synonym resolution performance of ConceptResolver
We attribute the differences in precision across
categories to the different relations available for
each category. For example, none of the relations for
athlete uniquely identify a single athlete, and there-
fore synonymy cannot be accurately represented in
the relation view. Adding more relations to NELL?s
ontology may improve performance in these cases.
We note that the synonym resolution portion of
ConceptResolver is tuned for precision, and that per-
fect recall is not necessarily attainable. Many word
senses participate in only one relation, which may
not provide enough evidence to detect synonymy.
As NELL continually extracts more knowledge, it
is reasonable for ConceptResolver to abstain from
these decisions until more evidence is available.
6 Discussion
In order for information extraction systems to ac-
curately represent knowledge, they must represent
noun phrases, concepts, and the many-to-many map-
ping from noun phrases to concepts they denote. We
present ConceptResolver, a system which takes ex-
tracted relations between noun phrases and identifies
latent concepts that the noun phrases refer to. Two
lessons from ConceptResolver are that (1) ontolo-
gies aid word sense induction, as the senses of pol-
ysemous words tend to have distinct semantic types,
and (2) redundant information, in the form of string
similarity and extracted relations, helps train accu-
rate synonym classifiers.
An interesting aspect of ConceptResolver is that
its performance should improve as NELL?s ontol-
ogy and knowledge base grow in size. Defining
finer-grained categories will improve performance
at word sense induction, as more precise categories
will contain fewer ambiguous noun phrases. Both
extracting more relation instances and adding new
relations to the ontology will improve synonym res-
olution. These scaling properties allow manual ef-
fort to be spent on high-level ontology operations,
not on labeling individual instances. We are inter-
ested in observing ConceptResolver?s performance
as NELL?s ontology and knowledge base grow.
For simplicity of exposition, we have implicitly
assumed thus far that the categories in NELL?s on-
tology are mutually exclusive. However, the ontol-
ogy contains compatible categories like male and
politician, where a single concept can belong to
both categories. In these situations, the one-sense-
per-category assumption may create too many word
senses. We currently address this problem with a
heuristic post-processing step: we merge all pairs of
concepts that belong to compatible categories and
share at least one referring noun phrase. This heuris-
tic typically works well, however there are prob-
lems. An example of a problematic case is ?obama,?
which NELL believes is a male, female, and politi-
cian. In this case, the heuristic cannot decide which
?obama? (the male or female) is the politician. As
such cases are fairly rare, we have not developed a
more sophisticated solution to this problem.
ConceptResolver has been integrated into NELL?s
continual learning process. NELL?s current set of
concepts can be viewed through the knowledge base
browser on NELL?s website, http://rtw.ml.
cmu.edu.
Acknowledgments
This work is supported in part by DARPA (under
contract numbers FA8750-08-1-0009 and AF8750-
09-C-0179) and by Google. We also gratefully ac-
knowledge the contributions of our colleagues on the
NELL project, Jamie Callan for the ClueWeb09 web
crawl and Yahoo! for use of their M45 computing
cluster. Finally, we thank the anonymous reviewers
for their helpful comments.
578
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task
02: Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the 4th International
Workshop on Semantic Evaluations, pages 7?12.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the web. In Proceedings of the
Twentieth International Joint Conference on Artificial
Intelligence, pages 2670?2676.
Sugato Basu, Mikhail Bilenko, and Raymond J. Mooney.
2004. A probabilistic framework for semi-supervised
clustering. In Proceedings of the Tenth ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, pages 59?68.
Indrajit Bhattacharya and Lise Getoor. 2006. A latent
dirichlet model for unsupervised entity resolution. In
Proceedings of the 2006 SIAM International Confer-
ence on Data Mining, pages 47?58.
Indrajit Bhattacharya and Lise Getoor. 2007. Collective
entity resolution in relational data. ACM Transactions
on Knowledge Discovery from Data, 1(1).
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the Eleventh Annual Conference on Computa-
tional Learning Theory, pages 92?100.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management of
Data, pages 1247?1250.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth AAAI Conference on Artificial Intelli-
gence.
William W. Cohen, Pradeep Ravikumar, and Stephen E.
Fienberg. 2003. A Comparison of String Distance
Metrics for Name-Matching Tasks. In Proceedings
of the IJCAI-03 Workshop on Information Integration,
pages 73?78, August.
Ivan P. Fellegi and Alan B. Sunter. 1969. A theory for
record linkage. Journal of the American Statistical As-
sociation, 64:1183?1210.
Lise Getoor and Christopher P. Diehl. 2005. Link min-
ing: a survey. SIGKDD Explorations Newsletter, 7:3?
12.
Aria Haghighi and Dan Klein. 2010. Coreference res-
olution in a modular, entity-centered model. In Pro-
ceedings of the 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 385?393, June.
Abraham Kaplan. 1955. An experimental study of ambi-
guity and context. Mechanical Translation, 2:39?46.
Dan Klein, Sepandar D. Kamvar, and Christopher D.
Manning. 2002. From instance-level constraints
to space-level constraints: Making the most of prior
knowledge in data clustering. In Proceedings of
the Nineteenth International Conference on Machine
Learning, pages 307?314.
Dekang Lin and Patrick Pantel. 2002. Concept discovery
from text. In Proceedings of the 19th International
Conference on Computational linguistics - Volume 1,
pages 1?7.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. Semeval-2010
task 14: Word sense induction & disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 63?68.
Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In Advances in Neural Information Pro-
cessing Systems 18.
Andrew McCallum, Kamal Nigam, and Lyle H. Un-
gar. 2000. Efficient clustering of high-dimensional
data sets with application to reference matching. In
Proceedings of the Sixth ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, pages 169?178.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38:39?41.
Alvaro Monge and Charles Elkan. 1996. The field
matching problem: Algorithms and applications. In
Proceedings of the Second International Conference
on Knowledge Discovery and Data Mining, pages
267?270.
Vincent Ng. 2008. Unsupervised models for coreference
resolution. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 640?649.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of ACM SIGKDD
Conference on Knowledge Discovery and Data Min-
ing, pages 613?619.
Hoifung Poon and Pedro Domingos. 2007. Joint infer-
ence in information extraction. In Proceedings of the
22nd AAAI Conference on Artificial Intelligence - Vol-
ume 1, pages 913?918.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with markov logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?08, pages 650?659.
579
Pradeep Ravikumar and William W. Cohen. 2004. A hi-
erarchical graphical model for record linkage. In Pro-
ceedings of the 20th Conference on Uncertainty in Ar-
tificial Intelligence, pages 454?461.
Ariel S. Schwartz and Marti A. Hearst. 2003. A sim-
ple algorithm for identifying abbreviation definitions
in biomedical text. In Proceedings of the Pacific Sym-
posium on BIOCOMPUTING 2003, pages 451?462.
Parag Singla and Pedro Domingos. 2006. Entity reso-
lution with markov logic. In Proceedings of the Sixth
International Conference on Data Mining, pages 572?
582.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th An-
nual Meeting of the Association for Computational
Linguistics, pages 801?808, Morristown, NJ, USA.
Rion Snow, Sushant Prakash, Daniel Jurafsky, and An-
drew Y. Ng. 2007. Learning to merge word senses.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1005?1014, June.
Richard C. Wang and William W. Cohen. 2007.
Language-independent set expansion of named enti-
ties using the web. In Proceedings of the Seventh IEEE
International Conference on Data Mining, pages 342?
350.
William E. Winkler. 1999. The state of record linkage
and current research problems. Technical report, Sta-
tistical Research Division, U.S. Census Bureau.
Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, and Stuart
Russell. 2003. Distance metric learning, with applica-
tion to clustering with side-information. In Advances
in Neural Information Processing Systems 17, pages
505?512.
Alexander Yates and Oren Etzioni. 2007. Unsupervised
resolution of objects and relations on the web. In Pro-
ceedings of the 2007 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
580
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1188?1198,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Joint Syntactic and Semantic Parsing with
Combinatory Categorial Grammar
Jayant Krishnamurthy
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
jayantk@cs.cmu.edu
Tom M. Mitchell
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
tom.mitchell@cmu.edu
Abstract
We present an approach to training a joint
syntactic and semantic parser that com-
bines syntactic training information from
CCGbank with semantic training informa-
tion from a knowledge base via distant su-
pervision. The trained parser produces a
full syntactic parse of any sentence, while
simultaneously producing logical forms
for portions of the sentence that have a se-
mantic representation within the parser?s
predicate vocabulary. We demonstrate our
approach by training a parser whose se-
mantic representation contains 130 pred-
icates from the NELL ontology. A seman-
tic evaluation demonstrates that this parser
produces logical forms better than both
comparable prior work and a pipelined
syntax-then-semantics approach. A syn-
tactic evaluation on CCGbank demon-
strates that the parser?s dependency F-
score is within 2.5% of state-of-the-art.
1 Introduction
Integrating syntactic parsing with semantics has
long been a goal of natural language processing
and is expected to improve both syntactic and se-
mantic processing. For example, semantics could
help predict the differing prepositional phrase at-
tachments in ?I caught the butterfly with the net?
and ?I caught the butterfly with the spots.? A joint
analysis could also avoid propagating syntactic
parsing errors into semantic processing, thereby
improving performance.
We suggest that a large populated knowledge
base should play a key role in syntactic and se-
mantic parsing: in training the parser, in resolv-
ing syntactic ambiguities when the trained parser
is applied to new text, and in its output semantic
representation. Using semantic information from
the knowledge base at training and test time will
ideally improve the parser?s ability to solve diffi-
cult syntactic parsing problems, as in the exam-
ples above. A semantic representation tied to a
knowledge base allows for powerful inference op-
erations ? such as identifying the possible entity
referents of a noun phrase ? that cannot be per-
formed with shallower representations (e.g., frame
semantics (Baker et al, 1998) or a direct conver-
sion of syntax to logic (Bos, 2005)).
This paper presents an approach to training a
joint syntactic and semantic parser using a large
background knowledge base. Our parser produces
a full syntactic parse of every sentence, and fur-
thermore produces logical forms for portions of
the sentence that have a semantic representation
within the parser?s predicate vocabulary. For ex-
ample, given a phrase like ?my favorite town in
California,? our parser will assign a logical form
like ?x.CITY(x) ? LOCATEDIN(x,CALIFORNIA)
to the ?town in California? portion. Additionally,
the parser uses predicate and entity type informa-
tion during parsing to select a syntactic parse.
Our parser is trained by combining a syntactic
parsing task with a distantly-supervised relation
extraction task. Syntactic information is provided
by CCGbank, a conversion of the Penn Treebank
into the CCG formalism (Hockenmaier and Steed-
man, 2002a). Semantics are learned by training
the parser to extract knowledge base relation in-
stances from a corpus of unlabeled sentences, in
a distantly-supervised training regime. This ap-
proach uses the knowledge base to avoid expen-
sive manual labeling of individual sentence se-
mantics. By optimizing the parser to perform both
tasks simultaneously, we train a parser that pro-
duces accurate syntactic and semantic analyses.
We demonstrate our approach by training a joint
syntactic and semantic parser, which we call ASP.
ASP produces a full syntactic analysis of every
sentence while simultaneously producing logical
forms containing any of 61 category and 69 re-
1188
lation predicates from NELL. Experiments with
ASP demonstrate that jointly analyzing syntax
and semantics improves semantic parsing perfor-
mance over comparable prior work and a pipelined
syntax-then-semantics approach. ASP?s syntactic
parsing performance is within 2.5% of state-of-
the-art; however, we also find that incorporating
semantic information reduces syntactic parsing ac-
curacy by ? 0.5%.
2 Prior Work
This paper combines two lines of prior work:
broad coverage syntactic parsing with CCG and
semantic parsing.
Broad coverage syntactic parsing with CCG has
produced both resources and successful parsers.
These parsers are trained and evaluated using
CCGbank (Hockenmaier and Steedman, 2002a),
an automatic conversion of the Penn Treebank
into the CCG formalism. Several broad cover-
age parsers have been trained using this resource
(Hockenmaier and Steedman, 2002b; Hocken-
maier, 2003b). The parsing model in this paper is
loosely based on C&C (Clark and Curran, 2007b;
Clark and Curran, 2007a), a discriminative log-
linear model for statistical parsing. Some work
has also attempted to automatically derive logi-
cal meaning representations directly from syntac-
tic CCG parses (Bos, 2005; Lewis and Steedman,
2013). However, these approaches to semantics do
not ground the text to beliefs in a knowledge base.
Meanwhile, work on semantic parsing has fo-
cused on producing semantic parsers for answer-
ing simple natural language questions (Zelle and
Mooney, 1996; Ge and Mooney, 2005; Wong and
Mooney, 2006; Wong and Mooney, 2007; Lu et
al., 2008; Kate and Mooney, 2006; Zettlemoyer
and Collins, 2005; Kwiatkowski et al, 2011). This
line of work has typically used a corpus of sen-
tences with annotated logical forms to train the
parser. Recent work has relaxed the requisite su-
pervision conditions (Clarke et al, 2010; Liang et
al., 2011), but has still focused on simple ques-
tions. Finally, some work has looked at applying
semantic parsing to answer queries against large
knowledge bases, such as YAGO (Yahya et al,
2012) and Freebase (Cai and Yates, 2013b; Cai
and Yates, 2013a; Kwiatkowski et al, 2013; Be-
rant et al, 2013). Although this work considers
a larger number (thousands) of predicates than we
do, none of these systems are capable of parsing
open-domain text. Our approach is most closely
related to the distantly-supervised approach of Kr-
ishnamurthy and Mitchell (2012).
The parser presented in this paper can be viewed
as a combination of both a broad coverage syn-
tactic parser and a semantic parser trained using
distant supervision. Combining these two lines
of work has synergistic effects ? for example, our
parser is capable of semantically analyzing con-
junctions and relative clauses based on the syn-
tactic annotation of these categories in CCGbank.
This synergy gives our parser a richer semantic
representation than previous work, while simulta-
neously enabling broad coverage.
3 Parser Design
This section describes the Combinatory Categorial
Grammar (CCG) parsing model used by ASP. The
input to the parser is a part-of-speech tagged sen-
tence, and the output is a syntactic CCG parse tree,
along with zero or more logical forms representing
the semantics of subspans of the sentence. These
logical forms are constructed using category and
relation predicates from a broad coverage knowl-
edge base. The parser also outputs a collection of
dependency structures summarizing the sentence?s
predicate-argument structure. Figure 1 illustrates
ASP?s input/output specification.
3.1 Knowledge Base
The parser uses category and relation predicates
from a broad coverage knowledge base both to
construct logical forms and to parametrize the
parsing model. The knowledge base is assumed
to have two kinds of ontological structure: a gen-
eralization/subsumption hierarchy and argument
type constraints. This paper uses NELL?s ontology
(Carlson et al, 2010), which, for example, speci-
fies that the category ORGANIZATION is a general-
ization of SPORTSTEAM, and that both arguments
to the LOCATEDIN relation must have type LOCA-
TION. These type constraints are enforced during
parsing. Throughout this paper, predicate names
are shown in SMALLCAPS.
3.2 Syntax
ASP uses a lexicalized and semantically-
typed Combinatory Categorial Grammar
(CCG) (Steedman, 1996). Most gram-
matical information in CCG is encoded in
a lexicon ?, containing entries such as:
1189
area / NN
N
?x.LOCATION(x)
that / WDT
(N
1
\N
1
)/(S[dcl]\NP
1
)
2
?f.?g.?z.g(z) ? f(?y.y = z)
includes / VBZ
(S[dcl]\NP
1
)/NP
2
?f.?g.?x, y.g(x) ? f(y)
? LOCATEDIN(y, x)
beautiful / JJ
N
1
/N
1
?f.f
London / NNP
N
?x.M(x, ?london?, CITY)
N : ?x.M(x, ?london?, CITY)
(S[dcl]\NP
1
) :
?g.?x, y.g(x) ? M(y, ?london?, CITY) ? LOCATEDIN(y, x)
N
1
\N
1
: ?g.?z.?x, y.g(z) ? x = z ? M(y, ?london?, CITY) ? LOCATEDIN(y, x)
N : ?z.?x, y.LOCATION(z) ? x = z ? M(y, ?london?, CITY) ? LOCATEDIN(y, x)
Head Argument
word POS semantic type index syntactic category arg. num. word POS semantic type index
that WDT ? 1 (N
1
\N
1
)/(S\NP
1
)
2
1 area NN LOCATION 0
that WDT ? 1 (N
1
\N
1
)/(S\NP
1
)
2
2 includes VBZ LOCATEDIN
?1
2
includes VBZ LOCATEDIN
?1
2 (S[dcl]\NP
1
)/NP
2
1 area NN LOCATION 0
includes VBZ LOCATEDIN
?1
2 (S[dcl]\NP
1
)/NP
2
2 ENTITY:CITY NNP CITY 4
beautiful JJ ? 3 N
1
/N
1
1 ENTITY:CITY NNP CITY 4
Figure 1: Example input and output for ASP. Given a POS-tagged sentence, the parser produces a CCG
syntactic tree and logical form (top), and a collection of dependency structures (bottom).
person := N : PERSON : ?x.PERSON(x)
London := N : CITY : ?x.M(x, ?london?, CITY)
great := N
1
/N
1
: ? : ?f.?x.f(x)
bought :=
(S[dcl]\NP
1
)/NP
2
: ACQUIRED :
?f.?g.?x, y.f(y) ? g(x) ? ACQUIRED(x, y)
Each lexicon entry maps a word to a syntactic
category, semantic type, and logical form. CCG
has two kinds of syntactic categories: atomic and
functional. Atomic categories include N for noun
and S for sentence. Functional categories are
functions constructed recursively from atomic cat-
egories; these categories are denoted using slashes
to separate the category?s argument type from its
return type. The argument type appears on the
right side of the slash, and the return type on the
left. The direction of slash determines where the
argument must appear ? / means an argument on
the right, and \ means an argument on the left.
Syntactic categories in ASP are annotated with
two additional kinds of information. First, atomic
categories may have associated syntactic features
given in square brackets. These features are used
in CCGbank to distinguish variants of atomic syn-
tactic categories, e.g., S[dcl] denotes a declara-
tive sentence. Second, each category is anno-
tated with head and dependency information us-
ing subscripts. These subscripts are used to pop-
ulate predicate-argument dependencies (described
below), and to pass head information using unifi-
cation. For example, the head of the parse in Fig-
ure 1 is ?area,? due to the coindexing of the argu-
ment and return categories in the categoryN
1
\N
1
.
In addition to the syntactic category, each lexi-
con entry has a semantic type and a logical form.
The semantic type is a category or relation pred-
icate that concisely represents the word?s seman-
tics. The semantic type is used to enforce type
constraints during parsing and to include seman-
tics in the parser?s parametrization. The logi-
cal form gives the full semantics of the word in
lambda calculus. The parser also allows lexicon
entries with the semantic type ???, representing
words whose semantics cannot be expressed using
predicates from the ontology.
Parsing in CCG combines adjacent categories
using a small number of combinators, such as
function application:
X/Y : f Y : g =? X : f(g)
Y : g X\Y : f =? X : f(g)
The first rule states that the category X/Y can
be applied to the category Y , returning category
X , and that the logical form f is applied to g to
produce the logical form for the returned category.
Head words and semantic types are also propa-
gated to the returned category based on the anno-
tated head-passing markup.
3.3 Dependency Structures
Parsing a sentence produces a collection of depen-
dency structures which summarize the predicate-
argument structure of the sentence. Dependency
structures are 10-tuples, of the form:
< head word, head POS, head semantic type, head word
index, head word syntactic category, argument number, ar-
gument word, argument POS, argument semantic type, argu-
ment word index >
A dependency structure captures a relationship
between a head word and its argument. During
parsing, whenever a subscripted argument of a
syntactic category is filled, a dependency structure
1190
is created between the head of the applied func-
tion and its argument. For example, in Figure 1,
the first application fills argument 1 of ?beautiful?
with ?London,? creating a dependency structure.
3.4 Logical Forms
ASP performs a best-effort semantic analysis of
every parsed sentence, producing logical forms for
subspans of the sentence when possible. Logical
forms are designed so that the meaning of a sen-
tence is a universally- and existentially-quantified
conjunction of predicates with partially shared ar-
guments. This representation allows the parser to
produce semantic analyses for a reasonable subset
of language, including prepositions, verbs, nouns,
relative clauses, and conjunctions.
Figure 1 shows a representative sample of a log-
ical form produced by ASP. Generally, the parser
produces a lambda calculus statement with sev-
eral existentially-quantified variables ranging over
entities in the knowledge base. The only excep-
tion to this rule is conjunctions, which are rep-
resented using a scoped universal quantifier over
the conjoined predicates. Entity mentions appear
in logical forms via a special mention predicate,
M, instead of as database constants. For exam-
ple, ?London? appears as M(x, ?london?, CITY),
instead of as a constant like LONDON. The mean-
ing of this mention predicate is that x is an en-
tity which can be called ?london? and belongs to
the CITY category. This representation propagates
uncertainty about entity references into the logical
form where background knowledge can be used
for disambiguation. For example, ?London, Eng-
land? is assigned a logical form that disambiguates
?London? to a ?London? located in ?England.?
1
Lexicon entries without a semantic type are au-
tomatically assigned logical forms based on their
head passing markup. For example, in Figure 1,
the adjective ?beautiful? is assigned ?f.f . This
approach allows a logical form to be derived for
most sentences, but (somewhat counterintuitively)
can lose interesting logical forms from constituent
subspans. For example, the preposition ?in? has
syntactic category (N
1
\N
1
)/N
2
, which results in
the logical form ?f.?g.g. This logical form dis-
cards any information present in the argument f .
We avoid this problem by extracting a logical form
from every subtree of the CCG parse.
1
Specifically, ?x.?y.CITYLOCATEDINCOUNTRY(x, y) ?
M(x, ?london?, CITY) ? M(y, ?england?, COUNTRY)
3.5 Parametrization
The parser ? is trained as a discriminative linear
model of the following form:
?(`, d, t|s; ?) = ?
T
?(d, t, s)
Given a parameter vector ? and a sentence s, the
parser produces a score for a syntactic parse tree
t, a collection of dependency structures d and a
logical form `. The score depends on features of
the parse produced by the feature function ?.
? contains four classes of features: lexicon
features, combinator features, dependency fea-
tures and dependency distance features (Table 1).
These features are based on those of C&C (Clark
and Curran, 2007b), modified to include seman-
tic types. The features are designed to share syn-
tactic information about a word across its distinct
semantic realizations in order to transfer syntactic
information from CCGbank to semantic parsing.
The parser also includes a hard type-checking
constraint to ensure that logical forms are well-
typed. This constraint states that dependency
structures with a head semantic type only accept
arguments that (1) have a semantic type, and (2)
are within the domain/range of the head type.
4 Parameter Estimation
This section describes the training procedure for
ASP. Training is performed by minimizing a joint
objective function combining a syntactic parsing
task and a distantly-supervised relation extraction
task. The input training data includes:
1. A collection L of sentences s
i
with annotated
syntactic trees t
i
(e.g., CCGbank).
2. A corpus of sentences S (e.g., Wikipedia).
3. A knowledge base K (e.g., NELL), contain-
ing relation instances r(e
1
, e
2
) ? K.
4. A CCG lexicon ? (see Section 5.2).
Given these resources, the algorithm described
in this section produces parameters ? for a se-
mantic parser. Our parameter estimation proce-
dure constructs a joint objective functionO(?) that
decomposes into syntactic and semantic compo-
nents: O(?) = O
syn
(?) + O
sem
(?). The syntac-
tic component O
syn
is a standard syntactic pars-
ing objective constructed using the syntactic re-
source L. The semantic component O
sem
is a
distantly-supervised relation extraction task based
on the semantic constraint from Krishnamurthy
and Mitchell (2012). These components are de-
scribed in more detail in the following sections.
1191
Lexicon features: word, POS := X : t : `
Word/syntactic category word, X
POS/syntactic category POS, X
Word semantics word, X, t
Combinator features: X Y ? Z or X ? Z
Binary combinator indicator X Y ? Z
Unary combinator indicator X ? Z
Root syntactic category Z
Dependency Features: < h
w
, h
p
, h
t
, h
i
, s, n, a
w
, a
p
, a
t
, a
i
>
Predicate-Argument Indicator < h
w
,?, h
t
,?, s, n, a
w
,?, a
t
,? >
Word-Word Indicator < h
w
,?,?,?, s, n, a
w
,?,?,? >
Predicate-POS Indicator < h
w
,?, h
t
,?, s, n,?, a
p
,?,? >
Word-POS Indicator < h
w
,?,?,?, s, n,?, a
p
,?,? >
POS-Argument Indicator < ?, h
p
,?,?, s, n, a
w
,?, a
t
,? >
POS-Word Indicator < ?, h
p
,?,?, s, n, a
w
,?,?,? >
POS-POS Indicator < ?, h
p
,?,?, s, n,?, a
p
,?,? >
Dependency Distance Features:
Token distance h
w
, h
t
,?, s, n, d d = Number of tokens between h
i
and a
i
: 0, 1, 2 or more.
Token distance word backoff h
w
,?, s, n, d d = Number of tokens between h
i
and a
i
: 0, 1, 2 or more.
Token distance POS backoff ?,?, h
p
, s, n, d d = Number of tokens between h
i
and a
i
: 0, 1, 2 or more.
(The above distance features are repeated using the number of intervening verbs and punctuation marks.)
Table 1: Listing of parser feature templates used in the feature function ?. Each feature template repre-
sents a class of indicator features that fire during parsing when lexicon entries are used, combinators are
applied, or dependency structures are instantiated.
4.1 Syntactic Objective
The syntactic objective is the structured percep-
tron objective instantiated for a syntactic parsing
task. This objective encourages the parser to accu-
rately reproduce the syntactic parses in the anno-
tated corpus L = {(s
i
, t
i
)}
n
i=1
:
O
syn
(?) =
n
?
i=1
|max
?
`,
?
d,
?
t
?(
?
`,
?
d,
?
t|s
i
; ?)?
max
`
?
,d
?
?(`
?
, d
?
, t
i
|s
i
; ?)|
+
The first term in the above expression represents
the best CCG parse of the sentence s
i
according to
the current model. The second term is the best
parse of s
i
whose syntactic tree equals the true
syntactic tree t
i
. In the above equation | ? |
+
de-
notes the positive part of the expression. Minimiz-
ing this objective therefore finds parameters ? that
reproduce the annotated syntactic trees.
4.2 Semantic Objective
The semantic objective corresponds to a distantly-
supervised relation extraction task that constrains
the logical forms produced by the semantic parser.
Distant supervision is provided by the following
constraint: every relation instance r(e
1
, e
2
) ? K
must be expressed by at least one sentence in
S
(e
1
,e
2
)
, the set of sentences that mention both e
1
and e
2
(Hoffmann et al, 2011). If this constraint
is empirically true and sufficiently constrains the
parser?s logical forms, then optimizing the seman-
tic objective produces an accurate semantic parser.
A training example in the semantic objective
consists of the set of sentences mentioning a pair
of entities, S
(e
1
,e
2
)
= {s
1
, s
2
, ...}, paired with a
binary vector representing the set of relations that
the two entities participate in, y
(e
1
,e
2
)
. The distant
supervision constraint ? forces the logical forms
predicted for the sentences to entail the relations
in y
(e
1
,e
2
)
. ? is a deterministic OR constraint that
checks whether each logical form entails the re-
lation instance r(e
1
, e
2
), deterministically setting
y
r
= 1 if any logical form entails the instance and
y
r
= 0 otherwise.
Let (`,d, t) represent a collection of seman-
tic parses for the sentences S = S
(e
1
,e
2
)
. Let
?(`,d, t|S; ?) =
?
|S|
i=1
?(`
i
, d
i
, t
i
|s
i
; ?) represent
the total weight assigned by the parser to a collec-
tion of parses for the sentences S. For the pair of
entities (e
1
, e
2
), the semantic objective is:
O
sem
(?) = |max
?
`,
?
d,
?
t
?(
?
`,
?
d,
?
t|S; ?)? max
`
?
,d
?
,t
?
(
?(y
(e
1
,e
2
)
, `
?
,d
?
, t
?
) + ?(`
?
,d
?
, t
?
|S; ?)
)
|
+
4.3 Optimization
Training minimizes the joint objective using the
structured perceptron algorithm, which can be
viewed as the stochastic subgradient method
(Ratliff et al, 2006) applied to the objective
O(?). We initialize the parameters to zero, i.e.,
?
0
= 0. On each iteration, we sample either a
syntactic example (s
i
, t
i
) or a semantic example
(S
(e
1
,e
2
)
, y
(e
1
,e
2
)
). If a syntactic example is sam-
pled, we apply the following parameter update:
?
`,
?
d,
?
t ? arg max
`,d,t
?(`, d, t|s
i
; ?
t
)
`
?
, d
?
? arg max
`,d
?(`, d, t
i
|s
i
; ?
t
)
?
t+1
? ?
t
+ ?(d
?
, t
i
, s
i
)? ?(
?
d,
?
t, s
i
)
This update moves the parameters toward the fea-
tures of the best parse with the correct syntactic
derivation, ?(d
?
, t
i
, s
i
). If a semantic example is
1192
Labeled Dependencies Unlabeled Dependencies
P R F P R F Coverage
ASP 85.58 85.31 85.44 91.75 91.46 91.60 99.63
ASP-SYN 86.06 85.84 85.95 92.13 91.89 92.01 99.63
C&C (Clark and Curran, 2007b) 88.34 86.96 87.64 93.74 92.28 93.00 99.63
(Hockenmaier, 2003a) 84.3 84.6 84.4 91.8 92.2 92.0 99.83
Table 2: Syntactic parsing results for Section 23 of CCGbank. Parser performance is measured using
precision (P), recall (R) and F-measure (F) of labeled and unlabeled dependencies.
sampled, we instead apply the following update:
?
`,
?
d,
?
t? arg max
`,d,t
?(`,d, t|S
(e
1
,e
2
)
; ?
t
)
`
?
,d
?
, t
?
? arg max
`,d,t
?(`,d, t|S
(e
1
,e
2
)
; ?
t
)
+ ?(y
(e
1
,e
2
)
, `,d, t)
?
t+1
? ?
t
+ ?(d
?
, t
?
,S
(e
1
,e
2
)
)
? ?(
?
d,
?
t,S
(e
1
,e
2
)
)
This update moves the parameters toward the fea-
tures of the best set of parses that satisfy the distant
supervision constraint. Training outputs the aver-
age of each iteration?s parameters,
?
? =
1
n
?
n
t=1
?
t
.
In practice, we train the parser by performing a
single pass over the examples in the data set.
All of the maximizations above can be per-
formed exactly using a CKY-style chart parsing
algorithm, except for the last one. This maxi-
mization is intractable due to the coupling between
logical forms in ` caused by enforcing the dis-
tant supervision constraint. We approximate this
maximization in two steps. First, we perform a
beam search to produce a list of candidate parses
for each sentence s ? S
(e
1
,e
2
)
. We then extract
relation instances from each parse and apply the
greedy inference algorithm from Hoffmann et al,
(2011) to identify the best set of parses that satisfy
the distant supervision constraint. The procedure
skips any examples with sentences that cannot be
parsed (due to beam search failures) or where the
distant supervision constraint cannot be satisfied.
5 Experiments
The experiments below evaluate ASP?s syntactic
and semantic parsing ability. The parser is trained
on CCGbank and a corpus of Wikipedia sentences,
using NELL?s predicate vocabulary. The syntactic
analyses of the trained parser are evaluated against
CCGbank, and its logical forms are evaluated on
an information extraction task and against an an-
notated test set of Wikipedia sentences.
5.1 Data Sets
The data sets for the evaluation consist of CCG-
bank, a corpus of dependency-parsed Wikipedia
sentences, and a logical knowledge base derived
from NELL and Freebase. Sections 02-21 of
CCGbank were used for training, Section 00 for
validation, and Section 23 for the final results. The
knowledge base?s predicate vocabulary is taken
from NELL, and its instances are taken from Free-
base using a manually-constructed mapping be-
tween Freebase and NELL. Using Freebase rela-
tion instances produces cleaner training data than
NELL?s automatically-extracted instances.
Using the relation instances and Wikipedia sen-
tences, we constructed a data set for distantly-
supervised relation extraction. We identified men-
tions of entities in each sentence using simple
string matching, then aggregated these sentences
by entity pair. 20% of the entity pairs were set
aside for validation. In the remaining training
data, we downsampled entity pairs that did not
participate in at least one relation. We further
eliminated sentences containing more than 30 to-
kens. The resulting training corpus contains 25k
entity pairs (half of which participate in a relation),
41k sentences, and 71 distinct relation predicates.
5.2 Grammar Construction
The grammar for ASP contains the annotated lex-
icon entries and grammar rules in Sections 02-21
of CCGbank, and additional semantic entries pro-
duced using a set of dependency parse heuristics.
The lexicon ? contains all words that occur at
least 20 times in CCGbank. Rare words are re-
placed by their part of speech. The head pass-
ing and dependency markup was generated using
the rules of the C&C parser (Clark and Curran,
2007b). These lexicon entries are also annotated
with logical forms capturing their head passing re-
lationship. For example, the adjective category
N
1
/N
1
is annotated with the logical form ?f.f .
These entries are all assigned semantic type ?.
We augment this lexicon with additional entries
1193
Sentence Extracted Logical Form
St. John, a Mexican-American born in San Francisco, Califor-
nia, her family comes from Zacatecas, Mexico.
?x.?y, z.M(x, ?st. john?) ? M(y, ?san francisco?) ?
PERSONBORNINLOCATION(x, y) ?
CITYLOCATEDINSTATE(y, z) ? M(z, ?california?)
The capital and largest city of Laos is Vientiane and other major
cities include Luang Prabang, Savannakhet and Pakse.
?x, y.M(x, ?vientiane?) ? CITY(x) ?
CITYCAPITALOFCOUNTRY(x, y) ? M(y, ?laos?)
Gellar next played a lead role in James Toback ?s critically
unsuccessful independent ?Harvard Man? (2001), where she
played the daughter of a mobster.
?x.?y.M(y, ?james toback?) ?
DIRECTORDIRECTEDMOVIE(y, x) ?
M(x, ?harvard man?)
Figure 2: Logical forms produced by ASP for sentences in the information extraction corpus. Each
logical form is extracted from the underlined sentence portion.
ASP
PIPELINE
K&M-2012
0 300 600 900
0
0.2
0.4
0.6
0.8
1.0
Figure 3: Logical form precision as a function of
the expected number of correct extracted logical
forms. ASP extracts more correct logical forms
because it jointly analyzes syntax and semantics.
mapping words to logical forms with NELL pred-
icates. These entries are instantiated using a set
of dependency parse patterns, listed in an online
appendix.
2
These patterns are applied to the train-
ing corpus, heuristically identifying verbs, prepo-
sitions, and possessives that express relations, and
nouns that express categories. The patterns also
include special cases for forms of ?to be.? This
process generates ?4000 entries (not counting en-
tity names), representing 69 relations and 61 cate-
gories from NELL. Section 3.2 shows several lex-
icon entries generated by this process.
The parser?s combinators include function ap-
plication, composition, and crossed composition,
as well as several binary and unary type-changing
rules that occur in CCGbank. All combinators
were restricted to only apply to categories that
combine in Sections 02-21. Finally, the grammar
includes a number of heuristically-instantiated bi-
nary rules of the form , N ? N\N that instanti-
ate a relation between adjacent nouns. These rules
capture appositives and some other constructions.
5.3 Supertagging
Parsing in practice can be slow because the
parser?s lexicalized grammar permits a large num-
ber of parses for a sentence. We improve parser
performance by performing supertagging (Banga-
2
http://rtw.ml.cmu.edu/acl2014_asp/
lore and Joshi, 1999; Clark and Curran, 2004).
We trained a logistic regression classifier to pre-
dict the syntactic category of each token in a sen-
tence from features of the surrounding tokens and
POS tags. Subsequent parsing is restricted to only
consider categories whose probability is within a
factor of ? of the highest-scoring category. The
parser uses a backoff strategy, first attempting to
parse with the supertags from ? = 0.01, backing
off to ? = 0.001 if the initial parsing attempt fails.
5.4 Syntactic Evaluation
The syntactic evaluation measures ASP?s ability
to reproduce the predicate-argument dependencies
in CCGbank. As in previous work, our evalu-
ation uses labeled and unlabeled dependencies.
Labeled dependencies are dependency structures
with both words and semantic types removed,
leaving two word indexes, a syntactic category,
and an argument number. Unlabeled dependen-
cies further eliminate the syntactic category and
argument number, leaving a pair of word indexes.
Performance is measured using precision, recall,
and F-measure against the annotated dependency
structures in CCGbank. Precision is the fraction
of predicted dependencies which are in CCGbank,
recall is the fraction of CCGbank dependencies
produced by the parser, and F-measure is the har-
monic mean of precision and recall.
For comparison, we also trained a syntactic ver-
sion of our parser, ASP-SYN, using only the CCG-
bank lexicon and grammar. Comparing against
this parser lets us measure the effect of the rela-
tion extraction task on syntactic parsing.
Table 2 shows the results of our evaluation.
For comparison, we include results for two ex-
isting syntactic CCG parsers: C&C, the current
state-of-the-art CCG parser (Clark and Curran,
2007b), and the next best system (Hockenmaier,
2003a). Both ASP and ASP-SYN perform rea-
sonably well, within 2.5% of the performance of
C&C at the same coverage level. However, ASP-
1194
Logical Form Extraction Extraction
Accuracy Precision Recall
ASP 0.28 0.90 0.32
K&M-2012 0.14 1.00 0.06
PIPELINE 0.2 0.63 0.17
Table 3: Logical form accuracy and extraction pre-
cision/recall on the annotated test set. The high
extraction recall for ASP shows that it produces
more complete logical forms than either baseline.
SYN outperforms ASP by around 0.5%, suggesting
that ASP?s additional semantic knowledge slightly
hurts syntactic parsing performance. This perfor-
mance loss appears to be largely due to poor en-
tity mention detection, as we found that not us-
ing entity mention lexicon entries at test time im-
proves ASP?s labeled and unlabeled F-scores by
0.3% on Section 00. The knowledge base contains
many infrequently-mentioned entities with com-
mon names; these entities contribute incorrect se-
mantic type information that confuses the parser.
5.5 Semantic Evaluation
We performed two semantic evaluations to bet-
ter understand ASP?s ability to construct logical
forms. The first evaluation emphasizes precision
over recall, and the second evaluation accurately
measures recall using a manually labeled test set.
5.5.1 Baselines
For comparison, we also trained two base-
line models. The first baseline, PIPELINE, is
a pipelined syntax-then-semantics approach de-
signed to mimic Boxer (Bos, 2005). This base-
line first syntactically parses each sentence using
ASP-SYN, then produces a semantic analysis by
assigning a logical form to each word. We train
this baseline using the semantic objective (Section
4.2) while holding fixed the syntactic parse of each
sentence. Note that, unlike Boxer, this baseline
learns which logical form to assign each word, and
its logical forms contain NELL predicates.
The second baseline, K&M-2012, is the ap-
proach of Krishnamurthy and Mitchell (2012),
representing the state-of-the-art in distantly-
supervised semantic parsing. This approach trains
a semantic parser by combining distant seman-
tic supervision with syntactic supervision from
dependency parses. The best performing vari-
ant of this system also uses dependency parses
at test time to constrain the interpretation of
test sentences ? hence, this system also uses a
pipelined syntax-then-semantics approach. To im-
prove comparability, we reimplemented this ap-
proach using our parsing model, which has richer
features than were used in their paper.
5.5.2 Information Extraction Evaluation
The information extraction evaluation uses each
system to extract logical forms from a large cor-
pus of sentences, then measures the fraction of
extracted logical forms that are correct. The test
set consists of 8.5k sentences sampled from the
held-out Wikipedia sentences. Each system was
run on this data set, extracting all logical forms
from each sentence that entailed at least one cat-
egory or relation instance. We ranked these ex-
tractions using the parser?s inside chart score, then
manually annotated a sample of 250 logical forms
from each system for correctness. Logical forms
were marked correct if all category and relation
instances entailed by the logical form were ex-
pressed by the sentence. Note that a correct logical
form need not entail all of the relations expressed
by the sentence, reflecting an emphasis on preci-
sion over recall. Figure 2 shows some example
logical forms produced by ASP in the evaluation.
The annotated sample of logical forms allows
us to estimate precision for each system as a func-
tion of the number of correct extractions (Figure
3). The number of correct extractions is directly
proportional to recall, and was estimated from the
total number of extractions and precision at each
rank in the sample. All three systems initially
have high precision, implying that their extracted
logical forms express facts found in the sentence.
However, ASP produces 3 times more correct log-
ical forms than either baseline because it jointly
analyzes syntax and semantics. The baselines suf-
fer from reduced recall because they depend on re-
ceiving an accurate syntactic parse as input; syn-
tactic parsing errors cause these systems to fail.
Examining the incorrect logical forms produced
by ASP reveals that incorrect mention detection is
by far the most common source of mistakes. Ap-
proximately 50% of errors are caused by marking
common nouns as entity mentions (e.g., marking
?coin? as a COMPANY). These errors occur be-
cause the knowledge base contains many infre-
quently mentioned entities with relatively com-
mon names. Another 30% of errors are caused by
assigning an incorrect type to a common proper
noun (e.g, marking ?Bolivia? as a CITY). This
analysis suggests that performing entity linking
before parsing could significantly reduce errors.
1195
Sentence: De Niro and Joe Pesci in ?Goodfellas? offered a virtuoso display of the director?s bravura cinematic
technique and reestablished, enhanced, and consolidated his reputation.
Annotation:
LF: ?x.?p ? {?d.M(d, ?de niro?), ?j.M(j, ?joe pesci?)}?y.p(x) ? STARREDINMOVIE(x, y) ? M(y, ?goodfellas?)
Instances: STARREDINMOVIE(de niro, goodfellas), STARREDINMOVIE(joe pesci, goodfellas)
Prediction:
LF: ?x.?p ? {?d.M(d, ?de niro?), ?j.M(j, ?joe pesci?)}?y.p(x) ? STARREDINMOVIE(x, y) ? M(y, ?goodfellas?)
Instances: STARREDINMOVIE(de niro, goodfellas), STARREDINMOVIE(joe pesci, goodfellas)
Logical form accuracy: 1 / 1 Extraction Precision: 2 / 2 Extraction Recall: 2 / 2
Sentence: In addition to the University of Illinois, Champaign is also home to Parkland College.
Annotation:
LF: ?c, p.M(c, ?champaign?) ? CITY(c) ? M(p, ?parkland college?) ? UNIVERSITYINCITY(p, c)
Instances: CITY(champaign), UNIVERSITYINCITY(parkland college, champaign)
Prediction:
LF 1: ?x.?yM(y, ?illinois?) ? M(x, ?university?) ? CITYLOCATEDINSTATE(x, y)
LF 2: ?c, p.M(c, ?champaign?) ? CITY(c) ? M(p, ?parkland college?) ? UNIVERSITYINCITY(p, c)
Instances: CITY(champaign), UNIVERSITYINCITY(parkland college, champaign),
CITYLOCATEDINSTATE(university, illinois)
Logical form accuracy: 1 / 1 Extraction Precision: 2 / 3 Extraction Recall: 2 / 2
Figure 4: Two test examples with ASP?s predictions and error calculations. The annotated logical forms
are for the italicized sentence spans, while the extracted logical forms are for the underlined spans.
5.5.3 Annotated Sentence Evaluation
A limitation of the previous evaluation is that it
does not measure the completeness of predicted
logical forms, nor estimate what portion of sen-
tences are left unanalyzed. We conducted a second
evaluation to measure these quantities.
The data for this evaluation consists of sen-
tences annotated with logical forms for subspans.
We manually annotated Wikipedia sentences from
the held-out set with logical forms for the largest
subspans for which a logical form existed. To
avoid trivial cases, we only annotated logical
forms containing at least one category or relation
predicate and at least one mention. We also chose
not to annotate mentions of entities that are not in
the knowledge base, as no system would be able
to correctly identify them. The corpus contains 97
sentences with 100 annotated logical forms.
We measured performance using two met-
rics: logical form accuracy, and extraction preci-
sion/recall. Logical form accuracy examines the
predicted logical form for the smallest subspan of
the sentence containing the annotated span, and
marks this prediction correct if it exactly matches
the annotation. A limitation of this metric is that it
does not assign partial credit to logical forms that
are close to, but do not exactly match, the anno-
tation. The extraction metric assigns partial credit
by computing the precision and recall of the cat-
egory and relation instances entailed by the pre-
dicted logical form, using those entailed by the an-
notated logical form as the gold standard. Figure
4 shows the computation of both error metrics on
two examples from the test corpus.
Table 3 shows the results of the annotated sen-
tence evaluation. ASP outperforms both baselines
in logical form accuracy and extraction recall, sug-
gesting that it produces more complete analyses
than either baseline. The extraction precision of
90% suggests that ASP rarely extracts incorrect in-
formation. Precision is higher in this evaluation
because every sentence in the data set has at least
one correct extraction.
6 Discussion
We present an approach to training a joint syntac-
tic and semantic parser. Our parser ASP produces
a full syntactic parse of any sentence, while simul-
taneously producing logical forms for sentence
spans that have a semantic representation within
its predicate vocabulary. The parser is trained
by jointly optimizing performance on a syntac-
tic parsing task and a distantly-supervised rela-
tion extraction task. Experimental results demon-
strate that jointly analyzing syntax and semantics
triples the number of extracted logical forms over
approaches that first analyze syntax, then seman-
tics. However, we also find that incorporating se-
mantics slightly reduces syntactic parsing perfor-
mance. Poor entity mention detection is a major
source of error in both cases, suggesting that fu-
ture work should consider integrating entity link-
ing with joint syntactic and semantic parsing.
Acknowledgments
This work was supported in part by DARPA under
award FA8750-13-2-0005. We additionally thank
Jamie Callan and Chris R?e?s Hazy group for col-
lecting and processing the Wikipedia corpus.
1196
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 17th International Conference on Com-
putational Linguistics - Volume 1.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: an approach to almost parsing. Compu-
tational Linguistics, 25(2):237?265.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing.
Johan Bos. 2005. Towards wide-coverage seman-
tic interpretation. In In Proceedings of Sixth In-
ternational Workshop on Computational Semantics
IWCS-6.
Qingqing Cai and Alexander Yates. 2013a. Large-
scale Semantic Parsing via Schema Matching and
Lexicon Extension. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Qingqing Cai and Alexander Yates. 2013b. Semantic
Parsing Freebase: Towards Open-domain Semantic
Parsing. In Proceedings of the Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM).
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth AAAI Conference on Artificial Intel-
ligence.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of the 20th International Con-
ference on Computational Linguistics.
Stephen Clark and James R. Curran. 2007a. Per-
ceptron training for a wide-coverage lexicalized-
grammar parser. In Proceedings of the Workshop on
Deep Linguistic Processing.
Stephen Clark and James R. Curran. 2007b. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
the world?s response. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning.
Ruifang Ge and Raymond J. Mooney. 2005. A statis-
tical semantic parser that integrates syntax and se-
mantics. In Proceedings of the Ninth Conference on
Computational Natural Language Learning.
Julia Hockenmaier and Mark Steedman. 2002a.
Acquiring compact lexicalized grammars from a
cleaner treebank. In Proceedings of Third Interna-
tional Conference on Language Resources and Eval-
uation.
Julia Hockenmaier and Mark Steedman. 2002b. Gen-
erative models for statistical parsing with combina-
tory categorial grammar. In Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics.
Julia Hockenmaier. 2003a. Data and Models for Sta-
tistical Parsing with Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
Julia Hockenmaier. 2003b. Parsing with generative
models of predicate-argument structure. In Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics - Volume 1.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke S. Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In The 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies.
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, Proceedings
of the Conference.
Jayant Krishnamurthy and Tom M. Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in CCG grammar induction for semantic pars-
ing. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing.
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. Transactions
of the Association for Computational Linguistics,
1:179?192.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the Association for Compu-
tational Linguistics, Portland, Oregon. Association
for Computational Linguistics.
1197
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Nathan D. Ratliff, J. Andrew Bagnell, and Martin A.
Zinkevich. 2006. (online) subgradient methods
for structured prediction. Artificial Intelligence and
Statistics.
Mark Steedman. 1996. Surface Structure and Inter-
pretation. The MIT Press, Cambridge, MA, USA.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL.
Yuk Wah Wong and Raymond J. Mooney. 2007.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In Proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics.
Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural language questions for
the web of data. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the thirteenth na-
tional conference on Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: struc-
tured classification with probabilistic categorial
grammars. In UAI ?05, Proceedings of the 21st Con-
ference in Uncertainty in Artificial Intelligence.
1198
Transactions of the Association for Computational Linguistics, 1 (2013) 193?206. Action Editor: Jason Eisner.
Submitted 10/2012; Revised 3/2013; Published 5/2013. c?2013 Association for Computational Linguistics.
Jointly Learning to Parse and Perceive: Connecting
Natural Language to the Physical World
Jayant Krishnamurthy
Computer Science Department
Carnegie Mellon University
jayantk@cs.cmu.edu
Thomas Kollar
Computer Science Department
Carnegie Mellon University
tkollar@andrew.cmu.edu
Abstract
This paper introduces Logical Semantics with
Perception (LSP), a model for grounded lan-
guage acquisition that learns to map natu-
ral language statements to their referents in
a physical environment. For example, given
an image, LSP can map the statement ?blue
mug on the table? to the set of image seg-
ments showing blue mugs on tables. LSP
learns physical representations for both cate-
gorical (?blue,? ?mug?) and relational (?on?)
language, and also learns to compose these
representations to produce the referents of en-
tire statements. We further introduce a weakly
supervised training procedure that estimates
LSP?s parameters using annotated referents
for entire statements, without annotated ref-
erents for individual words or the parse struc-
ture of the statement. We perform experiments
on two applications: scene understanding and
geographical question answering. We find
that LSP outperforms existing, less expressive
models that cannot represent relational lan-
guage. We further find that weakly supervised
training is competitive with fully supervised
training while requiring significantly less an-
notation effort.
1 Introduction
Learning the mapping from natural language to
physical environments is a central problem for natu-
ral language semantics. Understanding this mapping
is necessary to enable natural language interactions
with robots and other embodied systems. For exam-
ple, for an autonomous robot to understand the sen-
tence ?The blue mug is on the table,? it must be able
to identify (1) the objects in its environment corre-
sponding to ?blue mug? and ?table,? and (2) the ob-
jects which participate in the spatial relation denoted
by ?on.? If the robot can successfully identify these
objects, it understands the meaning of the sentence.
The problem of learning to map from natural lan-
guage expressions to their referents in an environ-
ment is known as grounded language acquisition.
In embodied settings, environments consist of raw
sensor data ? for example, an environment could be
an image collected from a robot?s camera. In such
applications, grounded language acquisition has two
subproblems: parsing, learning the compositional
structure of natural language; and perception, learn-
ing the environmental referents of individual words.
Acquiring both kinds of knowledge is necessary to
understand novel language in novel environments.
Unfortunately, perception is often ignored in work
on language acquisition. Other variants of grounded
language acquisition eliminate the need for percep-
tion by assuming access to a logical representation
of the environment (Zettlemoyer and Collins, 2005;
Wong and Mooney, 2006; Matuszek et al, 2010;
Chen and Mooney, 2011; Liang et al, 2011). The
existing work which has jointly addressed both pars-
ing and perception has significant drawbacks, in-
cluding: (1) fully supervised models requiring large
amounts of manual annotation and (2) limited se-
mantic representations (Kollar et al, 2010; Tellex et
al., 2011; Matuszek et al, 2012).
This paper introduces Logical Semantics with
Perception (LSP), a model for grounded language
acquisition that jointly learns to semantically parse
language and perceive the world. LSP models a
mapping from natural language queries to sets of ob-
jects in a real-world environment. The input to LSP
is an environment containing objects, such as a seg-
193
(a) An environment containing 4
objects (image segments).
Environment:
(image on left)
Knowledge Base
Query:
?things to the right
of the blue mug?
Semantic Parse
Grounding: {(2, 1), (3, 1)}
Denotation: {2, 3}
(b) LSP predicting the environmental referents of
a natural language query.
Language Denotation
The mugs {1, 3}
The objects on the table {1, 2, 3}
There is an LCD monitor {2}
Is the blue mug right {}of the monitor?
The monitor is behind {2}the blue cup.
(c) Training examples for weakly su-
pervised training.
Figure 1: LSP applied to scene understanding. Given an environment containing a set of objects (left), and a
natural language query, LSP produces a semantic parse, logical knowledge base, grounding and denotation
(middle), using only language/denotation pairs (right) for training.
mented image (Figure 1a), and a natural language
query, such as ?the things to the right of the blue
mug.? Given these inputs, LSP produces (1) a logi-
cal knowledge base describing objects and relation-
ships in the environment and (2) a semantic parse of
the query capturing its compositional structure. LSP
combines these two outputs to produce the query?s
grounding, which is the set of object referents of the
query?s noun phrases, and its denotation, which is
the query?s answer (Figure 1b).1 Weakly supervised
training estimates parameters for LSP using queries
annotated with their denotations in an environment
(Figure 1c).
This work has two contributions. The first con-
tribution is LSP, which is more expressive than pre-
vious models, representing both one-argument cat-
egories and two-argument relations over sets of ob-
jects in the environment. The second contribution
is a weakly supervised training procedure that esti-
mates LSP?s parameters without annotated semantic
parses, noun phrase/object mappings, or manually-
constructed knowledge bases.
We perform experiments on two different applica-
tions. The first application is scene understanding,
where LSP grounds descriptions of images in im-
age segments. The second application is geograph-
ical question answering, where LSP learns to an-
swer questions about locations, represented as poly-
gons on a map. In geographical question answering,
1We treat declarative sentences as if they were queries about
their subject, e.g., the denotation of ?the mug is on the table? is
the set of mugs on tables. Typically, the denotation of a sentence
is either true or false; our treatment is strictly more general, as
a sentence?s denotation is nonempty if and only if the sentence
is true.
LSP correctly answers 34% more questions than the
most comparable state-of-the-art model (Matuszek
et al, 2012). In scene understanding, accuracy sim-
ilarly improves by 16%. Furthermore, weakly su-
pervised training achieves an accuracy within 6% of
that achieved by fully supervised training, while re-
quiring significantly less annotation effort.
2 Prior Work
Logical Semantics with Perception (LSP) is related
to work from planning, natural language processing,
computer vision and robotics. Much of the related
work focuses on interpreting natural language us-
ing a fixed formal representation. Some work con-
structs integrated systems which execute plans in re-
sponse to natural language commands (Winograd,
1970; Hsiao et al, 2003; Roy et al, 2003; Skubic
et al, 2004; MacMahon et al, 2006; Levit and Roy,
2007; Kruijff et al, 2007). These systems parse
natural language to a formal representation which
can be executed using a set of fixed control pro-
grams. Similarly, work on semantic parsing learns
to map natural language to a given formal repre-
sentation. Semantic parsers can be trained using
sentences annotated with their formal representation
(Zelle and Mooney, 1996; Zettlemoyer and Collins,
2005; Kate and Mooney, 2006; Kwiatkowski et al,
2010) or various less restrictive annotations (Clarke
et al, 2010; Liang et al, 2011; Krishnamurthy and
Mitchell, 2012). Finally, work on grounded lan-
guage acquisition leverages semantic parsing to map
from natural language to a formal representation of
an environment (Kate and Mooney, 2007; Chen and
Mooney, 2008; Shimizu and Haas, 2009; Matuszek
194
Environment d Know. Base ?
mug(1)
mug(3)
blue(1)
table(4)
on-rel(1, 4)
on-rel(3, 4)
...
(a) Perception fper produces a logical knowl-
edge base ? from the environment d using an
independent classifier for each category and
relation.
Language z
?blue mug on table?
Logical form `
?x.?y.blue(x) ?
mug(x) ?
on-rel(x, y) ?
table(y)
(b) Semantic parsing fprs
maps language z to a log-
ical form `.
Grounding: g = {(1, 4)}, Denotation: ? = {1}
{1}
{1}
blue(x)
{1, 3}
mug(x)
{(1, 4), (3, 4)}
{(1, 4), (3, 4)}
on-rel(x, y)
{4}
table(y)
(c) Evaluation feval evaluates a logical form ` on a
logical knowledge base ? to produce a grounding g
and denotation ?.
Figure 2: Overview of Logical Semantics with Perception (LSP).
et al, 2010; Dzifcak et al, 2009; Cantrell et al,
2010; Chen and Mooney, 2011). All of this work as-
sumes that the formal environment representation is
given, while LSP learns to produce this formal rep-
resentation from raw sensor input.
Most similar to LSP is work on simultaneously
understanding natural language and perceiving the
environment. This problem has been addressed in
the context of robot direction following (Kollar et
al., 2010; Tellex et al, 2011) and visual attribute
learning (Matuszek et al, 2012). However, this
work is less semantically expressive than LSP and
trained using more supervision. TheG3 model (Kol-
lar et al, 2010; Tellex et al, 2011) assumes a one-
to-one mapping from noun phrases to entities and
is trained using full supervision, while LSP allows
one-to-many mappings from noun phrases to entities
and can be trained using minimal annotation. Ma-
tuszek et al (2012) learns only one-argument cate-
gories (?attributes?) and requires a fully supervised
initial training phase. In contrast, LSP models two-
argument relations and allows for weakly supervised
supervised training throughout.
3 Logical Semantics with Perception
Logical Semantics with Perception (LSP) is a model
for grounded language acquisition. LSP accepts as
input a natural language statement and an environ-
ment and outputs the objects in the environment de-
noted by the statement. The LSP model has three
components: perception, parsing and evaluation (see
Figure 2). The perception component constructs
logical knowledge bases from low-level feature-
based representations of environments. The pars-
ing component semantically parses natural language
into lambda calculus queries against the constructed
knowledge base. Finally, the evaluation compo-
nent deterministically executes this query against the
knowledge base to produce LSP?s output.
The output of LSP can be either a denotation or
a grounding. A denotation is the set of entity refer-
ents for the phrase as a whole, while a grounding is
the set of entity referents for each component of the
phrase. The distinction between these two outputs is
shown in Figure 1b. In this example, the denotation
is the set of ?things to the right of the blue mug,?
which does not include the blue mug itself. On the
other hand, the grounding includes both the refer-
ents of ?things? and ?blue mug.? Only denotations
are used during training, so we ignore groundings in
the following model description. However, ground-
ings are used in our evaluation, as they are a more
complete description of the model?s understanding.
Formally, LSP is a linear model f that predicts a
denotation ? given a natural language statement z in
an environment d. As shown in Figure 3, the struc-
ture of LSP factors into perception (fper), semantic
parsing (fprs) and evaluation (feval) components us-
ing several latent variables:
f(?,?, `, t, z,d; ?) = fper(?, d; ?per)+
fprs(`, t, z; ?prs) + feval(?,?, `)
LSP assumes access to a set of predicates that
take either one argument, called categories (c ? C)
or two arguments, called relations (r ? R).2 These
predicates are the interface between LSP?s percep-
tion and parsing components. The perception func-
tion fper takes an environment d and produces a log-
2The set of predicates are derived from our training data.
See Section 5.3.
195
?feval
?d fper
? zfprs
t
Figure 3: Factor graph of LSP. The environment d
and language z are given as input, from which the
model predicts a logical knowledge base ?, logical
form `, syntactic tree t and denotation ?.
ical knowledge base ? that assigns truth values to
instances of these predicates using parameters ?per.
This function uses an independent classifier to pre-
dict the instances of each predicate. The seman-
tic parser fprs takes a natural language statement z
and produces a logical form ` and syntactic parse
t using parameters ?prs. The logical form ` is a
database query expressed in lambda calculus nota-
tion, constructed by logically combining the given
predicates. Finally, the evaluation function feval de-
terministically evaluates the logical form ` on the
knowledge base ? to produce a denotation ?. These
components are illustrated in Figure 2.
The following sections describe the percep-
tion function (Section 3.1), semantic parser (Sec-
tion 3.2), evaluation function (Section 3.3), and in-
ference (Section 3.4) in more detail.
3.1 Perception Function
The perception function fper constructs a logical
knowledge base ? given an environment d. The per-
ception function assumes that an environment con-
tains a collection of entities e ? Ed. The knowl-
edge base produced by perception is a collection of
ground predicate instances using these entities. For
example, in Figure 2a, the entire image is the envi-
ronment, and each image segment is an entity. The
logical knowledge base ? contains the shown pred-
icate instances, where the categories include blue,
mug and table, and the relations include on-rel.
The perception function scores logical knowledge
bases using a set of per-predicate binary classifiers.
These classifiers independently assign a score to
whether each entity (entity pair) is an element of
each category (relation). Let ?c ? ? denote the set
of entities which are elements of category c; simi-
larly, let ?r ? ? denote the set of entity pairs which
are elements of the relation r. Given these sets, the
score of a logical knowledge base ? factors into per-
relation and per-category scores h:
fper(?, d; ?per) =
?
c?C
h(?c, d; ?cper)
+
?
r?R
h(?r, d; ?rper)
The per-predicate scores are in turn given by a
sum of per-element classification scores:
h(?c, d; ?cper) =
?
e?Ed
?c(e)(?cper)T?cat(e)
h(?r, d; ?rper) =
?
(e1,e2)?Ed
?r(e1, e2)(?rper)T?rel(e1, e2)
Each term in the above sums represents a single
binary classification, determining the score for a sin-
gle entity (entity pair) belonging to a particular cat-
egory (relation). We treat ?c and ?r as indicator
functions for the sets they denote, i.e., ?c(e) = 1
for entities e in the set, and 0 otherwise. Similarly,
?r(e1, e2) = 1 for entity pairs (e1, e2) in the set,
and 0 otherwise. The features of these classifiers are
given by ?cat and ?rel, which are feature functions
that map entities and entity pairs to feature vectors.
The parameters of these classifiers are given by ?cper
and ?rper. The perception parameters ?per contain
one such set of parameters for every category and re-
lation, i.e., ?per = {?cper : c ? C} ? {?rper : r ? R}.
3.2 Semantic Parser
The goal of semantic parsing is to identify which
portions of the input natural language denote enti-
ties and relationships between entities in the envi-
ronment. Semantic parsing accomplishes this goal
by mapping from natural language to a logical form
that explicitly describes the language?s entity refer-
ents using one- and two-argument predicates. The
logical form is combined with instances of these
predicates to produce the statement?s denotation.
LSP?s semantic parser is defined using Combina-
tory Categorial Grammar (CCG) (Steedman, 1996).
The grammar of the parser is given by a lexicon ?
which maps words to syntactic categories and logi-
cal forms. For example, ?mug? may have the syn-
tactic category N for noun, and the logical form
?x.mug(x), denoting the set of all entities x such
that mug is true. During parsing, the logical forms
for adjacent phrases are combined to produce the
logical form for the complete statement.
196
the
N/N
?f.f
mugs
N
?x.mug(x)
N : ?x.mug(x)
are
(S\N)/N
?f.?g.?x.g(x) ? f(x)
right
N/PP
?f.?x.?y.right-rel(x, y) ? f(y)
of
PP/N
?f.f
the
N/N
?f.f
monitor
N
?x.monitor(x)
N : ?x.monitor(x)
PP : ?x.monitor(x)
N : ?x.?y.right-rel(x, y) ? monitor(y)
S\N : ?g.?x.?y.g(x) ? right-rel(x, y) ? monitor(y)
S : ?x.?y.mug(x) ? right-rel(x, y) ? monitor(y)
Figure 4: Example parse of ?the mugs are right of the monitor.? The first row of the derivation retrieves
lexical categories from the lexicon, while the remaining rows represent applications of CCG combinators.
Figure 4 illustrates how CCG parsing produces a
syntactic tree t and a logical form `. The top row
of the parse represents retrieving a lexicon entry for
each word. Each successive row combines a pair of
entries by applying a logical form to an adjacent ar-
gument. A given sentence may have multiple parses
like the one shown, using a different set of lexicon
entries or a different order of function applications.
The semantic parser scores each such parse, learning
to distinguish correct and incorrect parses.
The semantic parser in LSP is a linear model over
CCG parses (`, t) given language z:
fprs(`, t, z; ?prs) = ?Tprs?prs(`, t, z)
Here, ?prs(`, t, z) represents a feature function map-
ping CCG parses to vectors of feature values. ?prs
factorizes according to the tree structure of the CCG
parse; it contains features for local parsing opera-
tions which are summed to produce the feature val-
ues for a tree. If the parse tree is a terminal, then:
?prs(`, t, z) = 1(lexicon entry)
The notation 1(x) denotes a vector with a single one
entry whose position is determined by x. The termi-
nal features are indicator features for each lexicon
entry, as shown in the top row of Figure 4. These
features allow the model to learn the correct syntac-
tic and semantic function of each word. If the parse
tree is a nonterminal, then:
?prs(`, t, z) = ?prs(left(`, t, z))
+ ?prs(right(`, t, z)) + 1(combinator)
These nonterminal features are defined over combi-
nator rules in the parse tree, as in the remaining rows
of Figure 4. These features allow the model to learn
which adjacent parse trees are likely to combine. We
refer the reader to Zettlemoyer and Collins (2005)
for more information about CCG semantic parsing.
3.3 Evaluation Function
The evaluation function feval deterministically
scores denotations given a logical form ` and a
logical knowledge base ?. Intuitively, the evalu-
ation function simply evaluates the query ` on the
database ? to produce a denotation. The evaluation
function then assigns score 0 to this denotation, and
score ?? to all other denotations.
We describe feval by giving a recurrence for com-
puting the denotation ? of a logical form ` on a log-
ical knowledge base ?. This evaluation takes the
form of a tree, as in Figure 2c. The base cases are:
? If ` = ?x.c(x) then ? = ?c.
? If ` = ?x.?y.r(x, y), then ? = ?r.
The denotations for more complex logical forms
are computed recursively by decomposing ` accord-
ing to its logical structure. Our logical forms con-
tain only conjunctions and existential quantifiers;
the corresponding recursive computations are:
? If ` = ?x.`1(x) ? `2(x), then
?(e) = 1 iff ?1(e) = 1 ? ?2(e) = 1.
? If ` = ?x.?y.`1(x, y), then
?(e1) = 1 iff ?e2.?1(e1, e2) = 1.
Note that a similar recurrence can be used to com-
pute groundings: simply retain the satisfying assign-
ments to existentially-quantified variables.
3.4 Inference
The basic inference problem in LSP is to predict a
denotation ? given language z and an environment
d. This inference problem is straightforward due
to the deterministic structure of feval. The highest-
scoring ? can be found by independently maximiz-
ing fprs and fper to find the highest-scoring logical
form ` and logical knowledge base ?. Deterministi-
cally evaluating the recurrence for feval using these
values yields the highest-scoring denotation.
197
Another inference problem occurs during train-
ing: identify the highest-scoring logical form and
knowledge base which produce a particular denota-
tion. Our approximate inference algorithm for this
problem is described in Section 4.2.
4 Weakly Supervised Parameter
Estimation
This section describes a weakly supervised training
procedure for LSP, which estimates parameters us-
ing a corpus of sentences with annotated denota-
tions. The algorithm jointly trains both the pars-
ing and the perception components of LSP to best
predict the denotations of the observed training sen-
tences. Our approach trains LSP as a maximum
margin Markov network using the stochastic subgra-
dient method. The main difficulty is computing the
subgradient, which requires computing values for
the model?s hidden variables, i.e., the logical knowl-
edge base ? and semantic parse ` that are responsi-
ble for the model?s prediction.
4.1 Stochastic Subgradient Method
The training procedure trains LSP as a maximum
margin Markov network (Taskar et al, 2004), a
structured analog of a support vector machine. The
training data for our weakly supervised algorithm is
a collection {(zi, ?i, di)}ni=1, consisting of language
zi paired with its denotation ?i in environment di.
Given this data, the parameters ? = [?prs, ?per]
are estimated by minimizing the following objective
function:
O(?) = ?2 ||?||
2 + 1n
[ n?
i=1
?i
]
(1)
where ? is a regularization parameter that controls
the trade-off between model complexity and slack
penalties. The slack variable ?i represents a margin
violation penalty for the ith training example, de-
fined as:
?i = max?,?,`,t
[
f(?,?, `, t, zi, di; ?) + cost(?, ?i)
]
?max
?,`,t
f(?i,?, `, t, zi, di; ?)
The above expression is the structured counterpart
of the hinge loss, where cost(?, ?i) is the margin
by which ?i?s score must exceed ??s score. We let
cost(?, ?i) be the Hamming cost; it adds a cost of 1
for each entity e such that ?i(e) 6= ?(e).
We optimize this objective using the stochastic
subgradient method (Ratliff et al, 2006). To com-
pute the subgradient gi, first compute the highest-
scoring assignments to the model?s hidden variables:
??, ??, ?`, t?? arg max
?,?,`,t
f(?,?, `, t, zi, di; ?j)
+ cost(?, ?i) (2)
??, `?, t? ? arg max
?,`,t
f(?i,?, `, t, zi, di; ?j) (3)
The first set of values (e.g., ?`) are the best ex-
planation for the denotation ?? which most violates
the margin constraint. The second set of values
(e.g., `?) are the best explanation for the true de-
notation ?i. The subgradient update increases the
weights of features that explain the true denotation,
while decreasing the weights of features that explain
the denotation violating the margin. The subgradi-
ent factors into parsing and perception components:
gi = [giprs, giper]. The parsing subgradient is:
giprs = ?prs(?`, t?, zi)? ?prs(`?, t?, zi)
The subgradient of the perception parameters ?per
factors into subgradients of the category and relation
classifier parameters. Recall that ?per = {?cper : c ?
C}?{?rper : r ? R}. Let ??c ? ?? be the best margin-
violating set of entities for c, and ?c? ? ?? be the
best truth-explaining set of entities. Similarly define
??r and ?r?. The subgradients of the category and
relation classifier parameters are:
gi,cper =
?
e?Edi
(??c(e)? ?c?(e))?cat(e)
gi,rper =
?
(e1,e2)?Edi
(??r(e1, e2)? ?r?(e1, e2))?rel(e1, e2)
4.2 Inference: Computing the Subgradient
Solving the maximizations in Equations 2 and 3 is
challenging because the weights placed on the de-
notation ? couple fprs and fper. Due to this cou-
pling, exactly solving these problems requires (1)
enumerating all possible logical forms `, and (2) for
each logical form, finding the highest-scoring logi-
cal knowledge base ? by propagating the weights on
? back through feval.
We use a two-step approximate inference algo-
rithm for both maximizations. The first step per-
forms a beam search over CCG parses, producing
198
k possible logical forms `1, ..., `k. The second step
uses an integer linear program (ILP) to find the best
logical knowledge base ? given each parse `i. In
our experiments, we parse with a beam size of 1000,
then solve an ILP for each of the 10 highest-scoring
logical forms. The highest-scoring parse/logical
knowledge base pair is the approximate maximizer.
Given a logical form ` output by beam search, the
second step of inference computes the best values
for the logical knowledge base ? and denotation ?:
max
?,?
fper(?, d; ?per) + feval(?, `,?) + ?(?) (4)
Here, ?(?) = ?e?Ed ?e?(e) represents a set ofweights on the entities in the predicted denotation ?.
For Equation 2, ? represents cost(?, ?i). For Equa-
tion 3, ? is a hard constraint encoding ? = ?i (i.e.,
?(?) = ?? when ? 6= ?i and 0 otherwise).
We encode the maximization in Equation 4 as an
ILP. For each category c and relation r, we create bi-
nary variables ?c(e1) and ?r(e1, e2) for each entity
in the environment, e1, e2 ? Ed. We similarly create
binary variables ?(e) for the denotation ?. Using the
fact that fper is a linear function of these variables,
we write the ILP objective as:
fper(?, d; ?per) + ?(?) =
?
e1?Ed
?
c?C
wce1?
c(e1)
+
?
e1,e2?Ed
?
r?R
wre1,e2?
r(e1, e2) +
?
e1?Ed
?e1?(e1)
where the weights wce1 and wre1,e2 determine howlikely it is that each entity or entity pair belongs to
the predicates c and r:
wce1 = (?
c
per)T?cat(e1)
wre1,e2 = (?
r
per)T?rel(e1, e2)
The ILP also includes constraints and additional
auxiliary variables that represent feval. These con-
straints couple the denotation ? and the logical
knowledge base ? such that ? is the result of evaluat-
ing ` on ?. ` is recursively decomposed as in Section
3.3, and each intermediate set of entities in the recur-
rence is given its own set of |Ed| (or |Ed|2) variables.
These variables are then logically constrained to en-
force `?s structure.
5 Evaluation
Our evaluation performs three major comparisons.
First, we examine the performance impact of weakly
supervised training by comparing weakly and fully
supervised variants of LSP. Second, we examine the
performance impact of modelling relations by com-
paring against a category-only baseline, which is an
ablated version of LSP similar to the model of Ma-
tuszek et al (2012). Finally, we examine the causes
of errors by performing an error analysis of LSP?s
semantic parser and perceptual classifiers.
Before describing our results, we first describe
some necessary set-up for the experiments. These
sections describe the data sets, features, construc-
tion of the CCG lexicon, and details of the models.
Our data sets and additional evaluation resources are
available online from http://rtw.ml.cmu.edu/
tacl2013_lsp/.
5.1 Data Sets
We evaluate LSP on two applications: scene un-
derstanding (SCENE) and geographical question an-
swering (GEOQA). These data sets are collections
{(zi, ?i, di, `i,?i)}ni=1, consisting of a number of
natural language statements zi with annotated deno-
tations ?i in environments di. For fully supervised
training, each statement is annotated with a gold
standard logical form `i, and each environment with
a gold standard logical knowledge base ?i. Statistics
of these data sets are shown in Table 1, and example
environments and statements are shown in Figure 5.
The SCENE data set consists of segmented images
of indoor environments containing a number of or-
dinary objects such as mugs and monitors. Each
image is an environment, and each image segment
(bounding box) is an entity. We collected natural
language descriptions of each scene from members
of our lab and Amazon Mechanical Turk, asking
subjects to describe the objects in each scene. The
authors then manually annotated the collected state-
ments with their denotations and logical forms. In
this data set, each image contains the same set of ob-
jects; note that this does not trivialize the task, as the
model only observes visual features of the objects,
which are not consistent across environments.
The GEOQA data set consists of several maps
containing entities such as cities, states, national
parks, lakes and oceans. Each map is an envi-
ronment, and its component entities are given by
polygons of latitude/longitude coordinates marking
199
Data Set Statistics SCENE GEOQA
# of environments 15 10
Mean entities / environment d 4.1 6.9
Mean # of entities in denotation ? 1.5 1.2
# of statements 284 263
Mean words / statement 6.3 6.3
Mean predicates / log. form 2.6 2.8
# of preds. in annotated worlds 46 38
Lexicon Statistics
# of words in lexicon 169 288
# of lexicon entries 712 876
Mean parses / statement 15.0 8.9
Table 1: Statistics of the two data sets used in our
evaluation, and of the generated lexicons.
their boundaries.3 Furthermore, each entity has one
known name (e.g., ?Greenville?). In this data set,
distinct entities occur on average in 1.25 environ-
ments; repeated entities are mostly large locations,
such as states and oceans. The language for this
data set was contributed by members of our research
group, who were instructed to provide a mixture of
simple and complex geographical questions. The
authors then manually annotated each question with
a denotation (its answer) and a logical form.
5.2 Features
The features of both applications are intended to
capture properties of entities and relations between
them. As such, both applications share a set of phys-
ical features which are functions of the bounding
polygons of each entity. Example category features
(?cat) are the area and perimeter of the entity, and
an example relation feature (?rel) is the distance be-
tween entity centroids.
The SCENE data set additionally includes visual
appearance features in ?cat to capture visual proper-
ties of objects. These features include a Histogram
of Oriented Gradients (HOG) (Dalal and Triggs,
2005) and an RGB color histogram.
The GEOQA data set additionally includes dis-
tributional semantic features to distinguish between
different types of entities (e.g., states vs. lakes)
and to capture non-spatial relations (e.g., capitals
of states). These features are derived from phrase
co-occurrences with entity names in the Clueweb09
3Polygons were extracted from OpenStreetMap, http://
www.openstreetmap.org/.
web corpus.4 The category features ?cat include in-
dicators for the 20 contexts which most frequently
occur with an entity?s name (e.g., ?X is a city?).
Similarly, the relation features ?rel include indica-
tors for the 20 most frequent contexts between two
entities? names (e.g., ?X in eastern Y ?).
5.3 Lexicon Induction
One of the inputs to the semantic parser (Section 3.2)
is a lexicon that lists possible syntactic and seman-
tic functions for each word. Together, these per-
word entries define the set of possible logical forms
for every statement. Each word may have mul-
tiple lexicon entries to capture uncertainty about
its meaning. For example, the word ?right? may
have entries N : ?x.right(x) and N/PP :
?f.?x.?y.right-rel(x, y)? f(y). The semantic
parser learns to distinguish among these interpreta-
tions to produce good logical forms.
We automatically generated lexicons for both
applications using part-of-speech tag heuristics.5
These heuristics map words to lexicon entries con-
taining category and relation predicates derived
from the word?s lemma. Nouns and adjectives pro-
duce lexicon entries containing either categories or
relations (as shown above for ?right?). Mapping
these parts-of-speech to relations is necessary for
phrases like ?to the right of,? where the noun ?right?
denotes a relation. Verbs and prepositions pro-
duce lexicon entries containing relations. Additional
heuristics generate semantically-empty lexicon en-
tries, allowing words like determiners to have no
physical interpretation. Finally, there are special
heuristics for forms of ?to be? and, in GEOQA, to
handle known entity names. The complete set of
lexicon induction heuristics is available online.
The automatically generated lexicon makes it dif-
ficult to compare semantic parses across models,
since the correctness of a semantic parse depends on
the learned perceptual classifiers. To facilitate such
a comparison (Section 5.6), we filtered out lexicon
entries containing predicates which were not used in
any of the annotated logical forms. Statistics of the
filtered lexicons are shown in Table 1.
4http://www.lemurproject.org/clueweb09/
5We used the Stanford POS tagger (Toutanova et al, 2003).
200
Environment d Language z and predicted logical form ` Predicted grounding True grounding
monitor to the left of the mugs {(2,1), (2,3)} {(2,1), (2,3)}
?x.?y.monitor(x) ? left-rel(x, y) ? mug(y)
mug to the left of the other mug {(3,1)} {(3,1)}
?x.?y.mug(x) ? left-rel(x, y) ? mug(y)
objects on the table {(1,4), (2,4) {(1,4), (2,4),
?x.?y.object(x) ? on-rel(x, y) ? table(y) (3,4)} (3,4)}
two blue cups are placed near to the computer screen {(1)} {(1,2), (3,2)}
?x.blue(x) ? cup(x) ? comp.(x) ? screen(x)
What cities are in North Carolina? {(CH,NC), (GB,NC) {(CH,NC), (GB,NC)
?x.?y.city(x) ? in-rel(x, y) ? y = NC (RA,NC)} (RA,NC)}
What city is east of Greensboro in North Carolina? {(RA,GB,NC), {(RA,GB,NC)}
?x.?y, z.city(x) ? east-rel(x, y) (MB,GB,NC)}
? y = GB ? in-rel(y, z) ? z = NC
What cities are on the ocean? {(CH,AO), (GB,AO), {(MB,AO)}
?x.?y.city(x) ? on-rel(x, y) ? ocean(y) (MB,AO), (RA,AO)}
Figure 5: Example environments, statements, and model predictions from the SCENE and GEOQA data sets.
5.4 Models and Training
The evaluation compares three models. The first
model is LSP-W, which is LSP trained using the
weakly supervised algorithm described in Section 4.
The second model, LSP-CAT, replicates the model
of Matuszek et al (2012) by restricting LSP to
use category predicates. LSP-CAT is constructed by
removing all relation predicates in lexicon entries,
mapping entries like ?f.?g.?x.?y.r(x, y) ? g(x) ?
f(y) to ?f.?g.?x.?y.g(x) ? f(y). This model is
also trained using our weakly supervised algorithm.
The third model, LSP-F, is LSP trained with full
supervision, using the manually annotated semantic
parses and logical knowledge bases in our data sets.
Given these annotations, training LSP amounts to
independently training a semantic parser (using sen-
tences with annotated logical forms, {(zi, `i)}) and
a set of perceptual classifiers (using environments
with annotated logical knowledge bases, {(di,?i)}).
This model measures the performance achievable
with LSP given significantly more supervision.
All three variants of LSP were trained using the
same hyperparameters. For SCENE, we computed
subgradients in 5 example minibatches and per-
formed 100 passes over the data using ? = 0.03. For
GEOQA, we computed subgradients in 8 example
minibatches, again performing 100 passes over the
data using ? = 0.02. We tried varying the regular-
ization parameter, but found that performance was
relatively stable under ? ? 0.05. All experiments
use leave-one-environment-out cross-validation to
estimate model performance. We hold out each en-
vironment in turn, train each model on the remaining
environments, then test on the held-out environment.
5.5 Results
We consider two prediction problems in the eval-
uation. The first problem is to predict the correct
denotation ?i for a statement zi in an environment
di. A correct prediction on this task corresponds
to a correctly answered question. A weakness of
this task is that it is possible to guess the right de-
notation without fully understanding the language.
For example, given a query like ?mugs on the ta-
ble,? it might be possible to guess the denotation
based solely on ?mugs,? ignoring ?table? altogether.
The grounding prediction task corrects for this prob-
lem. Here, each model predicts a grounding, which
is the set of all satisfying assignments to the vari-
ables in a logical form. For example, for the log-
ical form ?x.?y.left-rel(x, y) ? mug(y), the
grounding is the set of (x, y) tuples for which both
left-rel(x, y) and mug(y) return true. Note
that, if the predicted semantic parse is incorrect, the
predicted grounding for a statement may contain a
different number of variables than the true ground-
ing; such groundings are incorrect. Figure 5 shows
model predictions for the grounding task.
Performance on both tasks is measured using ex-
act match accuracy. This metric is the fraction of
examples for which the predicted set of entities (be
it the denotation or grounding) exactly equals the
annotated set. This is a challenging metric, as the
201
Denotation ? 0 rel. 1 rel. other total
LSP-CAT 0.94 0.45 0.20 0.51
LSP-F 0.89 0.81 0.20 0.70
LSP-W 0.89 0.77 0.16 0.67
Grounding g 0 rel. 1 rel. other total
LSP-CAT 0.94 0.37 0.00 0.42
LSP-F 0.89 0.80 0.00 0.65
LSP-W 0.89 0.70 0.00 0.59
% of data 23 56 21 100
(a) Results on the SCENE data set.
Denotation ? 0 rel. 1 rel. other total
LSP-CAT 0.22 0.19 0.07 0.17
LSP-F 0.64 0.53 0.21 0.48
LSP-W 0.64 0.58 0.21 0.51
Grounding g 0 rel. 1 rel. other total
LSP-CAT 0.22 0.19 0.00 0.16
LSP-F 0.64 0.53 0.17 0.47
LSP-W 0.64 0.58 0.15 0.50
% of data 8 72 20 100
(b) Results on the GEOQA data set.
Table 2: Model performance on the SCENE and GEOQA datasets. LSP-CAT is an ablated version of LSP
that only learns categories (similar to Matuszek et al (2012)), LSP-F is LSP trained with annotated seman-
tic parses and logical knowledge bases, and LSP-W is LSP trained on sentences with annotated denotations.
Results are separated by the number of relations in each test natural language statement.
number of possible sets grows exponentially in the
number of entities in the environment. Say an en-
vironment has 5 entities and a logical form has two
variables; then there are 25 possible denotations and
225 possible groundings. To quantify this difficulty,
note that selecting a denotation uniformly at random
achieves 6% accuracy on SCENE, and 1% accuracy
on GEOQA; selecting a random grounding achieves
1% and 0% accuracy, respectively.
Table 2 shows results for both applications us-
ing exact match accuracy. To better understand the
performance of each model, we break down perfor-
mance according to linguistic complexity. We com-
pute the number of relations in the annotated logical
form for each statement, and show separate results
for 0 and 1 relations. We also include an ?other?
category to capture sentences with more than one
relation (very infrequent), or that include quanti-
fiers, comparatives, or other linguistic phenomena
not captured by LSP.
The results from these experiments suggest three
conclusions. First, we find that modelling relations
is important for both applications, as (1) the major-
ity of examples contain relational language, and (2)
LSP-W and LSP-F dramatically outperform LSP-
CAT on these examples. The low performance of
LSP-CAT suggests that many denotations cannot
be predicted from only the first noun phrase in a
statement, demonstrating that both applications re-
quire an understanding of relations. Second, we find
that weakly supervised training and fully supervised
training perform similarly, with accuracy differences
in the range of 3%-6%. Finally, we find that LSP-W
performs similarly on both the denotation and com-
plete grounding tasks; this result suggests that when
LSP-W predicts a correct denotation, it does so be-
cause it has identified the correct entity referents of
each portion of the statement.
5.6 Component Error Analysis
We performed an error analysis of each model com-
ponent to better understand the causes of errors. Ta-
ble 3 shows the accuracy of the semantic parser from
each trained model. Each held-out sentence zi is
parsed to produce a logical form `, which is marked
correct if it exactly matches our manual annotation
`i. A correct logical form implies a correct ground-
ing for the statement when the parse is evaluated in
the gold standard logical knowledge base. These re-
sults show that both LSP-W and LSP-F have rea-
sonably accurate semantic parsers, given the restric-
tions on possible logical forms. Common mistakes
include missing lexicon entries (e.g., ?borders? is
POS-tagged as a noun, so the GEOQA lexicon does
not include a verb for it) and prepositional phrase
attachments (e.g., 6th example in Figure 5).
Table 4 shows the precision and recall of the in-
dividual perceptual classifiers. We computed these
metrics by comparing each annotated predicate in
the held-out environment with the model?s predic-
tions for the same predicate, treating each entity (or
entity pair) as an independent example for classifi-
202
SCENE GEOQA
LSP-CAT 0.21 0.17
LSP-W 0.72 0.71
LSP-F 0.73 0.75
Upper Bound 0.79 0.87
Table 3: Accuracy of the semantic parser from each
trained model. Upper bound is the highest accu-
racy achievable without modelling comparatives and
other linguistic phenomena not captured by LSP.
cation. Fully supervised training appears to produce
better perceptual classifiers than weakly supervised
training; however, this result conflicts with the full
system evaluation in Table 2, where both systems
perform equally well. There are two causes for this
phenomenon: uninformative adjectives and unim-
portant relation instances.
Uninformative adjective predicates are responsi-
ble for the low performance of the category classi-
fiers in SCENE. Phrases like ?LCD screen? in this
domain are annotated with logical forms such as
?x.lcd(x) ? screen(x). Here, lcd is uninfor-
mative, since screen already denotes a unique ob-
ject in the environment. Therefore, it is not impor-
tant to learn an accurate classifier for lcd. Weakly
supervised training learns that lcd is meaningless,
yet predicts the correct denotation for ?x.lcd(x) ?
screen(x) using its screen classifier.
The discrepancy in relation performance occurs
because the relation evaluation weights every rela-
tion equally, whereas in reality some relations are
more frequent. Furthermore, even within a single
relation, each entity pair is not equally important ?
for example, people tend to ask what is in a state, but
not what is in an ocean. To account for these factors,
we define a reweighted relation metric using the an-
notated logical forms containing only one relation,
of the form ?x.?y.c1(x) ? r(x, y) ? c2(y). Using
these logical forms, we measure the performance of
r on the set of x, y pairs such that c1(x)?c2(y), then
average this over all examples. Table 4 shows that,
using this metric, both training regimes have similar
performance. This result suggests that weakly su-
pervised training adapts LSP?s relation classifiers to
the relation instances which are empirically impor-
tant for grounding natural language.
SCENE GEOQA
Categories P R F1 P R F1
LSP-CAT 0.40 0.86 0.55 0.78 0.25 0.38
LSP-W 0.40 0.84 0.54 0.85 0.63 0.73
LSP-F 0.98 0.96 0.97 0.89 0.63 0.74
Relations P R F1 P R F1
LSP-W 0.40 0.42 0.41 0.34 0.51 0.41
LSP-F 0.99 0.87 0.92 0.70 0.46 0.55
Relations (rw) P R F1 P R F1
LSP-W 0.98 0.98 0.98 0.86 0.72 0.79
LSP-F 0.98 0.95 0.96 0.89 0.66 0.76
Table 4: Perceptual classifier performance, mea-
sured against the gold standard logical knowledge
bases. LSP-CAT is excluded from the relation eval-
uations, since it does not learn relations. Relations
(rw) is the reweighted metric (see text for details).
6 Conclusions
This paper introduces Logical Semantics with Per-
ception (LSP), a model for mapping natural lan-
guage statements to their referents in a physical en-
vironment. LSP jointly models perception and lan-
guage understanding, simultaneously learning (1)
to map from environments to logical knowledge
bases containing instances of both one-argument
categories and two-argument relations, and (2) to
semantically parse natural language. Furthermore,
we introduce a weakly supervised training proce-
dure that trains LSP using only sentences with anno-
tated denotations, without annotated semantic parses
or noun phrase/entity mappings. An experimen-
tal evaluation reveals that this procedure performs
nearly as well fully supervised training, while re-
quiring significantly less annotation effort. Our ex-
periments also find that LSP?s ability to learn rela-
tions improves performance over comparable prior
work (Matuszek et al, 2012).
Acknowledgments
This research has been supported in part by DARPA
under award FA8750-13-2-0005, and in part by a
gift from Google. We also gratefully acknowledge
the CMU Read the Web group for assistance with
data set construction, and Tom Mitchell, Manuela
Veloso, Brendan O?Connor, Felix Duvallet, Robert
Fisher and the anonymous reviewers for helpful dis-
cussions and comments on the paper.
203
References
Rehj Cantrell, Matthias Scheutz, Paul Schermerhorn,
and Xuan Wu. 2010. Robust spoken instruc-
tion understanding for HRI. In Proceedings of the
5th ACM/IEEE International Conference on Human-
Robot Interaction.
David L. Chen and Raymond J. Mooney. 2008. Learning
to sportscast: a test of grounded language acquisition.
In Proceedings of the 25th International Conference
on Machine learning.
David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the 25th
AAAI Conference on Artificial Intelligence.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
the world?s response. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning.
Navneet Dalal and Bill Triggs. 2005. Histograms of
oriented gradients for human detection. In Proceed-
ings of the 2005 IEEE Computer Society Conference
on Computer Vision and Pattern Recognition.
Juraj Dzifcak, Matthias Scheutz, Chitta Baral, and Paul
Schermerhorn. 2009. What to do and how to do
it: translating natural language directives into tempo-
ral and dynamic logic representation for goal manage-
ment and action execution. In Proceedings of the 2009
IEEE International Conference on Robotics and Au-
tomation.
Kai-yuh Hsiao, Nikolaos Mavridis, and Deb Roy.
2003. Coupling perception and simulation: Steps
towards conversational robotics. In Proceedings of
the IEEE/RSJ International Conference on Intelligent
Robots and Systems.
Rohit J. Kate and Raymond J. Mooney. 2006. Using
string-kernels for learning semantic parsers. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting of
the ACL.
Rohit J. Kate and Raymond J. Mooney. 2007. Learning
language semantics from ambiguous supervision. In
Proceedings of the 22nd Conference on Artificial In-
telligence.
Thomas Kollar, Stefanie Tellex, Deb Roy, and Nicholas
Roy. 2010. Toward understanding natural language
directions. In Proceedings of the 5th ACM/IEEE In-
ternational Conference on Human-Robot Interaction.
Jayant Krishnamurthy and Tom M. Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning.
Geert-Jan M. Kruijff, Hendrik Zender, Patric Jensfelt,
and Henrik I. Christensen. 2007. Situated dialogue
and spatial organization: What, where... and why. In-
ternational Journal of Advanced Robotic Systems.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilistic
CCG grammars from logical form with higher-order
unification. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing.
Michael Levit and Deb Roy. 2007. Interpretation of spa-
tial language in a map navigation task. IEEE Transac-
tions on Systems, Man, and Cybernetics, Part B.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional semantics.
In Proceedings of the Association for Computational
Linguistics.
Matthew MacMahon, Brian Stankiewicz, and Benjamin
Kuipers. 2006. Walk the talk: connecting language,
knowledge, and action in route instructions. In Pro-
ceedings of the 21st National Conference on Artificial
Intelligence.
Cynthia Matuszek, Dieter Fox, and Karl Koscher. 2010.
Following directions using statistical machine transla-
tion. In Proceedings of the 5th ACM/IEEE Interna-
tional Conference on Human-Robot Interaction.
Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A joint
model of language and perception for grounded at-
tribute learning. In Proceedings of the 29th Interna-
tional Conference on Machine Learning.
Nathan D. Ratliff, J. Andrew Bagnell, and Martin A.
Zinkevich. 2006. (online) subgradient methods
for structured prediction. Artificial Intelligence and
Statistics.
Deb Roy, Kai-Yuh Hsiao, and Nikolaos Mavridis. 2003.
Conversational robots: building blocks for grounding
word meaning. In Proceedings of the HLT-NAACL
2003 Workshop on Learning Word Meaning from Non-
linguistic Data.
Nobuyuki Shimizu and Andrew Haas. 2009. Learning to
follow navigational route instructions. In Proceedings
of the 21st international joint conference on artifical
intelligence.
Marjorie Skubic, Dennis Perzanowski, Sam Blisard, Alan
Schultz, William Adams, Magda Bugajska, and Derek
Brock. 2004. Spatial language for human-robot di-
alogs. IEEE Transactions on Systems, Man, and Cy-
bernetics, Part C: Applications and Reviews.
Mark Steedman. 1996. Surface Structure and Interpre-
tation.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2004.
Max-margin markov networks. In Advances in Neural
Information Processing Systems.
204
Stefanie Tellex, Thomas Kollar, Steven Dickerson,
Matthew Walter, Ashis Banerjee, Seth Teller, and
Nicholas Roy. 2011. Understanding natural language
commands for robotic navigation and mobile manipu-
lation. In AAAI Conference on Artificial Intelligence.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology.
Terry Winograd. 1970. Procedures as a representation
for data in a computer program for understanding nat-
ural language. Ph.D. thesis, Massachusetts Institute of
Technology.
Yuk Wah Wong and Raymond J. Mooney. 2006. Learn-
ing for semantic parsing with statistical machine trans-
lation. In Proceedings of the main conference on Hu-
man Language Technology Conference of NAACL.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic pro-
gramming. In Proceedings of the Thirteenth National
Conference on Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
Proceedings of the 21st Conference in Uncertainty in
Artificial Intelligence.
205
206
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 1?10,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Vector Space Semantic Parsing: A Framework for
Compositional Vector Space Models
Jayant Krishnamurthy
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
jayantk@cs.cmu.edu
Tom M. Mitchell
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
tom.mitchell@cmu.edu
Abstract
We present vector space semantic parsing
(VSSP), a framework for learning compo-
sitional models of vector space semantics.
Our framework uses Combinatory Cate-
gorial Grammar (CCG) to define a cor-
respondence between syntactic categories
and semantic representations, which are
vectors and functions on vectors. The
complete correspondence is a direct con-
sequence of minimal assumptions about
the semantic representations of basic syn-
tactic categories (e.g., nouns are vectors),
and CCG?s tight coupling of syntax and
semantics. Furthermore, this correspon-
dence permits nonuniform semantic repre-
sentations and more expressive composi-
tion operations than previous work. VSSP
builds a CCG semantic parser respecting
this correspondence; this semantic parser
parses text into lambda calculus formulas
that evaluate to vector space representa-
tions. In these formulas, the meanings of
words are represented by parameters that
can be trained in a task-specific fashion.
We present experiments using noun-verb-
noun and adverb-adjective-noun phrases
which demonstrate that VSSP can learn
composition operations that RNN (Socher
et al, 2011) and MV-RNN (Socher et al,
2012) cannot.
1 Introduction
Vector space models represent the semantics of
natural language using vectors and operations on
vectors (Turney and Pantel, 2010). These models
are most commonly used for individual words and
short phrases, where vectors are created using dis-
tributional information from a corpus. Such mod-
els achieve impressive performance on standard-
ized tests (Turney, 2006; Rapp, 2003), correlate
well with human similarity judgments (Griffiths et
al., 2007), and have been successfully applied to a
number of natural language tasks (Collobert et al,
2011).
While vector space representations for indi-
vidual words are well-understood, there remains
much uncertainty about how to compose vector
space representations for phrases out of their com-
ponent words. Recent work in this area raises
many important theoretical questions. For exam-
ple, should all syntactic categories of words be
represented as vectors, or are some categories,
such as adjectives, different? Using distinct se-
mantic representations for distinct syntactic cate-
gories has the advantage of representing the opera-
tional nature of modifier words, but the disadvan-
tage of more complex parameter estimation (Ba-
roni and Zamparelli, 2010). Also, does semantic
composition factorize according to a constituency
parse tree (Socher et al, 2011; Socher et al,
2012)? A binarized constituency parse cannot di-
rectly represent many intuitive intra-sentence de-
pendencies, such as the dependence between a
verb?s subject and its object. What is needed to
resolve these questions is a comprehensive theo-
retical framework for compositional vector space
models.
In this paper, we observe that we already have
such a framework: Combinatory Categorial Gram-
mar (CCG) (Steedman, 1996). CCG provides a
tight mapping between syntactic categories and
semantic types. If we assume that nouns, sen-
tences, and other basic syntactic categories are
represented by vectors, this mapping prescribes
semantic types for all other syntactic categories.1
For example, we get that adjectives are functions
from noun vectors to noun vectors, and that prepo-
1It is not necessary to assume that sentences are vectors.
However, this assumption simplifies presentation and seems
like a reasonable first step. CCG can be used similarly to
explore alternative representations.
1
Input: Log. Form:
?red ball??
semantic
parsing ?Aredvball? evaluation ?
?
?
?
?
? ?
Lexicon: red:= ?x.Aredx
ball:= vball
Params.:Ared =
?
? ?
? ?
?
vball =
?
?
?
?
Figure 1: Overview of vector space semantic pars-
ing (VSSP). A semantic parser first translates nat-
ural language into a logical form, which is then
evaluated to produce a vector.
sitions are functions from a pair of noun vectors
to a noun vector. These semantic type specifica-
tions permit a variety of different composition op-
erations, many of which cannot be represented in
previously-proposed frameworks. Parsing in CCG
applies these functions to each other, naturally de-
riving a vector space representation for an entire
phrase.
The CCG framework provides function type
specifications for each word?s semantics, given its
syntactic category. Instantiating this framework
amounts to selecting particular functions for each
word. Vector space semantic parsing (VSSP) pro-
duces these per-word functions in a two-step pro-
cess. The first step chooses a parametric func-
tional form for each syntactic category, which con-
tains as-yet unknown per-word and global param-
eters. The second step estimates these parameters
using a concrete task of interest, such as predicting
the corpus statistics of adjective-noun compounds.
We present a stochastic gradient algorithm for this
step which resembles training a neural network
with backpropagation. These parameters may also
be estimated in an unsupervised fashion, for ex-
ample, using distributional statistics.
Figure 1 presents an overview of VSSP. The
input to VSSP is a natural language phrase and
a lexicon, which contains the parametrized func-
tional forms for each word. These per-word repre-
sentations are combined by CCG semantic pars-
ing to produce a logical form, which is a sym-
bolic mathematical formula for producing the vec-
tor for a phrase ? for example, Aredvball is a for-
mula that performs matrix-vector multiplication.
This formula is evaluated using learned per-word
and global parameters (values for Ared and vball)
to produce the language?s vector space representa-
tion.
The contributions of this paper are threefold.
First, we demonstrate how CCG provides a the-
oretical basis for vector space models. Second,
we describe VSSP, which is a method for con-
cretely instantiating this theoretical framework.
Finally, we perform experiments comparing VSSP
against other compositional vector space mod-
els. We perform two case studies of composition
using noun-verb-noun and adverb-adjective-noun
phrases, finding that VSSP can learn composition
operations that existing models cannot. We also
find that VSSP produces intuitively reasonable pa-
rameters.
2 Combinatory Categorial Grammar for
Vector Space Models
Combinatory Categorial Grammar (CCG) (Steed-
man, 1996) is a lexicalized grammar formalism
that has been used for both broad coverage syntac-
tic parsing and semantic parsing. Like other lexi-
calized formalisms, CCG has a rich set of syntac-
tic categories, which are combined using a small
set of parsing operations. These syntactic cate-
gories are tightly coupled to semantic represen-
tations, and parsing in CCG simultaneously de-
rives both a syntactic parse tree and a seman-
tic representation for each node in the parse tree.
This coupling between syntax and semantics moti-
vates CCG?s use in semantic parsing (Zettlemoyer
and Collins, 2005), and provides a framework for
building compositional vector space models.
2.1 Syntax
The intuition embodied in CCG is that, syntac-
tically, words and phrases behave like functions.
For example, an adjective like ?red? can com-
bine with a noun like ?ball? to produce another
noun, ?red ball.? Therefore, adjectives are natu-
rally viewed as functions that apply to nouns and
return nouns. CCG generalizes this idea by defin-
ing most parts of speech in terms of such func-
tions.
Parts of speech in CCG are called syntactic cat-
egories. CCG has two kinds of syntactic cat-
egories: atomic categories and functional cate-
gories. Atomic categories are used to represent
phrases that do not accept arguments. These cate-
gories includeN for noun,NP for noun phrase, S
for sentence, and PP for prepositional phrase. All
other parts of speech are represented using func-
tional categories. Functional categories are written
as X/Y or X\Y , where both X and Y are syn-
2
Part of speech Syntactic category Example usage Semantic type Example log. form
Noun N person : N Rd vperson
Adjective N/Nx good person : N ?Rd,Rd? ?x.Agoodx
Determiner NP/Nx the person : NP ?Rd,Rd? ?x.x
Intrans. Verb S\NPx the person ran : S ?Rd,Rd? ?x.Aranx+ bran
Trans. Verb S\NPy/NPx the person ran home : S ?Rd, ?Rd,Rd?? ?x.?y.(Tranx)y
Adverb (S\NP )\(S\NP ) ran lazily : S\NP ??Rd,Rd?, ?Rd,Rd?? [?y.Ay ? ?y.(TlazyA)y]
(S\NP )/(S\NP ) lazily ran : S\NP ??Rd,Rd?, ?Rd,Rd?? [?y.Ay ? ?y.(TlazyA)y]
(N/N)/(N/N) very good person : N ??Rd,Rd?, ?Rd,Rd?? [?y.Ay ? ?y.(TveryA)y]
Preposition (N\Ny)/Nx person in France : N ?Rd, ?Rd,Rd?? ?x.?y.(Tinx)y
(S\NPy)\(S\NP )f/NPx ran in France : S\NP ?Rd, ??Rd,Rd?, ?Rd,Rd??? ?x.?f.?y.(Tinx)(f(y))
Table 1: Common syntactic categories in CCG, paired with their semantic types and example logical
forms. The example usage column shows phrases paired with the syntactic category that results from
using the exemplified syntactic category for the bolded word. For ease of reference, each argument to
a syntactic category on the left is subscripted with its corresponding semantic variable in the example
logical form on the right. The variables x, y, b, v denote vectors, f denotes a function, A denotes a
matrix, and T denotes a tensor. Subscripted variables (Ared) denote parameters. Functions in logical
forms are specified using lambda calculus; for example ?x.Ax is the function that accepts a (vector)
argument x and returns the vector Ax. The notation [f ? g] denotes the higher-order function that,
given input function f , outputs function g.
tactic categories. These categories represent func-
tions that accept an argument of category Y and
return a phrase of category X . The direction of
the slash defines the expected location of the argu-
ment: X/Y expects an argument on the right, and
X\Y expects an argument on the left.2 For ex-
ample, adjectives are represented by the category
N/N ? a function that accepts a noun on the right
and returns a noun.
The left part of Table 1 shows examples of
common syntactic categories, along with exam-
ple uses. Note that some intuitive parts of speech,
such as prepositions, are represented by multiple
syntactic categories. Each of these categories cap-
tures a different use of a preposition, in this case
the noun-modifying and verb-modifying uses.
2.2 Semantics
Semantics in CCG are given by first associating a
semantic type with each syntactic category. Each
word in a syntactic category is then assigned a
semantic representation of the corresponding se-
mantic type. These semantic representations are
known as logical forms. In our case, a logical form
is a fragment of a formula for computing a vector
space representation, containing word-specific pa-
rameters and specifying composition operations.
In order to construct a vector space model, we
associate all of the atomic syntactic categories,
2As a memory aid, note that the top of the slash points in
the direction of the expected argument.
N , NP , S, and PP , with the type Rd. Then,
the logical form for a noun like ?ball? is a vec-
tor vball ? Rd. The functional categories X/Y
and X\Y are associated with functions from the
semantic type of X to the semantic type of Y . For
example, the semantic type of N/N is ?Rd,Rd?,
representing the set of functions from Rd to Rd.3
This semantic type captures the same intuition as
adjective-noun composition models: semantically,
adjectives are functions from noun vectors to noun
vectors.
The right portion of Table 1 shows semantic
types for several syntactic categories, along with
example logical forms. All of these mappings
are a direct consequence of the assumption that
all atomic categories are semantically represented
by vectors. Interestingly, many of these semantic
types contain functions that cannot be represented
in other frameworks. For example, adverbs have
type ??Rd,Rd?, ?Rd,Rd??, representing functions
that accept an adjective argument and return an
adjective. In Table 1, the example logical form
applies a 4-mode tensor to the adjective?s matrix.
Another powerful semantic type is ?Rd, ?Rd,Rd??,
which corresponds to transitive verbs and prepo-
3The notation ?A,B? represents the set of functions
whose domain isA and whose range isB. Somewhat confus-
ingly, the bracketing in this notation is backward relative to
the syntactic categories ? the syntactic category (N\N)/N
has semantic type ?Rd, ?Rd,Rd??, where the inner ?Rd,Rd?
corresponds to the left (N\N).
3
the
NP/N
?x.x
red
N/N
?x.Aredx
ball
N
vball
N : Aredvball
NP : Aredvball
on
(NP\NP )/NP
?x.?y.Aonx+Bony
the
NP/N
?x.x
table
N
vtable
NP : vtable
NP\NP : ?y.Aonvtable +Bony
NP : Aonvtable +BonAredvball
Figure 2: Syntactic CCG parse and corresponding vector space semantic derivation.
sitions. This type represents functions from two
argument vectors to an output vector, which have
been curried to accept one argument vector at a
time. The example logical form for this type uses
a 3-mode tensor to capture interactions between
the two arguments.
Note that this semantic correspondence permits
a wide range of logical forms for each syntactic
category. Each logical form can have an arbitrary
functional form, as long as it has the correct se-
mantic type. This flexibility permits experimenta-
tion with different composition operations. For ex-
ample, adjectives can be represented nonlinearly
by using a logical form such as ?x. tanh(Ax). Or,
adjectives can be represented nonparametrically
by using kernel regression to learn the appropriate
function from vectors to vectors. We can also in-
troduce simplifying assumptions, as demonstrated
by the last entry in Table 1. CCG treats preposi-
tions as modifying intransitive verbs (the category
S\N ). In the example logical form, the verb?s
semantics are represented by the function f , the
verb?s subject noun is represented by y, and f(y)
represents the sentence vector created by compos-
ing the verb with its argument. By only operating
on f(y), this logical form assumes that the action
of a preposition is conditionally independent of the
verb f and noun y, given the sentence f(y).
2.3 Lexicon
The main input to a CCG parser is a lexicon, which
is a mapping from words to syntactic categories
and logical forms. A lexicon contains entries such
as:
ball := N : vball
red := N/N : ?x.Aredx
red := N : vred
flies := ((S\NP )/NP ) : ?x.?y.(Tfliesx)y
Each entry of the lexicon associates a word
(ball) with a syntactic category (N ) and a logical
form (vball) giving its vector space representation.
Note that a word may appear multiple times in the
lexicon with distinct syntactic categories and log-
ical forms. Such repeated entries capture words
with multiple possible uses; parsing must deter-
mine the correct use in the context of a sentence.
2.4 Parsing
Parsing in CCG has two stages. First, a category
for each word in the input is retrieved from the lex-
icon. Second, adjacent categories are iteratively
combined by applying one of a small number of
combinators. The most common combinator is
function application:
X/Y : f Y : g =? X : f(g)
Y : g X\Y : f =? X : f(g)
The function application rule states that a cate-
gory of the form X/Y behaves like a function that
accepts an input category Y and returns category
X . The rule also derives a logical form for the re-
sult by applying the function f (the logical form
for X/Y ) to g (the logical form for Y ). Figure 2
shows how repeatedly applying this rule produces
a syntactic parse tree and logical form for a phrase.
The top row of the parse represents retrieving a
lexicon entry for each word in the input. Each
following line represents a use of the function ap-
plication combinator to syntactically and semanti-
cally combine a pair of adjacent categories. The
order of these operations is ambiguous, and dif-
ferent orderings may result in different parses ? a
CCG parser?s job is to find a correct ordering. The
result of parsing is a syntactic category for the en-
tire phrase, coupled with a logical form giving the
phrase?s vector space representation.
3 Vector Space Semantic Parsing
Vector space semantic parsing (VSSP) is an
approach for constructing compositional vector
space models based on the theoretical framework
of the previous section. VSSP concretely instanti-
ates CCG?s syntactic/semantic correspondence by
adding appropriately-typed logical forms to a syn-
tactic CCG parser?s lexicon. Parsing a sentence
with this lexicon and evaluating the resulting logi-
4
Semantic type Example syntactic categories Logical form template
Rd N,NP, PP, S vw
?Rd,Rd? N/N , NP/N , S/S, S\NP ?x.?(Awx)
?Rd, ?Rd,Rd?? (S\NP )/NP , (NP\NP )/NP ?x.?y.?((Twx)y)
??Rd,Rd?, ?Rd,Rd?? (N/N)/(N/N) [?y.?(Ay)? ?y.?((TwA)y)]
Table 2: Lexicon templates used in this paper to produce a CCG semantic parser. ? represents the
sigmoid function, ?(x) = e
x
1+ex .
cal form produces the sentence?s vector space rep-
resentation.
While it is relatively easy to devise vector space
representations for individual nouns, it is more
challenging to do so for the fairly complex func-
tion types licensed by CCG. VSSP defines these
functions in two phases. First, we create a lexi-
con mapping words to parametrized logical forms.
This lexicon specifies a functional form for each
word, but leaves free some per-word parame-
ters. Parsing with this lexicon produces logical
forms that are essentially functions from these
per-word parameters to vector space representa-
tions. Next, we train these parameters to pro-
duce good vector space representations in a task-
specific fashion. Training performs stochastic gra-
dient descent, backpropagating gradient informa-
tion through the logical forms.
3.1 Producing the Parametrized Lexicon
We create a lexicon using a set of manually-
constructed templates that associate each syntactic
category with a parametrized logical form. Each
template contains variables that are instantiated to
define per-word parameters. The output of this
step is a CCG lexicon which can be used in a
broad coverage syntactic CCG parser (Clark and
Curran, 2007) to produce logical forms for input
language.4
Table 2 shows some templates used to create
logical forms for syntactic categories. To reduce
annotation effort, we define one template per se-
mantic type, covering all syntactic categories with
that type. These templates are instantiated by re-
placing the variable w in each logical form with
the current word. For example, instantiating the
second template for ?red? produces the logical
form ?x.?(Aredx), where Ared is a matrix of pa-
rameters.
4In order to use the lexicon in an existing parser, the gen-
erated syntactic categories must match the parser?s syntac-
tic categories. Then, to produce a logical form for a sen-
tence, simply syntactically parse the sentence, generate log-
ical forms for each input word, and retrace the syntactic
derivation while applying the corresponding semantic oper-
ations to the logical forms.
Note that Table 2 is a only starting point ? devis-
ing appropriate functional forms for each syntactic
category is an empirical question that requires fur-
ther research. We use these templates in our ex-
periments (Section 4), suggesting that they are a
reasonable first step. More complex data sets will
require more complex logical forms. For example,
to use high-dimensional vectors, all matrices and
tensors will have to be made low rank. Another
possible improvement is to tie the parameters for
a single word across related syntactic categories
(such as the transitive and intransitive forms of a
verb).
3.2 Training the Logical Form Parameters
The training problem in VSSP is to optimize the
logical form parameters to best perform a given
task. Our task formulation subsumes both clas-
sification and regression: we assume the input is
a logical form, and the output is a vector. Given a
data set of this form, training can be performed us-
ing stochastic gradient descent in a fashion similar
to backpropagation in a neural network.
The data set for training consists of tuples,
{(`i, yi)}ni=1, where ` is a logical form and y is a
label vector representing the expected task output.
Each logical form ` is treated as a function from
parameter vectors ? to vectors in Rd. For example,
the logical form Aredvball is a function from Ared
and vball to a vector. We use ? to denote the set
of all parameters; for example, ? = {Ared, vball}.
We further assume a loss function L defined over
pairs of label vectors. The training problem is
therefore to minimize the objective:
O(?) =
n?
i=1
L(yi, g(`i(?)) +
?
2
||?||2
Above, g represents a global postprocessing func-
tion which is applied to the output of VSSP to
make a task-specific prediction. This function may
also be parametrized, but we suppress these pa-
rameters for simplicity. As a concrete example,
consider a classification task (as in our evaluation).
In this case, y represents a target distribution over
labels, L is the KL divergence between the pre-
5
dicted and target distributions, and g represents a
softmax classifier.
We optimize the objective O by running
stochastic gradient descent. The gradients of the
parameters ? can be computed by iteratively ap-
plying the chain rule to `, which procedurally
resembles performing backpropagation in a neu-
ral network (Rumelhart et al, 1988; Goller and
Ku?chler, 1996).
4 Comparing Models of Semantic
Composition
This section compares the expressive power of
VSSP to previous work. An advantage of VSSP
is its ability to assign complex logical forms to
categories like adverbs and transitive verbs. This
section examines cases where such complex logi-
cal forms are necessary, using synthetic data sets.
Specifically, we create simple data sets mimick-
ing expected forms of composition in noun-verb-
noun and adverb-adjective-noun phrases. VSSP
is able to learn the correct composition operations
for these data sets, but previously proposed mod-
els cannot.
We compare VSSP against RNN (Socher et al,
2011) and MV-RNN (Socher et al, 2012), two
recursive neural network models which factorize
composition according to a binarized constituency
parse tree. The RNN model represents the seman-
tics of each parse tree node using a single vector,
while the MV-RNN represents each node using
both a matrix and a vector. These representations
seem sufficient for adjectives and nouns, but it is
unclear how they generalize to other natural lan-
guage constructions.
In these experiments, each model is used to map
an input phrase to a vector, which is used to train a
softmax classifier that predicts the task output. For
VSSP, we use the lexicon templates from Table 2.
All nouns are represented as two-dimensional vec-
tors, and all matrices and tensors are full rank. The
parameters of each model (i.e., the per-word vec-
tors, matrices and tensors) and the softmax classi-
fier are trained as described in Section 3.2.
4.1 Propositional Logic
The propositional logic experiment examines the
impact of VSSP?s representation of transitive
verbs. VSSP directly represents these verbs as
two-argument functions, allowing it to learn op-
erations with complex interactions between both
false and false 0,1 false or false 0,1 false xor false 0,1
true and false 0,1 true or false 1,0 true xor false 1,0
false and true 0,1 false or true 1,0 false xor true 1,0
true and true 1,0 true or true 1,0 true xor true 0,1
Table 3: Data for propositional logic experiment.
Composition Formula KL divergence
RNN 0.44
MV-RNN 0.12
VSSP 0.01
Table 4: Training error on the propositional logic
data set. VSSP achieves zero error because its
verb representation can learn arbitrary logical op-
erations.
arguments. In contrast, the RNN and MV-RNN
models learn a set of global weights which are
used to combine the verb with its arguments. The
functional forms of these models limit the kinds of
interactions that can be captured by verbs.
We evaluated the learnability of argument in-
teractions using the simple data set shown in Ta-
ble 3. In this data set, the words ?and,? ?or,? and
?xor? are treated as transitive verbs, while ?true?
and ?false? are nouns. The goal is to predict the
listed truth values, which are represented as two-
dimensional distributions over true and false.
Table 4 shows the training error of each model
on this data set, measured in terms of KL diver-
gence between the model?s predictions and the
true values. VSSP achieves essentially zero train-
ing error because its 3-mode tensor representa-
tion of transitive verbs is trivially able to learn
arbitrary logical operations. RNN and MV-RNN
can learn each logical operation independently, but
cannot learn all three at the same time ? this phe-
nomenon occurs because XOR requires different
global weight matrices than AND/OR. As a re-
sult, these models learn both AND and OR, but fail
to learn XOR. This result suggests that much of
the learning in these models occurs in the global
weight matrices, while the verb representations
can have only limited influence.
Although this data set is synthetic, the interac-
tion given by XOR seems necessary to represent
real verbs. To learn AND and OR, the arguments
need not interact ? it is sufficient to detect a set
of appropriate subject and object arguments, then
threshold the number of such arguments. This
information is essentially type constraints for the
subject and object of a verb. However, type con-
straints are insufficient for real verbs. For exam-
ple, consider the verb ?eats.? All animals eat and
6
very big elephant 1,0 very big mouse 0.3,0.7
pretty big elephant 0.9,0.1 pretty big mouse 0.2,0.8
pretty small elephant 0.8,0.2 pretty small mouse 0.1,0.9
very small elephant 0.7,0.3 very small mouse 0,1
Table 5: Data for adverb-adjective-noun compo-
sition experiment. Higher first dimension values
represent larger objects.
Composition Model KL divergence
RNN 0.10
MV-RNN 0.10
VSSP 0.00
Table 6: Training error of each composition model
on the adverb-adjective-noun experiment.
can be eaten, but not all animals eat all other an-
imals; whether or not ?X eats Y ? is true depends
on an interaction between X and Y .
4.2 Adverb-Adjective-Noun Composition
Adverbs can enhance or attenuate the properties
of adjectives, which in turn can enhance or attenu-
ate the properties of nouns. The adverb-adjective-
noun experiment compares each model?s ability
to learn these effects using a synthetic object size
data set, shown in Table 5. The task is to predict
the size of each described object, which is repre-
sented as a two-dimensional distribution over big
and small. The challenge of this data set is that
an adverb?s impact on size depends on the adjec-
tive being modified ? a very big elephant is big-
ger than a big elephant, but a very small elephant
is smaller than a small elephant. Note that this
task is more difficult than adverb-adjective com-
position (Socher et al, 2012), since in this task
the adverb has to enhance/attenuate the enhanc-
ing/attenuating properties of an adjective.
Table 6 shows the training error of each model
on this data set. VSSP achieves zero training error
because its higher-order treatment of adverbs al-
lows it to accurately represent their enhancing and
attenuating effects. However, none of the other
models are capable of representing these effects.
This result is unsurprising, considering that the
RNN and MV-RNN models essentially add the
adverb and adjective parameters using a learned
linear operator (followed by a nonlinearity). Such
additive combination forces adverbs to have a con-
sistent direction of effect on the size of the noun,
which is incompatible with the desired enhancing
and attenuating behavior.
Examining VSSP?s learned parameters clearly
demonstrates its ability to learn enhancing and
?elephant?
?
1.6
?0.1
?
?mouse?
?
?0.1
1.6
?
?small? ? 0.22 0
0 1.7
? ?big? ? 1.7 ?1.1
0 0.22
?
?very small? ? 0.25 ?.12
?1.34 2.3
? ?very big?? 2.3 ?1.34
?0.12 0.25
?
Figure 3: Parameters for nouns, adjectives and ad-
jective phrases learned by VSSP. When the adverb
?very? is applied to ?small? and ?big,? it enhances
their effect on a modified noun.
attenuating phenomena. Figure 3 demonstrates
VSSP?s learned treatment of ?very.? In the fig-
ure, a high first dimension value represents a large
object, while a high second dimension value rep-
resents a small object; hence the vectors for ele-
phant and mouse show that, by default, elephants
are larger than mice. Similarly, the matrices for
big and small scale up the appropriate dimension
while shrinking the other dimension. Finally, we
show the computed matrices for ?very big? and
?very small? ? this operation is possible because
these phrases have an adjective?s syntactic cate-
gory, N/N . These matrices have the same di-
rection of effect as their unenhanced versions, but
produce a larger scaling in that direction.
5 Related Work
Several models for compositionality in vector
spaces have been proposed in recent years. Much
work has focused on evaluating composition oper-
ations for word pairs (Mitchell and Lapata, 2010;
Widdows, 2008). Many operations have been pro-
posed, including various combinations of addition,
multiplication, and linear operations (Mitchell and
Lapata, 2008), holographic reduced representa-
tions (Plate, 1991) and others (Kintsch, 2001).
Other work has used regression to train models
for adjectives in adjective-noun phrases (Baroni
and Zamparelli, 2010; Guevara, 2010). All of this
work is complementary to ours, as these composi-
tion operations can be used within VSSP by appro-
priately choosing the logical forms in the lexicon.
A few comprehensive frameworks for compo-
sition have also been proposed. One approach
is to take tensor outer products of word vec-
tors, following syntactic structure (Clark and Pul-
man, 2007). However, this approach results in
differently-shaped tensors for different grammati-
cal structures. An improvement of this framework
uses a categorial grammar to ensure that similarly-
7
typed objects lie in the same vector space (Clark
et al, 2008; Coecke et al, 2010; Grefenstette
and Sadrzadeh, 2011). VSSP generalizes this
work by allowing nonlinear composition opera-
tions and considering supervised parameter esti-
mation. Several recent neural network models im-
plicitly use a framework which assumes that com-
position factorizes according to a binarized con-
stituency parse, and that words and phrases have
uniform semantic representations (Socher et al,
2011; Socher et al, 2012). Notably, Hermann
and Blunsom (2013) instantiate such a framework
using CCG. VSSP generalizes these approaches,
as they can be implemented within VSSP by
choosing appropriate logical forms. Furthermore,
our experiments demonstrate that VSSP can learn
composition operations that cannot be learned by
these approaches.
The VSSP framework uses semantic parsing to
define a compositional vector space model. Se-
mantic parsers typically map sentences to logi-
cal semantic representations (Zelle and Mooney,
1996; Kate and Mooney, 2006), with many sys-
tems using CCG as the parsing formalism (Zettle-
moyer and Collins, 2005; Kwiatkowski et al,
2011; Krishnamurthy and Mitchell, 2012). Al-
though previous work has focused on logical se-
mantics, it has demonstrated that semantic parsing
is an elegant technique for specifying models of
compositional semantics. In this paper, we show
how to use semantic parsing to produce composi-
tional models of vector space semantics.
6 Discussion and Future Work
We present vector space semantic parsing (VSSP),
a general framework for building compositional
models of vector space semantics. Our frame-
work is based on Combinatory Categorial Gram-
mar (CCG), which defines a correspondence be-
tween syntactic categories and semantic types rep-
resenting vectors and functions on vectors. A
model in VSSP instantiates this mapping in a CCG
semantic parser. This semantic parser parses nat-
ural language into logical forms, which are in turn
evaluated to produce vector space representations.
We further propose a method for constructing such
a semantic parser using a small number of logi-
cal form templates and task-driven estimation of
per-word parameters. Synthetic data experiments
show that VSSP?s treatment of adverbs and tran-
sitive verbs can learn more functions than prior
work.
An interesting aspect of VSSP is that it high-
lights cases where propositional semantics seem
superior to vector space semantics. For example,
compare ?the ball that I threw? and ?I threw the
ball.? We expect the semantics of these phrases
to be closely related, differing only in that one
phrase refers to the ball, while the other refers to
the throwing event. Therefore, our goal is to de-
fine a logical form for ?that? which appropriately
relates the semantics of the above expressions. It is
easy to devise such a logical form in propositional
semantics, but difficult in vector space semantics.
Producing vector space solutions to such problems
is an area for future work.
Another direction for future work is joint train-
ing of both the semantic parser and vector space
representations. Our proposed approach of adding
logical forms to a broad CCG coverage parser has
the advantage of allowing VSSP to be applied to
general natural language. However, using the syn-
tactic parses from this parser may not result in
the best possible factorization of semantic com-
position. Jointly training the semantic parser and
the vector space representations may lead to better
models of semantic composition.
We also plan to apply VSSP to real data sets.
We have made some progress applying VSSP to
SemEval Task 8, learning to extract relations be-
tween nominals (Hendrickx et al, 2010). Al-
though our work thus far is preliminary, we have
found that the generality of VSSP makes it easy
to experiment with different models of composi-
tion. To swap between models, we simply mod-
ify the CCG lexicon templates ? all of the remain-
ing infrastructure is unchanged. Such preliminary
results suggest the power of VSSP as a general
framework for learning vector space models.
Acknowledgments
This research has been supported in part by
DARPA under award FA8750-13-2-0005, and in
part by a gift from Google. We thank Matt
Gardner, Justin Betteridge, Brian Murphy, Partha
Talukdar, Alona Fyshe and the anonymous review-
ers for their helpful comments.
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
8
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Stephen Clark and Stephen Pulman. 2007. Combining
symbolic and distributional models of meaning. In
Proceedings of AAAI Spring Symposium on Quan-
tum Interaction.
Stephen Clark, Bob Coecke, and Mehrnoosh
Sadrzadeh. 2008. A compositional distribu-
tional model of meaning. Proceedings of the
Second Symposium on Quantum Interaction.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical Foundations for a Com-
positional Distributed Model of Meaning. Lambek
Festschirft, Linguistic Analysis, 36.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537, November.
Christoph Goller and Andreas Ku?chler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In Proceedings
of the International Conference on Neural Networks
(ICNN-96), pages 347?352. IEEE.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing.
Thomas L. Griffiths, Joshua B. Tenenbaum, and Mark
Steyvers. 2007. Topics in semantic representation.
Psychological Review 114.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
Geometrical Models of Natural Language Seman-
tics.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid O?. Se?aghdha, Sebastian
Pado?, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2010. Semeval-2010 task 8:
Multi-way classification of semantic relations be-
tween pairs of nominals. In Proceedings of the 5th
International Workshop on Semantic Evaluation.
Karl Moritz Hermann and Phil Blunsom. 2013. The
Role of Syntax in Vector Space Models of Composi-
tional Semantics. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics.
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, Proceedings
of the Conference.
Walter Kintsch. 2001. Predication. Cognitive Science,
25(2).
Jayant Krishnamurthy and Tom M. Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical general-
ization in ccg grammar induction for semantic pars-
ing. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive
Science, 34(8):1388?1429.
Tony Plate. 1991. Holographic reduced represen-
tations: convolution algebra for compositional dis-
tributed representations. In Proceedings of the 12th
International Joint Conference on Artificial Intelli-
gence - Volume 1.
Reinhard Rapp. 2003. Word sense discovery based on
sense descriptor dissimilarity. In Proceedings of the
Ninth Machine Translation Summit.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1988. Neurocomputing: foundations of
research. chapter Learning internal representations
by error propagation.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-Supervised Recursive Autoencoders for Pre-
dicting Sentiment Distributions. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP).
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Composi-
tionality Through Recursive Matrix-Vector Spaces.
In Proceedings of the 2012 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
Mark Steedman. 1996. Surface Structure and Inter-
pretation. The MIT Press.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1), January.
9
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3), September.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Proceedings of the
Second AAAI Symposium on Quantum Interaction.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the thirteenth na-
tional conference on Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: struc-
tured classification with probabilistic categorial
grammars. In UAI ?05, Proceedings of the 21st Con-
ference in Uncertainty in Artificial Intelligence.
10
