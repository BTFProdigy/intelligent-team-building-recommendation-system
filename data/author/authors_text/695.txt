An Empirical Evaluation of LFG-DOP 
Rens Bod 
Infonnatics Research Institute, University of Leeds, Leeds LS2 9JT, & 
Institute for Logic, Language and Computation, University of Amsterdam 
mns@scs.leeds.ac.uk 
Abst ract  
This paper presents an empirical assessment of the LFG- 
DOP model introduced by Bed & Kaplan (1998). The 
parser we describe uses fragments l'rom LFG-aunotated 
sentences to parse new sentences and Monte Carlo 
techniques to compute the most probable parse. While 
our main goal is to test Bed & Kaplan's model, we will 
also test a version of LFG-DOP which treats generalized 
fragments as previously unseen events. Experiments with 
the Verbmobil and Itomecentre corpora show that our 
version of LFG-DOP outperforms Bed & Kaplan's 
model, and that LFG's functional information improves 
the parse accuracy of Iree structures. 
1 Introduction 
We present an empirical ewduation of the LFG-DOP 
model introduced by Bed & Kaplan (1998). LFG-DOP is 
a Data-Oriented Parsing (DOP) model (Bed 1993, 98) 
based on the syntactic representations of Lexical- 
Functional Grammar (Kaplan & Bresnan 1982). A DOP 
model provides linguistic representations lot- an tmlimit- 
cd set of sentences by generalizing from a given corptts 
of annotated exemphu's, it operates by decomposing the 
given representations into (arbitrarily large) fi'agments 
and recomposing those pieces to analyze new sentences. 
The occurrence-frequencies of the fragments are used to 
determine the most probable analysis of a sentence. 
So far, DOP models have been implelnented for 
phrase-structure trees and logical-semantic represent- 
ations (cf. Bed 1993, 98; Sima'an 1995, 99; Bonnema el 
al. 1997; Goodman 1998). However, these DOP models 
are limited in that they cannot accotmt for underlying 
syntactic and semantic dependencies that are not 
reflected directly in a surface tree. DOP models for a 
number of richer representations have been explored 
(van den Berg et al 1994; Tugwell 1995), but these 
approaches have remained context-free in their 
generative power. In contrast, Lexical-Functional 
Grammar (Kaplan & Bresnan 1982) is known to be 
beyond context-free. In Bed & Kaplan (1998), a first 
DOP model was proposed based on representations 
defined by LFG theory ("LFG-DOP"). I This model was 
I DOP models have recently also been proposed for Tree- 
Adjoining Grammar and Head-driven Phrase Structure 
Grammar (cf. Neumann & Flickinger 1999). 
studied fi'om a mathematical perspective by Cormons 
(1999) who also accomplished a first simple experinacnt 
with LFG-DOP. Next, Way (1999) studied LFG-DOP as 
an architecture for machine translation. The current 
paper contains tile first extensive empMeal evaluation 
of LFG-DOP on the currently available LFG-annotatcd 
corpora: the Verbmobil corpus and the Itomecentre 
corpus. Both corpora were annotated at Xerox PARC. 
Out" parser uses fragments from LFG-annotated 
sentences to parse new sentences, and Monte Carlo 
lechniques to compute the most probable parse. 
Although our main goal is to lest Bed & Kaplan's LFG- 
l)OP model, we will also test a modified version o1' 
LFG-DOP which uses a different model for computing 
fragment probabilities. While Bed & Kaplan treat all 
fragments probabil istically equal regardless whether 
they contain generalized features, we will propose a 
more fine-grained probabil ity model which treats 
fragments with generalized features as previously 
unseen events and assigns probabil i t ies to these 
fi'agments by means of discotmting. The experiments 
indicate that our probability model outperforms Bed & 
Kaplan's probabil ity model on the Verbmobil and 
Homecentre corpora. 
The rest of this paper is organized as follows: 
we first summarize the LFG-DOP model and go into our 
proposed extension. Next, we explain the Monte Carlo 
parsing technique for estimating lhe most probable LFG- 
parse o1' a sentence. In section 3, we test our parser on 
sentences from the LFG-annotated corpora. 
2 Summary of LFG-DOP and an Extension 
In accordance with Bed (1998), a particular DOP model 
is described by specifying settings for the following four 
parameters: 
? a formal definition of a well-formed representation for 
tltlcl'allcc (lllalys~s, 
? a set of decomposition operations that divide a given 
utterance analysis into a set of.fragments, 
? a set of composition operations by which such 
fragments may be recombined to derive an analysis of a 
new utterance, and 
? a probabili O, model that indicates how the probability 
of a new utterance analysis is computed. 
62 
In defining a l)OP model for l .exicaI-Fuuctional 
Grammar representations, Bed & Kaphm (1998) give 
the following sctlings for l)OP's four parameters. 
2.1 Representations 
The representations u ed by LFG-I)OP are direclly taken 
from LFG: they consist of a c-structure, an f-strt|ett|re 
and a mapping q~ between them (sue Kaplan & thesnan 
1982). The fol lowing figure shows an example 
representation for the utterance Kim eats. (We leave out 
somK features to keep the example simple.) 
~/~~ I"RH' 'Kim' l 
" K im caL~; l'Rlil) 'L I{(,SUIH) '
Figttt'c I. A representation Ibr Kim eat.~ 
Bed & Kaphm also introduce the notion of accessibility 
which they later use for defining the decomposition 
operations of LFG-DOP: 
An f-struc{t|re unit./'is ?p-accessible f|om a node n iff 
either ;t is ~l)-Iinked to j' (that is, f=  ~l)(;;) ) or./' is 
contained within d)(n) (that is, there is a chain (31 
atlribu(es that leads from qb(n) Iof). 
According to tire IA;G representation theory, c-st|'uctures 
and f-structures must salisfy certain fo|n|al well- 
formedness conditions. A c-strt|cture/f-structu|e pair is a 
val id LFG represcntalion only if it satisfies the 
Nonbranching Dominance, UniquelleSs, Coherence and 
Completeness conditions (see Kaplan & Bresnan 1982). 
2.2 Decomposition operations and FragmelflS 
The l'x'agmellts for LFG-I)OP consist (3t" connected 
sublrees whose nodes are in <l~-correslxmdence with Ihe 
cor|'epondi|lg sul>units of f-structures. To give a precise 
definition of L1;G-I)OP fragments, it is convenient o 
recall the decomposition operations employed by the 
simpler "Tree-l)OP" model which is based on phrase- 
structt|,'e treks only (Bed 1998): 
(I) Root: the Root operation selects any node of a trek 
to be the root o1' the new subtree and erases all 
nodes except the selected node and the nodes it 
dominates. 
(2) Frontier: the Frontier operation then chooses a set 
(possibly en|pty) of nodes in tile new subtree 
different from its ,'oot and erases all subtrees 
dominated by the chosen nodes. 
Bed & Kaplau extend Tree-1)OP's Root and Frontier 
operations o that they also apply to the nodes of the c- 
structure in L1;G, while respecting the fundamental 
principles of c-structure/f-structure correspotKlence. 
When a node is selected by the Root operation, 
all nodes outside of that node's subtree are erased, jttst 
as in Tree-DOP. Further, I'or LFG-DOP, all d? l inks 
leaving the erased nodes arc removed and all f-structure 
traits that are not (~-accessible from the remaining nodes 
are erased. For example, if Root selects the NP in figure 
1, then the f-structure corresponding lo the S node is 
erased, giving figure 2 as a possible fl'agment: 
lqgurc 2. An LFG-DOI ~ fragment obtained by Root 
In addition tile Root  Ol)eratioll deletes from the 
remaining f-st,ucturo all semantic forlns lhat are local to 
f-structu|'es thai correspond to el'used c-slructure nodes, 
and it thereby also maintains the fundamental two-way 
connection between words and meanings. Thus, if Root 
selects the VP node so thai the NP is erased, the subject 
semantic form "Kim" is also deleted: 
i f  
cats  I'RI;I) 'Ca|(SUIH)'  
Figure 3. Another I.FG-I)O1 > fragment 
As with Tree-1)OP, the Frontier operation thel~ selects a 
set of f,outier nodes and deletes all subtrees they 
dominate. Like Root, it also removes Ihe q~ links of the 
deleted nodes and erases any semantic form that 
cOlleSpolldS to ~.llly (3|" those nodes. Frontier does not 
delete any other f-structure fealures, however. For 
instance, if tire NP in figure I is sclec{ed as a l't'onlim" 
node, Frontier erases the predicate "Kim" from the 
fragment: 
- 
PI(I~I) 'eat(SUll J) '  C~IIS 
Figure 4. A I:ronlier-gcneratcd fragment 
Finally, Bed & Kaplan present a third decomposition 
operation, Discard, defined to conslruct generalizations 
of the fragments upplied by Root and Frontier. Discard 
acts to delete combinations of altrilmte-value pairs 
subject Io Ihe following condition: Discard does not 
delete pairs whose values ~-correspond to relnaining c- 
63 
structure nodes. Discard produces fragments uch as in 
figure 5, where the subject's number in figure 3 has been 
deleted: 
eats 
SUItJ \[ \] 
'l'liNSli PRt!S 
I'Rlil) 'eat(suB J)' 
Figure 5. A l)iscard-generated lYagment 
2.3 The composition operation 
In LFG-DOP the operation for combining fragments, 
indicated by o, is carried out in two steps. First the c- 
structures are combined by left-most substitution subject 
to the category-matching condition, .just as in Tree-DOP 
(cf. Bed 1993, 98). This is followed by the recursive 
unil'ication of the f-structures corresponding to the 
matching nodes. A derivation for an LFG-DOP 
representation R is a sequence o1' fragments the first of 
which is labeled with S and for which the itcrative 
application of the composition operation produces R. 
The two-stage composit ion operation is 
illustrated by a simple example. We therefore assume a 
corpus containing the representation i  figure 1 for the 
sentence Kim eats  and the representation i  figure 6 for 
the sentence John fell. 
? : I I'RH,'J?h.' \]
l 
i'17 :7: n 'fall(SUl~l)' 
Figure 6. Corpus represeutation for John fell 
Figure 7 shows the effect of the LFG-DOP composition 
operation using two fragments from this corpus, resulting 
in a representation for the new sentence Kim fell. 
I'RID 'Killl' = 
fell \[ PRI:,) 'fl~H(SUIJJ)' i 
StJBJ PRED 'K\[III' 1 
1 
- Kim fi.er PRI{')  f Ill(SUB J) 
Figure 7. Illustration of the composition operation 
This representation satisfies the well- formedness 
conditions and is therefore valid. Note that the sentence 
Kim fell can be parsed by fragments that are generated 
by the decomposition operations Root and Frontier only, 
without using generalized fragments (i.e. fragments 
generated by tile Discard operation). Bed & Kaplan 
(1998) call a sentence "granunatical with respect o a 
corpus" if it call be parsed without general ized 
fragments. Generalized fragments are needed ouly to 
parse sentences that a,e "ungrammatical with respect o 
the corpus". 
2.4 Probability models 
As in Tree-DOP, an LFG-DOP representation R can 
typically be derived in many different ways. If each 
derivation D has a probability P(D), then the probability 
of deriving R is the sum of the individual derivation 
probabilities, as shown in (1): 
(1) P(R) = ~I.) derives R P(D) 
An LFG-DOP derivation is produced by a stochastic 
process which starts by randomly choosing a fraglnent 
whose c-structure is labeled with the initial category 
(e.g. S). At each subsequent step, a next fragment is 
chosen at random from among the fragments that can be 
composed with the current subanalysis. The chosen 
fragment is composed with the current subanalysis to 
produce a new one; the process stops when an analysis 
results with no non-te,'minal leaves. We will call the set 
of composable fragments at a certain step in the 
stochastic process the competition set at that step. Let 
CP(f l  CS) denote the probability of choosing a fragment 
f f rom a competition set CS containing J; then the 
probability of a derivation D = <fJ,f2 ...Jk> is 
C2) P(<ag,f, ...fk>) = H i  cpq} I csi)  
where the compet i t io ,  l~robability CP0el CS) is ex- 
pressed in terms of fragment probabilities Pq): 
(3) CP(f I CS) - PCt) 
Z,,,~ cs P(/+') 
Bed & Kaplan give three definitions of increasing 
complexity for the competition set: the first definition 
groups all fi'agments that only satisfy the Category- 
matching condition o1' the composition operation (thus 
leaving out the Uniqueness, Coherence and Complete- 
uess conditions); the second definition groups all 
fragments which satisfy both Category-matching and 
Uniqueness; and the third defi,fition groups all fragments 
which satisfy Category-matching, Uniqueness and 
Coherence. Bed & Kaplan point out that the 
Completeness condition cannot be enforced at each step 
of the stochastic derivation process. It is a property of 
the final representation which can only be enforced by 
sampling valid representations from the outpt, t of the 
stochastic process. 
In this paper, we will only deal with the third 
definition of competition set, as it selects only those 
64 
l'ragments at each derivation step that may finally restdt 
in a valid LFG representation, lhus reducing lhe off-line 
validity checking just to the Completeness condition. 
Notice that the computation o1' the competition 
probability in (3) still reqttires a del:inition for the 
fragment probabil ity P(f). Bed & Kaplan define the 
probability of a fragment simply as its relative frequency 
in the bag of all fragments generated from the corpus. 
Thus Bed & Kaplan do not distinguish between 
Root~Frontier-generated l 'ragments a11d Discard- 
generated l'ragments, the latter being generalizations 
over Root~Frontier-generated fragments. Although Bed 
& Kaplan illustrate with a simple example that their 
probability model exhibits a preference for the most 
specific representation containing the fewest feature 
generalizations (mainly because specific representations 
tend to have more derivations than general ized 
representations), they do not perform an empirical 
evaluation el' lheir model. We will assess their model on 
the LFG-annotated Verbmobil and Itomecentre corpora 
in section 3 of this paper. 
However, we will also assess an alternative 
definition of fragment probability which is a refinement 
of Bed & Kaplan's model. This definition does 
distinguish between fragments upplied by Root~Frontier 
and fragments upplied by Discard. We will treat the 
first type el' fragments as seen events, and the second 
type of fragments as previously unseen events. We thus 
create two separate bags corresponding to two separate 
distributions: a bag with l'ragments generated by Root 
and Frontier, and a bag with l'ragments generated by 
Discard. We assign probability mass to the fragments of 
each bag by means of discounting: the relative 
l'requencies of seen events are discounted and the 
gained probability mass is reserved for Ihe bag el' unseen 
events (cf. Ney et al 1997). We accolnplish lhis by a 
very simple estimalor: lhe Turing-Good estimator (Good 
1953) which computes lhe probability mass of unseen 
events as n l /N  where n I is the ,mmber of singleton 
events and N is the total number of seen events. This 
probability mass is assigned to the bag o1' Discard- 
generated fragments. The remaining mass (1 -n l /N) i s  
assigned to the bag o1' Root~Front ier-generated 
l 'ragments. Thus tim total probabi l i ty mass is 
redistributed over tim seen and unseen fragments. The 
probability of each l'ragment is then computed as its 
relative frequency 2 in its bag multiplied by the prol)a- 
bility mass assigned to this bag. Let Ill denote the 
frequency of a fragment f, then its probability is given 
by: 
2 Bed (2000) discusses ome alternative fragment probability 
estimators, e.g. based on maximum likelihood. 
(4) P(/'ll'is generated byRootlFrontier) =
( I  - n i /N)  
Ifl 
"~-"J': .fis generated by Rool/Fro,,tier I:'l 
(5) PUlJis generated byDiscard)  = 
Ill 0q/N) 
"~'f:fisgene,'aledby Discard I.rl 
Note that this probability model assigns less probability 
mass to Discard-generated fragments than Bed & 
Kaphm's model. For each Root~Frontier-generated 
fragment there are exponential ly many Discard- 
generated fragments (exponential in the number of 
features the fragment contains), which means that in 
Bed & Kaphm's model the Discard-generated IYagnaents 
absorb a vast amount of probability mass. Our model, on 
the other hand, assigns a fixed probability mass to the 
distribution of Discard-generated l 'ragments and 
lherefore the exponential explosion of these fi'agments 
does not affect the probabilities of Root~Frontier- 
generated fragments. 
3 Testing tile LFG-DOP model 
3.1 Computing tile most probable analysis 
In his PhD-thesis, C(nmous (1999) describes a parsing 
algorithnl for I~FG-DOP which is based on tile Tree-DOP 
parsing teclmique given in Bed (1998). Cormons first 
converts LFG-representat ions into more compact 
indexed lrees: each node in the c-structure is assigned 
an index which refers to the ~-c(wresponding f-struclure 
unit. For example, the rcpresentalion in figure 6 is 
indexed as 
(S. 1 (NP.2 John.2) 
(VP. 1 fell. 1 )) 
where 
1 --> \[ (SUB J=2)  
(TENSE = PAST) 
(PRED = fall(SUBJ)) \] 
2 --> \[ (PRED = John) 
(NUM = SG) \] 
The indexed trees are then fragmented by applying the 
Tree-DOP decomposition operations described in section 
2. Next, the LFG-DOP decomposition operations Root, 
Frontier and Discard are applied to the f-structure units 
that correspond to the indices in the e-structure subtrees. 
ltaving obtained the set of LFG-DOP fragments in this 
way, each test sentence is parsed by a bottom-up chart 
parser using initially the indexed subtrees only. Thus 
only the Category-matching condition is enforced during 
65 
the chart-parsing process. Tile Uniqueness and 
Coherence conditions of the corresponding f-structure 
units are enforced during tile disambiguation (or chart- 
decoding) process. Disambiguation is accomplished by 
computing a large number o1' random derivations from 
the chart; this technique is known as "Monte Carlo 
disambiguation" and has been extensively described ill 
the literature (e.g. Bed 1998; Chappelier & Rajman 
1998; Goodman 1998). Sampling a random deriwttion 
l'rom the chart consists of choosing at random one o1' the 
fragments fi'om the sel of composable fragments at every 
labeled chart-entry (ill a top-down, leftmost order so as 
to maintain (he LFG-DOP derivation order). Thus the 
competition set of composable fragments is computed 
on the fly at each derivation step during the Monte 
Carlo sampling process by grouping the f-structure units 
that unify and that are coherent with the subderivation 
built so far. 
As mentioned in 2.4, the Completeness 
condition can only be checked after the derivation 
process. Incomplete derivations are simply removed 
from the sampling distribution. After sampling a large 
number of random derivations that satisfy the LFG 
validity requirements, the most probable analysis is 
estimated by the analysis which results most often from 
the sampled erivations. For our experiments in section 
3.2, we used a sample size of N = 10,000 derivations 
which corresponds to a maximal standard error o" of 
0.005 (o_< 1/(2~/N), see Bed 1998). 
3.2 Experiments with LFG-DOP 
We tested LFG-DOP on two LFG-anuotated corpora: the 
Verbmobil corpus, which contains appointment planning 
dialogues, and the Homecentre corpus, which contains 
Xerox printer documentation. Both corpora have been 
annotated by Xerox PARC. They contain packed LFG- 
representations (Maxwell & Kaplan 1991) of the 
grammatical parses of each sentence together with an 
indication which of these parses is the correct one. The 
parses are represented in a binary form and were 
debinarized using software provided to us by Xerox 
PARC. 3 For our experiments we only used the correct 
parses of each sentence resulting ill 540 Verbmobil 
parses and 980 Homecentre parses. Each corpus was 
divided into a 90% trai,ting set and a 10% test set. This 
division was random except for one constraint: hat all 
the words ill the test set actually occurred in the training 
set. The sentences from the test set were parsed and 
disambiguated by means of the fragments from the 
training set. Due to memory limitations, we limited the 
depth of the indexed subtrees to 4. Because of the small 
3 Thanks to Hadar Shemtov for providing us wi~.h the relevant 
software. 
size of the corpora we averaged our results on 10 
different raining/test et splits. Besides an exact match 
accuracy metric, we also used a more fine-grained 
metric based ell the well-known PARSEVAL metrics 
that evaluate phrase-structure tr es (Black et al 1991). 
The PARSEVAL metrics compare a proposed parse P 
with the corresponding correct treebank parse 7" as 
follows: 
# correct constituents in1' 
Precision = 
# constituents inP 
# correct constituents in P 
Recall = 
# constituents in 7 
In order to apply these metrics to LFG analyses, we 
extend the PARSEVAL notion of "correct constituent" in
the following way: a constituent in P is correct if there 
exists a constituent ill T of the same label that spans the 
same words and that ?l)-corresponds to the same f- 
structure unit. 
We illustrate the evaluation metrics with a 
simple example. In the next figure, a proposed parse P is 
compared with the correct parse T for the test sentence 
Kim fell .  The proposed parse is incorrect since it has the 
incorrect feature value for the TENSE attribute. Thus, il' 
this were the only test sentence, the exact match would 
be 0%. The precision, on the other hand, is higher than 
0% as it compares the parse on a constiluent basis. Both 
the proposed parse and the correct parse contain three 
constituents: S, NP and VP. While all three constituents 
ill P have the same label and span the same words as in 
T, only the NP constituent in P also maps to the same f- 
structure unit as ill T. The precision is thus equal to 1/3. 
Note that in this example the recall is equal to the 
precision, but this need not always be the case. 
, SIIIIJ \[ PI~EI)'KiI11'J 
1 
- Wun fell" PRI!') 'fall(SUllJ)' 
Proposed parse P 
Correct parse 7" 
Ill out" expm'iments we are first of all interested ill 
comparing the performance of Bed & Kaplan's 
probability model against our probability model (its 
explained ill section 2.4). Moreover, we also want to 
66 
study tile contribution of l)iscard-gelmrated fragments to 
the parse accuracy. We therefore created for each 
training sol two sets of fragments: one which contains 
all fragments (up to depth 4) and one which excludes 
the fragmenls generated by Discard. The exclusion of 
the \])iscard-generated fragments means lhat all 
prolmbility mass goes to tile fraglnents generated by 
Root and I"rot+lier in which case our model is equivalent 
to Bed & Kaplan's. The following two tables present he 
results of our experiments where +l)iscard refers to lilt+' 
full set of fragments and -Discard refers to the fragment 
set without Discard-generated fragments. 
Exact Match l'lecision Recall 
+I)iscad-I)iscard +Discard-Discard +I)iscard-l)iscafd 
I~ud&Kaplam98 I.I+,4 3525{ 13.89~ 76.0~,{ 11.5~,~ 7-1.95{ 
OurModcl 35.9~A 35.2rA 77.YA 76.0cA 76.,F,4 74.9c/~ 
Table I. l{xperimental results on tile Verbmobil colpus 
lixact Match Ihvcisi0n Recall 
+Discard q)iscm,I +l)iscmxl q)iscm'd 4I)iscm,I q)iscm'd 
l~0d&Kaplan98 2.7~{ 37.9g 17.15i 77.85{ 15.5+,~ 77.2g 
OurM0del 38.4~/~ 37.9c7< 80.05~ 77.85{ 78.65{ 77.2g 
Table 2. l+,xperiincntal ,esults on the l lomccentre corpus 
The tables show that Bed & Kaplan's model scores 
exlremely bad if all fragments are used: the exacl lnatch 
is only 1.1% on tile Verbmobil corptts alld 2.7% on the 
l \ ]o l l lecentre corpus,  whereas our model  scores 
respectively 35.9% and 38.4% on these corpora. Also the 
nlore fine-grained precision and recall scores of P, od & 
Kaplan's model are quite low: e.g. 13.8% and 11.5% on 
the Verbmobil corpus, where our tnodel obtains 77.5% 
and 76.4%. We l'()ulld ()tit Ihat even for the few lest 
senlences that occur literally in tile training set, Bed & 
Kaplan's model does not always generate tile correct 
analysis, whereas our model does. Inlerestingly, lhe 
accuracy of P, od & Kaphm's model is much higher if 
Discard-generated fragments are excluded. This suggests 
that treating generalized fragments l)robabilisl,ically in 
the same way as ungeueralized fragments ix ha,mful. 
Connons (1999) has made a mathematical obserwllion 
which also shows that generalized fragments can get too 
much probability mass. 
The tables also show l,hat Otll + way ()f assigning 
probabilities to Discard-generated fragments leads only 
Io a sl ight accuracy increase (compared to tile 
experiments ill which l)iscard-generated fragments a,'e 
excluded). According to paired t-testing none of these 
diffefences ill accuracy were statistically signil icant. 
This suggests that Discard-goner;lied fragtnents do not 
significanlly conl,'ibute to tile parse accuracy, or that 
perhaps these fragments are too ntllllerous to be reliably 
estimated Ol1 the basis of our small corpora. We also 
varied the f~lolmbilil,y mass assigned to Discard- 
generated llaglllenls: eXCel)l, for very small (_< 0 .01)or  
large values (_>.0.88), which led to an accuracy 
decrease, there was no significant change. 4
It is difficult to say how good or bad our results 
are with respect to other approaches. The only other 
published results on tile LFG-annotated Verbmobil and 
llomecetll,re corpora are by Johnson el, al. (1999) and 
Johnson & P, iezler (2000) who use a log-linear model lo 
estimale probabilities. But while we first parse the lest 
sentences with l'ragnlenls froln tile training set and 
sul)sequently c()tnpule the IllOSl, pr()bal)le parse, Jollns()ll 
cl al. directly use the packed lJ+G-representations from 
lhe lest set to select the most probable parse, Ihetel)y 
completely skipping the parsing phase (Mark Johnson, 
p.o.). Moreover,  42% of the Verbmobil sentences and 
51% of the l lomecentre sentences arc unaml+iguous (i.e. 
their packed IAVG-representations contain only one 
analysis), which makes J()hns<.m et als lask completely 
trivial for these sentences. In our apl)foaeh, all tosl 
Selltences wefe alllbiguous, Iestdling i l l  :.l I/IUC\]I nlol'e 
difficult lask. A quantitative comparison between our 
model and Johnson el, al.'s is therefore meaningless. 
l: inally, we are interested in the impact of 
funct ional  structures on predict ing lhe co l ree l  
constituet~t structures. We therefore removed all f- 
structure ut/ils from the fragtnetll,s (Ihus yielding a Trce- 
1)O1' model) and compared the results against our 
version of LFG-I)() I  ~ (which inclttde tile Discard- 
generated fraglnenls). We ewlluated tile parse accuracy 
on tile l,ree-slructures only, using exact match together 
with tile PARSIr+VAI, measures. We used tile same 
training/test set splits as in the previot, s experiments and 
limited tile maximum sublree depth again to 4. The 
lollowing tables show the results. 
Exact Match Precision P.ccall 
Trcc-I)Ol ~ 46.6% 88.9% 86.7% 
LFG-I)OI' 50.8% 90.3% 88A% 
Table 3. C-structure accuracy on the Verbmobil 
1 Although generalized tfagnlcnls thus seem statistically 
unimportant fen these cm-pora, they rclnail~ ilnportant for 
parsing ungrammatical sentences (which was the original 
motivation for including them -- see Bed & Kaplan 1998). 
67 
Exact Match Precision Recall 
Tree-DOP 49.0% 93.4% 92.1% 
LFG-DOP 53.2% 95.8% 94.7% 
Table 4. C-structure accuracy on the Homecentre 
The results indicate that LFG-DOP's functional 
structures help to improve the parse accuracy of tree- 
structures. In other words, LFG-DOP outperforms Tree- 
DOP if evaluated on tree-structures only. According to 
paired t-tests the differences in accuracy were statis- 
tically significant. 
4 Conclusion 
We have given an empirical assessment of the LFG- 
DOP model introduced by Bed & Kaplan (1998). We 
developed a new probability model for LFG-DOP which 
treats fragments with generalized features as previously 
unseen events. The experiments howed that our 
probability model outperforms Bed & Kaplan's model on 
the Verbmobil and Homeceutre corpora. Moreover, Bed 
& Kaplan's model turned out to be inadequate in dealing 
with generalized fragments. We also established that the 
contribution of generalized fragments to the parse 
accuracy in our model is minimal and statistically 
insignil'icant. Finally, we showed that LFG's l'unctional 
structures contribute to significantly higher parse 
accuracy on tree structures. This suggests that our model 
may be successfully used to exploit the functional 
annotations in the Penn Treebank (Marcus et al 1994), 
provided that these annotations can be converted into 
LFG-style l'unctional structures. As future research, we 
want to test LFG-DOP using log-linear models, as such 
models maximize the likelihood o1' the traiuing corpus. 
References 
M. van den Berg, R. Bed and R. Scha, 1994. "A Corpus-Based 
Approach to Semantic Interpretation", Proceedings 
Ninth Amsterdam Colloquium, Amsterdam, The 
Netherlands. 
E. Black et al, 1991. "A Procedure for Quantitatively 
Comparing the Syntactic Coverage of English", 
Proceedings DARPA Speech and Natural Language 
Workshop, Pacific Grove, Morgan Kaufinann. 
R. Bed, 1993. "Using an Annotated Language Corpus as a 
Virtual Stochastic Grammar", Proceedings AAAl'93, 
Washington D.C. 
R. Bed, 1998. Beyond Grammar, CSLI Publications, Cambridge 
University Press. 
R. Bed, 2000. "Parsing with the Shortest t)erivation", Proceed- 
ings COLING-2000, Saarbrticken, Gerlnany. 
R. Bed and R. Kaplan, 1998. "A Probabilistic Corpus-l)riven 
Model for Lexical Functional Analysis", Proceedings 
COLING-ACL'98, Montreal, Canada. 
R. Botmema, R. Bed and R. Scha, 1997. "A DOP Model for 
Semantic Interpretation", Proceedings AClJEACL-97, 
Madrid, Spain. 
J. Chal~pelier and M. P, ajman, 1998. "Extraction stochastique 
d'arbres d'analyse pour le mod61e DOP", Proceedings 
TALN'98, Paris, France. 
B. Cormons, 1999. Analyse t dd.~ambiguisation: U e approche h
base de corpus (Data-Oriented l'arsing) pour les 
r@resentations lexicales fonctionnelles. Phi) thesis, 
Universit6 de Rennes, France. 
I. Good, 1953. "The Population Frequencies of Species and the 
Estimation of Population Parameters", Biometrika 40, 
237-264. 
J. Goodman, 1998. Palwing Inside-Out, PhD thesis, Harwud 
University, Mass. 
M. Johnson, S. Geman, S. Canon, Z. Chi and S. Pdezler, 1999. 
"Estimators for Stochastic Unification-Based Gram- 
mars", Proceedings ACL'99, Maryland. 
M. Johnson and S. Riezler, 2000. "Exploiting Auxiliary 
I)istributions in Stochastic Unification-Based Gram- 
mars", Proceedings ANLP-NAACL-2000, Seattle, 
Washington. 
R. Kaplan, and J. Bresnan, 1982. "Lexical-Functional 
Grammar: A Formal System for Grammatical 
l~,epresentation", in J. Bresnan (ed.), The Mental 
Representation of Grammatical Relations, The MIT 
Press, Cambridge, Mass. 
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre, A. Bies, 
M. Ferguson, K. Katz and B. Schasberger, 1994. "The 
Penn Treebank: Annotating Predicate Argument 
Structure". In: ARPA Human Language Technology 
Workshop, I I 0-115. 
J. Maxwell and R. Kaplan, 1991. "A Method for Disjunctive 
Constraint Satisfaction", in M. Tomita (ed.), Current 
lssttes in Parsing Technology, Kluwer Academic 
Publishers. 
G. Neumann and D. Flickinger, 1999. "Learning Stochastic 
Lexicalized Tree Grammars from HPSG", I)FKI 
Technical Report, Saarbrackcn, Germany. 
H. Ney, S. Martin and F. Wessel, 1997. "Statistical Language 
Modeling Using Leaving-One-Out", in S. Young & G. 
Bloothooft (eds.), Corpus-Based Methods in Language 
and Speech Processing, Kluwer Academic Publishers. 
K. Sima'an, 1995. "An optimized algorithm for Data Oriented 
Parsing", in R. Mitkov and N. Nicolov (cds.), Recent 
Advances in Natural Language Plvcessing 1995, vohune 
136 of Current Issues in Linguistic Theot3,. John 
Benjamins, Amsterdam. 
K. Sima'an, 1999. Learning E\[ficient Disambiguation. PhD 
thesis, ILLC dissertation series number 1999-02. 
Utrecht / Amsterdam. 
D. Tugwell, 1995. "A State-Transition Grammar for l)ata- 
Oriented Parsing", PIwceedings Ettropean Chapter of the 
ACL95, Dublin, Ireland. 
A. Way, 1999. "A Hybrid Architecture for Robust MT using 
LFG-DOP", Journal of Experimental and 77teoretical 
Artificial Intelligence I I (Special Issue on Memory- 
Based Language Processing) 
68 
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 238?241,
Paris, October 2009. c?2009 Association for Computational Linguistics
A generative re-ranking model for dependency parsing
Federico Sangati, Willem Zuidema and Rens Bod
Institute for Logic, Language and Computation
University of Amsterdam
Science Park 904, 1098 XH Amsterdam, The Netherlands
{f.sangati,zuidema,rens.bod}@uva.nl
Abstract
We propose a framework for dependency
parsing based on a combination of dis-
criminative and generative models. We
use a discriminative model to obtain a k-
best list of candidate parses, and subse-
quently rerank those candidates using a
generative model. We show how this ap-
proach allows us to evaluate a variety of
generative models, without needing differ-
ent parser implementations. Moreover, we
present empirical results that show a small
improvement over state-of-the-art depen-
dency parsing of English sentences.
1 Introduction
Probabilistic generative dependency models de-
fine probability distributions over all valid depen-
dency structures, and thus provide a useful inter-
mediate representation that can be used for many
NLP tasks including parsing and language mod-
eling. In recent evaluations of supervised de-
pendency parsing, however, generative approaches
are consistently outperformed by discriminative
models (Buchholz et al, 2006; Nivre et al,
2007), which treat the task of assigning the cor-
rect structure to a given sentence as a classifica-
tion task. In this category we include both transi-
tion based (Nivre and Hall , 2005) and graph based
parsers (McDonald, 2006).
In this paper, we explore a reranking approach
that combines a generative and a discrimative
model and tries to retain the strengths of both.
The idea of combining these two types of models
through re-ranking is not new, although it has been
mostly explored in constituency parsing (Collins
et al, 2002). This earlier work, however, used the
generative model in the first step, and trained the
discriminative model over its k-best candidates. In
this paper we reverse the usual order of the two
models, by employing a generative model to re-
score the k-best candidates provided by a discrim-
inative model. Moreover, the generative model of
the second phase uses frequency counts from the
training set but is not trained on the k-best parses
of the discriminative model.
The main motivation for our approach is that
it allows for efficiently evaluating many gener-
ative models, differing from one another on (i)
the choice of the linguistic units that are gener-
ated (words, pairs of words, word graphs), (ii) the
generation process (Markov process, top-down,
bottom-up), and (iii) the features that are consid-
ered to build the event space (postags/words, dis-
tance). Although efficient algorithms exist to cal-
culate parse forests (Eisner, 1996a), each choice
gives rise to different parser instantiations.
1.1 A generative model for re-ranking
In our re-ranking perspective, all the generative
model has to do is to compute the probability of
k pre-generated structures, and select the one with
maximum probability. In a generative model, ev-
ery structure can be decomposed into a series of
independent events, each mapped to a correspond-
ing conditioning event. As an example, if a gener-
ative model chooses D as the right dependent of a
certain word H , conditioned uniquely on their rel-
ative position, we can define the event as D is the
right dependent of H , and the conditioning event
as H has a right dependent.
As a preprocessing step, every sentence struc-
ture in the training corpus is decomposed into a se-
ries of independent events, with their correspond-
ing conditioning events. During this process, our
model updates two tables containing the frequency
of events and their conditioning counterparts.
In the re-ranking phase, a given candidate struc-
ture can be decomposed into independent events
(e1, e2, . . . , en) and corresponding conditioning
events (c1, c2, . . . , cn) as in the training phase.
238
The probability of the structure can then be cal-
culated as
n?
i=1
f(ei)
f(ci) (1)
where f(x) returns the frequency of x previously
stored in the tables.
It is important to stress the point that the only
specificity each generative model introduces is in
the way sentence structures are decomposed into
events; provided a generic representation for the
(conditioning) event space, both training phase
and probability calculation of candidate structures
can be implemented independently from the spe-
cific generative model, through the implementa-
tion of generic tables of (conditioning) events.
In this way the probabilities of candidate struc-
tures are exact probabilities, and do not suf-
fer from possible approximation techniques that
parsers often utilize (i.e., pruning). On the other
hand the most probable parse is selected from the
set of the k candidates generated by the discrimi-
native model, and it will equal with the most prob-
able parse among all possible structures, only for
sufficiently high k.
2 MST discriminative model
In order to generate a set of k-candidate struc-
tures for every test sentence, we use a state-of-
the-art discriminative model (McDonald, 2006).
This model treats every dependency structure as
a set of word-dependent relations, each described
by a high dimensional feature representation. For
instance, if in a certain sentence word i is the
head of word j, v(i, j) is the vector describing
all the features of such relation (i.e., labels of the
two words, their postag, and other information
including words in between them, and ancestral
nodes). During the training phase the model learns
a weight vector w which is then used to find the
best dependency structure y for a given test sen-
tence x. The score that needs to be maximized is
defined as?(i,j)?y w ?v(i, j), and the best candi-
date is called the maximum spanning tree (MST).
Assuming we have the weight vector and we
only consider projective dependency structures,
the search space can be efficiently computed by
using a dynamic algorithm on a compact repre-
sentation of the parse forest (Eisner, 1996a). The
training phase is more complex; for details we re-
fer to (McDonald, 2006). Roughly, the model em-
ploys a large-margin classifier which iterates over
the structures of the training corpus, and updates
the weight vector w trying to keep the score of the
correct structure above the scores of the incorrect
ones by an amount which is proportional to how
much they differ in accuracy.
3 Generative model
3.1 Eisner model
As a generative framework we have chosen to use
a variation of model C in (Eisner, 1996a). In
this approach nodes are generated recursively in
a top-down manner starting from the special sym-
bol EOS (end of sentence). At any given node, left
and right children are generated as two separate
Markov sequences of nodes1, each conditioned on
ancestral and sibling information (which, for now,
we will simply refer to as context).
One of the relevant variations with respect to
the original model is that in our version the direc-
tion of the Markov chain sequence is strictly left
to right, instead of the usual inside outwards.
More formally, given a dependency structure T ,
and any of its node N , the probability of generat-
ing the fragment T (N) of the dependency struc-
ture rooted in N is defined as:
P (T (N)) =
L?
l=1
P (N2l)|context) ? P (T (N2l))
?
R?
r=1
P (N3r)|context) ? P (T (N3r)) (2)
where L and R are the number of left and right
children of N in T (L,R > 0), N2l is the left
daughter of N at position l in T (analogously for
right daughters). The probability of the entire de-
pendency structure T is computed as P (T (EOS)).
In order to illustrate how a dependency struc-
ture can be decomposed into events, we present
in table 1 the list of events and the correspond-
ing conditioning events extracted from the depen-
dency structure illustrated in figure 1. In this sim-
ple example, each node is identified with its word,
and the context is composed of the direction with
respect to the head node, the head node, and the
previously chosen daughter (or NONE if it is the
first). While during the training phase the event
tables are updated with these events, in the test
phase they are looked-up to compute the structure
probability, as in equation 1.
1Every sequence ends with the special symbol EOC.
239
NObama
V
won
Dthe Jpresidential
N
election
EOS
Figure 1: Dependency tree of the sentence
?Obama won the presidential election?.
3.2 Model extension
In equation 2 we have generically defined the
probability of choosing a daughter D based on
specific features associated with D and the con-
text in which it occurs. In our implementation,
this probability is instantiated as in equation 3.
The specific features associated with D are: the
distance2 dist(H,D) between D and its head H ,
the flag term(D) which specifies whether D has
more dependents, and the lexical and postag repre-
sentation of D. The context in which D occurs is
defined by features of the head node H , the previ-
ously chosen sister S, the grandparent G, and the
direction dir (left or right).
Equation 3 is factorized in four terms, each em-
ploying an appropriate backoff reduction list re-
ported in descending priority3.
P (D|context) = (3)
P (dist(H,D), term(D), word(D), tag(D)|H,S,G, dir) =
P (tag(D)|H,S,G, dir)
reduction list:
wt(H), wt(S), wt(G), dir
wt(H), wt(S), t(G), dir{ wt(H), t(S), t(G), dir
t(H), wt(S), t(G), dir
t(H), t(S), t(G), dir
? P (word(D)|tag(D), H, S,G, dir)
reduction list: wt(H), t(S), dirt(H), t(S), dir
? P (term(D)|word(D), tag(D), H, S,G, dir)
reduction list: tag(D), wt(H), t(S), dirtag(D), t(H), t(S), dir
? P (dist(P,D)|term(D), word(D), tag(D), H, S,G, dir)
reduction list: word(D), tag(D), t(H), t(S), dirtag(D), t(H), t(S), dir
2In our implementation distance values are grouped in 4
categories: 1, 2, 3? 6, 7??.
3In the reduction lists, wt(N) stands for the string in-
corporating both the postag and the word of N , and t(N)
stands for its postag. This second reduction is never applied
to closed class words. All the notation and backoff parame-
ters are identical to (Eisner, 1996b), and are not reported here
for reasons of space.
4The counts are extracted from a two-sentence corpus
which also includes ?Obama lost the election.?
Events Freq. Conditioning Events Freq.won L EOS NONE 1 L EOS NONE 2EOC L EOS won 1 L EOS won 1EOC R EOS NONE 2 R EOS NONE 2Obama L won NONE 1 L won NONE 1EOC L won Obama 1 L won Obama 1election R won NONE 1 R won NONE 1EOC R won election 1 R won election 1EOC L Obama NONE 2 L Obama NONE 2EOC R Obama NONE 2 R Obama NONE 2the L election NONE 2 L election NONE 2presidential L election the 1 L election the 2EOC L election presidential 1 L election presidential 1EOC R election NONE 2 R election NONE 2EOC L the NONE 2 L the NONE 2EOC R the NONE 2 R the NONE 2EOC L presidential NONE 1 L presidential NONE 1EOC R presidential NONE 1 R presidential NONE 1
Table 1: Events occurring when generating the de-
pendency structure in figure 1, for the event space
(dependent | direction, head, sister). According to
the reported frequency counts4, the structure has a
associated probability of 1/4.
4 Results
In our investigation, we have tested our model
on the Wall Street Journal corpus (Marcus et al,
1993) with sentences up to 40 words in length,
converted to dependency structures. Although
several algorithms exist to perform such a conver-
sion (Sangati and Zuidema, 2008), we have fol-
lowed the scheme in (Collins, 1999). Section 2-21
was used as training, and section 22 as test set.
The MST discriminative parser was provided with
the correct postags of the words in the test set, and
it was run in second-order5 and projective mode.
Results are reported in table 2, as unlabeled attach-
ment score (UAS). The MST dependency parser
obtains very high results when employed alone
(92.58%), and generates a list of k-best-candidates
which can potentially achieve much better results
(an oracle would score above 95% when selecting
from the first 5-best, and above 99% from the first
1000-best). The decrease in performance of the
generative model, as the number of the candidate
increases, suggests that its performance would be
lower than a discriminative model if used alone.
On the other hand, our generative model is able to
select better candidates than the MST parser, when
their number is limited to a few dozens, yielding a
maximum accuracy for k = 7 where it improves
accuracy on the discriminative model by a 0.51%
(around 7% error reduction).
5The features of every dependency relation include infor-
mation about the previously chosen sister of the dependent.
240
k-best Oracle best Oracle worst Reranked
1 92.58 92.58 92.58
2 94.22 88.66 92.89
3 95.05 87.04 93.02
4 95.51 85.82 93.02
5 95.78 84.96 93.02
6 96.02 84.20 93.06
7 96.23 83.62 93.09
8 96.40 83.06 93.02
9 96.54 82.57 92.97
10 96.64 82.21 92.96
100 98.48 73.30 92.32
1000 99.34 64.86 91.47 91.00%92.00%93.00%94.00%
95.00%96.00%97.00%98.00%99.00%
100.00%
1 2 3 4 5 6 7 8 9 10 100 1000
Oracle-BestRerankedMST
Figure 2: UAS accuracy of the MST discriminative and re-ranking parser on section 22 of the WSJ.
Oracle best: always choosing the best result in the k-best, Oracle worst: always choosing the worst,
Reranked: choosing the most probable candidate according to the generative model.
5 Conclusions
We have presented a general framework for depen-
dency parsing based on a combination of discrim-
inative and generative models. We have used this
framework to evaluate and compare several gener-
ative models, including those of Eisner (1996) and
some of their variations. Consistently with earlier
results, none of these models performs better than
the discriminative baseline when used alone. We
have presented an instantiation of this framework
in which our newly defined generative model leads
to an improvement of the state-of-the-art parsing
results, when provided with a limited number of
best candidates. This result suggests that discrim-
inative and generative model are complementary:
the discriminative model is very accurate to filter
out ?bad? candidates, while the generative model
is able to further refine the selection among the
few best candidates. In our set-up it is now pos-
sible to efficiently evaluate many other generative
models and identify the most promising ones for
further investigation. And even though we cur-
rently still need the input from a discriminative
model, our promising results show that pessimism
about the prospects of probabilistic generative de-
pendency models is premature.
Acknowledgments We gratefully acknowledge
funding by the Netherlands Organization for
Scientific Research (NWO): FS and RB are
funded through a Vici-grant ?Integrating Cogni-
tion? (277.70.006) to RB, and WZ through a Veni-
grant ?Discovering Grammar? (639.021.612) of
NWO. We also thank 3 anonymous reviewers for
useful comments.
References
S. Buchholz, and E. Marsi. 2006. CoNLL-X Shared
Task on Multilingual Dependency Parsing. In Proc.
of the 10th CoNLL Conference, pp. 149?164.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
M. Collins, N. Duffy, and F. Park. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In In
Proceedings of the ACL 2002, pp. 263?270.
J. Eisner. 1996a. Three New Probabilistic Models for
Dependency Parsing: An Exploration. In Proc. of
the 16th International Conference on Computational
Linguistics (COLING-96), pp. 340?345.
J. Eisner. 1996b. An Empirical Comparison of Proba-
bility Models for Dependency Grammar. Technical
Report number IRCS-96-11, Univ. of Pennsylvania.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. In Computational Linguistics,
19(2), pp. 313?330.
R. McDonald. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
J. Nivre and J. Hall. 2005. MaltParser: A Language-
Independent System for Data-Driven Dependency
Parsing. In Proc. of the Fourth Workshop on Tree-
banks and Linguistic Theories, pp. 137?148.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son,S. Riedel, and D. Yuret. 2007. The CONLL
2007 shared task on dependency parsing. In Proc.
of the CoNLL 2007 Shared Task Session, pp. 915?
932.
F. Sangati and W. Zuidema. 2008. Unsupervised
Methods for Head Assignments. In Proc. of the
EACL 2009 Conference, pp. 701?709.
241
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 865?872,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An All-Subtrees Approach to Unsupervised Parsing
Rens Bod
School of Computer Science
University of St Andrews
North Haugh, St Andrews
 KY16 9SX Scotland, UK
rb@dcs.st-and.ac.uk
Abstract
We investigate generalizations of the all-
subtrees "DOP" approach to unsupervised
parsing. Unsupervised DOP models assign
all possible binary trees to a set of sentences
and next use (a large random subset of) all
subtrees from these binary trees to compute
the most probable parse trees. We will test
both a relative frequency estimator for
unsupervised DOP and a maximum
likelihood estimator which is known to be
statistically consistent. We report state-of-
the-art results on English (WSJ), German
(NEGRA) and Chinese (CTB) data. To the
best of our knowledge this is the first paper
which tests a maximum likelihood estimator
for DOP on the Wall Street Journal, leading
to the surprising result that an unsupervised
parsing model beats a widely used
supervised model (a treebank PCFG).
1   Introduction
The problem of bootstrapping syntactic structure
from unlabeled data has regained considerable
interest. While supervised parsers suffer from
shortage of hand-annotated data, unsupervised
parsers operate with unlabeled raw data of which
unlimited quantities are available. During the last
few years there has been steady progress in the field.
Where van Zaanen (2000) achieved 39.2%
unlabeled f-score on ATIS word strings, Clark
(2001) reports 42.0% on the same data, and Klein
and Manning (2002) obtain 51.2% f-score on ATIS
part-of-speech strings using a constituent-context
model called CCM. On Penn Wall Street Journal p-
o-s-strings ? 10 (WSJ10), Klein and Manning
(2002) report 71.1% unlabeled f-score with CCM.
And the hybrid approach of Klein and Manning
(2004), which combines constituency and
dependency models, yields 77.6% f-score.
Bod (2006) shows that a further improve-
ment on the WSJ10 can be achieved by an unsuper-
vised generalization of the all-subtrees approach
known as Data-Oriented Parsing (DOP). This
unsupervised DOP model, coined U-DOP, first
assigns all possible unlabeled binary trees to a set of
sentences and next uses all subtrees from (a large
subset of) these trees to compute the most probable
parse trees. Bod (2006) reports that U-DOP not
only outperforms previous unsupervised parsers but
that its performance is as good as a binarized super-
vised parser (i.e. a treebank PCFG) on the WSJ.
A possible drawback of U-DOP, however,
is the statistical inconsistency of its estimator
(Johnson 2002) which is inherited from the DOP1
model (Bod 1998). That is, even with unlimited
training data, U-DOP's estimator is not guaranteed
to converge to the correct weight distribution.
Johnson (2002: 76) argues in favor of a maximum
likelihood estimator for DOP which is statistically
consistent. As it happens, in Bod (2000) we already
developed such a DOP model, termed ML-DOP,
which reestimates the subtree probabilities by a
maximum likelihood procedure based on
Expectation-Maximization. Although cross-
validation is needed to avoid overlearning, ML-DOP
outperforms DOP1 on the OVIS corpus (Bod
2000).
This raises the question whether we can
create an unsupervised  DOP model which is also
865
statistically consistent. In this paper we will show
that an unsupervised version of ML-DOP can be
constructed along the lines of U-DOP. We will start
out by summarizing DOP, U-DOP and ML-DOP,
and next create a new unsupervised model called
UML-DOP. We report that UML-DOP not only
obtains higher parse accuracy than U-DOP on three
different domains, but that it also achieves this with
fewer  subtrees than U-DOP. To the best of our
knowledge, this paper presents the first
unsupervised  parser that outperforms a widely used
supervised  parser on the WSJ, i.e. a treebank
PCFG. We will raise the question whether the end
of supervised parsing is in sight.
2  DOP
The key idea of DOP is this: given an annotated
corpus, use all subtrees, regardless of size, to parse
new sentences. The DOP1 model in Bod (1998)
computes the probabilities of parse trees and
sentences from the relative frequencies of the
subtrees. Although it is now known that DOP1's
relative frequency estimator is statistically
inconsistent (Johnson 2002), the model yields
excellent empirical results and has been used in
state-of-the-art systems. Let's illustrate DOP1 with a
simple example. Assume a corpus consisting of
only two trees, as given in figure 1.
NP VP
S
NP
Mary
V
likes
John
NP VP
S
NPVPeter
hates Susan
Figure 1. A corpus of two trees
New sentences may be derived by combining
fragments, i.e. subtrees, from this corpus, by means
of a node-substitution operation indicated as ?.
Node-substitution identifies the leftmost
nonterminal frontier node of one subtree with the
root node of a second subtree (i.e., the second
subtree is substituted  on the leftmost nonterminal
frontier node of the first subtree). Thus a new
sentence such as Mary likes Susan  can be derived by
combining subtrees from this corpus, shown in
figure 2.
NP VP
S
NPV
likes
NP
Mary
NP
Susan NP VP
S
NPMary V
likes Susan
=? ?
Figure 2. A derivation for Mary likes Susan
Other derivations may yield the same tree, e.g.:
NP VP
S
NPV
NP
Mary NP VP
S
NPMary V
likes Susan
=
Susan
V
likes
? ?
Figure 3. Another derivation yielding same tree
DOP1 computes the probability of a subtree t as the
probability of selecting t among all corpus subtrees
that can be substituted on the same node as t. This
probability is computed as the number of
occurrences of t in the corpus, | t |, divided by the
total number of occurrences of all subtrees t' with
the same root label as t .1 Let r(t) return the root label
of t. Then we may write:
P(t)  =   | t |
?  t': r(t')= r(t)  | t' |
The probability of a derivation t1?...?tn is computed
by the product of the probabilities of its subtrees t i:
P(t1?...?tn)  =  ?i P(ti)
As we have seen, there may be several distinct
derivations that generate the same parse tree. The
probability of a parse tree T is the sum of the
1
 This subtree probability is redressed by a simple
correction factor discussed in Goodman (2003: 136)
and Bod (2003).
866
probabilities of its distinct derivations. Let tid be the
i-th subtree in the derivation d that produces tree T ,
then the probability of T is given by
P(T)  =  ?d?i P(tid)
Thus DOP1 considers counts of subtrees of a wide
range of sizes: everything from counts of single-
level rules to entire trees is taken into account to
compute the most probable parse tree of a sentence.
A disadvantage of the approach may be that an
extremely large number of subtrees (and
derivations) must be considered. Fortunately there
exists a compact isomorphic PCFG-reduction of
DOP1 whose size is linear rather than exponential in
the size of the training set (Goodman 2003).
Moreover, Collins and Duffy (2002) show how a
tree kernel can be applied to DOP1's all-subtrees
representation. The currently most successful
version of DOP1 uses a PCFG-reduction of the
model with an n-best parsing algorithm (Bod 2003).
3  U-DOP
U-DOP extends DOP1 to unsupervised parsing
(Bod 2006). Its key idea is to assign all unlabeled
binary trees to a set of sentences and to next use (in
principle) all subtrees from these binary trees to
parse new sentences. U-DOP thus proposes one of
the richest possible models in bootstrapping trees.
Previous models like Klein and Manning's (2002,
2005) CCM model limit the dependencies to
"contiguous subsequences of a sentence". This
means that CCM neglects dependencies that are
non-contiguous such as between more  and than in
"BA carried more people than cargo". Instead, U-
DOP's all-subtrees  approach captures both
contiguous and non-contiguous lexical dependen-
cies.
As with most other unsupervised parsing
models, U-DOP induces trees for p-o-s strings
rather than for word strings. The extension to word
strings is straightforward as there exist highly
accurate unsupervised part-of-speech taggers (e.g.
Sch?tze 1995) which can be directly combined with
unsupervised parsers.
To give an illustration of U-DOP, consider
the WSJ p-o-s string NNS VBD JJ NNS which
may correspond for instance to the sentence
Investors suffered heavy losses . U-DOP starts by
assigning all possible binary trees to this string,
where each root node is labeled S and each internal
node is labeled X. Thus NNS VBD JJ NNS has a
total of five binary trees shown in figure 4 -- where
for readability we add words as well.
NNS VBD JJ NNS
Investors suffered heavy losses
X
X
S
  
NNS VBD JJ NNS
Investors suffered heavy losses
X
X
S
NNS VBD JJ NNS
Investors suffered heavy losses
X
X
S
  
NNS VBD JJ NNS
Investors suffered heavy losses
X
X
S
NNS VBD JJ NNS
Investors suffered heavy losses
XX
S
Figure 4. All binary trees for NNS VBD JJ NNS
(Investors suffered heavy losses)
While we can efficiently represent the set of all
binary trees of a string by means of a chart, we need
to unpack the chart if we want to extract subtrees
from this set of binary trees. And since the total
number of binary trees for the small WSJ10 is
almost 12 million, it is doubtful whether we can
apply the unrestricted U-DOP model to such a
corpus. U-DOP therefore randomly samples a large
subset from the total number of parse trees from the
chart (see Bod 2006) and next converts the subtrees
from these parse trees into a PCFG-reduction
(Goodman 2003). Since the computation of the
most probable parse tree is NP-complete (Sima'an
1996), U-DOP estimates the most probable tree
from the 100 most probable derivations using
Viterbi n-best parsing. We could also have used the
more efficient k-best hypergraph parsing technique
by Huang and Chiang (2005), but we have not yet
incorporated this into our implementation.
To give an example of the dependencies that
U-DOP can take into account, consider the
following subtrees in figure 5 from the trees in
867
figure 4 (where we again add words for readability).
These subtrees show that U-DOP takes into account
both contiguous and non-contiguous substrings.
NNS VBD
Investors suffered
X
X
S
VBD
suffered
X
X
NNS NNS
Investors losses
X
X
S
JJ NNS
heavy losses
XX
S
JJ NNS
heavy losses
X
NNS VBD
Investors suffered
X
VBD JJ
suffered heavy
X
Figure 5. Some subtrees from trees in figure 4
Of course, if we only had the sentence Investors
suffered heavy losses  in our corpus, there would be
no difference in probability between the five parse
trees in figure 4. However, if we also have a
different sentence where JJ NNS ( heavy losses)
appears in a different context, e.g. in Heavy losses
were reported , its covering subtree gets a relatively
higher frequency and the parse tree where heavy
losses  occurs as a constituent gets a higher total
probability.
4  ML-DOP
ML-DOP (Bod 2000) extends DOP with a
maximum likelihood reestimation technique based
on the expectation-maximization (EM) algorithm
(Dempster et al 1977) which is known to be
statistically consistent (Shao 1999). ML-DOP
reestimates DOP's subtree probabilities in an
iterative way until the changes become negligible.
The following exposition of ML-DOP is heavily
based on previous work by Bod (2000) and
Magerman (1993).
It is important to realize that there is an
implicit assumption in DOP that all possible
derivations of a parse tree contribute equally to the
total probability of the parse tree. This is equivalent
to saying that there is a hidden component to the
model, and that DOP can be trained using an EM
algorithm to determine the maximum likelihood
estimate for the training data. The EM algorithm for
this ML-DOP model is related to the Inside-Outside
algorithm for context-free grammars, but the
reestimation formula is complicated by the presence
of subtrees of depth greater than 1. To derive the
reestimation formula, it is useful to consider the
state space of all possible derivations of a tree.
The derivations of a parse tree T  can be
viewed as a state trellis, where each state contains a
partially constructed tree in the course of a leftmost
derivation of T. st denotes a state containing the tree
t which is a subtree of T . The state trellis is defined
as follows.
The initial state, s0, is a tree with depth zero,
consisting of simply a root node labeled with S. The
final state, sT, is the given parse tree T.
A state st is connected forward to all states
stf such that tf  = t ? t', for some t' . Here the
appropriate t'  is defined to be tf ? t.
A state s t is connected backward to all states
stb such that t = tb ? t', for some t' . Again, t'  is
defined to be t ? tb.
The construction of the state lattice and
assignment of transition probabilities according to
the ML-DOP model is called the forward pass. The
probability of a given state, P (s), is referred to as
?(s). The forward probability of a state s t is
computed recursively
?(st) = ? ?(st  ) P(t ? tb).b
s tb
The backward probability of a state, referred to as
?(s), is calculated according to the following
recursive formula:
?(st) = ? ?(st  ) P(tf ? t)f
fst
where the backward probability of the goal state is
set equal to the forward probability of the goal state,
?(sT) = ?(sT).
The update formula for the count of a
subtree t is (where r(t) is the root label of t):
868
ct(t) =      ?
      ?(st  )?(st  )P(t | r(t))f b
?(sgoal)st  :?st ,tb?t=tfb f
The updated probability distribution, P'(t  | r(t)), is
defined to be
P' (t | r(t)) =  ct(t)
ct(r(t))
where ct(r(t)) is defined as
ct(r(t)) = ? ct(t')
t ': r( t')=r( t)
In practice, ML-DOP starts out by assigning the
same relative frequencies to the subtrees as DOP1,
which are next reestimated by the formulas above.
We may in principle start out with any initial
parameters, including random initializations, but
since ML estimation is known to be very sensitive
to the initialization of the parameters, it is convenient
to start with parameters that are known to perform
well.
To avoid overtraining, ML-DOP uses the
subtrees from one half of the training set to be
trained on the other half, and vice versa. This cross-
training is important since otherwise UML-DOP
would assign the training set trees their empirical
frequencies and assign zero weight to all other
subtrees (cf. Prescher et al 2004). The updated
probabilities are iteratively reestimated until the
decrease in cross-entropy becomes negligible.
Unfortunately, no compact PCFG-reduction of ML-
DOP is known. As a consequence, parsing with
ML-DOP is very costly and the model has hitherto
never been tested on corpora larger than OVIS
(Bonnema et al 1997). Yet, we will show that by
clever pruning we can extend our experiments not
only to the WSJ, but also to the German NEGRA
and the Chinese CTB. (Zollmann and Sima'an 2005
propose a different consistent estimator for DOP,
which we cannot go into here).
5  UML-DOP
Analogous to U-DOP, UML-DOP is an
unsupervised generalization of ML-DOP: it first
assigns all unlabeled binary trees to a set of
sentences and next extracts a large (random) set of
subtrees from this tree set. It then reestimates the
initial probabilities of these subtrees by ML-DOP on
the sentences from a held-out part of the tree set.
The training is carried out by dividing the tree set
into two equal parts, and reestimating the subtrees
from one part on the other. As initial probabilities
we use the subtrees' relative frequencies as described
in section 2 (smoothed by Good-Turing -- see Bod
1998), though it would also be interesting to see
how the model works with other initial parameters,
in particular with the usage frequencies proposed by
Zuidema (2006).
As with U-DOP, the total number of
subtrees that can be extracted from the binary tree
set is too large to be fully taken into account.
Together with the high computational cost of
reestimation we propose even more drastic pruning
than we did in Bod (2006) for U-DOP. That is,
while for sentences ? 7 words we use all binary
trees, for each sentence ? 8 words we randomly
sample a fixed number of 128 trees (which
effectively favors more frequent trees). The resulting
set of trees is referred to as the binary tree set. Next,
we randomly extract for each subtree-depth a fixed
number of subtrees, where the depth of subtree is
the longest path from root to any leaf. This has
roughly the same effect as the correction factor used
in Bod (2003, 2006). That is, for each particular
depth we sample subtrees by first randomly
selecting a node in a random tree from the binary
tree set after which we select random expansions
from that node until a subtree of the particular depth
is obtained. For our experiments in section 6, we
repeated this procedure 200,000 times for each
depth. The resulting subtrees are then given to ML-
DOP's reestimation procedure. Finally, the
reestimated subtrees are used to compute the most
probable parse trees for all sentences using Viterbi
n-best, as described in section 3, where the most
probable parse is estimated from the 100 most
probable derivations.
A potential criticism of (U)ML-DOP is that
since we use DOP1's relative frequencies as initial
parameters, ML-DOP may only find a local
maximum nearest to DOP1's estimator. But this is
of course a criticism against any iterative ML
approach: it is not guaranteed that the global
maximum is found (cf. Manning and Sch?tze 1999:
401). Nevertheless we will see that our reestimation
869
procedure leads to significantly better accuracy
compared to U-DOP (the latter would be equal to
UML-DOP under 0 iterations). Moreover, in
contrast to U-DOP, UML-DOP can  be theoretically
motivated: it maximizes the likelihood of the data
using the statistically consistent EM algorithm.
6  Experiments: Can we beat supervised by
unsupervised parsing?
To compare UML-DOP to U-DOP, we started out
with the WSJ10 corpus, which contains 7422
sentences ? 10 words after removing empty
elements and punctuation. We used the same
evaluation metrics for unlabeled precision (UP) and
unlabeled recall (UR) as defined in Klein (2005: 21-
22). Klein's definitions differ slightly from the
standard PARSEVAL metrics: multiplicity of
brackets is ignored, brackets of span one are ignored
and the bracket labels are ignored. The two metrics
of UP and UR are combined by the unlabeled f -
score F1 which is defined as the harmonic mean of
UP and UR: F1 = 2*UP*UR/(UP+UR).
For the WSJ10, we obtained a binary tree
set of 5.68 * 105 trees, by extracting the binary trees
as described in section 5. From this binary tree set
we sampled 200,000 subtrees for each subtree-
depth. This resulted in a total set of roughly 1.7 *
1 0 6  subtrees that were reestimated by our
maximum-likelihood procedure. The decrease in
cross-entropy became negligible after 14 iterations
(for both halfs of WSJ10). After computing the
most probable parse trees, UML-DOP achieved an
f-score of 82.9% which is a 20.5% error reduction
compared to U-DOP's f-score of 78.5% on the
same data (Bod 2006).
We next tested UML-DOP on two
additional domains which were also used in Klein
and Manning (2004) and Bod (2006): the German
NEGRA10 (Skut et al 1997) and the Chinese
CTB10 (Xue et al 2002) both containing 2200+
sentences ? 10 words after removing punctuation.
Table 1 shows the results of UML-DOP compared
to U-DOP, the CCM model by Klein and Manning
(2002), the DMV dependency learning model by
Klein and Manning (2004) as well as their
combined model DMV+CCM.
Table 1 shows that UML-DOP scores better
than U-DOP and Klein and Manning's models in all
cases. It thus pays off to not only use subtrees rather
than substrings (as in CCM) but to also reestimate
the subtrees' probabilities by a maximum-likelihood
procedure rather than using their (smoothed) relative
frequencies (as in U-DOP). Note that UML-DOP
achieves these improved results with fewer  subtrees
than U-DOP, due to UML-DOP's more drastic
pruning of subtrees. It is also noteworthy that UML-
DOP, like U-DOP, does not employ a separate class
for non-constituents, so-called distituents, while
CCM and CCM+DMV do. (Interestingly, the top
10 most frequently learned constituents by UML-
DOP were exactly the same as by U-DOP -- see the
relevant table in Bod 2006).
Model English German Chinese(WSJ10) (NEGRA10) (CTB10)
CCM 71.9 61.6 45.0
DMV 52.1 49.5 46.7
DMV+CCM 77.6 63.9 43.3
U-DOP 78.5 65.4 46.6
UML-DOP 82.9 67.0 47.2
Table 1. F-scores of UML-DOP compared to
previous models on the same data
We were also interested in testing UML-DOP on
longer sentences. We therefore included all WSJ
sentences up to 40 words after removing empty
elements and punctuation (WSJ40) and again
sampled 200,000 subtrees for each depth, using the
same method as before. Furthermore, we compared
UML-DOP against a supervised binarized PCFG,
i.e. a treebank PCFG whose simple relative
frequency estimator corresponds to maximum
likelihood (Chi and Geman 1998), and which we
shall refer to as "ML-PCFG". To this end, we used
a random 90%/10% division of WSJ40 into a
training set and a test set. The ML-PCFG had thus
access to the Penn WSJ trees in the training set,
while UML-DOP had to bootstrap all structure from
the flat strings from the training set to next parse the
10% test set -- clearly a much more challenging
task. Table 2 gives the results in terms of f-scores.
The table shows that UML-DOP scores
better than U-DOP, also for WSJ40. Our results on
WSJ10 are somewhat lower than in table 1 due to
the use of a smaller training set of 90% of the data.
But the most surprising result is that UML-DOP's f-
870
score is higher than the supervised  binarized tree-
bank PCFG (ML-PCFG) for both WSJ10 and
WSJ40. In order to check whether this difference is
statistically significant, we additionally tested on 10
different 90/10 divisions of the WSJ40 (which were
the same splits as in Bod 2006). For these splits,
UML-DOP achieved an average f-score of 66.9%,
while ML-PCFG obtained an average f-score of
64.7%. The difference in accuracy between UML-
DOP and ML-PCFG was statistically significant
according to paired t-testing (p?0.05). To the best of
our knowledge this means that we have shown for
the first time that an unsupervised parsing model
(UML-DOP) outperforms a widely used supervised
parsing model (a treebank PCFG) on the WSJ40 .
Model       WSJ10  WSJ40
U-DOP 78.1 63.9
UML-DOP 82.5 66.4
ML-PCFG 81.5 64.6
Table 2. F-scores of U-DOP, UML-DOP and a
supervised treebank PCFG (ML-PCFG) for a
random 90/10 split of WSJ10 and WSJ40.
We should keep in mind that (1) a treebank PCFG
is not state-of-the-art: its performance is mediocre
compared to e.g. Bod (2003) or McClosky et al
(2006), and (2) that our treebank PCFG is binarized
as in Klein and Manning (2005) to make results
comparable. To be sure, the unbinarized version of
the treebank PCFG obtains 89.0% average f-score
on WSJ10 and 72.3% average f-score on WSJ40.
Remember that the Penn Treebank annotations are
often exceedingly flat, and many branches have arity
larger than two. It would be interesting to see how
UML-DOP performs if we also accept ternary (and
wider) branches -- though the total number of
possible trees that can be assigned to strings would
then further explode.
UML-DOP's performance still remains
behind that of supervised  (binarized) DOP parsers,
such as DOP1, which achieved 81.9% average f-
score on the 10 WSJ40 splits, and ML-DOP, which
performed slightly better with 82.1% average f-
score. And if DOP1 and ML-DOP are not
binarized, their average f-scores are respectively
90.1% and 90.5% on WSJ40. However, DOP1 and
ML-DOP heavily depend on annotated data whereas
UML-DOP only needs unannotated data. It would
thus be interesting to see how close UML-DOP can
get to ML-DOP's performance if we enlarge the
amount of training data.
7  Conclusion: Is the end of supervised
parsing in sight?
Now that we have outperformed a well-known
supervised parser by an unsupervised one, we may
raise the question as to whether the end of
supervised NLP comes in sight. All supervised
parsers are reaching an asymptote and further
improvement does not seem to come from more
hand-annotated data but by adding unsupervised or
semi-unsupervised techniques (cf. McClosky et al
2006). Thus if we modify our question as: does the
exclusively  supervised approach to parsing come to
an end, we believe that the answer is certainly yes.
Yet we should neither rule out the
possibility that entirely unsupervised methods will
in fact surpass semi-supervised methods. The main
problem is how to quantitatively compare these
different parsers, as any evaluation on hand-
annotated data (like the Penn treebank) will
unreasonably favor semi-supervised parsers. There
is thus is a quest for designing an annotation-
independent evaluation scheme. Since parsers are
becoming increasingly important in applications like
syntax-based machine translation and structural
language models for speech recognition, one way to
go would be to compare these different parsing
methods by isolating their contribution in improving
a concrete NLP system, rather than by testing them
against gold standard annotations which are
inherently theory-dependent.
The initially disappointing results of
inducing trees entirely from raw text was not so
much due to the difficulty of the bootstrapping
problem per se , but to (1) the poverty of the initial
models and (2) the difficulty of finding theory-
independent evaluation criteria. The time has come
to fully reappraise unsupervised parsing models
which should be trained on massive amounts of
data, and be evaluated in a concrete application.
There is a final question as to how far the
DOP approach to unsupervised parsing can be
stretched. In principle we can assign all possible
syntactic categories, semantic roles, argument
871
structures etc. to a set of given sentences and let the
statistics decide which assignments are most useful
in parsing new sentences. Whether such a massively
maximalist approach is feasible can only be
answered by empirical investigation in due time.
Acknowledgements
Thanks to Willem Zuidema, David Tugwell and
especially to three anonymous reviewers whose
unanymous suggestions on DOP and EM
considerably improved the original paper. A
substantial part of this research was carried out in
the context of the NWO Exact project
"Unsupervised Stochastic Grammar Induction from
Unlabeled Data", project number 612.066.405.
References
Bod, R. 1998. Beyond Grammar: An Experience-Based
Theory of Language, CSLI Publications, distributed
by Cambridge University Press.
Bod, R. 2000. Combining semantic and syntactic
structure for language modeling. Proceedings ICSLP
2000, Beijing.
Bod, R. 2003. An efficient implementation of a new
DOP model. Proceedings EACL 2003, Budapest.
Bod, R. 2006. Unsupervised Parsing with U-DOP.
Proceedings CONLL 2006, New York.
Bonnema, R., R. Bod and R. Scha, 1997. A DOP
model for semantic interpretation, Proceedings
ACL/EACL 1997, Madrid.
Chi, Z. and S. Geman 1998. Estimation of
Probab i l i s t i c  Con tex t -F ree  Grammars .
Computational Linguistics 24, 299-305.
Clark, A. 2001. Unsupervised induction of stochastic
context-free grammars using distributional
clustering. Proceedings CONLL 2001.
Collins, M. and N. Duffy 2002. New ranking
algorithms for parsing and tagging: kernels over
discrete structures, and the voted perceptron.
Proceedings ACL 2002, Philadelphia.
Dempster, A., N. Laird and D. Rubin, 1977. Maximum
Likelihood from Incomplete Data via the EM
Algorithm, Journal of the Royal Statistical Society
39, 1-38.
Goodman, J. 2003. Efficient algorithms for the DOP
model. In R. Bod, R. Scha and K. Sima'an (eds.).
Data-Oriented Parsing, University of Chicago Press.
Huang, L. and D. Chiang 2005. Better k-best parsing.
Proceedings IWPT 2005, Vancouver.
Johnson, M. 2002. The DOP estimation method is
biased and inconsistent. Computational Linguistics
28, 71-76.
Klein, D. 2005. The Unsupervised Learning of Natural
Language Structure . PhD thesis, Stanford
University.
Klein, D. and C. Manning 2002. A general
constituent-context model for improved grammar
induction. Proceedings ACL 2002, Philadelphia.
Klein, D. and C. Manning 2004. Corpus-based
induction of syntactic structure: models of
dependency and constituency. Proceedings ACL
2004, Barcelona.
Klein, D. and C. Manning 2005. Natural language
grammar induction with a generative constituent-
context model. Pattern Recognition 38, 1407-1419.
Magerman, D. 1993. Expectation-Maximization for
Data-Oriented Parsing, IBM Technical Report,
Yorktown Heights, NY.
McClosky, D., E. Charniak and M. Johnson 2006.
Effective self-training for parsing. Proceedings HLT-
NAACL 2006, New York.
Manning, C. and H. Sch?tze 1999. Foundations of
Statistical Natural Language Processing. The MIT
Press.
Prescher, D., R. Scha, K. Sima'an and A. Zollmann
2004. On the statistical consistency of DOP
estimators. Proceedings CLIN 2004, Leiden.
Sch?tze, H. 1995. Distributional part-of-speech
tagging. Proceedings ACL 1995, Dublin.
Shao, J. 1999. Mathematical Statistics. Springer
Verlag, New York.
Sima'an, K. 1996. Computational complexity of
probabilistic disambiguation by means of tree
grammars. Proceedings COLING 1996, Copenhagen.
Skut, W., B. Krenn, T. Brants and H. Uszkoreit 1997.
An annotation scheme for free word order
languages. Proceedings ANLP 1997.
Xue, N., F. Chiou and M. Palmer 2002. Building a
large-scale annotated Chinese corpus. Proceedings
COLING 2002, Taipei.
van Zaanen, M. 2000. ABL: Alignment-Based
Learning. Proceedings COLING 2000, Saarbr?cken.
Zollmann, A. and K. Sima'an 2005. A consistent and
efficient estimator for data-oriented parsing. Journal
of Automata, Languages and Combinatorics, in press.
Zuidema, W. 2006. What are the productive units of
natural language grammar? A DOP approach to the
automatic identification of constructions.
Proceedings CONLL 2006, New York.
872
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 85?92, New York City, June 2006. c?2006 Association for Computational Linguistics
Unsupervised Parsing with U-DOP
Rens Bod
School of Computer Science
University of St Andrews
North Haugh, St Andrews
KY16 9SX Scotland, UK
rb@dcs.st-and.ac.uk
Abstract
We propose a generalization of the super-
vised DOP model to unsupervised learning.
This new model, which we call U-DOP,
initially assigns all possible unlabeled binary
trees to a set of sentences and next uses all
subtrees from (a large subset of) these binary
trees to compute the most probable parse
trees. We show how U-DOP can be
implemented by a PCFG-reduction tech-
nique and report competitive results on
English (WSJ), German (NEGRA) and
Chinese (CTB) data. To the best of our
knowledge, this is the first paper which
accurately bootstraps structure for Wall
Street Journal sentences up to 40 words
obtaining roughly the same accuracy as a
binarized supervised PCFG. We show that
previous approaches to unsupervised parsing
have shortcomings in that they either
constrain the lexical or the structural context,
or both.
1   Introduction
How can we learn syntactic structure from unlabeled
data in an unsupervised way? The importance of
unsupervised parsing is nowadays widely acknow-
ledged. While supervised parsers suffer from
shortage of hand-annotated data, unsupervised
parsers operate with unlabeled raw data, of which
unlimited quantities are available. During the last
few years there has been considerable progress in
unsupervised parsing. To give a brief overview: van
Zaanen (2000) achieved 39.2% unlabeled f-score on
ATIS word strings by a sentence-aligning technique
called ABL. Clark (2001) reports 42.0% unlabeled
f-score on the same data using distributional
clustering, and Klein and Manning (2002) obtain
51.2% unlabeled f-score on ATIS part-of-speech
strings using a constituent-context model called
CCM. Moreover, on Penn Wall Street Journal p-o-
s-strings ? 10 (WSJ10), Klein and Manning (2002)
report 71.1% unlabeled f-score. And the hybrid
approach of Klein and Manning (2004), which
combines a constituency and a dependency model,
leads to a further increase of 77.6% f-score.
Although there has thus been steady
progress in unsupervised parsing, all these
approaches have shortcomings in that they either
constrain the lexical or the structural context that is
taken into account, or both. For example, the CCM
model by Klein and Manning (2005) is said to
describe "all contiguous subsequences of a
sentence" (Klein and Manning 2005: 1410). While
this is a very rich lexical model, it is still limited in
that it neglects dependencies that are non-contiguous
such as between more  and than  in "BA carried
more people than cargo". Moreover, by using an
"all-substrings" approach, CCM risks to under-
represent structural context. Similar shortcomings
can be found in other unsupervised models.
In this paper we will try to directly model
structural as well as lexical context without
constraining any dependencies beforehand. An
approach that may seem apt in this respect is an all-
subtrees approach (e.g Bod 2003; Goodman 2003;
Collins and Duffy 2002). Subtrees can model both
contiguous and non-contiguous lexical dependencies
(see section 2) and they also model constituents in a
hierarchical context. Moreover, we can view the all-
subtrees approach as a generalization of Klein and
Manning's all-substrings approach and van Zaanen's
ABL model.
In the current paper, we will use the all-
subtrees approach as proposed in Data-Oriented
85
Parsing or DOP (Bod 1998). We will generalize the
supervised version of DOP to unsupervised parsing.
The key idea of our approach is to initially assign all
possible unlabeled binary trees to a set of given
sentences, and to next use counts of all subtrees
from (a large random subset of) these binary trees to
compute the most probable parse trees. To the best
of our knowledge, such a model has never been
tried out. We will refer to this unsupervised DOP
model as U-DOP , while the supervised DOP model
(which uses hand-annotated trees) will be referred to
as S-DOP. Moreover, we will continue to refer to
the general approach simply as DOP .
U-DOP is not just an engineering approach
to unsupervised learning but can also be motivated
from a cognitive perspective (Bod 2006): if we don't
have a clue which trees should be assigned to
sentences in the initial stages of language acquisit-
ion, we can just as well assume that initially all trees
are possible. Only those (sub)trees that partake in
computing the most probable parse trees for new
sentences are actually "learned". We have argued in
Bod (2006) that such an integration of unsupervised
and supervised methods results in an integrated
model for language learning and language use.
In the following we will first explain how
U-DOP works, and how it can be approximated by
a PCFG-reduction technique. Next, in section 3 we
discuss a number of experiments with U-DOP and
compare it to previous models on English (WSJ),
German (NEGRA) and Chinese (CTB) data. To the
best of our knowledge, this is the first paper which
bootstraps structure for WSJ sentences up to 40
words obtaining roughly the same accuracy as a
binarized supervised PCFG. This is remarkable
since unsupervised models are clearly at a
disavantage compared to supervised models which
can literally reuse manually annotated data.
2   Unsupervised data-oriented parsing
At a general level, U-DOP consists of the following
three steps:
1. Assign all possible binary trees to a set of
    sentences
2. Convert the binary trees into a PCFG-reduction
    of DOP
3. Compute the most probable parse tree for each
    sentence
Note that in unsupervised parsing we do not need to
split the data into a training and a test set. In this
paper, we will present results both on entire corpora
and on 90-10 splits of such corpora so as to make
our results comparable to a supervised  PCFG using
the treebank grammars of the same data ("S-
PCFG" ).
In the following we will first describe each
of the three steps given above where we initially
focus on inducing trees for p-o-s strings for the
WSJ10 (we will deal with other corpora and the
much larger WSJ40 in section 3). As shown by
Klein and Manning (2002, 2004), the extension to
inducing trees for words instead of p-o-s tags is
rather straightforward since there exist several
unsupervised part-of-speech taggers with high
accuracy, which can be combined with unsupervised
parsing (see e.g. Sch?tze 1996; Clark 2000).
Step 1: Assign all binary trees to p-o-s strings
from the WSJ10
The WSJ10 contains 7422 sentences ? 10 words
after removing empty elements and punctuation. We
assigned all possible binary trees to the
corresponding part-of-speech sequences of these
sentences, where each root node is labeled S and
each internal node is labeled X . As an example,
consider the p-o-s string NNS VBD JJ NNS, which
may correspond for instance to the sentence
Investors suffered heavy losses . This string has a
total of five binary trees shown in figure 1 -- where
for readability we add words as well.
NNS VBD JJ NNS
Investors suffered heavy losses
X
X
S
  
NNS VBD JJ NNS
Investors suffered heavy losses
X
X
S
NNS VBD JJ NNS
Investors suffered heavy losses
X
X
S
  
NNS VBD JJ NNS
Investors suffered heavy losses
X
X
S
NNS VBD JJ NNS
Investors suffered heavy losses
XX
S
Figure 1. All binary trees for NNS VBD JJ NNS
(Investors suffered heavy losses)
86
The total number of binary trees for a sentence of
length n  is given by the Catalan number Cn?1,
where Cn = (2n)!/((n+1)!n!). Thus while a sentence
of 4 words has 5 binary trees, a sentence of 8 words
has already 429 binary trees, and a sentence of 10
words has 4862 binary trees. Of course, we can
represent the set of binary trees of a string in
polynomial time and space by means of a chart,
resulting in a chart-like parse forest if we also
include pointers. But if we want to extract rules or
subtrees from these binary trees -- as in DOP -- we
need to unpack the parse forest. And since the total
number of binary trees that can be assigned to the
WSJ10 is almost 12 million, it is doubtful whether
we can apply the unrestricted U-DOP model to such
a corpus.
However, for longer sentences the binary
trees are highly redundant. In these larger trees, there
are many rules like X ? XX  which bear little
information. To make parsing with U-DOP possible
we therefore applied a simple heuristic which takes
random samples from the binary trees for sentences
? 7 words before they are fed to the DOP parser.
These samples were taken from the distribution of
all binary trees by randomly choosing nodes and
their expansions from the chart-like parse forests of
the sentences (which effectively favors trees with
more frequent subtrees). For sentences of 7 words
we randomly sample 60% of the trees, and for
sentences of 8, 9 and 10 words we sample
respectively 30%, 15% and 7.5% of the trees. In this
way, the set of remaining binary trees contains 8.23
* 105 trees, which we will refer to as the binary
tree-set. Although it can happen that the correct tree
is deleted for some sentence in the binary tree-set,
there is enough redundancy in the tree-set such that
either the correct binary tree can be generated by
other subtrees or that a remaining tree only
minimally differs from the correct tree. Of course,
we may expect better results if all binary trees are
kept, but this involves enormous computational
resources which will be postponed to future
research.
Step 2: Convert the trees into a PCFG-
reduction of DOP
The underlying idea of U-DOP is to take all subtrees
from the binary tree-set to compute the most
probable tree for each sentence. Subtrees from the
trees in figure 1 include for example the subtrees in
figure 2 (where we again added words for
readability). Note that U-DOP takes into account
both contiguous and non-contiguous substrings.
NNS VBD
Investors suffered
X
X
S
VBD
suffered
X
X
NNS NNS
Investors losses
X
X
S
JJ NNS
heavy losses
XX
S
JJ NNS
heavy losses
X
NNS VBD
Investors suffered
X
VBD JJ
suffered heavy
X
Figure 2. Some subtrees from the binary trees  for
NNS VBD JJ NNS given in figure 1
As in the supervised DOP approach (Bod 1998), U-
DOP parses a sentence by combining corpus-
subtrees from the binary tree-set by means of a
leftmost node substitution operation, indicated as ?.
The probability of a parse tree is computed by
summing up the probabilities of all derivations
producing it, while the probability of a derivation is
computed by multiplying the (smoothed) relative
frequencies of its subtrees. That is, the probability of
a subtree t  is taken as the number of occurrences of t
in the binary tree-set, | t |, divided by the total
number of occurrences of all subtrees t' with the
same root label as t. Let r(t) return the root label of t:
P(t)  =   | t |
? t': r( t')=r( t)   | t' |
The subtree probabilities are smoothed by applying
simple Good-Turing to the subtree distribution (see
Bod 1998: 85-87). The probability of a derivation
t1?...?tn  is computed by the product of the
probabilities of its subtrees t i:
P(t1?...?tn)  =  ?i P(ti)
Since there may be distinct derivations that generate
the same parse tree, the probability of a parse tree T
87
is the sum of the probabilities of its distinct
derivations. Let ti d  be the i-th subtree in the
derivation d that produces tree T , then the probability
of T is given by
P(T)  =  ?d?i P(tid)
As we will explain under step 3, the most probable
parse tree of a sentence is estimated by Viterbi n-
best summing up the probabilities of derivations that
generate the same tree.
It may be evident that had we only the
sentence Investors suffered heavy losses in our
corpus, there would be no difference in probability
between the five parse trees in figure 1, and U-DOP
would not be able to distinguish between the
different trees. However, if we have a different
sentence where JJ NNS (heavy losses)  appears in a
different context, e.g. in Heavy losses were
reported , its covering subtree gets a relatively higher
frequency and the parse tree where heavy losses
occurs as a constituent gets a higher total probability
than alternative parse trees. Of course, it is left to the
experimental evaluation whether non-constituents
("distituents") such as VBD JJ will be ruled out by
U-DOP (section 3).
An important feature of (U-)DOP is that it
considers counts of subtrees of a wide range of
sizes: everything from counts of single-level rules to
entire trees. A disadvantage of the approach is that
an extremely large number of subtrees (and
derivations) must be taken into account. Fortunately,
there exists a rather compact PCFG-reduction of
DOP which can also be used for U-DOP
(Goodman 2003). Here we will only give a short
summary of this PCFG-reduction. (Collins and
Duffy 2002 show how a tree kernel can be used for
an all-subtrees representation, which we will not
discuss here.)
Goodman's reduction method first assigns
every node in every tree a unique number which is
called its address. The notation A@k denotes the
node at address k where A is the nonterminal
labeling that node. A new nonterminal is created for
each node in the training data. This nonterminal is
called Ak. Let a j represent the number of subtrees
headed by the node A@j . Let a represent the number
of subtrees headed by nodes with nonterminal A,
that is a = ?ja j. Goodman then gives a small PCFG
with the following property: for every subtree in the
training corpus headed by A, the grammar will
generate an isomorphic subderivation with
probability 1/a. For a node A@j(B@k, C@l) , the
following eight PCFG rules in figure 3 are
generated, where the number in parentheses
following a rule is its probability.
Aj ? BC        (1/aj)    A ? BC         (1/a)
Aj ? BkC      (bk/aj)    A ? BkC       (bk/a)
Aj ? BCl       (c l/aj)    A ? BC l         (cl/a)
Aj ? BkCl     (bkc l/aj)    A ? BkCl       (bkcl/a)
Figure 3. PCFG-reduction of DOP
In this PCFG reduction, bk represents the number of
subtrees headed by the node B@k, and cl refers to
the number of subtrees headed by the node C@l.
Goodman shows by simple induction that his
construction produces PCFG derivations
isomorphic to (U-)DOP derivations with equal
probability (Goodman 2003: 130-133). This means
that summing up over derivations of a tree in DOP
yields the same probability as summing over all the
isomorphic derivations in the PCFG.1
The PCFG-reduction for U-DOP is slightly
simpler than in figure 3 since the only labels are S
and X , and the part-of-speech tags. For the tree-set
of 8.23 * 105 binary trees generated under step 1,
Goodman's reduction method results in a total
number of 14.8 * 106 distinct PCFG rules. While it
is still feasible to parse with a rule-set of this size, it
is evident that our approach can deal with longer
sentences only if we further reduce the size of our
binary tree-set.
It should be kept in mind that while the
probabilities of all parse trees generated by DOP
sum up to 1, these probabilities do not converge to
the "true" probabilities if the corpus grows to
infinity (Johnson 2002). In fact, in Bod et al (2003)
we showed that the most probable parse tree as
defined above has a tendency to be constructed by
the shortest derivation (consisting of the fewest and
thus largest subtrees). A large subtree is overruled
only if the combined relative frequencies of smaller
subtrees yields a larger score. We refer to Zollmann
and Sima'an (2005) for a recently proposed
estimator that is statistically consistent (though it is
not yet known how this estimator performs on the
WSJ) and to Zuidema (2006) for a theoretical
comparison of existing estimators for DOP.
1
 As in Bod (2003) and Goodman (2003: 136), we
additionally use a correction factor to redress DOP's
bias discussed in Johnson (2002).
88
Step 3: Compute the most probable parse tree
for each WSJ10 string
While Goodman's reduction method allows for
efficiently computing the most probable derivation
for each sentence (i.e. the Viterbi parse), it does not
allow for an efficient computation of (U-)DOP's
most probable parse tree since there may be
exponentially many derivations for each tree whose
probabilities have to be summed up. In fact, the
problem of computing the most probable tree in
DOP is known to be NP hard (Sima'an 1996). Yet,
the PCFG reduction in figure 4 can be used to
estimate  DOP's most probable parse tree by a
Viterbi n-best search in combination with a CKY
parser which computes the n  most likely derivations
and next sums up the probabilities of the derivations
producing the same tree. (We can considerably
improve efficiency by using k-best hypergraph
parsing as recently proposed by Huang and Chiang
2005, but this will be left to future research).
In this paper, we estimate the most probable
parse tree from the 100 most probable derivations
(at least for the relatively small WSJ10). Although
such a heuristic does not guarantee that the most
probable parse is actually found, it is shown in Bod
(2000) to perform at least as well as the estimation
of the most probable parse with Monte Carlo
techniques. However, in computing the 100 most
probable derivations by means of Viterbi it is
prohibitive to keep track of all subderivations at each
edge in the chart. We therefore use a pruning
technique which deletes any item with a probability
less than 10?5 times of that of the best item from
the chart.
To make our parse results comparable to
those of Klein and Manning (2002, 2004, 2005), we
will use exactly the same evaluation metrics for
unlabeled precision (UP) and unlabeled recall (UR),
defined in Klein (2005: 21-22). Klein's definitions
slightly differ from the standard PARSEVAL
metrics: multiplicity of brackets is ignored, brackets
of span one are ignored and the bracket labels are
ignored. The two metrics of UP and UR are
combined by the unlabled f-score F1 which is
defined as the harmonic mean of UP and UR: F1 =
2*UP*UR/(UP+UR). It should be kept in mind that
these evaluation metrics were clearly inspired by the
evaluation of supervised  parsing which aims at
mimicking given  tree annotations as closely as
possible. Unsupervised parsing is different in this
respect and it is questionable whether an evaluation
on a pre-annotated corpus such as the WSJ is the
most appropriate one. For a subtle discussion on
this issue, see Clark (2001) or Klein (2005).
3   Experiments
3.1 Comparing U-DOP to previous work
Using the method described above, our parsing
experiment with all p-o-s strings from the WSJ10
results in an f-score of 78.5%. We next tested U-
DOP on two additional domains from Chinese and
German which were also used in Klein and
Manning (2002, 2004): the Chinese treebank (Xue
et al 2002) and the NEGRA corpus (Skut et al
1997). The CTB10 is the subset of p-o-s strings
from the Penn Chinese treebank containing 10
words or less after removal of punctuation (2437
strings). The NEGRA10 is the subset of p-o-s
strings of the same length from the NEGRA corpus
using the supplied converson into Penn treebank
format (2175 strings). Table 1 shows the results of
U-DOP in terms of UP, UR and F1 compared to
the results of the CCM model by Klein and
Manning (2002), the DMV dependency learning
model by Klein and Manning (2004) together with
their combined model DMV+CCM.
Model English German Chinese(WSJ10) (NEGRA10) (CTB10)
UP UR F1 UP UR F1 UP UR F1
CCM 64.2 81.6 71.9 48.1 85.5 61.6 34.6 64.3 45.0
DMV 46.6 59.2 52.1 38.4 69.5 49.5 35.9 66.7 46.7
DMV+CCM 69.3 88.0 77.6 49.6 89.7 63.9 33.3 62.0 43.3
U-DOP 70.8 88.2 78.5 51.2 90.5 65.4 36.3 64.9 46.6
Table 1. Results of U-DOP compared to previous
models on the same data
Table 1 indicates that our model scores slightly
better than Klein and Manning's combined
DMV+CCM model, although the differences are
small (note that for Chinese the single DMV model
scores better than the combined model and slightly
better than U-DOP). But where Klein and
Manning's combined model is based on both a
constituency and a dependency model, U-DOP is,
like CCM, only based on a notion of constituency.
Compared to CCM alone, the all-subtrees approach
employed by U-DOP shows a clear improvement
(except perhaps for Chinese). It thus seems to pay
off to use all subtrees rather than just all
(contiguous) substrings in bootstrapping
89
constituency. It would be interesting to investigate
an extension of U-DOP towards dependency
parsing, which we will leave for future research. It is
also noteworthy that U-DOP does not employ a
separate class for non-constituents, so-called
distituents, while CCM does. Thus good results can
be obtained without keeping track of distituents but
by simply assigning all binary trees to the strings
and letting the DOP model decide which substrings
are most likely to form constituents.
To give an idea of the constituents learned
by U-DOP for the WSJ10, table 2 shows the 10
most frequently constituents in the trees induced by
U-DOP together with the 10 actually most
frequently occurring constituents in the WSJ10 and
the 10 most frequently occurring part-of-speech
sequences (bigrams) in the WSJ10.
Rank Most frequent Most Frequent Most frequent
U-DOP constituents WSJ10 constituents WSJ10 substrings
1 DT NN DT NN NNP NNP
2 NNP NNP NNP NNP DT NN
3 DT JJ NN CD CD JJ NN
4 IN DT NN JJ NNS IN DT
5 CD CD DT JJ NN NN IN
6 DT NNS DT NNS DT JJ
7 JJ NNS JJ NN JJ NNS
8 JJ NN CD NN NN NN
9 VBN IN IN NN CD CD
10 VBD NNS IN DT NN NN VBZ
Table 2. Most frequently learned constituents by
U-DOP together with most frequently occurring
constituents and p-o-s sequences (for WSJ10)
Note that there are no distituents among U-DOP's
10 most frequently learned constituents, whilst the
third column shows that distituents such as IN DT
or DT JJ occur very frequently as substrings in the
WSJ10. This may be explained by the fact that (the
constituent) DT NN occurs more frequently as a
substring in the WSJ10 than (the distituent) IN DT,
and therefore U-DOP's probability model will favor
a covering subtree for IN DT NN which consists of
a division into IN X and DT NN rather than into IN
DT and X NN, other things being equal. The same
kind reasoning can be made for a subtree for DT JJ
NN where the constituent JJ NN occurs more
frequently as a substring than the distituent DT JJ.
Of course the situation is somewhat more complex
in DOP's sum-of-products model, but our argument
may illustrate why distituents like IN DT or DT JJ
are not proposed among the most frequent
constituents by U-DOP while larger constituents
like IN DT NN and DT JJ NN are in fact proposed.
3.2 Testing U-DOP on held-out sets and longer
sentences (up to 40 words)
We were also interested in U-DOP's performance
on a held-out test set such that we could compare the
model with a supervised PCFG treebank grammar
trained and tested on the same data (S-PCFG). We
started by testing U-DOP on 10 different 90%/10%
splits of the WSJ10, where 90% was used for
inducing the trees, and 10% to parse new sentences
by subtrees from the binary trees from the training
set (or actually a PCFG-reduction thereof). The
supervised PCFG was right-binarized as in Klein
and Manning (2005). The following table shows the
results.
Model UP UR F1
U-DOP 70.6 88.1 78.3
S-PCFG 84.0 79.8 81.8
Table 3. Average f-scores of U-DOP compared to a
supervised PCFG (S-PCFG) on 10 different 90-10
splits of the WSJ10
Comparing table 1 with table 3, we see that on 10
held-out WSJ10 test sets U-DOP performs with an
average f-score of 78.3% (SD=2.1%) only slightly
worse than when using the entire WSJ10 corpus
(78.5%). Next, note that U-DOP's results come near
to the average performance of a binarized supervised
PCFG which achieves 81.8% unlabeled f-score
(SD=1.8%). U-DOP's unlabeled recall is even
higher than that of the supervised PCFG. Moreover,
according to paired t-testing, the differences in f-
scores were not  statistically significant. (If the
PCFG was not post-binarized, its average f-score
was 89.0%.)
As a final test case for this paper, we were
interested in evaluating U-DOP on WSJ sentences ?
40 words, i.e. the WSJ40, which is with almost
50,000 sentences a much more challenging test case
than the relatively small WSJ10. The main problem
for U-DOP is the astronomically large number of
possible binary trees for longer sentences, which
therefore need to be even more heavily pruned than
before.
We used a similar sampling heuristic as in
section 2. We started by taking 100% of the trees for
sentences ? 7 words. Next, for longer sentences we
reduced this percentage with the relative increase of
the Catalan number. This effectively means that we
randomly selected the same number of trees for
each sentence ? 8 words, which is 132 (i.e. the
90
number of possible binary trees for a 7-word
sentence). As mentioned in section 2, our sampling
approach favors more frequent trees, and trees with
more frequent subtrees. The binary tree-set obtained
in this way for the WSJ40 consists of 5.11 * 106
different trees. This resulted in a total of 88+ million
distinct PCFG rules according to the reduction
technique in section 2. As this is the largest PCFG
we have ever attempted to parse with, it was
prohibitive to estimate the most probable parse tree
from 100 most probable derivations using Viterbi n-
best. Instead, we used a beam of only 15 most
probable derivations, and selected the most probable
parse from these. (The number 15 is admittedly ad
hoc, and was inspired by the performance of the so-
called SL-DOP model in Bod 2002, 2003). The
following table shows the results of U-DOP on the
WSJ40 using 10 different 90-10 splits, compared to
a  supervised binarized PCFG (S-PCFG) and a
supervised binarized DOP model (S-DOP) on the
same data.
Model F1
U-DOP 64.2
S-PCFG 64.7
S-DOP 81.9
Table 4. Performance of U-DOP on WSJ40
using10 different 90-10 splits, compared to a
binarized  S-PCFG and a binarized S-DOP model.
Table 4 shows that U-DOP obtains about the same
results as a binarized supervised PCFG on WSJ
sentences ? 40 words. Moreover, the differences
between U-DOP and S-PCFG were not statistically
significant. This result is important as it shows that
it is possible to parse the rather challinging WSJ in a
completely unsupervised  way obtaining roughly the
same accuracy as a supervised PCFG. This seems
to be in contrast with the CCM model which quickly
degrades if sentence length is increased (see Klein
2005). As Klein (2005: 97) notes, CCM's strength
is finding common short constituent chunks. U-
DOP on the other hand has a preference for large
(even largest possible) constituent chunks. Klein
(2005: 97) reports that the combination of CCM and
DMV seems to be more stable with increasing
sentence length. It would be extremely interesting to
see how DMV+CCM performs on the WSJ40.
It should be kept in mind that simple
treebank PCFGs do not constitute state-of-the-art
supervised parsers. Table 4 indicates that U-DOP's
performance remains still far behind that of S-DOP
(and indeed of other state-of-the-art supervised
parsers such as Bod 2003 or Charniak and Johnson
2005). Moreover, if S-DOP is not post-binarized, its
average f-score on the WSJ40 is 90.1% -- and there
are some hybrid DOP models that obtain even
higher scores (see Bod 2003). Our long-term goal is
to try to outperform S-DOP by U-DOP. An
important advantage of U-DOP is of course that it
only needs unannotated data of which unlimited
quanitities are available. Thus it would be interesting
to test how U-DOP performs if trained on e.g. 100
times more data. Yet, as long as we compute our f-
scores on hand-annotated data like Penn's WSJ, the
S-DOP model is clearly at an advantage. We
therefore plan to compare U-DOP and S-DOP (and
other supervised parsers) in a concrete application
such as phrase-based machine translation or as a
language model for speech recognition.
4   Conclusions
We have shown that the general DOP approach can
be generalized to unsupervised learning, effectively
leading to a single model for both supervised and
unsupervised parsing. Our new model, U-DOP,
uses all subtrees from (in principle) all binary trees
of a set of sentences to compute the most probable
parse trees for (new) sentences. Although heavy
pruning of trees is necessary to make our approach
feasible in practice, we obtained competitive results
on English, German and Chinese data. Our parsing
results are similar to the performance of a binarized
supervised PCFG on the WSJ ? 40 sentences. This
triggers the provocative question as to whether it is
possible to beat supervised parsing by unsupervised
parsing. To cope with the problem of evaluation, we
propose to test U-DOP in specific applications
rather than on hand-annotated data.
References
Bod, R. 1998. Beyond Grammar: An Experience-
Based Theory of Language, Stanford: CSLI
Publications (Lecture notes number 88),
distributed by Cambridge University Press.
Bod, R. 2000. An improved parser for data-oriented
lexical-functional analysis.  Proceedings
ACL'2000 , Hong Kong.
Bod, R. 2002. A unified model of structural
organization in language and music. Journal of
91
Artificial Intelligence Research 17(2002), 289-
308.
Bod, R., R. Scha and K. Sima'an (eds.) 2003. Data-
Oriented Parsing . CSLI Publications/University
of Chicago Press.
Bod, R. 2003. An efficient implementation of a new
DOP model.  Proceedings EACL'2003,
Budapest.
Bod, R. 2006. Exemplar-based syntax: How to get
productivity from examples? The Linguistic
Review 23(3), Special Isssue on Exemplar-
Based Models in Linguistics.
Charniak, E. and M. Johnson 2005. Coarse-to-fine
n-best parsing and Max-Ent discriminative
reranking. Proceedings ACL'2005 , Ann-Arbor.
Clark, A. 2000. Inducing syntactic categories by
context distribution clustering. Proceedings
CONLL'2000.
Clark, A. 2001. Unsupervised induction of
stochastic context-free grammars using
distributional clustering. Proceed ings
CONLL'2001 .
Collins, M. and N. Duffy 2002. New ranking
algorithms for parsing and tagging: kernels over
discrete structures, and the voted perceptron.
Proceedings ACL'2002 , Philadelphia.
Goodman, J. 2003. Efficient algorithms for the
DOP model. In R. Bod, R. Scha and K. Sima'an
(eds.). Data-Oriented Parsing , The University
of Chicago Press.
Huang, L. and Chiang D. 2005. Better k-best
parsing. Proceedings IWPT'2005, Vancouver.
Johnson, M. 2002. The DOP estimation method is
biased and inconsistent. Computational
Linguistics  28, 71-76.
Klein, D. 2005. The Unsupervised Learning of
Natural Language Structure. PhD thesis,
Stanford University.
Klein, D. and C. Manning 2002. A general
constituent-context model for improved
grammar induction. Proceedings ACL'2002 ,
Philadelphia.
Klein, D. and C. Manning 2004. Corpus-based
induction of syntactic structure: models of
dependency and constituency. Proceedings
ACL'2004 , Barcelona.
Klein, D. and C. Manning 2005. Natural language
grammar induction with a generative constituent-
context model. Pattern Recognition  38, 1407-
1419.
Sch?tze, H. 1995. Distributional part-of-speech
tagging. Proceedings ACL'1995, Dublin.
Sima'an, K. 1996. Computational complexity of
probabilistic disambiguation by means of tree
grammars. Proceedings COLING'1996,
Copenhagen.
Skut, W., B. Krenn, T. Brants and H. Uszkoreit
1997. An annotation scheme for free word order
languages. Proceedings ANLP'97.
Xue, N., F. Chiou and M. Palmer 2002. Building a
large-scale annotated Chinese corpus.
Proceedings COLING 2002 , Taipei.
van Zaanen, M. 2000. ABL: Alignment-Based
Learning. Proceedings COLING'2000 ,
Saarbr?cken.
Zollmann, A. and K. Sima'an 2005. A consistent
and efficient estimator for data-oriented parsing.
Journal of Automata, Languages and
Combinatorics, in press.
Zuidema, W. 2006. Theoretical evaluation of
estimation methods for data-oriented parsing.
Proceedings EACL'2006, Trento.
92
19
20
21
22
23
24
25
26
Parsing with the Shortest Derivation 
Rens Bod 
htformatics P.cseareh Institute, University of Leeds, Leeds LS2 9JT, & 
Institute for Logic, Language and Computation, University of Amsterdam 
tens @ scs.lecd s.ac.uk 
Abstract 
Common wisdom has it that tile bias of stochastic 
grammars in favor of shorter deriwttions of a sentence 
is hamfful and should be redressed. We show that the 
common wisdom is wrong for stochastic grammars 
that use elementary trees instead o1' conlext-l 'ree 
rules, such as Stochastic Tree-Substitution Grammars 
used by Data-Oriented Parsing models. For such 
grammars a non-probabi l ist ic metric based on tile 
shortest derivation outperforms a probabilistic metric 
on the ATIS and OVIS corpora, while it obtains 
competitive results on the Wall Street Journal (WSJ) 
corpus. This paper also contains the first publislmd 
experiments with DOP on the WSJ. 
1. Introduction 
A well-known property of stochastic grammars is their 
prope,lsity to assign highe, probabil it ies to shorter 
derivations o1' a sentence (cf. Chitrao & Grishman 
1990; Magerman & Marcus 1991; Briscoe & Carroll 
1993; Charniak 1996). This propensity is due to the 
probabil ity o1' a derivation being computed as tile 
product of the rule probabil it ies, and thus shorter 
derivations involving fewer rules tend to have higher 
probabilities, ahnost regardless of the training data. 
While this bias may seem interesting in the light of 
the principle of cognitive economy, shorter derivat- 
ions generate smaller parse h'ees (consisting of fewer 
nodes) whiclt are not warranted by tile correct parses 
of sentences. Most systems therefore redress this bias, 
for instance by normalizillg the derivation probability 
(see Caraballo & Charniak 1998). 
However, for stochastic grammars lhat use 
elementary trees instead o1' context-l'ree rules, the 
propensity to assign higher probabil it ies to shorter 
derivations does not necessarily lead to a bias in 
favor of smaller parse trees, because lementary trees 
may differ in size and lexicalization. For Stochastic 
Tree-Substitution Grammars (STSG) used by Data- 
Oriented Parsing (DOP) models, it has been observed 
lhat the shortest derivation of a sentence consists of 
the largest subtrees een in a treebank thai generate 
that sentence (of. Bed 1992, 98). We may therefore 
wonder whether for STSG lhe bias in favor of shorter 
derivations is perhaps beneficial rather than llarmful. 
To investigate this question we created a new 
STSG-DOP model which uses this bias as a feature. 
This non-probabi l i s t i c  DOP model parses each 
sentence by returning its shortest der ivat ion 
(consisting of tile fewest subtrees een in ttle corpus). 
Only if there is more than one shortest derivation the 
model backs off to a frequency ordering o1' the corpus- 
subtrees and chooses the shortest deriwttion with most 
highest ranked subtrees. We compared this non- 
probabil ist ic DOP model against tile probabil ist ic 
DOP model (which estimales the most probable parse 
for each sentence) on three different domains: tbe 
Penn ATIS treebank (Marcus et al 1993), the Dutch 
OVIS treebank (Bonnema el al. 1997) and tile Penn 
Wall Street Journal (WSJ) treebank (Marcus el al. 
1993). Stwprisingly, the non-probabilistic DOP model 
oult~erforms the probabilistic I)OP model on both lhe 
ATIS and OVIS treebanks, while it obtains competit- 
ive resuhs on the WSJ trcebank. We conjectu,c thai 
any stochastic granlnlar which uses units of flexible 
size can be turned into an accurate non-probabilistic 
version. 
Tile rest of this paper is organized as follows: 
we first explain botll the probabil ist ic and non- 
prolmbil istic DOP model. Next, we go into tile 
computational aspccls of these models, and finally 
we compare lhe performance of the models on the 
three treebanks. 
2. Probabilistic vs. Non-Probabilistic 
Data-Oriented Parsing 
Both probabil istic and non-probabil ist ic DOP are 
based on the DOP model in Bod (1992) which 
extracts a Stochastic Tree-Substitution Grammar fi'om 
a treebank ("STSG-DOP"). I STSG-DOP uses subtrees 
J Note that the l)OP-approach of extracting rammars f,om 
corpora has been applied to a wide variety of other 
grammatical fimncworks, including Tree-lnsertio,~ Grmnmar 
69 
from parse trees in a corpus as elementary trees, and 
leftmost-substitution t  combine subtrees into new 
trees. As at\] example, consider a very simple corpus 
consisting of only two trees (we leave out some 
subcategorizations to keel) the example simt}le): 
S s / x  /X  
NP VP \] ~ NP VP 
she V NP \] 
\] ~ she VI' 1'1' 
....... ted A A 
NP PP V NP P NP /N /N  IA  
saw the dog wilh Ihe telescope 
tile dress i } A 
{}I1 Ihe rack 
Figure 1. A simple corpus o1' two trees. 
A new sentence such as She saw the dress with the 
te lescope can be parsed by combining subtrees from 
this corpus by means of lel 'tmost-substitution 
(indicated as ?): 
S o X o = ,S / x  & /x  
P 
i" v,. ..... IA  i P /x.. 
A with the telescope 
she VP PP s e VI' 
A A 7 ,  
SaW SaW I\]IC dl'CSs whh the telescope 
Figure 2. Derivation and parse tree for the sentence She saw 
lhe dress with the telescope 
Note that other derivations, involving different 
subtrees, may yield the same parse tree; for instance: 
S o NP 
NP VP the dress 
sl!e VP PP 
A A 
V NP P 
i IA  
saw with file telescope 
S 
NP VP 
VP PP 
A A 
V NP P NP 
saw the dB:ss with the telescope 
Figure 3. l)ifferent derivation yielding the same parse true 
lbr She saw tile &'ess with tile telescope 
Note also that, given this example corpus, the 
sentence we considered is ambiguous; by combining 
(Hoogweg 2000), Tree-Adjoining Grammar (Neumalm 
1998), Lexical-Functional Grammar (Bed & Kaplan 1998; 
Way 1999; Bed 2000a), Head-driven Phrase Structure 
Grammar (Neumann & Flickinger 1999), and Montague 
Grammar (van den Berg et al 1994; Bed 1998). For the 
relation between DOP and Memory-Based Learning, see 
Daelemans (1999). 
other subtrees, a dilTerent parse may be derived, 
which is analogous to the first rather than the second 
corpus  Sel l lOl lCe:  
S o V o pp S 
A I A A NP VP NP VP saw I} NP 
IA  i /X  IA  
she V NP with Ih? telescol}e she V NP 
NP PP NP PP 
P NP 
Ihe dless the dress I A 
with tile telese:}pe 
Figure 4. Different derivation yielding a different parse tree 
lbr Site saw the dress with the telescope 
The probabilistic and non-probabilistie DOP models 
differ in the way they define the best parse tree of a 
sentence. We now discuss these models separately. 
2.1 The probabil ist ic DOP model 
The probabilistic DOP model introduced in Bed 
(1992, 93) computes the most probable parse tree of a 
sentence from the normalized subtree l'requencies in 
the corpus. The probability of a subtree t is estimated 
as the nunlber of occurrences of t seen in the corpus, 
divided by the total number of occurrences of corpus- 
subtrees that have the same root label as t. Let It I 
rett, rn the number of occurrences of t in the corpus 
and let r ( t )  return the root label of t then: 
l'(t)=ltl/Zt':r(r)=,.(t)lt' \[.2 Tim probability of a 
derivation is computed as the product of the 
probabilities of the subtrees involved in it. The 
probability of a parse tree is computed as the sum of 
the probabil it ies ol' all distinct derivations that 
produce that tree. The parse tree with the highest 
2 It should be stressed that there may be several other ways 
to estimate subtree probabilities in I)OP. For example, 
Bonnema et al (1999) estimate the probability era subtree 
as the probability that it has been involved in the derivation 
of a corpus tree. It is not yet known whether this alternative 
probability model outperforms the model in Bed (1993). 
Johnson (1998) pointed out that the subtree estimator in 
Bed (1993) yields a statistically inconsistent model. This 
means that as the training corpus increases the 
corresponding sequences of probability distributions do not 
converge to the true distribution that generated the training 
data. Experiments with a consistent maximum likelihood 
estimator (based on the inside-outside algorithln in Lari and 
Yot, ng 1990), leads however to a significant decrease in 
parse accuracy on the ATIS and OVIS corpora. This 
indicates that statistically consistency does not necessarily 
lead to better performance. 
70 
probalfil ity is defined as the best parse tree of a 
Sel ltence. 
The fn'obabilistie DOP model thus considers 
counts of subtrees of a wide range o1' sizes in 
computing the probability of a tree: everything from 
counts ?51' single-level rules to cotmts of entire trees. 
2.2 The noi l -probabi l ist ic I )OP model 
Tim non-prolmlfi l istic I)OP model uses a rather 
different definition of the best parse tree. instead of 
conqmting the most probable patse of a sentence, it 
computes the parse tree which can be generated by 
the fewest eorpus-stibtrees, i.e., by the shortest 
deriwltion independent of the subtree prolmbilities. 
Since stlblrees are allowed to be of arbitrary size, tile 
shortest derivation typically corresponds 1(5 the pa.rse 
tree which consists of largest possible corpus- 
subtrees, thus maximizing syntaclic context. For 
cxmnple, given the corpus in Figure 1, the best parse 
tree for She saw the dress with the telescope is given 
in Figure 3, since that parse tree can be generated by 
a derivation of only two corpus-sublrees, while tile 
parse tree in Figure 4 needs at least three corpus- 
sublrees to be generated. (Interestingly, the parse lree 
with the sho,'test derivation in Figure 3 is also tile 
most probable parse tree according to pl'obalfilistic 
1)O1 ) for this corpus, but this need not always be so. 
As mentioned, the probabil ist ic 1)O1' lnodel has 
ah'eady a bias l(5 assign higher probabilities to parse 
trees that can be generaled 153, shorter deriwtlions. The 
non-pvobabi l ist ic  I)OP model makes this bias 
absolute.) 
The shortest deriwttion may not t)e unique: it 
may happen that different parses of a sentence are 
generated by tile same mininml nmnl)er of corpus- 
subtrees. In lhat ease the model backs off to a 
l'requency ordering (51' the subtrees. That is, all 
subtrees of each root label arc assigned a rank 
according to their frequency ill the co,pus: the most 
frequent sublree (or subtrees) (51" each root label get 
rank 1, the second most frequent subtree gels rank 2, 
etc. Next, the rank of each (shortest) derivation is 
computed its the sum of the ranks (51" tile subtrecs 
involved. The deriwttion with the smallest sum, or 
highest rank, is taken as the best derivation producing 
the best parse tree. 
The way we compute the rank of a de,'ivalion 
by surmrdng up lhe ranks of its subtrees may seem 
rather ad hoc. However, it is possible to provide an 
information-theoretical ,notivation for this model. 
According to Zipl"s law, rank is roughly prolxsrtional 
to tile negative logaritlun of frequency (Zipf 1935). In 
Slmnnon's Information Theory (Shannon 194~,), tile 
negative logaritlun (of base 2) of the probability of an 
event is belter known as the information (51' that event. 
Thus, tile rank of a subtree is roughly proportional to 
its information. It follows that minimizing the sum of 
the sublrce ranks in a derivation corresponds to 
minimizing the (self-)information of a derivation. 
3. Computational Aspects 
3.1 Computing the most probable  parse 
Bed (1993) showed how standard chart parsing 
techniques can be applied to probabilistic DOP. Each 
corpus-subtree t is converted into a context-free rule r 
where the lefthand side <51" r corresponds to tile root 
label of t and tile righthand side of r corresponds to 
the fronlier labels of t. Indices link the rules to the 
original subtrees so as to maintain the sublree's 
internal structure and probability. These rules are used 
lO Cl'e~:lte il. der ivat ion  forest for a sentenc/2, illld the 
most p,obable parse is computed by sampling a 
sufficiently large number of random deriwttions from 
the forest ("Monte Carlo disamt~iguation", see Bed 
1998; Chappel ier  & Rajman 2000). Whi le this 
technique has been sttccessfully applied to parsing 
lhe ATIS portion in the Penn Treebank (Marcus et al 
1993), it is extremely time consuming. This is mainly 
because lhe nun/bcr of random derivations thai should 
be sampled to reliably estimate tile most prolmble 
parse increases exponential ly with the sentence 
length (see Goodman 1998). It is therefore question- 
able whether Bed's slunpling teclmique can be scaled 
to larger corpora such as tile OVIS and the WSJ 
corpora. 
Goodman (199g) showed how tile probabil- 
istic I)OP model can be reduced to a compact 
stochastic context-free grammar (SCFG) which 
contains exactly eight SCFG rules for each node in 
the training set trues. Although Goodman's rcductkm 
method does still not al low for an eff ic ient 
computation {51 tile most probable parse in DOP (ill 
fact, the prol~lem of computing the most prolmble 
parse is NP-hard -- sue Sima'an 1996), his method 
does al low for an eff icient computation o1' the 
"nmximun~ constituents parse", i.e., the parse tree that 
is most likely to have the largest number of correct 
constitueuts (also called the "labeled recall parse"). 
Goodman has shown on tile ATIS corpus that the 
nla.xinltllll constituents parse perfor,ns at least as well 
as lhe most probable parse if all subtl'ees are used. 
Unfortunately, Goodman's reduction method remains 
71 
beneficial only if indeed all treebank subtrces arc 
used (see Sima'an 1999: 108), while maximum parse 
accuracy is typical ly obtained with a snbtree set 
which is smalle," than the total set of subtrees (this is 
probably due to data-sparseness effects -- see 
Bonnema et al 1997; Bod 1998; Sima'an 1999). 
In this paper we will use Bod's subtree-to-rule 
conversion method for studying the behavior of 
probabi l ist ic  against non-probabi l ist ic  DOP for 
different maximtnn subtree sizes. However, we will 
not use Bod's Monte Carlo sampling technique from 
complete derivation forests, as this turns out to be 
computationally impractical for our larger corpora. 
Instead, we use a Viterbi n-best search and estimate 
the most probable parse fi'mn the 1,000 most probable 
deriwltions, summing up tile probabilities hi' derivat- 
ions that generate the same tree. Tile algorithm for 
computing n most probable deriwttions fol lows 
straightforwardly from the algorithm which computes 
the most probable derivation by means of Viterbi 
optimization (see Sima'an 1995, 1999). 
3.2 Comput ing the shortest der ivat ion 
As with the probabilistic DOP model, we first convert 
the corpus-subtrees into rewrite rules. Next, the 
shortest derivation can be computed in the same way 
as the most probable deriwltion (by Viterbi) if we 
give all rules equal probabilities, in which case tile 
shortest derivation is equal to the most probable 
deriwltion. This can be seen as follows: if each rule 
has a probability p then the probability of a derivation 
involving n rules is equal to pn, and since 0<p<l the 
derivation with the fewest rules has the greatest 
probability. In out" experiments, we gave each rule a 
probability mass equal to I/R, where R is the ntunbcr 
of distinct rules derived by Bod's method. 
As mentioned above, the shortest derivation 
may not be unique. In that case we compute all 
shortest derivations of a sentence and then apply out" 
ranking scheme to these derivations. Note that this 
ranking scheme does distinguish between snbtrees or 
different root labels, as it ranks the subtrecs given 
their root label. The ranks of the shortest derivations 
are computed by summing up the ranks of the 
subtrees they involve. The shortest derivation with the 
smallest stun o1' subtree ranks is taken to produce tile 
best parse tree. 3 
3 It may happcn that different shortest derivations generate 
the same tree. We will not distinguish between these cases, 
however, and co,npt, te only the shortest derivation with the 
highest rank. 
4. Experimental Comparison 
4.1 Experiments on the ATIS corpus 
For our first comparison, we used I0 splits from the 
Penn ATIS corpus (Marcus et al 1993) into training 
sets of 675 sentences and test sets of 75 sentences. 
These splits were random except for one constraint: 
tbat all words in the test set actually occurred in the 
training set. As in Bod (1998), we el iminated all 
epsilon productions and all "pseudo-attachments". As 
accuracy metric we used the exact match defined as 
the percentage of the best parse trees that are 
identical to the test set parses. Since the Penn ATIS 
portion is relatively small, we were able to compute 
the most probable parse both by means of Monte 
Carlo sampling and by means of Viterbi n-best. Table 
1 shows the means o1' tile exact match accuracies for 
increasing maximum subtrec depths (up to depth 6). 
Depth of Probabilistic DOP Non-probabilistic 
subtrees Monte Carlo Viterbi n-best l)OP 
l 46.7 46.7 24.8 
<2 67.5 67.5 40.3 
__.<3 78. l 78.2 57.1 
__<4 83.6 83.0 81.5 
-<5 83.9 83.4 83.6 
-<6 84. I 84.0 85.6 
Tablc 1. Exact match accuracies/'or the ATIS corpus 
Tile table shows that tile two methods for probabilistic 
DOP score roughly tile same: at dcpfll _< 6, the Monte 
Carlo method obtains 84.1% while the Viterbi n-best 
method obtains 84.0%. These differences are not 
statistically significant. The table also shows that for 
small subtree depths the non-probabilistic DOP model 
performs considerably worse than the probabil istic 
model. This may not be surprising since for small 
subtrecs tile shortest derivation corresponds to tile 
smallest parse tree which is known to be a bad 
prediction of the correct parse tree. Only il' the 
subtrees are larger than depth 4, the non-probabilistic 
DOP model  scores roughly the same as its 
probabil istic ounterpart. At subtree depth < 6, the 
non-probabilistic DOP model scores 1.5% better than 
the best score of the probabilistic DOP model, which 
is statistically significant according to paired t-tests. 
4.2 Experiments on tile OVIS corpus 
For out" comparison on tile OVIS corpus (Bonnema ct 
al. 1997; Bod 1998) we again used 10 random splits 
under tile condition that all words in tile test set 
occurred in the training set (9000 sentences for 
72 
training, 1000 sentences for testing). The ()VIS trees 
contain both syntactic and se,nantic annotations, but 
no epsilon productions. As in Bod (1998), we lreated 
the syntactic and semantic annotations of each node 
as one label. Consequently, the labels are very 
restrictive and col lecting statistics over them is 
difficult. Bonncma et al (1997) and Sima'an (1999) 
report that (probal)ilislic) I)OP sulTers considerably 
from data-sparseness on OVIS, yielding a decrease in 
parse accuracy if subtrees larger lh'an depth 4 are 
included. Thus it is interesting to investigate how non- 
probabil istic DOP behaves on this corpus. Table 2 
shows the means of the exact match accuracies for 
increasing subtree depths. 
l)epth of I'rolmbilistic Non-probabilistic 
subtrecs 1)OP D()P 
1 83.1 70.4 
~2 87.6 85.1 
_<3 89.6 89.5 
_<4 90.0 90.9 
_<5 89.7 91.5 
_<6 88.8 92.2 
Table 2. Exact match accuracies for the OV1S corpus 
We again sue that the non-pl'olmlfilistic l)()P model 
performs badly fOl small subtree depths while it 
outperforms the probabi l is l ic DOP model if the 
sublrees gel larger (in this case for depth > 3). Bul 
while lhe accuracy of probabilislic l)()P deteriorates 
after depth 4, the accuracy of non-prolmbilistic 1)O1 + 
contintms to grow. Thus non-prolmlfilistic \])()P seems 
relatively insensitive to tile low frequency of larger 
subtrees. This properly may be especially useful if no 
meaningful  stat ist ics can be co l lected while 
sentences can still be parsed by large chunks. At 
depth ___ 6, non-probabilislic DOP scores 3.4% better 
than probalfilistic DOP, which is statistically signifi- 
cant using paired t-tests. 
4.3 Exper iments on the WSJ  corpus 
Both the ATIS and OVIS corpus represent restricted 
domains. In order to extend ()tit" results to a broad- 
coverage domain, we tested tile two models also on 
tile Wall Street Journal portion in the Penn Treebank 
(Marcus et ill. 1993). 
To make our results comparable to ()tilers, we 
did not test on different random splits but used the 
now slandard division of the WSJ with seclions 2-21 
for training (approx. 40,000 sentences) and section 23 
for testing (see Collins 1997, 1999; Charniak 1997, 
2000; l~,atnalmrkhi 1999); we only tested on sentences 
_< 40 words (2245 sentences). All trees were stripped 
off their Selllalltic lags, co-reference information and 
quotation marks. We used all training set sublrees o1' 
depth 1, but due to memory limitations we used a 
subset of the subtrees larger than depth l by taking for 
each depth a random sample o1' 400,000 subtrecs. No 
subtrces larger than depth 14 were used. This resulted 
into a total set of 5,217,529 subtrees which were 
smoothed by Good-Turing (see Bod 1996). We did not 
employ a separate part-of-speech tagger: tile test 
sentences were directly parsed by the training set 
subtrees. For words that were unknown in tile training 
set, we guessed their categories by means of the 
method described in Weischedel et al (1993) which 
uses statistics on word-endings, hyphenation and 
capital izat ion.  The guessed category for each 
llllklloWn Wol'd was converted into a depth-I subtree 
and assigned a probabil ity (or frequency for non- 
probabilistic I)OP) by means of simple Good-Turing. 
As accuracy metric we used the standard 
PAP, SEVAI, scores (Black et al 1991) to compare a 
proposed parse 1' with tile corresponding correct 
treebank parse 7' as follows: 
# correct constituents in P 
l.abcled Precision - 
# constilucnts in 1' 
# COI'I'CCI. COllstittlcnts ill l ~ 
Labeled Recall = 
# constituents in T 
A constituent in P is "correct" if there exisls a 
constituent in 7' of tile sanle label that spans the same 
words. As in other work, we collapsed AI)VP and 
Pl?Jl" to the same label when calculating these scores 
(see Collins 1997; I~,atnaparkhi 1999; Charniak 1997). 
Table 3 shows the labeled precision (LP) and 
labeled recall (LR) scores for probabilistic and non- 
probabilistic DOP for six different maximum subtree 
depths. 
Depth of l'robabilistic I)OP Non-probabilislic 
subtrecs l)OP 
1,1 ~ LR LP LR 
<_4 84.7 84.1 81.6 80.1 
<_6 86.2 86.0 85.0 84.7 
_<8 87.9 87.1 87.2 87.0 
_< 10 88.6 88.0 86.8 86.5 
_<12 89.1 88.8 87.1 86.9 
_< 14 89.5 89.3 87.2 86.9 
Table 3. Scores on tile WSJ corpus (sentences _<40 words) 
73 
The table shows that probabil istic DOP outperl'orms 
non-probabilistic DOP for maximum subtree depths 4 
and 6, while the models yield rather similar results for 
maximum subtree depth 8. Surprisingly, the scores of 
nonq~robabilistic DOP deteriorate if the subtrees are 
further enlarged, while tile scores of probabil istic 
DOP continue to grow, up to 89.5% LP and 89.3% 
LR. These scores are higher than those of several 
other parsers (e.g. Collins 1997, 99; Charniak 1997), 
but remain behind tim scores of Charniak (2000) who 
obtains 90.1% LP and 90.1% LR for sentences _< 40 
words. However, in Bod (2000b) we show that even 
higher scores can be obtained with probabilistic DOP 
by restricting tile number of words in the subtree 
frontiers to 12 and restricting the depth of unlexical- 
ized subtrees to 6; with these restrictions an LP of 
90.8% and an LR of 90.6% is achieved. 
We may raise the question as to whether we 
actually need these extremely large subtrees to obtain 
our best results. One could argue that DOP's gain in 
parse accuracy with increasing subtree depth is due to 
tile model becoming sensitive to the int'luence o1' 
lexical heads higher in tile lree, and that this gain 
could also be achieved by a more compact depth-1 
DOP model (i.e. an SCFG) which annotates the 
nonterminals with headwords. However, such a head- 
lexical ized stochastic grammar does not capture 
dependencies between nonheadwords (such as more 
aud than in tile W,qJ construction carry more people 
than cargo where neither more nor  th\[lll are headwords 
ol' tile NP-constitucnt lllore people than cargo)), 
whe,eas a frontier-lexicalized DOP model using large 
subtrecs does capture these dependencies ince it 
includes subtrees in which e.g. more and than are the 
only frontier words. In order to isolate tile contribution 
of nonheadword dependencies, we el iminated all 
subtrees containing two or more nonheadwnrds (where 
a nonheadword of a subtl'ec is a word which is not a 
headword of the subtree's root nonterminal -- although 
such a nonheadword may be a headword of one of the 
subtree's internal nodes). On the WSJ this led to a 
decrease in LP/LR of 1.2%/1.0% for probabil istic 
DOP. Thus nonheadword ependencies contribute to 
higher parse accuracy, and should not be discarded. 
This goes against common wisdom that the relevant 
lexical dependencies can be restricted to the locality 
of beadwords of constituents (as advocated in Collins 
1999). It also shows that DOP's frontier lexicalization 
is a viable alternative to constituent lexicalization 
(as proposed in Charniak 1997; Collins 1997, 99; 
Eisner 1997). Moreover, DOP's use of large subtrees 
makes tim model not only more lexically but also 
more structurally sensitive. 
5. Conclusion 
Commou wisdom has it that tile bias o1' stochastic 
grammars ill favor of shorter derivations is harnfful 
and should be redressed. We have shown that the 
common wisdom is wrong for stochastic trek- 
substitution grammars that use elementary trees (11' 
flexible size. For such grammars, a non-probabilistic 
metric based on the shortest derivation outperforms a
probabilistic metric on tile ATIS and OVIS corpora, 
while it obtains competit ive results on tile Wall 
Street Journal corpus. We have seen that a non- 
probabil istic version o1' DOP performed especially 
well on corpora for which collecting sublree statistics 
is difficult, while sentences can still be parsed by 
relatively large chunks. We have also seen that 
probabilistic DOP obtains very competitive results on 
the WSJ corpus. Final ly, we conjecture thai any 
stochastic grammar which uses elementary treks 
rather than context-free rules can be turned into an 
accurate non-probabilistic version (e.g. Tree-Insertion 
Grammar and Tree-At\[joining Grammar). 
Acknowledgements 
Thanks to Khali l  Sima'an and three anonymous 
reviewers for useful suggestions. 
References 
Berg, M. van den, R. Bod and R. Scha, 1994. "A Corpus- 
Based Approach to Semantic Interpretation", 
l'roceedings Ninth Amsterdam Colloquium, 
Amsterdam, The Netherlands. 
Black, E. et al 1991, A Procedure for Quantitatively 
Comparing the Syntactic Coverage of English, 
Proceedings DARPA Speech and Natural Language 
Workshop, Pacific Grove, Morgan Kattfinann. 
Bod, P,. 1992. "l)ata-Oriented Parsing (I)OP)", Proceedings 
COLING-92, Nantes, France. 
Bod, R. 1993. "Using an Annotated Corpus as a Stochastic 
Grammar", l'roceedings European ChcqJter q/" the 
ACL'93, Utrecht, The Netherlands. 
Bod, R. 1996. "Two Questions about l)ata Oriented 
Parsing", l'roceedings Fourth Workshop on Very Large 
Corpora, Copenhagen, 1)cnmark. 
Bod, R. 1998. Beyond Grammar: An EaT,erience-l~ased 
Theot3, of Language, CSLI Publications, Cambridge 
University Press. 
Bod, R. 2000a. "An Empirical Evaluation of LI~G-I)OP '', 
Proceedings COLING-2000, Saarbriickcn, Germany. 
74 
Bed, R. 200Oh. "Redundancy and Minimality in Statistical 
Parsing with Ihe I)OP Model", submitted for 
publicalion. 
Bed, P,. and 1(. l(al)lan, 1998,. "A Probal)ilistic Corpus- 
l)rivcn Model lkw l,cxical Ftmctional Analysis", 
l't'oceedings COIJNf;-ACI,'9& Montreal, CanadzL 
llonncma, R., R. Bed and R. Scha, 1997. '% 1)OI ) Model Ik)l 
Semantic Inlerpretation", Proceedings ACI/l';ACI.-97, 
Madrid, Spain. 
Bonncma, R., t'. Buying and P,. Scha, 1999. "A New 
I'robalfilily Model for I)ata-()ricntcd Parsing", 
lb-oceedings o.f the Amsterdam Colloqttittm 1999, 
Amsterdam, The Nethcrhmds. 
Briscoe, T. and J. Carroll, 1993. "Generalized I'robabilistic 
IA~. I'arsing of Natural l,anguage (Corpora) with 
Uni ficalion-I~ased Gramnlars",  Comlnaatioual 
Linq, uistics 19(1 ), 25-59. 
('araballo, S. and E. Charniak, 1998. "New Figures of Merit 
for Best-First Probabil istic Chart Parsing", 
Comlmtational Lit~gttistics 2d, 275-298. 
Chappclicr, J. and M. l,'ajman, 2000. "Monte Carlo 
Sampling Ior Nl'-hard Maximization I'roblems in the 
l"ramework of  Wc ghtcd Parsing", iu Natmcd 
l.altguag<! l)roce.ssing - -  NI.I' 2000, l.eclure Not,:!.,; i,q 
Arttl/h:ial nlelligence 1835, 1). Chrislodoulakis (ed.), 
2000, 106-117. 
Charniak, 1';. 1996. "Tree-bank Grammars", Proceedings 
AAAI-96, Menlo Park, Ca. 
Charniak, 1!. 1997. "Statistical Parsing wilh a Contcxl-l:rcc 
Grammar and Word Statistics", l'roceedinw AAAI-97. 
Menlo Park. Ca. 
Charniak, I:_ 2000. "A Maximtm>lintropy-hlspircd Parser", 
Proceedings ANI,I~-NAACI,'2000, Scnttlc, Washington. 
Chitrao, M. and P,. Grishman, 1990. "Stalislical Parsing of 
Messages", Proceedings I)ARPA Speech a/ul l,angtta,~,,e 
Workshol~ 1990. 
Collins, M. 1997. "Three generalive lexicaliscd models tbr 
statistical parsing", l'roceedittgs EACI/ACL'97, 
Madrid, Spain. 
Collins, M. 1999. llead-l)riven Statistical Models./br Natural 
I,anguagc I'arsing, Phl)-thesis, University of 
Pennsylvania, PA. 
I)aclcmans, W. (ed.) 1999. Memmy-Ilased Language 
Processing, Journal./br l';.vperimental and Theoretical 
Arti/i'cial Inlelligence, 11(3). 
Eisner, J. 1997. "Bilexical Granlnlars and a Cubic-Time 
Probabilistic Parser", I'mceedings l,'\[/?h International 
Workshop on l'alwing 7?clmologies, I{oslon, Mass. 
Goodman, J. 1998. Palwing lnsMe-Out, Ph.l). thesis, l larwu'd 
University, Mass. 
l loogweg, 1.. 2000. Extending DOPI with the Insertion 
Operation, Maslel"S thesis, University of Amsterdam, 
The Netherhmds. 
Johnson, M. 1998. "The I)OP Estimatkm Method is Biased 
and Inconsistent", squib. 
l,ari, K. and S. Young 1990. "The Estimation of Stochastic 
Context-Free Granunars Using the lnside-()utsidc 
Algorithm", ()mtl~ltlel" ,~'1~(3cc11 alld l,angttctge, d, 35-56. 
Magerman, l). and M. Marcus, 1991. "Pearl: A Probabilislic 
Chart Parser", Proceedings I'ACL'91, Berlin, 
(iermany. 
Marcus, M., B. Santorini and M. Marcinkiewicz, 1993. 
"lhtikling a l'u-e,~ ~ Annotaled Corpus of Fm~lish:~ the 
Pcnt~ Trcebank", Comlmtatioual l,ingttistics 19(2). 
Nculnann, (~. 1998. "Automatic Extraction of Slochastic 
Imxicalizcd Tree C;r~.lllllllals flonl Treebanks", 
Proceedings of the 4th Workshop on 7)'ee-Adjoining 
(;rammcuw and Related I,)ameworl<s, Philadclplaia, PA. 
Neumann, (3. and I). Flickinger, 1999. "l~earning Stochastic 
l,exicalizcd Tree (\]lanllnars frolll IIPSG", I)FKI 
Technical (epoit, Saartwiicken, Germany. 
Ilalnaparkhi, A. 1999. "l,carning to Parse Natural 14mguage 
with Maximum linlropy Models", Machit~e /.earning 
34. 151-176. 
S\[laIlllOll, C. 1948. A Mathematical Theory of Communic- 
ation. IMII System Technical .Iota'hal. 27, 379-423, 623- 
656. 
Sima'an, K. 1995. "An optimized algorilhin for l)ata 
Oriented Parsing", in: R. Milkov and N, Nicolov 
(eds.), Recent Advam:es in Natttral l,alsguage 
l'rocessing 1995, w>lume 136 of Current lsstws in 
Linguistic T/woO'. JohI1 Bcnjalll.ins, Aiilslerdali1. 
Sima'an, K. 1996. "Comlmtational Complcxity of 
Plobabilistic l)isambiguation hy means of Tree 
(;l'\[lllllll\[ll'S", l'rocc('dings C()I.IN(;-9(~, Copenhagen, 
I)cnmark. 
Sima'an, K. 1999. l.earni/tg l:i/.\]icient l)isambigua/i:m. 113,C 
Dissertation SOlits 199%02, l.Jtrcdlt University / 
University of Amsterdam, March 1999, The 
Nclherlands. 
Wcischedel, R., M. Mercer, R, Schwarz, L. Ramshaw and 
J. Pahnucci, 1993. "Coping with Ambiguity and 
Unknown Words through Probabilistic Models", 
Comlmtational Linguistics, 19(2), 359-382. 
Way, A. 1999. "A t lybrid Architecture for I~.obus! MT using 
I,t"G-I)OP", .hmrnal o/" l'2xlwrimental m/ Theoretical 
Arti/}'cial Intelligence I1 (Special Issue on Memory- 
Based l~anguage Processing). 
Zipf, G. 1935. The l'sycho-Biology of Language, l loughton 
Mifflin. 
75 
An Improved Parser for Data-Oriented Lexical-Functional Analysis
Rens Bod
Informatics Research Institute, University of Leeds, Leeds LS2 9JT, UK, &
Institute for Logic, Language and Computation, University of Amsterdam
rens@scs.leeds.ac.uk
Abstract
We present an LFG-DOP parser which uses
fragments from LFG-annotated sentences to parse
new sentences. Experiments with the Verbmobil
and Homecentre corpora show that (1) Viterbi n
best search performs about 100 times faster than
Monte Carlo search while both achieve the same
accuracy; (2) the DOP hypothesis which states
that parse accuracy increases with increasing frag-
ment size is confirmed for LFG-DOP; (3) LFG-
DOP's relative frequency estimator performs
worse than a discounted frequency estimator; and
(4) LFG-DOP significantly outperforms Tree-
DOP if evaluated on tree structures only.
1  Introduction
Data-Oriented Parsing (DOP) models learn how
to provide linguistic representations for an
unlimited set of utterances by generalizing from a
given corpus of properly annotated exemplars.
They operate by decomposing the given
representations into (arbitrarily large) fragments
and recomposing those pieces to analyze new
utterances. A probability model is used to choose
from the collection of different fragments of
different sizes those that make up the most
appropriate analysis of an utterance.
DOP models have been shown to achieve
state-of-the-art parsing performance on
benchmarks such as the Wall Street Journal
corpus (see Bod 2000a). The original DOP model
in Bod (1993) was based on utterance analyses
represented as surface trees, and is equivalent to a
Stochastic Tree-Substitution Grammar. But the
model has also been applied to several other
grammatical frameworks, e.g. Tree-Insertion
Grammar (Hoogweg 2000), Tree-Adjoining
Grammar (Neumann 1998), Lexical-Functional
Grammar (Bod & Kaplan 1998; Cormons 1999),
Head-driven Phrase Structure Grammar
(Neumann & Flickinger 1999), and Montague
Grammar (Bonnema et al 1997; Bod 1999).
Most probability models for DOP use the relative
frequency estimator to estimate fragment
probabilities, although Bod (2000b) trains
fragment probabilities by a maximum likelihood
reestimation procedure belonging to the class of
expectation-maximization algorithms. The DOP
model has also been tested as a model for human
sentence processing (Bod 2000d).
This paper presents ongoing work on
DOP models for Lexical-Functional Grammar
representations, known as LFG-DOP (Bod &
Kaplan 1998). We develop a parser which uses
fragments from LFG-annotated sentences to parse
new sentences, and we derive some experimental
properties of LFG-DOP on two LFG-annotated
corpora: the Verbmobil and Homecentre corpus.
The experiments show that the DOP hypothesis,
which states that there is an increase in parse
accuracy if larger fragments are taken into account
(Bod 1998), is confirmed for LFG-DOP. We
report on an improved search technique for
estimating the most probable analysis. While a
Monte Carlo search converges provably to the
most probable parse, a Viterbi n best search
performs as well as Monte Carlo while its
processing time is two orders of magnitude faster.
We also show that LFG-DOP outperforms Tree-
DOP if evaluated on tree structures only.
2  Summary of LFG-DOP
In accordance with Bod (1998), a particular DOP
model is described by
?  a definition of a well-formed representation for
utterance analyses,
?  a set of decomposition operations  that divide a
given utterance analysis into a set of fragments,
?  a set of composition operations  by which such
fragments may be recombined to derive an
analysis of a new utterance, and
?  a definition of a probability model that indicates
how the probability of a new utterance analysis is
computed.
In defining a DOP model for LFG
representations, Bod & Kaplan (1998) give the
following settings for DOP's four parameters.
2.1  Representations
The representations used by LFG-DOP are
directly taken from LFG: they consist of a c-
structure, an f-structure and a mapping ? between
them. The following figure shows an example
representation for Kim eats. (We leave out some
features to keep the example simple.)
S
NP VP
Kim eats
PRED   'Kim'
NUM	    SG  
SUBJ
TENSE      PRES
PRED       'eat(SUBJ)'
Figure 1
Bod & Kaplan also introduce the notion of
accessibility which they later use for defining the
decomposition operations of LFG-DOP:
An f-structure unit f is ?-accessible from a node
n iff either n is ?-linked to f (that is, f = ?(n) )
or f is contained within ?(n) (that is, there is a
chain of attributes that leads from ?(n) to f).
According to the LFG representation theory, c-
structures and f-structures must satisfy certain
formal well-formedness conditions. A c-
structure/f-structure pair is a valid LFG represent-
ation only if it satisfies the Nonbranching
Dominance, Uniqueness, Coherence and Com-
pleteness conditions (Kaplan & Bresnan 1982).
2.2  Decomposition operations and Fragments
The fragments for LFG-DOP consist of
connected subtrees whose nodes are in ?-
correspondence with the correponding sub-units
of f-structures. To give a precise definition of
LFG-DOP fragments, it is convenient to recall the
decomposition operations employed by the
orginal DOP model which is also known as the
"Tree-DOP" model (Bod 1993, 1998):
(1)  Root: the Root operation selects any node of
a tree to be the root of the new subtree and
erases all nodes except the selected node and the
nodes it dominates.
(2)  Frontier : the Frontier operation then
chooses a set (possibly empty) of nodes in the
new subtree different from its root and erases all
subtrees dominated by the chosen nodes.
Bod & Kaplan extend Tree-DOP's Root and
Frontier  operations so that they also apply to the
nodes of the c-structure in LFG, while respecting
the principles of c/f-structure correspondence.
When a node is selected by the Root
operation, all nodes outside of that node's subtree
are erased, just as in Tree-DOP. Further, for
LFG-DOP, all ? links leaving the erased nodes
are removed and all f-structure units that are not
?-accessible from the remaining nodes are erased.
For example, if Root selects the NP in figure 1,
then the f-structure corresponding to the S node is
erased, giving figure 2 as a possible fragment:
NP
Kim
PRED   'Kim'
NUM	    SG  
Figure 2
In addition the Root operation deletes from the
remaining f-structure all semantic forms that are
local to f-structures that correspond to erased c-
structure nodes, and it thereby also maintains the
fundamental two-way connection between words
and meanings. Thus, if Root selects the VP node
so that the NP is erased, the subject semantic
form "Kim" is also deleted:
VP
eats
NUM	    SG  SUBJ
TENSE      PRES
PRED      'eat(SUBJ)'
Figure 3
As with Tree-DOP, the Frontier operation then
selects a set of frontier nodes and deletes all
subtrees they dominate. Like Root , it also
removes the ? links of the deleted nodes and
erases any semantic form that corresponds to any
of those nodes. For instance, if the NP in figure 1
is selected as a frontier node, Frontier erases the
predicate "Kim" from the fragment:
eats
S
NP VP
NUM	   SG  SUBJ
TENSE      PRES
PRED      'eat(SUBJ)'
Figure 4
Finally, Bod & Kaplan present a third
decomposition operation, Discard , defined to
construct generalizations of the fragments
supplied by Root and Frontier . Discard acts to
delete combinations of attribute-value pairs
subject to the following condition: Discard does
not delete pairs whose values ? -correspond to
remaining c-structure nodes. According to Bod &
Kaplan (1998), Discard-generated fragments are
needed to parse sentences that are "ungrammatical
with respect to the corpus", thus increasing the
robustness of the model.
2.3  The composition operation
In LFG-DOP the operation for combining
fragments is carried out in two steps. First the c-
structures are combined by leftmost substitution
subject to the category-matching condition, as in
Tree-DOP. This is followed by the recursive
unification of the f-structures corresponding to the
matching nodes. A derivation for an LFG-DOP
representation R  is a sequence of fragments the
first of which is labeled with S and for which the
iterative application of the composition operation
produces R . For an illustration of the composition
operation, see Bod & Kaplan (1998).
2.4  Probability models
As in Tree-DOP, an LFG-DOP representation R
can typically be derived in many different ways. If
each derivation D has a probability P(D), then the
probability of deriving R  is the sum of the
individual derivation probabilities:
(1) P(R)  =  ?D derives R P(D)
An LFG-DOP derivation is produced by a
stochastic process which starts by randomly
choosing a fragment whose c-structure is labeled
with the initial category. At each subsequent step,
a next fragment is chosen at random from among
the fragments that can be composed with the
current subanalysis. The chosen fragment is
composed with the current subanalysis to produce
a new one; the process stops when an analysis
results with no non-terminal leaves. We will call
the set of composable fragments at a certain step
in the stochastic process the competition set at that
step. Let CP( f | CS) denote the probability of
choosing a fragment f from a competition set CS
containing f, then the probability of a derivation D
= < f1, f2 ... fk> is
(2) P(< f1, f2 ... fk>)  =  ?i CP(f i | CSi)
where the competition probability  CP(f | CS) is
expressed in terms of fragment probabilities P( f):
?f'?CS  P(f')
P( f)  (3) CP(f | CS)  =
Bod & Kaplan give three definitions of increasing
complexity for the competition set: the first
definition groups all fragments that only satisfy
the Category-matching condition of the
composition operation; the second definition
groups all fragments which satisfy both Category-
matching and Uniqueness; and the third definition
groups all fragments which satisfy Category-
matching, Uniqueness and Coherence. Bod &
Kaplan point out that the Completeness condition
cannot be enforced at each step of the stochastic
derivation process, and is a property of the final
representation which can only be enforced by
sampling valid representations from the output of
the stochastic process. In this paper, we will only
deal with the third definition of competition set, as
it selects only those fragments at each derivation
step that may finally result into a valid LFG
representation, thus reducing the off-line validity
checking to the Completeness condition.
Note that the computation of the
competition probability in the above formulas still
requires a definition for the fragment probability
P(f). Bod and Kaplan define the probability of a
fragment simply as its relative frequency in the
bag of all fragments generated from the corpus,
just as in most Tree-DOP models. We will refer
to this fragment estimator as "simple relative
frequency" or "simple RF".
We will also use an alternative definition
of fragment probability which is a refinement of
simple RF. This alternative fragment probability
definition distinguishes between fragments
supplied by Root/Frontier  and fragments
supplied by Discard . We will treat the first type
of fragments as seen events, and the second type
of fragments as previously unseen events. We
thus create two separate bags corresponding to
two separate distributions: a bag with fragments
generated by Root and Frontier , and a bag with
fragments generated by Discard. We assign
probability mass to the fragments of each bag by
means of discounting : the relative frequencies of
seen events are discounted and the gained
probability mass is reserved for the bag of unseen
events (cf. Ney et al 1997). We accomplish this
by a very simple estimator: the Turing-Good
estimator (Good 1953) which computes the
probability mass of unseen events as n1/N where
n1 is the number of singleton events and N is the
total number of seen events. This probability
mass is assigned to the bag of Discard-generated
fragments. The remaining mass (1 ?  n1/N ) is
assigned to the bag of Root/Frontier-generated
fragments. The probability of each fragment is
then computed as its relative frequency in its bag
multiplied by the probability mass assigned to this
bag. Let | f | denote the frequency of a fragment f,
then its probability is given by:
| f |
?f': f' is generated by Root/Frontier | f'|
(1 ? n1/N)
(4) P(f | f is generated by Root/Frontier)  =
(5) P(f | f is generated by Discard)  =
(n1/N)
| f |
?f': f' is generated by Discard | f'|
We will refer to this fragment probability
estimator as "discounted relative frequency" or
"discounted RF".
4  Parsing with LFG-DOP
In his PhD-thesis, Cormons (1999) presents a
parsing algorithm for LFG-DOP which is based
on the Tree-DOP parsing technique described in
Bod (1998). Cormons first converts LFG-
representations into more compact indexed trees:
each node in the c-structure is assigned an index
which refers to the ?-corresponding f-structure
unit. For example, the representation in figure 1 is
indexed as
(S.1 (NP.2 Kim.2)
(VP.1 eats.1))
where
1 --> [  (SUBJ = 2)
(TENSE = PRES)
(PRED = eat(SUBJ)) ]
2 --> [  (PRED = Kim)
            (NUM = SG) ]
The indexed trees are then fragmented by
applying the Tree-DOP decomposition operations
described in section 2. Next, the LFG-DOP
decomposition operations Root, Frontier  and
Discard  are applied to the f-structure units that
correspond to the indices in the c-structure
subtrees. Having obtained the set of LFG-DOP
fragments in this way, each test sentence is parsed
by a bottom-up chart parser using initially the
indexed subtrees only.
Thus only the Category-matching
condition is enforced during the chart-parsing
process. The Uniqueness and Coherence
conditions of the corresponding f-structure units
are enforced during the disambiguation or chart -
decoding process.  Disambiguation is
accomplished by computing a large number of
random derivations from the chart and by
selecting the analysis which results most often
from these derivations. This technique is known
as "Monte Carlo disambiguation" and has been
extensively described in the literature (e.g. Bod
1993, 1998; Chappelier & Rajman 2000;
Goodman 1998; Hoogweg 2000). Sampling a
random derivation from the chart consists of
choosing at random one of the fragments from
the set of composable  fragments at every labeled
chart-entry (where the random choices at each
chart-entry are based on the probabilities of the
fragments). The derivations are sampled in a top-
down, leftmost order so as to maintain the LFG-
DOP derivation order. Thus the competition sets
of composable fragments are computed on the fly
during the Monte Carlo sampling process by
grouping the f-structure units that unify and that
are coherent with the subderivation built so far.
As mentioned in section 3, the
Completeness condition can only be checked after
the derivation process. Incomplete derivations are
simply removed from the sampling distribution.
After sampling a sufficiently large number of
random derivations that satisfy the LFG validity
requirements, the most probable analysis is
estimated by the analysis which results most often
from the sampled derivations. As a stop condition
on the number of sampled derivations, we
compute the probability of error, which is the
probability that the analysis that is most frequently
generated by the sampled derivations is not equal
to the most probable analysis, and which is set to
0.05 (see Bod 1998). In order to rule out the
possibility that the sampling process never stops,
we use a maximum sample size of 10,000
derivations.
While the Monte Carlo disambiguation
technique converges provably to the most
probable analysis, it is quite inefficient. It is
possible to use an alternative, heuristic search
based on Viterbi n  best (we will not go into the
PCFG-reduction technique presented in Goodman
(1998) since that heuristic only works for Tree-
DOP and is beneficial only if all subtrees are
taken into account and if the so-called "labeled
recall parse" is computed). A Viterbi n  best search
for LFG-DOP estimates the most probable
analysis by computing n  most probable
derivations, and by then summing up the
probabilities of the valid derivations that produce
the same analysis. The algorithm for computing n
most probable derivations follows straight-
forwardly from the algorithm which computes the
most probable derivation by means of Viterbi
optimization (see e.g. Sima'an 1999).
5  Experimental Evaluation
We derived some experimental properties of
LFG-DOP by studying its behavior on the two
LFG-annotated corpora that are currently
available: the Verbmobil corpus and the
Homecentre corpus. Both corpora were annotated
at Xerox PARC. They contain packed LFG-
representations (Maxwell & Kaplan 1991) of the
grammatical parses of each sentence together with
an indication which of these parses is the correct
one. For our experiments we only used the correct
parses of each sentence resulting in 540
Verbmobil parses and 980 Homecentre parses.
Each corpus was divided into a 90% training set
and a 10% test set. This division was random
except for one constraint: that all the words in the
test set actually occurred in the training set. The
sentences from the test set were parsed and
disambiguated by means of the fragments from
the training set. Due to memory limitations, we
restricted the maximum depth of the indexed
subtrees to 4. Because of the small size of the
corpora we averaged our results on 10 different
training/test set splits. Besides an exact match
accuracy metric, we also used a more fine-grained
score based on the well-known PARSEVAL
metrics that evaluate phrase-structure trees (Black
et al 1991). The PARSEVAL metrics compare a
proposed parse P with the corresponding correct
treebank parse T  as follows:
Precision = 
 # correct constituents in P
# constituents in P  
 # correct constituents in P
# constituents in T 
Recall = 
 
A constituent in P  is correct if there exists a
constituent in T of the same label that spans the
same words and that ?-corresponds to the same
f-structure unit (see Bod 2000c for some
illustrations of these metrics for LFG-DOP).
5.1 Comparing the two fragment estimators
We were first interested in comparing the
performance of the simple RF estimator against
the discounted RF estimator. Furthermore, we
want to study the contribution of generalized
fragments to the parse accuracy. We therefore
created for each training set two sets of fragments:
one which contains all fragments (up to depth 4)
and one which excludes the generalized fragments
as generated by Discard . The exclusion of these
Discard-generated fragments means that all
probability mass goes to the fragments generated
by Root and Frontier  in which case the two
estimators are equivalent. The following two
tables present the results of our experiments
where +Discard refers to the full set of fragments
and ?Discard refers to the fragment set without
Discard-generated fragments.
              Exact Match         Precision   Recall
+Discard  ?Discard +Discard  ?Discard +Discard  ?Discard
Simple RF 1.1%       35.2% 13.8%        76.0% 11.5%       74.9%
35.9%        35.2% 77.5%        76.0% 76.4%       74.9%Discounted RF
Estimator
Table 1. Experimental results on the Verbmobil
              Exact Match         Precision   Recall
+Discard  ?Discard +Discard  ?Discard +Discard  ?Discard
2.7%       37.9% 17.1%        77.8% 15.5%       77.2%
38.4%        37.9% 80.0%        77.8% 78.6%       77.2%
Simple RF
Discounted RF
Estimator
Table 2. Experimental results on the Homecentre
The tables show that the simple RF estimator
scores extremely bad if all fragments are used: the
exact match is only 1.1% on the Verbmobil
corpus and 2.7% on the Homecentre corpus,
whereas the discounted RF estimator scores
respectively 35.9% and 38.4% on these corpora.
Also the more fine-grained precision and recall
scores obtained with the simple RF estimator are
quite low: e.g. 13.8% and 11.5% on the
Verbmobil corpus, where the discounted RF
estimator obtains 77.5% and 76.4%. Interestingly,
the accuracy of the simple RF estimator is much
higher if Discard-generated fragments are
excluded. This suggests that treating generalized
fragments probabilistically in the same way as
ungeneralized fragments is harmful.
The tables also show that the inclusion of
Discard-generated fragments leads only to a
slight accuracy increase under the discounted RF
estimator. Unfortunately, according to paired t -
testing only the differences for the precision
scores on the Homecentre corpus were
statistically significant.
5.2 Comparing different fragment sizes
We were also interested in the impact of fragment
size on the parse accuracy. We therefore
performed a series of experiments where the
fragment set is restricted to fragments of a certain
maximum depth (where the depth of a fragment
is defined as the longest path from root to leaf of
its c-structure unit). We used the same
training/test set splits as in the previous
experiments and used both ungeneralized and
generalized fragments together with the
discounted RF estimator.
Fragment Depth Exact Match Precision Recall
1 30.6% 74.2% 72.2%
?2 34.1% 76.2% 74.5%
?3 35.6% 76.8% 75.9%
?4 35.9% 77.5% 76.4%
Table 3. Accuracies on the Verbmobil
Fragment Depth Exact Match Precision Recall
1 31.3% 75.0% 71.5%
?2 36.3% 77.1% 74.7%
?3 37.8% 77.8% 76.1%
?4 38.4% 80.0% 78.6%
Table 4. Accuracies on the Homecentre
Tables 3 and 4 show that there is a consistent
increase in parse accuracy for all metrics if larger
fragments are included, but that the increase itself
decreases. This phenomenon is also known as the
DOP hypothesis (Bod 1998), and has been
confirmed for Tree-DOP on the ATIS, OVIS and
Wall Street Journal treebanks (see Bod 1993,
1998, 1999, 2000a; Sima'an 1999; Bonnema et al
1997; Hoogweg 2000). The current result thus
extends the validity of the DOP hypothesis to
LFG annotations. We do not yet know whether
the accuracy continues to increase if even larger
fragments are included (for Tree-DOP it has been
shown that the accuracy decreases  after a certain
depth, probably due to overfitting -- cf. Bonnema
et al 1997; Bod 2000a).
5.3 Comparing LFG-DOP to Tree-DOP
In the following experiment, we are interested in
the impact of functional structures on predicting
the correct tree structures. We therefore removed
all f-structure units from the fragments, thus
yielding a Tree-DOP model, and compared the
results against the full LFG-DOP model (using
the discounted RF estimator and all fragments up
to depth 4). We evaluated the parse accuracy on
the tree structures only, using exact match
together with the standard PARSEVAL
measures. We used the same training/test set
splits as in the previous experiments.
        Exact Match   Precision   Recall
Tree-DOP     46.6%     88.9% 86.7%
LFG-DOP     50.8%     90.3%         88.4%
Model
Table 5. Tree accuracy on the Verbmobil
        Exact Match   Precision   Recall
Tree-DOP     49.0%     93.4% 92.1%
LFG-DOP     53.2%     95.8%         94.7%
Model
Table 6. Tree accuracy on the Homecentre
The results indicate that LFG-DOP's functional
structures help to improve the parse accuracy of
tree structures. In other words, LFG-DOP
outperforms Tree-DOP if evaluated on tree
structures only. According to paired t-tests all
differences in accuracy were statistically
significant. This result is promising since Tree-
DOP has been shown to obtain state-of-the-art
performance on the Wall Street Journal corpus
(see Bod 2000a).
5.4 Comparing Viterbi n  best to Monte Carlo
Finally, we were interested in comparing an
alternative, more efficient search method for
estimating the most probable analysis. In the
following set of experiments we use a Viterbi n
best search heuristic (as explained in section 4),
and let n range from 1 to 10,000 derivations. We
also compute the results obtained by Monte Carlo
for the same number of derivations. We used the
same training/test set splits as in the previous
experiments and used both ungeneralized and
generalized fragments up to depth 4 together with
the discounted RF estimator.
Nr. of derivations Viterbi n best Monte Carlo
1 74.8% 20.1%
10 75.3% 36.7%
100 77.5% 67.0%
1,000 77.5% 77.1%
10,000 77.5% 77.5%
Table 7. Precision on the Verbmobil
Nr. of derivations Viterbi n best Monte Carlo
1 75.6% 25.6%
10 76.2% 44.3%
100 79.1% 74.6%
1,000 79.8% 79.1%
10,000 79.8% 80.0%
Table 8. Precision on the Homecentre
The tables show that Viterbi n best already
achieves a maximum accuracy at 100 derivations
(at least on the Verbmobil corpus) while Monte
Carlo needs a much larger number of derivations
to obtain these results. On the Homecentre
corpus, Monte Carlo slightly outperforms Viterbi
n best at 10,000 derivations, but these differences
are not statistically significant. Also remarkable
are the relatively high results obtained with Viterbi
n best if only one derivation is used. This score
corresponds to the analysis generated by the most
probable (valid) derivation. Thus Viterbi n  best is
a promising alternative to Monte Carlo resulting
in a speed up of about two orders of magnitude.
6  Conclusion
We presented a parser which analyzes new input
by probabilistically combining fragments from
LFG-annotated corpora into new analyses. We
have seen that the parse accuracy increased with
increasing fragment size, and that LFG's
functional structures contribute to significantly
higher parse accuracy on tree structures. We
tested two search techniques for the most
probable analysis, Viterbi n best  and Monte Carlo.
While these two techniques achieved about the
same accuracy, Viterbi n best was about 100
times faster than Monte Carlo.
References
E. Black et al, 1991. "A Procedure for
Quantitatively Comparing the Syntactic
Coverage of English", Proceedings DARPA
Workshop, Pacific Grove, Morgan Kaufmann.
R. Bod, 1993. "Using an Annotated Language
Corpus as a Virtual Stochastic Grammar",
Proceedings AAAI'93, Washington D.C.
R. Bod, 1998. Beyond Grammar: An Experience-
Based Theory of Language, CSLI Publications,
Cambridge University Press.
R. Bod 1999. "Context-Sensitive Dialogue
Processing with the DOP Model", Natural
Language Engineering 5(4), 309-323.
R. Bod, 2000a. "Parsing with the Shortest
Derivation", Proceedings COLING-2000,
Saarbr?cken, Germany.
R. Bod 2000b. "Combining Semantic and Syntactic
Structure for Language Modeling", Proceed-
ings ICSLP-2000, Beijing, China.
R. Bod 2000c. "An Empirical Evaluation of LFG-
DOP", Proceedings COLING-2000, Saar-
br?cken, Germany.
R. Bod, 2000d. "The Storage and Computation of
Frequent Sentences", Proceedings AMLAP-
2000, Leiden, The Netherlands.
R. Bod and R. Kaplan, 1998. "A Probabilistic
Corpus-Driven Model for Lexical Functional
Analysis", Proceedings COLING-ACL'98,
Montreal, Canada.
R. Bonnema, R. Bod and R. Scha, 1997. "A DOP
Model for Semantic Interpretation",
Proceedings ACL/EACL-97, Madrid, Spain.
J. Chappelier and M. Rajman, 2000. "Monte Carlo
Sampling for NP-hard Maximization Problems
in the Framework of Weighted Parsing", in
NLP 2000, Lecture Notes in Artificial
Intelligence 1835, 106-117.
B. Cormons, 1999. Analyse et d?sambiguisation:
Une approche ? base de corpus (Data-Oriented
Parsing) pour les r?presentations lexicales
fonctionnelles . PhD thesis, Universit? de
Rennes, France.
I. Good, 1953. "The Population Frequencies of
Species and the Estimation of Population
Parameters", Biometrika 40, 237-264.
J. Goodman, 1998. Parsing Inside-Out, PhD thesis,
Harvard University, Mass.
L. Hoogweg, 2000. Enriching DOP1 with the
Insertion Operation, MSc Thesis, Dept. of
Computer Science, University of Amsterdam.
R. Kaplan, and J. Bresnan, 1982. "Lexical-
Functional Grammar: A Formal System for
Grammatical Representation", in J. Bresnan
(ed.), The Mental Representation of
Grammatical Relations, The MIT Press,
Cambridge, Mass.
J. Maxwell and R. Kaplan, 1991. "A Method for
Disjunctive Constraint Satisfaction", in M.
Tomita (ed.), Current Issues in Parsing
Technology, Kluwer Academic Publishers.
G. Neumann, 1998. "Automatic Extraction of
Stochastic Lexicalized Tree Grammars from
Treebanks", Proceedings of the 4th Workshop
on Tree-Adjoining Grammars and Related
Frameworks, Philadelphia, PA.
G. Neumann and D. Flickinger, 1999. "Learning
Stochastic Lexicalized Tree Grammars from
HPSG", DFKI Technical Report, Saarbr?cken,
Germany.
H. Ney, S. Martin and F. Wessel, 1997. "Statistical
Language Modeling Using Leaving-One-Out",
in S. Young & G. Bloothooft (eds.), Corpus-
Based Methods in Language and Speech
Processing, Kluwer Academic Publishers.
K. Sima'an, 1999. Learning Efficient Disambigu-
ation. PhD thesis, ILLC dissertation series
number 1999-02. Utrecht / Amsterdam.
What is the Minimal Set of Fragments that Achieves
Maximal Parse Accuracy?
Rens Bod
School of Computing
University of Leeds, Leeds LS2 9JT, &
Institute for Logic, Language and Computation
University of Amsterdam, Spuistraat 134, 1012 VB Amsterdam
rens@comp.leeds.ac.uk
Abstract
We aim at finding the minimal set of
fragments which achieves maximal parse
accuracy in Data Oriented Parsing. Expe-
riments with the Penn Wall Street
Journal treebank show that counts of
almost arbitrary fragments within parse
trees are important, leading to improved
parse accuracy over previous models
tested on this treebank (a precis -
ion of 90.8% and a recall of 90.6%). We
isolate some dependency relations which
previous models neglect but which
contribute to higher parse accuracy.
1 Introduction
One of the goals in statistical natural language
parsing is to find the minimal set of statistical
dependencies (between words and syntactic
structures) that achieves maximal parse accuracy.
Many stochastic parsing models use linguistic
intuitions to find this minimal set, for example by
restricting the statistical dependencies to the
locality of headwords of constituents (Collins
1997, 1999; Eisner 1997), leaving it as an open
question whether there exist important statistical
dependencies that go beyond linguistically
motivated dependencies. The Data Oriented
Parsing (DOP) model, on the other hand, takes a
rather extreme view on this issue: given an
annotated corpus, all fragments (i.e. subtrees)
seen in that corpus, regardless of size and
lexicalization, are in principle taken to form a
grammar (see Bod 1993, 1998; Goodman 1998;
Sima'an 1999). The set of subtrees that is used is
thus very large and extremely redundant. Both
from a theoretical and from a computational
perspective we may wonder whether it is
possible to impose constraints on the subtrees
that are used, in such a way that the accuracy of
the model does not deteriorate or perhaps even
improves. That is the main question addressed in
this paper. We report on experiments carried out
with the Penn Wall Street Journal (WSJ)
treebank to investigate several strategies for
constraining the set of subtrees. We found that
the only constraints that do not decrease the parse
accuracy consist in an upper bound of the
number of words in the subtree frontiers and an
upper bound on the depth of unlexicalized
subtrees. We also found that counts of subtrees
with several nonheadwords are important,
resulting in improved parse accuracy over
previous parsers tested on the WSJ.
2 The DOP1 Model
To-date, the Data Oriented Parsing model has
mainly been applied to corpora of trees whose
labels consist of primitive symbols (but see Bod
& Kaplan 1998; Bod 2000c, 2001). Let us illus-
trate the original DOP model presented in Bod
(1993), called DOP1, with a simple example.
Assume a corpus consisting of only two trees:
NP VP
S
NP
Mary
V
likes
John
NP VP
S
NPVPeter
hates Susan
Figure 1. A corpus of two trees
New sentences may be derived by combining
fragments, i.e. subtrees, from this corpus, by
means of a node-substitution operation indicated
as ?. Node-substitution identifies the leftmost
nonterminal frontier node of one subtree with the
root node of a second subtree (i.e., the second
subtree is substituted  on the leftmost nonterminal
frontier node of the first subtree). Thus a new
sentence such as Mary likes Susan  can be derived
by combining subtrees from this corpus:
NP VP
S
NPV
likes
NP
Mary
NP
Susan NP VP
S
NPMary V
likes Susan
=? ?
Figure 2. A derivation for Mary likes Susan
Other derivations may yield the same tree, e.g.:
NP VP
S
NPV
NP
Mary NP VP
S
NPMary V
likes Susan
=
Susan
V
likes
? ?
Figure 3. Another derivation yielding same tree
DOP1 computes the probability of a subtree t as
the probability of selecting t among all corpus
subtrees that can be substituted on the same node
as t. This probability is equal to the number of
occurrences of t , | t |, divided by the total number
of occurrences of all subtrees t' with the same
root label as t. Let r(t) return the root label of t.
Then we may write:
P(t)  =   | t |
?  t': r(t')= r(t)  | t' |
In most applications of DOP1, the subtree
probabilities are smoothed by the technique
described in Bod (1996) which is based on
Good-Turing. (The subtree probabilities are not
smoothed by backing off to smaller subtrees,
since these are taken into account by the parse
tree probability, as we will see.)
The probability of a derivation t1?...?tn  is
computed by the product of the probabilities of
its subtrees ti:
P(t1?...?tn)  =  ?i  P(ti)
As we have seen, there may be several distinct
derivations that generate the same parse tree. The
probability of a parse tree T is thus the sum of the
probabilities of its distinct derivations. Let tid be
the i-th subtree in the derivation d that produces
tree T, then the probability of T is given by
P(T)  =  ?d?i P(tid)
Thus the DOP1 model considers counts of
subtrees of a wide range of sizes in computing
the probability of a tree: everything from counts
of single-level rules to counts of entire trees. This
means that the model is sensitive to the frequency
of large subtrees while taking into account the
smoothing effects of counts of small subtrees.
Note that the subtree probabilities in DOP1
are directly estimated from their relative frequen-
cies. A number of alternative subtree estimators
have  been proposed for DOP1 (cf. Bonnema et
al 1999), including maximum likelihood
estimation (Bod 2000b). But since the relative
frequency estimator has so far not been outper -
formed by any other estimator for DOP1, we
will stick to this estimator in the current paper.
3 Computational Issues
Bod (1993) showed how standard chart parsing
techniques can be applied to DOP1. Each corpus-
subtree t is converted into a context-free rule r
where the lefthand side of r corresponds to the
root label of t  and the righthand side of r
corresponds to the frontier labels of t. Indices link
the rules to the original subtrees so as to maintain
the subtree's internal structure and probability.
These rules are used to create a derivation forest
for a sentence (using a CKY parser), and the
most probable parse is computed by sampling a
sufficiently large number of random derivations
from the forest ("Monte Carlo disambiguation",
see Bod 1998). While this technique has been
successfully applied to parsing the ATIS portion
in the Penn Treebank (Marcus et al 1993), it is
extremely time consuming. This is mainly
because the number of random derivations that
should be sampled to reliably estimate the most
probable parse increases exponentially with the
sentence length (see Goodman 1998). It is
therefore questionable whether Bod's sampling
technique can be scaled to larger domains such as
the WSJ portion in the Penn Treebank.
Goodman (1996, 1998) showed how DOP1
can be reduced to a compact stochastic context-
free grammar (SCFG) which contains exactly
eight SCFG rules for each node in the training set
trees. Although Goodman's method does still not
allow for an efficient computation of the most
probable parse (in fact, the problem of computing
the most probable parse in DOP1 is NP-hard --
see Sima'an 1999), his method does allow for an
efficient computation of the "maximum constit-
uents parse", i.e. the parse tree that is most likely
to have the largest number of correct constituents.
Goodman has shown on the ATIS corpus that
the maximum constituents parse performs at
least as well as the most probable parse if all
subtrees are used. Unfortunately, Goodman's
reduction method is only beneficial if indeed all
subtrees are used. Sima'an (1999: 108) argues
that there may still be an isomorphic SCFG for
DOP1 if the corpus-subtrees are restricted in size
or lexicalization, but that the number of the rules
explodes in that case.
In this paper we will use Bod's subtree-to-
rule conversion method for studying the impact
of various subtree restrictions on the WSJ
corpus. However, we will not use Bod's Monte
Carlo sampling technique from complete
derivation forests, as this turned out to be
prohibitive for WSJ sentences. Instead, we
employ a Viterbi n-best search using a CKY
algorithm and estimate the most probable parse
from the 1,000 most probable derivations,
summing up the probabilities of derivations that
generate the same tree. Although this heuristic
does not guarantee that the most probable parse is
actually found, it is shown in Bod (2000a) to
perform at least as well as the estimation of the
most probable parse with Monte Carlo
techniques. However, in computing the 1,000
most probable derivations by means of Viterbi it
is prohibitive to keep track of all subderivations at
each edge in the chart (at least for such a large
corpus as the WSJ). As in most other statistical
parsing systems we therefore use the pruning
technique described in Goodman (1997) and
Collins (1999: 263-264) which assigns a score to
each item in the chart equal to the product of the
inside probability of the item and its prior
probability. Any item with a score less than 10?5
times of that of the best item is pruned from the
chart.
4 What is the Minimal Subtree Set that
Achieves Maximal Parse Accuracy?
4.1 The base line
For our base line parse accuracy, we used the
now standard division of the WSJ (see Collins
1997, 1999; Charniak 1997, 2000; Ratnaparkhi
1999) with sections 2 through 21 for training
(approx. 40,000 sentences) and section 23 for
testing (2416 sentences ? 100 words); section 22
was used as development set. All trees were
stripped off their semantic tags, co-reference
information and quotation marks. We used all
training set subtrees of depth 1, but due to
memory limitations we used a subset of the
subtrees larger than depth 1, by taking for each
depth a random sample of 400,000 subtrees.
These random subtree samples were not selected
by first exhaustively computing the complete set
of subtrees (this was computationally prohibit -
ive). Instead, for each particular depth > 1 we
sampled subtrees by randomly selecting a node
in a random tree from the training set, after which
we selected random expansions from that node
until a subtree of the particular depth was
obtained. We repeated this procedure 400,000
times for each depth > 1 and ? 14. Thus no
subtrees of depth > 14 were used. This resulted
in a base line subtree set of 5,217,529 subtrees
which were smoothed by the technique described
in Bod (1996) based on Good-Turing. Since our
subtrees are allowed to be lexicalized (at their
frontiers), we did not use a separate part-of-
speech tagger: the test sentences were directly
parsed by the training set subtrees. For words
that were unknown in our subtree set, we
guessed their categories by means of the method
described in Weischedel et al (1993) which uses
statistics on word-endings, hyphenation and
capitalization. The guessed category for each
unknown word was converted into a depth-1
subtree and assigned a probability by means of
simple Good-Turing estimation (see Bod 1998).
The most probable parse for each test sentence
was estimated from the 1,000 most probable
derivations of that sentence, as described in
section 3.
We used "evalb"1 to compute the standard
PARSEVAL scores for our parse results. We
focus on the Labeled Precision (LP) and Labeled
Recall (LR) scores only in this paper, as these are
commonly used to rank parsing systems.
Table 1 shows the LP and LR scores
obtained with our base line subtree set, and
compares these scores with those of previous
stochastic parsers tested on the WSJ (respectively
Charniak 1997, Collins 1999, Ratnaparkhi 1999,
and Charniak 2000).
        The table shows that by using the base line
subtree set, our parser outperforms most
previous parsers but it performs worse than the
parser in Charniak (2000). We will use our
scores of 89.5% LP and 89.3% LR (for test
sentences ? 40 words) as the base line result
against which the effect of various subtree
restrictions is investigated. While most subtree
restrictions diminish the accuracy scores, we will
see that there are restrictions that improve our
scores, even beyond those of Charniak (2000).
1
 http://www.cs.nyu.edu/cs/projects/proteus/evalb/
We will initially study our subtree restrictions
only for test sentences ? 40 words (2245
sentences), after which we will give in 4.6 our
results for all test sentences ? 100 words (2416
sentences). While we have tested all subtree
restrictions initially on the development set
(section 22 in the WSJ), we believe that it is
interesting and instructive to report these subtree
restrictions on the test set (section 23) rather than
reporting our best result only.
Parser LP LR
? 40 words
Char97 87.4 87.5
Coll99 88.7 88.5
Char00 90.1 90.1
Bod00 89.5 89.3
? 100 words
Char97 86.6 86.7
Coll99 88.3 88.1
Ratna99 87.5 86.3
Char00 89.5 89.6
Bod00 88.6 88.3
Table 1. Parsing results with the base line subtree
set compared to previous parsers
4.2 The impact of subtree size
Our first subtree restriction is concerned with
subtree size. We therefore performed experi-
ments with versions of DOP1 where the base
line subtree set is restricted to subtrees with a
certain maximum depth. Table 2 shows the
results of these experiments.
    depth of
    subtrees  LP  LR
  1 76.0 71.8
?2 80.1 76.5
?3 82.8 80.9
?4 84.7 84.1
?5 85.5 84.9
?6 86.2 86.0
?8 87.9 87.1
?10 88.6 88.0
?12 89.1 88.8
?14 89.5 89.3
Table 2. Parsing results for different subtree
depths (for test sentences ? 40 words)
Our scores for subtree-depth 1 are comparable to
Charniak's treebank grammar if tested on word
strings (see Charniak 1997). Our scores are
slightly better, which may be due to the use of a
different unknown word model. Note that the
scores consistently improve if larger subtrees are
taken into account. The highest scores are
obtained if the full base line subtree set is used,
but they remain behind the results of Charniak
(2000). One might expect that our results further
increase if even larger subtrees are used; but due
to memory limitations we did not perform
experiments with subtrees larger than depth 14.
4.3 The impact of lexical context
The more words a subtree contains in its frontier,
the more lexical dependencies can be taken into
account. To test the impact of the lexical context
on the accuracy, we performed experiments with
different versions of the model where the base
line subtree set is restricted to subtrees whose
frontiers contain a certain maximum number of
words; the subtree depth in the base line subtree
set was not constrained (though no subtrees
deeper than 14 were in this base line set). Table 3
shows the results of our experiments.
   # words
  in subtrees  LP  LR
?1 84.4 84.0
?2 85.2 84.9
?3 86.6 86.3
?4 87.6 87.4
?6 88.0 87.9
?8 89.2 89.1
?10 90.2 90.1
?11 90.8 90.4
?12 90.8 90.5
?13 90.4 90.3
?14 90.3 90.3
?16 89.9 89.8
  unrestricted 89.5 89.3
Table 3. Parsing results for different subtree
lexicalizations (for test sentences ? 40 words)
We see that the accuracy initially increases when
the lexical context is enlarged, but that the
accuracy decreases if the number of words in the
subtree frontiers exceeds 12 words. Our highest
scores of 90.8% LP and 90.5% LR outperform
the scores of the best previously published parser
by Charniak (2000) who obtains 90.1% for both
LP and LR. Moreover, our scores also outper-
form the reranking technique of Collins (2000)
who reranks the output of the parser of Collins
(1999) using a boosting method based on
Schapire & Singer (1998), obtaining 90.4% LP
and 90.1% LR. We have thus found a subtree
restriction which does not decrease the parse
accuracy but even improves it. This restriction
consists of an upper bound of 12 words in the
subtree frontiers, for subtrees ? depth 14. (We
have also tested this lexical restriction in
combination with subtrees smaller than depth 14,
but this led to a decrease in accuracy.)
4.4 The impact of structural context
Instead of investigating the impact of lexical
context, we may also be interested in studying the
importance of structural context. We may raise
the question as to whether we need all unlexica-
lized subtrees, since such subtrees do not contain
any lexical information, although they may be
useful to smooth lexicalized subtrees. We accom-
plished a set of experiments where unlexicalized
subtrees of a certain minimal depth are deleted
from the base line subtree set, while all
lexicalized subtrees up to 12 words are retained.
depth of deleted  
  unlexicalized  
     subtrees  LP  LR
  ?1 79.9 77.7
  ?2 86.4 86.1
  ?3 89.9 89.5
  ?4 90.6 90.2
  ?5 90.7 90.6
  ?6 90.8 90.6
  ?7 90.8 90.5
  ?8 90.8 90.5
  ?10 90.8 90.5
  ?12 90.8 90.5
Table 4. Parsing results for different structural
context (for test sentences ? 40 words)
Table 4 shows that the accuracy increases if
unlexicalized subtrees are retained, but that
unlexicalized subtrees larger than depth 6 do not
contribute to any further increase in accuracy. On
the contrary, these larger subtrees even slightly
decrease the accuracy. The highest scores
obtained are: 90.8% labeled precision and 90.6%
labeled recall. We thus conclude that pure
structural context without any lexical information
contributes to higher parse accuracy (even if there
exists an upper bound for the size of structural
context). The importance of structural context is
consonant with Johnson (1998) who showed that
structural context from higher nodes in the tree
(i.e. grandparent nodes) contributes to higher
parse accuracy. This mirrors our result of the
importance of unlexicalized subtrees of depth 2.
But our results show that larger structural context
(up to depth 6) also contributes to the accuracy.
4.5 The impact of nonheadword dependencies
We may also raise the question as to whether we
need almost arbitrarily large lexicalized  subtrees
(up to 12 words) to obtain our best results. It
could be the case that DOP's gain in parse
accuracy with increasing subtree depth is due to
the model becoming sensitive to the influence of
lexical heads higher in the tree, and that this gain
could also be achieved by a more compact model
which associates each nonterminal with its
headword, such as a head-lexicalized SCFG.
Head-lexicalized stochastic grammars have
recently become increasingly popular (see Collins
1997, 1999; Charniak 1997, 2000). These
grammars are based on Magerman's head-
percolation scheme to determine the headword of
each nonterminal (Magerman 1995). Unfortunat-
ely this means that head-lexicalized stochastic
grammars are not able to capture dependency
relations between words that according to
Magerman's head-percolation scheme are
"nonheadwords" -- e.g. between more and than
in the WSJ construction carry more people than
cargo  where neither more  nor than are head-
words of the NP constituent more people than
cargo . A frontier-lexicalized DOP model, on the
other hand, captures these dependencies since it
includes subtrees in which more and than are the
only frontier words. One may object that this
example is somewhat far-fetched, but Chiang
(2000) notes that head-lexicalized stochastic
grammars fall short in encoding even simple
dependency relations such as between left  and
John in the sentence John should have left . This
is because Magerman's head-percolation scheme
makes should  and have  the heads of their
respective VPs so that there is no dependency
relation between the verb left  and its subject John.
Chiang observes that almost a quarter of all
nonempty subjects in the WSJ appear in such a
configuration.
In order to isolate the contribution of
nonheadword dependencies to the parse accuracy,
we eliminated all subtrees containing a certain
maximum number of nonheadwords, where a
nonheadword of a subtree is a word which
according to Magerman's scheme is not a
headword of the subtree's root nonterminal
(although such a nonheadword may of course be
a headword of one of the subtree's internal
nodes). In the following experiments we used the
subtree set for which maximum accuracy was
obtained in our previous experiments, i.e.
containing all lexicalized subtrees with maximally
12 frontier words and all unlexicalized subtrees
up to depth 6.
# nonheadwords  
    in subtrees  LP  LR
    0 89.6 89.6
   ?1 90.2 90.1
   ?2 90.4 90.2
   ?3 90.3 90.2
   ?4 90.6 90.4
   ?5 90.6 90.6
   ?6 90.6 90.5
   ?7 90.7 90.7
   ?8 90.8 90.6
    unrestricted 90.8 90.6
Table 5. Parsing results for different number of
nonheadwords (for test sentences ? 40 words)
Table 5 shows that nonheadwords contribute to
higher parse accuracy: the difference between
using no and all nonheadwords is 1.2% in LP
and 1.0% in LR. Although this difference is
relatively small, it does indicate that nonhead-
word dependencies should preferably not be
discarded in the WSJ. We should note, however,
that most other stochastic parsers do include
counts of single  nonheadwords: they appear in
the backed-off statistics of these parsers (see
Collins 1997, 1999; Charniak 1997; Goodman
1998). But our parser is the first parser that also
includes counts between two or more non-
headwords, to the best of our knowledge, and
these counts lead to improved performance, as
can be seen in table 5.
4.6 Results for all sentences
We have seen that for test sentences ? 40 words,
maximal parse accuracy was obtained by a
subtree set which is restricted to subtrees with not
more than 12 words and which does not contain
unlexicalized subtrees deeper than 6.2 We used
2
 It may be noteworthy that for the development
set (section 22 of WSJ), maximal parse accuracy
was obtained with exactly the same subtree
restrictions. As explained in 4.1, we initially tested
all restrictions on the development set, but we
preferred to report the effects of these restrictions
for the test set.
these restrictions to test our model on all
sentences ? 100 words from the WSJ test set.
This resulted in an LP of 89.7% and an LR of
89.7%. These scores slightly outperform the best
previously published parser by Charniak (2000),
who obtained 89.5% LP and 89.6% LR for test
sentences ? 100 words. Only the reranking
technique proposed by Collins (2000) slightly
outperforms our precision score, but not our
recall score: 89.9% LP and 89.6% LR.
5   Discussion: Converging Approaches
The main goal of this paper was to find the
minimal set of fragments which achieves
maximal parse accuracy in Data Oriented
Parsing. We have found that this minimal set of
fragments is very large and extremely redundant:
highest parse accuracy is obtained by employing
only two constraints on the fragment set: a
restriction of the number of words in the
fragment frontiers to 12 and a restriction of the
depth of unlexicalized fragments to 6. No other
constraints were warranted.
There is an important question why
maximal parse accuracy occurs with exactly these
constraints. Although we do not know the
answer to this question, we surmise that these
constraints differ from corpus to corpus and are
related to general data sparseness effects. In
previous experiments with DOP1 on smaller and
more restricted domains we found that the parse
accuracy decreases also after a certain maximum
subtree depth (see Bod 1998; Sima'an 1999). We
expect that also for the WSJ the parse accuracy
will decrease after a certain depth, although we
have not been able to find this depth so far.
A major difference between our approach
and most other models tested on the WSJ is that
the DOP model uses frontier lexicalization while
most other models use constituent lexicalization
(in that they associate each constituent non -
terminal with its lexical head -- see Collins 1996,
1999; Charniak 1997; Eisner 1997). The results
in this paper indicate that frontier lexicalization is
a promising alternative to constituent lexicaliza-
tion. Our results also show that the linguistically
motivated constraint which limits the statistical
dependencies to the locality of headwords of
constituents is too narrow. Not only are counts of
subtrees with nonheadwords important, also
counts of unlexicalized subtrees up to depth 6
increase the parse accuracy.
The only other model that uses frontier
lexicalization and that was tested on the standard
WSJ split is Chiang (2000) who extracts a
stochastic tree-insertion grammar or STIG
(Schabes & Waters 1996) from the WSJ,
obtaining 86.6% LP and 86.9% LR for sentences
? 40 words. However, Chiang's approach is
limited in at least two respects. First, each
elementary tree in his STIG is lexicalized with
exactly one lexical item, while our results show
that there is an increase in parse accuracy if more
lexical items and also if unlexicalized trees are
included (in his conclusion Chiang acknowledges
that "multiply anchored trees" may be important).
Second, Chiang computes the probability of a
tree by taking into account only one derivation,
while in STIG, like in DOP1, there can be several
derivations that generate the same tree.
Another difference between our approach
and most other models is that the underlying
grammar of DOP is based on a treebank
grammar (cf. Charniak 1996, 1997), while most
current stochastic parsing models use a "markov
grammar" (e.g. Collins 1999; Charniak 2000).
While a treebank grammar only assigns
probabilities to rules or subtrees that are seen in a
treebank, a markov grammar assigns proba-
bilities to any possible rule, resulting in a more
robust model. We expect that the application of
the markov grammar approach to DOP will
further improve our results. Research in this
direction is already ongoing, though it has been
tested for rather limited subtree depths only (see
Sima'an 2000).
Although we believe that our main result is
to have shown that almost arbitrary fragments
within parse trees are important, it is surprising
that a relatively simple model like DOP1
outperforms most other stochastic parsers on the
WSJ. Yet, to the best of our knowledge, DOP is
the only model which does not a priori  restrict
the fragments that are used to compute the most
probable parse. Instead, it starts out by taking into
account all fragments seen in a treebank and then
investigates fragment restrictions to discover the
set of relevant fragments. From this perspective,
the DOP approach can be seen as striving for the
same goal as other approaches but from a dif-
ferent direction. While other approaches usually
limit the statistical dependencies beforehand (for
example to headword dependencies) and then try
to improve parse accuracy by gradually letting in
more dependencies, the DOP approach starts out
by taking into account as many dependencies as
possible and then tries to constrain them without
losing parse accuracy. It is not unlikely that these
two opposite directions will finally converge to
the same, true set of statistical dependencies for
natural language parsing.
As it happens, quite some convergence has
already taken place. The history of stochastic
parsing models shows a consistent increase in the
scope of statistical dependencies that are captured
by these models. Figure 4 gives a (very)
schematic overview of this increase (see Carroll
& Weir 2000, for a more detailed account of a
subsumption lattice where SCFG is at the bottom
and DOP at the top).
  
    
context-free rulesCharniak (1996)
Collins (1996),
Eisner (1996) 
context-free rules,   
headwords
Charniak (1997) context-free rules,
headwords,
grandparent nodes
Collins (2000) context-free rules,   
headwords,
grandparent nodes/rules,   
bigrams, two-level rules,   
two-level bigrams,   
nonheadwords
Bod (1992) all fragments within   
parse trees
Scope of Statistical
Dependencies Model
Figure 4. Schematic overview of the increase of
statistical dependencies by stochastic parsers
Thus there seems to be a convergence towards a
maximalist model which "takes all fragments [...]
and lets the statistics decide" (Bod 1998: 5).
While early head-lexicalized grammars restricted
the fragments to the locality of headwords (e.g.
Collins 1996; Eisner 1996), later models showed
the importance of including context from higher
nodes in the tree (Charniak 1997; Johnson 1998).
This mirrors our result of the utility of
(unlexicalized) fragments of depth 2 and larger.
The importance of including single nonhead-
words is now also uncontroversial (e.g. Collins
1997, 1999; Charniak 2000), and the current
paper has shown the importance of including two
and more nonheadwords. Recently, Collins
(2000) observed that "In an ideal situation we
would be able to encode arbitrary features hs,
thereby keeping track of counts of arbitrary
fragments within parse trees". This is in perfect
correspondence with the DOP philosophy.
References
R. Bod, 1992. Data Oriented Parsing, Proceedings
COLING'92, Nantes, France.
R. Bod, 1993. Using an Annotated Language
Corpus as a Virtual Stochastic Grammar,
Proceedings AAAI'93, Washington D.C.
R. Bod, 1996. Two Questions about Data-Oriented
Parsing, Proceedings 4th Workshop on Very
Large Corpora, COLING'96, Copenhagen,
Denmark.
R. Bod, 1998. Beyond Grammar: An Experience-
Based Theory of Language, Stanford, CSLI
Publications, distributed by Cambridge Uni-
versity Press.
R. Bod, 2000a. Parsing with the Shortest
Derivation, Proceedings COLING'2000,
Saarbr?cken, Germany.
R. Bod, 2000b. Combining Semantic and Syntactic
Structure for Language Modeling, Proceed-
ings ICSLP-2000, Beijing, China.
R. Bod, 2000c. An Improved Parser for Data-
Oriented Lexical-Functional Analysis, Proc-
eedings ACL-2000, Hong Kong, China.
R. Bod, 2001. Using Natural Language Processing
Techniques for Musical Parsing, Proceed-
ings ACH/ALLC'2001, New York, NY.
R. Bod and R. Kaplan, 1998. A Probabilistic
Corpus-Driven Model for Lexical-Functional
Analysis, Proceedings COLING-ACL'98,
Montreal, Canada.
R. Bonnema, P. Buying and R. Scha, 1999. A New
Probability Model for Data-Oriented Parsing,
Proceedings of the Amsterdam Colloqui-
um'99, Amsterdam, Holland.
J. Carroll and D. Weir, 2000. Encoding Frequency
Information in Lexicalized Grammars, in H.
Bunt and A. Nijholt (eds.), Advances in
Probabilistic and Other Parsing Technolo-
gies, Kluwer Academic Publishers.
E. Charniak, 1996. Tree-bank Grammars, Procee-
dings AAAI'96, Menlo Park, Ca.
E. Charniak, 1997. Statistical Parsing with a
Context-Free Grammar and Word Statistics,
Proceedings AAAI-97, Menlo Park, Ca.
E. Charniak, 2000. A Maximum-Entropy-Inspired
Parser. Proceedings ANLP-NAACL'2000,
Seattle, Washington.
D. Chiang, 2000. Statistical parsing with an
automatically extracted tree adjoining
grammar, Proceedings ACL'2000, Hong
Kong, China.
M. Collins  1996. A new statistical parser based on
bigram lexical dependencies, Proceedings
ACL'96, Santa Cruz, Ca.
M. Collins, 1997. Three generative lexicalised
models for statistical parsing, Proceedings
ACL'97, Madrid, Spain.
M. Collins, 1999. Head-Driven Statistical Models
for Natural Language Parsing, PhD thesis,
University of Pennsylvania, PA.
M. Collins, 2000. Discriminative Reranking for
Natural Language Parsing, Proceedings
ICML-2000, Stanford, Ca.
J. Eisner, 1996. Three new probabilistic models for
dependency parsing: an exploration, Proc-
eedings COLING-96, Copenhagen, Denmark.
J. Eisner, 1997. Bilexical Grammars and a Cubic-
Time Probabilistic Parser, Proceedings Fifth
International Workshop on Parsing Techno-
logies, Boston, Mass.
J. Goodman, 1996. Efficient Algorithms for Parsing
the DOP Model, Proceedings Empirical
Methods in Natural Language Processing,
Philadelphia, PA.
J. Goodman, 1997. Global Thresholding and
Multiple-Pass Parsing, Proceedings EMNLP-
2, Boston, Mass.
J. Goodman, 1998. Parsing Inside-Out, Ph.D. thesis,
Harvard University, Mass.
M. Johnson, 1998. PCFG Models of Linguistic
Tree Representations, Computational Ling-
uistics 24(4), 613-632.
D. Magerman, 1995. Statistical Decision-Tree
Models for Parsing, Proceedings ACL'95,
Cambridge, Mass.
M. Marcus, B. Santorini and M. Marcinkiewicz,
1993. Building a Large Annotated Corpus of
English: the Penn Treebank, Computational
Linguistics 19(2).
A. Ratnaparkhi, 1999. Learning to Parse Natural
Language with Maximum Entropy Models,
Machine Learning 34, 151-176.
Y. Schabes and R. Waters, 1996. Stochastic
Lexicalized Tree-Insertion Grammar. In H.
Bunt and M. Tomita (eds.) Recent Advances
in Parsing Technology. Kluwer Academic
Publishers.
R. Schapire and Y. Singer, 1998. Improved
Boosting Algorithms Using Confedence-
Rated Predictions, Proceedings 11th Annual
Conference on Computational Learning
Theory. Morgan Kaufmann, San Francisco.
K. Sima'an, 1999. Learning Efficient Disambig-
uation. PhD thesis, University of Amster-
dam, The Netherlands.
K. Sima'an, 2000. Tree-gram Parsing: Lexical
Dependencies and Structural Relations,
Proceedings ACL'2000, Hong Kong, China.
R. Weischedel, M. Meteer, R, Schwarz, L.
Ramshaw and J. Palmucci, 1993. Coping
with Ambiguity and Unknown Words through
Probabilistic Models, C o m p u t a t i o n a l
Linguistics, 19(2).
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 400?407,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Is the End of Supervised Parsing in Sight? 
 
 
Rens Bod 
School of Computer Science 
University of St Andrews, ILLC, University of Amsterdam 
rb@cs.st-and.ac.uk 
 
 
 
 
Abstract 
How far can we get with unsupervised 
parsing if we make our training corpus 
several orders of magnitude larger than has 
hitherto be attempted? We present a new 
algorithm for unsupervised parsing using 
an all-subtrees model, termed U-DOP*, 
which parses directly with packed forests 
of all binary trees. We train both on Penn?s 
WSJ data and on the (much larger) NANC 
corpus, showing that U-DOP* outperforms 
a treebank-PCFG on the standard WSJ test 
set. While U-DOP* performs worse than 
state-of-the-art supervised parsers on hand-
annotated sentences, we show that the 
model outperforms supervised parsers 
when evaluated as a language model in 
syntax-based machine translation on 
Europarl. We argue that supervised parsers 
miss the fluidity between constituents and 
non-constituents and that in the field of 
syntax-based language modeling the end of 
supervised parsing has come in sight. 
1    Introduction 
 
A major challenge in natural language parsing is 
the unsupervised induction of syntactic structure. 
While most parsing methods are currently 
supervised or semi-supervised (McClosky et al 
2006; Henderson 2004; Steedman et al 2003), they 
depend on hand-annotated data which are difficult 
to come by and which exist only for a few 
languages. Unsupervised parsing methods are 
becoming increasingly important since they 
operate with raw, unlabeled data of which 
unlimited quantities are available. 
There has been a resurgence of interest in 
unsupervised parsing during the last few years. 
Where van Zaanen (2000) and Clark (2001) 
induced unlabeled phrase structure for small 
domains like the ATIS, obtaining around 40% 
unlabeled f-score, Klein and Manning (2002) 
report 71.1% f-score on Penn WSJ part-of-speech 
strings ? 10 words (WSJ10) using a constituent-
context model called CCM. Klein and Manning 
(2004) further show that a hybrid approach which 
combines constituency and dependency models, 
yields 77.6% f-score on WSJ10. 
While Klein and Manning?s approach may 
be described as an ?all-substrings? approach to 
unsupervised parsing, an even richer model 
consists of an ?all-subtrees? approach to 
unsupervised parsing, called U-DOP (Bod 2006). 
U-DOP initially assigns all unlabeled binary trees 
to a training set, efficiently stored in a packed 
forest, and next trains subtrees thereof on a held-
out corpus, either by taking their relative 
frequencies, or by iteratively training the subtree 
parameters using the EM algorithm (referred to as 
?UML-DOP?). The main advantage of an all-
subtrees approach seems to be the direct inclusion 
of discontiguous context that is not captured by 
(linear) substrings. Discontiguous context is 
important not only for learning structural 
dependencies but also for learning a variety of non-
contiguous constructions such as nearest ? to? or 
take ? by surprise. Bod (2006) reports 82.9% 
unlabeled f-score on the same WSJ10 as used by 
Klein and Manning (2002, 2004). Unfortunately, 
his experiments heavily depend on a priori 
sampling of subtrees, and the model becomes 
highly inefficient if larger corpora are used or 
longer sentences are included. 
In this paper we will also test an 
alternative model for unsupervised all-subtrees 
400
parsing, termed U-DOP*, which is based on the 
DOP* estimator by Zollmann and Sima?an (2005), 
and which computes the shortest derivations for 
sentences from a held-out corpus using all subtrees 
from all trees from an extraction corpus. While we 
do not achieve as high an f-score as the UML-DOP 
model in Bod (2006), we will show that U-DOP* 
can operate without subtree sampling, and that the 
model can be trained on corpora that are two 
orders of magnitude larger than in Bod (2006). We 
will extend our experiments to 4 million sentences 
from the NANC corpus (Graff 1995), showing that 
an f-score of 70.7% can be obtained on the 
standard Penn WSJ test set by means of 
unsupervised parsing. Moreover, U-DOP* can be 
directly put to use in bootstrapping structures for 
concrete applications such as syntax-based 
machine translation and speech recognition. We 
show that U-DOP* outperforms the supervised 
DOP model if tested on the German-English 
Europarl corpus in a syntax-based MT system. 
In the following, we first explain the 
DOP* estimator and discuss how it can be 
extended to unsupervised parsing. In section 3, we 
discuss how a PCFG reduction for supervised DOP  
can be applied to packed parse forests. In section 4, 
we will go into an experimental evaluation of U-
DOP* on annotated corpora, while in section 5 we 
will evaluate U-DOP* on unlabeled corpora in an 
MT application.  
 
2     From DOP* to U-DOP* 
 
DOP* is a modification of the DOP model in Bod 
(1998) that results in a statistically consistent 
estimator and in an efficient training procedure 
(Zollmann and Sima?an 2005). DOP* uses the all-
subtrees idea from DOP: given a treebank, take all 
subtrees, regardless of size, to form a stochastic 
tree-substitution grammar (STSG). Since a parse 
tree of a sentence may be generated by several 
(leftmost) derivations, the probability of a tree is 
the sum of the probabilities of the derivations 
producing that tree. The probability of a derivation 
is the product of the subtree probabilities. The 
original DOP model in Bod (1998) takes the 
occurrence frequencies of the subtrees in the trees 
normalized by their root frequencies as subtree 
parameters. While efficient algorithms have been 
developed for this DOP model by converting it into 
a PCFG reduction (Goodman 2003), DOP?s 
estimator was shown to be inconsistent by Johnson 
(2002). That is, even with unlimited training data, 
DOP's estimator is not guaranteed to converge to 
the correct distribution.  
Zollmann and Sima?an (2005) developed a 
statistically consistent estimator for DOP which is 
based on the assumption that maximizing the joint 
probability of the parses in a treebank can be 
approximated by maximizing the joint probability 
of their shortest derivations (i.e. the derivations 
consisting of the fewest subtrees). This assumption 
is in consonance with the principle of simplicity, 
but there are also empirical reasons for the shortest 
derivation assumption: in Bod (2003) and Hearne 
and Way (2006), it is shown that DOP models that 
select the preferred parse of a test sentence using 
the shortest derivation criterion perform very well. 
On the basis of this shortest-derivation 
assumption, Zollmann and Sima?an come up with a 
model that uses held-out estimation: the training 
corpus is randomly split into two parts proportional 
to a fixed ratio: an extraction corpus EC and a 
held-out corpus HC. Applied to DOP, held-out 
estimation would mean to extract fragments from 
the trees in EC and to assign their weights such 
that the likelihood of HC is maximized. If we 
combine their estimation method with Goodman?s 
reduction of DOP, Zollman and Sima?an?s 
procedure operates as follows: 
 
(1) Divide a treebank into an EC and HC 
(2) Convert the subtrees from EC into a PCFG 
reduction 
(3) Compute the shortest derivations for the 
sentences in HC (by simply assigning each 
subtree equal weight and applying Viterbi 
1-best) 
(4) From those shortest derivations, extract the 
subtrees and their relative frequencies in 
HC to form an STSG 
 
Zollmann and Sima?an show that the resulting 
estimator is consistent. But equally important is the 
fact that this new DOP* model does not suffer 
from a decrease in parse accuracy if larger subtrees 
are included, whereas the original DOP model 
needs to be redressed by a correction factor to 
maintain this property (Bod 2003). Moreover, 
DOP*?s estimation procedure is very efficient, 
while the EM training procedure for UML-DOP 
401
proposed in Bod (2006) is particularly time 
consuming and can only operate by randomly 
sampling trees. 
 Given the advantages of DOP*, we  will 
generalize this model in the current paper to 
unsupervised parsing. We will use the same all-
subtrees methodology as in Bod (2006), but now 
by applying the efficient and consistent DOP*-
based estimator. The resulting model, which we 
will call U-DOP*, roughly operates as follows: 
 
(1) Divide a corpus into an EC and HC 
(2) Assign all unlabeled binary trees to the 
sentences in EC, and store them in a 
shared parse forest 
(3) Convert the subtrees from the parse forests 
into a compact PCFG reduction (see next 
section) 
(4) Compute the shortest derivations for the 
sentences in HC (as in DOP*) 
(5) From those shortest derivations, extract the 
subtrees and their relative frequencies in 
HC to form an STSG 
(6) Use the STSG to compute the most 
probable parse trees for new test data by 
means of Viterbi n-best (see next section) 
 
We will use this U-DOP* model to investigate our 
main research question: how far can we get with 
unsupervised parsing if we make our training 
corpus several orders of magnitude larger than 
has hitherto be attempted?  
 
3  Converting shared parse forests into 
PCFG reductions 
 
The main computational problem is how to deal 
with the immense number of subtrees in U-DOP*. 
There exists already an efficient supervised 
algorithm that parses a sentence by means of all 
subtrees from a treebank. This algorithm was 
extensively described in Goodman (2003) and 
converts a DOP-based STSG into a compact PCFG 
reduction that generates eight rules for each node 
in the treebank. The reduction is based on the 
following idea: every node in every treebank tree is 
assigned a unique number which is called its 
address. The notation A@k denotes the node at 
address k where A is the nonterminal labeling that 
node. A new nonterminal is created for each node 
in the training data. This nonterminal is called Ak. 
Let aj represent the number of subtrees headed by 
the node A@j, and let a represent the number of 
subtrees headed by nodes with nonterminal A, that 
is a = ?j aj. Then there is a PCFG with the 
following property: for every subtree in the 
training corpus headed by A, the grammar will 
generate an isomorphic subderivation. For 
example, for a node (A@j (B@k, C@l)), the 
following eight PCFG rules in figure 1 are 
generated, where the number following a rule is its 
weight.  
 
Aj ? BC       (1/aj) A ? BC        (1/a) 
Aj ? BkC      (bk/aj) A ? BkC      (bk/a) 
Aj ? BCl      (cl/aj) A ? BCl         (cl/a) 
Aj ? BkCl     (bkcl/aj) A ? BkCl       (bkcl/a) 
 
Figure 1. PCFG reduction of supervised DOP 
 
By simple induction it can be shown that this 
construction produces PCFG derivations 
isomorphic to DOP derivations (Goodman 2003: 
130-133). The PCFG reduction is linear in the 
number of nodes in the corpus. 
While Goodman?s reduction method was 
developed for supervised DOP where each training 
sentence is annotated with exactly one tree, the 
method can be generalized to a corpus where each 
sentence is annotated with all possible binary trees 
(labeled with the generalized category X), as long 
as we represent these trees by a shared parse forest. 
A shared parse forest can be obtained by adding 
pointers from each node in the chart (or tabular 
diagram) to the nodes that caused it to be placed in 
the chart. Such a forest can be represented in cubic 
space and time (see Billot and Lang 1989). Then, 
instead of assigning a unique address to each node 
in each tree, as done by the PCFG reduction for 
supervised DOP, we now assign a unique address 
to each node in each parse forest for each sentence. 
However, the same node may be part of more than 
one tree. A shared parse forest is an AND-OR 
graph where AND-nodes correspond to the usual 
parse tree nodes, while OR-nodes correspond to 
distinct subtrees occurring in the same context. The 
total number of nodes is cubic in sentence length n. 
This means that there are O(n3) many nodes that 
receive a unique address as described above, to 
which next our PCFG reduction is applied. This is 
a huge reduction compared to Bod (2006) where 
402
the number of subtrees of all trees increases with 
the Catalan number, and only ad hoc sampling 
could make the method work. 
Since U-DOP* computes the shortest 
derivations (in the training phase) by combining 
subtrees from unlabeled binary trees, the PCFG 
reduction in figure 1 can be represented as in 
figure 2, where X refers to the generalized category 
while B and C either refer to part-of-speech 
categories or are equivalent to X. The equal 
weights follow from the fact that the shortest 
derivation is equivalent to the most probable 
derivation if all subtrees are assigned equal 
probability (see Bod 2000; Goodman 2003). 
 
Xj ? BC        1  X ? BC        0.5 
Xj ? BkC      1  X ? BkC       0.5 
Xj ? BCl       1  X ? BCl         0.5 
Xj ? BkCl      1  X ? BkCl       0.5 
 
Figure 2. PCFG reduction for U-DOP* 
 
Once we have parsed HC with the shortest 
derivations by the PCFG reduction in figure 2, we 
extract the subtrees from HC to form an STSG. 
The number of subtrees in the shortest derivations 
is linear in the number of nodes (see Zollmann and 
Sima?an 2005, theorem 5.2). This means that U-
DOP* results in an STSG which is much more 
succinct than previous DOP-based STSGs. 
Moreover, as in Bod (1998, 2000), we use an 
extension of Good-Turing to smooth the subtrees 
and to deal with ?unknown? subtrees. 
Note that the direct conversion of parse 
forests into a PCFG reduction also allows us to 
efficiently implement the maximum likelihood 
extension of U-DOP known as UML-DOP (Bod 
2006). This can be accomplished by training the 
PCFG reduction on the held-out corpus HC by 
means of the expectation-maximization algorithm, 
where the weights in figure 1 are taken as initial 
parameters. Both U-DOP*?s and UML-DOP?s 
estimators are known to be statistically consistent. 
But while U-DOP*?s training phase merely 
consists of the computation of the shortest 
derivations and the extraction of subtrees, UML-
DOP involves iterative training of the parameters. 
Once we have extracted the STSG, we 
compute the most probable parse for new 
sentences by Viterbi n-best, summing up the 
probabilities of derivations resulting in the same 
tree (the exact computation of the most probable 
parse is NP hard ? see Sima?an 1996). We have 
incorporated the technique by Huang and Chiang 
(2005) into our implementation which allows for 
efficient Viterbi n-best parsing.  
 
4    Evaluation on hand-annotated corpora 
 
To evaluate U-DOP* against UML-DOP and other 
unsupervised parsing models, we started out with 
three corpora that are also used in Klein and 
Manning (2002, 2004) and Bod (2006): Penn?s 
WSJ10 which contains 7422 sentences ? 10 words 
after removing empty elements and punctuation, 
the German NEGRA10 corpus and the Chinese 
Treebank CTB10 both containing 2200+ sentences 
? 10 words after removing punctuation. As with 
most other unsupervised parsing models, we train 
and test on p-o-s strings rather than on word 
strings. The extension to word strings is 
straightforward as there exist highly accurate 
unsupervised part-of-speech taggers (e.g. Sch?tze 
1995) which can be directly combined with 
unsupervised parsers, but for the moment we will 
stick to p-o-s strings (we will come back to word 
strings in section 5). Each corpus was divided into 
10 training/test set splits of 90%/10% (n-fold 
testing), and each training set was randomly 
divided into two equal parts, that serve as EC and 
HC and vice versa. We used the same evaluation 
metrics for unlabeled precision (UP) and unlabeled 
recall (UR) as in Klein and Manning (2002, 2004). 
The two metrics of UP and UR are combined by 
the unlabeled f-score F1 = 2*UP*UR/(UP+UR). 
All trees in the test set were binarized beforehand, 
in the same way as in Bod (2006). 
 For UML-DOP the decrease in cross-
entropy became negligible after maximally 18 
iterations. The training for U-DOP* consisted in 
the computation of the shortest derivations for the 
HC from which the subtrees and their relative 
frequencies were extracted. We used the technique 
in Bod (1998, 2000) to include ?unknown? 
subtrees. Table 1 shows the f-scores for U-DOP* 
and UML-DOP against the f-scores for U-DOP 
reported in Bod (2006), the CCM model in Klein 
and Manning (2002), the DMV dependency model 
in Klein and Manning (2004) and their combined 
model DMV+CCM.  
403
 Model English 
(WSJ10) 
German 
(NEGRA10) 
Chinese 
(CTB10) 
CCM 71.9 61.6 45.0 
DMV 52.1 49.5 46.7 
DMV+CCM 77.6 63.9 43.3 
U-DOP 78.5 65.4 46.6 
U-DOP* 77.9 63.8 42.8 
UML-DOP 79.4 65.2 45.0 
 
Table 1. F-scores of U-DOP* and UML-DOP 
compared to other models on the same data. 
 
It should be kept in mind that an exact comparison 
can only be made between U-DOP* and UML-
DOP in table 1, since these two models were tested 
on 90%/10% splits, while the other models were 
applied to the full WSJ10, NEGRA10 and CTB10 
corpora. Table 1 shows that U-DOP* performs 
worse than UML-DOP in all cases, although the 
differences are small and was statistically 
significant only for WSJ10 using paired t-testing. 
As explained above, the main advantage of 
U-DOP* over UML-DOP is that it works with a 
more succinct grammar extracted from the shortest 
derivations of HC. Table 2 shows the size of the 
grammar (number of rules or subtrees) of the two 
models for resp. Penn WSJ10, the entire Penn WSJ 
and the first 2 million sentences from the NANC 
(North American News Text) corpus which 
contains a total of approximately 24 million 
sentences from different news sources. 
 
Model Size of 
STSG 
for WSJ10 
Size of 
STSG 
for Penn 
WSJ 
 
Size of STSG 
for 2,000K 
NANC  
U-DOP* 2.2 x 104 9.8 x 105 7.2 x 106 
UML-DOP 1.5 x 106 8.1 x 107 5.8 x 109 
 
Table 2. Grammar size of U-DOP* and UML-DOP 
for WSJ10 (7,7K sentences), WSJ (50K sentences) 
and the first 2,000K sentences from NANC. 
 
Note that while U-DOP* is about 2 orders of 
magnitudes smaller than UML-DOP for the 
WSJ10, it is almost 3 orders of magnitudes smaller 
for the first 2 million sentences of the NANC 
corpus. Thus even if U-DOP* does not give the 
highest f-score in table 1, it is more apt to be 
trained on larger data sets. In fact, a well-known 
advantage of unsupervised methods over 
supervised methods is the availability of almost 
unlimited amounts of text. Table 2 indicates that 
U-DOP*?s grammar is still of manageable size 
even for text corpora that are (almost) two orders 
of magnitude larger than Penn?s WSJ. The NANC 
corpus contains approximately 2 million WSJ 
sentences that do not overlap with Penn?s WSJ and 
has been previously used by McClosky et al 
(2006) in improving a supervised parser by self-
training. In our experiments below we will start by 
mixing subsets from the NANC?s WSJ data with 
Penn?s WSJ data. Next, we will do the same with 2 
million sentences from the LA Times in the NANC 
corpus, and finally we will mix all data together for 
inducing a U-DOP* model. From Penn?s WSJ, we 
only use sections 2 to 21 for training (just as in 
supervised parsing) and section 23 (?100 words) 
for testing, so as to compare our unsupervised 
results with some binarized supervised parsers. 
The NANC data was first split into 
sentences by means of a simple discriminitive 
model. It was next p-o-s tagged with the the TnT 
tagger (Brants 2000) which was trained on the 
Penn Treebank such that the same tag set was used. 
Next, we added subsets of increasing size from the 
NANC p-o-s strings to the 40,000 Penn WSJ p-o-s 
strings. Each time the resulting corpus was split 
into two halfs and the shortest derivations were 
computed for one half by using the PCFG-
reduction from the other half and vice versa. The 
resulting trees were used for extracting an STSG 
which in turn was used to parse section 23 of 
Penn?s WSJ. Table 3 shows the results. 
 
# sentences added  f-score by 
adding WSJ 
data 
f-score by 
adding LA 
Times data 
0 (baseline) 62.2 62.2 
100k 64.7 63.0 
250k 66.2 63.8 
500k 67.9 64.1 
1,000k 68.5 64.6 
2,000k 69.0 64.9 
 
Table 3. Results of U-DOP* on section 23 from 
Penn?s WSJ by adding sentences from NANC?s 
WSJ and NANC?s LA Times 
 
404
Table 3 indicates that there is a monotonous 
increase in f-score on the WSJ test set if NANC 
text is added to our training data in both cases, 
independent of whether the sentences come from 
the WSJ domain or the LA Times domain. 
Although the effect of adding LA Times data is 
weaker than adding WSJ data, it is noteworthy that 
the unsupervised induction of trees from the LA 
Times domain still improves the f-score even if the 
test data are from a different domain.  
We also investigated the effect of adding 
the LA Times data to the total mix of Penn?s WSJ 
and NANC?s WSJ. Table 4 shows the results of 
this experiment, where the baseline of 0 sentences 
thus starts with the 2,040k sentences from the 
combined Penn-NANC WSJ data. 
 
Sentences added 
from LA Times to 
Penn-NANC WSJ 
f-score by 
adding LA 
Times data 
0 69.0 
100k 69.4 
250k 69.9 
500k 70.2 
1,000k 70.4 
2,000k 70.7 
 
Table 4. Results of U-DOP* on section 23 from 
Penn?s WSJ by mixing sentences from the 
combined Penn-NANC WSJ with additions from 
NANC?s LA Times. 
 
As seen in table 4, the f-score continues to increase 
even when adding LA Times data to the large 
combined set of Penn-NANC WSJ sentences. The 
highest f-score is obtained by adding 2,000k 
sentences, resulting in a total training set of 4,040k 
sentences. We believe that our result is quite 
promising for the future of unsupervised parsing.  
In putting our best f-score in table 4 into 
perspective, it should be kept in mind that the gold 
standard trees from Penn-WSJ section 23 were 
binarized. It is well known that such a binarization 
has a negative effect on the f-score. Bod (2006) 
reports that an unbinarized treebank grammar 
achieves an average 72.3% f-score on WSJ 
sentences ? 40 words, while the binarized version 
achieves only 64.6% f-score. To compare U-
DOP*?s results against some supervised parsers, 
we additionally evaluated a PCFG treebank 
grammar and the supervised DOP* parser using 
the same test set. For these supervised parsers, we 
employed the standard training set, i.e. Penn?s WSJ 
sections 2-21, but only by taking the p-o-s strings 
as we did for our unsupervised U-DOP* model. 
Table 5 shows the results of this comparison. 
 
Parser f-score 
U-DOP* 70.7 
Binarized treebank PCFG 63.5 
Binarized DOP* 80.3 
 
Table 5. Comparison between the (best version of) 
U-DOP*, the supervised treebank PCFG and the 
supervised DOP* for section 23 of Penn?s WSJ 
 
As seen in table 5, U-DOP* outperforms the 
binarized treebank PCFG on the WSJ test set. 
While a similar result was obtained in Bod (2006), 
the absolute difference between unsupervised 
parsing and the treebank grammar was extremely 
small in Bod (2006): 1.8%, while the difference in 
table 5 is 7.2%, corresponding to 19.7% error 
reduction. Our f-score remains behind the 
supervised version of DOP* but the gap gets 
narrower as more training data is being added to 
U-DOP*.  
 
5   Evaluation on unlabeled corpora in a 
practical application 
 
Our experiments so far have shown that despite the 
addition of large amounts of unlabeled training 
data, U-DOP* is still outperformed by the 
supervised DOP* model when tested on hand-
annotated corpora like the Penn Treebank. Yet it is 
well known that any evaluation on hand-annotated 
corpora unreasonably favors supervised parsers. 
There is thus a quest for designing an evaluation 
scheme that is independent of annotations. One 
way to go would be to compare supervised and 
unsupervised parsers as a syntax-based language 
model in a practical application such as machine 
translation (MT) or speech recognition.  
 In Bod (2007), we compared U-DOP* and 
DOP* in a syntax-based MT system known as 
Data-Oriented Translation or DOT (Poutsma 2000; 
Groves et al 2004). The DOT model starts with a 
bilingual treebank where each tree pair constitutes 
an example translation and where translationally 
equivalent constituents are linked. Similar to DOP, 
405
the DOT model uses all linked subtree pairs from 
the bilingual treebank to form an STSG of linked 
subtrees, which are used to compute the most 
probable translation of a target sentence given a 
source sentence (see Hearne and Way 2006).   
What we did in Bod (2007) is to let both 
DOP* and U-DOP* compute the best trees directly 
for the word strings in the German-English 
Europarl corpus (Koehn 2005), which contains 
about 750,000 sentence pairs. Differently from U-
DOP*, DOP* needed to be trained on annotated 
data, for which we used respectively the Negra and 
the Penn treebank. Of course, it is well-known that 
a supervised parser?s f-score decreases if it is 
transferred to another domain: for example, the 
(non-binarized) WSJ-trained DOP model in Bod 
(2003) decreases from around 91% to 85.5% f-
score if tested on the Brown corpus. Yet, this score 
is still considerably higher than the accuracy 
obtained by the unsupervised U-DOP model, 
which achieves 67.6% unlabeled f-score on Brown 
sentences. Our main question of interest is in how 
far this difference in accuracy on hand-annotated 
corpora carries over when tested in the context of a 
concrete application like MT. This is not a trivial 
question, since U-DOP* learns ?constituents? for 
word sequences such as Ich m?chte (?I would like 
to?) and There are (Bod 2007), which are usually 
hand-annotated as non-constituents. While U-
DOP* is punished for this ?incorrect? prediction if 
evaluated on the Penn Treebank, it may be 
rewarded for this prediction if evaluated in the 
context of machine translation using the Bleu score 
(Papineni et al 2002). Thus similar to Chiang 
(2005), U-DOP can discover non-syntactic 
phrases, or simply ?phrases?, which are typically 
neglected by linguistically syntax-based MT 
systems. At the same time, U-DOP* can also learn 
discontiguous constituents that are neglected by 
phrase-based MT systems (Koehn et al 2003). 
In our experiments, we used both U-DOP* 
and DOP* to predict the best trees for the German-
English Europarl corpus. Next, we assigned links 
between each two nodes in the respective trees for 
each sentence pair. For a 2,000 sentence test set 
from a different part of the Europarl corpus we 
computed the most probable target sentence (using 
Viterbi n best). The Bleu score was used to 
measure translation accuracy, calculated by the 
NIST script with its default settings. As a baseline 
we compared our results with the publicly 
available phrase-based system Pharaoh (Koehn et 
al. 2003), using the default feature set. Table 6 
shows for each system the Bleu score together with 
a description of the productive units. ?U-DOT? 
refers to ?Unsupervised DOT? based on U-DOP*, 
while DOT is based on DOP*. 
 
System Productive Units Bleu-score 
U-DOT / U-DOP* Constituents and Phrases 0.280 
DOT / DOP* Constituents only 0.221 
Pharaoh Phrases only 0.251 
 
Table 6. Comparing U-DOP* and DOP* in syntax-
based MT on the German-English Europarl corpus 
against the Pharaoh system. 
 
The table shows that the unsupervised U-DOT 
model outperforms the supervised DOT model 
with 0.059. Using Zhang?s significance tester 
(Zhang et al 2004), it turns out that this difference 
is statistically significant (p < 0.001). Also the 
difference between U-DOT and the baseline 
Pharaoh is statistically significant (p < 0.008). 
Thus even if supervised parsers like DOP* 
outperform unsupervised parsers like U-DOP* on 
hand-parsed data with >10%, the same supervised 
parser is outperformed by the unsupervised parser 
if tested in an MT application. Evidently, U-DOP?s 
capacity to capture both constituents and phrases 
pays off in a concrete application and shows the 
shortcomings of models that only allow for either 
constituents (such as linguistically syntax-based 
MT) or phrases (such as phrase-based MT). In Bod 
(2007) we also show that U-DOT obtains virtually 
the same Bleu score as Pharaoh after eliminating 
subtrees with discontiguous yields. 
 
6    Conclusion: future of supervised parsing 
 
In this paper we have shown that the accuracy of 
unsupervised parsing under U-DOP* continues to 
grow when enlarging the training set with 
additional data. However, except for the simple 
treebank PCFG, U-DOP* scores worse than 
supervised parsers if evaluated on hand-annotated 
data. At the same time U-DOP* significantly 
outperforms the supervised DOP* if evaluated in a 
practical application like MT. We argued that this 
can be explained by the fact that U-DOP learns 
406
both constituents and (non-syntactic) phrases while 
supervised parsers learn constituents only. 
What should we learn from these results? 
We believe that parsing, when separated from a 
task-based application, is mainly an academic 
exercise. If we only want to mimick a treebank or 
implement a linguistically motivated grammar, 
then supervised, grammar-based parsers are 
preferred to unsupervised parsers. But if we want 
to improve a practical application with a syntax-
based language model, then an unsupervised parser 
like U-DOP* might be superior. 
 The problem with most supervised (and 
semi-supervised) parsers is their rigid notion of 
constituent which excludes ?constituents? like the 
German Ich m?chte or the French Il y a. Instead, it 
has become increasingly clear that the notion of 
constituent is a fluid which may sometimes be in 
agreement with traditional syntax, but which may 
just as well be in opposition to it. Any sequence of 
words can be a unit of combination, including non-
contiguous word sequences like closest X to Y. A 
parser which does not allow for this fluidity may 
be of limited use as a language model. Since 
supervised parsers seem to stick to categorical 
notions of constituent, we believe that in the field 
of syntax-based language models the end of 
supervised parsing has come in sight. 
 
Acknowledgements   
Thanks to Willem Zuidema and three anonymous 
reviewers for useful comments and suggestions on 
the future of supervised parsing. 
 
References 
Billot, S. and B. Lang, 1989. The Structure of Shared 
Forests in Ambiguous Parsing. In ACL 1989. 
Bod, R. 1998. Beyond Grammar: An Experience-Based 
Theory of Language, CSLI Publications. 
Bod, R. Parsing with the Shortest Derivation. In 
COLING 2000, Saarbruecken. 
Bod, R. 2003. An efficient implementation of a new 
DOP model. In EACL 2003, Budapest. 
Bod, R. 2006. An All-Subtrees Approach to 
Unsupervised Parsing. In ACL-COLING 2006, 
Sydney. 
Bod, R. 2007. Unsupervised Syntax-Based Machine 
Translation. Submitted for publication. 
Brants, T. 2000. TnT - A Statistical Part-of-Speech 
Tagger. In ANLP 2000. 
Chiang, D. 2005. A Hierarchical Phrase-Based Model 
for Statistical Machine Translation. In ACL 2005, 
Ann Arbor. 
Clark, A. 2001. Unsupervised induction of stochastic 
context-free grammars using distributional clustering. 
In CONLL 2001. 
Goodman, J. 2003. Efficient algorithms for the DOP 
model. In R. Bod, R. Scha and K. Sima'an (eds.). 
Data-Oriented Parsing, CSLI Publications. 
Graff, D. 1995. North American News Text Corpus. 
Linguistic Data Consortium. LDC95T21. 
Groves, D., M. Hearne and A. Way, 2004. Robust Sub-
Sentential Alignment of Phrase-Structure Trees. In 
COLING 2004, Geneva. 
Hearne, M and A. Way, 2006. Disambiguation 
Strategies for Data-Oriented Translation. Proceedings 
of the 11th Conference of the European Association 
for Machine Translation, Oslo. 
Henderson, J. 2004. Discriminative training of a neural 
network statistical parser. In ACL 2004, Barcelona. 
Huang, L. and D. Chiang 2005. Better k-best parsing. In 
IWPT 2005, Vancouver. 
Johnson, M. 2002. The DOP estimation method is 
biased and inconsistent. Computational Linguistics 
28, 71-76. 
Klein, D. and C. Manning 2002. A general constituent-
context model for improved grammar induction. In 
ACL 2002, Philadelphia. 
Klein, D. and C. Manning 2004. Corpus-based 
induction of syntactic structure: models of 
dependency and constituency. ACL 2004, Barcelona. 
Koehn, P., Och, F. J., and Marcu, D. 2003. Statistical 
phrase based translation. In HLT-NAACL 2003. 
Koehn, P. 2005. Europarl: a parallel corpus for 
statistical machine translation. In MT Summit 2005. 
McClosky, D., E. Charniak and M. Johnson 2006. 
Effective self-training for parsing. In HLT-NAACL 
2006, New York. 
Poutsma, A. 2000. Data-Oriented Translation. In 
COLING 2000, Saarbruecken. 
Sch?tze, H. 1995. Distributional part-of-speech tagging. 
In ACL 1995, Dublin. 
Sima'an, K. 1996. Computational complexity of 
probabilistic disambiguation by means of tree 
grammars. In COLING 1996, Copenhagen. 
Steedman, M. M. Osborne, A. Sarkar, S. Clark, R. Hwa, 
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim. 
2003. Bootstrapping statistical parsers from small 
datasets. In EACL 2003, Budapest. 
van Zaanen, M. 2000. ABL: Alignment-Based 
Learning. In COLING 2000, Saarbr?cken. 
Zhang, Y., S. Vogel and A. Waibel, 2004. Interpreting 
BLEU/NIST scores: How much improvement do we 
need to have a better system? Proceedings of the 
Fourth International Conference on Language 
Resources and Evaluation (LREC). 
Zollmann, A. and K. Sima'an 2005. A consistent and 
efficient estimator for data-oriented parsing. Journal 
of Automata, Languages and Combinatorics, Vol. 10 
(2005) Number 2/3, 367-388. 
407
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 1?8,
Prague, Czech Republic, June 2007 c?2007 Association for Computational Linguistics
     A Linguistic Investigation into Unsupervised DOP 
 
 
Rens Bod 
School of Computer Science 
University of St Andrews 
ILLC, University of Amsterdam 
rb@cs.st-and.ac.uk 
 
 
 
Abstract 
 
Unsupervised Data-Oriented Parsing models 
(U-DOP) represent a class of structure 
bootstrapping models that have achieved 
some of the best unsupervised parsing results 
in the literature. While U-DOP was 
originally proposed as an engineering 
approach to language learning (Bod 2005, 
2006a), it turns out that the model has a 
number of properties that may also be of 
linguistic and cognitive interest. In this paper 
we will focus on the original U-DOP model 
proposed in Bod (2005) which computes the 
most probable tree from among the shortest 
derivations of sentences. We will show that 
this U-DOP model can learn both rule-based 
and exemplar-based aspects of language, 
ranging from agreement and movement 
phenomena to discontiguous contructions, 
provided that productive units of arbitrary 
size are allowed. We argue that our results 
suggest a rapprochement between nativism 
and empiricism. 
 
1  Introduction 
 
This paper investigates a number of linguistic and 
cognitive aspects of the unsupervised data-oriented 
parsing framework, known as U-DOP (Bod 2005, 
2006a, 2007). U-DOP is a generalization of the DOP 
model which was originally proposed for supervised 
language processing (Bod 1998). DOP produces and 
analyzes new sentences out of largest and most 
probable subtrees from previously analyzed 
sentences. DOP maximizes what has been called the 
?structural analogy? between a sentence and a corpus 
of previous sentence-structures (Bod 2006b). While 
DOP has been successful in some areas, e.g. in 
ambiguity resolution, there is also a serious 
shortcoming to the approach: it does not account for 
the acquisition of initial structures. That is, DOP 
assumes that the structures of previous linguistic 
experiences are already given and stored in a corpus. 
As such, DOP can at best account for adult language 
use and has nothing to say about language acquisition. 
In Bod (2005, 2006a), DOP was extended to 
unsupervised parsing in a rather straightforward way. 
This new model, termed U-DOP, again starts with the 
notion of tree. But since in language learning we do 
not yet know which trees should be assigned to initial 
sentences, it is assumed that a language learner will 
initially allow (implicitly) for all possible trees and let 
linguistic experience decide which trees are actually 
learned. That is, U-DOP generates a new sentence by 
reconstructing it out of the largest possible and most 
frequent subtrees from all possible (binary) trees of 
previous sentences. This has resulted in state-of-the-
art performance for English, German and Chinese 
corpora (Bod 2007). 
Although we do not claim that U-DOP provides 
any near-to-complete theory of language acquisition, 
we intend to show in this paper that it can learn a 
variety of linguistic phenomena, some of which are 
exemplar-based, such as idiosyncratic constructions, 
others of which are typically viewed as rule-based, 
such as auxiliary fronting and subject-verb agreement. 
We argue that U-DOP can be seen as a 
rapprochement between nativism and empiricism. In 
particular, we argue that there is a fallacy in the 
argument that for syntactic facets to be learned they 
must be either innate or in the input data: they can just 
as well emerge from an analogical process without 
ever hearing the particular facet and without assuming 
that it is hard-wired in the mind.  
In the following section, we will start by 
reviewing the original DOP framework in Bod 
(1998). In section 3 we will show how DOP can be 
1
generalized to language learning, resulting in U-DOP. 
Next, in section 4, we show that the approach can 
learn idiosyncratic constructions. In section 5 we 
discuss how U-DOP can learn agreement phenomena, 
and in section 6 we extend our argument to auxiliary 
movement. We end with a conclusion. 
 
2  Review of ?supervised? DOP 
 
In its original version, DOP derives new sentences by 
combining subtrees from previously derived sentences. 
One of the main motivations behind the DOP 
framework was to integrate rule-based and exemplar-
based aspects of language processing (Bod 1998). A 
simple example may illustrate the approach. Consider 
an extremely small corpus of only two phrase-structure 
trees that are labeled by traditional categories, shown in 
figure 1. 
 
 
the
NPP
on rack
PP
the
NP
dress
NP
V
wanted
VP
NP
she
S
    
the
NPP
with telescope
PP
the
NP
saw dog
VP
V
VP
NP
she
S
 
 
Figure 1. An extremely small corpus of two trees 
 
A new sentence can be derived by combining subtrees 
from the trees in the corpus. The combination 
operation between subtrees is called label 
substitution, indicated as ?. Starting out with the 
corpus of figure 1, for instance, the sentence She saw 
the dress with the telescope may be derived as shown 
in figure 2. 
 
the
NPP
with telescope
PP
NP
saw
VP
V
VP
NP
she
S
the
NP
dress
=
the
NPP
with telescope
PP
the
NP
saw
VP
V
VP
NP
she
S
?
dress
 
Figure 2. Analyzing a new sentence by combining subtrees 
from figure 1 
 
We can also derive an alternative tree structure for 
this sentence, namely by combining three (rather than 
two) subtrees from figure 1, as shown in figure 3. We 
will write (t ? u) ? v  as t ? u ? v with the convention 
that ? is left-associative. 
 
PP
the
NP
dress
NP
V
VP
NP
she
S
saw
V
the
NPP
with telescope
PP =
? ?
PP
the
NP
dress
NP
V
VP
NP
she
S
saw the
NPP
with telescope
 
Figure 3. A different derivation for the same sentence 
 
DOP?s subtrees can be of arbitrary size: they 
can range from context-free rewrite rules to entire 
sentence-analyses. This makes the model sensitive to 
multi-word units, idioms and other idiosyncratic 
constructions, while still maintaining full 
productivity. DOP is consonant with the view, as 
expressed by certain usage-based and constructionist 
accounts in linguistics, that any string of words can 
function as a construction (Croft 2001; Tomasello 
2003; Goldberg 2006; Bybee 2006). In DOP such 
constructions are formalized as lexicalized subtrees, 
which form the productive units of a Stochastic Tree-
Substitution Grammar or STSG. 
Note that an unlimited number of sentences 
can be derived by combining subtrees from the corpus 
in figure 1. However, virtually every sentence 
generated in this way is highly ambiguous, yielding 
several syntactic analyses. Yet, most of these analyses 
do not correspond to the structure humans perceive. 
Initial DOP models proposed an exclusively 
probabilistic metric to rank different candidates, 
where the ?best? tree was computed from the 
frequencies of subtrees in the corpus (see Bod 1998). 
 While it is well known that the frequency of a 
structure is a very important factor in language 
comprehension and production (Jurafsky 2003), it is 
not the only factor. Discourse context, semantics and 
recency also play an important role. DOP can 
straightforwardly take into account discourse and 
semantic information if we have corpora with such 
information from which we take our subtrees, and the 
notion of recency can be incorporated by a frequency-
adjustment function (Bod 1998). There is, however, 
an important other factor which does not correspond 
to the notion of frequency: this is the simplicity of a 
structure (cf. Frazier 1978; Chater 1999). In Bod 
(2002), the simplest structure was formalized by the 
shortest derivation of a sentence consisting of the 
fewest subtrees from the corpus. Note that the shortest 
derivation will include the largest possible subtrees 
from the corpus, thereby maximizing the structural 
overlap between a sentence and previous sentence-
2
structures. Only in case the shortest derivation is not 
unique, the frequencies of the subtrees are used to 
break ties among the shortest derivations. This DOP 
model assumes that language users maximize what 
has been called the structural analogy between a 
sentence and previous sentence-structures by 
computing the most probable tree with largest 
structural overlaps between a sentence and a corpus. 
We will use this DOP  model from Bod (2002) as the 
basis for our investigation of unsupervised DOP. 
 We can illustrate DOP?s notion of structural 
analogy with the examples given in the figures above. 
DOP predicts that the tree structure in figure 2 is 
preferred because it can be generated by just two 
subtrees from the corpus. Any other tree structure, 
such as in figure 3, would need at least three subtrees 
from the training set in figure 1. Note that the tree 
generated by the shortest derivation indeed tends to be 
structurally more similar to the corpus (i.e. having a 
larger overlap with one of the corpus trees) than the 
tree generated by the longer derivation. Had we 
restricted the subtrees to smaller sizes -- for example 
to depth-1 subtrees, which makes DOP equivalent to a 
(probabilistic) context-free grammar -- the shortest 
derivation would not be able to distinguish between 
the two trees in figures 2 and 3 as they would both be 
generated by 9 rewrite rules. 
When the shortest derivation is not unique, we 
use the subtree frequencies to break ties. The ?best 
tree? of a sentence is defined as the most probable tree 
generated by a shortest derivation of the sentence, 
also referred to as ?MPSD?. The probability of a tree 
can be computed from the relative frequencies of its 
subtrees, and the definitions are standard for 
Stochastic Tree-Substitution Grammars (STSGs), see 
e.g. Manning and Sch?tze (1999) or Bod (2002). 
Interestingly, we will see that the exact computation 
of probabilities is not necessary for our arguments in 
this paper. 
 
3  U-DOP: from sentences to structures 
 
DOP can be generalized to language learning by using 
the same principle as before: language users 
maximize the structural analogy between a new 
sentence and previous sentence-structures by 
computing the most probable shortest derivation. 
However, in language learning we cannot assume that 
the correct phrase-structures of previously heard 
sentences are already known. Bod (2005) therefore 
proposed the following generalization of DOP, which 
we will simply refer to as U-DOP: if a language 
learner does not know which syntactic tree should be 
assigned to a sentence, s/he initially allows 
(implicitly) for all possible trees and let linguistic 
experience decide which is the ?best? tree by 
maximizing structural analogy (i.e. the MPSD). 
Although several alternative versions of U-
DOP have been proposed (e.g. Bod 2006a, 2007), we 
will stick to the computation of the MPSD for the 
current paper. Due to its use of the shortest derivation, 
the model?s working can often be predicted without 
any probabilistic computations, which makes it 
especially apt to investigate linguistic facets such as 
auxiliary fronting (see section 6). 
From a conceptual perspective we can 
distinguish three learning phases under U-DOP, 
which we shall discuss in more detail below. 
 
(i) Assign all unlabeled binary trees to a set of 
sentences  
Suppose that a language learner hears the following 
two (?Childes-like?) sentences: watch the dog and the 
dog barks. How could a rational learner figure out the 
appropriate tree structures for these sentences? U-
DOP conjectures that a learner does so by allowing 
any fragment of the heard sentences to form a 
productive unit and to try to reconstruct these 
sentences out of most probable shortest combinations.  
 Consider the set of all unlabeled binary trees for 
the sentences watch the dog and the dog barks given 
in figure 4. Each node in each tree is assigned the 
same category label X, since we do not (yet) know 
what label each phrase will receive.  
 
watch the dog
X
X
     
X
watch the dog
X
 
 
 
the dog
X
X
barks
     
X
X
the dog barks
 
 
Figure 4. The unlabeled binary tree set for watch the dog 
and the dog barks 
 
Although the number of possible binary trees for a 
sentence grows exponentially with sentence length, 
these binary trees can be efficiently represented by 
means of a chart or tabular diagram. By adding 
pointers between the nodes we obtain a structure 
3
known as a shared parse forest (Billot and Lang 
1989).  
 
(ii) Divide the binary trees into all subtrees  
Figure 5 exhaustively lists the subtrees that can be 
extracted from the trees in figure 4. The first subtree 
in each row represents the whole sentence as a chunk, 
while the second and the third are ?proper? subtrees.  
 
watch the dog
X
X
dog
X
X
watch the
X
 
 
X
watch the dog
X
X
watch
X the dog
X
 
 
the dog
X
X
barks
  
X
X
barks
  
the dog
X
 
 
X
X
the dog barks
X
X
the
X
dog barks
 
 
Figure 5. The subtree set for the binary trees in figure 4. 
 
Note that while most subtrees occur once, the subtree 
[the dog]X occurs twice. There exist efficient 
algorithms to convert all subtrees into a compact 
representation (Goodman 2003) such that standard 
best-first parsing algorithms can be applied to the 
model (see Bod 2007). 
 
 (iii) Compute the ?best? tree for each sentence 
Given the subtrees in figure 5, the language learner 
can now induce analyses for a sentence such as the 
dog barks in various ways. The phrase structure [the 
[dog barks]X]X can be produced by two different 
derivations, either by selecting the large subtree that 
spans the whole sentence or by combining two 
smaller subtrees: 
 
X
X
the dog barks
 or     
X
X
the
X
dog barks
o
 
 
Figure 6. Deriving the dog barks from figure 5 
 
Analogously, the competing phrase structure [[the 
dog]X barks]X  can also produced by two derivations: 
 
the dog
X
X
barks
  
or   
X
X
barks
  
the dog
Xo
 
 
Figure 7. Other derivations for the dog barks  
 
Note that the shortest derivation is not unique: the 
sentence the dog barks can be trivially parsed by any 
of its fully spanning trees. Such a situation does not 
usually occur when structures for new sentences are 
learned, i.e. when we induce structures for a held-out 
test set  using all subtrees from all possible trees 
assigned to a training set. For example, the shortest 
derivation for the new ?sentence? watch dog barks is 
unique, given the set of subtrees in figure 5. But in the 
example above we need subtree frequencies to break 
ties, i.e. U-DOP computes the most probable tree 
from among the shortest derivations, the MPSD. The 
probability of a tree is compositionally computed 
from the frequencies of its subtrees, in the same way 
as in the supervised version of DOP (see Bod 1998, 
2002). Since the subtree [the dog]X is the only subtree 
that occurs more than once, we can predict that the 
most probable tree corresponds to the structure [[the 
dog]X barks]X in figure 7 where the dog is a 
constituent. This can also be shown formally, but a 
precise computation is unnecessary for this example.  
 
4  Learning constructions by U-DOP 
 
For the sake of simplicity, we have only considered 
subtrees without lexical labels in the previous section. 
Now, if we also add an (abstract) label to each word 
in figure 4, then a possible subtree would the subtree 
in figure 9, which has a discontiguous yield watch X 
dog, and which we will therefore refer to as a 
?discontiguous subtree?. 
 
X
watch dog
X
X X X
 
 
Figure 9. A discontiguous subtree 
 
Thus lexical labels enlarge the space of dependencies 
covered by our subtree set. In order for U-DOP to 
4
take into account both contiguous and non-contiguous 
patterns, we will define the total tree-set of a sentence 
as the set of all unlabeled trees that are unary at the 
word level and binary at all higher levels. 
 Discontiguous subtrees, like in figure 9, are 
important for acquiring a variety of constructions as 
in (1)-(4): 
 
(1) Show me the nearest airport to Leipzig. 
(2) BA carried more people than cargo in 2005. 
(3) What is this scratch doing on the table? 
(4) Don?t take him by surprise. 
 
These constructions have been discussed at various 
places in the literature, and all of them are 
discontiguous in that the constructions do not appear 
as contiguous word strings. Instead the words are 
separated by ?holes? which are sometimes represented 
by dots as in more ? than ?, or by variables as in 
What is X doing Y (cf. Kay and Fillmore 1999). In 
order to capture the syntactic structure of 
discontiguous constructions we need a model that 
allows for productive units that can be partially 
lexicalized, such as subtrees. For example, the 
construction more ... than ? in (2) can be represented 
by a subtree as in figure 10.  
  
more than
XX
X X
X
X X
 
 
Figure 10. Discontiguous subtree for more...than... 
 
U-DOP can learn the structure in figure 10 from a few 
sentences only, using the mechanism described in 
section 3. While we will go into the details of learning 
discontiguous subtrees in section 5, it is easy to see 
that U-DOP will prefer the structure in figure 10 over 
a structure where e.g. [X than] forms a constituent. 
First note that the substring more X can occur at the 
end of a sentence (in e.g. Can I have more milk?), 
whereas the substring X than cannot occur at the end 
of a sentence. This means that [more X] will be 
preferred as a constituent in [more X than X]. The 
same is the case for than X in e.g. A is cheaper than 
B. Thus both [more X] and [than X] can appear 
separately from the construction and will win out in 
frequency, which means that U-DOP will learn the 
structure in figure 10 for the construction more ? 
than ?. 
 Once it is learned, (supervised) DOP enforces 
the application of the subtree in figure 10 whenever a 
new form using the construction more ... than ... is 
perceived or produced because the particular subtree 
will directly cover it and lead to the shortest 
derivation.  
 
5  Learning agreement by U-DOP 
 
Discontiguous context is important not only for 
learning constructions but also for learning various 
syntactic regularities. Consider the following sentence 
(5):  
 
(5) Swimming in rivers is dangerous 
 
How can U-DOP deal with the fact that human 
language learners will perceive an agreement relation 
between swimming and is, and not between rivers and 
is? We will rephrase this question as follows: which 
sentences must be perceived such that U-DOP can 
assign as the best structure for swimming in rivers is 
dangerous the tree 16(a) which attaches the 
constituent is dangerous to swimming in rivers, and 
not an incorrect tree like 16(b) which attaches is 
dangerous to rivers? Note that tree (a) correctly 
represents the dependency between swimming and is 
dangerous, while tree (b) misrepresents a dependency 
between rivers and is dangerous. 
 
 
swimming is 
X 
X 
X 
X 
X 
X X 
in rivers 
X X 
dangerous 
   
swimming is
XX
X
X
X
X X
in rivers
X X
dangerous
 
 
      (a)     (b) 
 
Figure 16. Two possible trees for Swimming in rivers is 
dangerous 
 
It turns out that we need to observe only one 
additional sentence to overrule tree (b), i.e. sentence 
(6):  
 
(6) Swimming together is fun  
 
The word together can be attached either to swimming 
or to is fun, as illustrated respectively by 17(a) and 
17(b) (of course, together can also be attached to is 
alone, and the resulting phrase together is to fun, but 
our argument still remains valid): 
 
5
swimming is
X
X
X
X
X
X X
together fun
 
swimming is
XX
X
X
X
XX
together fun
 
 
           (a)             (b) 
 
Figure 17. Two possible trees for Swimming together is fun 
 
First note that there is a large common subtree 
between 16(a) and 17(a), as shown in figure 18. 
 
swimming is
X
X
X
X
X
X X
 
 
Figure 18. Common subtree in the trees 16(a) and 17(a) 
 
Next note that there is not such a large common 
subtree between 16(b) and 17(b). Since the shortest 
derivation is not unique (as both trees can be 
produced by directly using the largest tree from the 
binary tree set), we must rely on the frequencies of 
the subtrees. It is easy to see that the trees 16(a) and 
17(a) will overrule respectively 16(b) and 17(b), 
because 16(a) and 17(a) share the largest subtree. 
Although 16(b) and 17(b) also share subtrees, they 
cover a smaller part of the sentence than does the 
subtree in figure 18. Next note that for every common 
subtree between 16(a) and 17(a) there exists a 
corresponding common subtree between 16(b) and 
17(b) except for the common subtree in figure 18 (and 
one of its sub-subtrees by abstracting from 
swimming). Since the frequencies of all subtrees of a 
tree contribute to its probability, if follows that figure 
18 will be part of the most probable tree, and thus 
16(a) and 17(a) will overrule respectively 16(b) and 
17(b). 
 However, our argument is not yet complete: we 
have not yet ruled out another possible analysis for 
swimming in rivers is dangerous where in rivers 
forms a constituent together with is dangerous. 
Interestingly, it suffices to perceive a sentence like 
(7): He likes swimming in river. The occurrence of 
swimming in rivers at the end of this sentence will 
lead to a preference for 16(a) because it will get a 
higher frequency as a group. An implementation of 
U-DOP confirmed our informal argument. 
 We conclude that U-DOP only needs three 
sentences to learn the correct tree structure displaying 
the dependency between the subject swimming and 
the verb is, known otherwise as ?agreement?. Once 
we have learned the correct structure for subject-verb 
agreement by the subtree in figure 18, (U-)DOP 
enforces agreement by the shortest derivation. 
It can also be shown that U-DOP still learns the 
correct agreement if sentences with incorrect 
agreement, like *Swimming in rivers are dangerous, 
are heard as long as the correct agreement has a 
higher frequency than the incorrect agreement during 
the learning process. 
 
6  Learning ?movement? by U-DOP  
 
We now come to what is often assumed to be the 
greatest challenge for models of language learning, 
and what Crain (1991) calls the ?parade case of an 
innate constraint?: the problem of auxiliary 
movement, also known as auxiliary fronting or 
inversion. Let?s start with the typical examples, which 
are similar to those used in Crain (1991), 
MacWhinney (2005), Clark and Eyraud (2006) and 
many others:  
 
(8) The man is hungry 
 
If we turn sentence (8) into a (polar) interrogative, the 
auxiliary is is fronted, resulting in sentence (9). 
 
(9) Is the man hungry? 
 
A language learner might derive from these two 
sentences that the first occurring auxiliary is fronted. 
However, when the sentence also contains a relative 
clause with an auxiliary is, it should not be the first 
occurrence of is that is fronted but the one in the main 
clause, as shown in sentences (11) and (12). 
 
(11) The man who is eating is hungry 
(12) Is the man who is eating hungry? 
 
There is no reason that children should favor the 
correct auxiliary fronting. Yet children do produce the 
correct sentences of the form (12) and rarely if ever of 
the form (13) even if they have not heard the correct 
form before (see Crain and Nakayama 1987). 
 
 (13) *Is the man who eating is hungry? 
 
How can we account for this phenomenon? 
According to the nativist view, sentences of the type 
6
in (12) are so rare that children must have innately 
specified knowledge that allows them to learn this 
facet of language without ever having seen it (Crain 
and Nakayama 1987). On the other hand, it has been 
claimed that this type of sentence is not rare at all and 
can thus be learned from experience (Pullum and 
Scholz 2002). We will not enter the controversy on 
this issue, but believe that both viewpoints overlook a 
very important alternative possibility, namely that 
auxiliary fronting needs neither be innate nor in the 
input data to be learned, but may simply be an 
emergent property of the underlying model. 
 How does (U-)DOP account for this 
phenomenon? We will show that the learning of 
auxiliary fronting can proceed with only two 
sentences: 
 
(14) The man who is eating is hungry  
(15) Is the boy hungry? 
 
Note that these sentences do not contain an example 
of the fact that an auxiliary should be fronted from the 
main clause rather than from the relative clause. 
 For reasons of space, we will have to skip the 
induction of the tree structures for (14) and (15), 
which can be derived from a total of six sentences 
using similar reasoning as in section 5, and which are 
given in figure 20a,b (see Bod forthcoming, for more 
details and a demonstration that the induction of these 
two tree structures is robust).  
 
is
X
X
X
X
X
X
is eating
X X
hungry
X
the man
X X
X
who
X
    
X
X
X
X
X
the boy
X X
is hungry
 
 
           (a)       (b) 
 
Figure 20. Tree structures for the man who is eating is 
hungry and is the boy hungry? learned by U-DOP 
 
Given the trees in figure 20, we can now easily show 
that U-DOP?s shortest derivation produces the correct 
auxiliary fronting, without relying on any probability 
calculations. That is, in order to produce the correct 
interrogative, Is the man who is eating hungry, we 
only need to combine the following two subtrees from 
the acquired structures in figure 20, as shown in 
figure 21 (note that the first subtree is discontiguous): 
 
 
X
 
X
 
X
 
X 
X
 
is hungry 
X 
X 
is
 
eating
 
X X 
X 
the
 
man
 
X X 
X
 
who
 
X 
o 
 
Figure 21. Producing the correct auxiliary fronting by 
combining two subtrees from figure 20 
 
On the other hand, to produce the sentence with 
incorrect auxiliary fronting *Is the man who eating is 
hungry? we need to combine many more subtrees 
from figure 20. Clearly the derivation in figure 21 is 
the shortest one and produces the correct sentence, 
thereby blocking the incorrect form.1 
Thus the phenomenon of auxiliary fronting 
needs neither be innate nor in the input data to be 
learned. By using the notion of shortest derivation, 
auxiliary fronting can be learned from just a couple 
sentences only. Arguments about frequency and 
?poverty of the stimulus? (cf. Crain 1991; 
MacWhinney 2005) are therefore irrelevant ? 
provided that we allow our productive units to be of 
arbitrary size. (Moreover, learning may be further 
eased once the syntactic categories have been 
induced. Although we do not go into category 
induction in the current paper, once unlabeled 
structures have been found, category learning turns 
out to be a relatively easy problem). 
 Auxiliary fronting has been previously dealt 
with in other probabilistic models of structure 
learning. Perfors et al (2006) show that Bayesian 
model selection can choose the right grammar for 
auxiliary fronting. Yet, their problem is different in 
that Perfors et al start from a set of given grammars 
from which their selection model has to choose the 
correct one. Our approach is more congenial to Clark 
and Eyraud (2006) who show that by distributional 
analysis in the vein of Harris (1954) auxiliary fronting 
can be correctly predicted. However, different from 
Clark and Eyraud, we have shown that U-DOP can 
also learn complex, discontiguous constructions. In 
order to learn both rule-based phenomena like 
auxiliary inversion and exemplar-based phenomena 
like idiosyncratic constructions, we believe we need 
                                                 
1
  We are implicitly assuming here an extension of DOP 
which computes the most probable shortest derivation given 
a certain meaning to be conveyed. This semantic DOP 
model was worked out in Bonnema et al (1997) where the 
meaning of a sentence was represented by its logical form. 
7
the richness of a probabilistic tree grammar rather 
than a probabilistic context-free grammar. 
 
7  Conclusion 
 
We have shown that various syntactic phenomena can 
be learned by a model that only assumes (1) the 
notion of recursive tree structure, and (2) an 
analogical matching algorithm which reconstructs a 
new sentence out of largest and most frequent 
fragments from previous sentences. The major 
difference between our model and other 
computational learning models (such as Klein and 
Manning 2005 or Clark and Eyraud 2006) is that we 
start with trees. But since we do not know which trees 
are correct, we initially allow for all of them and let 
analogy decide. Thus we assume that the language 
faculty (or ?Universal Grammar?) has knowledge 
about the notion of tree structure but no more than 
that. Although we do not claim that we have 
developed any near-to-complete theory of all 
language acquisition, our argument to use only 
recursive structure as the core of language knowledge 
has a surprising precursor. Hauser, Chomksy and 
Fitch (2002) claim that the core language faculty 
comprises just ?recursion? and nothing else. If one 
takes this idea seriously, then U-DOP is probably the 
first fully computational model that instantiates it: U-
DOP?s trees encode the ultimate notion of recursion 
where every label can be recursively substituted for 
any other label. All else is analogy. 
 
References 
Billot, S. and B. Lang, 1989. The Structure of Shared 
Forests in Ambiguous Parsing. Proceedings ACL 1989. 
Bod, R. 1998. Beyond Grammar. Stanford: CSLI 
Publications. 
Bod, R. 2002. A Unified Model of Structural Organization 
in Language and Music, Journal of Artificial Intelligence 
Research, 17, 289-308. 
Bod, R. 2005. Combining Supervised and Unsupervised 
Natural Language Processing. The 16th Meeting of 
Computational Linguistics in the Netherlands (CLIN 
2005). 
Bod, R. 2006a. An All-Subtrees Approach to Unsupervised 
Parsing. Proceedings ACL-COLING 2006, 865-872. 
Bod, R. 2006b. Exemplar-Based Syntax: How to Get 
Productivity from Examples. The Linguistic Review 23, 
291-320. 
Bod, 2007. Is the End of Supervised Parsing in Sight?. 
Proceedings ACL 2007, Prague. 
Bod, forthcoming. From Exemplar to Grammar: How 
Analogy Guides Language Acquisition. In J. Blevins and 
J. Blevins (eds.) Analogy in Grammar, Oxford 
University Press. 
Bonnema, R., R. Bod and R. Scha, 1997. A DOP Model for 
Semantic Interpretation. Proceedings ACL/EACL 1997, 
Madrid, Spain, 159-167. 
Bybee, J. 2006. From Usage to Grammar: The Mind?s 
Response to Repetition. Language 82(4), 711-733. 
Chater, N. 1999. The Search for Simplicity: A Fundamental 
Cognitive Principle? The Quarterly Journal of 
Experimental Psychology, 52A(2), 273-302. 
Clark, A. and R. Eyraud, 2006. Learning Auxiliary 
Fronting with Grammatical Inference. Proceedings 
CONLL 2006, New York. 
Crain, S. 1991. Language Acquisition in the Absence of 
Experience. Behavorial and Brain Sciences 14, 597-612. 
Crain, S. and M. Nakayama, 1987. Structure Dependence 
in Grammar Formation. Language 63, 522-543. 
Croft, B. 2001. Radical Construction Grammar. Oxford 
University Press. 
Frazier, L. 1978. On Comprehending Sentences: Syntactic 
Parsing Strategies. PhD. Thesis, U. of Connecticut. 
Goldberg, A. 2006. Constructions at Work: the nature of 
generalization in language. Oxford University Press. 
Goodman, J. 2003. Efficient algorithms for the DOP 
model. In R. Bod, R. Scha and K. Sima'an (eds.). Data-
Oriented Parsing, CSLI Publications, 125-146. 
Harris, Z. 1954. Distributional Structure. Word 10, 146-
162. 
Hauser, M., N. Chomsky and T. Fitch, 2002. The Faculty 
of Language: What Is It, Who Has It, and How Did It 
Evolve?, Science 298, 1569-1579. 
Jurafsky, D. 2003. Probabilistic Modeling in 
Psycholinguistics. In Bod, R., J. Hay and S. Jannedy 
(eds.), Probabilistic Linguistics, The MIT Press, 39-96. 
Kay, P. and C. Fillmore 1999. Grammatical constructions 
and linguistic generalizations: the What's X doing Y? 
construction. Language, 75, 1-33. 
Klein, D. and C. Manning 2005. Natural language grammar 
induction with a generative constituent-context model. 
Pattern Recognition 38, 1407-1419. 
MacWhinney, B. 2005. Item-based Constructions and the 
Logical Problem. Proceedings of the Second Workshop 
on Psychocomputational Models of Human Language 
Acquisition, Ann Arbor. 
Manning, C. and H. Sch?tze 1999. Foundations of 
Statistical Natural Language Processing. The MIT Press. 
Perfors, A., Tenenbaum, J., Regier, T. 2006. Poverty of the 
Stimulus? A rational approach. Proceedings 28th Annual 
Conference of the Cognitive Science Society. Vancouver 
Pullum, G. and B. Scholz 2002. Empirical assessment of 
stimulus poverty arguments. The Linguistic Review 19, 
9-50. 
Tomasello, M. 2003. Constructing a Language. Harvard 
University Press. 
8
Proceedings of the EACL 2012 Workshop on Computational Models of Language Acquisition and Loss, pages 10?18,
Avignon, France, April 24 2012. c?2012 Association for Computational Linguistics
Empiricist Solutions to Nativist Puzzles by means of Unsupervised TSG 
 
 
Rens Bod Margaux Smets 
Institute for Logic, Language & Computation Institute for Logic, Language & Computation 
University of Amsterdam University of Amsterdam 
Science Park 904, 1098XH Amsterdam, NL Science Park 904, 1098XH Amsterdam, NL 
rens.bod@uva.nl margauxsmets@gmail.com 
 
 
 
 
 
Abstract 
While the debate between nativism and em-
piricism exists since several decades, sur-
prisingly few common learning problems 
have been proposed for assessing the two 
opposing views. Most empiricist researchers 
have focused on a relatively small number of 
linguistic problems, such as Auxiliary Front-
ing or Anaphoric One. In the current paper 
we extend the number of common test cases 
to a much larger series of problems related 
to wh-questions, relative clause formation, 
topicalization, extraposition from NP and 
left dislocation. We show that these hard 
cases can be empirically solved by an unsu-
pervised tree-substitution grammar inferred 
from child-directed input in the Adam cor-
pus (Childes database). 
 
1 Nativism versus Empiricism 
How much knowledge of language is innate and 
how much is learned through experience? The na-
tivist view endorses that there is an innate lan-
guage-specific component and that human 
language acquisition is guided by innate rules and 
constraints (?Universal Grammar?). The empiricist 
view assumes that there is no language-specific 
component and that language acquisition is the 
product of abstractions from empirical input by 
means of general cognitive capabilities. Despite 
the apparent opposition between these two views, 
the essence of the debate lies often in the relative 
contribution of prior knowledge and linguistic ex-
perience (cf. Lidz et al 2003; Clark and Lappin 
2011; Ambridge & Lieven 2011). Following the 
nativist view, the linguistic evidence is so hope-
lessly underdetermined that innate components are 
necessary. This Argument from the Poverty of the 
Stimulus can be phrased as follows (see Pullum & 
Scholz 2002 for a detailed discussion): 
 
(i) Children acquire a certain linguistic phe-
nomenon 
(ii) The linguistic input does not give enough 
evidence for acquiring the phenomenon 
(iii) There has to be an innate component for 
the phenomenon 
 
In this paper we will falsify step (ii) for a large 
number of linguistic phenomena that have been 
considered ?parade cases? of innate constraints 
(Crain 1991; Adger 2003; Crain and Thornton 
2006). We will show that even if a linguistic phe-
nomenon is not in a child?s input, it can be learned 
by an ?ideal? learner from a tiny fraction of child-
directed utterances, namely by combining frag-
ments from these utterances using the Adam cor-
pus in the Childes database (MacWhinney 2000). 
Previous work on empirically solving na-
tivist puzzles, focused on a relatively small set of 
phenomena such as auxiliary fronting (Reali & 
Christiansen 2005; Clark and Eyraud 2006) and 
Anaphoric One (Foraker et al 2009). Some of the 
proposed solutions were based on linear models, 
such as trigram models (Reali & Christiansen 
2005), though Kam et al (2008) showed that the 
success of these models depend on accidental Eng-
lish facts. Other empiricist approaches have taken 
the notion of structural dependency together with a 
10
combination operation as minimal requirements 
(e.g. Bod 2009), which overcomes the problems 
raised by Kam et al (2008). Yet, it remains an 
open question which of the many other syntactic 
phenomena in the nativist literature can be ac-
quired by such a general learning method on the 
basis of child-directed speech.  
In this paper we will deal with a much lar-
ger set of problems than used before in empiricist 
computational models. These problems are well-
known in the generativist literature (e.g. Ross 
1967; Adger 2003; Borsley 2004) and are related 
to wh-questions, relative clause formation, topical-
ization, extraposition and left dislocation. It turns 
out that these hard cases can be learned by a simple 
unsupervised grammar induction algorithm that 
returns the sentence with the best-ranked deriva-
tion for a particular phenomenon, using only a very 
small fraction of the input a child receives. 
2 Methodology 
Our methodology is very simple: by means of an 
induced Tree-Substitution Grammar or TSG (see 
Bod 2009 for an in-depth study), we compute from 
the alternative sentences of a syntactic phenome-
non reported in the generativist literature -- see 
below -- the sentence with the best-ranked shortest 
derivation (see Section 3) according to the unsu-
pervised TSG. Next, we check whether this sen-
tence corresponds with the grammatical sentence. 
For example, given a typical nativist prob-
lem like auxiliary fronting, the question is: how do 
we choose the correct sentence from among the 
alternatives (0) to (2): 
 
(0) Is the boy who is eating hungry? 
(1) *Is the boy who eating is hungry? 
(2) *Is the boy who is eating is hungry? 
 
According to Adger (2003), Crain (1991) and oth-
ers, this phenomenon is regulated by an innate 
principle. In our empiricist approach, instead, we 
parse all three sentences by our TSG. Next, the 
sentence with the best-ranked shortest derivation is 
compared with the grammatical expression.  
Ideally, rather than selecting from given 
sentences, we would like to have a model that 
starts with a certain meaning representation for 
which next the best sentence is generated. In the 
absence of such a semantic component, we let our 
model select directly from the set of possible sen-
tences as they are provided in the literature as al-
ternatives, where we will mostly focus on the 
classical work by Ross (1967) supplemented by the 
more recent work of Adger (2003) and Borsley 
(2004). In section 9 we will discuss the shortcom-
ings of our approach and suggest some improve-
ments for future research. 
3 Grammar induction with TSG: the 
best-ranked k-shortest derivation 
For our induced grammar, we use the formalism of 
Tree-Substitution Grammar. This formalism has 
recently generated considerable interest in the field 
of grammar induction (e.g. Bod 2006; O?Donnell 
et al 2009; Post and Gildea 2009; Cohn et al 
2010). As noted by Cohn et al (2010) and others, 
this formalism has a number of advantages. For 
example, its productive units (elementary trees of 
arbitrary size) allow for both structural and lexical 
sensitivity (see Bod et al 2003), while grammars in 
this formalism are still efficiently learnable from a 
corpus of sentences in cubic time and space. 
 As an example, figure 1 gives two TSG 
derivations and parse trees for the sentence She 
saw the dress with the telescope. Note that the first 
derivation corresponds to the shortest derivation, 
as it consists of only two elementary trees. 
 
the
NPP
with telescope
PP
NP
saw
VP
V
VP
NP
she
S
the
NP
dress
=
the
NPP
with telescope
PP
the
NP
saw
VP
V
VP
NP
she
S?
dress
 
 
PP
the
NP
dress
NP
V
VP
NP
she
S
saw
V
the
NPP
with telescope
PP =? ?
PP
the
NP
dress
NP
V
VP
NP
she
S
saw the
NPP
with telescope
 
Figure 1. Two TSG derivations, resulting in differ-
ent parse trees, for the sentence She saw the dress 
with the telescope 
 
Our induction algorithm is similar to Bod 
(2006) where first, all binary trees are assigned to a 
set of sentences, and next, the relative frequencies 
of the subtrees in the binary trees (using a PCFG 
11
reduction, see below) are used to compute the most 
probable trees. While we will use Bod?s  method of 
assigning all binary trees to a set of sentences, we 
will not compute the most probable tree or sen-
tence. Instead we compute the k-shortest deriva-
tions for each sentence after which the sum of 
ranks of the subtrees in the k derivations deter-
mines the best-ranked shortest derivation (Bod 
2000). This last step is important, since the shortest 
derivation alone is known to perform poorly 
(Bansal and Klein 2011). In Zollmann and Sima?an 
(2005) it is shown that training by means of short-
est derivations corresponds to maximum likelihood 
training in the limit if the corpus grows to infinity. 
 Our approach to focus on the k shortest deri-
vation rather than the most probable tree or most 
probable sentence is partly motivated by our dif-
ferent task: it is well-known that the probability of 
a sentence decreases exponentially with sentence 
length. This is problematic since, when choosing 
among alternative sentences, the longest sentence 
may be (the most) grammatical. Instead, by focus-
ing on the (k-) shortest derivations this problem 
can ? at least partly ? be overcome.  
From an abstract level, our grammar induction 
algorithm works as follows (see also Zollmann and 
Sima?an 2005). Given a corpus of sentences: 
 
1. Divide the corpus into a 50% Extraction Cor-
pus (EC) and a 50% Held out Corpus (HC). 
2. Assign all unlabeled binary trees to the sen-
tences in EC and store them in a parse forest. 
3. Convert the subtrees from the parse forests 
into a compact PCFG reduction (Goodman 
2003). 
4. Compute the k-shortest derivations for the 
sentences in HC using the PCFG reduction. 
5. Compute the best-ranked derivation for each 
sentence by the sum of the ranks of the sub-
trees (where the most frequent subtrees get 
rank 1, next most frequent subtrees get rank 2, 
etc., thus the best-ranked derivation is the one 
with the lowest total score). 
6. Use the subtrees in the trees generated by the 
best-ranked derivations to form the TSG (fol-
lowing Zollmann & Sima?an 2005). 
 
The learning algorithm above does not induce 
POS-tags. In fact, in our experiments below we test 
directly on POS-strings. This makes sense because 
the nativist constraints are also defined on catego-
ries of words, and not on specific sentences. Of 
course, future work should also parse directly with 
word strings instead of with POS strings (for which 
unsupervised POS-taggers may be used). 
Rather than using the (exponentially many) 
subtrees from the binary trees to construct our TSG, 
we convert them into a more compact homomor-
phic PCFG. We employ Goodman?s reduction 
method where each node in a tree is converted into 
exactly 8 PCFG rules (Goodman 2003). This 
PCFG reduction is linear in the number of nodes in 
the corpus (Goodman 2003, pp. 130-133). 
        The k-shortest derivations can be computed 
by Viterbi by assigning each elementary tree equal 
probability (Bod 2000). We follow the third algo-
rithm in Huang and Chiang (2005), where first a 
traditional Viterbi-chart is created, which enumer-
ates in an efficient way all possible subderivations. 
Next, the algorithm starts at the root node and re-
cursively looks for the k-best derivations, where 
we used k = 100. In addition, we employed the 
size reduction technique developed in Teichmann 
(2011) for U-DOP/TSG. 
We used all 12K child-directed utterances in 
the Adam corpus from the Childes database 
(MacWhinney 2000). These utterances come with 
POS-tags, which were stripped off the sentences 
and fed to our TSG induction algorithm. The child-
directed sentences were randomly split into 50% 
EC and 50% HC. The subtrees from EC were used 
to derive a TSG for the POS-strings from HC. The 
resulting TSG consisted of 914,744 different sub-
trees. No smoothing was used. With the methodol-
ogy explained in Section 2, we used this TSG to 
test against a number of well-known nativist prob-
lems from the literature (Ross 1967; Adger 2003). 
It may be important to stress that the Adam 
corpus is based on only 2 hours of recordings per 
fortnight. This corresponds to just a tiny fraction of 
the total number of utterances heard by Adam. 
Thus our TSG has access only to this very small 
fraction of Adam?s linguistic input, and we do not 
assume that our model (let alne a child) literally 
stores all previously heard utterances. 
4 The problem of wh-questions 
The study of wh-questions or wh-movement is one 
of oldest in syntactic theory (Ross 1967) and is 
usually dealt with by a specific set of ?island con-
straints?, where islands are constituents out of 
12
which wh-elements cannot move. These con-
straints are incorporated in the more recent Mini-
malist framework (Adger 2003, pp. 389ff). Of 
course, our goal is different from Minimalism (or 
generative grammar in general). Rather than trying 
to explain the phenomenon by separate constraints, 
we try to model them by just one, more general 
constraint: the best-ranked (k-shortest) derivation. 
We do not intend to show that the constraints pro-
posed by Ross, Adger and others are incorrect. We 
want to demonstrate that these constraints can also 
be modeled by a more general principle. Addition-
ally, we intend to show that the phenomena related 
to wh-questions can be modeled by using only a 
tiny fraction of child-directed speech. 
 
4.1 Unbounded scope of wh-questions 
 
First of all we must account for the seemingly un-
bounded scope of wh-movement: wh-questions can 
have infinitely deep levels of embedding. The puz-
zle lies in the fact that children only hear construc-
tions of level 1, e.g. (3), but how then is it possible 
that they can generalize (certainly as adults) this 
simple construction to more complex ones of lev-
els 2 and 3 (e.g. (4) and (5))? 
 
(3) who did you steal from? 
(4) who did he say you stole from? 
(5) who did he want her to say you stole from? 
 
The initial nativist answer developed by Ross 
(1967) was to introduce a transformational rule 
with variables, and in the more recent Minimalist 
framework it is explained by a complex interplay 
between the so-called Phase Impenetrability Con-
straint and the Feature Checking Requirement 
(Adger 2003). 
 Our model proposes instead to build con-
structions like (4) and (5) by simply using frag-
ments children heard before. When we let our 
induced TSG parse sentence (3), we obtain the fol-
lowing derivation consisting of 3 subtrees (where 
the operation ?o? stands for leftmost node substitu-
tion of TSG-subtrees). For reasons of space, we 
represent the unlabeled subtrees by squared brack-
ets, and for reasons of readability we substitute the 
POS-tags with the words. (As mentioned above we 
trained and tested only with POS-strings.) 
 
[X [who [X [did X]]] o [X [X from]] o [X [you 
steal]] = 
 
[X [who [X [did [X [[X [you steal]]  from]]]]] 
 
Although this derivation is not the shortest one in 
terms of number of subtrees, it obtained the best 
ranking (sum of subtree ranks) among the 100-
shortest derivations. In fact, the derivation above 
consists of three highly frequent subtrees with (re-
spective) ranking of 1,153 + 7 + 488 = 1,648. The 
absolute shortest derivation (k=1) consisted of only 
one subtree (i.e. the entire tree) but had a ranking 
of 26,223. 
 Sentences (4) and (5) could also be parsed 
by combinations of three subtrees, which in this 
case were also the shortest derivations. The follow-
ing is the shortest derivation for (4): 
 
[X [who [X [did he say X]]] o [X [X from]] o [X 
[you stole]] = 
 
[X [who [X [did he say [X [[X [you stole]]  
from]]]]] 
 
It is important to note that when looking at the 
speech produced by Adam himself, he only pro-
duced (3) but not (4) and (5) ? and neither had he 
heard these sentences as a whole. It thus turns out 
that our induced TSG can deal with the presumed 
unbounded scope of wh-questions on the basis of 
simple combination of fragments heard before.  
 
4.2 Complex NP constraint 
 
The first constraint-related problem we deal with is 
the difference in grammaticality between sentences 
(4), (5) and (6), (7): 
 
(6) *who did you he say stole from? 
(7) * who did you he want her to say stole from? 
 
The question usually posed is: how do children 
know that they can generalize from what they hear 
in sentence (3) to sentences (4) and (5) but not to 
(6) and (7). This phenomenon is dealt with in gen-
erative grammar by introducing a specific restric-
tion: the complex NP constraint (see Adger 2003). 
But we can also solve it by the best-ranked deriva-
tion. To do so, we compare sentences with the 
same level of embedding, i.e. (4) and (6), both of 
13
level 2, and (5) and (7), of level 3. We thus view 
respectively (4), (6) and (5), (7) as competing ex-
pressions. 
 It turns out that (6) like (4) can be derived 
by minimally 3 subtrees, but with a worse ranking 
score. Similarly, (7) can also be derived by mini-
mally 3 subtrees with a worse ranking score than 
(5). Since we tested on POS-strings, the result 
holds not only for these sentences of respective 
levels 2 and 3, but for all sentences of this type. 
Thus rather than assuming that the complex NP 
constraint must be innate, it can be modelled by 
recombining fragments from a fraction of previous 
utterances on the basis of the best-ranked deriva-
tion. 
 
4.3 Left branch condition 
 
The second wh-phenomenon we will look into is 
known as the Left Branch Condition (Ross 1967; 
Adger 2003). This condition has to do with the 
difference in grammaticality between (8) and (9): 
 
(8) which book did you read? 
(9) *which did you read book? 
 
When we let our TSG parse these two sentences, 
we get the respective derivations (8?) and (9?), 
where for reasons of readability we now give the 
substree-yields only: 
 
(8?)  [X you read] o [which X] o [book did] 
 
ranking: 608 + 743 + 8,708 = 10,059 
 
(9?)  [which did X] o [you read book] 
 
ranking: 12,809 + 1 = 12,810 
 
Here we thus have a situation that, when looking at 
the 100-best derivations, the subtree ranking over-
rules the shortest derivation: although (9?) is 
shorter than (8?), the rank of (8?) nevertheless 
overrules (9?), leading to the correct alternative. Of 
course, it has to be seen whether this perhaps coin-
cidentally positive result can be confirmed on other 
child-directed corpora. 
 
4.4 Subject wh-questions 
 
An issue that is not considered in early work on 
wh-questions (such as Ross 1967), but covered in 
the minimalist framework is the phenomenon that 
arises with subject wh-questions. We have to ex-
plain how children know that (10) is the grammati-
cal way of asking the particular question, and (11), 
(12) and (13) are not. 
 
(10)  who kissed Bella 
(11) *kissed who Bella 
(12) *did who kiss Bella 
(13) *who did kiss Bella 
 
When we let our model parse these sentences, we 
obtain the following four derivations (where we 
give again only the subtree-yields): 
 
(10?)  [who X] o [kissed Bella] 
 
ranking: 22 + 6,694 = 6,716 
 
(11?)  [X Bella] o [kissed who] 
 
ranking: 24 + 6,978 = 7,002 
 
(12?)  [did X Bella] o [who kiss] 
 
ranking: 4,230 + 8,527 = 12,757 
 
(13?)  [X kiss Bella] o [who did] 
 
ranking: 4,636 + 2,563 = 7,199 
 
Although all derivations are equally short, the best 
(= lowest) ranking score prefers the correct alterna-
tive. 
 
4.5 Other wh-constraints modelled empirically  
 
Besides the constraints given above, there are vari-
ous other constraints related to wh-questions. 
These include:  
 
? Sentential Subject Constraint 
? WH-questions in situ 
? Embedded WH-questions 
? WH-islands  
? Superiority 
? Coordinate Structure Constraint 
 
All but one of these constraints could be correctly 
modelled by our TSG, preferring the correct alter-
native on the basis of the best-ranked derivation 
and a fraction of a child?s input. The only excep-
14
tion is the Coordinate Structure Constraint, as in 
(14) and (15): 
 
(14) you love chicken and what? 
(15) *what do you love chicken and? 
 
Contrary to the ungrammaticality of (15), our TSG 
parser assigned the best rank to the derivation of 
(15). Of course it has to be seen how our TSG 
would perform on a corpus that is larger than 
Adam. Moreover, we will see that our TSG can 
correctly model the Coordinate Structure Con-
straint for other phenomena, even on the basis of 
the Adam corpus. 
5 The problem of Relative clause formation 
A phenomenon closely related to wh-questions is 
relative clause formation. As in 4.2, generativ-
ist/nativist approaches use the same complex NP 
constraint to distinguish between the grammatical 
sentence (16) and the ungrammatical sentence 
(17). The complex NP constraint is in fact believed 
to be universal.  
 
(16) the vampire who I read a book about is dan-
gerous 
(17) *the vampire who I read a book which was 
about is dangerous 
 
In (16), the ?moved? phrase `the vampire' is taken 
out of the non-complex NP `a book about <the 
vampire>; in (17), however, `the vampire' is 
?moved? out of the complex NP `a book which was 
about <the vampire>?, which is not allowed. 
Yet our TSG could also predict the correct 
alternative by means of the best ranked derivation 
alone, by respectively derivations (16?) and (17?): 
 
(16?) [the vampire X is dangerous] o [who I read 
X] o [a book about] 
 
ranking: 1,585,992 + 123,195 + 5,719 = 1,714,906 
 
(17?) [the vampire X is dangerous] o [who I read 
X] o [a book which X] o [was about] 
 
ranking: 1,585,992 + 123,195 + 184,665 + 12,745 
= 1,906,597 
 
Besides the complex NP constraint, the phenome-
non of relative clause formation also uses most 
other constraints related to wh-questions: Left 
branch condition, Sentential Subject Constraint and 
Coordinate Structure Constraint. All these con-
straints could be modelled with the best-ranked 
derivation ? this time including Coordinate struc-
tures (as e.g. (18) and (19)) that were unsuccess-
fully predicted by our TSG for wh-questions. 
 
(18) Bella loves vampires and werewolves who are 
unstable 
(19) *werewolves who Bella loves vampires and 
are unstable 
6 The problem of Extraposition from NP 
A problematic case for many nativist approaches is 
the so-called ?Extraposition from NP? problem for 
which only ad hoc solutions exist. None of the 
constraints previously mentioned can explain (20) 
and (21): 
 
(20) that Jacob picked Bella up who loves Edward 
is possible 
(21) *that Jacob picked Bella up is possible who 
loves Edward 
 
As Ross (1967), Borsley (2004) and others note, 
the Complex NP Constraint cannot explain (20) 
and (21), because it applies to elements of a sen-
tence dominated by an NP, and here the moved 
constituent `who loves Edward' is a sentence 
dominated by an NP. Therefore, an additional con-
cept needs to be introduced: `upward bounded-
ness', where a rule is said to be upward bounded if 
elements moved by that rule cannot be moved over 
the boundaries of the first sentence above the ele-
ments being operated on (Ross 1967; Borsley 
2004). 
 Thus additional machinery is needed to 
explain the phenomenon of Extraposition from NP. 
Instead, our notion of best ranked derivation needs 
no additional machinery and can do the job, as 
shown by derivations (20?) and (21?): 
 
(20?)  [X is possible] o [that Jacob picked X] o 
[Bella up X] o [who loves Edward] 
 
ranking: 175 + 465,494 + 149,372 + 465,494 = 
1,080,535 
 
(21?)  [X is possible X] o [that Jacob picked X] o 
[Bella up] o [who loves Edward] 
 
15
ranking: 3,257 + 465,494 + 176,910 + 465,494 = 
1,111,155 
7 The problem of Topicalization  
Also the phenomenon of Topicalization is sup-
posed to follow the Complex NP constraint, Left 
branch condition, Sentential Subject Constraint and 
Coordinate Structure Constraint, all of which can 
again be modelled by the best ranked derivation. 
For example, the topicalization in (22) is fine but 
in (23) it is not. 
 
(22) Stephenie's book I read 
(23) * Stephenie's I read book 
 
Our TSG predicts the correct alternative by means 
of the best ranked derivation: 
 
(22?)  [X I read] o [Stephenie?s book] 
 
ranking: 608 + 2,784 = 3,392 
 
(23?)  [Stephenie?s X book] o [I read] 
 
ranking: 3,139 + 488 = 3,627 
8 The problem of Left dislocation 
The phenomenon of Left dislocation provides a 
particular challenge to nativist approaches since it 
shows that there are grammatical sentences that do 
not obey the Coordinate Structure Constraint (see 
Adger 2003; Borsley 2004). A restriction that is 
mentioned but not explained by Ross (1967), is the 
fact that in Left dislocation the moved constituent 
must be moved to the left of the main clause. In-
stead, movement merely to the left of a subordinate 
clause results in an ungrammatical sentence. For 
example, (24) is grammatical, because `Edward' is 
moved to the left of the main clause. Sentence 
(25), on the other hand, is ungrammatical, because 
`Edward' is only moved to the left of the subordi-
nate clause `that you love <Edward>'. 
 
(24) Edward, that you love him is obvious 
(25) *that Edward, you love him is obvious 
 
Our TSG has no problem in distinguishing be-
tween these two alternatives, as is shown below: 
 
(24?)  [Edward X is obvious] o [that you love him] 
 
ranking: 590,659 + 57,785 = 648,444 
 
(25?)  [that X is obvious] o [Edward you love him] 
 
ranking: 876,625 + 415,940 = 1,292,565 
9 Discussion and conclusion 
We have shown that an unsupervised TSG can cap-
ture virtually all phenomena related to wh-
questions in a simple and uniform way. Further-
more, we have shown that our model can be ex-
tended to cover other phenomena, even phenomena 
that fall out of the scope of the traditional nativist 
account. Hence, for at least these phenomena, Ar-
guments from Poverty of Stimulus can no longer 
be invoked. That is, step (ii) in Section 1 where it 
is claimed that children cannot learn the phenome-
non on the basis of input alone, is refuted. 
 
Phenomenon          Succesful? 
Subject Auxiliary Fronting  yes 
WH-Questions 
Unbounded Scope   yes 
Complex NP Constraint  yes 
Coordinate Structure Constraint  no 
Left Branch Condition   yes 
Subject WH-questions    yes 
WH in situ     yes 
Superiority     yes 
Extended Superiority    yes 
Embedded WH-questions  yes 
WH-islands    yes 
Relative Clause Formation 
Complex NP Constraint   yes 
Coordinate Structure Constraint yes 
Sentential Subject Constraint   yes 
Left Branch Condition   yes 
Extraposition from NP   yes 
Topicalization 
Complex NP Constraint  yes 
Coordinate Structure Constraint  yes 
Sentential Subject Constraint  yes 
Left Branch Condition   yes 
Left Dislocation 
Coordinate Structure Constraint  yes 
Restriction     yes 
Table 1. Overview of empiricist solutions to nativist 
problems tested so far (using as input the child-directed 
sentences in the Adam corpus of the Childes database), 
and whether they were successful. 
16
 Table 1 gives an overview of all phenomena we 
have tested so far with our model, and whether 
they can be successfully explained by the best-
ranked k-shortest derivation (not all of these phe-
nomena could be explicitly dealt with in the cur-
rent paper). 
Previous empiricist computational models 
that dealt with learning linguistic phenomena typi-
cally focused on auxiliary fronting (and sometimes 
on a couple of other problems ? see Clark and Ey-
raud 2006). MacWhinney (2004) also describes 
ways to model some other language phenomena 
empirically, but this has not resulted into a compu-
tational framework. To the best of our knowledge, 
ours is the first empiricist computational model 
that also deals with the problems of wh-questions, 
relative clause formation, topicalization, extraposi-
tion from NP and left dislocation.  
Many other computational models of lan-
guage learning focus either on inducing syntactic 
structure (e.g. Klein and Manning 2005), or on 
evaluating which sentences can be generated by a 
model with which precision and recall (e.g. Ban-
nard et al 2009; Waterfall et al 2010). Yet that 
work leaves the presumed ?hard cases? from the 
generativist literature untouched. This may be ex-
plained by the fact that most empiricist models do 
not deal with the concept of (absolute) grammati-
cality, which is a central concept in the generativist 
framework. It may therefore seem that the two op-
posing approaches are incommensurable. But this 
is only partly so: most empiricist models do have 
an implicit notion of relative grammaticality or 
some other ranking method for sentences and their 
structures. In some cases, like our model, the top-
ranking can simply be equated with the notion of 
grammaticality. In this way empiricist and genera-
tivist models can be evaluated on the same prob-
lems. 
There remains a question what our unsu-
pervised TSG then exactly explains. It may be 
quite successful in refuting step (ii) in the Argu-
ment from the Poverty of the Stimulus, but it does 
not really explain where the preferences of chil-
dren come from. Actually it only explains that 
these preferences come from child-directed input 
provided by caregivers. Thus the next question is: 
where do the caregivers get their preferences from? 
From their caregivers -- ad infinitum? It is exactly 
the goal of generative grammar to try to answer 
these questions. But as we have shown in this pa-
per, these answers are motivated by an argument 
that does not hold. Thus our work should be seen 
as (1) a refutation of this argument (of the Poverty 
of the Stimulus) and (2) an alternative approach 
that can model all the hard phenomena on the basis 
of just one principle (the best-ranked derivation). 
The question where the preferences may eventually 
come from, should be answered within the field of 
language evolution. 
While our TSG could successfully learn a 
number of linguistic phenomena, it still has short-
comings. We already explained that we have only 
tested on part of speech strings. While this is not 
essentially different from how the nativist ap-
proach defines their constraints (i.e. on categories 
and functions of words, not on specific words 
themselves), we believe that any final model 
should be tested on word strings. Moreover, we 
have tested only on English. There is a major ques-
tion how our approach performs on other lan-
guages, for example, with rich morphology. 
So far, our model only ranks alternative 
sentences (for a certain phenomenon). Ideally, we 
would want to test a system that produces for a 
given meaning to be conveyed the various possible 
sentences ordered in terms of their rankings, from 
which the top-ranked sentence is taken. In the ab-
sence of a semantic component in our model, we 
could only test the already given alternative sen-
tences and assess whether our model could predict 
the correct one. 
 Despite these problems, our main result is 
that with just a tiny fraction of a child?s input the 
correct sentence can be predicted by an unsuper-
vised TSG for virtually all phenomena related to 
wh-questions as well as for a number of other phe-
nomena that even fall out of the scope of the tradi-
tional generativist account.  
Finally it should be noted that our result is 
not in contrast with all generativist work. For ex-
ample, in Hauser et al (2002), it was proposed that 
the core language faculty comprises just recursive 
tree structure and nothing else. The work presented 
in this paper may be the first to show that one gen-
eral grammar induction algorithm makes language 
learning possible for a much wider set of pheno-
mena than has previously been endeavored. 
If empiricist models want to compete with 
generativist models, they should compete in the 
same arena, with the same phenomena. 
17
References  
D. Adger, 2003. Core syntax: A minimalist approach. 
Oxford University Press, 2003. 
 
B. Ambridge and E. Lieven, 2011). Child Language 
Acquisition. Contrasting Theoretical Approaches.  
Cambridge University Press. 
 
M. Bansal and D. Klein, 2011. The Surprising Variance 
in Shortest-Derivation Parsing, Proceedings ACL-
HLT 2011. 
 
C. Bannard, E. Lieven and M. Tomasello, 2009. Model-
ing Children?s Early Grammatical Knowledge, Pro-
ceedings of the National Academy of Sciences, 106, 
17284-89. 
 
R. Bod, R. Scha and K. Sima?an (eds.), 2003. Data-
Oriented Parsing, CSLI Publications/University of 
Chicago Press. 
 
R. Bod, 2006. An all-subtrees approach to unsupervised 
parsing. Proceedings ACL-COLING. 
 
R. Bod, 2009.  From Exemplar to Grammar: A Proba-
bilistic Analogy-based Model of Language Learn-
ing. Cognitive Science, 33(5), 752-793. 
 
R. Borsley, 2004. Syntactic Theory: A Unified Ap-
proach, Oxford University Press. 
 
A. Clark and R. Eyraud, 2006. Learning Auxiliary 
Fronting with Grammatical Inference. Proceedings 
CONLL 2006. 
 
A. Clark and S. Lappin, 2011. Linguistic Nativism and 
the Poverty of the Stimulus, Wiley-Blackwell. 
 
T. Cohn, P. Blunsom, and S. Goldwater, 2010. Inducing 
Tree-Substitution Grammars, Journal of Machine 
Learning Research, JMLR 11, 3053-3096. 
 
S. Crain, 1991. Language acquisition in the absence of 
experience. Behavorial and Brain Sciences, 14, 597-
612. 
 
S. Crain and R. Thornton. Acquisition of syntax and 
semantics, 2006. In M. Traxler and M. Gernsbacher, 
editors, Handbook of Psycholinguistics. Elsevier. 
 
S. Foraker, T. Regier, N. Khetarpal, A. Perfors, and J. 
Tenenbaum, 2009. Indirect Evidence and the Pov-
erty of the Stimulus: The Case of Anaphoric One. 
Cognitive Science, 33, 287-300. 
 
J. Goodman, 2003. Efficient parsing of DOP with 
PCFG-reductions. In R. Bod, R. Scha & K. Sima?an 
(Eds.), Data-oriented parsing, 125?146. CSLI Pubs. 
 
M. Hauser, N. Chomsky and T. Fitch, 2002. The faculty 
of language: What is it, who has it, and how did it 
evolve? Science, 298, 1569?1579. 
 
L . Huang  and D. Chiang, 2005. Better k-best parsing. 
In Proceedings IWPT 2005, pp. 53?64. 
 
X. Kam, L. Stoyneshka, L. Tornyova, J. Fodor and W. 
Sakas, 2008. Bigrams and the Richness of the 
Stimulus. Cognitive Science, 32, 771-787. 
 
D. Klein and C. Manning, 2005 Natural language 
grammar induction with a generative constituent-
context model. Pattern Recognition, 38, 1407?1419. 
 
J. Lidz, S. Waxman and J. Freedman, 2003. What in-
fants know about syntax but couldn?t have learned: 
experimental evidence for syntactic structure at 18 
months. Cognition, 89, B65?B73 
 
B. MacWhinney, 2000. The CHILDES project: Tools 
for analyzing talk. Mawah, NJ: Erlbaum 
 
B. MacWhinney, 2004. A multiple process solution to 
the logical problem of language acquisition. Journal 
of Child Language, 3, 883- 914. 
 
T. O?Donnell, N. Goodman, and J. Tenenbaum, 2009. 
Fragment grammar: Exploring reuse in hierarchical 
generative processes. Technical Report MIT-CSAIL-
TR-2009-013, MIT. 
 
M. Post and D. Gildea, 2009. Bayesian learning of a tree 
substitution grammar. In Proceedings of the ACL-
IJCNLP 2009. 
 
G. Pullum and B. Scholz, 2002. Empirical assessment of 
stimulus povery arguments. The Linguist Review, 
19(2002), 9-50. 
 
F. Reali and M. Christiansen, 2005. Uncovering the 
richness of the stimulus: structure dependence and 
indirect statistical evidence. Cognitive Science, 29, 
1007-1028. 
 
J. Ross, 1967. Constraints on variables in syntax. PhD 
thesis, Massachusetts Institute of Technology. 
 
C. Teichmann, 2011. Reducing the size of the represen-
tation for the uDOP-estimate. Proceedings EMNLP 
2011. 
 
H. Waterfall, B. Sandbank, L. Onnis, and S. Edelman, 
2010. An empirical generative framework for com-
putational modeling of language acquisition. Journal 
of Child Language, 37, 671-703. 
 
A. Zollmann and K. Sima'an. 2005. A Consistent and 
Efficient Estimator for Data-Oriented Parsing. 
In Journal of Automata, Languages and Combina-
torics, 10 (2005), 367-388. 
18
Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 46?54,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
A Usage-Based Model of Early Grammatical Development
Barend Beekhuizen
LUCL
Leiden University
b.f.beekhuizen@hum.leidenuniv.nl
Rens Bod
ILLC
University of Amsterdam
l.w.m.bod@uva.nl
Afsaneh Fazly and Suzanne Stevenson
Department of Computer Science
University of Toronto
afsaneh,suzanne@cs.toronto.edu
Arie Verhagen
LUCL
Leiden University
a.verhagen@hum.leidenuniv.nl
Abstract
The representations and processes yield-
ing the limited length and telegraphic style
of language production early on in acqui-
sition have received little attention in ac-
quisitional modeling. In this paper, we
present a model, starting with minimal lin-
guistic representations, that incrementally
builds up an inventory of increasingly long
and abstract grammatical representations
(form+meaning pairings), in line with the
usage-based conception of language ac-
quisition. We explore its performance on
a comprehension and a generation task,
showing that, over time, the model bet-
ter understands the processed utterances,
generates longer utterances, and better ex-
presses the situation these utterances in-
tend to refer to.
1 Introduction
A striking aspect of language acquisition is the dif-
ference between children?s and adult?s utterances.
Simulating early grammatical production requires
a specification of the nature of the linguistic repre-
sentations underlying the short, telegraphic utter-
ances of children. In the usage-based view, young
children?s grammatical representions are thought
to be less abstract than adults?, e.g. by having
stricter constraints on what can be combined with
them (cf. Akhtar and Tomasello 1997; Bannard
et al. 2009; Ambridge et al. 2012). The represen-
tations and processes yielding the restricted length
of these early utterances, however, have received
little attention. Following Braine (1976), we adopt
the working hypothesis that the early learner?s
grammatical representations are more limited in
length (or: arity) than those of adults.
Similarly, in computational modeling of gram-
mar acquisition, comprehension has received more
attention than language generation. In this pa-
per we attempt to make the mechanisms underly-
ing early production explicit within a model that
can parse and generate utterances, and that in-
crementally learns constructions (Goldberg, 1995)
on the basis of its previous parses. The model?s
search through the hypothesis space of possible
grammatical patterns is highly restricted. Start-
ing from initially small and concrete representa-
tions, it learns incrementally long representations
(syntagmatic growth) as well as more abstract
ones (paradigmatic growth). Several models ad-
dress either paradigmatic (Alishahi and Stevenson,
2008; Chang, 2008; Bannard et al., 2009) or syn-
tagmatic (Freudenthal et al., 2010) growth. This
model aims to explain both, thereby contribut-
ing to the understanding of how different learning
mechanisms interact. As opposed to other models
involving grammars with semantic representations
(Alishahi and Stevenson, 2008; Chang, 2008), but
similar to Kwiatkowski et al. (2012), the model
starts without an inventory of mappings of single
words to meanings.
Based on motivation from usage-based and con-
struction grammar approaches, we define several
learning principles that allow the model to build
up an inventory of linguistic representations. The
model incrementally processes pairs of an utter-
ance U , consisting of a string of words w
1
. . . w
n
,
and a set of situations S, one of which is the situa-
tion the speaker intends to refer to. The other situ-
ations contribute to propositional uncertainty (the
uncertainty over which proposition the speaker is
trying to express; Siskind 1996). The model tries
to identify the intended situation and to understand
how parts of the utterance refer to certain parts of
that situation. To do so, the model uses its growing
inventory of linguistic representations (Section 2)
to analyze U , producing a set of structured seman-
tic analyses or parses (Fig. 1, arrow 1; Section 3).
46
The resulting best parse, U and the selected situa-
tion are then stored in a memory buffer (arrow 2),
which is used to learn new constructions (arrow
3) using several learning mechanisms (Section 4).
The learned constructions can then be used to gen-
erate utterances as well. We describe two experi-
ments: in the comprehension experiment (Section
5), we evaluate the model?s ability to parse the
stream of input items. In the generation experi-
ment (Section 6), the model generates utterances
on the basis of a given situation and its linguistic
knowledge. We evaluate the generated utterances
given different amounts of training items to con-
sider the development of the model over time.
2 Representations
We represent linguistic knowledge as construc-
tions: pairings of a signifying form and a signi-
fied (possibly incomplete) semantic representation
(Goldberg, 1995). The meaning is represented as
a graph with the nodes denoting entities, events,
and their relations, connected by directed unla-
beled edges. The conceptual content of each node
is given by a set of semantic features. We assume
that meaning representations are rooted trees. The
signifying form consists of a positive number of
constituents. Every constituent has two elements:
a phonological form, and a pointer to a node in the
signified meaning (in line with Verhagen 2009).
Both can be specified, or one can be left empty.
Constituents with unspecified phonological forms
are called open, denoted with  in the figures. The
head constituent of a construction is defined as
the constituent that has a pointer to the root node
of the signified meaning. We furthermore require
that no two constituents point to the same node of
the signified meaning.
This definition generalizes over lexical ele-
ments (one phonologically specified constituent)
as well as larger linguistic patterns. Fig. 2, for in-
stance, shows two larger constructions being com-
bined with each other. We call the set of construc-
tions the learner has at some moment in time the
constructicon C (cf. Goldberg 2003).
3 Parsing
3.1 Parsing operations
We first define a derivation d as an assembly
of constructions in C, using four parsing opera-
tions defined below. In parsing, derivations are
constrained by the utterance U and the situations
utterancesituation 1
situation n
situation 2...
situationsinput item
construction 1construction 2construction 3construction nconstructicon
analysis
(utterance, intended situation, analysis)
...
...
memory buffer
1 12
3(utterance, intended situation, analysis)(utterance, intended situation, analysis)
Figure 1: The global flow of the model
S, whereas in production, only a situation s con-
strains the derivation. The leaf nodes of a deriva-
tion must consist of phonological constraints of
constructions that (in parsing) are satisfied by U .
All constructions used in a derivation must map to
the same situation s ? S. A construction cmaps to
s iff the meaning of c constitutes a subgraph of s,
with the features on each of the nodes in the mean-
ing of c being a subset of the features on the corre-
sponding node of s. Moreover, each construction
must map to a different part of s. This constitutes
a mutual exclusivity effect in analyzing U : every
part of the analysis must contribute to the compos-
ite meaning. A derivation d thus gives us a map-
ping between the composed meaning of all con-
structions used in d and one situation s ? S. The
aggregate mapping specifies a subgraph of s that
constitutes the interpretation of that derivation.
The central parsing operation is the COMBINA-
TION operator ?. In c
i
? c
j
, the leftmost open con-
stituent of c
i
is combined with c
j
. Fig. 2 illus-
trates COMBINATION. COMBINATION succeeds if
both the semantic pointer of the leftmost open con-
stituent of c
i
and the semantic pointer of the head
constituent of c
j
map to the same semantic node
of a situation s
Initially, the model has few constructions to an-
alyze the utterance with. Therefore, we define
three other operations that allow the model to cre-
ate a derivation over the full utterance without
combining constructions. First, a known or un-
known word that cannot be fit into a derivation,
can be IGNOREd. Second, an unknown word can
be used to fill an open constituent slot of a con-
struction with the BOOTSTRAP operator. Boot-
strapping entails that the unknown word will be
associated with the semantics of the node. Finally,
the learner can CONCATENATE multiple deriva-
tions, by linearly sequencing them, thus creating a
more complex derivation without combining con-
47
uteranucsaio tera1n u2e.copei sec2nuciptcoa m3ctn uly1(c.an
,d)bf n???????f ,d)bf? ????f
uteranucsaio tera1n u2e.copei sec2nuciptcoa m3ctn
?? ? ?? ?
) ?
,d)bf n??????f ,d)bf? ????f
uteranu?copaio tera3n u2e.copei sec2nue??.o aiopo? uly1(c.an
u?copaio tera3nue??.o aiopo? ,d)bf n???????f ,d)bf? ????f
uteranucsaio tera1n u2e.copei sec2nuciptcoa m3ctn
,d)bf n??????f ,d)bf? ????f
uteranu?copaio tera3n u2e.copei sec2nue??.o aiopo? uly1(c.an
??????n????n????????????n??????? ?? ?n??????n?n??????????n????????
Figure 2: Combining constructions. The dashed lines represent semantic pointers, either from con-
stituents to the constructional meaning (black) or from the constructions to the situation (red and blue).
uttereanc
n s
io ta1
21n i o. p . m
21n i o. pn p n m n 3
. p
21n i o.t .ne1 ne1
Figure 3: The CONCATENATE, IGNORE and
BOOTSTRAP operators (internal details of the con-
structions left out).
structions. This allows the learner to interpret a
larger part of the situation than with COMBINA-
TION only. The resulting sequences may be ana-
lyzed in the learning process as constituting one
larger construction, consisting of the parts of the
concatenated derivations. Fig. 3 illustrates these
three operations.
3.2 Selecting the best analysis
Multiple derivations can be highly similar in the
way they map parts of U to parts of an s ? S. We
define a parse to be a set of derivations that have
the same internal structure and the same mappings
to a situation, but that use different constructions
in doing so (cf. multiple licensing; Kay 2002). We
take the most probable parse of U to be the best
analysis of U . The most probable parse points to a
situation, which the model then assumes to be the
identified situation or s
identified
. If no parse can be
made, s
identified
is selected at random from S.
The probability of a parse p is given by the sum
of the probabilities of the derivations d subsumed
under that parse, which in turn are defined as the
product of the probabilities of the constructions c
used in d.
P (p) =
?
d?p
P (d) (1)
P (d) =
?
c?d
P (c) (2)
The probability of a construction P (c) is given
by its relative frequency (count) in the construc-
ticon C, smoothed with Laplace smoothing. We
assume that the simple parsing operations of IG-
NORE, BOOTSTRAP, and CONCATENATION reflect
usages of an unseen construction with a count of
0.
P (c) =
c.count+ 1
?
c
?
?C
c
?
.count+ |C|+ 1
(3)
The most probable parse, U and s
identified
are
added to the memory buffer. The memory buffer
has a pre-set maximal length, discarding the oldest
exemplars upon reaching this length. In the future,
we plan to consider more realistic mechanisms for
the memory buffer, such as graceful degradation,
and attention effects.
48
4 Learning mechanisms
The model uses the best parse of the utterance to
develop its knowledge of the constructions in the
constructicon C. Two simple operations, UPDATE
and ASSOCIATION, are used to create initial con-
structions and reinforce existing ones respectively.
Two additional operations, PARADIGMATIZATION
and SYNTAGMATIZATION, are key to the model?s
ability to extend these initial representations by
inducing novel constructions that are richer and
more abstract than existing ones.
4.1 Direct learning from the best parse
The best parse is used to UPDATE C. For this
mechanism, the model uses the concrete mean-
ing of s
identified
rather than the (potentially more
abstract) meaning of the constructions in the best
parse.
1
Every construction in the parse is assigned
the subgraph of s
identified
it maps to as its new
meaning, and the count of the adjusted construc-
tion is incremented with 1, or added to C with a
count of 1, if it does not yet exist. This includes
applications of the BOOTSTRAP operation, creat-
ing a mapping of the previously unknown word to
a situational meaning.
ASSOCIATE constitutes a form of simple cross-
situational learning over the memory buffer. The
intuition is that co-occurring word sequences
and meaning components that remain unanalyzed
across multiple parses might themselves comprise
the form-meaning pairing of a construction. If the
unanalyzed parts of two situations contain an over-
lapping subgraph, and the unanalyzed parts of two
utterances an overlapping subsequence of words,
the two are mapped to each other and added to C
with a count of 0.
4.2 Qualitative extension of the best parse
Syntagmatization Some of the processes de-
scribed thus far yield analyses of the input in
which constructions are linearly associated but
lack appropriate relational structure among them.
The model requires a process, which we call SYN-
TAGMATIZATION, that enables it to induce further
hierarchical structure.
In order for the learner to acquire constructions
in which the different constituents point to differ-
ent parts of the construction?s meaning, the ASSO-
1
This follows Langacker?s (2009) claim that the processed
concrete usage events should leave traces in the learner?s
mind.
CIATE operation does not suffice. We assume that
the learner is able to learn such constructions by
using concatenated derivations. The process we
propose is SYNTAGMATIZATION. In this process,
the various concatenated derivations are taken as
constituents of a novel construction. This instanti-
ates the idea that joint processing of two (or more)
events gradually leads to a joint representation of
these, previously independent, events.
More precisely, the process starts by taking the
top nodes T of the derivations in the best parse,
where T consists of the single top node if no CON-
CATENATION has been applied, or the set of con-
catenated nodes of the parse tree if CONCATENA-
TION has been applied (e.g. for the derivation in
Fig. 3, |T | = 2). For each top node t ? T , we take
the root node of the construction?s meaning, and
define its semantic frame to consist of all children
(roles) and grandchildren (role-fillers) of the node
in the situation it maps to. The model then forms a
novel construction c
syn
by taking all the construc-
tions in the parse whose semantic root nodes point
to a node in this semantic frame, referring to those
as the set R of semantically related constructions.
As the novel meaning of c
syn
, the model takes the
subgraph of the situation mapped to by the joint
mapping of all constructional meanings of con-
structions in R.
R, as well as all phonologically specified con-
stituents of t itself, are then linearized as the con-
stituents of c
syn
. The novel construction thus con-
stitutes a construction with a higher arity, ?joining?
several previously independent constructions. Fig.
4 illustrates the syntagmatization mechanism.
Paradigmatization Due to our usage-driven ap-
proach, all learning mechanisms so far give us
maximally concrete constructions. In order for the
model to generalize beyond the observed input,
some degree of abstraction is needed. The model
does so with the PARADIGMATIZATION mecha-
nism. This mechanism recursively looks for min-
imal abstractions (cf. Tomasello 2003, 123) over
the constructions in C and adds those to C, thus
creating a full-inheritance network (cf. Langacker
1989, 63-76).
An abstraction over a set of constructions is
made if there is an overlapping subgraph between
the meanings of the constructions, where every
node of the subgraph is the non-empty feature
set intersection between two mapped nodes of the
constructional meanings. Furthermore, the con-
49
uterauncsiricots 111autoi2tr. p.tm.ma
uio3.l.o3.ory.(i,ra
d)b d)bf??? n? f??? ?
uter 2cn.auncsiricots 111a uio3.l.o3.ory.(i,rad)bf??? ??
ecoetr.otr.
uc?.er .orir? ?ssa
d)bf??? ??
uterauncsiricots 111autoi2tr. p.tm.ma
uio3.l.o3.ory.(i,ra
d)b d)bf??? n? f??? ??
uc?.er .orir? ?ssa
d)bf??? ??
????????????????? ??rt??r ? ??????????????????
Figure 4: The SYNTAGMATIZATION mechanism. The mechanism takes a derivation as its input and
reinterprets it as a novel construction of higher arity).
stituents must be mappable: both constructions
have the same number of constituents and the
paired constituents point to a mapped node of the
meaning. The meaning of the abstracted construc-
tion is then set to this overlapping subgraph, which
is the lowest possible semantic abstraction over
the constructions. The constituents of this new ab-
straction have a specified phonological form if the
more concrete constructions share the same word,
and an unspecified one otherwise. The count of an
abstracted construction is given by the cardinality
of the set of its direct descendants in the network.
This generalizes Bybee?s (1995) idea about type
frequency as a proxy for productivity to a network
structure. Fig. 5 illustrates the paradigmatization
mechanism.
5 Experimental set-up
The model is incrementally presented with U, S
pairings based on Alishahi & Stevenson?s (2010)
generation procedure. In this procedure, an utter-
ance and a semantic frame expressing its meaning
(a situation) are generated. The generation pro-
cedure follows distributions occurring in a corpus
of child-directed speech. As we are interested in
the performance of the model under propositional
uncertainty, we add a parametrized number of ran-
domly sampled situations, so that S consists of the
situation the speaker intends to refer to (s
correct
)
and a number of situations the speaker does not
intend to refer to.
2
Here, we set the number of ad-
2
We are currently researching the effects of sampling non-
correct situations that have a greater likelihood of overlap
ditional situations to be 1 or 5; the other parameter
of the model, the size of the memory buffer, is set
to 5 exemplars.
For the comprehension experiment, we eval-
uate the model?s performance parsing the input
items, averaging over every 50 U, S pairs. We
track the ability to identify the intended situation
from S. Identification succeeds if the best parse
maps to s
correct
, i.e. if s
identified
= s
correct
. Next,
situation coverage expresses what proportion of
s
identified
has been interpreted and thus how rich the
meanings of the used constructions are. It is de-
fined as the number of nodes of the interpretation
of the best parse, divided by the number of nodes
of s
identified
. Finally, utterance coverage tells us
what proportion of U has been parsed with con-
structions (excluding IGNORED; including BOOT-
STRAPPED words). The measure expresses the
proportion of the signal that the learner (correctly
or incorrectly) is able to interpret.
For exploring language production, the model
receives a situation, and (given the constructicon)
finds the most probable, maximally expressive,
fully lexicalized derivation expressing it. That is:
among all derivations terminating in phonologi-
cally specified constituents, it selects the deriva-
tions that cover the most semantic nodes of the
given situation. In the case of multiple such
derivations, it selects the most probable one, fol-
lowing the probability model in Section 3. We
only allow for the COMBINATION operator in the
derivations, as BOOTSTRAPPING and IGNORE re-
with the intended situation, to reflect more realistic input (cf.
Siskind 1996).
50
uterancsion uoi12.2ipe1cmmm uep2se.nc3nelnl uye.2np.cmmm u1ite.2ipcnp.2.(ct3e2l 
,d) ,d) ,d)bf?? n? bf?? ?? bf?? ???
uterancsion uoi12.2ipe1cmmm uep2se.nc3nelnl uye.2np.cmmm u1ite.2ipcnp.2.(c.e?n 
uterancsion uoi12.2ipe1cmmm uep2se.nc3nelnl uye.2np.cmmm u1ite.2ipcnp.2.( 
,d) ,d) ,d)bf?? n? bf?? ?? bf?? ?
,d) ,d) ,d)bf?? n? bf?? ?? bf?? ???
?? ???? ?? ???n???n????? ???? ? ??? ?? ??? ??? t3e2l??? .e?n ?
? ?????? ????? ????u1ite.2ipcnp.2.(ct3e2l ??? ?u1ite.2ipcnp.2.(c?.e?n 
Figure 5: The PARADIGMATIZATION mechanism. The construction on top is an abstraction obtained
over the two constructions at the bottom.
fer to words in a given U , and CONCATENATE is a
back-off method for analyzing more of U than the
constructicon allows for. The situations used in the
generation experiment do not occur in the training
items, so that we truly measure the model?s ability
to generate utterances for novel situations.
The phonologically specified leaf nodes of the
best derivation constitute the generated utterance
U
gen
. U
gen
is evaluated on the basis of its mean
length, in number of words, its situation cover-
age, as defined in the comprehension experiment,
and its utterance precision and utterance recall.
To calculate these, we take the maximally overlap-
ping subsequenceU
overlap
between the actual utter-
ance U
act
associated with the situation and U
gen
.
Utterance precision (how many words are gener-
ated correctly) and utterance recall (how many of
the correct words are generated) are defined as:
Utterance precision =
|U
overlap
|
|U
gen
|
(4)
Utterance recall =
|U
overlap
|
|U
act
|
(5)
Because the U, S-pairs on which the model was
trained, are generated randomly, we show results
for comprehension and production averaged over
5 simulations.
6 Experiments
A central motivation for the development of this
model is to account for early grammatical produc-
tion: can we simulate the developmental pattern
of the growth of utterance length and a growing
potential for generalization? The same construc-
tions underlying these productions should, at the
same time, also account for the learner?s increas-
ing grasp of the meaning of U . To explore the
model?s performance in both domains, we present
a comprehension and a generation experiment.
6.1 Comprehension results
Fig. 6a gives us the results over time of the com-
prehension measures given a propositional un-
certainty of 1, i.e. one situation besides s
correct
in S. Overall, the model understands the utter-
ances increasingly well. After 2000 input items,
the model identifies s
correct
in 95% of the cases.
With higher levels of propositional uncertainty
(not shown here), performance is still relatively
robust: given 5 incorrect situations in S, s
correct
is identified in 62% of all cases (random guess-
ing gives a score of 17%, or
1
6
). Similarly, the
proportion of the situation interpreted and the pro-
portion of the utterance analyzed go up over time.
This means that the model builds up an increasing
repertoire of constructions that allow it to analyze
larger parts of the utterance and the situations it
identifies. It is important to realize that these mea-
51
0 500 1000 1500 2000
0.0
0.2
0.4
0.6
0.8
1.0
time
prop
ortion
measures
situation coverageutterance coverageidentification
(a) Comprehension results over time
0 500 1000 1500 2000
0.0
0.5
1.0
1.5
2.0
2.5
3.0
time
utter
anc
e len
gth in
 word
s
(b) Length of U
gen
over time
0 500 1000 1500 2000
0.0
0.2
0.4
0.6
0.8
1.0
time
prop
ortion
measures
situation coverageutterance precisionutterance recall
(c) Generation results over time
Figure 6: Quantitative results for the comprehension and generation experiments
sures do not display what proportion of the utter-
ance or situation is analyzed correctly.
6.2 Generation results
Quantitative results Fig. 6b shows that the av-
erage utterance length increases over time. This
indicates that the number of constituents of the
used constructions grows. Next, Fig. 6c shows the
performance of the model on the generation task.
After 2000 input items, the model generates pro-
ductions expressing 93% of the situation, with an
utterance precision of 0.91, and an utterance recall
of 0.81. Given a propositional uncertainty of 5,
these go down to 79%, 0.76 and 0.59 respectively.
Comparing the utterance precision and recall
over time, we can see that the utterance preci-
sion is high from the start, whereas the recall
gradually increases. This is in line with the ob-
servation that children predominantly produce er-
rors of omission (leaving linguistic material out an
adult speaker would produce), and few errors of
comission (producing linguistic material an adult
speaker would not produce).
Qualitative results Tracking individual produc-
tions given specific situations over time allows us
to study in detail what the model is doing. Here,
we look at one case qualitatively. Given the sit-
uation for which the U
act
is she put them away,
the model generates, over time, the utterances in
Table 1. The brackets show the internal hierarchi-
cal structure of the derivation. This development
illustrates several interesting aspects of the model.
First, as discussed earlier, the model mostly makes
errors of omission: earlier productions leave out
more words found in the adult utterances. Only at
t = 550, the model makes an error of commission,
using the word in erroneously.
[
[
s
h
e
]
p
u
t
]
[
s
h
e
[
p
u
t
]
]
[
[
s
h
e
]
[
p
u
t
]
[
i
n
]
]
[
[
s
h
e
]
p
u
t
t
h
e
m
[
a
w
a
y
]
]
[
[
s
h
e
]
p
u
t
[
t
h
e
m
]
]
[
[
s
h
e
]
p
u
t
t
h
e
m
[
a
w
a
y
]
]
[
[
s
h
e
]
p
u
t
[
t
h
e
m
]
a
w
a
y
]
[
[
s
h
e
]
p
u
t
t
h
e
m
a
w
a
y
]
t 50 500 550 600 950 1000 1050 1400
Table 1: Generations over time t for one situation.
Starting from t = 600 (except at t = 950),
the model generates the correct utterance, but the
derivations leading to this production differ. At
t = 550, for instance, the learner combines a
completely non-phonologically specific construc-
tion for which the constituents refer to the agent,
action and goal location, with three ?lexical? con-
structions that fill in the words for those items..
The constructions used after t = 550 are all more
specific, combining 3, or even only 2 constructions
(t ? 1400) where the entire sequence of words
?put them away? arises from a single construction.
Using less abstract constructions over time
seems contrary to the usage-based idea that con-
structions become more abstract over the course of
acquisition. However, this result follows from the
way the probability model is defined. More spe-
cific constructions that are able to account for the
input will entail fewer combinations, and a deriva-
tion with fewer combination operations will often
be more likely than one with more such opera-
tions. Given equal expressivity of the situation,
the former derivation will be selected over the lat-
ter in generation.
The effect is indeed in line with another concept
hypothesized to play a role in language acquisition
on a usage-based account, viz. pre-emption (Gold-
52
uterancsion uoi12.2ipe1cmmmccteran31ite.2ip u1ite.2ipclna.2pe.2ip uep2se.ncmm ueyynt.nlcmmmca.e.2ipe(, uid)nt.ce(.nyet. uid)nt.cbe(.nyet. f?? f?? f?? f?? f?????b n?????b ? ???b ? ???b ? ???b ?
u(na.cet. uoi12.2ipe1cbmmm uep2se.ncmmm ulna.2pe.2ipc1ite.2ip u1ite.2ipcnp.2., f?? f?? f????? ? ??? ? ??? ?
uet. uoi12.2ipe1cmmm uep2se.nc?ne(n( u2pln?np3lnp.3n?a. uid)nt.ce(.nyet. f?? f?? f?????b ? ? ???b ????
(b) (c)
(a)
Figure 7: Some representations at t = 2000
berg, 2006, 94-95). Pre-emption is the effect that
a language user will select a more concrete rep-
resentation over the combination of more abstract
ones. The effect can be reconceptualized in this
model as an epiphenomenon of the way the prob-
ability model works: simply because combining
fewer constructions in a derivation is often more
probable than combining more constructions, the
former derivation will be selected over the lat-
ter. Pre-emption is typically invoked to explain the
blocking of overgeneralization patterns, and an in-
teresting future step will be to see if the model can
simulate developmental patterns for well-known
cases of overgeneralization errors.
The potential for abstraction The paradigma-
tization operation allows the model to go beyond
observed concrete instances of form-meaning
pairings: without it, unseen situations could never
be fully expressed. Despite this potential, we have
seen that the model relies on highly concrete con-
structions. The concreteness of the used patterns,
however, does not imply the absence of more ab-
stract representations. Fig. 7 gives three exam-
ples of constructions in C in one simulation. Con-
struction (a) could be seen as a verb-island con-
struction (Tomasello, 1992, 23-24). The second
constituent is phonologically specified with put,
and the other arguments are open, but mapped to
specific semantic functions. This pattern allows
for the expression of many caused-motion events.
Construction (b) is the inverse of (a): the argu-
ments are phonologically specified, but the verb-
slot is open. This would be a case of a pronominal
argument frame [you V it], which have been found
to be helpful in the bootstrapping of verbal mean-
ings (Tomasello, 2001). Finally, (c) presents a case
of full abstraction. This construction licenses ut-
terances such as I sit here, you stay there and er-
roneous ones like he sits on (which, again, will be
pre-empted in the generation of utterances if more
concrete constructions licence he sits on it).
Summarizing, abstract constructions are ac-
quired, but only used for those cases in which no
concrete construction is available. This is in line
with the usage-based hypotheses that abstract con-
structions do emerge, but that for much of lan-
guage production, a language user can rely on
highly concrete patterns. A next step will be
to measure the development of abstractness and
length over the constructions themselves, rather
than the parses and generations they allow.
7 Conclusion
This, admittedly complex, model forms an attempt
to model different learning mechanisms in interac-
tion from a usage-based constructionist perspec-
tive. Starting with an empty set of linguistic rep-
resentations, the model acquires words and gram-
matical constructions simultaneously. The learn-
ing mechanisms allow the model to build up in-
creasingly abstract, as well as increasingly long
constructions. With these developing representa-
tions, we showed how the model gets better over
time at understanding the input item, performing
relatively robustly under propositional uncertainty.
Moreover, in the generation experiment, the
model shows patterns of production (increasingly
long utterances) similar to those of children. An
important future step will be to look at these pro-
ductions more closely and investigate if they also
converge on more detailed patterns of develop-
ment in the production of children (e.g. item-
specificity, as hypothesized on the usage-based
view). Despite highly concrete constructions suf-
ficing for most of production, inspection of the ac-
quired representations tells us that more abstract
constructions are learned as well. Here, an inter-
esting next step would be to simulate patterns of
overgeneralization in children?s production.
Acknowledgements
We would like to thank three anonymous review-
ers for their valuable and thoughtful comments.
We gratefully acknowledge the funding of BB
through NWO of the Netherlands (322.70.001)
and AF and SS through NSERC of Canada.
53
References
Nameera Akhtar and Michael Tomasello. 1997.
Young Children?s Productivity With Word Or-
der and Verb Morphology. Developmental Psy-
chology, 33(6):952?965.
Afra Alishahi and Suzanne Stevenson. 2008 A
Computational Model of Early Argument Struc-
ture Acquisition. Cognitive Science, 32(5):789?
834.
Afra Alishahi and Suzanne Stevenson. 2010. A
computational model of learning semantic roles
from child-directed language. Language and
Cognitive Processes, 25(1):50?93.
Ben Ambridge, Julian M Pine, and Caroline F
Rowland. 2012. Semantics versus statistics in
the retreat from locative overgeneralization er-
rors. Cognition, 123(2):260?79.
Colin Bannard, Elena Lieven, and Michael
Tomasello. 2009. Modeling children?s early
grammatical knowledge. Proceedings of the
National Academy of Sciences of the United
States of America, 106(41):17284?9.
Martin D.S. Braine. 1976. Children?s first word
combinations. University of Chicago Press,
Chicago, IL.
Joan Bybee. 1995. Regular morphology and the
lexicon. Language and Cognitive Processes, 10
(5):425?455.
Nancy C.-L. Chang. 2008. Constructing Gram-
mar: A computational model of the emergence
of early constructions. Dissertation, University
of California, Berkeley.
Daniel Freudenthal, Julian Pine, and Fernand Go-
bet. 2010. Explaining quantitative variation in
the rate of Optional Infinitive errors across lan-
guages: a comparison of MOSAIC and the Vari-
ational Learning Model. Journal of Child Lan-
guage, 37(3):643?69.
Adele E. Goldberg. 1995. Constructions. A
Construction Grammar Approach to Argument
Structure. Chicago University Press, Chicago,
IL.
Adele E Goldberg. 2003. Constructions: a new
theoretical approach to language. Trends in
Cognitive Sciences, 7(5):219?224.
Adele E. Goldberg. 2006. Constructions at Work.
The Nature of Generalization in Language. Ox-
ford University Press, Oxford.
Paul Kay. 2002. An Informal Sketch of a Formal
Architecture for Construction Grammar. Gram-
mars, 5:1?19.
Tom Kwiatkowski, Sharon Goldwater, Luke
Zettlemoyer, and Mark Steedman. 2012. A
Probabilistic Model of Syntactic and Seman-
tic Acquisition from Child-Directed Utterances
and their Meanings. In Proceedings EACL.
Ronald W. Langacker. 1989. Foundations of Cog-
nitive Grammar, Volume I. Stanford University
Press.
Ronald W. Langacker. 2009. A dynamic view of
usage and language acquisition. Cognitive Lin-
guistics, 20(3):627?640.
Jeffrey M Siskind. 1996. A computational study of
cross-situational techniques for learning word-
to-meaning mappings. Cognition, 61(1-2):39?
91.
Michael Tomasello. 1992. First Verbs: A study
of early grammatical development. Cambridge
University Press, Cambridge, UK.
Michael Tomasello. 2001 Perceiving intentions
and learning words in the second year of life.
In Melissa Bowerman and Stephen C. Levinson,
editors, Language Acquisition and Conceptual
Development, chapter 5, pages 132?158. Cam-
bridge University Press, Cambridge, UK.
Michael Tomasello. 2003. Constructing a lan-
guage: A Usage-Based Theory of Language
Acquisition. Harvard University Press, Cam-
bridge, MA.
Arie Verhagen. 2009 The conception of construc-
tions as complex signs. Emergence of struc-
ture and reduction to usage. Constructions and
Frames, 1:119?152.
54
