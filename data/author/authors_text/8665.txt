Improving Statistical Word Alignment with a Rule-Based Machine 
Translation System 
WU Hua, WANG Haifeng  
Toshiba (China) Research & Development Center 
5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng District 
Beijing, China, 100738 
{wuhua, wanghaifeng}@rdc.toshiba.com.cn 
 
Abstract 
The main problems of statistical word alignment 
lie in the facts that source words can only be 
aligned to one target word, and that the inappro-
priate target word is selected because of data 
sparseness problem. This paper proposes an ap-
proach to improve statistical word alignment 
with a rule-based translation system. This ap-
proach first uses IBM statistical translation 
model to perform alignment in both directions 
(source to target and target to source), and then 
uses the translation information in the rule-based 
machine translation system to improve the statis-
tical word alignment. The improved alignments 
allow the word(s) in the source language to be 
aligned to one or more words in the target lan-
guage. Experimental results show a significant 
improvement in precision and recall of word 
alignment. 
1 Introduction 
                                                          
Bilingual word alignment is first introduced as an 
intermediate result in statistical machine transla-
tion (SMT) (Brown et al 1993). Besides being 
used in SMT, it is also used in translation lexicon 
building (Melamed 1996), transfer rule learning 
(Menezes and Richardson 2001), example-based 
machine translation (Somers 1999), etc. In previ-
ous alignment methods, some researches mod-
eled the alignments as hidden parameters in a 
statistical translation model (Brown et al 1993; 
Och and Ney 2000) or directly modeled them 
given the sentence pairs (Cherry and Lin 2003). 
Some researchers used similarity and association 
measures to build alignment links (Ahrenberg et 
al. 1998; Tufis and Barbu 2002). In addition, Wu 
(1997) used a stochastic inversion transduction 
grammar to simultaneously parse the sentence 
pairs to get the word or phrase alignments. 
Generally speaking, there are four cases in 
word alignment: word to word alignment, word 
to multi-word alignment, multi-word to word 
alignment, and multi-word to multi-word align-
ment. One of the most difficult tasks in word 
alignment is to find out the alignments that in-
clude multi-word units. For example, the statisti-
cal word alignment in IBM translation models 
(Brown et al 1993) can only handle word to 
word and multi-word to word alignments.  
Some studies have been made to tackle this 
problem. Och and Ney (2000) performed transla-
tion in both directions (source to target and target 
to source) to extend word alignments. Their re-
sults showed that this method improved precision 
without loss of recall in English to German align-
ments. However, if the same unit is aligned to 
two different target units, this method is unlikely 
to make a selection. Some researchers used 
preprocessing steps to identity multi-word units 
for word alignment (Ahrenberg et al 1998; 
Tiedemann 1999; Melamed 2000). The methods 
obtained multi-word candidates based on con-
tinuous N-gram statistics. The main limitation of 
these methods is that they cannot handle sepa-
rated phrases and multi-word units in low fre-
quencies. 
In order to handle all of the four cases in word 
alignment, our approach uses both the alignment 
information in statistical translation models and 
translation information in a rule-based machine 
translation system. It includes three steps. (1) A 
statistical translation model is employed to per-
form word alignment in two directions1 (English 
to Chinese, Chinese to English). (2) A rule-based 
English to Chinese translation system is em-
ployed to obtain Chinese translations for each 
English word or phrase in the source language. (3) 
The translation information in step (2) is used to 
improve the word alignment results in step (1).  
A critical reader may pose the question ?why 
1 We use English-Chinese word alignment as a case study.  
not use a translation dictionary to improve statis-
tical word alignment?? Compared with a transla-
tion dictionary, the advantages of a rule-based 
machine translation system lie in two aspects: (1) 
It can recognize the multi-word units, particularly 
separated phrases, in the source language. Thus, 
our method is able to handle the multi-word 
alignments with higher accuracy, which will be 
described in our experiments. (2) It can perform 
word sense disambiguation and select appropriate 
translations while a translation dictionary can 
only list all translations for each word or phrase. 
Experimental results show that our approach im-
proves word alignments in both precision and 
recall as compared with the state-of-the-art tech-
nologies. 
2 
                                                          
Statistical Word Alignment 
Statistical translation models (Brown, et al 1993) 
only allow word to word and multi-word to word 
alignments. Thus, some multi-word units cannot 
be correctly aligned. In order to tackle this prob-
lem, we perform translation in two directions 
(English to Chinese and Chinese to English) as 
described in Och and Ney (2000). The GIZA++ 
toolkit is used to perform statistical alignment. 
Thus, for each sentence pair, we can get two 
alignment results. We use  and  to represent 
the alignment sets with English as the source lan-
guage and Chinese as the target language or vice 
versa. For alignment links in both sets, we use i 
for English words and j for Chinese words. 
1S 2S
}0  },{|),{(1 ?== jjjj aaAjAS  
}0  },{|),{(2 ?== iiii aaAAiS  
 Where, represents the index posi-
tion of the source word aligned to the target word 
in position x. For example, if a Chinese word in 
position j is connected to an English word in po-
sition i, then . If a Chinese word in position 
j is connected to English words in positions i  
and , then .
),( jixax =
ia j =
,{ 21 iiA j =
)1( >k
1
2i }
2  We call an element in 
the alignment set an alignment link. If the link 
includes a word that has no translation, we call it 
a null link. If k words have null links, we 
treat them as k different null links, not just one 
link. 
2 In the following of this paper, we will use the position 
number of a word to refer to the word.   
Based on  and , we obtain their intersec-
tion set, union set and subtraction set.  
1S 2S
Intersection: 21 SSS ?=  
Union: 21 SSP ?=  
Subtraction: S?= PF  
Thus, the subtraction set contains two differ-
ent alignment links for each English word.  
3 Rule-Based Translation System 
We use the translation information in a rule-
based English-Chinese translation system3 to im-
prove the statistical word alignment result. This 
translation system includes three modules: source 
language parser, source to target language trans-
fer module, and target language generator.  
From the transfer phase, we get Chinese trans-
lation candidates for each English word. This 
information can be considered as another word 
alignment result, which is denoted as 
)},{(3 kCkS = . C  the set including the trans-
lation candidates for the k-th  English word or 
phrase. The difference between S  and the 
common alignment set is that each English word 
or phrase in S  has one or more translation can-
didates. A translation example for the English 
sentence ?He is used to pipe smoking.? is shown 
in Table 1.  
k  is
3
3
English Words Chinese Translations 
He ? 
is used to ?? 
pipe ????? 
smoking ???? 
Table 1. Translation Example 
From Table 1, it can be seen that (1) the trans-
lation system can recognize English phrases (e.g. 
is used to); (2) the system can provide one or 
more translations for each source word or phrase; 
(3) the translation system can perform word se-
lection or word sense disambiguation. For exam-
ple, the word ?pipe? has several meanings such 
as ?tube?, ?tube used for smoking? and ?wind 
instrument?. The system selects ?tube used for 
smoking? and translates it into Chinese words 
???? and ????. The recognized translation 
                                                          
3 This system is developed based on the Toshiba English-
Japanese translation system (Amano et al 1989). It achieves 
above-average performance as compared with the English-
Chinese translation systems available in the market. 
candidates will be used to improve statistical 
word alignment in the next section.  
4 
4.1 
Word Alignment Improvement 
As described in Section 2, we have two align-
ment sets for each sentence pair, from which we 
obtain the intersection set S  and the subtraction 
set . We will improve the word alignments in S 
and  with the translation candidates produced 
by the rule-based machine translation system. In 
the following sections, we will first describe how 
to calculate monolingual word similarity used in 
our algorithm. Then we will describe the algo-
rithm used to improve word alignment results.  
F
F
Word Similarity Calculation 
This section describes the method for monolin-
gual word similarity calculation. This method 
calculates word similarity by using a bilingual 
dictionary, which is first introduced by Wu and 
Zhou (2003). The basic assumptions of this 
method are that the translations of a word can 
express its meanings and that two words are simi-
lar in meanings if they have mutual translations. 
Given a Chinese word, we get its translations 
with a Chinese-English bilingual dictionary. The 
translations of a word are used to construct its 
feature vector. The similarity of two words is 
estimated through their feature vectors with the 
cosine measure as shown in (Wu and Zhou 2003). 
If there are a Chinese word or phrase w  and a 
Chinese word set Z , the word similarity between 
them is calculated as shown in Equation (1). 
))',((),(
'
wwsimMaxZwsim
Zw?
=  (1)
4.2 Alignment Improvement Algorithm 
As the word alignment links in the intersection 
set are more reliable than those in the subtraction 
set, we adopt two different strategies for the 
alignments in the intersection set S  and the sub-
traction set . For alignments in S, we will mod-
ify them when they are inconsistent with the 
translation information in S . For alignments in 
, we classify them into two cases and make se-
lection between two different alignment links or 
modify them into a new link. 
F
3
F
In the intersection set S , there are only word 
to word alignment links, which include no multi-
word units. The main alignment error type in this 
set is that some words should be combined into 
one phrase and aligned to the same word(s) in the 
target sentence. For example, for the sentence 
pair in Figure 1, ?used? is aligned to the Chinese 
word ????, and ?is? and ?to? have null links in 
. But in the translation set , ?is used to" is a 
phrase. Thus, we combine the three alignment 
links into a new link. The words ?is?, ?used? and 
? to? are all aligned to the Chinese word ????, 
denoted as (is used to, ??). Figure 2 describes 
the algorithm employed to improve the word 
alignment in the intersection set S . 
S 3S
)j
 phk ,
3S
 
 
Figure 1. Multi-Word Alignment Example 
Input: Intersection set S , Translation set , 3S
            Final word alignment set WA  
For each alignment link?  in , do:  ,i S
(1) If all of the following three conditions are 
satisfied, add the new alignment link 
WA phk ??? w,  to WA . 
a) There is an element? , and 
the English word i is a constituent of the 
phrase  . 
3) SCk ?
kph
b) The other words in the phrase ph  also 
have alignment links in S .  
k
c) For each word s in ph , we get k
}),|{ St(stT ?= and combine 4  all words 
in T into a phrase w , and the similar-
ity 1),( ?>kCwsim .  
(2) Otherwise, add?  to WA .  ), ji
Output: Word alignment set WA  
Figure 2. Algorithm for the Intersection Set 
In the subtraction set, there are two different 
links for each English word. Thus, we need to 
select one link or to modify the links according to 
the translation information in .  
For each English word i in the subtraction set, 
there are two cases:  
                                                         
4 We define an operation ?combine? on a set consisting of 
position numbers of words. We first sort the position num-
bers in the set ascendly and then regard them as a phrase. 
For example, there is a set {{2,3}, 1, 4}, the result after 
applying the combine operation is (1, 2, 3, 4). 
Case 1: In , there is a word to word alignment 
link? . In , there is a word to word or 
word to multi-word alignment link
1S
1S ), ji ? 2S
 2), SAi i ?? 5.  
Case 2: In , there is a multi-word to word 
alignment link ( . In S , there 
is a word to word or word to multi-word align-
ment link? . 
1S
, Ai
jj AiSjA ?? &), 1
2) S?
2
 i
For Case 1, we first examine the translation 
set . If there is an element? , we cal-
culate the Chinese word similarity between j in 
 and  with Equation (1) shown in 
Section 4.1. We also combine the words in A  
) into a phrase and get the word simi-
larity between this new phrase and C . The align-
ment link with a higher similarity score is 
selected and added to WA .  
3S
)j ?
), Ai
 3), SCi i ?
i
1, Si?
( Si ??
iC
i
2
Input: Alignment sets S  and  1 2S
Translation unit?   3), SCph kk ?
(1) For each sub-sequence6 s of , get the 
sets and 
 
kph
}1)?,(|{ 111 StstT =
}) 22 St ?,(|{ 22 stT =
(2) Combine words in T  and T  into phrases 
 and  respectively. 
1 2
1w 2w
(3) Obtain the word similarities 
 and .  ),Csim(wws k11 = ),Csim(wws k22 =
(4) Add a new alignment link to WA  
according to the following steps. 
a) If ws and 21 ws> 11 ?>ws , add ?  
to WA ;  
), 1wphk
b) If ws  and 12 ws> 12 ?>ws , add?  
to WA ;  
 ), 2wphk
c) If 121 ?>= wsws
), 2wk
, add ?  or 
 to WA  randomly. 
), 1wphk
ph?
Output: Updated alignment set WA  
Figure 3. Multi-Word to Multi-Word Align-
ment Algorithm 
If, in S , there is an element?  and i 
is a constituent of , the English word i of the 
alignment links in both S  and  should be 
3  ), kk Cph
2S
kph
1
combined with other words to form phrases. In 
this case, we modify the alignment links into a 
multi-word to multi-word alignment link. The 
algorithm is described in Figure 3. 
                                                          
5  ? ), iAi represents both the word to word and word to 
multi-word alignment links. 
6  If a phrase consists of three words w , the sub-
sequences of this phrase are w . 
321 ww
221 ,, www 3321 ,, www
For example, given a sentence pair in Figure 4,  
in S , the word ?whipped? is aligned to ???? 
and ?out? is aligned to ????. In S , the word 
?whipped? is aligned to both ???? and ???? 
and ?out? has a null link. In , ?whipped out? is 
a phrase and translated into ?????". And the 
word similarity between ?????? and ????
?? is larger than the threshold 
1
2
1
3S
? . Thus, we 
combine the aligned target words in the Chinese 
sentence into ??????. The final alignment 
link should be (whipped out, ?? ??). 
 
Figure 4. Multi-Word to Multi-Word Alignment 
Example 
For Case 2, we first examine S  to see 
whether there is an element? . If true, 
we combine the words in  (? ) into a 
word or phrase and calculate the similarity be-
tween this new word or phrase and C  in the 
same way as in Case 1. If the similarity is higher 
than a threshold 
3
3S
2S?
i
 ), Ci i ?
), Ai iiA
1? , we add the alignment link 
 into WA .  ), iAi?
If there is an element?  and i is a 
constituent of ph , we combine the English 
words in A  ( ) into a phrase. If it is 
the same as the phrase  and 
 3), SCph kk ?
1
kph (
k
, jA jj )( S?
1), ?>kCjsim , 
we add (  into WA . Otherwise, we use the 
multi-word to multi-word alignment algorithm in 
Figure 3 to modify the links.  
),A j j
After applying the above two strategies, there 
are still some words not aligned. For each sen-
tence pair, we use E and C to denote the sets of 
the source words and the target words that are not 
aligned, respectively. For each source word in E, 
we construct a link with each target word in C. 
We use L },|),{( CjEiji ??=  to denote the 
alignment candidates. For each candidate in L, 
we look it up in the translation set S . If there is 
an element 
3
3), SCi i ??  and 2)( , ?>j iCsim , we 
add the link into the set WA . 
|C
|| C
C
S
S?
S
C
C =
|
|
5 Experiments 
5.1 
5.2 
Training and Testing Set 
We did experiments on a sentence aligned Eng-
lish-Chinese bilingual corpus in general domains. 
There are about 320,000 bilingual sentence pairs 
in the corpus, from which, we randomly select 
1,000 sentence pairs as testing data. The remain-
der is used as training data.  
The Chinese sentences in both the training set 
and the testing set are automatically segmented 
into words. The segmentation errors in the testing 
set are post-corrected. The testing set is manually 
annotated. It has totally 8,651 alignment links 
including 2,149 null links. Among them, 866 
alignment links include multi-word units, which 
accounts for about 10% of the total links. 
Experimental Results 
There are several different evaluation methods 
for word alignment (Ahrenberg et al 2000). In 
our evaluation, we use evaluation metrics similar 
to those in Och and Ney (2000). However, we do 
not classify alignment links into sure links and 
possible links. We consider each alignment as a 
sure link.  
If we use S  to indicate the alignments iden-
tified by the proposed methods and S  to denote 
the reference alignments, the precision, recall and 
f-measure are calculated as described in Equation 
(2), (3) and (4). According to the definition of the 
alignment error rate (AER) in Och and Ney 
(2000), AER can be calculated with Equation (5).  
G
C
|S|
SS|
G
G ?=precision       (2)
|S|
 |SS|
C
CG ?=recall    (3)
||
||*2
G
G
S
S
fmeasure +=  (4)
fmeasure
SS
S
AER
G
G ?+
??= 1
|||
|*2
1 (5)
In this paper, we give two different alignment 
results in Table 2 and Table 3. Table 2 presents 
alignment results that include null links. Table 3 
presents alignment results that exclude null links. 
The precision and recall in the tables are obtained 
to ensure the smallest AER for each method. 
 Precision Recall AER 
Ours 0.8531 0.7057 0.2276 
Dic 0.8265 0.6873 0.2495 
IBM E-C 0.7121 0.6812 0.3064 
IBM C-E 0.6759 0.7209 0.3023 
IBM Inter 0.8756 0.5516 0.3233 
IBM Refined 0.7046 0.6532 0.3235 
Table 2. Alignment Results Including Null Links 
 Precision Recall AER 
Ours 0.8827 0.7583 0.1842 
Dic 0.8558 0.7317 0.2111 
IBM E-C 0.7304 0.7136 0.2781 
IBM C-E 0.6998 0.6725 0.3141 
IBM Inter 0.9392 0.5513 0.3052 
IBM refined 0.8152 0.6926 0.2505 
Table 3. Alignment Results Excluding Null Links 
In the above tables, the row ?Ours? presents 
the result of our approach. The results are ob-
tained by setting the word similarity thresholds to 
1.01??  and 5.02?? . The Chinese-English dic-
tionary used to calculate the word similarity has 
66,696 entries. Each entry has two English trans-
lations on average. The row ?Dic? shows the re-
sult of the approach that uses a bilingual 
dictionary instead of the rule-based machine 
translation system to improve statistical word 
alignment. The dictionary used in this method is 
the same translation dictionary used in the rule-
based machine translation system. It includes 
57,684 English words and each English word has 
about two Chinese translations on average. The 
rows ?IBM E-C? and ?IBM C-E? show the re-
sults obtained by IBM Model-4 when treating 
English as the source and Chinese as the target or 
vice versa. The row ?IBM Inter? shows results 
obtained by taking the intersection of the align-
ments produced by ?IBM E-C? and ?IBM C-E?. 
The row ?IBM Refined? shows the results by 
refining the results of ?IBM Inter? as described in 
Och and Ney (2000). 
Generally, the results excluding null links are 
better than those including null links. This indi-
cates that it is difficult to judge whether a word 
has counterparts in another language. It is be-
cause the translations of some source words can 
be omitted. Both the rule-based translation sys-
tem and the bilingual dictionary provide no such 
information.  
It can be also seen that our approach performs 
the best among others in both cases. Our ap-
proach achieves a relative error rate reduction of 
26% and 25% when compared with ?IBM E-C? 
and ?IBM C-E? respectively7. Although the pre-
cision of our method is lower than that of the 
?IBM Inter? method, it achieves much higher 
recall, resulting in a 30% relative error rate re-
duction. Compared with the ?IBM refined? 
method, our method also achieves a relative error 
rate reduction of 30%. In addition, our method is 
better than the ?Dic? method, achieving a relative 
error rate reduction of 8.8%.  
In order to provide the detailed word align-
ment information, we classify word alignment 
results in Table 3 into two classes. The first class 
includes the alignment links that have no multi-
word units. The second class includes at least one 
multi-word unit in each alignment link. The de-
tailed information is shown in Table 4 and Table 
5. In Table 5, we do not include the method ?In-
ter? because it has no multi-word alignment links.  
 Precision Recall AER 
Ours 0.9213 0.8269 0.1284 
Dic 0.8898 0.8215 0.1457 
IBM E-C 0.8202 0.7972 0.1916 
IBM C-E 0.8200 0.7406 0.2217 
IBM Inter 0.9392 0.6360 0.2416 
IBM Refined 0.8920 0.7196 0.2034 
Table 4. Single Word Alignment Results 
 Precision Recall AER 
Ours 0.5123 0.3118 0.6124 
Dic 0.3585 0.1478 0.7907 
IBM E-C 0.1682 0.1697 0.8311 
IBM C-E 0.1718 0.2298 0.8034 
IBM Refined 0.2105 0.2910 0.7557 
Table 5. Multi-Word Alignment Results  
All of the methods perform better on single 
word alignment than on multi-word alignment. In 
Table 4, the precision of our method is close to 
the ?IBM Inter? approach, and the recall of our 
method is much higher, achieving a 47% relative 
error rate reduction. Our method also achieves a 
37% relative error rate reduction over the ?IBM 
Refined? method.  Compared with the ?Dic? 
method, our approach achieves much higher pre-
cision without loss of recall, resulting in a 12% 
relative error rate reduction. 
                                                          
6 Discussion 
7 The error rate reductions in this paragraph are obtained 
from Table 2. The error rate reductions in Table 3 are 
omitted.  
Our method also achieves much better results 
on multi-word alignment than other methods. 
However, our method only obtains one third of 
the correct alignment links. It indicates that it is 
the hardest to align the multi-word units. 
Readers may pose the question ?why the rule-
based translation system performs better on word 
alignment than the translation dictionary?? For 
single word alignment, the rule-based translation 
system can perform word sense disambiguation, 
and select the appropriate Chinese words as 
translation. On the contrary, the dictionary can 
only list all translations. Thus, the alignment pre-
cision of our method is higher than that of the 
dictionary method. Figure 5 shows alignment 
precision and recall values under different simi-
larity values for single word alignment including 
null links. From the figure, it can be seen that our 
method consistently achieves higher precisions as 
compared with the dictionary method. The t-
score value (t=10.37, p=0.05) shows the im-
provement is statistically significant.   
Figure 5. Recall-Precision Curves  
For multi-word alignment links, the translation 
system also outperforms the translation diction-
ary. The result is shown in Table 5 in Section 5.2. 
This is because (1) the translation system can 
automatically recognize English phrases with 
higher accuracy than the translation dictionary; (2) 
The translation system can detect separated 
phrases while the dictionary cannot. For example, 
for the sentence pairs in Figure 6, the solid link 
lines describe the alignment result of the rule-
base translation system while dashed lines indi-
cate the alignment result of the translation dic-
tionary. In example (1), the phrase ?be going to? 
indicates the tense not the phrase ?go to? as the 
dictionary shows. In example (2), our method 
detects the separated phrase ?turn ? on? while 
the dictionary does not. Thus, the dictionary 
method produces the wrong alignment link. 
 
Figure 6. Alignment Comparison Examples 
7 Conclusion and Future Work 
This paper proposes an approach to improve sta-
tistical word alignment results by using a rule-
based translation system. Our contribution is that, 
given a rule-based translation system that pro-
vides appropriate translation candidates for each 
source word or phrase, we select appropriate 
alignment links among statistical word alignment 
results or modify them into new links. Especially, 
with such a translation system, we can identify 
both the continuous and separated phrases in the 
source language and improve the multi-word 
alignment results. Experimental results indicate 
that our approach can achieve a precision of 85% 
and a recall of 71% for word alignment including 
null links in general domains. This result signifi-
cantly outperforms those of the methods that use 
a bilingual dictionary to improve word alignment, 
and that only use statistical translation models. 
Our future work mainly includes three tasks. 
First, we will further improve multi-word align-
ment results by using other technologies in natu-
ral language processing. For example, we can use 
named entity recognition and transliteration tech-
nologies to improve person name alignment. Sec-
ond, we will extract translation rules from the 
improved word alignment results and apply them 
back to our rule-based machine translation sys-
tem. Third, we will further analyze the effect of 
the translation system on the alignment results. 
References 
Lars Ahrenberg, Magnus Merkel, and Mikael Anders-
son 1998. A Simple Hybrid Aligner for Generating 
Lexical Correspondences in Parallel Texts. In Proc. 
of the 36th Annual Meeting of the Association for 
Computational Linguistics and the 17th Int. Conf. 
on Computational Linguistics, pp. 29-35. 
 Lars Ahrenberg, Magnus Merkel, Anna Sagvall Hein 
and Jorg Tiedemann 2000. Evaluation of word 
alignment systems. In Proc. of the Second Int. Conf. 
on Linguistic Resources and Evaluation, pp. 1255-
1261. 
ShinYa Amano, Hideki Hirakawa, Hiroyasu Nogami, 
and Akira Kumano 1989. Toshiba Machine Trans-
lation System. Future Computing Systems, 
2(3):227-246. 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra and Robert L. Mercer 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2):263-311. 
Colin Cherry and Dekang Lin 2003. A Probability 
Model to Improve Word Alignment. In Proc. of the 
41st Annual Meeting of the Association for Com-
putational Linguistics, pp. 88-95. 
I. Dan Melamed 1996. Automatic Construction of 
Clean Broad-Coverage Translation Lexicons. In 
Proc. of the 2nd Conf. of the Association for Ma-
chine Translation in the Americas, pp. 125-134. 
I. Dan Melamed 2000. Word-to-Word Models of 
Translational Equivalence among Words. Compu-
tational Linguistics, 26(2): 221-249. 
Arul Menezes and Stephan D. Richardson 2001. A 
Best-first Alignment Algorithm for Automatic Ex-
traction of Transfer Mappings from Bilingual Cor-
pora. In Proc. of the ACL 2001 Workshop on Data-
Driven Methods in Machine Translation, pp. 39-46. 
Franz Josef Och and Hermann Ney 2000. Improved 
Statistical Alignment Models. In Proc.of the 38th 
Annual Meeting of the Association for Computa-
tional Linguistics, pp. 440-447. 
Harold Somers 1999. Review Article: Example-Based 
Machine Translation. Machine Translation 14:113-
157. 
Jorg Tiedemann 1999. Word Alignment ? Step by Step. 
In Proc. of the 12th Nordic Conf. on Computational 
Linguistics, pp. 216-227. 
Dan Tufis and Ana Maria Barbu. 2002. Lexical Token 
Alignment: Experiments, Results and Application. 
In Proc. of the Third Int. Conf. on Language Re-
sources and Evaluation, pp. 458-465. 
Dekai Wu 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3):377-403. 
Hua Wu and Ming Zhou 2003. Optimizing Synonym 
Extraction Using Monolingual and Bilingual Re-
sources. In Proc. of the 2nd Int. Workshop on Para-
phrasing, pp. 72-79. 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 297?304
Manchester, August 2008
Dependency-Based N-Gram Models for
General Purpose Sentence Realisation
Yuqing Guo
NCLT, School of Computing
Dublin City University
Dublin 9, Ireland
yguo@computing.dcu.ie
Josef van Genabith
NCLT, School of Computing
Dublin City University
IBM CAS, Dublin, Ireland
josef@computing.dcu.ie
Haifeng Wang
Toshiba (China)
Research & Development Center
Beijing, 100738, China
wanghaifeng@rdc.toshiba.com.cn
Abstract
We present dependency-based n-gram
models for general-purpose, wide-
coverage, probabilistic sentence realisa-
tion. Our method linearises unordered
dependencies in input representations
directly rather than via the application
of grammar rules, as in traditional chart-
based generators. The method is simple,
efficient, and achieves competitive accu-
racy and complete coverage on standard
English (Penn-II, 0.7440 BLEU, 0.05
sec/sent) and Chinese (CTB6, 0.7123
BLEU, 0.14 sec/sent) test data.
1 Introduction
Sentence generation,1 or surface realisation can be
described as the problem of producing syntacti-
cally, morphologically, and orthographically cor-
rect sentences from a given semantic or syntactic
representation.
Most general-purpose realisation systems de-
veloped to date transform the input into sur-
face form via the application of a set of gram-
mar rules based on particular linguistic theories,
e.g. Lexical Functional Grammar (LFG), Head-
Driven Phrase Structure Grammar (HPSG), Com-
binatory Categorial Grammar (CCG), Tree Ad-
joining Grammar (TAG) etc. These grammar rules
are either carefully handcrafted, as those used in
FUF/SURGE (Elhadad, 1991), LKB (Carroll et al,
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1In this paper, the term ?generation? is used generally for
what is more strictly referred to by the term ?tactical genera-
tion? or ?surface realisation?.
1999), OpenCCG (White, 2004) and XLE (Crouch
et al, 2007), or created semi-automatically (Belz,
2007), or fully automatically extracted from an-
notated corpora, like the HPSG (Nakanishi et
al., 2005), LFG (Cahill and van Genabith, 2006;
Hogan et al, 2007) and CCG (White et al,
2007) resources derived from the Penn-II Treebank
(PTB) (Marcus et al, 1993).
Over the last decade, probabilistic models have
become widely used in the field of natural lan-
guage generation (NLG), often in the form of a re-
alisation ranker in a two-stage generation architec-
ture. The two-stage methodology is characterised
by a separation between generation and selection,
in which rule-based methods are used to generate a
space of possible paraphrases, and statistical meth-
ods are used to select the most likely realisation
from the space. By and large, two statistical mod-
els are used in the rankers to choose output strings:
? N-gram language models over different units,
such as word-level bigram/trigram mod-
els (Bangalore and Rambow, 2000; Langk-
ilde, 2000), or factored language models inte-
grated with syntactic tags (White et al, 2007).
? Log-linear models with different syntactic
and semantic features (Velldal and Oepen,
2005; Nakanishi et al, 2005; Cahill et al,
2007).
To date, however, probabilistic models learn-
ing direct mappings from generation input to sur-
face strings, without the effort to construct a gram-
mar, have rarely been explored. An exception is
Ratnaparkhi (2000), who presents maximum en-
tropy models to learn attribute ordering and lexi-
cal choice for sentence generation from a semantic
representation of attribute-value pairs, restricted to
an air travel domain.
297
SNP
PRP
We
VP
VBP
believe
PP
IN
in
NP
NP
DT
the
NN
law
PP
IN
of
NP
NNS
averages
f
1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
PRED ?believe?
TENSE pres
SUBJ f
2
?
?
?
PRED ?pro?
PERS 1
NUM pl
?
?
?
OBL f
3
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
PFORM ?in?
OBJ f
4
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
PRED ?law?
PERS 3
NUM sg
SPEC f
5
[
DET f
6
[
PRED ?the?
]
]
ADJ
?
?
?
?
?
?
?
?
f
7
?
?
?
?
?
?
PFORM ?of?
OBJ f
8
?
?
?
PRED ?average?
PERS 3
NUM pl
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(a.) c-structure (b.) f-structure
string We believe in the law of averages
position 1 2 3 4 5 6 7
f
1
SUBJ PRED OBL
f
3
PFORM OBJ
f
4
SPEC PRED ADJ
f
7
PFORM OBJ
(c.) linearised grammatical functions / bilexical dependencies
Figure 1: C- and f-structures for the sentence We believe in the law of averages.
In this paper, we develop an efficient, wide-
coverage generator based on simple n-gram mod-
els to directly linearise dependency relations from
the input representations. Our work is aimed at
general-purpose sentence generation but couched
in the framework of Lexical Functional Grammar.
We give an overview of LFG and the dependency
representations we use in Section 2. We describe
the general idea of our dependency-based gener-
ation in Section 3 and give details of the n-gram
generation models in Section 4. Section 5 explains
the experiments and provides results for both En-
glish and Chinese data. Section 6 compares the re-
sults with previous work and between languages.
Finally we conclude with a summary and outline
future work.
2 LFG-Based Generation
2.1 Lexical Functional Grammar
Lexical Functional Grammar (Kaplan and Bres-
nan, 1982) is a constraint-based grammar for-
malism which postulates (minimally) two lev-
els of representation: c(onstituent)-structure and
f(unctional)-structure. As illustrated in Figure 1,
a c-structure is a conventional phrase structure
tree and captures surface grammatical configu-
rations. The f-structure encodes more abstract
functional relations like SUBJ(ect), OBJ(ect) and
ADJ(unct). F-structures are hierarchical attribute-
value matrix representations of bilexical labelled
dependencies, approximating to basic predicate-
argument/adjunct structures.2 Attributes in f-
structure come in two different types:
? Grammatical Functions (GFs) indicate the re-
lationship between the predicate and depen-
dents. GFs can be divided into:
? arguments are subcategorised for by the
predicate, such as SUBJ(ect), OBJ(ect),
and thus can only occur once in each lo-
cal f-structure.
? modifiers like ADJ(unct), COORD(inate)
are not subcategorised for by the predi-
cate, and can occur any number of times
in a local f-structure.
? Atomic-valued features describe linguistic
properties of the predicate, such as TENSE,
ASPECT, MOOD, PERS, NUM, CASE etc.
2.2 Generation from F-Structures
Work on generation in LFG generally assumes that
the generation task is to determine the set of strings
of the language that corresponds to a specified f-
structure, given a particular grammar (Kaplan and
Wedekind, 2000). Previous work on generation
2F-structures can be also interpreted as quasi-logical
forms (van Genabith and Crouch, 1996), which more closely
resemble inputs used by some other generators.
298
within LFG includes the XLE,3 Cahill and van
Genabith (2006), Hogan et al (2007) and Cahill et
al. (2007). The XLE generates sentences from f-
structures according to parallel handcrafted gram-
mars for English, French, German, Norwegian,
Japanese, and Urdu. Based on the German XLE
resources, Cahill et al (2007) describe a two-stage,
log-linear generation model. Cahill and van Gen-
abith (2006) and Hogan et al (2007) present a
chart generator using wide-coverage PCFG-based
LFG approximations automatically acquired from
treebanks (Cahill et al, 2004).
3 Dependency-Based Generation: the
Basic Idea
Traditional LFG generation models can be re-
garded as the reverse process of parsing, and
use bi-directional f-structure-annotated CFG rules.
In a sense, the generation process is driven by
an input dependency (or f-structure) representa-
tion, but proceeds through the ?detour? of us-
ing dependency-annotated CFG (or PCFG) gram-
mars and chart-based generators. In this paper,
we develop a simple n-gram and dependency-
based, wide-coverage, robust, probabilistic gener-
ation model, which cuts out the middle-man from
previous approaches: the CFG-component.
Our approach is data-driven: following the
methodology in (Cahill et al, 2004; Guo et al,
2007), we automatically convert the English Penn-
II treebank and the Chinese Penn Treebank (Xue
et al, 2005) into f-structure banks. F-structures
such as Figure 1(b.) are unordered, i.e. they do
not carry information on to the relative surface or-
der of local GFs. In order to generate a string
from an f-structure, we need to linearise the GFs
(at each level of embedding) in the f-structure (and
map lemmas and features to surface forms). We
do this in terms of n-gram models over GFs. In or-
der to build the n-gram models, we linearise the f-
structures automatically produced from treebanks
by associating the numerical string position (word
offset from start of the sentence) with the predicate
in each local f-structure, producing GF sequences
as in Figure 1(c.).
Even though the n-gram models are exemplified
using LFG f-structures, they are general-purpose
models and thus suitable for any bilexical labelled
dependency (Nivre, 2006) or predicate-argument
type representations, such as the labelled feature-
3http://www2.parc.com/isl/groups/nltt/xle/
value structures used in HALogen and the func-
tional descriptions in the FUF/SURGE system.
4 N-Gram Models for Dependency-Based
Generation
4.1 Basic N-Gram Model
The primary task of a sentence generator is to de-
termine the linear order of constituents and words,
represented as lemmas in predicates in f-structures.
At a particular local f-structure, the task of gen-
erating a string covered by the local f-structure
is equivalent to linearising all the GFs present at
that local f-structure. E.g. in f
4
in Figure 1, the
unordered set of local GFs {SPEC, PRED, ADJ}
generates the surface sequence ?the law of aver-
ages?. We linearise the GFs in the set by com-
puting n-gram models, similar to traditional word-
based language models, except using the names of
GFs (including PRED) instead of words. Given
a (sub-) f-structure F containing m GFs, the n-
gram model searches for the best surface sequence
S
m
1
=s
1
...s
m
generated by the GF linearisation
GF
m
1
= GF
1
...GF
m
, which maximises the prob-
ability P (GFm
1
). Using n-gram models, P (GFm
1
)
is calculated according to Eq.(1).
P (GF
m
1
) = P (GF
1
...GF
m
)
=
m
?
k=1
P (GF
k
|GF
k?1
k?n+1
) (1)
4.2 Factored N-Gram Models
In addition to the basic n-gram model over bare
GFs, we integrate contextual and fine-grained
lexical information into several factored models.
Eq.(2) additionally conditions the probability of
the n-gram on the parent GF label of the cur-
rent local f-structure f
i
, Eq.(3) on the instantiated
PRED of the local f-structure f
i
, and Eq.(4) lexi-
calises the model, where each GF is augmented
with its own predicate lemma.
P
g
(GF
m
1
) =
m
?
k=1
P (GF
k
|GF
k?1
k?n+1
, GF
i
) (2)
P
p
(GF
m
1
) =
m
?
k=1
P (GF
k
|GF
k?1
k?n+1
, P red
i
) (3)
P
l
(GF
m
1
) =
m
?
k=1
P (Lex
k
|Lex
k?1
k?n+1
) (4)
299
To avoid data sparseness, the factored n-gram
models P f are smoothed by linearly interpolating
the basic n-gram model P , as in Eq.(5).
?
P
f
(GF
m
1
) = ?P
f
(GF
m
1
) + (1? ?)P (GF
m
1
) (5)
Additionally, the lexicalised n-gram models P l
are combined with the other two models con-
ditioned on the additional parent GF P g and
PRED P p, as shown in Eqs. (6) & (7), respectively.
?
P
lg
(GF
m
1
) = ?
1
P
l
(GF
m
1
) + ?
2
P
g
(GF
m
1
)
+?
3
P (GF
m
1
) (6)
?
P
lp
(GF
m
1
) = ?
1
P
l
(GF
m
1
) + ?
2
P
p
(GF
m
1
)
+?
3
P (GF
m
1
) (7)
where
?
?
i
= 1
Table 1 exemplifies the different n-gram models
for the local f-structure f
4
in Figure 1.
Model N-grams Cond.
basic (P ) SPEC PRED ADJ
gf (P g ) SPEC PRED ADJ OBL
pred (Pp) SPEC PRED ADJ ?law?
lex (P l) SPEC PRED[?law?] ADJ[?of?]
Table 1: Examples of n-grams for f
4
in Figure 1
Besides grammatical functions, we also make
use of atomic-valued features like TENSE, PERS,
NUM (etc.) to aid linearisation. The attributes and
values of these features are integrated into the GF
n-grams for disambiguation (see Section 5.2).
4.3 Generation Algorithm
Our basic n-gram based generation model im-
plements the simplifying assumption that lineari-
sation at one sub-f-structure is independent of
linearisation at any other sub-f-structures. This
assumption is feasible for projective dependen-
cies. In most cases (at least in English and
Chinese), non-projective dependencies are only
used to account for Long-Distance Dependen-
cies (LDDs). Consider sentence (1) discussed
in Carroll et al (1999) and its corresponding f-
structure in Figure 2. In LFG f-structures, LDDs
are represented via reentrancies between ?dislo-
cated? TOPIC, TOPIC REL, FOCUS (etc.) GFs and
?source? GFs subcategorised for by local predi-
cates, but only the dislocated GFs are instantiated
in generation. Therefore traces of the source GFs
in input f-structures are removed before genera-
tion, and non-projective dependencies are trans-
formed into simple projective dependencies.
(1) How quickly did the newspapers say the ath-
lete ran?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
FOCUS
?
?
?
PRED ?quickly?
ADJ
{
[
PRED ?how?
]
}
?
?
?
1
PRED ?say?
SUBJ
?
?
PRED ?newspaper?
SPEC
[
PRED ?the?
]
?
?
COMP
?
?
?
?
?
?
?
?
PRED ?run?
SUBJ
?
?
PRED ?athlete?
SPEC
[
PRED ?the?
]
?
?
ADJ 1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 2: schematic f-structure for How quickly
did the newspapers say the athlete ran?
In summary, given an input f-structure f , the
core algorithm of the generator recursively tra-
verses f and at each sub-f-structure f
i
:
1. instantiates the local predicate at f
i
and per-
forms inflections/declensions if necessary
2. calculates the GF linearisations present at f
i
by n-gram models
3. finds the most probable GF sequence among
all possibilities by Viterbi search
4. generates the string covered by f
i
according
to the linearised GFs
5 Experiments and Evaluation
To test the performance and coverage of our n-
gram-based generation models, experiments are
carried out for both English and Chinese, two lan-
guages with distinct properties.
5.1 Experiment Design
Experiments on English data are carried out on
the WSJ portion of the PTB, using standard train-
ing/test/development splits, viz 39,832 sentences
from sections 02-21 are used for training, 2,416
sentences from section 23 for testing, while 1,700
sentences from section 22 are held out for develop-
ment. The latest version of the Penn Chinese Tree-
bank 6.0 (CTB6), excluding the portion of ACE
broadcast news, is used for experiments on Chi-
nese data.4 We follow the recommended splits (in
the list-of-file of CTB6) to divide the data into test
set, development set and training set. The training
set includes 756 files with a total of 15,663 sen-
tences. The test set includes 84 files with 1,708
4Sentences labelled as fragment are not included in our
development and test set.
300
sentences. The development set includes 50 files
with 1,116 sentences. Table 2 shows some of the
characteristics of the English and Chinese data ob-
tained from the development sets.
Development Set English Chinese
num of sent 1,700 1,116
max length of sent (#words) 110 145
ave length of sent (#words) 23 31
num of local fstr 23,289 15,847
num of local fstr per sent 13.70 14.20
max length of local fstr (#gfs) 12 16
ave length of local fstr (#gfs) 2.56 2.90
Table 2: Comparison English and Chinese data
The n-gram models are created using the
SRILM toolkit (Stolcke, 2002) with Good-Turning
smoothing for both the Chinese and English data.
For morphological realisation of English, a set of
lexical macros is automatically extracted from the
training data. This is not required for Chinese sur-
face realisation as Chinese has very little morphol-
ogy. Lexical macro examples are listed in Table 3.
lexical macro surface word
pred=law, num=sg, pers=3 law
pred=average, num=pl, pers=3 averages
pred=believe, num=pl, tense=pres believe
Table 3: Examples of lexical macros
The input to our generator are unordered f-
structures automatically derived from the develop-
ment and test set trees of our treebanks, which do
not contain any string position information. But,
due to the particulars of the automatic f-structure
annotation algorithm, the order of sub-f-structures
in set-valued GFs, such as ADJ, COORD, happens
to correspond to their surface order. To avoid un-
fairly inflating evaluation results, we lexically re-
order the GFs in each sub-f-structure of the devel-
opment and test input before the generation pro-
cess. This resembles the ?permute, no dir? type
experiment in (Langkilde, 2002).
5.2 Experimental Results
Following (Langkilde, 2002) and other work
on general-purpose generators, BLEU score (Pa-
pineni et al, 2002), average NIST simple
string accuracy (SSA) and percentage of exactly
matched sentences are adopted as evaluation met-
rics. As our system guarantees that all input f-
structures can generate a complete sentence, spe-
cial coverage-dependent evaluation (as has been
adopted in most grammar-based generation sys-
tems) is not necessary in our experiments.
Experiments are carried out on an Intel Pentium
4 server, with a 3.80GHz CPU and 3GB mem-
ory. It takes less than 2 minutes to generate all
2,416 sentences (with average sentence length of
21 words) of WSJ section 23 (average 0.05 sec per
sentence), and approximately 4 minutes to gener-
ate 1,708 sentences (with average sentence length
of 30 words) of CTB test data (average 0.14 sec
per sentence), using 4-gram models in all experi-
ments. Our evaluation results for English and Chi-
nese data are shown in Tables 4 and 5, respectively.
Different n-gram models perform nearly consis-
tently in all the experiments on both English and
Chinese data. The results show that factored n-
gram models outperform the basic n-gram models,
and in turn the combined n-gram models outper-
form single n-gram models. The combined model
interpolating n-grams over lexicalised GFs with n-
grams conditioned on PRED achieves the best re-
sults in both experiments on English (with feature
names) and Chinese (with feature names & val-
ues), with BLEU scores of 0.7440 and 0.7123 re-
spectively, and full coverage.
Lexicalisation plays an important role in both
English and Chinese, boosting the BLEU score
without features from 0.5074 to 0.6741 for En-
glish, and from 0.5752 to 0.6639 for Chinese.
Atomic-valued features play an important role
in English, and boost the BLEU score from 0.5074
in the baseline model to 0.6842 when feature
names are integrated into the n-gram models.
However, feature names in Chinese only increase
the BLEU score from 0.5752 to 0.6160. This
is likely to be the case as English has a richer
morphology than Chinese, and important func-
tion words such as ?if?, ?to?, ?that? are encoded
in atomic-valued features in English f-structures,
which helps to determine string order. However,
combined feature names and values work better on
Chinese data, but turn out to hurt the n-gram model
performance for English data. This may suggest
that the feature names in English already include
enough information, while the value of morpho-
logical features, such as TENSE, NUM does not pro-
vide any new information to help determine word
order, but aggravate data sparseness instead.
301
WSJ Sec23 Without Features Feature Names Feature Names & Values
Model ExMatch BLEU SSA ExMatch BLEU SSA ExMatch BLEU SSA
baseline 5.30% 0.5074 57.29% 15.27% 0.6842 69.48% 15.15% 0.6829 69.15%
gf 6.62% 0.5318 60.06% 16.76% 0.6969 71.51% 16.68% 0.6977 71.55%
pred 8.03% 0.5697 60.73% 16.72% 0.7035 70.12% 16.76% 0.7042 71.08%
lex 12.87% 0.6741 69.43% 19.41% 0.7384 74.76% 18.96% 0.7375 74.12%
lex+gf 12.62% 0.6611 69.41% 19.70% 0.7388 74.98% 19.74% 0.7405 75.08%
lex+pred 12.25% 0.6569 68.04% 19.83% 0.7440 75.34% 19.58% 0.7422 75.04%
Table 4: Results for English Penn-II WSJ section 23
Test Without Features Feature Names Feature Names & Values
Model ExMatch BLEU SSA ExMatch BLEU SSA ExMatch BLEU SSA
baseline 8.96% 0.5752 51.92% 11.77% 0.6160 54.64% 12.30% 0.6239 55.20%
gf 9.54% 0.6009 53.02% 12.53% 0.6391 55.78% 13.47% 0.6486 56.60%
pred 10.07% 0.6180 53.80% 13.35% 0.6608 56.72% 14.46% 0.6720 57.67%
lex 13.93% 0.6639 59.61% 15.16% 0.6770 60.44% 15.98% 0.6804 60.20%
lex+gf 14.81% 0.6773 59.92% 15.52% 0.6911 60.97% 16.80% 0.6957 61.07%
lex+pred 16.04% 0.6952 60.82% 16.22% 0.7060 61.45% 17.51% 0.7123 61.54%
Table 5: Results for Chinese CTB6 test data
WSJ Sec23 Sentence length ? 20 words All sentences
Coverage ExMatch BLEU SSA Coverage ExMatch BLEU SSA
Langkilde(2002) 82.7% 28.2% 0.757 69.6%
Callaway(2003) 98.7% 49.0% 88.84%
Nakanishi(2005) 90.75% 0.7733 83.6% 0.705
Cahill(2006) 98.65% 0.7077 73.73% 98.05% 0.6651 68.08%
Hogan(2007) 100% 0.7139 99.96% 0.6882 70.92%
White(2007) 94.3% 6.9% 0.5768
this paper 100% 35.40% 0.7625 81.09% 100% 19.83% 0.7440 75.34%
Table 6: Cross system comparison of results for English WSJ section 23
6 Discussion
6.1 Comparison to Previous Work
It is very difficult to compare sentence generators
since the information contained in the input rep-
resentation varies greatly between systems. The
most direct comparison is between our system and
those presented in Cahill and van Genabith (2006)
and Hogan et al (2007), as they also use treebank-
based automatically generated f-structures as the
generator inputs. The labelled feature-value struc-
tures used in HALogen (Langkilde, 2002) and
functional descriptions in FUF/SURGE (Callaway,
2003) also bear some broad similarities to our f-
structures. A number of systems using different
input but adopting the same evaluation metrics and
testing on the same data are listed in Table 6.
Surprisingly (or not), the best results are
achieved by a purely symbolic generation
system?FUF/SURGE (Callaway, 2003). How-
ever the approach uses handcrafted grammars
which are very time-consuming to produce and
adapt to different languages and domains. Langk-
ilde (2002) reports results for experiments with
varying levels of linguistic detail in the input
given to the generator. The type ?permute, no dir?
is most comparable to the level of information
contained in our f-structure in that the modifiers
(adjuncts, coordinates etc.) in the input are not
ordered. However her labelled feature-value
structure is more specific than our f-structure
as it also includes syntactic properties such as
part-of-speech, which might contribute to the
higher BLEU score of HALogen. And moreover,
in HALogen nearly 20% of the sentences are only
partially generated (or not at all). Nakanishi et
al. (2005) carry out experiments on sentences up
to 20 words, with BLEU scores slightly higher
than ours. However their results without sentence
length limitation (listed in the right column), for
500 sentences randomly selected from WSJ Sec22
are lower than ours, even at a lower coverage.
Overall our system is competitive, with best results
for coverage (100%), second best for BLEU and
SSA scores, and third best overall on exact match.
However, we admit that automatic metrics such as
BLEU are not fully reliable to compare different
systems, and results vary widely depending on the
coverage of the systems and the specificity of the
generation input.
302
6.2 Error Analysis and Differences Between
the Languages
Though our dependency-based n-gram models per-
form well in both the English and Chinese exper-
iments, we are surprised that experiments on En-
glish data produce better results than those for Chi-
nese. It is widely accepted that English generation
is more difficult than Chinese, due to morpholog-
ical inflections and the somewhat less predictable
word order of English compared to Chinese. This
is reflected by the results of the baseline models.
Chinese has a BLEU score of 0.5752 and 8.96%
exact match, both are higher than those of English.
However with feature augmentation and lexicali-
sation, the results for English data exceed Chinese.
This is probably because of the following reasons:
Data size of the English training set is more than
twice that of Chinese.
Grammatical functions are more fine-grained
in English f-structures than those in Chinese.
There are 32 GFs defined for English compared to
20 for Chinese in our input f-structures.
Properties of the languages and data sets are
different. For example, due to lack of inflection
and case markers, many sequences of VPs in Chi-
nese have to be treated as coordinates, whereas
their counterparts in English act as different gram-
matical functions, e.g. (2).
(2) ?? z ,?????
invest million build this construction
?invest million yuan to build the construction?
This results in a total of 7,377 coordinates (4.32
per sentence) in the Chinese development data,
compared to 2,699 (1.12 per sentence) in the En-
glish data. The most extreme case in the Chinese
data features 14 coordinates of country names in
a local f-structure. This may account for the low
SSA score for the Chinese experiments, as many
coordinates are tied in the n-gram scoring method
and can not be ordered correctly. Examining the
development data shows different types of coordi-
nation errors:
? syntactic coordinates, but not semantic coor-
dinates, as in sentence (2).
? syntactic and semantic coordinates, but usu-
ally expressed in a fixed order, e.g. (3).
(3) U? m?
reform opening-up
?reform and opening up?
? syntactic and semantic coordinates, which
can freely swap positions, e.g. (4).
(4) ? ?? ? ?$g?
plentiful energy and quick thinking
?energetic and agile?
At the current stage, our n-gram generation
model only keeps the most likely realisation for
each local f-structure. We believe that packing all
equivalent elements, like coordinates in a local f-
structure into equivalent classes, and outputing n-
best candidate realisations will greatly increase the
SSA score and may also further benefit the effi-
ciency of the algorithm.
7 Conclusions and Further Work
We have described a number of increasingly so-
phisticated n-gram models for sentence genera-
tion from labelled bilexical dependencies, in the
form of LFG f-structures. The models include
additional conditioning on parent GFs and differ-
ent degrees of lexicalisation. Our method is sim-
ple, highly efficient, broad coverage and accurate
in practice. We present experiments on English
and Chinese, showing that the method generalises
well to different languages and data sets. We are
currently exploring further combinations of con-
ditioning context and lexicalisation, application to
different languages and to dependency represen-
tations used to train state-of-the-art dependency
parsers (Nivre, 2006).
Acknowledgments
This research is funded by Science Foundation Ire-
land grant 04/IN/I527. We thank Aoife Cahill for
providing the treebank-based LFG resources for
the English data. We gratefully acknowledge the
feedback provided by our anonymous reviewers.
References
Bangalore, Srinivas and Rambow, Owen. 2000. Ex-
ploiting a Probabilistic Hierarchical Model for Gen-
eration. Proceedings of the 18th International
Conference on Computational Linguistics, 42?48.
Saarbru?cken, Germany.
Belz, Anja. 2007. Probabilistic Generation of Weather
Forecast Texts. Proceedings of the Conference of the
North American Chapter of the Association for Com-
putational Linguistics, 164?171. New York.
Cahill, Aoife, Burke, Michael, O?Donovan, Ruth, van
Genabith, Josef and Way, Andy. 2004. Long-
Distance Dependency Resolution in Automatically
303
Acquired Wide-Coverage PCFG-Based LFG Ap-
proximations. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics, 320-327. Barcelona, Spain.
Cahill, Aoife and van Genabith, Josef. 2006. Ro-
bust PCFG-Based Generation Using Automatically
Acquired LFG Approximations. Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, 1033?1040.
Sydney, Australia.
Cahill, Aoife, Forst, Martin and Rohrer, Christian.
2007. Stochastic Realisation Ranking for a Free
Word Order Language. Proceedings of the 11th Eu-
ropean Workshop on Natural Language Generation,
17?24. Schloss Dagstuhl, Germany.
Callaway, Charles B.. 2003. Evaluating Coverage for
Large Symbolic NLG Grammars. Proceedings of the
Eighteenth International Joint Conference on Artifi-
cial Intelligence, 811?817. Acapulco, Mexico.
Carroll, John, Copestake, Ann, Flickinger, Dan and
Poznanski, Victor. 1999. An efficient chart gen-
erator for (semi-)lexicalist grammars. Proceedings
of the 7th European Workshop on Natural Language
Generation, 86?95. Toulouse, France.
Crouch, Dick, Dalrymple, Mary, Kaplan, Ron, King,
Tracy, Maxwell, John and Newman, Paula. 2007.
XLE Documentation. Palo Alto Research Center,
CA.
Elhadad, Michael. 1991. FUF: The universal unifier
user manual version 5.0. Technical Report CUCS-
038-91. Dept. of Computer Science, Columbia Uni-
versity.
Guo, Yuqing and van Genabith, Josef and Wang,
Haifeng. 2007. Treebank-based Acquisition of LFG
Resources for Chinese. Proceedings of LFG07 Con-
ference, 214?232. Stanford, CA, USA.
Hogan, Deirdre Cafferkey, Conor Cahill, Aoife and van
Genabith, Josef. 2007. Exploiting Multi-Word Units
in History-Based Probabilistic Generation. Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
CoNLL, 267?276. Prague, Czech Republic.
Kaplan, Ronald and Bresnan, Joan. 1982. Lexical
Functional Grammar: a Formal System for Gram-
matical Representation. The Mental Representation
of Grammatical Relations, 173?282. MIT Press,
Cambridge.
Kaplan, Ronald and Wedekind, Jurgen. 2000. LFG
Generation Produces Context-free Languages. Pro-
ceedings of the 18th International Conference on
Computational Linguistics, 425?431. Saarbru?cken,
Germany.
Langkilde, Irene. 2000. Forest-Based Statistical Sen-
tence Generation. Proceedings of 1st Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, 170?177. Seattle, WA.
Langkilde, Irene. 2002. An Empirical Verification
of Coverage and Correctness for a General-Purpose
Sentence Generator. Proceedings of the Second In-
ternational Conference on Natural Language Gener-
ation, 17?24. New York, USA.
Marcus, Mitchell P., Santorini, Beatrice and
Marcinkiewicz, Mary Ann. 1993. Building a
large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2).
Nakanishi, Hiroko and Nakanishi, Yusuke and Tsu-
jii, Jun?ichi. 2005. Probabilistic Models for Dis-
ambiguation of an HPSG-Based Chart Generator.
Proceedings of the 9th International Workshop on
Parsing Technology, 93?102. Vancouver, British
Columbia.
Nivre, Joakim. 2006. Inductive Dependency Parsing.
Springer.
Papineni, Kishore, Roukos, Salim, Ward, Todd and
Zhu, Wei-Jing. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, 311-318. Philadelphia, USA.
Ratnaparkhi, Adwait. 2000. Trainable methods for nat-
ural language generation. Proceedings of NAACL
2000, 194?201. Seattle, WA.
Stolcke, Andreas. 2002. SRILM-An Extensible Lan-
guage Modeling Toolkit. Proceedings of Interna-
tional Conference of Spoken Language Processing.
Denver, Colorado.
van Genabith, Josef and Crouch, Dick. 1996. Di-
rect and underspecified interpretations of LFG f-
structures. Proceedings of the 16th conference on
Computational linguistics, 262?267. Copenhagen,
Denmark
Velldal, Erik and Oepen, Stephan. 2005. Maximum
entropy models for realization ranking. Proceedings
of the MTSummit ?05.
White, Michael. 2004. Reining in CCG Chart Realiza-
tion. Proceedings of the third International Natural
Language Generation Conference. Hampshire, UK.
White, Michael, Rajkumar, Rajakrishnan and Martin,
Scott. 2007. Towards Broad Coverage Surface Re-
alization with CCG. Proceedings of the MT Summit
XI Workshop, 22?30. Copenhagen, Danmark.
Xue, Nianwen, Xia, Fei, Chiou, Fu dong and Palmer,
Martha. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural
Language Engineering, 11(2): 207?238.
304
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 833?840
Manchester, August 2008
Prediction of Maximal Projection for Semantic Role Labeling
Weiwei Sun
?
, Zhifang Sui
Institute of Computational Linguistics
Peking University
Beijing, 100871, China
{ws, szf}@pku.edu.cn
Haifeng Wang
Toshiba (China) R&D Center
501, Tower W2, Oriental Plaza
Beijing, 100738, China
wanghaifeng@rdc.toshiba.com.cn
Abstract
In Semantic Role Labeling (SRL), argu-
ments are usually limited in a syntax sub-
tree. It is reasonable to label arguments lo-
cally in such a sub-tree rather than a whole
tree. To identify active region of argu-
ments, this paper models Maximal Pro-
jection (MP), which is a concept in D-
structure from the projection principle of
the Principle and Parameters theory. This
paper makes a new definition of MP in S-
structure and proposes two methods to pre-
dict it: the anchor group approach and the
single anchor approach. The anchor group
approach achieves an accuracy of 87.75%
and the single anchor approach achieves
83.63%. Experimental results also indicate
that the prediction of MP improves seman-
tic role labeling.
1 Introduction
Semantic Role Labeling (SRL) has gained the in-
terest of many researchers in the last few years.
SRL consists of recognizing arguments involved
by predicates of a given sentence and labeling their
semantic types. As a well defined task of shallow
semantic parsing, SRL has a variety of applications
in many kinds of NLP tasks.
A variety of approaches has been proposed
for the different characteristics of SRL. More re-
cent approaches have involved calibrating features
(Gildea and Jurafsky, 2002; Xue and Palmer, 2004;
?
This work was partial completed while this author was at
Toshiba (China) R&D Center.
?
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Pradhan et al, 2005), analyzing the complex input
? syntax trees (Moschitti, 2004; Liu and Sarkar,
2007), exploiting the complicated output ? the
predicate-structure (Toutanova et al, 2005), as
well as capturing paradigmatic relations between
predicates (Gordon and Swanson, 2007).
In prior SRL methods, role candidates are ex-
tracted from a whole syntax tree. Though sev-
eral pruning algorithms have been raised (Xue and
Palmer, 2004), the policies are all in global style.
In this paper, a statistical analysis of Penn Prop-
Bank indicates that arguments are limited in a local
syntax sub-tree rather than a whole one. Prior SRL
methods do not take such locality into account and
seek roles in a wider area. The neglect of local-
ity of arguments may cause labeling errors such
as constituents outside active region of arguments
may be falsely recognized as roles.
This paper uses insights from generative lin-
guistics to guide the solution of locality of argu-
ments. In particular, Maximal Projection (MP)
which dominates
1
active region of arguments ac-
cording to the projection principle of principle and
parameters. Two methods, the anchor group ap-
proach and the single anchor approach, are pro-
posed to find the active sub-tree which is rooted by
MP and covers all roles. The solutions put forward
in this paper borrow ideas from NP-movement
principle in generative linguistics and are in statis-
tical flavor. The anchor group approach achieves
an accuracy of 87.75%, and the single anchor ap-
proach achieves 83.63%. Though the accuracy is
lower, the single anchor approach fits SRL better.
1
Dominate is an concept in X-bar theory are modeled. As-
suming ? and ? are two nodes in a syntax tree: ? dominates
? means ? is ancestor of ?.
833
Figure 1: A sentence from WSJ test corpus of CoNLL-2005 shared task
2 Maximal Projection and Its
Government of Arguments
2.1 Maximal Projection
Principle and parameters theory is a framework of
generative grammar. X-bar theory, as a module
of principle and parameters, restricts context-free
phrase structure rules as follows:
1. a phrase always contains a head of the same
type, i.e. NPs Ns, VPs Vs, PPs Ps, etc.
2. XP(X?) ? specifier X?
3. X??X complement(s)
These structural properties are conventionally rep-
resented as shown in figure 2.
Figure 2: X-bar structure
X is the head of the phrase XP. X? and XP(X?)
are called projections of X. The head is also called
the zero projection. X-bar structure is integrated
with the properties of lexical items via the Projec-
tion Principle of principle and parameters. This
principle is summed up as the properties of lexi-
cal information project onto the syntax of the sen-
tence. For instance:
? Sue likes Picasso
? *Sue likes
The subcategorization frame of the lexical item
like [ ,NP] ensures that the verb is followed by an
NP and the second sentence is of ungrammatical
form.
Maximal Projection (MP) is the constituent
which is projected to the highest level of an X-bar
structure from lexical entities and is therefore the
top node XP of the X-bar structure.
Take figure 1 for instance, S is the MP of the
predicate come. Though the syntax tree is not in D-
structure (deep structure), the S-structure (surface
structure) headed by come is similar to its genuine
D-structure. In a latter part of this section, a spe-
cific definition of MP in S-structure will be given
for application.
2.2 MP Limits Active Region of Arguments
MP holds all lexical properties of heads. In partic-
ular, the MP of a predicate holds predicate struc-
ture information and the constituents out of its do-
main cannot occupy argument positions. ?-theory
and government are two modules of principle and
parameters. They both suggest that the possi-
ble positions of semantic roles are in the sub-tree
rooted by MP.
834
Concerning assignment of semantic roles to
constituents, ?-theory suggests that semantic roles
are assigned by predicates to their sisters (Chom-
sky, 1986). Furthermore, in a X-bar theory, com-
plements are assigned semantic roles by the pred-
icate and specifiers get roles from the V?. In both
situations the process of roles assignment is in sis-
terhood condition and limited in the sub-structure
which is dominated by the MP. Only constituents
under MP can get semantic roles. The Case As-
signment Principle also points out: Case is as-
signed under government (Chomsky, 1981). Take
figure 1 for instance, only NP-1 and PP-2 can get
semantic roles of the head come.
From generative linguists? point, MP limits sub-
tree of arguments. Therefore, finding the MP is
equivalent to finding the active region of predicate
structure.
2.3 Definition of MP in S-structure
Though a clear enough definition of MP in D-
structure has been previously illustrated, it is still
necessary to define a specific one in S-structure
for application, especially for automatic parsing
which are not exactly correct. This paper de-
fines MP in S-structure (hereinafter denote MP
for short) as following: for every predicate p in the
syntax tree T , there exists one and only one MP
mp s.t.
1. mp dominates all arguments of p;
2. all descendent nodes of mp don?t satisfy the
former condition.
Due to its different characteristics from argu-
ments, adjunct-like arguments are excluded from
the set of arguments in generative grammar and
many other linguistic theories. For this reason, this
paper does not take them into account.
For gold syntax tree, there exists a one-to-one
mapping between arguments and nodes of syn-
tax trees, whereas automatic syntactic parsing con-
tains no such mapping. This paper do not take
arguments which cannot get corresponding con-
stituents into account to reduce the influence of au-
tomatic parsing error.
Take the sentence of figure 1 to illustrate our
definition of MP: S is MP of come since NP-1 and
PP-2 are arguments of it. There is no node map-
ping to the argument Wall Street professionals in
the parsing tree. Instead of covering argument?s
fragments, we simply take it PP-4 as MP.
2.4 Using MP Information in SRL
The boundaries of a predicate structure are two
word positions of the sentence. It is difficult to
model these two words. On the contrary, MP, as
one ancestor of predicate, has a clear-cut meaning
and is ideal for modeling. In this paper, the pol-
icy to predict MP rather than two word positions is
carried out to deal with locality of arguments.
Automatic prediction of MP can be viewed as a
preprocessing especially a pruning preprocessing
for SRL. Given a sentence and its parsing, SRL
systems can take seeking the active sub-tree rooted
by MP as the first step. Then SRL systems can
work on the shrunk syntax tree, and follow-up la-
beling processes can be in a various form. Most
of previous SRL methods still work without spe-
cial processing. Take figure 1 for example: when
labeling include, as the MP is PP-4, just NP-7 will
be extracted as argument candidate.
3 Analysis of Locality of Arguments
Principle and parameters suggests that MP bounds
arguments. Additionally, a statistical analysis
shows that possible positions of arguments are lim-
ited in a narrow region of syntax tree. An opposite
experiment also shows that MP information is use-
ful for SRL.
3.1 Data and Baseline System
In this paper, CoNLL-2005 SRL shared task
data (Carreras and M`arquez, 2005) is used as cor-
pus. The data consists of the Wall Street Jour-
nal (WSJ) part of the Penn TreeBank with infor-
mation on predicate argument structures extracted
from the PropBank corpus. In addition, the test
set of the shared task includes three sections of the
Brown corpus. Statistical analysis is based on sec-
tion 02-21 of WSJ. Experiments are conducted on
WSJ and Brown corpus. As defined by the shared
task, section 02-21 of PropBank are used for train-
ing models while section 23 and Brown corpus are
used for test. In terms of syntax information, we
use Charniak parser for POS tagging and full pars-
ing.
A majority of prior SRL approaches formulate
the SRL propblem as a multi-class classification
propblem. Generally speaking, these SRL ap-
proaches use a two-stage architecture: i) argument
identification; ii) argument classification, to solve
the task as a derivation of Gildea and Jurafsky?s
pioneer work (Gildea and Jurafsky, 2002). UIUC
835
Precision Recall F
?=1
Arg0 86.28% 87.01% 86.64
Arg1 79.37% 75.06% 77.15
Arg2 69.48% 62.97% 66.07
Arg3 69.01% 56.65% 62.22
Arg4 72.64% 75.49% 74.04
Table 1: SRL performance of UIUC SRLer
Precision Recall F
?=1
Arg0 91.84% 89.98% 90.90
Arg1 81.73% 75.93% 78.72
Arg2 69.86% 63.06% 66.29
Arg3 71.13% 58.38% 64.13
Arg4 73.08% 74.51% 73.79
Table 2: SRL performance of UIUC SRLer using
information of gold MP
Semantic Role Labeler
2
(UIUC SRLer) is a state-
of-the-art SRL system that based on the champion
system of CoNLL-2005 shared task (Carreras and
M`arquez, 2005). It is utilized as a baseline system
in this paper. The system participated in CoNLL-
2005 is based on several syntactic parsing results.
However, experiments of this paper just use the
best parsing result from Charniak parser. Param-
eters for training SRL models are the same as de-
scribed in (Koomen, 2005).
3.2 Active Region of Arguments
According to a statistical analysis, the average
depth from a target predicate to the root of a syntax
tree is 5.03, and the average depth from a predicate
to MP is just 3.12. This means about 40% of an-
cestors of a predicate do not dominate arguments
directly. In addition, the quantity of leaves in syn-
tax tree is another measure to analyze the domain.
On average, a syntax tree covers 28.51 leaves, and
MP dominates only 18.19. Roughly speaking, only
about 60% of words are valid for semantic roles.
Statistics of corpora leads to the following conclu-
sion: arguments which are assigned semantic roles
are in a local region of a whole syntax tree.
3.3 Typical Errors Caused by Neglect of
Locality of Arguments
The neglect of the locality of arguments in prior
SRL methods shows that it may cause errors.
Some constituents outside active region of argu-
ments may be falsely labeled as roles especially for
those being arguments of other predicates. A sta-
tistical analysis shows 20.62% of falsely labeled
arguments are constituents out of MP domain in
labeling results of UIUC SRLer. Take figure 1 for
instance, UIUC SRLer makes a mistake when la-
beling NP-1 which is Arg1 of the predicate come
for the target include; it labels Arg0 to NP. In fact,
the active region of include is the sub-tree rooted
2
http://l2r.cs.uiuc.edu/ cogcomp/srl-demo.php
by PP-4. Since NP-1 is an argument of another
predicate, some static properties of NP-1 make it
confusing as an argument.
3.4 SRL under Gold MP
If MP has been found before labeling semantic
roles, the set of role candidates will be shrunk,
and the capability to identify semantic roles may
be improved. An opposite experiment verifies this
idea. In the first experiment, UIUC SRLer is re-
trained as a baseline. For comparison, during the
second experiment, syntax sub-trees dominated by
gold MP are used as syntactic information. Both
training and test data are preprocessed with gold
MP information. That is to say we use pruned data
for training, and test is conducted on pruned syntax
sub-trees.
Table 1 and 2 show that except for Arg4, all ar-
guments get improved labeling performance, espe-
cially Arg0. Since arguments except for Arg0 are
realized as objects on the heel of predicate in most
case, the information of MP is not so useful for
them as Arg0. The experiment suggests that high
performance prediction of MP can improve SRL.
4 Prediction of MP
Conforming to government and ?-theory, MP is
not too difficult to predict in D-structure. Unfor-
tunately, sentences being looked at are in their sur-
face form and region of arguments has been ex-
panded. Simple rules alone are not adequate for
finding MP owing to a variety of movement be-
tween D-structure and S-structure. This paper de-
signs two data driven algorithms based on move-
ment principles for prediction of MP.
4.1 NP-movement and Prediction of MP
4.1.1 NP-movement in Principle and
Parameters
The relationship between D-structure and S-
structure is movement: S-structure equals D-
836
structure plus movement. NP-movement prin-
ciple in principle and parameters indicates that
noun phrases only move from A-positions (argu-
ment position) which have been assigned roles
to A-positions which have not, leaving an NP-
trace. On account of ?-theory and government, A-
positions are nodes m-commanded
3
by predicates
in D-structure. In NP-movement, arguments move
to positions which are C-commanded
4
by target
predicate and m-commanded by other predicates.
Broadly speaking, A-positions are C-commanded
by predicates after NP-movement. The key of the
well-known pruning algorithm raised in (Xue and
Palmer, 2004) is extracting sisters of ancestors as
role candidates. Those candidate nodes are all C-
commanders of a predicate. NP-movement can
give an explanation why the algorithm works.
4.1.2 Definition of Argument Anchor
To capture the characteristics of A-positions, we
make definition of A-anchor as following. For ev-
ery predicate p in the syntax tree T , denote A the
set of C-commanders of p:
? a left-A-anchor satisfies:
1. left-A-anchor belongs to A;
2. left-A-anchor is a noun phrase (includ-
ing NNS, NNP, etc.) or simple declara-
tive clause (S);
3. left-A-anchor is on the left hand of p.
? a right-A-anchor satisfies:
1. right-A-anchor belongs to A;
2. right-A-anchor is a noun phrase (includ-
ing NNS, NNP, etc.);
3. right-A-anchor is on the right hand of p.
Take figure 1 for example, NP-1, NP-4 and NP-
6 are left-A-anchors of include, and no right-A-
anchor. There is a close link between A-position
and the A-anchor that we defined, since A-anchors
occupy A-positions.
4.1.3 Anchor Model for Prediction of MP
Parents of A-anchors and first branching ances-
tor of the predicate can cover 96.25% of MP and
the number of those ancestors is 2.78 times of the
3
M-command is an concept in X-bar syntax. Assuming
? and ? are two nodes in a syntax tree: ? m-commands ?
means ? C-commands ? and the MP of ? dominates ?
4
C-command is an concept in X-bar theory. Assuming ?
and ? are two nodes in a syntax tree: ? C-commands ? means
every parent of ? is ancestor of ?.
number of MP. The number of all ancestors is 6.65
times. The data suggests that taking only these
kinds of ancestors as MP candidates can shrink the
candidate set with a relatively small loss.
4.2 Anchor Group Approach
MP is one ancestor of a predicate. An natural ap-
proach to predict MP is searching the set of all
ancestors. This idea encounters the difficulty that
there are too many ancestors. In order to reduce
the noise brought by non-anchors? parents, the an-
chor group approach prunes away useless ances-
tors which are neither parents of A-anchors nor
first branching node upon predicate from MP can-
didate set. Then the algorithm scores all candidates
and chooses the MP in argmax flavor. Formally,
we denote the set of MP candidates C and the score
function S(.).
m?p = argmax
c?C
S(mp|c)
Probability function is chosen as score func-
tion in this paper. In estimating of the probability
P (MP |C), log-linear model is used. This model is
often called maximum entropy model in research
of NLP. Let the set {1,-1} denotes whether a con-
stituent is MP and ?(c, {?1, 1}) ? R
s
denotes
a feature map from a constituent and the possible
class to the vector space R
s
. Formally, the model
of our system is defined as:
m?p = argmax
c?C
e
<?(c,1),?>
e
<?(c,1),?>
+e
<?(c,0),?>
The algorithm is also described in pseudo code
as following.
Ancestor Algorithm:
1: collect parents of anchors and the first
branching ancestor, denote them set C
2: for every c ? C
3: calculate P (mp|c)
4: return c? that gets the maximal P (mp|c)
4.2.1 Features
We use some features to represent various as-
pects of the syntactic structure as well as lexical
information. The features are listed as follows:
Path The path features are similar to the path
feature which is designed by (Gildea and Jurafsky,
2002).A path is a sequential collection of phrase
tags. There are two kinds of path features here: one
is from target predicate through to the candidate;
the other is from the candidate to the root of the
syntax tree. For include in the sentence of figure 1,
the first kind of path of PP-2 is VBG+PP+NP+PP
and the second is PP+VP+S.
837
C-commander Thread As well as path features,
C-commander threads are other features which
reflect aspects of the syntactic structures. C-
commander thread features are sequential contain-
ers of constituents which C-command the target
predicate. We design three kinds of C-commander
threads: 1) down thread collects C-commanders
from the anchor to the target predicate; 2) up
thread collects C-commanders from the anchor to
the left/right most C-commander; 3) full thread
collects all C-commanders in the left/right direc-
tion from the target predicate. Direction is depen-
dent on the type of the anchor - left or right anchor.
Considering the grammatical characteristics of
phrase, we make an equivalence between such
phrase types:
? JJ, JJR, JJS, ADJP
? NN, NNP, NNS, NNPS, NAC, NX, NP
Besides the equivalent constituents, we discard
these types of phrases:
? MD, RB, RBS, RBR, ADVP
For include in figure 1, the up thread of
NP-4 is VBG+,+NP+NP; the down thread
is NP+IN+VBD+NP; the full thread is
VBG+,+NP+NP+IN+VBD+NP.
The phrase type of candidate is an important fea-
ture for prediction
Candidate of MP. We also select the rank num-
ber of the current candidate and the number of all
candidates as features. For the former example,
the two features for PP-2 are 2 and 3, since NP-
4 is the second left-A-anchor and there are three
A-anchors of include.
Anchor Features of anchor include the head
word of the anchor, the boundary words and their
POS, and the number of the words in the anchor.
Those features are clues of judgment of whether
the anchor?s position is an A-position.
Forward predicate For the former example, the
forward predicate of NP-4 is come. The features
include the predicate itself, the Levin class and the
SCF of the predicate.
predicate Features of predicate include lemma,
Levin class, POS and SCF of the predicate.
Figure 3: Flow diagram of the single anchor ap-
proach
Formal Subject An anchor may be formal sub-
ject. Take It is easy to say the specialist is not do-
ing his job for example, the formal subject will be
recognized as anchor of do. We use a heuristic rule
to extract this feature: if the first NP C-commander
of the anchor is ?it? and the left word of predicate
is ?to?, the value of this feature is 1; otherwise 0.
The Maximal Length of C-commanders Con-
stituent which consists of many words may be a
barrier between the predicate and an A-position.
For the former example, if the target predicate is
include, this feature of NP-1 is 2, since the largest
constituent NP-4 is made up of two words.
4.3 Single Anchor Approach
Among all A-anchors, the right most left-A-anchor
such as NP-6 of include in figure 1 is the most im-
portant one for MP prediction. The parent of this
kind of left-A-anchor is the MP of the predicate,
obtaining a high probability of 84.59%. The single
anchor approach is designed based on right most
left-A-anchor. The key of this approach is an ac-
tion prediction that when right most left-A-anchor
is found, the algorithm predicts next action to re-
turn which node of syntax tree as MP. There is
a label set of three types for learning ? here, up,
down. After action is predicted, several simple
rules are executed as post process of this predic-
tion: i) if there is no left-A-anchor, return the root
of the whole syntax tree as MP; ii)if the predicted
label is here, return the parent of right most left-
A-anchor; iii) if the predicted label is down, return
838
Prediction Accuracy
Corpus Action MP
WSJ ? 87.75%
Brown ? 88.84%
Table 3: Accuracy of the anchor group ap-
proach
Prediction Accuracy
Corpus Action MP
WSJ 88.45% 83.63%
Brown 90.10% 85.70%
Table 4: Accuracy of the single anchor ap-
proach
Precision Recall F
?=1
Arg0 86.23% 87.90% 87.06
Arg1 80.21% 74.79% 77.41
Arg2 70.09% 62.70% 66.19
Arg3 71.74% 57.23% 63.67
Arg4 74.76% 75.49% 75.12
Table 5: SRL performance of UIUC SRLer us-
ing information of predicted MP; the anchor
group approach; WSJ test corpus
Precision Recall F
?=1
Arg0 87.03% 87.59% 87.31
Arg1 80.24% 74.77% 77.41
Arg2 70.35% 63.06% 66.51
Arg3 71.43% 57.80% 63.90
Arg4 73.33% 75.49% 74.40
Table 6: SRL performance of UIUC SRLer us-
ing information of predicted MP; the single an-
chor approach; WSJ test corpus
the first branching node upon the predicate; iv) if
the predicted label is up, return the root. The ac-
tion prediction also uses maximum entropy model.
Figure 3 is the flow diagram of the single anchor
approach. Features for this approach are similar
to the former method. Features of the verb which
is between the anchor and the predicate are added,
including the verb itself and the Levin class of that
verb.
5 Experiments and Results
Experiment data and toolkit have been illustrated
in section 3. Maxent
5
, a maximum entropy model-
ing toolkit, is used as a classifier in the experiments
of MP prediction.
5.1 Experiments of Prediction of MP
The results are reported for both the anchor group
approach and the single anchor approach. Table 3
summaries the accuracy results of MP prediction
for the anchor group approach; table 4 summaries
results of both action prediction and MP prediction
for the single anchor approach. Both the anchor
group approach and the single anchor approach
have better prediction performance in Brown test
set, though the models are trained on WSJ cor-
pus. These results illustrate that anchor approaches
which are based on suitable linguistic theories have
robust performance and overcome limitations of
training corpus.
5
http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
5.2 Experiments of SRL Using MP Prediction
Like the experiments in the end of section 3, we
perform similar experiments under predicted MP.
Both training and test corpus make use of predicted
MP information. It is an empirical tactic that pre-
dicted information of maximal projection, instead
of gold information, is chosen for a training set.
Experiments suggest predicted information is bet-
ter. Table 5 is SRL performance using the anchor
group approach to predict MP; Table 6 is SRL per-
formance using the single anchor approach.
Compared with table 1 on page 4, table 5 and
table 6 both indicate the predicted MP can help to
label semantic roles. However, there is an interest-
ing phenomenon. Even though the anchor group
approach achieves a higher performance of MP,
the single anchor approach is more helpful to SRL.
18.56% of falsely labeled arguments are out of MP
domain using the single anchor approach to predict
MP, compared to 20.62% of the baseline system.
In order to test robustness of the contribution
of MP prediction to SRL, another opposite exper-
iment is performed using the test set from Brown
corpus. Table 7 is the SRL performance of UIUC
SRLer on Brown test set. Table 8 is the corre-
sponding performance using MP information pre-
dicted by the single anchor approach. Comparison
between table 7 and table 8 indicates the approach
of MP prediction proposed in this paper adapts to
other genres of corpora.
Capability of labeling Arg0 gets significant im-
provement. Subject selection rule, a part of the-
839
Precision Recall F
?=1
Arg0 82.88% 85.51% 84.17
Arg1 66.30% 63.17% 64.70
Arg2 50.00% 45.58% 47.69
Arg3 0.00% 0.00% 0.00
Arg4 60.00% 20.00% 30.00
Table 7: SRL performance of UIUC SRLer;
Brown test corpus
Precision Recall F
?=1
Arg0 83.85% 86.22% 85.02
Arg1 66.67% 63.02% 64.79
Arg2 50.38% 44.90% 47.48
Arg3 0.00% 0.00% 0.00
Arg4 60.00% 20.00% 30.00
Table 8: SRL performance of UIUC SRLer us-
ing information of predicted MP; the single an-
chor approach; Brown test corpus
matic hierarchy theory, states that the argument
that the highest role (i.e. proto-agent, Arg0 in
PropBank) is the subject. This means that Arg0 is
usually realized as a constituent preceding a predi-
cate and has a long distance from the predicate. As
a solution of finding active region of arguments,
MP prediction is helpful to shrink the searching
range of arguments preceding the predicate. From
this point, we give a rough explanation why exper-
iment results for Arg0 are better.
6 Conclusion
Inspired by the locality phenomenon that argu-
ments are usually limited in a syntax sub-tree, this
paper proposed to label semantic roles locally in
the active region arguments dominated by maximal
projection, which is a concept in D-structure from
the projection principle of the principle and param-
eters theory. Statistical analysis showed that MP
information was helpful to avoid errors in SRL,
such as falsely recognizing constituents outside ac-
tive region as arguments. To adapt the projection
concept to label semantic roles, this paper defined
MP in S-structure and proposed two methods to
predict MP, namely the anchor group approach and
the single anchor approach. Both approaches were
based on NP-movement principle of principle and
parameters. Experimental results indicated that
our MP prediction methods improved SRL.
Acknowlegements
The work is supported by the National Natu-
ral Science Foundation of China under Grants
No. 60503071, 863 the National High Technol-
ogy Research and Development Program of China
under Grants No.2006AA01Z144, 973 Natural
Basic Research Program of China under Grants
No.2004CB318102.
References
Carreras, Xavier and Llu??s M`arquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: semantic role
labeling. In Proceedings of Conference on Natural
Language Learning.
Chomsky, Noam. 1981. Lectures on Government and
Binding. Foris Publications, Dordrecht.
Chomsky, Noam. 1986. Barriers. MIT Press, Barriers.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computional Linguis-
tics, 28(3):245?288.
Gordon, Andrew and Reid Swanson. 2007. Generaliz-
ing Semantic Role Annotations Across Syntactically
Similar Verbs. In Proceedings of Conference on As-
sociation for Computational Linguistics.
Koomen, Peter, Vasina Punyakanok, Dan Roth and
Wen-tau Yih. 2005. Generalized Inference with
Multiple Semantic Role Labeling Systems. In Pro-
ceedings of Conference on Natural Language Learn-
ing.
Liu, Yudong and Anoop Sarkar. 2004. Experimen-
tal Evaluation of LTAG-Based Features for Semantic
Role Labeling. In Proceedings of Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning.
Mocshitti, Alessandro. 2004. A Study on Convolu-
tion Kernels for Shallow Semantic Parsing. In Pro-
ceedings of Conference on Association for Compu-
tational Linguistics.
Pradhan, Sameer, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James Martin and Daniel Jurafsky.
2005. Support Vector Learning for Semantic Argu-
ment Classification. In Proceedings of Conference
on Association for Computational Linguistics.
Toutanova, Kristina, Aria Haghighi and Christopher
Manning. 2005. Joint Learning Improves Seman-
tic Role Labeling. In Proceedings of Conference on
Association for Computational Linguistics.
Xue, Nianwen and Martha Palmer. 2004. Calibrating
Features for Semantic Role Labeling. In Proceed-
ings of Empirical Methods in Natural Language Pro-
cessing.
840
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 993?1000
Manchester, August 2008
Domain Adaptation for Statistical Machine Translation with Domain 
Dictionary and Monolingual Corpora 
Hua Wu,  Haifeng Wang 
Toshiba (China) R&D Center 
Beijing, 100738, China 
 wuhua@rdc.toshiba.com.cn 
wanghaifeng@rdc.toshiba.com.cn 
Chengqing Zong 
NLPR, Institute of Automation 
Chinese Academy of Sciences 
 Beijing 100080, China 
cqzong@nlpr.ia.ac.cn 
 
Abstract tra
Statistical machine translation systems 
are usually trained on large amounts of 
bilingual text and monolingual text. In 
this paper, we propose a method to per-
form domain adaptation for statistical 
machine translation, where in-domain bi-
lingual corpora do not exist. This method 
first uses out-of-domain corpora to train a 
baseline system and then uses in-domain 
translation dictionaries and in-domain 
monolingual corpora to improve the in-
domain performance. We propose an al-
gorithm to combine these different re-
sources in a unified framework. Experi-
mental results indicate that our method 
achieves absolute improvements of 8.16 
and 3.36 BLEU scores on Chinese to 
English translation and English to French 
translation respectively, as compared 
with the baselines using only out-of-
domain corpora. 
1 Introduction 
In statistical machine translation (SMT), the 
translation process is modeled to obtain the 
translation  of the source sentence f  by 
maximizing the following posterior probability 
(Brown et al, 1993). 
beste
)()(maxarg
)(maxarg
|
|
ee
fee
fe
e
LM
best
pp
p
=
=
 (1)
State-of-the-art SMT systems are trained on 
large collections of bilingual corpora for the 
                                                 
?C 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
nslation model )( | efp  and monolingual tar-
get language corpora for the language model 
(LM) )(eLMp . The trained SMT systems are 
suitable for translating texts in the same domain 
as the training corpus. However, for some spe-
cific domains, it is difficult to obtain a bilingual 
corpus. In this case, the performance of SMT 
systems will be degraded. 
Generally, it is easier to obtain in-domain 
monolingual corpora in either source or target 
language. Moreover, in some specific domains, 
although in-domain bilingual corpora do not ex-
ist, in-domain translation dictionaries, which 
usually contain domain-specific terms and their 
translations, are available. And even if such dic-
tionaries are not available, it is easier to compile 
one than to build a bilingual corpus. Thus, in this 
paper, we address the problem of domain-
specific SMT, where only domain-specific dic-
tionaries and/or monolingual corpora exist. In a 
specific domain, there are two kinds of words: 
common words, which also frequently occur in 
out-of-domain corpora, and domain-specific 
words, which only occur in the specific domain. 
Thus, we can combine the out-of-domain bilin-
gual corpus, the in-domain translation dictionary, 
and monolingual corpora for in-domain transla-
tion. 
If an in-domain translation dictionary is avail-
able, we combine it with the out-of-domain 
translation model to improve translation quality. 
If an in-domain target language corpus (TLC) is 
available, we use it to build an in-domain lan-
guage model, which can be combined with the 
out-of-domain language model to further im-
prove translation quality. Moreover, if an in-
domain source language corpus (SLC) is avail-
able, we automatically translate it and obtain a 
synthetic in-domain bilingual corpus. By adding 
this synthetic bilingual corpus to the training data, 
we rebuild the translation model to improve 
993
translation quality. We can repeatedly translate 
the in-domain source language corpus with the 
improved model until no more improvement can 
be made. This is similar to transductive learning 
described in (Ueffing et al, 2007). 
We perform domain adaptation experiments 
on two tasks: one is the Chinese to English trans-
lation, using the test set released by the Interna-
tio
inese to English translation 
an
MT sys-
tem
lation model and language model adapta-
in domain adaptation for 
del adaptation has been 
 
shared task focused on do-
main adaptation for machine translation among 
Eu
rpora. Adding the extracted bilin-
gu
the performance of a SMT system 
tra
Moses (Koehn et al, 2007). In 
babilities, reorder-
e model probabili-
nal Workshop on Spoken Language Transla-
tion 2006 (IWSLT 2006), and the other is the 
English to French translation, using the data re-
leased by the Second Workshop on Statistical 
Machine Translation (WMT 2007) (Callison-
Burch et al, 2007). 
Experimental results indicate that our method 
achieves absolute improvements of 8.16 and 3.36 
BLEU scores on Ch
d English to French translation respectively, as 
compared with the baselines only using the out-
of-domain corpora. The results on both transla-
tion tasks also show that the translation quality 
achieved by our methods is comparable to that of 
the method using both in-domain and out-of-
domain bilingual corpora. Moreover, even if in-
domain and out-of-domain bilingual corpora are 
available, adding an in-domain dictionary also 
helps to improve the translation quality. 
The remainder of the paper is organized as fol-
lows. In section 2, we describe the related work. 
Section 3 briefly introduces the baseline 
 used in our experiments. Section 4 describes 
our domain adaptation method of using in-
domain dictionary and monolingual corpora. And 
then we present the experimental results in sec-
tions 5. In the last section, we conclude this pa-
per. 
2 Related Work 
Trans
tion are usually used 
SMT. Language mo
widely used in speech recognition (Bacchiani 
and Roark, 2003). In recent years, language 
model adaptation has also been studied for SMT 
(Bulyko et al, 2007). They explored discrimina-
tive estimation of language model weights by 
directly optimizing machine translation perform-
ances such as BLEU score (Papineni et al, 2002).
Their experiments indicated about 0.4 BLEU 
score improvement. 
A shared task is organized as part of the Sec-
ond Workshop on Statistical Machine Transla-
tion. A part of this 
ropean languages. Several studies investigated 
mixture model adaptation for both translation 
model and language model in SMT (Civera and 
Juan, 2007; Foster and Kuhn, 2007). Koehn and 
Schroeder (2007) investigated different adapta-
tion methods for SMT. Their experiments indi-
cate an absolute improvement of more than 1 
BLEU score. 
To enlarge the in-domain bilingual corpus, 
Munteanu and Marcu (2005) automatically ex-
tracted in-domain bilingual sentence pairs from 
comparable co
al corpus to the training data improved the per-
formance of the MT system. In addition, Ueffing 
et al (2007) explored transductive learning for 
SMT, where source language corpora are used to 
train the models. They repeatedly translated 
source sentences from the development set and 
test set. Then the generated translations are used 
to improve the performance of the SMT system. 
This kind of transductive learning can be seen as 
a means to adapt the SMT system to a new type 
of texts. 
In this paper, we use an in-domain translation 
dictionary and/or in-domain monolingual corpora 
(in both source language and target language) to 
improve 
ined on the out-of-domain corpora. Thus, our 
method uses these resources, instead of an in-
domain bilingual corpus, to adapt a baseline sys-
tem trained on the out-of-domain corpora to in-
domain texts. 
3 Baseline MT System 
The phrase-based SMT system used in our ex-
periments is 
Moses, phrase translation pro
ing probabilities, and languag
ties are combined in the log-linear model to ob-
tain the best translation beste  of the source sen-
tence f : 
?
=
?
=
M
p | )(maxarg fee ebest
 (2)
m
mmh
1
,(maxarg f)ee ?
The weights are set by a discriminative train-
ing method using a held-out data set as describ
in (Och, 2003). The models or features which are 
employed by the decoder are (a) one or several 
ph
ed 
rases tables, (b) one or more language models 
trained with SRILM toolkit (Stolcke, 2002), (c) 
distance-based and lexicalized distortion models, 
(d) word penalty, (e) phrase penalty. 
994
Input  Out-of-domain training data OL  
ary D  In-domain translation diction  I
In-domain target language corpus IT  (optional) 
           In-d
, where  represents the general model. 
e 
f 
If  
         
e
ing step: Translate  with  to get a synthetic bilingual corpus 
Un
E
End 
Outpu l  for in-domain translation 
omain source language corpus I  (optional) S
Begin Assign translation probabilities to ID  
If IT  is available 
Training step: (L Estimate= ),, IIO TD?
Els
?
Training step: ),( IO DL Estimate=?  
End i
 S  is availableI
    ? =)0( ?,  0=i
R peat 
 
1+= ii  
Label IS
)1( ?i? IL  
Training step: ),, IIO
til no more improvement can be achieved 
()(i LDL estimate-Re=?  
)(i?  ?=
nd if 
t  Mode  ?
Figure 1.The domain adaptation algorithm 
4 The Framework 
n about the algorithm is 
 our algorithm, a phrase 
available, we train an in-domain LM, which is 
co
ilable, we use the built -linear 
m
). With the 
 order to 
 to assign prob-
nary.  
m
4.1 The Algorithm 
The detailed informatio
shown in Figure 1. In
table and a language model are first constructed 
based on the out-of-domain corpus OL . Then 
probabilities are automatically assigned to the 
entries in the in-domain translation dictionary 
ID , from which another phrase table is con-
structed. At last, the two phrase tables are com-
d. This is the procedure of the training step 
),( IO DL Estimate=? . 
If an in-domain target language corpus is 
bine
mbined with the out-of-domain LM. The built 
phrase tables and LMs are integrated in the log-
linear model as described in section 3. This is the 
procedure of ),,( IIO TDL Estimate=? . 
Moreover, if an in-domain source language 
corpus is ava log
odel to translate the in-domain source texts and 
obtain a synthetic bilingual corpus. And then we 
add the synthetic bilingual corpus into the train-
ing data to improve the current log-linear model 
improved model, we repeatedly translate the in-
domain source texts until no more improvement 
on a development set can be achieved.  
4.2 Dictionary Probabilities 
In general, there is no translation probability in a 
manually-made translation dictionary. In
( estima-Re=? ),,()( IIOi LDL te
construct a phrase table, we have
abilities to the entries in the dictio
Uniform Translation Probability: Since we 
have no parallel corpus to estimate the translation 
probabilities, we simply assign uniform prob-
abilities to the entries in the dictionary. With this 
ethod, if a source word has n  translations, then 
we assign n1  to each translation of this phrase 
for all the four scores of the phrase pair. 
Constant Translation Probability: For each 
entry in the dictionary, we as ign a fixed score. 
In this case e sum of the translation probability 
is not necessarily equal to 1. 
s
, th
anslation probabili-
ties for the entries in the dictionary. And for the 
Corpus Translation Probability: If an in-
domain monolingual source corpus exists, we 
translate it with the method as described in Fig-
ure 1, and then estimate the tr
995
en
s.  
mmonly-used 
tries whose translation probabilities are not 
estimated, we assign average probabilities that 
are calculated from the entries that have obtained 
probabilities. 
4.3 Combining Phrase Tables  
In the algorithm, there are two kinds of phrase 
tables. We need to combine them to translate the 
in-domain text
Mixture Model: The most co
method is linear interpolation.  
)()1()()( ||| fefefe oI ppp ?? ?+=  (3)
W
ion probabili-
ties. 
here )( | feIp  and )( | feop  are the in-
domain and out-of-domain translat
?  is the interpolation weight. 
Discriminative Model: An alternative is to 
two tables in the log-linea  
ranslation oses, for 
en uses them 
fo
he out-of-domain lan-
tigate two 
ation and 
two different tasks: one 
is the Chinese to English translation in IWSLT 
 the other is the English to 
n adaptation translation in the 
red task. 
airs, with about 3 mil-
lio
                                                
combine the r model.
During t with M each phrase in 
the sentence, the decoder obtains all of its trans-
lations in both phrase tables, and th
r translation expansion. 
4.4 Combining Language Models 
If an in-domain target language corpus exists, we 
use it to construct an in-domain language model, 
which is combined with t
guage model. In this paper, we inves
combination methods: linear interpol
log-linear interpolation. 
5 Experiments  
5.1 Setting 
We ran experiments on 
2006 evaluation, and
French domai
WMT 2007 sha
For the Chinese to English translation task, we 
use the Chinese-English bilingual corpus pro-
vided by the Chinese Linguistic Data Consortium 
(CLDC)2 as the out-of-domain corpus. It con-
tains 156,840 sentence p
n English words and about 5 million Chinese 
characters.  In addition, we use the Basic Travel-
ing Expression Corpus (BTEC) released by 
IWSLT 2006 (Paul, 2006) to construct an in-
domain phrase table, as a comparison with that 
one constructed with the in-domain dictionary. 
 
2  http://www.chineseldc.org/EN/index.htm. The catalog 
number is CLDC-LAC-2003-004. It is a balanced corpus 
containing sentence pairs in multiple domains. 
Corpora Sentences OOV 
CLDC 156,840 89 (6.31%) 
BTEC 39,953 179 (12.69%) 
IWSLT06-dev4 489 NA 
IWSLT06-test 500 NA 
Table 1. Ch sh cor
aries Entries 
inese-Engli pora 
Diction OOV 
LDC 82,090 228 (16.16%) 
in-domain 32  .57%) ,821 121 (8
T s 
ces 
able 2. Chinese-English dictionarie
Corpora Senten OOV 
Europarl 949,410 412 (5.90%) 
NC 43,060 599 (8.58%) 
WMT07 dev 1,057 NA 
WMT07 test 2,007 NA 
Table 3. E ch cor
I art and rt a sed 
a in m ual  ex-
p  ta rts of  CLDC and 
BTEC are used for language m  construction 
(see Tab uation, 
nglish-Fren pora 
ts source p  target p
g
a re separately u
ra in ours the in-doma
e
onolin corpo
eriments. Th rget pa  both
odel
le 1). From the IWSLT 2006 eval
we choose the devset4 as our development data. 
Evaluation was performed on IWSLT 2006 test 
set. The references for the test set contain lower-
case words and punctuations. The detailed in-
formation is shown in Table 1. 
We use two kinds of manually-made diction-
aries for comparison: one is the LDC Chinese-
English Translation Lexicon Version 3.0 
(LDC2002L27), and the other is the in-domain 
spoken language dictionary made by ourselves, 
which contains in-domain Chinese words and 
their English translations. The dictionary is ma-
nually constructed. Some entries of the diction-
ary are collected from phrase books. Some of 
them are collected from the general-domain dic-
tionaries. And then, the entries are filtered and 
modified by a Chinese native speaker specialized 
in English. The detailed information is shown in 
Table 2. If a source word has two translations, it 
is counted as two entries. The OOV rates of the 
test set uncovered by the LDC dictionary and the 
in-domain dictionary are 16.16% and 8.57%, 
respectively. 
For the English to French translation task, the 
out-of-domain corpus is the Europarl corpus dis-
tributed for the shared task of WMT 2007 (Calli-
son-Burch et al, 2007)3. We filter the sentence 
pairs whose lengths are above 40 words. For the 
                                                 
3 http://www.statmt.org/wmt07/shared-task.html 
996
in-domain corpus, we use the News Commentary 
(NC) corpus distributed in WMT 2007. We also 
use the same development set and test set in the 
domain adaptation shared task (see Table 3). We 
manually built an in-domain English-French dic-
tionary according to the in-domain bilingual cor-
pus, which includes 26,821 entries. It contains 
in-domain English words and their French trans-
lations. The OOV rate of the test set uncovered 
by this dictionary is 22.34%. 
5.2 Evaluation Measures 
To perform phrase-based SMT, we use the 
rt training scripts. 
efault settings and 
ary 
rase table. With the in-
domain translation dictionary, we construct in-
do
lts, log-linear 
tra
tionary into the training corpus. In 
th
omain 
corpus to train a phrase table. Then we use both  
Moses decoder and its suppo
We run the decoder with its d
then use Moses' implementation of minimum 
error rate training (Och, 2003) to tune the feature 
weights on the development set. Translation 
quality was evaluated using BLEU score (Pap-
ineni et al, 2002).  
5.3 Results on Chinese-English Translation 
Translation Diction
With the out-of-domain bilingual corpus, we 
train an out-of-domain ph
main phrase tables by assigning different 
translation probabilities with two different meth-
ods: uniform and constant. For the constant 
translation probability, we set the score using the 
development set. In our experiments, we set it to 
1. We use the target part of the out-of-domain 
corpus to train a language model4. 
With two phrase tables, we combine them in a 
linear or log-linear method as described in sec-
tion 4.3. In our experimental resu
nslation models outperform the linear models 
(16.38 vs. 15.12), where the entries of the dic-
tionary are assigned with the constant translation 
probabilities. Thus, we will use log-linear models 
for phrase table combination in the following 
experiments. 
Another method to combine the out-of-domain 
corpus and the translation dictionary is to add the 
in-domain dic
is case, only one phrase table is trained. 
Table 4 describes the results using the out-of-
domain corpus and the in-domain dictionary. The 
baseline method only uses the out-of-d
                                                 
4 We also used LDC English Gigaword to train a large lan-
guage model. However, this language model did not im-
prove the translation quality. 
Methods Resources Used BLEU(%)
baseline out-of-domain corpus 13.59 
+dictionary as corpus 15.52 
+uniform prob. 16.00 
+constant prob. 16.38 
baseline + 
dictionary
+corpus prob. 16.72 
Table 4. Translation results of using out-of-
d ictionary
the out-of-dom  the in-dom c-
5 . The 
 also 
im
bine it with the 
 linear inter-
polation and log-linear interpolation. The ex-
pe
oves 
th
ifference be-
tw
 
                                                
omain corpus and in-domain d  
ain corpus and ain di
tionary. The results indicate that adding an in- 
domain dictionary significantly improves the 
translation quality by 2.79 BLEU score
methods using the dictionary as a phrase table 
outperform the method adding it to the training 
corpus. And the method using constant transla-
tion probabilities significantly outperforms that 
using the uniform translation probabilities. 
For comparison, we also assign corpus prob-
abilities to the entries in the dictionary by trans-
lating the source part of the BTEC corpus with 
the method described in Section 4.2. This
proves the translation quality. 
In-Domain Monolingual Corpora 
We use the target part of the BTEC corpus to 
train an in-domain LM. We com
out-of-domain LM in two methods:
rimental results indicate that linear interpola-
tion outperforms log-linear interpolation (17.16 
vs. 16.20). Thus, we will use linear interpolation 
for LMs in all of the following experiments. 
Table 5 describes the results of using the in-
terpolated language model. As compared with 
the results in Table 4, it can be seen that adding 
the in-domain language model greatly impr
e translation quality. It achieves an absolute 
improvement of 3.57 BLEU score as compared 
with the baseline model. If the in-domain transla-
tion dictionary is used, the translation quality is 
further improved by 4 BLEU score. 
If the in-domain source language data is avail-
able, we translate it and obtain a synthetic bilin-
gual corpus. Then we perform transductive learn-
ing as described in Figure 1. The d
een our method and that in (Ueffing et al, 
2007) is that we translate a larger in-domain 
source corpus, and we use 1-best translation 
 
5 We use the method described in (Koehn and Monz, 2006) 
for significance test. In this paper, significant improvement 
means method A outperforms method B on a significance 
level of no less than 95%. 
997
Methods Models  Resources used BLEU(%)
baseline Model 1 out-of-domain corpus 13.59 
baseline + TLC  Model 2 + in-domain TLC 17.16 
Model 3 + in-domain TLC + dictionary (uniform prob.) 20.83 baseline + TLC 
 dictionary (constant prob.) + dictionary Model 4 + in-domain TLC + 21.16 
Model 5 + in-domain SLC 15.98 
Model 6 + in-domain SLC and TLC 18.19 
transductive 
learning 
nd TLC + dictionary (corpus prob.) Model 7 + in-domain SLC a 21.75 
Table 5. 
Diction  BLEU(%)
Translation results of using in-domain resources 
ary types Entries OOV
general domain LDC 228 (1 %) 82,090 6.16 19.11 
manual 121 ) 32,821 (8.57% 21.16 
in-domain 
extracted 11,765 330 (23.39%) 19.88 
LD al C + manu 106,572 45 (3.19%) 21.34 combined 
LDC + extracted 95,660 202 (14.31%) 20.49 
Table 6. Comparison of ictionar
result with full re-training.
that transductive learning 
ed, the transla-
tio
tionaries 
with concern to the translation quality. Besides 
ld as described in (Wu and 
Wang, 2007). 
 phrase table, extract the 
 their translations. 
 
us
 when an in-domain bi-
m
d  different ies 
 The results indicate 
improves translation 
? From the filtered
Chinese words and
quality in all cases. For example, Model 5 
achieves an absolute improvement of 2.39 BLEU 
score over Model 1, and Model 6 achieves 1.03 
BLEU score improvement over Model 2. Model 
7 uses the in-domain dictionary with corpus 
translation probabilities, which are obtained from 
the phrase table trained with the synthetic bilin-
gual corpus. The results indicate that Model 7 
outperforms Model 4, with a significant im-
provement of 0.59 BLEU score.  
The results also indicate that when only the in-
domain monolingual corpus is us
n quality is improved by 4.6 BLEU score 
(Model 6 vs. Model 1). By adding the in-domain 
dictionary, the translation quality is further im-
proved, achieving an absolute improvement of 
8.16 BLEU score (Model 7 vs. Model 1). 
Comparison of Different Dictionaries 
We compare the effects of different dic
the manually-made in-domain dictionary, we use 
other two dictionaries: the LDC dictionary and 
an automatically built dictionary, which is ex-
tracted from the BTEC corpus. This extracted 
dictionary only contains Chinese words and their 
translations. The extraction method is as follows:  
? Build a phrase table with the in-domain bi-
lingual corpus. 
? Filter those phrase pairs whose values are 
below a thresho
? Assign constant translation probabilities to 
the entries of the extracted dictionary. 
Table 6 shows the translation results. All of 
the methods use the out-of-domain corpus, the 
in-domain target language corpus, and the corre-
sponding translation dictionaries with constant 
translation probabilities. The results indicate that
ing the general-domain dictionary also im-
proves translation quality, achieving an im-
provement of about 2 BLEU score as compared 
with Model 2 in Table 5. It can also be seen that 
the in-domain dictionaries significantly outper-
form the LDC dictionary although the extracted 
dictionary has a higher OOV rate than the LDC 
dictionary. Further analysis shows that the LDC 
dictionary does not contain the in-domain trans-
lations of some words. Results also indicate that 
combining the two kinds of dictionaries helps to 
slightly improve translation quality since the 
OOV rates are reduced. 
Comparison with In-domain Bilingual Corpus 
The aim of this section is to investigate 
whether the in-domain dictionary helps to im-
prove translation quality
lingual corpus is available. And we will also 
compare the translation results with those of the 
ethods only using in-domain dictionaries and 
monolingual corpora. 
To train the in-domain translation model, we 
use the BTEC corpus. The translation results are 
998
13
14
15
16
17
18
19
20
21
22
23
100k 200k 300k all
In-domain sentence pairs
BL
EU
 (
%)
CLDC
BTEC
CLDC+BTEC
CLDC+BTEC+Dic
CLDC+Mono+Dic
Figure 2. Comparison of different methods using 
different resources. 
shown in Figure 2. CLDC and BTEC represent 
s linear interpolation of the 
Interpolated LM" means that 
the methods that only use the out-of-domain and 
the in-domain corpus, respectively. The method 
"CLDC+BTEC" use
phrase tables and LMs trained with CLDC and 
BTEC.   "Dic" means using the in-domain dic-
tionary, and "Mono" means using in-domain 
source and target language corpora. 
From the results, it can be seen that (a) even if 
an in-domain bilingual corpus exists, the in-
domain dictionary also helps to improve the 
translation quality, as "CLDC+BTEC+Dic" 
achieves an improvement of about 1 BLEU score 
in comparison with "CLDC+BTEC"; (b) the 
method "CLDC+Mono+Dic", which uses both 
the in-domain monolingual corpora and the in-
domain dictionary, achieves high translation 
quality. It achieves slightly higher translation 
quality than "CLDC+BTEC" that uses the in-
domain bilingual corpus (21.75 vs.  21.62)  and 
achieves slightly lower translation quality than 
"CLDC+BTEC+Dic" (21.75 vs. 22.05). But the 
differences are not significant. This indicates that 
our method using an in-domain dictionary and 
in-domain monolingual corpora is effective for 
domain adaptation. 
5.4 Results on English-French Translation 
We perform the same experiments for English to 
French translation. Table 7 describes the domain 
adaptation results. "
we use the target part of the NC corpus to train 
an in-domain LM, and then linearly interpolate it 
with the out-of-domain LM trained with the Eu-
roparl corpus. The results indicate that using an 
in-domain target corpus significantly improves 
the translation quality, achieving an improve-
ment of 2.19 BLEU score (from 25.44 to 27.63).  
Methods Out-of-domain LM 
Interpolated 
LM 
Europarl 25.44 27.63 
Europarl+Dic 26.24 28.22 
transductive 
learning - 2  8.80
Europarl+NC - 29.19 
Europarl+NC+Dic - 29.41 
T ation result f using i in 
d onolingual corpora 
Using the in-domain translation dictionary im-
used (from 
29.19 to 29.41). 
n Table 7. The results indicate 
th
28.80). Although the translation quality 
is 
This 
 an out-of-domain corpus to 
ystem, and then used an in-
ion dictionary, to improve the transla-
able 7. Transl
ictionary and m
s o n-doma
proves translation quality in all cases, even when 
the in-domain bilingual corpus is 
We also perform transductive learning with 
the source part of the NC corpus. The model 
used to translate the corpus is that one created by 
"Europarl+Dic" i
at transductive learning significantly improves 
translation quality, achieving an absolute im-
provement of 0.58 BLEU score (from 28.22 to 
28.80).  
In summary, using an in-domain dictionary 
and in-domain monolingual corpora improves the 
translation quality by 3.36 BLEU score (from 
25.44 to 
slightly lower than that method of using both 
in-domain and out-of-domain bilingual corpora, 
the difference is not statistically significant. 
6 Conclusion 
This paper proposed a domain adaptation ap-
proach for statistical machine translation. 
approach first used
build a baseline s
domain translation dictionary and in-domain 
monolingual corpora to adapt it to the in-domain 
texts. The contribution of this paper lies in the 
following points: 
? We proposed a method to integrate a do-
main-specific translation dictionary into a 
phrase-based SMT system for domain adap-
tation. 
? We investigated the way of using in-domain 
monolingual corpora in either source or tar-
get language, together with the in-domain 
translat
tion quality of a baseline system. 
999
We performed experiments on both Chinese to 
English and English to French translation. Ex-
perimental results on Chinese to English transla-
tio
-
vised Language Model Adaptation. In Proc. of the 
ational Conference on Acoustics, 
 Signal Processing (ICASSP-2003), 
Br
n. Computational Linguistics, 
Bu
f the 32nd International Confer-
Ca
 Statistical Ma-
Ci
 
, pages 177-180. 
Fo
Ko
ation of Machine Translation be-
Ko
erico, Nicola Bertoldi, 
Ko
istical Ma-
M
Oc
ranslation. In Proc. of 
Pa
2. BLEU: a Method for Auto-
Pa
ation Campaign. In Proc. of the International 
St
 Proc. of International 
Ue
rning for Statistical 
W
ics and Phrase-
n indicate that all of the in-domain resources 
are useful to improve in-domain translation qual-
ity, with an overall improvement of 8.16 BLEU 
score as compared with the baseline trained with 
out-of-domain corpora. Results on English to 
French translation also show that using in-
domain translation dictionaries and in-domain 
monolingual corpora is effective for domain ad-
aptation, achieving an absolute improvement of 
3.36 BLEU score. And the results on both trans-
lation tasks indicate that the translation quality 
achieved by our methods is comparable with that 
of the method using both in-domain and out-of-
domain bilingual corpora. Moreover, even if in-
domain and out-of-domain bilingual corpora are 
available, adding an in-domain dictionary also 
helps to improve the translation quality. 
In the future work, we will investigate to as-
sign translation probabilities to the dictionaries 
using comparable in-domain corpora and exam-
ine its effect on the MT performance. And we 
will also examine the effect of an in-domain dic-
tionary on transductive learning in more details. 
References 
Bacchiani, Michiel and Brian Roark. 2003. Unsuper
28th Intern
Speech, and
pages 224-227. 
own, Peter F., Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimatio
19(2): 263-311. 
lyko, Ivan, Spyros Matsoukas, Richard Schwartz, 
Long Nguyen, and John Makhoul. 2007. Language 
Model Adaptation in Machine Translation from 
Speech. In Proc. o
ence on Acoustics, Speech, and Signal Processing 
(ICASSP-2007), pages 117-120. 
llison-Burch, Chris, Cameron Fordyce, Philipp 
Koehn, Christof Monz, and Josh Schroeder. 2007. 
(Meta-) Evaluation of Machine Translation. In 
Proc. of the Second Workshop on
chine Translation, pages 136-158. 
vera, Jorge and Alfons Juan. 2007. Domain Adapta-
tion in Statistical Machine Translation with Mix-
ture Modelling. In Proc. of the Second Workshop
on Statistical Machine Translation
ster, George and Roland Kuhn. 2007. Mixture-
Model Adaptation for SMT. In Proc. of the Second 
Workshop on Statistical Machine Translation, 
pages 128-135. 
ehn, Philipp and Christof Monz. 2006. Manual and 
Automatic Evalu
tween European Languages. In Proc. of the HLT-
NAACL 2006 Workshop on Statistical Machine 
Translation, pages 102-121. 
ehn, Philipp, Hieu Hoang, Alexanda Birch, Chris 
Callison-Burch, Marcello Fed
Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
In Proc. of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-2007), 
demonstration session, pages 177-180. 
ehn, Philipp and Josh Schroeder. 2007. Experi-
ments in Domain Adaptation for Stat
chine Translation. In Proc. of the Second Workshop 
on Statistical Machine Translation, pages 224-227. 
unteanu, Dragos Stefan, and Daniel Marcu. 2005. 
Improving Machine Translation Performance by 
Exploiting Non-Parallel Corpora. Computational 
Linguistics, 31(4): 477-504. 
h, Franz Josef. 2003. Minimum Error Rate Train-
ing in Statistical Machine T
the 41st Annual Meeting of the Association for 
Computational Linguistics (ACL-2003), pages 160-
167. 
pineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 200
matic Evaluation of Machine Translation. In Proc. 
of the 40th Annual Meeting of the Association of 
Computational Linguistics (ACL-2002), pages 311-
318. 
ul, Michael. 2006. Overview of the IWSLT 2006 
Evalu
Workshop on Spoken Language Translation 
(IWSLT-2006), pages 1-15. 
olcke, Andrea. 2002. SRILM -- an Extensible Lan-
guage Modeling Toolkit. In
Conference on Spoken Language Processing 
(ICSLP-2002), pages 901-904. 
ffing, Nicola, Gholamreza Haffari, and Anoop 
Sarkar. 2007. Transductive Lea
Machine Translation. In Proc. of 45th Annual 
Meeting of the Association of Computational Lin-
guistics (ACL-2007), pages 25-32. 
u, Hua and Haifeng Wang. 2007. Comparative 
Study of Word Alignment Heurist
Based SMT. In Proc. of Machine Translation 
Summit XI, pages 507-514. 
1000
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 257?266, Prague, June 2007. c?2007 Association for Computational Linguistics
Recovering Non-Local Dependencies for Chinese
Yuqing Guo
NCLT, School of Computing
Dublin City University
Dublin 9, Ireland
yguo@computing.dcu.ie
Haifeng Wang
Toshiba (China)
Research and Development Center
Beijing, 100738, China
wanghaifeng@rdc.toshiba.com.cn
Josef van Genabith
NCLT, School of Computing
Dublin City University
IBM CAS, Dublin, Ireland
josef@computing.dcu.ie
Abstract
To date, work on Non-Local Dependencies
(NLDs) has focused almost exclusively on
English and it is an open research question
how well these approaches migrate to other
languages. This paper surveys non-local de-
pendency constructions in Chinese as repre-
sented in the Penn Chinese Treebank (CTB)
and provides an approach for generating
proper predicate-argument-modifier struc-
tures including NLDs from surface context-
free phrase structure trees. Our approach re-
covers non-local dependencies at the level
of Lexical-Functional Grammar f-structures,
using automatically acquired subcategorisa-
tion frames and f-structure paths linking an-
tecedents and traces in NLDs. Currently our
algorithm achieves 92.2% f-score for trace
insertion and 84.3% for antecedent recovery
evaluating on gold-standard CTB trees, and
64.7% and 54.7%, respectively, on CTB-
trained state-of-the-art parser output trees.
1 Introduction
A substantial number of linguistic phenomena such
as topicalisation, relativisation, coordination and
raising & control constructions, permit a constituent
in one position to bear the grammatical role asso-
ciated with another position. These relationships
are referred to Non-Local Dependencies (NLDs),
where the surface location of the constituent is
called /antecedent0, and the site where the an-
tecedent should be interpreted semantically is called
/trace0. Capturing non-local dependencies is cru-
cial to the accurate and complete determination of
semantic interpretation in the form of predicate-
argument-modifier structures or deep dependencies.
However, with few exceptions (Model 3 of
Collins, 1999; Schmid, 2006), output trees pro-
duced by state-of-the-art broad coverage statistical
parsers (Charniak, 2000; Bikel, 2004) are only sur-
face context-free phrase structure trees (CFG-trees)
without empty categories and coindexation to repre-
sent displaced constituents. Because of the impor-
tance of non-local dependencies in the proper de-
termination of predicate-argument structures, recent
years have witnessed a considerable amount of re-
search on reconstructing such hidden relationships
in CFG-trees. Three strategies have been proposed:
(i) post-processing parser output with pattern match-
ers (Johnson, 2002), linguistic principles (Campbell,
2004) or machine learning methods (Higgins, 2003;
Levy and Manning, 2004; Gabbard et al, 2006) to
recover empty nodes and identify their antecedents;1
(ii) integrating non-local dependency recovery into
the parser by enriching a simple PCFG model with
GPSG-style gap features (Collins, 1999; Schmid,
2006); (iii) pre-processing the input sentence with
a finite-state trace tagger which detects empty nodes
before parsing, and identify the antecedents on the
parser output with the gap information (Dienes and
Dubey, 2003a; Dienes and Dubey, 2003b).
In addition to CFG-oriented approaches, a num-
ber of richer treebank-based grammar acquisition
and parsing methods based on HPSG (Miyao et
al., 2003), CCG (Clark and Hockenmaier, 2002),
LFG (Riezler et al, 2002; Cahill et al, 2004) and
Dependency Grammar (Nivre and Nilsson, 2005)
incorporate non-local dependencies into their deep
syntactic or semantic representations.
A common characteristic of all these approaches
1(Jijkoun, 2003; Jijkoun and Rijke, 2004) also describe post-
processing methods to recover NLDs, which are applied to syn-
tactic dependency structures converted from CFG-trees.
257
is that, to date, the research has focused almost
entirely on English,2 despite the disparity in type
and frequency of non-local dependencies for vari-
ous languages. In this paper, we address recover-
ing non-local dependencies for Chinese, a language
drastically different from English and whose spe-
cial features such as lack of morphological inflection
make NLD recovery more challenging. Inspired by
(Cahill et al, 2004)?s methodology which was origi-
nally designed for English and Penn-II treebank, our
approach to Chinese non-local dependency recovery
is based on Lexical-Functional Grammar (LFG), a
formalism that involves both phrase structure trees
and predicate-argument structures. NLDs are re-
covered in LFG f-structures using automatically ac-
quired subcategorisation frames and finite approxi-
mations of functional uncertainty equations describ-
ing NLD paths at the level of f-structures.
The paper is structured as follows: in Section 2 we
outline the distinguishing features of Chinese non-
local dependencies compared to English. In Section
3 we review (Cahill et al, 2004)?s method for recov-
ering English NLDs in treebank-based LFG approx-
imations. In Section 4, we describe how we mod-
ify and substantially extend the previous method
to recover all types of NLDs for Chinese data.
We present experiments and provide a dependency-
based evaluation in Section 5. Finally we conclude
and summarise future work.
2 Non-Local Dependencies in Chinese
In the Penn Chinese Treebank (CTB) (Xue et al,
2002) non-local dependencies are represented in
terms of empty categories (ECs) and (for some of
them) coindexation with antecedents, as exemplified
in Figure 1. Following previous work for English
and the CTB annotation scheme, we use /non-
local dependencies0as a cover term for all miss-
ing or dislocated elements represented in the CTB
as an empty category (with or without coindexa-
tion/antecedent), and our use of the term remains ag-
nostic about fine-grained distinctions between non-
local dependencies drawn in the theoretical linguis-
tics literature.
In order to give an overview on the character-
2 (Levy and Manning, 2004) is the only approach we are
aware of that has been applied to both English and German.
(1) ? ?u? ?k d?  # ?[
not want look-for train have potential DE new writer
?(People) don?t want to look for and train new writers who
have potential.?
IP
NP-SBJ
-NONE-
*pro*
VP
ADVP
AD
?
not
VP
VV
?
want
IP-OBJ
NP-SBJ
-NONE-
*PRO*
VP
VP
VV
u?
look for
NP-OBJ
-NONE-
*RNR*-2
PU
!
VP
VV
?
train
NP-OBJ-2
CP
WHNP-1
-NONE-
*OP*
CP
IP
NP-SBJ
-NONE-
*T*-1
VP
VE
k
have
NP
NN
d?
potential
DEC

DE
ADJP
JJ
#
new
NP
NN
?[
writer
Figure 1: Example of non-local annotations in CTB,
including dropped subject (*pro*), control subject
(*PRO*), relative clause (*T*), and coordination
(*RNR*).
istics of Chinese non-local dependencies, we ex-
tracted all empty categories together with coindexed
antecedents from the Penn Chinese Treebank ver-
sion 5.1 (CTB5.1). Table 1 gives a breakdown of the
most frequent types of empty categories and their
antecedents, which account for 43,791 of the total
43,954 (99.6%) ECs in CTB5.1.3
According to their different linguistics properties,
we divide the empty nodes listed in Table 1 into
three major types: null relative pronouns, locally
mediated dependencies, and long-distance depen-
dencies.
Null Relative Pronouns (lines 2, 7) themselves
are local dependencies, and thus are not coindexed
with an antecedent. But they mediate non-local de-
pendencies by functioning as antecedents for the dis-
3An extensive description of the types of empty categories
and the use of coindexation in CTB can be found in Section VI
of the bracketing guidelines.
258
Antecedent POS Label Count Description
1 WHNP NP *T* 11670 WH trace (e.g. *OP*?I/Chinau/launch*T*/DE?(/satellite)
2 WHNP *OP* 11621 Empty relative pronouns (e.g. *OP*?I/Chinau/launch/DE?(/satellite)
3 NP *PRO* 10946 Control constructions (e.g. ?p/here?/notN/allow*PRO*??/smoke)
4 NP *pro* 7481 Pro-drop situations (e.g. *pro*?/notQ/ever?/encounter/DE?K/problem)
5 IP IP *T* 575 Topicalisation (e.g. ??/weU/canI/win?/he`/say*T*)
6 WHPP PP *T* 337 WH trace (e.g. *OP*<?/population*T*?8/dense/?/area)
7 WHPP *OP* 337 Empty relative pronouns (e.g. *OP*<?/population?8/dense/?/area)
8 NP NP * 291 Raising & passive constructions (e.g. ??/weProceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 287?295, Prague, June 2007. c?2007 Association for Computational Linguistics
Using RBMT Systems to Produce Bilingual Corpus for SMT 
Xiaoguang Hu, Haifeng Wang, Hua Wu 
Toshiba (China) Research and Development Center 
5/F., Tower W2, Oriental Plaza 
No.1, East Chang An Ave., Dong Cheng District 
Beijing, 100738, China 
{huxiaoguang, wanghaifeng, wuhua}@rdc.toshiba.com.cn 
 
 
Abstract 
This paper proposes a method using the ex-
isting Rule-based Machine Translation 
(RBMT) system as a black box to produce 
synthetic bilingual corpus, which will be 
used as training data for the Statistical Ma-
chine Translation (SMT) system. We use 
the existing RBMT system to translate the 
monolingual corpus into synthetic bilingual 
corpus. With the synthetic bilingual corpus, 
we can build an SMT system even if there 
is no real bilingual corpus. In our experi-
ments using BLEU as a metric, the system 
achieves a relative improvement of 11.7% 
over the best RBMT system that is used to 
produce the synthetic bilingual corpora. 
We also interpolate the model trained on a 
real bilingual corpus and the models 
trained on the synthetic bilingual corpora. 
The interpolated model achieves an abso-
lute improvement of 0.0245 BLEU score 
(13.1% relative) as compared with the in-
dividual model trained on the real bilingual 
corpus. 
1 Introduction 
Within the Machine Translation (MT) field, by far 
the most dominant paradigm is SMT, but many 
existing commercial systems are rule-based. In this 
research, we are interested in answering the ques-
tion of whether the existing RBMT systems could 
be helpful to the development of an SMT system. 
To find the answer, let us first consider the follow-
ing facts: 
? Existing RBMT systems are usually pro-
vided as a black box. To make use of such 
systems, the most convenient way might 
be working on the translation results di-
rectly. 
? SMT methods rely on bilingual corpus. As 
a data driven method, SMT usually needs 
large bilingual corpus as the training data. 
Based on the above facts, in this paper we pro-
pose a method using the existing RBMT system as 
a black box to produce a synthetic bilingual cor-
pus1, which will be used as the training data for the 
SMT system. 
For a given language pair, the monolingual cor-
pus is usually much larger than the real bilingual 
corpus. We use the existing RBMT system to 
translate the monolingual corpus into synthetic 
bilingual corpus. Then, even if there is no real bi-
lingual corpus, we can train an SMT system with 
the monolingual corpus and the synthetic bilingual 
corpus. If there exist n available RBMT systems 
for the desired language pair, we use the n systems 
to produce n synthetic bilingual corpora, and n 
translation models are trained with the n corpora 
respectively. We name such a model the synthetic 
model. An interpolated translation model is built 
by linear interpolating the n synthetic models. In 
our experiments using BLEU (Papineni et al, 2002) 
as the metric, the interpolated synthetic model 
achieves a relative improvement of 11.7% over the 
best RBMT system that is used to produce the syn-
thetic bilingual corpora.  
                                                 
1 In this paper, to be distinguished from the real bilingual cor-
pus, the bilingual corpus generated by the RBMT system is 
called a synthetic bilingual corpus.  
287
Moreover, if a real bilingual corpus is available 
for the desired language pair, we build another 
translation model, which is named the standard 
model. Then we can build an interpolated model 
by interpolating the standard model and the syn-
thetic models. Experimental results show that the 
interpolated model achieves an absolute improve-
ment of 0.0245 BLEU score (13.1% relative) as 
compared with the standard model. 
The remainder of this paper is organized as fol-
lows. In section 2 we summarize the related work. 
We then describe our method Using RBMT sys-
tems to produce bilingual corpus for SMT in sec-
tion 3. Section 4 describes the resources used in the 
experiments. Section 5 presents the experiment 
result, followed by the discussion in section 6. Fi-
nally, we conclude and present the future work in 
section 7. 
2 Related Work 
In the MT field, by far the most dominant 
paradigm is SMT. SMT has evolved from the 
original word-based approach (Brown et al, 1993) 
into phrase-based approaches (Koehn et al, 2003; 
Och and Ney, 2004) and syntax-based approaches 
(Wu, 1997; Alshawi et al, 2000; Yamada and 
Knignt, 2001; Chiang, 2005). On the other hand, 
much important work continues to be carried out in 
Example-Based Machine Translation (EBMT) 
(Carl et al, 2005; Way and Gough, 2005), and 
many existing commercial systems are rule-based. 
Although we are not aware of any previous at-
tempt to use an existing RBMT system as a black 
box to produce synthetic bilingual training corpus 
for general purpose SMT systems, there exists a 
great deal of work on MT hybrids and Multi-
Engine Machine Translation (MEMT). 
Research into MT hybrids has increased over the 
last few years. Some research focused on the hy-
brid of various corpus-based MT methods, such as 
SMT and EBMT (Vogel and Ney, 2000; Marcu, 
2001; Groves and Way, 2006; Menezes and Quirk, 
2005). Others tried to exploit the advantages of 
both rule-based and corpus-based methods. Habash 
et al (2006) built an Arabic-English generation-
heavy MT system and boosted it with SMT com-
ponents. METIS-II is a hybrid machine translation 
system, in which insights from SMT, EBMT, and 
RBMT are used (Vandeghinste et al, 2006). Seneff 
et al (2006) combined an interlingual translation 
framework with phrase-based SMT for spoken 
language translation in a limited domain. They 
automatically generated a corpus of English-
Chinese pairs from the same interlingual represen-
tation by parsing the English corpus and then para-
phrasing each utterance into both English and Chi-
nese. 
Frederking and Nirenburg (1994) produced the 
first MEMT system by combining outputs from 
three different MT engines based on their knowl-
edge of the inner workings of the engines. Nomoto 
(2004) used voted language models to select the 
best output string at sentence level. Some recent 
approaches to MEMT used word alignment tech-
niques for comparison between the MT systems 
(Jayaraman and Lavie, 2005; Zaanen and Somers, 
2005; Matusov et al 2006). All the above MEMT 
systems operate on MT outputs for complete input 
sentences. Mellebeek et al (2006) presented a dif-
ferent approach, using a recursive decomposition 
algorithm that produces simple chunks as input to 
the MT engines. A consensus translation is pro-
duced by combining the best chunk translation. 
This paper uses RBMT outputs to improve the 
performance of SMT systems. Instead of RBMT 
outputs, other researchers have used SMT outputs 
to boost translation quality. Callision-Burch and 
Osborne (2003) used co-training to extend existing 
parallel corpora, wherein machine translations are 
selectively added to training corpora with multiple 
source texts. They also created training data for a 
language pair without a parallel corpus by using 
multiple source texts. Ueffing (2006) explored 
monolingual source-language data to improve an 
existing machine translation system via self-
training. The source data is translated by a SMT 
system, and the reliable translations are automati-
cally identified. Both of the methods improved 
translation quality. 
3 Method 
In this paper, we use the synthetic and real bilin-
gual corpus to train the phrase-based translation 
models. 
3.1  Phrase-Based Models 
According to the translation model presented in 
(Koehn et al, 2003), given a source sentence f , 
the best target translation  can be obtained 
using the following model 
beste
288
)()()(maxarg
)(maxarg
|
|
e
e
e
eef
fee
length
LM
best
?pp
p
=
=
 (1)
Where the translation model can be 
decomposed into  
)( | efp
?
=
??=
I
i
iiiiii
II
aefpbadef
efp
1
1
11
),|()()|(
)|(
?? w
(2)
Where )|( ii ef?  is the phrase translation prob-
ability.  denotes the start position of the source 
phrase that was translated into the ith target phrase, 
and  denotes the end position of the source 
phrase translated into the (i-1)th target phrase. 
 is the distortion probability. 
ia
1?ib
)( 1?? ii bad
),|( aefp iiw  is the lexical weight, and ?  is the 
strength of the lexical weight. 
3.2 Interpolated Models 
We train synthetic models with the synthetic bilin-
gual corpus produced by the RBMT systems. We 
can also train a translation model, namely standard 
model, if a real bilingual corpus is available. In 
order to make full use of these two kinds of cor-
pora, we conduct linear interpolation between them. 
In this paper, the distortion probability in equa-
tion (2) is estimated during decoding, using the 
same method as described in Pharaoh (Koehn, 
2004). For the phrase translation probability and 
lexical weight, we interpolate them as shown in (3) 
and (4). 
?
=
=
n
i
ii efef
0
)|()|( ???  (3)
?
=
=
n
i
ii aefpaefp
0
),|(),|( w,w ?  (4)
Where )|(0 ef?  and ),|( aefpw,0  denote the 
phrase translation probability and lexical weight 
trained with the real bilingual corpus, respectively. 
)|( efi?  and ),|( aefp iw,  ( ) are the 
phrase translation probability and lexical weight 
estimated by n  synthetic corpora produced by the 
RBMT systems. 
ni ,...,1=
i?  and i?  are interpolation coef-
ficients, ensuring  and . 1
0
=?
=
n
i
i? 1
0
=?
=
n
i
i?
4 Resources Used in Experiments 
4.1 Data 
In the experiments, we take English-Chinese trans-
lation as a case study. The real bilingual corpus 
includes 494,149 English-Chinese bilingual sen-
tence pairs. The monolingual English corpus is 
selected from the English Gigaword Second Edi-
tion, which is provided by Linguistic Data Consor-
tium (LDC) (catalog number LDC2005T12). The 
selected monolingual corpus includes 1,087,651 
sentences. 
For language model training, we use part of the 
Chinese Gigaword Second Edition provided by 
LDC (catalog number LDC2005T14). We use 
41,418 documents selected from the ZaoBao 
Newspaper and 992,261 documents from the Xin-
Hua News Agency to train the Chinese language 
model, amounting to 5,398,616 sentences. 
The test set and the development set are from 
the corpora distributed for the 2005 HTRDP 2  
evaluation of machine translation.  It can be ob-
tained from Chinese Linguistic Data Consortium 
(catalog number 2005-863-001). We use the same 
494 sentences in the test set and 278 sentences in 
the development set. Each source sentence in the 
test set and the development set has 4 different ref-
erences. 
4.2 Tools 
In this paper, we use two off-the-shelf commercial 
English to Chinese RBMT systems to produce the 
synthetic bilingual corpus. 
We also need a trainer and a decoder to perform 
phrase-based SMT. We use Koehn's training 
scripts 3  to train the translation model, and the 
SRILM toolkit (Stolcke, 2002) to train language 
model. For the decoder, we use Pharaoh (Koehn, 
2004). We run the decoder with its default settings 
(maximum phrase length 7) and then use Koehn's 
implementation of minimum error rate training 
(Och, 2003) to tune the feature weights on the de-
                                                 
2 The full name of HTRDP is National High Technology Re-
search and Development Program of China, also named as 863 
Program. 
3  It is located at http://www.statmt.org/wmt06/shared-
task/baseline.html. 
289
velopment set. The translation quality is evaluated 
using a well-established automatic measure: BLEU 
score (Papineni et al, 2002). We use the same 
method described in (Koehn and Monz, 2006) to 
perform the significance test. 
5 Experimental Results 
5.1 Results on Synthetic Corpus Only 
With the monolingual English corpus and the Eng-
lish side of the real bilingual corpus, we translate 
them into Chinese using the two commercial 
RBMT systems and produce two synthetic bilin-
gual corpora. With the corpora, we train two syn-
thetic models as described in section 3.1. Based on 
the synthetic models, we also perform linear inter-
polation as shown in section 3.2, without the stan-
dard models. We tune the interpolation weights 
using the development set, and achieve the best 
performance when 58.01 =? , 42.02 =? , 
58.01 =? , and 42.02 =? . The translation results 
on the test set are shown in Table 1. Synthetic 
model 1 and 2 are trained using the synthetic bilin-
gual corpora produced by RBMT system 1 and 
RBMT system 2, respectively. 
Method BLEU 
RBMT system 1 0.1681 
RBMT system 2 0.1453 
Synthetic Model 1 0.1644 
Synthetic Model 2 0.1668 
Interpolated Synthetic Model 0.1878 
Table 1. Translation Results Using Synthetic Bi-
lingual Corpus 
From the results, it can be seen that the interpo-
lated synthetic model obtains the best result, with 
an absolute improvement of the 0.0197 BLEU 
(11.7% relative) as compared with RBMT system 
1, and 0.0425 BLEU (29.2% relative) as compared 
with RBMT system 2. It is very promising that our 
method can build an SMT system that significantly 
outperforms both of the two RBMT systems, using 
the synthetic bilingual corpus produced by two 
RBMT systems. 
5.2 Results on Real and Synthetic Corpus 
With the real bilingual corpus, we build a standard 
model. We interpolate the standard model with the 
two synthetic models built in section 5.1 to obtain 
interpolated models. The translation results are 
shown in Table 2. The interpolation coefficients 
are both for phrase table probabilities and lexical 
weights. They are also tuned using the develop-
ment set.  
From the results, it can be seen that all the three 
interpolated models perform not only better than 
the RBMT systems but also better than the SMT 
system trained on the real bilingual corpus. The 
interpolated model combining the standard model 
and the two synthetic models performs the best, 
achieving a statistically significant improvement of 
about 0.0245 BLEU (13.1% relative) as compared 
with the standard model with no synthetic corpus. 
It also achieves 26.1% and 45.8% relative im-
provement as compared with the two RBMT sys-
tems respectively. The results indicate that using 
the corpus produced by RBMT systems, the per-
formance of the SMT system can be greatly im-
proved. The results also indicate that the more the 
RBMT systems are used, the better the translation 
quality is. 
Interpolation Coefficients 
Standard 
model 
Synthetic 
Model 1 
Synthetic 
Model 2 
BLEU 
1 ? ? 0.1874 
0.90 0.10 ? 0.2056 
0.86 ? 0.14 0.2040 
0.70 0.12 0.18 0.2119 
Table 2. Translation Results Using Standard and 
Synthetic Bilingual Corpus 
5.3 Effect of Synthetic Corpus Size 
To explore the relationship between the translation 
quality and the scale of the synthetic bilingual cor-
pus, we interpolate the standard model with the 
synthetic models trained with synthetic bilingual 
corpus of different sizes. In order to simplify the 
procedure, we only use RBMT system 1 to trans-
late the 1,087,651 monolingual English sentences 
to produce the synthetic bilingual corpus.  
We randomly select 20%, 40%, 60%, 80%, and 
100% of the synthetic bilingual corpus to train dif-
ferent synthetic models. The translation results of 
the interpolated models are shown in Figure 1. The 
results indicate that the larger the synthetic bilin-
gual corpus is, the better translation performance 
would be. 
290
0.13
0.15
0.17
0.19
0.21
20 40 60 80 100
Synthetic Bilingual Corpus (%)
B
L
E
U
Interpolated 
Standard
Synthetic
 
0.12
0.14
0.16
0.18
0.2
0.22
20 40 60 80 100
Real Bilingual Corpus (%)
B
L
E
U
Interpolated 
Standard
Synthetic
 
Figure 1. Comparison of Translation Results Using 
Synthetic Bilingual Corpus of Different Sizes 
Figure 2. Comparison of Translation Results Using 
Real Bilingual Corpus of Different Sizes 
5.4 Effect of Real Corpus Size Interpolation Coefficients 
Standard 
model 
Synthetic 
Model 1 
Synthetic 
Model 2 
BLEU 
1 ? ? 0.1874 
? 1 ? 0.1560 
? ? 1 0.1522 
0.80 0.10 0.10 0.1972 
Another issue is the relationship between the SMT 
performance and the size of the real bilingual cor-
pus. To train different standard models, we ran-
domly build five corpora of different sizes, which 
contain 20%, 40%, 60%, 80%, and 100% sentence 
pairs of the real bilingual corpus, respectively. As 
to the synthetic model, we use the same synthetic 
model 1 that is described in section 5.1. Then we 
build five interpolated models by performing linear 
interpolation between the synthetic model and the 
five standard models respectively.  The translation 
results are shown in Figure 2.  
Table 3. Translation Results without Additional 
Monolingual Corpus 
 Standard Model 
Synthetic 
Model 1 
Synthetic 
Model 2 
Standard 
Model 6,105,260 ? ? 
Synthetic 
Model 1 356,795 12,062,068 ? 
Synthetic 
Model 2 357,489 881,921 9,216,760
From the results, we can see that the larger the 
real bilingual corpus is, the better the performance 
of both standard models and interpolated models 
would be. The relative improvement of BLEU 
scores is up to 27.5% as compared with the corre-
sponding standard models. 
Table 4. Numbers of Phrase Pairs  5.5 Results without Additional Monolingual 
Corpus cant improvement of about 0.01 BLEU (5.2% rela-
tive) as compared with the standard model without 
using the synthetic corpus. In all the above experiments, we use an additional English monolingual corpus to get more synthetic 
bilingual corpus. We are also interested in the re-
sults without the additional monolingual corpus. In 
such case, the only English monolingual corpus is 
the English side of the real bilingual corpus. We 
use this smaller size of monolingual corpus and the 
real bilingual corpus to conduct similar experi-
ments as in section 5.2. The translation results are 
shown in Table 3. 
In order to further analyze the translation results, 
we examine the overlap and the difference among 
the phrase tables. The analytic results are shown in 
Table 4. More phrase pairs are extracted by the 
synthetic models, about twice by the synthetic 
model 1 in particular, than those extracted by the 
standard model. The overlap between each model 
is very low. For example, about 6% phrase pairs 
extracted by the standard model make appearance 
in both the standard model and the synthetic model 
1. This also explains why the interpolated model 
outperforms that of the standard model in Table 3.  
From the results, it can be seen that our method 
works well even if no additional monolingual cor-
pus is available. We achieve a statistically signifi- 
291
Methods English Sentence / Chinese Translations BLEU
 
This move helps spur the enterprise to strengthen technical innovation, man-
agement innovation and the creation of a brand name and to strengthen mar-
keting, after-sale service, thereby fundamentally enhance the enterprise's 
competitiveness; 
 
Standard 
model 
? ? ?? ??? ?? ?? ?? ?? ?? ? ?? ?? ? ?? ?? 
?? ? ?? ? ?? ? ?? ? ?? ?? ? ??? ? ? ?? ?? 0.5022
RBMT Sys-
tem 1 
?? ?? ?? ?? ?? ?? ?? ? ?? ? ?? ?? ? ?? ?? 
?? ?? ?? ?? ? ? ?? ?? ? ??? ?? ?? ?? ? ?
? ? 
0.1535
RBMT Sys-
tem 2 
?? ?? ?? ?? ?? ?? ?? ?? ?? ? ?? ?? ? ? ? 
?? ? ?? ? ?? ?? ?? ? ?? ???? ?? ?? ?? ?? 
? ?? ? 
0.1485
Interpolated 
Model 
? ? ?? ??? ?? ?? ?? ?? ?? ? ?? ?? ? ?? ?? 
? ?? ?? ?? ? ???? ? ?? ? ?? ? ?? ?? ? ??
? ? 
0.7198
Table 5. Translation Example 
This move  ? ? ?? This move  ? ? ?? 
helps  ??? helps  ??? 
spur  ?? spur  ?? 
the enterprise  ?? the enterprise  ?? 
to strengthen  ?? to strengthen  ?? 
technical  ?? technical  ?? 
innovation  ?? innovation  ?? 
, management  ? ?? , management  ? ?? 
innovation  ?? innovation  ?? 
and the creation of a  ? ?? and the creation of  ? ?? 
  (he jianli)  (he chuangzao) 
brand name  ?? a brand name  ?? 
  (pinpai)  (pinpai) 
and to strengthen  ?? ? and to strengthen  ? ?? 
marketing ,  ?? marketing ,  ?? ?? ? 
  (fuwu) after-sale service  ???? 
after-sale  ? ??  (shouhoufuwu) 
service  ? ?? ? , thereby  ? ?? 
, thereby  ?? fundamentally  ? ?? ? 
fundamentally  ?? enhance the  ?? 
enhance  ? ??? enterprise 's  ?? ? 
the enterprise  ? competitiveness  ??? 
's competitiveness  ? ?? ;  ? 
;  ??   
  (shouhou)   
(a) Results Produced by the Standard Model (b) Results Produced by the Interpolated Model 
Figure 3. Phrase Pairs Used for Translation 
 
292
6 Discussion 
6.1 Model Interpolation vs. Corpus Merge 
In section 5, we make use of the real bilingual cor-
pus and the synthetic bilingual corpora by perform-
ing model interpolation. Another available way is 
directly combining these two kinds of corpora to 
train a translation model, namely corpus merge. In 
order to compare these two methods, we use 
RBMT system 1 to translate the 1,087,651 mono-
lingual English sentences to produce synthetic bi-
lingual corpus. Then we train an SMT system with 
the combination of this synthetic bilingual corpus 
and the real bilingual corpus. The BLEU score of 
such system is 0.1887, while that of the model in-
terpolation system is 0.2020. It indicates that the 
model interpolation method is significantly better 
than the corpus merge method. 
6.2 Result Analysis 
As discussed in Section 5.5, the number of the 
overlapped phrase pairs among the standard model 
and the synthetic models is very small. The newly 
added phrase pairs from the synthetic models can 
assist to improve the translation results of the in-
terpolated model. In this section, we will use an 
example to further discuss the reason behind the 
improvement of the SMT system by using syn-
thetic bilingual corpus. Table 5 shows an English 
sentence and its Chinese translations produced by 
different methods. And Figure 3 shows the phrase 
pairs used for translation. The results show that 
imperfect translations of RBMT systems can be 
also used to boost the performance of an SMT sys-
tem. 
 Phrase Pairs 
Phrase 
Pairs 
Used 
New 
Pairs 
Used 
Standard 
Model 6,105,260 5,509 ? 
Interpolated 
Model 73,221,525 5,306 1993 
Table 6. Statistics of Phrase Pairs 
Further analysis is shown in Table 6. After add-
ing the synthetic corpus produced by the RBMT 
systems, the interpolated model outperforms the 
standard models mainly for the following two rea-
sons: (1) some new phrase pairs are added into the 
interpolated model. 37.6% phrase pairs (1993 out 
of 5306) are newly learned and used for translation. 
For example, the phrase pair "after-sale service <-> 
???? (shouhoufuwu)" is added; (2) The prob-
ability distribution of the phrase pairs is changed. 
For example, the probabilities of the two pairs "a 
brand name <-> ?? (pinpai)" and "and the crea-
tion of <-> ? ?? (he chuangzao)" increase. The 
probabilities of the other two pairs "brand name <-
> ?? (pinpai)" and "and the creation of a <-> ? 
??  (he jianli)" decrease. We found that 930 
phrase pairs, which are also in the phrase table of 
the standard model, are used by the interpolated 
model for translation but not used by the standard 
model. 
6.3 Human Evaluation 
According to (Koehn and Monz, 2006; Callison-
Burch et al, 2006), the RBMT systems are usually 
not adequately appreciated by BLEU. We also 
manually evaluated the RBMT systems and SMT 
systems in terms of both adequacy and fluency as 
defined in (Koehn and Monz, 2006). The evalua-
tion results show that the SMT system with the 
interpolated model, which achieves the highest 
BLEU scores in Table 2, achieves slightly better 
adequacy and fluency scores than the two RBMT 
systems. 
7 Conclusion and Future Work 
We presented a method using the existing RBMT 
system as a black box to produce synthetic bilin-
gual corpus, which was used as training data for 
the SMT system. We used the existing RBMT sys-
tem to translate the monolingual corpus into a syn-
thetic bilingual corpus. With the synthetic bilingual 
corpus, we could build an SMT system even if 
there is no real bilingual corpus. In our experi-
ments using BLEU as the metric, such a system 
achieves a relative improvement of 11.7% over the 
best RBMT system that is used to produce the syn-
thetic bilingual corpora. It indicates that using the 
existing RBMT systems to produce a synthetic bi-
lingual corpus, we can build an SMT system that 
outperforms the existing RBMT systems. 
We also interpolated the model trained on a real 
bilingual corpus and the models trained on the syn-
thetic bilingual corpora, the interpolated model 
achieves an absolute improvement of 0.0245 
BLEU score (13.1% relative) as compared with the 
individual model trained on the real bilingual cor-
293
pus. It indicates that we can build a better SMT 
system by leveraging the real and the synthetic bi-
lingual corpus. 
Further result analysis shows that after adding 
the synthetic corpus produced by the RBMT sys-
tems, the interpolated model outperforms the stan-
dard models mainly because of two reasons: (1) 
some new phrase pairs are added to the interpo-
lated model; (2) the probability distribution of the 
phrase pairs is changed. 
In the future work, we will investigate the possi-
bility of training a reverse SMT system with the 
RBMT systems. For example, we will investigate 
to train Chinese-to-English SMT system based on 
natural English and RBMT-generated synthetic 
Chinese. 
References 
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas. 
2000. Learning Dependency Translation Models as 
Collections of Finite-State Head Transducers. Com-
putational Linguistics, 26(1): 45-60. 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics, 19(2): 
263-311. 
Chris Callison-Burch and Miles Osborne. 2003. Boot-
strapping Parallel Corpora. In Proceedings of the 
Human Language Technology conference / North 
American chapter of the Association for Computa-
tional Linguistics (HLT/NAACL-2003) Workshop on 
Building and Using Parallel Texts: Data Driven Ma-
chine Translation and Beyond, pages 44-49. 
Chris Callison-Burch, Miles Osborne and Philipp 
Koehn, 2006. Re-evaluating the Role of Bleu in Ma-
chine Translation Research. In Proceedings of the 
11th Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL-
2006), pages 249-256. 
Michel Carl, Paul Schmidt, and Jorg Schutz. 2005. Re-
versible Template-based Shake & Bake Generation. 
In Proceedings of the 10th Machine Translation 
Summit Workshop on Example-Based Machine 
Translation, pages 17-25. 
David Chiang. 2005. A Hierarchical Phrase-Based 
Model for Statistical Machine Translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-2005), 
pages 263-270. 
Robert Frederking and Sergei Nirenburg. 1994. Three 
Heads Are Better Than One. In Proceedings of the 
4th Applied Natural Language Processing Confer-
ence (ANLP-1994), pages 95-100. 
Declan Groves and Andy Way. 2006. Hybridity in MT: 
Experiments on the Europarl Corpus. In Proceedings 
of the 11th Annual Conference of the European As-
sociation for Machine Translation (EAMT-2006), 
pages 115-124. 
Nizar Habash, Bonnie Dorr, and Christof Monz. 2006 
Challenges in Building an Arabic-English GHMT 
System with SMT Components. In Proceedings of 
the 11th Annual Conference of the European Asso-
ciation for Machine Translation (EAMT-2006), pages 
56-65. 
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine Machine Translation Guided by Explicit 
Word Matching. In Proceedings of the 10th Annual 
Conference of the European Association for Machine 
Translation (EAMT-2005), pages 143-152. 
Philipp Koehn. 2004. Pharaoh: A Beam Search Decoder 
For Phrase-Based Statistical Machine Translation 
Models. In Proceedings of the 6th Conference of the 
Association for Machine Translation in the Americas 
(AMTA-2004), pages 115-124. 
Philipp Koehn and Christof Monz. 2006. Manual and 
Automatic Evaluation of Machine Translation be-
tween European Languages. In Proceedings of the 
Human Language Technology conference / North 
American Chapter of the Association for Computa-
tional Linguistics (HLT/NAACL-2006) Workshop on 
Statistical Machine Translation, pages 102-121. 
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the Human Language Technology con-
ference / North American Chapter of the Association 
for Computational Linguistics (HLT/NAACL-2003), 
pages 127-133. 
Daniel Marcu. 2001. Towards a Unified Approach to 
Memory- and Statistical-based Machine Translation. 
In Proceedings of the Association for Computational 
Linguistics / European Chapter of the Association for 
Computational Linguistics (ACL/EACL-2001), pages 
378-385. 
Evgeny Matusov, Nicola Ueffing, and Hermann Ney. 
2006. Computing Consensus Translation from Multi-
ple Machine Translation Systems Using Enhanced 
Hypotheses Alignment. In Proceedings of the 11th 
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL-2006), 
pages 33-40. 
294
Bart Mellebeek, Karolina Owczarzak, Josef Van 
Genabith, and Andy Way. 2006. Multi-engine Ma-
chine Translation by Recursive Sentence Decomposi-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas 
(AMTA-2006), pages 110-118. 
Arul Menezes and Chris Quirk. 2005. Dependency 
treelet translation: the convergence of statistical and 
example-based machine-translation? In Proceedings 
of the 10th Machine Translation Summit Workshop 
on Example-Based Machine Translation, pages 99-
108. 
Tadashi Nomoto. 2004. Multi-Engine machine transla-
tion with voted language model. In Proceedings of 
the 42nd Annual Meeting of the Association for 
Computational Linguistics (ACL-2004), pages 494-
501. 
Franz Josef Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proceedings of 
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL-2003), pages 160-167. 
Franz Josef Och and Hermann Ney. 2004. The Align-
ment Template Approach To Statistical Machine 
Translation. Computational Linguistics, 30(4):417-
449. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. In Proceedings 
of the 40th Annual Meeting of the Association for 
Computational Linguistics (ACL-2002), pages 311-
318. 
Stephanie Seneff, Chao Wang, and John Lee. 2006. 
Combining Linguistic and Statistical Methods for Bi-
Directional English Chinese Translation in the Flight 
Domain. In Proceedings of the 7th Conference of the 
Association for Machine Translation in the Americas 
(AMTA-2006), pages 213-222. 
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the 5th 
International Conference on Spoken Language Proc-
essing (ICSLP-2002), pages 901-904. 
Nicola Ueffing. 2006. Using Monolingual Source-
Language Data to Improve MT Performance.  In 
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT-2006), pages 174-
181. 
Vincent Vandeghinste, Ineka Schuurman, Michael Carl, 
Stella Markantonatou, and Toni Badia. 2006. Metis-
II: Machine Translation for Low-Resource Lan-
guages. In Proceedings of the 5th International Con-
ference on Language Resources and Evaluation (L-
REC-2006), pages 1284-1289. 
Stephan Vogel and Hermann Ney. 2000. Construction 
of a Hierarchical Translation Memory. In Proceed-
ings of the 18th International Conference on Compu-
tational Linguistics (COLING-2000), pages 1131-
1135. 
Andy Way and Nano Gough. 2005. Comparing Exam-
ple-Based and Statistical Machine Translation. Natu-
ral Language Engineering, 11(3): 295-309. 
Dekai Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Corpora. 
Computational Linguistics, 23(3): 377-403. 
Kenji Yamada and Kevin Knight. 2001. A Syntax Based 
Statistical Translation Model. In Proceedings of the 
Association for Computational Linguistics / Euro-
pean Chapter of the Association for Computational 
Linguistics (ACL/EACL-2001), pages 523-530. 
Menno van Zaanen and Harold Somers. 2005. DE-
MOCRAT: Deciding between Multiple Outputs Cre-
ated by Automatic Translation. In Proceedings of the 
10th Machine Translation Summit, pages 173-180. 
 
 
295
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 487?495,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Collocation Extraction Using Monolingual Word Alignment Method 
 
Zhanyi Liu1,2, Haifeng Wang2, Hua Wu2, Sheng Li1 
1Harbin Institute of Technology, Harbin, China 
2Toshiba (China) Research and Development Center, Beijing, China 
{liuzhanyi,wanghaifeng,wuhua}@rdc.toshiba.com.cn 
lisheng@hit.edu.cn 
 
  
 
Abstract 
Statistical bilingual word alignment has been 
well studied in the context of machine trans-
lation. This paper adapts the bilingual word 
alignment algorithm to monolingual scenario 
to extract collocations from monolingual cor-
pus. The monolingual corpus is first repli-
cated to generate a parallel corpus, where 
each sentence pair consists of two identical 
sentences in the same language. Then the 
monolingual word alignment algorithm is 
employed to align the potentially collocated 
words in the monolingual sentences. Finally 
the aligned word pairs are ranked according 
to refined alignment probabilities and those 
with higher scores are extracted as colloca-
tions. We conducted experiments using Chi-
nese and English corpora individually. Com-
pared with previous approaches, which use 
association measures to extract collocations 
from the co-occurring word pairs within a 
given window, our method achieves higher 
precision and recall. According to human 
evaluation in terms of precision, our method 
achieves absolute improvements of 27.9% on 
the Chinese corpus and 23.6% on the English 
corpus, respectively. Especially, we can ex-
tract collocations with longer spans, achiev-
ing a high precision of 69% on the long-span 
(>6) Chinese collocations. 
1 Introduction 
Collocation is generally defined as a group of 
words that occur together more often than by 
chance (McKeown and Radev, 2000). In this pa-
per, a collocation is composed of two words oc-
curring as either a consecutive word sequence or 
an interrupted word sequence in sentences, such 
as "by accident" or "take ? advice". The collo-
cations in this paper include phrasal verbs (e.g. 
"put on"), proper nouns (e.g. "New York"), idi-
oms (e.g. "dry run"), compound nouns (e.g. "ice 
cream"), correlative conjunctions (e.g. "either ? 
or"), and the other commonly used combinations 
in following types: verb+noun, adjective+noun, 
adverb+verb, adverb+adjective and adjec-
tive+preposition (e.g. "break rules", "strong tea", 
"softly whisper", "fully aware", and "fond of"). 
Many studies on collocation extraction are 
carried out based on co-occurring frequencies of 
the word pairs in texts (Choueka et al, 1983; 
Church and Hanks, 1990; Smadja, 1993; Dun-
ning, 1993; Pearce, 2002; Evert, 2004). These 
approaches use association measures to discover 
collocations from the word pairs in a given win-
dow. To avoid explosion, these approaches gen-
erally limit the window size to a small number. 
As a result, long-span collocations can not be 
extracted1. In addition, since the word pairs in 
the given window are regarded as potential col-
locations, lots of false collocations exist. Al-
though these approaches used different associa-
tion measures to filter those false collocations, 
the precision of the extracted collocations is not 
high. The above problems could be partially 
solved by introducing more resources into collo-
cation extraction, such as chunker (Wermter and 
Hahn, 2004), parser (Lin, 1998; Seretan and We-
hrli, 2006) and WordNet (Pearce, 2001). 
This paper proposes a novel monolingual 
word alignment (MWA) method to extract collo-
cation of higher quality and with longer spans 
only from monolingual corpus, without using 
any additional resources. The difference between 
MWA and bilingual word alignment (Brown et 
al., 1993) is that the MWA method works on 
monolingual parallel corpus instead of bilingual 
corpus used by bilingual word alignment. The 
                                                 
1  Here, "span of collocation" means the distance of two 
words in a collocation. For example, if the span of the col-
location (w1, w2) is 6, it means there are 5 words interrupt-
ing between w1 and w2 in a sentence. 
487
monolingual corpus is replicated to generate a 
parallel corpus, where each sentence pair con-
sists of two identical sentences in the same lan-
guage, instead of a sentence in one language and 
its translation in another language. We adapt the 
bilingual word alignment algorithm to the mono-
lingual scenario to align the potentially collo-
cated word pairs in the monolingual sentences, 
with the constraint that a word is not allowed to 
be aligned with itself in a sentence. In addition, 
we propose a ranking method to finally extract 
the collocations from the aligned word pairs. 
This method assigns scores to the aligned word 
pairs by using alignment probabilities multiplied 
by a factor derived from the exponential function 
on the frequencies of the aligned word pairs. The 
pairs with higher scores are selected as colloca-
tions. 
The main contribution of this paper is that the 
well studied bilingual statistical word alignment 
method is successfully adapted to monolingual 
scenario for collocation extraction. Compared 
with the previous approaches, which use associa-
tion measures to extract collocations, our method 
achieves much higher precision and slightly 
higher recall. The MWA method has the follow-
ing three advantages. First, it explicitly models 
the co-occurring frequencies and position infor-
mation of word pairs, which are integrated into a 
model to search for the potentially collocated 
word pairs in a sentence. Second, a new feature, 
fertility, is employed to model the number of 
words that a word can collocate with in a sen-
tence. Finally, our method can obtain the long-
span collocations. Human evaluations on the ex-
tracted Chinese collocations show that 69% of 
the long-span (>6) collocations are correct. Al-
though the previous methods could also extract 
long-span collocations by setting the larger win-
dow size, the precision is very low. 
In the remainder of this paper, Section 2 de-
scribes the MWA model for collocation extrac-
tion. Section 3 describes the initial experimental 
results. In Section 4, we propose a method to 
improve the MWA models. Further experiments 
are shown in Sections 5 and 6, followed by a dis-
cussion in Section 7. Finally, the conclusions are 
presented in Section 8. 
2 Collocation Extraction With Mono-
lingual Word Alignment Method 
2.1 Monolingual Word Alignment 
Given a bilingual sentence pair, a source lan-
guage word can be aligned with its correspond- 
 
Figure 1. Bilingual word alignment 
ing target language word. Figure 1 shows an ex-
ample of Chinese-to-English word alignment. 
In Figure 1, a word in one language is aligned 
with its counterpart in the other language. For 
examples, the Chinese word "??/tuan-dui" is 
aligned with its English translation "team", while 
the Chinese word "???/fu-ze-ren" is aligned 
with its English translation "leader". 
In the Chinese sentence in Figure 1, there are 
some Chinese collocations, such as (??/tuan-
dui, ???/fu-ze-ren). There are also some Eng-
lish collocations in the English sentence, such as 
(team, leader). We separately illustrate the collo-
cations in the Chinese sentence and the English 
sentence in Figure 2, where the collocated words 
are aligned with each other. 
 
(a) Collocations in the Chinese sentence 
 
(b) Collocations in the English sentence 
Figure 2. Word alignments of collocations in 
sentence 
Comparing the alignments in Figures 1 and 2, 
we can see that the task of monolingual colloca-
tions construction is similar to that of bilingual 
word alignment. In a bilingual sentence pair, a 
source word is aligned with its corresponding 
target word, while in a monolingual sentence, a 
word is aligned with its collocates. Therefore, it 
is reasonable to regard collocation construction 
as a task of aligning the collocated words in 
monolingual sentences. 
?? ??? ? ??  ??  ?  ? ??  ??   ? 
tuan-dui fu-ze-ren zai xiang-mu jin-xing zhong qi guan-jian zuo-yong . 
The team leader plays a key role in the project undertaking . 
The team leader plays a key role in the project undertaking.
The team leader plays a key role in the project undertaking. 
?? ??? ? ??  ??  ?  ? ??  ??   ? 
tuan-dui fu-ze-ren zai xiang-mu jin-xing zhong qi guan-jian zuo-yong .
??  ??? ? ??  ??  ?  ? ??  ??   ?
tuan-dui fu-ze-ren zai xiang-mu jin-xing zhong qi guan-jian zuo-yong . 
488
Statistical bilingual word alignment method, 
which has been well studied in the context of 
machine translation, can extract the aligned bi-
lingual word pairs from a bilingual corpus. This 
paper adapts the bilingual word alignment algo-
rithm to monolingual scenario to align the collo-
cated words in a monolingual corpus. 
Given a sentence with l words },...,{ 1 lwwS = , 
the word alignments ]},1[|),{( liaiA i ?=  can be 
obtained by maximizing the word alignment 
probability of the sentence, according to Eq. (1). 
)|(maxarg SApA
A
?=
??
                    (1) 
Where Aai i ?),(  means that the word iw  is 
aligned with the word 
ia
w . 
In a monolingual sentence, a word never col-
locates with itself. Thus the alignment set is de-
noted as }&],1[|),{( ialiaiA ii ??= . 
We adapt the bilingual word alignment model, 
IBM Model 3 (Brown et al, 1993), to monolin-
gual word alignment. The probability of the 
alignment sequence is calculated using Eq. (2). 
???
==
l
j
jaj
l
i
ii lajdwwtwnSAp j
11
),|()|()|()|( ?   (2) 
Where i?  denotes the number of words that are 
aligned with iw . Three kinds of probabilities are 
involved: 
- Word collocation probability )|(
jaj
wwt , 
which describes the possibility of wj collo-
cating with 
ja
w ;  
- Position collocation probability d(j, aj, l), 
which describes the probability of a word 
in position aj collocating with another 
word in position j; 
- Fertility probability )|( ii wn ? , which de-
scribes the probability of the number of 
words that a word wi can collocate with 
(refer to subsection 7.1 for further discus-
sion). 
Figure 3 shows an example of word alignment 
on the English sentence in Figure 2 (b) with the 
MWA method. In the sentence, the 7th word 
"role" collocates with both the 4th word "play" 
and the 6th word "key". Thus, )|( 74 wwt  and 
)|( 76 wwt  describe the probabilities that the 
word "role" collocates with "play" and "key",  
 
Figure 3. Results of MWA method 
respectively. )12,7|4(d  and )12,7|6(d  describe 
the probabilities that the word in position 7 col-
locates with the words in position 4 and 6 in a 
sentence with 12 words. For the word "role", 7?  
is 2, which indicates that the word "role" collo-
cates with two words in the sentence. 
To train the MWA model, we implement a 
MWA tool for collocation extraction, which uses 
similar training methods for bilingual word 
alignment, except that a word can not be aligned 
to itself. 
2.2 Collocation Extraction 
Given a monolingual corpus, we use the trained 
MWA model to align the collocated words in 
each sentence. As a result, we can generate a set 
of aligned word pairs on the corpus. According 
to the alignment results, we calculate the fre-
quency for two words aligned in the corpus, de-
noted as ),( ji wwfreq . In our method, we filtered 
those aligned word pairs whose frequencies are 
lower than 5. Based on the alignment frequency, 
we estimate the alignment probabilities for each 
aligned word pair as shown in Eq. (3) and (4). 
? ?=
?w j
ji
ji wwfreq
wwfreq
wwp
),(
),(
)|(  (3) 
? ?=
?w i
ji
ij wwfreq
wwfreq
wwp
),(
),(
)|(  (4) 
With alignment probabilities, we assign scores 
to the aligned word pairs and those with higher 
scores are selected as collocations, which are 
estimated as shown in Eq. (5). 
2
)|()|(
),( ijjiji
wwpwwp
wwp
+=      (5) 
3 Initial Experiments 
In this experiment, we used the method as de-
scribed in Section 2 for collocation extraction. 
Since our method does not use any linguistic in-
formation, we compared our method with the  
The team leader plays a key role in the project undertaking . 
(1)        (2)           (3)           (4)      (5)   (6)      (7)      (8)    (9)        (10)               (11)               (12) 
The team leader plays a key role in the project undertaking .
(1)        (2)           (3)           (4)      (5)   (6)      (7)      (8)    (9)        (10)               (11)              (12) 
489
02
4
6
8
10
12
0 20 40 60 80 100 120 140 160 180 200
Top-N collocations (K)
Pr
ec
is
io
n 
(%
)
Our method (Probability)
Log-likelihood ratio
 
Figure 4. Precision of collocations 
baseline methods without using linguistic knowl-
edge. These baseline methods take all co-
occurring word pairs within a given window as 
collocation candidates, and then use association 
measures to rank the candidates. Those candi-
dates with higher association scores are extracted 
as collocations. In this paper, the window size is 
set to [-6, +6]. 
3.1 Data 
The experiments were carried out on a Chinese 
corpus, which consists of one year (2004) of the 
Xinhua news corpus from LDC 2 , containing 
about 28 millions of Chinese words. Since punc-
tuations are rarely used to construct collocations, 
they were removed from the corpora. To auto-
matically estimate the precision of extracted col-
locations on the Chinese corpus, we built a gold 
set by collecting Chinese collocations from 
handcrafted collocation dictionaries, containing 
56,888 collocations. 
3.2 Results 
The precision is automatically calculated against 
the gold set according to Eq. (6). 
)(#
)(#
Top
goldTop
N
N
C
CC
precision
?
?= I            (6) 
Where CTop-N and Cgold denote the top colloca-
tions in the N-best list and the collocations in the 
gold set, respectively. 
We compared our method with several base-
line methods using different association meas-
ures3: co-occurring frequency, log-likelihood 
                                                 
2 Available at: http://www.ldc.upenn.edu/Catalog/Catalog 
Entry.jsp?catalogId=LDC2007T03 
3 The definitions of these measures can be found in Man-
ning and Sch?tze (1999). 
0
20
40
60
80
100
0.0 2.5 3.7 4.8 5.8 6.8 7.8 8.9
log(frequency)
(%
)
Precision
Alignment Probability
 
Figure 5. Frequency vs. precision/alignment 
probability 
ratio, chi-square test, mutual information, and t-
test. Among them, the log-likelihood ratio meas-
ure achieves the best performance. Thus, in this 
paper, we only show the performance of the log-
likelihood ratio measure. 
Figure 4 shows the precisions of the top N col-
locations as N steadily increases with an incre-
ment of 1K, which are extracted by our method 
and the baseline method using log-likelihood 
ratio as the association measure. 
The absolute precision of collocations is not 
high in the figure. For example, among the top 
200K collocations, about 4% of the collocations 
are correct. This is because our gold set contains 
only about 57K collocations. Even if all colloca-
tions in the gold set are included in the 200K-
best list, the precision is only 28%. Thus, it is 
more useful to compare precision curves for col-
locations in the N-best lists extracted by different 
methods. In addition, since this gold set only in-
cludes a small number of collocations, the preci-
sion curves of our method and the baseline 
method are getting closer, as N increases. For 
example, when N is set to 200K, our method and 
the baseline method achieved precisions of 
4.09% and 3.12%, respectively. And when N is 
set to 400K, they achieved 2.78% and 2.26%, 
respectively. For convenience of comparison, we 
set N up to 200K in the experiments. 
From the results, it can also be seen that, 
among the N-best lists with N less than 20K, the 
precision of the collocations extracted by our 
method is lower than that of the collocations ex-
tracted by the baseline, and became higher when 
N is larger than 20K. 
In order to analyze the possible reasons, we 
investigated the relationships among the fre-
quencies of the aligned word pairs, the alignment 
490
xy
b =4
b =2
 
Figure 6.  xbey /?=  
probabilities, and precisions of collocations, 
which are shown in Figure 5. From the figure, 
we can see (1) that the lower the frequencies of 
the aligned word pairs are, the higher the align-
ment probabilities are; and (2) that the precisions 
of the aligned word pairs with lower frequencies 
is lower. According to the above observations, 
we conclude that it is the word pairs with lower 
frequencies but higher probabilities that caused 
the lower precision of the top 20K collocations 
extracted by our method. 
4 Improved MWA Method 
According to the analysis in subsection 3.2, we 
need to penalize the aligned word pairs with 
lower frequencies. In order to achieve the above 
goal, we need to refine the alignment probabili-
ties by using a penalization factor derived from a 
function on the frequencies of the aligned word 
pairs. This function )(xfy =  should satisfy the 
following two conditions, where x  represents 
the log function of frequencies. 
(1) The function is monotonic. When x  is set to 
a smaller number, y  is also small. This re-
sults in the penalization on the aligned word 
pairs with lower frequencies. 
(2) When ??x , y  is set to 1. This means that 
we don?t penalize the aligned word pairs 
with higher frequencies. 
According to the above descriptions, we pro-
pose to use the exponential function in Eq. (7).  
    xbey /?=  (7)
Figure 6 describes this function. The constant 
b in the function is used to adjust the shape of the 
line. The line is sharp with b set to a small num-
ber, while the line is flat with b set to a larger 
number. In our case, if b is set to a larger number,  
0
5
10
15
20
25
0 20 40 60 80 100 120 140 160 180 200
Top-N collocations (K)
Pr
ec
is
io
n 
(%
)
Refined probability
Probability
Baseline (Log-likelihood ratio)
 
Figure 7. Precision of collocations extracted by 
the improved method 
we assign a larger penalization weight to those 
aligned word pairs with lower frequencies. 
According to the above discussion, we can use 
the following measure to assign scores to the 
aligned words pairs generated by the MWA 
method. 
)),(log(
 
2
)|()|(
),(
ji wwfreq
b
ijji
jir
e
wwpwwp
wwp
?
?+=
  (8) 
Where wi and wj are two aligned words. p(wi|wj) 
and p(wj|wi) are alignment probabilities as shown 
in Eq. (3) and (4). )),(log( ji wwfreq  is the log 
function of the frequencies of the aligned word 
pairs (wi, wj). 
5 Evaluation on Chinese corpus 
We used the same Chinese corpus described in 
Section 3 to evaluate the improved method as 
shown in Section 4. In the experiments, b  was 
tuned by using a development set and set to 25. 
5.1 Precision 
In this section, we evaluated the extracted collo-
cations in terms of precision using both auto-
matic evaluation and human evaluation. 
Automatic Evaluation 
Figure 7 shows the precisions of the colloca-
tions in the N-best lists extracted by our method 
and the baseline method against the gold set in 
Section 3. For our methods, we used two differ-
ent measures to rank the aligned word pairs: 
alignment probabilities in Eq. (5) and refined 
491
 Our method Baseline 
True 569 290 
A 25 16 
B 5 4 
C 240 251 
False 
D 161 439 
Table 1. Manual evaluation of the top 1K Chi-
nese collocations. The precisions of our method 
and the baseline method are 56.9% and 29.0%, 
respectively. 
alignment probabilities in Eq. (8). From the re-
sults, it can be seen that with the refined align-
ment probabilities, our method achieved the 
highest precision on the N-best lists, which 
greatly outperforms the best baseline method. 
For example, in the top 1K list, our method 
achieves a precision of 20.6%, which is much 
higher than the precision of the baseline method 
(11.7%). This indicates that the exponential func-
tion used to penalize the alignment probabilities 
plays a key role in demoting most of the aligned 
word pairs with low frequencies. 
Human Evaluation 
In automatic evaluation, the gold set only con-
tains collocations in the existing dictionaries. 
Some collocations related to specific corpora are 
not included in the set. Therefore, we selected 
the top 1K collocations extracted by our im-
proved method to manually estimate the preci-
sion. During human evaluation, the true colloca-
tions are denoted as "True" in our experiments. 
The false collocations were further classified into 
the following classes. 
A: The candidate consists of two words that 
are semantically related, such as (?? doctor,  
?? nurse). 
B: The candidate is a part of the multi-word 
(? 3) collocation. For example, (?? self, ??  
mechanism) is a part of the three-word colloca-
tion (?? self, ?? regulating, ?? mecha-
nism). 
C: The candidates consist of the adjacent 
words that frequently occur together, such as (? 
he, ? say) and (? very, ? good). 
D: Two words in the candidates have no rela-
tionship with each other, but occur together fre-
quently, such as (?? Beijing, ? month) and 
(? and, ? for). 
Table 1 shows the evaluation results. Our 
method extracted 569 true collocations, which  
0
2
4
6
8
10
12
0 1 2 3 4 5 6 7 8 9 10 11 12
Training corpus (Months)
Pr
ec
is
io
n 
(%
)
Our method
Baseline
 
Figure 8. Corpus size vs. precision 
are much more than those extracted by the base-
line method. Further analysis shows that, in addi-
tion to extracting short-span collocations, our 
method extracted collocations with longer spans 
as compared with the baseline method. For ex-
ample, (?? in, ?? state) and (?? because, 
?? so) are two long-span collocations. Among 
the 1K collocations, there are 48 collocation can-
didates whose spans are larger than 6, which are 
not covered by the baseline method since the 
window size is set to 6.  And 33 of them are true 
collocations, with a higher precision of 69%. 
Classes C and D account for the most part of 
the false collocations. Although the words in 
these two classes co-occur frequently, they can 
not be regarded as collocations. And we also 
found out that the errors in class D produced by 
the baseline method are much more than that of 
those produced by our method. This indicates 
that our MWA method can remove much more 
noise from the frequently occurring word pairs. 
In Class A, the two words are semantically re-
lated and occur together in the corpus. These 
kinds of collocations can not be distinguished 
from the true collocations by our method without 
additional resources. 
Since only bigram collocations were extracted 
by our method, the multi-word (? 3) collocations 
were split into bigram collocations, which caused 
the error collocations in Class B4. 
Corpus size vs. precision 
Here, we investigated the effect of the corpus 
size on the precision of the extracted collocations. 
We evaluated the precision against the gold set 
as shown in the automatic evaluation. First, the 
whole corpus (one year of newspaper) was split 
into 12 parts according to the published months. 
Then we calculated the precisions as the training 
                                                 
4 Since only a very small faction of collocations contain 
more than two words, a few error collocations belong to 
Class B. 
492
020
40
60
80
100
0 20 40 60 80 100 120 140 160 180 200
Top-N collocations (K)
R
ec
al
l (
%
)
Our method
Baseline
 
Figure 9. Recall on the Chinese corpus 
corpus increases part by part. The top 20K collo-
cations were selected for evaluation. 
Figure 8 shows the experimental results. The 
precision of collocations extracted by our method 
is obviously higher than that of collocations ex-
tracted by the baseline method. When the size of 
the training corpus became larger, the difference 
between our method and the baseline method 
also became bigger. When the training corpus 
contains more than 9 months of corpora, the pre-
cision of collocations extracted by the baseline 
method did not increase anymore. However, the 
precision of collocations extracted by our method 
kept on increasing. This indicates the MWA 
method can extract more true collocations of 
higher quality when it is trained with larger size 
of training data. 
5.2 Recall 
Recall was evaluated on a manually labeled sub-
set of the training corpus. The subset contains 
100 sentences that were randomly selected from 
the whole corpus. The sentence average length is 
24. All true collocations (660) were labeled 
manually. The recall was calculated according to 
Eq. (9). 
)(#
)(#
subset
subsetTop
C
CC
recall N
I?=               (9) 
Here, CTop-N denotes the top collocations in the 
N-best list and Csubset denotes the true colloca-
tions in the subset. 
Figure 9 shows the recalls of collocations ex-
tracted by our method and the baseline method 
on the labeled subset. The results show that our 
method can extract more true collocations than 
the baseline method. 
0
20
40
60
80
100
0 20 40 60 80 100 120 140 160 180 200
Top-N collocations (K)
R
ec
al
l (
%
)
Our method
Baseline
 
Figure 10. Recall on the English corpus 
 Our method Baseline 
True 591 355 
A 11 4 
B 19 20 
C 200 136 
False
D 179 485 
Table 2. Manual evaluation of the top 1K Eng-
lish collocations. The precisions of our method 
and the baseline method are 59.1% and 35.5%, 
respectively. 
In our experiments, the baseline method ex-
tracts about 20 millions of collocation candidates, 
while our method only extracts about 3 millions 
of collocation candidates5. Although the colloca-
tions of our method are much less than that of the 
baseline, the experiments show that the recall of 
our method is higher. This again proved that our 
method has the stronger ability to distinguish 
true collocations from false collocations. 
6 Evaluation on English corpus 
We also manually evaluated the proposed 
method on an English corpus, which is a subset 
randomly extracted from the British National 
Corpus6. The English corpus contains about 20 
millions of words. 
6.1 Precision 
We estimated the precision of the top 1K collo-
cations. Table 2 shows the results. The classifica-
tion of the false collocations is the same as that 
in Table 1. The results show that our methods 
outperformed the baseline method using log- 
                                                 
5 We set the threshold to 7.88 with a confidence level  of 
005.0=?  (cf. page 174 of Chapter 5 in (McKeown and 
Radev, 2000) for more details). 
6 Available at: http://www.hcu.ox.ac.uk/BNC/ 
493
05
10
15
20
0 20 40 60 80 100 120 140 160 180
Top-N collocation (K)
Pr
ec
is
io
n 
(%
)
 
Figure 11. Fertility vs. precision 
likelihood ratio. And the distribution of the false 
collocations is similar to that on the Chinese cor-
pus. 
6.2 Recall 
We used the method described in subsection 5.2 
to calculate the recall. 100 English sentences 
were labeled manually, obtaining 205 true collo-
cations. Figure 10 shows the recall of the collo-
cations in the N-best lists. From the figure, it can 
be seen that the trend on the English corpus is 
similar to that on the Chinese corpus, which in-
dicates that our method is language-independent. 
7 Discussion 
7.1 The Effect of Fertility 
In the MWA model as described in subsection 
2.1, i?  denotes the number of words that can 
align with iw . Since a word only collocates with 
a few other words in a sentence, we should set a 
maximum number for ? , denote as max? . 
In order to set max? , we examined the true col-
locations in the manually labeled set described in 
subsection 5.2. We found that 78% of words col-
locate with only one word, and 17% of words 
collocate with two words. In sum, 95% of words 
in the corpus can only collocate with at most two 
words. According to the above observation, we 
set max?  to 2. 
In order to further examine the effect of max?  
on collocation extraction, we used several differ-
ent max?  in our experiments. The comparison 
0
1
2
3
4
5
6
7
8
0 20 40 60 80 100
Span of collocation
lo
g(
#(
al
ig
ne
d 
w
or
d 
pa
irs
))
 
Figure 12. Distribution of spans 
results are shown in Figure 11. The highest pre-
cision is achieved when max?  is set to 2. This 
result verifies our observation on the corpus. 
7.2 Span of Collocation 
One of the advantages of our method is that 
long-span collocations can be reliably extracted. 
In this subsection, we investigate the distribution 
of the span of the aligned word pairs. For the 
aligned word pairs occurring more than once, we 
calculated the average span as shown in Eq. (10). 
),(
);,(
),(
ji
corpuss
ji
ji wwfreq
swwSpan
wwAveSpan
?
= ?  (10) 
Where, );,( swwSpan ji  is the span of the words 
wi and wj in the sentence s; ),( ji wwAveSpan  is 
the average span. 
The distribution is shown in Figure 12. It can 
be seen that the number of the aligned word pairs 
decreased exponentially as the average span in-
creased. About 17% of the aligned word pairs 
have spans longer than 6. According to the hu-
man evaluation result for precision in subsection 
5.1, the precision of the long-span collocations is 
even higher than that of the short-span colloca-
tions. This indicates that our method can extract 
reliable collocations with long spans. 
8 Conclusion 
We have presented a monolingual word align-
ment method to extract collocations from mono-
lingual corpus. We first replicated the monolin-
gual corpus to generate a parallel corpus, in 
which each sentence pair consists of the two 
identical sentences in the same language. Then 
we adapted the bilingual word alignment algo-
rithm to the monolingual scenario to align the 
10
3
2
1
max
max
max
max
=
=
=
=
?
?
?
?
494
potentially collocated word pairs in the monolin-
gual sentences. In addition, a ranking method 
was proposed to finally extract the collocations 
from the aligned word pairs. It scores collocation 
candidates by using alignment probabilities mul-
tiplied by a factor derived from the exponential 
function on the frequencies. Those with higher 
scores are selected as collocations. Both Chinese 
and English collocation extraction experiments 
indicate that our method outperforms previous 
approaches in terms of both precision and recall. 
For example, according to the human evaluations 
on the Chinese corpus, our method achieved a 
precision of 56.9%, which is much higher than 
that of the baseline method (29.0%). Moreover, 
we can extract collocations with longer span. 
Human evaluation on the extracted Chinese col-
locations shows that 69% of the long-span (>6) 
collocations are correct. 
References 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2): 263-311. 
Yaacov Choueka, S.T. Klein, and E. Neuwitz. 1983. 
Automatic Retrieval of Frequent Idiomatic and 
Collocational Expressions in a Large Corpus. 
Journal for Literary and Linguistic computing, 
4(1):34-38. 
Kenneth Church and Patrick Hanks. 1990. Word As-
sociation Norms, Mutual Information, and Lexi-
cography. Computational Linguistics, 16(1):22-29. 
Ted Dunning. 1993. Accurate Methods for the Statis-
tics of Surprise and Coincidence. Computational 
Linguistics, 19(1): 61-74. 
Stefan Evert. 2004. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis, 
University of Stuttgart. 
Dekang Lin. 1998. Extracting Collocations from Text 
Corpora. In Proceedings of the 1st Workshop on 
Computational Terminology, pp. 57-63. 
Christopher D. Manning and Hinrich Sch?tze. 1999. 
Foundations of Statistical Natural Language Proc-
essing, Cambridge, MA; London, U.K.: Bradford 
Book & MIT Press. 
Kathleen R. McKeown and Dragomir R. Radev. 2000. 
Collocations. In Robert Dale, Hermann Moisl, and 
Harold Somers (Ed.), A Handbook of Natural Lan-
guage Processing, pp. 507-523. 
Darren Pearce. 2001. Synonymy in Collocation Ex-
traction. In Proceedings of NAACL-2001 Workshop 
on Wordnet and Other Lexical Resources: Applica-
tions, Extensions and Customizations, pp. 41-46. 
Darren Pearce. 2002. A Comparative Evaluation of 
Collocation Extraction Techniques. In Proceedings 
of the 3rd International Conference on Language 
Resources and Evaluation, pp. 651-658. 
Violeta Seretan and Eric Wehrli. 2006. Accurate Col-
location Extraction Using a Multilingual Parser. In 
Proceedings of the 21st International Conference 
on Computational Linguistics and 44th Annual 
Meeting of the Association for Computational Lin-
guistics (COLING/ACL-2006), pp. 953-960 
Frank Smadja. 1993. Retrieving Collocations from 
Text: Xtract. Computational Linguistics, 19(1): 
143-177. 
Joachim Wermter and Udo Hahn. 2004. Collocation 
Extraction Based on Modifiability Statistics. In 
Proceedings of the 20th International Conference 
on Computational Linguistics (COLING-2004), pp. 
980-986. 
495
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 462 ? 473, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Improving Statistical Word Alignment with Ensemble 
Methods 
Hua Wu and Haifeng Wang 
Toshiba (China) Research and Development Center, 5/F., Tower W2, Oriental Plaza, 
No.1, East Chang An Ave., Dong Cheng District, Beijing, 100738, China 
{wuhua, wanghaifeng}@rdc.toshiba.com.cn 
Abstract. This paper proposes an approach to improve statistical word align-
ment with ensemble methods. Two ensemble methods are investigated: bagging 
and cross-validation committees. On these two methods, both weighted voting 
and unweighted voting are compared under the word alignment task. In addi-
tion, we analyze the effect of different sizes of training sets on the bagging 
method. Experimental results indicate that both bagging and cross-validation 
committees improve the word alignment results regardless of weighted voting 
or unweighted voting. Weighted voting performs consistently better than un-
weighted voting on different sizes of training sets. 
1   Introduction 
Bilingual word alignment is first introduced as an intermediate result in statistical 
machine translation (SMT) [3]. Besides being used in SMT, it is also used in transla-
tion lexicon building [9], transfer rule learning [10], example-based machine transla-
tion [14], etc. In previous alignment methods, some researchers employed statistical 
word alignment models to build alignment links [3], [4], [8], [11], [16]. Some re-
searchers used similarity and association measures to build alignment links [1], [15].  
One issue about word alignment is how to improve the performance of a word 
aligner when the training data are fixed. One possible solution is to use ensemble 
methods [5], [6]. The ensemble methods were proposed to improve the performance 
of classifiers. An ensemble of classifiers is a set of classifiers whose individual deci-
sions are combined in some way (weighted or unweighted voting) to classify new 
examples. Many methods for constructing ensembles have been developed [5]. One 
kind of methods is to resample the training examples. These methods include bagging 
[2], cross-validation committees [12] and boosting [7]. The two former methods gen-
erate the classifiers in parallel while boosting generates the classifiers sequentially. In 
addition, boosting changes the weights of the training instance that is provided as 
input to each inducer based on the previously built classifiers. 
In this paper, we propose an approach to improve word alignment with ensemble 
methods. Although word alignment is not a classification problem, we can still build 
different word aligners by resampling the training data. If these aligners perform  
accurately and diversely on the corpus [6], they can be employed to improve the word 
alignment results. Here, we investigate two ensemble methods: bagging and  
 Improving Statistical Word Alignment with Ensemble Methods 463 
cross-validation committees. For both of the ensemble methods, we employ weighted 
and unweighted voting to build different ensembles. Experimental results indicate that 
both bagging and cross-validation committees improve the word alignment results. 
The weighted ensembles perform much better than the unweighted ensembles accord-
ing to our word alignment results. In addition, we analyze the effect of different sizes 
of training data on the bagging algorithm. Experimental results also show that the 
weighted bagging ensembles perform consistently better than the unweighted bagging 
ensembles on different sizes of training sets. 
The remainder of the paper is organized as follows. Section 2 describes statistical 
word alignment. Section 3 describes the bagging algorithm. Section 4 describes the 
cross-validation committees. Section 5 describes how to calculate the weights used 
for voting. Section 6 presents the evaluation results. Section 7 discusses why the en-
semble methods used in this paper are effective for the word alignment task. The last 
section concludes this paper and presents the future work. 
2   Statistical Word Alignment 
In this paper, we use the IBM model 4 as our statistical word alignment model [3]. 
This model only allows word to word and multi-word to word alignments. Thus, some 
multi-word units cannot be correctly aligned. In order to tackle this problem, we per-
form word alignment in two directions (source to target and target to source) as de-
scribed in [11]. In this paper, we call these two aligners bi-directional aligners.1 Thus, 
for each sentence pair, we can get two alignment results. We use 1S  and 2S  to repre-
sent the bi-directional alignment sets. For alignment links in both sets, we use i for 
source words and j for target words. 
}}0  ,|{|),{(1 ?=== jjjj aaiiAjAS  (1) 
}}0  ,|{|),{(2 ?=== jjii aiajAAiS  (2) 
Where, aj represents the index position of the source word aligned to the target word 
in position j. For example, if a target word in position j is connected to a source word 
in position i, then aj=i. If a target word in position j is connected to source words in 
positions i1 and i2, then Aj={i1,i2}. We name an element in the alignment set an 
alignment link.2 
3   Bagging 
The bagging algorithm (derived from bootstrap aggregating) votes classifiers gener-
ated by different bootstrap replicates [2]. A bootstrap replicate is generated by uni-
formly sampling m instances from the training set with replacement. In general, T  
                                                          
1
  The GIZA++ toolkit is used to perform statistical alignment. It is located at 
http://www.fjoch.com/GIZA++.html. 
2
  Our definition of alignment link is different from that in [11]. In [11], alignment links are 
classified into possible links and sure links. In our paper, both one-to-one and non one-to-one 
links are taken as sure links.   
464 H. Wu and H. Wang 
bootstrap replicates are built in the sampling process. And T  different classifiers are 
built based on the bootstrap replicates. A final classifier is built from these T  sub-
classifiers using weighted voting or unweighted voting. The original unweighted 
bagging algorithm is shown in Figure 1. 
Input:  a training set }} ..., ,1{),,{( mixyS ii ?=  
an induction algorithm ?  
(1) For Tj    to1=  { 
(2) jS = bootstrap replicate of S  by sampling m  items from S  with replace-
ment 
(3) )( jj SC ?=  
(4) } 
(5) Create a final classifier with majority voting: 
?
?
=
j
j
Yy
yxCxC )),((maxarg)(* ?  
Where, 1),( =yx? if yx = ; else 0),( =yx? . 
Output: Classifier *C  
Fig. 1. The Unweighted Bagging Algorithm 
3.1   Bagging the Statistical Word Aligner 
In this section, we apply the technique of bagging to word alignment, the detailed 
algorithm is shown in Figure 2. In the algorithm, we first resample the training data 
to train the word aligners. We choose to resample the training set in the same 
way as the original bagging algorithm. With these different bootstrap repli-
cates, we build the different word aligners. As described in Section 2, we per-
form word alignment in two directions to improve multiword alignment. Thus, on 
each bootstrap replicate, we train a word aligner in the source to target direction and 
another word aligner in the target to source direction, which is described in b) of 
step (1). 
After building the different word aligners, we combine or aggregate the align-
ments generated by the individual alignment models to create the final alignments for 
each sentence pair. In this paper, the final alignment link for each word is chosen by 
performing a majority voting on the alignments provided by each instance of the 
model. The majority voting can be weighted or unweighted. For weighted voting, the 
weights of word alignment links produced by the bi-directional word aligners are 
trained from the training data, which will be further described in section 5. For un-
weighted voting, the best alignment link for a specific word or unit is voted by more 
than half of the word aligners in the ensemble. For those words that have no majority 
choice, the system simply does not align them. 
 Improving Statistical Word Alignment with Ensemble Methods 465 
Input: a training set }}...1{),,{( mixyS ii ?=   
a word alignment model M  
(1) For Tj    to1=   
a) jS = bootstrap replicate of S  by sampling m  items from S  with re-
placement 
b) Train the bi-directional alignment models stjM  and tsjM  with the 
bootstrap replicate jS  
(2) For Nk    to1=  (N is the number of sentence pairs) 
For each word s : 
a) For weighted voting 
    ? +=
j
ts
j
st
jj
t
tksMtksMtsWksM ))),,(()),,(((*),(maxarg),(* ??  
 t is the word or phrase in the target sentence; 
    ),( tsW j is the weight for the alignment link )  ,( ts  produced by the 
aligner stjM  or tsjM ; 
1),( =yx? if yx = ; else 0),( =yx? . 
b) For unweighted voting 
?
=>
+=
T
j
ts
j
st
j
T
tnt
tksMtksMksM
1
2
)(:
* ))),,(()),,(((maxarg),( ??  
where, n(t)= ?
=
+
T
j
ts
j
st
j tksMtksM
1
))),,(()),,((( ??  
Output: The final word alignment results 
Fig. 2. The Bagging Algorithm for Word Alignment 
4   Cross-Validation Committee 
The difference between bagging and cross-validation committees lies in the way to 
resample the training set. The cross-validation committees construct the training sets 
by leaving out disjoint subsets of the training data. For example, the training set can 
be randomly and evenly divided into N disjoint subsets. Then N overlapping training 
sets can be constructed by dropping out a different one of these N subsets. This pro-
cedure is the same as the one to construct training sets for N-fold cross-validation. 
Thus, ensembles constructed in this way are called cross-validation committees. 
For word alignment, we also divide the training set into N even parts and build N 
overlapping training sets. With the N sets, we build N alignment models as described 
466 H. Wu and H. Wang 
above. Since the training sets are different, the word alignment results may be differ-
ent for individual words. Using the same majority voting as described in Figure 2, we 
get the final word alignment results. 
5   Weight Calculation 
In this paper, we compare both weighted voting and unweighted voting under our 
word alignment task. The algorithm in Figure 2 shows that the weights are related 
with the specific word alignment links and the specific word aligner. We calculate the 
weights based on the word alignment results on the training data. 
As described in Section 3.1, on each bootstrap replicate j, we train a word aligner 
st
jM  in the source to target direction and a word aligner tsjM  in the target to source 
direction. That is to say, we obtain two different word alignment sets  stjS  and tsjS  for 
each of the bootstrap replicate. For each word alignment link )  ,( ts  produced by stjM  
or tsjM ,  we calculate its weight as shown in (3). This weight measures the association 
of the source part and the target part in an alignment link.  This measure is like the 
Dice Coefficient. Smadja et al [13] showed that the Dice Coefficient is a good indica-
tor of translation association. 
?? +=
''
),'()',(
),(*2),(
st
i
tscounttscount
tscount
tsW  (3) 
Where, ),( tscount  is the occurring frequency of the alignment link tsjstj SSts ??)  ,( .  
6   Experiments 
6.1   Training and Testing Set 
We perform experiments on a sentence aligned English-Chinese bilingual corpus in 
general domain. There are about 320,000 bilingual sentence pairs in the corpus, from 
which, we randomly select 1,000 sentence pairs as testing data. The remainder is used 
as training data. In the sentence pairs, the average length of the English sentences is 
13.6 words while the average length of the Chinese sentences is 14.2 words.  
The Chinese sentences in both the training set and the testing set are automatically 
segmented into words. The segmentation errors in the testing set are post-corrected. 
The testing set is manually annotated. It has totally 8,651 alignment links. Among 
them, 866 alignment links include multiword units, which accounts for about 10% of 
the total links.  
6.2   Evaluation Metrics 
We use the same evaluation metrics as in [17]. If we use GS  to represent the set of 
alignment links identified by the proposed methods and RS  to denote the reference 
 Improving Statistical Word Alignment with Ensemble Methods 467 
alignment set, the methods to calculate the precision, recall, f-measure, and alignment 
error rate (AER) are shown in Equation (4), (5), (6), and (7). In addition, t-test is used 
for testing statistical significance. From the evaluation metrics, it can be seen that the 
higher the f-measure is, the lower the alignment error rate is. Thus, we will only show 
precision, recall and AER scores in the experimental results. 
|S|
|SS|
G
RG ?
=precision      
  (4) 
|S|
 |SS|
R
RG ?
=recall   
       (5) 
||||
||*2
RG
RG
SS
SSfmeasure
+
?
=  (6) 
fmeasure
SS
SS
AER
RG
RG
?=
+
?
?= 1||||
||*21  (7) 
6.3   Evaluation Results for Bagging 
For the bagging method, we use ten word aligners trained on five different bootstrap 
replicates. Among them, five aligners are trained in the source to target direction. The 
other five aligners are trained in the target to source direction. The bagging method 
will be compared with a baseline method using the entire training data. For this base-
line method, we also train bi-directional models. Based on the alignment results on 
the entire training data, we calculate the alignment weights for the two word aligners 
as described in Section 5.  
The results using weighted voting are shown in Table 1. The number in brackets of 
the first column describes the number of word aligners used in the ensembles. For 
example, in the ensemble ?bagging (4)?, two word aligners are trained in the source to 
target direction and the other two are trained in the target to source direction. 
From the results, it can be seen that the bagging methods obtain significantly better 
results than the baseline. The best ensemble achieves an error rate reduction of 7.34% 
as compared with the baseline. The results show that increasing the number of word 
aligner does not greatly reduce the word alignment error rate. The reduction is even 
smaller when the number increases from 8 to 10.  
Table 1. Weighted Bagging Results 
Method Precision Recall AER 
Bagging (4) 0.8035 0.7898 0.2034 
Bagging (6) 0.8048 0.7922 0.2015 
Bagging (8) 0.8061 0.7948 0.1996 
Bagging (10) 0.8064 0.7948 0.1994 
Baseline  0.7870 0.7826 0.2152 
468 H. Wu and H. Wang 
In order to further analyze the effect of the weights on the word alignment results, 
we also use unweighted voting in the ensembles. The results are shown in Table 2. 
The baseline method also trains bi-directional aligners with the entire training data. 
The final word alignment results are obtained by taking an unweighted voting on the 
two alignment results produced by the bi-directional aligners. That is the same as that 
by taking the intersection of the two word alignment results. 
Table 2. Unweighted Bagging Results 
Method Precision Recall AER 
Bagging (4) 0.9230 0.6073 0.2674 
Bagging (6) 0.9181 0.6200 0.2598 
Bagging (8) 0.9167 0.6307 0.2527 
Bagging (10) 0.9132 0.6347 0.2511 
Baseline  0.9294 0.5756 0.2810 
Increasing the number of word aligners in the ensembles, the unweighted bagging 
method does not greatly reduce AER. However, the ensembles obtain much lower 
error rate as compared with the baseline. The best ensemble achieves a relative error 
rate reduction of 10.64%, indicating a significant improvement. From the experimen-
tal results, we find that there are no multiword alignment links selected in the ensem-
bles. This is because unweighted voting in this paper requires more than half of the 
word aligners in the ensembles to vote for the same link. Thus, there should be bi-
directional word aligners voting for the target algnment link. The intersection of bi-
directional word alignment results produced by the IBM models only creates single 
word alignments. It can also be seen from the Equations (1) and (2) in Section 2.  
Comparing the results obtained using weighted voting in Table 1 and those ob-
tained using unweighted voting in Table 2, we find that (1) the weighted bagging 
methods are much better than the unweighted bagging methods; (2) the ensembles 
using unweighted voting obtain higher precision but lower recall than those using 
weighted voting. For example, the weighted voting ?bagging (10)? achieves a relative 
error rate reduction of 20.59% as compared with the corresponding unweighted vot-
ing. This indicates that the method used to calculate voting weights described in sec-
tion 5 is very effective.  
6.4   Evaluation Results for Cross-Validation Committees 
For the cross-validation committees, we divide the entire training data into five dis-
joint subsets. For each bootstrap replicate, we leave one out. Thus, each replicate 
includes 80% sentence pairs of the full training data. For each replicate, we train bi-
directional word alignment models. Thus, we totally obtain ten individual word align-
ers. The baseline is the same as shown in Table 1. The results obtained using 
weighted voting are shown in Table 3.  The number in the brackets of the first column 
describes the number of word aligners used in the ensembles. 
 Improving Statistical Word Alignment with Ensemble Methods 469 
Table 3. Evaluation Results for Weighted Cross-Validation Committees 
Method Precision Recall AER 
Validation (4) 0.8059 0.7913 0.2015 
Validation (6) 0.8070 0.7928 0.2002 
Validation (8) 0.8063 0.7933 0.2002 
Validation (10) 0.8068 0.7947 0.1993 
Baseline  0.7870 0.7826 0.2152 
From the results, it can be seen that the cross-validation committees perform better 
than the baseline. The best ensemble ?validation (10)? achieves an error rate reduction 
of 7.39% as compared with the baseline, indicating a significant improvement. The 
results also show that increasing the number of word aligner does not greatly reduce 
the word alignment error rate.  
As described in section 6.3, we also use unweighted voting for the cross-validation 
committees. The results are shown in Table 4. The baseline is the same as described 
in Table 2.  
Table 4. Evaluation Results for Unweighted Cross-Validation Committees 
Method Precision Recall AER 
Validation (4) 0.9199 0.5943 0.2779 
Validation (6) 0.9174 0.6124 0.2655 
Validation (8) 0.9154 0.6196 0.2610 
Validation (10) 0.9127 0.6245 0.2584 
Baseline  0.9294 0.5756 0.2810 
From the results, it can be seen that increasing the number of word aligners in the 
ensembles, the alignment error rate is reduced. The best ensemble achieves a relative 
error rate reduction of 8.04% as compared with the baseline, indicating a significant 
improvement. Comparing the results in Table 3 and Table 4, we find that the 
weighted methods are also much better than the unweighted ones. For example, the 
weighted method ?Validation (10)? achieves an error rate reduction of 22.87% as 
compared with the corresponding unweighted method. 
6.5   Bagging vs. Cross-Validation Committees 
According to the evaluation results, bagging and cross-validation committees achieve 
comparable results. In order to further compare bagging and cross-validation commit-
tees, we classify the alignment links in the weighted ensembles into two classes: sin-
gle word alignment links (SWA) and multiword alignment links (MWA). SWA links 
only include one-to-one alignments. MWA links refer to those including multiword 
units in the source language or/and in the target language. The SWA and MWA for 
the bagging ensembles are shown in Table 5 and Table 6. The SWA and MWA for 
the cross-validation committees are shown in Table 7 and Table 8. The AERs of the 
baselines for SWA and MWA are 0.1531 and 0.8469, respectively. 
470 H. Wu and H. Wang 
Table 5. Single Word Alignment Results for the Weighted Bagging Methods 
Method Precision Recall AER 
Bagging (4) 0.8263 0.8829 0.1463 
Bagging (6) 0.8270 0.8845 0.1452 
Bagging (8) 0.8270 0.8877 0.1437 
Bagging (10) 0.8265 0.8876 0.1440 
Table 6. Multiword Alignment Results for the Weighted Bagging Methods 
Method Precision Recall AER 
Bagging (4) 0.4278 0.1815 0.7451 
Bagging (6) 0.4432 0.1896 0.7344 
Bagging (8) 0.4540 0.1884 0.7336 
Bagging (10) 0.4620 0.1896 0.7311 
Table 7. Single Word Alignment Results for Weighted Cross-Validation Committees 
Method Precision Recall AER 
Validation (4) 0.8282 0.8833 0.1452 
Validation (6) 0.8285 0.8847 0.1443 
Validation (8) 0.8275 0.8851 0.1447 
Validation (10) 0.8277 0.8867 0.1438 
Table 8. Multiword Alignment Results for Weighted Cross-Validation Committees 
Method Precision Recall AER 
Validation (4) 0.4447 0.1908 0.7330 
Validation (6) 0.4538 0.1931 0.7291 
Validation (8) 0.4578 0.1942 0.7273 
Validation (10) 0.4603 0.1942 0.7268 
From the results, it can be seen that the single word alignment results are much bet-
ter than the multiword alignment results for both of the two methods. This indicates 
that it is more difficult to align the multiword units than to align single words. 
Comparing the bagging methods and validation committees, we find that these two 
methods obtain comparable results on both the single word alignment links and mul-
tiword alignment links. This indicates that the different resampling methods in these 
two ensemble methods do not much affect the results on our word alignment task. 
6.6   Different Sizes of Training Data 
In this section, we investigate the effect of the size of training data on the ensemble 
methods. Since the difference between bagging and cross-validation committees is 
very small, we only investigate the effect on the bagging ensembles.  
 Improving Statistical Word Alignment with Ensemble Methods 471 
We randomly select training data from the original training set described in Section 
6.1 to construct different training sets. We construct three training sets, which include 
1/4, 1/2 and 3/4 of sentence pairs of the original training set, respectively.    
For each of the training set, we obtain five bootstrap replicates and train ten word 
aligners. The results of ensembles consisting of ten word aligners are shown in Table 
9 and Table 10. Table 9 and Table 10 show the weighted and unweighted bagging 
results, respectively. The methods to construct the baselines for different training sets 
in Table 9 and Table 10 are the same as those in Table 1 and Table 2, respectively. 
For convenience, we also list the results using the original training set in the tables. 
The first column describes the size of the training sets used for the ensembles. The 
last column presents the relative error rate reduction (RERR) of the ensembles as 
compared with the corresponding baselines. From the results, it can be seen that both 
weighted and unweighted bagging ensembles are effective to improve word alignment 
results. The weighted ensembles perform consistently better than the unweigted en-
sembles on different sizes of training sets. 
Table 9. Weighted Bagging Results on Different Sizes of Training Sets 
Data Precision Recall AER Baseline (AER) RERR 
1/4 0.7684 0.7517 0.2316 0.2464 6.00% 
1/2 0.7977 0.7775 0.2125 0.2293 7.33% 
3/4 0.8023 0.7869 0.2055 0.2184 5.89% 
All 0.8064 0.7948 0.1994 0.2152 7.34% 
Table 10. Unweighted Bagging Results on Different Sizes of Training Sets 
Data Precision Recall AER Baseline (AER) RERR 
1/4 0.8960 0.6033 0.2789 0.3310 15.72% 
1/2 0.9077 0.6158 0.2662 0.3050 12.72% 
3/4 0.9140 0.6270 0.2562 0.2943 12.95% 
All 0.9132 0.6347 0.2511 0.2810 10.64% 
7   Discussion 
Both bagging and cross-validation committees utilize multiple classifiers to make 
different assumptions about the learning system. Bagging requires that the learning 
system should not be stable, so that small changes to the training set would lead to 
different classifiers. Breiman [2] also noted that poor predicators could be trans-
formed into worse ones by bagging.  
In this paper, the learning system is the word alignment model described in Section 
2. The classifiers refer to the different word aligners trained on different bootstrap 
replicates. In our experiments, although word alignment models do not belong to 
unstable learning systems, bagging obtains better results on all of the datasets. This is 
472 H. Wu and H. Wang 
because the training data is insufficient or subject to data sparseness problem. Thus, 
changing the training data or resampling the training data causes the alternation of the 
trained parameters of the alignment model. The word aligners trained on a different 
bootstrap replicate produce different word alignment links for individual words. Us-
ing majority voting, the ensembles can improve the alignment precision and recall, 
resulting in lower alignment error rates. 
The experiments also show that weighted voting is better than unweighted voting. 
The advantage of weighted voting is that it can select the good word alignment link 
even if only one aligner votes for it in the ensembles. This is because the selected 
alignment link gets much higher weight than the other links. 
8   Conclusion and Future Work 
Two ensemble methods are employed in this paper to improve word alignment re-
sults: bagging and cross-validation committees. Both of these two methods obtain 
better results than the original word aligner without increasing any training data. In 
this paper, we use two different voting methods: weighted voting and unweighted 
voting. Experimental results show that the weighted bagging method and weighted 
cross-validation committees achieve an error rate reduction of 7.34% and 7.39% re-
spectively, as compared with the original word aligner. Results also show that 
weighted voting is much better than unweighted voting on the word alignment task. 
Unweighted voting obtains higher precision but lower recall than weighted voting. In 
addition, the weighted voting used in this paper obtains multiword alignment links 
while the unweighted voting cannot. 
We also compare the two ensemble methods on the same training data and testing 
data. Bagging and cross-validation committees obtain comparable results on both 
single word alignment links and multiword alignment links. This indicates that the 
different resampling methods in these two ensemble methods do not much affect the 
results under our word alignment task. 
We also investigate the bagging method on different sizes of training sets. The re-
sults show that both weighted voting and unweighted voting are effective to improve 
word alignment results. Weighted voting performs consistently better than unweigted 
voting on different sizes of training sets.  
In future work, we will investigate more ensemble methods on the word alignment 
task such as the boosting algorithm. In addition, we will do more research on the 
weighting schemes in voting. 
References 
1. Ahrenberg, L., Merkel, M., Andersson, M.: A Simple Hybrid Aligner for Generating Lexi-
cal Correspondences in Parallel Texts. In Proc. of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and the 17th Int. Conf. on Computational Linguistics 
(ACL/COLING-1998), 29-35 
2. Breiman, L.: Bagging Predicators. Machine Learning (1996), 24(1): 123-140 
3. Brown, P. F., Pietra, S. D., Pietra, V. D., Mercer, R.: The Mathematics of Statistical Ma-
chine Translation: Parameter Estimation. Computational Linguistics (1993), 19(2): 263-311 
 Improving Statistical Word Alignment with Ensemble Methods 473 
4. Cherry, C., Lin, D.: A Probability Model to Improve Word Alignment. In Proc. of the 41st 
Annual Meeting of the Association for Computational Linguistics (ACL-2003), pp. 88-95 
5. Dietterich, T.: Machine Learning Research: Four Current Directions. AI Magazine (1997), 
18 (4): 97-136 
6. Dietterich, T.: Ensemble Methods in Machine Learning. In Proc. of the First Int. Work-
shop on Multiple Classifier Systems (2000), 1-15 
7. Freund, Y., Schapire, R.: Experiments with a new boosting algorithm. In Machine Learn-
ing: Proc. of the Thirteenth International Conference (1996), 148-156 
8. Matusov, E., Zens, R., Ney H.: Symmetric Word Alignments for Statistical Machine 
Translation. In Proc. of the 20th Int. Conf. on Computational Linguistics (COLING-2004), 
219-225  
9. Melamed, I. D.: Automatic Construction of Clean Broad-Coverage Translation Lexicons. 
In Proc. of the 2nd Conf. of the Association for Machine Translation in the Americas 
(AMTA-1996), 125-134 
10. Menezes, A., Richardson, S.D.: A Best-first Alignment Algorithm for Automatic Extrac-
tion of Transfer Mappings from Bilingual Corpora. In Proc. of the ACL 2001 Workshop 
on Data-Driven Methods in Machine Translation (2001), 39-46 
11. Och, F. J., Ney, H.: Improved Statistical Alignment Models. In Proc. of the 38th Annual 
Meeting of the Association for Computational Linguistics (ACL-2000), 440-447 
12. Parmanto, B., Munro, P., Doyle, H.: Improving Committee Diagnosis with Resampling 
Techniques. In Touretzky, D., Mozer, M., Hasselmo, M. (Ed..): Advances in Neural In-
formation Processing Systems (1996), Vol. 8,  882-888 
13. Smadja, F. A., McKeown, K. R., Hatzivassiloglou, V.: Translating Collocations 
for Bilingual Lexicons: a Statistical Approach. Computational Linguistics (1996), 
22 (1):1-38 
14. Somers, H.: Review Article: Example-Based Machine Translation. Machine Translation 
(1999), 14: 113-157 
15. Tufis, D., Barbu, M.: Lexical Token Alignment: Experiments, Results and Application. In 
Proc. of the 3rd Int. Conf. on Language Resources and Evaluation (LREC-2002), 458-465 
16. Wu, D.: Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel 
Corpora. Computational Linguistics (1997), 23(3): 377-403 
17. Wu, H., Wang, H.: Improving Domain-Specific Word Alignment with a General Bilingual 
Corpus. In Frederking R., Taylor, K. (Eds.): Machine Translation: From Real Users to Re-
search: 6th Conf. of the Association for Machine Translation in the Americas (AMTA-
2004), 262-271 
Improving Domain-Specific Word Alignment for Computer Assisted 
Translation 
WU Hua, WANG Haifeng 
Toshiba (China) Research and Development Center 
5/F., Tower W2, Oriental Plaza 
No.1, East Chang An Ave., Dong Cheng District 
Beijing, China, 100738 
{wuhua, wanghaifeng}@rdc.toshiba.com.cn 
 
Abstract 
This paper proposes an approach to improve 
word alignment in a specific domain, in which 
only a small-scale domain-specific corpus is 
available, by adapting the word alignment 
information in the general domain to the 
specific domain. This approach first trains two 
statistical word alignment models with the 
large-scale corpus in the general domain and the 
small-scale corpus in the specific domain 
respectively, and then improves the 
domain-specific word alignment with these two 
models. Experimental results show a significant 
improvement in terms of both alignment 
precision and recall. And the alignment results 
are applied in a computer assisted translation 
system to improve human translation efficiency. 
1 Introduction 
Bilingual word alignment is first introduced as an 
intermediate result in statistical machine translation 
(SMT) (Brown et al, 1993). In previous alignment 
methods, some researchers modeled the alignments 
with different statistical models (Wu, 1997; Och and 
Ney, 2000; Cherry and Lin, 2003). Some researchers 
use similarity and association measures to build 
alignment links (Ahrenberg et al, 1998; Tufis and 
Barbu, 2002). However, All of these methods 
require a large-scale bilingual corpus for training. 
When the large-scale bilingual corpus is not 
available, some researchers use existing dictionaries 
to improve word alignment (Ker and Chang, 1997).  
However, few works address the problem of 
domain-specific word alignment when neither the 
large-scale domain-specific bilingual corpus nor the 
domain-specific translation dictionary is available. 
This paper addresses the problem of word 
alignment in a specific domain, where only a small 
domain-specific corpus is available. In the 
domain-specific corpus, there are two kinds of 
words. Some are general words, which are also 
frequently used in the general domain. Others are 
domain-specific words, which only occur in the 
specific domain. In general, it is not quite hard to 
obtain a large-scale general bilingual corpus while 
the available domain-specific bilingual corpus is 
usually quite small. Thus, we use the bilingual 
corpus in the general domain to improve word 
alignments for general words and the corpus in the 
specific domain for domain-specific words. In other 
words, we will adapt the word alignment 
information in the general domain to the specific 
domain. 
In this paper, we perform word alignment 
adaptation from the general domain to a specific 
domain (in this study, a user manual for a medical 
system) with four steps. (1) We train a word 
alignment model using the large-scale bilingual 
corpus in the general domain; (2) We train another 
word alignment model using the small-scale 
bilingual corpus in the specific domain; (3) We build 
two translation dictionaries according to the 
alignment results in (1) and (2) respectively; (4) For 
each sentence pair in the specific domain, we use the 
two models to get different word alignment results 
and improve the results according to the translation 
dictionaries. Experimental results show that our 
method improves domain-specific word alignment in 
terms of both precision and recall, achieving a 
21.96% relative error rate reduction. 
The acquired alignment results are used in a 
generalized translation memory system (GTMS, a 
kind of computer assisted translation systems) 
(Simard and Langlais, 2001). This kind of system 
facilitates the re-use of existing translation pairs to 
translate documents. When translating a new 
sentence, the system tries to provide the 
pre-translated examples matched with the input and 
recommends a translation to the human translator, 
and then the translator edits the suggestion to get a 
final translation. The conventional TMS can only 
recommend translation examples on the sentential 
level while GTMS can work on both sentential and 
sub-sentential levels by using word alignment results. 
These GTMS are usually employed to translate 
various documents such as user manuals, computer 
operation guides, and mechanical operation manuals. 
2 
2.1 
Word Alignment Adaptation 
Bi-directional Word Alignment 
In statistical translation models (Brown et al, 1993), 
only one-to-one and more-to-one word alignment 
links can be found. Thus, some multi-word units 
cannot be correctly aligned. In order to deal with this 
problem, we perform translation in two directions 
(English to Chinese, and Chinese to English) as 
described in (Och and Ney, 2000). The GIZA++ 
toolkit 1  is used to perform statistical word 
alignment.  
For the general domain, we use  and  
to represent the alignment sets obtained with English 
as the source language and Chinese as the target 
language or vice versa. For alignment links in both 
sets, we use i for English words and j for Chinese 
words. 
1SG 2SG
}0 },{|),{(1 ?== jjjj aaAjASG  
}0  },{|),{(2 ?== iiii aaAAiSG  
Where, is the position of the source 
word aligned to the target word in position k. The set 
 indicates the words aligned to the same 
source word k. For example, if a Chinese word in 
position j is connect to an English word in position i, 
then . And if a Chinese word in position j is 
connect to English words in position i and k, then 
. 
),( jikak =
),( jikAk =
ia j =
},{ kiA j =
Based on the above two alignment sets, we 
obtain their intersection set, union set 2  and 
subtraction set.  
Intersection:  21 SGSGSG ?=
Union:  21 SGSGPG ?=
Subtraction:  SGMG ?= PG
For the specific domain, we use  and  
to represent the word alignment sets in the two 
directions. The symbols , 
1SF 2SF
SF PF  and MF  
represents the intersection set, union set and the 
subtraction set, respectively. 
2.2 
                                                       
 Translation Dictionary Acquisition 
When we train the statistical word alignment model 
with a large-scale bilingual corpus in the general 
domain, we can get two word alignment results for 
the training data. By taking the intersection of the 
two word alignment results, we build a new 
alignment set. The alignment links in this 
intersection set are extended by iteratively adding 
word alignment links into it as described in (Och and 
Ney, 2000).  
 
1 It is located at http://www.isi.edu/~och/GIZA++.html  
2  In this paper, the union operation does not remove the 
replicated elements. For example, if set one includes two 
elements {1, 2} and set two includes two elements {1, 3}, then 
the union of these two sets becomes {1, 1, 2, 3}. 
Based on the extended alignment links, we build 
an English to Chinese translation dictionary  
with translation probabilities. In order to filter some 
noise caused by the error alignment links, we only 
retain those translation pairs whose translation 
probabilities are above a threshold 
1D
1?  or 
co-occurring frequencies are above a threshold 2? . 
When we train the IBM statistical word 
alignment model with a limited bilingual corpus in 
the specific domain, we build another translation 
dictionary  with the same method as for the 
dictionary . But we adopt a different filtering 
strategy for the translation dictionary . We use 
log-likelihood ratio to estimate the association 
strength of each translation pair because Dunning 
(1993) proved that log-likelihood ratio performed 
very well on small-scale data. Thus, we get the 
translation dictionary  by keeping those entries 
whose log-likelihood ratio scores are greater than a 
threshold 
2D
1D
3
2D
2D
? .  
2.3 Word Alignment Adaptation Algorithm 
Based on the bi-directional word alignment, we 
define  as SI SFSGSI ?= and as UG
SIPFPGUG ??= . The word alignment links in 
the set SI  are very reliable. Thus, we directly 
accept them as correct links and add them into the 
final alignment set . WA
Input: Alignment set and  SI UG
(1) For alignment links in , we directly add 
them into the final alignment set . 
SI
WA
(2) For each English word i in the , we first 
find its different alignment links, and then do 
the following: 
UG
a) If there are alignment links found in 
dictionary , add the link with the largest 
probability to . 
1D
WA
b) Otherwise, if there are alignment links found 
in dictionary , add the link with the 
largest log-likelihood ratio score to . 
2D
WA
c) If both a) and b) fail, but three links select the 
same target words for the English word i, we 
add this link into . WA
d) Otherwise, if there are two different links for 
this word: one target is a single word, and 
the other target is a multi-word unit and the 
words in the multi-word unit have no link in 
, add this multi-word alignment link to 
. 
WA
WA
Output: Updated alignment set  WA
Figure 1. Word Alignment Adaptation Algorithm
For each source word in the set , there are 
two to four different alignment links. We first use 
translation dictionaries to select one link among 
them. We first examine the dictionary  and then 
 to see whether there is at least an alignment link 
of this word included in these two dictionaries.  If it 
is successful, we add the link with the largest 
probability or the largest log-likelihood ratio score to 
the final set . Otherwise, we use two heuristic 
rules to select word alignment links. The detailed 
algorithm is described in Figure 1. 
UG
1D
2D
WA
 
Figure 2. Alignment Example 
Figure 2 shows an alignment result obtained with 
the word alignment adaptation algorithm. For 
example, for the English word ?x-ray?, we have two 
different links in UG . One is (x-ray, X) and the 
other is (x-ray, X ??). And the single Chinese 
words ??? and ??? have no alignment links in the 
set . According to the rule d), we select the link 
(x-ray, X??). 
WA
3 Evaluation 
3.1 
3.2 
We compare our method with three other methods. 
The first method ?Gen+Spec? directly combines the 
corpus in the general domain and in the specific 
domain as training data. The second method ?Gen? 
only uses the corpus in the general domain as 
training data. The third method ?Spec? only uses the 
domain-specific corpus as training data. With these 
training data, the three methods can get their own 
translation dictionaries. However, each of them can 
only get one translation dictionary. Thus, only one 
of the two steps a) and b) in Figure 1 can be applied 
to these methods. The difference between these three 
methods and our method is that, for each word, our 
method has four candidate alignment links while the 
other three methods only has two candidate 
alignment links. Thus, the steps c) and d) in Figure 1 
should not be applied to these three methods. 
Training and Testing Data 
We have a sentence aligned English-Chinese 
bilingual corpus in the general domain, which 
includes 320,000 bilingual sentence pairs, and a 
sentence aligned English-Chinese bilingual corpus in 
the specific domain (a medical system manual), 
which includes 546 bilingual sentence pairs. From 
this domain-specific corpus, we randomly select 180 
pairs as testing data. The remained 366 pairs are 
used as domain-specific training data. 
The Chinese sentences in both the training set 
and the testing set are automatically segmented into 
words. In order to exclude the effect of the 
segmentation errors on our alignment results, we 
correct the segmentation errors in our testing set. 
The alignments in the testing set are manually 
annotated, which includes 1,478 alignment links. 
Overall Performance 
We use evaluation metrics similar to those in (Och 
and Ney, 2000). However, we do not classify 
alignment links into sure links and possible links. 
We consider each alignment as a sure link. If we use 
 to represent the alignments identified by the 
proposed methods and  to denote the reference 
alignments, the methods to calculate the precision, 
recall, and f-measure are shown in Equation (1), (2) 
and (3). According to the definition of the alignment 
error rate (AER) in (Och and Ney, 2000), AER can 
be calculated with Equation (4). Thus, the higher the 
f-measure is, the lower the alignment error rate is. 
Thus, we will only give precision, recall and AER 
values in the experimental results. 
GS
CS
|S|
|SS|
G
CG ?=precision       (1) 
|S|
 |SS|
C
CG ?=recall   (2) 
||||
||*2
CG
CG
SS
SS
fmeasure +
?=  (3) 
fmeasure
SS
SS
AER
CG
CG ?=+
??= 1
||||
||*2
1  (4) 
 
Method Precision Recall AER 
Ours 0.8363 0.7673 0.1997
Gen+Spec 0.8276 0.6758 0.2559
Gen 0.8668 0.6428 0.2618
Spec 0.8178 0.4769 0.3974
Table 1. Word Alignment Adaptation Results 
We get the alignment results shown in Table 1 by 
setting the translation probability threshold to 
1.01 =? , the co-occurring frequency threshold to 
52 =?  and log-likelihood ratio score to 503 =? . 
From the results, it can be seen that our approach 
performs the best among others, achieving much 
higher recall and comparable precision. It also 
achieves a 21.96% relative error rate reduction 
compared to the method ?Gen+Spec?. This indicates 
that separately modeling the general words and 
domain-specific words can effectively improve the 
word alignment in a specific domain. 
4 Computer Assisted Translation System  
A direct application of the word alignment result to 
the GTMS is to get translations for sub-sequences in 
the input sentence using the pre-translated examples. 
For each sentence, there are many sub-sequences. 
GTMS tries to find translation examples that match 
the longest sub-sequences so as to cover as much of 
the input sentence as possible without overlapping. 
Figure 3 shows a sentence translated on the 
sub-sentential level. The three panels display the 
input sentence, the example translations and the 
translation suggestion provided by the system, 
respectively. The input sentence is segmented to 
three parts. For each part, the GTMS finds one 
example to get a translation fragment according to 
the word alignment result. By combining the three 
translation fragments, the GTMS produces a correct 
translation suggestion ??????? CT ????? 
Without the word alignment information, the 
conventional TMS cannot find translations for the 
input sentence because there are no examples closely 
matched with it. Thus, word alignment information 
can improve the translation accuracy of the GTMS, 
which in turn reduces editing time of the translators 
and improves translation efficiency. 
 
Figure 3. A Snapshot of the Translation System 
5 Conclusion 
This paper proposes an approach to improve 
domain-specific word alignment through alignment 
adaptation. Our contribution is that our approach 
improves domain-specific word alignment by 
adapting word alignment information from the 
general domain to the specific domain. Our 
approach achieves it by training two alignment 
models with a large-scale general bilingual corpus 
and a small-scale domain-specific corpus. Moreover, 
with the training data, two translation dictionaries 
are built to select or modify the word alignment 
links and further improve the alignment results. 
Experimental results indicate that our approach 
achieves a precision of 83.63% and a recall of 
76.73% for word alignment on a user manual of a 
medical system, resulting in a relative error rate 
reduction of 21.96%. Furthermore, the alignment 
results are applied to a computer assisted translation 
system to improve translation efficiency.  
Our future work includes two aspects. First, we 
will seek other adaptation methods to further 
improve the domain-specific word alignment results. 
Second, we will use the alignment adaptation results 
in other applications. 
References 
Lars Ahrenberg, Magnus Merkel and Mikael 
Andersson. 1998. A Simple Hybrid Aligner for 
Generating Lexical Correspondences in Parallel 
Tests. In Proc. of the 36th Annual Meeting of the 
Association for Computational Linguistics and the 
17th International Conference on Computational 
Linguistics, pages 29-35. 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2): 263-311. 
Colin Cherry and Dekang Lin. 2003. A Probability 
Model to Improve Word Alignment. In Proc. of 
the 41st Annual Meeting of the Association for 
Computational Linguistics, pages 88-95. 
Ted Dunning. 1993. Accurate Methods for the 
Statistics of Surprise and Coincidence. 
Computational Linguistics, 19(1): 61-74. 
Sue J. Ker, Jason S. Chang. 1997. A Class-based 
Approach to Word Alignment. Computational 
Linguistics, 23(2): 313-343. 
Franz Josef Och and Hermann Ney. 2000. Improved 
Statistical Alignment Models. In Proc. of the 38th 
Annual Meeting of the Association for 
Computational Linguistics, pages 440-447. 
Michel Simard and Philippe Langlais. 2001. 
Sub-sentential Exploitation of Translation 
Memories. In Proc. of MT Summit VIII, pages 
335-339. 
Dan Tufis and Ana Maria Barbu. 2002. Lexical 
Token Alignment: Experiments, Results and 
Application. In Proc. of the Third International 
Conference on Language Resources and 
Evaluation, pages 458-465. 
Dekai Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel 
Corpora. Computational Linguistics, 23(3): 
377-403. 
Proceedings of the 43rd Annual Meeting of the ACL, pages 467?474,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Alignment Model Adaptation for Domain-Specific Word Alignment 
WU Hua, WANG Haifeng, LIU Zhanyi 
Toshiba (China) Research and Development Center 
5/F., Tower W2, Oriental Plaza 
No.1, East Chang An Ave., Dong Cheng District 
Beijing, 100738, China  
{wuhua, wanghaifeng, liuzhanyi}@rdc.toshiba.com.cn   
 
Abstract 
This paper proposes an alignment 
adaptation approach to improve 
domain-specific (in-domain) word 
alignment. The basic idea of alignment 
adaptation is to use out-of-domain corpus 
to improve in-domain word alignment 
results. In this paper, we first train two 
statistical word alignment models with the 
large-scale out-of-domain corpus and the 
small-scale in-domain corpus respectively, 
and then interpolate these two models to 
improve the domain-specific word 
alignment. Experimental results show that 
our approach improves domain-specific 
word alignment in terms of both precision 
and recall, achieving a relative error rate 
reduction of 6.56% as compared with the 
state-of-the-art technologies. 
1 Introduction 
Word alignment was first proposed as an 
intermediate result of statistical machine 
translation (Brown et al, 1993). In recent years, 
many researchers have employed statistical models 
(Wu, 1997; Och and Ney, 2003; Cherry and Lin, 
2003) or association measures  (Smadja et al, 
1996; Ahrenberg et al, 1998; Tufis and Barbu, 
2002) to build alignment links. In order to achieve 
satisfactory results, all of these methods require a 
large-scale bilingual corpus for training. When the 
large-scale bilingual corpus is not available, some 
researchers use existing dictionaries to improve 
word alignment (Ker and Chang, 1997). However, 
only a few studies (Wu and Wang, 2004) directly 
address the problem of domain-specific word 
alignment when neither the large-scale 
domain-specific bilingual corpus nor the 
domain-specific translation dictionary is available. 
In this paper, we address the problem of word 
alignment in a specific domain, in which only a 
small-scale corpus is available. In the 
domain-specific (in-domain) corpus, there are two 
kinds of words: general words, which also 
frequently occur in the out-of-domain corpus, and 
domain-specific words, which only occur in the 
specific domain. Thus, we can use the 
out-of-domain bilingual corpus to improve the 
alignment for general words and use the in-domain 
bilingual corpus for domain-specific words. We 
implement this by using alignment model 
adaptation. 
Although the adaptation technology is widely 
used for other tasks such as language modeling 
(Iyer et al, 1997), only a few studies, to the best of 
our knowledge, directly address word alignment 
adaptation. Wu and Wang (2004) adapted the 
alignment results obtained with the out-of-domain 
corpus to the results obtained with the in-domain 
corpus. This method first trained two models and 
two translation dictionaries with the in-domain 
corpus and the out-of-domain corpus, respectively. 
Then these two models were applied to the 
in-domain corpus to get different results. The 
trained translation dictionaries were used to select 
alignment links from these different results. Thus, 
this method performed adaptation through result 
combination. The experimental results showed a 
significant error rate reduction as compared with 
the method directly combining the two corpora as 
training data.  
In this paper, we improve domain-specific word 
alignment through statistical alignment model 
adaptation instead of result adaptation. Our method 
includes the following steps: (1) two word 
alignment models are trained using a small-scale 
in-domain bilingual corpus and a large-scale 
467
out-of-domain bilingual corpus, respectively. (2) A 
new alignment model is built by interpolating the 
two trained models. (3) A translation dictionary is 
also built by interpolating the two dictionaries that 
are trained from the two training corpora. (4) The 
new alignment model and the translation dictionary 
are employed to improve domain-specific word 
alignment results. Experimental results show that 
our approach improves domain-specific word 
alignment in terms of both precision and recall, 
achieving a relative error rate reduction of 6.56% 
as compared with the state-of-the-art technologies. 
The remainder of the paper is organized as 
follows. Section 2 introduces the statistical word 
alignment model. Section 3 describes our 
alignment model adaptation method. Section 4 
describes the method used to build the translation 
dictionary. Section 5 describes the model 
adaptation algorithm. Section 6 presents the 
evaluation results. The last section concludes our 
approach. 
2 Statistical Word Alignment 
According to the IBM models (Brown et al, 1993), 
the statistical word alignment model can be 
generally represented as in Equation (1).  
?=
'
)|,'(
)|,(
),|(
a
ap
ap
ap
ef
efef  (1)
In this paper, we use a simplified IBM model 4 
(Al-Onaizan et al, 1999), which is shown in 
Equation (2). This simplified version does not take 
word classes into account as described in (Brown 
et al, 1993). 
))))(()](([                  
))()](([(                    
)|( )|(                     
                 
)|,Pr()|,(
0,1
1
0,1
1
11
1
2
0
0
0
),(
00
?
?
??
?
?=
>
?=
==
?
???
+??=
??
????
?
???
? ?=
=
m
aj
j
m
aj
j
m
j
aj
l
i
ii
m
j
j
ja
j
jpjdahj
cjdahj
eften
pp
m
ap
?
??
??
?
?
?
?? eef
 
(2)
ml,  are the lengths of the target sentence and the  
source sentence respectively. 
j  is the position index of the source word. 
ja  is the position of the target word aligned to 
    the jth source word. 
i?  is the fertility of . ie
1p  is the fertility probability for e , and 
. 
0
110 =+ pp
)
jaj|et(f  is the word translation probability. 
)|( ii en ?  is the fertility probability. 
)(1 jacjd ??  is the distortion probability for the  
head of each cept1. 
))((1 jpjd ?>  is the distortion probability for the  
remaining words of the cept. 
}:{min)( kk
aikih == is the head of cept i. 
}:{max)( kj
jk
aakjp ==
<
 
i?  is the first word before  with non-zero 
fertility. If , 
; else . 
ie
0?
}i
0|}0:{| '' ' ><<> iii i?
00 'i <<? 0=i?:max{ ' 'i ii >= ??
i
j j
i
jia
c ?
? ?== ][  is the center of cept i. 
During the training process, IBM model 3 is 
first trained, and then the parameters in model 3 
are employed to train model 4. During the testing 
process, the trained model 3 is also used to get an 
initial alignment result, and then the trained model 
4 is employed to improve this alignment result. For 
convenience, we describe model 3 in Equation (3). 
The main difference between model 3 and model 4 
lies in the calculation of distortion probability. 
??
??
?
?=
==
?
?
??
????
?
???
? ?=
=
m
aj
j
m
j
aj
l
i
i
l
i
ii
m
j
j
mlajdeft
en
pp
m
ap
0:1
11
1
2
0
0
0
),(
),,|()|(                     
!  )|(                     
                   
)|,Pr()|,(
00
??
?
?
??
??
??
eef
(3)
                                                           
1 A cept is defined as the set of target words connected to a source word 
(Brown et al, 1993).  
468
However, both model 3 and model 4 do not 
take the multiword cept into account. Only 
one-to-one and many-to-one word alignments are 
considered. Thus, some multi-word units in the 
domain-specific corpus cannot be correctly aligned. 
In order to deal with this problem, we perform 
word alignment in two directions (source to target, 
and target to source) as described in (Och and Ney, 
2000). The GIZA++ toolkit2 is used to perform 
statistical word alignment. 
We use  and  to represent the 
bi-directional alignment sets, which are shown in 
Equation (4) and (5). For alignment in both sets, 
we use j for source words and i for target words. If 
a target word in position i is connected to source 
words in positions  and , then . 
We call an element in the alignment set an 
alignment link. 
1SG 2SG
2j1j },{ 21 jjAi =
}}0 ,|{|),{(1 ?=== jjii aiajAiASG  (4)
}}0  ,|{|),{(2 ?=== jjjj aaiiAAjSG (5)
3 Word Alignment Model Adaptation 
In this paper, we first train two models using the 
out-of-domain training data and the in-domain 
training data, and then build a new alignment 
model through linear interpolation of the two 
trained models. In other words, we make use of the 
out-of-domain training data and the in-domain 
training data by interpolating the trained alignment 
models. One method to perform model adaptation 
is to directly interpolate the alignment models as 
shown in Equation (6).  
),|()1(),|(),|( efapefapefap OI ??+?= ??
 
(6)
),|( efapI  and  are the alignment 
model trained using the in-domain corpus and the 
out-of-domain corpus, respectively.
),|( efapO
?  is an 
interpolation weight. It can be a constant or a 
function of  and . f e
However, in both model 3 and model 4, there 
are mainly three kinds of parameters: translation 
probability, fertility probability and distortion 
probability. These three kinds of parameters have 
their own interpretation in these two models. In 
order to obtain fine-grained interpolation models, 
we separate the alignment model interpolation into 
three parts: translation probability interpolation, 
fertility probability interpolation and distortion 
probability interpolation. For these probabilities, 
we use different interpolation methods to calculate 
the interpolation weights. After interpolation, we 
replace the corresponding parameters in equation 
(2) and (3) with the interpolated probabilities to get 
new alignment models. 
                                                          
2 It is located at http://www.fjoch.com/GIZA++.html. 
In the following subsections, we will perform 
linear interpolation for word alignment in the 
source to target direction. For the word alignment 
in the target to source direction, we use the same 
interpolation method. 
3.1 Translation Probability Interpolation 
The word translation probability  is 
very important in translation models. The same 
word may have different distributions in the 
in-domain corpus and the out-of-domain corpus. 
Thus, the interpolation weight for the translation 
probability is taken as a variant. The interpolation 
model for  is described in Equation (7).  
)|(
jaj eft
)|(
jaj eft
)|())(1(                      
)|()()|(
jj
jjj
ajOat
ajIataj
efte
efteeft
??
+?=
?
?
 (7)
The interpolation weight  in (7) is a 
function of . It is calculated as shown in 
Equation (8).  
)(
jat e?
jae
?
? ???
?
???
?
+= )()(
)(
)(
jj
j
j
aOaI
aI
at epep
ep
e  (8)
)(
jaI ep  and  are the relative 
frequencies of  in the in-domain corpus and in 
the out-of-domain corpus, respectively. 
)(
jaO ep
jae
?  is an 
adaptation coefficient, such that 0?? . 
Equation (8) indicates that if a word occurs 
more frequently in a specific domain than in the 
general domain, it can usually be considered as a 
domain-specific word (Pe?as et al, 2001). For 
example, if  is much larger than , 
the word  is a domain-specific word and the 
interpolation weight approaches to 1. In this case, 
we trust more on the translation probability 
obtained from the in-domain corpus than that 
obtained from the out-of-domain corpus. 
)(
jaI ep
ja
)(
jaO ep
e
469
3.2 
3.3 
4 
Fertility Probability Interpolation 
The fertility probability describes the 
distribution of the number of words that  is 
aligned to. The interpolation model is shown in (9). 
)|( ii en ?
ie
)|()1()|()|( iiOniiInii enenen ????? ??+?= (9)
Where,  is a constant. This constant is obtained 
using a manually annotated held-out data set. In 
fact, we can also set the interpolation weight to be 
a function of the word . From the word 
alignment results on the held-out set, we conclude 
that these two weighting schemes do not perform 
quite differently. 
n?
ie
Distortion Probability Interpolation 
The distortion probability describes the distribution 
of alignment positions. We separate it into two 
parts: one is the distortion probability in model 3, 
and the other is the distortion probability in model 
4. The interpolation model for the distortion 
probability in model 3 is shown in (10). Since the 
distortion probability is irrelevant with any specific 
source or target words, we take  as a constant. 
This constant is obtained using the held-out set. 
d?
),,|()1(                          
),,|(),,|(
mlajd
mlajdmlajd
jOd
jIdj
??
+?=
?
?
 (10)
For the distortion probability in model 4, we 
use the same interpolation method and take the 
interpolation weight as a constant.  
Translation Dictionary Acquisition 
We use the translation dictionary trained from the 
training data to further improve the alignment 
results. When we train the bi-directional statistical 
word alignment models with the training data, we 
get two word alignment results for the training data. 
By taking the intersection of the two word 
alignment results, we build a new alignment set. 
The alignment links in this intersection set are 
extended by iteratively adding word alignment 
links into it as described in (Och and Ney, 2000). 
Based on the extended alignment links, we build a 
translation dictionary. In order to filter the noise 
caused by the error alignment links, we only retain 
those translation pairs whose log-likelihood ratio 
scores (Dunning, 1993) are above a threshold. 
Based on the alignment results on the 
out-of-domain corpus, we build a translation 
dictionary  filtered with a threshold . Based 
on the alignment results on a small-scale 
in-domain corpus, we build another translation 
dictionary  filtered with a threshold .  
1D
2D
1?
2?
After obtaining the two dictionaries, we 
combine two dictionaries through linearly 
interpolating the translation probabilities in the two 
dictionaries, which is shown in (11). The symbols f 
and e represent a single word or a phrase in the 
source and target languages. This differs from the 
translation probability in Equation (7), where these 
two symbols only represent single words. 
)|())(1()|()()|( efpeefpeefp OI ??+?= ?? (11)
The interpolation weight is also a function of e. It 
is calculated as shown in (12)3. 
)()(
)(
)(
epep
ep
e
OI
I
+=?  (12)
)(epI  and  represent the relative 
frequencies of e  in the in-domain corpus and 
out-of-domain corpus, respectively.  
)(epO
5 
6 Evaluation 
                                                          
Adaptation Algorithm 
The adaptation algorithms include two parts: a 
training algorithm and a testing algorithm. The 
training algorithm is shown in Figure 1.  
After getting the two adaptation models and the 
translation dictionary, we apply them to the 
in-domain corpus to perform word alignment. Here 
we call it testing algorithm. The detailed algorithm 
is shown in Figure 2. For each sentence pair, there 
are two different word alignment results, from 
which the final alignment links are selected 
according to their translation probabilities in the 
dictionary D. The selection order is similar to that 
in the competitive linking algorithm (Melamed, 
1997). The difference is that we allow many-to-one 
and one-to-many alignments. 
We compare our method with four other methods. 
The first method is descried in (Wu and Wang, 
2004). We call it ?Result Adaptation (ResAdapt)?. 
3 We also tried an adaptation coefficient to calculate the 
interpolation weight as in (8). However, the alignment results 
are not improved by using this coefficient for the dictionary. 
470
Input: In-domain training data 
      Out-of-domain training data 
(1) Train two alignment models 
(source to target) and  (target to 
source) using the in-domain corpus. 
st
IM
ts
IM
(2) Train the other two alignment models 
 and  using the out-of-domain 
corpus. 
st
OM
ts
OM
(3) Build an adaptation model stM  based on 
 and , and build the other 
adaptation model 
st
IM
st
OM
tsM  based on 
and  using the interpolation methods 
described in section 3. 
ts
IM
ts
OM
(4) Train a dictionary  using the 
alignment results on the in-domain 
training data. 
1D
(5) Train another dictionary  using the 
alignment results on the out-of-domain 
training data. 
2D
(6) Build an adaptation dictionary D  based 
on  and  using the interpolation 
method described in section 4. 
1D 2D
Output: Alignment models stM  and tsM  
       Translation dictionary D  
Figure 1. Training Algorithm 
Input: Alignment models stM  and tsM , 
translation dictionary D , and testing 
data 
(1) Apply the adaptation model stM and 
tsM  to the testing data to get two 
different alignment results. 
(2) Select the alignment links with higher 
translation probability in the translation 
dictionary D . 
Output: Alignment results on the testing data
Figure 2. Testing Algorithm 
The second method ?Gen+Spec? directly combines 
the out-of-domain corpus and the in-domain corpus 
as training data. The third method ?Gen? only uses 
the out-of-domain corpus as training data. The 
fourth method ?Spec? only uses the in-domain 
corpus as training data. For each of the last three 
methods, we first train bi-directional alignment 
models using the training data. Then we build a 
translation dictionary based on the alignment 
results on the training data and filter it using 
log-likelihood ratio as described in section 4. 
6.1 
6.2 
Training and Testing Data 
In this paper, we take English-Chinese word 
alignment as a case study. We use a sentence- 
aligned out-of-domain English-Chinese bilingual 
corpus, which includes 320,000 bilingual sentence 
pairs. The average length of the English sentences 
is 13.6 words while the average length of the 
Chinese sentences is 14.2 words. 
We also use a sentence-aligned in-domain 
English-Chinese bilingual corpus (operation 
manuals for diagnostic ultrasound systems), which 
includes 5,862 bilingual sentence pairs. The 
average length of the English sentences is 12.8 
words while the average length of the Chinese 
sentences is 11.8 words. From this domain-specific 
corpus, we randomly select 416 pairs as testing 
data. We also select 400 pairs to be manually 
annotated as held-out set (development set) to 
adjust parameters. The remained 5,046 pairs are 
used as domain-specific training data. 
The Chinese sentences in both the training set 
and the testing set are automatically segmented 
into words. In order to exclude the effect of the 
segmentation errors on our alignment results, the 
segmentation errors in our testing set are 
post-corrected. The alignments in the testing set 
are manually annotated, which includes 3,166 
alignment links. Among them, 504 alignment links 
include multiword units.  
Evaluation Metrics 
We use the same evaluation metrics as described in 
(Wu and Wang, 2004). If we use  to represent 
the set of alignment links identified by the 
proposed methods and  to denote the reference 
alignment set, the methods to calculate the 
precision, recall, f-measure, and alignment error 
rate (AER) are shown in Equation (13), (14), (15), 
and (16). It can be seen that the higher the 
f-measure is, the lower the alignment error rate is. 
Thus, we will only show precision, recall and AER 
scores in the evaluation results. 
GS
CS
|S|
|SS|
G
CG ?=precision      
(13)
471
|S|
 |SS|
C
CG ?=recall  
(14)
||||
||2
CG
CG
SS
SS
fmeasure +
??=  (15)
fmeasure
SS
SS
AER
CG
CG ?=+
???= 1
||||
||2
1 (16)
 
6.3 Evaluation Results 
We use the held-out set described in section 6.1 to 
set the interpolation weights. The coefficient ?  in 
Equation (8) is set to 0.8, the interpolation weight 
 in Equation (9) is set to 0.1, the interpolation 
weight  in model 3 in Equation (10) is set to 
0.1, and the interpolation weight  in model 4 is 
set to 1. In addition, log-likelihood ratio score 
thresholds are set to  and . With 
these parameters, we get the lowest alignment error 
rate on the held-out set. 
n?
d?
d?
301 =? 252 =?
Using these parameters, we build two 
adaptation models and a translation dictionary on 
the training data, which are applied to the testing 
set. The evaluation results on our testing set are 
shown in Table 1. From the results, it can be seen 
that our approach performs the best among all of 
the methods, achieving the lowest alignment error 
rate. Compared with the method ?ResAdapt?, our 
method achieves a higher precision without loss of 
recall, resulting in an error rate reduction of 6.56%. 
Compared with the method ?Gen+Spec?, our 
method gets a higher recall, resulting in an error 
rate reduction of 17.43%. This indicates that our 
model adaptation method is very effective to 
alleviate the data-sparseness problem of 
domain-specific word alignment. 
Method Precision Recall AER 
Ours 0.8490 0.7599 0.1980
ResAdapt 0.8198 0.7587 0.2119
Gen+Spec 0.8456 0.6905 0.2398
Gen 0.8589 0.6576 0.2551
Spec 0.8386 0.6731 0.2532
Table 1. Word Alignment Adaptation Results 
The method that only uses the large-scale 
out-of-domain corpus as training data does not 
produce good result. The alignment error rate is 
almost the same as that of the method only using 
the in-domain corpus. In order to further analyze 
the result, we classify the alignment links into two 
classes: single word alignment links (SWA) and 
multiword alignment links (MWA). Single word 
alignment links only include one-to-one 
alignments. The multiword alignment links include 
those links in which there are multiword units in 
the source language or/and the target language. 
The results are shown in Table 2. From the results, 
it can be seen that the method ?Spec? produces 
better results for multiword alignment while the 
method ?Gen? produces better results for single 
word alignment. This indicates that the multiword 
alignment links mainly include the domain-specific 
words. Among the 504 multiword alignment links, 
about 60% of the links include domain-specific 
words. In Table 2, we also present the results of 
our method. Our method achieves the lowest error 
rate results on both single word alignment and 
multiword alignment.  
Method Precision Recall AER 
Ours (SWA) 0.8703 0.8621 0.1338
Ours (MWA) 0.5635 0.2202 0.6833
Gen (SWA) 0.8816 0.7694 0.1783
Gen (MWA) 0.3366 0.0675 0.8876
Spec (SWA) 0.8710 0.7633 0.1864
Spec (MWA) 0.4760 0.1964 0.7219
Table 2. Single Word and Multiword Alignment 
Results 
In order to further compare our method with the 
method described in (Wu and Wang, 2004). We do 
another experiment using almost the same-scale 
in-domain training corpus as described in (Wu and 
Wang, 2004). From the in-domain training corpus, 
we randomly select about 500 sentence pairs to 
build the smaller training set. The testing data is 
the same as shown in section 6.1. The evaluation 
results are shown in Table 3. 
Method Precision Recall AER 
Ours 0.8424 0.7378 0.2134
ResAdapt 0.8027 0.7262 0.2375
Gen+Spec 0.8041 0.6857 0.2598
Table 3. Alignment Adaptation Results Using a 
Smaller In-Domain Corpus 
Compared with the method ?Gen+Spec?, our 
method achieves an error rate reduction of 17.86% 
472
while the method ?ResAdapt? described in (Wu 
and Wang, 2004) only achieves an error rate 
reduction of 8.59%. Compared with the method 
?ResAdapt?, our method achieves an error rate 
reduction of 10.15%. 
This result is different from that in (Wu and 
Wang, 2004), where their method achieved an 
error rate reduction of 21.96% as compared with 
the method ?Gen+Spec?. The main reason is that 
the in-domain training corpus and testing corpus in 
this paper are different from those in (Wu and 
Wang, 2004). The training data and the testing data 
described in (Wu and Wang, 2004) are from a 
single manual. The data in our corpus are from 
several manuals describing how to use the 
diagnostic ultrasound systems. 
In addition to the above evaluations, we also 
evaluate our model adaptation method using the 
"refined" combination in Och and Ney (2000) 
instead of the translation dictionary. Using the 
"refined" method to select the alignments produced 
by our model adaptation method (AER: 0.2371) 
still yields better result than directly combining 
out-of-domain and in-domain corpora as training 
data of the "refined" method (AER: 0.2290). 
6.4 The Effect of In-Domain Corpus 
In general, it is difficult to obtain large-scale 
in-domain bilingual corpus. For some domains, 
only a very small-scale bilingual sentence pairs are 
available. Thus, in order to analyze the effect of the 
size of in-domain corpus, we randomly select 
sentence pairs from the in-domain training corpus 
to generate five training sets. The numbers of 
sentence pairs in these five sets are 1,010, 2,020, 
3,030, 4,040 and 5,046. For each training set, we 
use model 4 in section 2 to train an in-domain 
model. The out-of-domain corpus for the 
adaptation experiments and the testing set are the 
same as described in section 6.1. 
# Sentence 
Pairs Precision Recall AER 
1010 0.8385 0.7394 0.2142
2020 0.8388 0.7514 0.2073
3030 0.8474 0.7558 0.2010
4040 0.8482 0.7555 0.2008
5046 0.8490 0.7599 0.1980
Table 4. Alignment Adaptation Results Using 
In-Domain Corpora of Different Sizes 
# Sentence 
Pairs Precision Recall AER 
1010 0.8737 0.6642 0.2453
2020 0.8502 0.6804 0.2442
3030 0.8473 0.6874 0.2410
4040 0.8430 0.6917 0.2401
5046 0.8456 0.6905 0.2398
Table 5. Alignment Results Directly Combining 
Out-of-Domain and In-Domain Corpora  
The results are shown in Table 4 and Table 5. 
Table 4 describes the alignment adaptation results 
using in-domain corpora of different sizes. Table 5 
describes the alignment results by directly 
combining the out-of-domain corpus and the 
in-domain corpus of different sizes.  From the 
results, it can be seen that the larger the size of 
in-domain corpus is, the smaller the alignment 
error rate is. However, when the number of the 
sentence pairs increase from 3030 to 5046, the 
error rate reduction in Table 4 is very small. This is 
because the contents in the specific domain are 
highly replicated. This also shows that increasing 
the domain-specific corpus does not obtain great 
improvement on the word alignment results. 
Comparing the results in Table 4 and Table 5, we 
find out that our adaptation method reduces the 
alignment error rate on all of the in-domain 
corpora of different sizes.  
6.5 The Effect of Out-of-Domain Corpus 
In order to further analyze the effect of the 
out-of-domain corpus on the adaptation results, we 
randomly select sentence pairs from the 
out-of-domain corpus to generate five sets. The 
numbers of sentence pairs in these five sets are 
65,000, 130,000, 195,000, 260,000, and 320,000 
(the entire out-of-domain corpus). In the adaptation 
experiments, we use the entire in-domain corpus 
(5046 sentence pairs). The adaptation results are 
shown in Table 6. 
From the results in Table 6, it can be seen that 
the larger the size of out-of-domain corpus is, the 
smaller the alignment error rate is. However, when 
the number of the sentence pairs is more than 
130,000, the error rate reduction is very small. This 
indicates that we do not need a very large bilingual 
out-of-domain corpus to improve domain-specific 
word alignment results. 
 
473
# Sentence 
Pairs (k) Precision Recall AER 
65 0.8441 0.7284 0.2180
130 0.8479 0.7413 0.2090
195 0.8454 0.7461 0.2073
260 0.8426 0.7508 0.2059
320 0.8490 0.7599 0.1980
Table 6. Adaptation Alignment Results Using 
Out-of-Domain Corpora of Different Sizes 
7 Conclusion 
This paper proposes an approach to improve 
domain-specific word alignment through alignment 
model adaptation. Our approach first trains two 
alignment models with a large-scale out-of-domain 
corpus and a small-scale domain-specific corpus. 
Second, we build a new adaptation model by 
linearly interpolating these two models. Third, we 
apply the new model to the domain-specific corpus 
and improve the word alignment results. In 
addition, with the training data, an interpolated 
translation dictionary is built to select the word 
alignment links from different alignment results. 
Experimental results indicate that our approach 
achieves a precision of 84.90% and a recall of 
75.99% for word alignment in a specific domain. 
Our method achieves a relative error rate reduction 
of 17.43% as compared with the method directly 
combining the out-of-domain corpus and the 
in-domain corpus as training data.  It also 
achieves a relative error rate reduction of 6.56% as 
compared with the previous work in (Wu and 
Wang, 2004). In addition, when we train the model 
with a smaller-scale in-domain corpus as described 
in (Wu and Wang, 2004), our method achieves an 
error rate reduction of 10.15% as compared with 
the method in (Wu and Wang, 2004). 
We also use in-domain corpora and 
out-of-domain corpora of different sizes to perform 
adaptation experiments. The experimental results 
show that our model adaptation method improves 
alignment results on in-domain corpora of different 
sizes.  The experimental results also show that 
even a not very large out-of-domain corpus can 
help to improve the domain-specific word 
alignment through alignment model adaptation. 
References 
L. Ahrenberg, M. Merkel, M. Andersson. 1998. A 
Simple Hybrid Aligner for Generating Lexical 
Correspondences in Parallel Tests. In Proc. of 
ACL/COLING-1998, pp. 29-35. 
Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Lafferty, 
D. Melamed, F. J. Och, D. Purdy, N. A. Smith, D. 
Yarowsky. 1999. Statistical Machine Translation 
Final Report. Johns Hopkins University Workshop. 
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, R. 
Mercer. 1993. The Mathematics of Statistical 
Machine Translation: Parameter Estimation. 
Computational Linguistics, 19(2): 263-311. 
C. Cherry and D. Lin. 2003. A Probability Model to 
Improve Word Alignment. In Proc. of ACL-2003, pp. 
88-95. 
T. Dunning. 1993. Accurate Methods for the Statistics of 
Surprise and Coincidence. Computational Linguistics, 
19(1): 61-74. 
R. Iyer,  M. Ostendorf,  H. Gish.  1997. Using 
Out-of-Domain Data to Improve In-Domain 
Language Models. IEEE Signal Processing Letters, 
221-223. 
S. J. Ker and J. S. Chang. 1997. A Class-based 
Approach to Word Alignment. Computational 
Linguistics, 23(2): 313-343. 
I. D. Melamed. 1997. A Word-to-Word Model of 
Translational Equivalence. In Proc. of ACL 1997, pp. 
490-497. 
F. J. Och and H. Ney. 2000. Improved Statistical 
Alignment Models. In Proc. of ACL-2000, pp. 
440-447. 
A. Pe?as, F. Verdejo, J. Gonzalo. 2001. Corpus-based 
Terminology Extraction Applied to Information 
Access. In Proc. of the Corpus Linguistics 2001, vol. 
13. 
F. Smadja, K. R. McKeown, V. Hatzivassiloglou. 1996. 
Translating Collocations for Bilingual Lexicons: a 
Statistical Approach. Computational Linguistics, 
22(1): 1-38. 
D. Tufis and A. M. Barbu. 2002. Lexical Token 
Alignment: Experiments, Results and Application. In 
Proc. of LREC-2002, pp. 458-465. 
D. Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel 
Corpora. Computational Linguistics, 23(3): 377-403. 
H. Wu and H. Wang. 2004. Improving Domain-Specific 
Word Alignment with a General Bilingual Corpus. In 
R. E. Frederking and K. B. Taylor (Eds.), Machine 
Translation: From Real Users to Research: 6th 
conference of AMTA-2004, pp. 262-271. 
474
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 457?464,
Sydney, July 2006. c?2006 Association for Computational Linguistics
 An Equivalent Pseudoword Solution to Chinese  
Word Sense Disambiguation 
 
Zhimao Lu+    Haifeng Wang++    Jianmin Yao+++    Ting Liu+    Sheng Li+
+ Information Retrieval Laboratory, School of Computer Science and Technology,  
Harbin Institute of Technology, Harbin, 150001, China 
{lzm, tliu, lisheng}@ir-lab.org 
++ Toshiba (China) Research and Development Center 
5/F., Tower W2, Oriental Plaza, No. 1, East Chang An Ave., Beijing, 100738, China 
wanghaifeng@rdc.toshiba.com.cn 
+++ School of Computer Science and Technology 
Soochow University, Suzhou, 215006, China 
jyao@suda.edu.cn 
 
  
 
Abstract 
This paper presents a new approach 
based on Equivalent Pseudowords (EPs) 
to tackle Word Sense Disambiguation 
(WSD) in Chinese language. EPs are par-
ticular artificial ambiguous words, which 
can be used to realize unsupervised WSD. 
A Bayesian classifier is implemented to 
test the efficacy of the EP solution on 
Senseval-3 Chinese test set. The per-
formance is better than state-of-the-art 
results with an average F-measure of 0.80. 
The experiment verifies the value of EP 
for unsupervised WSD. 
1 Introduction 
Word sense disambiguation (WSD) has been a 
hot topic in natural language processing, which is 
to determine the sense of an ambiguous word in 
a specific context. It is an important technique 
for applications such as information retrieval, 
text mining, machine translation, text classifica-
tion, automatic text summarization, and so on. 
Statistical solutions to WSD acquire linguistic 
knowledge from the training corpus using ma-
chine learning technologies, and apply the 
knowledge to disambiguation. The first statistical 
model of WSD was built by Brown et al (1991). 
Since then, most machine learning methods have 
been applied to WSD, including decision tree, 
Bayesian model, neural network, SVM, maxi-
mum entropy, genetic algorithms, and so on. For 
different learning methods, supervised methods 
usually achieve good performance at a cost of 
human tagging of training corpus. The precision 
improves with larger size of training corpus. 
Compared with supervised methods, unsuper-
vised methods do not require tagged corpus, but 
the precision is usually lower than that of the 
supervised methods. Thus, knowledge acquisi-
tion is critical to WSD methods.  
This paper proposes an unsupervised method 
based on equivalent pseudowords, which ac-
quires WSD knowledge from raw corpus. This 
method first determines equivalent pseudowords 
for each ambiguous word, and then uses the 
equivalent pseudowords to replace the ambigu-
ous word in the corpus. The advantage of this 
method is that it does not need parallel corpus or 
seed corpus for training. Thus, it can use a large-
scale monolingual corpus for training to solve 
the data-sparseness problem. Experimental re-
sults show that our unsupervised method per-
forms better than the supervised method. 
The remainder of the paper is organized as fol-
lows. Section 2 summarizes the related work. 
Section 3 describes the conception of Equivalent 
Pseudoword. Section 4 describes EP-based Un-
supervised WSD Method and the evaluation re-
sult. The last section concludes our approach. 
2 Related Work 
For supervised WSD methods,  a knowledge ac-
quisition bottleneck is to prepare the manually 
457
tagged corpus. Unsupervised method is an alter-
native, which often involves automatic genera-
tion of tagged corpus, bilingual corpus alignment, 
etc. The value of unsupervised methods lies in 
the knowledge acquisition solutions they adopt. 
2.1 Automatic Generation of Training Corpus 
Automatic corpus tagging is a solution to WSD, 
which generates large-scale corpus from a small 
seed corpus. This is a weakly supervised learning 
or semi-supervised learning method. This rein-
forcement algorithm dates back to Gale et al 
(1992a). Their investigation was based on a 6-
word test set with 2 senses for each word. 
Yarowsky (1994 and 1995), Mihalcea and 
Moldovan (2000), and Mihalcea (2002) have 
made further research to obtain large corpus of 
higher quality from an initial seed corpus. A 
semi-supervised method proposed by Niu et al 
(2005) clustered untagged instances with tagged 
ones starting from a small seed corpus, which 
assumes that similar instances should have simi-
lar tags. Clustering was used instead of boot-
strapping and was proved more efficient.  
2.2 Method Based on Parallel Corpus 
Parallel corpus is a solution to the bottleneck of 
knowledge acquisition. Ide et al (2001 and 
2002), Ng et al (2003), and Diab (2003, 2004a, 
and 2004b) made research on the use of align-
ment for WSD.  
Diab and Resnik (2002) investigated the feasi-
bility of automatically annotating large amounts 
of data in parallel corpora using an unsupervised 
algorithm, making use of two languages simulta-
neously, only one of which has an available 
sense inventory. The results showed that word-
level translation correspondences are a valuable 
source of information for sense disambiguation. 
The method by Li and Li (2002) does not re-
quire parallel corpus. It avoids the alignment 
work and takes advantage of bilingual corpus. 
In short, technology of automatic corpus tag-
ging is based on the manually labeled corpus. 
That is to say, it still need human intervention 
and is not a completely unsupervised method. 
Large-scale parallel corpus; especially word-
aligned corpus is highly unobtainable, which has 
limited the WSD methods based on parallel cor-
pus.  
3 Equivalent Pseudoword 
This section describes how to obtain equivalent 
pseudowords without a seed corpus. 
Monosemous words are unambiguous priori 
knowledge. According to our statistics, they ac-
count for 86%~89% of the instances in a diction-
ary and 50% of the items in running corpus, they 
are potential knowledge source for WSD.  
A monosemous word is usually synonymous 
to some polysemous words. For example the 
words "?? , ?? , ?? ?? ?? ??, , , , 
??" has similar meaning as one of the senses 
of the ambiguous word "??", while "??, ?
?, ?? ?? ??, , ?? ?? ?? ??, , , , , 
?? ?? ?? ??, , , " are the same for "??". 
This is quite common in Chinese, which can be 
used as a knowledge source for WSD. 
3.1 Definition of Equivalent Pseudoword 
If the ambiguous words in the corpus are re-
placed with its synonymous monosemous word, 
then is it convenient to acquire knowledge from 
raw corpus? For example in table 1, the ambigu-
ous word "??" has three senses, whose syn-
onymous monosemous words are listed on the 
right column. These synonyms contain some in-
formation for disambiguation task. 
An artificial ambiguous word can be coined 
with the monosemous words in table 1. This 
process is similar to the use of general pseu-
dowords (Gale et al, 1992b; Gaustad, 2001; Na-
kov and Hearst, 2003), but has some essential 
differences. This artificial ambiguous word need 
to simulate the function of the real ambiguous 
word, and to acquire semantic knowledge as the 
real ambiguous word does. Thus, we call it an 
equivalent pseudoword (EP) for its equivalence 
with the real ambiguous word. It's apparent that 
the equivalent pseudoword has provided a new 
way to unsupervised WSD. 
S1 ??/??? 
S2 ??/??/??/??/????(ba3 wo4)
S3 ??/??/??/??/??
Table 1. Synonymous Monosemous Words for 
the Ambiguous Word "??" 
The equivalence of the EP with the real am-
biguous word is a kind of semantic synonym or 
similarity, which demands a maximum similarity 
between the two words. An ambiguous word has 
the same number of EPs as of senses. Each EP's 
sense maps to a sense of ambiguous word. 
The semantic equivalence demands further 
equivalence at each sense level. Every corre-
458
sponding sense should have the maximum simi-
larity, which is the strictest limit to the construc-
tion of an EP. 
The starting point of unsupervised WSD based 
on EP is that EP can substitute the original word 
for knowledge acquisition in model training. 
Every instance of each morpheme of the EP can 
be viewed as an instance of the ambiguous word, 
thus the training set can be enlarged easily. EP is 
a solution to data sparseness for lack of human 
tagging in WSD. 
3.2 Basic Assumption for EP-based WSD 
It is based on the following assumptions that EPs 
can substitute the original ambiguous word for 
knowledge acquisition in WSD model training. 
Assumption 1: Words of the same meaning 
play the same role in a language. The sense is an 
important attribute of a word. This plays as the 
basic assumption in this paper. 
Assumption 2: Words of the same meaning 
occur in similar context. This assumption is 
widely used in semantic analysis and plays as a 
basis for much related research. For example, 
some researchers cluster the contexts of ambigu-
ous words for WSD, which shows good perform-
ance (Schutze, 1998). 
Because an EP has a higher similarity with the 
ambiguous word in syntax and semantics, it is a 
useful knowledge source for WSD. 
3.3 Design and Construction of EPs 
Because of the special characteristics of EPs, it's 
more difficult to construct an EP than a general 
pseudo word. To ensure the maximum similarity 
between the EP and the original ambiguous word, 
the following principles should be followed. 
1) Every EP should map to one and only one 
original ambiguous word. 
2) The morphemes of an EP should map one 
by one to those of the original ambiguous word. 
3) The sense of the EP should be the same as 
the corresponding ambiguous word, or has the 
maximum similarity with the word. 
4) The morpheme of a pseudoword stands for 
a sense, while the sense should consist of one or 
more morphemes.  
5) The morpheme should be a monosemous 
word. 
The fourth principle above is the biggest dif-
ference between the EP and a general pseudo 
word. The sense of an EP is composed of one or 
several morphemes. This is a remarkable feature 
of the EP, which originates from its equivalent 
linguistic function with the original word. To 
construct the EP, it must be ensured that the 
sense of the EP maps to that of the original word. 
Usually, a candidate monosemous word for a 
morpheme stands for part of the linguistic func-
tion of the ambiguous word, thus we need to 
choose several morphemes to stand for one sense.  
The relatedness of the senses refers to the 
similarity of the contexts of the original ambigu-
ous word and its EP. The similarity between the 
words means that they serve as synonyms for 
each other. This principle demands that both se-
mantic and pragmatic information should be 
taken into account in choosing a morpheme word. 
3.4 Implementation of the EP-based Solution 
An appropriate machine-readable dictionary is 
needed for construction of the EPs. A Chinese 
thesaurus is adopted and revised to meet this de-
mand. 
Extended Version of TongYiCiCiLin 
To extend the TongYiCiCiLin (Cilin) to hold 
more words, several linguistic resources are 
adopted for manually adding new words. An ex-
tended version of the Cilin is achieved, which 
includes 77,343 items. 
A hierarchy of three levels is organized in the 
extended Cilin for all items. Each node in the 
lowest level, called a minor class, contains sev-
eral words of the same class. The words in one 
minor class are divided into several groups ac-
cording to their sense similarity and relatedness, 
and each group is further divided into several 
lines, which can be viewed as the fifth level of 
the thesaurus. The 5-level hierarchy of the ex-
tended Cilin is shown in figure 1. The lower the 
level is, the more specific the sense is. The fifth 
level often contains a few words or only one 
word, which is called an atom word group, an 
atom class or an atom node. The words in the 
same atom node hold the smallest semantic dis-
tance. 
From the root node to the leaf node, the sense 
is described more and more detailed, and the 
words in the same node are more and more re-
lated. Words in the same fifth level node have 
the same sense and linguistic function, which 
ensures that they can substitute for each other 
without leading to any change in the meaning of 
a sentence. 
 
 
459
 ?  ? 
?
?? ?? 
? ? ? ?
? ? ?
 
? ? ? ?
Level 1 
Level 2 
Level 3 
Level 4 
Level 5 
?  ? 
Figure 1. Organization of Cilin (extended) 
 
The extended version of extended Cilin is 
freely downloadable from the Internet and has 
been used by over 20 organizations in the world1. 
Construction of EPs 
According to the position of the ambiguous word, 
a proper word is selected as the morpheme of the 
EP. Almost every ambiguous word has its corre-
sponding EP constructed in this way. 
The first step is to decide the position of the 
ambiguous word starting from the leaf node of 
the tree structure. Words in the same leaf node 
are identical or similar in the linguistic function 
and word sense. Other words in the leaf node of 
the ambiguous word are called brother words of 
it. If there is a monosemous brother word, it can 
be taken as a candidate morpheme for the EP. If 
there does not exist such a brother word, trace to 
the fourth level. If there is still no monosemous 
brother word in the fourth level, trace to the third 
level. Because every node in the third level con-
tains many words, candidate morpheme for the 
ambiguous can usually be found. 
In most cases, candidate morphemes can be 
found at the fifth level. It is not often necessary 
to search to the fourth level, less to the third. Ac-
cording to our statistics, the extended Cilin con-
tains about monosemous words for 93% of the 
ambiguous words in the fifth level, and 97% in 
the fourth level. There are only 112 ambiguous 
words left, which account for the other 3% and 
mainly are functional words. Some of the 3% 
words are rarely used, which cannot be found in 
even a large corpus. And words that lead to se-
mantic misunderstanding are usually content 
words. In WSD research for English, only nouns, 
verbs, adjectives and adverbs are considered. 
                                                 
1 It is located at http://www.ir-lab.org/. 
From this aspect, the extended version of Cilin 
meets our demand for the construction of EPs. 
If many monosemous brother words are found 
in the fourth or third level, there are many candi-
date morphemes to choose from. A further selec-
tion is made based on calculation of sense simi-
larity. More similar brother words are chosen. 
Computing of EPs 
Generally, several morpheme words are needed 
for better construction of an EP. We assume that 
every morpheme word stands for a specific sense 
and does not influence each other. It is more 
complex to construct an EP than a common 
pseudo word, and the formulation and statistical 
information are also different. 
An EP is described as follows:  
 
iikiiii
k
k
WWWWS
WWWWS
WWWWS
L
MMMMMM
L
L
,,,:
,,,:
,,,:
321
22322212
11312111
2
1
 
WEP?????????? 
Where WEP is the EP word, Si is a sense of the 
ambiguous word, and Wik is a morpheme word of 
the EP. 
The statistical information of the EP is calcu-
lated as follows: 
1? stands for the frequency of the S)( iSC i : 
?=
k
iki WCSC )()(  
2? stands for the co-occurrence fre-
quency of S
),( fi WSC
i and the contextual word Wf : 
?=
k
fikfi WWCWSC ),(),(  
460
 Ambiguous word citation (Qin and Wang, 2005) Ours Ambiguous word 
citation (Qin and 
Wang, 2005) Ours 
??(ba3 wo4) 0.56 0.87 ??(mei2 you3) 0.75 0.68 
?(bao1) 0.59 0.75 ??(qi3 lai2) 0.82 0.54 
??(cai2 liao4) 0.67 0.79 ?(qian2) 0.75 0.62 
??(chong1 ji1) 0.62 0.69 ??(ri4 zi3) 0.75 0.68 
?(chuan1) 0.80 0.57 ?(shao3) 0.69 0.56 
??(di4 fang1) 0.65 0.65 ??(tu1 chu1) 0.82 0.86 
??(fen1 zi3) 0.91 0.81 ??(yan2 jiu1) 0.69 0.63 
??(yun4 dong4) 0.61 0.82 ??(huo2 dong4) 0.79 0.88 
?(lao3) 0.59 0.50 ?(zou3) 0.72 0.60 
?(lu4) 0.74 0.64 ?(zuo4) 0.90 0.73 
Average 0.72 0.69 Note: Average of the 20 words 
Table 2. The F-measure for the Supervised WSD 
 
4 EP-based Unsupervised WSD Method 
EP is a solution to the semantic knowledge ac-
quisition problem, and it does not limit the 
choice of statistical learning methods. All of the 
mathematical modeling methods can be applied 
to EP-based WSD methods. This section focuses 
on the application of the EP concept to WSD, 
and chooses Bayesian method for the classifier 
construction. 
4.1 A Sense Classifier Based on the Bayes-
ian Model 
Because the model acquires knowledge from the 
EPs but not from the original ambiguous word, 
the method introduced here does not need human 
tagging of training corpus. 
In the training stage for WSD, statistics of EPs 
and context words are obtained and stored in a 
database. Senseval-3 data set plus unsupervised 
learning method are adopted to investigate into 
the value of EP in WSD. To ensure the compara-
bility of experiment results, a Bayesian classifier 
is used in the experiments. 
Bayesian Classifier 
Although the Bayesian classifier is simple, it is 
quite efficient, and it shows good performance 
on WSD. 
The Bayesian classifier used in this paper is 
described in (1) 
???
?
???
?
+= ?
? ij
k
cv
kjkSi SvPSPwS )|(log)(logmaxarg)( (1)
Where wi is the ambiguous word,  is the 
occurrence probability of the sense S
)( kSP
k,  
is the conditional probability of the context word 
v
)|( kj SvP
j, and ci is the set of the context words. 
To simplify the experiment process, the Naive 
Bayesian modeling is adopted for the sense clas-
sifier. Feature selection and ensemble classifica-
tion are not applied, which is both to simplify the 
calculation and to prove the effect of EPs in 
WSD. 
Experiment Setup and Results  
The Senseval-3 Chinese ambiguous words are 
taken as the testing set, which includes 20 words, 
each with 2-8 senses. The data for the ambiguous 
words are divided into a training set and a testing 
set by a ratio of 2:1. There are 15-20 training 
instances for each sense of the words, and occurs 
by the same frequency in the training and test set. 
Supervised WSD is first implemented using 
the Bayesian model on the Senseval-3 data set. 
With a context window of (-10, +10), the open 
test results are shown in table 2. 
The F-measure in table 2 is defined in (2). 
RP
RP
F +
??= 2  (2) 
461
Where P and R refer to the precision and recall 
of the sense tagging respectively, which are cal-
culated as shown in (3) and (4) 
)tagged(
)correct(
C
C
P =  (3) 
)all(
)correct(
C
C
R =  (4) 
Where C(tagged) is the number of tagged in-
stances of senses, C(correct) is the number of 
correct tags, and C(all) is the number of tags in 
the gold standard set. Every sense of the am-
biguous word has a P value, a R value and a F 
value. The F value in table 2 is a weighted aver-
age of all the senses. 
In the EP-based unsupervised WSD experi-
ment, a 100M corpus (People's Daily for year 
1998) is used for the EP training instances. The 
Senseval-3 data is used for the test. In our ex-
periments, a context window of (-10, +10) is 
taken. The detailed results are shown in table 3. 
4.2 Experiment Analysis and Discussion 
Experiment Evaluation Method 
Two evaluation criteria are used in the experi-
ments, which are the F-measure and precision. 
Precision is a usual criterion in WSD perform-
ance analysis. Only in recent years, the precision, 
recall, and F-measure are all taken to evaluate 
the WSD performance. 
In this paper, we will only show the f-measure 
score because it is a combined score of precision 
and recall. 
Result Analysis on Bayesian Supervised WSD 
Experiment 
The experiment results in table 2 reveals that the 
results of supervised WSD and those of (Qin and 
Wang, 2005) are different. Although they are all 
based on the Bayesian model, Qin and Wang 
(2005) used an ensemble classifier. However, the 
difference of the average value is not remarkable. 
As introduced above, in the supervised WSD 
experiment, the various senses of the instances 
are evenly distributed. The lower bound as Gale 
et al (1992c) suggested should be very low and 
it is more difficult to disambiguate if there are 
more senses. The experiment verifies this reason-
ing, because the highest F-measure is less than 
90%, and the lowest is less than 60%, averaging 
about 70%. 
With the same number of senses and the same 
scale of training data, there is a big difference 
between the WSD results. This shows that other 
factors exist which influence the performance 
other than the number of senses and training data 
size. For example, the discriminability among the 
senses is an important factor. The WSD task be-
comes more difficult if the senses of the ambigu-
ous word are more similar to each other. 
Experiment Analysis of the EP-based WSD 
The EP-based unsupervised method takes the 
same open test set as the supervised method. The 
unsupervised method shows a better performance, 
with the highest F-measure score at 100%, low-
est at 59% and average at 80%. The results 
shows that EP is useful in unsupervised WSD. 
 
Sequence 
Number Ambiguous word F-measure
Sequence 
Number Ambiguous word 
F-measure 
(%) 
1 ??(ba3 wo4) 0.93 11 ??(mei2 you3) 1.00 
2 ?(bao1) 0.74 12 ??(qi3 lai2) 0.59 
3 ?(cai2 liao4) 0.80 13 ?(qian2) 0.71 
4 ??(chong1 ji1) 0.85 14 ??(ri4 zi3) 0.62 
5 ?(chuan1) 0.79 15 ?(shao3) 0.82 
6 ??(di4 fang1) 0.78 16 ??(tu1 chu1) 0.93 
7 ??(fen1 zi3) 0.94 17 ??(yan2 jiu1) 0.71 
8 ??(yun4 
dong4) 
0.94 18 ??(huo2 dong4) 0.89 
9 ?(lao3) 0.85 19 ?(zou3) 0.68 
10 ?(lu4) 0.81 20 ?(zuo4) 0.67 
Average 0.80 Note: Average of the 20 words 
Table 3. The Results for Unsupervised WSD based on EPs 
462
 
From the results in table 2 and table 3, it can 
be seen that 16 among the 20 ambiguous words 
show better WSD performance in unsupervised 
SWD than in supervised WSD, while only 2 of 
them shows similar results and 2 performs worse . 
The average F-measure of the unsupervised 
method is higher by more than 10%. The reason 
lies in the following aspects: 
1) Because there are several morpheme words 
for every sense of the word in construction of the 
EP, rich semantic information can be acquired in 
the training step and is an advantage for sense 
disambiguation. 
2) Senseval-3 has provided a small-scale train-
ing set, with 15-20 training instances for each 
sense, which is not enough for the WSD model-
ing. The lack of training information leads to a 
low performance of the supervised methods. 
3) With a large-scale training corpus, the un-
supervised WSD method has got plenty of train-
ing instances for a high performance in disam-
biguation. 
4) The discriminability of some ambiguous 
word may be low, but the corresponding EPs 
could be easier to disambiguate. For example, 
the ambiguous word "?" has two senses which 
are difficult to distinguish from each other, but 
its Eps' senses of "??/??/??" and "?/?/
?/?"can be easily disambiguated. It is the same 
for the word "??", whose Eps' senses are "?
?/?? /??" and "??/??". EP-based 
knowledge acquisition of these ambiguous words 
for WSD has helped a lot to achieve high per-
formance. 
5 Conclusion 
As discussed above, the supervised WSD method 
shows a low performance because of its depend-
ency on the size of the training data. This reveals 
its weakness in knowledge acquisition bottleneck. 
EP-based unsupervised method has overcame 
this weakness. It requires no manually tagged 
corpus to achieve a satisfactory performance on 
WSD. Experimental results show that EP-based 
method is a promising solution to the large-scale 
WSD task. In future work, we will examine the 
effectiveness of EP-based method in other WSD 
techniques. 
References 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1991. Word-
Sense Disambiguation Using Statistical Methods. 
In Proc. of the 29th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-1991), 
pages 264-270. 
Mona Talat Diab. 2003. Word Sense Disambiguation 
Within a Multilingual Framework. PhD thesis, 
University of Maryland College Park. 
Mona Diab. 2004a. Relieving the Data Acquisition 
Bottleneck in Word Sense Disambiguation. In Proc. 
of the 42nd Annual Meeting of the Association for 
Computational Linguistics (ACL-2004), pages 303-
310. 
Mona T. Diab. 2004b. An Unsupervised Approach for 
Bootstrapping Arabic Sense Tagging. In Proc. of 
Arabic Script Based Languages Workshop at COL-
ING 2004, pages 43-50. 
Mona Diab and Philip Resnik. 2002. An Unsuper-
vised Method for Word Sense Tagging Using Par-
allel Corpora. In Proc. of the 40th Annual Meeting 
of the Association for Computational Linguistics 
(ACL-2002), pages 255-262. 
William Gale, Kenneth Church, and David Yarowsky. 
1992a. Using Bilingual Materials to Develop Word 
Sense Disambiguation Methods. In Proc. of the 4th 
International Conference on Theoretical and Meth-
odolgical Issues in Machine Translation(TMI-92), 
pages 101-112. 
William Gale, Kenneth Church, and David Yarowsky. 
1992b. Work on Statistical Methods for Word 
Sense Disambiguation. In Proc. of AAAI Fall Sym-
posium on Probabilistic Approaches to Natural 
Language, pages 54-60. 
William Gale, Kenneth Ward Church, and David 
Yarowsky. 1992c. Estimating Upper and Lower 
Bounds on the Performance of Word Sense Disam-
biguation Programs. In Proc. of the 30th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL-1992), pages 249-256. 
Tanja Gaustad. 2001. Statistical Corpus-Based Word 
Sense Disambiguation: Pseudowords vs. Real Am-
biguous Words. In Proc. of the 39th ACL/EACL, 
Student Research Workshop, pages 61-66. 
Nancy Ide, Tomaz Erjavec, and Dan Tufi?. 2001. 
Automatic Sense Tagging Using Parallel Corpora. 
In Proc. of the Sixth Natural Language Processing 
Pacific Rim Symposium, pages 83-89. 
Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2002. 
Sense Discrimination with Parallel Corpora. In 
Workshop on Word Sense Disambiguation: Recent 
Successes and Future Directions, pages 54-60. 
Cong Li and Hang Li. 2002. Word Translation Dis-
ambiguation Using Bilingual Bootstrapping. In 
Proc. of the 40th Annual Meeting of the Association 
463
for Computational Linguistics (ACL-2002), pages 
343-351. 
Rada Mihalcea and Dan Moldovan. 2000. An Iterative 
Approach to Word Sense Disambiguation. In Proc. 
of Florida Artificial Intelligence Research Society 
Conference (FLAIRS 2000), pages 219-223. 
Rada F. Mihalcea. 2002. Bootstrapping Large Sense 
Tagged Corpora. In Proc. of the 3rd International 
Conference on Languages Resources and Evalua-
tions (LREC 2002), pages 1407-1411. 
Preslav I. Nakov and Marti A. Hearst. 2003. Cate-
gory-based Pseudowords. In Companion Volume to 
the Proceedings of HLT-NAACL 2003, Short Pa-
pers, pages 67-69. 
Hwee Tou. Ng, Bin Wang, and Yee Seng Chan. 2003. 
Exploiting Parallel Texts for Word Sense Disam-
biguation: An Empirical Study. In Proc. of the 41st 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2003), pages 455-462. 
Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan. 
2005. Word Sense Disambiguation Using Label 
Propagation Based Semi-Supervised Learning. In 
Proc. of the 43th Annual Meeting of the Association 
for Computational Linguistics (ACL-2005), pages 
395-402. 
Ying Qin and Xiaojie Wang. 2005. A Track-based 
Method on Chinese WSD. In Proc. of Joint Sympo-
sium of Computational Linguistics of China (JSCL-
2005), pages 127-133. 
Hinrich. Schutze. 1998. Automatic Word Sense Dis-
crimination. Computational Linguistics, 24(1): 97-
123. 
David Yarowsky. 1994. Decision Lists for Lexical 
Ambiguity Resolution: Application to Accent Res-
toration in Spanish and French. In Proc. of the 32nd 
Annual Meeting of the Association for Computa-
tional Linguistics(ACL-1994), pages 88-95. 
David Yarowsky. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. In 
Proc. of the 33rd Annual Meeting of the Association 
for Computational Linguistics (ACL-1995), pages 
189-196. 
 
464
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 593?600,
Sydney, July 2006. c?2006 Association for Computational Linguistics
The Effect of Translation Quality in MT-Based 
Cross-Language Information Retrieval 
 
Jiang Zhu      Haifeng Wang 
Toshiba (China) Research and Development Center 
5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng District 
Beijing, 100738, China 
{zhujiang, wanghaifeng}@rdc.toshiba.com.cn 
 
  
 
Abstract 
This paper explores the relationship be-
tween the translation quality and the re-
trieval effectiveness in Machine Transla-
tion (MT) based Cross-Language Infor-
mation Retrieval (CLIR). To obtain MT 
systems of different translation quality, 
we degrade a rule-based MT system by 
decreasing the size of the rule base and 
the size of the dictionary. We use the de-
graded MT systems to translate queries 
and submit the translated queries of vary-
ing quality to the IR system. Retrieval ef-
fectiveness is found to correlate highly 
with the translation quality of the queries. 
We further analyze the factors that affect 
the retrieval effectiveness. Title queries 
are found to be preferred in MT-based 
CLIR. In addition, dictionary-based deg-
radation is shown to have stronger impact 
than rule-based degradation in MT-based 
CLIR. 
1 Introduction 
Cross-Language Information Retrieval (CLIR) 
enables users to construct queries in one lan-
guage and search the documents in another lan-
guage. CLIR requires that either the queries or 
the documents be translated from a language into 
another, using available translation resources. 
Previous studies have concentrated on query 
translation because it is computationally less ex-
pensive than document translation, which re-
quires a lot of processing time and storage costs 
(Hull & Grefenstette, 1996). 
There are three kinds of methods to perform 
query translation, namely Machine Translation 
(MT) based methods, dictionary-based methods 
and corpus-based methods. Corresponding to 
these methods, three types of translation re-
sources are required: MT systems, bilingual 
wordlists and parallel or comparable corpora. 
CLIR effectiveness depends on both the design 
of the retrieval system and the quality of the 
translation resources that are used. 
In this paper, we explore the relationship be-
tween the translation quality of the MT system 
and the retrieval effectiveness. The MT system 
involved in this research is a rule-based English-
to-Chinese MT (ECMT) system. We degrade the 
MT system in two ways. One is to degrade the 
rule base of the system by progressively remov-
ing rules from it. The other is to degrade the dic-
tionary by gradually removing word entries from 
it. In both methods, we observe successive 
changes on translation quality of the MT system. 
We conduct query translation with the degraded 
MT systems and obtain translated queries of 
varying quality. Then we submit the translated 
queries to the IR system and evaluate the per-
formance. Retrieval effectiveness is found to be 
strongly influenced by the translation quality of 
the queries. We further analyze the factors that 
affect the retrieval effectiveness. Title queries are 
found to be preferred in MT-based query transla-
tion. In addition, the size of the dictionary is 
shown to have stronger impact on retrieval effec-
tiveness than the size of the rule base in MT-
based query translation. 
The remainder of this paper is organized as 
follows. In section 2, we briefly review related 
work. In section 3, we introduce two systems 
involved in this research: the rule-based ECMT 
system and the KIDS IR system. In section 4, we 
describe our experimental method.  Section 5 and 
section 6 reports and discusses the experimental 
results. Finally we present our conclusion and 
future work in section 7. 
593
2 Related Work 
2.1 Effect of Translation Resources 
Previous studies have explored the effect of 
translation resources such as bilingual wordlists 
or parallel corpora on CLIR performance. 
Xu and Weischedel (2000) measured CLIR 
performance as a function of bilingual dictionary 
size. Their English-Chinese CLIR experiments 
on TREC 5&6 Chinese collections showed that 
the initial retrieval performance increased 
sharply with lexicon size but the performance 
was not improved after the lexicon exceeded 
20,000 terms. Demner-Fushman and Oard (2003) 
identified eight types of terms that affected re-
trieval effectiveness in CLIR applications 
through their coverage by general-purpose bilin-
gual term lists. They reported results from an 
evaluation of the coverage of 35 bilingual term 
lists in news retrieval application. Retrieval ef-
fectiveness was found to be strongly influenced 
by term list size for lists that contain between 
3,000 and 30,000 unique terms per language. 
Franz et al (2001) investigated the CLIR per-
formance as a function of training corpus size for 
three different training corpora and observed ap-
proximately logarithmically increased perform-
ance with corpus size for all the three corpora. 
Kraaij (2001) compared three types of translation 
resources for bilingual retrieval based on query 
translation: a bilingual machine-readable diction-
ary, a statistical dictionary based on a parallel 
web corpus and the Babelfish MT service. He 
drew a conclusion that the mean average preci-
sion of a run was proportional to the lexical cov-
erage. McNamee and Mayfield (2002) examined 
the effectiveness of query expansion techniques 
by using parallel corpora and bilingual wordlists 
of varying quality. They confirmed that retrieval 
performance dropped off as the lexical coverage 
of translation resources decreased and the rela-
tionship was approximately linear. 
Previous research mainly focused on studying 
the effectiveness of bilingual wordlists or parallel 
corpora from two aspects: size and lexical cover-
age. Kraaij (2001) examined the effectiveness of 
MT system, but also from the aspect of lexical 
coverage. Why lack research on analyzing effect 
of translation quality of MT system on CLIR 
performance? The possible reason might be the 
problem on how to control the translation quality 
of the MT system as what has been done to bi-
lingual wordlists or parallel corpora. MT systems 
are usually used as black boxes in CLIR applica-
tions. It is not very clear how to degrade MT 
software because MT systems are usually opti-
mized for grammatically correct sentences rather 
than word-by-word translation. 
2.2 MT-Based Query Translation 
MT-based query translation is perhaps the most 
straightforward approach to CLIR. Compared 
with dictionary or corpus based methods, the 
advantage of MT-based query translation lies in 
that technologies integrated in MT systems, such 
as syntactic and semantic analysis, could help to 
improve the translation accuracy (Jones et al, 
1999). However, in a very long time, fewer ex-
periments with MT-based methods have been 
reported than with dictionary-based methods or 
corpus-based methods. The main reasons include: 
(1) MT systems of high quality are not easy to 
obtain; (2) MT systems are not available for 
some language pairs; (3) queries are usually 
short or even terms, which limits the effective-
ness of MT-based methods. However, recent re-
search work on CLIR shows a trend to adopt 
MT-based query translation. At the fifth NTCIR 
workshop, almost all the groups participating in 
Bilingual CLIR and Multilingual CLIR tasks 
adopt the query translation method using MT 
systems or machine-readable dictionaries (Ki-
shida et al, 2005). Recent research work also 
proves that MT-based query translation could 
achieve comparable performance to other meth-
ods (Kishida et al, 2005; Nunzio et al, 2005). 
Considering more and more MT systems are be-
ing used in CLIR, it is of significance to care-
fully analyze how the performance of MT system 
may influence the retrieval effectiveness. 
3 System Description 
3.1 The Rule-Based ECMT System 
The MT system used in this research is a rule-
based ECMT system. The translation quality of 
this ECMT system is comparable to the best 
commercial ECMT systems. The basis of the 
system is semantic transfer (Amano et al, 1989). 
Translation resources comprised in this system 
include a large dictionary and a rule base. The 
rule base consists of rules of different functions 
such as analysis, transfer and generation. 
3.2 KIDS IR System 
KIDS is an information retrieval engine that is 
based on morphological analysis (Sakai et al, 
2003). It employs the Okapi/BM25 term weight-
ing scheme, as fully described in (Robertson & 
Walker, 1999; Robertson & Sparck Jones, 1997). 
594
To focus our study on the relationship between 
MT performance and retrieval effectiveness, we 
do not use techniques such as pseudo-relevance 
feedback although they are available and are 
known to improve IR performance. 
4 Experimental Method 
To obtain MT systems of varying quality, we 
degrade the rule-based ECMT system by impair-
ing the translation resources comprised in the 
system. Then we use the degraded MT systems 
to translate the queries and evaluate the transla-
tion quality. Next, we submit the translated que-
ries to the KIDS system and evaluate the re-
trieval performance. Finally we calculate the cor-
relation between the variation of translation qual-
ity and the variation of retrieval effectiveness to 
analyze the relationship between MT perform-
ance and CLIR performance. 
4.1 Degradation of MT System 
In this research, we degrade the MT system in 
two ways. One is rule-based degradation, which 
is to decrease the size of the rule base by ran-
domly removing rules from the rule base. For 
sake of simplicity, in this research we only con-
sider transfer rules that are used for transferring 
the source language to the target language and 
keep other kinds of rules untouched. That is, we 
only consider the influence of transfer rules on 
translation quality1. We first randomly divide the 
rules into segments of equal size. Then we re-
move the segments from the rule base, one at 
each time and obtain a group of degraded rule 
bases. Afterwards, we use MT systems with the 
degraded rule bases to translate the queries and 
get groups of translated queries, which are of 
different translation quality. 
The other is dictionary-based degradation, 
which is to decrease the size of the dictionary by 
randomly removing a certain number of word 
entries from the dictionary iteratively. Function 
words are not removed from the dictionary. Us-
ing MT systems with the degraded dictionaries, 
we also obtain groups of translated queries of 
different translation quality. 
4.2 Evaluation of Performance 
We measure the performance of the MT system 
by translation quality and use NIST score as the 
evaluation measure (Doddington, 2002). The 
                                                 
1 In the following part of this paper, rules refer to transfer 
rules unless explicitly stated. 
NIST scores reported in this paper are generated 
by NIST scoring toolkit2. 
For retrieval performance, we use Mean Aver-
age Precision (MAP) as the evaluation measure 
(Voorhees, 2003). The MAP values reported in 
this paper are generated by trec_eval toolkit 3 , 
which is the standard tool used by TREC for 
evaluating an ad hoc retrieval run. 
5 Experiments 
5.1 Data 
The experiments are conducted on the TREC5&6 
Chinese collection. The collection consists of 
document set, topic set and the relevance judg-
ment file. 
The document set contains articles published 
in People's Daily from 1991 to 1993, and news 
articles released by the Xinhua News Agency in 
1994 and 1995. It includes totally 164,789 
documents. The topic set contains 54 topics. In 
the relevance judgment file, a binary indication 
of relevant (1) or non-relevant (0) is given. 
<top> 
<num> Number: CH41 
<C-title> ??????????? 
<E-title> Bridge and Tunnel Construction for 
 the Beijing-Kowloon Railroad  
<C-desc> Description: 
?????????????????? 
<E-desc> Description: 
Beijing-Kowloon Railroad, bridge, tunnel, 
connection, very large bridge 
<C-narr> Narrative: 
?????????????????? 
??????????????? 
<E-narr> Narrative: 
A relevant document discusses bridge and  
tunnel construction for the Beijing-Kowloon  
Railroad, including location, construction  
status, span or length. 
</top> 
Figure 1. Example of TREC Topic 
5.2 Query Formulation & Evaluation 
For each TREC topic, three fields are provided: 
title, description and narrative, both in Chinese 
and English, as shown in figure 1. The title field 
is the statement of the topic. The description 
                                                 
2 The toolkit could be downloaded from: 
http://www.nist.gov/speech/tests/mt/resources/scoring.htm 
3 The toolkit could be downloaded from: 
http://trec.nist.gov/trec_eval/trec_eval.7.3.tar.gz 
595
field lists some terms that describe the topic. The 
narrative field provides a complete description 
of document relevance for the assessors. In our 
experiments, we use two kinds of queries: title 
queries (use only the title field) and desc queries 
(use only the description field). We do not use 
narrative field because it is the criteria used by 
the assessors to judge whether a document is 
relevant or not, so it usually contains quite a 
number of unrelated words. 
Title queries are one-sentence queries. When 
use NIST scoring tool to evaluate the translation 
quality of the MT system, reference translations 
of source language sentences are required. NIST 
scoring tool supports multi references. In our 
experiments, we introduce two reference transla-
tions for each title query. One is the Chinese title 
(C-title) in title field of the original TREC topic 
(reference translation 1); the other is the transla-
tion of the title query given by a human transla-
tor (reference translation 2). This is to alleviate 
the bias on translation evaluation introduced by 
only one reference translation. An example of 
title query and its reference translations are 
shown in figure 2. Reference 1 is the Chinese 
title provided in original TREC topic. Reference 
2 is the human translation of the query. For this 
query, the translation output generated by the 
MT system is "???????????". If 
only use reference 1 as reference translation, the 
system output will not be regarded as a good 
translation. But in fact, it is a good translation for 
the query. Introducing reference 2 helps to alle-
viate the unfair evaluation. 
Title Query: CH27 
<query> 
Robotics Research in China 
<reference 1> 
??????????? 
<reference 2> 
???????? 
Figure 2. Example of Title Query 
A desc query is not a sentence but a string of 
terms that describes the topic. The term in the 
desc query is either a word, a phrase or a string 
of words. A desc query is not a proper input for 
the MT system. But the MT system still works. It 
translates the desc query term by term. When the 
term is a word or a phrase that exists in the dic-
tionary, the MT system looks up the dictionary 
and takes the first translation in the entry as the 
translation of the term without any further analy-
sis. When the term is a string of words such as 
"number(??) of(?) infections(??)", the sys-
tem translates the term into "????". Besides 
using the Chinese description (C-desc) in the 
description field of the original TREC topic as 
the reference translation of each desc query, we 
also have the human translator give another ref-
erence translation for each desc query. Compari-
son on the two references shows that they are 
very similar to each other. So in our final ex-
periments, we use only one reference for each 
desc query, which is the Chinese description (C-
desc) provided in the original TREC topic. An 
example of desc query and its reference transla-
tion is shown in figure 3. 
Desc Query: CH22 
<query> 
malaria, number of deaths, number of infections
<reference> 
???????????? 
Figure 3. Example of Desc Query 
5.3 Runs 
Previous studies (Kwok, 1997; Nie et al, 2000) 
proved that using words and n-grams indexes 
leads to comparable performance for Chinese IR. 
So in our experiments, we use bi-grams as index 
units. 
We conduct following runs to analyze the rela-
tionship between MT performance and CLIR 
performance: 
? rule-title: MT-based title query transla-
tion with degraded rule base 
? rule-desc: MT-based desc query transla-
tion with degraded rule base 
? dic-title: MT-based title query translation 
with degraded dictionary 
? dic-desc: MT-based desc query transla-
tion with degraded dictionary 
For baseline comparison, we conduct Chinese 
monolingual runs with title queries and desc que-
ries. 
5.4 Monolingual Performance 
The results of Chinese monolingual runs are 
shown in Table 1. 
Run MAP 
title-cn1 0.3143 
title-cn2 0.3001 
desc-cn 0.3514 
Table 1. Monolingual Results 
596
title-cn1: use reference translation 1 of each ti-
tle query as Chinese query 
title-cn2: use reference translation 2 of each ti-
tle query as Chinese query 
desc-cn: use reference translation of each desc 
query as Chinese query 
Among all the three monolingual runs, desc-cn 
achieves the best performance. Title-cn1 
achieves better performance than title-cn2, which 
indicates directly using Chinese title as Chinese 
query performs better than using human transla-
tion of title query as Chinese query. 
5.5 Results on Rule-Based Degradation 
There are totally 27,000 transfer rules in the rule 
base. We use all these transfer rules in the ex-
periment of rule-based degradation. The 27,000 
rules are randomly divided into 36 segments, 
each of which contains 750 rules. To degrade the 
rule base, we start with no degradation, then we 
remove one segment at each time, up to a com-
plete degradation with all segments removed. 
With each of the segment removed from the rule 
base, the MT system based on the degraded rule 
base produces a group of translations for the in-
put queries. The completely degraded system 
with all segments removed could produce a 
group of rough translations for the input queries. 
Figure 4 and figure 5 show the experimental 
results on title queries (rule-title) and desc que-
ries (rule-desc) respectively. 
Figure 4(a) shows the changes of translation 
quality of the degraded MT systems on title que-
ries. From the result, we observe a successive 
change on MT performance. The fewer rules, the 
worse translation quality achieves. The NIST 
score varies from 7.3548 at no degradation to 
5.9155 at complete degradation. Figure 4(b) 
shows the changes of retrieval performance by 
using the translations generated by the degraded 
MT systems as queries. The MAP varies from 
0.3126 at no degradation to 0.2810 at complete 
degradation. Comparison on figure 4(a) and 4(b) 
indicates similar variations between translation 
quality and retrieval performance. The better the 
translation quality, the better the retrieval per-
formance is. 
Figure 5(a) shows the changes of translation 
quality of the degraded MT systems on desc que-
ries. Figure 5(b) shows the corresponding 
changes of retrieval performance. We observe a 
similar relationship between MT performance 
and retrieval performance as to the results based 
5.8000
6.0000
6.2000
6.4000
6.6000
6.8000
7.0000
7.2000
7.4000
7.6000
0 4 8 12 16 20 24 28 32 36 40
MT System with Degraded Rule Base
N
IS
T
 S
co
re
Figure 4(a). MT Performance on Rule-based 
Degradation with Title Query 
4.8400
4.8600
4.8800
4.9000
4.9200
4.9400
4.9600
4.9800
5.0000
5.0200
5.0400
0 4 8 12 16 20 24 28 32 36 40
MT System with Degraded Rule Base
N
IS
T
 S
co
re
Figure 5(a). MT Performance on Rule-based 
Degradation with Desc Query 
0.2800
0.2850
0.2900
0.2950
0.3000
0.3050
0.3100
0.3150
0.3200
0 4 8 12 16 20 24 28 32 36 40
MT System with Degraded Rule Base
M
A
P
 
Figure 4(b). Retrieval Effectiveness on Rule-
based Degradation with Title Query 
0.2750
0.2770
0.2790
0.2810
0.2830
0.2850
0.2870
0.2890
0 4 8 12 16 20 24 28 32 36 40
MT System with Degraded Rule Base
M
A
P
Figure 5(b). Retrieval Effectiveness on Rule-
based Degradation with Desc Query 
597
5.8000
6.0000
6.2000
6.4000
6.6000
6.8000
7.0000
7.2000
7.4000
7.6000
0 4 8 12 16 20 24 28 32 36 40
MT System with Degraded Dcitionary
N
IS
T
 S
co
re
Figure 6(a). MT Performance on Dictionary-
based Degradation with Title Query 
4.4000
4.5000
4.6000
4.7000
4.8000
4.9000
5.0000
5.1000
0 4 8 12 16 20 24 28 32 36 40
MT System with Degraded Dictionary
N
IS
T
 S
co
re
Figure 7(a). MT Performance on Dictionary-
based Degradation with Desc Query 
0.1800
0.2000
0.2200
0.2400
0.2600
0.2800
0.3000
0.3200
0 4 8 12 16 20 24 28 32 36 40
MT System with Degraded Dcitionary
M
A
P
Figure 6(b). Retrieval Effectiveness on Diction-
ary-based Degradation with Title Query 
0.2400
0.2450
0.2500
0.2550
0.2600
0.2650
0.2700
0.2750
0.2800
0.2850
0.2900
0 4 8 12 16 20 24 28 32 36 40
MT System with Degraded Dictionary
M
A
P
Figure 7(b). Retrieval Effectiveness on Diction-
ary-based Degradation with Desc Query 
on title queries. The NIST score varies from 
5.0297 at no degradation to 4.8497 at complete 
degradation. The MAP varies from 0.2877 at no 
degradation to 0.2759 at complete degradation. 
5.6 Results on Dictionary-Based Degrada-
tion 
The dictionary contains 169,000 word entries. To 
make the results on dictionary-based degradation 
comparable to the results on rule-based degrada-
tion, we degrade the dictionary so that the varia-
tion interval on translation quality is similar to 
that of the rule-based degradation. We randomly 
select 43,200 word entries for degradation. These 
word entries do not include function words. We 
equally split these word entries into 36 segments. 
Then we remove one segment from the diction-
ary at each time until all the segments are re-
moved and obtain 36 degraded dictionaries. We 
use the MT systems with the degraded dictionar-
ies to translate the queries and observe the 
changes on translation quality and retrieval per-
formance. The experimental results on title que-
ries (dic-title) and desc queries (dic-desc) are 
shown in figure 6 and figure 7 respectively. 
From the results, we also observe a similar rela-
tionship between translation quality and retrieval 
performance as what we have observed in the 
rule-based degradation. For both title queries and 
desc queries, the larger the dictionary size, the 
better the NIST score and MAP is. For title que-
ries, the NIST score varies from 7.3548 at no 
degradation to 6.0067 at complete degradation. 
The MAP varies from 0.3126 at no degradation 
to 0.1894 at complete degradation. For desc que-
ries, the NIST score varies from 5.0297 at no 
degradation to 4.4879 at complete degradation. 
The MAP varies from 0.2877 at no degradation 
to 0.2471 at complete degradation. 
5.7 Summary of the Results 
Here we summarize the results of the four runs in 
Table 2. 
Run NIST Score MAP 
title queries 
No degradation 7.3548 0.3126
Complete: rule-title 5.9155 0.2810
Complete: dic-title 6.0067 0.1894
desc queries 
No degradation 5.0297 0.2877
Complete: rule-desc 4.8497 0.2759
Complete: dic-desc 4.4879 0.2471
Table 2. Summary of Runs 
598
6 Discussion 
Based on our observations, we analyze the corre-
lations between NIST scores and MAPs, as listed 
in Table 3. In general, there is a strong correla-
tion between translation quality and retrieval ef-
fectiveness. The correlations are above 95% for 
all of the four runs, which means in general, a 
better performance on MT will lead to a better 
performance on retrieval. 
Run Correlation 
rule-title 0.9728 
rule-desc 0.9500 
dic-title 0.9521 
dic-desc 0.9582 
Table 3. Correlation Between Translation Qual-
ity & Retrieval Effectiveness 
6.1 Impacts of Query Format 
For Chinese monolingual runs, retrieval based on 
desc queries achieves better performance than 
the runs based on title queries. This is because a 
desc query consists of terms that relate to the 
topic, i.e., all the terms in a desc query are pre-
cise query terms. But a title query is a sentence, 
which usually introduces words that are unre-
lated to the topic. 
Results on bilingual retrieval are just contrary 
to monolingual ones. Title queries perform better 
than desc queries. Moreover, MAP at no degra-
dation for title queries is 0.3126, which is about 
99.46% of the performance of monolingual run 
title-cn1, and outperforms the performance of 
title-cn2 run. But MAP at no degradation for 
desc queries is 0.2877, which is just 81.87% of 
the performance of the monolingual run desc-cn. 
Comparison on the results shows that the MT 
system performs better on title queries than on 
desc queries. This is reasonable because desc 
queries are strings of terms, however the MT 
system is optimized for grammatically correct 
sentences rather than word-by-word translation. 
Considering the correlation between translation 
quality and retrieval effectiveness, it is rational 
that title queries achieve better results on re-
trieval than desc queries. 
6.2 Impacts of Rules and Dictionary 
Table 4 shows the fall of NIST score and MAP at 
complete degradation compared with NIST score 
and MAP achieved at no degradation. 
Comparison on the results of title queries 
shows that similar variation of translation quality 
leads to quite different variation on retrieval ef-
fectiveness. For rule-title run, 19.57% reduction 
in translation quality results in 10.11% reduction 
in retrieval effectiveness. But for dic-title run, 
18.33% reduction in translation quality results in 
39.41% reduction in retrieval effectiveness. This 
indicates that retrieval effectiveness is more sen-
sitive to the size of the dictionary than to the size 
of the rule base for title queries. Why dictionary-
based degradation has stronger impact on re-
trieval effectiveness than rule-based degradation? 
This is because retrieval systems are typically 
more tolerant of syntactic than semantic transla-
tion errors (Fluhr, 1997). Therefore although 
syntactic errors caused by the degradation of the 
rule base result in a decrease of translation qual-
ity, they have smaller impacts on retrieval effec-
tiveness than the word translation errors caused 
by the degradation of dictionary. 
For desc queries, there is no big difference be-
tween dictionary-based degradation and rule-
based degradation. This is because the MT sys-
tem translates the desc queries term by term, so 
degradation of rule base mainly results in word 
translation errors instead of syntactic errors. 
Thus, degradation of dictionary and rule base has 
similar effect on retrieval effectiveness. 
Run NIST Score Fall MAP Fall 
title queries 
rule-title 19.57% 10.11% 
dic-title 18.33% 39.41% 
desc queries 
rule-desc 3.58% 4.10% 
dic-desc 10.77% 14.11% 
Table 4. Fall on Translation Quality & Retrieval 
Effectiveness 
7 Conclusion and Future Work 
In this paper, we investigated the effect of trans-
lation quality in MT-based CLIR. Our study 
showed that the performance of MT system and 
IR system correlates highly with each other. We 
further analyzed two main factors in MT-based 
CLIR. One factor is the query format. We con-
cluded that title queries are preferred for MT-
based CLIR because MT system is usually opti-
mized for translating sentences rather than words. 
The other factor is the translation resources com-
prised in the MT system. Our observation 
showed that the size of the dictionary has a 
stronger effect on retrieval effectiveness than the 
size of the rule base in MT-based CLIR. There-
fore in order to improve the retrieval effective-
ness of a MT-based CLIR application, it is more 
599
effective to develop a larger dictionary than to 
develop more rules. This introduces another in-
teresting question relating to MT-based CLIR. 
That is how CLIR can benefit further from MT. 
Directly using the translations generated by the 
MT system may not be the best choice for the IR 
system. There are rich features generated during 
the translation procedure. Will such features be 
helpful to CLIR? This question is what we would 
like to answer in our future work. 
References 
Shin-ya Amano, Hideki Hirakawa, Hirosysu Nogami, 
and Akira Kumano. 1989. The Toshiba Machine 
Translation System. Future Computing System, 
2(3):227-246. 
Dina Demner-Fushman, and Douglas W. Oard. 2003. 
The Effect of Bilingual Term List Size on Diction-
ary-Based Cross-Language Information Retrieval. 
In Proc. of the 36th Hawaii International Confer-
ence on System Sciences (HICSS-36), pages 108-
117. 
George Doddington. 2002. Automatic Evaluation of 
Machine Translation Quality Using N-gram Co-
occurrence Statistics. In Proc. of the Second Inter-
national Conference on Human Language Tech-
nology (HLT-2002), pages 138-145. 
Christian Fluhr. 1997. Multilingual Information Re-
trieval. In Ronald A Cole, Joseph Mariani, Hans 
Uszkoreit, Annie Zaenen, and Victor Zue (Eds.), 
Survey of the State of the Art in Human Language 
Technology, pages 261-266, Cambridge University 
Press, New York. 
Martin Franz, J. Scott McCarley, Todd Ward, and 
Wei-Jing Zhu. 2001. Quantifying the Utility of 
Parallel Corpora. In Proc. of the 24th Annual ACM 
Conference on Research and Development in In-
formation Retrieval (SIGIR-2001), pages 398-399. 
David A. Hull and Gregory Grefenstette. 1996. Que-
rying Across Languages: A Dictionary-Based Ap-
proach to Multilingual Information Retrieval. In 
Proc. of the 19th Annual ACM Conference on Re-
search and Development in Information Retrieval 
(SIGIR-1996), pages 49-57. 
Gareth Jones, Tetsuya Sakai, Nigel Collier, Akira 
Kumano and Kazuo Sumita. 1999. Exploring the 
Use of Machine Translation Resources for English-
Japanese Cross-Language Infromation Retrieval. In 
Proc. of MT Summit VII Workshop on Machine 
Translation for Cross Language Information Re-
trieval, pages 15-22. 
Kazuaki Kishida, Kuang-hua Chen, Sukhoon Lee, 
Kazuko Kuriyama, Noriko Kando, Hsin-Hsi Chen, 
and Sung Hyon Myaeng. 2005. Overview of CLIR 
Task at the Fifth NTCIR Workshop. In Proc. of the 
NTCIR-5 Workshop Meeting, pages 1-38. 
Wessel Kraaij. 2001. TNO at CLEF-2001: Comparing 
Translation Resources. In Proc. of the CLEF-2001 
Workshop, pages 78-93. 
Kui-Lam Kwok. 1997. Comparing Representation in 
Chinese Information Retrieval. In Proc. of the 20th 
Annual ACM Conference on Research and Devel-
opment in Information Retrieval (SIGIR-1997), 
pages 34-41. 
Paul McNamee and James Mayfield. 2002. Compar-
ing Cross-Language Query Expansion Techniques 
by Degrading Translation Resources. In Proc. of 
the 25th Annual ACM Conference on Research and 
Development in Information Retrieval (SIGIR-
2002), pages 159-166. 
Jian-Yun Nie, Jianfeng Gao, Jian Zhang, and Ming 
Zhou. 2000. On the Use of Words and N-grams for 
Chinese Information Retrieval. In Proc. of the Fifth 
International Workshop on Information Retrieval 
with Asian Languages (IRAL-2000), pages 141-148. 
Giorgio M. Di Nunzio, Nicola Ferro, Gareth J. F. 
Jones, and Carol Peters. 2005. CLEF 2005: Ad 
Hoc Track Overview. In C. Peters (Ed.), Working 
Notes for the CLEF 2005 Workshop. 
Stephen E. Robertson and Stephen Walker. 1999. 
Okapi/Keenbow at TREC-8. In Proc. of the Eighth 
Text Retrieval Conference (TREC-8), pages 151-
162. 
Stephen E. Robertson and Karen Sparck Jones. 1997. 
Simple, Proven Approaches to Text Retrieval. 
Technical Report 356, Computer Laboratory, Uni-
versity of Cambridge, United Kingdom. 
Tetsuya Sakai, Makoto Koyama, Masaru Suzuki, and 
Toshihiko Manabe. 2003. Toshiba KIDS at 
NTCIR-3: Japanese and English-Japanese IR. In 
Proc. of the Third NTCIR Workshop on Research 
in Information Retrieval, Automatic Text Summari-
zation and Question Answering (NTCIR-3), pages 
51-58. 
Ellen M. Voorhees. 2003. Overview of TREC 2003. 
In Proc. of the Twelfth Text Retrieval Conference 
(TREC 2003), pages 1-13. 
Jinxi Xu and Ralph Weischedel. 2000. Cross-lingual 
Information Retrieval Using Hidden Markov Mod-
els. In Proc. of the 2000 Joint SIGDAT Conference 
on Empirical Methods in Natural Language Proc-
essing and Very Large Corpora (EMNLP/VLC-
2000), pages 95-103. 
600
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1001?1008,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Discriminative Pruning of Language Models for 
Chinese Word Segmentation 
 
Jianfeng Li      Haifeng Wang      Dengjun Ren      Guohua Li 
Toshiba (China) Research and Development Center 
5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng District 
Beijing, 100738, China 
{lijianfeng, wanghaifeng, rendengjun, 
liguohua}@rdc.toshiba.com.cn 
 
  
 
Abstract 
This paper presents a discriminative 
pruning method of n-gram language 
model for Chinese word segmentation. 
To reduce the size of the language model 
that is used in a Chinese word segmenta-
tion system, importance of each bigram is 
computed in terms of discriminative 
pruning criterion that is related to the per-
formance loss caused by pruning the bi-
gram. Then we propose a step-by-step 
growing algorithm to build the language 
model of desired size. Experimental re-
sults show that the discriminative pruning 
method leads to a much smaller model 
compared with the model pruned using 
the state-of-the-art method. At the same 
Chinese word segmentation F-measure, 
the number of bigrams in the model can 
be reduced by up to 90%. Correlation be-
tween language model perplexity and 
word segmentation performance is also 
discussed. 
1 Introduction 
Chinese word segmentation is the initial stage of 
many Chinese language processing tasks, and 
has received a lot of attention in the literature 
(Sproat et al, 1996; Sun and Tsou, 2001; Zhang 
et al, 2003; Peng et al, 2004). In Gao et al 
(2003), an approach based on source-channel 
model for Chinese word segmentation was pro-
posed. Gao et al (2005) further developed it to a 
linear mixture model. In these statistical models, 
language models are essential for word segmen-
tation disambiguation. However, an uncom-
pressed language model is usually too large for 
practical use since all realistic applications have 
memory constraints. Therefore, language model 
pruning techniques are used to produce smaller 
models. Pruning a language model is to eliminate 
a number of parameters explicitly stored in it, 
according to some pruning criteria. The goal of 
research for language model pruning is to find 
criteria or methods, using which the model size 
could be reduced effectively, while the perform-
ance loss is kept as small as possible. 
A few criteria have been presented for lan-
guage model pruning, including count cut-off 
(Jelinek, 1990), weighted difference factor 
(Seymore and Rosenfeld, 1996), Kullback-
Leibler distance (Stolcke, 1998), rank and en-
tropy (Gao and Zhang, 2002). These criteria are 
general for language model pruning, and are not 
optimized according to the performance of lan-
guage model in specific tasks. 
In recent years, discriminative training has 
been introduced to natural language processing 
applications such as parsing (Collins, 2000), ma-
chine translation (Och and Ney, 2002) and lan-
guage model building (Kuo et al, 2002; Roark et 
al., 2004). To the best of our knowledge, it has 
not been applied to language model pruning. 
In this paper, we propose a discriminative 
pruning method of n-gram language model for 
Chinese word segmentation. It differentiates 
from the previous pruning approaches in two 
respects. First, the pruning criterion is based on 
performance variation of word segmentation. 
Second, the model of desired size is achieved by 
adding valuable bigrams to a base model, instead 
of by pruning bigrams from an unpruned model. 
We define a misclassification function that 
approximately represents the likelihood that a 
sentence will be incorrectly segmented. The 
1001
variation value of the misclassification function 
caused by adding a parameter to the base model 
is used as the criterion for model pruning. We 
also suggest a step-by-step growing algorithm 
that can generate models of any reasonably de-
sired size. We take the pruning method based on 
Kullback-Leibler distance as the baseline. Ex-
perimental results show that our method outper-
forms the baseline significantly with small model 
size. With the F-Measure of 96.33%, number of 
bigrams decreases by up to 90%. In addition, by 
combining the discriminative pruning method 
with the baseline method, we obtain models that 
achieve better performance for any model size. 
Correlation between language model perplexity 
and system performance is also discussed. 
The remainder of the paper is organized as fol-
lows. Section 2 briefly discusses the related work 
on language model pruning. Section 3 proposes 
our discriminative pruning method for Chinese 
word segmentation. Section 4 describes the ex-
perimental settings and results. Result analysis 
and discussions are also presented in this section. 
We draw the conclusions in section 5. 
2 Related Work 
A simple way to reduce the size of an n-gram 
language model is to exclude those n-grams oc-
curring infrequently in training corpus. It is 
named as count cut-off method (Jelinek, 1990). 
Because counts are always integers, the size of 
the model can only be reduced to discrete values. 
Gao and Lee (2000) proposed a distribution-
based pruning. Instead of pruning n-grams that 
are infrequent in training data, they prune n-
grams that are likely to be infrequent in a new 
document. Experimental results show that it is 
better than traditional count cut-off method. 
Seymore and Rosenfeld (1996) proposed a 
method to measure the difference of the models 
before and after pruning each n-gram, and the 
difference is computed as: 
)]|(log)|([log),( jijiij hwPhwPwhN ????  (1)
Where P(wi|hj) denotes the conditional prob-
abilities assigned by the original model, and 
P?(wi|hj) denotes the probabilities in the pruned 
model. N(hj, wi) is the discounted frequency of n-
gram event hjwi. Seymore and Rosenfeld (1996) 
showed that this method is more effective than 
the traditional cut-off method. 
Stolcke (1998) presented a more sound crite-
rion for computing the difference of models be-
fore and after pruning each n-gram, which is 
called relative entropy or Kullback-Leibler dis-
tance. It is computed as: 
? ???
ji hw
jijiji hwPhwPhwP
,
)]|(log)|()[log,(   (2) 
The sum is over all words wi and histories hj. 
This criterion removes some of the approxima-
tions employed in Seymore and Rosenfeld 
(1996). In addition, Stolcke (1998) presented a 
method for efficient computation of the Kull-
back-Leibler distance of each n-gram. 
In Gao and Zhang (2002), three measures are 
studied for the purpose of language model prun-
ing. They are probability, rank, and entropy. 
Among them, probability is very similar to that 
proposed by Seymore and Rosenfeld (1996). Gao 
and Zhang (2002) also presented a method of 
combining two criteria, and showed the combi-
nation of rank and entropy achieved the smallest 
models. 
3 Discriminative Pruning for Chinese 
Word Segmentation 
3.1 Problem Definition 
In this paper, discussions are restricted to bigram 
language model P(wy|wx). In a bigram model, 
three kinds of parameters are involved: bigram 
probability Pm(wy|wx) for seen bigram wxwy in 
training corpus, unigram probability Pm(w) and 
backoff coefficient ?m(w) for any word w. For 
any wx and wy in the vocabulary, bigram prob-
ability P(wy|wx) is computed as: 
??
?
=?
>=
0),()()(
0),()|(
)|(
yxymxm
yxxym
xy wwcifwPw
wwcifwwP
wwP ?  (3) 
As equation (3) shows, the probability of an 
unseen bigram is computed by the product of the 
unigram probability and the corresponding back-
off coefficient. If we remove a seen bigram from 
the model, we can still yield a bigram probability 
for it, by regarding it as an unseen bigram. Thus, 
we can reduce the number of bigram probabili-
ties explicitly stored in the model. By doing this, 
model size decreases. This is the foundation for 
bigram model pruning. 
The research issue is to find an effective crite-
rion to compute "importance" of each bigram. 
Here, "importance" indicates the performance 
loss caused by pruning the bigram. Generally, 
given a target model size, the method for lan-
guage model pruning is described in Figure 1. 
In fact, deciding which bigrams should be ex-
cluded from the model is equivalent to deciding 
1002
which bigrams should be included in the model. 
Hence, we suggest a growing algorithm through 
which a model of desired size can also be 
achieved. It is illustrated in Figure 2. Here, two 
terms are introduced. Full-bigram model is the 
unpruned model containing all seen bigrams in 
training corpus. And base model is currently the 
unigram model. 
For the discriminative pruning method sug-
gested in this paper, growing algorithm instead 
of pruning algorithm is applied to generate the 
model of desired size. In addition, "importance" 
of each bigram indicates the performance im-
provement caused by adding a bigram into the 
base model. 
 
Figure 1. Language Model Pruning Algorithm 
 
Figure 2. Growing Algorithm for Language 
Model Pruning 
3.2 Discriminative Pruning Criterion 
Given a Chinese character string S, a word seg-
mentation system chooses a sequence of words 
W* as the segmentation result, satisfying: 
))|(log)((logmaxarg* WSPWPW
W
+=  (4)
The sum of the two logarithm probabilities in 
equation (4) is called discriminant function: 
)|(log)(log),;,( WSPWPWSg +=??  (5)
Where ? denotes a language model that is 
used to compute P(W), and ? denotes a genera-
tive model that is used to compute P(S|W). In 
language model pruning, ? is an invariable. 
The discriminative pruning criterion is in-
spired by the comparison of segmented sentences 
using full-bigram model ?F and using base model 
?B. Given a sentence S, full-bigram model 
chooses  as the segmentation result, and base 
model chooses  as the segmentation result, 
satisfying: 
B
*
FW
*
BW
),;,(maxarg* F
W
F WSgW ??=                        (6) 
1. Given the desired model size, compute 
the number of bigrams that should be 
pruned. The number is denoted as m; 
2. Compute "importance" of each bigram; 
3. Sort all bigrams in the language model, 
according to their "importance"; 
4. Remove m most "unimportant" bigrams 
from the model; 
5. Re-compute backoff coefficients in the 
model. 
),;,(maxarg* B
W
B WSgW ??=                       (7) 
Here, given a language model ?, we define a 
misclassification function representing the differ-
ence between discriminant functions of  and 
: 
*
FW
*
BW
),;,(),;,(),;( ** ?????=?? FB WSgWSgSd            (8) 
The misclassification function reflects which 
one of  and  is inclined to be chosen as 
the segmentation result. If , we may 
extract some hints from the comparison of them, 
and select a few valuable bigrams. By adding 
these bigrams to base model, we should make the 
model choose the correct answer between  
and . If , no hints can be extracted. 
*
FW
*
BW
**
BF WW ?
*
FW
*
BW
**
BF WW =
1. Given the desired model size, compute 
the number of bigrams that should be 
added into the base model. The number 
is denoted as n; 
2. Compute "importance" of each bigram 
included in the full-bigram model but 
excluded from the base model; 
3. Sort the bigrams according to their "im-
portance"; 
4. Add n most "important" bigrams into 
the base model; 
5. Re-compute backoff coefficients in the 
base model. 
Let W0 be the known correct word sequence. 
Under the precondition , we describe 
our method in the following three cases. 
**
BF WW ?
Case 1:  and  0
* WWF = 0* WWB ?
Here, full-bigram model chooses the correct 
answer, while base model does not. Based on 
equation (6), (7) and (8), we know that d(S;?,?B) 
> 0 and d(S;?,?F) < 0. It implies that adding bi-
grams into base model may lead the misclassifi-
cation function from positive to negative. Which 
bigram should be added depends on the variation 
of misclassification function caused by adding it. 
If adding a bigram makes the misclassification 
function become smaller, it should be added with 
higher priority. 
We add each bigram individually to ?B, and 
then compute the variation of the misclassifica-
tion function. Let ?? denotes the model after add-
B
1003
ing bigram wxwy into ?BB. According to equation 
(5) and (8), we can write the misclassification 
function using ?B and ?? separately: B
)|(log)(log
)|(log)(log),;(
**
**
FFB
BBBB
WSPWP
WSPWPSd
?
?
??
+=??
     (9) 
)|(log)(log
)|(log)(log),;(
**
**
FF
BB
WSPWP
WSPWPSd
?
?
???
+?=???
       (10) 
Where PB(.), P?(.), PB
]
]
?(.) represent probabilities 
in base model, model ?? and model ? separately. 
The variation of the misclassification function is 
computed as: 
)](log)([log
)](log)([log
),;(),;();(
**
**
BBB
FBF
Byx
WPWP
WPWP
SdSdwwSd
???
??=
??????=?
      (11) 
Because the only difference between base 
model and model ?? is that model ?? involves the 
bigram probability P?(wy|wx), we have: 
)](log
)(log)|()[log,(
]|(log)|([log
)(log)(log
*
*
)1(
*
)(
*
)1(
*
)(
**
xB
yBxyyxF
i
iFiFBiFiF
FBF
w
wPwwPwwWn
wwPwwP
WPWP
??
??=
??=
??
? ??
 (12) 
Where  denotes the number of 
times the bigram w
),( * yxF wwWn
xwy appears in sequence . 
Note that in equation (12), base model is treated 
as a bigram model instead of a unigram model. 
The reason lies in two respects. First, the uni-
gram model can be regarded as a particular bi-
gram model by setting all backoff coefficients to 
1. Second, the base model is not always a uni-
gram model during the step-by-step growing al-
gorithm, which will be discussed in the next sub-
section. 
*
FW
In fact, bigram probability P?(wy|wx) is ex-
tracted from full-bigram model, so P?(wy|wx) = 
PF(wy|wx). In addition, similar deductions can be 
conducted to the second bracket in equation (11). 
Thus, we have: 
[
[ )(log)(log)|(log
),(),();( **
xByBxyF
yxByxFyx
wwPwwP
wwWnwwWnwwSd
????
?=?
 (13) 
Note that d(S;?,?) approximately indicates the 
likelihood that S will be incorrectly segmented, 
so ?d(S;wxwy) represents the performance im-
provement caused by adding wxwy. Thus, "impor-
tance" of bigram wxwy on S is computed as: 
);();( yxyx wwSdSwwimp ?=                     (14) 
Case 2: and  0
* WWF ? 0* WWB =
Here, it is just contrary to case 1. In this way, 
we have: 
);();( yxyx wwSdSwwimp ??=                   (15) 
Case 3:  *0
*
BF WWW ??
In case 1 and 2, bigrams are added so that dis-
criminant function of correct word sequence be-
comes bigger, and that of incorrect word se-
quence becomes smaller. In case 3, both  and 
 are incorrect. Thus, the misclassification 
function in equation (8) does not represent the 
likelihood that S will be incorrectly segmented. 
Therefore, variation of the misclassification 
function in equation (13) can not be used to 
measure the "importance" of a bigram. Here, sen-
tence S is ignored, and the "importance" of all 
bigrams on S are zero. 
*
FW
*
BW
The above three cases are designed for one 
sentence. The "importance" of each bigram on 
the whole training corpus is the sum of its "im-
portance" on each single sentence, as equation 
(16) shows.  
?=
S
yxyx Swwimpwwimp );()(                      (16) 
To sum up, the "importance" of each bigram is 
computed as Figure 3 shows.  
 
1. For each wxwy, set imp(wxwy) = 0; 
2. For each sentence in training corpus: 
For each wxwy: 
if W  and W : 0
* WF = B ?
F ? B =
0
* W
imp(wxwy) += ?d(S;wxwy); 
else if W and W : 0
* W 0
* W
imp(wxwy) ?= ?d(S;wxwy); 
Figure 3. Calculation of "Importance"  
of Bigrams 
We illustrate the process of computing "im-
portance" of bigrams with a simple example. 
Suppose S is "? (zhe4)? (yang4)? (cai2)?
(neng2) ? (geng4) ? (fang1) ? (bian4)". The 
segmented result using full-bigram model is "?
?(zhe4yang4)/?(cai2)/?(neng2)/?(geng4)/?
?(fang1bian4)", which is the correct word se-
quence. The segmented result using base model 
1004
is " ? ? (zhe4yang4)/ ? ? (cai2neng2)/ ?
(geng4)/ ? ? (fang1bian4)". Obviously, it 
matches case 1. For bigram "??(zhe4yang4)?
(cai2)", it occurs in  once, and does not occur 
in . According to equation (13), its "impor-
tance" on sentence S is: 
*
FW
*
BW
imp(??(zhe4yang4)?(cai2);S) 
= logPF(?(cai2)|??(zhe4yang4)) ? 
[logPB(?(cai2)) + log?B BB(??(zhe4yang4))] 
For bigram "? (geng4)?? (fang1bian4)", 
since it occurs once both in  and , its 
"importance" on S is zero. 
*
FW
*
BW
3.3 Step-by-step Growing 
Given the target model size, we can add exact 
number of bigrams to the base model at one time 
by using the growing algorithm illustrated in 
Figure 2. But it is more suitable to adopt a step-
by-step growing algorithm illustrated in Figure 4. 
As shown in equation (13), the "importance" 
of each bigram depends on the base model. Ini-
tially, the base model is set to the unigram model. 
With bigrams added in, it becomes a growing 
bigram model. Thus,  and *BW )(log xB w?  will 
change. So, the added bigrams will affect the 
calculation of "importance" of bigrams to be 
added. Generally, adding more bigrams at one 
time will lead to more negative impacts. Thus, it 
is expected that models produced by step-by-step 
growing algorithm may achieve better perform-
ance than growing algorithm, and smaller step 
size will lead to even better performance. 
 
Figure 4. Step-by-step Growing Algorithm 
4 Experiments 
4.1 Experiment Settings 
The training corpus comes from People's daily 
2000, containing about 25 million Chinese char-
acters. It is manually segmented into word se-
quences, according to the word segmentation 
specification of Peking University (Yu et al, 
2003). The testing text that is provided by Peking 
University comes from the second international 
Chinese word segmentation bakeoff organized 
by SIGHAN. The testing text is a part of Peo-
ple's daily 2001, consisting of about 170K Chi-
nese characters. 
The vocabulary is automatically extracted 
from the training corpus, and the words occur-
ring only once are removed. Finally, about 67K 
words are included in the vocabulary. The full-
bigram model and the unigram model are trained 
by CMU language model toolkit (Clarkson and 
Rosenfeld, 1997). Without any count cut-off, the 
full-bigram model contains about 2 million bi-
grams. 
The word segmentation system is developed 
based on a source-channel model similar to that 
described in (Gao et al, 2003). Viterbi algorithm 
is applied to find the best word segmentation 
path. 
4.2 Evaluation Metrics 
The language models built in our experiments 
are evaluated by two metrics. One is F-Measure 
of the word segmentation result; the other is lan-
guage model perplexity. 
For F-Measure evaluation, we firstly segment 
the raw testing text using the model to be evalu-
ated. Then, the segmented result is evaluated by 
comparing with the gold standard set. The 
evaluation tool is also from the word segmenta-
tion bakeoff. F-Measure is calculated as: 
1. Given step size s; 
2. Set the base model to be the unigram 
model; 
3. Segment corpus with full-bigram model; 
4. Segment corpus with base model; 
5. Compute "importance" of each bigram 
included in the full-bigram model but ex-
cluded from the base model; 
6. Sort the bigrams according to their "im-
portance"; 
7. Add s bigrams with the biggest "impor-
tance" to the base model; 
8. Re-compute backoff coefficients in the 
base model; 
9. If the base model is still smaller than the 
desired size, go to step 4; otherwise, stop.
F-Measure
RecallPrecision
RecallPrecision2
+
??=           (17) 
For perplexity evaluation, the language model 
to be evaluated is used to provide the bigram 
probabilities for each word in the testing text. 
The perplexity is the mean logarithm probability 
as shown in equation (18): 
?= ??=
N
i ii
wwP
NMPP 1
12 )|(log
1
2)(                       (18) 
4.3 Comparison of Pruning Methods 
The Kullback-Leibler Distance (KLD) based 
method is the state-of-the-art method, and is 
1005
taken as the baseline1. Pruning algorithm illus-
trated in Figure 1 is used for KLD based pruning. 
Growing algorithms illustrated in Figure 2 and 
Figure 4 are used for discriminative pruning 
method. Growing algorithms are not applied to 
KLD based pruning, because the computation of 
KLD is independent of the base model. 
At step 1 for KLD based pruning, m is set to 
produce ten models containing 10K, 20K, ?, 
100K bigrams. We apply each of the models to 
the word segmentation system, and evaluate the 
segmented results with the evaluation tool. The 
F-Measures of the ten models are illustrated in 
Figure 5, denoted by "KLD". 
For the discriminative pruning criterion, the 
growing algorithm illustrated in Figure 2 is 
firstly used. Unigram model acts as the base 
model. At step 1, n is set to 10K, 20K, ?, 100K 
separately. At step 2, "importance" of each bi-
gram is computed following Figure 3. Ten mod-
els are produced and evaluated. The F-Measures 
are also illustrated in Figure 5, denoted by "Dis-
crim". 
By adding bigrams step by step as illustrated 
in Figure 4, and setting step size to 10K, 5K, and 
2K separately, we obtain other three series of 
models, denoted by "Step-10K", "Step-5K" and 
"Step-2K" in Figure 5. 
We also include in Figure 5 the performance 
of the count cut-off method. Obviously, it is infe-
rior to other methods. 
96.0
96.1
96.2
96.3
96.4
96.5
96.6
1 2 3 4 5 6 7 8 9 10
Bigram Num(10K)
F-
Me
as
ur
e(
%)
KLD Discrim
Step-10K Step-5K
Step-2K Cut-off
 
Figure 5. Performance Comparison of Different 
Pruning Methods 
First, we compare the performance of "KLD" 
and "Discrim". When the model size is small, 
                                                 
1 Our pilot study shows that the method based on Kullback-
Leibler distance outperforms methods based on other crite-
ria introduced in section 2. 
such as those models containing less than 70K 
bigrams, the performance of "Discrim" is better 
than "KLD". For the models containing more 
than 70K bigrams, "KLD" gets better perform-
ance than "Discrim". The reason is that the added 
bigrams affect the calculation of "importance" of 
bigrams to be added, which has been discussed 
in section 3.3. 
If we add the bigrams step by step, better per-
formance is achieved. From Figure 5, it can be 
seen that all of the models generated by step-by-
step growing algorithm outperform "KLD" and 
"Discrim" consistently. Compared with the base-
line KLD based method, step-by-step growing 
methods result in at least 0.2 percent improve-
ment for each model size. 
Comparing "Step-10K", "Step-5K" and "Step-
2K", they perform differently before the 60K-
bigram point, and perform almost the same after 
that. The reason is that they are approaching their 
saturation states, which will be discussed in sec-
tion 4.5. Before 60K-bigram point, smaller step 
size yields better performance. 
An example of detailed comparison result is 
shown in Table 1, where the F-Measure is 
96.33%. The last column shows the relative 
model sizes with respect to the KLD pruned 
model. It shows that with the F-Measure of 
96.33%, number of bigrams decreases by up to 
90%. 
 # of bigrams % of KLD 
KLD 100,000   100%   
Step-10K 25,000   25%   
Step-5K 15,000   15%   
Step-2K 10,000   10%   
Table 1. Comparison of Number of Bigrams  
at F-Measure 96.33% 
4.4 Correlation between Perplexity and F-
Measure 
Perplexities of the models built above are evalu-
ated over the gold standard set. Figure 6 shows 
how the perplexities vary with the bigram num-
bers in models. Here, we notice that the KLD 
models achieve the lowest perplexities. It is not a 
surprising result, because the goal of KLD based 
pruning is to minimize the Kullback-Leibler dis-
tance that can be interpreted as a relative change 
of perplexity (Stolcke, 1998). 
Now we compare Figure 5 and Figure 6. Per-
plexities of KLD models are much lower than 
that of the other models, but their F-Measures are 
much worse than that of step-by-step growing 
1006
models. It implies that lower perplexity does not 
always lead to higher F-Measure. 
However, when the comparison is restricted in 
a single pruning method, the case is different. 
For each pruning method, as more bigrams are 
included in the model, the perplexity curve falls, 
and the F-Measure curve rises. It implies there 
are correlations between them. We compute the 
Pearson product-moment correlation coefficient 
for each pruning method, as listed in Table 2. It 
shows that the correlation between perplexity 
and F-Measure is very strong. 
To sum up, the correlation between language 
model perplexity and system performance (here 
represented by F-Measure) depends on whether 
the models come from the same pruning method. 
If so, the correlation is strong. Otherwise, the 
correlation is weak. 
300
350
400
450
500
550
600
650
700
1 2 3 4 5 6 7 8 9 10
Bigram Num(10K)
Pe
rp
le
xi
ty
KLD Discrim
Step-10K Step-5K
Step-2K Cut-off
 
Figure 6. Perplexity Comparison of Different 
Pruning Methods 
Pruning Method Correlation 
Cut-off -0.990 
KLD -0.991 
Discrim -0.979 
Step-10K -0.985 
Step-5K -0.974 
Step-2K -0.995 
Table 2. Correlation between Perplexity  
and F-Measure 
4.5 Combination of Saturated Model and 
KLD 
The above experimental results show that step-
by-step growing models achieve the best per-
formance when less than 100K bigrams are 
added in. Unfortunately, they can not grow up 
into any desired size. A bigram has no chance to 
be added into the base model, unless it appears in 
the mis-aligned part of the segmented corpus, 
where ? . It is likely that not all bigrams 
have the opportunity. As more and more bigrams 
are added into the base model, the segmented 
training corpus using the current base model ap-
proaches to that using the full-bigram model. 
Gradually, none bigram can be added into the 
current base model. At that time, the model stops 
growing, and reaches its saturation state. The 
model that reaches its saturation state is named 
as saturated model. In our experiments, three 
step-by-step growing models reach their satura-
tion states when about 100K bigrams are added 
in. 
*
FW
*
BW
By combining with the baseline KLD based 
method, we obtain models that outperform the 
baseline for any model size. We combine them 
as follows. If the desired model size is smaller 
than that of the saturated model, step-by-step 
growing is applied. Otherwise, Kullback-Leibler 
distance is used for further growing over the 
saturated model. For instance, by growing over 
the saturated model of "Step-2K", we obtain 
combined models containing from 100K to 2 
million bigrams. The performance of the com-
bined models and that of the baseline KLD mod-
els are illustrated in Figure 7. It shows that the 
combined model performs consistently better 
than KLD model over all of bigram numbers. 
Finally, the two curves converge at the perform-
ance of the full-bigram model. 
96.3
96.4
96.5
96.6
96.7
96.8
96.9
97.0
10 30 50 70 90 11
0
13
0
15
0
17
0
19
0
20
7
Bigram Num(10K)
F-
Me
as
ur
e(
%)
KLD
Combined Model
 
Figure 7. Performance Comparison of Combined 
Model and KLD Model 
5 Conclusions and Future Work 
A discriminative pruning criterion of n-gram lan-
guage model for Chinese word segmentation was 
proposed in this paper, and a step-by-step grow-
ing algorithm was suggested to generate the 
model of desired size based on a full-bigram 
model and a base model. Experimental results 
1007
showed that the discriminative pruning method 
achieves significant improvements over the base-
line KLD based method. At the same F-measure, 
the number of bigrams can be reduced by up to 
90%. By combining the saturated model and the 
baseline KLD based method, we achieved better 
performance for any model size. Analysis shows 
that, if the models come from the same pruning 
method, the correlation between perplexity and 
performance is strong. Otherwise, the correlation 
is weak. 
The pruning methods discussed in this paper 
focus on bigram pruning, keeping unigram prob-
abilities unchanged. The future work will attempt 
to prune bigrams and unigrams simultaneously, 
according to a same discriminative pruning crite-
rion. And we will try to improve the efficiency of 
the step-by-step growing algorithm. In addition, 
the method described in this paper can be ex-
tended to other applications, such as IME and 
speech recognition, where language models are 
applied in a similar way. 
References 
Philip Clarkson and Ronald Rosenfeld. 1997. Statisti-
cal Language Modeling Using the CMU-
Cambridge Toolkit. In Proc. of the 5th European 
Conference on Speech Communication and Tech-
nology (Eurospeech-1997), pages 2707-2710. 
Michael Collins. 2000. Discriminative Reranking for 
Natural Language Parsing. In Machine Learning: 
Proc. of 17th International Conference (ICML-
2000), pages 175-182. 
Jianfeng Gao and Kai-Fu Lee. 2000. Distribution-
based pruning of backoff language models. In Proc. 
of the 38th Annual Meeting of Association for Com-
putational Linguistics (ACL-2000), pages 579-585. 
Jianfeng Gao, Mu Li, and Chang-Ning Huang. 2003. 
Improved Source-channel Models for Chinese 
Word Segmentation. In Proc. of the 41st Annual 
Meeting of Association for Computational Linguis-
tics (ACL-2003), pages 272-279. 
Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning 
Huang. 2005. Chinese Word Segmentation and 
Named Entity Recognition: A Pragmatic Approach. 
Computational Linguistics, 31(4): 531-574. 
Jianfeng Gao and Min Zhang. 2002. Improving Lan-
guage Model Size Reduction using Better Pruning 
Criteria. In Proc. of the 40th Annual Meeting of the 
Association for Computational Linguistics (ACL-
2002), pages 176-182. 
Fredrick Jelinek. 1990. Self-organized language mod-
eling for speech recognition. In Alexander Waibel 
and Kai-Fu Lee (Eds.), Readings in Speech Recog-
nition, pages 450-506. 
Hong-Kwang Jeff Kuo, Eric Fosler-Lussier, Hui Jiang, 
and Chin-Hui Lee. 2002. Discriminative Training 
of Language Models for Speech Recognition. In 
Proc. of the 27th International Conference On 
Acoustics, Speech and Signal Processing (ICASSP-
2002), pages 325-328. 
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native Training and Maximum Entropy Models for 
Statistical Machine Translation. In Proc. of the 40th 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2002), pages 295-302. 
Fuchun Peng, Fangfang Feng, and Andrew McCallum. 
2004. Chinese Segmentation and New Word De-
tection using Conditional Random Fields. In Proc. 
of the 20th International Conference on Computa-
tional Linguistics (COLING-2004), pages 562-568. 
Brian Roark, Murat Saraclar, Michael Collins, and 
Mark Johnson. 2004. Discriminative Language 
Modeling with Conditional Random Fields and the 
Perceptron Algorithm. In Proc. of the 42nd Annual 
Meeting of the Association for Computational Lin-
guistics (ACL-2004), pages 47-54. 
Kristie Seymore and Ronald Rosenfeld. 1996. Scal-
able Backoff Language Models. In Proc. of the 4th 
International Conference on Spoken Language 
Processing (ICSLP-1996), pages. 232-235. 
Richard Sproat, Chilin Shih, William Gale, and 
Nancy Chang. 1996. A Stochastic Finite-state 
Word-segmentation Algorithm for Chinese. Com-
putational Linguistics,  22(3): 377-404. 
Andreas Stolcke. 1998. Entropy-based Pruning of 
Backoff Language Models. In Proc. of DARPA 
News Transcription and Understanding Workshop, 
pages 270-274. 
Maosong Sun and Benjamin K. Tsou. 2001. A Re-
view and Evaluation on Automatic Segmentation 
of Chinese. Contemporary Linguistics, 3(1): 22-32. 
Shiwen Yu, Huiming Duan, Xuefeng Zhu, Bin Swen, 
and Baobao Chang. 2003. Specification for Corpus 
Processing at Peking University: Word Segmenta-
tion, POS Tagging and Phonetic Notation. Journal 
of Chinese Language and Computing, 13(2): 121-
158. 
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and 
Qun Liu. 2003. HHMM-based Chinese Lexical 
Analyzer ICTCLAS, In Proc. of the ACL-2003 
Workshop on Chinese Language Processing 
(SIGHAN), pages 184-187. 
1008
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 874?881,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Word Alignment for Languages with Scarce Resources 
Using Bilingual Corpora of Other Language Pairs 
 
Haifeng Wang      Hua Wu      Zhanyi Liu 
Toshiba (China) Research and Development Center 
5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng District 
Beijing, 100738, China 
{wanghaifeng, wuhua, liuzhanyi}@rdc.toshiba.com.cn 
 
  
 
Abstract 
This paper proposes an approach to im-
prove word alignment for languages with 
scarce resources using bilingual corpora 
of other language pairs. To perform word 
alignment between languages L1 and L2, 
we introduce a third language L3. Al-
though only small amounts of bilingual 
data are available for the desired lan-
guage pair L1-L2, large-scale bilingual 
corpora in L1-L3 and L2-L3 are available. 
Based on these two additional corpora 
and with L3 as the pivot language, we 
build a word alignment model for L1 and 
L2. This approach can build a word 
alignment model for two languages even 
if no bilingual corpus is available in this 
language pair. In addition, we build an-
other word alignment model for L1 and 
L2 using the small L1-L2 bilingual cor-
pus. Then we interpolate the above two 
models to further improve word align-
ment between L1 and L2. Experimental 
results indicate a relative error rate reduc-
tion of 21.30% as compared with the 
method only using the small bilingual 
corpus in L1 and L2. 
1 Introduction 
Word alignment was first proposed as an inter-
mediate result of statistical machine translation 
(Brown et al, 1993). Many researchers build 
alignment links with bilingual corpora (Wu, 
1997; Och and Ney, 2003; Cherry and Lin, 2003; 
Zhang and Gildea, 2005). In order to achieve 
satisfactory results, all of these methods require a 
large-scale bilingual corpus for training. When 
the large-scale bilingual corpus is unavailable, 
some researchers acquired class-based alignment 
rules with existing dictionaries to improve word 
alignment (Ker and Chang, 1997). Wu et al 
(2005) used a large-scale bilingual corpus in 
general domain to improve domain-specific word 
alignment when only a small-scale domain-
specific bilingual corpus is available. 
This paper proposes an approach to improve 
word alignment for languages with scarce re-
sources using bilingual corpora of other language 
pairs. To perform word alignment between lan-
guages L1 and L2, we introduce a third language 
L3 as the pivot language. Although only small 
amounts of bilingual data are available for the 
desired language pair L1-L2, large-scale bilin-
gual corpora in L1-L3 and L2-L3 are available. 
Using these two additional bilingual corpora, we 
train two word alignment models for language 
pairs L1-L3 and L2-L3, respectively. And then, 
with L3 as a pivot language, we can build a word 
alignment model for L1 and L2 based on the 
above two models. Here, we call this model an 
induced model. With this induced model, we per-
form word alignment between languages L1 and 
L2 even if no parallel corpus is available for this 
language pair. In addition, using the small bilin-
gual corpus in L1 and L2, we train another word 
alignment model for this language pair. Here, we 
call this model an original model. An interpo-
lated model can be built by interpolating the in-
duced model and the original model. 
As a case study, this paper uses English as the 
pivot language to improve word alignment be-
tween Chinese and Japanese. Experimental re-
sults show that the induced model performs bet-
ter than the original model trained on the small 
Chinese-Japanese corpus. And the interpolated 
model further improves the word alignment re-
sults, achieving a relative error rate reduction of 
874
21.30% as compared with results produced by 
the original model. 
The remainder of this paper is organized as 
follows. Section 2 discusses the related work. 
Section 3 introduces the statistical word align-
ment models. Section 4 describes the parameter 
estimation method using bilingual corpora of 
other language pairs. Section 5 presents the in-
terpolation model. Section 6 reports the experi-
mental results. Finally, we conclude and present 
the future work in section 7. 
2 Related Work 
A shared task on word alignment was organized 
as part of the ACL 2005 Workshop on Building 
and Using Parallel Texts (Martin et al, 2005). 
The focus of the task was on languages with 
scarce resources. Two different subtasks were 
defined: Limited resources and Unlimited re-
sources. The former subtask only allows partici-
pating systems to use the resources provided. 
The latter subtask allows participating systems to 
use any resources in addition to those provided. 
For the subtask of unlimited resources, As-
wani and Gaizauskas (2005) used a multi-feature 
approach for many-to-many word alignment on 
English-Hindi parallel corpora. This approach 
performed local word grouping on Hindi sen-
tences and used other methods such as dictionary 
lookup, transliteration similarity, expected Eng-
lish words, and nearest aligned neighbors. Martin 
et al (2005) reported that this method resulted in 
absolute improvements of up to 20% as com-
pared with the case of only using limited re-
sources. Tufis et al (2005) combined two word 
aligners: one is based on the limited resources 
and the other is based on the unlimited resources.  
The unlimited resource consists of a translation 
dictionary extracted from the alignment of Ro-
manian and English WordNet. Lopez and Resnik 
(2005) extended the HMM model by integrating 
a tree distortion model based on a dependency 
parser built on the English side of the parallel 
corpus. The latter two methods produced compa-
rable results with those methods using limited 
resources. All the above three methods use some 
language dependent resources such as dictionary, 
thesaurus, and dependency parser. And some 
methods, such as transliteration similarity, can 
only be used for very similar language pairs. 
In this paper, besides the limited resources for 
the given language pair, we make use of large 
amounts of resources available for other lan-
guage pairs to address the alignment problem for 
languages with scarce resources. Our method 
does not need language-dependent resources or 
deep linguistic processing. Thus, it is easy to 
adapt to any language pair where a pivot lan-
guage and corresponding large-scale bilingual 
corpora are available. 
3 Statistical Word Alignment 
According to the IBM models (Brown et al, 
1993), the statistical word alignment model can 
be generally represented as in equation (1).  
?=
a'
c|f,a'
c|fa,
c|fa,
)Pr(
)Pr(
)Pr(  
(1)
Where,  and  represent the source sentence 
and the target sentence, respectively
c f
1. 
In this paper, we use a simplified IBM model 
4 (Al-Onaizan et al, 1999), which is shown in 
equation (2). This version does not take into ac-
count word classes in Brown et al (1993). 
))))(()](([            
))()](([(           
)|( )|(             
   
         
)Pr(
0,1
1
0,1
11
11
1
2
0
0
0 00
?
?
??
?=
>
?=
?
==
?
???
+??=
??
????
?
???
? ?=
m
aj
j
m
aj
ij
m
j
aj
l
i
ii
m
j
j
j
jpjdahj
jdahj
cftcn
pp
m
?
?
?
? ??
c|fa,
 
(2)
ml,  are the lengths of the source sentence and 
the target sentence respectively. 
j  is the position index of the target word. 
ja  is the position of the source word aligned to 
the jth target word. 
i?  is the fertility of . ic
0p ,  are the fertility probabilities for , 
and 
1p 0c
110 =+ pp . 
)|
jaj
ct(f  is the word translation probability. 
)|( ii cn ?  is the fertility probability. 
)( 11 ?? ijd ?  is the distortion probability for the 
head word of the cept. 
))((1 jpjd ?>  is the distortion probability for 
the non-head words of the cept. 
                                                 
1 This paper uses c and f to represent a Chinese sentence 
and a Japanese sentence, respectively. And e represents an 
English sentence. 
875
}:{min)( k
k
aikih == is the head of cept i. 
}:{max)( kj
jk
aakjp ==
<
. 
i?  is the center of cept i. 
During the training process, IBM model 3 is 
first trained, and then the parameters in model 3 
are employed to train model 4. For convenience, 
we describe model 3 in equation (3). The main 
difference between model 3 and model 4 lies in 
the calculation of distortion probability. 
??
??
?==
==
?
?
??
????
?
???
? ?=
m
aj
j
m
j
aj
l
i
i
l
i
ii
m
j
j
mlajdcft
cn
pp
m
0,11
11
1
2
0
0
0
),,|()|(                   
!  )|(                   
   
)Pr( 00
??
?
? ??c|fa,
 
(3)
4 Parameter Estimation Using Bilingual 
Corpora of Other Language Pairs 
As shown in section 3, the word alignment 
model mainly has three kinds of parameters that 
must be specified, including the translation prob-
ability, the fertility probability, and the distortion 
probability. The parameters are usually estimated 
by using bilingual sentence pairs in the desired 
languages, namely Chinese and Japanese here. In 
this section, we describe how to estimate the pa-
rameters without using the Chinese-Japanese 
bilingual corpus. We introduce English as the 
pivot language, and use the Chinese-English and 
English-Japanese bilingual corpora to estimate 
the parameters of the Chinese-Japanese word 
alignment model. With these two corpora, we 
first build Chinese-English and English-Japanese 
word alignment models as described in section 3. 
Then, based on these two models, we estimate 
the parameters of Chinese-Japanese word align-
ment model. The estimated model is named in-
duced model. 
The following subsections describe the 
method to estimate the parameters of Chinese-
Japanese alignment model. For reversed Japa-
nese-Chinese word alignment, the parameters 
can be estimated with the same method. 
4.1  Translation Probability 
Basic Translation Probability  
We use the translation probabilities trained 
with Chinese-English and English-Japanese cor-
pora to estimate the Chinese-Japanese probabil-
ity as shown in equation (4). In (4), we assume 
that the translation probability  is 
independent of the Chinese word . 
),|(EJ ikj ceft
ic
)|()|(     
)|(),|(      
)|(
CEEJ
CEEJ
CJ
ik
e
kj
ik
e
ikj
ij
ceteft
cetceft
cft
k
k
?
?
?=
?=  
(4)
Where  is the translation probability 
for Chinese-Japanese word alignment. 
is the translation probability trained 
using the English-Japanese corpus.  is 
the translation probability trained using the Chi-
nese-English corpus. 
)|(CJ ij cft
)|(EJ kj eft
)|(CE ik cet
Cross-Language Word Similarity 
In any language, there are ambiguous words 
with more than one sense. Thus, some noise may 
be introduced by the ambiguous English word 
when we estimate the Chinese-Japanese transla-
tion probability using English as the pivot lan-
guage. For example, the English word "bank" has 
at least two senses, namely: 
bank1 - a financial organization 
bank2 - the border of a river 
Let us consider the Chinese word: 
?? - bank2 (the border of a river) 
And the Japanese word: 
?? - bank1 (a financial organization) 
In the Chinese-English corpus, there is high 
probability that the Chinese word "??(bank2)"  
would be translated into the English word "bank". 
And in the English-Japanese corpus, there is also 
high probability that the English word "bank" 
would be translated into the Japanese word "?
?(bank1)". 
As a result, when we estimate the translation 
probability using equation (4), the translation 
probability of "?? (bank1)" given "??
(bank2)" is high. Such a result is not what we 
expect. 
In order to alleviate this problem, we intro-
duce cross-language word similarity to improve 
translation probability estimation in equation (4). 
The cross-language word similarity describes 
how likely a Chinese word is to be translated into 
a Japanese word with an English word as the 
pivot. We make use of both the Chinese-English 
corpus and the English-Japanese corpus to calcu-
late the cross language word similarity between a 
Chinese word c and a Japanese word f given an 
876
Input: An English word e , a Chinese word , and a Japanese word ; c f
The Chinese-English corpus; The English-Japanese corpus. 
(1) Construct Set 1: identify those Chinese-English sentence pairs that include the given Chinese 
word  and English word , and put the English sentences in the pairs into Set 1. c e
(2) Construct Set 2: identify those English-Japanese sentence pairs that include the given English 
word  and Japanese word , and put the English sentences in the pairs into Set 2. e f
(3) Construct the feature vectors  and  of the given English word using all other words as 
context in Set 1 and Set 2, respectively. 
CEV EJV
>=< ),(, ... ),,(),,( 1122111CE nn ctectecteV  
>=< ),(, ... ),,(),,( 2222211EJ nn ctectecteV  
Where  is the frequency of the context word . ijct je 0=ijct  if  does not occur in Set i . je
(4) Given the English word e , calculate the cross-language word similarity between the Chinese 
word  and the Japanese word  as in equation (5) c f
??
?
?
?
==
j
j
j
j
j
jj
ctct
ctct
VVefcsim
2
2
2
1
21
EJCE
)()(
),cos();,(                                     (5) 
Output: The cross language word similarity  of the Chinese word c and the Japanese 
word given the English word  
);,( efcsim
f e
Figure 1. Similarity Calculation 
English word e. For the ambiguous English word 
e, both the Chinese word c and the Japanese 
word f can be translated into e. The sense of an 
instance of the ambiguous English word e can be 
determined by the context in which the instance 
appears. Thus, the cross-language word similar-
ity between the Chinese word c and the Japanese 
word f can be calculated according to the con-
texts of their English translation e. We use the 
feature vector constructed using the context 
words in the English sentence to represent the 
context. So we can calculate the cross-language 
word similarity using the feature vectors. The 
detailed algorithm is shown in figure 1. This idea 
is similar to translation lexicon extraction via a 
bridge language (Schafer and Yarowsky, 2002). 
For example, the Chinese word "??" and its 
English translation "bank" (the border of a river) 
appears in the following Chinese-English sen-
tence pair: 
(a) ?????????? 
(b) They walked home along the river bank. 
The Japanese word "??" and its English 
translation "bank" (a financial organization) ap-
pears in the following English-Japanese sentence 
pair: 
(c) He has plenty of money in the bank. 
(d) ???????????? 
The context words of the English word "bank" in 
sentences (b) and (c) are quite different. The dif-
ference indicates the cross language word simi-
larity of the Chinese word "??" and the Japa-
nese word "??" is low. So they tend to have 
different senses. 
Translation Probability Embedded with Cross 
Language Word Similarity 
Based on the cross language word similarity 
calculation in equation (5), we re-estimate the 
translation probability as shown in (6). Then we 
normalize it in equation (7). 
The word similarity of the Chinese word "?
? (bank2)" and the Japanese word " ? ?
(bank1)" given the word English word "bank" is 
low. Thus, using the updated estimation method, 
the translation probability of "?? (bank1)" 
given "??(bank2)" becomes low. 
));,()|()|((
)|('
CEEJ
CJ
kjiik
e
kj
ij
efcsimceteft
cft
k
??= ?
 
(6)
?=
'
CJ
CJ
CJ )|'('
)|('
)|(
f
i
ij
ij cft
cft
cft  (7)
4.2  Fertility Probability 
The induced fertility probability is calculated as 
shown in (8). Here, we assume that the probabil-
877
ity ),|(EJ iki cen ?  is independent of the Chinese 
word . ic
)|()|(
)|(),|(
)|(
CEEJ
CEEJ
CJ
ik
e
ki
ik
e
iki
ii
ceten
cetcen
cn
k
k
?=
?=
?
?
?
?
?
 
(8)
Where, )|(CJ ii cn ?  is the fertility probability for 
the Chinese-Japanese alignment. )|(EJ ki en ?  is 
the trained fertility probability for the English-
Japanese alignment. 
4.3  Distortion Probability in Model 3 
With the English language as a pivot language, 
we calculate the distortion probability of model 3. 
For this probability, we introduce two additional 
parameters: one is the position of English word 
and the other is the length of English sentence. 
The distortion probability is estimated as shown 
in (9). 
)),,|Pr(),,,|Pr(         
),,,,|(Pr(
),,|,Pr(),,,,|Pr(
),,|,,Pr(
),,|(
,
,
,
CJ
mlinmlink
mlinkj
mlinkmlinkj
mlinkj
mlijd
nk
nk
nk
?
?=
?=
=
?
?
?
 
(9)
Where, is the estimated distortion 
probability.  is the introduced position of an 
English word. n  is the introduced length of an 
English sentence.  
).,|(CJ mlijd
k
In the above equation, we assume that the po-
sition probability  is independent 
of the position of the Chinese word and the 
length of the Chinese sentence. And we assume 
that the position probability  is in-
dependent of the length of Japanese sentence. 
Thus, we rewrite these two probabilities as fol-
lows. 
),,,,|Pr( mlinkj
),,,|Pr( mlink
),,|(),,|Pr(),,,,|Pr( EJ mnkjdmnkjmlinkj =?  
),,|(),,|Pr(),,,|Pr( CE nlikdnliknmlik =?  
For the length probability, the English sen-
tence length n  is independent of the word posi-
tions i . And we assume that it is uniformly dis-
tributed. Thus, we take it as a constant, and re-
write it as follows.  
constant),|Pr(),,|Pr( == mlnmlin  
According to the above three assumptions, we 
ignore the length probability . Equa-
tion (9) is rewritten in (10).  
),|Pr( mln
? ?=
nk
nlikdmnkjd
mlijd
,
CEEJ
CJ
),,|(),,|(
).,|(
 (10)
4.4  Distortion Probability in Model 4 
In model 4, there are two parameters for the dis-
tortion probability: one for head words and the 
other for non-head words.  
Distortion Probability for Head Words 
The distortion probability for head 
words represents the relative position of the head 
word of the i
)( 11 ?? ijd ?
th cept and the center of the (i-1)th 
cept. Let 1??=? ijj ? , then  is independent of 
the absolute position. Thus, we estimate the dis-
tortion probability by introducing another rela-
tive position 
j?
'j? of English words, which is 
shown in (11).    
?
?
?
????=
?=?
'
EJCE,1
1CJ,1
)'|(Pr)'(
)(
j
i
jjjd
jjd ?
 (11)
Where, )( 1CJ1, ??=? ijjd ? is the estimated dis-
tortion probability for head words in Chinese-
Japanese alignment. is the distortion 
probability for head word in Chinese-English 
alignment. 
)'(CE1, jd ?
)'|(PrEJ jj ??  is the translation prob-
ability of relative Japanese position given rela-
tive English position.  
In order to simplify , we introduce 
and  and let 
)'|(PrEJ jj ??
'j 1'?i? 1''' ??=? ijj ? , where  and 
 are positions of English words. We rewrite 
'j
1'?i?
)'|(PrEJ jj ??  in (12).   
?
?=?
?=?
??
??
??
??
=
??=
??
'':,'
:,
1'1EJ
1'1EJ
EJ
1'1'
11
),'|,(Pr
)'|(Pr
)'|(Pr
jjj
jjj
ii
ii
ii
ii
jj
jj
jj
??
??
??
??  
(12)
The English word in position  is aligned to 
the Japanese word in position , and the English 
word in position  is aligned to the Japanese 
word in position . 
'j
j
1'?i?
1?i?
We assume that  and  are independent, 
 only depends on , and  only depends 
on . Then  can be esti-
mated as shown in (13). 
j 1?i?
j 'j 1?i?
1'?i? ),'|,(Pr 1'1EJ ?? ii jj ??
878
)|(Pr)'|(Pr
),'|,(Pr
1'1EJEJ
1'1EJ
??
??
?= ii
ii
jj
jj
??
??
 (13)
Both of the two parameters in (13) represent 
the position translation probabilities. Thus, we 
can estimate them from the distortion probability 
in model 3.  is estimated as shown in 
(14).  And  can be estimated in 
the same way. In (14), we also assume that the 
sentence length distribution  is inde-
pendent of the word position and that it is uni-
formly distributed. 
)'|(PrEJ jj
)|(Pr 1'1EJ ?? ii ??
)'|,Pr( jml
?
?
?
=
?=
=
ml
ml
ml
mljjd
jmlmljjd
jmljjj
,
EJ
,
EJ
,
EJEJ
),,'|(
)'|,Pr(),,'|(
)'|,,(Pr)'|(Pr
 (14)
Distortion Probability for Non-Head Words 
The distortion probability de-
scribes the distribution of the relative position of 
non-head words. In the same way, we introduce 
relative position of English words, and model 
the probability in (15). 
))((1 jpjd ?>
'j?
?
?
>
>
????=
?=?
'
EJCE,1
CJ,1
)'|(Pr)'(
))((
j
jjjd
jpjjd
 (15)
))((CJ1, jpjjd ?=?> is the estimated distortion 
probability for the non-head words in Chinese-
Japanese alignment.  is the distortion 
probability for non-head words in Chinese-
English alignment. 
)'(CE1, jd ?>
)'|(PrEJ jj ?? is the translation 
probability of the relative Japanese position 
given the relative English position.  
In fact,  has the same interpreta-
tion as in (12). Thus, we introduce two parame-
ters and  and let , where 
and  are positions of English words. The 
final distortion probability for non-head words 
can be estimated as shown in (16). 
)'|(PrEJ jj ??
'j )'( jp )'('' jpjj ?=?
'j )'( jp
)))'(|)((Pr)'|(Pr
)'(())((
')'(':)'(,'
)(:)(,
EJEJ
'
CE1,CJ1,
?
?
?=? ?=?
?
>>
?
??=?=?
jjpjjpj
jjpjjpj
j
jpjpjj
jdjpjjd
 (16)
5 Interpolation Model 
With the Chinese-English and English-Japanese 
corpora, we can build the induced model for Chi-
nese-Japanese word alignment as described in 
section 4. If we have small amounts of Chinese-
Japanese corpora, we can build another word 
alignment model using the method described in 
section 3, which is called the original model here. 
In order to further improve the performance of 
Chinese-Japanese word alignment, we build an 
interpolated model by interpolating the induced 
model and the original model.  
Generally, we can interpolate the induced 
model and the original model as shown in equa-
tion (17). 
)(Pr)1( )(Pr
)Pr(
IO c|fa,c|fa,
c|fa,
??+?= ??  (17)
Where is the original model trained 
from the Chinese-Japanese corpus, and 
 is the induced model trained from the 
Chinese-English and English-Japanese corpora. 
)(PrO c|fa,
)(PrI c|fa,
?  is an interpolation weight. It can be a constant 
or a function of f  and . c
 In both model 3 and model 4, there are mainly 
three kinds of parameters: translation probability, 
fertility probability and distortion probability. 
These three kinds of parameters have their own 
interpretation in these two models. In order to 
obtain fine-grained interpolation models, we in-
terpolate the three kinds of parameters using dif-
ferent weights, which are obtained in the same 
way as described in Wu et al (2005). t?  repre-
sents the weights for translation probability. n?  
represents the weights for fertility probability. 
d3?  and d4?  represent the weights for distortion 
probability in model 3 and in model 4, respec-
tively. d4?  is set as the interpolation weight for 
both the head words and the non-head words. 
The above four weights are obtained using a 
manually annotated held-out set. 
6 Experiments 
In this section, we compare different word 
alignment methods for Chinese-Japanese align-
ment. The "Original" method uses the original 
model trained with the small Chinese-Japanese 
corpus.  The "Basic Induced" method uses the 
induced model that employs the basic translation 
probability without introducing cross-language 
word similarity. The "Advanced Induced" 
method uses the induced model that introduces 
the cross-language word similarity into the calcu-
lation of the translation probability. The "Inter-
polated" method uses the interpolation of the 
word alignment models in the "Advanced In-
duced" and "Original" methods. 
879
6.1 Data 
There are three training corpora used in this pa-
per: Chinese-Japanese (CJ) corpus, Chinese-
English (CE) Corpus, and English-Japanese (EJ) 
Corpus. All of these tree corpora are from gen-
eral domain. The Chinese sentences and Japa-
nese sentences in the data are automatically seg-
mented into words. The statistics of these three 
corpora are shown in table 1. "# Source Words" 
and "# Target Words" mean the word number of 
the source and target sentences, respectively. 
Language 
Pairs 
#Sentence 
Pairs 
# Source 
Words 
# Target 
Words 
CJ 21,977 197,072 237,834 
CE 329,350 4,682,103 4,480,034
EJ 160,535 1,460,043 1,685,204
Table 1. Statistics for Training Data 
Besides the training data, we also have held-
out data and testing data. The held-out data in-
cludes 500 Chinese-Japanese sentence pairs, 
which is used to set the interpolated weights de-
scribed in section 5. We use another 1,000 Chi-
nese-Japanese sentence pairs as testing data, 
which is not included in the training data and the 
held-out data. The alignment links in the held-out 
data and the testing data are manually annotated. 
Testing data includes 4,926 alignment links2. 
6.2  Evaluation Metrics 
We use the same metrics as described in Wu et al 
(2005), which is similar to those in (Och and Ney, 
2000). The difference lies in that Wu et al (2005) 
took all alignment links as sure links. 
If we use  to represent the set of alignment 
links identified by the proposed methods and  
to denote the reference alignment set, the meth-
ods to calculate the precision, recall, f-measure, 
and alignment error rate (AER) are shown in 
equations (18), (19), (20), and (21), respectively. 
It can be seen that the higher the f-measure is, 
the lower the alignment error rate is. Thus, we 
will only show precision, recall and AER scores 
in the evaluation results. 
GS
CS
||
||
G
CG
S
SS
precision
?=      (18)
||
 ||
C
CG
S
SS
recall
?=  (19)
                                                 
2 For a non one-to-one link, if m source words are aligned to 
n target words, we take it as one alignment link instead of 
m?n alignment links. 
||||
||2
CG
CG
SS
SS
fmeasure +
?=  (20)
fmeasure
SS
SS
AER ?=+
??= 1
||||
||2
1
CG
CG  (21)
6.3 Experimental Results 
We use the held-out data described in section 6.1 
to set the interpolation weights in section 5. t?  is 
set to 0.3, n?  is set to 0.1, d3?  for model 3  is set 
to 0.5, and d4?  for model 4 is set to 0.1. With 
these parameters, we get the lowest alignment 
error rate on the held-out data. 
For each method described above, we perform 
bi-directional (source to target and target to 
source) word alignment and obtain two align-
ment results. Based on the two results, we get a 
result using "refined" combination as described 
in (Och and Ney, 2000). Thus, all of the results 
reported here describe the results of the "refined" 
combination. For model training, we use the 
GIZA++ toolkit3. 
Method Precision Recall AER 
Interpolated 0.6955 0.5802 0.3673
Advanced 
Induced 0.7382 0.4803 0.4181
Basic  
Induced 0.6787 0.4602 0.4515
Original 0.6026 0.4783 0.4667
Table 2. Word Alignment Results 
The evaluation results on the testing data are 
shown in table 2.  From the results, it can be seen 
that both of the two induced models perform bet-
ter than the "Original" method that only uses the 
limited Chinese-Japanese sentence pairs. The 
"Advanced Induced" method achieves a relative 
error rate reduction of 10.41% as compared with 
the "Original" method. Thus, with the Chinese-
English corpus and the English-Japanese corpus, 
we can achieve a good word alignment results 
even if no Chinese-Japanese parallel corpus is 
available. After introducing the cross-language 
word similarity into the translation probability, 
the "Advanced Induced" method achieves a rela-
tive error rate reduction of 7.40% as compared 
with the "Basic Induced" method. It indicates 
that cross-language word similarity is effective in 
the calculation of the translation probability. 
Moreover, the "interpolated" method further im-
proves the result, which achieves relative error 
                                                 
3 It is located at http://www.fjoch.com/ GIZA++.html. 
880
rate reductions of 12.51% and 21.30% as com-
pared with the "Advanced Induced" method and 
the "Original" method. 
7 Conclusion and Future Work 
This paper presented a word alignment approach 
for languages with scarce resources using bilin-
gual corpora of other language pairs. To perform 
word alignment between languages L1 and L2, 
we introduce a pivot language L3 and bilingual 
corpora in L1-L3 and L2-L3. Based on these two 
corpora and with the L3 as a pivot language, we 
proposed an approach to estimate the parameters 
of the statistical word alignment model. This ap-
proach can build a word alignment model for the 
desired language pair even if no bilingual corpus 
is available in this language pair. Experimental 
results indicate a relative error reduction of 
10.41% as compared with the method using the 
small bilingual corpus. 
In addition, we interpolated the above model 
with the model trained on the small L1-L2 bilin-
gual corpus to further improve word alignment 
between L1 and L2. This interpolated model fur-
ther improved the word alignment results by 
achieving a relative error rate reduction of 
12.51% as compared with the method using the 
two corpora in L1-L3 and L3-L2, and a relative 
error rate reduction of 21.30% as compared with 
the method using the small bilingual corpus in 
L1 and L2. 
In future work, we will perform more evalua-
tions. First, we will further investigate the effect 
of the size of corpora on the alignment results. 
Second, we will investigate different parameter 
combination of the induced model and the origi-
nal model. Third, we will also investigate how 
simpler IBM models 1 and 2 perform, in com-
parison with IBM models 3 and 4. Last, we will 
evaluate the word alignment results in a real ma-
chine translation system, to examine whether 
lower word alignment error rate will result in 
higher translation accuracy. 
References 
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin 
Knight, John Lafferty, Dan Melamed, Franz-Josef 
Och, David Purdy, Noah A. Smith, and David 
Yarowsky. 1999. Statistical Machine Translation 
Final Report. Johns Hopkins University Workshop. 
Niraj Aswani and Robert Gaizauskas. 2005. Aligning 
Words in English-Hindi Parallel Corpora. In Proc. 
of the ACL 2005 Workshop on Building and Using 
Parallel Texts: Data-driven Machine Translation 
and Beyond, pages 115-118.  
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2): 263-311. 
Colin Cherry and Dekang Lin. 2003. A Probability 
Model to Improve Word Alignment. In Proc. of the 
41st  Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2003), pages 88-95. 
Sue J. Ker and Jason S. Chang. 1997. A Class-based 
Approach to Word Alignment. Computational Lin-
guistics, 23(2): 313-343. 
Adam Lopez and Philip Resnik. 2005. Improved 
HMM Alignment Models for Languages with 
Scarce Resources. In Proc. of the ACL-2005 Work-
shop on Building and Using Parallel Texts: Data-
driven Machine Translation and Beyond, pages 83-
86. 
Joel Martin, Rada Mihalcea, and Ted Pedersen. 2005. 
Word Alignment for Languages with Scarce Re-
sources. In Proc. of the ACL-2005 Workshop on 
Building and Using Parallel Texts: Data-driven 
Machine Translation and Beyond, pages 65-74. 
Charles Schafer and David Yarowsky. 2002. Inducing 
Translation Lexicons via Diverse Similarity Meas-
ures and Bridge Languages. In Proc. of the 6th 
Conference on Natural Language Learning 2002 
(CoNLL-2002), pages 1-7. 
Dan Tufis, Radu Ion, Alexandru Ceausu, and Dan 
Stefanescu. 2005. Combined Word Alignments. In 
Proc. of the ACL-2005 Workshop on Building and 
Using Parallel Texts: Data-driven Machine Trans-
lation and Beyond, pages 107-110. 
Franz Josef Och and Hermann Ney. 2000. Improved 
Statistical Alignment Models. In Proc. of the 38th 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2000), pages 440-447. 
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment 
Models. Computational Linguistics, 29(1):19-51. 
Dekai Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3):377-403. 
Hua Wu, Haifeng Wang, and Zhanyi Liu. 2005. 
Alignment Model Adaptation for Domain-Specific 
Word Alignment. In Proc. of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL-2005), pages 467-474. 
Hao Zhang and Daniel Gildea. 2005. Stochastic Lexi-
calized Inversion Transduction Grammar for 
Alignment. In Proc. of the 43rd Annual Meeting of 
the Association for Computational Linguistics 
(ACL-2005), pages 475-482. 
881
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 913?920,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Boosting Statistical Word Alignment Using  
Labeled and Unlabeled Data 
 
Hua Wu      Haifeng Wang      Zhanyi Liu 
Toshiba (China) Research and Development Center 
5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng District 
Beijing, 100738, China 
{wuhua, wanghaifeng, liuzhanyi}@rdc.toshiba.com.cn 
 
  
 
Abstract 
This paper proposes a semi-supervised 
boosting approach to improve statistical 
word alignment with limited labeled data 
and large amounts of unlabeled data. The 
proposed approach modifies the super-
vised boosting algorithm to a semi-
supervised learning algorithm by incor-
porating the unlabeled data. In this algo-
rithm, we build a word aligner by using 
both the labeled data and the unlabeled 
data. Then we build a pseudo reference 
set for the unlabeled data, and calculate 
the error rate of each word aligner using 
only the labeled data. Based on this semi-
supervised boosting algorithm, we inves-
tigate two boosting methods for word 
alignment. In addition, we improve the 
word alignment results by combining the 
results of the two semi-supervised boost-
ing methods. Experimental results on 
word alignment indicate that semi-
supervised boosting achieves relative er-
ror reductions of 28.29% and 19.52% as 
compared with supervised boosting and 
unsupervised boosting, respectively. 
1 Introduction 
Word alignment was first proposed as an inter-
mediate result of statistical machine translation 
(Brown et al, 1993). In recent years, many re-
searchers build alignment links with bilingual 
corpora (Wu, 1997; Och and Ney, 2003; Cherry 
and Lin, 2003; Wu et al, 2005; Zhang and 
Gildea, 2005). These methods unsupervisedly 
train the alignment models with unlabeled data. 
A question about word alignment is whether 
we can further improve the performances of the 
word aligners with available data and available 
alignment models. One possible solution is to use 
the boosting method (Freund and Schapire, 
1996), which is one of the ensemble methods 
(Dietterich, 2000). The underlying idea of boost-
ing is to combine simple "rules" to form an en-
semble such that the performance of the single 
ensemble is improved. The AdaBoost (Adaptive 
Boosting) algorithm by Freund and Schapire 
(1996) was developed for supervised learning. 
When it is applied to word alignment, it should 
solve the problem of building a reference set for 
the unlabeled data. Wu and Wang (2005) devel-
oped an unsupervised AdaBoost algorithm by 
automatically building a pseudo reference set for 
the unlabeled data to improve alignment results. 
In fact, large amounts of unlabeled data are 
available without difficulty, while labeled data is 
costly to obtain. However, labeled data is valu-
able to improve performance of learners. Conse-
quently, semi-supervised learning, which com-
bines both labeled and unlabeled data, has been 
applied to some NLP tasks such as word sense 
disambiguation (Yarowsky, 1995; Pham et al, 
2005), classification (Blum and Mitchell, 1998; 
Thorsten, 1999), clustering (Basu et al, 2004), 
named entity classification (Collins and Singer, 
1999), and parsing (Sarkar, 2001). 
In this paper, we propose a semi-supervised 
boosting method to improve statistical word 
alignment with both limited labeled data and 
large amounts of unlabeled data. The proposed 
approach modifies the supervised AdaBoost al-
gorithm to a semi-supervised learning algorithm 
by incorporating the unlabeled data. Therefore, it 
should address the following three problems. The 
first is to build a word alignment model with 
both labeled and unlabeled data. In this paper, 
with the labeled data, we build a supervised 
model by directly estimating the parameters in 
913
the model instead of using the Expectation 
Maximization (EM) algorithm in Brown et al 
(1993). With the unlabeled data, we build an un-
supervised model by estimating the parameters 
with the EM algorithm. Based on these two word 
alignment models, an interpolated model is built 
through linear interpolation. This interpolated 
model is used as a learner in the semi-supervised 
AdaBoost algorithm. The second is to build a 
reference set for the unlabeled data. It is auto-
matically built with a modified "refined" combi-
nation method as described in Och and Ney 
(2000). The third is to calculate the error rate on 
each round. Although we build a reference set 
for the unlabeled data, it still contains alignment 
errors. Thus, we use the reference set of the la-
beled data instead of that of the entire training 
data to calculate the error rate on each round.  
With the interpolated model as a learner in the 
semi-supervised AdaBoost algorithm, we inves-
tigate two boosting methods in this paper to im-
prove statistical word alignment. The first 
method uses the unlabeled data only in the inter-
polated model. During training, it only changes 
the distribution of the labeled data. The second 
method changes the distribution of both the la-
beled data and the unlabeled data during training. 
Experimental results show that both of these two 
methods improve the performance of statistical 
word alignment. 
In addition, we combine the final results of the 
above two semi-supervised boosting methods. 
Experimental results indicate that this combina-
tion outperforms the unsupervised boosting 
method as described in Wu and Wang (2005), 
achieving a relative error rate reduction of 
19.52%. And it also achieves a reduction of 
28.29% as compared with the supervised boost-
ing method that only uses the labeled data. 
The remainder of this paper is organized as 
follows. Section 2 briefly introduces the statisti-
cal word alignment model. Section 3 describes 
parameter estimation method using the labeled 
data. Section 4 presents our semi-supervised 
boosting method. Section 5 reports the experi-
mental results. Finally, we conclude in section 6. 
2 Statistical Word Alignment Model 
According to the IBM models (Brown et al, 
1993), the statistical word alignment model can 
be generally represented as in equation (1).  
?=
a'
e|f,a'
e|fa,
e|fa,
)Pr(
)Pr(
)Pr(  
(1)
Where  and f  represent the source sentence 
and the target sentence, respectively. 
e
In this paper, we use a simplified IBM model 
4 (Al-Onaizan et al, 1999), which is shown in 
equation (2). This simplified version does not 
take into account word classes as described in 
Brown et al (1993). 
))))(()](([                
))()](([(                 
)|( )|(                 
   
 )Pr(
0,1
1
0,1
1
11
1
2
0
0
0 00
?
?
??
?=
>
?=
==
?
???
+??=
??
????
?
???
? ?=
m
aj
j
m
aj
j
m
j
aj
l
i
ii
m
j
j
ja
j
jpjdahj
cjdahj
eften
pp
m
?
??
?
?
?
e|fa,
(2)
ml,  are the lengths of the source sentence and 
the target sentence respectively. 
j  is the position index of the target word. 
ja  is the position of the source word aligned to 
the  target word. thj
i?  is the number of target words that  is 
aligned to. 
ie
0p ,  are the fertility probabilities for , and 1p 0e
110 =+ pp . 
)|
jaj
et(f  is the word translation probability. 
)|( ii en ?  is the fertility probability. 
)(1
ja
cjd ??  is the distortion probability for the 
head word of cept1 i. 
))((1 jpjd ?>  is the distortion probability for the 
non-head words of cept i. 
}:{min)( k
k
aikih ==  is the head of cept i. 
}:{max)( kj
jk
aakjp ==
<
. 
i?  is the first word before  with non-zero  ie
fertility.  
ic  is the center of cept i. 
3 Parameter Estimation with Labeled 
Data 
With the labeled data, instead of using EM algo-
rithm, we directly estimate the three main pa-
rameters in model 4: translation probability, fer-
tility probability, and distortion probability. 
                                                 
1 A cept is defined as the set of target words connected to a source word 
(Brown et al, 1993).  
914
3.1 Translation Probability Where 1),( =yx?  if yx = . Otherwise, 0),( =yx? .  
The translation probability is estimated from the 
labeled data as described in (3). 4 Boosting with Labeled Data and 
Unlabeled Data 
?=
'
)',(
),(
)|(
f
i
ji
ij
fecount
fecount
eft  
(3) In this section, we first propose a semi-
supervised AdaBoost algorithm for word align-
ment, which uses both the labeled data and the 
unlabeled data. Based on the semi-supervised 
algorithm, we describe two boosting methods for 
word alignment. And then we develop a method 
to combine the results of the two boosting meth-
ods. 
Where  is the occurring frequency of 
 aligned to  in the labeled data. 
),( ji fecount
ie jf
3.2 Fertility Probability 
The fertility probability )|( ii en ?  describes the 
distribution of the numbers of words that  is 
aligned to. It is estimated as described in (4).  
ie 4.1 Semi-Supervised AdaBoost Algorithm 
for Word Alignment 
?=
'
),'(
),(
)|(
?
?
??
i
ii
ii ecount
ecount
en  
(4)
Figure 1 shows the semi-supervised AdaBoost 
algorithm for word alignment by using labeled 
and unlabeled data. Compared with the super-
vised Adaboost algorithm, this semi-supervised 
AdaBoost algorithm mainly has five differences.  
Where ),( ii ecount ? describes the occurring fre-
quency of word  aligned to ie i?  target words in 
the labeled data.  Word Alignment Model  
0p  and   describe the fertility probabilities 
for .  And  and  sum to 1. We estimate 
 directly from the labeled data, which is 
shown in (5). 
1p
0e 0p 1p
0p
The first is the word alignment model, which 
is taken as a learner in the boosting algorithm. 
The word alignment model is built using both the 
labeled data and the unlabeled data. With the 
labeled data, we train a supervised model by di-
rectly estimating the parameters in the IBM 
model as described in section 3. With the unla-
beled data, we train an unsupervised model using 
the same EM algorithm in Brown et al (1993). 
Then we build an interpolation model by linearly 
interpolating these two word alignment models, 
which is shown in (8). This interpolated model is 
used as the model  described in figure 1. lM
 
Aligned
NullAligned
p
#
##
0
?=  (5)
Where  is the occurring frequency of 
the target words that have counterparts in the 
source language. is the occurring fre-
quency of the target words that have no counter-
parts in the source language. 
Aligned#
Null#
3.3 Distortion Probability 
)(Pr)1()(Pr
)Pr(
US e|fa,e|fa,
e|fa,
??+?= ??  (8)There are two kinds of distortion probability in 
model 4: one for head words and the other for 
non-head words. Both of the distortion probabili-
ties describe the distribution of relative positions 
Thus, if we let 
i
cjj ??=? 1  and )(1 jpjj ?=? > , 
the distortion probabilities for head words and 
non-head words are estimated in (6) and (7) with 
the labeled data, respectively. 
Where  and  are the 
trained supervised model and unsupervised 
model, respectively. 
)(PrS e|fa, )(PrU e|fa,
?  is an interpolation weight. 
We train the weight in equation (8) in the same 
way as described in Wu et al (2005).  
Pseudo Reference Set for Unlabeled Data 
??
?
?
??
??
=?
'
1 '
'
'
,
''
1
,
1
11
),(
),(
)(
j cj
cj
i
i
i
i
cjj
cjj
jd
?
?
?
?
?
?
 (6)
? ?
?
>?
>
>
>> ??
??
=?
'
1
'' )(,
'''
1
)(,
1
11
))(,(
))(,(
)(
j jpj
jpj
jpjj
jpjj
jd ?
?
 (7)
The second is the reference set for the unla-
beled data. For the unlabeled data, we automati-
cally build a pseudo reference set. In order to 
build a reliable pseudo reference set, we perform 
bi-directional word alignment on the training 
data using the interpolated model trained on the 
first round. Bi-directional word alignment in-
cludes alignment in two directions (source to 
915
Input: A training set  including m  bilingual sentence pairs;  TS
The reference set  for the training data; TR
The reference sets  and  ( ) for the labeled data  and the unlabeled 
data  respectively, where 
LR UR TUL , RRR ? LS
US LUT SSS ?=  and NULLLU =? SS ; 
A loop count L. 
(1) Initialize the weights: 
mimiw ,...,1,/1)(1 ==  
(2) For , execute steps (3) to (9).  L l to1=
(3) For each sentence pair i, normalize the 
weights on the training set: 
? ==
j
lll mijwiwip ,...,1),(/)()(  
(4) Update the word alignment model  
based on the weighted training data. 
lM
(5) Perform word alignment on the training set 
with the alignment model :  lM
)( lll pMh =  
(6) Calculate the error of  with the reference 
set : 
lh
LR ? ?=
i
ll iip )()( ??  
Where )(i?  is calculated as in equation (9). 
(7) If 2/1>l? , then let , and end the 
training process. 
1?= lL
(8) Let )1/( lll ??? ?= . 
(9) For all i, compute new weights: 
nknkiwiw lll /))(()()(1 ???+?=+  
where, n represents n alignment links in 
the ith sentence pair. k represents the num-
ber of error links as compared with . TR
Output: The final word alignment result for a source word e : 
?
=
??==
L
l
ll
lff
fehfeWTfeRSeh
1
F )),((),()
1
(logmaxarg),(maxarg)( ??  
Where 1),( =yx?  if yx = . Otherwise, 0),( =yx? .  is the weight of the alignment link 
 produced by the model , which is calculated as described in equation (10). 
),( feWTl
),( fe lM
Figure 1. The Semi-Supervised Adaboost Algorithm for Word Alignment 
target and target to source) as described in Och 
and Ney (2000). Thus, we get two sets of align-
ment results  and  on the unlabeled data. 
Based on these two sets, we use a modified "re-
fined" method (Och and Ney, 2000) to construct 
a pseudo reference set .  
1A 2A
UR
(1) The intersection  is added to the 
reference set . 
21 AAI ?=
UR
(2) We add  to  if a) is satis-
fied or both b) and c) are satisfied.  
21)  ,( AAfe ?? UR
a) Neither  nor  has an alignment in  
and  is greater than a threshold 
e f UR
)|( efp 1? . 
?=
'
)',(
),(
)|(
f
fecount
fecount
efp  
Where  is the occurring fre-
quency of the alignment link  in 
the bi-directional word alignment results. 
),( fecount
)  ,( fe
b)  has a horizontal or a vertical 
neighbor that is already in . 
)  ,( fe
UR
c) The set does not contain 
alignments with both horizontal and ver-
tical neighbors. 
),(U feR ?
 Error of Word Aligner 
The third is the calculation of the error of the 
individual word aligner on each round. For word 
alignment, a sentence pair is taken as a sample. 
Thus, we calculate the error rate of each sentence 
pair as described in (9), which is the same as de-
scribed in Wu and Wang (2005).  
 
||||
||2
1)(
RW
RW
SS
SS
i +
??=?  (9)
Where  represents the set of alignment 
links of a sentence pair i identified by the indi-
vidual interpolated model on each round.  is 
the reference alignment set for the sentence pair. 
WS
RS
With the error rate of each sentence pair, we 
calculate the error of the word aligner on each 
round. Although we build a pseudo reference set 
 for the unlabeled data, it contains alignment 
errors. Thus, the weighted sum of the error rates 
of sentence pairs in the labeled data instead of 
that in the entire training data is used as the error 
of the word aligner. 
UR
 
916
 Weights Update for Sentence Pairs  
The forth is the weight update for sentence 
pairs according to the error and the reference set. 
In a sentence pair, there are usually several word 
alignment links. Some are correct, and others 
may be incorrect. Thus, we update the weights 
according to the number of correct and incorrect 
alignment links as compared with the reference 
set, which is shown in step (9) in figure 1.  
 Weights for Word Alignment Links  
The fifth is the weights used when we con-
struct the final ensemble. Besides the weight 
)/1log( l? , which is the confidence measure of 
the  word aligner, we also use the weight 
 to measure the confidence of each 
alignment link produced by the model . The 
weight  is calculated as shown in (10). 
Wu and Wang (2005) proved that adding this 
weight improved the word alignment results. 
thl
),( feWTl
lM
),( feWTl
?? +
?=
''
),'()',(
),(2
),(
ef
l fecountfecount
fecount
feWT
(10) 
Where  is the occurring frequency 
of the alignment link  in the word align-
ment results of the training data produced by the 
model . 
),( fecount
)  ,( fe
lM
4.2 Method 1 
This method only uses the labeled data as train-
ing data. According to the algorithm in figure 1, 
we obtain  and . Thus, we only 
change the distribution of the labeled data. How-
ever, we build an unsupervised model using the 
unlabeled data. On each round, we keep this un-
supervised model unchanged, and we rebuild the 
supervised model by estimating the parameters 
as described in section 3 with the weighted train-
ing data. Then we interpolate the supervised 
model and the unsupervised model to obtain an 
interpolated model as described in section 4.1. 
The interpolated model is used as the alignment 
model  in figure 1. Thus, in this interpolated 
model, we use both the labeled and unlabeled 
data. On each round, we rebuild the interpolated 
model using the rebuilt supervised model and the 
unchanged unsupervised model. This interpo-
lated model is used to align the training data.  
LT SS = LT RR =
lM
According to the reference set of the labeled 
data, we calculate the error of the word aligner 
on each round. According to the error and the 
reference set, we update the weight of each sam-
ple in the labeled data. 
4.3 Method 2 
This method uses both the labeled data and the 
unlabeled data as training data. Thus, we set 
ULT SSS ?=  and ULT RRR ?=  as described in 
figure 1. With the labeled data, we build a super-
vised model, which is kept unchanged on each 
round.2 With the weighted samples in the train-
ing data, we rebuild the unsupervised model with 
EM algorithm on each round. Based on these two 
models, we built an interpolated model as de-
scribed in section 4.1. The interpolated model is 
used as the alignment model  in figure 1. On 
each round, we rebuild the interpolated model 
using the unchanged supervised model and the 
rebuilt unsupervised model. Then the interpo-
lated model is used to align the training data. 
lM
Since the training data includes both labeled 
and unlabeled data, we need to build a pseudo 
reference set  for the unlabeled data using the 
method described in section 4.1.  According to 
the reference set  of the labeled data, we cal-
culate the error of the word aligner on each 
round. Then, according to the pseudo reference 
set  and the reference set , we update the 
weight of each sentence pair in the unlabeled 
data and in the labeled data, respectively.  
UR
LR
UR LR
There are four main differences between 
Method 2 and Method 1.  
(1) On each round, Method 2 changes the distri-
bution of both the labeled data and the unla-
beled data, while Method 1 only changes the 
distribution of the labeled data. 
(2) Method 2 rebuilds the unsupervised model, 
while Method 1 rebuilds the supervised 
model.  
(3) Method 2 uses the labeled data instead of the 
entire training data to estimate the error of 
the word aligner on each round. 
(4) Method 2 uses an automatically built pseudo 
reference set to update the weights for the 
sentence pairs in the unlabeled data. 
4.4 Combination 
In the above two sections, we described two 
semi-supervised boosting methods for word 
alignment. Although we use interpolated models 
                                                 
2 In fact, we can also rebuild the supervised model accord-
ing to the weighted labeled data. In this case, as we know, 
the error of the supervised model increases. Thus, we keep 
the supervised model unchanged in this method. 
917
for word alignment in both Method 1 and 
Method 2, the interpolated models are trained 
with different weighted data. Thus, they perform 
differently on word alignment. In order to further 
improve the word alignment results, we combine 
the results of the above two methods as described 
in (11). 
  )),(),((maxarg
)(
2211
F3,
feRSfeRS
eh
f
?+?= ??
ods to calculate the precision, recall, f-measure, 
and alignment error rate (AER) are shown in 
equations (12), (13), (14), and (15). It can be 
seen that the higher the f-measure is, the lower 
the alignment error rate is.  
|S|
|SS|
G
CG ?=precision      (12)
|S|
 |SS|
C
CG ?=recall  (11) (13)
||||
||2
CG
CG
SS
SS
fmeasure +
??=  Where  is the combined hypothesis for 
word alignment.  and  are the 
two ensemble results as shown in figure 1 for 
Method 1 and Method 2, respectively. 
)(F3, eh
),(1 feRS ),(2 feRS
1?  and 2?  
are the constant weights. 
(14)
fmeasure
SS
SS
AER ?=+
???= 1
||||
||2
1
CG
CG  (15)
5.3 Experimental Results 
5 Experiments With the data in section 5.1, we get the word 
alignment results shown in table 2. For all of the 
methods in this table, we perform bi-directional 
(source to target and target to source) word 
alignment, and obtain two alignment results on 
the testing set. Based on the two results, we get 
the "refined" combination as described in Och 
and Ney (2000). Thus, the results in table 2 are 
those of the "refined" combination. For EM 
training, we use the GIZA++ toolkit4. 
In this paper, we take English to Chinese word 
alignment as a case study. 
5.1 Data 
We have two kinds of training data from general 
domain: Labeled Data (LD) and Unlabeled Data 
(UD). The Chinese sentences in the data are 
automatically segmented into words. The statis-
tics for the data is shown in Table 1. The labeled 
data is manually word aligned, including 156,421 
alignment links. 
Data # Sentence Pairs 
# English 
Words 
 Results of Supervised Methods  
Using the labeled data, we use two methods to 
estimate the parameters in IBM model 4: one is 
to use the EM algorithm, and the other is to esti-
mate the parameters directly from the labeled 
data as described in section 3.  In table 2, the 
method "Labeled+EM" estimates the parameters 
with the EM algorithm, which is an unsupervised 
method without boosting. And the method "La-
beled+Direct" estimates the parameters directly 
from the labeled data, which is a supervised 
method without boosting. "Labeled+EM+Boost" 
and "Labeled+Direct+Boost" represent the two 
supervised boosting methods for the above two 
parameter estimation methods.  
# Chinese 
Words 
LD 31,069 255,504 302,470 
UD 329,350 4,682,103 4,480,034
Table 1. Statistics for Training Data 
We use 1,000 sentence pairs as testing set, 
which are not included in LD or UD. The testing 
set is also manually word aligned, including 
8,634 alignment links in the testing set3.  
5.2 Evaluation Metrics 
We use the same evaluation metrics as described 
in Wu et al (2005), which is similar to those in 
(Och and Ney, 2000). The difference lies in that 
Wu et al (2005) take all alignment links as sure 
links. 
Our methods that directly estimate parameters 
in IBM model 4 are better than that using the EM 
algorithm.  "Labeled+Direct" is better than "La-
beled+EM", achieving a relative error rate reduc-
tion of 22.97%. And "Labeled+Direct+Boost" is 
better than "Labeled+EM+Boost", achieving a 
relative error rate reduction of 22.98%. In addi-
tion, the two boosting methods perform better 
than their corresponding methods without
 If we use  to represent the set of alignment 
links identified by the proposed method and  
to denote the reference alignment set, the meth-
GS
CS
                                                 
3 For a non one-to-one link, if m source words are aligned to 
n target words, we take it as one alignment link instead of 
m?n alignment links. 
                                                 
4 It is located at http://www.fjoch.com/ GIZA++.html. 
918
Method Precision Recall F-Measure AER 
Labeled+EM 0.6588 0.5210 0.5819 0.4181 
Labeled+Direct 0.7269 0.6609 0.6924 0.3076 
Labeled+EM+Boost 0.7384 0.5651 0.6402 0.3598 
Labeled+Direct+Boost 0.7771 0.6757 0.7229 0.2771 
Unlabeled+EM 0.7485 0.6667 0.7052 0.2948 
Unlabeled+EM+Boost 0.8056 0.7070 0.7531 0.2469 
Interpolated 0.7555 0.7084 0.7312 0.2688 
Method 1 0.7986 0.7197 0.7571 0.2429 
Method 2 0.8060 0.7388 0.7709 0.2291 
Combination 0.8175 0.7858 0.8013 0.1987 
Table 2. Word Alignment Results 
boosting. For example, "Labeled+Direct+Boost" 
achieves an error rate reduction of 9.92% as 
compared with "Labeled+Direct". 
Results of Unsupervised Methods   
With the unlabeled data, we use the EM algo-
rithm to estimate the parameters in the model. 
The method "Unlabeled+EM" represents an un-
supervised method without boosting. And the 
method "Unlabeled+EM+Boost" uses the same 
unsupervised Adaboost algorithm as described in 
Wu and Wang (2005). 
The boosting method "Unlabeled+EM+Boost" 
achieves a relative error rate reduction of 16.25% 
as compared with "Unlabeled+EM". In addition, 
the unsupervised boosting method "Unla-
beled+EM+Boost" performs better than the su-
pervised boosting method "Labeled+Direct+ 
Boost", achieving an error rate reduction of 
10.90%. This is because the size of labeled data 
is too small to subject to data sparseness problem.  
Results of Semi-Supervised Methods    
By using both the labeled and the unlabeled 
data, we interpolate the models trained by "La-
beled+Direct" and "Unlabeled+EM" to get an 
interpolated model. Here, we use "interpolated" 
to represent it. "Method 1" and  "Method 2" rep-
resent the semi-supervised boosting methods de-
scribed in section 4.2 and section 4.3, respec-
tively. "Combination" denotes the method de-
scribed in section 4.4, which combines "Method 
1" and "Method 2".  Both of the weights 1?  and 
2?  in equation (11) are set to 0.5. 
 "Interpolated" performs better than the meth-
ods using only labeled data or unlabeled data. It 
achieves relative error rate reductions of 12.61% 
and 8.82% as compared with "Labeled+Direct" 
and "Unlabeled+EM", respectively. 
Using an interpolation model, the two semi-
supervised boosting methods "Method 1" and 
"Method 2" outperform the supervised boosting 
method "Labeled+Direct+Boost", achieving a 
relative error rate reduction of 12.34% and 
17.32% respectively. In addition, the two semi-
supervised boosting methods perform better than 
the unsupervised boosting method "Unlabeled+ 
EM+Boost". "Method 1" performs slightly better 
than "Unlabeled+EM+Boost". This is because 
we only change the distribution of the labeled 
data in "Method 1". "Method 2" achieves an er-
ror rate reduction of 7.77% as compared with 
"Unlabeled+EM+Boost". This is because we use 
the interpolated model in our semi-supervised 
boosting method, while "Unlabeled+EM+Boost" 
only uses the unsupervised model. 
Moreover, the combination of the two semi-
supervised boosting methods further improves 
the results, achieving relative error rate reduc-
tions of 18.20% and 13.27% as compared with 
"Method 1" and "Method 2", respectively. It also 
outperforms both the supervised boosting 
method "Labeled+Direct+Boost" and the unsu-
pervised boosting method "Unlabeled+EM+ 
Boost", achieving relative error rate reductions of 
28.29% and 19.52% respectively.  
Summary of the Results    
From the above result, it can be seen that all 
boosting methods perform better than their corre-
sponding methods without boosting. The semi-
supervised boosting methods outperform the su-
pervised boosting method and the unsupervised 
boosting method. 
6 Conclusion and Future Work 
This paper proposed a semi-supervised boosting 
algorithm to improve statistical word alignment 
with limited labeled data and large amounts of 
unlabeled data. In this algorithm, we built an in-
terpolated model by using both the labeled data 
919
and the unlabeled data. This interpolated model 
was employed as a learner in the algorithm. Then, 
we automatically built a pseudo reference for the 
unlabeled data, and calculated the error rate of 
each word aligner with the labeled data.  Based 
on this algorithm, we investigated two methods 
for word alignment. In addition, we developed a 
method to combine the results of the above two 
semi-supervised boosting methods. 
Experimental results indicate that our semi-
supervised boosting method outperforms the un-
supervised boosting method as described in Wu 
and Wang (2005), achieving a relative error rate 
reduction of 19.52%. And it also outperforms the 
supervised boosting method that only uses the 
labeled data, achieving a relative error rate re-
duction of 28.29%. Experimental results also 
show that all boosting methods outperform their 
corresponding methods without boosting. 
In the future, we will evaluate our method 
with an available standard testing set. And we 
will also evaluate the word alignment results in a 
machine translation system, to examine whether 
lower word alignment error rate will result in 
higher translation accuracy. 
References 
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin 
Knight, John Lafferty, Dan Melamed, Franz-Josef 
Och, David Purdy, Noah A. Smith, and David 
Yarowsky. 1999. Statistical Machine Translation 
Final Report. Johns Hopkins University Workshop. 
Sugato Basu, Mikhail Bilenko, and Raymond J. 
Mooney.  2004. Probabilistic Framework for Semi-
Supervised Clustering. In Proc. of the 10th ACM 
SIGKDD International Conference on Knowledge 
Discovery and Data Mining (KDD-2004), pages 
59-68.  
Avrim Blum and Tom Mitchell. 1998. Combing La-
beled and Unlabeled Data with Co-training. In 
Proc. of the 11th Conference on Computational 
Learning Theory (COLT-1998), pages1-10.  
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2): 263-311. 
Colin Cherry and Dekang Lin. 2003. A Probability 
Model to Improve Word Alignment. In Proc. of the 
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2003), pages 88-95. 
Michael Collins and Yoram Singer. 1999. Unsuper-
vised Models for Named Entity Classification. In 
Proc. of the Joint SIGDAT Conference on Empiri-
cal Methods in Natural Language Processing and 
Very Large Corpora (EMNLP/VLC-1999), pages 
100-110. 
Thomas G. Dietterich. 2000. Ensemble Methods in 
Machine Learning. In Proc. of the First Interna-
tional Workshop on Multiple Classifier Systems 
(MCS-2000), pages 1-15. 
Yoav Freund and Robert E. Schapire. 1996. Experi-
ments with a New Boosting Algorithm. In Proc. of 
the 13th International Conference on Machine 
Learning (ICML-1996), pages 148-156. 
Franz Josef Och and Hermann Ney. 2000. Improved 
Statistical Alignment Models. In Proc. of the 38th 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2000), pages 440-447. 
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment 
Models. Computational Linguistics, 29(1):19-51. 
Thanh Phong Pham, Hwee Tou Ng, and Wee Sun Lee 
2005. Word Sense Disambiguation with Semi-
Supervised Learning. In Proc. of the 20th National 
Conference on Artificial Intelligence (AAAI 2005), 
pages 1093-1098. 
Anoop Sarkar. 2001. Applying Co-Training Methods 
to Statistical Parsing. In Proc. of the 2nd Meeting of 
the North American Association for Computational 
Linguistics( NAACL-2001), pages 175-182. 
Joachims Thorsten. 1999. Transductive Inference for 
Text Classification Using Support Vector Ma-
chines. In Proc. of the 16th International Confer-
ence on Machine Learning (ICML-1999), pages 
200-209. 
Dekai Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3): 377-403. 
Hua Wu and Haifeng Wang. 2005. Boosting Statisti-
cal Word Alignment. In Proc. of the 10th Machine 
Translation Summit, pages 313-320. 
Hua Wu, Haifeng Wang, and Zhanyi Liu. 2005. 
Alignment Model Adaptation for Domain-Specific 
Word Alignment. In Proc. of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL-2005), pages 467-474. 
David Yarowsky. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. In 
Proc. of the 33rd Annual Meeting of the Association 
for Computational Linguistics (ACL-1995), pages 
189-196.  
Hao Zhang and Daniel Gildea. 2005. Stochastic Lexi-
calized Inversion Transduction Grammar for 
Alignment. In Proc. of the 43rd Annual Meeting of 
the Association for Computational Linguistics 
(ACL-2005), pages 475-482. 
920
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 856?863,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Pivot Language Approach for Phrase-Based Statistical Machine 
Translation 
Hua Wu and Haifeng Wang 
Toshiba (China) Research and Development Center 
5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng District 
Beijing, 100738, China 
{wuhua,wanghaifeng}@rdc.toshiba.com.cn 
 
 
Abstract 
This paper proposes a novel method for 
phrase-based statistical machine translation 
by using pivot language. To conduct trans-
lation between languages Lf and Le with a 
small bilingual corpus, we bring in a third 
language Lp, which is named the pivot lan-
guage. For Lf-Lp and Lp-Le, there exist 
large bilingual corpora. Using only Lf-Lp 
and Lp-Le bilingual corpora, we can build a 
translation model for Lf-Le. The advantage 
of this method lies in that we can perform 
translation between Lf and Le even if there 
is no bilingual corpus available for this 
language pair. Using BLEU as a metric, 
our pivot language method achieves an ab-
solute improvement of 0.06 (22.13% rela-
tive) as compared with the model directly 
trained with 5,000 Lf-Le sentence pairs for 
French-Spanish translation. Moreover, with 
a small Lf-Le bilingual corpus available, 
our method can further improve the transla-
tion quality by using the additional Lf-Lp 
and Lp-Le bilingual corpora. 
1 Introduction 
For statistical machine translation (SMT), phrase-
based methods (Koehn et al, 2003; Och and Ney, 
2004) and syntax-based methods (Wu, 1997; Al-
shawi et al 2000; Yamada and Knignt, 2001; 
Melamed, 2004; Chiang, 2005; Quick et al, 2005; 
Mellebeek et al, 2006) outperform word-based 
methods (Brown et al, 1993). These methods need 
large bilingual corpora. However, for some lan-
guages pairs, only a small bilingual corpus is 
available, which will degrade the performance of 
statistical translation systems. 
To solve this problem, this paper proposes a 
novel method for phrase-based SMT by using a 
pivot language. To perform translation between 
languages Lf and Le, we bring in a pivot language 
Lp, for which there exist large bilingual corpora for 
language pairs Lf-Lp and Lp-Le. With the Lf-Lp and 
Lp-Le bilingual corpora, we can build a translation 
model for Lf-Le by using Lp as the pivot language. 
We name the translation model pivot model. The 
advantage of this method lies in that we can con-
duct translation between Lf and Le even if there is 
no bilingual corpus available for this language pair. 
Moreover, if a small corpus is available for Lf-Le, 
we build another translation model, which is 
named standard model. Then, we build an interpo-
lated model by performing linear interpolation on 
the standard model and the pivot model. Thus, the 
interpolated model can employ both the small Lf-
Le corpus and the large Lf-Lp and Lp-Le corpora. 
We perform experiments on the Europarl corpus 
(Koehn, 2005). Using BLEU (Papineni et al, 2002) 
as a metric, our method achieves an absolute im-
provement of 0.06 (22.13% relative) as compared 
with the standard model trained with 5,000 Lf-Le 
sentence pairs for French-Spanish translation. The 
translation quality is comparable with that of the 
model trained with a bilingual corpus of 30,000 Lf-
Le sentence pairs. Moreover, translation quality is 
further boosted by using both the small Lf-Le bilin-
gual corpus and the large Lf-Lp and Lp-Le corpora. 
Experimental results on Chinese-Japanese trans-
lation also indicate that our method achieves satis-
factory results using English as the pivot language.  
856
The remainder of this paper is organized as fol-
lows. In section 2, we describe the related work. 
Section 3 briefly introduces phrase-based SMT. 
Section 4 and Section 5 describes our method for 
phrase-based SMT using pivot language. We de-
scribe the experimental results in sections 6 and 7. 
Lastly, we conclude in section 8. 
2 Related Work 
Our method is mainly related to two kinds of 
methods: those using pivot language and those 
using a small bilingual corpus or scarce resources.  
For the first kind, pivot languages are employed 
to translate queries in cross-language information 
retrieval (CLIR) (Gollins and Sanderson, 2001; 
Kishida and Kando, 2003). These methods only 
used the available dictionaries to perform word by 
word translation. In addition, NTCIR 4 workshop 
organized a shared task for CLIR using pivot lan-
guage. Machine translation systems are used to 
translate queries into pivot language sentences, and 
then into target sentences (Sakai et al, 2004). 
Callison-Burch et al (2006) used pivot lan-
guages for paraphrase extraction to handle the un-
seen phrases for phrase-based SMT. Borin (2000) 
and Wang et al (2006) used pivot languages to 
improve word alignment. Borin (2000) used multi-
lingual corpora to increase alignment coverage. 
Wang et al (2006) induced alignment models by 
using two additional bilingual corpora to improve 
word alignment quality. Pivot Language methods 
were also used for translation dictionary induction 
(Schafer and Yarowsky, 2002), word sense disam-
biguation (Diab and Resnik, 2002), and so on. 
For the second kind, Niessen and Ney (2004) 
used morpho-syntactic information for translation 
between language pairs with scarce resources. 
Vandeghinste et al (2006) used translation dic-
tionaries and shallow analysis tools for translation 
between the language pair with low resources. A 
shared task on word alignment was organized as 
part of the ACL 2005 Workshop on Building and 
Using Parallel Texts (Martin et al, 2005). This 
task focused on languages with scarce resources. 
For the subtask of unlimited resources, some re-
searchers (Aswani and Gaizauskas, 2005; Lopez 
and Resnik, 2005; Tufis et al, 2005) used lan-
guage-dependent resources such as dictionary, the-
saurus, and dependency parser to improve word 
alignment results. 
In this paper, we address the translation problem 
for language pairs with scarce resources by bring-
ing in a pivot language, via which we can make 
use of large bilingual corpora. Our method does 
not need language-dependent resources or deep 
linguistic processing. Thus, the method is easy to 
be adapted to any language pair where a pivot lan-
guage and corresponding large bilingual corpora 
are available. 
3 Phrase-Based SMT 
According to the translation model presented in 
(Koehn et al, 2003), given a source sentence f , 
the best target translation beste  can be obtained 
according to the following model 
)()()|(maxarg
)|(maxarg
e
e
e
eef
fee
length
LM
best
?pp
p
=
=
 (1)
Where the translation model )|( efp can be 
decomposed into  
?
=
??=
I
i
iiiiii
II
aefpbadef
efp
1
1
11
),|()()|(
)|(
?? w
(2)
Where )|( ii ef?  and )( 1?? ii bad  denote phrase 
translation probability and distortion probability, 
respectively. ),|( aefp iiw  is the lexical weight, 
and ?  is the strength of the lexical weight. 
4 Phrase-Based SMT Via Pivot Language 
This section will introduce the method that per-
forms phrase-based SMT for the language pair Lf-
Le by using the two bilingual corpora of Lf-Lp and 
Lp-Le. With the two additional bilingual corpora, 
we train two translation models for Lf-Lp and Lp-Le, 
respectively. Based on these two models, we build 
a pivot translation model for Lf-Le, with Lp as a 
pivot language. 
According to equation (2), the phrase translation 
probability and the lexical weight are language 
dependent. We will introduce them in sections 4.1 
and 4.2, respectively. 
4.1 Phrase Translation Probability 
Using the Lf-Lp and Lp-Le bilingual corpora, we 
train two phrase translation probabilities 
857
)|( ii pf?  and )|( ii ep? , where ip  is the phrase 
in the pivot language Lp. Given the phrase 
translation probabilities )|( ii pf?  and )|( ii ep? , 
we obtain the phrase translation probability 
)|( ii ef?  according to the following model. 
?=
ip
iiiiiii epepfef )|(),|()|( ???  (3)
The phrase translation probability ),|( iii epf?  
does not depend on the phase ie  in the language Le, 
since it is estimated from the Lf-Lp bilingual corpus. 
Thus, equation (3) can be rewritten as  
?=
ip
iiiiii eppfef )|()|()|( ???  (4)
4.2 Lexical Weight 
Given a phrase pair ),( ef  and a word alignment 
a  between the source word positions ni ,...,1=  
and the target word positions mj ,...,1= , the 
lexical weight can be estimated according to the 
following method (Koehn et al, 2003). 
? ?
= ???=
n
i aji
ji efwajij
aefp
1 ),(
)|(
),(|
1
),|(w
 (5)
In order to estimate the lexical weight, we first 
need to obtain the alignment information a  be-
tween the two phrases f  and e , and then estimate 
the lexical translation probability )|( efw  accord-
ing to the alignment information. The alignment 
information of the phrase pair ),( ef  can be in-
duced from the two phrase pairs ),( pf  and ),( ep . 
 
Figure 1. Alignment Information Induction 
Let 1a  and 2a  represent the word alignment in-
formation inside the phrase pairs ),( pf  and ),( ep  
respectively, then the alignment information a  
inside ),( ef  can be obtained as shown in (6). An 
example is shown in Figure 1. 
}),(&),(:|),{( 21 aepapfpefa ???=  (6)
With the induced alignment information, this 
paper proposes a method to estimate the probabil-
ity directly from the induced phrase pairs. We 
name this method phrase method. If we use K to 
denote the number of the induced phrase pairs, we 
estimate the co-occurring frequency of the word 
pair ),( ef  according to the following model. 
??
==
=
n
i
ai
K
k
k i
eeffef
efcount
11
),(),()|(
),(
???  (7)
Where )|( efk?  is the phrase translation probabil-
ity for phrase pair k . 1),( =yx?  if yx = ; other-
wise, 0),( =yx? . Thus, lexical translation prob-
ability can be estimated as in (8). 
?=
'
),'(
),(
)|(
f
efcount
efcount
efw  (8)
We also estimate the lexical translation prob-
ability )|( efw  using the method described in 
(Wang et al, 2006), which is shown in (9). We 
named it word method in this paper. 
);,()|()|()|( pefsimepwpfwefw
p
?= (9)
Where )|( pfw  and )|( epw  are two lexical 
probabilities, and );,( pefsim  is the cross-
language word similarity. 
5 Interpolated Model 
If we have a small Lf-Le bilingual corpus, we can 
employ this corpus to estimate a translation model 
as described in section 3. However, this model may 
perform poorly due to the sparseness of the data. In 
order to improve its performance, we can employ 
the additional Lf-Lp and Lp-Le bilingual corpora. 
Moreover, we can use more than one pivot lan-
guage to improve the translation performance if the 
corresponding bilingual corpora exist. Different 
pivot languages may catch different linguistic phe-
858
nomena, and improve translation quality for the 
desired language pair Lf-Le in different ways. 
If we include n  pivot languages, n  pivot mod-
els can be estimated using the method as described 
in section 4. In order to combine these n  pivot 
models with the standard model trained with the 
Lf-Le corpus, we use the linear interpolation 
method. The phrase translation probability and the 
lexical weight are estimated as shown in (10) and 
(11), respectively. 
?
=
=
n
i
ii efef
0
)|()|( ???  (10)
?
=
=
n
i
ii aefpaefp
0
),|(),|( w,w ?  (11)
Where )|(0 ef?  and ),|( aefpw,0  denote the 
phrase translation probability and lexical weight 
trained with the Lf-Le bilingual corpus, respec-
tively. )|( efi?  and ),|( aefp iw,  ( ni ,...,1= ) are 
the phrase translation probability and lexical 
weight estimated by using the pivot languages. i?  
and i?  are the interpolation coefficients. 
6 Experiments on the Europarl Corpus 
6.1 Data 
A shared task to evaluate machine translation per-
formance was organized as part of the 
NAACL/HLT 2006 Workshop on Statistical Ma-
chine Translation (Koehn and Monz, 2006). The 
shared task used the Europarl corpus (Koehn, 
2005), in which four languages are involved: Eng-
lish, French, Spanish, and German. The shared task 
performed translation between English and the 
other three languages. In our work, we perform 
translation from French to the other three lan-
guages. We select French to Spanish and French to 
German translation that are not in the shared task 
because we want to use English as the pivot lan-
guage. In general, for most of the languages, there 
exist bilingual corpora between these languages 
and English since English is an internationally 
used language. 
Table 1 shows the information about the bilin-
gual training data. In the table, "Fr", "En", "Es", 
and "De" denotes "French", "English", "Spanish", 
and "German", respectively. For the language pairs 
Lf-Le not including English, the bilingual corpus is 
Language 
Pairs 
Sentence 
Pairs 
Source 
Words 
Target 
Words 
Fr-En 688,031 15,323,737 13,808,104
Fr-Es 640,661 14,148,926 13,134,411
Fr-De 639,693 14,215,058 12,155,876
Es-En 730,740 15,676,710 15,222,105
De-En 751,088 15,256,793 16,052,269
De-Es 672,813 13,246,255 14,362,615
Table 1. Training Corpus for European Languages 
extracted from Lf-English and English-Le since 
Europarl corpus is a multilingual corpus.  
For the language models, we use the same data 
provided in the shared task. We also use the same 
development set and test set provided by the shared 
task. The in-domain test set includes 2,000 sen-
tences and the out-of-domain test set includes 
1,064 sentences for each language. 
6.2 Translation System and Evaluation 
Method 
To perform phrase-based SMT, we use Koehn's 
training scripts1 and the Pharaoh decoder (Koehn, 
2004). We run the decoder with its default settings 
and then use Koehn's implementation of minimum 
error rate training (Och, 2003) to tune the feature 
weights on the development set. 
The translation quality was evaluated using a 
well-established automatic measure: BLEU score 
(Papineni et al, 2002). And we also use the tool 
provided in the NAACL/HLT 2006 shared task on 
SMT to calculate the BLEU scores. 
6.3 Comparison of Different Lexical Weights 
As described in section 4, we employ two methods 
to estimate the lexical weight in the translation 
model. In order to compare the two methods, we 
translate from French to Spanish, using English as 
the pivot language. We use the French-English and 
English-Spanish corpora described in Table 1 as 
training data.  During training, before estimating 
the Spanish to French phrase translation probabil-
ity, we filter those French-English and English-
Spanish phrase pairs whose translation probabili-
ties are below a fixed threshold 0.001.2 The trans-
lation results are shown in Table 2. 
                                                 
1  It is located at http://www.statmt.org/wmt06/shared-
task/baseline.htm  
2 In the following experiments using pivot languages, we use 
the same filtering threshold for all of the language pairs. 
859
The phrase method proposed in this paper per-
forms better than the word method proposed in 
(Wang et al, 2006). This is because our method 
uses phrase translation probability as a confidence 
weight to estimate the lexical translation probabil-
ity. It strengthens the frequently aligned pairs and 
weakens the infrequently aligned pairs. Thus, the 
following sections will use the phrase method to 
estimate the lexical weight. 
Method In-Domain Out-of-Domain
Phrase  0.3212 0.2098 
Word 0.2583 0.1672 
Table 2. Results with Different Lexical Weights 
6.4 Results of Using One Pivot Language 
This section describes the translation results by 
using only one pivot language. For the language 
pair French and Spanish, we use English as the 
pivot language. The entire French-English and 
English-Spanish corpora as described in section 4 
are used to train a pivot model for French-Spanish. 
As described in section 5, if we have a small Lf-
Le bilingual corpus and large Lf-Lp and Lp-Le bilin-
gual corpora, we can obtain interpolated models. 
In order to conduct the experiments, we ran-
domly select 5K, 10K, 20K, 30K, 40K, 50K, and 
100K sentence pairs from the French-Spanish cor-
pus. Using each of these corpora, we train a stan-
dard translation model.  
For each standard model, we interpolate it with 
the pivot model to get an interpolated model. The 
interpolation weights are tuned using the develop-
ment set. For all the interpolated models, we set 
9.00 =? , 1.01 =? , 9.00 =? , and 1.01 =? . We 
test the three kinds of models on both the in-
domain and out-of-domain test sets. The results are 
shown in Figures 2 and 3.  
The pivot model achieves BLEU scores of 
0.3212 and 0.2098 on the in-domain and out-of-
domain test set, respectively. It achieves an abso-
lute improvement of 0.05 on both test sets (16.92% 
and 35.35% relative) over the standard model 
trained with 5,000 French-Spanish sentence pairs. 
And the performance of the pivot models are com-
parable with that of the standard models trained 
with 20,000 and 30,000 sentence pairs on the in-
domain and out-of-domain test set, respectively. 
When the French-Spanish training corpus is in-
creased, the standard models quickly outperform 
the pivot model. 
25
27
29
31
33
35
37
5 10 20 30 40 50 100
Fr-Es Data (k pairs)
B
L
E
U
 (%
)
Interpolated
Standard
Pivot 
  
Figure 2. In-Domain French-Spanish Results 
14
16
18
20
22
24
26
5 10 20 30 40 50 100
Fr-Es Data (K pairs)
B
L
E
U
 (%
)
Interpolated
Standard
Pivot 
 
Figure 3. Out-of-Domain French-Spanish Results 
18
20
22
24
26
28
30
5 10 20 30 40 50 100
Fr-En Data (k Pairs)
B
L
E
U
 (%
)
Interpolated
Standard
Pivot 
  
Figure 4. In-Domain French-English Results 
9
10
11
12
13
14
15
16
17
5 10 20 30 40 50 100
Fr-De Data (k Pairs)
B
L
E
U
 (%
)
Interpolated
Standard
Pivot 
 
Figure 5. In-Domain French-German Results 
When only a very small French-Spanish bilin-
gual corpus is available, the interpolated method 
can greatly improve the translation quality. For 
example, when only 5,000 French-Spanish sen-
tence pairs are available, the interpolated model 
outperforms the standard model by achieving a 
relative improvement of 17.55%, with the BLEU 
score improved from 0.2747 to 0.3229. With 
50,000 French-Spanish sentence pairs available, 
the interpolated model significantly3 improves the 
translation quality by achieving an absolute im-
                                                 
3 We conduct the significance test using the same method as 
described in (Koehn and Monz, 2006). 
860
provement of 0.01 BLEU. When the French-
Spanish training corpus increases to 100,000 sen-
tence pairs, the interpolated model achieves almost 
the same result as the standard model. This indi-
cates that our pivot language method is suitable for 
the language pairs with small quantities of training 
data available. 
Besides experiments on French-Spanish transla-
tion, we also conduct translation from French to 
English and French to German, using German and 
English as the pivot language, respectively. The 
results on the in-domain test set4 are shown in Fig-
ures 4 and 5. The tendency of the results is similar 
to that in Figure 2. 
6.5 Results of Using More Than One Pivot 
Language 
For French to Spanish translation, we also intro-
duce German as a pivot language besides English. 
Using these two pivot languages, we build two dif-
ferent pivot models, and then perform linear inter-
polation on them. The interpolation weights for the 
English pivot model and the German pivot model 
are set to 0.6 and 0.4 respectively5. The translation 
results on the in-domain test set are 0.3212, 0.3077, 
and 0.3355 for the pivot models using English, 
German, and both German and English as pivot 
languages, respectively. 
With the pivot model using both English and 
German as pivot languages, we interpolate it with 
the standard models trained with French-Spanish 
corpora of different sizes as described in the above 
section. The comparison of the translation results 
among the interpolated models, standard models, 
and the pivot model are shown in Figure 6. 
It can be seen that the translation results can be 
further improved by using more than one pivot 
language. The pivot model "Pivot-En+De" using 
two pivot languages achieves an absolute im-
provement of 0.06 (22.13% relative) as compared 
with the standard model trained with 5,000 sen-
tence pairs. And it achieves comparable translation 
result as compared with the standard model trained 
with 30,000 French-Spanish sentence pairs. 
The results in Figure 6 also indicate the interpo-
lated models using two pivot languages achieve the 
                                                 
4 The results on the out-of-domain test set are similar to that in 
Figure 3. We only show the in-domain translation results in all 
of the following experiments because of space limit. 
5 The weights are tuned on the development set. 
best results of all. Significance test shows that the 
interpolated models using two pivot languages sig-
nificantly outperform those using one pivot lan-
guage when less than 50,000 French-Spanish sen-
tence pairs are available. 
27
28
29
30
31
32
33
34
35
36
37
5 10 20 30 40 50 100
Fr-Es Data (k Pairs)
B
L
E
U
 (%
)
Interpolated-En+De
Interpolated-En
Interpolated-De
Standard
Pivot-En+De
 
Figure 6. In-Domain French-Spanish Translation 
Results by Using Two Pivot Languages 
6.6 Results by Using Pivot Language Related 
Corpora of Different Sizes 
In all of the above results, the corpora used to train 
the pivot models are not changed. In order to ex-
amine the effect of the size of the pivot corpora, 
we decrease the French-English and English-
French corpora. We randomly select 200,000 and 
400,000 sentence pairs from both of them to train 
two pivot models, respectively. The translation 
results on the in-domain test set are 0.2376, 0.2954, 
and 0.3212 for the pivot models trained with 
200,000, 400,000, and the entire French-English 
and English-Spanish corpora, respectively. The 
results of the interpolated models and the standard 
models are shown in Figure 7. The results indicate 
that the larger the training corpora used to train the 
pivot model are, the better the translation quality is. 
27
28
29
30
31
32
33
34
35
36
37
5 10 20 30 40 50 100
Fr-Es Data (k pairs)
B
L
E
U
 (%
)
Interpolated-All
interpolated-400k
Interpolated-200k
Standard
 
Figure 7. In-Domain French-Spanish Results by 
Using Lf-Lp and Lp-Le Corpora of Different Sizes 
861
7 Experiments on Chinese to Japanese 
Translation 
In section 6, translation results on the Europarl 
multilingual corpus indicate the effectiveness of 
our method. To investigate the effectiveness of our 
method by using independently sourced parallel 
corpora, we conduct Chinese-Japanese translation 
using English as a pivot language in this section, 
where the training data are not limited to a specific 
domain. 
The data used for this experiment is the same as 
those used in (Wang et al, 2006). There are 21,977, 
329,350, and 160,535 sentence pairs for the lan-
guage pairs Chinese-Japanese, Chinese-English, 
and English-Japanese, respectively. The develop-
ment data and testing data include 500 and 1,000 
Chinese sentences respectively, with one reference 
for each sentence. For Japanese language model 
training, we use about 100M bytes Japanese corpus. 
The translation result is shown in Figure 8. The 
pivot model only outperforms the standard model 
trained with 2,500 sentence pairs. This is because 
(1) the corpora used to train the pivot model are 
smaller as compared with the Europarl corpus; (2) 
the training data and the testing data are not limited 
to a specific domain; (3) The languages are not 
closely related. 
6
8
10
12
14
16
18
2.5 5 10 21.9
Chinese-Japanese Data (k pairs)
B
L
E
U
 (%
)
Interpolated
Standard
Pivot 
  
Figure 8. Chinese-Japanese Translation Results 
The interpolated models significantly outper-
form the other models. When only 5,000 sentence 
pairs are available, the BLEU score increases rela-
tively by 20.53%. With the entire (21,977 pairs) 
Chinese-Japanese available, the interpolated model 
relatively increases the BLEU score by 5.62%, 
from 0.1708 to 0.1804. 
8 Conclusion 
This paper proposed a novel method for phrase-
based SMT on language pairs with a small bilin-
gual corpus by bringing in pivot languages. To per-
form translation between Lf and Le, we bring in a 
pivot language Lp, via which the large corpora of 
Lf-Lp and Lp-Le can be used to induce a translation 
model for Lf-Le. The advantage of this method is 
that it can perform translation between the lan-
guage pair Lf-Le even if no bilingual corpus for this 
pair is available. Using BLEU as a metric, our 
method achieves an absolute improvement of 0.06 
(22.13% relative) as compared with the model di-
rectly trained with 5,000 sentence pairs for French-
Spanish translation. And the translation quality is 
comparable with that of the model directly trained 
with 30,000 French-Spanish sentence pairs. The 
results also indicate that using more pivot lan-
guages leads to better translation quality. 
With a small bilingual corpus available for Lf-Le, 
we built a translation model, and interpolated it 
with the pivot model trained with the large Lf-Lp 
and Lp-Le bilingual corpora. The results on both 
the Europarl corpus and Chinese-Japanese transla-
tion indicate that the interpolated models achieve 
the best results. Results also indicate that our pivot 
language approach is suitable for translation on 
language pairs with a small bilingual corpus. The 
less the Lf-Le bilingual corpus is, the bigger the 
improvement is. 
We also performed experiments using Lf-Lp and 
Lp-Le corpora of different sizes. The results indi-
cate that using larger training corpora to train the 
pivot model leads to better translation quality. 
References 
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas. 
2000. Learning Dependency Translation Models as 
Collections of Finite-State Head Transducers. Com-
putational Linguistics, 26(1):45-60. 
Niraj Aswani and Robert Gaizauskas. 2005. Aligning 
Words in English-Hindi Parallel Corpora. In Proc. of 
the ACL 2005 Workshop on Building and Using Par-
allel Texts: Data-driven Machine Translation and 
Beyond, pages 115-118. 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics, 19(2): 
263-311. 
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Transla-
862
tion Using Paraphrases. In Proc. of NAACL-2006, 
pages 17-24. 
Lars Borin. 2000. You'll Take the High Road and I'll 
Take the Low Road: Using a Third Language to Im-
prove Bilingual Word Alignment. In Proc. of COL-
ING-2000, pages 97-103. 
David Chiang. 2005. A Hierarchical Phrase-Based 
Model for Statistical Machine Translation. In Proc. 
of ACL-2005, pages 263-270. 
Mona Diab  and  Philip Resnik. 2002. An Unsupervised 
Method for Word Sense Tagging using Parallel Cor-
pora. In Proc. of ACL-2002, pages 255-262. 
Tim Gollins and Mark Sanderson. 2001. Improving 
Cross Language Information Retrieval with Triangu-
lated Translation. In Proc. of ACM SIGIR-2001, 
pages 90-95. 
Kazuaki Kishida and Noriko Kando.  2003. Two-Stage 
Refinement of Query Translation in a Pivot Lan-
guage Approach to Cross-Lingual Information Re-
trieval: An Experiment at CLEF 2003. In Proc. of 
CLEF-2003. pages 253-262. 
Philipp Koehn. 2004. Pharaoh: A Beam Search Decoder 
for Phrase-Based Statistical Machine Translation 
Models. In Proc. of AMTA-2004, pages 115-124. 
Philipp Koehn. 2005. Europarl: A Parallel Corpus for 
Statistical Machine Translation. In Proc. of MT 
Summit X, pages 79-86. 
Philipp Koehn and Christof Monz. 2006. Manual and 
Automatic Evaluation of Machine Translation be-
tween European Languages. In Proc. of the 2006 
HLT-NAACL Workshop on Statistical Machine 
Translation, pages 102-121. 
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In Proc. 
of HLT-NAAC- 2003, pages 127-133. 
Adam Lopez and Philip Resnik. 2005. Improved HMM 
Alignment Models for Languages with Scarce Re-
sources. In Proc. of the ACL-2005 Work-shop on 
Building and Using Parallel Texts: Data-driven Ma-
chine Translation and Beyond, pages 83-86. 
Joel Martin, Rada Mihalcea, and Ted Pedersen. 2005. 
Word Alignment for Languages with Scarce Re-
sources. In Proc. of the ACL-2005 Workshop on 
Building and Using Parallel Texts: Data-driven Ma-
chine Translation and Beyond, pages 65-74. 
Dan Melamed. 2004. Statistical Machine Translation by 
Parsing. In Proc. of ACL-2004, pages 653-660. 
Bart Mellebeek, Karolina Owczarzak, Declan Groves, 
Josef Van Genabith, and Andy Way. 2006. A Syntac-
tic Skeleton for Statistical Machine Translation. In 
Proc. of EAMT-2006, pages 195-202. 
Sonja Niessen and Hermann Ney. 2004. Statistical 
Machine Translation with Scarce Resources Using 
Morpho-Syntactic Information. Computational 
linguistics, 30(2): 181-204. 
Franz Josef Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proc. of ACL-
2003, pages 160-167. 
Franz Josef Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine 
Translation. Computational Linguistics, 30(4):417-
449. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. In Proc. of ACL-
2002, pages 311-318. 
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. 
Dependency Treelet Translation: Syntactically In-
formed Phrasal SMT. In Proc. of ACL-2005, pages 
271-279. 
Tetsuya Sakai, Makoto Koyama, Akira Kumano, and 
Toshihiko Manabe. 2004. Toshiba BRIDJE at 
NTCIR-4 CLIR: Monolingual/Bilingual IR and 
Flexible Feedback. In Proc. of NTCIR 4. 
Charles Schafer and David Yarowsky. 2002. Inducing 
Translation Lexicons via Diverse Similarity Meas-
ures and Bridge Languages. In Proc. of CoNLL-2002, 
pages 1-7. 
Haifeng Wang, Hua Wu, and Zhanyi Liu. 2006. Word 
Alignment for Languages with Scarce Resources Us-
ing Bilingual Corpora of Other Language Pairs. In 
Proc. of COLING/ACL-2006 Main Conference 
Poster Sessions, pages 874-881. 
Dan Tufis, Radu Ion, Alexandru Ceausu, and Dan Ste-
fanescu. 2005. Combined Word Alignments. In Proc. 
of the ACL-2005 Workshop on Building and Using 
Parallel Texts: Data-driven Machine Translation and 
Beyond, pages 107-110. 
Vincent Vandeghinste, Ineka Schuurman, Michael Carl, 
Stella Markantonatou, and Toni Badia. 2006. 
METIS-II: Machine Translation for Low-Resource 
Languages. In Proc. of LREC-2006, pages 1284-1289. 
Dekai Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Corpora. 
Computational Linguistics, 23(3):377-403. 
Kenji Yamada and Kevin Knight. 2001. A Syntax Based 
Statistical Translation Model. In Proc. of ACL-2001, 
pages 523-530. 
863
Proceedings of ACL-08: HLT, pages 780?788,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Pivot Approach for Extracting Paraphrase Patterns from Bilingual Corpora
Shiqi Zhao1, Haifeng Wang2, Ting Liu1, Sheng Li1
1Harbin Institute of Technology, Harbin, China
{zhaosq,tliu,lisheng}@ir.hit.edu.cn
2Toshiba (China) Research and Development Center, Beijing, China
wanghaifeng@rdc.toshiba.com.cn
Abstract
Paraphrase patterns are useful in paraphrase
recognition and generation. In this paper, we
present a pivot approach for extracting para-
phrase patterns from bilingual parallel cor-
pora, whereby the English paraphrase patterns
are extracted using the sentences in a for-
eign language as pivots. We propose a log-
linear model to compute the paraphrase likeli-
hood of two patterns and exploit feature func-
tions based on maximum likelihood estima-
tion (MLE) and lexical weighting (LW). Us-
ing the presented method, we extract over
1,000,000 pairs of paraphrase patterns from
2M bilingual sentence pairs, the precision
of which exceeds 67%. The evaluation re-
sults show that: (1) The pivot approach is
effective in extracting paraphrase patterns,
which significantly outperforms the conven-
tional method DIRT. Especially, the log-linear
model with the proposed feature functions
achieves high performance. (2) The coverage
of the extracted paraphrase patterns is high,
which is above 84%. (3) The extracted para-
phrase patterns can be classified into 5 types,
which are useful in various applications.
1 Introduction
Paraphrases are different expressions that convey
the same meaning. Paraphrases are important in
plenty of natural language processing (NLP) ap-
plications, such as question answering (QA) (Lin
and Pantel, 2001; Ravichandran and Hovy, 2002),
machine translation (MT) (Kauchak and Barzilay,
2006; Callison-Burch et al, 2006), multi-document
summarization (McKeown et al, 2002), and natural
language generation (Iordanskaja et al, 1991).
Paraphrase patterns are sets of semantically
equivalent patterns, in which a pattern generally
contains two parts, i.e., the pattern words and slots.
For example, in the pattern ?X solves Y?, ?solves? is
the pattern word, while ?X? and ?Y? are slots. One
can generate a text unit (phrase or sentence) by fill-
ing the pattern slots with specific words. Paraphrase
patterns are useful in both paraphrase recognition
and generation. In paraphrase recognition, if two
text units match a pair of paraphrase patterns and the
corresponding slot-fillers are identical, they can be
identified as paraphrases. In paraphrase generation,
a text unit that matches a pattern P can be rewritten
using the paraphrase patterns of P.
A variety of methods have been proposed on para-
phrase patterns extraction (Lin and Pantel, 2001;
Ravichandran and Hovy, 2002; Shinyama et al,
2002; Barzilay and Lee, 2003; Ibrahim et al, 2003;
Pang et al, 2003; Szpektor et al, 2004). However,
these methods have some shortcomings. Especially,
the precisions of the paraphrase patterns extracted
with these methods are relatively low.
In this paper, we extract paraphrase patterns from
bilingual parallel corpora based on a pivot approach.
We assume that if two English patterns are aligned
with the same pattern in another language, they are
likely to be paraphrase patterns. This assumption
is an extension of the one presented in (Bannard
and Callison-Burch, 2005), which was used for de-
riving phrasal paraphrases from bilingual corpora.
Our method involves three steps: (1) corpus prepro-
cessing, including English monolingual dependency
780
parsing and English-foreign language word align-
ment, (2) aligned patterns induction, which produces
English patterns along with the aligned pivot pat-
terns in the foreign language, (3) paraphrase pat-
terns extraction, in which paraphrase patterns are ex-
tracted based on a log-linear model.
Our contributions are as follows. Firstly, we are
the first to use a pivot approach to extract paraphrase
patterns from bilingual corpora, though similar
methods have been used for learning phrasal para-
phrases. Our experiments show that the pivot ap-
proach significantly outperforms conventional meth-
ods. Secondly, we propose a log-linear model for
computing the paraphrase likelihood. Besides, we
use feature functions based on maximum likeli-
hood estimation (MLE) and lexical weighting (LW),
which are effective in extracting paraphrase patterns.
Using the proposed approach, we extract over
1,000,000 pairs of paraphrase patterns from 2M
bilingual sentence pairs, the precision of which is
above 67%. Experimental results show that the pivot
approach evidently outperforms DIRT, a well known
method that extracts paraphrase patterns frommono-
lingual corpora (Lin and Pantel, 2001). Besides, the
log-linear model is more effective than the conven-
tional model presented in (Bannard and Callison-
Burch, 2005). In addition, the coverage of the ex-
tracted paraphrase patterns is high, which is above
84%. Further analysis shows that 5 types of para-
phrase patterns can be extracted with our method,
which can by used in multiple NLP applications.
The rest of this paper is structured as follows.
Section 2 reviews related work on paraphrase pat-
terns extraction. Section 3 presents our method in
detail. We evaluate the proposed method in Section
4, and finally conclude this paper in Section 5.
2 Related Work
Paraphrase patterns have been learned and used in
information extraction (IE) and answer extraction of
QA. For example, Lin and Pantel (2001) proposed a
method (DIRT), in which they obtained paraphrase
patterns from a parsed monolingual corpus based on
an extended distributional hypothesis, where if two
paths in dependency trees tend to occur in similar
contexts it is hypothesized that the meanings of the
paths are similar. The examples of obtained para-
(1) X solves Y
Y is solved by X
X finds a solution to Y
......
(2) born in <ANSWER> , <NAME>
<NAME> was born on <ANSWER> ,
<NAME> ( <ANSWER> -
......
(3) ORGANIZATION decides ?
ORGANIZATION confirms ?
......
Table 1: Examples of paraphrase patterns extracted with
the methods of Lin and Pantel (2001), Ravichandran and
Hovy (2002), and Shinyama et al (2002).
phrase patterns are shown in Table 1 (1).
Based on the same hypothesis as above, some
methods extracted paraphrase patterns from the web.
For instance, Ravichandran and Hovy (2002) de-
fined a question taxonomy for their QA system.
They then used hand-crafted examples of each ques-
tion type as queries to retrieve paraphrase patterns
from the web. For instance, for the question type
?BIRTHDAY?, The paraphrase patterns produced by
their method can be seen in Table 1 (2).
Similar methods have also been used by Ibrahim
et al (2003) and Szpektor et al (2004). The main
disadvantage of the above methods is that the pre-
cisions of the learned paraphrase patterns are rela-
tively low. For instance, the precisions of the para-
phrase patterns reported in (Lin and Pantel, 2001),
(Ibrahim et al, 2003), and (Szpektor et al, 2004)
are lower than 50%. Ravichandran and Hovy (2002)
did not directly evaluate the precision of the para-
phrase patterns extracted using their method. How-
ever, the performance of their method is dependent
on the hand-crafted queries for web mining.
Shinyama et al (2002) presented a method that
extracted paraphrase patterns from multiple news ar-
ticles about the same event. Their method was based
on the assumption that NEs are preserved across
paraphrases. Thus the method acquired paraphrase
patterns from sentence pairs that share comparable
NEs. Some examples can be seen in Table 1 (3).
The disadvantage of this method is that it greatly
relies on the number of NEs in sentences. The preci-
781
start Palestinian suicide bomber blew himself up in SLOT1 on SLOT2
killing SLOT3 other people and injuringwounding SLOT4 end
detroit the*e*
a
?s*e* buildingbuilding in detroit
flattened
groundlevelled
to
blastedleveled*e*wasreduced
razedleveled
to downrubble
into ashes*e*
to *e*
(1)
(2)
Figure 1: Examples of paraphrase patterns extracted by
Barzilay and Lee (2003) and Pang et al (2003).
sion of the extracted patterns may sharply decrease
if the sentences do not contain enough NEs.
Barzilay and Lee (2003) applied multi-sequence
alignment (MSA) to parallel news sentences and in-
duced paraphrase patterns for generating new sen-
tences (Figure 1 (1)). Pang et al (2003) built finite
state automata (FSA) from semantically equivalent
translation sets based on syntactic alignment. The
learned FSAs could be used in paraphrase represen-
tation and generation (Figure 1 (2)). Obviously, it
is difficult for a sentence to match such complicated
patterns, especially if the sentence is not from the
same domain in which the patterns are extracted.
Bannard and Callison-Burch (2005) first ex-
ploited bilingual corpora for phrasal paraphrase ex-
traction. They assumed that if two English phrases
e1 and e2 are aligned with the same phrase c in
another language, these two phrases may be para-
phrases. Specifically, they computed the paraphrase
probability in terms of the translation probabilities:
p(e2|e1) =
?
c
pMLE(c|e1)pMLE(e2|c) (1)
In Equation (1), pMLE(c|e1) and pMLE(e2|c) are
the probabilities of translating e1 to c and c to e2,
which are computed based on MLE:
pMLE(c|e1) =
count(c, e1)
?
c? count(c
?, e1)
(2)
where count(c, e1) is the frequency count that
phrases c and e1 are aligned in the corpus.
pMLE(e2|c) is computed in the same way.
This method proved effective in extracting high
quality phrasal paraphrases. As a result, we extend
it to paraphrase pattern extraction in this paper.
S T E (take)
should
We take
market
into
consideration
take
market
into
consideration
take
into
consideration
P S T E (take)
first
T E
demand
demand
Figure 2: Examples of a subtree and a partial subtree.
3 Proposed Method
3.1 Corpus Preprocessing
In this paper, we use English paraphrase patterns ex-
traction as a case study. An English-Chinese (E-
C) bilingual parallel corpus is employed for train-
ing. The Chinese part of the corpus is used as pivots
to extract English paraphrase patterns. We conduct
word alignment with Giza++ (Och and Ney, 2000) in
both directions and then apply the grow-diag heuris-
tic (Koehn et al, 2005) for symmetrization.
Since the paraphrase patterns are extracted from
dependency trees, we parse the English sentences
in the corpus with MaltParser (Nivre et al, 2007).
Let SE be an English sentence, TE the parse tree
of SE , e a word of SE , we define the subtree and
partial subtree following the definitions in (Ouan-
graoua et al, 2007). In detail, a subtree STE(e)
is a particular connected subgraph of the tree TE ,
which is rooted at e and includes all the descendants
of e. A partial subtree PSTE(e) is a connected sub-
graph of the subtree STE(e), which is rooted at e but
does not necessarily include all the descendants of e.
For instance, for the sentence ?We should first take
market demand into consideration?, STE(take) and
PSTE(take) are shown in Figure 21.
3.2 Aligned Patterns Induction
To induce the aligned patterns, we first induce the
English patterns using the subtrees and partial sub-
trees. Then, we extract the pivot Chinese patterns
aligning to the English patterns.
1Note that, a subtree may contain several partial subtrees. In
this paper, all the possible partial subtrees are considered when
extracting paraphrase patterns.
782
Algorithm 1: Inducing an English pattern
1: Input: words in STE(e) : wiwi+1...wj
2: Input: PE(e) = ?
3: For each wk (i ? k ? j)
4: If wk is in PSTE(e)
5: Append wk to the end of PE(e)
6: Else
7: Append POS(wk) to the end of PE(e)
8: End For
Algorithm 2: Inducing an aligned pivot pattern
1: Input: SC = t1t2...tn
2: Input: PC = ?
3: For each tl (1 ? l ? n)
4: If tl is aligned with wk in SE
5: If wk is a word in PE(e)
6: Append tl to the end of PC
7: If POS(wk) is a slot in PE(e)
8: Append POS(wk) to the end of PC
9: End For
Step-1 Inducing English patterns. In this paper, an
English pattern PE(e) is a string comprising words
and part-of-speech (POS) tags. Our intuition for
inducing an English pattern is that a partial sub-
tree PSTE(e) can be viewed as a unit that conveys
a definite meaning, though the words in PSTE(e)
may not be continuous. For example, PSTE(take)
in Figure 2 contains words ?take ... into consid-
eration?. Therefore, we may extract ?take X into
consideration? as a pattern. In addition, the words
that are in STE(e) but not in PSTE(e) (denoted as
STE(e)/PSTE(e)) are also useful for inducing pat-
terns, since they can constrain the pattern slots. In
the example in Figure 2, the word ?demand? indi-
cates that a noun can be filled in the slot X and the
pattern may have the form ?take NN into considera-
tion?. Based on this intuition, we induce an English
pattern PE(e) as in Algorithm 12.
For the example in Figure 2, the generated pat-
tern PE(take) is ?take NN NN into considera-
tion?. Note that the patterns induced in this way
are quite specific, since the POS of each word in
STE(e)/PSTE(e) forms a slot. Such patterns are
difficult to be matched in applications. We there-
2POS(wk) in Algorithm 1 denotes the POS tag of wk.
N N _1 ?? NN _2 NN_1 ?? N N _2
NN_1NN_2 considered byis NN_1 consider NN_2
Figure 3: Aligned patterns with numbered slots.
fore take an additional step to simplify the patterns.
Let ei and ej be two words in STE(e)/PSTE(e),
whose POS posi and posj are slots in PE(e). If ei
is a descendant of ej in the parse tree, we remove
posi from PE(e). For the example above, the POS
of ?market? is removed, since it is the descendant of
?demand?, whose POS also forms a slot. The sim-
plified pattern is ?take NN into consideration?.
Step-2 Extracting pivot patterns. For each En-
glish pattern PE(e), we extract an aligned Chinese
pivot pattern PC . Let a Chinese sentence SC be the
translation of the English sentence SE , PE(e) a pat-
tern induced from SE , we extract the pivot pattern
PC aligning to PE(e) as in Algorithm 2. Note that
the Chinese patterns are not extracted from parse
trees. They are only sequences of Chinese words
and POSes that are aligned with English patterns.
A pattern may contain two or more slots shar-
ing the same POS. To distinguish them, we assign
a number to each slot in the aligned E-C patterns. In
detail, the slots having identical POS in PC are num-
bered incrementally (i.e., 1,2,3...), while each slot in
PE(e) is assigned the same number as its aligned
slot in PC . The examples of the aligned patterns
with numbered slots are illustrated in Figure 3.
3.3 Paraphrase Patterns Extraction
As mentioned above, if patterns e1 and e2 are
aligned with the same pivot pattern c, e1 and e2 may
be paraphrase patterns. The paraphrase likelihood
can be computed using Equation (1). However, we
find that using only the MLE based probabilities can
suffer from data sparseness. In order to exploit more
and richer information to estimate the paraphrase
likelihood, we propose a log-linear model:
score(e2|e1) =
?
c
exp[
N?
i=1
?ihi(e1, e2, c)] (3)
where hi(e1, e2, c) is a feature function and ?i is the
783
weight. In this paper, 4 feature functions are used in
our log-linear model, which include:
h1(e1, e2, c) = scoreMLE(c|e1)
h2(e1, e2, c) = scoreMLE(e2|c)
h3(e1, e2, c) = scoreLW (c|e1)
h4(e1, e2, c) = scoreLW (e2|c)
Feature functions h1(e1, e2, c) and h2(e1, e2, c)
are based on MLE. scoreMLE(c|e) is computed as:
scoreMLE(c|e) = log pMLE(c|e) (4)
scoreMLE(e|c) is computed in the same way.
h3(e1, e2, c) and h4(e1, e2, c) are based on LW.
LW was originally used to validate the quality of a
phrase translation pair in MT (Koehn et al, 2003). It
checks how well the words of the phrases translate
to each other. This paper uses LW to measure the
quality of aligned patterns. We define scoreLW (c|e)
as the logarithm of the lexical weight3:
scoreLW (c|e) =
1
n
n?
i=1
log(
1
|{j|(i, j) ? a}|
?
?(i,j)?a
w(ci|ej)) (5)
where a denotes the word alignment between c and
e. n is the number of words in c. ci and ej are words
of c and e. w(ci|ej) is computed as follows:
w(ci|ej) =
count(ci, ej)
?
c?i
count(c?i, ej)
(6)
where count(ci, ej) is the frequency count of
the aligned word pair (ci, ej) in the corpus.
scoreLW (e|c) is computed in the same manner.
In our experiments, we set a threshold T . If the
score between e1 and e2 based on Equation (3) ex-
ceeds T , e2 is extracted as the paraphrase of e1.
3.4 Parameter Estimation
Five parameters need to be estimated, i.e., ?1, ?2,
?3, ?4 in Equation (3), and the threshold T . To
estimate the parameters, we first construct a devel-
opment set. In detail, we randomly sample 7,086
3The logarithm of the lexical weight is divided by n so as
not to penalize long patterns.
groups of aligned E-C patterns that are obtained as
described in Section 3.2. The English patterns in
each group are all aligned with the same Chinese
pivot pattern. We then extract paraphrase patterns
from the aligned patterns as described in Section 3.3.
In this process, we set ?i = 1 (i = 1, ..., 4) and as-
sign T a minimum value, so as to obtain all possible
paraphrase patterns.
A total of 4,162 pairs of paraphrase patterns have
been extracted and manually labeled as ?1? (correct
paraphrase patterns) or ?0? (incorrect). Here, two
patterns are regarded as paraphrase patterns if they
can generate paraphrase fragments by filling the cor-
responding slots with identical words. We use gra-
dient descent algorithm (Press et al, 1992) to esti-
mate the parameters. For each set of parameters, we
compute the precision P , recall R, and f-measure
F as: P = |set1?set2||set1| , R =
|set1?set2|
|set2| , F =
2PR
P+R ,
where set1 denotes the set of paraphrase patterns ex-
tracted under the current parameters. set2 denotes
the set of manually labeled correct paraphrase pat-
terns. We select the parameters that can maximize
the F-measure on the development set4.
4 Experiments
The E-C parallel corpus in our experiments was con-
structed using several LDC bilingual corpora5. After
filtering sentences that are too long (> 40 words) or
too short (< 5 words), 2,048,009 pairs of parallel
sentences were retained.
We used two constraints in the experiments to im-
prove the efficiency of computation. First, only sub-
trees containing no more than 10 words were used to
induce English patterns. Second, although any POS
tag can form a slot in the induced patterns, we only
focused on three kinds of POSes in the experiments,
i.e., nouns (tags include NN, NNS, NNP, NNPS),
verbs (VB, VBD, VBG, VBN, VBP, VBZ), and ad-
jectives (JJ, JJS, JJR). In addition, we constrained
that a pattern must contain at least one content word
4The parameters are: ?1 = 0.0594137, ?2 = 0.995936,
?3 = ?0.0048954, ?4 = 1.47816, T = ?10.002.
5The corpora include LDC2000T46, LDC2000T47,
LDC2002E18, LDC2002T01, LDC2003E07, LDC2003E14,
LDC2003T17, LDC2004E12, LDC2004T07, LDC2004T08,
LDC2005E83, LDC2005T06, LDC2005T10, LDC2006E24,
LDC2006E34, LDC2006E85, LDC2006E92, LDC2006T04,
LDC2007T02, LDC2007T09.
784
Method #PP (pairs) Precision
LL-Model 1,058,624 67.03%
MLE-Model 1,015,533 60.60%
DIRT top-1 1,179 19.67%
DIRT top-5 5,528 18.73%
Table 2: Comparison of paraphrasing methods.
so as to filter patterns like ?the [NN 1]?.
4.1 Evaluation of the Log-linear Model
As previously mentioned, in the log-linear model of
this paper, we use both MLE based and LW based
feature functions. In this section, we evaluate the
log-linear model (LL-Model) and compare it with
the MLE based model (MLE-Model) presented by
Bannard and Callison-Burch (2005)6.
We extracted paraphrase patterns using two mod-
els, respectively. From the results of each model,
we randomly picked 3,000 pairs of paraphrase pat-
terns to evaluate the precision. The 6,000 pairs of
paraphrase patterns were mixed and presented to the
human judges, so that the judges cannot know by
which model each pair was produced. The sampled
patterns were then manually labeled and the preci-
sion was computed as described in Section 3.4.
The number of the extracted paraphrase patterns
(#PP) and the precision are depicted in the first two
lines of Table 2. We can see that the numbers of
paraphrase patterns extracted using the two mod-
els are comparable. However, the precision of LL-
Model is significantly higher than MLE-Model.
Actually, MLE-Model is a special case of LL-
Model and the enhancement of the precision is
mainly due to the use of LW based features.
It is not surprising, since Bannard and Callison-
Burch (2005) have pointed out that word alignment
error is the major factor that influences the perfor-
mance of the methods learning paraphrases from
bilingual corpora. The LW based features validate
the quality of word alignment and assign low scores
to those aligned E-C pattern pairs with incorrect
alignment. Hence the precision can be enhanced.
6In this experiment, we also estimated a threshold T ? for
MLE-Model using the development set (T ? = ?5.1). The pat-
tern pairs whose score based on Equation (1) exceed T ? were
extracted as paraphrase patterns.
4.2 Comparison with DIRT
It is necessary to compare our method with another
paraphrase patterns extraction method. However, it
is difficult to find methods that are suitable for com-
parison. Some methods only extract paraphrase pat-
terns using news articles on certain topics (Shinyama
et al, 2002; Barzilay and Lee, 2003), while some
others need seeds as initial input (Ravichandran and
Hovy, 2002). In this paper, we compare our method
with DIRT (Lin and Pantel, 2001), which does not
need to specify topics or input seeds.
As mentioned in Section 2, DIRT learns para-
phrase patterns from a parsed monolingual corpus
based on an extended distributional hypothesis. In
our experiment, we implemented DIRT and ex-
tracted paraphrase patterns from the English part of
our bilingual parallel corpus. Our corpus is smaller
than that reported in (Lin and Pantel, 2001). To alle-
viate the data sparseness problem, we only kept pat-
terns appearing more than 10 times in the corpus for
extracting paraphrase patterns. Different from our
method, no threshold was set in DIRT. Instead, the
extracted paraphrase patterns were ranked accord-
ing to their scores. In our experiment, we kept top-5
paraphrase patterns for each target pattern.
From the extracted paraphrase patterns, we sam-
pled 600 groups for evaluation. Each group com-
prises a target pattern and its top-5 paraphrase pat-
terns. The sampled data were manually labeled and
the top-n precision was calculated as
PN
i=1 ni
N?n , where
N is the number of groups and ni is the number of
correct paraphrase patterns in the top-n paraphrase
patterns of the i-th group. The top-1 and top-5 re-
sults are shown in the last two lines of Table 2. Al-
though there are more correct patterns in the top-5
results, the precision drops sequentially from top-1
to top-5 since the denominator of top-5 is 4 times
larger than that of top-1.
Obviously, the number of the extracted para-
phrase patterns is much smaller than that extracted
using our method. Besides, the precision is also
much lower. We believe that there are two reasons.
First, the extended distributional hypothesis is not
strict enough. Patterns sharing similar slot-fillers do
not necessarily have the same meaning. They may
even have the opposite meanings. For example, ?X
worsens Y? and ?X solves Y? were extracted as para-
785
Type Count Example
trivial change 79 (e1) all the members of [NNPS 1] (e2) all members of [NNPS 1]
phrase replacement 267 (e1) [JJ 1] economic losses (e2) [JJ 1] financial losses
phrase reordering 56 (e1) [NN 1] definition (e2) the definition of [NN 1]
structural paraphrase 71 (e1) the admission of [NNP 1] to the wto (e2) the [NNP 1] ?s wto accession
information + or - 27 (e1) [NNS 1] are in fact women (e2) [NNS 1] are women
Table 3: The statistics and examples of each type of paraphrase patterns.
phrase patterns by DIRT. The other reason is that
DIRT can only be effective for patterns appearing
plenty of times in the corpus. In other words, it seri-
ously suffers from data sparseness. We believe that
DIRT can perform better on a larger corpus.
4.3 Pivot Pattern Constraints
As described in Section 3.2, we constrain that the
pattern words of an English pattern e must be ex-
tracted from a partial subtree. However, we do not
have such constraint on the Chinese pivot patterns.
Hence, it is interesting to investigate whether the
performance can be improved if we constrain that
the pattern words of a pivot pattern c must also be
extracted from a partial subtree.
To conduct the evaluation, we parsed the Chinese
sentences of the corpus with a Chinese dependency
parser (Liu et al, 2006). We then induced English
patterns and extracted aligned pivot patterns. For the
aligned patterns (e, c), if c?s pattern words were not
extracted from a partial subtree, the pair was filtered.
After that, we extracted paraphrase patterns, from
which we sampled 3,000 pairs for evaluation.
The results show that 736,161 pairs of paraphrase
patterns were extracted and the precision is 65.77%.
Compared with Table 2, the number of the extracted
paraphrase patterns gets smaller and the precision
also gets lower. The results suggest that the perfor-
mance of the method cannot be improved by con-
straining the extraction of pivot patterns.
4.4 Analysis of the Paraphrase Patterns
We sampled 500 pairs of correct paraphrase pat-
terns extracted using our method and analyzed the
types. We found that there are 5 types of para-
phrase patterns, which include: (1) trivial change,
such as changes of prepositions and articles, etc; (2)
phrase replacement; (3) phrase reordering; (4) struc-
tural paraphrase, which contain both phrase replace-
ments and phrase reordering; (5) adding or reducing
information that does not change the meaning. Some
statistics and examples are shown in Table 3.
The paraphrase patterns are useful in NLP appli-
cations. Firstly, over 50% of the paraphrase patterns
are in the type of phrase replacement, which can
be used in IE pattern reformulation and sentence-
level paraphrase generation. Compared with phrasal
paraphrases, the phrase replacements in patterns are
more accurate due to the constraints of the slots.
The paraphrase patterns in the type of phrase re-
ordering can also be used in IE pattern reformula-
tion and sentence paraphrase generation. Especially,
in sentence paraphrase generation, this type of para-
phrase patterns can reorder the phrases in a sentence,
which can hardly be achieved by the conventional
MT-based generation method (Quirk et al, 2004).
The structural paraphrase patterns have the advan-
tages of both phrase replacement and phrase reorder-
ing. More paraphrase sentences can be generated
using these patterns.
The paraphrase patterns in the type of ?informa-
tion + and -? are useful in sentence compression and
expansion. A sentence matching a long pattern can
be compressed by paraphrasing it using shorter pat-
terns. Similarly, a short sentence can be expanded
by paraphrasing it using longer patterns.
For the 3,000 pairs of test paraphrase patterns, we
also investigate the number and type of the pattern
slots. The results are summarized in Table 4 and 5.
From Table 4, we can see that more than 92%
of the paraphrase patterns contain only one slot,
just like the examples shown in Table 3. In addi-
tion, about 7% of the paraphrase patterns contain
two slots, such as ?give [NN 1] [NN 2]? vs. ?give
[NN 2] to [NN 1]?. This result suggests that our
method tends to extract short paraphrase patterns,
786
Slot No. #PP Percentage Precision
1-slot 2,780 92.67% 66.51%
2-slots 218 7.27% 73.85%
?3-slots 2 <1% 50.00%
Table 4: The statistics of the numbers of pattern slots.
Slot Type #PP Percentage Precision
N-slots 2,376 79.20% 66.71%
V-slots 273 9.10% 70.33%
J-slots 438 14.60% 70.32%
Table 5: The statistics of the type of pattern slots.
which is mainly because the data sparseness prob-
lem is more serious when extracting long patterns.
From Table 5, we can find that near 80% of the
paraphrase patterns contain noun slots, while about
9% and 15% contain verb slots and adjective slots7.
This result implies that nouns are the most typical
variables in paraphrase patterns.
4.5 Evaluation within Context Sentences
In Section 4.1, we have evaluated the precision of
the paraphrase patterns without considering context
information. In this section, we evaluate the para-
phrase patterns within specific context sentences.
The open test set includes 119 English sentences.
We parsed the sentences with MaltParser and in-
duced patterns as described in Section 3.2. For each
pattern e in sentence SE , we searched e?s paraphrase
patterns from the database of the extracted para-
phrase patterns. The result shows that 101 of the
119 sentences contain at least one pattern that can
be paraphrased using the extracted paraphrase pat-
terns, the coverage of which is 84.87%.
Furthermore, since a pattern may have several
paraphrase patterns, we exploited a method to au-
tomatically select the best one in the given context
sentence. In detail, a paraphrase pattern e? of e was
reranked based on a language model (LM):
score(e?|e, SE) =
?scoreLL(e
?|e) + (1 ? ?)scoreLM (e
?|SE) (7)
7Notice that, a pattern may contain more than one type of
slots, thus the sum of the percentages is larger than 1.
Here, scoreLL(e?|e) denotes the score based on
Equation (3). scoreLM (e?|SE) is the LM based
score: scoreLM (e?|SE) = 1n logPLM (S
?
E), where
S?E is the sentence generated by replacing e in SE
with e?. The language model in the experiment was
a tri-gram model trained using the English sentences
in the bilingual corpus. We empirically set ? = 0.7.
The selected best paraphrase patterns in context
sentences were manually labeled. The context infor-
mation was also considered by our judges. The re-
sult shows that the precision of the best paraphrase
patterns is 59.39%. To investigate the contribution
of the LM based score, we ran the experiment again
with ? = 1 (ignoring the LM based score) and found
that the precision is 57.09%. It indicates that the LM
based reranking can improve the precision. How-
ever, the improvement is small. Further analysis
shows that about 70% of the correct paraphrase sub-
stitutes are in the type of phrase replacement.
5 Conclusion
This paper proposes a pivot approach for extracting
paraphrase patterns from bilingual corpora. We use
a log-linear model to compute the paraphrase like-
lihood and exploit feature functions based on MLE
and LW. Experimental results show that the pivot ap-
proach is effective, which extracts over 1,000,000
pairs of paraphrase patterns from 2M bilingual sen-
tence pairs. The precision and coverage of the ex-
tracted paraphrase patterns exceed 67% and 84%,
respectively. In addition, the log-linear model with
the proposed feature functions significantly outper-
forms the conventional models. Analysis shows that
5 types of paraphrase patterns are extracted with our
method, which are useful in various applications.
In the future we wish to exploit more feature func-
tions in the log-linear model. In addition, we will try
to make better use of the context information when
replacing paraphrase patterns in context sentences.
Acknowledgments
This research was supported by National Nat-
ural Science Foundation of China (60503072,
60575042). We thank Lin Zhao, Xiaohang Qu, and
Zhenghua Li for their help in the experiments.
787
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proceed-
ings of ACL, pages 597-604.
Regina Barzilay and Lillian Lee. 2003. Learning to Para-
phrase: An Unsupervised Approach Using Multiple-
Sequence Alignment. In Proceedings of HLT-NAACL,
pages 16-23.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of HLT-
NAACL, pages 17-24.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Extract-
ing Structural Paraphrases from Aligned Monolingual
Corpora. In Proceedings of IWP, pages 57-64.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgue`re. 1991. Lexical Selection and Paraphrase in a
Meaning-Text Generation Model. In Ce?cile L. Paris,
William R. Swartout, and William C. Mann (Eds.):
Natural Language Generation in Artificial Intelligence
and Computational Linguistics, pages 293-312.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of HLT-
NAACL, pages 455-462.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
In Proceedings of IWSLT.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT-NAACL, pages 127-133.
De-Kang Lin and Patrick Pantel. 2001. Discovery of
Inference Rules for Question Answering. In Natural
Language Engineering 7(4): 343-360.
Ting Liu, Jin-Shan Ma, Hui-Jia Zhu, and Sheng Li. 2006.
Dependency Parsing Based on Dynamic Local Opti-
mization. In Proceedings of CoNLL-X, pages 211-215.
Kathleen R. Mckeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and Summarizing News on
a Daily Basis with Columbia?s Newsblaster. In Pro-
ceedings of HLT, pages 280-285.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A Language-
Independent System for Data-Driven Dependency
Parsing. In Natural Language Engineering 13(2): 95-
135.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of ACL,
pages 440-447.
A??da Ouangraoua, Pascal Ferraro, Laurent Tichit, and
Serge Dulucq. 2007. Local Similarity between Quo-
tiented Ordered Trees. In Journal of Discrete Algo-
rithms 5(1): 23-35.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based Alignment of Multiple Translations: Ex-
tracting Paraphrases and Generating New Sentences.
In Proceedings of HLT-NAACL, pages 102-109.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 1992. Numerical Recipes
in C: The Art of Scientific Computing. Cambridge
University Press, Cambridge, U.K., 1992, 412-420.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of EMNLP, pages 142-
149.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing Surface Text Patterns for a Question Answering
System. In Proceedings of ACL, pages 41-47.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic Paraphrase Acquisition from News
Articles. In Proceedings of HLT, pages 40-46.
Idan Szpektor, Hristo Tanev, Ido Dagan and Bonaven-
tura Coppola. 2004. Scaling Web-based Acquisition
of Entailment Relations. In Proceedings of EMNLP,
pages 41-48.
788
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 46?54,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Exploiting Heterogeneous Treebanks for Parsing
Zheng-Yu Niu, Haifeng Wang, Hua Wu
Toshiba (China) Research and Development Center
5/F., Tower W2, Oriental Plaza, Beijing, 100738, China
{niuzhengyu,wanghaifeng,wuhua}@rdc.toshiba.com.cn
Abstract
We address the issue of using heteroge-
neous treebanks for parsing by breaking
it down into two sub-problems, convert-
ing grammar formalisms of the treebanks
to the same one, and parsing on these
homogeneous treebanks. First we pro-
pose to employ an iteratively trained tar-
get grammar parser to perform grammar
formalism conversion, eliminating prede-
fined heuristic rules as required in previ-
ous methods. Then we provide two strate-
gies to refine conversion results, and adopt
a corpus weighting technique for parsing
on homogeneous treebanks. Results on the
Penn Treebank show that our conversion
method achieves 42% error reduction over
the previous best result. Evaluation on
the Penn Chinese Treebank indicates that a
converted dependency treebank helps con-
stituency parsing and the use of unlabeled
data by self-training further increases pars-
ing f-score to 85.2%, resulting in 6% error
reduction over the previous best result.
1 Introduction
The last few decades have seen the emergence of
multiple treebanks annotated with different gram-
mar formalisms, motivated by the diversity of lan-
guages and linguistic theories, which is crucial to
the success of statistical parsing (Abeille et al,
2000; Brants et al, 1999; Bohmova et al, 2003;
Han et al, 2002; Kurohashi and Nagao, 1998;
Marcus et al, 1993; Moreno et al, 2003; Xue et
al., 2005). Availability of multiple treebanks cre-
ates a scenario where we have a treebank anno-
tated with one grammar formalism, and another
treebank annotated with another grammar formal-
ism that we are interested in. We call the first
a source treebank, and the second a target tree-
bank. We thus encounter a problem of how to
use these heterogeneous treebanks for target gram-
mar parsing. Here heterogeneous treebanks refer
to two or more treebanks with different grammar
formalisms, e.g., one treebank annotated with de-
pendency structure (DS) and the other annotated
with phrase structure (PS).
It is important to acquire additional labeled data
for the target grammar parsing through exploita-
tion of existing source treebanks since there is of-
ten a shortage of labeled data. However, to our
knowledge, there is no previous study on this is-
sue.
Recently there have been some works on us-
ing multiple treebanks for domain adaptation of
parsers, where these treebanks have the same
grammar formalism (McClosky et al, 2006b;
Roark and Bacchiani, 2003). Other related works
focus on converting one grammar formalism of a
treebank to another and then conducting studies on
the converted treebank (Collins et al, 1999; Forst,
2003; Wang et al, 1994; Watkinson and Manand-
har, 2001). These works were done either on mul-
tiple treebanks with the same grammar formalism
or on only one converted treebank. We see that
their scenarios are different from ours as we work
with multiple heterogeneous treebanks.
For the use of heterogeneous treebanks1, we
propose a two-step solution: (1) converting the
grammar formalism of the source treebank to the
target one, (2) refining converted trees and using
them as additional training data to build a target
grammar parser.
For grammar formalism conversion, we choose
the DS to PS direction for the convenience of the
comparison with existing works (Xia and Palmer,
2001; Xia et al, 2008). Specifically, we assume
that the source grammar formalism is dependency
1Here we assume the existence of two treebanks.
46
grammar, and the target grammar formalism is
phrase structure grammar.
Previous methods for DS to PS conversion
(Collins et al, 1999; Covington, 1994; Xia and
Palmer, 2001; Xia et al, 2008) often rely on pre-
defined heuristic rules to eliminate converison am-
biguity, e.g., minimal projection for dependents,
lowest attachment position for dependents, and the
selection of conversion rules that add fewer num-
ber of nodes to the converted tree. In addition, the
validity of these heuristic rules often depends on
their target grammars. To eliminate the heuristic
rules as required in previous methods, we propose
to use an existing target grammar parser (trained
on the target treebank) to generate N-best parses
for each sentence in the source treebank as conver-
sion candidates, and then select the parse consis-
tent with the structure of the source tree as the con-
verted tree. Furthermore, we attempt to use con-
verted trees as additional training data to retrain
the parser for better conversion candidates. The
procedure of tree conversion and parser retraining
will be run iteratively until a stopping condition is
satisfied.
Since some converted trees might be imper-
fect from the perspective of the target grammar,
we provide two strategies to refine conversion re-
sults: (1) pruning low-quality trees from the con-
verted treebank, (2) interpolating the scores from
the source grammar and the target grammar to se-
lect better converted trees. Finally we adopt a cor-
pus weighting technique to get an optimal combi-
nation of the converted treebank and the existing
target treebank for parser training.
We have evaluated our conversion algorithm on
a dependency structure treebank (produced from
the Penn Treebank) for comparison with previous
work (Xia et al, 2008). We also have investi-
gated our two-step solution on two existing tree-
banks, the Penn Chinese Treebank (CTB) (Xue et
al., 2005) and the Chinese Dependency Treebank
(CDT)2 (Liu et al, 2006). Evaluation on WSJ data
demonstrates that it is feasible to use a parser for
grammar formalism conversion and the conversion
benefits from converted trees used for parser re-
training. Our conversion method achieves 93.8%
f-score on dependency trees produced from WSJ
section 22, resulting in 42% error reduction over
the previous best result for DS to PS conversion.
Results on CTB show that score interpolation is
2Available at http://ir.hit.edu.cn/.
more effective than instance pruning for the use
of converted treebanks for parsing and converted
CDT helps parsing on CTB. When coupled with
self-training technique, a reranking parser with
CTB and converted CDT as labeled data achieves
85.2% f-score on CTB test set, an absolute 1.0%
improvement (6% error reduction) over the previ-
ous best result for Chinese parsing.
The rest of this paper is organized as follows. In
Section 2, we first describe a parser based method
for DS to PS conversion, and then we discuss pos-
sible strategies to refine conversion results, and
finally we adopt the corpus weighting technique
for parsing on homogeneous treebanks. Section
3 provides experimental results of grammar for-
malism conversion on a dependency treebank pro-
duced from the Penn Treebank. In Section 4, we
evaluate our two-step solution on two existing het-
erogeneous Chinese treebanks. Section 5 reviews
related work and Section 6 concludes this work.
2 Our Two-Step Solution
2.1 Grammar Formalism Conversion
Previous DS to PS conversion methods built a
converted tree by iteratively attaching nodes and
edges to the tree with the help of conversion
rules and heuristic rules, based on current head-
dependent pair from a source dependency tree and
the structure of the built tree (Collins et al, 1999;
Covington, 1994; Xia and Palmer, 2001; Xia et
al., 2008). Some observations can be made on
these methods: (1) for each head-dependent pair,
only one locally optimal conversion was kept dur-
ing tree-building process, at the risk of pruning
globally optimal conversions, (2) heuristic rules
are required to deal with the problem that one
head-dependent pair might have multiple conver-
sion candidates, and these heuristic rules are usu-
ally hand-crafted to reflect the structural prefer-
ence in their target grammars. To overcome these
limitations, we propose to employ a parser to gen-
erate N-best parses as conversion candidates and
then use the structural information of source trees
to select the best parse as a converted tree.
We formulate our conversion method as fol-
lows.
Let CDS be a source treebank annotated with
DS and CPS be a target treebank annotated with
PS. Our goal is to convert the grammar formalism
of CDS to that of CPS .
We first train a constituency parser on CPS
47
Input: CPS , CDS , Q, and a constituency parser Output: Converted trees CDSPS
1. Initialize:
? Set CDS,0PS as null, DevScore=0, q=0;
? Split CPS into training set CPS,train and development set CPS,dev;
? Train the parser on CPS,train and denote it by Pq?1;
2. Repeat:
? Use Pq?1 to generate N-best PS parses for each sentence in CDS , and convert PS to DS for each parse;
? For each sentence in CDS Do
? t?=argmaxtScore(xi,t), and select the t?-th parse as a converted tree for this sentence;
? Let CDS,qPS represent these converted trees, and let Ctrain=CPS,train
?CDS,qPS ;
? Train the parser on Ctrain, and denote the updated parser by Pq;
? Let DevScoreq be the f-score of Pq on CPS,dev;
? If DevScoreq > DevScore Then DevScore=DevScoreq, and CDSPS =CDS,qPS ;
? Else break;
? q++;
Until q > Q
Table 1: Our algorithm for DS to PS conversion.
(90% trees in CPS as training set CPS,train, and
other trees as development set CPS,dev) and then
let the parser generate N-best parses for each sen-
tence in CDS .
Let n be the number of sentences (or trees) in
CDS and ni be the number of N-best parses gen-
erated by the parser for the i-th (1 ? i ? n) sen-
tence in CDS . Let xi,t be the t-th (1 ? t ? ni)
parse for the i-th sentence. Let yi be the tree of the
i-th (1 ? i ? n) sentence in CDS .
To evaluate the quality of xi,t as a conversion
candidate for yi, we convert xi,t to a dependency
tree (denoted as xDSi,t ) and then use unlabeled de-
pendency f-score to measure the similarity be-
tween xDSi,t and yi. Let Score(xi,t) denote the
unlabeled dependency f-score of xDSi,t against yi.
Then we determine the converted tree for yi by
maximizing Score(xi,t) over the N-best parses.
The conversion from PS to DS works as fol-
lows:
Step 1. Use a head percolation table to find the
head of each constituent in xi,t.
Step 2. Make the head of each non-head child
depend on the head of the head child for each con-
stituent.
Unlabeled dependency f-score is a harmonic
mean of unlabeled dependency precision and unla-
beled dependency recall. Precision measures how
many head-dependent word pairs found in xDSi,t
are correct and recall is the percentage of head-
dependent word pairs defined in the gold-standard
tree that are found in xDSi,t . Here we do not take
dependency tags into consideration for evaluation
since they cannot be obtained without more so-
phisticated rules.
To improve the quality of N-best parses, we at-
tempt to use the converted trees as additional train-
ing data to retrain the parser. The procedure of
tree conversion and parser retraining can be run it-
eratively until a termination condition is satisfied.
Here we use the parser?s f-score on CPS,dev as a
termination criterion. If the update of training data
hurts the performance on CPS,dev, then we stop
the iteration.
Table 1 shows this DS to PS conversion algo-
rithm. Q is an upper limit of the number of loops,
and Q ? 0.
2.2 Target Grammar Parsing
Through grammar formalism conversion, we have
successfully turned the problem of using hetero-
geneous treebanks for parsing into the problem of
parsing on homogeneous treebanks. Before using
converted source treebank for parsing, we present
two strategies to refine conversion results.
Instance Pruning For some sentences in
CDS , the parser might fail to generate high qual-
ity N-best parses, resulting in inferior converted
trees. To clean the converted treebank, we can re-
move the converted trees with low unlabeled de-
pendency f-scores (defined in Section 2.1) before
using the converted treebank for parser training
48
Figure 1: A parse tree in CTB for a sentence of
/?.<world> ?<every> I<country> <
?<people> ?<all> r<with> 81<eyes>
? ?<cast> ? l<Hong Kong>0with
/People from all over the world are cast-
ing their eyes on Hong Kong0as its English
translation.
because these trees are/misleading0training in-
stances. The number of removed trees will be de-
termined by cross validation on development set.
Score Interpolation Unlabeled dependency
f-scores used in Section 2.1 measure the quality of
converted trees from the perspective of the source
grammar only. In extreme cases, the top best
parses in the N-best list are good conversion can-
didates but we might select a parse ranked quite
low in the N-best list since there might be con-
flicts of syntactic structure definition between the
source grammar and the target grammar.
Figure 1 shows an example for illustration of
a conflict between the grammar of CDT and
that of CTB. According to Chinese head percola-
tion tables used in the PS to DS conversion tool
/Penn2Malt03 and Charniak?s parser4, the head
of VP-2 is the word /r0(a preposition, with
/BA0as its POS tag in CTB), and the head of
IP-OBJ is ??0. Therefore the word /?
?0depends on the word/r0. But according
to the annotation scheme in CDT (Liu et al, 2006),
the word/r0is a dependent of the word/?
?0. The conflicts between the two grammars
may lead to the problem that the selected parses
based on the information of the source grammar
might not be preferred from the perspective of the
3Available at http://w3.msi.vxu.se/?nivre/.
4Available at http://www.cs.brown.edu/?ec/.
target grammar.
Therefore we modified the selection metric in
Section 2.1 by interpolating two scores, the prob-
ability of a conversion candidate from the parser
and its unlabeled dependency f-score, shown as
follows:
S?core(xi,t) = ??Prob(xi,t)+(1??)?Score(xi,t). (1)
The intuition behind this equation is that converted
trees should be preferred from the perspective of
both the source grammar and the target grammar.
Here 0 ? ? ? 1. Prob(xi,t) is a probability pro-
duced by the parser for xi,t (0 ? Prob(xi,t) ? 1).
The value of ? will be tuned by cross validation on
development set.
After grammar formalism conversion, the prob-
lem now we face has been limited to how to build
parsing models on multiple homogeneous tree-
bank. A possible solution is to simply concate-
nate the two treebanks as training data. However
this method may lead to a problem that if the size
of CPS is significantly less than that of converted
CDS , converted CDS may weaken the effect CPS
might have. One possible solution is to reduce the
weight of examples from converted CDS in parser
training. Corpus weighting is exactly such an ap-
proach, with the weight tuned on development set,
that will be used for parsing on homogeneous tree-
banks in this paper.
3 Experiments of Grammar Formalism
Conversion
3.1 Evaluation on WSJ section 22
Xia et al (2008) used WSJ section 19 from the
Penn Treebank to extract DS to PS conversion
rules and then produced dependency trees from
WSJ section 22 for evaluation of their DS to PS
conversion algorithm. They showed that their
conversion algorithm outperformed existing meth-
ods on the WSJ data. For comparison with their
work, we conducted experiments in the same set-
ting as theirs: using WSJ section 19 (1844 sen-
tences) as CPS , producing dependency trees from
WSJ section 22 (1700 sentences) as CDS5, and
using labeled bracketing f-scores from the tool
/EVALB0on WSJ section 22 for performance
evaluation.
5We used the tool/Penn2Malt0to produce dependency
structures from the Penn Treebank, which was also used for
PS to DS conversion in our conversion algorithm.
49
All the sentences
DevScore LR LP F
Models (%) (%) (%) (%)
The best result of
Xia et al (2008) - 90.7 88.1 89.4
Q-0-method 86.8 92.2 92.8 92.5
Q-10-method 88.0 93.4 94.1 93.8
Table 2: Comparison with the work of Xia et al
(2008) on WSJ section 22.
All the sentences
DevScore LR LP F
Models (%) (%) (%) (%)
Q-0-method 91.0 91.6 92.5 92.1
Q-10-method 91.6 93.1 94.1 93.6
Table 3: Results of our algorithm on WSJ section
2?18 and 20?22.
We employed Charniak?s maximum entropy in-
spired parser (Charniak, 2000) to generate N-best
(N=200) parses. Xia et al (2008) used POS
tag information, dependency structures and depen-
dency tags in test set for conversion. Similarly, we
used POS tag information in the test set to restrict
search space of the parser for generation of better
N-best parses.
We evaluated two variants of our DS to PS con-
version algorithm:
Q-0-method: We set the value of Q as 0 for a
baseline method.
Q-10-method: We set the value of Q as 10 to
see whether it is helpful for conversion to retrain
the parser on converted trees.
Table 2 shows the results of our conversion al-
gorithm on WSJ section 22. In the experiment
of Q-10-method, DevScore reached the highest
value of 88.0% when q was 1. Then we used
CDS,1PS as the conversion result. Finally Q-10-
method achieved an f-score of 93.8% on WSJ sec-
tion 22, an absolute 4.4% improvement (42% er-
ror reduction) over the best result of Xia et al
(2008). Moreover, Q-10-method outperformed Q-
0-method on the same test set. These results indi-
cate that it is feasible to use a parser for DS to PS
conversion and the conversion benefits from the
use of converted trees for parser retraining.
3.2 Evaluation on WSJ section 2?18 and
20?22
In this experiment we evaluated our conversion al-
gorithm on a larger test set, WSJ section 2?18 and
20?22 (totally 39688 sentences). Here we also
used WSJ section 19 as CPS . Other settings for
All the sentences
LR LP F
Training data (%) (%) (%)
1? CTB + CDTPS 84.7 85.1 84.9
2? CTB + CDTPS 85.1 85.6 85.3
5? CTB + CDTPS 85.0 85.5 85.3
10? CTB + CDTPS 85.3 85.8 85.6
20? CTB + CDTPS 85.1 85.3 85.2
50? CTB + CDTPS 84.9 85.3 85.1
Table 4: Results of the generative parser on the de-
velopment set, when trained with various weight-
ing of CTB training set and CDTPS .
this experiment are as same as that in Section 3.1,
except that here we used a larger test set.
Table 3 provides the f-scores of our method with
Q equal to 0 or 10 on WSJ section 2?18 and
20?22.
With Q-10-method, DevScore reached the high-
est value of 91.6% when q was 1. Finally Q-
10-method achieved an f-score of 93.6% on WSJ
section 2?18 and 20?22, better than that of Q-0-
method and comparable with that of Q-10-method
in Section 3.1. It confirms our previous finding
that the conversion benefits from the use of con-
verted trees for parser retraining.
4 Experiments of Parsing
We investigated our two-step solution on two ex-
isting treebanks, CDT and CTB, and we used CDT
as the source treebank and CTB as the target tree-
bank.
CDT consists of 60k Chinese sentences, anno-
tated with POS tag information and dependency
structure information (including 28 POS tags, and
24 dependency tags) (Liu et al, 2006). We did not
use POS tag information as inputs to the parser in
our conversion method due to the difficulty of con-
version from CDT POS tags to CTB POS tags.
We used a standard split of CTB for perfor-
mance evaluation, articles 1-270 and 400-1151 as
training set, articles 301-325 as development set,
and articles 271-300 as test set.
We used Charniak?s maximum entropy inspired
parser and their reranker (Charniak and Johnson,
2005) for target grammar parsing, called a gener-
ative parser (GP) and a reranking parser (RP) re-
spectively. We reported ParseVal measures from
the EVALB tool.
50
All the sentences
LR LP F
Models Training data (%) (%) (%)
GP CTB 79.9 82.2 81.0
RP CTB 82.0 84.6 83.3
GP 10? CTB + CDTPS 80.4 82.7 81.5
RP 10? CTB + CDTPS 82.8 84.7 83.8
Table 5: Results of the generative parser (GP) and
the reranking parser (RP) on the test set, when
trained on only CTB training set or an optimal
combination of CTB training set and CDTPS .
4.1 Results of a Baseline Method to Use CDT
We used our conversion algorithm6 to convert the
grammar formalism of CDT to that of CTB. Let
CDTPS denote the converted CDT by our method.
The average unlabeled dependency f-score of trees
in CDTPS was 74.4%, and their average index in
200-best list was 48.
We tried the corpus weighting method when
combining CDTPS with CTB training set (abbre-
viated as CTB for simplicity) as training data, by
gradually increasing the weight (including 1, 2, 5,
10, 20, 50) of CTB to optimize parsing perfor-
mance on the development set. Table 4 presents
the results of the generative parser with various
weights of CTB on the development set. Consid-
ering the performance on the development set, we
decided to give CTB a relative weight of 10.
Finally we evaluated two parsing models, the
generative parser and the reranking parser, on the
test set, with results shown in Table 5. When
trained on CTB only, the generative parser and the
reranking parser achieved f-scores of 81.0% and
83.3%. The use of CDTPS as additional training
data increased f-scores of the two models to 81.5%
and 83.8%.
4.2 Results of Two Strategies for a Better Use
of CDT
4.2.1 Instance Pruning
We used unlabeled dependency f-score of each
converted tree as the criterion to rank trees in
CDTPS and then kept only the top M trees
with high f-scores as training data for pars-
ing, resulting in a corpus CDTPSM . M var-
ied from 100%?|CDTPS | to 10%?|CDTPS |
with 10%?|CDTPS | as the interval. |CDTPS |
6The setting for our conversion algorithm in this experi-
ment was as same as that in Section 3.1. In addition, we used
CTB training set as CPS,train, and CTB development set as
CPS,dev .
All the sentences
LR LP F
Models Training data (%) (%) (%)
GP CTB + CDTPS? 81.4 82.8 82.1
RP CTB + CDTPS? 83.0 85.4 84.2
Table 6: Results of the generative parser and the
reranking parser on the test set, when trained on
an optimal combination of CTB training set and
converted CDT.
is the number of trees in CDTPS . Then
we tuned the value of M by optimizing the
parser?s performance on the development set with
10?CTB+CDTPSM as training data. Finally the op-
timal value of M was 100%?|CDT|. It indicates
that even removing very few converted trees hurts
the parsing performance. A possible reason is that
most of non-perfect parses can provide useful syn-
tactic structure information for building parsing
models.
4.2.2 Score Interpolation
We used ?Score(xi,t)7 to replace Score(xi,t) in
our conversion algorithm and then ran the updated
algorithm on CDT. Let CDTPS? denote the con-
verted CDT by this updated conversion algorithm.
The values of ? (varying from 0.0 to 1.0 with 0.1
as the interval) and the CTB weight (including 1,
2, 5, 10, 20, 50) were simultaneously tuned on the
development set8. Finally we decided that the op-
timal value of ? was 0.4 and the optimal weight of
CTB was 1, which brought the best performance
on the development set (an f-score of 86.1%). In
comparison with the results in Section 4.1, the
average index of converted trees in 200-best list
increased to 2, and their average unlabeled depen-
dency f-score dropped to 65.4%. It indicates that
structures of converted trees become more consis-
tent with the target grammar, as indicated by the
increase of average index of converted trees, fur-
ther away from the source grammar.
Table 6 provides f-scores of the generative
parser and the reranker on the test set, when
trained on CTB and CDTPS? . We see that the
performance of the reranking parser increased to
7Before calculating S?core(xi,t), we normal-
ized the values of Prob(xi,t) for each N-best list
by (1) Prob(xi,t)=Prob(xi,t)-Min(Prob(xi,?)),
(2)Prob(xi,t)=Prob(xi,t)/Max(Prob(xi,?)), resulting
in that their maximum value was 1 and their minimum value
was 0.
8Due to space constraint, we do not show f-scores of the
parser with different values of ? and the CTB weight.
51
All the sentences
LR LP F
Models Training data (%) (%) (%)
Self-trained GP 10?T+10?D+P 83.0 84.5 83.7
Updated RP CTB+CDTPS? 84.3 86.1 85.2
Table 7: Results of the self-trained gen-
erative parser and updated reranking parser
on the test set. 10?T+10?D+P stands for
10?CTB+10?CDTPS? +PDC.
84.2% f-score, better than the result of the rerank-
ing parser with CTB and CDTPS as training data
(shown in Table 5). It indicates that the use of
probability information from the parser for tree
conversion helps target grammar parsing.
4.3 Using Unlabeled Data for Parsing
Recent studies on parsing indicate that the use of
unlabeled data by self-training can help parsing
on the WSJ data, even when labeled data is rel-
atively large (McClosky et al, 2006a; Reichart
and Rappoport, 2007). It motivates us to em-
ploy self-training technique for Chinese parsing.
We used the POS tagged People Daily corpus9
(Jan. 1998?Jun. 1998, and Jan. 2000?Dec.
2000) (PDC) as unlabeled data for parsing. First
we removed the sentences with less than 3 words
or more than 40 words from PDC to ease pars-
ing, resulting in 820k sentences. Then we ran the
reranking parser in Section 4.2.2 on PDC and used
the parses on PDC as additional training data for
the generative parser. Here we tried the corpus
weighting technique for an optimal combination
of CTB, CDTPS? and parsed PDC, and chose the
relative weight of both CTB and CDTPS? as 10
by cross validation on the development set. Fi-
nally we retrained the generative parser on CTB,
CDTPS? and parsed PDC. Furthermore, we used
this self-trained generative parser as a base parser
to retrain the reranker on CTB and CDTPS? .
Table 7 shows the performance of self-trained
generative parser and updated reranker on the test
set, with CTB and CDTPS? as labeled data. We see
that the use of unlabeled data by self-training fur-
ther increased the reranking parser?s performance
from 84.2% to 85.2%. Our results on Chinese data
confirm previous findings on English data shown
in (McClosky et al, 2006a; Reichart and Rap-
poport, 2007).
9Available at http://icl.pku.edu.cn/.
4.4 Comparison with Previous Studies for
Chinese Parsing
Table 8 and 9 present the results of previous stud-
ies on CTB. All the works in Table 8 used CTB
articles 1-270 as labeled data. In Table 9, Petrov
and Klein (2007) trained their model on CTB ar-
ticles 1-270 and 400-1151, and Burkett and Klein
(2008) used the same CTB articles and parse trees
of their English translation (from the English Chi-
nese Translation Treebank) as training data. Com-
paring our result in Table 6 with that of Petrov
and Klein (2007), we see that CDTPS? helps pars-
ing on CTB, which brought 0.9% f-score improve-
ment. Moreover, the use of unlabeled data further
boosted the parsing performance to 85.2%, an ab-
solute 1.0% improvement over the previous best
result presented in Burkett and Klein (2008).
5 Related Work
Recently there have been some studies address-
ing how to use treebanks with same grammar for-
malism for domain adaptation of parsers. Roark
and Bachiani (2003) presented count merging and
model interpolation techniques for domain adap-
tation of parsers. They showed that their sys-
tem with count merging achieved a higher perfor-
mance when in-domain data was weighted more
heavily than out-of-domain data. McClosky et al
(2006b) used self-training and corpus weighting to
adapt their parser trained on WSJ corpus to Brown
corpus. Their results indicated that both unla-
beled in-domain data and labeled out-of-domain
data can help domain adaptation. In comparison
with these works, we conduct our study in a dif-
ferent setting where we work with multiple het-
erogeneous treebanks.
Grammar formalism conversion makes it possi-
ble to reuse existing source treebanks for the study
of target grammar parsing. Wang et al (1994)
employed a parser to help conversion of a tree-
bank from a simple phrase structure to a more in-
formative phrase structure and then used this con-
verted treebank to train their parser. Collins et al
(1999) performed statistical constituency parsing
of Czech on a treebank that was converted from
the Prague Dependency Treebank under the guid-
ance of conversion rules and heuristic rules, e.g.,
one level of projection for any category, minimal
projection for any dependents, and fixed position
of attachment. Xia and Palmer (2001) adopted bet-
ter heuristic rules to build converted trees, which
52
? 40 words All the sentences
LR LP F LR LP F
Models (%) (%) (%) (%) (%) (%)
Bikel & Chiang (2000) 76.8 77.8 77.3 - - -
Chiang & Bikel (2002) 78.8 81.1 79.9 - - -
Levy & Manning (2003) 79.2 78.4 78.8 - - -
Bikel?s thesis (2004) 78.0 81.2 79.6 - - -
Xiong et. al. (2005) 78.7 80.1 79.4 - - -
Chen et. al. (2005) 81.0 81.7 81.2 76.3 79.2 77.7
Wang et. al. (2006) 79.2 81.1 80.1 76.2 78.0 77.1
Table 8: Results of previous studies on CTB with CTB articles 1-270 as labeled data.
? 40 words All the sentences
LR LP F LR LP F
Models (%) (%) (%) (%) (%) (%)
Petrov & Klein (2007) 85.7 86.9 86.3 81.9 84.8 83.3
Burkett & Klein (2008) - - - - - 84.2
Table 9: Results of previous studies on CTB with more labeled data.
reflected the structural preference in their target
grammar. For acquisition of better conversion
rules, Xia et al (2008) proposed to automati-
cally extract conversion rules from a target tree-
bank. Moreover, they presented two strategies to
solve the problem that there might be multiple
conversion rules matching the same input depen-
dency tree pattern: (1) choosing the most frequent
rules, (2) preferring rules that add fewer number
of nodes and attach the subtree lower.
In comparison with the works of Wang et al
(1994) and Collins et al (1999), we went fur-
ther by combining the converted treebank with the
existing target treebank for parsing. In compar-
ison with previous conversion methods (Collins
et al, 1999; Covington, 1994; Xia and Palmer,
2001; Xia et al, 2008) in which for each head-
dependent pair, only one locally optimal conver-
sion was kept during tree-building process, we
employed a parser to generate globally optimal
syntactic structures, eliminating heuristic rules for
conversion. In addition, we used converted trees to
retrain the parser for better conversion candidates,
while Wang et al (1994) did not exploit the use of
converted trees for parser retraining.
6 Conclusion
We have proposed a two-step solution to deal with
the issue of using heterogeneous treebanks for
parsing. First we present a parser based method
to convert grammar formalisms of the treebanks to
the same one, without applying predefined heuris-
tic rules, thus turning the original problem into the
problem of parsing on homogeneous treebanks.
Then we present two strategies, instance pruning
and score interpolation, to refine conversion re-
sults. Finally we adopt the corpus weighting tech-
nique to combine the converted source treebank
with the existing target treebank for parser train-
ing.
The study on the WSJ data shows the benefits of
our parser based approach for grammar formalism
conversion. Moreover, experimental results on the
Penn Chinese Treebank indicate that a converted
dependency treebank helps constituency parsing,
and it is better to exploit probability information
produced by the parser through score interpolation
than to prune low quality trees for the use of the
converted treebank.
Future work includes further investigation of
our conversion method for other pairs of grammar
formalisms, e.g., from the grammar formalism of
the Penn Treebank to more deep linguistic formal-
ism like CCG, HPSG, or LFG.
References
Anne Abeille, Lionel Clement and Francois Toussenel. 2000.
Building a Treebank for French. In Proceedings of LREC
2000, pages 87-94.
Daniel Bikel and David Chiang. 2000. Two Statistical Pars-
ing Models Applied to the Chinese Treebank. In Proceed-
ings of the Second SIGHAN workshop, pages 1-6.
Daniel Bikel. 2004. On the Parameter Space of Generative
Lexicalized Statistical Parsing Models. Ph.D. thesis, Uni-
versity of Pennsylvania.
Alena Bohmova, Jan Hajic, Eva Hajicova and Barbora
Vidova-Hladka. 2003. The Prague Dependency Tree-
bank: A Three-Level Annotation Scenario. Treebanks:
53
Building and Using Annotated Corpora. Kluwer Aca-
demic Publishers, pages 103-127.
Thorsten Brants, Wojciech Skut and Hans Uszkoreit. 1999.
Syntactic Annotation of a German Newspaper Corpus. In
Proceedings of the ATALA Treebank Workshop, pages 69-
76.
David Burkett and Dan Klein. 2008. Two Languages are
Better than One (for Syntactic Parsing). In Proceedings of
EMNLP 2008, pages 877-886.
Eugene Charniak. 2000. A Maximum Entropy Inspired
Parser. In Proceedings of NAACL 2000, pages 132-139.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-Fine
N-Best Parsing and MaxEnt Discriminative Reranking. In
Proceedings of ACL 2005, pages 173-180.
Ying Chen, Hongling Sun and Dan Jurafsky. 2005. A Cor-
rigendum to Sun and Jurafsky (2004) Shallow Semantic
Parsing of Chinese. University of Colorado at Boulder
CSLR Tech Report TR-CSLR-2005-01.
David Chiang and Daniel M. Bikel. 2002. Recovering La-
tent Information in Treebanks. In Proceedings of COL-
ING 2002, pages 1-7.
Micheal Collins, Lance Ramshaw, Jan Hajic and Christoph
Tillmann. 1999. A Statistical Parser for Czech. In Pro-
ceedings of ACL 1999, pages 505-512.
Micheal Covington. 1994. GB Theory as Dependency
Grammar. Research Report AI-1992-03.
Martin Forst. 2003. Treebank Conversion - Establishing
a Testsuite for a Broad-Coverage LFG from the TIGER
Treebank. In Proceedings of LINC at EACL 2003, pages
25-32.
Chunghye Han, Narae Han, Eonsuk Ko and Martha Palmer.
2002. Development and Evaluation of a Korean Treebank
and its Application to NLP. In Proceedings of LREC 2002,
pages 1635-1642.
Sadao Kurohashi and Makato Nagao. 1998. Building a
Japanese Parsed Corpus While Improving the Parsing Sys-
tem. In Proceedings of LREC 1998, pages 719-724.
Roger Levy and Christopher Manning. 2003. Is It Harder to
Parse Chinese, or the Chinese Treebank? In Proceedings
of ACL 2003, pages 439-446.
Ting Liu, Jinshan Ma and Sheng Li. 2006. Building a Depen-
dency Treebank for Improving Chinese Parser. Journal of
Chinese Language and Computing, 16(4):207-224.
Mitchell P. Marcus, Beatrice Santorini and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated Cor-
pus of English: The Penn Treebank. Computational Lin-
guistics, 19(2):313-330.
David McClosky, Eugene Charniak and Mark Johnson.
2006a. Effective Self-Training for Parsing. In Proceed-
ings of NAACL 2006, pages 152-159.
David McClosky, Eugene Charniak and Mark Johnson.
2006b. Reranking and Self-Training for Parser Adapta-
tion. In Proceedings of COLING/ACL 2006, pages 337-
344.
Antonio Moreno, Susana Lopez, Fernando Sanchez and
Ralph Grishman. 2003. Developing a Syntactic Anno-
tation Scheme and Tools for a Spanish Treebank. Tree-
banks: Building and Using Annotated Corpora. Kluwer
Academic Publishers, pages 149-163.
Slav Petrov and Dan Klein. 2007. Improved Inference for
Unlexicalized Parsing. In Proceedings of HLT/NAACL
2007, pages 404-411.
Roi Reichart and Ari Rappoport. 2007. Self-Training for En-
hancement and Domain Adaptation of Statistical Parsers
Trained on Small Datasets. In Proceedings of ACL 2007,
pages 616-623.
Brian Roark and Michiel Bacchiani. 2003. Supervised and
Unsupervised PCFG Adaptation to Novel Domains. In
Proceedings of HLT/NAACL 2003, pages 126-133.
Jong-Nae Wang, Jing-Shin Chang and Keh-Yih Su. 1994.
An Automatic Treebank Conversion Algorithm for Corpus
Sharing. In Proceedings of ACL 1994, pages 248-254.
Mengqiu Wang, Kenji Sagae and Teruko Mitamura. 2006. A
Fast, Accurate Deterministic Parser for Chinese. In Pro-
ceedings of COLING/ACL 2006, pages 425-432.
Stephen Watkinson and Suresh Manandhar. 2001. Translat-
ing Treebank Annotation for Evaluation. In Proceedings
of ACL Workshop on Evaluation Methodologies for Lan-
guage and Dialogue Systems, pages 1-8.
Fei Xia and Martha Palmer. 2001. Converting Dependency
Structures to Phrase Structures. In Proceedings of HLT
2001, pages 1-5.
Fei Xia, Rajesh Bhatt, Owen Rambow, Martha Palmer
and Dipti Misra. Sharma. 2008. Towards a Multi-
Representational Treebank. In Proceedings of the 7th In-
ternational Workshop on Treebanks and Linguistic Theo-
ries, pages 159-170.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin and
Yueliang Qian. 2005. Parsing the Penn Chinese Tree-
bank with Semantic Knowledge. In Proceedings of IJC-
NLP 2005, pages 70-81.
Nianwen Xue, Fei Xia, Fu-Dong Chiou and Martha Palmer.
2005. The Penn Chinese TreeBank: Phrase Structure An-
notation of a Large Corpus. Natural Language Engineer-
ing, 11(2):207-238.
54
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 154?162,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Revisiting Pivot Language Approach for Machine Translation
Hua Wu and Haifeng Wang
Toshiba (China) Research and Development Center
5/F., Tower W2, Oriental Plaza, Beijing, 100738, China
{wuhua, wanghaifeng}@rdc.toshiba.com.cn
Abstract
This paper revisits the pivot language ap-
proach for machine translation. First,
we investigate three different methods
for pivot translation. Then we employ
a hybrid method combining RBMT and
SMT systems to fill up the data gap
for pivot translation, where the source-
pivot and pivot-target corpora are inde-
pendent. Experimental results on spo-
ken language translation show that this
hybrid method significantly improves the
translation quality, which outperforms the
method using a source-target corpus of
the same size. In addition, we pro-
pose a system combination approach to
select better translations from those pro-
duced by various pivot translation meth-
ods. This method regards system com-
bination as a translation evaluation prob-
lem and formalizes it with a regression
learning model. Experimental results in-
dicate that our method achieves consistent
and significant improvement over individ-
ual translation outputs.
1 Introduction
Current statistical machine translation (SMT) sys-
tems rely on large parallel and monolingual train-
ing corpora to produce translations of relatively
higher quality. Unfortunately, large quantities of
parallel data are not readily available for some lan-
guages pairs, therefore limiting the potential use
of current SMT systems. In particular, for speech
translation, the translation task often focuses on a
specific domain such as the travel domain. It is es-
pecially difficult to obtain such a domain-specific
corpus for some language pairs such as Chinese to
Spanish translation.
To circumvent the data bottleneck, some re-
searchers have investigated to use a pivot language
approach (Cohn and Lapata, 2007; Utiyama and
Isahara, 2007; Wu and Wang 2007; Bertoldi et al,
2008). This approach introduces a third language,
named the pivot language, for which there exist
large source-pivot and pivot-target bilingual cor-
pora. A pivot task was also designed for spoken
language translation in the evaluation campaign of
IWSLT 2008 (Paul, 2008), where English is used
as a pivot language for Chinese to Spanish trans-
lation.
Three different pivot strategies have been in-
vestigated in the literature. The first is based
on phrase table multiplication (Cohn and Lap-
ata 2007; Wu and Wang, 2007). It multiples
corresponding translation probabilities and lexical
weights in source-pivot and pivot-target transla-
tion models to induce a new source-target phrase
table. We name it the triangulation method. The
second is the sentence translation strategy, which
first translates the source sentence to the pivot sen-
tence, and then to the target sentence (Utiyama and
Isahara, 2007; Khalilov et al, 2008). We name it
the transfer method. The third is to use existing
models to build a synthetic source-target corpus,
from which a source-target model can be trained
(Bertoldi et al, 2008). For example, we can ob-
tain a source-pivot corpus by translating the pivot
sentence in the source-pivot corpus into the target
language with pivot-target translation models. We
name it the synthetic method.
The working condition with the pivot language
approach is that the source-pivot and pivot-target
parallel corpora are independent, in the sense that
they are not derived from the same set of sen-
tences, namely independently sourced corpora.
Thus, some linguistic phenomena in the source-
pivot corpus will lost if they do not exist in the
pivot-target corpus, and vice versa. In order to fill
up this data gap, we make use of rule-based ma-
chine translation (RBMT) systems to translate the
pivot sentences in the source-pivot or pivot-target
154
corpus into target or source sentences. As a re-
sult, we can build a synthetic multilingual corpus,
which can be used to improve the translation qual-
ity. The idea of using RBMT systems to improve
the translation quality of SMT sysems has been
explored in Hu et al (2007). Here, we re-examine
the hybrid method to fill up the data gap for pivot
translation.
Although previous studies proposed several
pivot translation methods, there are no studies to
combine different pivot methods for translation
quality improvement. In this paper, we first com-
pare the individual pivot methods and then in-
vestigate to improve pivot translation quality by
combining the outputs produced by different sys-
tems. We propose to regard system combination
as a translation evaluation problem. For transla-
tions from one of the systems, this method uses the
outputs from other translation systems as pseudo
references. A regression learning method is used
to infer a function that maps a feature vector
(which measures the similarity of a translation to
the pseudo references) to a score that indicates the
quality of the translation. Scores are first gener-
ated independently for each translation, then the
translations are ranked by their respective scores.
The candidate with the highest score is selected
as the final translation. This is achieved by opti-
mizing the regression learning model?s output to
correlate against a set of training examples, where
the source sentences are provided with several ref-
erence translations, instead of manually labeling
the translations produced by various systems with
quantitative assessments as described in (Albrecht
and Hwa, 2007; Duh, 2008). The advantage of
our method is that we do not need to manually la-
bel the translations produced by each translation
system, therefore enabling our method suitable for
translation selection among any systems without
additional manual work.
We conducted experiments for spoken language
translation on the pivot task in the IWSLT 2008
evaluation campaign, where Chinese sentences in
travel domain need to be translated into Spanish,
with English as the pivot language. Experimen-
tal results show that (1) the performances of the
three pivot methods are comparable when only
SMT systems are used. However, the triangulation
method and the transfer method significantly out-
perform the synthetic method when RBMT sys-
tems are used to improve the translation qual-
ity; (2) The hybrid method combining SMT and
RBMT system for pivot translation greatly im-
proves the translation quality. And this translation
quality is higher than that of those produced by the
system trained with a real Chinese-Spanish cor-
pus; (3) Our sentence-level translation selection
method consistently and significantly improves
the translation quality over individual translation
outputs in all of our experiments.
Section 2 briefly introduces the three pivot
translation methods. Section 3 presents the hy-
brid method combining SMT and RBMT sys-
tems. Section 4 describes the translation selec-
tion method. Experimental results are presented
in Section 5, followed by a discussion in Section
6. The last section draws conclusions.
2 Pivot Methods for Phrase-based SMT
2.1 Triangulation Method
Following the method described in Wu and Wang
(2007), we train the source-pivot and pivot-target
translation models using the source-pivot and
pivot-target corpora, respectively. Based on these
two models, we induce a source-target translation
model, in which two important elements need to
be induced: phrase translation probability and lex-
ical weight.
Phrase Translation Probability We induce the
phrase translation probability by assuming the in-
dependence between the source and target phrases
when given the pivot phrase.
?(s?|t?) =
?
p?
?(s?|p?)?(p?|t?) (1)
Where s?, p? and t? represent the phrases in the lan-
guages Ls, Lp and Lt, respectively.
Lexical Weight According to the method de-
scribed in Koehn et al (2003), there are two im-
portant elements in the lexical weight: word align-
ment information a in a phrase pair (s?, t?) and lex-
ical translation probability w(s|t).
Let a1 and a2 represent the word alignment in-
formation inside the phrase pairs (s?, p?) and (p?, t?)
respectively, then the alignment information inside
(s?, t?) can be obtained as shown in Eq. (2).
a = {(s, t)|?p : (s, p) ? a1 & (p, t) ? a2} (2)
Based on the the induced word alignment in-
formation, we estimate the co-occurring frequen-
cies of word pairs directly from the induced phrase
155
pairs. Then we estimate the lexical translation
probability as shown in Eq. (3).
w(s|t) = count(s, t)?
s? count(s?, t)
(3)
Where count(s, t) represents the co-occurring fre-
quency of the word pair (s, t).
2.2 Transfer Method
The transfer method first translates from the
source language to the pivot language using a
source-pivot model, and then from the pivot lan-
guage to the target language using a pivot-target
model. Given a source sentence s, we can trans-
late it into n pivot sentences p1, p2, ..., pn using a
source-pivot translation system. Each pi can be
translated into m target sentences ti1, ti2, ..., tim.
We rescore all the n ? m candidates using both
the source-pivot and pivot-target translation scores
following the method described in Utiyama and
Isahara (2007). If we use hfp and hpt to denote the
features in the source-pivot and pivot-target sys-
tems, respectively, we get the optimal target trans-
lation according to the following formula.
t? = argmax
t
L?
k=1
(?spk hspk (s, p)+?ptk hptk (p, t)) (4)
Where L is the number of features used in SMT
systems. ?sp and ?pt are feature weights set by
performing minimum error rate training as de-
scribed in Och (2003).
2.3 Synthetic Method
There are two possible methods to obtain a source-
target corpus using the source-pivot and pivot-
target corpora. One is to obtain target transla-
tions for the source sentences in the source-pivot
corpus. This can be achieved by translating the
pivot sentences in source-pivot corpus to target
sentences with the pivot-target SMT system. The
other is to obtain source translations for the tar-
get sentences in the pivot-target corpus using the
pivot-source SMT system. And we can combine
these two source-target corpora to produced a fi-
nal synthetic corpus.
Given a pivot sentence, we can translate it into
n source or target sentences. These n translations
together with their source or target sentences are
used to create a synthetic bilingual corpus. Then
we build a source-target translation model using
this corpus.
3 Using RBMT Systems for Pivot
Translation
Since the source-pivot and pivot-target parallel
corpora are independent, the pivot sentences in the
two corpora are distinct from each other. Thus,
some linguistic phenomena in the source-pivot
corpus will lost if they do not exist in the pivot-
target corpus, and vice versa. Here we use RBMT
systems to fill up this data gap. For many source-
target language pairs, the commercial pivot-source
and/or pivot-target RBMT systems are available
on markets. For example, for Chinese to Span-
ish translation, English to Chinese and English to
Spanish RBMT systems are available.
With the RBMT systems, we can create a syn-
thetic multilingual source-pivot-target corpus by
translating the pivot sentences in the pivot-source
or pivot-target corpus. The source-target pairs ex-
tracted from this synthetic multilingual corpus can
be used to build a source-target translation model.
Another way to use the synthetic multilingual cor-
pus is to add the source-pivot or pivot-target sen-
tence pairs in this corpus to the training data to re-
build the source-pivot or pivot-target SMT model.
The rebuilt models can be applied to the triangula-
tion method and the transfer method as described
in Section 2.
Moreover, the RBMT systems can also be used
to enlarge the size of bilingual training data. Since
it is easy to obtain monolingual corpora than bilin-
gual corpora, we use RBMT systems to translate
the available monolingual corpora to obtain syn-
thetic bilingual corpus, which are added to the
training data to improve the performance of SMT
systems. Even if no monolingual corpus is avail-
able, we can also use RBMT systems to translate
the sentences in the bilingual corpus to obtain al-
ternative translations. For example, we can use
source-pivot RBMT systems to provide alternative
translations for the source sentences in the source-
pivot corpus.
In addition to translating training data, the
source-pivot RBMT system can be used to trans-
late the test set into the pivot language, which
can be further translated into the target language
with the pivot-target RBMT system. The trans-
lated test set can be added to the training data to
further improve translation quality. The advantage
of this method is that the RBMT system can pro-
vide translations for sentences in the test set and
cover some out-of-vocabulary words in the test set
156
that are uncovered by the training data. It can also
change the distribution of some phrase pairs and
reinforce some phrase pairs relative to the test set.
4 Translation Selection
We propose a method to select the optimal trans-
lation from those produced by various translation
systems. We regard sentence-level translation se-
lection as a machine translation (MT) evaluation
problem and formalize this problem with a regres-
sion learning model. For each translation, this
method uses the outputs from other translation
systems as pseudo references. The regression ob-
jective is to infer a function that maps a feature
vector (which measures the similarity of a trans-
lation from one system to the pseudo references)
to a score that indicates the quality of the transla-
tion. Scores are first generated independently for
each translation, then the translations are ranked
by their respective scores. The candidate with the
highest score is selected.
The similar ideas have been explored in previ-
ous studies. Albrecht and Hwa (2007) proposed
a method to evaluate MT outputs with pseudo
references using support vector regression as the
learner to evaluate translations. Duh (2008) pro-
posed a ranking method to compare the transla-
tions proposed by several systems. These two
methods require quantitative quality assessments
by human judges for the translations produced by
various systems in the training set. When we apply
such methods to translation selection, the relative
values of the scores assigned by the subject sys-
tems are important. In different data conditions,
the relative values of the scores assigned by the
subject systems may change. In order to train a re-
liable learner, we need to prepare a balanced train-
ing set, where the translations produced by differ-
ent systems under different conditions are required
to be manually evaluated. In extreme cases, we
need to relabel the training data to obtain better
performance. In this paper, we modify the method
in Albrecht and Hwa (2007) to only prepare hu-
man reference translations for the training exam-
ples, and then evaluate the translations produced
by the subject systems against the references us-
ing BLEU score (Papineni et al, 2002). We use
smoothed sentence-level BLEU score to replace
the human assessments, where we use additive
smoothing to avoid zero BLEU scores when we
calculate the n-gram precisions. In this case, we
ID Description
1-4 n-gram precisions against pseudo refer-
ences (1 ? n ? 4)
5-6 PER and WER
7-8 precision, recall, fragmentation from
METEOR (Lavie and Agarwal, 2007)
9-12 precisions and recalls of non-
consecutive bigrams with a gap
size of m (1 ? m ? 2)
13-14 longest common subsequences
15-19 n-gram precision against a target cor-
pus (1 ? n ? 5)
Table 1: Feature sets for regression learning
can easily retrain the learner under different con-
ditions, therefore enabling our method to be ap-
plied to sentence-level translation selection from
any sets of translation systems without any addi-
tional human work.
In regression learning, we infer a function
f that maps a multi-dimensional input vec-
tor x to a continuous real value y, such that
the error over a set of m training examples,
(x1, y1), (x2, y2), ..., (xm, ym), is minimized ac-
cording to a loss function. In the context of trans-
lation selection, y is assigned as the smoothed
BLEU score. The function f represents a math-
ematic model of the automatic evaluation metrics.
The input sentence is represented as a feature vec-
tor x, which are extracted from the input sen-
tence and the comparisons against the pseudo ref-
erences. We use the features as shown in Table 1.
5 Experiments
5.1 Data
We performed experiments on spoken language
translation for the pivot task of IWSLT 2008. This
task translates Chinese to Spanish using English
as the pivot language. Table 2 describes the data
used for model training in this paper, including the
BTEC (Basic Travel Expression Corpus) Chinese-
English (CE) corpus and the BTEC English-
Spanish (ES) corpus provided by IWSLT 2008 or-
ganizers, the HIT olympic CE corpus (2004-863-
008)1 and the Europarl ES corpus2. There are
two kinds of BTEC CE corpus: BTEC CE1 and
1http://www.chineseldc.org/EN/purchasing.htm
2http://www.statmt.org/europarl/
157
Corpus Size SW TW
BTEC CE1 20,000 164K 182K
BTEC CE2 18,972 177K 182K
HIT CE 51,791 490K 502K
BTEC ES 19,972 182K 185K
Europarl ES 400,000 8,485K 8,219K
Table 2: Training data. SW and TW represent
source words and target words, respectively.
BTEC CE2. BTEC CE1 was distributed for the
pivot task in IWSLT 2008 while BTEC CE2 was
for the BTEC CE task, which is parallel to the
BTEC ES corpus. For Chinese-English transla-
tion, we mainly used BTEC CE1 corpus. We used
the BTEC CE2 corpus and the HIT Olympic cor-
pus for comparison experiments only. We used the
English parts of the BTEC CE1 corpus, the BTEC
ES corpus, and the HIT Olympic corpus (if in-
volved) to train a 5-gram English language model
(LM) with interpolated Kneser-Ney smoothing.
For English-Spanish translation, we selected 400k
sentence pairs from the Europarl corpus that are
close to the English parts of both the BTEC CE
corpus and the BTEC ES corpus. Then we built
a Spanish LM by interpolating an out-of-domain
LM trained on the Spanish part of this selected
corpus with the in-domain LM trained with the
BTEC corpus.
For Chinese-English-Spanish translation, we
used the development set (devset3) released for
the pivot task as the test set, which contains 506
source sentences, with 7 reference translations in
English and Spanish. To be capable of tuning pa-
rameters on our systems, we created a develop-
ment set of 1,000 sentences taken from the training
sets, with 3 reference translations in both English
and Spanish. This development set is also used to
train the regression learning model.
5.2 Systems and Evaluation Method
We used two commercial RBMT systems in our
experiments: System A for Chinese-English bidi-
rectional translation and System B for English-
Chinese and English-Spanish translation. For
phrase-based SMT translation, we used the Moses
decoder (Koehn et al, 2007) and its support train-
ing scripts. We ran the decoder with its default
settings and then used Moses? implementation of
minimum error rate training (Och, 2003) to tune
the feature weights on the development set.
To select translation among outputs produced
by different pivot translation systems, we used
SVM-light (Joachins, 1999) to perform support
vector regression with the linear kernel.
Translation quality was evaluated using both the
BLEU score proposed by Papineni et al (2002)
and also the modified BLEU (BLEU-Fix) score3
used in the IWSLT 2008 evaluation campaign,
where the brevity calculation is modified to use
closest reference length instead of shortest refer-
ence length.
5.3 Results by Using SMT Systems
We conducted the pivot translation experiments
using the BTEC CE1 and BTEC ES described
in Section 5.1. We used the three methods de-
scribed in Section 2 for pivot translation. For the
transfer method, we selected the optimal transla-
tions among 10? 10 candidates. For the synthetic
method, we used the ES translation model to trans-
late the English part of the CE corpus to Spanish to
construct a synthetic corpus. And we also used the
BTEC CE1 corpus to build a EC translation model
to translate the English part of ES corpus into Chi-
nese. Then we combined these two synthetic cor-
pora to build a Chinese-Spanish translation model.
In our experiments, only 1-best Chinese or Span-
ish translation was used since using n-best results
did not greatly improve the translation quality. We
used the method described in Section 4 to select
translations from the translations produced by the
three systems. For each system, we used three
different alignment heuristics (grow, grow-diag,
grow-diag-final4) to obtain the final alignment re-
sults, and then constructed three different phrase
tables. Thus, for each system, we can get three
different translations for each input. These differ-
ent translations can serve as pseudo references for
the outputs of other systems. In our case, for each
sentence, we have 6 pseudo reference translations.
In addition, we found out that the grow heuristic
performed the best for all the systems. Thus, for
an individual system, we used the translation re-
sults produced using the grow alignment heuristic.
The translation results are shown in Table 3.
ASR and CRR represent different input condi-
tions, namely the result of automatic speech recog-
3https://www.slc.atr.jp/Corpus/IWSLT08/eval/IWSLT08
auto eval.tgz
4A description of the alignment heuristics can be found at
http://www.statmt.org/jhuws/?n=FactoredTraining.Training
Parameters
158
Method BLEU BLEU-Fix
Triangulation 33.70/27.46 31.59/25.02
Transfer 33.52/28.34 31.36/26.20
Synthetic 34.35/27.21 32.00/26.07
Combination 38.14/29.32 34.76/27.39
Table 3: CRR/ASR translation results by using
SMT systems
nition and correct recognition result, respectively.
Here, we used the 1-best ASR result. From the
translation results, it can be seen that three meth-
ods achieved comparable translation quality on
both ASR and CRR inputs, with the translation re-
sults on CRR inputs are much better than those on
ASR inputs because of the errors in the ASR in-
puts. The results also show that our translation se-
lection method is very effective, which achieved
absolute improvements of about 4 and 1 BLEU
scores on CRR and ASR inputs, respectively.
5.4 Results by Using both RBMT and SMT
Systems
In order to fill up the data gap as discussed in Sec-
tion 3, we used the RBMT System A to translate
the English sentences in the ES corpus into Chi-
nese. As described in Section 3, this corpus can
be used by the three pivot translation methods.
First, the synthetic Chinese-Spanish corpus can be
combined with those produced by the EC and ES
SMT systems, which were used in the synthetic
method. Second, the synthetic Chinese-English
corpus can be added into the BTEC CE1 corpus to
build the CE translation model. In this way, the in-
tersected English phrases in the CE corpus and ES
corpus becomes more, which enables the Chinese-
Spanish translation model induced using the trian-
gulation method to cover more phrase pairs. For
the transfer method, the CE translation quality can
be also improved, which would result in the im-
provement of the Spanish translation quality.
The translation results are shown in the columns
under ?EC RBMT? in Table 4. As compared with
those in Table 3, the translation quality was greatly
improved, with absolute improvements of at least
5.1 and 3.9 BLEU scores on CRR and ASR inputs
for system combination results. The above results
indicate that RBMT systems indeed can be used to
fill up the data gap for pivot translation.
In our experiments, we also used a CE RBMT
system to enlarge the size of training data by pro-
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1 2 3 4 5 6 7
Phrase length
Co
ve
rag
e
SMT (Triangulation)
+EC RBMT
+EC RBMT+CE RBMT
+EC RBMT+CE RBMT+Test Set
Figure 1: Coverage on test source phrases
viding alternative English translations for the Chi-
nese part of the CE corpus. The translation results
are shown in the columns under ?+CE RBMT? in
Table 4. From the translation results, it can be
seen that, enlarging the size of training data with
RBMT systems can further improve the translation
quality.
In addition to translating the training data, the
CE RBMT system can be also used to translate the
test set into English, which can be further trans-
lated into Spanish with the ES RBMT system B.56
The translated test set can be further added to the
training data to improve translation quality. The
columns under ?+Test Set? in Table 4 describes
the translation results. The results show that trans-
lating the test set using RBMT systems greatly im-
proved the translation result, with further improve-
ments of about 2 and 1.5 BLEU scores on CRR
and ASR inputs, respectively.
The results also indicate that both the triangula-
tion method and the transfer method greatly out-
performed the synthetic method when we com-
bined both RBMT and SMT systems in our exper-
iments. Further analysis shows that the synthetic
method contributed little to system combination.
The selection results are almost the same as those
selected from the translations produced by the tri-
angulation and transfer methods.
In order to further analyze the translation re-
sults, we evaluated the above systems by examin-
ing the coverage of the phrase tables over the test
phrases. We took the triangulation method as a
case study, the results of which are shown in Fig-
5Although using the ES RBMT system B to translate the
training data did not improve the translation quality, it im-
proved the translation quality by translating the test set.
6The RBMT systems achieved a BLEU score of 24.36 on
the test set.
159
EC RBMT + CE RBMT + Test Set
Method BLEU BLEU-Fix BLEU BLEU-Fix BLEU BLEU-Fix
Triangulation 40.69/31.02 37.99/29.15 41.59/31.43 39.39/29.95 44.71/32.60 42.37/31.14
Transfer 42.06/31.72 39.73/29.35 43.40/33.05 40.73/30.06 45.91/34.52 42.86/31.92
Synthetic 39.10/29.73 37.26/28.45 39.90/30.00 37.90/28.66 41.16/31.30 37.99/29.36
Combination 43.21/33.23 40.58/31.17 45.09/34.10 42.88/31.73 47.06/35.62 44.94/32.99
Table 4: CRR/ASR translation results by using RBMT and SMT systems
Method BLEU BLEU-Fix
Triangulation 45.64/33.15 42.11/31.11
Transfer 47.18/34.56 43.61/32.17
Combination 48.42/36.42 45.42/33.52
Table 5: CRR/ASR translation results by using ad-
ditional monolingual corpora
ure 1. It can be seen that using RBMT systems
to translate the training and/or test data can cover
more source phrases in the test set, which results
in translation quality improvement.
5.5 Results by Using Monolingual Corpus
In addition to translating the limited bilingual cor-
pus, we also translated additional monolingual
corpus to further enlarge the size of the training
data. We assume that it is easier to obtain a mono-
lingual pivot corpus than to obtain a monolingual
source or target corpus. Thus, we translated the
English part of the HIT Olympic corpus into Chi-
nese and Spanish using EC and ES RBMT sys-
tems. The generated synthetic corpus was added to
the training data to train EC and ES SMT systems.
Here, we used the synthetic CE Olympic corpus
to train a model, which was interpolated with the
CE model trained with both the BTEC CE1 cor-
pus and the synthetic BTEC corpus to obtain an
interpolated CE translation model. Similarly, we
obtained an interpolated ES translation model. Ta-
ble 5 describes the translation results.7 The results
indicate that translating monolingual corpus using
the RBMT system further improved the translation
quality as compared with those in Table 4.
6 Discussion
6.1 Effects of Different RBMT Systems
In this section, we compare the effects of two
commercial RBMT systems with different transla-
7Here we excluded the synthetic method since it greatly
falls behind the other two methods.
Method Sys. A Sys. B Sys. A+B
Triangulation 40.69 39.28 41.01
Transfer 42.06 39.57 43.03
Synthetic 39.10 38.24 39.26
Combination 43.21 40.59 44.27
Table 6: CRR translation results (BLEU scores)
by using different RBMT systems
tion accuracy on spoken language translation. The
goals are (1) to investigate whether a RBMT sys-
tem can improve pivot translation quality even if
its translation accuracy is not high, and (2) to com-
pare the effects of RBMT system with different
translation accuracy on pivot translation. Besides
the EC RBMT system A used in the above section,
we also used the EC RBMT system B for this ex-
periment.
We used the two systems to translate the test set
from English to Chinese, and then evaluated the
translation quality against Chinese references ob-
tained from the IWSLT 2008 evaluation campaign.
The BLEU scores are 43.90 and 29.77 for System
A and System B, respectively. This shows that
the translation quality of System B on spoken lan-
guage corpus is much lower than that of System A.
Then we applied these two different RBMT sys-
tems to translate the English part of the BTEC ES
corpus into Chinese as described in Section 5.4.
The translation results on CRR inputs are shown
in Table 6.8 We replicated some of the results in
Table 4 for the convenience of comparison. The
results indicate that the higher the translation ac-
curacy of the RBMT system is, the better the pivot
translation is. If we compare the results with those
only using SMT systems as described in Table 3,
the translation quality was greatly improved by at
least 3 BLEU scores, even if the translation ac-
8We omitted the ASR translation results since the trends
are the same as those for CRR inputs. And we only showed
BLEU scores since the trend for BLEU-Fix scores is similar.
160
Method Multilingual + BTEC CE1
Triangulation 41.86/39.55 42.41/39.55
Transfer 42.46/39.09 43.84/40.34
Standard 42.21/40.23 42.21/40.23
Combination 43.75/40.34 44.68/41.14
Table 7: CRR translation results by using multilin-
gual corpus. ?/? separates the BLEU and BLEU-
fix scores.
curacy of System B is not so high. Combining
two RBMT systems further improved the transla-
tion quality, which indicates that the two systems
complement each other.
6.2 Results by Using Multilingual Corpus
In this section, we compare the translation results
by using a multilingual corpus with those by us-
ing independently sourced corpora. BTEC CE2
and BTEC ES are from the same source sentences,
which can be taken as a multilingual corpus. The
two corpora were employed to build CE and ES
SMT models, which were used in the triangula-
tion method and the transfer method. We also ex-
tracted the Chinese-Spanish (CS) corpus to build a
standard CS translation system, which is denoted
as Standard. The comparison results are shown
in Table 7. The translation quality produced by
the systems using a multilingual corpus is much
higher than that produced by using independently
sourced corpora as described in Table 3, with an
absolute improvement of about 5.6 BLEU scores.
If we used the EC RBMT system, the translation
quality of those in Table 4 is comparable to that by
using the multilingual corpus, which indicates that
our method using RBMT systems to fill up the data
gap is effective. The results also indicate that our
translation selection method for pivot translation
outperforms the method using only a real source-
target corpus.
For comparison purpose, we added BTEC CE1
into the training data. The translation quality was
improved by only 1 BLEU score. This again
proves that our method to fill up the data gap is
more effective than that to increase the size of the
independently sourced corpus.
6.3 Comparison with Related Work
In IWSLT 2008, the best result for the pivot task
is achieved by Wang et al (2008). In order to
compare the results, we added the bilingual HIT
Ours Wang TSAL
BLEU 49.57 - 48.25
BLEU-Fix 46.74 45.10 45.27
Table 8: Comparison with related work
Olympic corpus into the CE training data.9 We
also compared our translation selection method
with that proposed in (Wang et al, 2008) that
is based on the target sentence average length
(TSAL). The translation results are shown in Ta-
ble 8. ?Wang? represents the results in Wang et al
(2008). ?TSAL? represents the translation selec-
tion method proposed in Wang et al (2008), which
is applied to our experiment. From the results, it
can be seen that our method outperforms the best
system in IWSLT 2008 and that our translation se-
lection method outperforms the method based on
target sentence average length.
7 Conclusion
In this paper, we have compared three differ-
ent pivot translation methods for spoken language
translation. Experimental results indicated that the
triangulation method and the transfer method gen-
erally outperform the synthetic method. Then we
showed that the hybrid method combining RBMT
and SMT systems can be used to fill up the data
gap between the source-pivot and pivot-target cor-
pora. By translating the pivot sentences in inde-
pendent corpora, the hybrid method can produce
translations whose quality is higher than those pro-
duced by the method using a source-target corpus
of the same size. We also showed that even if the
translation quality of the RBMT system is low, it
still greatly improved the translation quality.
In addition, we proposed a system combination
method to select better translations from outputs
produced by different pivot methods. This method
is developed through regression learning, where
only a small size of training examples with ref-
erence translations are required. Experimental re-
sults indicate that this method can consistently and
significantly improve translation quality over indi-
vidual translation outputs. And our system out-
performs the best system for the pivot task in the
IWSLT 2008 evaluation campaign.
9We used about 70k sentence pairs for CE model training,
while Wang et al (2008) used about 100k sentence pairs, a
CE translation dictionary and more monolingual corpora for
model training.
161
References
Joshua S. Albrecht and Rebecca Hwa. 2007. Regres-
sion for Sentence-Level MT Evaluation with Pseudo
References. In Proceedings of the 45th Annual
Meeting of the Accosiation of Computational Lin-
guistics, pages 296?303.
Nicola Bertoldi, Madalina Barbaiani, Marcello Fed-
erico, and Roldano Cattoni. 2008. Phrase-Based
Statistical Machine Translation with Pivot Lan-
guages. In Proceedings of the International Work-
shop on Spoken Language Translation, pages 143-
149.
Tevor Cohn and Mirella Lapata. 2007. Machine Trans-
lation by Triangulation: Making Effective Use of
Multi-Parallel Corpora. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 348?355.
Kevin Duh. 2008. Ranking vs. Regression in Machine
Translation Evaluation. In Proceedings of the Third
Workshop on Statistical Machine Translation, pages
191?194.
Xiaoguang Hu, Haifeng Wang, and Hua Wu. 2007.
Using RBMT Systems to Produce Bilingual Corpus
for SMT. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 287?295.
Thorsten Joachims. 1999. Making Large-Scale
SVM Learning Practical. In Bernhard Scho?elkopf,
Christopher Burges, and Alexander Smola, edi-
tors, Advances in Kernel Methods - Support Vector
Learning. MIT Press.
Maxim Khalilov, Marta R. Costa-Jussa`, Carlos A.
Henr??quez, Jose? A.R. Fonollosa, Adolfo Herna?ndez,
Jose? B. Marin?o, Rafael E. Banchs, Chen Boxing,
Min Zhang, Aiti Aw, and Haizhou Li. 2008. The
TALP & I2R SMT Systems for IWSLT 2008. In
Proceedings of the International Workshop on Spo-
ken Language Translation, pages 116?123.
Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL: Human Language Technology Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexanda Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the 45th Annual Meeting of the
Associa-tion for Computational Linguistics, demon-
stration session, pages 177?180.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An Automatic Metric for MT Evaluation with High
Levels of Correlation with Human Judgments. In
Proceedings of Workshop on Statistical Machine
Translation at the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 228?
231.
Franz J. Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th annual meeting of the Association for
Computational Linguistics, pages 311?318.
Michael Paul. 2008. Overview of the IWSLT 2008
Evaluation Campaign. In Proceedings of the In-
ternational Workshop on Spoken Language Trans-
lation, pages 1?17.
Masao Utiyama and Hitoshi Isahara. 2007. A Com-
parison of Pivot Methods for Phrase-Based Statisti-
cal Machine Translation. In Proceedings of human
language technology: the Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 484?491.
Haifeng Wang, Hua Wu, Xiaoguang Hu, Zhanyi Liu,
Jianfeng Li, Dengjun Ren, and Zhengyu Niu. 2008.
The TCH Machine Translation System for IWSLT
2008. In Proceedings of the International Workshop
on Spoken Language Translation, pages 124?131.
Hua Wu and Haifeng Wang. 2007. Pivot Lan-
guage Approach for Phrase-Based Statistical Ma-
chine Translation. In Proceedings of 45th Annual
Meeting of the Association for Computational Lin-
guistics, pages 856?863.
162
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 809?816,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Dependency Based Chinese Sentence Realization 
 
 
Wei He1, Haifeng Wang2, Yuqing Guo2, Ting Liu1 
1Information Retrieval Lab, Harbin Institute of Technology, Harbin, China 
{whe,tliu}@ir.hit.edu.cn 
2Toshiba (China) Research and Development Center, Beijing, China 
{wanghaifeng,guoyuqing}@rdc.toshiba.com.cn 
 
  
Abstract 
This paper describes log-linear models for a 
general-purpose sentence realizer based on de-
pendency structures. Unlike traditional realiz-
ers using grammar rules, our method realizes 
sentences by linearizing dependency relations 
directly in two steps. First, the relative order 
between head and each dependent is deter-
mined by their dependency relation. Then the 
best linearizations compatible with the relative 
order are selected by log-linear models. The 
log-linear models incorporate three types of 
feature functions, including dependency rela-
tions, surface words and headwords. Our ap-
proach to sentence realization provides sim-
plicity, efficiency and competitive accuracy. 
Trained on 8,975 dependency structures of a 
Chinese Dependency Treebank, the realizer 
achieves a BLEU score of 0.8874. 
1 Introduction 
Sentence realization can be described as the 
process of converting the semantic and syntactic 
representation of a sentence or series of sen-
tences into meaningful, grammatically correct 
and fluent text of a particular language. 
Most previous general-purpose realization sys-
tems are developed via the application of a set of 
grammar rules based on particular linguistic 
theories, e.g. Lexical Functional Grammar (LFG), 
Head Driven Phrase Structure Grammar (HPSG), 
Combinatory Categorical Grammar (CCG), Tree 
Adjoining Grammar (TAG) etc. The grammar 
rules are either developed by hand, such as those 
used in LinGo (Carroll et al, 1999), OpenCCG 
(White, 2004) and XLE (Crouch et al, 2007), or 
extracted automatically from annotated corpora, 
like the HPSG (Nakanishi et al, 2005), LFG 
(Cahill and van Genabith, 2006; Hogan et al, 
2007) and CCG (White et al, 2007) resources 
derived from the Penn-II Treebank. 
Over the last decade, there has been a lot of in-
terest in a generate-and-select paradigm for sur-
face realization. The paradigm is characterized 
by a separation between realization and selection, 
in which rule-based methods are used to generate 
a space of possible paraphrases, and statistical 
methods are used to select the most likely reali-
zation from the space. Usually, two statistical 
models are used to rank the output candidates. 
One is n-gram model over different units, such as 
word-level bigram/trigram models (Bangalore 
and Rambow, 2000; Langkilde, 2000), or fac-
tored language models integrated with syntactic 
tags (White et al 2007). The other is log-linear 
model with different syntactic and semantic fea-
tures (Velldal and Oepen, 2005; Nakanishi et al, 
2005; Cahill et al, 2007). 
However, little work has been done on proba-
bilistic models learning direct mapping from in-
put to surface strings, without the effort to con-
struct a grammar. Guo et al (2008) develop a 
general-purpose realizer couched in the frame-
work of Lexical Functional Grammar based on 
simple n-gram models. Wan et al (2009) present 
a dependency-spanning tree algorithm for word 
ordering, which first builds dependency trees to 
decide linear precedence between heads and 
modifiers then uses an n-gram language model to 
order siblings. Compared with n-gram model, 
log-linear model is more powerful in that it is 
easy to integrate a variety of features, and to tune 
feature weights to maximize the probability. A 
few papers have presented maximum entropy 
models for word or phrase ordering (Ratnaparkhi, 
2000; Filippova and Strube, 2007). However, 
those attempts have been limited to specialized 
applications, such as air travel reservation or or-
dering constituents of a main clause in German.  
This paper presents a general-purpose realizer 
based on log-linear models for directly lineariz-
ing dependency relations given dependency 
structures. We reduce the generation space by 
809
two techniques: the first is dividing the entire 
dependency tree into one-depth sub-trees and 
solving linearization in sub-trees; the second is 
the determination of relative positions between 
dependents and heads according to dependency 
relations. Then the best linearization for each 
sub-tree is selected by the log-linear model that 
incorporates three types of feature functions, in-
cluding dependency relations, surface words and 
headwords. The evaluation shows that our realiz-
er achieves competitive generation accuracy. 
The paper is structured as follows. In Section 
2, we describe the idea of dividing the realization 
procedure for an entire dependency tree into a 
series of sub-procedures for sub-trees. We de-
scribe how to determine the relative positions 
between dependents and heads according to de-
pendency relations in Section 3. Section 4 gives 
details of the log-linear model and the feature 
functions used for sentence realization. Section 5 
explains the experiments and provides the results. 
2 Sentence Realization from Dependen-
cy Structure  
2.1 The Dependency Input  
The input to our sentence realizer is a dependen-
cy structure as represented in the HIT Chinese 
Dependency Treebank (HIT-CDT)1. In our de-
pendency tree representations, dependency rela-
tions are represented as arcs pointing from a head 
to a dependent. The types of dependency arcs 
indicate the semantic or grammatical relation-
ships between the heads and the dependents, 
which are recorded in the dependent nodes. Fig-
ure 1 gives an example of dependency tree repre-
sentation for the sentence: 
 
(1) ? ? ?? ?? 
 this is Wuhan Airline 
 ?? ?? ?? ?? 
 first time buy Boeing airliner 
?This is the first time for Airline Wuhan to buy 
Boeing airliners.? 
In a dependency structure, dependents are un-
ordered, i.e. the string position of each node is 
not recorded in the representation. Our sentence 
realizer takes such an unordered dependency tree 
as input, determines the linear order of the words 
                                                 
1 HIT-CDT (http://ir.hit.edu.cn) includes 10,000 sentences 
and 215,334 words, which are manually annotated with 
part-of-speech tags and dependency labels. (Liu et al, 
2006a) 
as encoded in the nodes of the dependency struc-
ture and produces a grammatical sentence. As the 
dependency structures input to our realizer have 
been lexicalized, lexical selection is not involved 
during the surface realization. 
2.2 Divide and Conquer Strategy for Linea-
rization 
For determining the linear order of words 
represented by nodes of the given dependency 
structure, in principle, the sentence realizer has 
to produce all possible sequences of the nodes 
from the input tree and selects the most likely 
linearization among them. If the dependency tree 
consists of a considerable number of nodes, this 
procedure would be very time-consuming.  To 
reduce the number of possible realizations, our 
generation algorithm adopts a divide-and-
conquer strategy, which divides the whole tree 
into a set of sub-trees of depth one and recursive-
ly linearizes the sub-trees in a bottom-up fashion. 
As illustrated in Figure 2, sub-trees c and d, 
which are at the bottom of the tree, are linearized 
first, then sub-tree b is processed, and finally 
sub-tree a.  
The procedure imposes a projective constraint 
on the dependency structures, viz. each head 
dominates a continuous substring of the sentence 
realization. This assumption is feasible in the 
application of the dependency-based generation, 
because: (i) it has long been observed that the 
dependency structures of a vast majority of sen-
tences in the languages of the world are projec-
tive (Igor, 1988) and (ii) non-projective depen-
dencies in Chinese, for the most part, are used to 
account for non-local dependency phenomena. 
Figure 1: The dependency tree for the sentence 
???????????????? 
??(HED)
is 
??(SBV)
this 
???(VOB) 
buy 
???(ADV)
first time 
???(VOB) 
airliner 
???(SBV)
airline 
???(ATT)
Wuhan 
???(ATT) 
Boeing 
810
Though non-local dependencies are important for 
accurate semantic analysis, they can be easily 
converted to local dependencies conforming to 
the projective constraint. In fact, we find that the 
10, 000 manually-build dependency trees of the 
HIT-CDT do not contain any non-projective de-
pendencies. 
3 Relative Position Determination 
In dependency structures, the semantic or gram-
matical roles of the nodes are indicated by types 
of dependency relations. For example, the VOB 
dependency relation, which stands for the verb-
object structure, means that the head is a verb 
and the dependent is an object of the verb; the 
ATT relation, means that the dependent is an 
attribute of the head. In languages with fairly 
rigid word order, the relative position between 
the head and dependent of a certain relation is in 
a fixed order. For example in Chinese, the object 
almost always occurs behind its dominating verb; 
the attribute modifier always occurs in front of 
its head word. Therefore, we can draw a conclu-
sion that the relative positions between head and 
dependent of VOB and ATT can be determined 
by the types of dependency relations. 
We make a statistic on the relative positions 
between head and dependent for each dependen-
cy relation type. Following (Covington, 2001), 
we call a dependent that precedes its head prede-
pendent, a dependent that follows its head post-
dependent. The corpus used to gather appropriate 
statistics is HIT-CDT. Table 1 gives the numbers 
??(HED) 
is 
??(SBV) 
this 
?   ?   ???????????? 
?  
???(VOB) 
buy 
???(ADV)
first time 
?  ?  
????   ??   ??   ???? 
???(VOB)
airliner 
???(ATT)
Boeing 
??   ??
???(SBV) 
Airline 
???(ATT) 
Wuhan 
??   ?? 
sub-tree a 
sub-tree b 
sub-tree c sub-tree d 
Figure 2: Illustration of the linearization procedure 
Relation Description Postdep. Predep.
ADV adverbial 1 25977
APP appositive 807 0
ATT attribute 0 47040
CMP complement 2931 3
CNJ conjunctive 0 2124
COO coordinate 6818 0
DC dep. clause 197 0
DE DE phrase 0 10973
DEI DEI phrase 131 3
DI DI phrase 0 400
IC indep.clause 3230 0
IS indep.structure 125 794
LAD left adjunct 0 2644
MT mood-tense 3203 0
POB prep-obj 7513 0
QUN quantity 0 6092
RAD right adjunct 1332 1
SBV subject-verb 6 16016
SIM similarity 0 44
VOB verb-object 23487 21
VV verb-verb 6570 2
Table 1: Numbers of pre/post-dependents for each 
dependency relation 
811
of predependent/postdependent for each type of 
dependency relations and its descriptions. 
Table 1 shows that 100% dependents of ATT 
relation are predependents and 23,487(99.9%) 
against 21(0.1%) VOB dependents are postde-
pendents. Almost all the dependency relations 
have a dominant dependent type?predependent 
or postdependent. Although some dependency 
relations have exceptional cases (e.g. VOB), the 
number is so small that it can be ignored. The 
only exception is the IS relation, which has 
794(86.4%) predependents and 125(13.6%) 
postdependents. The IS label is an abbreviation 
for independent structure. This type of depen-
dency relation is usually used to represent inter-
jections or comments set off by brackets, which 
usually has little grammatical connection with 
the head. Figure 3 gives an example of indepen-
dent structure. This example is from a news re-
port, and the phrase ??????? (set apart by 
brackets in the original text) is a supplementary 
explanation for the source of the news. The con-
nection between this phrase and the main clause 
is so weak that either it precedes or follows the 
head verb is acceptable in grammar. However, 
this kind of news-source-explanation is customa-
ry to place at the beginning of a sentence in Chi-
nese. This can probably explain the majority of 
the IS-tagged dependents are predependents. 
If we simply treat all the IS dependents as pre-
dependents, we can assume that every dependen-
cy relation has only one type of dependent, either 
predependent or postdependent. Therefore, the 
relative position between head and dependent 
can be determined just by the types of dependen-
cy relations. 
In the light of this assumption, all dependents 
in a sub-tree can be classified into two groups?
predependents and postdependents. The prede-
pendents must precede the head, and the postde-
pendents must follow the head. This classifica-
tion not only reduces the number of possible se-
quences, but also solves the linearization of a 
sub-tree if the sub-tree contains only one depen-
dent, or two dependents of different types, viz. 
one predependent and one postdependent. In sub-
tree c of Figure 2, the dependency relation be-
tween the only dependent and the head is ATT, 
which indicates that the dependent is a prede-
pendent. Therefore, node 7 is bound to precede 
node 5, and the only linearization result is ???
???. In sub-tree a of the same figure, the clas-
sification for SBV is predependent, and for VOB 
is postdependent, so the only linearization is 
<node 2, node 1, node 3>.  
In HIT-CDT, there are 108,086 sub-trees in 
the 10,000 sentences, 65% sub-trees have only 
one dependent, and 7% sub-trees have two de-
pendents of different types (one predependent 
and one postdependent). This means that the 
relative position classification can deterministi-
cally linearize 72% sub-trees, and only the rest 
28% sub-trees with more than one predependent 
or postdependent need to be further determined. 
4 Log-linear Models 
We use log-linear models for selecting the se-
quence with the highest probability from all the 
possible linearizations of a sub-tree. 
4.1 The Log-linear Model 
Log-linear models employ a set of feature func-
tions to describe properties of the data, and a set 
of learned weights to determine the contribution 
of each feature. In this framework, we have a set 
of M feature functions Mmtrhm ,...,1),,( = . 
For each feature function, there exists a model 
parameter Mmtrm ,...,1),,( =?  that is fitted to 
optimize the likelihood of the training data. A 
conditional log-linear model for the probability 
of a realization r given the dependency tree t, has 
the general parametric form 
)],(exp[
)(
1
)|(
1
trh
tZ
trp m
M
m
m?
=
= ?
?
?            (1) 
where )(tZ?  is a normalization factor defined as 
? ?
? =
=
)(' 1
)],'(exp[)(
tYr
m
M
m
m trhtZ ??                    (2) 
And Y(t) gives the set of all possible realizations 
of the dependency tree t. 
4.2 Feature Functions 
We use three types of feature functions for cap-
turing relations among nodes on the dependency 
tree. In order to better illustrate the feature func-
tions used in the log-linear model, we redraw 
sub-tree b of Figure 2 in Figure 4. Here we as-
sume the linearizations of sub-tree c and d have 
Figure 3: Example of independent structure 
???(HED) 
serious 
??????(IS) 
Xinhua news 
?????(SBV) 
southern snowstorm 
812
been finished, and the strings of linearizing re-
sults are recorded in nodes 5 and 6. 
The sub-tree in Figure 4 has two predepen-
dents (SBV and ADV) and one postdependent 
(VOB). As a result of this classification, the only 
two possible linearizations of the sub-tree are 
<node 4, node 6, node 3, node 5> and <node 6, 
node 4, node 3, node 5>. Then the log-linear 
model that incorporates three types of feature 
functions is used to make further selection. 
Dependency Relation Model: For a particular 
sub-tree structure, the task of generating a string 
covered by the nodes on the sub-tree is equiva-
lent to linearizing all the dependency relations in 
that sub-tree. We linearize the dependency rela-
tions by computing n-gram models, similar to 
traditional word-based language models, except 
using the names of dependency relations instead 
of words. For the two linearizations of Figure 4, 
the corresponding dependency relation sequences 
are ?ADV SBV VOB VOB? and ?SBV ADV 
VOB VOB?. The dependency relation model 
calculates the probability of dependency relation 
n-gram P(DR) according to Eq.(3). The probabil-
ity score is integrated into the log-linear model as 
a feature.  
)...()( 11 m
m DRDRPDRP =  (3) 
       )|( 1 1
1
?
+?
=
?= k nkm
k
k DRDRP  
Word Model: We integrate an n-gram word 
model into the log-linear model for capturing the 
relation between adjacent words. For a string of 
words generated from a possible sequence of 
sub-tree nodes, the word models calculate word-
based n-gram probabilities of the string. For ex-
ample, in Figure 4, the strings generated by the 
two possible sequences are ????? ?? ?
? ????? and ??? ???? ?? ???
??. The word model takes these two strings as 
input, and calculates the n-gram probabilities. 
Headword Model: 2 In dependency representa-
tions, heads usually play more important roles 
than dependents. The headword model calculates 
the n-gram probabilities of headwords, without 
regard to the words occurring at dependent nodes, 
in that dependent words are usually less impor-
tant than headwords. In Figure 4, the two possi-
ble sequences of headwords are ??? ?? ?
?  ??? and ???  ??  ??  ???. The 
headword strings are usually more generic than 
the strings including all words, and thus the 
headword model is more likely to relax the data 
sparseness. 
   Table 2 gives some examples of all the features 
used in the log-linear model. The examples listed 
in the table are features of the linearization 
<node 6, node 4, node 3, node 5>, extracted from 
the sub-tree in Figure 4. 
In this paper, all the feature functions used in 
the log-linear model are n-gram probabilities. 
However, the log-linear framework has great 
potential for including other types of features. 
4.3 Parameter Estimation 
BLEU score, a method originally proposed to 
automatically evaluate machine translation quali-
ty (Papineni et al, 2002), has been widely used 
as a metric to evaluate general-purpose sentence 
generation (Langkilde, 2002; White et al, 2007; 
Guo et al 2008, Wan et al 2009). The BLEU 
measure computes the geometric mean of the 
precision of n-grams of various lengths between 
a sentence realization and a (set of) reference(s).  
To estimate the parameters ),...,( 1 M??  for the 
feature functions ),...,( 1 Mhh , we use BLEU
3 as 
optimization objective function and adopt the 
approach of minimum error rate training 
                                                 
2 Here the term ?headword? is used to describe the word 
that occurs at head nodes in dependency trees.  
3 The BLEU scoring script is supplied by NIST Open Ma-
chine Translation Evaluation at 
ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 
Feature function Examples of features 
Dependency Relation ?SBV ADV VOB?  ?ADV VOB VOB? 
Word Model ???????? ???????? ????????????????
Headword Model ?????? ?????? ?????? 
Table 2: Examples of feature functions 
???(VOB) 
buy 
???(ADV) 
first time 
???(VOB) 
airliner 
?????? 
airliners of Boeing 
???(SBV)
Airline 
?????? 
Airline Wuhan
Figure 4: Sub-tree with multiple predependents
813
(MERT), which is popular in statistical machine 
translation (Och, 2003).   
4.4 The Realization Algorithm 
The realization algorithm is a recursive proce-
dure that starts from the root node of the depen-
dency tree, and traverses the tree by depth-first 
search. The pseudo code of the realization algo-
rithm is shown in Figure 5. 
5 Experiments 
5.1 Experimental Design 
Our experiments are carried out on HIT-CDT. 
We randomly select 526 sentences as the test set, 
and 499 sentences as the development set for 
optimizing the model parameters. The rest 8,975 
sentences of the HIT-CDT are used for training 
of the dependency relation model. For training of 
word models, we use the Xinhua News part 
(6,879,644 words) of Chinese Gigaword Second 
Edition (LDC2005T14), segmented by the Lan-
guage Technology Platform (LTP) 4 . And for 
training the headword model, we use both the 
HIT-CDT and the HIT Chinese Skeletal Depen-
dency Treebank (HIT-CSDT). HIT-CSDT is a 
                                                 
4 http://ir.hit.edu.cn/demo/ltp  
component of LTP and contains 49,991 sen-
tences in dependency structure representation 
(without dependency relation labels). 
As the input dependency representation does 
not contain punctuation information, we simply 
remove all punctuation marks in the test and de-
velopment sets. 
5.2 Evaluation Metrics 
In addition to BLEU score, percentage of exactly 
matched sentences and average NIST simple 
string accuracy (SSA) are adopted as evaluation 
metrics. The exact match measure is percentage 
of the generated string that exactly matches the 
corresponding reference sentence. The average 
NIST simple string accuracy score reflects the 
average number of insertion (I), deletion (D), and 
substitution (S) errors between the output sen-
tence and the reference sentence. Formally, SSA 
= 1 ? (I + D + S) / R, where R is the number of 
tokens in the reference sentence. 
5.3 Experimental Results 
All the evaluation results are shown in Table 3. 
The first experiment, which is a baseline experi-
ment, ignores the tree structure and randomly 
chooses position for every word. From the 
second experiment, we begin to utilize the tree 
structure and apply the realization algorithm de-
scribed in Section 4.4. In the second experiment, 
predependents are distinguished from postdepen-
dents by the relative position determination me-
thod (RPD), then the orders inside predependents 
and postdependents are chosen randomly. From 
the third experiments, the log-linear models are 
used for scoring the generated sequences, with 
the aid of three types of feature functions as de-
scribed in Section 4.2. First, the feature functions 
of trigram dependency relation model (DR), bi-
gram word model (Bi-WM), trigram word model 
(Tri-WM) (with Katz backoff) and trigram 
headword model (HW) are used separately in 
experiments 3-6. Then we combine the feature 
1:procedure SEARCH 
2:input: sub-tree T {head:H dep.:D1?Dn} 
3:  if n = 0 then return 
4:  for i := 1 to n 
5:    SEARCH(Di) 
6:  Apre := {} 
7:  Apost := {} 
8:  for i := 1 to n 
9:    if PRE-DEP(Di) then Apre:=Apre?{Di} 
10:   if POST-DEP(Di) then Apost:=Apost?{Di} 
11: for all permutations p1 of Apre 
12:   for all permutations p2 of Apost 
13:     sequence s := JOIN(p1,H,p2) 
14:     score r := LOG-LINEAR(s) 
15:     if best-score(r) then RECORD(r,s) 
Figure 5: The algorithm for linearizations of sub-
trees 
 Model BLEU ExMatch SSA 
1 Random 0.1478 0.0038 0.2044 
2 RPD + Random 0.5943 0.1274 0.6369 
3 RPD + DR 0.7204 0.2167 0.7683 
4 RPD + Bi-WM 0.8289 0.4125 0.8270 
5 RPD + Tri-WM 0.8508 0.4715 0.8415 
6 RPD + HW 0.7592 0.2909 0.7638 
7 RPD + DR + Bi-WM 0.8615 0.4810 0.8723 
8 RPD + DR + Tri-WM 0.8772 0.5247 0.8817 
9 RPD + DR + Tri-WM + HW 0.8874 0.5475 0.8920 
Table 3: BLEU, ExMatch and SSA scores on the test set 
814
functions incrementally based on the RPD and 
DR model. 
The relative position determination plays an 
important role in the realization algorithm. We 
observe that the BLEU score is boosted from 
0.1478 to 0.5943 by using the RPD method. This 
can be explained by the reason that the lineariza-
tions of 72% sub-trees can be definitely deter-
mined by the RPD method. All of the four fea-
ture functions we have tested achieve considera-
ble improvement in BLEU scores. The depen-
dency relation model achieves 0.7204, the bi-
gram word model 0.8289, the trigram word mod-
el 0.8508 and the headword model achieves 
0.7592. While the combined models perform bet-
ter than any of their individual component mod-
els. On the foundation of relative position deter-
mination method, the combination of dependen-
cy relation and bigram word model achieves a 
BLEU score of 0.8615, and the combination of 
dependency relation and trigram word model 
achieves a BLEU score of 0.8772. Finally the 
combination of dependency relation model, tri-
gram word model and headword model achieves 
the best result 0.8874.  
5.4 Discussion 
We first inspected the errors made by the relative 
position determination method. In the treebank-
tree test set, there are 7 predependents classified 
as postdependents and 3 postdependents classi-
fied as predependents by error. Among the 9,384 
dependents, the error rate of the relative position 
determination method is very small (0.1%). 
Then we make a classification on the errors in 
the experiment of dependency relation model 
(with relative position determination method). 
Table 4 shows the distribution of the errors. 
The first type of errors is caused by duplicate 
dependency relations, i.e. a head with two or 
more dependents that have the same dependency 
relations. In this situation, only using the depen-
dency relation model cannot generate the right 
linearization. However, word models, which util-
ize the word information, can make distinctions 
between the dependencies. The reason for the 
errors of SBV-ADV and ATT-QUN is probably 
because the order of these pairs of grammar roles 
is somewhat flexible. For example, the strings of 
???(ADV)/today ?(SBV)/I? and ??(SBV)/I 
??(ADV)/today? are both very common and 
acceptable in Chinese. 
The word models tend to combine the nodes 
that have strong correlation together. For exam-
ple in Figure 6, node 2 is more likely to precede 
node 3 because the words ??? /protect? and 
??? /future? have strong correlation, but the 
correct order is <node 3, node 2>. 
Headword model only consider the words oc-
cur at head nodes, which is helpful in the situa-
tion like Figure 6. In our experiments, the head-
word model gets a relatively low performance by 
itself, however, the addition of headword model 
to the combination of the other two feature func-
tions improves the result from 0.8772 to 0.8874. 
This indicates that the headword model is com-
plementary to the other feature functions. 
6 Conclusions 
We have presented a general-purpose realizer 
based on log-linear models, which directly maps 
dependency relations into surface strings. The 
linearization of a whole dependency tree is di-
vided into a series of sub-procedures on sub-trees. 
The dependents in the sub-trees are classified 
into two groups, predependents or postdepen-
dents, according to their dependency relations. 
The evaluation shows that this relative position 
determination method achieves a considerable 
result. The log-linear model, which incorporates 
three types of feature functions, including de-
pendency relation, surface words and headwords, 
successfully captures factors in sentence realiza-
tion and demonstrates competitive performance.  
 
References  
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a Probabilistic Hierarchical Model for 
Generation. In Proceedings of the 18th Interna-
tional Conference on Computational Linguistics, 
pages 42-48. Saarbr?cken, Germany. 
 Error types Proportion
1 Duplicate dependency relations 60.0% 
2 SBV-ADV 20.3% 
3 ATT-QUN 6.3% 
4 Other 13.4% 
Table 4: Error types in the RPD+DR experiment 
Figure 6: Sub-tree for ??????????? 
??? 
work
???(ATT) 
protect 
??? ??? 
?birds protecting?
??(SBV) 
of 
??? ?? 
future 
815
Aoife Cahill and Josef van Genabith. 2006. Robust 
PCFG-Based Generation Using Automatically Ac-
quired LFG Approximations. In Proceedings of the 
21st International Conference on Computational 
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1033-
1040. Sydney, Australia. 
Aoife Cahill, Martin Forst and Christian Rohrer. 2007. 
Stochastic Realisation Ranking for a Free Word 
Order language. In Proceedings of 11th European 
Workshop on Natural Language Generation, pages 
17-24. Schloss Dagstuhl, Germany. 
John Carroll, Ann Copestake, Dan Flickinger, and 
Victor Poznanski. 1999. An Efficient Chart Gene-
rator for (Semi-)Lexicalist Grammars. In Proceed-
ings of the 7th European Workshop on Natural 
Language Generation, pages 86-95, Toulouse. 
Michael A. Covington. 2001. A Fundamental Algo-
rithm for Dependency Parsing. In Proceedings of 
the 39th Annual ACM Southeast Conference, pages 
95?102. 
Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy 
King, John Maxwell, and Paula Newman. 2007. 
XLE documentation. Palo Alto Research Center, 
CA. 
Katja Filippova and Michael Strube. 2007. Generating 
Constituent Order in German Clauses. In Proceed-
ings of the 45th Annual Meeting of the Association 
of Computational Linguistics, pages 320-327. Pra-
gue, Czech Republic. 
Yuqing Guo, Haifeng Wang and Josef van Genabith. 
2008. Dependency-Based N-Gram Models for 
General Purpose Sentence Realisation. In Proceed-
ings of the 22th International Conference on Com-
putational Linguistics, pages 297-304. Manchester, 
UK. 
Deirdre Hogan, Conor Cafferkey, Aoife Cahill and 
Josef van Genabith. 2007. Exploiting Multi-Word 
Units in History-Based Probabilistic Generation. In 
Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing 
and CoNLL, pages 267-276. Prague, Czech Repub-
lic. 
Mel'?uk Igor. 1988. Dependency syntax: Theory and 
practice. In Suny Series in Linguistics. State Uni-
versity of New York Press, New York, USA. 
Irene Langkilde. 2000. Forest-Based Statistical Sen-
tence Generation. In Proceedings of 1st Meeting of 
the North American Chapter of the Association for 
Computational Linguistics, pages 170-177. Seattle, 
WA. 
Irene Langkilde. 2002. An Empirical Verification of 
Coverage and Correctness for a General-Purpose 
Sentence Generator. In Proceedings of the Second 
International Conference on Natural Language 
Generation, pages 17-24. New York, USA. 
Ting Liu, Jinshan Ma, and Sheng Li. 2006a. Building 
a Dependency Treebank for Improving Chinese 
Parser. Journal of Chinese Language and Compu-
ting, 16(4): 207-224. 
Ting Liu, Jinshan Ma, Huijia Zhu, and Sheng Li. 
2006b. Dependency Parsing Based on Dynamic 
Local Optimization. In Proceedings of CoNLL-X, 
pages 211-215, New York, USA. 
Hiroko Nakanishi, Yusuke Miyao and Jun?ichi Tsujii. 
2005. Probabilistic Models for Disambiguation of 
an HPSG-Based Chart Generator. In Proceedings 
of the 9th International Workshop on Parsing 
Technology, pages 93-102. Vancouver, British Co-
lumbia. 
Franz Josef Och. 2003. Minimum Error Rate Training 
in Statistical Machine Translation. In Proceedings 
of the 41st Annual Meeting of the Association for 
Computational Linguistics, pages 160-167, Sappo-
ro, Japan. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. In Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 311-
318. Philadelphia, PA. 
Adwait Ratnaparkhi. 2000. Trainable Methods for 
Natural Language Generation. In Proceedings of 
North American Chapter of the Association for 
Computational Linguistics, pages 194-201. Seattle, 
WA. 
Erik Velldal and Stephan Oepen. 2005. Maximum 
Entropy Models for Realization Ranking. In Pro-
ceedings of the 10th Machine Translation Summit, 
pages 109-116. Phuket, Thailand,  
Stephen Wan, Mark Dras, Robert Dale, C?cile Paris. 
2009. Improving Grammaticality in Statistical Sen-
tence Generation: Introducing a Dependency Span-
ning Tree Algorithm with an Argument Satisfac-
tion Model. In Proceedings of the 12th Conference 
of the European Chapter of the ACL, pages 852-
860. Athens, Greece. 
Michael White. 2004. Reining in CCG Chart Realiza-
tion. In Proceedings of the third International Nat-
ural Language Generation Conference, pages 182-
191. Hampshire, UK. 
Michael White, Rajakrishnan Rajkumar and Scott 
Martin. 2007. Towards Broad Coverage Surface 
Realization with CCG. In Proceedings of the Ma-
chine Translation Summit XI Workshop, pages 22-
30. Copenhagen, Danmark. 
816
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1317?1325,
Beijing, August 2010
Paraphrasing with Search Engine Query Logs
Shiqi Zhao??, Haifeng Wang?, and Ting Liu?
?Baidu Inc.
?HIT Center for Information Retrieval, Harbin Institute of Technology
{zhaoshiqi, wanghaifeng}@baidu.com, tliu@ir.hit.edu.cn
Abstract
This paper proposes a method that extracts
paraphrases from search engine query
logs. The method first extracts paraphrase
query-title pairs based on an assumption
that a search query and its correspond-
ing clicked document titles may mean the
same thing. It then extracts paraphrase
query-query and title-title pairs from the
query-title paraphrases with a pivot ap-
proach. Paraphrases extracted in each step
are validated with a binary classifier. We
evaluate the method using a query log
from Baidu1, a Chinese search engine.
Experimental results show that the pro-
posed method is effective, which extracts
more than 3.5 million pairs of paraphrases
with a precision of over 70%. The results
also show that the extracted paraphrases
can be used to generate high-quality para-
phrase patterns.
1 Introduction
The use of paraphrases is ubiquitous in hu-
man languages, which also presents a challenge
for natural language processing (NLP). Previous
studies have shown that paraphrasing can play im-
portant roles in plenty of areas, such as machine
translation (MT) (Callison-Burch et al, 2006;
Kauchak and Barzilay, 2006), question answer-
ing (QA) (Duboue and Chu-Carroll, 2006; Riezler
et al, 2007), natural language generation (NLG)
(Iordanskaja et al, 1991), and so on. As a result,
the research on paraphrasing and its applications
have attracted significant interest.
1www.baidu.com
This paper proposes a method that uses search
engine query logs for extracting paraphrases,
which is illustrated in Figure 1. Specifically, three
kinds of paraphrases can be extracted with our
method, which include (1) query-title (Q-T): a
query and a document title that users clicked on;
(2) query-query (Q-Q): two queries, for which
users clicked on the same document title; (3) title-
title (T-T): two titles that users clicked on for the
same query. We train a classifier for each kind to
filter incorrect pairs and refine the paraphrases.
Extracting paraphrases using query logs has
many advantages. First, query logs keep growing,
which have no scale limitation. Second, query
logs reflect web users? real needs, hence the ex-
tracted paraphrases may be more useful than that
from other kinds of corpora. Third, paraphrases
extracted from query logs can be directed applied
in search engines for query suggestion and doc-
ument reranking. In addition, we find that both
queries and titles contain a good many question
sentences, which can be useful in developing QA
systems.
We conduct experiments using a query log of
a commercial Chinese search engine Baidu, from
which we extracted about 2.7 million pairs of
paraphrase Q-T, 0.4 million pairs of paraphrase Q-
Q, and 0.4 million pairs of paraphrase T-T. The
precision of the paraphrases is above 70%. In
addition, we generate paraphrase patterns using
the extracted paraphrases. The results show that
73,484 pairs of paraphrase patterns have been gen-
erated, with a precision of over 78%.
In the rest of the paper, we first review related
work in Section 2. Section 3 describes our method
in detail. Section 4 presents the evaluation and re-
1317
paraphrase Q-T extraction
query title both query and title
paraphrase Q-Q extraction paraphrase T-T extraction
paraphrase relation
Figure 1: Illustration of the proposed method.
sults. Section 5 concludes the paper and discusses
future directions.
2 Related Work
In this section, we briefly review previous studies
on paraphrase extraction and query log mining in
information retrieval (IR).
2.1 Paraphrase Extraction
A variety of data resources have been exploited
for paraphrase extraction. For example, some re-
searchers extract paraphrases from multiple trans-
lations of the same foreign novel (Barzilay and
McKeown, 2001; Ibrahim et al, 2003), while
some others make use of comparable news arti-
cles that report on the same event within a small
time interval (Shinyama et al, 2002; Barzilay and
Lee, 2003; Dolan et al, 2004). Besides the mono-
lingual corpora, bilingual parallel corpora have
also been used for extracting paraphrases (Ban-
nard and Callison-Burch, 2005; Callison-Burch,
2008; Zhao et al, 2008). Their basic assumption
is that phrases that align with the same foreign
phrase may have the same meaning.
The above methods have achieved promising
results. However, their performances are usually
constrained due to the scale and domain limita-
tion. As an alternative, researchers have tried
to acquire paraphrases from large-scale web cor-
pora (Lin and Pantel, 2001; Pas?ca and Dienes,
2005; Bhagat and Ravichandran, 2008) or directly
based on web mining (Ravichandran and Hovy,
2002). These methods are guided by an extended
version of distributional hypothesis, namely, if
two phrases often occur in similar contexts, their
meanings tend to be similar. The disadvantage
of these methods is that the underlying assump-
tion does not always hold. Phrases with opposite
meanings can also occur in similar contexts, such
as ?X solves Y? and ?X worsens Y? (Lin and Pan-
tel, 2001). In addition, the extracted paraphrases
are generally short fragments with two slots (vari-
ables) at both ends.
2.2 Query Log Mining in IR
Query logs are widely used in the IR commu-
nity, especially for mining similar queries. For ex-
ample, Wen et al (2002) clustered queries based
on user click information. Their basic idea is
that if some queries result in similar user clicks,
the meanings of these queries should be similar.
Such methods have also been investigated in (Gao
et al, 2007) for cross-lingual query suggestion
and (Zhao et al, 2007) for synonymous questions
identification. This paper is partly inspired by
their studies. However, we do not simply use click
information as clues for mining similar queries.
Instead, we mine paraphrases across queries and
clicked document titles.
In addition, query logs can be used for query
expansion. For instance, Cui et al (2002)
extract probabilistic correlations between query
terms and document terms by analyzing query
logs, which are then used to select high-quality
1318
H1: If a query q hits a title t, then q and
t are likely to be paraphrases.
H2: If queries q1 and q2 hit the same title t,
q1 and q2 are likely to be paraphrases.
H3: If a query q hits titles t1 and t2, then
t1 and t2 are likely to be paraphrases.
Table 1: Hypotheses for extracting paraphrases.
expansion terms for new queries. Note that the
expansion terms are merely related terms of the
queries, not necessarily paraphrases.
There are other studies that use query logs
for constructing ontologies (Sekine and Suzuki,
2007), learning named entities (Pas?ca, 2007),
building user profiles (Richardson, 2008), correct-
ing spelling errors (Ahmad and Kondrak, 2005),
and so forth.
3 The Proposed Method
3.1 Basic Idea
Nowadays, more and more users tend to search
long queries with search engines. Many users
even directly search questions to get exact an-
swers. By analyzing our query log that records
rich information including user queries, clicked
urls, titles, etc., we find that most titles of clicked
documents are highly related with search queries.
Especially, paraphrases can be easily found from
long queries and the corresponding clicked ti-
tles. This motivates us to extract paraphrases from
query-title pairs. Here we introduce a concept hit
that will be frequently used: given a query q, a
web document d, and d?s title t, if there exist some
users that click on d when searching q, then we
say q hits t.
The hypothesis for extracting paraphrase Q-T
is shown in Table 1 (H1). In addition, we find
that when several queries hit the same title, the
queries are likely to be paraphrases of each other.
The other way round, when a query hits several
titles, paraphrases can also be found among the ti-
tles. We therefore further extract paraphrase Q-Q
and T-T from the paraphrase Q-T. The underly-
ing hypotheses can be found in Table 1 (H2 and
INPUT: Q: query space, T : title space
OUTPUT: Pqt: the set of paraphrase Q-T,
Pqq: the set of paraphrase Q-Q,
Ptt: the set of paraphrase T-T,
ParaSet: the set of paraphrases
1. FOR any q ? Q and t ? T
2. IF q hits t
3. IF IsParaphrase(q, t)
4. Add ?q, t? to Pqt
5. END IF
6. END IF
7. END FOR
8. FOR any q1, q2 ? Q and t ? T
9. IF ?q1, t? ? Pqt and ?q2, t? ? Pqt
10. IF IsParaphrase(q1, q2)
11. Add ?q1, q2? to Pqq
12. END IF
13. END IF
14. END FOR
15. FOR any t1, t2 ? T and q ? Q
16. IF ?q, t1? ? Pqt and ?q, t2? ? Pqt
17. IF IsParaphrase(t1, t2)
18. Add ?t1, t2? to Ptt
19. END IF
20. END IF
21. END FOR
22. RETURN ParaSet = Pqt ? Pqq ? Ptt
Table 2: Algorithm for extracting paraphrases.
H3). Note that, based on H2 and H3, paraphrase
Q-Q and T-T can be directly extracted from raw
Q-T pairs. However, in consideration of preci-
sion, we extract them from paraphrase Q-T. We
call our paraphrase Q-Q and T-T extraction ap-
proach as a pivot approach, since we use titles as
pivots (queries as targets) when extracting para-
phrase Q-Q and use queries as pivots (titles as tar-
gets) when extracting paraphrase T-T.
3.2 Algorithm
Our paraphrase extraction algorithm is shown in
Table 2. In particular, lines 1?7 extract para-
1319
phrase Q-T from the query log. Lines 8?14 and
15?21 extract paraphrase Q-Q and T-T, respec-
tively. Line 22 combines the paraphrase Q-T, Q-
Q, and T-T together. To filter noise, the extracted
Q-T, Q-Q, and T-T pairs are all validated using
a function IsParaphrase(s1, s2). In this work,
we recast paraphrase validation as a binary clas-
sification problem. Any pair of ?s1, s2? is classi-
fied as 1 (paraphrase) or 0 (non-paraphrase) with
a support vector machine (SVM) classifier. The
features used for classification will be detailed in
Section 3.3.
In practice, we exploit a query log that contains
287 million Q-T pairs, which are then filtered us-
ing the following constraints: (1) exclude Q-T
pairs that are too short, i.e., either query q or tittle
t contains less than three terms; (2) exclude Q-T
pairs where q subsumes t or vice versa, e.g., ??
? (beef)? and ?????? (cooking method of
beef)?; (3) exclude Q-T pairs in which the similar-
ity between q and t is below a predefined threshold
T 2; (4) exclude Q-T pairs whose t contains fre-
quent internet terms, such as ??? (home page)?,
??? (web site)?, ??? (online)?, since such ti-
tles are mostly organization home pages, online
videos, downloadable resources, etc., which are
useless for our purpose of paraphrase extraction.
3.3 Features for Paraphrase Validation
Given a pair of candidate paraphrases ?s1, s2?, in
which s1 and s2 can be either a query or a title, we
exploit the following features in the classification-
based paraphrase validation.
? Frequency Feature FF . FF is defined based
on each ?s1, s2??s frequency. We expect that more
frequent ?s1, s2? should be more reliable.
FF (s1, s2) = {
c(s1,s2)
C if c(s1, s2) < C
1 if c(s1, s2) ? C
(1)
where c(s1, s2) denotes the number of times that
the ?s1, s2? pair occurs in the corpus. C is a nor-
malizing factor (C = 10 in our experiments).
2The similarity is computed based on word overlap rate,
which will be described in detail in section 3.3. We set T =
0.6 in the experiments.
? Length Rate Feature FLR:
FLR(s1, s2) =
min{cw(s1), cw(s2)}
max{cw(s1), cw(s2)}
(2)
where cw(s) denotes the number of words in s.
? Word Overlap Rate Feature FWOR:
FWOR(s1, s2) =
cw(s1 ? s2)
max{cw(s1), cw(s2)}
(3)
where ?s1 ? s2? is the intersection of s1 and s2.
? Character Overlap Rate Feature FCOR. Chi-
nese words are composed of characters. It is quite
often that words with similar characters share
similar meanings, such as ??? (comfortable)?
and ??? (comfortable)?, ??? (sell)? and ??
? (sell)?. Here we use FCOR to measure the sim-
ilarity between s1 and s2 at the character level.
Detailedly, we segment s1 and s2 into sets of
characters and compute the overlap rate based on
Equation (3)3.
? Cosine Similarity Feature FCS . In FCS , both
s1 and s2 are represented as vectors and their co-
sine similarity is computed as:
FCS(s1, s2) =
vecw(s1) ? vecw(s2)
?vecw(s1)? ? ?vecw(s2)?
(4)
where vecw(s) is the vector of words in s, ??? de-
notes the dot product of two vectors, ?vecw(s)?
is the norm of a vector. Here, the weight of each
word w in a vector is computed using a heuristic
similar to tf-idf:
W (w) = tf(w)? log( Nc(w) + 0.1) (5)
where tf(w) is the frequency of w in the given s,
c(w) is the number of times that w occurs in the
corpus, N = maxw c(w).
? Edit Distance Feature FED. Let ED(s1, s2)
be the edit distance at the word level between s1
and s2, we compute FED as follows:
FED(s1, s2) = 1?
ED(s1, s2)
max{cw(s1), cw(s2)}
(6)
3In FCOR, cw(s) of Equation (3) denotes the number of
characters in s.
1320
? Named Entity (NE) Similarity Feature FNE .
NE information is critical in paraphrase identifica-
tion (Shinyama et al, 2002). We therefore com-
pute the NE similarity between s1 and s2 and take
it as a feature. We employ a Chinese NE recog-
nition tool that can recognize person names, loca-
tions, organizations, and numerals. The NE simi-
larity is computed as:
FNE(s1, s2) =
cne(s1 ? s2) + 1
max{cne(s1), cne(s2)}+ 1
(7)
where cne(s) denotes the number of NEs in s.
Equation (7) guarantees FNE = 1 if there are no
NEs in either s1 or s2.
? Pivot Fertility Feature FPF : FPF is a fea-
ture specially designed for paraphrase Q-Q and
T-T extraction, which are based on the pivot ap-
proach4. Specifically, we define fertility of a pivot
as the number of targets it corresponds to. Our ob-
servation indicates that the larger the fertility of a
pivot is, the more noisy the targets are. Hence we
define FPF as:
FPF (s1, s2) = maxp
1
f(p) (8)
where s1 = q1, s2 = q2, p = t when classifying
Q-Q, while s1 = t1, s2 = t2, p = q when classi-
fying T-T. f(p) denotes the fertility of the pivot p.
The value is maximized over p if s1 and s2 can be
extracted with multiple pivots.
3.4 Generating Paraphrase Patterns
A key feature of our method is that the extracted
paraphrases are particularly suitable for generat-
ing paraphrase patterns, especially for the hot do-
mains that are frequently searched. For example,
there are quite a few paraphrases concerning the
therapy of various diseases, from which we can
easily induce patterns expressing the meaning of
?How to treat [X] disease?, such as ?[X] ? ?
? ???, ??? ?? [X] ??, and ?[X] ? ?
?? ???. Therefore, in this work, we try to
generate paraphrase patterns using the extracted
paraphrases.
In our preliminary experiments, we only induce
paraphrase patterns from paraphrases that contain
4FPF is not used in paraphrase Q-T validation.
SAME RELA DIFF
percent (%) 55.92 44.08 -
Table 3: Human labeling of candidate Q-T.
no more than 6 words. In addition, only one slot
is allowed in each pair of paraphrase patterns. Let
s1 and s2 be a pair of paraphrases extracted above.
If there exist words w ? s1 and v ? s2 that satisfy
(1) w = v, (2) w and v are not stop words, then
we can induce a pair of paraphrase patterns by re-
placing w in s1 and v in s2 with a slot ?[X]?. It is
obvious that several pairs of paraphrase patterns
may be induced from one pair of paraphrases.
4 Experiments
We experiment with a query log that contains a
total of 284,316,659 queries. Statistics reveal that
170,315,807 queries (59.90%) lead to at least one
user click, each having 1.69 clicks on average. We
extract 287,129,850 raw Q-T pairs using the query
log, from which 4,448,347 pairs of candidate Q-
T are left after filtering as described in Section
3.2. Almost all queries and titles are written in
Chinese, though some of them contain English or
Japanese words. The preprocessing of candidate
Q-T includes Chinese word segmentation (WSeg)
and NE recognition (NER). Our WSeg tool is im-
plemented based on forward maximum matching,
while the NER tool is based on a NE dictionary
mined from the web.
4.1 Evaluation of Candidate Q-T
We first evaluate candidate Q-T without valida-
tion. To this end, we randomly sampled 5000
pairs of candidate Q-T and labeled them manu-
ally. Each pair is labeled into one of the 3 classes:
SAME - q and t have the same meaning; RELA - q
and t have related meanings; DIFF - q and t have
clearly different meanings. The labeling results
are listed in Table 3. We can see that no candidate
Q-T is in the DIFF class. This is not surprising,
since users are unlikely to click on web pages un-
related to their queries.
To gain a better insight into the data, we ana-
lyzed the subtle types of candidate Q-T in both
SAME and RELA classes. In detail, we sampled
1321
1000 pairs of candidate Q-T from the 5000 pairs
labeled above, in which 563 are in the SAME
class, while the other 437 are in the RELA class.
Our analysis suggests that candidate Q-T in the
SAME class can be divided into 4 subtle types:
? Trivial change (12.61%): changes of punctu-
ation or stop words, such as ??? ?? ?
??? and ??????????.
? Word or phrase replacement (68.38%): re-
placements of synonymous words or phrases,
such as ??? ? ? ?? ?? ? (how
mach is ...)? and ??? ? ? ?? ??
??? (what is the price of ...)?.
? Structure change (7.10%): changes of both
words and word orders, such as ?????
? ?? ? ?? (what fruit can I eat on a
diet)? and ?? ?? ?? ?? ?? (what
fruit can help loss weight)?.
? Others (11.90%): candidate Q-T that cannot
be classified into the 3 types above.
The above analysis reveals that more than two
thirds of candidate Q-T in the SAME class are in
the ?word or phrase replacement? type, while the
ones with structure changes are slightly more than
7%. We believe this is mainly because queries
and titles are relatively short and their structures
are simple. Thus structure rewriting can hardly be
conducted. This distribution is in line with that
reported in (Zhao et al, 2008).
As for the RELA class, we find that 42.33% of
such candidate Q-T share a problem of named en-
tity mismatch, such as ??? (US) ?? ??
??? and ??? (China) ?? ?? ?? ?
??. This indicates that the NE similarity feature
is necessary in paraphrase validation.
4.2 Evaluation of Paraphrase Q-T
The candidate Q-T extracted above are classified
with a SVM classifier5 under its default setting.
To evaluate the classifier, we run 5-fold cross val-
idation with the 5000 human annotated data, in
which we use 4000 for training and the rest 1000
for testing in each run. The evaluation criteria are
5We use libsvm-2.82 toolkit, which can be downloaded
from http://www.csie.ntu.edu.tw/ cjlin/libsvm/
precision (P), recall (R), and f-measure (F), which
are defined as follows:
P = ?Sa ? Sm??Sa?
(9)
R = ?Sa ? Sm??Sm?
(10)
F = 2? P ?RP +R (11)
where Sa is the set of paraphrases automatically
recognized with the classifier, Sm is the set of
paraphrases manually annotated. Precision, re-
call, and f-measure are averaged over 5 runs in
the 5-fold cross validation.
Figure 2 (a) shows the classification results
(dark bars). For comparison, we also show the
precision, recall6, and f-measure of the candidate
Q-T (light bars). As can be seen, the precision is
improved from 0.5592 to 0.7444 after classifica-
tion. F-measure is also evidently enhanced. This
result indicates that the classification-based para-
phrase validation is effective. We then use all of
the 5000 annotated data to train a classifier and
classify all the candidate Q-T. Results show that
2,762,291 out of 4,448,347 pairs of candidate Q-
T are classified as paraphrases.
4.3 Evaluation of Paraphrase Q-Q and T-T
From the paraphrase Q-T, we further extracted
934,758 pairs of candidate Q-Q and 438,954 pairs
of candidate T-T (without validation). We ran-
domly sampled 5000 from each for human an-
notation. The results show that the precisions of
candidate Q-Q and T-T are 0.4672 and 0.6860, re-
spectively. As can be seen, the precision of can-
didate Q-Q is much lower than that of candidate
T-T. Our analysis reveals that it is mainly because
candidate Q-Q are more noisy, since user queries
contain quite a lot of spelling mistakes and infor-
mal expressions.
The candidate Q-Q and T-T are also refined
based on classification. We first evaluate the clas-
sification performance using the 5000 human la-
beled data. The experimental setups for Q-Q and
6We assume all possible paraphrases are included in the
candidates, thus its recall is 100%.
1322
(a) Q-T classification
0
0.2
0.4
0.6
0.8
1
1.2
cand. 0.5592 1 0.7173
para. 0.7444 0.8391 0.7887
P R F
(b) Q-Q classification
0
0.2
0.4
0.6
0.8
1
1.2
cand. 0.4672 1 0.6369
para. 0.7345 0.6575 0.6938
P R F
(c) T-T classification
0
0.2
0.4
0.6
0.8
1
1.2
cand. 0.686 1 0.8138
para. 0.7056 0.9776 0.8196
P R F
Figure 2: Classification precision (P), recall (R), and f-measure (F).
T-T classification are the same as that of Q-T clas-
sification, in which we run 5-fold cross validation
with a SVM classifier using its default parameters.
Figure 2 (b) and (c) give the classification results
(dark bars) as well as the precision, recall, and f-
measure of the candidates (light bars).
We can see that the precision of Q-Q is signifi-
cantly enhanced from 0.4672 to 0.7345 after clas-
sification, which suggests that a substantial part
of errors and noise are removed. The increase of
f-measure demonstrates the effectiveness of clas-
sification despite the decrease of recall. Mean-
while, the quality of candidate T-T is not clearly
improved after classification. The reason should
be that the precision of candidate T-T is already
pretty high. We then use all 5000 human labeled
data to train a classifier for Q-Q and T-T respec-
tively and classify all candidate Q-Q and T-T. Re-
sults show that 390,920 pairs of paraphrase Q-Q
and 415,539 pairs of paraphrase T-T are extracted
after classification.
4.4 Evaluation of Paraphrase Patterns
Using the method introduced in Section 3.4, we
have generated 73,484 pairs of paraphrase pat-
terns that appear at least two times in the cor-
pus. We randomly selected 500 pairs and labeled
them manually. The results show that the preci-
sion is 78.4%. Two examples are shown in Ta-
ble 4, in which p1 and p2 are paraphrase patterns.
Some slot fillers are also listed below. We real-
p1 [X]??????
p2 ???? [X]??
(how to open [X] file)
slot 7z; ashx; aspx; bib; cda; cdfs; cmp;
cpi; csf; csv; cur; dat; dek...
p1 ?? [X]???
p2 ?? [X]???
(poems about [X])
slot ?? (prairies);?? (Yangtze River);
?? (Mount Tai);?? (nostalgia)...
Table 4: Examples of paraphrase patterns.
ize that the method currently used for inducing
paraphrase patterns is simple. Hence we will im-
prove the method in our following experiments.
Specifically, multiple slots will be allowed in a
pair of patterns. In addition, we will try to ap-
ply the alignment techniques in the generation of
paraphrase patterns, as Zhao et al (2008) did.
4.5 Analysis
Feature Contribution. To investigate the contri-
butions of different features used in classification,
we tried different feature combinations for each of
our three classifiers. The results are shown in Ta-
ble 5, in which ?+? means the feature has contri-
bution to the corresponding classifier. As can be
seen, the character overlap rate feature (FCOR),
cosine similarity feature (FCS), and NE similarity
1323
Feature Q-T Q-Q T-T
FF +
FLR +
FWOR
FCOR + + +
FCS + + +
FED +
FNE + + +
FPF +
Table 5: Feature contribution.
feature (FNE) are the most useful, which play im-
portant roles in all the three classifiers. The other
features are useful in some of the classifiers ex-
cept the word overlap rate feature (FWOR). The
classification results reported in prior sections are
all achieved with the optimal feature combination.
Analysis of the Paraphrases. We combine the
extracted paraphrase Q-T, Q-Q and T-T and get
a total of 3,560,257 pairs of unique paraphrases.
Statistics show that only 8380 pairs (0.24%) are
from more than one source, which indicates that
the intersection among the three sets is very small.
Further statistics show that the average length of
the queries and titles in the paraphrases is 6.69
(words).
To have a detailed analysis of the extracted
paraphrases, we randomly selected 1000 pairs and
manually labeled the precision, types, and do-
mains. It is found that more than 43% of the para-
phrases are paraphrase questions, in which how
(36%), what (19%), and yes/no (14%) questions
are the most common. In addition, we find that
the precision of paraphrase questions (84.26%)
is evidently higher than non-question paraphrases
(65.14%). Those paraphrase questions are useful
in question analysis and expansion in QA, which
can hardly be extracted from other kinds of cor-
pora.
As expected, the paraphrases we extract cover
a variety of domains. However, around 50% of
them are in the 7 most popular domains7, includ-
ing: (1) health and medicine, (2) documentary
download, (3) entertainment, (4) software, (5) ed-
7Note that pornographic queries have been filtered from
the query log beforehand.
ucation and study, (6) computer game, (7) econ-
omy and finance. This analysis reflects what web
users are most concerned about. These domains,
especially (4) and (6), are not well covered by the
parallel and comparable corpora previously used
for paraphrase extraction.
5 Conclusions and Future Directions
In this paper, we put forward a novel method that
extracts paraphrases from search engine query
logs. Our contribution is that we, for the first
time, propose to extract paraphrases from user
queries and the corresponding clicked document
titles. Specifically, three kinds of paraphrases
are extracted, which can be (1) a query and a
hit title, (2) two queries that hit the same title,
and (3) two titles hit by the same query. The
extracted paraphrases are refined based on clas-
sification. Using the proposed method, we ex-
tracted over 3.5 million pairs of paraphrases from
a query log of Baidu. Human evaluation results
show that the precision of the paraphrases is above
70%. The results also show that we can gener-
ate high-quality paraphrase patterns from the ex-
tracted paraphrases.
Our future research will be conducted along the
following directions. Firstly, we will use a much
larger query log for paraphrase extraction, so as to
enhance the coverage of paraphrases. Secondly,
we plan to have a deeper study of the transitivity
of paraphrasing. Simply speaking, we want to find
out whether we can extract ?s1, s3? as paraphrases
given that ?s1, s2? and ?s2, s3? are paraphrases.
6 Acknowledgments
We would like to thank Wanxiang Che, Hua Wu,
and the anonymous reviewers for their useful
comments on this paper.
References
Farooq Ahmad and Grzegorz Kondrak. 2005. Learn-
ing a Spelling Error Model from Search Query
Logs. In Proceedings of HLT/EMNLP, pages 955-
962.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of ACL, pages 597-604.
1324
Regina Barzilay and Lillian Lee. 2003. Learning
to Paraphrase: An Unsupervised Approach Using
Multiple-Sequence Alignment. In Proceedings of
HLT-NAACL, pages 16-23.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting Paraphrases from a Parallel Corpus. In
Proceedings of ACL/EACL, pages 50-57.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
Scale Acquisition of Paraphrases for Learning Sur-
face Patterns. In Proceedings of ACL-08: HLT,
pages 674-682.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of HLT-
NAACL, pages 17-24.
Chris Callison-Burch. 2008. Syntactic Constraints
on Paraphrases Extracted from Parallel Corpora. In
Proceedings of EMNLP, pages 196-205.
Hang Cui, Ji-Rong Wen, Jian-Yun Nie, Wei-Ying Ma.
2002. Probabilistic Query Expansion Using Query
Logs In Proceedings of WWW, pages 325-332.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised Construction of Large Paraphrase
Corpora: Exploiting Massively Parallel News
Sources. In Proceedings of COLING, pages 350-
356.
Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006.
Answering the Question You Wish They Had
Asked: The Impact of Paraphrasing for Question
Answering. In Proceedings of HLT-NAACL, pages
33-36.
Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Jian
Hu, Kam-Fai Wong, and Hsiao-Wuen Hon. 2007.
Cross-Lingual Query Suggestion Using Query Logs
of Different Languages. In Proceedings of SIGIR,
pages 463-470.
Ali Ibrahim, Boris Katz, Jimmy Lin. 2003. Extract-
ing Structural Paraphrases from Aligned Monolin-
gual Corpora. In Proceedings of IWP, pages 57-64.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgue`re. 1991. Lexical Selection and Paraphrase
in a Meaning-Text Generation Model. In Ce?cile L.
Paris, William R. Swartout, and William C. Mann
(Eds.): Natural Language Generation in Artificial
Intelligence and Computational Linguistics, pages
293-312.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for Automatic Evaluation. In Proceedings
of HLT-NAACL, pages 455-462.
De-Kang Lin and Patrick Pantel. 2001. Discovery of
Inference Rules for Question Answering. In Natu-
ral Language Engineering 7(4): 343-360.
Marius Pas?ca and Pe?ter Dienes. 2005. Aligning Nee-
dles in a Haystack: Paraphrase Acquisition Across
the Web. In Proceedings of IJCNLP, pages 119-
130.
Marius Pas?ca. 2007. Weakly-supervised Discovery
of Named Entities using Web Search Queries. In
Proceedings of CIKM, pages 683-690.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing Surface Text Patterns for a Question Answering
System. In Proceedings of ACL, pages 41-47.
Matthew Richardson. 2008. Learning about the World
through Long-Term Query Logs. In ACM Transac-
tions on the Web 2(4): 1-27.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal and Yi Liu. 2007.
Statistical Machine Translation for Query Expan-
sion in Answer Retrieval. In Proceedings of ACL,
pages 464-471.
Satoshi Sekine and Hisami Suzuki. 2007. Acquiring
Ontological Knowledge from Query Logs. In Pro-
ceedings of WWW, pages 1223-1224.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic Paraphrase Acquisition from
News Articles. In Proceedings of HLT, pages 40-
46.
Ji-Rong Wen, Jian-Yun Nie, and Hong-Jiang Zhang.
2002. Query Clustering Using User Logs. In ACM
Transactions on Information Systems 20(1): 59-81,
2002.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. In Proceedings of
ACL-08:HLT, pages 780-788.
Shiqi Zhao, Ming Zhou, and Ting Liu. 2007. Learning
Question Paraphrases for QA from Encarta Logs. In
Proceedings of IJCAI, pages 1795-1800.
1325
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1326?1334,
Beijing, August 2010
Leveraging Multiple MT Engines for Paraphrase Generation
Shiqi Zhao??, Haifeng Wang?, Xiang Lan?, and Ting Liu?
?Baidu Inc.
?HIT Center for Information Retrieval, Harbin Institute of Technology
{zhaoshiqi, wanghaifeng}@baidu.com,
{xlan, tliu}@ir.hit.edu.cn
Abstract
This paper proposes a method that lever-
ages multiple machine translation (MT)
engines for paraphrase generation (PG).
The method includes two stages. Firstly,
we use a multi-pivot approach to acquire
a set of candidate paraphrases for a source
sentence S. Then, we employ two kinds
of techniques, namely the selection-based
technique and the decoding-based tech-
nique, to produce a best paraphrase T for
S using the candidates acquired in the first
stage. Experimental results show that:
(1) The multi-pivot approach is effective
for obtaining plenty of valuable candi-
date paraphrases. (2) Both the selection-
based and decoding-based techniques can
make good use of the candidates and pro-
duce high-quality paraphrases. Moreover,
these two techniques are complementary.
(3) The proposed method outperforms a
state-of-the-art paraphrase generation ap-
proach.
1 Introduction
This paper addresses the problem of paraphrase
generation (PG), which seeks to generate para-
phrases for sentences. PG is important in many
natural language processing (NLP) applications.
For example, in machine translation (MT), a
sentence can be paraphrased so as to make it
more translatable (Zhang and Yamamoto, 2002;
Callison-Burch et al, 2006). In question answer-
ing (QA), a question can be paraphrased to im-
prove the coverage of answer extraction (Duboue
and Chu-Carroll, 2006; Riezler et al, 2007). In
natural language generation (NLG), paraphrasing
can help to increase the expressive power of the
NLG systems (Iordanskaja et al, 1991).
In this paper, we propose a novel PG method.
For an English sentence S, the method first ac-
quires a set of candidate paraphrases with a multi-
pivot approach, which uses MT engines to auto-
matically translate S into multiple pivot languages
and then translate them back into English. Fur-
thermore, the method employs two kinds of tech-
niques to produce a best paraphrase T for S us-
ing the candidates, i.e., the selection-based and
decoding-based techniques. The former selects
a best paraphrase from the candidates based on
Minimum Bayes Risk (MBR), while the latter
trains a MT model using the candidates and gen-
erates paraphrases with a MT decoder.
We evaluate our method on a set of 1182 En-
glish sentences. The results show that: (1) al-
though the candidate paraphrases acquired by MT
engines are noisy, they provide good raw ma-
terials for further paraphrase generation; (2) the
selection-based technique is effective, which re-
sults in the best performance; (3) the decoding-
based technique is promising, which can generate
paraphrases that are different from the candidates;
(4) both the selection-based and decoding-based
techniques outperform a state-of-the-art approach
SPG (Zhao et al, 2009).
2 Related Work
2.1 Methods for Paraphrase Generation
MT-based method is the mainstream method on
PG. It regards PG as a monolingual machine trans-
lation problem, i.e., ?translating? a sentence S
into another sentence T in the same language.
1326
Quirk et al (2004) first presented the MT-based
method. They trained a statistical MT (SMT)
model on a monolingual parallel corpus extracted
from comparable news articles and applied the
model to generate paraphrases. Their work shows
that SMT techniques can be extended to PG. How-
ever, its usefulness is limited by the scarcity of
monolingual parallel data.
To overcome the data sparseness problem, Zhao
et al (2008a) improved the MT-based PG method
by training the paraphrase model using multi-
ple resources, including monolingual parallel cor-
pora, monolingual comparable corpora, bilingual
parallel corpora, etc. Their results show that bilin-
gual parallel corpora are the most useful among
the exploited resources. Zhao et al (2009) further
improved the method by introducing a usability
sub-model into the paraphrase model so as to gen-
erate varied paraphrases for different applications.
The main disadvantage of the MT-based
method is that its performance heavily depends on
the fine-grained paraphrases, such as paraphrase
phrases and patterns, which provide paraphrase
options in decoding. Hence one has to first ex-
tract fine-grained paraphrases from various cor-
pora with different methods (Zhao et al, 2008a;
Zhao et al, 2009), which is difficult and time-
consuming.
In addition to the MT-based method, re-
searchers have also investigated other methods for
paraphrase generation, such as the pattern-based
methods (Barzilay and Lee, 2003; Pang et al,
2003), thesaurus-based methods (Bolshakov and
Gelbukh, 2004; Kauchak and Barzilay, 2006),
and NLG-based methods (Kozlowski et al, 2003;
Power and Scott, 2005).
2.2 Pivot Approach for Paraphrasing
Bannard and Callison-Burch (2005) introduced
the pivot approach to extracting paraphrase
phrases from bilingual parallel corpora. Their ba-
sic assumption is that two English phrases aligned
with the same phrase in a foreign language (also
called a pivot language) are potential paraphrases.
Zhao et al (2008b) extended the approach and
used it to extract paraphrase patterns. Both of the
above works have proved the effectiveness of the
pivot approach in paraphrase extraction.
Pivot approach can also be used in paraphrase
generation. It generates paraphrases by translating
sentences from a source language to one (single-
pivot) or more (multi-pivot) pivot languages and
then translating them back to the source language.
Duboue et al (2006) first proposed the multi-
pivot approach for paraphrase generation, which
was specially designed for question expansion in
QA. In addition, Max (2009) presented a single-
pivot approach for generating sub-sentential para-
phrases. A clear difference between our method
and the above works is that we propose selection-
based and decoding-based techniques to gener-
ate high-quality paraphrases using the candidates
yielded from the pivot approach.
3 Multi-pivot Approach for Acquiring
Candidate Paraphrases
A single-pivot PG approach paraphrases a sen-
tence S by translating it into a pivot language
PL with a MT engine MT1 and then translat-
ing it back into the source language with MT2.
In this paper, a single-pivot PG system is repre-
sented as a triple (MT1, PL, MT2). A multi-
pivot PG system is made up of a set of single-pivot
systems with various pivot languages and MT en-
gines. Given m pivot languages and n MT en-
gines, we can build a multi-pivot PG system con-
sisting of N (N ? n ? m ? n) single-pivot ones,
where N = n ? m ? n iff all the n MT engines
can perform bidirectional translation between the
source and each pivot language.
In this work, we experiment with 6 pivot lan-
guages (Table 1) and 3 MT engines (Table 2) in
the multi-pivot approach. All the 3 MT engines
are off-the-shelf systems, in which Google and
Microsoft translators are SMT engines, while Sys-
tran translator is a rule-based MT engine. Each
MT engine can translate English to all the 6 pivot
languages and back to English. We thereby con-
struct a multi-pivot PG system consisting of 54
(3*6*3) single-pivot systems.
The advantages of the multi-pivot PG approach
lie in two aspects. First, it effectively makes use
of the vast bilingual data and translation rules un-
derlying the MT engines. Second, the approach is
simple, which just sends sentences to the online
MT engines and gets the translations back.
1327
Source Sentence he said there will be major cuts in the salaries of high-level civil servants .
(GG, G, MS) he said there are significant cuts in the salaries of high-level officials .
(GG, F , GG) he said there will be significant cuts in the salaries of top civil level .
(MS, C, MS) he said that there will be a major senior civil service pay cut .
(MS, F , ST ) he said there will be great cuts in the wages of the high level civils servant .
(ST , G, GG) he said that there are major cuts in the salaries of senior government officials .
Table 3: Examples of candidate paraphrases obtained using the multi-pivot approach.
1 French (F) 4 Italian (I)
2 German (G) 5 Portuguese (P)
3 Spanish (S) 6 Chinese (C)
Table 1: Pivot languages used in the approach.
1 Google Translate (GG)
(translate.google.com)
2 Microsoft Translator (MS)
(www.microsofttranslator.com)
3 Systran Online Translation (ST)
(www.systransoft.com)
Table 2: MT engines utilized in the approach.
4 Producing High-quality Paraphrases
using the Candidates
Table 3 shows some examples of candidate para-
phrases for a sentence. As can be seen, the can-
didates do provide some correct and useful para-
phrase substitutes (in bold) for the source sen-
tence. However, they also contain quite a few er-
rors (in italic) due to the limited translation qual-
ity of the MT engines. The problem is even
worse when the source sentences get longer and
more complicated. Therefore, we need to com-
bine the outputs of the multiple single-pivot PG
systems and produce high-quality paraphrases out
of them. To this end, we investigate two tech-
niques, namely, the selection-based and decoding-
based techniques.
4.1 Selection-based Technique
Given a source sentence S along with a set D of
candidate paraphrases {T1, T2, ..., Ti, ...TN}, the
goal of the selection-based technique is to select
the best paraphrase T?i for S from D. The para-
phrase selection technique we propose is based on
Minimum Bayes Risk (MBR). In detail, the MBR
based technique first measures the quality of each
candidate paraphrase Ti ? D in terms of Bayes
risk (BR), and then selects the one with the min-
imum BR as the best paraphrase. In detail, given
S, a candidate Ti ? D, a reference paraphrase
T 1, and a loss function L(T, Ti) that measures the
quality of Ti relative to T , we define the Bayes
risk as follows:
BR(Ti) = EP (T,S)[L(T, Ti)], (1)
where the expectation is taken under the true dis-
tribution P (T, S) of the paraphrases. According
to (Bickel and Doksum, 1977), the candidate para-
phrase that minimizes the Bayes risk can be found
as follows:
T?i = arg minTi?D
?
T?T
L(T, Ti)P (T |S), (2)
where T represents the space of reference para-
phrases. In practice, however, the collection of
reference paraphrases is not available. We thus
construct a set D? = D?{S} to approximate T 2.
In addition, we cannot estimate P (T |S) in Equa-
tion (2), either. Therefore, we make a simplifica-
tion by assigning a constant c to P (T |S) for each
T ? D?, which can then be removed:
T?i = arg minTi?D
?
T?D?
L(T, Ti). (3)
Equation (3) can be further rewritten using a gain
function G(T, Ti) instead of the loss function:
1Here we assume that we have the collection of all possi-
ble paraphrases of S, which are used as references.
2The source sentence S is included in D? based on the
consideration that a sentence is allowed to keep unchanged
during paraphrasing.
1328
T?i = arg maxTi?D
?
T?D?
G(T, Ti). (4)
We define the gain function based on BLEU:
G(T, Ti) = BLEU(T, Ti). BLEU is a
widely used metric in the automatic evaluation of
MT (Papineni et al, 2002). It measures the sim-
ilarity of two sentences by counting the overlap-
ping n-grams (n=1,2,3,4 in our experiments):
BLEU(T, Ti) = BP ?exp(
4?
n=1
wn log pn(T, Ti)),
where pn(T, Ti) is the n-gram precision of Ti and
wn = 1/4. BP (? 1) is a brevity penalty that
penalizes Ti if it is shorter than T .
In summary, for each sentence S, the MBR
based technique selects a paraphrase that is the
most similar to all candidates and the source sen-
tence. The underlying assumption is that correct
paraphrase substitutes should be common among
the candidates, while errors committed by the
single-pivot PG systems should be all different.
We denote this approach as S-1 hereafter.
Approaches for comparison. In the experiments,
we also design another two paraphrase selection
approaches S-2 and S-3 for comparison with S-1.
S-2: S-2 selects the best single-pivot PG
system from all the 54 ones. The selection
is also based on MBR and BLEU. For each
single-pivot PG system, we sum up its gain
function values over a set of source sentences
(i.e., ?S
?
TS?D?S G(TS , TSi)). Then we se-lect the one with the maximum gain value as
the best single-pivot system. In our experi-
ments, the selected best single-pivot PG system is
(ST, P,GG), the candidate paraphrases acquired
by which are then returned as the best paraphrases
in S-2.
S-3: S-3 is a simple baseline, which just ran-
domly selects a paraphrase from the 54 candidates
for each source sentence S.
4.2 Decoding-based Technique
The selection-based technique introduced above
has an inherent limitation that it can only select
a paraphrase from the candidates. That is to say, it
major cuts high-level civil servants
significant cuts senior officials
major cuts* high-level officials
important cuts senior civil servants
big cuts
great cuts
Table 4: Extracted phrase pairs. (*This is called
a self-paraphrase of the source phrase, which
is generated when a phrase keeps unchanged in
some of the candidate paraphrases.)
can never produce a perfect paraphrase if all the
candidates have some tiny flaws. To solve this
problem, we propose the decoding-based tech-
nique, which trains a MT model using the can-
didate paraphrases of each source sentence S and
generates a new paraphrase T for S with a MT
decoder.
In this work, we implement the decoding-based
technique using Giza++ (Och and Ney, 2000) and
Moses (Hoang and Koehn, 2008), both of which
are commonly used SMT tools. For a sentence
S, we first construct a set of parallel sentences
by pairing S with each of its candidate para-
phrases: {(S,T1),(S,T2),...,(S,TN )} (N = 54).
We then run word alignment on the set using
Giza++ and extract aligned phrase pairs as de-
scribed in (Koehn, 2004). Here we only keep the
phrase pairs that are aligned ?3 times on the set,
so as to filter errors brought by the noisy sentence
pairs. The extracted phrase pairs are stored in a
phrase table. Table 4 shows some extracted phrase
pairs.
Note that Giza++ is sensitive to the data size.
Hence it is interesting to examine if the alignment
can be improved by augmenting the parallel sen-
tence pairs. To this end, we have tried augmenting
the parallel set for each sentence S by pairing any
two candidate paraphrases. In this manner, C2N
sentence pairs are augmented for each S. We con-
duct word alignment using the (N+C2N ) sentence
pairs and extract aligned phrases from the original
N pairs. However, we have not found clear im-
provement after observing the results. Therefore,
we do not adopt the augmentation strategy in our
experiments.
1329
Using the extracted phrasal paraphrases, we
conduct decoding for the sentence S with Moses,
which is based on a log-linear model. The default
setting of Moses is used, except that the distortion
model for phrase reordering is turned off3. The
language model in Moses is trained using a 9 GB
English corpus. We denote the above approach as
D-1 in what follows.
Approach for comparison. The main advantage
of the decoding-based technique is that it allows
us to customize the paraphrases for different re-
quirements through tailoring the phrase table or
tuning the model parameters. As a case study,
this paper shows how to generate paraphrases with
varied paraphrase rates4.
D-2: The extracted phrasal paraphrases (in-
cluding self-paraphrases) are stored in a phrase ta-
ble, in which each phrase pair has 4 scores mea-
suring their alignment confidence (Koehn et al,
2003). Our basic idea is to control the paraphrase
rate by tuning the scores of the self-paraphrases.
We thus extend D-1 to D-2, which assigns a
weight ? (? > 0) to the scores of the self-
paraphrase pairs. Obviously, if we set ? < 1,
the self-paraphrases will be penalized and the de-
coder will prefer to generate a paraphrase with
more changes. If we set ? > 1, the decoder will
tend to generate a paraphrase that is more similar
to the source sentence. In our experiments, we set
? = 0.1 in D-2.
5 Experimental Setup
Our test sentences are extracted from the paral-
lel reference translations of a Chinese-to-English
MT evaluation5, in which each Chinese sentence
c has 4 English reference translations, namely e1,
e2, e3, and e4. We use e1 as a test sentence to para-
phrase and e2, e3, e4 as human paraphrases of e1
for comparison with the automatically generated
paraphrases. We process the test set by manually
filtering ill-formed sentences, such as the ungram-
matical or incomplete ones. 1182 out of 1357
3We conduct monotone decoding as previous work
(Quirk et al, 2004; Zhao et al, 2008a, Zhao et al, 2009).
4The paraphrase rate reflects how different a paraphrase
is from the source sentence.
52008 NIST Open Machine Translation Evaluation: Chi-
nese to English Task.
Score Adequacy Fluency
5 All Flawless English
4 Most Good English
3 Much Non-native English
2 Little Disfluent English
1 None Incomprehensible
Table 5: Five point scale for human evaluation.
test sentences are retained after filtering. Statistics
show that about half of the test sentences are from
news and the other half are from essays. The aver-
age length of the test sentences is 34.12 (words).
Manual evaluation is used in this work. A para-
phrase T of a sentence S is manually scored based
on a five point scale, which measures both the ?ad-
equacy? (i.e., how much of the meaning of S is
preserved in T ) and ?fluency? of T (See Table 5).
The five point scale used here is similar to that in
the human evaluation of MT (Callison-Burch et
al., 2007). In MT, adequacy and fluency are eval-
uated separately. However, we find that there is a
high correlation between the two aspects, which
makes it difficult to separate them. Thus we com-
bine them in this paper.
We compare our method with a state-of-the-
art approach SPG6 (Zhao et al, 2009), which
is a statistical approach specially designed for
PG. The approach first collects a large volume of
fine-grained paraphrase resources, including para-
phrase phrases, patterns, and collocations, from
various corpora using different methods. Then it
generates paraphrases using these resources with
a statistical model7.
6 Experimental Results
We evaluate six approaches, i.e., S-1, S-2, S-3, D-
1, D-2 and SPG, in the experiments. Each ap-
proach generates a 1-best paraphrase for a test
sentence S. We randomize the order of the 6 para-
phrases of each S to avoid bias of the raters.
6SPG: Statistical Paraphrase Generation.
7We ran SPG under the setting of baseline-2 as described
in (Zhao et al, 2009).
1330
00.5
1
1.52
2.53
3.54
4.5
score 3.92 3.52 2.78 3.62 3.36 3.47
S-1 S-2 S-3 D-1 D-2 SPG
Figure 1: Evaluation results of the approaches.
6.1 Human Evaluation Results
We have 6 raters in the evaluation, all of whom
are postgraduate students. In particular, 3 raters
major in English, while the other 3 major in com-
puter science. Each rater scores the paraphrases
of 1/6 test sentences, whose results are then com-
bined to form the final scoring result. The av-
erage scores of the six approaches are shown in
Figure 1. We can find that among the selection-
based approaches, the performance of S-3 is the
worst, which indicates that randomly selecting a
paraphrase from the candidates works badly. S-
2 performs much better than S-3, suggesting that
the quality of the paraphrases acquired with the
best single-pivot PG system are much higher than
the randomly selected ones. S-1 performs the best
in all the six approaches, which demonstrates the
effectiveness of the MBR-based selection tech-
nique. Additionally, the fact that S-1 evidently
outperforms S-2 suggests that it is necessary to ex-
tend a single-pivot approach to a multi-pivot one.
To get a deeper insight of S-1, we randomly
sample 100 test sentences and manually score all
of their candidates. We find that S-1 successfully
picks out a paraphrase with the highest score for
72 test sentences. We further analyze the remain-
ing 28 sentences for which S-1 fails and find that
the failures are mainly due to the BLEU-based
gain function. For example, S-1 sometimes se-
lects paraphrases that have correct phrases but in-
correct phrase orders, since BLEU is weak in eval-
uating phrase orders and sentence structures. In
the next step we shall improve the gain function
by investigating other features besides BLEU.
In the decoding-based approaches, D-1 ranks
the second in the six approaches only behind S-1.
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
S-1 S-2 S-3 D-1 D-2 SPG
r1 r2 r3 r4 r5 r6
Figure 2: Evaluation results from each rater.
We will further improve D-1 in the future rather
than simply use Moses in decoding with the de-
fault setting. However, the value of D-1 lies in
that it enables us to break down the candidates
and generate new paraphrases flexibly. The per-
formance decreases when we extend D-1 to D-2
to achieve a larger paraphrase rate. This is mainly
because more errors are brought in when more
parts of a sentence are paraphrased.
We can also find from Figure 1 that S-1, S-2,
and D-1 all get higher scores than SPG, which
shows that our method outperforms this state-of-
the-art approach. This is more important if we
consider that our method is lightweight, which
makes no effort to collect fine-grained paraphrase
resources beforehand. After observing the results,
we believe that the outperformance of our method
can be mainly ascribed to the selection-based and
decoding-based techniques, since we avoid many
errors by voting among the candidates. For in-
stance, an ambiguous phrase may be incorrectly
paraphrased by some of the single-pivot PG sys-
tems or the SPG approach. However, our method
may obtain the correct paraphrase through statis-
tics over all candidates and selecting the most
credible one.
The human evaluation of paraphrases is subjec-
tive. Hence it is necessary to examine the coher-
ence among the raters. The scoring results from
the six raters are depicted in Figure 2. As it can be
seen, they show similar trends though the raters
have different degrees of strictness.
1331
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
PR1 0.116 0.138 0.232 0.149 0.206 0.139 0.366 0.379 0.386PR2 0.211 0.272 0.427 0.22 0.3 0.234 0.607 0.602 0.694
S-1 S-2 S-3 D-1 D-2 SPG HP1 HP2 HP3
Figure 3: Paraphrase rates of the approaches.
6.2 Paraphrase Rate
Human evaluation assesses the quality of para-
phrases. However, the paraphrase rates cannot be
reflected. A paraphrase that is totally transformed
from the source sentence and another that is al-
most unchanged may get the same score. There-
fore, we propose two strategies, i.e., PR1 and PR2,
to compute the paraphrase rate:
PR1(T ) = 1? OL(S, T )L(S) ; PR2(T ) =
ED(S, T )
L(S) .
Here, PR1 is defined based on word overlapping
rate, in which OL(S, T ) denotes the number of
overlapping words between a paraphrase T and its
source sentence S, L(S) denotes the number of
words in S. PR2 is defined based on edit distance,
in which ED(S, T ) denotes the edit distance be-
tween T and S. Obviously, PR1 only measures
the percentage of words that are changed from
S to T , whereas PR2 further takes word order
changes into consideration. It should be noted that
PR1 and PR2 not only count the correct changes
between S and T , but also count the incorrect
ones. We compute the paraphrase rate for each
of the six approaches by averaging the paraphrase
rates over the whole test set. The results are shown
in the left part of Figure 3.
On the whole, the paraphrase rates of the ap-
proaches are not high. In particular, we can see
that the paraphrase rate of D-2 is clearly higher
than D-1, which is in line with our intention of de-
signing D-2. We can also see that the paraphrase
rate of S-3 is the highest among the approaches.
We find it is mainly because the paraphrases gen-
erated with S-3 contain quite a lot of errors, which
contribute most of the changes.
7 Analysis
7.1 Effectiveness of the Proposed Method
Our analysis starts from the candidate paraphrases
acquired with the multi-pivot approach. Actu-
ally, the results of S-3 reflect the average qual-
ity of the candidate paraphrases. A score of 2.78
(See Figure 1) indicates that the candidates are
unacceptable according to the human evaluation
metrics. This is in line with our expectation that
the automatically acquired paraphrases through a
two-way translation are noisy. However, the re-
sults of S-1 and D-1 demonstrate that, using the
selection-based and decoding-based techniques,
we can produce paraphrases of good quality. Es-
pecially, S-1 gets a score of nearly 4, which sug-
gests that the paraphrases are pretty good accord-
ing to our metrics. Moreover, our method out-
performs SPG built on pre-extracted fine-grained
paraphrases. It shows that our method makes good
use of the paraphrase knowledge from the large
volume of bilingual data underlying the multiple
MT engines.
7.2 How to Choose Pivot Languages and MT
Engines in the Multi-pivot Approach
In our experiments, besides the six pivot lan-
guages used in the multi-pivot system, we have
also tried another five pivot languages, including
Arabic, Japanese, Korean, Russian, and Dutch.
They are finally abandoned since we find that they
perform badly. Our experience on choosing pivot
languages is that: (1) a pivot language should be
a language whose translation quality can be well
guaranteed by the MT engines; (2) it is better to
choose a pivot language similar to the source lan-
guage (e.g., French - English), which is easier to
translate; (3) the translation quality of a pivot lan-
guage should not vary a lot among the MT en-
gines. On the other hand, it is better to choose
MT engines built on diverse models and corpora,
which can provide different paraphrase options.
We plan to employ a syntax-based MT engine in
our further experiments besides the currently used
phrase-based SMT and rule-based MT engines.
1332
S he said there will be major cuts in the salaries of high-level civil servants .
S-1 he said that there will be significant cuts in the salaries of senior officials .
S-2 he said there will be major cuts in salaries of civil servants high level .
S-3 he said that there will be significant cuts in the salaries of senior officials .
D-1 he said , there will be significant cuts in salaries of senior civil servants .
D-2 he said , there will be significant cuts in salaries of senior officials .
SPG he said that there will be the main cuts in the wages of high-level civil servants .
HP1 he said there will be a big salary cut for high-level government employees .
HP2 he said salaries of senior public servants would be slashed .
HP3 he claimed to implement huge salary cut to senior civil servants .
Table 6: Comparing the automatically generated paraphrases with the human paraphrases.
7.3 Comparing the Selection-based and
Decoding-based Techniques
It is necessary to compare the paraphrases gener-
ated via the selection-based and decoding-based
techniques. As stated above, the selection-based
technique can only select a paraphrase from the
candidates, while the decoding-based technique
can generate a paraphrase different from all can-
didates. In our experiments, we find that for
about 90% test sentences, the paraphrases gener-
ated by the decoding-based approach D-1 are out-
side the candidates. In particular, we compare the
paraphrases generated by S-1 and D-1 and find
that, for about 40% test sentences, S-1 gets higher
scores than D-1, while for another 21% test sen-
tences, D-1 gets higher scores than S-18. This
indicates that the selection-based and decoding-
based techniques are complementary. In addition,
we find examples in which the decoding-based
technique can generate a perfect paraphrase for
the source sentence, even if all the candidate para-
phrases have obvious errors. This also shows that
the decoding-based technique is promising.
7.4 Comparing Automatically Generated
Paraphrases with Human Paraphrases
We also analyze the characteristics of the gener-
ated paraphrases and compare them with the hu-
man paraphrases (i.e., the other 3 reference trans-
lations in the MT evaluation, see Section 5, which
are denoted as HP1, HP2, and HP3). We find that,
compared with the automatically generated para-
phrases, the human paraphrases are more com-
8For the rest 39%, S-1 and D-1 get identical scores.
plicated, which involve not only phrase replace-
ments, but also structure reformulations and even
inferences. Their paraphrase rates are also much
higher, which can be seen in the right part of Fig-
ure 3. We show the automatic and human para-
phrases for the example sentence of this paper in
Table 6. To narrow the gap between the automatic
and human paraphrases, it is necessary to learn
structural paraphrase knowledge from the candi-
dates in the future work.
8 Conclusions and Future Work
We put forward an effective method for para-
phrase generation, which has the following con-
tributions. First, it acquires a rich fund of para-
phrase knowledge through the use of multiple MT
engines and pivot languages. Second, it presents
a MBR-based technique that effectively selects
high-quality paraphrases from the noisy candi-
dates. Third, it proposes a decoding-based tech-
nique, which can generate paraphrases that are
different from the candidates. Experimental re-
sults show that the proposed method outperforms
a state-of-the-art approach SPG.
In the future work, we plan to improve the
selection-based and decoding-based techniques.
We will try some standard system combination
strategies, like confusion networks and consensus
decoding. In addition, we will refine our evalu-
ation metrics. In the current experiments, para-
phrase correctness (adequacy and fluency) and
paraphrase rate are evaluated separately, which
seem to be incompatible. We plan to combine
them together and propose a uniform metric.
1333
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of ACL, pages 597-604.
Regina Barzilay and Lillian Lee. 2003. Learning
to Paraphrase: An Unsupervised Approach Using
Multiple-Sequence Alignment. In Proceedings of
HLT-NAACL, pages 16-23.
Peter J. Bickel and Kjell A. Doksum. 1977. Mathe-
matical Statistics: Basic Ideas and Selected Topics.
Holden-Day Inc., Oakland, CA, USA.
Igor A. Bolshakov and Alexander Gelbukh. 2004.
Synonymous Paraphrasing Using WordNet and In-
ternet. In Proceedings of NLDB, pages 312-323.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz and Josh Schroeder. 2007.
(Meta-) Evaluation of Machine Translation. In Pro-
ceedings of ACL-2007 Workshop on Statistical Ma-
chine Translation, pages 136-158.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of HLT-
NAACL, pages 17-24.
Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006.
Answering the Question You Wish They Had
Asked: The Impact of Paraphrasing for Question
Answering. In Proceedings of HLT-NAACL, pages
33-36.
Hieu Hoang and Philipp Koehn. 2008. Design of the
Moses Decoder for Statistical Machine Translation.
In Proceedings of ACL Workshop on Software en-
gineering, testing, and quality assurance for NLP,
pages 58-65.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgue`re. 1991. Lexical Selection and Paraphrase
in a Meaning-Text Generation Model. In Ce?cile L.
Paris, William R. Swartout, and William C. Mann
(Eds.): Natural Language Generation in Artificial
Intelligence and Computational Linguistics, pages
293-312.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for Automatic Evaluation. In Proceedings
of HLT-NAACL, pages 455-462.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models: User Manual and Description for Ver-
sion 1.2.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT-NAACL, pages 127-133.
Raymond Kozlowski, Kathleen F. McCoy, and K.
Vijay-Shanker. 2003. Generation of single-
sentence paraphrases from predicate/argument
structure using lexico-grammatical resources. In
Proceedings of IWP, pages 1-8.
Aure?lien Max. 2009. Sub-sentential Paraphrasing by
Contextual Pivot Translation. In Proceedings of the
2009 Workshop on Applied Textual Inference, ACL-
IJCNLP 2009, pages 18-26.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of
ACL, pages 440-447.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based Alignment of Multiple Translations:
Extracting Paraphrases and Generating New Sen-
tences. In Proceedings of HLT-NAACL, pages 102-
109.
Kishore Papineni, Salim Roukos, ToddWard, Wei-Jing
Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of
ACL, pages 311-318.
Richard Power and Donia Scott. 2005. Automatic
generation of large-scale paraphrases. In Proceed-
ings of IWP, pages 73-79.
Chris Quirk, Chris Brockett, andWilliamDolan. 2004.
Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of EMNLP, pages 142-
149.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal and Yi Liu. 2007.
Statistical Machine Translation for Query Expan-
sion in Answer Retrieval. In Proceedings of ACL,
pages 464-471.
Yujie Zhang and Kazuhide Yamamoto. 2002. Para-
phrasing of Chinese Utterances. In Proceedings of
COLING, pages 1163-1169.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven Statistical Paraphrase Genera-
tion. In Proceedings of ACL-IJCNLP 2009, pages
834-842.
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and
Sheng Li. 2008a. CombiningMultiple Resources to
Improve SMT-based Paraphrasing Model. In Pro-
ceedings of ACL-08:HLT, pages 1021-1029.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008b. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. In Proceedings of
ACL-08:HLT, pages 780-788.
1334
Coling 2008: Paraphrases and Applications?Tutorial notes, pages 1?87,
Beijing, August 2010
Paraphrases and Applications
Shiqi Zhao
Baidu, Inc.
Haifeng Wang
Baidu, Inc.
Outline
? Part I
? Introduction
?Paraphrase Identification
?Paraphrase Extraction
? Part II
?Paraphrase Generation
?Applications of Paraphrases  
?Evaluation of Paraphrases
?Conclusions and Future work
1
? Paraphrase 
?Noun
Definition
? Alternative expressions of the same meaning
?Verb 
? Generate paraphrases for the input expression
? ?same meaning??
?Quite subjective
?Different degrees of strictness
?Depend on applications
Paraphrase (noun): Alternative expressions of the same meaning
Korean Kim Yuna won gold
with a world-record score in 
women's figure skating at the 
Vancouver Olympics Thursday.
Korean figure skater Kim 
Yuna has won the gold 
medal of women?s figure 
skating at the Winter 
Olympics in Vancouver
Kim Yu-Na (19) is a South 
Korean ice skater who took 
the gold medal at the 
Vancouver Olympics.
Kim Yuna, a South Korean 
figure skater has won the 
gold medal at the on-
going Winter Olympics 
2010.
Yuna Kim of South Korea 
won the women's figure 
skating gold medal at the 
Vancouver Olympics in 
record fashion.
2
Paraphrase (verb): Generate paraphrases for an input S.
Automatic 
S
paraphrase 
generation
T1 T2 T3 T4
Classification of Paraphrases
? According to granularity
?Surface paraphrases 
? Lexical level
? Phrase level
? Sentence level
? Discourse level
?Structural paraphrases
? Pattern level 
? Collocation level
3
Examples
? Lexical paraphrases (generally synonyms)
? solve and resolve
? Paraphrase phrases
? look after and take care of
? Paraphrase sentences
? The table was set up in the carriage shed.
? The table was laid under the cart-shed.
? Paraphrase patterns
[X] considers [Y]?   
? [X] takes [Y] into consideration
? Paraphrase collocations
? (turn on, OBJ, light)
? (switch on, OBJ, light)
? According to paraphrase style
?Trivial change
Classification of Paraphrases
 
?Phrase replacement
?Phrase reordering
?Sentence split & merge
?Complex paraphrases
4
Examples
? Trivial change
? all the members of and all members of 
? Phrase replacement
? He said there will be major cuts in the salaries of high-level civil servants.
? He said there will be major cuts in the salaries of senior officials.
? Phrase reordering
? Last night, I saw Tom in the shopping mall.
? I saw Tom in the shopping mall last night.
? Sentence split & merge   
? He bought a computer, which is very expensive.
? (1) He bought a computer. (2) The computer is very expensive.
? Complex paraphrases
? He said there will be major cuts in the salaries of high-level civil servants.
? He claimed to implement huge salary cut to senior civil servants.
Applications of Paraphrases
? Machine Translation (MT)
? Simplify input sentences
? Summarization 
? Sentence clustering
? Alleviate data sparseness 
? Parameter tuning
? Automatic evaluation
? Question Answering (QA)
? Question reformulation
? Information Extraction (IE)
? IE pattern expansion
? Automatic evaluation
? Natural Language Generation 
(NLG)
? Sentence rewriting 
? Others
? Changing writing style
? Text simplification  
? Information Retrieval (IR)
? Query reformulation 
? Identifying plagiarism 
? Text steganography
? ??
5
Research on Paraphrasing
? Paraphrase identification
?Identify (sentential) paraphrases  
? Paraphrase extraction
?Extract paraphrase instances (different granularities)
? Paraphrase generation
?Generate (sentential) paraphrases
? P h li tiarap rase app ca ons
?Apply paraphrases in other areas
Textual Entailment ? A Similar Direction
? Textual entailment:
?A directional relation between two text fragments      
? T: the entailing text
? H: the entailed hypothesis
?T entails H if, typically, a human reading T would infer 
that H is most likely true.
?Compare entailment with paraphrase
? P h i bidi i l ilarap rase s rect ona  enta ment 
6
Text Entailment ? A Similar Direction
? Recognizing Textual Entailment Track (RTE)
?RTE 1 (2004) to RTE 5 (2009)-    -  
?RTE-6 (2010) is in progress
? Example:
?T: A shootout at the Guadalajara airport in May, 1993, 
killed Cardinal Juan Jesus Posadas Ocampo.
?H: Juan Jesus Posadas Ocampo died in 1993      .
Outline 
? Part I
?Introduction
?Paraphrase Identification
?Paraphrase Extraction
? Part II
?Paraphrase Generation
?Applications of Paraphrases  
?Evaluation of Paraphrases
?Conclusions and Future work
7
Paraphrase Identification
? Specially refers to sentential paraphrase 
identification
?Given any pair of sentences, automatically identifies 
whether these two sentences are paraphrases
? Paraphrase identification is not trivial
Susan often goes to see movies with her boyfriend.       
Susan never goes to see movies with her boyfriend.
He said there will be major cuts in the salaries of high-level civil servants.
He claimed to implement huge salary cut to senior civil servants.
Overview 
? Classification based methods
?Reviewed as a binary classification problem i e     , . ., 
input s1 and s2 to a classifier and output 0/1
?Compute the similarities between s1 and s2 at different  
levels, which are then used as classification features
? Alignment based methods
?Align s1 and s2 first, and score the sentence pair 
based on the alignment results
? Alignment based on ITG
? Alignment based on quasi-synchronous dependency 
grammars 
8
Classification based Methods
? Brockett and Dolan, 2005
?Features:
? String similarity features
?Sentence length, word overlap, edit distance, ?
? Morphological variants
?Word pairs with the same stem
? WordNet lexical mappings
?Synonym pairs / word-hypernym pairs from WordNet
orbit | orbital 
operation | procedure 
? Word association pairs
?Automatically learned synonym pairs
?Classifier
? SVM classifier
vendors | suppliers 
Classification based Methods (cont?)
? Finch et al, 2005
?Using MT evaluation techniques to compute sentence       
similarities, which are then used as classification 
features
? WER, PER, BLEU, NIST
? Feature vector vec(s1, s2)
? vec1(s1, s2): s1 as reference, s2 as MT system output;
? vec2(s1, s2): s2 as reference, s1 as MT system output;
? vec(s1, s2): average of vec1(s1, s2) and vec2(s1, s2): 
?Classifier
? SVM classifier
9
Classification based Methods (cont?)
? Malakasiotis, 2009
?Combining multiple classification features   
? String similarity (various levels)
?Tokens, stems, POS tags, nouns only, verbs only, ?
? Different measures
?Edit distance, Jaro-Winkler distance, Manhattan distance?
? Synonym similarity
?Treat synonyms in two sentences as identical words
? Syntax similarity
?Dependency parsing of two sentences and compute the 
overlap of dependencies
?Classifier
? Maximum Entropy classifier
Alignment based Methods
? Wu, 2005
?Conduct alignment based on Inversion Transduction      
Grammars (ITG)
? Sensitive to the differences in sentence structures
? Without using any thesaurus to deal with lexical variation
?Performance is comparable to the classification 
based methods
Al f ll i i i t t l t il t? so per orms we  n recogn z ng ex ua  en a men
10
Alignment based Methods (cont?)
? Das and Smith, 2009
?Conduct alignment based on Quasi-Synchronous     
Dependency Grammar (QG)
? Alignment between two dependency trees
? Assumption: the dependency trees of two paraphrase 
sentences should be aligned closely
?Why does it work?
About 120 potential jurors were being asked to complete a lengthy questionnaire .
Align words that 
are not identical
?Performs competitively with classification based 
methods
The jurors were taken into the courtroom in groups of 40 and asked to fill out a questionnaire .
A Summary
? Classification based method is still the 
mainstream method since: , 
?Binary classification problem is well defined;
?Classification algorithms and tools are readily 
available;
?It can combine various features in a simple way;
?It achieves state-of-the-art performance.
11
References 
? Brockett and Dolan. 2005. Support Vector Machines for Paraphrase 
Identification and Corpus Construction.
? Fi h t l 2005 U i M hi T l ti E l ti T h i tnc  e  a . . s ng ac ne rans a on va ua on ec n ques o 
Determine Sentence-level Semantic Equivalence.
? Wu. 2005. Recognizing Paraphrases and Textual Entailment using 
Inversion Transduction Grammars.
? Malakasiotis. 2009. Paraphrase Recognition Using Machine Learning to 
Combine Similarity Measures.
? Das and Smith. 2009. Paraphrase Identification as Probabilistic Quasi-
Synchronous Recognition.
Outline 
? Part I
?Introduction
?Paraphrase Identification
?Paraphrase Extraction
? Part II
?Paraphrase Generation
?Applications of Paraphrases  
?Evaluation of Paraphrases
?Conclusions and Future work
12
Corpora Assumption Algorithm
Three Elements for Paraphrase 
Extraction
? thesauri
? monolingual parallel 
corpora
? monolingual compar-
able corpora
? bilingual parallel 
? Different translation 
versions preserve 
the meaning of the
original source
? Comparable news 
articles may contain 
distinct descriptions 
? co-training 
? classification 
? logistic regression
? clustering
? word alignment
???
corpora
? large web corpora
? search engine query
logs
? dictionary glosses
???
of the same facts
? Multiple phrases that 
align with the same 
foreign phrase may 
have the same mean-
ing
? Distributional hypoth-
esis
???
Outline
? Part I
?Introduction
?Paraphrase Identification
?Paraphrase Extraction
? From Thesauri
? From Monolingual Parallel Corpora
? From Monolingual Comparable Corpora
? F Bili l P ll l Crom ngua  ara e  orpora
? From Large Web Corpora
? From Other Resources
13
Method Overview
? Extract words with specific semantic relations as 
paraphrases
?Most common: synonyms
?Other relations: hypernyms, hyponyms?
? Widely used thesauri
?In English
? WordNet
?In other languages
? E.g., HowNet, Tongyici Cilin in Chinese
Pros and Cons
? Pros
?Existing resources 
?High quality
? Thesauri are hand crafted 
? Cons
?Language limitation
? Thesauri are not available in many languages
?Difficult to update
14
Outline 
? Part I
?Introduction
?Paraphrase Identification
?Paraphrase Extraction
? From Thesauri
? From Monolingual Parallel Corpora
? From Monolingual Comparable Corpora
? F Bili l P ll l Crom ngua  ara e  orpora
? From Large Web Corpora
? From Other Resources
Method Overview
? Corpus
?Multiple translations of the same foreign literary work       
? Assumption
?Different translation versions preserve the meaning of 
the original source, but may use different expressions
15
Vingt mille lieues sous les mers
(in French)
Example
20000 Leagues Under the Sea
(different English translation versions)
??
Sentence Alignment and Preprocessing
? Barzilay and McKeown, 2001
?Collected 11 English translations for 5 foreign novels       
? E.g., Madame Bovary, Fairy Tale, Twenty Thousand 
Leagues under the sea?
?Sentence alignment
? A dynamic programming algorithm 
? Produced 44,562 pairs of parallel sentences
? Precision is 94 5%  .
?Other preprocessing 
? POS tagging and chunking
? Phrases are the atomic units in paraphrase extraction
16
Paraphrase Phrase Extraction
? Barzilay and McKeown, 2001 (cont?)
?Extracting paraphrase phrases  
? Assumption: phrases in aligned sentences which appear in 
similar contexts are paraphrases
? Method: co-training
? Iteratively learn contexts and paraphrases
Left context right contextparaphrases
My imagination melted into hazy drowsiness , and I soon fell into an uneasy slumber .
My imagination wandered  into vague unconsciousness , and I soon fell into a deep sleep .
Pros and Cons
? Pros
?Easy to align monolingual parallel sentences     
? Cons
?Domain limitation
? Limited in literary works
?Scale limitation
? Th i f th i l ti l lle s ze o  e corpus s re a ve y sma
?Context dependence
? E.g., ?John said? and ?he said?
17
Outline
? Part I
?Introduction
?Paraphrase Identification
?Paraphrase Extraction
? From Thesauri
? From Monolingual Parallel Corpora
? From Monolingual Comparable Corpora
? F Bili l P ll l Crom ngua  ara e  orpora
? From Large Web Corpora
? From Other Resources
Method Overview
? Corpus
?News articles that report the same event within a brief          
period of time
? Produced by different news agencies
? Assumption
?Comparable news articles may contain distinct 
descriptions of the same facts
18
Example
Comparable documents
d1 d2
Procedure
News 
corpus
Paraphrase
phrases
Paraphrase
patterns
Paraphrase 
generation
Identify
comparable 
documents
Extract
paraphrase
phrases
Extract
paraphrase
patterns
MT-based
paraphrase
generation
model
Extract
parallel
sentences
Comparable 
documents
Parallel
corpus
19
Identify Comparable Documents
? Input
?News articles from different news agencies     
? E.g., CNN, New York Times, Washington Post?
? Processing
?Method-1: Retrieve documents on a given topic or event
? Needs predefined topics or events
?Method-2: Cluster documents
? Content similarity; time interval
? Output
?Corpus of comparable documents
Extract Parallel (Paraphrase) Sentences
? Input
?Corpus of comparable documents   
? Processing
?Sentence clustering
? Method-1: based on an assumption: first sentences of a 
news article usually summarize its content
? Method-2: based on computing the content similarity
? Output
?Corpus of parallel (paraphrase) sentences
20
Extract Paraphrase Patterns
? Using NEs as anchors
? Shinyama et al, 2002
? Basic idea: paraphrase sentences should contain comparable NEs
Comparable NEs
Slots of the same type
paraphrases
Extract Paraphrase Patterns
? Multiple-sequence alignment
? Barzilay and Lee, 2003
backbone slot
21
Pros and Cons
? Pros
?Language independent-
? Comparable news can be found in many languages
? Cons
?Domain-dependent
? Paraphrases are extracted from specific domains or topics
?Sentence clustering
? Either too strict or too loose
Outline 
? Part I
?Introduction
?Paraphrase Identification
?Paraphrase Extraction
? From Thesauri
? From Monolingual Parallel Corpora
? From Monolingual Comparable Corpora
? F Bili l P ll l Crom ngua  ara e  orpora
? From Large Web Corpora
? From Other Resources
22
Method Overview
? Corpus
?A parallel corpus of the source language and a         
foreign language
? Assumption
?Multiple phrases that align with the same foreign 
phrase may have the same meaning
? The method is also termed as ?pivot approach?       
Example 
source language
foreign language
(pivot language) 
Alignment
??
different parts
??
different places
??
????
??
????
ei
ej
cm
cn
??
various locations
??
??
ek
23
A Simple Version
? Takao et al, 2002 
?Basic idea: 
? Generating lexical paraphrases using 2-way dictionaries
? English word e1 can be translated to a Japanese word j with 
an E-J dic. D1, and then j can be translated back to an 
English word e2 with a J-E dictionary D2. e1 and e2 are 
extracted as paraphrases
Extracting Paraphrase Phrases
? Bannard and Callison-Burch, 2005 
?Word alignment and phrase extraction    
?Basic assumption:
? If two English phrases e1 and e2 can be aligned with the 
same foreign phrase f, e1 and e2 are likely to be paraphrases. 
?Paraphrase probability:
2 1
2 2 1? arg max ( | )e ee p e e?= Pivot in a foreign language
2 1
1 2arg max ( | ) ( | )e e
f
p f e p e f
?
= ?
Translation probability 
 
24
?should take the matter into consideration?
??????????
take the matter into consideration
Bannard & Callison-Burch (2005) ?s results:
?must take the matter into account?
??????????
The consideration of this matter will?
????????
take the matter into account
take the matter into consideration
the consideration of this matter
the consideration of this matter
take the matter into account
He?ll take the matter into consideration
????????
We need to consider this matter
??????????
consider this matter
take the matter into consideration
Add Syntactic Constraints
? Callison-Burch, 2008
?Basic idea: 
? Two paraphrase phrases should have the same syntactic 
type.
?Paraphrase probability:
2 2 1 2 1
2 2 1 1
: ( ) ( )
? arg max ( | , ( ))
arg max ( | ( )) ( | ( ))
e e e s e s e
e p e e s e
p f e s e p e f s e
? ? =
=
= ?
given the syntactic type
?Syntactic constraints are also used when substituting 
paraphrases in sentences
2 2 1 2 1
1 1 2 1
: ( ) ( )
, ,
e e e s e s e f? ? =
25
?should take the matter into consideration?
??????????
take the matter into consideration
take the matter into account
Callison-Burch (2008) ?s results:
?must take the matter into account?
??????????
The consideration of this matter will?
????????
    
take the matter into consideration
the consideration of this matter
the consideration of this matter
take the matter into account
He?ll take the matter into consideration
????????
We need to consider this matter
??????????
consider this matter
take the matter into consideration
Learning Paraphrases from Graphs
? Kok and Brockett, 2010 
?Basic idea: 
? Convert aligned phrases into a graph, extract paraphrases 
based on random walks and hitting times
26
?should take the matter into consideration?
??????????
take the matter into consideration
Kok and Brockett (2010) ?s results:
?must take the matter into account?
??????????
The consideration of this matter will?
????????
take the matter into account
consider this matter
take the matter into account
He?ll take the matter into consideration
????????
We need to consider this matter
??????????
consider this matter
take the matter into consideration
Extracting Paraphrase Patterns
? Zhao et al, 2008
?Basic idea: 
? Generate paraphrase patterns that include part-of-speech 
slots.
?Paraphrase probability:
2 1 1 2
1
( | ) exp[ ( , , )]
N
i i
c i
score e e h e e c?
=
=? ?
1 1 2 1
2 1 2 2
3 1 2 1
4 1 2 2
( , , ) ( | )
( , , ) ( | )
( , , ) ( | )
( , , ) ( | )
MLE
MLE
LW
LW
h e e c score c e
h e e c score e c
h e e c score c e
h e e c score e c
=
=
=
=
Based on maximum 
likelihood estimation
Based on lexical weighting
27
take
demand into
take market   demand into       consideration
Inducing English patterns Inducing Chinese patterns
Example
market consideration
take into considerationNN NN
take into considerationNN
?
?? ?? ??
take NN into        consideration
??
?
? NN
consider NN?
?? ? NN
Extract paraphrase patterns
take NN into consideration & consider NN
?should take the matter into consideration?
??????????
take [NN] into consideration
take [NN] into account
Zhao et al(2008) ?s results:
?must take the matter into account?
??????????
The consideration of this matter will?
????????
H ?ll t k th tt i t id ti
take [NN] into consideration
the consideration of [NN]
the consideration of [NN]
take [NN] into account
e  a e e ma er n o cons era on
????????
We need to consider this matter
??????????
consider [NN]
take [NN] into consideration
28
Pros and Cons
? Pros
?The method proves effective hence it?s widely used   ,    
? High precision
? Large scale
? Cons
?Language limitation
? Cannot work where the large-scale bilingual parallel corpora 
are not available
Outline
? Part I
?Introduction
?Paraphrase Identification
?Paraphrase Extraction
? From Thesauri
? From Monolingual Parallel Corpora
? From Monolingual Comparable Corpora
? F Bili l P ll l Crom ngua  ara e  orpora
? From Large Web Corpora
? From Other Resources
29
Method Overview
? Corpus
?Large corpus of web documents    
?Or directly based on web mining
? Assumption
?Distributional hypothesis
? If two words / phrases / patterns often occur in similar 
contexts, their meanings tend to be similar
Example
Shakespeare
Chekhov Merchant of Venice 
War and Peace
X wrote    Y
Maupassant
Hugo Gorky
Tagore
Murakami Tolstoy
Yasunari
Notre Dame de Paris 
   
Romeo and Juliet 
Madame Bovary 
Madame Bovary 
similar similarparaphrases
X is the author of    Y
Shakespeare
Maupassant
Hugo
Gorky
Hemingway
Balzac
Merchant of Venice 
Notre Dame de Paris 
The Old Man and Sea 
Romeo and Juliet 
30
Extracting Lexical Paraphrases (Word Clustering)
? Lin, 1998
?Basic idea 
? Measure words? similarity based on the distributional pattern 
of words
?Corpus
? A (dependency) parsed corpus
?Word similarity
Mutual 
information
1 2
1 2
1 2( , ) ( ) ( )
1 2
1 2( , ) ( ) ( , ) ( )
( ( , , ) ( , , ))
( , )
( , , ) ( , , )
r r
r r
r w T w T w
r w T w r w T w
I w r w I w r w
sim w w
I w r w I w r w
? ?
? ?
+= +
?
? ?
Extracting Syntactic Paraphrase Patterns
? Lin and Pantel, 2001
? Basic idea: extended distributional hypothesis
? Corpus: a large corpus of parsed monolingual sentences
? pattern pairs
X
solves
Y
X
finds
solution
toa
? Pattern similarity
1 2 1 2 1 2( , ) ( , ) ( , )sim p p sim SlotX SlotX sim SlotY SlotY= ?
Y
Similarity of the slot fillers
31
Extracting Surface Paraphrases
? Bhagat and Ravichandran, 2008
?Basic idea is the same as the above work        
?Corpus:
? a large corpus of monolingual sentences without parsing 
?150GB, 25 billion words
?Surface paraphrases
? Pairs of n-grams
E g ?X acquired Y? and ?X completed the acquisition of Y?? . .,        
?Techniques
? Apply locality sensitive hashing (LSH) to speed up the 
computation
Learning Unary Paraphrase Patterns
? Szpector and Dagan, 2008
?Binary paraphrase patterns (most of the previous work)       
? Each pattern has two slots at both ends
?E.g., ?X solves Y? and ?X found a solution to Y?
?Unary paraphrase patterns
? Each pattern has a single slot
?E.g., ?X take a nap? and ?X sleep?
?Method
sleep
kids
in
room
? The same with the above works
?Based on distributional hypothesis
the
X sleep
32
Extracting Paraphrases based on Web Mining
? Ravichandran and Hovy, 2002 
?Basic idea
? Learn paraphrase patterns with search engines
?Corpus
? The whole internet
?Method
? Extract paraphrase patterns for each type, e.g., ?BIRTHDAY?
? Provide hand-crafted seeds, e.g., ?Mozart, 1756? 
? Retrieve sentences containing the seeds from the web with a 
search engine
? Extract patterns, e.g.,
?born in <ANSWER> , <NAME>
?<NAME> was born on <ANSWER> ,
???
Pros and Cons
? Pros
?Language independent 
? Cons
?For methods based on large web corpora
? Computation complexity is high
?Needs to process an extremely large corpus
?Needs to compute pairwise similarity for all candidates
?For methods based on web mining
? Extract paraphrase patterns type by type
? Needs to prepare seeds beforehand
33
Outline
? Part I
?Introduction
?Paraphrase Identification
?Paraphrase Extraction
? From Thesauri
? From Monolingual Parallel Corpora
? From Monolingual Comparable Corpora
? F Bili l P ll l Crom ngua  ara e  orpora
? From Large Web Corpora
? From Other Resources
Paraphrasing with Search Engine Query Logs
? Zhao et al, 2010
?Corpus
? Query logs (queries and titles) of a search engine
?Assumption
? If a query q hits a title t, then q and t are likely to be 
paraphrases
? If queries q1 and q2 hit the same title t, then q1 and q2 are 
likely to be paraphrases
? If a query q hits titles t1 and t2, then t1 and t2 are likely to be 
paraphrases
34
Example
???????
???????
q1
t1
???????q2
Paraphrases:
<q1, t1>
<q1, t2>
< 2 t1>
query-title
???????
??
??
t2
q , 
<q1,q2>
<t1,t2>
query-query
title-title
Method
? Step-1: extracting <q, t> paraphrases
? Extracting candidate <q, t> pairs from query logs
? Paraphrase validation based on binary classification
? Combining multiple features
? Step-2: extracting <q, q> paraphrases
? Extracting candidate <q, q> from <q, t> paraphrases
? Paraphrase validation based on binary classification
? Step-3: extracting <t, t> paraphrases 
? Extracting candidate <t, t> from <q, t> paraphrases
? Paraphrase validation based on binary classification
35
Pros and Cons
? Pros
?No scale limitation  
? Query logs keep growing
? A large volume of paraphrases can be extracted
?Query logs reflect web users? real needs
? Cons
?Query logs data are only available in IR companies
?User queries are noisy
? Spelling mistakes, grammatical errors?
Extracting Paraphrases from Dictionary Glosses
? Corpus
?Glosses of dictionaries  
? Assumption 
?A word and its definition (gloss) in the dictionary have 
the same meaning
36
Example (Encarta Dictionary) 
hurricane
severe storm
high wind
fast and force person or thing
Method 
? Prune and reformulate the definitions
?For a verb v extracts the head of the definition (h)   ,        
and h?s adverb modifier m as v?s paraphrase
? Kaji et al, 2002
?Rule based method for extracting the appropriate part 
from the definition
? Higashinaka and Nagao, 2002
? E g w should not be in def; ignore contents in parentheses. .,          
in def; avoid double negation?
37
Pros and Cons
? Pros
?Explain unfamiliar words with simpler definitions     
? Cons
?Transformation of person, number, tense
president head of company
presidents
heads of company
head of companies
E.g.,
  
heads of companies
References 
? From monolingual parallel corpora
? Barzilay and McKeown. 2001. Extracting Paraphrases from a Parallel 
Corpus.
? From monolingual comparable corpora
? Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo. 2002. Automatic 
Paraphrase Acquisition from News Articles.
? Regina Barzilay and Lillian Lee. 2003. Learning to Paraphrase: An 
Unsupervised Approach Using Multiple-Sequence Alignment.
? Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised 
Construction of Large Paraphrase Corpora: Exploiting Massively       
Parallel News Sources.
38
References (cont?)
? From bilingual parallel corpora
? Takao et al 2002. Comparing and Extracting Paraphrasing Words with 
2-Way Bilingual Dictionaries.
? Bannard and Callison-Burch. 2005. Paraphrasing with Bilingual Parallel 
Corpora.
? Callison-Burch. 2008. Syntactic Constraints on Paraphrases Extracted 
from Parallel Corpora.
? Kok and Brockett. 2010. Hitting the Right Paraphrases in Good Time.
? Zhao et al 2008. Pivot Approach for Extracting Paraphrase Patterns 
from bilingual corpora  .
References (cont?)
? From large web corpora
? Lin. 1998. Automatic Retrieval and Clustering of Similar Words.
? Lin and Pantel. 2001. Discovery of Inference Rules for Question 
Answering.
? Bhagat and Ravichandran. 2008. Large Scale Acquisition of 
Paraphrases for Learning Surface Patterns. 
? Szpector and Dagan. 2008. Learning Entailment Rules for Unary 
Templates. 
? Ravichandran and Hovy. 2002. Learning Surface Text Patterns for a 
Question Answering System  .
39
References (cont?)
? From other resources
? Zhao et al 2010. Paraphrasing with Search Engine Query Logs. 
? Kaji et al 2002. Verb Paraphrase based on Case Frame Alignment. 
? Higashinaka and Nagao. 2002. Interactive Paraphrasing Based on 
Linguistic Annotation.
C ffee B eak!o r
40
Outline
? Part 2
?Paraphrase Generation 
? Rule based Method
? Thesaurus based Method
? NLG based Method
? MT based Method
? Pivot based Method
?Applications of Paraphrases  
?Evaluation of Paraphrases
?Conclusions and Future work
Rule based Method
? Two types:
?Based on hand crafted rules  -  
? Widely used in early studies of paraphrase generation
? McKeown, 1979; Zong et al, 2001; Tetsuro et al, 2001; 
Zhang and Yamamoto, 2002?? 
?Based on automatically extracted rules
? Extract paraphrase patterns from corpora
? Barzilay and Lee 2003 Zhao et al2009  , ,   ., ??
41
Based on Hand-crafted Rules
Sentence analysis
- morphological
- syntactic 
- semantic
- ?
Rule matching 
&
Paraphrase 
generation
S T
Paraphrase 
rule base
Compile 
paraphrase 
rules
? Examples of paraphrase rules
? Change the positions of adverbials
Based on Hand-crafted Rules
    
? He booked a single room in Beijing yesterday. =>
? Yesterday, he booked a single room in Beijing.
? Split a compound sentence into a group of simple sentences
? He booked a single room in Beijing yesterday =>
? He booked a single room in Beijing.
? He booked a single room yesterday.
? He booked a room.
? Rewrite a sentence using hand-crafted patterns
? Can I have a cup of tea? =>
? May I have a cup of tea?
? I would like a cup of tea, please.
? Give me a cup of tea.
42
Based on Automatically Extracted Rules
? Studies on paraphrase patterns extraction has been 
introduced above
? Some of them have tried to apply the extracted 
paraphrase patterns in paraphrase generation
? Complex paraphrase patterns
? Barzilay and Lee, 2003
? E.g., 
? Short and simple paraphrase patterns
? Zhao et al, 2009
? E.g., consider [NN] and take [NN] into consideration
Pros and Cons
? Methods based on hand-crafted rules
? Pros
? Can design paraphrase rules for specific applications and 
requirements
? Cons
? It is time-consuming to construct paraphrase rules
? Problem of rules conflict 
? Coverage of paraphrase rules is limited
? Methods based on automatically extracted rules
P? ros
? Can generate paraphrases with structural changes
? Cons
? Coverage of paraphrase rules is limited
43
References 
? McKeown. 1979. Paraphrasing Using Given and New Information in a 
Question-Answer System.
? Z t l 2001 A h t S k Chi P h i B dong e  a . . pproac  o po en nese arap ras ng ase  on 
Feature Extraction.
? Tetsuro et al. 2001. KURA: A Transfer-Based Lexico-Structural 
Paraphrasing Engine.
? Zhang and Yamamoto. 2002. Paraphrasing of Chinese Utterances.
? Barzilay and Lee. 2003. Learning to Paraphrase - An Unsupervised 
Approach Using Multiple-Sequence Alignment.
? Zhao et al 2009. Application-driven Statistic Paraphrase Generation.       
Outline
? Part 2
?Paraphrase Generation 
? Rule based Method
? Thesaurus based Method
? NLG based Method
? MT based Method
? Pivot based Method 
?Applications of Paraphrases  
?Evaluation of Paraphrases
?Conclusions and Future work
44
Thesaurus based Method
? Also known as lexical substitution
?Substitute words in a sentence with their synonyms        
that fit in the given context
?SemEval-2007: English lexical substitution task
?SemEval-2010: Cross-lingual lexical substitution
?Example:
? There will be major cuts in the salaries of high level civil        -   
servants. 
? There will be major cuts in the wages of high-level civil 
servants.
Thesaurus based Method
? Include two stages
?Stage 1: extract candidate substitutes from-     
predefined inventories.
? E.g., WordNet
?Stage-2: find substitutes that fit in the given context
? Using language model or web data (e.g., Google 5-gram) for 
evaluating the fitness in the context
? Disambiguation may also be useful    
45
Stage-1: Candidate Extraction
? Various thesauri have been tried
?WordNet: 
? the most commonly used
?Others: 
? Encarta, Roget, Oxford American Writer?s Thesaurus?
? Extracting different information as candidates
?Synsets (all synsets vs. best synset)
?Hypernyms, similar-to, also-see?
?Words in glosses
Example:
WordNet 
different
synsets
46
Example:
Encarta
definition of the synset   
synset
Stage-2: Substitute Selection
? Rank the candidates and select the one fits best 
in the given context   
? Context constraints
?Semantic constraints
? Select substitutes with the correct meaning wrt the given 
context
?Syntactic constraints
? The sentence generated after substitution should keep 
grammatical
47
SubFinder: A Lexical Substitution System
? SubFinder
?University of North Texas    
?Performs well in SemEval-2007 English lexical 
substitution task 
? Candidate extraction
?WordNet
?Encarta
?Others
? Prove to be useless 
SubFinder: A Lexical Substitution System
? Substitute selection (5 ranking methods R1~R5)
?Language model 
? Google 1T 5-gram (R1)
? Query search engine (R2)
?Latent semantic analysis (LSA) (R3)
? Rank a candidate by its relatedness to the context sentence
?Word sense disambiguation (WSD) (R4)
? Disambiguate the target word and select the synset of the 
right sense
?Pivot approach (R5)
? Check whether a candidate substitute can be generated via a 
2-way translation
48
SubFinder: A Lexical Substitution System
? Combine R1~R5:
?Voting mechanism 
?Contribution of each ranking method is not analyzed/
1
( )
i
i m m
m rankings c
score c
r
?
?
= ?
Ranks according 
to R1-R5
       
Pros and Cons
? Pros
?Based on existing inventories   
? Cons
?Cannot generate structural paraphrases
?Language limitation
? Question
H t diff t th i?? ow o merge eren  esaur
? Thesauri have different forms of synset clustering 
49
References 
? McCarthy and Navigli. 2007. SemEval-2007 Task 10: English Lexical 
Substitution Task.
? H t l 2007 UNT S bFi d C bi i K l d S fassan e  a . . : u n er: om n ng now e ge ources or 
Automatic Lexical Substitution.
? Yuret. 2007. KU: Word Sense Disambiguation by Substitution.
? Giuliano et al 2007. FBK-irst: Lexical Substitution Task Exploiting Domain 
and Syntagmatic Coherence.
? Martinez et al 2007. MELB-MKB: Lexical Substitution System based on 
Relatives in Context.
? Kauchak and Barzilay. 2006. Paraphrasing for Automatic Evaluation.       
Outline
? Part 2
?Paraphrase Generation 
? Rule based Method
? Thesaurus based Method
? NLG based Method
? MT based Method
? Pivot based Method 
?Applications of Paraphrases  
?Evaluation of Paraphrases
?Conclusions and Future work
50
Overview 
? Two steps
?(1) analysis and (2) generation     
NLU NLG
R
s t
paraphrases
NLG based Methods
? Kozlowski et al, 2003
?Generate single sentence paraphrases -  
?Input: predicate/argument structure
? Not natural language sentences/
?Based on lexico-grammatical resources
? Map elementary semantic structures with syntactic realization
51
NLG based Methods (cont?)
? Power and Scott, 2005
? Concerning larger-scale
Rhetorical 
structure tree 
paraphrases
? Paraphrases of multiple 
sentences or even the 
whole text
? Paraphrases vary not only 
at lexical and syntactic 
levels, but also in 
document structure and
generator
Different realizations
   
layout
? Problem:
? The input is not natural 
language texts/
t1 t2 t3 tn
NLG based Methods (cont?)
? Power and Scott, 2005 (cont?)
?Example: 
reason
NUCLEUS: recommend(doctors, elixir)
SATELLITE: conjunction
1: quick-results(elixir)
2: few-side-effects(elixir)
Rhetorical 
structure tree
Doctors recommend Elixir since it 
gives quick results and it has few 
side effects.
solution1
(1) Elixir gives quick results.
(2) Elixir has few side effects.
(3) Therefore, it is recommended 
by doctors.
solution2
52
NLG based Methods (cont?)
? Fujita et al, 2005
?Paraphrase light verb constructions (LVC) in -    
sentences
? LVC: consists of a light-verb that syntactically governs a 
deverbal noun
?Semantic representation
? LCS: Lexical Conceptual Structure
?Procedure
? Semantic analysis
? Semantic transformation
? Surface generation
Pros and Cons
? Pros 
?It simulates human being?s behavior when generating       
paraphrases:
? Step-1: understand the meaning of a sentence
? Step-2: generate a new sentence expressing the meaning 
? Cons 
?Both deep analysis of sentences and NLG are difficult 
to realize
53
References 
? Kozlowski et al 2003. Generation of single-sentence paraphrases from 
predicate/argument structure using lexico-grammatical resources. 
? P d S tt 2005 A t ti ti f l l hower an  co . . u oma c genera on o  arge-sca e parap rases. 
? Fujita et al 2005. Exploiting Lexical Conceptual Structure for Paraphrase 
Generation.
Outline
? Part 2
?Paraphrase Generation 
? Rule based Method
? Thesaurus based Method
? NLG based Method
? MT based Method
? Pivot based Method 
?Applications of Paraphrases  
?Evaluation of Paraphrases
?Conclusions and Future work
54
Machine Translation vs. Paraphrase Generation
Translations t
Language L1 Language L2
Paraphrasings
Language L1
t
For both machine translation and paraphrase generation:
(1) t should preserve the meaning of s
(2) t should be a fluent sentence 
Paraphrase Generation as Machine Translation
? Quirk et al, 2004
?First recast paraphrase generation as a monolingual       
machine translation task
Paraphrase 
generations t
A typical MT model 
(source channel model)
PT
paraphrase table 
From comparable 
news articles
55
Paraphrase Generation as Machine Translation 
(cont?)
? Model
?Source channel model  
* arg max ( | )
arg max ( | ) ( )
t
t
t p t s
p s t p t
=
=
Language model
?Translation? model
(based on a phrasal 
paraphrase table)
? Paraphrase table
?Monolingual parallel sentences
Paraphrase Generation as Machine Translation 
(cont?)
  
? Extracted from comparable news articles
? 139K pairs
?Word alignment & phrase pair extraction
? With Giza++
? Limitation
?Lack of monolingual parallel corpora to train the 
paraphrase table!!!
56
? Zhao et al, 2008
?Combine multiple resources to train the paraphrase
Paraphrase Generation as Machine Translation 
(cont?)
       
table
Paraphrase 
generations t
Log-linear model
PT1
Multiple paraphrase tables 
PT2 PTn?
From various resources
Paraphrase Generation as Machine Translation 
(cont?)
? Model
?Log linear model-  
_ _
1
* arg max{ ( , ) ( , )}
N
TM i TM i LM LMt
i
t h t s h t s? ?
=
= +?
N paraphrase tables, each feature 
corresponds to a paraphrase table
Language model
_
1
( , ) log ( , )
iK
TM i i k k
k
h t s score t s
=
= ?
57
Paraphrase Generation as Machine Translation 
(cont?)
? Paraphrase tables
? PT1: from word clusters
? Volumes of the PTs:
    
(Lin, 1998)
? PT2: from monolingual 
parallel corpora
? PT3: from monolingual 
comparable corpora
? PT4: from bilingual parallel 
corpora
? PT5: from Encarta 
dictionary glosses
? PT6: from clusters of 
similar user queries
Proves most useful!
? Differences between machine translation and 
paraphrase generation (Zhao et al2009):
Paraphrase Generation vs. Machine Translation
    ., 
MT has a unique purpose PG has distinct purposes in different applications
Machine Translation (MT) Paraphrase Generation (PG)
In MT, all words in a sentence 
h ld b t l t d
In PG, not all words need to be 
h ds ou  e rans a e parap rase
In MT, the bilingual parallel data
are easy to collect
In PG, multiple resources need 
to be combined
In MT, automatic evaluation 
metrics (e.g., BLEU) are available
In PG, automatic evaluation 
metrics are not available
58
Application-driven Statistical Paraphrase 
Generation 
? Zhao et al, 2009
?Propose a statistical model for paraphrase generation      
?Generate different paraphrases in different applications
Paraphrase 
plannings t
Sentence 
preprocessing
Paraphrase 
generation
A The given application
PT1
Multiple paraphrase tables 
PT2 PTn?
Also combine 
multiple resources
Self-paraphrase PT: 
allows words to 
keep unchanged in  
paraphrasing
? Paraphrase planning
?When an application A is given only the paraphrase
Application-driven Statistical Paraphrase 
Generation (cont?)
    ,    
pairs that can achieve A are kept
Paraphrase application: sentence compression
The US government should take the overall situation into consideration and actively promote bilateral high-tech trades.
Example:
The US government
The US administration
The US government on
overall situation 
overall interest
overall picture
overview
situation as a whole
whole situation
??
take [NN_1] into consideration  
consider [NN_1]
take into account [NN_1]
take account of [NN_1]
take [NN_1] into account
take into consideration [NN_1] 
??
<promote, OBJ, trades>  
<sanction, OBJ, trades>
<stimulate, OBJ, trades>
<strengthen, OBJ, trades>
<support, OBJ, trades>
<sustain, OBJ, trades>
59
? Model:
?Log linear model
Application-driven Statistical Paraphrase 
Generation (cont?)
-  
1
2 1
1
( | ) ( log ( , ))
log ( | )
i i
i
K
k k k k
k k
J
lm j j j
j
p s t
p t t t
? ?
?
=
? ?
=
=
+
? ?
?
t s Paraphrase model
Language model
1
( , )
I
um i i
i
s t? ?
=
+ ? Usability model 
(defined for each 
application)
References 
? Lin. 1998. Automatic Retrieval and Clustering of Similar Words.
? Quirk et al 2004. Monolingual Machine Translation for Paraphrase 
G tienera on.
? Finch et al 2004. Paraphrasing as Machine Translation.
? Zhao et al 2008. Combining Multiple Resources to Improve SMT-based 
Paraphrasing Model.
? Zhao et al 2009. Application-driven Statistical Paraphrase Generation.
60
Outline
? Part 2
?Paraphrase Generation 
? Rule based Method
? Thesaurus based Method
? NLG based Method
? MT based Method
? Pivot based Method
A li ti f P h? pp ca ons o  arap rases
?Evaluation of Paraphrases
?Conclusions and Future work
Overview 
? Basic idea
?We can generate a paraphrase t for a sentence s by         
translating s into  a foreign language, and then 
translating it back into the source language.
s t Source language 
MT1
p
MT2
Pivot language 
MT engines
61
? Example:
Overview (cont?)
What toxins are most hazardous to expectant mothers?English
? Single-pivot
      
Che tossine sono pi? pericolose alle donne incinte?
 
Italian 
What toxins are more dangerous to pregnant women?English 
?Using a single pivot language
? Multi-pivot
?Using multiple pivot languages 
Pivot based Methods
? Duboue and Chu-Carroll, 2006
?Applied in QA systems   
? Paraphrase the input questions so as to improve the 
coverage in answer extraction
?Pivot languages
? 11
?MT engines
? 2: Babelfish (B) and Google MT (G)      
? 4 combinations: B+B, B+G, G+G, G+B
62
Pivot based Methods (cont?)
? Duboue and Chu-Carroll, 2006 (cont?)
?Given a list of automatically generated paraphrases      , 
we need to select a best one.
? For QA, we need to select the paraphrase that can find the 
answer more easily than the original question.
Features for paraphrase selection (in a classification framework)
SUM IDF The sum of the IDF scores for all terms in the original question and the 
h ( f h ith i f ti t )parap rase. pre er parap rases w  more n orma ve erms
Lengths Number of query terms for each of the paraphrase and the original 
question. (prefer shorter paraphrases)
Cosine 
Distance
The distance between the vectors of both questions, IDF-weighted. 
(filter paraphrases that diverge too much from the original)
Answer 
Types
Whether answer types, as predicted by the question analyzer, are the 
same or overlap. (the answer type should be the same)
Pivot based Methods (cont?)
? Max, 2009
?Paraphrasing sub sentential fragments -  
? Allows the exploitation of context during both source-pivot 
translation and pivot-source back-translation
context constraints context 
constraints
paraphrase
63
Pivot based Methods (cont?)
? Max, 2009 (cont?)
?Application
? Text revision
?Pivot language
? English 
?Paraphrases are acquired for French sub-sentences
?MT engine
? S t t SMT (St t l 2007)ource con ex  aware  roppa e  a ., 
Pivot based Methods (cont?)
? Zhao et al, 2010
3 MT engines: (1) Google 
translator (GG), (2) Microsoft 
translator (MS), (3) Systran 
translator (ST)
6 pivot languages: (1) 
French (F) (2) German (G) ,   , 
(3) Spanish (S), (4) Italian (I), 
(5) Portuguese (P), (6) 
Chinese (C)
54 combinations
64
Pivot based Methods (cont?)
? Zhao et al, 2010 (cont?)
?Produce a high quality paraphrase using the list of  -       
candidates
Source he said there will be major cuts in the salaries of high-level civil servants
(GG, G, MS) he said there are significant cuts in the salaries of high-level officials
(GG, F, GG) he said there will be significant cuts in the salaries of top civil level
(GG, P, GG) he said there will be big cuts in salaries of high-level civil
(MS, C, MS) he said that there will be a major senior civil service pay cut             
(MS, S, GG) he said there will be significant cuts in the salaries of senior officials
(MS, F, ST) he said there will be great cuts in the wages of the high level civils servant
(ST, G, GG) he said that there are major cuts in the salaries of senior government officials
?? ??
Good paraphrases Bad paraphrases
? Zhao et al, 2010 (cont?)
?Two techniques for producing high quality
Pivot based Methods (cont?)
    -  
paraphrases using the candidates
? Selection-based technique
?Select a best paraphrase from the 54 candidates based on 
Minimum Bayes Risk (MBR)
? Decoding-based technique
?Train a MT model using the 54 candidates, and generates a 
h i h inew parap rase w t  t
65
References 
? Duboue and Chu-Carroll. 2006. Answering the Question You Wish They 
Had Asked: The Impact of Paraphrasing for Question Answering.
? St t l 2007 E l iti S Si il it f SMT i C t troppa e  a . . xp o ng ource m ar y or  us ng on ex -
informed Features.
? Max. 2009. Sub-sentential Paraphrasing by Contextual Pivot Translation.
? Zhao et al 2010. Leveraging Multiple MT Engines for Paraphrase 
Generation.
Outline
? Part 2
P h G ti? arap rase enera on
?Applications of Paraphrases
? Paraphrasing for MT
? Other Applications
?Evaluation of Paraphrases
?Conclusions and Future work
66
Paraphrasing for MT
? Applications:
?Translate unknown terms (phrases)   
?Expand training data
?Rewrite input sentences
?Improve automatic evaluation
?Tune parameters
Translate Unknown Terms (Phrases)
? Basic idea:
?In SMT when encountering an unknown source term ,       
(phrase), we can substitute a paraphrase for it and 
then proceed using the translation of that paraphrase
f1 -> f1?
f2 -> f2?
paraphrase table
f1 -> e1
f2 -> e2
SMT phrase table
new phrase pair  
?
fi -> fj
?
fm -> fm?
  
?
fj -> ej
?
fn -> en
unknown phrase
fi
fi -> ej
67
Translate Unknown Terms (Phrases) (cont?)
? Callison-Burch et al, 2006
?Paraphrases are extracted from bilingual parallel      
corpora using the pivot approach
?New phrase pairs generated through paraphrasing 
are incorporated into the phrase table
? The paraphrase probability is added as a new feature 
function:
paraphrase 
2 1 1
1 2
( | ) If phrase table entry ( ,  )
( , ) is generated from ( ,  )
1 Otherwise
p f f e f
h e f e f
??= ???
probability
Translate Unknown Terms (Phrases) (cont?)
? Marton et al, 2009
?Paraphrases are extracted from monolingual corpora     , 
based on distributional hypothesis
f
Unknown phrase
L1__R1
L2__R2
?
contexts paraphrase phrases
f1
f2
?
?Combine the new phrase pairs in the phrase table
1 2 1
1 2
( , ) If phrase table entry ( ,  )
( , ) is generated from ( ,  )
1 Otherwise
f fpsim DP DP e f
h e f e f
??= ???
Context 
similarity
68
? Mirkin et al, 2009
?Use not only paraphrases but also entailment rules
Translate Unknown Terms (Phrases) (cont?)
       
? From WordNet
?Paraphrases: synonyms in WordNet
?Entailment rules: hypernyms in WordNet
paraphrase 
generation
paraphrase 
selection
s
generated 
para. list top-k para.
SMT t
top-n 
tran. translation 
selection
WordNet
synonyms
hypernyms
context 
model
language 
model
? Onishi et al, 2010
?Using paraphrase lattices for SMT
Translate Unknown Terms (Phrases) (cont?)
    
? Step-1: Paraphrase the input sentence, and generate a 
paraphrase lattice
?Paraphrases are extracted from bilingual parallel corpora based 
on the pivot approach
? Step-2: Give the paraphrase lattice as the input to the lattice 
decoder
69
? Effectiveness 
?When the training data of SMT is small
Translate Unknown Terms (Phrases) (cont?)
       
? Effective?
?Problem of unknown terms is more serious when the training 
data is small
?When the training data of SMT is large
? Ineffective/
?Unknown terms can be covered by adding more training data
Expand Training Data
? Enlarge training data via paraphrasing the 
source-side sentences in the parallel corpus     
Original training data
e1
e2
?
en
f1
f2
?
fn
English Foreign 
expanded training data
e1
e2
?
f1
f2
?
f
English Foreign 
e1?
e2?
?
en?
paraphrasing
en
e1?
e2?
?
en?
n
f1
f2
?
fn
70
Rewrite Input Sentences
? Paraphrase the sentence to be translated, so as 
to make it more translatable    
?Yamamoto, 2002; Zhang and Yamamoto, 2002 
? Rule-based Paraphraser for simplifying the source sentences 
?Shimohata et al, 2004
? Shorten long sentences and sentences with redundant 
information in a speech translation system
Improve Automatic Evaluation
? Automatic evaluation of MT
?Based on counting the overlaps between the       
references and machine outputs
? E.g., BLEU, NIST?
?Only computing the surface similarity is limited
? A meaning may be expressed in a way that is not included in 
the references 
?Human references are expensive to produce     
?Solution: paraphrase the references so as to include 
as many correct expressions as possible!
71
Improve Automatic Evaluation (cont?)
? Kauchak and Barzilay, 2006
?Find a paraphrase of the reference that is closer in          
wording to the system output 
? Extract candidates from WordNet synonyms
It is hard to believe that such tremendous changes have taken place for those people 
and lands that I have never stopped missing while living abroad.
For someone born here but has been sentimentally attached to a foreign country far 
Correct Wrong 
Reference
System 
output
?Filter the invalid substitution given the context
? Binary classification
?Features: context n-grams and local collocations
from home, it is difficult to believe this kind of changes.
Improve Automatic Evaluation (cont?)
? Zhou et al, 2006
?ParaEval: Compute the similarity of reference and       
system output using paraphrases
? Paraphrases are learned from bilingual parallel corpora with 
a pivot approach
?Two-tier matching strategy for SMT evaluation
? First tier: paraphrase match
? Second tier: unigram match for words not matched by         
paraphrases
72
Tune Parameters
? Madnani et al 2007
?Similar to the studies using paraphrases to improve        
automatic evaluation of MT
?Parameter tuning in SMT also needs references
? Parameter estimation of SMT: 
?optimize BLEU on a development set
?Expand the references automatically via paraphrasing
? P h tiarap rase genera on 
?Paraphrase resources are acquired based on a pivot approach
?Recast paraphrase generation as a monolingual MT problem 
and decode with a typical SMT decoder
References 
? Translate unknown terms (phrases)
? Callison-Burch et al 2006. Improved Statistical Machine Translation 
Using Paraphrases.
? Marton et al 2009. Improved Statistical Machine Translation Using 
Monolingually-Derived Paraphrases.
? Mirkin et al 2009. Source-Language Entailment Modeling for 
Translating Unknown Terms.
? Onishi et al 2010. Paraphrase Lattice for Statistical Machine 
Translation.
? Expand training data  
? Nakov. 2008. Improved Statistical Machine Translation Using 
Monolingual Paraphrases.
? Bond et al 2008. Improving Statistical Machine Translation by 
Paraphrasing the Training Data.
73
References (cont?)
? Rewrite input sentences
? Yamamoto. 2002. Machine Translation by Interaction between 
Paraphraser and Transfer.
? Zhang and Yamamoto. 2002. Paraphrasing of Chinese Utterances.
? Shimohata et al 2004. Building a Paraphrase Corpus for Speech 
Translation.
? Improve automatic evaluation
? Kauchak and Barzilay. 2006. Paraphrasing for Automatic Evaluation.
? Zhou et al 2006. Re-evaluating Machine Translation Results with 
P h S tarap rase uppor . 
? Tune parameters
? Madnani et al 2007. Using Paraphrases for Parameter Tuning in 
Statistical Machine Translation.
Outline
? Part 2
P h G ti? arap rase enera on
?Applications of Paraphrases
? Paraphrasing for MT
? Other Applications
?Evaluation of Paraphrases
?Conclusions and Future work
74
Paraphrasing for QA
? Goal:
?Alleviate the problem of word mismatch between      
questions and answers
? Two directions:
?Paraphrase questions 
? Rewrite a question into a group of paraphrases, so as to 
improve the coverage in answer extraction
?Paraphrase answer extraction patterns
? Generate answer extraction patterns as many as possible 
Paraphrasing for QA
? Ravichandran and Hovy, 2002.
?Mining paraphrase patterns from the web     
? Using hand-crafted seeds (e.g., (Mozart, 1756) for BIRTHDAY)
? Mining patterns containing the seeds
Question taxonomy 
BIRTHDAY
1.00  <NAME> ( <ANSWER> - )
0.85  <NAME> was born on <ANSWER>,
0.60 <NAME> was born in <ANSWER>
scores Paraphrase
patterns
    
0.59  <NAME> was born <ANSWER>
0.53  <ANSWER> <NAME> was born
0.50  ? <NAME> ( <ANSWER>
0.36  <NAME> ( <ANSWER> -
Given seed (Mozart, 1756)
75
Paraphrasing for Summarization
? Improve automatic evaluation of summaries
?Zhou et al2006  ., 
?Similar to the automatic evaluation of MT
? Measure the similarity between references and system 
outputs using paraphrase match as well as exact match
? Improve sentence clustering
?Barzilay et al, 1999
?Considering paraphrase match when Computing 
sentence similarity
Other Applications
? Paraphrasing for NLG
?Text revision and transformation   
? Dras, 1997
?Text transformation in order to meet external constraints, such 
as length and readability
? Paraphrasing for IR
?Query rewriting 
? Z k d R k tti 2002u erman an  as u . .
?Paraphrase user queries with WordNet synonyms
76
Other Applications (cont?)
? Writing style transformation
?Kaji et al2004  ., 
? Paraphrasing predicates from written language to spoken 
language
? Text simplification
?Carroll et al 1999
? Simplifying texts for language-impaired readers or non-native 
kspea ers
? Identify plagiarism
?Uzuner et al 2005
? Using paraphrases to better identify plagiarism
References 
? Paraphrasing for QA
? Ravichandran and Hovy. 2002. Learning Surface Text Patterns for a 
Question Answering System.
? Duboue and Chu-Carroll. 2006. Answering the Question You Wish They 
Had Asked: The Impact of Paraphrasing for Question Answering.
? Paraphrasing for summarization
? Barzilay et al 1999. Information Fusion in the Context of Multi-
Document Summarization.
? Zhou et al 2006. ParaEval: Using Paraphrases to Evaluate Summaries 
Automatically.
? Paraphrasing for NLG
? Dras. 1997. Reluctant Paraphrase: Textual Restructuring under an 
Optimisation Model.
77
References (cont?) 
? Paraphrasing for IR
? Zukerman and Raskutti. 2002. Lexical Query Paraphrasing for 
Document Retrieval.
? Writing style transformation 
? Kaji et al 2004. Paraphrasing Predicates from Written Language to 
Spoken Language Using the Web.
? Text simplification
? Carroll et al 1999. Simplifying Text for Language-Impaired Readers.
? Identify plagiarism 
? Uzuner et al 2005. Using Syntactic Information to Identify Plagiarism.
Outline
? Part 2
P h G ti? arap rase enera on
?Applications of Paraphrases
? Paraphrasing for MT
? Other Applications
?Evaluation of Paraphrases
?Conclusions and Future work
78
Evaluation of Paraphrases 
? No widely accepted evaluation criteria/
?Problem 1: Researchers define various evaluation-     
methods in their studies
? Difficult to make a direct comparison among different works
?Problem-2: Human evaluation is commonly used
? Human evaluation is rather subjective
? Difficult to replicate 
Evaluation of Paraphrase Identification
? Human evaluation
? A tomatic e al ationu  v u
?Brockett and Dolan, 2005
?Alignment Error Rate (AER)
? AER is indicative of how far the corpus is from providing a 
solution under a standard SMT tool
| | | |A P A S? + ?
| |
AER
A S
= +
Automatic 
alignment
POSSIBLE + SURE 
alignment in the gold 
standard
SURE alignment in 
the gold standard
79
Evaluation of Lexical Substitution
? Automatic evaluation
?McCarthy and Navigli 2007  , 
?Construction of gold standard data
? Five annotators, who are native speakers
? For each test word, each annotator provides up to three 
substitutes
?Evaluation:
? Precision and Recall  
Evaluation of Paraphrase Phrases
? Human evaluation
?Ask judges: 
? Whether paraphrases were approximately conceptual 
equivalent
? Whether the paraphrases were roughly interchangeable 
given the genre 
? Whether the substitutions preserved the meaning and 
remained grammatical 
? ??
?The criteria above are vaguely defined and not easy 
to reproduce
80
Evaluation of Paraphrase Phrases (cont?)
? Automatic evaluation
?Callison Burch et al2008-   ., 
?Data:
? Parallel sentences, in which paraphrases are annotated 
through manual alignment (gold standard)
?Two fashions of evaluation
? Calculate how well an automatic paraphrasing technique can 
align the paraphrases in a sentence pair     
? Calculate the lower-bound precision and relative recall of 
a paraphrasing technique (which extracts paraphrases from 
other resources)
Evaluation of Paraphrase 
Phrases (cont?)
? Alignment precision and recall ? Lower-bound precision and 
relative recall
Manual alignment
1 2
1 2
Pr
1 2 1 2,
1 2,
| ( , , ) ( , , ) |
| ( , , ) |
ec
e e C
e e C
Align
PP e e S PP e e M
PP e e S
< >?
< >?
=
??
?
System alignment
 
R 1
,
Pr
| ( , ) ( , , ) |
| ( , ) |
MET EF
s G C p s MET
LB ecision
para p s para p s G
para p s< >? ?
? =
?? ?
Paraphrase acquired with 
a method MET
Paraphrase in the gold 
standard set
1 2
1 2
Re
1 2 1 2,
1 2,
| ( , , ) ( , , ) |
| ( , , ) |
call
e e C
e e C
Align
PP e e S PP e e M
PP e e M
< >?
< >?
=
??
? R 1, 1
Re Re
| ( , ) ( , , ) |
| ( , , ) |
MET EF
s G C p s REF
l call
para p s para p s G
para p s G< >? ?
? =
?? ?
81
Evaluation of Paraphrase Patterns
? Human evaluation 
?Paraphrase patterns cannot be evaluated without      
context information
? E.g., X acquire Y, X buy Y
?Correct or not? It depends on what fill in slots X and Y
? Common view:
?A pair of paraphrase patterns is considered correct if the judge 
could think of contexts under which it holds
? Problem:
?Different judges may think of totally distinct contexts, thus the 
agreement among the judges could be low
Evaluation of Paraphrase Patterns (cont?)
? Szpektor et al, 2007
?Evaluate paraphrase patterns (and entailment rules)      
with instances rather than directly evaluate patterns
? Judges are presented not only with a pair of patterns, but 
also a sample of sentences that match its left-hand side
? Judges assess whether two patterns are paraphrases under 
each specific example
? A pair of paraphrase patterns is considered as correct only 
when the percentage of correct examples is high enough
82
Evaluation of Paraphrase Generation
? Human evaluation 
?Similar to human evaluation of SMT     
?Criteria (Zhao et al, 2009, 2010)
? Adequacy: If the meaning of the source sentence is 
preserved in the paraphrase?
? Fluency: if the generated paraphrase is well-formed?
? Usability (Zhao et al, 2009): If the paraphrase meets the 
requirement of the given application?
? Paraphrase rate (Zhao et al, 2009): How different the 
paraphrase is from the source sentence?
Evaluation of Paraphrase Generation (cont?)
? Three scales for adequacy, fluency, and usability (Zhao 
et al, 2009)
Adequacy
1 The meaning is evidently changed.
2 The meaning is generally preserved.
3 The meaning is completely preserved.
Fluency
1 The paraphrase t is incomprehensible.
2 t is comprehensible.
3 t is a flawless sentence.
1 t is opposite to the application p rpose
? Five scales for adequacy and fluency (Zhao et al, 2010)
Usability
      u .
2 t does not achieve the application.
3 t achieves the application.
83
Evaluation of Paraphrase Generation (cont?)
? Paraphrase rate (Zhao et al, 2010):
?PR 1: based on word overlap rate-      
?PR-2: based on edit distance 
( , )
1( ) 1
( )
OL S T
PR T
L S
= ? Word overlap rate
Number of words 
in the source sen.
( , )
2( )
( )
ED S T
PR T
L S
= Edit distance
Evaluation of Paraphrase Generation (cont?)
? Two questions:
?Q1: Why not adopt automatic MT methods here e g      , . ., 
BLEU, NIST, TER??
? Reason-1: It is much more difficult to construct human 
references in paraphrase generation than MT
? Reason-2: Paraphrases that change less will get larger 
scores in criteria like BLEU
?Q2: How to combine the evaluation of paraphrase 
quality and paraphrase rate?
? They seem to be incompatible
84
Evaluation within Applications
? Evaluate the role of a paraphrasing module within 
a certain application system   
?E.g., in MT, examine whether a paraphrasing module 
helps to alleviate the unknown term problem
?E.g., in QA, whether paraphrasing the answer patterns 
can improve the coverage of answer extraction
? Problems:
?Whether the result can hold for a different application?
?How to evaluate the role of the paraphrase module 
independently (not influenced by other modules)?
References 
? Brockett and Dolan. 2005. Support Vector Machines for Paraphrase 
Identification.
? S kt t l 2007 I t b d E l ti f E t il t R lzpe or e  a . . ns ance- ase  va ua on o  n a men  u e 
Acquisition.
? McCarthy and Navigli. 2007. SemEval-2007 Task 10: English Lexical 
Substitution Task.
? Callison-Burch et al 2008. ParaMetric: An Automatic Evaluation Metric for 
Paraphrasing.
? Zhao et al 2009. Application-driven Statistical Paraphrase Generation.
? Zhao et al 2010. Leveraging Multiple MT Engines for Paraphrase          
Generation.
85
Outline
? Part 2
P h G ti? arap rase enera on
?Applications of Paraphrases
? Paraphrasing for MT
? Other Applications
?Evaluation of Paraphrases
?Conclusions and Future work
Conclusions and Future Work
? Conclusions
?Paraphrasing is important in various research areas      
?Many different kinds of corpora and data resources 
have been investigated for paraphrase extraction
?Paraphrase generation is a task similar to MT, but not 
the same
?Paraphrase evaluation is problematic. Automatic 
evaluation methods are in need
86
Conclusions and Future Work (cont?)
? Future work
?Paraphrase extraction 
? Improve the quality of the extracted paraphrases
?Paraphrase generation
? Application-driven paraphrase generation
?Paraphrase application
? Apply paraphrasing techniques in commercial NLP systems, 
rather than merely in labs    
?Paraphrase evaluation
? Come up with evaluation methods that can be widely 
accepted
Thanks!
QA
87
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 497?507, Dublin, Ireland, August 23-29 2014.
Learning Sense-specific Word Embeddings By Exploiting
Bilingual Resources
Jiang Guo
?
, Wanxiang Che
?
, Haifeng Wang
?
, Ting Liu
??
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
?
Baidu Inc., Beijing, China
{jguo, car, tliu}@ir.hit.edu.cn
wanghaifeng@baidu.com
Abstract
Recent work has shown success in learning word embeddings with neural network language
models (NNLM). However, the majority of previous NNLMs represent each word with a single
embedding, which fails to capture polysemy. In this paper, we address this problem by represent-
ing words with multiple and sense-specific embeddings, which are learned from bilingual parallel
data. We evaluate our embeddings using the word similarity measurement and show that our ap-
proach is significantly better in capturing the sense-level word similarities. We further feed our
embeddings as features in Chinese named entity recognition and obtain noticeable improvements
against single embeddings.
1 Introduction
Word embeddings are conventionally defined as compact, real-valued, and low-dimensional vector rep-
resentations for words. Each dimension of word embedding represents a latent feature of the word, hope-
fully capturing useful syntactic and semantic characteristics. Word embeddings can be used straightfor-
wardly for computing word similarities, which benefits many practical applications (Socher et al., 2011;
Mikolov et al., 2013a). They are also shown to be effective as input to NLP systems (Collobert et al.,
2011) or as features in various NLP tasks (Turian et al., 2010; Yu et al., 2013).
In recent years, neural network language models (NNLMs) have become popular architectures for
learning word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b). Most
of the previous NNLMs represent each word with a single embedding, which ignores polysemy. In an
attempt to better capture the multiple senses or usages of a word, several multi-prototype models have
been proposed (Reisinger and Mooney, 2010; Huang et al., 2012). These multi-prototype models simply
induce K prototypes (embeddings) for every word in the vocabulary, where K is predefined as a fixed
value. These models still may not capture the real senses of words, because different words may have
different number of senses.
We present a novel and simple method of learning sense-specific word embeddings by using bilingual
parallel data. In this method, word sense induction (WSI) is performed prior to the training of NNLMs.
We exploit bilingual parallel data for WSI, which is motivated by the intuition that the same word in the
source language with different senses is supposed to have different translations in the foreign language.
1
For instance,?? can be translated as investment / overpower / subdue / subjugate / uniform, etc. Among
all of these translations, subdue / overpower / subjugate express the same sense of??, whereas uniform
/ investment express a different sense. Therefore, we could effectively obtain the senses of one word by
clustering its translation words, exhibiting different senses in different clusters.
The created clusters are then projected back into the words in the source language texts, forming a
sense-labeled training data. The sense-labeled data are then trained with recurrent neural network langu-
gae model (RNNLM) (Mikolov, 2012), a kind of NNLM, to obtain sense-specific word embeddings. As
a concrete example, Figure 1 illustrates the process of learning sense-specific embeddings.
?
Email correspondence.
1
In this paper, source language refers to Chinese, whereas foreign language refers to English.
This work is licenced under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http:// creativecommons.org/licenses/by/4.0/
497
? Cluster
Monolingualsense-labeled data
? ?? #2 , ?? ? ?? ?? ?1
?? ? ? ?? #1  ? ?? ?2
? ?? #2 ? ?? ?3
? ?? ? ?? ?? ?? ?  ?? #1 ?4
??5
? Project
? Extract
? RNNLM
Bilingual data(E: English, C: Chinese)
E: ? subdue , conquer or control the opponent .
C: ? ?? , ?? ? ?? ?? ?1
E: The workers wearing the factory 's uniform ? 
C: ?? ? ? ?? ? ?? ? 2
E: She overpowered the burglars .
C: ? ?? ? ?? ?3
E: They wore their priestly vestment in Church .
C: ? ?? ? ?? ?? ?? ? ?? ?4
??5
SL word ??
Translations
subdueuniformoverpowersubjugatevestment??
uniformvestment
overpowersubjugate
subdue
??#1
??#2
(clothes)
(defeat)
Sense-specific word embeddings
< v1#1, v2#1, ..., vN#1 >
< v1#2, v2#2, ..., vN#2 >
?? #1?? #2
Figure 1: An illustration of the proposed method. SL stands for source language.
To evaluate the sense-specific word embeddings we have learned, we manually construct a Chinese
polysemous word similarity dataset that contains 401 pairs of words with human-judged similarities. The
performance of our method on this dataset shows that sense-specific embeddings are significantly better
in capturing the sense-level similarities for polysemous words.
We also evaluate our embeddings by feeding them as features to the task of Chinese named entity
recognition (NER), which is a simple semi-supervised learning mechanism (Turian et al., 2010). In or-
der to use sense-specific embeddings as features, we should discriminate the word senses for the NER
data first. Therefore, we further develop a novel monolingual word sense disambiguation (WSD) algo-
rithm based on the RNNLM we have already trained previously. NER results show that sense-specific
embeddings provide noticeable improvements over traditional single embeddings.
Our contribution in this paper is twofold:
? We propose a novel approach of learning sense-specific word embeddings by utilizing bilingual
parallel data (Section 3). Evaluation on a manually constructed polysemous word similarity dataset
shows that our approach better captures word similarities (Section 5.2).
? To use the sense-specific embeddings in practical applications, we develop a novel WSD algorithm
for monolingual data based on RNNLM (Section 4). Using the algorithm, we feed the sense-specific
embeddings as additional features to NER and achieve significant improvement (Section 5.3).
2 Background: Word Embedding and RNNLM
There has been a line of research on learning word embeddings via NNLMs (Bengio et al., 2003; Mnih
and Hinton, 2008; Mikolov et al., 2013b). NNLMs are language models that exploit neural networks to
make probabilistic predictions of the next word given preceding words. By training NNLMs, we obtain
both high performance language models and word embeddings.
Following Mikolov et al. (2013b), we use the recurrent neural network as the basic framework for train-
ing NNLMs. RNNLM has achieved the state-of-the-art performance in language modeling (Mikolov,
2012) and learned effective word embeddings for several tasks (Mikolov et al., 2013b). The architecture
of RNNLM is shown in Figure 2.
The input layer of RNNLM consists of two components: w(t) and h(t ? 1). w(t) is the one-hot
representation of the word at time step t,
2
h(t ? 1) is the output of hidden layer at the last time step.
Therefore, the input encodes all previous history when predicting the next word at time step t. Compared
2
A feature vector of the same size of the vocabulary, and only one dimension is on.
498
w(t)
y(t)
U
W
V
h(t)
h(t-1)
Figure 2: The basic architecture of RNNLM.
with other feed-forward NNLMs, the RNNLM can theoretically represent longer context patterns. The
output y(t) represents the probability distribution of the next word p(w(t + 1)|w(t),h(t ? 1)). The
output values are computed as follows:
h(t) = f(Uw(t) +Wh(t? 1)) (1)
y(t) = g(Vh(t)) (2)
where f is a sigmoid function and g is a softmax function.
The RNNLM is trained by maximizing the log-likelihood of the training data using stochastic gradi-
ent descent (SGD), in which back propagation through time (BPTT) is used to efficiently compute the
gradients. In the RNNLM, U is the embedding matrix, where each column vector represents a word.
As discussed in Section 1, the RNNLM and even most NNLMs ignore the polysemy phenomenon in
natural languages and induce a single embedding for each word. We address this issue and introduce an
effective approach for capturing polysemy in the next section.
3 Sense-specific Word Embedding Learning
In our approach, WSI is performed prior to the training of word embeddings. Inspired by Gale et al.
(1992) and Chan and Ng (2005), who used bilingual data for automatically generating training examples
of WSD, we present a bilingual approach for unsupervised WSI, as shown in Figure 1. First, we extract
the translations of the source language words from bilingual data (?). Since there may be multiple
translations for the same sense of a source language word, it is straightforward to cluster the translation
words, exhibiting different senses in different clusters (?).
Once word senses are effectively induced for each word, we are able to form the sense-labeled training
data of RNNLMs by tagging each word occurrence in the source language text with its associated sense
cluster (?). Finally, the sense-tagged corpus is used to train the sense-specific word embeddings in a
standard manner (?).
3.1 Translation Words Extraction
Given bilingual data after word alignment, we present a way of extracting translation words for source
language words by exploiting the translation probability produced by word alignment models (Brown et
al., 1993; Och and Ney, 2003; Liang et al., 2006).
More formally, we notate the Chinese sentence as c = (c
1
, ..., c
I
) and English sentence as e =
(e
1
, ..., e
J
). The alignment models can be generally factored as:
p(c|e) =
?
a
p(a, c|e) (3)
p(a, c|e) =
?
J
j=1
p
d
(a
j
|a
j?
, j)p
t
(c
j
|e
a
j
) (4)
where a is the alignment specifying the position of an English word aligned to each Chinese word,
p
d
(a
j
|a
j?
, j) is the distortion probability, and p
t
(c
j
|e
a
j
) is the translation probability which we use.
499
SL Word Translation Words Translation Word Clusters Nearest Neighbours
?? investment, overpower,
subdue, subjugate, uniform
investment, uniform ??
dress
,??
policeman uniform
subdue, subjugate, overpower ??
defeat
,??
beat
,??
conquer
? blossom, cost, flower,
spend, take, took
flower, blossom ?
greens
,?
leaf
,??
fruit
take, cost, spend ??
cost
,??
save
,??
rest
? act, code, France,
French, law, method
France, French ?
Germany
,?
Russia
,?
Britain
law, act, code ??
ordinance
,??
bill
,??
rule
method ??
concept
,??
scheme
,??
way
??
lead, leader, leadership
leader, leadership ??
chief
,??
boss
,??
chairman
lead ??
supervise
,??
decision
,??
work
Table 1: Results of our approach on a sample of polysemous words. The second column lists the extracted
translation words of the source language word (Section 3.1). The third column lists the clustering results
using affinity propagation (Section 3.2). The last column lists the nearest neighbour words computed
using the learned sense-specific word embeddings (Section 5.2.2).
In this paper, we use the alignment model proposed by Liang et al. (2006). We utilize the bidirectional
translation probabilities for the extraction of translations, where a foreign language word w
e
is deter-
mined as a translation of source language word w
c
only if both translation probabilities p
t
(w
c
|w
e
) and
p
t
(w
e
|w
c
) exceed some threshold 0 < ? < 1.
The second column of Table 1 presents the extraction results on a sample of source language words
with the corresponding translation words.
3.2 Clustering of Translation Words
For each source language word, its translation words are then clustered so as to separate different senses.
At the clustering time, we first represent each translation word with a feature vector (point), so that
we can measure the similarities between points. Then we perform clustering on these feature vectors,
representing different senses in different clusters.
Different from Apidianaki (2008) who represents all occurrences of the translation words with their
contexts in the foreign language for clustering, we adopt the embeddings of the translation words as
the representations and directly perform clustering on the translation words,
3
rather than the contexts of
occurrences. The embedding representation is chosen for two reasons: (1) Word embeddings encode rich
lexical semantics. They can be directly used to measure word similarities. (2) Embedding representation
of the translation words leads to extremely high-efficiency clustering, because the number of translation
words is orders of magnitude less than their occurrences.
Moreover, since the number of senses of different source language words is varied, the commonly-
used k-means algorithm becomes inappropriate for this situation. Instead, we employ affinity propaga-
tion (AP) algorithm (Frey and Dueck, 2007) for clustering. In AP, each cluster is represented by one
of the samples of it, which we call an exemplar. AP finds the exemplars iteratively based on the con-
cept of ?message passing?. AP has the major advantage that the number of the resulting clusters is
dynamic, which mainly depends on the distribution of the data. Compared with other possible clustering
approaches, such as hierarchical agglomerative clustering (Kartsaklis et al., 2013), AP determines the
number of resulting clusters automatically without using any partition criterions.
The third column of Table 1 lists the resulting clusters of the translation words for the sampled pol-
ysemous words. We can see that the resulting clusters are meaningful: senses are well represented by
clusters of translation words.
3.3 Cross-lingual Word Sense Projection
The produced clusters are then projected back into the source language to identify word senses.
3
The publicly available word embeddings proposed by Collobert et al. (2011) are used.
500
For each occurrence w
o
of the word w in the source language corpora, we first select the aligned word
with the highest marginal edge posterior (Liang et al., 2006) as its translation. We then identify the sense
of w
o
by computing the similarities of its translation word with each exemplar of the clusters, and select
the one with the maximum similarity. When w
o
is aligned with NULL, we heuristically identify its sense
as the most frequent sense of w that appears in the bilingual dataset.
After projecting the word senses into the source language, we obtain a sense-labeled corpus, which is
used to train the sense-specific word embeddings with RNNLM. The training process is exactly the same
as single embeddings, except that the words in our training corpus has been labeled with senses.
4 Application of Sense-specific Word Embeddings
One of the attractive characteristic of word embeddings is that they can be directly used as word features
in various NLP applications, including NER, chunking, etc. Despite of the usefulness of word embed-
dings on these applications, previous work seldom concerns that words may have multiple senses, which
cannot be effectively represented with single embeddings. In this section, we address this problem by
utilizing sense-specific word embeddings.
We take the task of Chinese NER as a case study. Intuitively, word senses are important in NER. For
instance,? is likely to be an NE of Location when it refers to America. However, when it expresses the
sense of beautiful, it should not be an NE.
Using sense-specific word embedding features for NER is not as straightforward as using single em-
beddings. For each word in the NER data, we first need to determine the correct word sense of it, which
is a typical WSD problem. Then we use the embedding which corresponds to that sense as features.
Here we treat WSD as a sequence labeling problem, and solve it with a very natural algorithm based on
RNNLM we have already trained (Section 3).
4.1 RNNLM-based Word Sense Disambiguation
Given the automatically induced word sense inventories and the RNNLM which has already been trained
on the sense-labeled data of source language, we first develop a greedy decoding algorithm for the
sequential WSD, which works deterministically. Then we improve it using beam search.
Greedy. For word w, we denote the sense-labeled w as w
s
k
, where s
k
represents the k
th
sense of w.
In each step, a single decision is made and the sense of next word (w(t + 1)) which has the maximum
RNNLM output is chosen, given the current (sense-labeled) word w(t)
s
?
and the hidden layer h(t? 1)
at the last time step as input. We simply need to compute a shortlist of y(t) associated with w(t + 1),
that is, y(t)|
w(t+1)
at each step. This process is illustrated in Figure 3.
Beam search. The greedy procedure described above can be improved using a left-to-right beam
search decoding for obtaining a better sequence. The beam-search decoding algorithm keeps B different
sequences of decisions in the agenda, and the sequence with the best overall score is chosen as the final
sense sequence.
Note that the dynamic programming decoding (e.g. viterbi) is not applicable here, because of the
recurrent characteristic of RNNLM. At each step, decisions made by RNNLM depends on all previous
decisions instead of the previous state only, hence markov assumption is not satisfied.
5 Experiments
5.1 Experimental Settings
The Chinese-English parallel datasets we use include LDC03E24, LDC04E12 (1998), the IWSLT 2008
evaluation campaign dataset and the PKU 863 parallel dataset. All corpora are sentence-aligned. After
cleaning and filtering the corpus,
4
we obtain 918,681 pairs of sentences (21.7M words).
In this paper, we use BerkeleyAligner to produce word alignments over the parallel dataset.
5
Berke-
leyAligner also gives translation probabilities and marginal edge posterior probabilities. We adopt the
4
Sentences that are too long (more than 40 words) or too short (less than 10 words) are discarded.
5
code.google.com/p/berkeleyaligner/
501
h(t-1)
U
W
V
h(t)
Next word (sense-labeled)Last word(sense-labeled)
w(t)s*
w(t+1)s1
y(t)|w(t+1)
w(t+1)s2
w(t+1)sK
?
y(t)
?
w(t+1)s*
max
w(t)s* w(t+1)s*
w(t+1)s* w(t+2)s*
w(t+2)s* w(t+3)s*
h(t)
h(t+1)
h(t+2)
h(t-1)
Shortlist? 
Figure 3: Using RNNLM for WSD by sequential labeling (left). Decision at each step of the RNNLM-
based WSD algorithm (right).
scikit-learn tool (Pedregosa et al., 2011) to implement the AP clustering algorithm.
6
The AP algorithm
is not fully automatic in deciding the cluster number. There is a tunable parameter calls preference. A
preference with a larger value encourages more clusters to be produced. We set the preference at the
median value of the input similarity matrix to obtain a moderate number of clusters. The rnnlm toolkit
developed by Mikolov et al. (2011) is used to train RNNLM and obtain word embeddings.
7
We induce
both single and sense-specific embeddings with 50 dimensions. Finally, We obtain embeddings of a
vocabulary of 217K words, with a proportion of 8.4% having multiple sense clusters.
5.2 Evaluation on Word Similarity
Word embeddings can be directly used for computing similarities between words, which benefits many
practical applications. Therefore, we first evaluate our embeddings using a similarity measurement.
Word similarities are calculated using the MaxSim and AvgSim metric (Reisinger and Mooney, 2010):
MaxSim(u, v) = max
1?i?k
u
,1?j?k
v
s(u
i
, v
j
) (5)
AvgSim(u, v) =
1
k
u
?k
v
?
k
u
i=1
?
k
v
j=1
s(u
i
, v
j
) (6)
where k
u
and k
v
are the number of the induced senses for words u and v, respectively. s(?, ?) can be any
standard similarity measure. In this study, we use the cosine similarity.
Previous works used the WordSim-353 dataset (Finkelstein et al., 2002) or the Chinese version (Jin and
Wu, 2012) for the evaluation of general word similarity. These datasets rarely contain polysemous words,
and thus is unsuitable for our evaluation. To the best of our knowledge, no datasets for polysemous word
similarity evaluation have been published yet, either in English or Chinese. In order to fill this gap in the
research community, we manually construct a Chinese polysemous word similarity dataset.
5.2.1 Chinese Polysemous Word Similarity Dataset Construction
We adopt the HowNet database (Dong and Dong, 2006) in constructing the dataset. HowNet is a Chinese
knowledge database that maintains comprehensive semantic definitions for each word in Chinese. The
process of the dataset construction includes three steps: (1) Commonly used polysemous words are
extracted according to their sense definitions in HowNet. (2) For each polysemous word, we select
several other words to form word pairs with it. (3) Each word pair is manually annotated with similarity.
In step (1), we mainly took advantage of HowNet for the selection of polysemous words. However,
the synsets defined in HowNet are often too fine-grained and many of them are difficult to distinguish,
6
scikit-learn.org
7
www.fit.vutbr.cz/
?
imikolov/rnnlm/
502
particularly for non-experts. Therefore, we manually discard those words with senses that are hard to
distinguish.
In step (2), for each polysemous word w selected in step 1, we sample several other words to form
word pairs withw. The sampled words can be roughly divided into two categories: related and unrelated.
The related words are sampled manually. They can be the hypernym, hyponym, sibling, (near-)synonym,
antonym, or topically related to one sense of w. The unrelated words are sampled randomly.
In step (3), we ask six graduate students who majored in computational linguistics to assign each word
pair a similarity score. Following the setting of WordSim-353, we restrict the similarity score in the range
(0.0, 10.0). To address the inconsistency of the annotations, we discard those word pairs with a standard
deviation greater than 1.0. We end up with 401 word pairs annotated with acceptable consistency. Unlike
the WordSim-353, in which most of the words are nouns, the words in our dataset are more diverse in
terms of part-of-speech tags.
Table 2 lists a sample of word pairs with annotated similarities from the dataset. The whole evaluation
dataset will be publicly available for the research community.
8
Word Paired word Category Mean.Sim Std.Dev
?? ??conquer synonym 8.60 0.29
??
key point
unrelated 0.12 0.19
? ?enter autonym 7.90 0.97
??
publish
near-synonym 7.86 0.76
? ?plant stem sibling 7.80 0.12
??
cost
topic-related 5.86 0.90
? ??
food
hypernym 6.50 0.71
Table 2: Sample word pairs of our dataset. The unrelated words are randomly sampled. Mean.Sim
represents the mean similarity of the annotations, Std.Dev represents the standard deviation.
5.2.2 Evaluation Results
Following Zou et al. (2013), we use Spearman?s ? correlation and Kendall?s ? correlation for evaluation.
The results are shown in Table 3. By utilizing sense-specific embeddings, our approach significantly
outperforms the single-version using either MaxSim or AvgSim measurement.
For comparison with multi-prototype methods, we borrow the context-clustering idea from Huang et
al. (2012), which was first presented by Sch?utze (1998). The occurrences of a word are represented by
the average embeddings of its context words. Following Huang et al.?s settings, we use a context window
of size 10 and all occurrences of a word are clustered using the spherical k-means algorithm, where k is
tuned with a development set and finally set to 2.
System
MaxSim AvgSim
? ?100 ? ?100 ? ?100 ? ?100
Ours 55.4 40.9 49.3 35.2
SingleEmb 42.8 30.6 42.8 30.6
Multi-prototype 40.7 29.1 38.3 27.4
Table 3: Spearman?s ? correlation and Kendall?s ? correlation evaluated on the polysemous dataset.
Surprisingly, the multi-prototype method performs even slightly worse than the single-version, which
suggests that learning a fixed number of embeddings for every word may even harm the embedding.
Additionally, the clustering process of the multi-prototype approach suffers from high memory and time
cost, especially for the high-frequency words.
8
ir.hit.edu.cn/
?
jguo
503
To obtain intuitive insight into the superior performance of sense-specific embeddings, we list in the
last column of Table 1 the nearest neighborhoods of the sampled words in the evaluation dataset. The list
shows that we are able to find the different meanings of a word by using sense-specific embeddings.
5.3 Application on Chinese NER
We further apply the sense-specific embeddings as features to Chinese NER. We first perform WSD on
the NER data using the algorithm introduced in Section 4. For beam search decoding, the beam size B
is tuned on a development set and is finally set to 16.
We conduct our experiments on data from People?s Daily (Jan. and Jun. 1998).
9
The original corpus
contains seven NE types.
10
In this study, we select the three most common NE types: Person, Location,
Organization. The data from January are chosen as the training set (37,426 sentences). The first 2,000
sentences from June are chosen as the development set and the next 8,000 sentences as the test set.
CRF models are used in our NER system and are optimized by L2-regularized SGD. We use the
CRFSuite (Okazaki, 2007) because it accepts feature vectors with numerical values. The state-of-the-art
features (Che et al., 2013) are used in our baseline system. For both single and sense-specific embedding
features, we use a window size of 4 (two words before and two words after).
5.3.1 Results
Table 4 demonstrates the performance of NER on the test set. As desired, the single embedding features
improve the performance of our baseline, which were also shown in (Turian et al., 2010). Furthermore,
the sense-specific embeddings outperform the single word embeddings by nearly 1% F-score (88.56 vs.
87.58), which is statistically significant (p-value < 0.01 using one-tail t-test).
System P R F
Baseline 93.27 81.46 86.97
+SingleEmb 93.55 82.32 87.58
+SenseEmb (greedy) 93.38 83.56 88.20
+SenseEmb (beam search) 93.59 84.05 88.56
Table 4: Performance of NER on test data.
According to our hypothesis, the sense-specific embeddings should bring considerable improvements
to the NER of polysemous words. To verify this, we evaluate the per-token accuracy of the polysemous
words in the NER test data. We again adopt HowNet to determine the polysemy. Words that are defined
with multiple senses are selected as test set. Figure 4 shows that the sense-specific embeddings indeed
improve the NE recognition of the polysemous words, whereas the single embeddings even decrease the
accuracy slightly. We also obtain improvements on the NE recognition of the monosemous words, which
provide evidences that more accurate prediction of polysemous words is beneficial for the prediction of
the monosemous words through contextual influence.
6 Related Work
Previous studies have explored the NNLMs, which predict the next word given some history or future
words as context within a neural network architecture. Schwenk and Gauvain (2002), Bengio et al.
(2003), Mnih and Hinton (2007), and Collobert et al. (2011) proposed language models based on feed-
forward neural networks. Mikolov et al. (2010) studied language models based on RNN, which managed
to represent longer history information for word-predicting and demonstrated outstanding performance.
Besides, researchers have also explored the word embeddings learned by NNLMs. Collobert et al.
(2011) used word embeddings as the input of various NLP tasks, including part-of-speech tagging,
chunking, NER, and semantic role labeling. Turian et al. (2010) made a comprehensive comparison
of various types of word embeddings as features for NER and chunking. In addition, word embeddings
9
www.icl.pku.edu.cn/icl groups/corpus/dwldform1.asp
10
Person, Location, Organization, Date, Time, Number and Miscellany
504
Polysemous(2) Polysemous(3) Monosemous
per?t
oken
 acc
urac
y
0.60
0.65
0.70
0.75
0.80
0.85
0.90 Baseline +SingleEmb +SenseEmb
Figure 4: Per-token accuracy on the polysemous and monosemous words in the NER test data. Polyse-
mous(k) represents the set of words that have more than or equal to k senses defined in HowNet.
are shown to capture many relational similarities, which can be recovered by vector arithmetic in the
embedding space (Mikolov et al., 2013b; Fu et al., 2014). Klementiev et al. (2012) and Zou et al. (2013)
learned cross-lingual word embeddings by utilizing MT word alignments in bilingual parallel data to
constrain translational equivalence.
Most previous NNLMs induce single embedding for each word, ignoring the polysemous property of
languages. In an attempt to capture the different senses or usage of a word, Reisinger and Mooney (2010)
and Huang et al. (2012) proposed multi-prototype models for inducing multiple embeddings for each
word. They did this by clustering the contexts of words. These multi-prototype models simply induced
a fixed number of embeddings for every word, regardless of the real sense capacity of the specific word.
There has been a lot of work on using bilingual resources for word sense disambiguation (Gale et
al., 1992; Chan and Ng, 2005). By using aligned bilingual data along with word sense inventories such
as WordNet, training examples for WSD can be automatically gathered. We employ this idea for word
sense induction in our study, which is free of any pre-defined word sense thesaurus.
The most similar work to our sense induction method is Apidianaki (2008). They presented a method
of sense induction by clustering all occurrences of each word?s translation words. In their approach,
occurrences are represented with their contexts. We suggest that clustering contexts suffer from high
memory and time cost, as well as data sparsity. In our method, by clustering the embeddings of transla-
tion words, we induce word senses much more efficiently.
To evaluate word similarity models, researchers often apply a dataset with human-judged similarities
on word pairs, such as WordSim-353 (Finkelstein et al., 2002), MC (Miller and Charles, 1991), RG
(Rubenstein and Goodenough, 1965) and Jin and Wu (2012). For context-based multi-prototype mod-
els, (Huang et al., 2012) constructs a dataset with context-dependent word similarity. To the best of
our knowledge, there is no publicly available datasets for context-unaware polysemous word similarity
evaluation yet. This paper fills this gap.
7 Conclusion
This paper presents a novel and effective approach of producing sense-specific word embeddings by
exploiting bilingual parallel data. The proposed embeddings are expected to capture the multiple senses
of polysemous words. Evaluation on a manually annotated Chinese polysemous word similarity dataset
shows that the sense-specific embeddings significantly outperforms the single embeddings and the multi-
prototype approach.
Another contribution of this study is the development of a beam-search decoding algorithm based on
RNNLM for monolingual WSD. This algorithm bridges the proposed sense-specific embeddings and
practical applications, where no bilingual information is provided. Experiments on Chinese NER show
that the sense-specific embeddings indeed improve the performance, especially for the recognition of the
polysemous words.
505
Acknowledgments
We are grateful to Dr. Zhenghua Li, Yue Zhang, Shiqi Zhao, Meishan Zhang and the anonymous review-
ers for their insightful comments and suggestions. This work was supported by the National Key Basic
Research Program of China via grant 2014CB340503 and 2014CB340505, the National Natural Science
Foundation of China (NSFC) via grant 61370164.
References
Marianna Apidianaki. 2008. Translation-oriented word sense induction based on parallel corpora. In In Proceed-
ings of the 6th Conference on Language Resources and Evaluation (LREC), Marrakech, Morocco.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language
model. The Journal of Machine Learning Research, 3:1137?1155.
Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263?311.
Yee Seng Chan and Hwee Tou Ng. 2005. Scaling up word sense disambiguation via parallel texts. In AAAI,
volume 5, pages 1037?1042.
Wanxiang Che, Mengqiu Wang, Christopher D. Manning, and Ting Liu. 2013. Named entity recognition with
bilingual constraints. In Proceedings of the 2013 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, pages 52?62, Atlanta, Georgia, June.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493?2537.
Zhendong Dong and Qiang Dong. 2006. HowNet and the Computation of Meaning. World Scientific.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.
2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116?
131.
Brendan J Frey and Delbert Dueck. 2007. Clustering by passing messages between data points. science,
315(5814):972?976.
Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Learning semantic hierar-
chies via word embeddings. In Proceedings of the 52th Annual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, Baltimore MD, USA.
William A Gale, Kenneth W Church, and David Yarowsky. 1992. Using bilingual materials to develop word sense
disambiguation methods. In Proceedings of the 4th International Conference on Theoretical and Methodologi-
cal Issues in Machine Translation, pages 101?112.
Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations
via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers-Volume 1, pages 873?882.
Peng Jin and Yunfang Wu. 2012. Semeval-2012 task 4: evaluating chinese word similarity. In Proceedings of the
First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference
and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,
pages 374?377.
Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen Pulman. 2013. Separating disambiguation from composi-
tion in distributional semantics. CoNLL-2013, pages 114?123.
Alexandre Klementiev, Ivan Titov, and Binod Bhattarai. 2012. Inducing crosslingual distributed representations
of words. In Proceedings of COLING 2012, pages 1459?1474, Mumbai, India, December.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the main conference
on Human Language Technology Conference of the North American Chapter of the Association of Computa-
tional Linguistics, pages 104?111.
Tomas Mikolov, Martin Karafi?at, Luk?a?s Burget, Jan Cernocky, and Sanjeev Khudanpur. 2010. Recurrent neural
network based language model. In Proceedings of Interspeech, pages 1045?1048.
506
Tomas Mikolov, Stefan Kombrink, Anoop Deoras, Lukar Burget, and J
?
Cernock`y. 2011. Rnnlm-recurrent neural
network language modeling toolkit. In Proc. of the 2011 ASRU Workshop, pages 196?201.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations
in vector space. arXiv preprint arXiv:1301.3781.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word
representations. In Proceedings of NAACL-HLT, pages 746?751.
Tomas Mikolov. 2012. Statistical Language Models Based on Neural Networks. Ph.D. thesis, Ph. D. thesis, Brno
University of Technology.
George A Miller and Walter G Charles. 1991. Contextual correlates of semantic similarity. Language and
cognitive processes, 6(1):1?28.
Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In
Proceedings of the 24th international conference on Machine learning, pages 641?648.
Andriy Mnih and Geoffrey E Hinton. 2008. A scalable hierarchical distributed language model. In Advances in
neural information processing systems, pages 1081?1088.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational linguistics, 29(1):19?51.
Naoaki Okazaki. 2007. Crfsuite: a fast implementation of conditional random fields (crfs). URL http://www.
chokkan. org/software/crfsuite.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-
learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825?2830.
Joseph Reisinger and Raymond J Mooney. 2010. Multi-prototype vector-space models of word meaning. In
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association
for Computational Linguistics, pages 109?117.
Herbert Rubenstein and John B Goodenough. 1965. Contextual correlates of synonymy. Communications of the
ACM, 8(10):627?633.
Hinrich Sch?utze. 1998. Automatic word sense discrimination. Computational linguistics, 24(1):97?123.
Holger Schwenk and Jean-Luc Gauvain. 2002. Connectionist language modeling for large vocabulary continuous
speech recognition. In Acoustics, Speech, and Signal Processing (ICASSP), 2002 IEEE International Confer-
ence on, volume 1, pages 765?768.
Richard Socher, Eric H Huang, Jeffrey Pennin, Christopher D Manning, and Andrew Ng. 2011. Dynamic pooling
and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing
Systems, pages 801?809.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics, pages 384?394.
Mo Yu, Tiejun Zhao, Daxiang Dong, Hao Tian, and Dianhai Yu. 2013. Compound embedding features for semi-
supervised learning. In Proceedings of NAACL-HLT, pages 563?568.
Will Y. Zou, Richard Socher, Daniel Cer, and Christopher D. Manning. 2013. Bilingual word embeddings for
phrase-based machine translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural
Language Processing, pages 1393?1398, Seattle, Washington, USA, October.
507
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 524?534,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Improving Pivot-Based Statistical Machine Translation 
Using Random Walk 
 
Xiaoning Zhu1*
Conghui Zhu1, and Tiejun Zhao1 
, Zhongjun He2, Hua Wu2, Haifeng Wang2,  
Harbin Institute of Technology, Harbin, China1 
Baidu Inc., Beijing, China2 
{xnzhu, chzhu, tjzhao}@mtlab.hit.edu.cn 
{hezhongjun,wu_hua,wanghaifeng}@baidu.com 
 
 
 
 
 
                                                          
* This work was done when the first author was visiting Baidu. 
Abstract 
This paper proposes a novel approach that uti-
lizes a machine learning method to improve 
pivot-based statistical machine translation 
(SMT). For language pairs with few bilingual 
data, a possible solution in pivot-based SMT 
using another language as a "bridge" to gen-
erate source-target translation. However, one 
of the weaknesses is that some useful source-
target translations cannot be generated if the 
corresponding source phrase and target phrase 
connect to different pivot phrases. To allevi-
ate the problem, we utilize Markov random 
walks to connect possible translation phrases 
between source and target language. Experi-
mental results on European Parliament data, 
spoken language data and web data show that 
our method leads to significant improvements 
on all the tasks over the baseline system. 
1 Introduction 
Statistical machine translation (SMT) uses bilin-
gual corpora to build translation models. The 
amount and the quality of the bilingual data 
strongly affect the performance of SMT systems. 
For resource-rich language pairs, such as Chinese-
English, it is easy to collect large amounts of bi-
lingual corpus. However, for resource-poor lan-
guage pairs, such as Chinese-Spanish, it is difficult 
to build a high-performance SMT system with the 
small scale bilingual data available.  
The pivot language approach, which performs 
translation through a third language, provides a 
possible solution to the problem. The triangulation 
method (Wu and Wang, 2007; Cohn and Lapata, 
2007) is a representative work for pivot-based ma-
chine translation. With a triangulation pivot ap-
proach, a source-target phrase table can be 
obtained by combining the source-pivot phrase 
table and the pivot-target phrase table. However, 
one of the weaknesses is that some corresponding 
source and target phrase pairs cannot be generated, 
because they are connected to different pivot 
phrases (Cui et al, 2013). As illustrated in Figure 
1, since there is no direct translation between ??
?? henkekou? and ?really delicious?, the trian-
gulation method is unable to establish a relation 
between ???? henkekou? and the two Spanish 
phrases. 
To solve this problem, we apply a Markov ran-
dom walk method to pivot-based SMT system. 
Random walk has been widely used. For example, 
Brin and Page (1998) used random walk to dis-
cover potential relations between queries and doc-
uments for link analysis in information retrieval. 
Analogous to link analysis, the aim of pivot-based 
translation is to discover potential translations be-
tween source and target language via the pivot 
language.  
524
The goal of this paper is to extend the previous 
triangulation approach by exploring implicit trans-
lation relations using random walk method. We 
evaluated our approach in several translation tasks, 
including translations between European lan-
guages; Chinese-Spanish spoken language transla-
tion and Chinese-Japanese translation with English 
as the pivot language. Experimental results show 
that our approach achieves significant improve-
ments over the conventional pivot-based method, 
triangulation method. 
The remainder of this paper is organized as fol-
lows. In section 2, we describe the related work. 
We review the triangulation method for pivot-
based machine translation in section 3. Section 4 
describes the random walk models. In section 5 
and section 6, we describe the experiments and 
analyze the performance, respectively. Section 7 
gives a conclusion of the paper. 
2 Related Work 
Several methods have been proposed for pivot-
based translation. Typically, they can be classified 
into 3 kinds of methods: 
Transfer Method: Within the transfer frame-
work (Utiyama and Isahara, 2007; Wang et al, 
2008; Costa-juss? et al, 2011), a source sentence 
is first translated to n pivot sentences via a source-
pivot translation system, and then each pivot sen-
tence is translated to m target sentences via a piv-
ot-target translation system. At each step (source 
to pivot and pivot to target), multiple translation 
outputs will be generated, thus a minimum Bayes-
risk system combination method is often used to 
select the optimal sentence (Gonz?lez-Rubio et al, 
2011; Duh et al, 2011). A problem with the trans-
fer method is that it needs to decode twice. On one 
hand, the time cost is doubled; on the other hand, 
the translation error of the source-pivot translation 
system will be transferred to the pivot-target trans-
lation. 
Synthetic Method: A synthetic method creates 
a synthetic source-target corpus using source-pivot 
translation model or pivot-target translation model 
(Utiyama et al, 2008; Wu and Wang, 2009). For 
example, we can translate each pivot sentence in 
the pivot-target corpus to source language with a 
pivot-source model, and then combine the translat-
ed source sentence with the target sentence to ob-
tain a synthetic source-target corpus, and vice 
versa. However, it is difficult to build a high quali-
ty translation system with a corpus created by a 
machine translation system. 
Triangulation Method: The triangulation 
method obtains source-target model by combining 
source-pivot and pivot-target translation models 
(Wu and Wang, 2007; Cohn and Lapata 2007), 
which has been shown to work better than the oth-
er pivot approaches (Utiyama and Isahara, 2007). 
As we mentioned earlier, the weakness of triangu-
lation is that the corresponding source and target 
phrase pairs cannot be connected in the case that 
they connect to different pivot phrases. 
3 The Triangulation Method 
In this section, we review the triangulation method 
for pivot-based translation. 
With the two additional bilingual corpora, the 
source-pivot and pivot-target translation models 
can be trained. Thus, a pivot model can be ob-
tained by merging these two models. In the trans-
lation model, the phrase translation probability and 
the lexical weight are language dependent, which 
will be introduced in the next two sub-sections. 
Figure 1: An example of random walk on phrase table. The dashed line indicates an implicit relation 
in the phrase table. 
???? 
feichanghaochi 
really delicious 
very tasty 
 
???
henkekou 
realmente delicioso 
 
Chinese English Spanish 
muy delicioso 
 
525
3.1 Phrase Translation Probability 
The triangulation method assumes that there exist 
translations between phrases s  and phrase p  in 
source and pivot languages, and between phrase 
p  and phrase t  in pivot and target languages. 
The phrase translation probability ?  between 
source and target languages is determined by the 
following model: 
( | ) ( | , ) ( | )
          ( | ) ( | )
p
p
s t s p t p t
s p p t
? ? ?
? ?
=
=
?
?
       (1) 
3.2 Lexical Weight 
Given a phrase pair ( , )s t and a word alignment 
a  between the source word positions 1, ,i n= ?  
and the target word positions 0,1, ,j m= ? , the 
lexical weight of phrase pair ( , )s t  can be calcu-
lated with the following formula (Koehn et al 
2003) : 
( , )1
1
( | , ) ( | )
{ | ( , ) }
n
i j
i j ai
p s t a s t
j i j a?
?
? ?=
=
? ?? (2) 
In formula 2, the lexical translation probability 
distribution ( | )s t?  between source word s  and 
target word t  can be estimated with formula 3. 
'
'
( , )
( | )
( , )
s
count s t
s t
count s t
? =
?
            (3) 
Thus the alignment a  between the source 
phrase s  and target phrase t  via pivot phrase p  
is needed for computing the lexical weight. The 
alignment a  can be obtained as follows: 
1 2{( , ) | : ( , ) & ( , ) }a s t p s p a p t a= ? ? ?    (4) 
where 1a  and 2a  indicate the word alignment be-
tween the phrase pair ( , )s p  and ( , )p t , respec-
tively. 
The triangulation method requires that both the 
source and target phrases connect to the same piv-
ot phrase. Otherwise, the source-target phrase pair 
cannot be discovered. As a result, some useful 
translation relations will be lost. In order to allevi-
ate this problem, we propose a random walk model, 
to discover the implicit relations among the source, 
pivot and target phrases. 
4 Random Walks on Translation Graph 
For phrase-based SMT, all source-target phrase 
pairs are stored in a phrase table. In our random 
walk approach, we first build a translation graph 
according to the phrase table. A translation graph 
contains two types of nodes: source phrase and 
target phrase. A source phrase s  and a target 
phrase t  are connected if exists a phrase pair 
( , )s t  in the phrase table. The edge can be 
weighted according to translation probabilities or 
alignments in the phrase table. For the pivot-based 
translation, the translation graph can be derived 
from the source-pivot phrase table and pivot-target 
phrase table.  
Our random walk model is inspired by two 
works (Szummer and Jaakkola, 2002; Craswell 
and Szummer,2007). The general process of ran-
dom walk can be described as follows: 
Let ( , )G V E= be a directed graph with n  ver-
tices and m  edges. For a vertex v V? , ( )v?  de-
notes the set of neighbors of v  in G . A random 
walk on G  follows the following process: start at 
a vertex 0v , chose and walk along a random 
neighbor 1v , with 1 0( )v v?? . At the second step, 
start from 1v  and chose a random neighbor 2v , and 
so on. 
Let S be the set of source phrases, and P be the 
set of pivot phrases. Then the nodes V are the un-
ion of S and P. The edges E correspond to the rela-
tions between phrase pairs.  
Let R represent the binary relations between 
source phrases and pivot phrases. Then the 1-step 
translation ikR from node i to node k can be direct-
ly obtained in the phrase table. 
Define operator ?  to denote the calculation of 
relation R. Then 2-step translation ijR  from node i 
to node j can be obtained with the following for-
mula.  
ij ik kjR R R= ?                           (4) 
We use |0 ( | )tR k i  to denote a t-step translation 
relation from node i to node k. In order to calculate 
the translation relations efficiently, we use a ma-
trix A to represent the graph. A t step translation 
probability can be denoted with the following for-
mula. 
526
|0 ( | ) [ ]
t
t ikP k i A=                         (5) 
where A is a matrix whose i,k-th element is ikR . 
4.1 Framework of Random Walk Approach 
The overall framework of random walk for pivot-
based machine translation is shown in Figure 2. 
Before using random walk model, we have two 
phrase tables: source-pivot phrase table (SP phrase 
table) and pivot-target phrase table (PT phrase ta-
ble). After applying the random walk approach, we 
can achieve two extended phrase table: extended 
source-pivot phrase table (S?P? phrase table) and 
extended pivot-target phrase table (P?T? phrase 
table). The goal of pivot-based SMT is to get a 
source-target phrase table (ST phrase table) via SP 
phrase table and PT phrase table.  
Our random walk was applied on SP phrase ta-
ble or PT phrase table separately. In next 2 sub-
sections, we will explain how the phrase transla-
tion probabilities and lexical weight are obtained 
with random walk model on the phrase table. 
Figure 3 shows some possible decoding pro-
cesses of random walk based pivot approach. In 
figure 3-a, the possible source-target phrase pair 
can be obtained directly via a pivot phrase, so it 
does not need a random walk model. In figure 3-b 
and figure 3-c, one candidate source-target phrase 
pair can be obtained by random walks on source-
pivot side or pivot-target side. Figure 3-d shows 
that the possible source-target can only by ob-
tained by random walks on source-pivot side and 
pivot-target side. 
4.2 Phrase Translation Probabilities 
For the translation probabilities, the binary relation 
R is the translation probabilities in the phrase table. 
The operator ?  is multiplication. According to 
formula 5, the random walk sums up the probabili-
ties of all paths of length t between the node i and 
k. 
Figure 2: Framework of random walk based pivot translation. The ST phrase table was generated by combin-
ing SP and PT phrase table through triangulation method. The phrase table with superscript ??? means that it 
was enlarged by random walk. 
 
S?P?
Phrase Table
P?T? 
Phrase Table
 SP 
Phrase Table
PT 
Phrase Table
ST 
Phrase Table
S?T?
Phrase Table
Pivot without 
random walk
Pivot with 
random walkrandom walk
random walk
Figure 3: Some possible decoding processes of random walk based pivot approach. The ? stands for the 
source phrase (S); the ? represents the pivot phrase (P) and the ? stands for the target phrase (T). 
 
(a) Pivot without  
       random walk 
S P T 
(d) Random walk on   
     both sides 
S P T 
(b) Random walk on  
      source-pivot side 
S P T 
(c) Random walk on 
      pivot-target side 
S P T 
527
Take source-to-pivot phrase graph as an exam-
ple; denote matrix A contains s+p nodes (s source 
phrases and p pivot phrases) to represent the trans-
lation graph.  
( ) ( )ij s p s p
A g
+ ? +
? ?= ? ?                         (6) 
where ijg  is the i,j-th elements of matrix A. 
We can split the matrix A into 4 sub-matrixes: 
0
0
s s sp
ps p p
A
A
A
?
?
? ?
= ? ?
? ?
                      (7) 
where the sub-matrix [ ]sp ik s pA p ?=  represents the 
translation probabilities from source to pivot lan-
guage, and psA  represents the similar meaning. 
Take 3 steps walks as an example: 
Step1: 
0
0
s s sp
ps p p
A
A
A
?
?
? ?
= ? ?
? ?
 
Step2: 
2
0
0
sp ps s p
p s ps sp
A A
A
A A
?
?
?? ?
= ? ??? ?
 
Step3: 
3
0
0
s s sp ps sp
ps sp ps p p
A A A
A
A A A
?
?
? ?? ?
= ? ?? ?? ?
 
For the 3 steps example, each step performs a 
translation process in the form of matrix?s self-
multiplication.  
1. The first step means the translation from 
source language to pivot language. The matrix 
A is derived from the phrase table directly and 
each element in the graph indicates a transla-
tion rule in the phrase table.  
2. The second step demonstrates a procedure: S-
P-S?. With 2 steps random walks, we can find 
the synonymous phrases, and this procedure is 
analogous to paraphrasing (Bannard and 
Callison-Burch, 2005). For the example shown 
in  figure 1 as an example, the hidden relation 
between ???? henkekou? and ?????
feichanghaochi? can be found through Step 2. 
3. The third step describes the following proce-
dure: S-P-S?-P?. An extended source-pivot 
phrase table is generated by 3-step random 
walks. Compared with the initial phrase table 
in Step1, although the number of phrases is 
not increased, the relations between phrase 
pairs are increased and more translation rules 
can be obtained. Still for the example in Fig-
ure 1 , the hidden relation between ????
henkekou? and ?really delicious? can be gen-
erated in Step 3. 
4.3 Lexical Weights 
To build a translation graph, the two sets of phrase 
translation probabilities are represented in the 
phrase tables. However, the two lexical weights 
are not presented in the graph directly. To deal 
with this, we should conduct a word alignment 
random walk model to obtain a new alignment a 
after t steps. For the computation of lexical 
weights, the relation R can be expressed as the 
word alignment in the phrase table. The operator 
?  can be induced with the following formula. 
1 2{( , ) | : ( , ) & ( , ) }a x y p x z a z y a= ? ? ?         (8) 
where a1 and a2 represent the word alignment 
information inside the phrase pairs ( , )x y  and 
( , )y z respectively. An example of word 
alignment inducing is shown in Figure 4. With a 
new word alignment, the two lexical weights can 
be calculated by formula 2 and formula 3. 
Figure 4: An example of word alignment induction with 3 steps random walks 
?   ??   ?   ?   ? 
could   you   fill   out   this   form ?   ?   ??   ??   ?? 
please   fill   out   this   form 
?   ??   ?   ?   ? 
could   you   fill   out   this   form 
step 1 
step 2 
step 3 
528
5 Experiments 
5.1 Translation System and Evaluation Met-
ric 
In our experiments, the word alignment was ob-
tained by GIZA++ (Och and Ney, 2000) and the 
heuristics ?grow-diag-final? refinement rule. 
(Koehn et al, 2003). Our translation system is an 
in-house phrase-based system using a log-linear 
framework including a phrase translation model, a 
language model, a lexicalized reordering model, a 
word penalty model and a phrase penalty model, 
which is analogous to Moses (Koehn et al, 2007). 
The baseline system is the triangulation method 
based pivot approach (Wu and Wang, 2007).  
To evaluate the translation quality, we used 
BLEU (Papineni et al, 2002) as our evaluation 
metric. The statistical significance using 95% con-
fidence intervals were measured with paired boot-
strap resampling (Koehn, 2004). 
5.2 Experiments on Europarl 
5.2.1. Data sets 
We mainly test our approach on Europarl1
We perform our experiments on different trans-
lation directions and via different pivot languages. 
As a most widely used language in the world 
(Mydans, 2011), English was used as the pivot 
language for granted when carrying out experi-
ments on different translation directions. For trans-
lating Portuguese to Swedish, we also tried to 
perform our experiments via different pivot lan-
 corpus, 
which is a multi-lingual corpus including 21 Euro-
pean languages. Due to the size of the data, we 
only select 11 languages which were added to 
Europarl from 04/1996 or 01/1997, including Dan-
ish (da), German (de), Greek (el), English (en), 
Spanish (es), Finnish (fi), French (fr), Italian (it) 
Dutch (nl) Portuguese (pt) and Swedish (sv). In 
order to avoid a trilingual scenario, we split the 
training corpus into 2 parts by the year of the data: 
the data released in odd years were used for train-
ing source-pivot model and the data released in 
even years were used for training pivot-target 
model.  
                                                          
1 http://www.statmt.org/europarl/ 
guages. Table 1 and Table 2 summarized the train-
ing data. 
 
Language 
Pairs  
(src-pvt) 
Sentence 
Pairs # 
Language 
Pairs 
(pvt-tgt) 
Sentence 
Pairs # 
da-en 974,189 en-da 953,002 
de-en 983,411 en-de 905,167 
el-en 609,315 en-el 596,331 
es-en 968,527 en-es 961,782 
fi-en 998,429 en-fi 903,689 
fr-en 989,652 en-fr 974,637 
it-en 934,448 en-it 938,573 
nl-en 982,696 en-nl 971,379 
pt-en 967,816 en-pt 960,214 
sv-en 960,631 en-sv 869,254 
 
Table1. Training data for experiments using English as 
the pivot language. For source-pivot (src-pvt; xx-en) 
model training, the data of odd years were used. Instead 
the data of even years were used for pivot-target (pvt-
src; en-xx) model training. 
 
 
Language 
Pairs  
(src-pvt) 
Sentence 
Pairs # 
Language 
Pairs 
(pvt-tgt) 
Sentence 
Pairs # 
pt-da 941,876 da-sv 865,020 
pt-de 939,932 de-sv 814,678 
pt-el 591,429 el-sv 558,765 
pt-es 934,783 es-sv 827,964 
pt-fi 950,588 fi-sv 872,182 
pt-fr 954,637 fr-sv 860,272 
pt-it 900,185 it-sv 813,000 
pt-nl 945,997 nl-sv 864,675 
 
Table2. Training data for experiments via different piv-
ot languages. For source-pivot (src-pvt; pt-xx) model 
training, the data of odd years were used. Instead the 
data of even years were used for pivot-target (pvt-src; 
xx-sv) model training. 
 
Test Set Sentence # Reference # 
WMT06 2,000 1 
WMT07 2,000 1 
WMT08 2,000 1 
 
Table3. Statistics of test sets. 
529
 
Several test sets have been released for the 
Europarl corpus. In our experiments, we used 
WMT20062, WMT20073 and WMT20084 as our 
test data. The original test data includes 4 lan-
guages and extended versions with 11 languages 
of these test sets are available by the EuroMatrix5
5.2.2. Experiments on Different Translation 
Directions 
  
project. Table 3 shows the test sets. 
We build 180 pivot translation systems6
The baseline system was built following the tra-
ditional triangulation pivot approach. Table 4 lists 
the results on Europarl training data. Limited by 
 (including 
90 baseline systems and 90 random walk based 
systems) using 10 source/target languages and 1 
pivot language (English).  
                                                          
2 http://www.statmt.org/wmt06/shared-task/ 
3 http://www.statmt.org/wmt07/shared-task.html 
4 http://www.statmt.org/wmt08/shared-task.html 
5 http://matrix.statmt.org/test_sets/list 
6 Given N languages, a total of N*(N-1) SMT systems should 
be build to cover the translation between each language.  
the length of the paper, we only show the results 
on WMT08, the tendency of the results on 
WMT06 and WMT07 is similar to WMT08. 
Several observations can be made from the table.  
1. In all 90 language pairs, our method achieves 
general improvements over the baseline system.  
2. Among 90 language pairs, random walk 
based approach is significantly better than the 
baseline system in 75 language pairs. 
3. The improvements of our approach are not 
equal in different translation directions. The im-
provement ranges from 0.06 (it-es) to 1.21 (pt-da). 
One possible reason is that the performance is re-
lated with the source and target language. For ex-
ample, when using Finnish as the target language, 
the improvement is significant over the baseline. 
This may be caused by the great divergence be-
tween Uralic language (Finnish) and Indo-
European language (the other European language 
in Table4). From the table we can find that the 
translation between languages in different lan-
guage family is worse than that in some language 
family. But our random walk approach can im-
 TGT 
SRC 
da de el es fi fr it nl pt sv 
Baseline 
RW 
da - 
19.83 
20.15* 
20.46 
21.02* 
27.59 
28.29* 
14.76 
15.63* 
24.11 
24.71* 
20.49 
20.82* 
22.26 
22.57* 
24.38 
24.88* 
28.33 
28.87* 
Baseline 
RW 
de 
23.35 
23.69* 
- 
19.83 
20.05 
26.21 
26.70* 
12.72 
13.57* 
22.43 
22.78* 
18.82 
19.32* 
23.74 
24.11* 
23.05 
23.35* 
21.17 
21.27 
Baseline 
RW 
el 
23.24 
23.82* 
18.12 
18.49* 
- 
32.28 
32.48 
13.31 
14.08* 
27.35 
27.67* 
23.19 
23.63* 
20.80 
21.26* 
27.62 
27.86 
22.70 
23.15* 
Baseline 
RW 
es 
25.34 
26.07* 
19.67 
20.17* 
27.24 
27.52 
- 
13.93 
14.61* 
32.91 
33.16 
27.67 
27.92 
22.37 
22.85* 
34.73 
34.93 
24.83 
25.50* 
Baseline 
RW 
fi 
18.29 
18.63* 
13.20 
13.40 
14.72 
15.00* 
20.17 
20.48* 
- 
17.52 
17.84* 
14.76 
15.01 
15.50 
16.04* 
17.30 
17.68* 
16.63 
16.79 
Baseline 
RW 
fr 
25.67 
26.51* 
20.02 
20.45* 
26.58 
26.75 
37.50 
37.80* 
13.90 
14.75* 
- 
28.51 
28.71 
22.65 
23.33* 
33.81 
33.93 
24.64 
25.59* 
Baseline 
RW 
it 
22.63 
23.27* 
17.81 
18.40* 
24.24 
24.66* 
34.36 
35.42* 
13.20 
14.11* 
30.16 
30.48* 
- 
21.37 
21.81* 
30.84 
30.92* 
22.12 
22.64* 
Baseline 
RW 
nl 
22.49 
22.76 
19.86 
20.45* 
18.56 
19.10* 
24.69 
25.19* 
11.96 
12.63* 
21.48 
22.05* 
18.36 
18.67* 
- 
21.71 
22.13* 
19.83 
22.17* 
Baseline 
RW 
pt 
24.08 
25.29* 
19.11 
19.83* 
25.30 
26.20* 
36.59 
37.13* 
13.33 
14.21* 
32.47 
32.78* 
28.08 
28.44* 
21.52 
22.46* 
- 
22.90 
23.90* 
Baseline 
RW 
sv 
31.24 
31.75* 
20.26 
20.74* 
22.06 
22.59* 
29.21 
29.87* 
15.39 
16.13* 
25.63 
26.18* 
21.25 
21.81* 
22.30 
22.62* 
25.60 
26.09* 
- 
Table4. Experimental results on Europarl with different translation directions (BLEU% on WMT08). 
RW=Random Walk. * indicates the results are significantly better than the baseline (p<0.05). 
530
prove the performance of translations between dif-
ferent language families. 
5.2.3. Experiments via Different Pivot Lan-
guages 
In addition to using English as the pivot language, 
we also try some other languages as the pivot 
language. In this sub-section, experiments were 
carried out from translating Portuguese to Swedish 
via different pivot languages.  
Table 5 summarizes the BLEU% scores of dif-
ferent pivot language when translating from Por-
tuguese to Swedish. Similar to Table 4, our 
approach still achieves general improvements over 
the baseline system even if the pivot language has 
been changed. From the table we can see that for 
most of the pivot language, the random walk based 
approach gains more than 1 BLEU score over the 
baseline. But when using Finnish as the pivot lan-
guage, the improvement is only 0.02 BLEU scores 
on WMT08. This phenomenon shows that the piv-
ot language can also influence the performance of 
random walk approach. One possible reason for 
the poor performance of using Finnish as the pivot 
language is that Finnish belongs to Uralic lan-
guage family, and the other languages belong to 
Indo-European family. The divergence between 
different language families led to a poor perfor-
mance. Thus how to select a best pivot language is 
our future work. 
The problem with random walk is that it will 
lead to a larger phrase table with noises. In this 
sub-section, a pre-pruning (before random walk) 
and a post-pruning (after random walk) method 
were introduced to deal with this problem.  
We used a naive pruning method which selects 
the top N phrase pairs in the phrase table. In our 
experiments, we set N to 20. For pre-pruning, we 
prune the SP phrase table and PT phrase table be-
fore applying random walks. Post-pruning means 
that we prune the ST phrase table after random 
walks. For the baseline system, we also apply a 
pruning method before combine the SP and PT 
phrase table. We test our pruning method on pt-en-
sv translation task. Table 6 shows the results. 
With a pre- and post-pruning method, the ran-
dom walk approach is able to achieve further im-
provements. Our approach achieved BLEU scores 
of 25.11, 24.69 and 24.34 on WMT06, WMT07 
and WMT08 respectively, which is much better 
than the baseline and the random walk approach 
with pruning.  Moreover, the size of the phrase 
table is about half of the no-pruning method. 
When adopting a post-pruning method, the per-
formance of translation did not improved signifi-
cantly over the pre-pruning, but the scale of the 
phrase table dropped to 69M, which is only about 
2 times larger than the triangulation method. 
Phrase table pruning is a key work to improve 
the performance of random walk. We plan to ex-
plore more approaches for phrase table pruning. 
E.g. using significance test (Johnson et al, 2007) 
or monolingual key phrases (He et al, 2009) to 
filter the phrase table. 
 
 
Table5. Experimental results on translating from Portu-
guese to Swedish via different pivot language. 
RW=Random Walk. * indicates the results are signifi-
cantly better than the baseline (p<0.05). 
 
 
Table6. Results of Phrase Table Filtering 
 
trans 
language 
WMT 
06 
WMT 
07 
WMT 
08 
Baseline 
RW 
pt-da-sv 
23.40 
24.47* 
22.80 
24.21* 
22.49 
23.75* 
Baseline 
RW 
pt-de-sv 
22.72 
23.12* 
22.21 
23.26* 
21.76 
22.35* 
Baseline 
RW 
pt-el-sv 
22.53 
23.75* 
22.19 
23.22* 
21.37 
22.40* 
Baseline 
RW 
pt-en-sv 
23.54 
24.66* 
23.24 
24.22* 
22.90 
23.90* 
Baseline 
RW 
pt-es-sv 
23.58 
24.65* 
23.37 
24.10* 
22.80 
23.77* 
Baseline 
RW 
pt-fi-sv 
21.06 
21.17 
20.06 
20.42* 
20.26 
20.28 
Baseline 
RW 
pt-fr-sv 
23.55 
24.75* 
23.09 
24.15* 
22.89 
23.96* 
Baseline 
RW 
pt-it-sv 
23.65 
24.74* 
22.96 
24.18* 
22.79 
24.02* 
Baseline 
RW 
pt-nl-sv 
21.87 
23.06* 
21.83 
22.76* 
21.36 
22.29* 
 WMT 
06 
WMT 
07 
WMT 
08 
Phrase 
Pairs # 
Baseline 
+pruning 
23.54 
24.05
* 
23.24 
23.70
* 
22.90 
23.59
* 
46M 
32M 
RW 
+pre-pruning 
+post-pruning 
24.66 
25.11 
25.19
* 
24.22 
24.69 
24.79
* 
23.90 
24.34 
24.41
* 
215M 
109M 
69M 
531
5.3 Experiments on Spoken Language 
The European languages show various degrees of 
similarity to one another. In this sub-section, we 
consider translation from Chinese to Spanish with 
English as the pivot language. Chinese belongs to 
Sino-Tibetan Languages and English/Spanish be-
longs to Indo-European Languages, the gap be-
tween two languages is wide. 
A pivot task was included in IWSLT 2008 in 
which the participants need to translate Chinese to 
Spanish via English. A Chinese-English and an 
English-Spanish data were supplied to carry out 
the experiments. The entire training corpus was 
tokenized and lowercased. Table 7 and Table 8 
summarize the training data and test data. 
Table 9 shows the similar tendency with Table 4. 
The random walk models achieved BLEU% scores 
32.09, which achieved an absolute improvement of 
2.08 percentages points on BLEU over the base-
line.   
 
Corpus 
Sentence 
pair # 
Source 
word # 
Target 
word # 
CE 20,000 135,518 182,793 
ES 19,972 153,178 147,560 
 
Table 7: Training Data of IWSLT2008 
 
Test Set Sentence # Reference # 
IWSLT08 507 16 
 
Table8. Test Data of IWSLT2008 
 
System BLEU% phrase pairs # 
Baseline 30.01 143,790 
+pruning 30.25 108,407 
RW 31.57 2,760,439 
+pre-pruning 31.99 1,845,648 
+post-pruning 32.09* 1,514,694 
 
Table9. Results on IWSLT2008 
5.4 Experiments on Web Data 
The setting with Europarl data is quite artificial as 
the training data for directly translating between 
source and target actually exists in the original 
data sets. The IWSLT data set is too small to rep-
resent the real scenario. Thus we try our experi-
ment on a more realistic scenario: translating from 
Chinese to Japanese via English with web crawled 
data. 
All the training data were crawled on the web. 
The scale of Chinese-English and English-
Japanese is 10 million respectively. The test set 
was built in house with 1,000 sentences and 4 ref-
erences. 
 
System BLEU% phrase pairs # 
Baseline 28.76 4.5G 
+pruning 28.90 273M 
RW 29.13 46G 
+pre-pruning 29.44 11G 
+post-pruning 29.51* 3.4G 
 
Table10. Results on Web Data 
 
Table 10 lists the results on web data. From the 
table we can find that the random walk model can 
achieve an absolute improvement of 0.75 percent-
ages points on BLEU over the baseline.  
In this subsection, the training data contains 
parallel sentences with different domains. And the 
two training corpora (Chinese-English and Eng-
lish-Japanese) are typically very different. It 
means that our random walk approach is robust in 
the realistic scenario. 
6 Discussions 
The random walk approach mainly improves the 
performance of pivot translation in two aspects: 
reduces the OOVs and provides more hypothesis 
phrases for decoding.  
6.1 OOV 
Out-of-vocabulary (OOV 7
We count the OOVs when decoding with trian-
gulation model and random walk model on 
IWSLT2008 data. The statistics shows that when 
using triangulation model, there are 11% OOVs 
when using triangulation model, compared with 
9.6% when using random walk model. Less OOV 
often lead to a better result. 
) terms cause serious 
problems for machine translation systems (Zhang 
et al, 2005). The random walk model can reduce 
the OOVs. As illustrated in Figure 1, the Chinese 
phrase ????henkekou? cannot be connected to 
any Spanish phrase, thus it is a OOV term.  
                                                          
7 OOV refer to phrases here. 
532
6.2 Hypothesis Phrases 
To illustrate how the random walk method helps 
improve the performance of machine translation, 
we illustrate an example as follows: 
 
- Source: ? ? ? ?? 
              wo xiang yao zhentou 
- Baseline trans: Quiero almohada 
- Random Walk trans: Quiero una almohada 
 
For translating a Chinese sentence ??????
wo xiang yao zhentou? to Spanish, we can get two 
candidate translations. In this case, the random 
walk translation is better than the baseline system. 
The key phrase in this sentence is ??? zhentou?, 
figure 5 shows the extension process. In this case, 
the article ?a? is hidden in the source-pivot phrase 
table. The same situation often occurs in articles 
and prepositions. Random walk is able to discover 
the hidden relations (hypothesis phrases) among 
source, pivot and target phrases. 
 
 
 
 
 
 
 
 
 
 
7 Conclusion and Future Work 
In this paper, we proposed a random walk method 
to improve pivot-based statistical machine transla-
tion. The random walk method can find implicit 
relations between phrases in the source and target 
languages. Therefore, more source-target phrase 
pairs can be obtained than conventional pivot-
based method. Experimental results show that our 
method achieves significant improvements over 
the baseline on Europarl corpus, spoken language 
data and the web data.  
A critical problem in the approach is the noise 
that may bring in. In this paper, we used a simple 
filtering to reduce the noise. Although the filtering 
method is effective, other method may work better. 
In the future, we plan to explore more approaches 
for phrase table pruning. 
Acknowledgments 
We would like to thank Jianyun Nie, Muyun Yang 
and Lemao Liu for insightful discussions, and 
three anonymous reviewers for many invaluable 
comments and suggestions to improve our paper. 
This work is supported by National Natural Sci-
ence Foundation of China (61100093), and the 
Key Project of the National High Technology Re-
search and Development Program of China 
(2011AA01A207). 
References  
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of the 43rd Annual Meeting of the 
Association for Computational Linguistics, pages 
597-604 
Sergey Brin and Lawrence Page. 1998. The Anatomy of 
a Large-Scale Hypertextual Web Search Engine. In 
Proceedings of the Seventh International World 
Wide Web Conference  
Trevor Cohn and Mirella Lapata. 2007. Machine Trans-
lation by Triangulation: Make Effective Use of Mul-
ti-Parallel Corpora. In Proceedings of 45th Annual 
Meeting of the Association for Computational Lin-
guistics, pages 828-735. 
Marta R. Costa-juss?, Carlos Henr?quez, and Rafael E. 
Banchs. 2011. Enhancing Scarce-Resource Language 
Translation through Pivot Combinations. In Proceed-
ings of the 5th International Joint Conference on 
Natural Language Processing, pages 1361-1365 
Nick Craswell and Martin Szummer. 2007. Random 
Walks on the Click Graph. In Proceedings of the 
30th annual international ACM SIGIR conference on 
Research and development in information retrieval, 
pages 239-246 
Yiming Cui, Conghui Zhu, Xiaoning Zhu, Tiejun Zhao 
and Dequan Zheng. 2013. Phrase Table Combination 
Deficiency Analyses in Pivot-based SMT. In Pro-
ceedings of 18th International Conference on Appli-
cation of Natural Language to Information Systems, 
pages 355-358. 
Kevin Duh, Katsuhito Sudoh, Xianchao Wu, Hajime 
Tsukada and Masaaki Nagata. 2011. Generalized 
Minimum Bayes Risk System Combination. In Pro-
ceedings of the 5th International Joint Conference 
on Natural Language Processing, pages 1356?1360 
Jes?s Gonz?lez-Rubio, Alfons Juan and Francisco 
Casacuberta. 2011. Minimum Bayes-risk System 
Figure 5: Phrase extension process. The dotted line 
indicates an implicit relation in the phrase table. 
??? 
ge zhentou 
?? 
zhentou 
pillow 
a pillow 
almohada 
una 
almohada 
533
Combination. In Proceedings of the 49th Annual 
Meeting of the Association for Computational Lin-
guistics, pages 1268?1277 
Zhongjun He, Yao Meng, Yajuan L?, Hao Yu and Qun 
Liu. 2009. Reducing SMT Rule Table with Mono-
lingual Key Phrase. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 121-
124 
Howard Johnson, Joel Martin, George Foster, and Ro-
land Kuhn. 2007. Improving  translation quality by 
discarding most of the phrase table. In Proceedings 
of the 2007 Joint Conference on Empirical Methods 
in Natural Language Processing and Computational 
Natural Language Learning, pages 967?975. 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical Phrase-Based Translation. In HLT-NAACL: 
Human Language Technology Conference of the 
North American Chapter of the Association for 
Computational Linguistics, pages 127-133 
Philipp Koehn. 2004. Statistical significance tests for 
machine translation evaluation. In Proceedings of the 
2004 Conference on Empirical Methods in Natural 
Language Processing (EMNLP), pages 388?395. 
Philipp Koehn. 2005. Europarl: A Parallel Corpus for 
Statistical Machine Translation. In Proceedings of 
MT Summit X, pages 79-86. 
Philipp Koehn, Hieu Hoang, Alexanda Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Rich-
ard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
In Proceedings of the 45th Annual Meeting of the 
Association for Computational Linguistics, demon-
stration session, pages 177?180. 
Franz Josef Och and Hermann Ney. 2000. A compari-
son of alignment models for statistical machine 
translation. In Proceedings of the 18th International 
Conference on Computational Linguistics, pages 
1086?1090 
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. In Proceedings 
of the 40th Annual Meeting of the Association for 
Computation Linguistics, pages 311-319 
Karl Pearson. 1905. The Problem of the Random Walk. 
Nature, 27(1865):294 
Mydans, Seth. 2011. Across cultures, English is the 
word. New York Times. 
Martin Szummer and Tommi Jaakkola. 2002. Partially 
Labeled Classification with Markov Random Walks. 
In Advances in Neural Information Processing Sys-
tems, pages 945-952 
Kristina Toutanova, Christopher D. Manning and An-
drew Y. Ng. 2004. Learning Random Walk Models 
for Inducting Word Dependency Distributions. In 
Proceedings of the 21st International Conference on 
Machine Learning.  
Masao Utiyama and Hitoshi Isahara. 2007. A Compari-
son of Pivot Methods for Phrase-Based Statistical 
Machine Translation. In Proceedings of Human 
Language Technology: the Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics, pages 484-491 
Masao Utiyama, Andrew Finch, Hideo Okuma, Michael 
Paul, Hailong Cao, Hirofumi Yamamoto, Keiji Ya-
suda, and Eiichiro Sumita. 2008. The NICT/ATR 
speech Translation System for IWSLT 2008. In Pro-
ceedings of the International Workshop on Spoken 
Language Translation, pages 77-84 
Haifeng Wang, Hua Wu, Xiaoguang Hu, Zhanyi Liu, 
Jianfeng Li, Dengjun Ren, and Zhengyu Niu. 2008. 
The TCH Machine Translation System for IWSLT 
2008. In Proceedings of the International Workshop 
on Spoken Language Translation, pages 124-131 
Hua Wu and Haifeng Wang. 2007. Pivot Language Ap-
proach for Phrase-Based Statistical Machine Transla-
tion. In Proceedings of 45th Annual Meeting of the 
Association for Computational Linguistics, pages 
856-863.  
Hua Wu and Haifeng Wang. 2009. Revisiting Pivot 
Language Approach for Machine Translation. In 
Proceedings of the 47th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 4th 
IJCNLP of the AFNLP, pages 154-162 
Ying Zhang, Fei Huang, Stephan Vogel. 2005. Mining 
translations of OOV terms from the web through 
cross-lingual query expansion. In Proceedings of the 
27th ACM SIGIR. pages 524-525 
 
 
534
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 57?67,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Policy Learning for Domain Selection in an Extensible Multi-domain
Spoken Dialogue System
Zhuoran Wang
Mathematical & Computer Sciences
Heriot-Watt University
Edinburgh, UK
zhuoran.wang@hw.ac.uk
Hongliang Chen, Guanchun Wang
Hao Tian, Hua Wu
?
, Haifeng Wang
Baidu Inc., Beijing, P. R. China
SurnameForename@baidu.com
?
wu hua@baidu.com
Abstract
This paper proposes a Markov Decision
Process and reinforcement learning based
approach for domain selection in a multi-
domain Spoken Dialogue System built on
a distributed architecture. In the proposed
framework, the domain selection prob-
lem is treated as sequential planning in-
stead of classification, such that confir-
mation and clarification interaction mech-
anisms are supported. In addition, it is
shown that by using a model parameter ty-
ing trick, the extensibility of the system
can be preserved, where dialogue com-
ponents in new domains can be easily
plugged in, without re-training the domain
selection policy. The experimental results
based on human subjects suggest that the
proposed model marginally outperforms a
non-trivial baseline.
1 Introduction
Due to growing demand for natural human-
machine interaction, over the last decade Spo-
ken Dialogue Systems (SDS) have been increas-
ingly deployed in various commercial applications
ranging from traditional call centre automation
(e.g. AT&T ?Lets Go!? bus information sys-
tem (Williams et al., 2010)) to mobile personal
assistants and knowledge navigators (e.g. Ap-
ple?s Siri
R
?
, Google Now
TM
, Microsoft Cortana,
etc.) or voice interaction for smart household ap-
pliance control (e.g. Samsung Evolution Kit for
Smart TVs). Furthermore, latest progress in open-
vocabulary Automatic Speech Recognition (ASR)
is pushing SDS from traditional single-domain in-
formation systems towards more complex multi-
domain speech applications, of which typical ex-
amples are those voice assistant mobile applica-
tions.
Recent advances in SDS have shown that sta-
tistical approaches to dialogue management can
result in marginal improvement in both the nat-
uralness and the task success rate for domain-
specific dialogues (Lemon and Pietquin, 2012;
Young et al., 2013). State-of-the-art statistical
SDS treat the dialogue problem as a sequential
decision making process, and employ established
planning models, such as Markov Decision Pro-
cesses (MDPs) (Singh et al., 2002) or Partially Ob-
servable Markov Decision Processes (POMDPs)
(Thomson and Young, 2010; Young et al., 2010;
Williams and Young, 2007), in conjunction with
reinforcement learning techniques (Jur?c???cek et al.,
2011; Jur?c???cek et al., 2012; Ga?si?c et al., 2013a)
to seek optimal dialogue policies that maximise
long-term expected (discounted) rewards and are
robust to ASR errors.
However, to the best of our knowledge, most of
the existing multi-domain SDS in public use are
rule-based (e.g. (Gruber et al., 2012; Mirkovic
and Cavedon, 2006)). The application of statistical
models in multi-domain dialogue systems is still
preliminary. Komatani et al. (2006) and Nakano
et al. (2011) utilised a distributed architecture (Lin
et al., 1999) to integrate expert dialogue systems in
different domains into a unified framework, where
a central controller trained as a data-driven clas-
sifier selects a domain expert at each turn to ad-
dress user?s query. Alternatively, Hakkani-T?ur et
al. (2012) adopted the well-known Information
State mechanism (Traum and Larsson, 2003) to
construct a multi-domain SDS and proposed a dis-
criminative classification model for more accurate
state updates. More recently, Ga?si?c et al. (2013b)
proposed that by a simple expansion of the kernel
function in Gaussian Process (GP) reinforcement
learning (Engel et al., 2005; Ga?si?c et al., 2013a),
one can adapt pre-trained dialogue policies to han-
dle unseen slots for SDS in extended domains.
In this paper, we use a voice assistant applica-
57
User Interface Manager
ASR
User Intention
Identier Central Controller
SLU NLG
Domain Expert
(Travel Info)
SLU NLG
Domain Expert
(Restaurant Search)
SLU NLG
Domain Expert
(Movie Search)
SLU NLG
Domain Expert
(etc.)
Web 
Search
Weather
Report
QA
etc.
Ou
t-o
f-d
om
ain
 Se
rvi
ce
s
Service
Ranker
Mobile Devices
Flight Ticket 
Booking
Train Ticket 
Booking
Hotel  
Booking
speech
text
text, clicks
query,
intention label,
condence
TTS Web PageRendering etc.
Figure 1: The distributed architecture of the voice assistant system (a simplified illustration).
tion (similar to Apple?s Siri but in Chinese lan-
guage) as an example to demonstrate a novel
MDP-based approach for central interaction man-
agement in a complex multi-domain dialogue sys-
tem. The voice assistant employs a distributed ar-
chitecture similar to (Lin et al., 1999; Komatani et
al., 2006; Nakano et al., 2011), and handles mixed
interactions of multi-turn dialogues across differ-
ent domains and single-turn queries powered by
a collection of information access services (such
as web search, Question Answering (QA), etc.).
In our system, the dialogues in each domain are
managed by an individual domain expert SDS, and
the single-turn services are used to handle those
so-called out-of-domain requests. We use fea-
turised representations to summarise the current
dialogue states in each domain (see Section 3 for
more details), and let the central controller (the
MDP model) choose one of the following system
actions at each turn: (1) addressing user?s query
based on a domain expert, (2) treating it as an
out-of-domain request, (3) asking user to confirm
whether he/she wants to continue a domain ex-
pert?s dialogue or to switch to out-of-domain ser-
vices, and (4) clarifying user?s intention between
two domains. The Gaussian Process Temporal
Difference (GPTD) algorithm (Engel et al., 2005;
Ga?si?c et al., 2013a) is adopted here for policy op-
timisation based on human subjects, where a pa-
rameter tying trick is applied to preserve the ex-
tensibility of the system, such that new domain
experts (dialogue systems) can be flexibly plugged
in without the need of re-training the central con-
troller.
Comparing to the previous classification-based
methods (Komatani et al., 2006; Nakano et al.,
2011), the proposed approach not only has the
advantage of action selection in consideration of
long-term rewards, it can also yield more robust
policies that allow clarifications and confirmations
to mitigate ASR and Spoken Language Under-
standing (SLU) errors. Our human evaluation re-
sults show that the proposed system with a trained
MDP policy achieves significantly better natural-
ness in domain switching tasks than a non-trivial
baseline with a hand-crafted policy.
The remainder of this paper is organised as
follows. Section 2 defines the terminology used
throughout the paper. Section 3 briefly overviews
the distributed architecture of our system. The
MDP model and the policy optimisation algorithm
are introduced in Section 4 and Section 5, respec-
tively. After this, experimental settings and eval-
uation results are described in Section 6. Finally,
we discuss some possible improvements in Sec-
tion 7 and conclude ourselves in Section 8.
2 Terminology
A voice assistant application provides a unified
speech interface to a collection of individual infor-
mation access systems. It aims to collect and sat-
isfy user requests in an interactive manner, where
58
different types of interactions can be involved.
Here we focus ourselves on two interaction scenar-
ios, i.e. task-oriented (multi-turn) dialogues and
single-turn queries.
According to user intentions, the dialogue inter-
actions in our voice assistant system can further be
categorised into different domains, of which each
is handled by a separate dialogue manager, namely
a domain expert. Example domains include travel
information, restaurant search, etc. In addition,
some domains in our system can be further de-
composed into sub-domains, e.g. the travel in-
formation domain consists of three sub-domains:
flight ticket booking, train ticket booking and hotel
reservation. We use an integrated domain expert to
address queries in all its sub-domains, so that rel-
evant information can be shared across those sub-
domains to allow intelligent induction in the dia-
logue flow.
For convenience of future reference, we call
those single-turn information access systems out-
of-domain services or simply services for short.
The services integrated in our system include web
search, semantic search, QA, system command ex-
ecution, weather report, chat-bot, and many more.
3 System Architecture
The voice assistant system introduced in this pa-
per is built on a distributed architecture (Lin et al.,
1999), as shown in Figure 1, where the dialogue
flow is processed as follows. Firstly, a user?s query
(either an ASR utterance or directly typed in text)
is passed to a user intention identifier, which la-
bels the raw query with a list of intention hypothe-
ses with confidence scores. Here an intention label
could be either a domain name or a service name.
After this, the central controller distributes the raw
query together with its intention labels and confi-
dence scores to all the domain experts and the ser-
vice modules, which will attempt to process the
query and return their results to the central con-
troller.
The domain experts in the current implementa-
tion of our system are all rule-based SDS follow-
ing the RavenClaw framework proposed in (Bo-
hus and Rudnicky, 2009). When receiving a query,
a domain expert will use its own SLU module to
parse the utterance or text input and try to update
its dialogue state in consideration of both the SLU
output and the intention labels. If the dialogue
state in the domain expert can be updated given
the query, it will return its output, internal ses-
sion record and a confidence score to the central
controller, where the output can be either a natu-
ral language utterance realised by its Natural Lan-
guage Generation (NLG) module or a set of data
records obtained from its database (if a database
search operation is triggered), or both. If the do-
main expert cannot update its state using the cur-
rent query, it will just return an empty result with
a low confidence score. Similar procedures ap-
ply to those out-of-domain services as well, but
there are no session records or confidence scores
returned. Finally, given all the returned informa-
tion, the central controller chooses, according to
its policy, the module (either a domain expert or a
service) whose results will be provided to the user.
When the central controller decides to pass a
domain expert?s output to the user, we regard the
domain expert as being activated. Also note here,
the updated state of a domain expert in a turn will
not be physically stored, unless the domain expert
is activated in that turn. This is a necessary mech-
anism to prevent an inactive domain expert being
misled by ambiguous queries in other domains.
In addition, we use a well-engineered priority
ranker to rank the services based on the num-
bers of results they returned as well as some prior
knowledge about the quality of their data sources.
When the central controller decides to show user
the results from an out-of-domain service, it will
choose the top one from the ranked list.
4 MDP Modelling of the Central Control
Process
The main focus of this paper is to seek a policy for
robustly switching the control flow among those
domain experts and services (the service ranker in
practice) during a dialogue, where the user may
have multiple or compound goals (e.g. booking a
flight ticket, booking a restaurant in the destina-
tion city and checking the weather report of the
departure or destination city).
In order to make the system robust to ASR er-
rors or ambiguous queries, the central controller
should also have basic dialogue abilities for confir-
mation and clarification purposes. Here we define
the confirmation as an action of asking whether a
user wants to continue the dialogue in a certain do-
main. If the system receives a negative response at
this point, it will switch to out-of-domain services.
On the other hand, the clarification action is de-
59
fined between domains, in which case, the system
will explicitly ask the user to choose between two
domain candidates before continuing the dialogue.
Due to the confirmation and clarification mech-
anisms defined above, the central controller be-
comes a sequential decision maker that must take
the overall smoothness of the dialogue into ac-
count. Therefore, we propose an MDP-based ap-
proach for learning an optimal central control pol-
icy in this section.
The potential state space of our MDP is huge,
which in principle consists of the combinations of
all possible situations of the domain experts and
the out-of-domain services, therefore function ap-
proximation techniques must be employed to en-
able tractable computations. However, when de-
veloping such a complex application as the voice
assistant here, one also needs to take the extensi-
bility of the system into account, so that new do-
main experts can be easily integrated into the sys-
tem without major re-training or re-engineering of
the existing components. Essentially, it requires
the state featurisation and the central control pol-
icy learnt here to be independent of the number of
domain experts. In Section 4.3, we show that such
a property can be achieved by a parameter tying
trick in the definition of the MDP.
4.1 MDP Preliminaries
Let P
X
denote the set of probability distributions
over a set X . An MDP is defined as a five tuple
?S,A, T,R, ??, where the components are defined
as follows. S and A are the sets of system states
and actions, respectively. T : S ? A ? P
S
is the
transition function, and T (s
?
|s, a) defines the con-
ditional probability of the system transiting from
state s ? S to state s
?
? S after taking action
a ? A. R : S ? A ? P
R
is the reward function
with R(s, a) specifying the distribution of the im-
mediate rewards for the system taking action a at
state s. In addition, 0 ? ? ? 1 is the discount
factor on the summed sequence of rewards.
A finite-horizon MDP operates as follows. The
system occupies a state s and takes an action a,
which then will make it transit to a next state s
?
?
T (?|s, a) and receive a reward r ? R(s, a). This
process repeats until a terminal state is reached.
For a given policy pi : S ? A, the value
function V
pi
is defined to be the expected cumula-
tive reward, as V
pi
(s
0
) = E
[
?
n
t=0
?
t
r
t
|
s
t
,pi(s
t
)
]
,
where s
0
is the starting state and n is the plan-
ning horizon. The aim of policy optimisation is
to seek an optimal policy pi
?
that maximises the
value function. If T and R are given, in conjunc-
tion with a Q-function, the optimal value V
?
can
be expressed by recursive equations as Q(s, a) =
R(s, a) + ?
?
s
?
?S
T (s
?
|s, a)V
?
(s
?
) and V
?
(s) =
max
a?A
Q(s, a) (here we assume R(s, a) is de-
terministic), which can be solved by dynamic pro-
gramming (Bellman, 1957). For problems with
unknown T or R, such as dialogue systems, the
Q-values are usually estimated via reinforcement
learning (Sutton and Barto, 1998).
4.2 Problem Definition
Let D denote the set of the domain experts in our
voice assistant system, and s
d
be the current di-
alogue state of domain expert d ? D at a certain
timestamp. We also define s
o
as an abstract state to
describe the current status of those out-of-domain
services. Then mathematically we can represent
the central control process as an MDP, where its
state s is a joint set of the states of all the domain
experts and the services, as s = {s
d
}
d?D
? {s
o
}.
Four types of system actions are defined as fol-
lows.
? present(d): presenting the output of do-
main expert d to user;
? present ood(null): presenting the re-
sults of the top-ranked out-of-domain service
given by the service ranker;
? confirm(d): confirming whether user
wants to continue with domain expert d (or
to switch to out-of-domain services);
? clarify(d,d
?
): asking user to clarify
his/her intention between domains d and d
?
.
For convenience of notations, we use a(x) to
denote a system action of our MDP, where a ?
{present,present ood,confirm,clarify},
x ? {d,null, (d, d
?
)}
d,d
?
?D,d6=d
?
, x = null
only applies to present ood, and x = (d, d
?
)
only applies to clarify actions.
4.3 Function Approximation
Function approximation is a commonly used tech-
nique to estimate the Q-values when the state
space of the MDP is huge. Concretely, in our case,
we assume that:
Q(s, a(x)) = f(?(s, a(x)); ?) (1)
60
where ? : S ? A ? R
K
is a feature function
that maps a state-action pair to an K-dimensional
feature vector, and f : R
K
? R is a function of
?(s, a(x)) parameterised by ?. A frequent choice
of f is the linear function, as:
Q(s, a(x)) = ?
>
?(s, a(x)) (2)
After this, the policy optimisation problem be-
comes learning the parameter ? to approximate the
Q-values based on example dialogue trajectories.
However, a crucial problem with the standard
formulation in Eq. (2) is that the feature function
? is defined over the entire state and action spaces.
In this case, when a new domain expert is inte-
grated into the system, both the state space and the
action space will be changed, therefore one will
have to re-define the feature function and conse-
quentially re-train the model. In order to achieve
an extensible system, we make some simplifica-
tion assumptions and decompose the feature func-
tion as follows. Firstly, we let:
?(s, a(x)) = ?
a
(s
x
) (3)
=
?
?
?
?
?
?
?
?
pr
(s
d
) if a(x) =present(d)
?
ood
(s
o
) if a(x) =present ood()
?
cf
(s
d
) if a(x) =confirm(d)
?
cl
(s
d
, s
d
?
) if a(x) =clarify(d,d
?
)
where the feature function is reduced to only de-
pend on the state of the action?s operand, instead
of the entire system state. Then, we make those ac-
tions a(x) that have a same action type (a) but op-
erate different domain experts (x) share the same
parameter, i.e.:
Q(s, a(x)) = ?
>
a
?
a
(s
x
) (4)
This decomposition and parameter tying trick pre-
serves the extensibility of the system, because both
?
>
a
and ?
a
are independent of x, when there is a
new domain expert
?
d, we can directly substitute
its state s
?
d
into Eq. (3) and (4) to compute its cor-
responding Q-values.
4.4 Features
Based on the problem formulation in Eq. (3) and
(4), we shall only select high-level summary fea-
tures to sketch the dialogue state and dialogue his-
tory of each domain expert, which must be ap-
plicable to all domain experts, regardless of their
domain-specific characteristics or implementation
differences. Suppose that the dialogue states of the
# Feature Range
1
the number of unfilled
required slots of a domain
expert
{0, . . . ,M}
2
the number of filled required
slots of a domain expert
{0, . . . ,M}
3
the number of filled optional
slots of a domain expert
{0, . . . , L}
4
whether a domain expert has
executed a database search
{0, 1}
5
the confidence score
returned by a domain expert
[0, 1.2]
6
the total number of turns that
a domain expert has been
activated during a dialogue
Z
+
7
e
?t
a
where t
a
denotes the
relative turn of a domain
expert being last activated,
or 0 if not applicable
[0, 1]
8
e
?t
c
where t
c
denotes the
relative turn of a domain
expert being last confirmed,
or 0 if not applicable
[0, 1]
9
the summed confidence
score from the user intention
identifier of a query being
for out-of-domain services
[0, 1.2N ]
Table 1: A list of all features used in our model.
M and L respectively denote the maximum num-
bers of required and optional slots for the domain
experts. N is the maximum number of hypotheses
that the intention identifier can return. Z
+
stands
for the non-negative integer set.
domain experts can be represented as slot-value
pairs
1
, and for each domain there are required slots
and optional slots, where all required slots must
be filled before the domain expert can execute a
database search operation. The features investi-
gated in the proposed framework are listed in Ta-
ble 1.
Detailed featurisation in Eq. (3) is explained
as follows. For ?
pr
, we choose the first 8 fea-
tures plus a bias dimension that is always set to
1
This is a rather general assumption. Informally speak-
ing, for most task-oriented SDS, one can extract a slot-value
representation from their dialogue models, of which exam-
ples include the RavenClaw architecture (Bohus and Rud-
nicky, 2009), the Information State dialogue engine (Traum
and Larsson, 2003), MDP-SDS (Singh et al., 2002) or
POMDP-SDS (Thomson and Young, 2010; Young et al.,
2010; Williams and Young, 2007).
61
?1. Whilst, feature #9 plus a bias is used to de-
fine ?
ood
. All the features are used in ?
cf
, as to
do a confirmation, one needs to consider the joint
situation in and out of the domain. Finally, the
feature function for a clarification action between
two domains d and d
?
is defined as ?
cl
(s
d
, s
d
?
) =
exp{?|?
pr
(s
d
) ? ?
pr
(s
d
?
)|}, where we use | ? |
to denote the element-wise absolute of a vector
operand. The intuition here is that the more dis-
tinguishable the (featurised) states of two domain
experts are, the less we tend to clarify them.
For those domain experts that have multiple
sub-domains with different numbers of required
and optional slots, the feature extraction procedure
only applies to the latest active sub-domain.
In addition, note that, the confidence scores pro-
vided by the user intention identifier are only used
as features for out-of-domain services. This is be-
cause in the current version of our system, the con-
fidence estimation of the intention identifier for
domain-dependent dialogue queries is less reliable
due to the lack of context information. In contrast,
the confidence scores returned by the domain ex-
perts will be more informative at this point.
5 Policy Learning with GPTD
In traditional statistical SDS, dialogue policies are
usually trained using reinforcement learning based
on simulated dialogue trajectories (Schatzmann
et al., 2007; Keizer et al., 2010; Thomson and
Young, 2010; Young et al., 2010). Although the
evaluation of the simulators themselves could be
an arguable issue, there are various advantages,
e.g. hundreds of thousands of data examples can
be easily generated for training and initial policy
evaluation purposes, and different reinforcement
learning models can be compared without incur-
ring notable extra costs.
However, for more complex multi-domain SDS,
particularly a voice assistant application like ours
that aims at handling very complicated (ideally
open-domain) dialogue scenarios, it would be dif-
ficult to develop a proper simulator that can rea-
sonably mimic real human behaviours. There-
fore, in this work, we learn the central control
policy directly with human subjects, for which
the following properties of the learning algorithm
are required. Firstly and most importantly, the
learner must be sample-efficient as the data collec-
tion procedure is costly. Secondly, the algorithm
should support batch reinforcement learning. This
is because when using function approximation, the
learning process may not strictly converge, and the
quality of the sequence of generated policies tends
to oscillate after a certain number of improving
steps at the beginning (Bertsekas and Tsitsiklis,
1996). If online reinforcement learning is used,
we will be unable to evaluate the generated policy
after each update, and hence will not know which
policy to keep for the final evaluation. Therefore,
we do a batch policy update and iterate the learn-
ing process for a number of batches, such that the
data collection phase in a new iteration yields an
evaluation of the policy obtained from the previ-
ous iteration at the same time.
To fulfill the above two requirements, the Gaus-
sian Process Temporal Difference (GPTD) algo-
rithm (Engel et al., 2005) is a proper choice, due to
its sample efficiency (Fard et al., 2011) and batch
learning ability (Engel et al., 2005), as well as its
previous success in dialogue policy learning with
human subjects (Ga?si?c et al., 2013a). Note that,
GPTD can also admit recursive (online) compu-
tations, but here we focus ourselves on the batch
version.
A Gaussian Process (GP) is a generative model
of Bayesian inference that can be used for func-
tion regression, and has the superiority of obtain-
ing good posterior estimates with just a few obser-
vations (Rasmussen and Williams, 2006). GPTD
models the Q-function as a zero mean GP which
defines correlations in different parts of the fea-
turised state and action spaces through a kernel
function ?, as:
Q(s, a(x)) ? GP(0, ?((s
x
, a), (s
x
, a))) (5)
Given a sequence of t state-action pairs X
t
=
[(s
0
, a
0
(x
0
)), . . . , (s
t
, a
t
(x
t
))] from a collection
of dialogues and their corresponding immedi-
ate rewards r
t
= [r
0
, . . . , r
t
], the posterior of
Q(s, a(x)) for an arbitrary new state-action pair
(s, a(x)) can be computed as:
Q(s, a(x))|
X
t
,r
t
? N
(
?
Q(s, a(x)), cov (s, a(x))
)
(6)
?
Q(s, a(x)) = k
t
(s
x
, a)
>
H
>
t
G
?1
t
r
t
(7)
cov (s, a(x)) = ?((s
x
, a), (s
x
, a))
? k
t
(s
x
, a)
>
H
>
t
G
?1
t
H
t
k
t
(s
x
, a) (8)
G
t
= H
t
K
t
H
>
t
+ ?
2
H
t
H
>
t
(9)
62
Ht
=
?
?
?
?
?
1 ?? ? ? ? 0 0
0 1 ? ? ? 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 ? ? ? 0 1 ??
?
?
?
?
?
(10)
where K
t
is the Gram matrix with elements
K
t
(i, j) = ?((s
i
x
i
, a
i
), (s
j
x
j
, a
j
)), k
t
(s
x
, a) =
[?((s
i
x
i
, a
i
), (s
x
, a))]
t
i=0
is a vector, and ? is a
hyperparameter specifying the diagonal covari-
ance values of the zero-mean Gaussian noise. In
addition, we use cov (s, a(x)) to denote (for short)
the self-covariance cov (s, a(x), s, a(x)).
In our case, as different feature functions ?
a
are
defined for different action types, the kernel func-
tion is defined to be:
?((s
x
, a), (s
?
x
?
, a
?
)) = [[a = a
?
]]?
a
(s
x
, s
?
x
?
) (11)
where [[?]] is an indicator function and ?
a
is the ker-
nel function defined corresponding to the feature
function ?
a
.
Given a state, a most straightforward policy is
to select the action that corresponds to the max-
imum mean Q-value estimated by the GP. How-
ever, since the objective is to learn the Q-function
associated with the optimal policy by interacting
directly with users, the policy must exhibit some
form of stochastic behaviour in order to explore
alternatives during the process of learning. In this
work, the strategy employed for the exploration-
exploitation trade-off is that, during exploration,
actions are chosen according to the variance of
the GP estimate for the Q-function, and during
exploitation, actions are chosen according to the
mean. That is:
pi(s) =
{
arg max
a(x)
?
Q(s, a(x)) : w.p. 1? 
arg max
a(x)
cov (s, a(x)) : w.p. 
(12)
where 0 <  < 1 is a pre-defined exploration rate,
and will be exponentially reduced at each batch
iteration during our learning process.
Note that, in practice, not all the actions are
valid at every possible state. For example, if a do-
main expert d has never been activated during a
dialogue and can neither process the user?s current
query, the actions with an operand d will be re-
garded as invalid at this state. When executing the
policy, we only consider those valid actions for a
given state.
Score Interpretation
5
The domain selections are totally
correct, and the entire dialogue flow
is fluent.
4
The domain selections are totally
correct, but the dialogue flow is
slightly redundant.
3
There are accidental domain
selections errors, or the dialogue
flow is perceptually redundant.
2
There are frequent domain selections
errors, or the dialogue flow is
intolerably redundant.
1
Most domain selections are
incorrect, or the dialogue is
incompletable.
Table 2: The scoring standard in our experiments.
6 Experimental Results
6.1 Training
We use the batch version of GPTD as described
in Section 5 to learn the central control policy
with human subjects. There are three domain ex-
perts available in our current system, but during
the training only two domains are used, which are
the travel information domain and the restaurant
search domain. We reserve a movie search domain
for evaluating the generalisation property of the
learnt policy (see Section 6.2). The learning pro-
cess started from a hand-crafted policy. Then 15
experienced users
2
volunteered to contribute dia-
logue examples with multiple or compound goals
(see Figure 4 for an instance), from whom we
collected around 50?70 dialogues per day for 5
days
3
. After each dialogue, the users were asked
to score the system from 5 to 1 according to a scor-
ing standard shown in Table 2. The scores were
taken as the (delayed) rewards to train the GPTD
model, where we set the rewards for intermediate
turns to 0. The working policy was updated daily
based on the data obtained up to that day. The
data collected on the first day was used for pre-
liminary experiments to choose the hyperparame-
2
Overall user satisfactions may rely on various aspects of
the entire system, e.g. the data source quality of the services,
the performance of each domain expert, etc. It will be diffi-
cult to make non-experienced users to score the central con-
troller isolatedly.
3
Not all the users participated the experiments everyday.
There were 311 valid dialogues received in total, with an av-
erage length of 9 turns.
63
2	 ?
3	 ?
4	 ?
5	 ?
Figure 2: Average scores and standard deviations
during policy iteration.
0.7	 ?
0.72	 ?
0.74	 ?
0.76	 ?
0.78	 ?
0.8	 ?
0.82	 ?
0.84	 ?
0.86	 ?
0.88	 ?
0.9	 ?
Figure 3: Domain selection accuracies during pol-
icy iteration.
ters of the model, such as the kernel function, the
kernel parameters (if applicable), and ? and ? in
the GPTD model. We initially experimented with
linear, polynomial and Gaussian kernels, with dif-
ferent configurations of ? and ? values, as well
as kernel parameter values. It was found that
the linear kernel in combination with ? = 5 and
? = 0.99 works more appropriate than the other
settings. This configuration was then fixed for the
rest iterations.
The learning process was iterated for 4 days af-
ter the first one. On each day, we computed the
mean and standard deviation of the user scores
as an evaluation of the policy learnt on the pre-
vious day. The learning curve is illustrated in Fig-
ure 2. Note here, as we were actually executing a
stochastic policy according to Eq. (12), to calcu-
late the values in Figure 2 we ignored those dia-
logues that contain any actions selected due to the
exploration. We also manually labelled the cor-
rectness of domain selection at every turn of the
dialogues. The domain selection accuracies of the
obtained policy sequence are shown in Figure 3,
where similarly, those exploration actions as well
Policy
Scenario
Baseline GPTD
p-value
(i) 4.5?0.8 4.2?0.8 0.387
(ii) 3.4?0.9 4.2?0.8 0.018
(iii) 4.1?1.0 4.3?1.0 0.0821
(iv) 3.9?1.1 4.5?0.8 0.0440
Table 3: Paired comparison experiments between
the system with a trained GPTD policy and the
rule-based baseline.
as the clarification and confirmation actions were
excluded from the calculations. Although the do-
main selection accuracy is not the target that our
learning algorithm aims to optimise, it reflects the
quality of the learnt policies from a different angle
of view.
It can be found in Figure 2 that the best policy
was obtained in the third iteration, and after that
the policy quality oscillated. The same finding is
indicated in Figure 3 as well, when we use the do-
main selection accuracy as the evaluation metric.
Therefore, we kept the policy corresponding to the
peak point of the learning curve for the compari-
son experiments below.
6.2 Comparison Experiments
We conducted paired comparison experiments in
four scenarios to compare between the system
with the GPTD-learnt central control policy and a
non-trivial baseline. The baseline is a publicly de-
ployed version of the voice assistant application.
The central control policy of the baseline system is
handcrafted, which has a separate list of semantic
matching rules for each domain to enable domain
switching.
The first two scenarios aim to test the perfor-
mance of the two systems on (i) switching between
a domain expert and out-of-domain services, and
(ii) switching between two domain experts, where
only the two training domains (travel information
and restaurant search) were considered. Scenar-
ios (iii) and (iv) are similar to scenarios (i) and (ii)
respectively, but at this time, the users were re-
quired to carry out the tests surrounding the movie
search domain (which is addressed by a new do-
main expert not used in the training phase). There
were 13 users who participated this experiment.
In each scenario, every user was required to test
the two systems with an identical goal and similar
queries. After each test, the users were asked to
64
score the two systems separately according to the
scoring standard in Table 2.
The average scores received by the two systems
are shown in Table 3, where we also compute the
statistical significance (the p-values) of the results
based on paired t-tests. It can be found that the
learnt policy works significantly better than the
rule-based policy in scenarios (ii) and (iv), but in
scenarios (i) and (iii) the differences between two
systems are statistically insignificant. Moreover,
the learnt policy preserves the extensibility of the
entire system as expected, of which strong evi-
dences are given by the results in scenarios (iii)
and (iv).
6.3 Policy Analysis
To better understand the policy learnt by the
GPTD model, we look into the obtained weight
vectors, as shown in Table 4. It can be found that
confidence score (#5) is an informative feature for
all the system actions, while the relative turn of a
domain being last activated (#7) is an additional
strong evidence for a confirmation decision. In
addition, the similarity between the dialogue com-
pletion status (#1 & #2) of two ambiguous domain
experts and the relative turns of them being last
confirmed (#8) tend to be extra dominating fea-
tures for clarification decisions, besides the close-
ness of the confidence scores returned by the two
domain experts.
A less noticeable but important phenomenon is
observed for feature #6, i.e. the total number of
active turns of a domain expert during a dialogue.
Concretely, feature #6 has a small negative effect
on presentation actions but a small positive con-
tribution to confirmation actions. Such weights
could correspond to the discount factor?s penalty
to long dialogues in the value function. How-
ever, it implies an unexpected effect in extreme
cases, which we explain in detail as follows. Al-
though the absolute weights for feature #6 are tiny
for both presentation and confirmation actions, the
feature value will grow linearly during a dialogue.
Therefore, when a dialogue in a certain domain
last rather long, there tend to be very frequent con-
firmations. A possible solution to this problem
could be either ignoring feature #6 or twisting it to
some nonlinear function, such that its value stops
increasing at a certain threshold point. In addition,
to cover sufficient amount of those ?extreme? ex-
amples in the training phase could also be an alter-
Feature Weights
#
present confirm clarify
1 0.09 0.02 0.60
p
r
e
s
e
n
t
o
o
d
2 0.20 0.29 0.53
3 0.18 0.29 0.16
4 -0.10 0.16 0.25
5 0.75 0.57 0.54
6 -0.02 0.11 0.13
7 0.25 1.19 0.36
8 -0.22 -0.19 0.69
9 ? 0.20 ? 0.47
Bias -1.79 ? ? -2.42
Table 4: Feature weights learnt by GPTD. See Ta-
ble 1 for the meanings of the features.
native solution, as our current training set contains
very few examples that exhibit extraordinary long
domain persistence.
7 Further Discussions
The proposed approach is a rather general frame-
work to learn extensible central control policies
for multi-domain SDS based on distributed archi-
tectures. It does not rely on any internal represen-
tations of those individual domain experts, as long
as a unified featurisation of their dialogue states
can be achieved.
However, from the entire system point of view,
the current implementation is still preliminary.
Particularly, the confirmation and clarification
mechanisms are isolated, for which the surface re-
alisations sometimes may sound stiff. This phe-
nomenon explains one of the reasons that make
the proposed system slightly less preferred by the
users than the baseline in scenario (i), when the
interaction flows are relative simple. A possi-
ble improvement here could be associating the
confirmation and clarification actions in the cen-
tral controller to the error handling mechanisms
within each domain expert, and letting domain ex-
perts generate their own utterances on receiving a
confirmation/clarification request from the central
controller.
Online reinforcement learning with real user
cases will be another undoubted direction of fur-
ther improvement of our system. The key chal-
lenge here is to automatically estimate user?s satis-
factions, which will be transformed to the rewards
for the reinforcement learners. Strong feedbacks
such as clicks or actual order placements can be
65
collected. But to regress user?s true satisfaction,
other environment features must also be taken into
account. Practical solutions are still an open issue
at this stage, and are left to our future work.
8 Conclusion
In this paper, we introduce an MDP framework
for learning domain selection policies in a com-
plex multi-domain SDS. Standard problem for-
mulation is modified with tied model parameters,
so that the entire system is extensible and new
domain experts can be easily integrated without
re-training the policy. This expectation is con-
firmed by empirical experiments with human sub-
jects, where the proposed system marginally beats
a non-trivial baseline and demonstrates proper ex-
tensibility. Several possible improvements are dis-
cussed, which will be the central arc of our future
research.
Acknowledgements
The research in this paper is supported by China?s
973 Programme no. 2014CB340505. The first au-
thor is partially funded by the EC FP7 programme
under grant agreement no. 287615 (PARLANCE)
and a SICSA PECE grant. The authors would also
like to thank Qiaoqiao She, Duo Cai and the HCI-
APP group at Baidu for volunteering to participate
in the human subject experiments.
References
Richard Ernest Bellman. 1957. Dynamic Program-
ming. Princeton University Press, Princeton, NJ.
Dimitri P. Bertsekas and John N. Tsitsiklis. 1996.
Neuro-Dynamic Programming. Athena Scientific,
Belmont, MA.
Dan Bohus and Alexander I. Rudnicky. 2009. The
RavenClaw dialog management framework: Archi-
tecture and systems. Computer Speech and Lan-
guage, 23(3):332?361.
Yaakov Engel, Shie Mannor, and Ron Meir. 2005. Re-
inforcement learning with Gaussian processes. In
Proceedings of the 22nd International Conference
on Machine Learning (ICML), pages 201?208.
Mahdi Milani Fard, Joelle Pineau, and Csaba
Szepesv?ari. 2011. PAC-Bayesian policy evaluation
for reinforcement learning. In Proceedings of the
27th Conference on Uncertainty in Artificial Intelli-
gence (UAI), pages 195?202.
USER/ASR:      ?????? (Flight ticket to Qingdao.)
SYSTEM: ?????????????????????        ???????        (Okay, flight ticket to Qingdao. When are you        departing, and where are you departing from?)     [Travel Info. Domain Expert]
USER/ASR:  ???? (I?m leaving tomorrow.)
SYSTEM: ???????????????????????     (Okay, flight ticket to Qingdao, leaving         tomorrow. Where are you departing from?)      [Travel Info. Domain Expert]
USER/ASR:  ?? (Beijing)
SYSTEM: [Results Displayed     - Travel Info. Domain Expert]
USER:    ??????????        (Are there any good Sichuan cuisines in Qingdao?)ASR:       ???????????        (ungrammatical utterance, partially with     ?delicious? and ?Sichuan food?.)
SYSTEM: ?????????    (Do you want to continue with flight ticket booking?)    [Confirmation - Central Controller]
USER:      ?? (No)
SYSTEM: [Web Search Service: ?????????????     ]
USER/ASR:  ????? (I?m going to Qingdao tomorrow.)
SYSTEM: [Results Displayed     - Travel Info. Domain Expert]
USER/ASR:  ????????? (Restaurants near Qingdao Airport)
SYSTEM: ????????   (I found the following restaurants for you.)        [Results Displayed     - Restaurant Search Domain Expert]
USER/ASR:  ????????? (Will it rain in Qingdao tomorrow?)
SYSTEM: ?????????16??20????5-6?     (Qingdao is rainy tomorrow. The temperature is 16 to 20         degree centigrade with moderate east wind.)     [Results Displayed     - Weather Report Service]
Figure 4: An example dialogue containing multi-
ple user goals. The icons stand for graphical user
interface based nonverbal outputs.
Milica Ga?si?c, Catherine Breslin, Matthew Henderson,
Dongho Kim, Martin Szummer, Blaise Thomson,
Pirros Tsiakoulis, and Steve Young. 2013a. On-
line policy optimisation of Bayesian spoken dia-
logue systems via human interaction. In Proceed-
ings of the IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), pages
8367?8371.
Milica Ga?si?c, Catherine Breslin, Matthew Hender-
son, Dongho Kim, Martin Szummer, Blaise Thom-
son, Pirros Tsiakoulis, and Steve Young. 2013b.
POMDP-based dialogue manager adaptation to ex-
tended domains. In Proceedings of the 14th annual
SIGdial Meeting on Discourse and Dialogue, pages
214?222.
Thomas Robert Gruber, Adam John Cheyer, Dag
66
Kittlaus, Didier Rene Guzzoni, Christopher Dean
Brigham, Richard Donald Giuli, Marcello Bastea-
Forte, and Harry Joseph Saddler. 2012. Intelligent
automated assistant. United States Patent No. US
20120245944 A1.
Dilek Z. Hakkani-T?ur, Gokhan T?ur, Larry P. Heck,
Ashley Fidler, and Asli C?elikyilmaz. 2012. A dis-
criminative classification-based approach to infor-
mation state updates for a multi-domain dialog sys-
tem. In Proceedings of the 13th Annual Conference
of the International Speech Communication Associ-
ation (INTERSPEECH).
Filip Jur?c???cek, Blaise Thomson, and Steve Young.
2011. Natural actor and belief critic: Reinforcement
algorithm for learning parameters of dialogue sys-
tems modelled as POMDPs. ACM Transactions on
Speech and Language Processing, 7(3):6:1?6:25.
Filip Jur?c???cek, Blaise Thomson, and Steve Young.
2012. Reinforcement learning for parameter esti-
mation in statistical spoken dialogue systems. Com-
puter Speech & Language, 26(3):168?192.
Simon Keizer, Milica Ga?si?c, Filip Jur?c???cek, Franc?ois
Mairesse, Blaise Thomson, Kai Yu, and Steve
Young. 2010. Parameter estimation for agenda-
based user simulation. In Proceedings of the 11th
annual SIGdial Meeting on Discourse and Dialogue,
pages 116?123.
Kazunori Komatani, Naoyuki Kanda, Mikio Nakano,
Kazuhiro Nakadai, Hiroshi Tsujino, Tetsuya Ogata,
and Hiroshi G. Okuno. 2006. Multi-domain spo-
ken dialogue system with extensibility and robust-
ness against speech recognition errors. In Proceed-
ings of the 7th SIGdial Workshop on Discourse and
Dialogue, pages 9?17.
Oliver Lemon and Olivier Pietquin, editors. 2012.
Data-Driven Methods for Adaptive Spoken Dia-
logue Systems: Computational Learning for Conver-
sational Interfaces. Springer.
Bor-shen Lin, Hsin-min Wang, and Lin-Shan Lee.
1999. A distributed architecture for cooperative
spoken dialogue agents with coherent dialogue state
and history. In Proceedings of the IEEE Automatic
Speech Recognition and Understanding Workshop
(ASRU).
Danilo Mirkovic and Lawrence Cavedon. 2006. Di-
alogue management using scripts. United States
Patent No. US 20060271351 A1.
Mikio Nakano, Shun Sato, Kazunori Komatani, Kyoko
Matsuyama, Kotaro Funakoshi, and Hiroshi G.
Okuno. 2011. A two-stage domain selection frame-
work for extensible multi-domain spoken dialogue
systems. In Proceedings of the 12th annual SIGdial
Meeting on Discourse and Dialogue, pages 18?29.
Carl Edward Rasmussen and Christopher K. I.
Williams, editors. 2006. Gaussian Processes for
Machine Learning. MIT Press.
Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve Young. 2007. Agenda-based
user simulation for bootstrapping a POMDP dia-
logue system. In Proceedings of Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Compu-
tational Linguistics; Companion Volume, Short Pa-
pers, pages 149?152.
Satinder Singh, Diane Litman, Michael Kearns, and
Marilyn Walker. 2002. Optimizing dialogue man-
agement with reinforcement learning: Experiments
with the NJFun system. Journal of Artificial Intelli-
gence Research, 16(1):105?133.
Richard S. Sutton and Andrew G. Barto. 1998. Re-
inforcement Learning: An Introduction. MIT Press,
Cambridge, MA.
Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A POMDP framework for
spoken dialogue systems. Computer Speech and
Language, 24(4):562?588.
David R. Traum and Staffan Larsson. 2003. The In-
formation State approach to dialogue management.
In Jan van Kuppevelt and Ronnie W. Smith, editors,
Current and New Directions in Discourse and Dia-
logue, pages 325?353. Springer.
Jason D. Williams and Steve Young. 2007. Partially
observable Markov decision processes for spoken
dialog systems. Computer Speech and Language,
21(2):393?422.
Jason D. Williams, Iker Arizmendi, and Alistair
Conkie. 2010. Demonstration of AT&T ?Let?s Go?:
A production-grade statistical spoken dialog system.
In Proceedings of the 3rd IEEE Workshop on Spoken
Language Technology (SLT).
Steve Young, Milica Ga?si?c, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The Hidden Information State model:
a practical framework for POMDP-based spoken di-
alogue management. Computer Speech and Lan-
guage, 24(2):150?174.
Steve Young, Milica Ga?si?c, Blaise Thomson, and Ja-
son D. Williams. 2013. POMDP-based statistical
spoken dialogue systems: a review. Proceedings of
the IEEE, PP(99):1?20.
67
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 110?120,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Revisiting Embedding Features for Simple Semi-supervised Learning
Jiang Guo
?
, Wanxiang Che
?
, Haifeng Wang
?
, Ting Liu
??
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
?
Baidu Inc., Beijing, China
{jguo, car, tliu}@ir.hit.edu.cn
wanghaifeng@baidu.com
Abstract
Recent work has shown success in us-
ing continuous word embeddings learned
from unlabeled data as features to improve
supervised NLP systems, which is re-
garded as a simple semi-supervised learn-
ing mechanism. However, fundamen-
tal problems on effectively incorporating
the word embedding features within the
framework of linear models remain. In
this study, we investigate and analyze three
different approaches, including a new pro-
posed distributional prototype approach,
for utilizing the embedding features. The
presented approaches can be integrated
into most of the classical linear models in
NLP. Experiments on the task of named
entity recognition show that each of the
proposed approaches can better utilize the
word embedding features, among which
the distributional prototype approach per-
forms the best. Moreover, the combination
of the approaches provides additive im-
provements, outperforming the dense and
continuous embedding features by nearly
2 points of F1 score.
1 Introduction
Learning generalized representation of words is
an effective way of handling data sparsity caused
by high-dimensional lexical features in NLP sys-
tems, such as named entity recognition (NER)
and dependency parsing. As a typical low-
dimensional and generalized word representa-
tion, Brown clustering of words has been stud-
ied for a long time. For example, Liang (2005)
and Koo et al. (2008) used the Brown cluster
features for semi-supervised learning of various
NLP tasks and achieved significant improvements.
?
Email correspondence.
Recent research has focused on a special fam-
ily of word representations, named ?word embed-
dings?. Word embeddings are conventionally de-
fined as dense, continuous, and low-dimensional
vector representations of words. Word embed-
dings can be learned from large-scale unlabeled
texts through context-predicting models (e.g., neu-
ral network language models) or spectral methods
(e.g., canonical correlation analysis) in an unsu-
pervised setting.
Compared with the so-called one-hot represen-
tation where each word is represented as a sparse
vector of the same size of the vocabulary and only
one dimension is on, word embedding preserves
rich linguistic regularities of words with each di-
mension hopefully representing a latent feature.
Similar words are expected to be distributed close
to one another in the embedding space. Conse-
quently, word embeddings can be beneficial for
a variety of NLP applications in different ways,
among which the most simple and general way is
to be fed as features to enhance existing supervised
NLP systems.
Previous work has demonstrated effectiveness
of the continuous word embedding features in sev-
eral tasks such as chunking and NER using gener-
alized linear models (Turian et al., 2010).
1
How-
ever, there still remain two fundamental problems
that should be addressed:
? Are the continuous embedding features fit for
the generalized linear models that are most
widely adopted in NLP?
? How can the generalized linear models better
utilize the embedding features?
According to the results provided by Turian et
1
Generalized linear models refer to the models that de-
scribe the data as a combination of linear basis functions,
either directly in the input variables space or through some
transformation of the probability distributions (e.g., log-
linear models).
110
al. (2010), the embedding features brought signif-
icantly less improvement than Brown clustering
features. This result is actually not reasonable be-
cause the expressing power of word embeddings
is theoretically stronger than clustering-based rep-
resentations which can be regarded as a kind of
one-hot representation but over a low-dimensional
vocabulary (Bengio et al., 2013).
Wang and Manning (2013) showed that linear
architectures perform better in high-dimensional
discrete feature space than non-linear ones,
whereas non-linear architectures are more effec-
tive in low-dimensional and continuous feature
space. Hence, the previous method that directly
uses the continuous word embeddings as features
in linear models (CRF) is inappropriate. Word
embeddings may be better utilized in the linear
modeling framework by smartly transforming the
embeddings to some relatively higher dimensional
and discrete representations.
Driven by this motivation, we present three
different approaches: binarization (Section 3.2),
clustering (Section 3.3) and a new proposed distri-
butional prototype method (Section 3.4) for better
incorporating the embeddings features. In the bi-
narization approach, we directly binarize the con-
tinuous word embeddings by dimension. In the
clustering approach, we cluster words based on
their embeddings and use the resulting word clus-
ter features instead. In the distributional prototype
approach, we derive task-specific features from
word embeddings by utilizing a set of automati-
cally extracted prototypes for each target label.
We carefully compare and analyze these ap-
proaches in the task of NER. Experimental results
are promising. With each of the three approaches,
we achieve higher performance than directly using
the continuous embedding features, among which
the distributional prototype approach performs the
best. Furthermore, by putting the most effective
two of these features together, we finally outper-
form the continuous embedding features by nearly
2 points of F1 Score (86.21% vs. 88.11%).
The major contribution of this paper is twofold.
(1) We investigate various approaches that can bet-
ter utilize word embeddings for semi-supervised
learning. (2) We propose a novel distributional
prototype approach that shows the great potential
of word embedding features. All the presented ap-
proaches can be easily integrated into most of the
classical linear NLP models.
2 Semi-supervised Learning with Word
Embeddings
Statistical modeling has achieved great success
in most NLP tasks. However, there still remain
some major unsolved problems and challenges,
among which the most widely concerned is the
data sparsity problem. Data sparsity in NLP is
mainly caused by two factors, namely, the lack
of labeled training data and the Zipf distribution
of words. On the one hand, large-scale labeled
training data are typically difficult to obtain, espe-
cially for structure prediction tasks, such as syn-
tactic parsing. Therefore, the supervised mod-
els can only see limited examples and thus make
biased estimation. On the other hand, the nat-
ural language words are Zipf distributed, which
means that most of the words appear a few times
or are completely absent in our texts. For these
low-frequency words, the corresponding parame-
ters usually cannot be fully trained.
More foundationally, the reason for the above
factors lies in the high-dimensional and sparse lex-
ical feature representation, which completely ig-
nores the similarity between features, especially
word features. To overcome this weakness, an ef-
fective way is to learn more generalized represen-
tations of words by exploiting the numerous un-
labeled data, in a semi-supervised manner. After
which, the generalized word representations can
be used as extra features to facilitate the super-
vised systems.
Liang (2005) learned Brown clusters of
words (Brown et al., 1992) from unlabeled data
and use them as features to promote the supervised
NER and Chinese word segmentation. Brown
clusters of words can be seen as a generalized
word representation distributed in a discrete and
low-dimensional vocabulary space. Contextually
similar words are grouped in the same cluster. The
Brown clustering of words was also adopted in de-
pendency parsing (Koo et al., 2008) and POS tag-
ging for online conversational text (Owoputi et al.,
2013), demonstrating significant improvements.
Recently, another kind of word representation
named ?word embeddings? has been widely stud-
ied (Bengio et al., 2003; Mnih and Hinton, 2008).
Using word embeddings, we can evaluate the sim-
ilarity of two words straightforward by comput-
ing the dot-product of two numerical vectors in the
Hilbert space. Two similar words are expected to
111
be distributed close to each other.
2
Word embeddings can be useful as input to an
NLP model (mostly non-linear) or as additional
features to enhance existing systems. Collobert
et al. (2011) used word embeddings as input to a
deep neural network for multi-task learning. De-
spite of the effectiveness, such non-linear models
are hard to build and optimize. Besides, these ar-
chitectures are often specialized for a certain task
and not scalable to general tasks. A simple and
more general way is to feed word embeddings as
augmented features to an existing supervised sys-
tem, which is similar to the semi-supervised learn-
ing with Brown clusters.
As discussed in Section 1, Turian et al. (2010)
is the pioneering work on using word embedding
features for semi-supervised learning. However,
their approach cannot fully exploit the potential
of word embeddings. We revisit this problem
in this study and investigate three different ap-
proaches for better utilizing word embeddings in
semi-supervised learning.
3 Approaches for Utilizing Embedding
Features
3.1 Word Embedding Training
In this paper, we will consider a context-
predicting model, more specifically, the Skip-gram
model (Mikolov et al., 2013a; Mikolov et al.,
2013b) for learning word embeddings, since it is
much more efficient as well as memory-saving
than other approaches.
Let?s denote the embedding matrix to be learned
by C
d?N
, where N is the vocabulary size and d is
the dimension of word embeddings. Each column
of C represents the embedding of a word. The
Skip-gram model takes the current word w as in-
put, and predicts the probability distribution of its
context words within a fixed window size. Con-
cretely, w is first mapped to its embedding v
w
by
selecting the corresponding column vector of C
(or multiplying C with the one-hot vector of w).
The probability of its context word c is then com-
puted using a log-linear function:
P (c|w; ?) =
exp(v
>
c
v
w
)
?
c
?
?V
exp(v
c
?
>
v
w
)
(1)
where V is the vocabulary. The parameters ? are
v
w
i
, v
c
i
for w, c ? V and i = 1, ..., d. Then, the
2
The term similar should be viewed depending on the spe-
cific task.
log-likelihood over the entire training dataset D
can be computed as:
J(?) =
?
(w,c)?D
log p(c|w; ?) (2)
The model can be trained by maximizing J(?).
Here, we suppose that the word embeddings
have already been trained from large-scale unla-
beled texts. We will introduce various approaches
for utilizing the word embeddings as features for
semi-supervised learning. The main idea, as in-
troduced in Section 1, is to transform the continu-
ous word embeddings to some relatively higher di-
mensional and discrete representations. The direct
use of continuous embeddings as features (Turian
et al., 2010) will serve as our baseline setting.
3.2 Binarization of Embeddings
One fairly natural approach for converting the
continuous-valued word embeddings to discrete
values is binarization by dimension.
Formally, we aim to convert the continuous-
valued embedding matrixC
d?N
, to another matrix
M
d?N
which is discrete-valued. There are various
conversion functions. Here, we consider a sim-
ple one. For the i
th
dimension of the word em-
beddings, we divide the corresponding row vector
C
i
into two halves for positive (C
i+
) and nega-
tive (C
i?
), respectively. The conversion function
is then defined as follows:
M
ij
= ?(C
ij
) =
?
?
?
?
?
U
+
, if C
ij
? mean(C
i+
)
B
?
, if C
ij
? mean(C
i?
)
0, otherwise
where mean(v) is the mean value of vector v, U
+
is a string feature which turns on when the value
(C
ij
) falls into the upper part of the positive list.
Similarly, B
?
refers to the bottom part of the neg-
ative list. The insight behind ? is that we only con-
sider the features with strong opinions (i.e., posi-
tive or negative) on each dimension and omit the
values close to zero.
3.3 Clustering of Embeddings
Yu et al. (2013) introduced clustering embeddings
to overcome the disadvantage that word embed-
dings are not suitable for linear models. They sug-
gested that the high-dimensional cluster features
make samples from different classes better sepa-
rated by linear models.
112
In this study, we again investigate this ap-
proach. Concretely, each word is treated as a sin-
gle sample. The batch k-means clustering algo-
rithm (Sculley, 2010) is used,
3
and each cluster
is represented as the mean of the embeddings of
words assigned to it. Similarities between words
and clusters are measured by Euclidean distance.
Moreover, different number of clusters n con-
tain information of different granularities. There-
fore, we combine the cluster features of different
ns to better utilize the embeddings.
3.4 Distributional Prototype Features
We propose a novel kind of embedding features,
named distributional prototype features for su-
pervised models. This is mainly inspired by
prototype-driven learning (Haghighi and Klein,
2006) which was originally introduced as a pri-
marily unsupervised approach for sequence mod-
eling. In prototype-driven learning, a few pro-
totypical examples are specified for each target
label, which can be treated as an injection of
prior knowledge. This sparse prototype informa-
tion is then propagated across an unlabeled corpus
through distributional similarities.
The basic motivation of the distributional pro-
totype features is that similar words are supposed
to be tagged with the same label. This hypothesis
makes great sense in tasks such as NER and POS
tagging. For example, suppose Michael is a pro-
totype of the named entity (NE) type PER. Using
the distributional similarity, we could link similar
words to the same prototypes, so the word David
can be linked to Michael because the two words
have high similarity (exceeds a threshold). Using
this link feature, the model will push David closer
to PER.
To derive the distributional prototype features,
first, we need to construct a few canonical exam-
ples (prototypes) for each target annotation label.
We use the normalized pointwise mutual informa-
tion (NPMI) (Bouma, 2009) between the label and
word, which is a smoothing version of the standard
PMI, to decide the prototypes of each label. Given
the annotated training corpus, the NPMI between
a label and word is computed as follows:
?
n
(label, word) =
?(label, word)
? ln p(label, word)
(3)
3
code.google.com/p/sofia-ml
NE Type Prototypes
B-PER Mark, Michael, David, Paul
I-PER Akram, Ahmed, Khan, Younis
B-ORG Reuters, U.N., Ajax, PSV
I-ORG Newsroom, Inc, Corp, Party
B-LOC U.S., Germany, Britain, Australia
I-LOC States, Republic, Africa, Lanka
B-MISC Russian, German, French, British
I-MISC Cup, Open, League, OPEN
O ., ,, the, to
Table 1: Prototypes extracted from the CoNLL-
2003 NER training data using NPMI.
where,
?(label, word) = ln
p(label, word)
p(label)p(word)
(4)
is the standard PMI.
For each target label l (e.g., PER, ORG, LOC),
we compute the NPMI of l and all words in the
vocabulary, and the top m words are chosen as the
prototypes of l. We should note that the proto-
types are extracted fully automatically, without in-
troducing additional human prior knowledge.
Table 1 shows the top four prototypes extracted
from the NER training corpus of CoNLL-2003
shared task (Tjong Kim Sang and De Meul-
der, 2003), which contains four NE types, namely,
PER, ORG, LOC, and MISC. Non-NEs are denoted
by O. We convert the original annotation to the
standard BIO-style. Thus, the final corpus con-
tains nine labels in total.
Next, we introduce the prototypes as features to
our supervised model. We denote the set of pro-
totypes for all target labels by S
p
. For each proto-
type z ? S
p
, we add a predicate proto = z, which
becomes active at each w if the distributional sim-
ilarity between z and w (DistSim(z, w)) is above
some threshold. DistSim(z, w) can be efficiently
calculated through the cosine similarity of the em-
beddings of z and w. Figure 1 gives an illustra-
tion of the distributional prototype features. Un-
like previous embedding features or Brown clus-
ters, the distributional prototype features are task-
specific because the prototypes of each label are
extracted from the training data.
Moreover, each prototype word is also its own
prototype (since a word has maximum similarity
to itself). Thus, if the prototype is closely related
to a label, all the words that are distributionally
113
i -1x ix1?iy iyO B-LOC
in
/IN
Hague
/NNP
O B-LOC1( , )? ? ?i if y y
( , )
 word = Hague
    pos = NNP
 proto = Britain  B-LOC
 proto = England 
 ...
?
? ?
? ?
? ?? ?
?? ?
? ?
? ?
? ?? ?
i if x y
Figure 1: An example of distributional prototype
features for NER.
similar to that prototype are pushed towards that
label.
4 Supervised Evaluation Task
Various tasks can be considered to compare and
analyze the effectiveness of the above three ap-
proaches. In this study, we partly follow Turian
et al. (2010) and Yu et al. (2013), and take NER as
the supervised evaluation task.
NER identifies and classifies the named entities
such as the names of persons, locations, and orga-
nizations in text. The state-of-the-art systems typ-
ically treat NER as a sequence labeling problem,
where each word is tagged either as a BIO-style
NE or a non-NE category.
Here, we use the linear chain CRF model, which
is most widely used for sequence modeling in the
field of NLP. The CoNLL-2003 shared task dataset
from the Reuters, which was used by Turian et
al. (2010) and Yu et al. (2013), was chosen as
our evaluation dataset. The training set contains
14,987 sentences, the development set contains
3,466 sentences and is used for parameter tuning,
and the test set contains 3,684 sentences.
The baseline features are shown in Table 2.
4.1 Embedding Feature Templates
In this section, we introduce the embedding fea-
tures to the baseline NER system, turning the su-
pervised approach into a semi-supervised one.
Dense embedding features. The dense con-
tinuous embedding features can be fed directly to
the CRF model. These embedding features can
be seen as heterogeneous features from the exist-
ing baseline features, which are discrete. There is
no effective way for dense embedding features to
be combined internally or with other discrete fea-
tures. So we only use the unigram embedding fea-
tures following Turian et al. (2010). Concretely,
the embedding feature template is:
Baseline NER Feature Templates
00: w
i+k
,?2 ? k ? 2
01: w
i+k
? w
i+k+1
,?2 ? k ? 1
02: t
i+k
,?2 ? k ? 2
03: t
i+k
? t
i+k+1
,?2 ? k ? 1
04: chk
i+k
,?2 ? k ? 2
05: chk
i+k
? chk
i+k+1
,?2 ? k ? 1
06: Prefix (w
i+k
, l),?2 ? k ? 2, 1 ? l ? 4
07: Suffix (w
i+k
, l),?2 ? k ? 2, 1 ? l ? 4
08: Type(w
i+k
),?2 ? k ? 2
Unigram Features
y
i
? 00? 08
Bigram Features
y
i?1
? y
i
Table 2: Features used in the NER system. t is
the POS tag. chk is the chunking tag. Prefix
and Suffix are the first and last l characters of a
word. Type indicates if the word is all-capitalized,
is-capitalized, all-digits, etc.
? de
i+k
[d], ?2 ? k ? 2, d ranges over the
dimensions of the dense word embedding de.
Binarized embedding features. The binarized
embedding feature template is similar to the dense
one. The only difference is that the feature val-
ues are discrete and we omit dimensions with zero
value. Therefore, the feature template becomes:
? bi
i+k
[d], ?2 ? k ? 2, where bi
i+k
[d] 6= 0,
d ranges over the dimensions of the binarized
vector bi of word embedding.
In this way, the dimension of the binarized em-
bedding feature space becomes 2 ? d compared
with the originally d of the dense embeddings.
Compound cluster features. The advantage of
the cluster features is that they can be combined
internally or with other features to form compound
features, which can be more discriminative. Fur-
thermore, the number of resulting clusters n can
be tuned, and different ns indicate different granu-
larities. Concretely, the compound cluster feature
template for each specific n is:
? c
i+k
, ?2 ? k ? 2.
? c
i+k
? c
i+k+1
,?2 ? k ? 1.
? c
i?1
? c
i+1
.
Distributional prototype features. The set of
prototypes is again denoted by S
p
, which is de-
114
cided by selecting the topm (NPMI) words as pro-
totypes of each label, where m is tuned on the de-
velopment set. For each word w
i
in a sequence,
we compute the distributional similarity between
w
i
and each prototype in S
p
and select the proto-
types zs that DistSim(z, w) ? ?. We set ? = 0.5
without manual tuning. The distributional proto-
type feature template is then:
? {proto
i+k
=z | DistSim(w
i+k
, z) ? ? & z ?
S
p
}, ?2 ? k ? 2 .
We only use the unigram features, since the
number of active distributional prototype features
varies for different words (positions). Hence,
these features cannot be combined effectively.
4.2 Brown Clustering
Brown clustering has achieved great success in
various NLP applications. At most time, it
provides a strong baseline that is difficult to
beat (Turian et al., 2010). Consequently, in our
study, we conduct comparisons among the embed-
ding features and the Brown clustering features,
along with further investigations of their combina-
tion.
The Brown algorithm is a hierarchical cluster-
ing algorithm which optimizes a class-based bi-
gram language model defined on the word clus-
ters (Brown et al., 1992). The output of the Brown
algorithm is a binary tree, where each word is
uniquely identified by its path from the root. Thus
each word can be represented as a bit-string with
a specific length.
Following the setting of Owoputi et al. (2013),
we will use the prefix features of hierarchical clus-
ters to take advantage of the word similarity in dif-
ferent granularities. Concretely, the Brown cluster
feature template is:
? bc
i+k
, ?2 ? k ? 2.
? prefix (bc
i+k
, p), p ? {2,4,6,...,16}, ?2 ?
k ? 2. prefix takes the p-length prefix of
the Brown cluster coding bc
i+k
.
5 Experiments
5.1 Experimental Setting
We take the English Wikipedia until August 2012
as our unlabeled data to train the word embed-
dings.
4
Little pre-processing is conducted for the
4
download.wikimedia.org.
training of word embeddings. We remove para-
graphs that contain non-roman characters and all
MediaWiki markups. The resulting text is tok-
enized using the Stanford tokenizer,
5
and every
word is converted to lowercase. The final dataset
contains about 30 million sentences and 1.52 bil-
lion words. We use a dictionary that contains
212,779 most common words (frequency ? 80) in
the dataset. An efficient open-source implementa-
tion of the Skip-gram model is adopted.
6
We ap-
ply the negative sampling
7
method for optimiza-
tion, and the asynchronous stochastic gradient de-
scent algorithm (Asynchronous SGD) for parallel
weight updating. In this study, we set the dimen-
sion of the word embeddings to 50. Higher di-
mension is supposed to bring more improvements
in semi-supervised learning, but its comparison is
beyond the scope of this paper.
For the cluster features, we tune the number
of clusters n from 500 to 3000 on the develop-
ment set, and finally use the combination of n =
500, 1000, 1500, 2000, 3000, which achieves the
best results. For the distributional prototype fea-
tures, we use a fixed number of prototype words
(m) for each target label. m is tuned on the devel-
opment set and is finally set to 40.
We induce 1,000 brown clusters of words, the
setting in prior work (Koo et al., 2008; Turian et
al., 2010). The training data of brown clustering is
the same with that of training word embeddings.
5.2 Results
Table 3 shows the performances of NER on the
test dataset. Our baseline is slightly lower than
that of Turian et al. (2010), because they use
the BILOU encoding of NE types which outper-
forms BIO encoding (Ratinov and Roth, 2009).
8
Nonetheless, our conclusions hold. As we can see,
all of the three approaches we investigate in this
study achieve better performance than the direct
use of the dense continuous embedding features.
To our surprise, even the binarized embedding
features (BinarizedEmb) outperform the continu-
ous version (DenseEmb). This provides clear evi-
dence that directly using the dense continuous em-
beddings as features in CRF indeed cannot fully
5
nlp.stanford.edu/software/tokenizer.
shtml.
6
code.google.com/p/word2vec/.
7
More details are analyzed in (Goldberg and Levy, 2014).
8
We use BIO encoding here in order to compare with most
of the reported benchmarks.
115
Setting F1
Baseline 83.43
+DenseEmb? 86.21
+BinarizedEmb 86.75
+ClusterEmb 86.90
+DistPrototype 87.44
+BinarizedEmb+ClusterEmb 87.56
+BinarizedEmb+DistPrototype 87.46
+ClusterEmb+DistPrototype 88.11
+Brown 87.49
+Brown+ClusterEmb 88.17
+Brown+DistPrototype 88.04
+Brown+ClusterEmb+DistPrototype 88.58
Finkel et al. (2005) 86.86
Krishnan and Manning (2006) 87.24
Ando and Zhang (2005) 89.31
Collobert et al. (2011) 88.67
Table 3: The performance of semi-supervised
NER on the CoNLL-2003 test data, using vari-
ous embedding features. ? DenseEmb refers to the
method used by Turian et al. (2010), i.e., the direct
use of the dense and continuous embeddings.
exploit the potential of word embeddings. The
compound cluster features (ClusterEmb) also out-
perform the DenseEmb. The same result is also
shown in (Yu et al., 2013). Further, the distribu-
tional prototype features (DistPrototype) achieve
the best performance among the three approaches
(1.23% higher than DenseEmb).
We should note that the feature templates used
for BinarizedEmb and DistPrototype are merely
unigram features. However, for ClusterEmb, we
form more complex features by combining the
clusters of the context words. We also consider
different number of clusters n, to take advantage
of the different granularities. Consequently, the
dimension of the cluster features is much higher
than that of BinarizedEmb and DistPrototype.
We further combine the proposed features to see
if they are complementary to each other. As shown
in Table 3, the cluster and distributional prototype
features are the most complementary, whereas the
binarized embedding features seem to have large
overlap with the distributional prototype features.
By combining the cluster and distributional pro-
totype features, we further push the performance
to 88.11%, which is nearly two points higher than
the performance of the dense embedding features
(86.21%).
9
We also compare the proposed features with
the Brown cluster features. As shown in Table 3,
the distributional prototype features alone achieve
comparable performance with the Brown clusters.
When the cluster and distributional prototype fea-
tures are used together, we outperform the Brown
clusters. This result is inspiring because we show
that the embedding features indeed have stronger
expressing power than the Brown clusters, as de-
sired. Finally, by combining the Brown cluster
features and the proposed embedding features, the
performance can be improved further (88.58%).
The binarized embedding features are not included
in the final compound features because they are al-
most overlapped with the distributional prototype
features in performance.
We also summarize some of the reported
benchmarks that utilize unlabeled data (with no
gazetteers used), including the Stanford NER tag-
ger (Finkel et al. (2005) and Krishnan and Man-
ning (2006)) with distributional similarity fea-
tures. Ando and Zhang (2005) use unlabeled data
for constructing auxiliary problems that are ex-
pected to capture a good feature representation of
the target problem. Collobert et al. (2011) adjust
the feature embeddings according to the specific
task in a deep neural network architecture. We
can see that both Ando and Zhang (2005) and Col-
lobert et al. (2011) learn task-specific lexical fea-
tures, which is similar to the proposed distribu-
tional prototype method in our study. We suggest
this to be the main reason for the superiority of
these methods.
Another advantage of the proposed discrete fea-
tures over the dense continuous features is tag-
ging efficiency. Table 4 shows the running time
using different kinds of embedding features. We
achieve a significant reduction of the tagging time
per sentence when using the discrete features. This
is mainly due to the dense/sparse battle. Al-
though the dense embedding features are low-
dimensional, the feature vector for each word is
much denser than in the sparse and discrete feature
space. Therefore, we actually need much more
computation during decoding. Similar results can
be observed in the comparison of the DistProto-
type and ClusterEmb features, since the density of
the DistPrototype features is higher. It is possible
9
Statistical significant with p-value < 0.001 by two-tailed
t-test.
116
Setting Time (ms) / sent
Baseline 1.04
+DenseEmb 4.75
+BinarizedEmb 1.25
+ClusterEmb 1.16
+DistPrototype 2.31
Table 4: Running time of different features on a
Intel(R) Xeon(R) E5620 2.40GHz machine.
to accelerate the DistPrototype, by increasing the
threshold of DistSim(z, w). However, this is in-
deed an issue of trade-off between efficiency and
accuracy.
5.3 Analysis
In this section, we conduct analyses to show the
reasons for the improvements.
5.3.1 Rare words
As discussed by Turian et al. (2010), much of the
NER F1 is derived from decisions regarding rare
words. Therefore, in order to show that the three
proposed embedding features have stronger abil-
ity for handling rare words, we first conduct anal-
ysis for the tagging errors of words with differ-
ent frequency in the unlabeled data. We assign the
word frequencies to several buckets, and evaluate
the per-token errors that occurred in each bucket.
Results are shown in Figure 2. In most cases, all
three embedding features result in fewer errors on
rare words than the direct use of dense continuous
embedding features.
Interestingly, we find that for words that are
extremely rare (0?256), the binarized embedding
features incur significantly fewer errors than other
approaches. As we know, the embeddings for the
rare words are close to their initial value, because
they received few updates during training. Hence,
these words are not fully trained. In this case,
we would like to omit these features because their
embeddings are not even trustable. However, all
embedding features that we proposed except Bi-
narizedEmb are unable to handle this.
In order to see how much we have utilized
the embedding features in BinarizedEmb, we cal-
culate the sparsity of the binarized embedding
vectors, i.e., the ratio of zero values in each
vector (Section 3.2). As demonstrated in Fig-
ure 3, the sparsity-frequency curve has good prop-
erties: higher sparsity for very rare words and
very frequent words, while lower sparsity for mid-
frequent words. It indicates that for words that are
very rare or very frequent, BinarizedEmb just omit
most of the features. This is reasonable also for
the very frequent words, since they usually have
rich and diverse context distributions and their
embeddings cannot be well learned by our mod-
els (Huang et al., 2012).
l
l
l
l l
l
l
l
l
l
l
Frequency of word in unlabeled data
Spars
ity
256 1k 4k 16k 64k0
.50
0.55
0.60
0.65
0.70
Figure 3: Sparsity (with confidence interval) of the
binarized embedding vector w.r.t. word frequency
in the unlabeled data.
Figure 2(b) further supports our analysis. Bina-
rizedEmb also reduce much of the errors for the
highly frequent words (32k-64k).
As expected, the distributional prototype fea-
tures produce fewest errors in most cases. The
main reason is that the prototype features are task-
specific. The prototypes are extracted from the
training data and contained indicative information
of the target labels. By contrast, the other em-
bedding features are simply derived from general
word representations and are not specialized for
certain tasks, such as NER.
5.3.2 Linear Separability
Another reason for the superiority of the proposed
embedding features is that the high-dimensional
discrete features are more linear separable than
the low-dimensional continuous embeddings. To
verify the hypothesis, we further carry out experi-
ments to analyze the linear separability of the pro-
posed discrete embedding features against dense
continuous embeddings.
We formalize this problem as a binary classi-
fication task, to determine whether a word is an
NE or not (NE identification). The linear support
vector machine (SVM) is used to build the clas-
sifiers, using different embedding features respec-
117
0?256 256?512 512?1k 1k?2kFrequency of word in unlabeled data
num
ber o
f per?
token
 erro
rs
0
50
100
150
200
250 DenseEmbBinarizedEmbClusterEmbDistPrototype
(a)
4k?8k 8k?16k 16k?32k 32k?64kFrequency of word in unlabeled data
num
ber o
f per?
token
 erro
rs
40
60
80
100
120 DenseEmbBinarizedEmbClusterEmbDistPrototype
(b)
Figure 2: The number of per-token errors w.r.t. word frequency in the unlabeled data. (a) For rare words
(frequency ? 2k). (b) For frequent words (frequency ? 4k).
Setting Acc. #features
DenseEmb 95.46 250
BinarizedEmb 94.10 500
ClusterEmb 97.57 482,635
DistPrototype 96.09 1,700
DistPrototype-binary 96.82 4,530
Table 5: Performance of the NE/non-NE classi-
fication on the CoNLL-2003 development dataset
using different embedding features.
tively. We use the LIBLINEAR tool (Fan et al.,
2008) as our SVM implementation. The penalty
parameter C is tuned from 0.1 to 1.0 on the devel-
opment dataset. The results are shown in Table 5.
As we can see, NEs and non-NEs can be better
separated using ClusterEmb or DistPrototype fea-
tures. However, the BinarizedEmb features per-
form worse than the direct use of word embedding
features. The reason might be inferred from the
third column of Table 5. As demonstrated in Wang
and Manning (2013), linear models are more ef-
fective in high-dimensional and discrete feature
space. The dimension of the BinarizedEmb fea-
tures remains small (500), which is merely twice
the DenseEmb. By contrast, feature dimensions
are much higher for ClusterEmb and DistProto-
type, leading to better linear separability and thus
can be better utilized by linear models.
We notice that the DistPrototype features per-
form significantly worse than ClusterEmb in NE
identification. As described in Section 3.4, in
previous experiments, we automatically extracted
prototypes for each label, and propagated the in-
formation via distributional similarities. Intu-
itively, the prototypes we used should be more ef-
fective in determining fine-grained NE types than
identifying whether a word is an NE. To verify
this, we extract new prototypes considering only
two labels, namely, NE and non-NE, using the
same metric in Section 3.4. As shown in the last
row of Table 5, higher performance is achieved.
6 Related Studies
Semi-supervised learning with generalized word
representations is a simple and general way of im-
proving supervised NLP systems. One common
approach for inducing generalized word represen-
tations is to use clustering (e.g., Brown clustering)
(Miller et al., 2004; Liang, 2005; Koo et al., 2008;
Huang and Yates, 2009).
Aside from word clustering, word embeddings
have been widely studied. Bengio et al. (2003)
propose a feed-forward neural network based lan-
guage model (NNLM), which uses an embedding
layer to map each word to a dense continuous-
valued and low-dimensional vector (parameters),
and then use these vectors as the input to predict
the probability distribution of the next word. The
NNLM can be seen as a joint learning framework
for language modeling and word representations.
Alternative models for learning word embed-
dings are mostly inspired by the feed-forward
NNLM, including the Hierarchical Log-Bilinear
Model (Mnih and Hinton, 2008), the recurrent
neural network language model (Mikolov, 2012),
the C&W model (Collobert et al., 2011), the log-
linear models such as the CBOW and the Skip-
118
gram model (Mikolov et al., 2013a; Mikolov et
al., 2013b).
Aside from the NNLMs, word embeddings can
also be induced using spectral methods, such as
latent semantic analysis and canonical correlation
analysis (Dhillon et al., 2011). The spectral meth-
ods are generally faster but much more memory-
consuming than NNLMs.
There has been a plenty of work that exploits
word embeddings as features for semi-supervised
learning, most of which take the continuous fea-
tures directly in linear models (Turian et al., 2010;
Guo et al., 2014). Yu et al. (2013) propose com-
pound k-means cluster features based on word em-
beddings. They show that the high-dimensional
discrete cluster features can be better utilized by
linear models such as CRF. Wu et al. (2013) fur-
ther apply the cluster features to transition-based
dependency parsing.
7 Conclusion and Future Work
This paper revisits the problem of semi-supervised
learning with word embeddings. We present three
different approaches for a careful comparison and
analysis. Using any of the three embedding fea-
tures, we obtain higher performance than the di-
rect use of continuous embeddings, among which
the distributional prototype features perform the
best, showing the great potential of word embed-
dings. Moreover, the combination of the proposed
embedding features provides significant additive
improvements.
We give detailed analysis about the experimen-
tal results. Analysis on rare words and linear sep-
arability provides convincing explanations for the
performance of the embedding features.
For future work, we are exploring a novel and a
theoretically more sounding approach of introduc-
ing embedding kernel into the linear models.
Acknowledgments
We are grateful to Mo Yu for the fruitful discus-
sion on the implementation of the cluster-based
embedding features. We also thank Ruiji Fu,
Meishan Zhang, Sendong Zhao and the anony-
mous reviewers for their insightful comments and
suggestions. This work was supported by the
National Key Basic Research Program of China
via grant 2014CB340503 and the National Natu-
ral Science Foundation of China (NSFC) via grant
61133012 and 61370164.
References
Rie Kubota Ando and Tong Zhang. 2005. A high-
performance semi-supervised learning method for
text chunking. In Proceedings of the 43rd annual
meeting on association for computational linguis-
tics, pages 1?9. Association for Computational Lin-
guistics.
Yoshua Bengio, R. E. Jean Ducharme, Pascal Vincent,
and Christian Janvin. 2003. A neural probabilistic
language model. The Journal of Machine Learning
Research, 3(Feb):1137?1155.
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation learning: A review and new
perspectives. Pattern Analysis and Machine Intelli-
gence, IEEE Transactions on, 35(8):1798?1828.
Gerlof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. Proceedings
of GSCL, pages 31?40.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Ronan Collobert, Jason Weston, L. E. On Bottou,
Michael Karlen, Koray Kavukcuoglu, and Pavel
Kuksa. 2011. Natural language processing (almost)
from scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Paramveer S. Dhillon, Dean P. Foster, and Lyle H. Un-
gar. 2011. Multi-view learning of word embeddings
via cca. In NIPS, volume 24 of NIPS, pages 199?
207.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363?370. Association for Computational Lin-
guistics.
Yoav Goldberg and Omer Levy. 2014. word2vec ex-
plained: deriving mikolov et al.?s negative-sampling
word-embedding method. CoRR, abs/1402.3722.
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Learning sense-specific word embed-
dings by exploiting bilingual resources. In Pro-
ceedings of COLING 2014, the 25th International
Conference on Computational Linguistics: Techni-
cal Papers, pages 497?507, Dublin, Ireland, August.
Dublin City University and Association for Compu-
tational Linguistics.
119
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the main conference on Human Language Technol-
ogy Conference of the North American Chapter of
the Association of Computational Linguistics, pages
320?327. Association for Computational Linguis-
tics.
Fei Huang and Alexander Yates. 2009. Distribu-
tional representations for handling sparsity in super-
vised sequence-labeling. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 1-
Volume 1, Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP: Volume 1-Volume 1, pages
495?503.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proc. of the Annual Meeting of the
Association for Computational Linguistics (ACL),
Proc. of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 873?882,
Jeju Island, Korea. ACL.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency pars-
ing. In Kathleen McKeown, Johanna D. Moore, Si-
mone Teufel, James Allan, and Sadaoki Furui, edi-
tors, Proc. of ACL-08: HLT, Proc. of ACL-08: HLT,
pages 595?603, Columbus, Ohio. ACL.
Vijay Krishnan and Christopher D Manning. 2006.
An effective two-stage model for exploiting non-
local dependencies in named entity recognition. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 1121?1128. Association for Compu-
tational Linguistics.
Percy Liang. 2005. Semi-supervised learning for natu-
ral language. Master thesis, Massachusetts Institute
of Technology.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word repre-
sentations in vector space. In Proc. of Workshop at
ICLR, Proc. of Workshop at ICLR, Arizona.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proc. of the NIPS, Proc. of the NIPS, pages
3111?3119, Nevada. MIT Press.
Tomas Mikolov. 2012. Statistical Language Models
Based on Neural Networks. Ph. d. thesis, Brno Uni-
versity of Technology.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In HLT-NAACL, volume 4, pages
337?342.
Andriy Mnih and Geoffrey E. Hinton. 2008. A scal-
able hierarchical distributed language model. In
Proc. of the NIPS, Proc. of the NIPS, pages 1081?
1088, Vancouver. MIT Press.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT, pages 380?390.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning, CoNLL ?09,
pages 147?155, Stroudsburg, PA, USA. Association
for Computational Linguistics.
D Sculley. 2010. Combined regression and ranking.
In Proceedings of the 16th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 979?988. ACM.
Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 142?147. Association for Computational Lin-
guistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Jan Hajic, San-
dra Carberry, and Stephen Clark, editors, Proc. of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL), Proc. of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 384?394, Uppsala, Sweden. ACL.
Mengqiu Wang and Christopher D. Manning. 2013.
Effect of non-linear deep architecture in sequence la-
beling. In Proc. of the Sixth International Joint Con-
ference on Natural Language Processing, Proc. of
the Sixth International Joint Conference on Natural
Language Processing, pages 1285?1291, Nagoya,
Japan. Asian Federation of Natural Language Pro-
cessing.
Xianchao Wu, Jie Zhou, Yu Sun, Zhanyi Liu, Dian-
hai Yu, Hua Wu, and Haifeng Wang. 2013. Gener-
alization of words for chinese dependency parsing.
IWPT-2013, page 73.
Mo Yu, Tiejun Zhao, Daxiang Dong, Hao Tian, and Di-
anhai Yu. 2013. Compound embedding features for
semi-supervised learning. In Proc. of the NAACL-
HLT, Proc. of the NAACL-HLT, pages 563?568, At-
lanta. NAACL.
120
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142?146,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Improve Statistical Machine Translation with Context-Sensitive
Bilingual Semantic Embedding Model
Haiyang Wu
1
Daxiang Dong
1
Wei He
1
Xiaoguang Hu
1
Dianhai Yu
1
Hua Wu
1
Haifeng Wang
1
Ting Liu
2
1
Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China
2
Harbin Institute of Technology, Harbin, China
wuhaiyang,dongdaxiang,hewei,huxiaoguang,yudianhai,
wu hua,wanghaifeng@baidu.com
tliu@ir.hit.edu.cn
Abstract
We investigate how to improve bilingual
embedding which has been successfully
used as a feature in phrase-based sta-
tistical machine translation (SMT). De-
spite bilingual embedding?s success, the
contextual information, which is of criti-
cal importance to translation quality, was
ignored in previous work. To employ
the contextual information, we propose
a simple and memory-efficient model for
learning bilingual embedding, taking both
the source phrase and context around the
phrase into account. Bilingual translation
scores generated from our proposed bilin-
gual embedding model are used as features
in our SMT system. Experimental results
show that the proposed method achieves
significant improvements on large-scale
Chinese-English translation task.
1 Introduction
In Statistical Machine Translation (SMT) sys-
tem, it is difficult to determine the translation of
some phrases that have ambiguous meanings.For
example, the phrase??? jieguo? can be trans-
lated to either ?results?, ?eventually? or ?fruit?,
depending on the context around it. There are two
reasons for the problem: First, the length of phrase
pairs is restricted due to the limitation of model
size and training data. Another reason is that SMT
systems often fail to use contextual information
in source sentence, therefore, phrase sense disam-
biguation highly depends on the language model
which is trained only on target corpus.
To solve this problem, we present to learn
context-sensitive bilingual semantic embedding.
Our methodology is to train a supervised model
where labels are automatically generated from
phrase-pairs. For each source phrase, the aligned
target phrase is marked as the positive label
whereas other phrases in our phrase table are
treated as negative labels. Different from previ-
ous work in bilingual embedding learning(Zou et
al., 2013; Gao et al., 2014), our framework is a
supervised model that utilizes contextual informa-
tion in source sentence as features and make use
of phrase pairs as weak labels. Bilingual seman-
tic embeddings are trained automatically from our
supervised learning task.
Our learned bilingual semantic embedding
model is used to measure the similarity of phrase
pairs which is treated as a feature in decoding. We
integrate our learned model into a phrase-based
translation system and experimental results indi-
cate that our system significantly outperform the
baseline system. On the NIST08 Chinese-English
translation task, we obtained 0.68 BLEU improve-
ment. We also test our proposed method on much
larger web dataset and obtain 0.49 BLEU im-
provement against the baseline.
2 Related Work
Using vectors to represent word meanings is
the essence of vector space models (VSM). The
representations capture words? semantic and syn-
tactic information which can be used to measure
semantic similarities by computing distance be-
tween the vectors. Although most VSMs represent
one word with only one vector, they fail to cap-
ture homonymy and polysemy of word. Huang
et al. (2012) introduced global document context
and multiple word prototypes which distinguishes
and uses both local and global context via a joint
training objective. Much of the research focus
on the task of inducing representations for sin-
gle languages. Recently, a lot of progress has
142
been made at representation learning for bilin-
gual words. Bilingual word representations have
been presented by Peirsman and Pad?o (2010) and
Sumita (2000). Also unsupervised algorithms
such as LDA and LSA were used by Boyd-Graber
and Resnik (2010), Tam et al. (2007) and Zhao and
Xing (2006). Zou et al. (2013) learn bilingual em-
beddings utilizes word alignments and monolin-
gual embeddings result, Le et al. (2012) and Gao et
al. (2014) used continuous vector to represent the
source language or target language of each phrase,
and then computed translation probability using
vector distance. Vuli?c and Moens (2013) learned
bilingual vector spaces from non-parallel data in-
duced by using a seed lexicon. However, none
of these work considered the word sense disam-
biguation problem which Carpuat and Wu (2007)
proved it is useful for SMT. In this paper, we learn
bilingual semantic embeddings for source content
and target phrase, and incorporate it into a phrase-
based SMT system to improve translation quality.
3 Context-Sensitive Bilingual Semantic
Embedding Model
We propose a simple and memory-efficient
model which embeds both contextual information
of source phrases and aligned phrases in target cor-
pus into low dimension. Our assumption is that
high frequent words are likely to have multiple
word senses; therefore, top frequent words are se-
lected in source corpus. We denote our selected
words as focused phrase. Our goal is to learn a
bilingual embedding model that can capture dis-
criminative contextual information for each fo-
cused phrase. To learn an effective context sensi-
tive bilingual embedding, we extract context fea-
tures nearby a focused phrase that will discrimi-
nate focused phrase?s target translation from other
possible candidates. Our task can be viewed as
a classification problem that each target phrase is
treated as a class. Since target phrases are usu-
ally in very high dimensional space, traditional
linear classification model is not suitable for our
problem. Therefore, we treat our problem as a
ranking problem that can handle large number of
classes and optimize the objectives with scalable
optimizer stochastic gradient descent.
3.1 Bilingual Word Embedding
We apply a linear embedding model for bilin-
gual embedding learning. Cosine similarity be-
tween bilingual embedding representation is con-
sidered as score function. The score function
should be discriminative between target phrases
and other candidate phrases. Our score function
is in the form:
f(x,y; W,U) = cos(W
T
x,U
T
y) (1)
where x is contextual feature vector in source sen-
tence, and y is the representation of target phrase,
W ? R
|X|?k
,U ? R
|Y|?k
are low rank ma-
trix. In our model, we allow y to be bag-of-words
representation. Our embedding model is memory-
efficient in that dimensionality of x and y can be
very large in practical setting. We use |X| and |Y|
means dimensionality of random variable x and y,
then traditional linear model such as max-entropy
model requires memory space of O(|X||Y|). Our
embedding model only requires O(k(|X|+ |Y|))
memory space that can handle large scale vocabu-
lary setting. To score a focused phrase and target
phrase pair with f(x,y), context features are ex-
tracted from nearby window of the focused phrase.
Target words are selected from phrase pairs. Given
a source sentence, embedding of a focused phrase
is estimated from W
T
x and target phrase embed-
ding can be obtained through U
T
y.
3.2 Context Sensitive Features
Context of a focused phrase is extracted from
nearby window, and in our experiment we choose
window size of 6 as a focused phrase?s con-
text. Features are then extracted from the focused
phrase?s context. We demonstrate our feature
extraction and label generation process from the
Chinese-to-English example in figure 1. Window
size in this example is three. Position features
and Part-Of-Speech Tagging features are extracted
from the focused phrase?s context. The word fruit
Figure 1: Feature extraction and label generation
143
is the aligned phrase of our focused phrase and is
treated as positive label. The phrase results is a
randomly selected phrase from phrase table results
of ??. Note that feature window is not well de-
fined near the beginning or the end of a sentence.
To conquer this problem, we add special padding
word to the beginning and the end of a sentence to
augment sentence.
3.3 Parameter Learning
To learn model parameter W and U, we ap-
ply a ranking scheme on candidates selected from
phrase table results of each focused phrase. In par-
ticular, given a focus phrase w, aligned phrase is
treated as positive label whereas phrases extracted
from other candidates in phrase table are treated
as negative label. A max-margin loss is applied in
this ranking setting.
I(?) =
1
m
m
?
i=1
(? ? f(x
i
, y
i
; ?)? f(x
i
, y
?
i
; ?))+
(2)
Where f(x
i
,y
i
) is previously defined, ? =
{W,U} and + means max-margin hinge loss. In
our implementation, a margin of ? = 0.15 is used
during training. Objectives are minimized through
stochastic gradient descent algorithm. For each
randomly selected training example, parameters
are updated through the following form:
? := ?? ?
?l(?)
??
(3)
where ? = {W,U}. Given an instance with pos-
itive and negative label pair {x,y,y
?
}, gradients
of parameter W and U are as follows:
?l(W,U)
?W
= qsx(W
T
x)
T
? pqs
3
x(U
T
y) (4)
?l(W,U)
?U
= qsy(U
T
y)
T
? pqs
3
y(W
T
x) (5)
Where we set p = (W
T
x)
T
(U
T
y), q =
1
||W
T
x||
2
and s =
1
||U
T
y||
2
. To initialize our model param-
eters with strong semantic and syntactic informa-
tion, word vectors are pre-trained independently
on source and target corpus through word2vec
(Mikolov et al., 2013). And the pre-trained word
vectors are treated as initial parameters of our
model. The learned scoring function f(x,y) will
be used during decoding phase as a feature in log-
linear model which we will describe in detail later.
4 Integrating Bilingual Semantic
Embedding into Phrase-Based SMT
Architectures
To incorporate the context-sensitive bilingual
embedding model into the state-of-the-art Phrase-
Based Translation model, we modify the decoding
so that context information is available on every
source phrase. For every phrase in a source sen-
tence, the following tasks are done at every node
in our decoder:
? Get the focused phrase as well as its context in the
source sentence.
? Extract features from the focused phrase?s context.
? Get translation candidate extracted from phrase pairs of
the focused phrase.
? Compute scores for any pair of the focused phrase and
a candidate phrase.
We get the target sub-phrase using word align-
ment of phrase, and we treat NULL as a common
target word if there is no alignment for the focused
phrase. Finally we compute the matching score for
source content and target word using bilingual se-
mantic embedding model. If there are more than
one word in the focus phrase, then we add all score
together. A penalty value will be given if target is
not in translation candidate list. For each phrase in
a given SMT input sentence, the Bilingual Seman-
tic score can be used as an additional feature in
log-linear translation model, in combination with
other typical context-independent SMT bilexicon
probabilities.
5 Experiment
Our experiments are performed using an in-
house phrase-based system with a log-linear
framework. Our system includes a phrase trans-
lation model, an n-gram language model, a lexi-
calized reordering model, a word penalty model
and a phrase penalty model, which is similar to
Moses (Koehn et al., 2007). The evaluation metric
is BLEU (Papineni et al., 2002).
5.1 Data set
We test our approach on LDC corpus first. We
just use a subset of the data available for NIST
OpenMT08 task
1
. The parallel training corpus
1
LDC2002E18, LDC2002L27, LDC2002T01,
LDC2003E07, LDC2003E14, LDC2004T07, LDC2005E83,
LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E24,
LDC2006E26, LDC2006E34, LDC2006E86, LDC2006E92,
LDC2006E93, LDC2004T08(HK News, HK Hansards )
144
Method
OpenMT08 WebData
BLEU BLEU
Our Baseline 26.24 29.32
LOC 26.78** 29.62*
LOC+POS 26.82** 29.81*
Table 1: Results of lowercase BLEU on NIST08
task. LOC is the location feature and POS is
the Part-of-Speech feature * or ** equals to sig-
nificantly better than our baseline(? < 0.05 or
? < 0.01, respectively)
contains 1.5M sentence pairs after we filter with
some simple heuristic rules, such as sentence be-
ing too long or containing messy codes. As mono-
lingual corpus, we use the XinHua portion of the
English GigaWord. In monolingual corpus we fil-
ter sentence if it contain more than 100 words
or contain messy codes, Finally, we get mono-
lingual corpus containing 369M words. In order
to test our approach on a more realistic scenario,
we train our models with web data. Sentence
pairs obtained from bilingual website and com-
parable webpage. Monolingual corpus is gained
from some large website such as WiKi. There are
50M sentence pairs and 10B words monolingual
corpus.
5.2 Results and Analysis
For word alignment, we align all of the train-
ing data with GIZA++ (Och and Ney, 2003), us-
ing the grow-diag-final heuristic to improve recall.
For language model, we train a 5-gram modified
Kneser-Ney language model and use Minimum
Error Rate Training (Och, 2003) to tune the SMT.
For both OpenMT08 task and WebData task, we
use NIST06 as the tuning set, and use NIST08 as
the testing set. Our baseline system is a standard
phrase-based SMT system, and a language model
is trained with the target side of bilingual corpus.
Results on Chinese-English translation task are re-
ported in Table 1. Word position features and part-
of-speech tagging features are both useful for our
bilingual semantic embedding learning. Based on
our trained bilingual embedding model, we can
easily compute a translation score between any
bilingual phrase pair. We list some cases in table
2 to show that our bilingual embedding is context
sensitive.
Contextual features extracted from source sen-
tence are strong enough to discriminate different
Source Sentence
4 Nearest Neighbor from
bilingual embedding
??????????
?????????
?????(Investors
can only get down to
business in a stable so-
cial environment)
will be, can only, will, can
??????????
?????????
?????(In compe-
titions, the Chinese Dis-
abled have shown ex-
traordinary athletic abil-
ities)
skills, ability, abilities, tal-
ent
??????????
?????????
????(In the natu-
ral environment of Costa
Rica, grapes do not nor-
mally yield fruit.)
fruit, outcome of, the out-
come, result
? ? ??????
???????(As
a result, Eastern District
Council passed a pro-
posal)
in the end, eventually, as a
result, results
Table 2: Top ranked focused phrases based on
bilingual semantic embedding
word senses. And we also observe from the word
??? jieguo? that Part-Of-Speech Tagging fea-
tures are effective in discriminating target phrases.
6 Conlusion
In this paper, we proposed a context-sensitive
bilingual semantic embedding model to improve
statistical machine translation. Contextual infor-
mation is used in our model for bilingual word
sense disambiguation. We integrated the bilingual
semantic model into the phrase-based SMT sys-
tem. Experimental results show that our method
achieves significant improvements over the base-
line on large scale Chinese-English translation
task. Our model is memory-efficient and practical
for industrial usage that training can be done on
large scale data set with large number of classes.
Prediction time is also negligible with regard to
SMT decoding phase. In the future, we will ex-
plore more features to refine the model and try to
utilize contextual information in target sentences.
Acknowledgments
We thank the three anonymous reviewers for
their valuable comments, and Niu Gang and Wu
Xianchao for discussions. This paper is supported
by 973 program No. 2014CB340505.
145
References
Jordan Boyd-Graber and Philip Resnik. 2010. Holis-
tic sentiment analysis across languages: Multilin-
gual supervised latent dirichlet allocation. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 45?55,
Cambridge, MA, October. Association for Compu-
tational Linguistics.
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61?72, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2014. Learning continuous phrase rep-
resentations for translation modeling. In Proc. ACL.
Eric Huang, Richard Socher, Christopher Manning,
and Andrew Ng. 2012. Improving word represen-
tations via global context and multiple word proto-
types. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 873?882, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies, pages 39?48, Montr?eal,
Canada, June. Association for Computational Lin-
guistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS, pages 3111?3119.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. In Computational Linguistics, Volume 29,
Number 1, March 2003. Computational Linguistics,
March.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Yves Peirsman and Sebastian Pad?o. 2010. Cross-
lingual induction of selectional preferences with
bilingual vector spaces. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 921?929, Los Ange-
les, California, June. Association for Computational
Linguistics.
Eiichiro Sumita. 2000. Lexical transfer using a vector-
space model. In Proceedings of the 38th Annual
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics,
August.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual-lsa based lm adaptation for spoken lan-
guage translation. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 520?527, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
Ivan Vuli?c and Marie-Francine Moens. 2013. Cross-
lingual semantic similarity of words as the similarity
of their semantic word responses. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 106?116, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Bing Zhao and Eric P. Xing. 2006. Bitam: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING/ACL 2006 Main Confer-
ence Poster Sessions, pages 969?976, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual word embed-
dings for phrase-based machine translation. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1393?
1398, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
146
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 147?152,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Transformation from Discontinuous to Continuous Word Alignment
Improves Translation Quality
Zhongjun He
1
Hua Wu
1
Haifeng Wang
1
Ting Liu
2
1
Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China
2
Harbin Institute of Technology, Harbin, China
{hezhongjun,wu hua,wanghaifeng}@baidu.com
tliu@ir.hit.edu.cn
Abstract
We present a novel approach to im-
prove word alignment for statistical ma-
chine translation (SMT). Conventional
word alignment methods allow discontin-
uous alignment, meaning that a source
(or target) word links to several target (or
source) words whose positions are dis-
continuous. However, we cannot extrac-
t phrase pairs from this kind of align-
ments as they break the alignment con-
sistency constraint. In this paper, we use
a weighted vote method to transform dis-
continuous word alignment to continuous
alignment, which enables SMT system-
s extract more phrase pairs. We carry
out experiments on large scale Chinese-
to-English and German-to-English trans-
lation tasks. Experimental results show
statistically significant improvements of
BLEU score in both cases over the base-
line systems. Our method produces a gain
of +1.68 BLEU on NIST OpenMT04 for
the phrase-based system, and a gain of
+1.28 BLEU on NIST OpenMT06 for the
hierarchical phrase-based system.
1 Introduction
Word alignment, indicating the correspondence
between the source and target words in bilingual
sentences, plays an important role in statistical
machine translation (SMT). Almost all of the SMT
models, not only phrase-based (Koehn et al.,
2003), but also syntax-based (Chiang, 2005; Liu
et al., 2006; Huang et al., 2006), derive translation
knowledge from large amount bilingual text anno-
tated with word alignment. Therefore, the quality
of the word alignment has big impact on the qual-
ity of translation output.
Word alignments are usually automatically ob-
tained from a large amount of bilingual training
corpus. The most widely used toolkit for word
alignment in SMT community is GIZA++ (Och
and Ney, 2004), which implements the well known
IBM models (Brown et al., 1993) and the HM-
M model (Vogel and Ney, 1996). Koehn et al.
(2003) proposed some heuristic methods (e.g. the
?grow-diag-final? method) to refine word align-
ments trained by GIZA++. Another group of word
alignment methods (Liu et al., 2005; Moore et
al., 2006; Riesa and Marcu, 2010) define feature
functions to describe word alignment. They need
manually aligned bilingual texts to train the mod-
el. However, the manually annotated data is too
expensive to be available for all languages. Al-
though these models reported high accuracy, the
GIZA++ and ?grow-diag-final? method are domi-
nant in practice.
However, automatic word alignments are usu-
ally very noisy. The example in Figure 1 shows
a Chinese and English sentence pair, with word
alignment automatically trained by GIZA++ and
the ?grow-diag-final? method. We find many er-
rors (dashed links) are caused by discontinuous
alignment (formal definition is described in Sec-
tion 2), a source (or target) word linking to sev-
eral discontinuous target (or source) words. This
kind of errors will result in the loss of many use-
ful phrase pairs that are learned based on bilingual
word alignment. Actually, according to the defini-
tion of phrases in a standard phrase-based model,
we cannot extract phrases from the discontinuous
alignment. The reason is that this kind of align-
ment break the alignment consistency constrain-
t (Koehn et al., 2003). For example, the Chi-
147
1{I
meiguo
2
?
shi
3
?
shaoshu
4
A?
jige
5
?
tou
6
e
xia
7
??
fandui
8
?
piao
9

de
10
I[
guojia
11
??
zhiyi
The
1
United
2
States
3
was
4
among
5
the
6
handful
7
of
8
nations
9
that
10
cast
11
a
12
nay
13
note
14
Figure 1: An example of word alignment between a Chinese and English sentence pair. The dashed links
are incorrect alignments.
nese word ?shi
2
?
1
is aligned to the English words
?was
4
? and ?that
10
?. However, these two English
words are discontinuous, and we cannot extract the
phrase pair ?(shi, was)?.
In this paper, we propose a simple weighed vote
method to deal with the discontinuous word align-
ment. Firstly, we split the discontinuous align-
ment into several continuous alignment group-
s, and consider each continuous alignment group
as a bucket. Secondly, we vote for each buck-
et with alignment score measured by word trans-
lation probabilities. Finally, we select the buck-
et with the highest score as the final alignment.
The strength of our method is that we refine word
alignment without using any external knowledge,
as the word translation probabilities can be esti-
mated from the bilingual corpus with the original
word alignment.
We notice that the discontinuous alignment is
helpful for hierarchical phrase-based model, as the
model allows discontinuous phrases. Thus, for
the hierarchical phrase-based model, our method
may lost some discontinuous phrases. To solve
the problem, we keep the original discontinuous
alignment in the training corpus.
We carry out experiment with the state-of-the-
art phrase-based and hierarchical phrase-based
(Chiang, 2005) SMT systems implemented in
Moses (Koehn et al., 2007). Experiments on large
scale Chinese-to-English and German-to-English
translation tasks demonstrate significant improve-
ments in both cases over the baseline systems.
2 The Weighted Vote Method
To refine the discontinuous alignment, we propose
a weighted vote method to transform discontinu-
ous alignment to continuous alignment by discard-
ing noisy links. We split discontinuous alignment
1
The subscript denotes the word position.
into several continuous groups, and select the best
group with the highest score computed by word
translation probabilities as the final alignment.
For further understanding, we first describe
some definitions. Given a word-aligned sentence
pair (F
I
1
, E
J
1
, A), an alignment set A
set
(i) is the
set of target word positions that aligned to the
source word F
i
i
:
A
set
(i) = {j|(i, j) ? A} (1)
For example, in Figure 1, the alignment set
for the Chinese word ?shaoshu
3
? is A
set
(3) =
{5, 7, 8, 10}. We define an alignment s-
pan A
span
(i) as [min(A
set
(i)),max(A
set
(i))].
Thus, the alignment span for the Chinese word
?shaoshu
3
? is A
span
(3) = [5, 10].
The alignment for F
i
i
is discontinuous if there
exist some target words in A
span
(i) linking to an-
other source word, i.e. ?(i
?
, j
?
) ? A, where i
?
6= i,
j
?
? A
span
(i). Otherwise, the alignment is contin-
uous. According to the definition, the alignment
for ?shaoshu
3
? is discontinuous. Because the tar-
get words ?the
6
? and ?nations
9
? in the alignmen-
t span link to another Chinese words ?de
9
? and
?guojia
10
?, respectively. For a target word E
j
j
, the
definition is similar.
If the alignment for F
i
i
is discontinuous, we
can split the alignment span A
span
(i) = [j
1
, j
2
]
into m continuous spans {[j
k
p
, j
k
q
]}, where k =
1, 2, ...,m, and j
k
p
, j
k
q
? [j
1
, j
2
]. Our goal is to se-
lect the best continuous span for the word F
i
i
. To
do this, we score each continuous span with word
translation probabilities:
S([j
k
p
, j
k
q
]) =
q
?
t=p
(Pr(E
j
k
t
|F
i
) + Pr(F
i
|E
j
k
t
))
(2)
where,
Pr(f |e) =
count(f, e)
?
f
?
count(f
?
, e)
(3)
148
am
o
n
g
t
h
e
h
a
n
d
f
u
l
o
f
n
a
t
i
o
n
s
t
h
a
t
?? shaoshu 0.1 0.5 0.2 0.1
Figure 2: An example of weighted voted method
for selecting the best continuous alignment from
the discontinuous alignment. The heavy shading
area is selected as the final alignment.
Pr(e|f) =
count(e, f)
?
e
?
count(f, e
?
)
(4)
The word translation probabilities can be comput-
ed from the bilingual corpus with the initial word
alignment. Finally, we select the span with the
highest score as the final alignment, and discard
all other alignments.
We illustrate our method in Figure 2, which
shows the source word ?shaoshu? and its align-
ment in Figure 1. We split the alignments into
three continuous alignment spans and compute s-
core for each span. Finally, the span with highest
score (heavy shading area) is selected as the final
alignment.
We conduct the procedure for each source and
target word, the improved alignment (solid links)
is shown in Figure 1.
3 Experiment
To demonstrate the effect of the proposed method,
we use the state-of-the-art phrase-based system
and hierarchical phrase-based system implement-
ed in Moses (Koehn et al., 2007). The phrase-
based system uses continuous phrase pair as the
main translation knowledge. While the hierarchi-
cal phrase-based system uses both continuous and
discontinuous phrase pairs, which has an ability to
capture long distance phrase reordering.
we carried out experiments on two translation
tasks: the Chinese-to-English task comes from the
NIST Open MT Evaluation, and the German-to-
English task comes from the Workshop on Ma-
chine Translation (WMT) shared task.
3.1 Training
The training data we used are listed in Table 1. For
the Chinese-English task, the bilingual data are s-
elected from LDC. We used NIST MT03 as the
development set and tested our system on NIST
MT evaluation sets from 2004 to 2008. For the
German-English task, the bilingual data are from
Task Src. Words Tgt. Words
Chinese-to-English 75M 78M
German-to- English 107M 113M
Table 1: Bilingual data for our experiments.
System N04 N05 N06 N08
Baseline 34.53 33.02 30.43 23.29
Refined 36.21 33.99 31.59 24.36
Table 2: Chinese-to-English translation quality of
the phrase-based system.
System W10 W11 W12 W13
Baseline 20.71 20.26 20.52 23.26
Refined 21.46 20.95 21.11 23.77
Table 3: German-to-English translation quality of
the phrase-based system.
the shared translation task 2013. We used WMT08
as the development set and tested our system on
WMT test sets from 2010 to 2013.
The baseline systems are trained on the training
corpus with initial word alignment, which was ob-
tained via GIZA++ and ?grow-diag-final? method.
Based on the initial word alignment, we comput-
ed word translation probabilities and used the pro-
posed method to obtain a refined word alignment.
Then we used the refined word alignment to train
our SMT systems.
The translation results are evaluated by case-
insensitive BLEU-4 (Papineni et al., 2002).
The feature weights of the translation system
are tuned with the standard minimum-error-rate-
training (Och, 2003) to maximize the systems
BLEU score on the development set.
3.2 Results
3.2.1 Phrase-based System
Table 2 shows Chinese-to-English translation
quality of the phrase-based system. We ob-
served that our refined method significantly out-
performed the baseline word alignment on all test
sets. The improvements are ranged from 0.97 to
1.68 BLEU%.
Table 3 shows German-to-English translation
quality of the phrase-based system. The improve-
ments are ranged from 0.51 to 0.75 BLEU%.
These results demonstrate that the proposed
method improves the translation quality for
149
System N04 N05 N06 N08
Baseline 37.33 34.81 32.20 25.33
Refined 37.91 35.36 32.75 25.40
Combined 38.13 35.63 33.48 25.66
Table 4: Chinese-to-English translation quality of
the hierarchical phrase-based system.
System W10 W11 W12 W13
Baseline 21.22 19.77 20.53 23.51
Refined 21.34 20.64 20.88 23.82
Combined 21.65 20.87 21.16 24.04
Table 5: German-to-English translation quality of
the hierarchical phrase-based system.
phrase-based system. The reason is that by dis-
carding noisy word alignments from the discon-
tinuous alignments, the phrase pairs constrained
by the noisy alignments can be extracted. Thus the
system utilized more phrase pairs than the baseline
did.
3.2.2 Hierarchical Phrase-based System
The hierarchical phrase-based system utilizes dis-
continuous phrase pairs for long distance phrase
reordering. Some of the discontinuous phrase
pairs are extracted from the discontinuous align-
ments. By transforming the discontinuous align-
ments to continuous alignments, on the one hand,
we may lost some discontinuous phrase pairs. On
the other hand, we may extract additional contin-
uous and discontinuous phrase pairs as the align-
ment restriction is loose.
See Figure 3 for illustration. From the initial
alignment, we can extract a hierarchical phrase
pair ?(dang X
1
shi, when X
1
)? from the discon-
tinuous alignment of the English word ?when?.
However, the hierarchical phrase pair cannot be
extracted from our refined alignment, because our
method discards the link between the Chinese
word ?dang? and the English word ?when?. In-
stead, we can extract another hierarchical phrase
pair ?(X
1
shi, when X
1
)?.
Does our method still obtain improvements on
the hierarchical phrase-based system? Table 4 and
Table 5 shows Chinese-to-English and German-
to-English translation quality of the hierarchical
phrase-based system, respectively. For Chinese-
to-English translation, the refined alignment ob-
tained improvements ranged from 0.07 to 0.58

dang
?
shigu
u)
fasheng
?
shi
when the accident
happend
Figure 3: Example of word alignment between a
Chinese and English sentence pair. The dashed
initial link is discarded by our method.
BLEU% on the test set ( the row ?Refined?).
While for German-to-English translation, the im-
provements ranged from 0.12 to 0.59 BLEU% on
the test set (the row ?Refined?).
We find that the improvements are less than
that of the phrase-based system. As discussed
above, our method may lost some hierarchical
phrase pairs that extracted from the discontinuous
alignments. To solve the problem, we combine
2
the initial alignments and the refined alignments
to train the SMT system. The results are shown
in the row ?Combined? in Table 4 and Table 5.
For Chinese-to-English translation, we obtained
an improvements of 1.28 BLEU% on NIST06 over
the baseline. While for German-to-English trans-
lation, the greatest improvements is 1.10 BLEU%
on WMT11.
4 Analyses
In order to further study the performance of the
proposed method, we analyze the word alignment
and the phrase table for Chinese-to-English trans-
lation. We find that our method improves the qual-
ity of word alignment. And as a result, more useful
phrase pairs are extracted from the refined word
alignment.
4.1 Word Alignment
The Chinese-to-English training corpus contains
4.5M sentence pairs. By applying GIZA++ and
the ?grow-diag-final? method, we obtained initial
alignments. We find that 4.0M (accounting for
89%) sentence pairs contain discontinuous align-
ments. We then used the proposed method to dis-
card noisy links. By doing this, the total links
between words in the training corpus are reduced
from 99.6M to 78.9M, indicating that 21% links
are discarded.
2
We do not perform combination for phrase-based sys-
tem, because the phrase table extracted from the initial align-
ment is a subset of that extracted from the refined alignment.
150
Alignment Precision Recall AER
Initial 62.94 89.55 26.07
Refined 73.43 87.82 20.01
Table 6: Precision, Recall and AER on Chinese-
to-English alignment.
Alignment StandPhr HierPhr
Initial 29M 86M
Refined 104M 436M
Table 7: The phrase number extracted from the
initial and refined alignment for the hierarchical
phrase-based system on Chinese-to-English trans-
lation. StandPhr is standard phrase, HierPhr is hi-
erarchical phrase.
We evaluated the alignment quality on 200 sen-
tence pairs. Results are shown in Table 6. It is
observed that our method improves the precision
and decreases the AER, while keeping a high re-
call. This means that our method effectively dis-
cards noisy links in the initial word alignments.
4.2 Phrase Table
According to the standard definition of phrase in
SMT, phrase pairs cannot be extracted from the
discontinuous alignments. By transforming dis-
continuous alignments into continuous alignmen-
t, we can extract more phrase pairs. Table 7
shows the number of standard phrases and hier-
archical phrases extracted from the initial and re-
fined word alignments. We find that the number of
both phrases and hierarchical phrases grows heav-
ily. This is because that the word alignment con-
straint for phrase extraction is loosed by removing
noisy links. Although the phrase table becomes
larger, fortunately, there are some methods (John-
son et al., 2007; He et al., 2009) to prune phrase
table without hurting translation quality.
For further illustration, we compare the phrase
pairs extracted from the initial alignment and re-
fined alignment in Figure 1. From the initial align-
ments, we extracted only 3 standard phrase pairs
and no hierarchical phrase pairs (Table 8). After
discarding noisy alignments (dashed links) by us-
ing the proposed method, we extracted 21 standard
phrase pairs and 36 hierarchical phrases. Table 9
and Table 10 show selected phrase pairs and hier-
archical phrase pairs, respectively.
Chinese English
meiguo The United States
guojia nations
piao note
Table 8: Phrase pairs extracted from the initial
alignment of Figure 1.
Chinese English
shi was
fandui piao a nay note
shaoshu jige the handful of
Table 9: Selected phrase pairs extracted from the
refined alignment of Figure 1.
Chinese English
X
1
zhiyi among X
1
X
1
de guojia nations that X
1
X
1
fandui piao X
2
X
2
X
1
a nay note
Table 10: Selected hierarchical phrase pairs ex-
tracted from the refined alignment of Figure 1.
5 Conclusion and Future Work
In this paper, we proposed a novel method to im-
prove word alignment for SMT. The method re-
fines initial word alignments by transforming dis-
continuous alignment to continuous alignment. As
a result, more useful phrase pairs are extracted
from the refined word alignment. Our method is
simple and efficient, since it uses only the word
translation probabilities obtained from the initial
alignments to discard noisy links. Our method
is independent of languages and can be applied
to most SMT models. Experimental results show
significantly improvements for the state-of-the-art
phrase-based and hierarchical phrase-based sys-
tems on all Chinese-to-English and German-to-
English translation tasks.
In the future, we will refine the method by con-
sidering neighbor words and alignments when dis-
carding noisy links.
Acknowlegement
This paper is supported by the 973 program No.
2014CB340505. We would like to thank Xuan Liu
and the anonymous reviewers for their insightful
comments.
151
References
Peter F. Brown, Stephen A. Della Pietra, Vincen-
t J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263?311.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 263?270.
Zhongjun He, Yao Meng, and Hao Yu. 2009. Dis-
carding monotone composed rule for hierarchical
phrase-based statistical machine translation. In Pro-
ceedings of the 3rd International Universal Commu-
nication Symposium, pages 25?29.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Bienni-
al Conference of the Association for Machine Trans-
lation in the Americas.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation quali-
ty by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 967?975, Prague, Czech Republic,
June.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of HLT-NAACL 2003, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL 2007 demonstration session.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Loglin-
ear models for word alignment. In Proceedings of
of ACL 2005, pages 459?466, Ann Arbor,Michigan,
June.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 44th Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 609?616.
Robert C. Moore, Wen tau Yih, and Andreas Bode.
2006. Improved discriminative bilingual word
alignment. In In Proceedings of COLING/ACL
2006, pages 513?520, Sydney, Australia, July.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. 30:417?449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 311?318.
Jason Riesa and Daniel Marcu. 2010. Hierarchical
search forword alignment. In In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 157?166, Uppsala, Swe-
den, July.
Stephan Vogel and Hermann Ney. 1996. Hmm-based
word alignment in statistical translation. In Pro-
ceedings of COLING 1996, pages 836?841, Copen-
hagen, Danmark, August.
152
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1665?1675,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Improving Pivot-Based Statistical Machine Translation by Pivoting 
the Co-occurrence Count of Phrase Pairs 
 
Xiaoning Zhu1*, Zhongjun He2, Hua Wu2, Conghui Zhu1,  
Haifeng Wang2, and Tiejun Zhao1 
Harbin Institute of Technology, Harbin, China1 
{xnzhu,chzhu,tjzhao}@mtlab.hit.edu.cn 
Baidu Inc., Beijing, China2 
{hezhongjun,wu_hua,wanghaifeng}@baidu.com 
 
 
                                                 
* This work was done when the first author was visiting Baidu. 
Abstract 
To overcome the scarceness of bilingual 
corpora for some language pairs in ma-
chine translation, pivot-based SMT uses 
pivot language as a "bridge" to generate 
source-target translation from source-
pivot and pivot-target translation. One of 
the key issues is to estimate the probabili-
ties for the generated phrase pairs. In this 
paper, we present a novel approach to 
calculate the translation probability by 
pivoting the co-occurrence count of 
source-pivot and pivot-target phrase pairs. 
Experimental results on Europarl data 
and web data show that our method leads 
to significant improvements over the 
baseline systems. 
1 Introduction 
Statistical Machine Translation (SMT) relies on 
large bilingual parallel data to produce high qual-
ity translation results. Unfortunately, for some 
language pairs, large bilingual corpora are not 
readily available. To alleviate the parallel data 
scarceness, a conventional solution is to intro-
duce a ?bridge? language (named pivot language) 
to connect the source and target language (de 
Gispert and Marino, 2006; Utiyama and Isahara, 
2007; Wu and Wang, 2007; Bertoldi et al., 2008; 
Paul et al., 2011; El Kholy et al., 2013; Zahabi et 
al., 2013), where there are large amounts of 
source-pivot and pivot-target parallel corpora. 
Among various pivot-based approaches, the 
triangulation method (Cohn and Lapata, 2007; 
Wu and Wang, 2007) is a representative work in 
pivot-based machine translation. The approach 
proposes to build a source-target phrase table by 
merging the source-pivot and pivot-target phrase 
table. One of the key issues in this method is to 
estimate the translation probabilities for the gen-
erated source-target phrase pairs. Conventionally, 
the probabilities are estimated by multiplying the 
posterior probabilities of source-pivot and pivot-
target phrase pairs. However, it has been shown 
that the generated probabilities are not accurate 
enough (Cui et al., 2013). One possible reason 
may lie in the non-uniformity of the probability 
space. Through Figure 1. (a), we can see that the 
probability distributions of source-pivot and piv-
ot-target language are calculated separately, and 
the source-target probability distributions are 
induced from the source-pivot and pivot-target 
probability distributions. Because of the absence 
of the pivot language (e.g., p2 is in source-pivot 
probability space but not in pivot-target one), the 
induced source-target probability distribution is 
not complete, which will result in inaccurate 
probabilities.  
To solve this problem, we propose a novel ap-
proach that utilizes the co-occurrence count of 
source-target phrase pairs to estimate phrase 
translation probabilities more precisely. Different 
from the triangulation method, which merges the 
source-pivot and pivot-target phrase pairs after 
training the translation model, we propose to 
merge the source-pivot and pivot-target phrase 
pairs immediately after the phrase extraction step, 
and estimate the co-occurrence count of the 
source-pivot-target phrase pairs. Finally, we 
compute the translation probabilities according 
to the estimated co-occurrence counts, using the 
standard training method in phrase-based SMT 
(Koehn et al., 2003). As Figure 1. (b) shows, the 
1665
source-target probability distributions are calcu-
lated in a complete probability space. Thus, it 
will be more accurate than the traditional trian-
gulation method. Figure 2. (a) and (b) show the 
difference between the triangulation method and 
our co-occurrence count method. 
Furthermore, it is common that a small stand-
ard bilingual corpus can be available between the 
source and target language. The direct translation 
model trained with the standard bilingual corpus 
exceeds in translation performance, but its weak-
ness lies in low phrase coverage. However, the 
pivot model has characteristics characters. Thus, 
it is important to combine the direct and pivot 
translation model to compensate mutually and 
further improve the translation performance. To 
deal with this problem, we propose a mixed 
model by merging the phrase pairs extracted by 
pivot-based method and the phrase pairs extract-
ed from the standard bilingual corpus. Note that, 
this is different from the conventional interpola-
tion method, which interpolates the direct and 
pivot translation model. See Figure 2. (b) and (c) 
for further illustration. 
(a) the triangulation method                         (b) the co-occurrence count method 
 
Figure 1: An example of probability space evolution in pivot translation. 
 
 
Large SP 
corpus
Large PT 
corpus
SP phrase 
pairs
PT phrase 
pairs
SP model PT model
ST pivot 
model
Phrase Extraction Phrase Extraction
Train Train
Merge
Standard 
ST corpus
ST phrase 
pairs
ST direct 
model
Phrase Extraction
Train
Interpolate
ST interpolated 
model
Large SP 
corpus
Large PT 
corpus
SP phrase 
pairs
ST pivot 
model
ST phrase 
pairs
Phrase Extraction Phrase Extraction
Train
PT phrase 
pairs
Merge
Standard 
ST corpus
ST phrase 
pairs
ST direct 
model
Phrase Extraction
Train
Interpolate
ST interpolated 
model
Large SP 
corpus
Large PT 
corpus
SP phrase 
pairs
PT phrase 
pairs
ST mixed 
pairs
ST phrase 
pairs
Phrase Extraction Phrase Extraction
Merge
Standard 
ST corpus
ST phrase 
pairs
Phrase Extraction
Train
ST mixed 
model
Mix
        (a) the triangulation method        (b) the co-occurrence count method            (c) the mixed model 
 
Figure 2: Framework of the triangulation method, the co-occurrence count method and the mixed 
model. The shaded box in (b) denotes difference between the co-occurrence count method and the 
triangulation method. The shaded box in (c) denotes the difference between the interpolation model 
and the mixed model. 
1666
The remainder of this paper is organized as 
follows. In Section 2, we describe the related 
work. We introduce the co-occurrence count 
method in Section 3, and the mixed model in 
Section 4. In Section 5 and Section 6, we de-
scribe and analyze the experiments. Section 7 
gives a conclusion of the paper. 
2 Related Work 
Several methods have been proposed for pivot-
based translation. Typically, they can be classi-
fied into 3 kinds as follows: 
Transfer Method: The transfer method 
(Utiyama and Isahara, 2007; Wang et al., 2008; 
Costa-juss? et al., 2011) connects two translation 
systems: a source-pivot MT system and a pivot-
target MT system. Given a source sentence, (1) 
the source-pivot MT system translates it into the 
pivot language, (2) and the pivot-target MT sys-
tem translates the pivot sentence into the target 
sentence. During each step (source to pivot and 
pivot to target), multiple translation outputs will 
be generated, thus a minimum Bayes-risk system 
combination method is often used to select the 
optimal sentence (Gonz?lez-Rubio et al., 2011; 
Duh et al., 2011). The problem with the transfer 
method is that it needs to decode twice. On one 
hand, the time cost is doubled; on the other hand, 
the translation error of the source-pivot transla-
tion system will be transferred to the pivot-target 
translation. 
Synthetic Method: It aims to create a synthet-
ic source-target corpus by: (1) translate the pivot 
part in source-pivot corpus into target language 
with a pivot-target model; (2) translate the pivot 
part in pivot-target corpus into source language 
with a pivot-source model; (3) combine the 
source sentences with translated target sentences 
or/and combine the target sentences with trans-
lated source sentences (Utiyama et al., 2008; Wu 
and Wang, 2009). However, it is difficult to 
build a high quality translation system with a 
corpus created by a machine translation system. 
Triangulation Method: The triangulation 
method obtains source-target phrase table by 
merging source-pivot and pivot-target phrase 
table entries with identical pivot language 
phrases and multiplying corresponding posterior 
probabilities (Wu and Wang, 2007; Cohn and 
Lapata, 2007), which has been shown to work 
better than the other pivot approaches (Utiyama 
and Isahara, 2007). A problem of this approach is 
that the probability space of the source-target 
phrase pairs is non-uniformity due to the mis-
matching of the pivot phrase.  
3 Our Approach 
In this section, we will introduce our method for 
learning a source-target phrase translation model 
with a pivot language as a bridge. We extract the 
co-occurrence count of phrase pairs for each lan-
guage pair with a source-pivot and a pivot-target 
corpus. Then we generate the source-target 
phrase pairs with induced co-occurrence infor-
mation. Finally, we compute translation proba-
bilities using the standard phrase-based SMT 
training method. 
3.1 Phrase Translation Probabilities 
Following the standard phrase extraction method 
(Koehn et al., 2003), we can extract phrase pairs 
???, ???  and ???, ???  from the corresponding word-
aligned source-pivot and pivot-target training 
corpus, where ?? , ??  and ??  denotes the phrase in 
source, pivot and target language respectively. 
Formally, given the co-occurrence count 
????, ??? and ????, ???, we can estimate  ????, ???  by 
Equation 1: 
????, ??? ? ???????, ???, ????, ????
??
 (1) 
where ????  is a function to merge the co-
occurrences count ????, ???  and ????, ??? . We pro-
pose four calculation methods for function ????. 
Given the co-occurrence count ????, ???  and 
????, ???, we first need to induce the co-occurrence 
count ????, ?,? ??? . The ????, ?,? ???  is counted when 
the source phrase, pivot phrase and target phrase 
occurred together, thus we can infer that 
????, ?,? ???  is smaller than ????, ???  and ????, ??? . In 
this circumstance, we consider that ????, ?,? ???  is 
approximately equal to the minimum value of 
????, ??? and ????, ???, as shown in Equation 2. 
????, ??, ??? ? ?min?????, ???, ????, ????
??
 (2) 
Because the co-occurrence count of source-
target phrase pairs needs the existence of pivot 
phrase ?? , we intuitively believe that the co-
occurrence count ????, ???  is equal to the co-
occurrence count ????, ?,? ???. Under this assump-
tion, we can obtain the co-occurrence count 
????, ??? as shown in Equation 3. Furthermore, to 
testify our assumption, we also try the maximum 
value (Equation 4) to infer the co-occurrence 
count of ???, ???  phrase pair. 
1667
????, ??? ? ?min?????, ???, ????, ????
??
 (3) 
????, ??? ? ?max?????, ???, ????, ????
??
 (4) 
In addition, if source-pivot and pivot-target 
parallel corpus greatly differ in quantities, then 
the minimum function would likely just take the 
counts from the smaller corpus. To deal with the 
problem of the imbalance of the parallel corpora, 
we also try the arithmetic mean (Equation 5) and 
geometric mean (Equation 6) function to infer 
the co-occurrence count of source-target phrase 
pairs. 
????, ??? ? ??????, ??? ? ????, ????/2
??
 (5) 
????, ??? ? ??????, ??? ? ????, ???
??
 (6) 
When the co-occurrence count of source-target 
language is calculated, we can estimate the 
phrase translation probabilities with the follow-
ing Equation 7 and Equation 8. 
?????|?? ? ????, ???? ????, ?????  (7) 
????|??? ? ????, ???? ????, ?????  (8) 
3.2 Lexical Weight 
Given a phrase pair ???, ??? and a word alignment 
a between the source word positions ? ? 1,? , ? 
and the target word positions ? ? 0,? ,? , the 
lexical weight of phrase pair ???, ??? can be calcu-
lated by the following Equation 9 (Koehn et al., 
2003). 
??????|?, ?? ??
1
|??|??, ?? ? ??| ? ????|??????,????
?
???
(9) 
The lexical translation probability distribution 
???|?? between source word s and target word t 
can be estimated with Equation 10. 
???|?? ? ???, ??? ????, ????  (10)
To compute the lexical weight for a phrase 
pair ???, ??? generated by ???, ??? and ???, ???, we need 
the alignment information ?, which can be ob-
tained as Equation 11 shows. 
? ? ???, ??|??: ??, ?? ? ??&??, ?? ? ??? (11)
where ??  and ??  indicate the word alignment 
information in the phrase pair ???, ???  and ???, ??? 
respectively. 
4 Integrate with Direct Translation 
If a standard source-target bilingual corpus is 
available, we can train a direct translation model. 
Thus we can integrate the direct model and the 
pivot model to obtain further improvements. We 
propose a mixed model by merging the co-
occurrence count in direct translation and pivot 
translation. Besides, we also employ an interpo-
lated model (Wu and Wang, 2007) by merging 
the direct translation model and pivot translation 
model using a linear interpolation. 
4.1 Mixed Model 
Given ?  pivot languages, the co-occurrence 
count can be estimated using the method de-
scribed in Section 3.1. Then the co-occurrence 
count and the lexical weight of the mixed model 
can be estimated with the following Equation 12 
and 13. 
???, ?? ??????, ??
?
???
 (12)
??????|?, ?? ????
?
???
??,?????|?, ?? (13)
where ????, ??  and ??,?????|?, ??  are the co-
occurrence count and lexical weight in the direct 
translation model respectively. ????, ??  and 
??,?????|?, ?? denote the co-occurrence count and 
lexical weight in the pivot translation model. ?? 
is the interpolation coefficient, requiring 
? ?????? ? 1. 
4.2 Interpolated Model 
Following Wu and Wang (2007), the interpolated 
model can be modelled with Equation 14. 
?????|?? ? ?????????|??
?
???
 (14)
where ??????|?? is the phrase translation probabil-
ity in direct translation model; ??????|??  is the 
phrase translation probability in pivot translation 
model. The lexical weight is obtained with Equa-
tion 13. ?? is the interpolation coefficient, requir-
ing ? ?? ? 1???? . 
1668
5 Experiments on Europarl Corpus 
Our first experiment is carried out on Europarl1 
corpus, which is a multi-lingual corpus including 
21 European languages (Koehn, 2005). In our 
work, we perform translations among French (fr), 
German (de) and Spanish (es). Due to the rich-
ness of available language resources, we choose 
English (en) as the pivot language. Table 1 
summarized the statistics of training data. For the 
language model, the same monolingual data ex-
tracted from the Europarl are used. 
The word alignment is obtained by GIZA++ 
(Och and Ney, 2000) and the heuristics ?grow-
diag-final? refinement rule (Koehn et al., 2003). 
Our translation system is an in-house phrase-
based system analogous to Moses (Koehn et al., 
2007). The baseline system is the triangulation 
method (Wu and Wang, 2007), including an in-
terpolated model which linearly interpolate the 
direct and pivot translation model. 
                                                 
1 http://www.statmt.org/europarl 
We use WMT082  as our test data, which con-
tains 2000 in-domain sentences and 2051 out-of-
domain sentences with single reference. The 
translation results are evaluated by case-
insensitive BLEU-4 metric (Papineni et al., 
2002). The statistical significance tests using 
95% confidence interval are measured with 
paired bootstrap resampling (Koehn, 2004). 
5.1 Results 
We compare 4 merging methods with the base-
line system. The results are shown in Table 2 and 
Table 3. We find that the minimum method out-
performs the others, achieving significant im-
provements over the baseline on all translation 
directions. The absolute improvements range 
from 0.61 (fr-de) to 1.54 (es-fr) in BLEU% score 
on in-domain test data, and range from 0.36 (fr-
de) to 2.05 (fr-es) in BLEU% score on out-of-
domain test data. This indicates that our method 
is effective and robust in general. 
                                                 
2 http://www.statmt.org/wmt08/shared-task.html 
Language 
Pairs 
Sentence 
Pairs 
Source 
Words
Target 
Words
de-en 1.9M 48.5M 50.9M
es-en 1.9M 54M 51.7M
fr-en 2M 58.1M 52.4M
 
Table 1: Training data of Europarl corpus 
 
System 
BLEU% 
de-es de-fr es-de es-fr fr-de fr-es 
Baseline 27.04 23.01 20.65 33.84 20.87 38.31 
Minimum 27.93* 23.94* 21.52* 35.38* 21.48* 39.62* 
Maximum 25.70 21.59 20.26 32.58 20.50 37.30 
Arithmetic mean 26.01 22.24 20.13 33.38 20.37 37.37 
Geometric mean 27.31 23.49* 21.10* 34.76* 21.15* 39.19* 
 
Table 2: Comparison of different merging methods on in-domain test set. * indicates the results are 
significantly better than the baseline (p<0.05). 
 
System 
BLEU% 
de-es de-fr es-de es-fr fr-de fr-es 
Baseline 15.34 13.52 11.47 21.99 12.19 25.00 
Minimum 15.77* 14.08* 11.99* 23.90* 12.55* 27.05* 
Maximum 13.41 11.83 10.17 20.48 10.83 22.75 
Arithmetic mean 13.96 12.10 10.57 21.07 11.30 23.70 
Geometric mean 15.09 13.30 11.52 23.32* 12.46* 26.22* 
 
Table 3: Comparison of different merging methods on out-of-domain test set. 
 
1669
The geometric mean method also achieves im-
provement, but not as significant as the minimum 
method. However, the maximum and the arith-
metic mean methods show a decrement in BLEU 
scores. This reminds us that how to choose a 
proper merging function for the co-occurrence 
count is a key problem.  In the future, we will 
explore more sophisticated method to merge co-
occurrence count. 
5.2 Analysis 
The pivot-based translation is suitable for the 
scenario that there exists large amount of source-
pivot and pivot-target bilingual corpora and only 
a little source-target bilingual data. Thus, we 
randomly select 10K, 50K, 100K, 200K, 500K, 
1M, 1.5M sentence pairs from the source-target 
bilingual corpora to simulate the lack of source-
target data. With these corpora, we train several 
direct translation models with different scales of 
bilingual data. We interpolate each direct transla-
tion model with the pivot model (both triangula-
tion method and co-occurrence count method) to 
obtain the interpolated model respectively. We 
also mix the direct model and pivot model using 
the method described in Section 4.1.  Following 
 
(a) German-English-Spanish                                        (b) German-English-French 
 
 
(c) Spanish-English-German                                        (d) Spanish-English-French 
 
 
(e) French-English-German                                         (f) French-English-Spanish 
 
Figure 3: Comparisons of pivot-based methods on different scales of source-target standard corpora. 
(direct: direct model; tri: triangulation model; co: co-occurrence count model; tri+inter: triangulation 
model interpolated with direct model ; co+inter: co-occurrence count model interpolated with direct 
model; co+mix: mixed model). X-axis represents the scale of the standard training data. 
22.5
23
23.5
24
24.5
25
25.5
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
26.5
27
27.5
28
28.5
29
29.5
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
33.5
34
34.5
35
35.5
36
36.5
37
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
19.5
20
20.5
21
21.5
22
22.5
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
37.5
38
38.5
39
39.5
40
40.5
41
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
19.5
20
20.5
21
21.5
22
22.5
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
1670
Wu and Wang (2007), we set ?? ? 0.9, ?? ? 0.1, 
?? ? 0.9  and ?? ? 0.1  empirically. The experi-
ments are carried out on 6 translation directions: 
German-Spanish, German-French, Spanish-
German, Spanish-French, French-German and 
French-Spanish. The results are shown in Figure 
3. We only list the results on in-domain test sets. 
The trend of the results on out-of domain test 
sets is similar with in-domain test sets. 
The results are explained as follows: 
(1) Comparison of Pivot Translation and Di-
rect Translation 
The pivot translation models are better than 
the direct translation models trained on a small 
source-target bilingual corpus. With the incre-
ment of source-target corpus, the direct model 
first outperforms the triangulation model and 
then outperforms the co-occurrence count model 
consecutively. 
Taking Spanish-English-French translation as 
an example, the co-occurrence count model 
achieves BLEU% scores of 35.38, which is close 
to the direct translation model trained with 200K 
source-target bilingual data. Compared with the 
co-occurrence count model, the triangulation 
model only achieves BLEU% scores of 33.84, 
which is close to the direct translation model 
trained with 50K source-target bilingual data. 
(2) Comparison of Different Interpolated 
Models 
For the pivot model trained by triangulation 
method and co-occurrence count method, we 
interpolate them with the direct translation model 
trained with different scales of bilingual data. 
Figure 3 shows the translation results of the dif-
ferent interpolated models. For all the translation 
directions, our co-occurrence count method in-
terpolated with the direct model is better than the 
triangulation model interpolated with the direct 
model.  
The two interpolated model are all better than 
the direct translation model. With the increment 
of the source-target training corpus, the gap be-
comes smaller. This indicates that the pivot mod-
el and its affiliated interpolated model are suita-
ble for language pairs with small bilingual data. 
Even if the scale of source-pivot and pivot-target 
corpora is close to the scale of source-target bi-
lingual corpora, the pivot translation model can 
help the direct translation model to improve the 
translation performance. Take Spanish-English-
French translation as an issue, when the scale of 
Spanish-French parallel data is 1.5M sentences 
pairs, which is close to the Spanish-English and 
English-French parallel data, the performance of 
co+mix model is still outperforms the direct 
translation model. 
(3) Comparison of Interpolated Model and 
Mixed Model 
When only a small source-target bilingual 
corpus is available, the mix model outperforms 
the interpolated model. With the increasing of 
source-target corpus, the mix model is close to 
the interpolated model or worse than the interpo-
lated model. This indicates that the mix model 
has a better performance when the source-target 
corpus is small which is close to the realistic sce-
nario. 
5.3 Integrate the Co-occurrence Count 
Model and Triangulation Model 
Experimental results in the previous section 
show that, our co-occurrence count models gen-
erally outperform the baseline system. In this 
section, we carry out experiments that integrates 
co-occurrence count model into the triangulation 
model. 
For French-English-German translation, we 
apply a linear interpolation method to integrate 
the co-occurrence count model into triangulation 
model following the method described in Section 
4.2.  We set ? as the interpolation coefficient of 
triangulation model and 1 ? ? as the interpola-
tion coefficient of co-occurrence count model 
respectively. The experiments take 9 values for 
interpolation coefficient, from 0.1 to 0.9. The 
results are shown in Figure 4. 
 
 
Figure 4: Results of integrating the co-
occurrence count model and the triangulation 
model. 
 
When using interpolation coefficient ranging 
from 0.2 to 0.7, the integrated models outperform 
the triangulation and the co-occurrence count 
model. However, for the other intervals, the inte-
20.4
20.6
20.8
21
21.2
21.4
21.6
21.8
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
BL
EU
%
Interpolation Coefficient 
integrated triangulation
co-occurrence
1671
grated models perform slightly lower than the 
co-occurrence count model, but still show better 
results than the triangulation model. The trend of 
the curve infers that the integrated model synthe-
sizes the contributions of co-occurrence count 
model and triangulation model. Additionally, it 
also indicates that, the choice of the interpolation 
coefficient affects the translation performances. 
6 Experiments on Web Data 
The experimental on Europarl is artificial, as the 
training data for directly translating between 
source and target language actually exists in the 
original data sets. Thus, we conducted several 
experiments on a more realistic scenario: trans-
lating Chinese (zh) to Japanese (jp) via English 
(en) with web crawled data. 
As mentioned in Section 3.1, the source-pivot 
and pivot-target parallel corpora can be imbal-
anced in quantities. If one parallel corpus was 
much larger than another, then minimum heuris-
tic function would likely just take the counts 
from the smaller corpus.  
In order to analyze this issue, we manually set 
up imbalanced corpora. For source-pivot parallel 
corpora, we randomly select 1M, 2M, 3M, 4M 
and 5M Chinese-English sentence pairs. On the 
other hand, we randomly select 1M English-
Japanese sentence pairs as pivot-target parallel 
corpora. The training data of Chinese-English 
and English-Japanese language pairs are summa-
rized in Table 4. For the Chinese-Japanese direct 
corpus, we randomly select 5K, 10K, 20K, 30K, 
40K, 50K, 60K, 70K, 80K, 90K and 100K sen-
tence pairs to simulate the lack of bilingual data. 
We built a 1K in-house test set with four refer-
ences. For Japanese language model training, we 
used the monolingual part of English-Japanese 
corpus. 
Table 5 shows the results of different co-
occurrence count merging methods. First, the 
minimum method and the geometric mean meth-
od outperform the other two merging methods 
and the baseline system with different training 
corpus. When the scale of source-pivot and piv-
ot-target corpus is roughly balanced (zh-en-jp-1), 
the minimum method achieves an absolute im-
provement of 2.06 percentages points on BLEU 
over the baseline, which is also better than the 
other merging methods. While, with the growth 
of source-pivot corpus, the gap between source-
pivot corpus and pivot-target corpus becomes 
bigger. In this circumstance, the geometric mean 
method becomes better than the minimum meth-
od. Compared to the minimum method, the geo-
metric mean method considers both the source-
pivot and the pivot-target corpus, which may 
lead to a better result in the case of imbalanced 
training corpus. 
Language 
Pairs 
Sentence 
Pairs 
Source 
Words
Target 
Words
zh-en-1 1M 18.1M 17.7M
zh-en-2 2M 36.2M 35.5M
zh-en-3 3M 54.2M 53.2M
zh-en-4 4M 72.3M 70.9M
zh-en-5 5M 90.4M 88.6M
en-jp 1M 9.2M 11.1M
 
Table 4: Training data of web corpus 
 
System 
BLEU% 
zh-en-jp-1* zh-en-jp-2 zh-en-jp-3 zh-en-jp-4 zh-en-jp-5
Baseline 29.07 29.39 29.44 29.67 29.80 
Minimum 31.13* 31.28* 31.43* 31.62* 32.02* 
Maximum 28.88 29.01 29.12 29.37 29.59 
Arithmetic mean 29.08 29.36 29.51 29.79 30.01 
Geometric mean 30.77* 31.30* 31.75* 32.07* 32.34* 
 
Table 5: Comparison of different merging methods on the imbalanced web data. ( zh-en-jp-1 means 
the translation system is trained with zh-en-1 as source-pivot corpus and en-jp as pivot-target corpus, 
and so on. ) 
1672
Furthermore, with the imbalanced corpus zh-
en-jp-5, we compared the translation perfor-
mance of our co-occurrence count model (with 
geometric mean merging method), triangulation 
model, interpolated model, mixed model and the 
direct translation models. Figure 5 summarized 
the results. 
The co-occurrence count model can achieve an 
absolute improvement of 2.54 percentages points 
on BLEU over the baseline. The triangulation 
method outperforms the direct translation when 
only 5K sentence pairs are available. Meanwhile, 
the number is 10K when using the co-occurrence 
count method. The co-occurrence count models 
interpolated with the direct model significantly 
outperform the other models. 
 
 
Figure 5: Results on Chinese-Japanese Web Data. 
X-axis represents the scale of the standard train-
ing data. 
 
In this experiment, the training data contains 
parallel sentences on various domains. And the 
training corpora (Chinese-English and English-
Japanese) are typically very different, since they 
are obtained on the web. It indicates that our co-
occurrence count method is robust in the realistic 
scenario. 
7 Conclusion 
This paper proposed a novel approach for pivot-
based SMT by pivoting the co-occurrence count 
of phrase pairs. Different from the triangulation 
method merging the source-pivot and pivot-
target language after training the translation 
model, our method merges the source-pivot and 
pivot-target language after extracting the phrase 
pairs, thus the computing for phrase translation 
probabilities is under the uniform probability 
space. The experimental results on Europarl data 
and web data show significant improvements 
over the baseline systems. We also proposed a 
mixed model to combine the direct translation 
and pivot translation, and the experimental re-
sults show that the mixed model has a better per-
formance when the source-target corpus is small 
which is close to the realistic scenario. 
A key problem in the approach is how to learn 
the co-occurrence count. In this paper, we use the 
minimum function on balanced corpora and the 
geometric mean function on imbalanced corpora 
to estimate the co-occurrence count intuitively. 
In the future, we plan to explore more effective 
approaches. 
Acknowledgments 
We would like to thank Yiming Cui for insight-
ful discussions, and three anonymous reviewers 
for many invaluable comments and suggestions 
to improve our paper. This work is supported by 
National Natural Science Foundation of China 
(61100093), and the State Key Development 
Program for Basic Research of China (973 Pro-
gram, 2014CB340505). 
Reference 
Nicola Bertoldi, Madalina Barbaiani, Marcello 
Federico, and Roldano Cattoni. 2008. Phrase-
Based statistical machine translation with Piv-
ot Languages. In Proceedings of the 5th Inter-
national Workshop on Spoken Language 
Translation (IWSLT), pages 143-149. 
Trevor Cohn and Mirella Lapata. 2007. Machine 
Translation by Triangulation: Make Effective 
Use of Multi-Parallel Corpora. In Proceedings 
of 45th Annual Meeting of the Association for 
Computational Linguistics, pages 828-735. 
Marta R. Costa-juss?, Carlos Henr?quez, and Ra-
fael E. Banchs. 2011. Enhancing Scarce-
Resource Language Translation through Pivot 
Combinations. In Proceedings of the 5th In-
ternational Joint Conference on Natural Lan-
guage Processing, pages 1361-1365. 
Yiming Cui, Conghui Zhu, Xiaoning Zhu, Tiejun 
Zhao and Dequan Zheng. 2013. Phrase Table 
Combination Deficiency Analyses in Pivot-
based SMT. In Proceedings of 18th Interna-
tional Conference on Application of Natural 
Language to Information Systems, pages 355-
358. 
Adria de Gispert and Jose B. Marino. 2006. 
Catalan-English statistical machine translation 
without parallel corpus: bridging through 
Spanish. In Proceedings of 5th International 
Conference on Language Resources and Eval-
uation (LREC), pages 65-68. 
29
31
33
35
37
39
5K 20K 40K 60K 80K 100K
BL
EU
%
direct
tri
co-occur
tri+inter
co+inter
co+mix
1673
Kevin Duh, Katsuhito Sudoh, Xianchao Wu, 
Hajime Tsukada and Masaaki Nagata. 2011. 
Generalized Minimum Bayes Risk System 
Combination. In Proceedings of the 5th Inter-
national Joint Conference on Natural Lan-
guage Processing, pages 1356-1360. 
Ahmed El Kholy, Nizar Habash, Gregor Leusch, 
Evgeny Matusov and Hassan Sawaf. 2013. 
Language Independent Connectivity Strength 
Features for Phrase Pivot Statistical Machine 
Translation. In Proceedings of the 51st Annual 
Meeting of the Association for Computational 
Linguistics, pages 412-418. 
Ahmed El Kholy, Nizar Habash, Gregor Leusch, 
Evgeny Matusov and Hassan Sawaf. 2013. Se-
lective Combination of Pivot and Direct Sta-
tistical Machine Translation Models. In Pro-
ceedings of the 6th International Joint Confer-
ence on Natural Language Processing, pages 
1174-1180. 
Jes?s Gonz?lez-Rubio, Alfons Juan and Francis-
co Casacuberta. 2011. Minimum Bayes-risk 
System Combination. In Proceedings of the 
49th Annual Meeting of the Association for 
Computational Linguistics, pages 1268-1277. 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In 
HLT-NAACL: Human Language Technology 
Conference of the North American Chapter of 
the Association for Computational Linguistics, 
pages 127-133. 
Philipp Koehn. 2004. Statistical significance 
tests for machine translation evaluation. In 
Proceedings of the 2004 Conference on Em-
pirical Methods in Natural Language Pro-
cessing (EMNLP), pages 388-395. 
Philipp Koehn. 2005. Europarl: A Parallel Cor-
pus for Statistical Machine Translation. In 
Proceedings of MT Summit X, pages 79-86. 
Philipp Koehn, Hieu Hoang, Alexanda Birch, 
Chris Callison-Burch, Marcello Federico, Ni-
cola Bertoldi, Brooke Cowan, Wade Shen, 
Christine Moran, Richard Zens, Chris Dyer, 
Ondrej Bojar, Alexandra Constantin, and Evan 
Herbst. 2007. Moses: Open Source Toolkit for 
Statistical Machine Translation. In Proceed-
ings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics, demon-
stration session, pages 177-180. 
Philipp Koehn, Alexandra Birch, and Ralf Stein-
berger. 2009. 462 Machine Translation Sys-
tems for Europe. In Proceedings of the MT 
Summit XII. 
Gregor Leusch, Aur?lien Max, Josep Maria 
Crego and Hermann Ney. 2010. Multi-Pivot 
Translation by System Combination. In Pro-
ceedings of the 7th International Workshop on 
Spoken Language Translation, pages 299-306. 
Franz Josef Och and Hermann Ney. 2000. A 
comparison of alignment models for statistical 
machine translation. In Proceedings of the 
18th International Conference on Computa-
tional Linguistics, pages 1086-1090. 
Michael Paul, Andrew Finch, Paul R. Dixon and 
Eiichiro Sumita. 2011. Dialect Translation: In-
tegrating Bayesian Co-segmentation Models 
with Pivot-based SMT. In Proceedings of the 
2011 Conference on Empirical Methods in 
Natural Language Processing, pages 1-9. 
Michael Paul and Eiichiro Sumita. 2011. Trans-
lation Quality Indicators for Pivot-based Sta-
tistical MT. In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language 
Processing, pages 811-818. 
Kishore Papineni, Salim Roukos, Todd Ward and 
Wei-Jing Zhu. 2002. BLEU: a Method for Au-
tomatic Evaluation of Machine Translation. In 
Proceedings of the 40th Annual Meeting of the 
Association for Computation Linguistics, pag-
es 311-319. 
Rie Tanaka, Yohei Murakami and Toru Ishida. 
2009. Context-Based Approach for Pivot 
Translation Services. In the Twenty-first In-
ternational Conference on Artificial Intelli-
gence, pages 1555-1561. 
J?rg Tiedemann. 2012. Character-Based Pivot 
Translation for Under-Resourced Languages 
and Domains. In Proceedings of the 13th Con-
ference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 
141-151. 
Masatoshi Tsuchiya, Ayu Purwarianti, Toshiyu-
kiWakita and Seiichi Nakagawa. 2007. Ex-
panding Indonesian-Japanese Small Transla-
tion Dictionary Using a Pivot Language. In 
Proceedings of the ACL 2007 Demo and Post-
er Sessions, pages 197-200. 
Takashi Tsunakawa, Naoaki Okazaki and 
Jun'ichi Tsujii. 2010. Building a Bilingual 
Lexicon Using Phrase-based Statistical Ma-
chine Translation via a Pivot Language. In 
1674
Proceedings of the 22th International Confer-
ence on Computational Linguistics (Coling), 
pages 127-130. 
Masao Utiyama and Hitoshi Isahara. 2007. A 
Comparison of Pivot Methods for Phrase-
Based Statistical Machine Translation. In Pro-
ceedings of Human Language Technology: the 
Conference of the North American Chapter of 
the Association for Computational Linguistics, 
pages 484-491. 
Masao Utiyama, Andrew Finch, Hideo Okuma, 
Michael Paul, Hailong Cao, Hirofumi Yama-
moto, Keiji Yasuda,and Eiichiro Sumita. 2008. 
The NICT/ATR speech Translation System for 
IWSLT 2008. In Proceedings of the Interna-
tional Workshop on Spoken Language Trans-
lation, pages 77-84. 
Haifeng Wang, Hua Wu, Xiaoguang Hu, Zhanyi 
Liu, Jianfeng Li, Dengjun Ren, and Zhengyu 
Niu. 2008. The TCH Machine Translation 
System for IWSLT 2008. In Proceedings of 
the International Workshop on Spoken Lan-
guage Translation, pages 124-131. 
Hua Wu and Haifeng Wang. 2007. Pivot Lan-
guage Approach for Phrase-Based Statistical 
Machine Translation. In Proceedings of 45th 
Annual Meeting of the Association for Compu-
tational Linguistics, pages 856-863. 
Hua Wu and Haifeng Wang. 2009. Revisiting 
Pivot Language Approach for Machine Trans-
lation. In Proceedings of the 47th Annual 
Meeting of the Association for Computational 
Linguistics and the 4th IJCNLP of the AFNLP, 
pages 154-162. 
Samira Tofighi Zahabi, Somayeh Bakhshaei and 
Shahram Khadivi. Using Context Vectors in 
Improving a Machine Translation System with 
Bridge Language. In Proceedings of the 51st 
Annual Meeting of the Association for Compu-
tational Linguistics, pages 318-322. 
 
 
1675
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 825?833,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Improving Statistical Machine Translation with 
Monolingual Collocation 
 
Zhanyi Liu1, Haifeng Wang2, Hua Wu2, Sheng Li1 
1Harbin Institute of Technology, Harbin, China 
2Baidu.com Inc., Beijing, China 
zhanyiliu@gmail.com 
{wanghaifeng, wu_hua}@baidu.com 
lisheng@hit.edu.cn 
 
Abstract? 
This paper proposes to use monolingual 
collocations to improve Statistical Ma-
chine Translation (SMT). We make use 
of the collocation probabilities, which are 
estimated from monolingual corpora, in 
two aspects, namely improving word 
alignment for various kinds of SMT sys-
tems and improving phrase table for 
phrase-based SMT. The experimental re-
sults show that our method improves the 
performance of both word alignment and 
translation quality significantly. As com-
pared to baseline systems, we achieve ab-
solute improvements of 2.40 BLEU score 
on a phrase-based SMT system and 1.76 
BLEU score on a parsing-based SMT 
system. 
1 Introduction 
Statistical bilingual word alignment (Brown et al 
1993) is the base of most SMT systems. As com-
pared to single-word alignment, multi-word 
alignment is more difficult to be identified. Al-
though many methods were proposed to improve 
the quality of word alignments (Wu, 1997; Och 
and Ney, 2000; Marcu and Wong, 2002; Cherry 
and Lin, 2003; Liu et al, 2005; Huang, 2009), 
the correlation of the words in multi-word 
alignments is not fully considered. 
In phrase-based SMT (Koehn et al, 2003), the 
phrase boundary is usually determined based on 
the bi-directional word alignments. But as far as 
we know, few previous studies exploit the collo-
cation relations of the words in a phrase. Some 
                                                 
This work was partially done at Toshiba (China) Research 
and Development Center. 
researches used soft syntactic constraints to pre-
dict whether source phrase can be translated to-
gether (Marton and Resnik, 2008; Xiong et al, 
2009). However, the constraints were learned 
from the parsed corpus, which is not available 
for many languages.  
In this paper, we propose to use monolingual 
collocations to improve SMT. We first identify 
potentially collocated words and estimate collo-
cation probabilities from monolingual corpora 
using a Monolingual Word Alignment (MWA) 
method (Liu et al, 2009), which does not need 
any additional resource or linguistic preprocess-
ing, and which outperforms previous methods on 
the same experimental data. Then the collocation 
information is employed to improve Bilingual 
Word Alignment (BWA) for various kinds of 
SMT systems and to improve phrase table for 
phrase-based SMT. 
To improve BWA, we re-estimate the align-
ment probabilities by using the collocation prob-
abilities of words in the same cept. A cept is the 
set of source words that are connected to the 
same target word (Brown et al, 1993). An 
alignment between a source multi-word cept and 
a target word is a many-to-one multi-word 
alignment. 
To improve phrase table, we calculate phrase 
collocation probabilities based on word colloca-
tion probabilities. Then the phrase collocation 
probabilities are used as additional features in 
phrase-based SMT systems. 
The evaluation results show that the proposed 
method in this paper significantly improves mul-
ti-word alignment, achieving an absolute error 
rate reduction of 29%. The alignment improve-
ment results in an improvement of 2.16 BLEU 
score on phrase-based SMT system and an im-
provement of 1.76 BLEU score on parsing-based 
SMT system. If we use phrase collocation proba-
bilities as additional features, the phrase-based 
825
SMT performance is further improved by 0.24 
BLEU score. 
The paper is organized as follows: In section 2, 
we introduce the collocation model based on the 
MWA method. In section 3 and 4, we show how 
to improve the BWA method and the phrase ta-
ble using collocation models respectively. We 
describe the experimental results in section 5, 6 
and 7. Lastly, we conclude in section 8. 
2 Collocation Model 
Collocation is generally defined as a group of 
words that occur together more often than by 
chance (McKeown and Radev, 2000). A colloca-
tion is composed of two words occurring as ei-
ther a consecutive word sequence or an inter-
rupted word sequence in sentences, such as "by 
accident" or "take ... advice". In this paper, we 
use the MWA method (Liu et al, 2009) for col-
location extraction. This method adapts the bi-
lingual word alignment algorithm to monolingual 
scenario to extract collocations only from mono-
lingual corpora. And the experimental results in 
(Liu et al, 2009) showed that this method 
achieved higher precision and recall than pre-
vious methods on the same experimental data. 
2.1 Monolingual word alignment 
The monolingual corpus is first replicated to 
generate a parallel corpus, where each sentence 
pair consists of two identical sentences in the 
same language. Then the monolingual word 
alignment algorithm is employed to align the 
potentially collocated words in the monolingual 
sentences. 
According to Liu et al (2009), we employ the 
MWA Model 3 (corresponding to IBM Model 3) 
to calculate the probability of the monolingual 
word alignment sequence, as shown in Eq. (1). 
? ?
???
?
?
l
j
jaj
l
i
ii
lajdwwt
wnSASp
j1
1
3 ModelMWA 
),|()|(
)|()|,( ?    (1) 
Where lwS 1?  is a monolingual sentence, i?  
denotes the number of words that are aligned 
with 
iw . Since a word never collocates with itself, 
the alignment set is denoted as 
}&],1[|),{( ialiaiA ii ??? . Three kinds of prob-
abilities are involved in this model: word collo-
cation probability 
)|( jaj wwt
, position colloca-
tion probability ),|( lajd j  and fertility probabili-
ty )|( ii wn ? . 
In the MWA method, the similar algorithm to 
bilingual word alignment is used to estimate the 
parameters of the models, except that a word 
cannot be aligned to itself.  
Figure 1 shows an example of the potentially 
collocated word pairs aligned by the MWA me-
thod. 
 
Figure 1. MWA Example 
2.2 Collocation probability 
Given the monolingual word aligned corpus, we 
calculate the frequency of two words aligned in 
the corpus, denoted as ),( ji wwfreq . We filtered 
the aligned words occurring only once. Then the 
probability for each aligned word pair is esti-
mated as follows: 
? ??
?w j
ji
ji wwfreq
wwfreqwwp ),(
),()|(
                 (2) 
? ??
?w i
ji
ij wwfreq
wwfreqwwp ),(
),()|(
                  (3) 
In this paper, the words of collocation are 
symmetric and we do not determine which word 
is the head and which word is the modifier. Thus, 
the collocation probability of two words is de-
fined as the average of both probabilities, as in 
Eq. (4). 
2
)|()|(),( ijjiji wwpwwpwwr ??
      (4) 
If we have multiple monolingual corpora to 
estimate the collocation probabilities, we interpo-
late the probabilities as shown in Eq. (5). 
),(),( jik kkji wwrwwr ?? ?
          (5) 
k?  denotes the interpolation coefficient for 
the probabilities estimated on the kth corpus. 
3 Improving Statistical Bilingual Word 
Alignment 
We use the collocation information to improve 
both one-directional and bi-directional bilingual 
word alignments. The alignment probabilities are 
re-estimated by using the collocation probabili-
ties of words in the same cept. 
The team leader plays a key role in the project undertaking. 
The team leader plays a key role in the project undertaking. 
 
826
3.1 Improving one-directional bilingual 
word alignment 
According to the BWA method, given a bilingual 
sentence pair leE 1?  and mfF 1? , the optimal 
alignment sequence A  between E and F can be 
obtained as in Eq. (6). 
)|,(maxarg* EAFpA A?
                   (6) 
The method is implemented in a series of five 
models (IBM Models). IBM Model 1 only em-
ploys the word translation model to calculate the 
probabilities of alignments. In IBM Model 2, 
both the word translation model and position dis-
tribution model are used. IBM Model 3, 4 and 5 
consider the fertility model in addition to the 
word translation model and position distribution 
model. And these three models are similar, ex-
cept for the word distortion models. 
One-to-one and many-to-one alignments could 
be produced by using IBM models. Although the 
fertility model is used to restrict the number of 
source words in a cept and the position distortion 
model is used to describe the correlation of the 
positions of the source words, the quality of 
many-to-one alignments is lower than that of 
one-to-one alignments. 
Intuitively, the probability of the source words 
aligned to a target word is not only related to the 
fertility ability and their relative positions, but 
also related to lexical tokens of words, such as 
common phrase or idiom. In this paper, we use 
the collocation probability of the source words in 
a cept to measure their correlation strength. Giv-
en source words }|{ iaf jj ?  aligned to ie , their 
collocation probability is calculated as in Eq. (7). 
)1(*
),(2
})|({
1
1 1
][][
?
? ?
??
?
? ??
ii
k kg
giki
jj
i i ffr
iafr ??
? ?
     (7) 
Here, 
kif ][
and 
gif ][
denote the thk  word and 
thg  word in }|{ iaf jj ? ; ),( ][][ giki ffr
 denotes 
the collocation probability of 
kif ][
and 
gif ][
, as 
shown in Eq. (4).  
Thus, the collocation probability of the align-
ment sequence of a sentence pair can be calcu-
lated according to Eq. (8). 
? ?? ?
l
i jj iafrEAFr 1 })|({)|,(
           (8) 
Based on maximum entropy framework, we 
combine the collocation model and the BWA 
model to calculate the word alignment probabili-
ty of a sentence pair, as shown in Eq. (9). 
? ? ?
?
?
'
)),,(exp(
)),,(exp(
)|,(
A i
ii
i
ii
r AEFh
AEFh
EAFp ?
?     (9) 
Here, ),,( AEFhi and i?  denote features and 
feature weights, respectively. We use two fea-
tures in this paper, namely alignment probabili-
ties and collocation probabilities. 
Thus, we obtain the decision rule: 
}),,({maxarg* ?? i iiA AEFhA ?
          (10) 
Based on the GIZA++ package 1 , we imple-
mented a tool for the improved BWA method. 
We first train IBM Model 4 and collocation 
model on bilingual corpus and monolingual cor-
pus respectively. Then we employ the hill-
climbing algorithm (Al-Onaizan et al, 1999) to 
search for the optimal alignment sequence of a 
given sentence pair, where the score of an align-
ment sequence is calculated as in Eq. (10). 
We note that Eq. (8) only deals with many-to-
one alignments, but the alignment sequence of a 
sentence pair also includes one-to-one align-
ments. To calculate the collocation probability of 
the alignment sequence, we should also consider 
the collocation probabilities of such one-to-one 
alignments. To solve this problem, we use the 
collocation probability of the whole source sen-
tence, )(Fr , as the collocation probability of 
one-word cept. 
3.2 Improving bi-directional bilingual word 
alignments 
In word alignment models implemented in GI-
ZA++, only one-to-one and many-to-one word 
alignment links can be found. Thus, some multi-
word units cannot be correctly aligned. The 
symmetrization method is used to effectively 
overcome this deficiency (Och and Ney, 2003). 
Bi-directional alignments are generally obtained 
from source-to-target algnments 
tsA 2  and target-
to-source alignments 
stA 2 , using some heuristic 
rules (Koehn et al, 2005). This method ignores 
the correlation of the words in the same align-
ment unit, so an alignment may include many 
unrelated words2 , which influences the perfor-
mances of SMT systems. 
                                                 
1 http://www.fjoch.com/GIZA++.html 
2 In our experiments, a multi-word unit may include up to 
40 words. 
827
In order to solve the above problem, we incor-
porate the collocation probabilities into the bi-
directional word alignment process. 
Given alignment sets 
tsA 2  and stA 2 . We can 
obtain the union 
sttsts AAA 22 ??? . The source 
sentence mf1  can be segmented into m?  cepts 
mf ?1 . The target sentence le1  can also be seg-
mented into l ?  cepts le ?1 . The words in the same 
cept can be a consecutive word sequence or an 
interrupted word sequence. 
Finally, the optimal alignments A  between 
mf ?1  and le ?1  can be obtained from tsA ?  using the 
following decision rule. 
})()(),({maxarg
),,(
321
),(
*'
1
'
1
????
??
???
? Afe
jiji
AA
ml
jits
frerfep
Afe     (11) 
Here, )( jfr  and )( ier  denote the collocation 
probabilities of the words in the source language 
and target language respectively, which are cal-
culated by using Eq. (7). ),( ji fep  denotes the 
word translation probability that is calculated 
according to Eq. (12). 
i?  denotes the weights of 
these probabilities. 
||*||
2/))|()|((
),(
ji
ee ff
ji fe
efpfep
fep i j
? ? ?
? ? ?
    (12) 
)|( fep  and )|( efp  are the source-to-target 
and target-to-source translation probabilities 
trained from the word aligned bilingual corpus. 
4 Improving Phrase Table 
Phrase-based SMT system automatically extracts 
bilingual phrase pairs from the word aligned bi-
lingual corpus. In such a system, an idiomatic 
expression may be split into several fragments, 
and the phrases may include irrelevant words. In 
this paper, we use the collocation probability to 
measure the possibility of words composing a 
phrase. 
For each bilingual phrase pair automatically 
extracted from word aligned corpus, we calculate 
the collocation probabilities of source phrase and 
target phrase respectively, according to Eq. (13). 
)1(*
),(2
)(
1
1 1
1 ?
? ?
?
?
? ??
nn
wwr
wr
n
i
n
ij
ji
n                  (13) 
Here, nw1  denotes a phrase with n words; 
),( ji wwr
 denotes the collocation probability of a 
Corpora 
Chinese 
words 
English 
words 
Bilingual corpus 6.3M 8.5M 
Additional monolingual 
corpora 
312M 203M 
Table 1. Statistics of training data 
word pair calculated according to Eq. (4). For the 
phrase only including one word, we set a fixed 
collocation probability that is the average of the 
collocation probabilities of the sentences on a 
development set. These collocation probabilities 
are incorporated into the phrase-based SMT sys-
tem as features.  
5 Experiments on Word Alignment 
5.1 Experimental settings 
We use a bilingual corpus, FBIS (LDC2003E14), 
to train the IBM models. To train the collocation 
models, besides the monolingual parts of FBIS, 
we also employ some other larger Chinese and 
English monolingual corpora, namely, Chinese 
Gigaword (LDC2007T38), English Gigaword 
(LDC2007T07), UN corpus (LDC2004E12), Si-
norama corpus (LDC2005T10), as shown in Ta-
ble 1. 
Using these corpora, we got three kinds of col-
location models: 
CM-1: the training data is the additional mo-
nolingual corpora; 
CM-2: the training data is either side of the bi-
lingual corpus; 
CM-3: the interpolation of CM-1 and CM-2. 
To investigate the quality of the generated 
word alignments, we randomly selected a subset 
from the bilingual corpus as test set, including 
500 sentence pairs. Then word alignments in the 
subset were manually labeled, referring to the 
guideline of the Chinese-to-English alignment 
(LDC2006E93), but we made some modifica-
tions for the guideline. For example, if a preposi-
tion appears after a verb as a phrase aligned to 
one single word in the corresponding sentence, 
then they are glued together. 
There are several different evaluation metrics 
for word alignment (Ahrenberg et al, 2000). We 
use precision (P), recall (R) and alignment error 
ratio (AER), which are similar to those in Och 
and Ney (2000), except that we consider each 
alignment as a sure link. 
828
Experiments 
Single word alignments Multi-word alignments 
P R AER P R AER 
Baseline 0.77 0.45 0.43 0.23 0.71 0.65 
Improved BWA methods 
CM-1 0.70 0.50 0.42 0.35 0.86 0.50 
CM-2 0.73 0.48 0.42 0.36 0.89 0.49 
CM-3 0.73 0.48 0.41 0.39 0.78 0.47 
Table 2. English-to-Chinese word alignment results 
 
Figure 2. Example of the English-to-Chinese word alignments generated by the BWA method and 
the improved BWA method using CM-3. " " denotes the alignments of our method; " " denotes 
the alignments of the baseline method. 
||
||
g
rg
S
SSP ??
                      (14) 
||
||
r
rg
S
SSR ??
                     (15) 
||||
||*21
rg
rg
SS
SSAER ???
?              (16) 
Where, 
gS
 and 
rS  denote the automatically 
generated alignments and the reference align-
ments. 
In order to tune the interpolation coefficients 
in Eq. (5) and the weights of the probabilities in 
Eq. (11), we also manually labeled a develop-
ment set including 100 sentence pairs, in the 
same manner as the test set. By minimizing the 
AER on the development set, the interpolation 
coefficients of the collocation probabilities on 
CM-1 and CM-2 were set to 0.1 and 0.9. And the 
weights of probabilities were set as 6.01 ?? , 
2.02 ?? and 2.03 ?? . 
5.2 Evaluation results 
One-directional alignment results 
To train a Chinese-to-English SMT system, 
we need to perform both Chinese-to-English and 
English-to-Chinese word alignment. We only 
evaluate the English-to-Chinese word alignment 
here. GIZA++ with the default settings is used as 
the baseline method. The evaluation results in 
Table 2 indicate that the performances of our 
methods on single word alignments are close to 
that of the baseline method. For multi-word 
alignments, our methods significantly outper-
form the baseline method in terms of both preci-
sion and recall, achieving up to 18% absolute 
error rate reduction. 
Although the size of the bilingual corpus is 
much smaller than that of additional monolingual 
corpora, our methods using CM-1 and CM-2 
achieve comparable performances. It is because 
CM-2 and the BWA model are derived from the 
same resource. By interpolating CM1 and CM2, 
i.e. CM-3, the error rate of multi-word alignment 
results is further reduced. 
Figure 2 shows an example of word alignment 
results generated by the baseline method and the 
improved method using CM-3. In this example, 
our method successfully identifies many-to-one 
alignments such as "the people of the world  
??". In our collocation model, the collocation 
probability of "the people of the world" is much 
higher than that of "people world". And our me-
thod is also effective to prevent the unrelated 
?? ? ???? ?? ?? ? ?? ? ?? ?? ? ?? ? 
China's science and technology research has made achievements which have gained the attention of the people of the world . 
??  ? ???? ?? ?? ? ?? ? ?? ?? ? ?? ? 
zhong-guo  de     ke-xue-ji-shu      yan-jiu      qu-de       le      xu-duo   ling   shi-ren     zhu-mu     de     cheng-jiu . 
china        DE    science and         research   obtain      LE      many     let    common    attract     DE  achievement . 
                             technology                                                                            people    attention   
829
Experiments 
Single word alignments Multi-word alignments All alignments 
P R AER P R AER P R AER 
Baseline 0.84 0.43 0.42 0.18 0.74 0.70 0.52 0.45 0.51 
Our methods 
WA-1 0.80 0.51 0.37 0.30 0.89 0.55 0.58 0.51 0.45 
WA-2 0.81 0.50 0.37 0.33 0.81 0.52 0.62 0.50 0.44 
WA-3 0.78 0.56 0.34 0.44 0.88 0.41 0.63 0.54 0.40 
Table 3. Bi-directional word alignment results 
words from being aligned. For example, in the 
baseline alignment "has made ... have ??", 
"have" and "has" are unrelated to the target word, 
while our method only generated "made  ?
?", this is because that the collocation probabili-
ties of "has/have" and "made" are much lower 
than that of the whole source sentence. 
Bi-directional alignment results 
We build a bi-directional alignment baseline 
in two steps: (1) GIZA++ is used to obtain the 
source-to-target and target-to-source alignments; 
(2) the bi-directional alignments are generated by 
using "grow-diag-final". We use the methods 
proposed in section 3 to replace the correspond-
ing steps in the baseline method. We evaluate 
three methods:  
WA-1: one-directional alignment method pro-
posed in section 3.1 and grow-diag-final; 
WA-2: GIZA++ and the bi-directional bilin-
gual word alignments method proposed in 
section 3.2; 
WA-3: both methods proposed in section 3. 
Here, CM-3 is used in our methods. The re-
sults are shown in Table 3. 
We can see that WA-1 achieves lower align-
ment error rate as compared to the baseline me-
thod, since the performance of the improved one-
directional alignment method is better than that 
of GIZA++. This result indicates that improving 
one-directional word alignment results in bi-
directional word alignment improvement. 
The results also show that the AER of WA-2 
is lower than that of the baseline. This is because 
the proposed bi-directional alignment method 
can effectively recognize the correct alignments 
from the alignment union, by leveraging colloca-
tion probabilities of the words in the same cept. 
Our method using both methods proposed in 
section 3 produces the best alignment perfor-
mance, achieving 11% absolute error rate reduc-
tion. 
Experiments BLEU (%) 
Baseline 29.62 
Our methods 
WA-1 
CM-1 30.85 
CM-2 31.28 
CM-3 31.48 
WA-2 
CM-1 31.00 
CM-2 31.33 
CM-3 31.51 
WA-3 
CM-1 31.43 
CM-2 31.62 
CM-3 31.78 
Table 4. Performances of Moses using the dif-
ferent bi-directional word alignments (Signifi-
cantly better than baseline with p < 0.01) 
6 Experiments on Phrase-Based SMT 
6.1 Experimental settings 
We use FBIS corpus to train the Chinese-to-
English SMT systems. Moses (Koehn et al, 2007) 
is used as the baseline phrase-based SMT system. 
We use SRI language modeling toolkit (Stolcke, 
2002) to train a 5-gram language model on the 
English sentences of FBIS corpus. We used the 
NIST MT-2002 set as the development set and 
the NIST MT-2004 test set as the test set. And 
Koehn's implementation of minimum error rate 
training (Och, 2003) is used to tune the feature 
weights on the development set. 
We use BLEU (Papineni et al, 2002) as eval-
uation metrics. We also calculate the statistical 
significance differences between our methods 
and the baseline method by using paired boot-
strap re-sample method (Koehn, 2004). 
6.2 Effect of improved word alignment on 
phrase-based SMT 
We investigate the effectiveness of the improved 
word alignments on the phrase-based SMT sys-
tem. The bi-directional alignments are obtained 
830
 Figure 3. Example of the translations generated by the baseline system and the system where the 
phrase collocation probabilities are added 
Experiments BLEU (%) 
Moses 29.62 
+ Phrase collocation probability 30.47 
+ Improved word alignments 
+ Phrase collocation probability 
32.02 
Table 5. Performances of Moses employing 
our proposed methods (Significantly better than 
baseline with p < 0.01) 
using the same methods as those shown in Table 
3. Here, we investigate three different collocation 
models for translation quality improvement. The 
results are shown in Table 4. 
From the results of Table 4, it can be seen that 
the systems using the improved bi-directional 
alignments achieve higher quality of translation 
than the baseline system. If the same alignment 
method is used, the systems using CM-3 got the 
highest BLEU scores. And if the same colloca-
tion model is used, the systems using WA-3 
achieved the higher scores. These results are 
consistent with the evaluations of word align-
ments as shown in Tables 2 and 3. 
6.3 Effect of phrase collocation probabili-
ties 
To investigate the effectiveness of the method 
proposed in section 4, we only use the colloca-
tion model CM-3 as described in section 5.1. The 
results are shown in Table 5. When the phrase 
collocation probabilities are incorporated into the 
SMT system, the translation quality is improved, 
achieving an absolute improvement of 0.85 
BLEU score. This result indicates that the collo-
cation probabilities of phrases are useful in de-
termining the boundary of phrase and predicting 
whether phrases should be translated together, 
which helps to improve the phrase-based SMT 
performance. 
Figure 3 shows an example: T1 is generated 
by the system where the phrase collocation prob-
abilities are used and T2 is generated by the 
baseline system. In this example, since the collo-
cation probability of "? ??" is much higher 
than that of "?? ?", our method tends to split 
"? ?? ?" into "(? ??) (?)", rather than 
"(?) (?? ?)". For the phrase "?? ??" in 
the source sentence, the collocation probability 
of the translation "in order to avoid" is higher 
than that of the translation "can we avoid". Thus, 
our method selects the former as the translation. 
Although the phrase "?? ?? ?? ?? ?
?" in the source sentence has the same transla-
tion "We must adopt effective measures", our 
method splits this phrase into two parts "?? ?
?" and "?? ?? ??", because two parts 
have higher collocation probabilities than the 
whole phrase. 
We also investigate the performance of the 
system employing both the word alignment im-
provement and phrase table improvement me-
thods. From the results in Table 5, it can be seen 
that the quality of translation is future improved. 
As compared with the baseline system, an abso-
lute improvement of 2.40 BLEU score is 
achieved. And this result is also better than  the 
results shown in Table 4. 
7 Experiments on Parsing-Based SMT 
We also investigate the effectiveness of the im-
proved word alignments on the parsing-based 
SMT system, Joshua (Li et al, 2009). In this sys-
tem, the Hiero-style SCFG model is used 
(Chiang, 2007), without syntactic information. 
The rules are extracted only based on the FBIS 
corpus, where words are aligned by "MW-3 & 
CM-3". And the language model is the same as 
that in Moses. The feature weights are tuned on 
the development set using the minimum error 
??  ??  ??  ??  ??  ??  ??  ?  ??  ? 
wo-men bi-xu      cai-qu   you-xiao  cuo-shi   cai-neng  bi-mian  chu      wen-ti      . 
we          must        use      effective   measure    can        avoid    out      problem  . 
We must  adopt effective measures  in order to avoid  problems  . 
 
 
We must adopt effective measures  can we avoid  out of the  question . 
T1: 
T2: 
831
Experiments BLEU (%) 
Joshua 30.05 
+ Improved word alignments 31.81 
Table 6. Performances of Joshua using the dif-
ferent word alignments (Significantly better than 
baseline with p < 0.01) 
rate training method. We use the same evaluation 
measure as described in section 6.1. 
The translation results on Joshua are shown in 
Table 6. The system using the improved word 
alignments achieves an absolute improvement of 
1.76 BLEU score, which indicates that the im-
provements of word alignments are also effective 
to improve the performance of the parsing-based 
SMT systems. 
8 Conclusion 
We presented a novel method to use monolingual 
collocations to improve SMT. We first used the 
MWA method to identify potentially collocated 
words and estimate collocation probabilities only 
from monolingual corpora, no additional re-
source or linguistic preprocessing is needed. 
Then the collocation information was employed 
to improve BWA for various kinds of SMT sys-
tems and to improve phrase table for phrase-
based SMT. 
To improve BWA, we re-estimate the align-
ment probabilities by using the collocation prob-
abilities of words in the same cept. To improve 
phrase table, we calculate phrase collocation 
probabilities based on word collocation probabil-
ities. Then the phrase collocation probabilities 
are used as additional features in phrase-based 
SMT systems. 
The evaluation results showed that the pro-
posed method significantly improved word 
alignment, achieving an absolute error rate re-
duction of 29% on multi-word alignment. The 
improved word alignment results in an improve-
ment of 2.16 BLEU score on a phrase-based 
SMT system and an improvement of 1.76 BLEU 
score on a parsing-based SMT system. When we 
also used phrase collocation probabilities as ad-
ditional features, the phrase-based SMT perfor-
mance is finally improved by 2.40 BLEU score 
as compared with the baseline system. 
Reference 
Lars Ahrenberg, Magnus Merkel, Anna Sagvall Hein, 
and Jorg Tiedemann. 2000. Evaluation of Word 
Alignment Systems. In Proceedings of the Second 
International Conference on Language Resources 
and Evaluation, pp. 1255-1261. 
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin 
Knight, John Lafferty, Dan Melamed, Franz-Josef 
Och, David Purdy, Noah A. Smith, and David Ya-
rowsky. 1999. Statistical Machine Translation. Fi-
nal Report. In Johns Hopkins University Workshop. 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert. L. Mercer. 1993. The Ma-
thematics of Statistical Machine Translation: Pa-
rameter estimation. Computational Linguistics, 
19(2): 263-311. 
Colin Cherry and Dekang Lin. 2003. A Probability 
Model to Improve Word Alignment. In Proceed-
ings of the 41st Annual Meeting of the Association 
for Computational Linguistics, pp. 88-95. 
David Chiang. 2007. Hierarchical Phrase-Based 
Translation. Computational Linguistics, 33(2): 
201-228. 
Fei Huang. 2009. Confidence Measure for Word 
Alignment. In Proceedings of the 47th Annual 
Meeting of the ACL and the 4th IJCNLP, pp. 932-
940. 
Philipp Koehn. 2004. Statistical Significance Tests for 
Machine Translation Evaluation. In Proceedings of 
the 2004 Conference on Empirical Methods in 
Natural Language Processing, pp. 388-395. 
Philipp Koehn, Amittai Axelrod, Alexandra Birch 
Mayne, Chris Callison-Burch, Miles Osborne, and 
David Talbot. 2005. Edinburgh System Description 
for the 2005 IWSLT Speech Translation Evalua-
tion. In Processings of the International Workshop 
on Spoken Language Translation 2005. 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical Phrase-based Translation. In Proceed-
ings of the Human Language Technology Confe-
rence and the North American Association for 
Computational Linguistics, pp. 127-133. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
In Proceedings of the 45th Annual Meeting of the 
ACL, Poster and Demonstration Sessions, pp. 177-
180. 
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ga-
nitkevitch, Sanjeev Khudanpur, Lane Schwartz, 
Wren Thornton, Jonathan Weese, and Omar Zaidan. 
2009. Demonstration of Joshua: An Open Source 
Toolkit for Parsing-based Machine Translation. In 
Proceedings of the 47th Annual Meeting of the As-
832
sociation for Computational Linguistics, Software 
Demonstrations, pp. 25-28. 
Yang Liu, Qun Liu, and Shouxun Lin. Log-linear 
Models for Word Alignment. 2005. In Proceedings 
of the 43rd Annual Meeting of the Association for 
Computational Linguistics, pp. 459-466. 
Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li. 
2009. Collocation Extraction Using Monolingual 
Word Alignment Method. In Proceedings of the 
2009 Conference on Empirical Methods in Natural 
Language Processing, pp. 487-495. 
Daniel Marcu and William Wong. 2002. A Phrase-
Based, Joint Probability Model for Statistical Ma-
chine Translation. In Proceedings of the 2002 Con-
ference on Empirical Methods in Natural Lan-
guage Processing,  pp. 133-139. 
Yuval Marton and Philip Resnik. 2008. Soft Syntactic 
Constraints for Hierarchical Phrase-Based Transla-
tion. In Proceedings of the 46st Annual Meeting of 
the Association for Computational Linguistics, pp. 
1003-1011. 
Kathleen R. McKeown and Dragomir R. Radev. 2000. 
Collocations. In Robert Dale, Hermann Moisl, and 
Harold Somers (Ed.), A Handbook of Natural Lan-
guage Processing, pp. 507-523. 
Franz Josef Och and Hermann Ney. 2000. Improved 
Statistical Alignment Models. In Proceedings of 
the 38th Annual Meeting of the Association for 
Computational Linguistics, pp. 440-447. 
Franz Josef Och. 2003. Minimum Error Rate Training 
in Statistical Machine Translation. In Proceedings 
of the 41st Annual Meeting of the Association for 
Computational Linguistics, pp. 160-167. 
Franz Josef Och and Hermann Ney. 2003. A Syste-
matic Comparison of Various Statistical Alignment 
Models. Computational Linguistics, 29(1): 19-52. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Weijing Zhu. 2002. BLEU: A Method for Auto-
matic Evaluation of Machine Translation. In Pro-
ceedings of 40th annual meeting of the Association 
for Computational Linguistics, pp. 311-318. 
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings for the In-
ternational Conference on Spoken Language 
Processing, pp. 901-904. 
Dekai Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3): 377-403. 
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 
2009. A Syntax-Driven Bracketing Model for 
Phrase-Based Translation. In Proceedings of the 
47th Annual Meeting of the ACL and the 4th 
IJCNLP, pp. 315-323. 
 
833
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1036?1044,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Reordering with Source Language Collocations 
 
 
Zhanyi Liu1,2, Haifeng Wang2, Hua Wu2, Ting Liu1, Sheng Li1 
1Harbin Institute of Technology, Harbin, China 
2Baidu Inc., Beijing, China 
{liuzhanyi, wanghaifeng, wu_hua}@baidu.com  
{tliu, lisheng}@hit.edu.cn 
 
 
 
Abstract 
This paper proposes a novel reordering model 
for statistical machine translation (SMT) by 
means of modeling the translation orders of 
the source language collocations. The model 
is learned from a word-aligned bilingual cor-
pus where the collocated words in source sen-
tences are automatically detected. During 
decoding, the model is employed to softly 
constrain the translation orders of the source 
language collocations, so as to constrain the 
translation orders of those source phrases con-
taining these collocated words. The experi-
mental results show that the proposed method 
significantly improves the translation quality, 
achieving the absolute improvements of 
1.1~1.4 BLEU score over the baseline me-
thods. 
1 Introduction 
Reordering for SMT is first proposed in IBM mod-
els (Brown et al, 1993), usually called IBM con-
straint model, where the movement of words 
during translation is modeled. Soon after, Wu 
(1997) proposed an ITG (Inversion Transduction 
Grammar) model for SMT, called ITG constraint 
model, where the reordering of words or phrases is 
constrained to two kinds: straight and inverted. In 
order to further improve the reordering perfor-
mance, many structure-based methods are pro-
posed, including the reordering model in 
hierarchical phrase-based SMT systems (Chiang, 
2005) and syntax-based SMT systems (Zhang et al, 
2007; Marton and Resnik, 2008; Ge, 2010; Vis-
weswariah et al, 2010). Although the sentence 
structure has been taken into consideration, these 
methods don?t explicitly make use of the strong 
correlations between words, such as collocations, 
which can effectively indicate reordering in the 
target language. 
In this paper, we propose a novel method to im-
prove the reordering for SMT by estimating the 
reordering score of the source-language colloca-
tions (source collocations for short in this paper). 
Given a bilingual corpus, the collocations in the 
source sentence are first detected automatically 
using a monolingual word alignment (MWA) me-
thod without employing additional resources (Liu 
et al, 2009), and then the reordering model based 
on the detected collocations is learned from the 
word-aligned bilingual corpus. The source colloca-
tion based reordering model is integrated into SMT 
systems as an additional feature to softly constrain 
the translation orders of the source collocations in 
the sentence to be translated, so as to constrain the 
translation orders of those source phrases contain-
ing these collocated words. 
This method has two advantages: (1) it can au-
tomatically detect and leverage collocated words in 
a sentence, including long-distance collocated 
words; (2) such a reordering model can be inte-
grated into any SMT systems without resorting to 
any additional resources. 
We implemented the proposed reordering mod-
el in a phrase-based SMT system, and the evalua-
tion results show that our method significantly 
improves translation quality. As compared to the 
baseline systems, an absolute improvement of 
1.1~1.4 BLEU score is achieved.  
1036
The paper is organized as follows: In section 2, 
we describe the motivation to use source colloca-
tions for reordering, and briefly introduces the col-
location extraction method. In section 3, we 
present our reordering model. And then we de-
scribe the experimental results in section 4 and 5. 
In section 6, we describe the related work.  Lastly, 
we conclude in section 7. 
2 Collocation 
A collocation is generally composed of a group of 
words that occur together more often than by 
chance. Collocations effectively reveal the strong 
association among words in a sentence and are 
widely employed in a variety of NLP tasks 
(Mckeown and Radey, 2000).   
Given two words in a collocation, they can be 
translated in the same order as in the source lan-
guage, or in the inverted order. We name the first 
case as straight, and the second inverted. Based on 
the observation that some collocations tend to have 
fixed translation orders such as ??? jin-rong ?fi-
nancial? ??  wei-ji ?crisis?? (financial crisis) 
whose English translation order is usually straight, 
and  ???  fa-lv ?law? ??  fan-wei ?scope?? 
(scope of law) whose English translation order is 
generally inverted, some methods have been pro-
posed to improve the reordering model for SMT 
based on the collocated words crossing the neigh-
boring components (Xiong et al, 2006). We fur-
ther notice that some words are translated in 
different orders when they are collocated with dif-
ferent words. For instance, when ??? chao-liu 
?trend?? is collocated with ??? shi-dai ?times??, 
they are often translated into the ?trend of times?; 
when collocated with ??? li-shi ?history??, the 
translation usually becomes the ?historical trend?. 
Thus, if we can automatically detect the colloca-
tions in the sentence to be translated and their or-
ders in the target language, the reordering 
information of the collocations could be used to 
constrain the reordering of phrases during decod-
ing. Therefore, in this paper, we propose to im-
prove the reordering model for SMT by estimating 
the reordering score based on the translation orders 
of the source collocations. 
In general, the collocations can be automatically 
identified based on syntactic information such as 
dependency trees (Lin, 1998). However these me-
thods may suffer from parsing errors. Moreover, 
for many languages, no valid dependency parser 
exists. Liu et al (2009) proposed to automatically 
detect the collocated words in a sentence with the 
MWA method. The advantage of this method lies 
in that it can identify the collocated words in a sen-
tence without additional resources. In this paper, 
we employ MWA Model l~3 described in Liu et al 
(2009) to detect collocations in sentences, which 
are shown in Eq. (1)~(3). 
?
?
?
l
j
cj jwwtSAp 11 ModelMWA 
)|()|(
 (1) 
?
?
??
l
j
jcj lcjdwwtSAp j12 ModelMWA 
),|()|()|(
 (2) 
?
?
?
?
?
???
l
j
jcj
l
i
ii
lcjdwwt
wnSAp
j
1
1
3 ModelMWA 
),|()|(
)|()|(
 (3) 
Where lwS 1?  is a monolingual sentence; i?  de-
notes the number of words collocating with 
iw ; 
}&],1[|),{( icliciA ii ???  denotes the potentially 
collocated words in S. 
The MWA models measure the collocated 
words under different constraints. MWA Model 1 
only models word collocation probabilities 
)|( jcj wwt
. MWA Model 2 additionally employs 
position collocation probabilities 
),|( lcjd j
. Be-
sides the features in MWA Model 2, MWA Model 
3 also considers fertility probabilities )|( ii wn ? . 
Given a sentence, the optimal collocated words 
can be obtained according to Eq. (4). 
)|(maxarg*  ModelMWA SApA iA?
           (4) 
Given a monolingual word aligned corpus, the 
collocation probabilities can be estimated as fol-
lows. 
2
)|()|(),( ijjiji wwpwwpwwr ??           
(5) 
Where, 
?
?
??
w
j
ji
ji wwcount
wwcountwwp ),(
),()|(
; 
),( ji ww  
denotes the collocated words in the corpus and 
),( ji wwcount
 denotes the co-occurrence frequency. 
1037
3 Reordering Model with Source Lan-
guage Collocations 
In this section, we first describe how to estimate 
the orientation probabilities for a given collocation, 
and then describe the estimation of the reordering 
score during translation. Finally, we describe the 
integration of the reordering model into the SMT 
system. 
3.1 Reordering probability estimation 
Given a source collocation ),( ji ff
 and its corres-
ponding translations 
),( ji aa ee
 in a bilingual sen-
tence pair, the reordering orientation of the 
collocation can be defined as in Eq. (6).  
??
?
????
?????
jiji
jiji
aaji aajiaaji
aajiaajio ji &or& ifinverted
or ifstraight
,,,
(6) 
In our method, only those collocated words in 
source language that are aligned to different target 
words, are taken into consideration, and those be-
ing aligned to the same target word are ignored. 
Given a word-aligned bilingual corpus where 
the collocations in source sentences are detected, 
the probabilities of the translation orientation of 
collocations in the source language can be esti-
mated, as follows: 
? ? ?
???
o ji
ji
ji ffocount
ffocountffop ),,(
),,straight(),|straight(
   (7) 
? ? ?
???
o ji
ji
ji ffocount
ffocountffop ),,(
),,inverted(),|inverted(
   
(8) 
Here, ),,( ji ffocount
 is collected according to 
the algorithm in Figure 1. 
3.2 Reordering model 
Given a sentence lfF 1?  to be translated, the col-
locations are first detected using the algorithm de-
scribed in Eq. (4). Then the reordering score is 
estimated according to the reordering probability 
weighted by the collocation probability of the col-
located words. Formally, for a generated transla-
tion candidate T , the reordering score is calculated 
as follows. 
),|(log),(),( ,,,),( iiciii i ciaacici ciO ffopffrTFP ??
    (9) 
Input: A word-aligned bilingual corpus where 
the source collocations are detected 
Initialization: 
),,( ji ffocount
=0 
for each sentence pair <F, E> in the corpus do 
for each collocated word pair 
),( ici ff
in F do 
        if 
icii aaci ?? &
or 
icii aaci ?? &
 then 
            
??? ),,( ici ffstraightocount
 
        if 
icii aaci ?? &
or 
icii aaci ?? &
 then 
            
??? ),,( ici ffinvertedocount  
Output: ),,( ji ffocount
 
Figure 1. Algorithm of estimating  
reordering frequency 
Here, 
),( ici ffr
 denotes the collocation probabil-
ity of 
if  and icf
 as shown in Eq. (5). 
In addition to the detected collocated words in 
the sentence, we also consider other possible word 
pairs whose collocation probabilities are higher 
than a given threshold.  Thus, the reordering score 
is further improved according to Eq. (10). 
?
?
??
?
??
????
),(&
)},{(),(
,,,
,,,
),(
)},|(log),(
),|(log),(),(
ji
i
ji
iicii
i
i
ffr
ciji
jiaajiji
ciaaci
ci
ciO
ffopffr
ffopffrTFP
 
(10) 
Where ? and ?  are two interpolation weights. 
?  is the threshold of collocation probability. The 
weights and the threshold can be tuned using a de-
velopment set. 
3.3 Integrated into SMT system 
The SMT systems generally employ the log-linear 
model to integrate various features (Chiang, 2005; 
Koehn et al, 2007). Given an input sentence F, the 
final translation E* with the highest score is chosen 
from candidates, as in Eq. (11). 
}),({maxarg*
1
?
?
? M
m
mmE
FEhE ?
 (11) 
Where hm(E, F) (m=1,...,M) denotes fea-
tures.
m?  is a feature weight. 
Our reordering model can be integrated into the 
system as one feature as shown in (10). 
1038
 Figure 2. An example for reordering 
4 Evaluation of Our Method 
4.1 Implementation 
We implemented our method in a phrase-based 
SMT system (Koehn et al, 2007). Based on the 
GIZA++ package (Och and Ney, 2003), we im-
plemented a MWA tool for collocation detection. 
Thus, given a sentence to be translated, we first 
identify the collocations in the sentence, and then 
estimate the reordering score according to the 
translation hypothesis. For a translation option to 
be expanded, the reordering score inside this 
source phrase is calculated according to their trans-
lation orders of the collocations in the correspond-
ing target phrase. The reordering score crossing the 
current translation option and the covered parts can 
be calculated according to the relative position of 
the collocated words. If the source phrase matched 
by the current translation option is behind the cov-
ered parts in the source sentence, then 
...)|staight(log ?op  is used, otherwise 
...)|inverted(log ?op . For example, in Figure 2, the 
current translation option is (
4332 eeff ? ). The 
collocations related to this translation option are 
),( 31 ff , ),( 32 ff , ),( 53 ff . The reordering scores 
can be estimated as follows: 
),|straight(log),( 3131 ffopffr ? 
),|inverted(log),( 3232 ffopffr ? 
),|inverted(log),( 5353 ffopffr ? 
In order to improve the performance of the de-
coder, we design a heuristic function to estimate 
the future score, as shown in Figure 3. For any un-
covered word and its collocates in the input sen-
tence, if the collocate is uncovered, then the higher 
reordering probability is used. If the collocate has 
been covered, then the reordering orientation can 
Input: Input sentence LfF 1?  
Initialization: Score = 0 
for each uncovered word 
if  do 
for each word
jf
(
icj ?  
or 
??)( , ji ffr
) do 
if 
jf
 is covered then 
if i > j then 
Score+=
),|straight(log)( , jiji ffopffr ?
 
else 
Score+=
),|inverted(log)( , jiji ffopffr ? 
else 
 Score +=
),|(log)(maxarg , jijio ffopffr
 
Output: Score 
Figure 3. Heuristic function for estimating future 
score 
be determined according to the relative positions of 
the words and the corresponding reordering proba-
bility is employed. 
4.2 Settings 
We use the FBIS corpus (LDC2003E14) to train a 
Chinese-to-English phrase-based translation model. 
And the SRI language modeling toolkit (Stolcke, 
2002) is used to train a 5-gram language model on 
the English sentences of FBIS corpus.  
We used the NIST evaluation set of 2002 as the 
development set to tune the feature weights of the 
SMT system and the interpolation parameters, 
based on the minimum error rate training method 
(Och, 2003), and the NIST evaluation sets of 2004 
and 2008 (MT04 and MT08) as the test sets. 
We use BLEU (Papineni et al, 2002) as evalua-
tion metrics. We also calculate the statistical signi-
ficance differences between our methods and the 
baseline method by using the paired bootstrap re-
sample method (Koehn, 2004). 
4.3 Translation results 
We compare the proposed method with various 
reordering methods in previous work. 
Monotone model: no reordering model is used. 
Distortion based reordering (DBR) model: a 
distortion based reordering method (Al-
Onaizan & Papineni, 2006). In this method, the 
distortion cost is defined in terms of words, ra-
ther than phrases. This method considers out-
bound, inbound, and pairwise distortions that  
f1    f2     f3     f4      f5 
e4 
e3 
e2 
e1 
1039
Reorder models MT04 MT08 
Monotone model 26.99 18.30 
DBR model 26.64 17.83 
MSDR model (Baseline) 28.77 18.42 
MSDR+ 
DBR model 28.91 18.58 
SCBR Model 1 29.21 19.28 
SCBR Model 2 29.44 19.36 
SCBR Model 3 29.50 19.44 
SCBR models (1+2) 29.65 19.57 
SCBR models (1+2+3) 29.75 19.61 
Table 1. Translation results on various reordering models 
 
T1: The two sides are also the basic stand of not relaxed. 
T2: The basic stance of the two sides have not relaxed. 
Reference: The basic stances of both sides did not move. 
Figure 4. Translation example.  (*/*) denotes (pstraight / pinverted)
 are directly estimated by simple counting over 
alignments in the word-aligned bilingual cor-
pus. This method is similar to our proposed 
method. But our method considers the transla-
tion order of the collocated words. 
msd-bidirectional-fe reordering (MSDR or 
Baseline) model: it is one of the reordering 
models in Moses. It considers three different 
orientation types (monotone, swap, and discon-
tinuous) on both source phrases and target 
phrases. And the translation orders of both the 
next phrase and the previous phrase in respect 
to the current phrase are modeled. 
Source collocation based reordering (SCBR) 
model: our proposed method. We investigate 
three reordering models based on the corres-
ponding MWA models and their combinations. 
In SCBR Model i (i=1~3), we use MWA Mod-
el i as described in section 2 to obtain the col-
located words and estimate the reordering 
probabilities according to section 3. 
The experiential results are shown in Table 1. 
The DBR model suffers from serious data sparse-
ness. For example, the reordering cases in the 
trained pairwise distortion model only covered 
32~38% of those in the test sets. So its perfor-
mance is worse than that of the monotone model. 
The MSDR model achieves higher BLEU scores 
than the monotone model and the DBR model. Our 
models further improve the translation quality, 
achieving better performance than the combination 
of MSDR model and DBR model. The results in 
Table 1 show that ?MSDR + SCBR Model 3? per-
forms the best among the SCBR models. This is 
because, as compared to MWA Model 1 and 2, 
MWA Model 3 takes more information into con-
sideration, including not only the co-occurrence 
information of lexical tokens and the position of 
words, but also the fertility of words in a sentence. 
And when the three SCBR models are combined, 
the performance of the SMT system is further im-
proved. As compared to other reordering models, 
our models achieve an absolute improvement of 
0.98~1.19 BLEU score on the test sets, which are 
statistically significant (p < 0.05).  
Figure 4 shows an example: T1 is generated by 
the baseline system and T2 is generated by the sys-
tem where the SCBR models (1+2+3)1 are used.  
                                                          
1 In the remainder of this paper, ?SCBR models? means the 
combination of the SCBR models (1+2+3) unless it is explicit-
ly explained.  
Input:  ??     ?   ??      ??  ?    ?  ??  ??   ? 
shuang-fang    DE    ji-ben       li-chang   ye      dou mei-you song-dong . 
(0.99/0.01) 
both-side       DE     basic          stance  also    both    not        loose     . 
(0.21/0.79) 
(0.95/0.05) 
1040
Reordering models MT04 MT08 
MSDR model 28.77 18.42 
MSDR+ 
DBR model 28.91 18.58 
CBR model 28.96 18.77 
WCBR model 29.15 19.10 
WCBR+SCBR 
models 
29.87 19.83 
Table 2. Translation results of co-occurrence 
based reordering models 
 CBR model 
SCBR 
Model3 
Consecutive words 77.9% 73.5% 
Interrupted words 74.1% 87.8% 
Total 74.3% 84.9% 
Table 3. Precisions of the reordering models on 
the development set 
The input sentence contains three collocations. The 
collocation (??, ??) is included in the same 
phrase and translated together as a whole. Thus its 
translation is correct in both translations. For the 
other two long-distance collocations (??, ??) 
and (??, ??), their translation orders are not 
correctly handled by the reordering model in the 
baseline system. For the collocation (??, ??), 
since the SCBR models indicate p(o=straight|??, 
??) < p(o=inverted|??, ??), the system fi-
nally generates the translation T2 by constraining 
their translation order with the proposed model. 
5 Collocations vs. Co-occurring Words 
We compared our method with the method that 
models the reordering orientations based on co-
occurring words in the source sentences, rather 
than the collocations.  
5.1 Co-occurrence based reordering model 
We use the similar algorithm described in section 3 
to train the co-occurrence based reordering (CBR) 
model, except that the probability of the reordering 
orientation is estimated on the co-occurring words 
and the relative distance. Given an input sentence 
and a translation candidate, the reordering score is 
estimated as shown in Eq. (12). 
? ??? ),( ,,, ),,|(log),( ji jijiaajiO ffopTFP ji
        (12) 
Here, 
ji??
 is the relative distance of two words 
in the source sentence.  
We also construct the weighted co-occurrence 
based reordering (WCBR) model. In this model, 
the probability of the reordering orientation is ad-
ditionally weighted by the pointwise mutual infor-
mation 2  score of the two words (Manning and 
Sch?tze, 1999), which is estimated as shown in Eq. 
(13). 
? ???
),(
,,,MI ),,|(log),(
),(
ji
jijiaajiji
O
ffopffs
TFP
ji
   (13) 
5.2 Translation results 
Table 2 shows the translation results. It can be seen 
that the performance of the SMT system is im-
proved by integrating the CBR model. The perfor-
mance of the CBR model is also better than that of 
the DBR model. It is because the former is trained 
based on all co-occurring aligned words, while the 
latter only considers the adjacent aligned words. 
When the WCBR model is used, the translation 
quality is further improved. However, its perfor-
mance is still inferior to that of the SCBR models, 
indicating that our method (SCBR models) of 
modeling the translation orders of source colloca-
tions is more effective. Furthermore, we combine 
the weighted co-occurrence based model and our 
method, which outperform all the other models. 
5.3 Result analysis 
Precision of prediction 
First of all, we investigate the performance of 
the reordering models by calculating precisions of 
the translation orders predicted by the reordering 
models. Based on the source sentences and refer-
ence translations of the development set, where the 
source words and target words are automatically 
aligned by the bilingual word alignment method, 
we construct the reference translation orders for 
two words. Against the references, we calculate 
three kinds of precisions as follows: 
|}1|||{|
|}&1{|
,
,,,,
CW ??
???? jio
ooj||iP
ji
aajiji ji
 (14) 
                                                          
2 For occurring words extraction, the window size is set to [-6, 
+6]. 
1041
|}1|||{|
|}&1{|
,
,,,,
IW ??
???? jio
ooj||iP
ji
aajiji ji
 (15) 
 |}{|
|}{|
,
,,,,
total
ji
aajiji
o
ooP ji??
 (16) 
Here, 
jio ,
 denotes the translation order of (
ji ff ,
) 
predicted by the reordering models. If 
)|straight( , ji ffop ?
>
),inverted( ji f|fop ?
, then 
straight, ?jio
, else if 
)|straight( , ji ffop ?
< 
),inverted( ji f|fop ?
, then
inverted, ?jio
. 
ji aajio ,,,
 
denotes the translation order derived from the word 
alignments. If 
ji aajiji oo ,,,, ?
, then the predicted 
translation order is correct, otherwise wrong. 
CWP  
and 
IWP  denote the precisions calculated on the 
consecutive words and the interrupted words in the 
source sentences, respectively. 
totalP  denotes the 
precision on both cases. Here, the CBR model and 
SCBR Model 3 are compared. The results are 
shown in Table 3.  
From the results in Table 3, it can be seen that 
the CBR model has a higher precision on the con-
secutive words than the SCBR model, but lower 
precisions on the interrupted words. It is mainly 
because the CBR model introduces more noise 
when the relative distance of words is set to a large 
number, while the MWA method can effectively 
detect the long-distance collocations in sentences 
(Liu et al, 2009). This explains why the combina-
tion of the two models can obtain the highest 
BLEU score as shown in Table 2. On the whole, 
the SCBR Model 3 achieves higher precision than 
the CBR model. 
Effect of the reordering model 
Then we evaluate the reordering results of the 
generated translations in the test sets. Using the 
above method, we construct the reference transla-
tion orders of collocations in the test sets. For a 
given word pair in a source sentence, if the transla-
tion order in the generated translation is the same 
as that in the reference translations, then it is cor-
rect, otherwise wrong. 
We compare the translations of the baseline me-
thod, the co-occurrence based method, and our me-
thod (SCBR models). The precisions calculated on 
both kinds of words are shown in Table 4. From 
Test sets 
Baseline 
(MSDR) 
MSDR+ 
WCBR 
MSDR+ 
SCBR 
MT04 78.9% 80.8% 82.5% 
MT08 80.7% 83.8% 85.0% 
Table 4. Precisions (total) of the reordering 
models on the test sets 
the results, it can be seen that our method achieves 
higher precisions than both the baseline and the 
method modeling the translation orders of the co-
occurring words. It indicates that the proposed me-
thod effectively constrains the reordering of source 
words during decoding and improves the transla-
tion quality. 
6 Related Work 
Reordering was first proposed in the IBM models 
(Brown et al, 1993), later was named IBM con-
straint by Berger et al (1996). This model treats 
the source word sequence as a coverage set that is 
processed sequentially and a source token is cov-
ered when it is translated into a new target token. 
In 1997, another model called ITG constraint was 
presented, in which the reordering order can be 
hierarchically modeled as straight or inverted for 
two nodes in a binary branching structure (Wu, 
1997). Although the ITG constraint allows more 
flexible reordering during decoding, Zens and Ney 
(2003) showed that the IBM constraint results in 
higher BLEU scores. Our method models the reor-
dering of collocated words in sentences instead of 
all words in IBM models or two neighboring 
blocks in ITG models. 
For phrase-based SMT models, Koehn et al 
(2003) linearly modeled the distance of phrase 
movements, which results in poor global reorder-
ing. More methods are proposed to explicitly mod-
el the movements of phrases (Tillmann, 2004; 
Koehn et al, 2005) or to directly predict the orien-
tations of phrases (Tillmann and Zhang, 2005; 
Zens and Ney, 2006), conditioned on current 
source phrase or target phrase. Hierarchical phrase-
based SMT methods employ SCFG bilingual trans-
lation model and allow flexible reordering (Chiang, 
2005). However, these methods ignored the corre-
lations among words in the source language or in 
the target language. In our method, we automati-
cally detect the collocated words in sentences and 
1042
their translation orders in the target languages, 
which are used to constrain the ordering models 
with the estimated reordering (straight or inverted) 
score. Moreover, our method allows flexible reor-
dering by considering both consecutive words and 
interrupted words. 
In order to further improve translation results, 
many researchers employed syntax-based reorder-
ing methods (Zhang et al, 2007; Marton and Res-
nik, 2008; Ge, 2010; Visweswariah et al, 2010). 
However these methods are subject to parsing er-
rors to a large extent. Our method directly obtains 
collocation information without resorting to any 
linguistic knowledge or tools, therefore is suitable 
for any language pairs. 
In addition, a few models employed the collo-
cation information to improve the performance of 
the ITG constraints (Xiong et al, 2006). Xiong et 
al. used the consecutive co-occurring words as col-
location information to constrain the reordering, 
which did not lead to higher translation quality in 
their experiments. In our method, we first detect 
both consecutive and interrupted collocated words 
in the source sentence, and then estimated the 
reordering score of these collocated words, which 
are used to softly constrain the reordering of source 
phrases. 
7 Conclusions 
We presented a novel model to improve SMT by 
means of modeling the translation orders of source 
collocations. The model was learned from a word-
aligned bilingual corpus where the potentially col-
located words in source sentences were automati-
cally detected by the MWA method. During 
decoding, the model is employed to softly con-
strain the translation orders of the source language 
collocations. Since we only model the reordering 
of collocated words, our methods can partially al-
leviate the data sparseness encountered by other 
methods directly modeling the reordering based on 
source phrases or target phrases. In addition, this 
kind of reordering information can be integrated 
into any SMT systems without resorting to any 
additional resources. 
The experimental results show that the pro-
posed method significantly improves the transla-
tion quality of a phrase based SMT system, 
achieving an absolute improvement of 1.1~1.4 
BLEU score over the baseline methods. 
References 
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion Models for Statistical Machine Translation. In 
Proceedings of the 21st International Conference on 
Computational Linguistics and 44th Annual Meeting 
of the ACL, pp. 529-536. 
Adam L. Berger, Peter F. Brown, Stephen A. Della Pie-
tra, Vincent J. Della Pietra, Andrew S. Kehler, and 
Robert L. Mercer. 1996. Language Translation Appa-
ratus and Method of Using Context-Based Transla-
tion Models. United States Patent, Patent Number 
5510981, April.  
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Del-
la Pietra, and Robert. L. Mercer. 1993. The Mathe-
matics of Statistical Machine Translation: Parameter 
estimation. Computational Linguistics, 19(2): 263-
311. 
David Chiang. 2005. A Hierarchical Phrase-based Mod-
el for Statistical Machine Translation. In Proceedings 
of the 43rd Annual Meeting of the Association for 
Computational Linguistics, pp. 263-270. 
Niyu Ge. 2010. A Direct Syntax-Driven Reordering 
Model for Phrase-Based Machine Translation. In 
Proceedings of Human Language Technologies: The 
2010 Annual Conference of the North American 
Chapter of the ACL, pp. 849-857. 
Philipp Koehn. 2004. Statistical Significance Tests for 
Machine Translation Evaluation. In Proceedings of 
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, pp. 388-395. 
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the Joint Conference on Human Lan-
guage Technologies and the Annual Meeting of the 
North American Chapter of the Association of Com-
putational Linguistics, pp. 127-133. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. In 
Proceedings of the 45th Annual Meeting of the ACL, 
Poster and Demonstration Sessions, pp. 177-180. 
Philipp Koehn, Amittai Axelrod, Alexandra Birch 
Mayne, Chris Callison-Burch, Miles Osborne, and 
David Talbot. 2005. Edinburgh System Description 
for the 2005 IWSLT Speech Translation Evaluation. 
In Proceedings of International Workshop on Spoken 
Language Translation. 
1043
Dekang Lin. 1998. Extracting Collocations from Text 
Corpora. In Proceedings of the 1st Workshop on 
Computational Terminology, pp. 57-63. 
Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li. 
2009. Collocation Extraction Using Monolingual 
Word Alignment Method. In Proceedings of the 2009 
Conference on Empirical Methods in Natural Lan-
guage Processing, pp. 487-495. 
Christopher D. Manning and Hinrich Sch?tze. 1999. 
Foundations of Statistical Natural Language 
Processing, Cambridge, MA; London, U.K.: Brad-
ford Book & MIT Press. 
Yuval Marton and Philip Resnik. 2008. Soft Syntactic 
Constraints for Hierarchical Phrased-based Transla-
tion. In Proceedings of the 46st Annual Meeting of 
the Association for Computational Linguistics: Hu-
man Language Technologies, pp. 1003-1011. 
Kathleen R. McKeown and Dragomir R. Radev. 2000. 
Collocations. In Robert Dale, Hermann Moisl, and 
Harold Somers (Ed.), A Handbook of Natural Lan-
guage Processing, pp. 507-523. 
Franz Josef Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proceedings of 
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pp. 160-167. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Models. 
Computational Linguistics, 29(1) : 19-51. 
Kishore Papineni, Salim Roukos, Todd Ward, and Weij-
ing Zhu. 2002. BLEU: A Method for Automatic 
Evaluation of Machine Translation. In Proceedings 
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pp. 311-318. 
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings for the In-
ternational Conference on Spoken Language 
Processing, pp. 901-904. 
Christoph Tillmann. 2004. A Unigram Orientation 
Model for Statistical Machine Translation. In Pro-
ceedings of the Joint Conference on Human Lan-
guage Technologies and the Annual Meeting of the 
North American Chapter of the Association of Com-
putational Linguistics, pp. 101-104. 
Christoph Tillmann and Tong Zhang. 2005. A Localized 
Prediction Model for Statistical Machine Translation. 
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, pp. 557-564. 
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen, 
Vijil Chenthamarakshan, and Nanda Kambhatla. 
2010. Syntax Based Reordering with Automatically 
Derived Rules for Improved Statistical Machine 
Translation. In Proceedings of the 23rd International 
Conference on Computational Linguistics, pp. 1119-
1127. 
Dekai Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Corpora. 
Computational Linguistics, 23(3):377-403. 
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for 
Statistical Machine Translation. In Proceedings of 
the 21st International Conference on Computational 
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pp. 521-528. 
Richard Zens and Herman Ney. 2003. A Comparative 
Study on Reordering Constraints in Statistical Ma-
chine Translation. In Proceedings of the 41st Annual 
Meeting of the Association for Computational Lin-
guistics, pp. 192-202. 
Richard Zens and Herman Ney. 2006. Discriminative 
Reordering Models for Statistical Machine Transla-
tion. In Proceedings of the Workshop on Statistical 
Machine Translation, pp. 55-63. 
Dongdong Zhang, Mu Li, Chi-Ho Li, and Ming Zhou. 
2007. Phrase Reordering Model Integrating Syntactic 
Knowledge for SMT. In Proceedings of the 2007 
Joint Conference on Empirical Methods in Natural 
Language Processing and Computational Natural 
Language Learning, pp. 533-540. 
 
 
 
1044
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459?468,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Translation Model Adaptation for Statistical Machine Translation with
Monolingual Topic Information?
Jinsong Su1,2, Hua Wu3, Haifeng Wang3, Yidong Chen1, Xiaodong Shi1,
Huailin Dong1, and Qun Liu2
Xiamen University, Xiamen, China1
Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China2
Baidu Inc., Beijing, China3
{jssu, ydchen, mandel, hldong}@xmu.edu.cn
{wu hua, wanghaifeng}@baicu.com
liuqun@ict.ac.cn
Abstract
To adapt a translation model trained from
the data in one domain to another, previous
works paid more attention to the studies of
parallel corpus while ignoring the in-domain
monolingual corpora which can be obtained
more easily. In this paper, we propose a
novel approach for translation model adapta-
tion by utilizing in-domain monolingual top-
ic information instead of the in-domain bilin-
gual corpora, which incorporates the topic in-
formation into translation probability estima-
tion. Our method establishes the relationship
between the out-of-domain bilingual corpus
and the in-domain monolingual corpora vi-
a topic mapping and phrase-topic distribution
probability estimation from in-domain mono-
lingual corpora. Experimental result on the
NIST Chinese-English translation task shows
that our approach significantly outperforms
the baseline system.
1 Introduction
In recent years, statistical machine translation(SMT)
has been rapidly developing with more and more
novel translation models being proposed and put in-
to practice (Koehn et al, 2003; Och and Ney, 2004;
Galley et al, 2006; Liu et al, 2006; Chiang, 2007;
Chiang, 2010). However, similar to other natural
language processing(NLP) tasks, SMT systems of-
ten suffer from domain adaptation problem during
practical applications. The simple reason is that the
underlying statistical models always tend to closely
?Part of this work was done during the first author?s intern-
ship at Baidu.
approximate the empirical distributions of the train-
ing data, which typically consist of bilingual sen-
tences and monolingual target language sentences.
When the translated texts and the training data come
from the same domain, SMT systems can achieve
good performance, otherwise the translation quality
degrades dramatically. Therefore, it is of significant
importance to develop translation systems which can
be effectively transferred from one domain to anoth-
er, for example, from newswire to weblog.
According to adaptation emphases, domain adap-
tation in SMT can be classified into translation mod-
el adaptation and language model adaptation. Here
we focus on how to adapt a translation model, which
is trained from the large-scale out-of-domain bilin-
gual corpus, for domain-specific translation task,
leaving others for future work. In this aspect, pre-
vious methods can be divided into two categories:
one paid attention to collecting more sentence pairs
by information retrieval technology (Hildebrand et
al., 2005) or synthesized parallel sentences (Ueffing
et al, 2008; Wu et al, 2008; Bertoldi and Federico,
2009; Schwenk and Senellart, 2009), and the other
exploited the full potential of existing parallel cor-
pus in a mixture-modeling (Foster and Kuhn, 2007;
Civera and Juan, 2007; Lv et al, 2007) framework.
However, these approaches focused on the studies of
bilingual corpus synthesis and exploitation while ig-
noring the monolingual corpora, therefore limiting
the potential of further translation quality improve-
ment.
In this paper, we propose a novel adaptation
method to adapt the translation model for domain-
specific translation task by utilizing in-domain
459
monolingual corpora. Our approach is inspired by
the recent studies (Zhao and Xing, 2006; Zhao and
Xing, 2007; Tam et al, 2007; Gong and Zhou, 2010;
Ruiz and Federico, 2011) which have shown that a
particular translation always appears in some spe-
cific topical contexts, and the topical context infor-
mation has a great effect on translation selection.
For example, ?bank? often occurs in the sentences
related to the economy topic when translated into
?y?inha?ng?, and occurs in the sentences related to the
geography topic when translated to ?he?a`n?. There-
fore, the co-occurrence frequency of the phrases in
some specific context can be used to constrain the
translation candidates of phrases. In a monolingual
corpus, if ?bank? occurs more often in the sentences
related to the economy topic than the ones related
to the geography topic, it is more likely that ?bank?
is translated to ?y?inha?ng? than to ?he?a`n?. With the
out-of-domain bilingual corpus, we first incorporate
the topic information into translation probability es-
timation, aiming to quantify the effect of the topical
context information on translation selection. Then,
we rescore all phrase pairs according to the phrase-
topic and the word-topic posterior distributions of
the additional in-domain monolingual corpora. As
compared to the previous works, our method takes
advantage of both the in-domain monolingual cor-
pora and the out-of-domain bilingual corpus to in-
corporate the topic information into our translation
model, thus breaking down the corpus barrier for
translation quality improvement. The experimental
results on the NIST data set demonstrate the effec-
tiveness of our method.
The reminder of this paper is organized as fol-
lows: Section 2 provides a brief description of trans-
lation probability estimation. Section 3 introduces
the adaptation method which incorporates the top-
ic information into the translation model; Section
4 describes and discusses the experimental results;
Section 5 briefly summarizes the recent related work
about translation model adaptation. Finally, we end
with a conclusion and the future work in Section 6.
2 Background
The statistical translation model, which contains
phrase pairs with bi-directional phrase probabilities
and bi-directional lexical probabilities, has a great
effect on the performance of SMT system. Phrase
probability measures the co-occurrence frequency of
a phrase pair, and lexical probability is used to vali-
date the quality of the phrase pair by checking how
well its words are translated to each other.
According to the definition proposed by (Koehn
et al, 2003), given a source sentence f = fJ1 =
f1, . . . , fj , . . . , fJ , a target sentence e = eI1 =
e1, . . . , ei, . . . , eI , and its word alignment a which
is a subset of the Cartesian product of word position-
s: a ? (j, i) : j = 1, . . . , J ; i = 1, . . . , I , the phrase
pair (f? , e?) is said to be consistent (Och and Ney,
2004) with the alignment if and only if: (1) there
must be at least one word inside one phrase aligned
to a word inside the other phrase and (2) no words
inside one phrase can be aligned to a word outside
the other phrase. After all consistent phrase pairs are
extracted from training corpus, the phrase probabil-
ities are estimated as relative frequencies (Och and
Ney, 2004):
?(e?|f?) =
count(f? , e?)
?
e??
count(f? , e??)
(1)
Here count(f? , e?) indicates how often the phrase pair
(f? , e?) occurs in the training corpus.
To obtain the corresponding lexical weight, we
first estimate a lexical translation probability distri-
bution w(e|f) by relative frequency from the train-
ing corpus:
w(e|f) =
count(f, e)
?
e?
count(f, e?)
(2)
Retaining the alignment a? between the phrase pair
(f? , e?), the corresponding lexical weight is calculated
as
pw(e?|f? , a?) =
|e?|?
i=1
1
|{j|(j, i) ? a?}|
?
?(j,i)?a?
w(ei|fj) (3)
However, the above-mentioned method only
counts the co-occurrence frequency of bilingual
phrases, assuming that the translation probability is
independent of the context information. Thus, the
statistical model estimated from the training data is
not suitable for text translation in different domains,
resulting in a significant drop in translation quality.
460
3 Translation Model Adaptation via
Monolingual Topic Information
In this section, we first briefly review the principle
of Hidden Topic Markov Model(HTMM) which is
the basis of our method, then describe our approach
to translation model adaptation in detail.
3.1 Hidden Topic Markov Model
During the last couple of years, topic models such
as Probabilistic Latent Semantic Analysis (Hof-
mann, 1999) and Latent Dirichlet Allocation mod-
el (Blei, 2003), have drawn more and more attention
and been applied successfully in NLP community.
Based on the ?bag-of-words? assumption that the or-
der of words can be ignored, these methods model
the text corpus by using a co-occurrence matrix of
words and documents, and build generative model-
s to infer the latent aspects or topics. Using these
models, the words can be clustered into the derived
topics with a probability distribution, and the corre-
lation between words can be automatically captured
via topics.
However, the ?bag-of-words? assumption is an
unrealistic oversimplification because it ignores the
order of words. To remedy this problem, Gruber et
al.(2007) propose HTMM, which models the topics
of words in the document as a Markov chain. Based
on the assumption that all words in the same sen-
tence have the same topic and the successive sen-
tences are more likely to have the same topic, HTM-
M incorporates the local dependency between words
by Hidden Markov Model for better topic estima-
tion.
HTMM can also be viewed as a soft clustering
tool for words in training corpus. That is, HT-
MM can estimate the probability distribution of a
topic over words, i.e. the topic-word distribution
P (word|topic) during training. Besides, HTMM
derives inherent topics in sentences rather than in
documents, so we can easily obtain the sentence-
topic distribution P (topic|sentence) in training
corpus. Adopting maximum likelihood estima-
tion(MLE), this posterior distribution makes it pos-
sible to effectively calculate the word-topic distri-
bution P (topic|word) and the phrase-topic distribu-
tion P (topic|phrase) both of which are very impor-
tant in our method.
3.2 Adapted Phrase Probability Estimation
We utilize the additional in-domain monolingual
corpora to adapt the out-of-domain translation mod-
el for domain-specific translation task. In detail, we
build an adapted translation model in the following
steps:
? Build a topic-specific translation model to
quantify the effect of the topic information on
the translation probability estimation.
? Estimate the topic posterior distributions of
phrases in the in-domain monolingual corpora.
? Score the phrase pairs according to the prede-
fined topic-specific translation model and the
topic posterior distribution of phrases.
Formally, we incorporate monolingual topic in-
formation into translation probability estimation,
and decompose the phrase probability ?(e?|f?)1 as
follows:
?(e?|f?) =
?
tf
?(e?, tf |f?)
=
?
tf
?(e?|f? , tf ) ? P (tf |f?) (4)
where ?(e?|f? , tf ) indicates the probability of trans-
lating f? into e? given the source-side topic tf ,
P (tf |f?) denotes the phrase-topic distribution of f? .
To compute ?(e?|f?), we first apply HTMM to re-
spectively train two monolingual topic models with
the following corpora: one is the source part of
the out-of-domain bilingual corpus Cf out, the oth-
er is the in-domain monolingual corpus Cf in in the
source language. Then, we respectively estimate
?(e?|f? , tf ) and P (tf |f?) from these two corpora. To
avoid confusion, we further refine ?(e?|f? , tf ) and
P (tf |f?) with ?(e?|f? , tf out) and P (tf in|f?), respec-
tively. Here, tf out is the topic clustered from the
corpus Cf out, and tf in represents the topic derived
from the corpus Cf in.
However, the two above-mentioned probabilities
can not be directly multiplied in formula (4) be-
cause they are related to different topic spaces from
1Due to the limit of space, we omit the description of the cal-
culation method of the phrase probability ?(f? |e?), which can be
adjusted in a similar way to ?(e?|f?) with the help of in-domain
monolingual corpus in the target language.
461
different corpora. Besides, their topic dimension-
s are not assured to be the same. To solve this
problem, we introduce the topic mapping probabili-
ty P (tf out|tf in) to map the in-domain phrase-topic
distribution into the one in the out-domain topic s-
pace. To be specific, we obtain the out-of-domain
phrase-topic distribution P (tf out|f?) as follows:
P (tf out|f?) =
?
tf in
P (tf out|tf in) ? P (tf in|f?) (5)
Thus formula (4) can be further refined as the fol-
lowing formula:
?(e?|f?) =
?
tf out
?
tf in
?(e?|f? , tf out)
?P (tf out|tf in) ? P (tf in|f?) (6)
Next we will give detailed descriptions of the cal-
culation methods for the three probability distribu-
tions mentioned in formula (6).
3.2.1 Topic-Specific Phrase Translation
Probability ?(e?|f? , tf out)
We follow the common practice (Koehn et al,
2003) to calculate the topic-specific phrase trans-
lation probability, and the only difference is that
our method takes the topical context information in-
to account when collecting the fractional counts of
phrase pairs. With the sentence-topic distribution
P (tf out|f) from the relevant topic model of Cf out,
the conditional probability ?(e?|f? , tf out) can be eas-
ily obtained by MLE method:
?(e?|f? , tf out)
=
?
?f ,e??Cout
count?f ,e?(f? , e?) ? P (tf out|f)
?
e??
?
?f ,e??Cout
count?f ,e?(f? , e??) ? P (tf out|f)
(7)
where Cout is the out-of-domain bilingual training
corpus, and count?f ,e?(f? , e?) denotes the number of
the phrase pair (f? , e?) in sentence pair ?f , e?.
3.2.2 Topic Mapping Probability P (tf out|tf in)
Based on the two monolingual topic models re-
spectively trained from Cf in and Cf out, we com-
pute the topic mapping probability by using source
word f as the pivot variable. Noticing that there
are some words occurring in one corpus only, we
use the words belonging to both corpora during the
mapping procedure. Specifically, we decompose
P (tf out|tf in) as follows:
P (tf out|tf in)
=
?
f?Cf out
?
Cf in
P (tf out|f) ? P (f |tf in) (8)
Here we first get P (f |tf in) directly from the top-
ic model related to Cf in. Then, considering the
sentence-topic distribution P (tf out|f) from the rel-
evant topic model of Cf out, we define the word-
topic distribution P (tf out|f) as:
P (tf out|f)
=
?
f?Cf out
countf (f) ? P (tf out|f)
?
tf out
?
f?Cf out
countf (f) ? P (tf out|f)
(9)
where countf (f) denotes the number of the word f
in sentence f .
3.2.3 Phrase-Topic Distribution P (tf in|f? )
A simple way to compute the phrase-topic distri-
bution is to take the fractional counts from Cf in
and then adopt MLE to obtain relative probability.
However, it is infeasible in our model because some
phrases occur in Cf out while being absent in Cf in.
To solve this problem, we further compute this pos-
terior distribution by the interpolation of two model-
s:
P (tf in|f?) = ? ? Pmle(tf in|f?) +
(1? ?) ? Pword(tf in|f?) (10)
where Pmle(tf in|f?) indicates the phrase-topic dis-
tribution by MLE, Pword(tf in|f?) denotes the
phrase-topic distribution which is decomposed into
the topic posterior distribution at the word level, and
? is the interpolation weight that can be optimized
over the development data.
Given the number of the phrase f? in sentence f
denoted as countf (f?), we compute the in-domain
phrase-topic distribution in the following way:
Pmle(tf in|f?)
=
?
f?Cf in
countf (f?) ? P (tf in|f)
?
tf in
?
f?Cf in
countf (f?) ? P (tf in|f)
(11)
462
Under the assumption that the topics of all word-
s in the same phrase are independent, we consid-
er two methods to calculate Pword(tf in|f?). One is
a ?Noisy-OR? combination method (Zens and Ney,
2004) which has shown good performance in calcu-
lating similarities between bags-of-words in differ-
ent languages. Using this method, Pword(tf in|f?) is
defined as:
Pword(tf in|f?)
= 1? Pword(t?f in|f?)
? 1?
?
fj?f?
P (t?f in|fj)
= 1?
?
fj?f?
(1? P (tf in|fj)) (12)
where Pword(t?f in|f?) represents the probability that
tf in is not the topic of the phrase f? . Similarly,
P (t?f in|fj) indicates the probability that tf in is not
the topic of the word fj .
The other method is an ?Averaging? combination
one. With the assumption that tf in is the topic of f?
if at least one of the words in f? belongs to this topic,
we derive Pword(tf in|f?) as follows:
Pword(tf in|f?) ?
?
fj?f?
P (tf in|fj)/|f? | (13)
where |f? | denotes the number of words in phrase f? .
3.3 Adapted Lexical Probability Estimation
Now we briefly describe how to estimate the adapted
lexical weight for phrase pairs, which can be adjust-
ed in a similar way to the phrase probability.
Specifically, adopting our method, each word is
considered as one phrase consisting of only one
word, so
w(e|f) =
?
tf out
?
tf in
w(e|f, tf out)
?P (tf out|tf in) ? P (tf in|f) (14)
Here we obtain w(e|f, tf out) with a simi-
lar approach to ?(e?|f? , tf out), and calculate
P (tf out|tf in) and P (tf in|f) by resorting to
formulas (8) and (9).
With the adjusted lexical translation probability,
we resort to formula (4) to update the lexical weight
for the phrase pair (f? , e?).
4 Experiment
We evaluate our method on the Chinese-to-English
translation task for the weblog text. After a brief de-
scription of the experimental setup, we investigate
the effects of various factors on the translation sys-
tem performance.
4.1 Experimental setup
In our experiments, the out-of-domain training cor-
pus comes from the FBIS corpus and the Hansard-
s part of LDC2004T07 corpus (54.6K documents
with 1M parallel sentences, 25.2M Chinese words
and 29M English words). We use the Chinese Sohu
weblog in 20091 and the English Blog Authorship
corpus2 (Schler et al, 2006) as the in-domain mono-
lingual corpora in the source language and target
language, respectively. To obtain more accurate top-
ic information by HTMM, we firstly filter the noisy
blog documents and the ones consisting of short sen-
tences. After filtering, there are totally 85K Chinese
blog documents with 2.1M sentences and 277K En-
glish blog documents with 4.3M sentences used in
our experiments. Then, we sample equal numbers of
documents from the in-domain monolingual corpo-
ra in the source language and the target language to
respectively train two in-domain topic models. The
web part of the 2006 NIST MT evaluation test da-
ta, consisting of 27 documents with 1048 sentences,
is used as the development set, and the weblog part
of the 2008 NIST MT test data, including 33 docu-
ments with 666 sentences, is our test set.
To obtain various topic distributions for the out-
of-domain training corpus and the in-domain mono-
lingual corpora in the source language and the tar-
get language respectively, we use HTMM tool devel-
oped by Gruber et al(2007) to conduct topic model
training. During this process, we empirically set the
same parameter values for the HTMM training of d-
ifferent corpora: topics = 50, ? = 1.5, ? = 1.01,
iters = 100. See (Gruber et al, 2007) for the
meanings of these parameters. Besides, we set the
interpolation weight ? in formula (10) to 0.5 by ob-
serving the results on development set in the addi-
tional experiments.
We choose MOSES, a famous open-source
1http://blog.sohu.com/
2http://u.cs.biu.ac.il/ koppel/BlogCorpus.html
463
phrase-based machine translation system (Koehn
et al, 2007), as the experimental decoder.
GIZA++ (Och and Ney, 2003) and the heuristics
?grow-diag-final-and? are used to generate a word-
aligned corpus, from which we extract bilingual
phrases with maximum length 7. We use SRILM
Toolkits (Stolcke, 2002) to train two 4-gram lan-
guage models on the filtered English Blog Author-
ship corpus and the Xinhua portion of Gigaword
corpus, respectively. During decoding, we set the
ttable-limit as 20, the stack-size as 100, and per-
form minimum-error-rate training (Och and Ney,
2003) to tune the feature weights for the log-linear
model. The translation quality is evaluated by
case-insensitive BLEU-4 metric (Papineni et al,
2002). Finally, we conduct paired bootstrap sam-
pling (Koehn, 2004) to test the significance in BLEU
score differences.
4.2 Result and Analysis
4.2.1 Effect of Different Smoothing Methods
Our first experiments investigate the effect of dif-
ferent smoothing methods for the in-domain phrase-
topic distribution: ?Noisy-OR? and ?Averaging?.
We build adapted phrase tables with these two meth-
ods, and then respectively use them in place of the
out-of-domain phrase table to test the system perfor-
mance. For the purpose of studying the generality of
our approach, we carry out comparative experiments
on two sizes of in-domain monolingual corpora: 5K
and 40K.
Adaptation
Method
(Dev) MT06
Web
(Tst) MT08
Weblog
Baseline 30.98 20.22
Noisy-OR (5K) 31.16 20.45
Averaging (5K) 31.51 20.54
Noisy-OR (40K) 31.87 20.76
Averaging (40K) 31.89 21.11
Table 1: Experimental results using different smoothing
methods.
Table 1 reports the BLEU scores of the translation
system under various conditions. Using the out-of-
domain phrase table, the baseline system achieves
a BLEU score of 20.22. In the experiments with
the small-scale in-domain monolingual corpora, the
BLEU scores acquired by two methods are 20.45
and 20.54, achieving absolute improvements of 0.23
and 0.32 on the test set, respectively. In the exper-
iments with the large-scale monolingual in-domain
corpora, similar results are obtained, with absolute
improvements of 0.54 and 0.89 over the baseline
system.
From the above experimental results, we know
that both ?Noisy-OR? and ?Averaging? combination
methods improve the performance over the base-
line, and ?Averaging? method seems to be slight-
ly better. This finding fails to echo the promis-
ing results in the previous study (Zens and Ney,
2004). This is because the ?Noisy-OR? method in-
volves the multiplication of the word-topic distribu-
tion (shown in formula (12)), which leads to much
sharper phrase-topic distribution than ?Averaging?
method, and is more likely to introduce bias to the
translation probability estimation. Due to this rea-
son, all the following experiments only consider the
?Averaging?method.
4.2.2 Effect of Combining Two Phrase Tables
In the above experiments, we replace the out-of-
domain phrase table with the adapted phrase table.
Here we combine these two phrase tables in a log-
linear framework to see if we could obtain further
improvement. To offer a clear description, we repre-
sent the out-of-domain phrase table and the adapted
phrase table with ?OutBP? and ?AdapBP?, respec-
tively.
Used Phrase
Table
(Dev) MT06
Web
(Tst) MT08
Weblog
Baseline 30.98 20.22
AdapBp (5K) 31.51 20.54
+ OutBp 31.84 20.70
AdapBp (40K) 31.89 21.11
+ OutBp 32.05 21.20
Table 2: Experimental results using different phrase ta-
bles. OutBp: the out-of-domain phrase table. AdapBp:
the adapted phrase table.
Table 2 shows the results of experiments using d-
ifferent phrase tables. Applying our adaptation ap-
proach, both ?AdapBP? and ?OutBP + AdapBP?
consistently outperform the baseline, and the lat-
464
Figure 1: Effect of in-domain monolingual corpus size on
translation quality.
ter produces further improvements over the former.
Specifically, the BLEU scores of the ?OutBP +
AdapBP? method are 20.70 and 21.20, which ob-
tain 0.48 and 0.98 points higher than the baseline
method, and 0.16 and 0.09 points higher than the
?AdapBP? method. The underlying reason is that the
probability distribution of each in-domain sentence
often converges on some topics in the ?AdapBP?
method and some translation probabilities are over-
estimated, which leads to negative effects on the
translation quality. By using two tables together, our
approach reduces the bias introduced by ?AdapBP?,
therefore further improving the translation quality.
4.2.3 Effect of In-domain Monolingual Corpus
Size
Finally, we investigate the effect of in-domain
monolingual corpus size on translation quality. In
the experiment, we try different sizes of in-domain
documents to train different monolingual topic mod-
els: from 5K to 80K with an increment of 5K each
time. Note that here we only focus on the exper-
iments using the ?OutBP + AdapBP? method, be-
cause this method performs better in the previous
experiments.
Figure 1 shows the BLEU scores of the transla-
tion system on the test set. It can be seen that the
more data, the better translation quality when the
corpus size is less than 30K. The overall BLEU
scores corresponding to the range of great N val-
ues are generally higher than the ones correspond-
ing to the range of small N values. For example, the
BLEU scores under the condition within the range
[25K, 80K] are all higher than the ones within the
range [5K, 20K]. When N is set to 55K, the BLEU
score of our system is 21.40, with 1.18 gains on the
baseline system. This difference is statistically sig-
nificant at P < 0.01 using the significance test tool
developed by Zhang et al(2004). For this experi-
mental result, we speculate that with the increment
of in-domain monolingual data, the corresponding
topic models provide more accurate topic informa-
tion to improve the translation system. However,
this effect weakens when the monolingual corpora
continue to increase.
5 Related work
Most previous researches about translation model
adaptation focused on parallel data collection. For
example, Hildebrand et al(2005) employed infor-
mation retrieval technology to gather the bilingual
sentences, which are similar to the test set, from
available in-domain and out-of-domain training da-
ta to build an adaptive translation model. With
the same motivation, Munteanu and Marcu (2005)
extracted in-domain bilingual sentence pairs from
comparable corpora. Since large-scale monolin-
gual corpus is easier to obtain than parallel corpus,
there have been some studies on how to generate
parallel sentences with monolingual sentences. In
this respect, Ueffing et al (2008) explored semi-
supervised learning to obtain synthetic parallel sen-
tences, and Wu et al (2008) used an in-domain
translation dictionary and monolingual corpora to
adapt an out-of-domain translation model for the in-
domain text.
Differing from the above-mentioned works on
the acquirement of bilingual resource, several stud-
ies (Foster and Kuhn, 2007; Civera and Juan, 2007;
Lv et al, 2007) adopted mixture modeling frame-
work to exploit the full potential of the existing par-
allel corpus. Under this framework, the training cor-
pus is first divided into different parts, each of which
is used to train a sub translation model, then these
sub models are used together with different weights
during decoding. In addition, discriminative weight-
ing methods were proposed to assign appropriate
weights to the sentences from training corpus (Mat-
soukas et al, 2009) or the phrase pairs of phrase ta-
ble (Foster et al, 2010). Final experimental result-
s show that without using any additional resources,
these approaches all improve SMT performance sig-
465
nificantly.
Our method deals with translation model adap-
tation by making use of the topical context, so let
us take a look at the recent research developmen-
t on the application of topic models in SMT. As-
suming each bilingual sentence constitutes a mix-
ture of hidden topics and each word pair follows a
topic-specific bilingual translation model, Zhao and
Xing (2006,2007) presented a bilingual topical ad-
mixture formalism to improve word alignment by
capturing topic sharing at different levels of linguis-
tic granularity. Tam et al(2007) proposed a bilin-
gual LSA, which enforces one-to-one topic corre-
spondence and enables latent topic distributions to
be efficiently transferred across languages, to cross-
lingual language modeling and translation lexicon
adaptation. Recently, Gong and Zhou (2010) also
applied topic modeling into domain adaptation in
SMT. Their method employed one additional feature
function to capture the topic inherent in the source
phrase and help the decoder dynamically choose re-
lated target phrases according to the specific topic of
the source phrase.
Besides, our approach is also related to context-
dependent translation. Recent studies have shown
that SMT systems can benefit from the utiliza-
tion of context information. For example, trigger-
based lexicon model (Hasan et al, 2008; Mauser et
al., 2009) and context-dependent translation selec-
tion (Chan et al, 2007; Carpuat and Wu, 2007; He
et al, 2008; Liu et al, 2008). The former gener-
ated triplets to capture long-distance dependencies
that go beyond the local context of phrases, and the
latter built the classifiers which combine rich con-
text information to better select translation during
decoding. With the consideration of various local
context features, these approaches all yielded stable
improvements on different translation tasks.
As compared to the above-mentioned works, our
work has the following differences.
? We focus on how to adapt a translation mod-
el for domain-specific translation task with the
help of additional in-domain monolingual cor-
pora, which are far from full exploitation in the
parallel data collection and mixture modeling
framework.
? In addition to the utilization of in-domain
monolingual corpora, our method is differen-
t from the previous works (Zhao and Xing,
2006; Zhao and Xing, 2007; Tam et al, 2007;
Gong and Zhou, 2010) in the following aspect-
s: (1) we use a different topic model ? HTMM
which has different assumption from PLSA and
LDA; (2) rather than modeling topic-dependent
translation lexicons in the training process, we
estimate topic-specific lexical probability by
taking account of topical context when extract-
ing word pairs, so our method can also be di-
rectly applied to topic-dependent phrase proba-
bility modeling. (3) Instead of rescoring phrase
pairs online, our approach calculate the transla-
tion probabilities offline, which brings no addi-
tional burden to translation systems and is suit-
able to translate the texts without the topic dis-
tribution information.
? Different from trigger-based lexicon model and
context-dependent translation selection both of
which put emphasis on solving the translation
ambiguity by the exploitation of the context in-
formation at the sentence level, we adopt the
topical context information in our method for
the following reasons: (1) the topic informa-
tion captures the context information beyond
the scope of sentence; (2) the topical context in-
formation is integrated into the posterior prob-
ability distribution, avoiding the sparseness of
word or POS features; (3) the topical context
information allows for more fine-grained dis-
tinction of different translations than the genre
information of corpus.
6 Conclusion and future work
This paper presents a novel method for SMT sys-
tem adaptation by making use of the monolingual
corpora in new domains. Our approach first esti-
mates the translation probabilities from the out-of-
domain bilingual corpus given the topic information,
and then rescores the phrase pairs via topic mapping
and phrase-topic distribution probability estimation
from in-domain monolingual corpora. Experimental
results show that our method achieves better perfor-
mance than the baseline system, without increasing
the burden of the translation system.
In the future, we will verify our method on oth-
466
er language pairs, for example, Chinese to Japanese.
Furthermore, since the in-domain phrase-topic dis-
tribution is currently estimated with simple smooth-
ing interpolations, we expect that the translation sys-
tem could benefit from other sophisticated smooth-
ing methods. Finally, the reasonable estimation of
topic number for better translation model adaptation
will also become our study emphasis.
Acknowledgement
The authors were supported by 863 State Key
Project (Grant No. 2011AA01A207), National
Natural Science Foundation of China (Grant Nos.
61005052 and 61103101), Key Technologies R&D
Program of China (Grant No. 2012BAH14F03). We
thank the anonymous reviewers for their insightful
comments. We are also grateful to Ruiyu Fang and
Jinming Hu for their kind help in data processing.
References
Michiel Bacchiani and Brian Roark. 2003. Unsuper-
vised Language Model Adaptation. In Proc. of ICAS-
SP 2003, pages 224-227.
Michiel Bacchiani and Brian Roark. 2005. Improving
Machine Translation Performance by Exploiting Non-
Parallel Corpora. Computational Linguistics, pages
477-504.
Nicola Bertoldi and Marcello Federico. 2009. Domain
Adaptation for Statistical Machine Translation with
Monolingual Resources. In Proc. of ACL Workshop
2009, pages 182-189.
David M. Blei. 2003. Latent Dirichlet Allocation. Jour-
nal of Machine Learning, pages 993-1022.
Ivan Bulyko, Spyros Matsoukas, Richard Schwartz, Long
Nguyen and John Makhoul. 2007. Language Model
Adaptation in Machine Translation from Speech. In
Proc. of ICASSP 2007, pages 117-120.
Marine Carpuat and Dekai Wu. 2007. Improving Statis-
tical Machine Translation Using Word Sense Disam-
biguation. In Proc. of EMNLP 2007, pages 61-72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2006.
Word sense disambiguation improves statistical ma-
chine translation. In Proc. of ACL 2007, pages 33-40.
Boxing Chen, George Foster and Roland Kuhn. 2010.
Bilingual Sense Similarity for Statistical Machine
Translation. In Proc. of ACL 2010, pages 834-843.
David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation. Computational Linguistics, pages 201-228.
David Chiang. 2010. Learning to Translate with Source
and Target Syntax. In Proc. of ACL 2010, pages 1443-
1452.
Jorge Civera and Alfons Juan. 2007. Domain Adaptation
in Statistical Machine Translation with Mixture Mod-
elling. In Proc. of the Second Workshop on Statistical
Machine Translation, pages 177-180.
Matthias Eck, Stephan Vogel and Alex Waibel. 2004.
Language Model Adaptation for Statistical Machine
Translation Based on Information Retrieval. In Proc.
of Fourth International Conference on Language Re-
sources and Evaluation, pages 327-330.
Matthias Eck, Stephan Vogel and Alex Waibel. 2005.
Low Cost Portability for Statistical Machine Transla-
tion Based on N-gram Coverage. In Proc. of MT Sum-
mit 2005, pages 227-234.
George Foster and Roland Kuhn. 2007. Mixture Model
Adaptation for SMT. In Proc. of the Second Workshop
on Statistical Machine Translation, pages 128-135.
George Foster, Cyril Goutte and Roland Kuhn. 2010.
Discriminative Instance Weighting for Domain Adap-
tation in Statistical Machine Translation. In Proc. of
EMNLP 2010, pages 451-459.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang and Ignacio Thay-
er. 2006. Scalable Inference and Training of Context-
Rich Syntactic Translation Models. In Proc. of ACL
2006, pages 961-968.
Zhengxian Gong and Guodong Zhou. 2010. Improve
SMT with Source-side Topic-Document Distributions.
In Proc. of MT SUMMIT 2010, pages 24-28.
Amit Gruber, Michal Rosen-Zvi and Yair Weiss. 2007.
Hidden Topic Markov Models. In Journal of Machine
Learning Research, pages 163-170.
Sas?a Hasan, Juri Ganitkevitch, Hermann Ney and Jesu?s
Andre?s-Ferrer 2008. Triplet Lexicon Models for S-
tatistical Machine Translation. In Proc. of EMNLP
2008, pages 372-381.
Zhongjun He, Qun Liu and Shouxun Lin. 2008. Improv-
ing Statistical Machine Translation using Lexicalized
Rule Selection. In Proc. of COLING 2008, pages 321-
328.
Almut Silja Hildebrand. 2005. Adaptation of the Trans-
lation Model for Statistical Machine Translation based
on Information Retrieval. In Proc. of EAMT 2005,
pages 133-142.
Thomas Hofmann. 1999. Probabilistic Latent Semantic
Indexing. In Proc. of SIGIR 1999, pages 50-57.
Franz Joseph Och and Hermann Ney. 2003. A Systemat-
ic Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics, pages 19-51.
Franz Joseph Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine Trans-
lation. Computational Linguistics, pages 417-449.
467
Philipp Koehn, Franz Josef Och and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of HLT-
NAACL 2003, pages 127-133.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proc. of EMNLP
2004, pages 388-395.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL 2007, Demonstration Session, pages 177-180.
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-
to-String Alignment Template for Statistical Machine
Translation. In Proc. of ACL 2006, pages 609-616.
Yajuan Lv, Jin Huang and Qun Liu. 2007. Improv-
ing Statistical Machine Translation Performance by
Training Data Selection and Optimization. In Proc.
of EMNLP 2007, pages 343-350.
Arne Mauser, Richard Zens and Evgeny Matusov, Sas?a
Hasan and Hermann Ney. 2006. The RWTH Statisti-
cal Machine Translation System for the IWSLT 2006
Evaluation. In Proc. of International Workshop on
Spoken Language Translation, pages 103-110.
Arne Mauser, Sas?a Hasan and Hermann Ney 2009. Ex-
tending Statistical Machine Translation with Discrimi-
native and Trigger-Based Lexicon Models. In Proc. of
ACL 2009, pages 210-218.
Spyros Matsoukas, Antti-Veikko I. Rosti and Bing Zhang
2009. Discriminative Corpus Weight Estimation for
Machine Translation. In Proc. of EMNLP 2009, pages
708-717.
Nick Ruiz and Marcello Federico. 2011. Topic Adapta-
tion for Lecture Translation through Bilingual Latent
Semantic Models. In Proc. of ACL Workshop 2011,
pages 294-302.
Kishore Papineni, Salim Roukos, Todd Ward and WeiJing
Zhu. 2002. BLEU: A Method for Automatic Evalu-
ation of Machine Translation. In Proc. of ACL 2002,
pages 311-318.
Jonathan Schler, Moshe Koppel, Shlomo Argamon and
James Pennebaker. 2006. Effects of Age and Gender
on Blogging. In Proc. of 2006 AAAI Spring Sympo-
sium on Computational Approaches for Analyzing We-
blogs.
Holger Schwenk and Jean Senellart. 2009. Translation
Model Adaptation for an Arabic/french News Transla-
tion System by Lightly-supervised Training. In Proc.
of MT Summit XII.
Andreas Stolcke. 2002. Srilm - An Extensible Language
Modeling Toolkit. In Proc. of ICSLP 2002, pages 901-
904.
Yik-Cheung Tam, Ian R. Lane and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for statistical machine
translation. Machine Translation, pages 187-207.
Nicola Ueffing, Gholamreza Haffari and Anoop Sarkar.
2008. Semi-supervised Model Adaptation for Statisti-
cal Machine Translation. Machine Translation, pages
77-94.
Hua Wu, Haifeng Wang and Chengqing Zong. 2008. Do-
main Adaptation for Statistical Machine Translation
with Domain Dictionary and Monolingual Corpora. In
Proc. of COLING 2008, pages 993-1000.
Richard Zens and Hermann Ney. 2004. Improvments in
phrase-based statistical machine translation. In Proc.
of NAACL 2004, pages 257-264.
Ying Zhang, Almut Silja Hildebrand and Stephan Vogel.
2006. Distributed Language Modeling for N-best List
Re-ranking. In Proc. of EMNLP 2006, pages 216-223.
Bing Zhao, Matthias Eck and Stephan Vogel. 2004.
Language Model Adaptation for Statistical Machine
Translation with Structured Query Models. In Proc.
of COLING 2004, pages 411-417.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
Topic AdMixture Models for Word Alignment. In
Proc. of ACL/COLING 2006, pages 969-976.
Bing Zhao and Eric P. Xing. 2007. HM-BiTAM: Bilin-
gual Topic Exploration, Word Alignment, and Trans-
lation. In Proc. of NIPS 2007, pages 1-8.
Qun Liu, Zhongjun He, Yang Liu and Shouxun Lin.
2008. Maximum Entropy based Rule Selection Model
for Syntax-based Statistical Machine Translation. In
Proc. of EMNLP 2008, pages 89-97.
468
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 979?987,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Improve SMT Quality with Automatically Extracted Paraphrase Rules 
 
 
Wei He1, Hua Wu2, Haifeng Wang2, Ting Liu1* 
1Research Center for Social Computing and Information 
Retrieval, Harbin Institute of Technology 
{whe,tliu}@ir.hit.edu.cn 
2Baidu 
{wu_hua,wanghaifeng}@baidu.com 
 
 
 
Abstract1 
We propose a novel approach to improve 
SMT via paraphrase rules which are 
automatically extracted from the bilingual 
training data. Without using extra 
paraphrase resources, we acquire the rules 
by comparing the source side of the parallel 
corpus with the target-to-source 
translations of the target side. Besides the 
word and phrase paraphrases, the acquired 
paraphrase rules mainly cover the 
structured paraphrases on the sentence 
level. These rules are employed to enrich 
the SMT inputs for translation quality 
improvement. The experimental results 
show that our proposed approach achieves 
significant improvements of 1.6~3.6 points 
of BLEU in the oral domain and 0.5~1 
points in the news domain. 
1 Introduction 
The translation quality of the SMT system is 
highly related to the coverage of translation models. 
However, no matter how much data is used for 
training, it is still impossible to completely cover 
the unlimited input sentences. This problem is 
more serious for online SMT systems in real-world 
applications. Naturally, a solution to the coverage 
problem is to bridge the gaps between the input 
sentences and the translation models, either from 
the input side, which targets on rewriting the input 
sentences to the MT-favored expressions, or from 
                                                          
This work was done when the first author was visiting Baidu. 
*Correspondence author: tliu@ir.hit.edu.cn 
the side of translation models, which tries to enrich 
the translation models to cover more expressions.  
In recent years, paraphrasing has been proven 
useful for improving SMT quality. The proposed 
methods can be classified into two categories 
according to the paraphrase targets: (1) enrich 
translation models to cover more bilingual 
expressions; (2) paraphrase the input sentences to 
reduce OOVs or generate multiple inputs. In the 
first category, He et al (2011), Bond et al (2008) 
and Nakov (2008) enriched the SMT models via 
paraphrasing the training corpora. Kuhn et al 
(2010) and Max (2010) used paraphrases to 
smooth translation models. For the second 
category, previous studies mainly focus on finding 
translations for unknown terms using phrasal 
paraphrases. Callison-Burch et al (2006) and 
Marton et al (2009) paraphrase unknown terms in 
the input sentences using phrasal paraphrases 
extracted from bilingual and monolingual corpora. 
Mirkin et al (2009) rewrite OOVs with 
entailments and paraphrases acquired from 
WordNet. Onishi et al (2010) and Du et al (2010) 
use phrasal paraphrases to build a word lattice to 
get multiple input candidates. In the above 
methods, only word or phrasal paraphrases are 
used for input sentence rewriting. No structured 
paraphrases on the sentence level have been 
investigated. However, the information in the 
sentence level is very important for disambiguation.  
For example, we can only substitute play with 
drama in a context related to stage or theatre. 
Phrasal paraphrase substitutions can hardly solve 
such kind of problems.  
In this paper, we propose a method that rewrites 
979
the input sentences of the SMT system using 
automatically extracted paraphrase rules which can 
capture structures on sentence level in addition to 
paraphrases on the word or phrase level. Without 
extra paraphrase resources, a novel approach is 
proposed to acquire paraphrase rules from the 
bilingual training corpus based on the results of 
Forward-Translation and Back-Translation. The 
rules target on rewriting the input sentences to an 
MT-favored expression to ensure a better 
translation. The paraphrase rules cover all kinds of 
paraphrases on the word, phrase and sentence 
levels, enabling structure reordering, word or 
phrase insertion, deletion and substitution. The 
experimental results show that our proposed 
approach achieves significant improvements of 
1.6~3.6 points of BLEU in the oral domain and 
0.5~1 points in the news domain. 
The remainder of the paper is organized as 
follows: Section 2 makes a comparison between 
the Forward-Translation and Back-Translation. 
Section 3 introduces our methods that extract 
paraphrase rules from the bilingual corpus of SMT. 
Section 4 describes the strategies for constructing 
word lattice with paraphrase rules. The 
experimental results and some discussions are 
presented in Section 5 and Section 6. Section 7 
compares our work to the previous researches. 
Finally, Section 8 concludes the paper and suggests 
directions for future work. 
2 Forward-Translation vs. Back-
Translation 
The Back-Translation method is mainly used for 
automatic MT evaluation (Rapp 2009). This 
approach is very helpful when no target language 
reference is available. The only requirement is that 
the MT system needs to be bidirectional. The 
procedure includes translating a text into certain 
foreign language with the MT system (Forward-
Translation), and translating it back into the 
original language with the same system (Back-
Translation). Finally the translation quality of 
Back-Translation is evaluated by using the original 
source texts as references. 
Sun et al (2010) reported an interesting 
phenomenon: given a bilingual text, the Back-
Translation results of the target sentences is better 
than the Forward-Translation results of the source 
sentences. Clearly, let (S0, T0) be the initial pair of 
bilingual text. A source-to-target translation system 
SYS_ST and a target-to-source translation system 
SYS_TS are trained using the bilingual corpus. 
?????  is a Forward-Translation function, and 
????? is a function of Back-Translation which can 
be deduced with two rounds of translations: 
????? ? ???_??????_??????. In the first round 
of translation, S0 and T0 are fed into SYS_ST and 
SYS_TS, and we get T1 and S1 as translation results. 
In the second round, we translate S1 back into the 
target side with SYS_ST, and get the translation T2. 
The procedure is illustrated in Figure 1, which can 
also formally be described as: 
1. T1 = FT(S0) = SYS_ST(S0). 
2. T2 = BT(T0), which can be decomposed into 
two steps: S1 = SYS_TS(T0), T2 = SYS_ST(S1). 
Using T0 as reference, an interesting result is 
reported in Sun et al (2010) that T2 achieves a 
higher score than T1 in automatic MT evaluation. 
This outcome is important because T2 is translated 
Figure 1: Procedure of Forward-Translation and Back-Translation. 
S0 T0 
S1 T1 
T2 
Source Language Target Language 
Initial Parallel Text 
1st Round Translation 
2nd Round Translation 
Forward- 
Translation
Back- 
Translation 
980
from a machine-generated text S1, but T1 is 
translated from a human-write text S0. Why the 
machine-generated text results in a better 
translation than the human-write text? Two 
possible reasons may explain this phenomenon: (1) 
in the first round of translation T0 ? S1, some 
target word orders are reserved due to the 
reordering failure, and these reserved orders lead to 
a better result in the second round of translation; (2) 
the text generated by an MT system is more likely 
to be matched by the reversed but homologous MT 
system.  
Note that all the texts of S0, S1, S2, T0 and T1 are 
sentence aligned because the initial parallel corpus 
(S0, T0) is aligned in the sentence level. The aligned 
sentence pairs in (S0, S1) can be considered as 
paraphrases. Since S1 has some MT-favored 
structures which may result in a better translation, 
an intuitive idea is whether we can learn these 
structures by comparing S1 with S0. This is the 
main assumption of this paper. Taking (S0, S1) as 
paraphrase resource, we propose a method that 
automatically extracts paraphrase rules to capture 
the MT-favored structures. 
3 Extraction of Paraphrase Rules 
3.1 Definition of Paraphrase Rules 
We define a paraphrase rule as follows: 
1. A paraphrase rule consists of two parts, left-
hand-side (LHS) and right-hand-side (RHS). 
Both of LHS and RHS consist of non-
terminals (slot) and terminals (words). 
2. LHS must start/end with a terminal. 
3. There must be at least one terminal between 
two non-terminals in LHS. 
A paraphrase rule in the format of:  
LHS ? RHS 
which means the words matched by LHS can be 
paraphrased to RHS. Taking Chinese as a case 
study, some examples of paraphrase rules are 
shown in Table 1. 
3.2  Selecting Paraphrase Sentence Pairs 
Following the methods in Section 2, the initial 
bilingual corpus is (S0, T0). We train a source-to-
target PBMT system (SYS_ST) and a target-to-
source PBMT system (SYS_TS) on the parallel 
corpus. Then a Forward-Translation is performed 
on S0 using SYS_ST, and a Back-Translation is 
performed on T0 using SYS_TS and SYS_ST. As 
mentioned above, the detailed procedure is: T1 = 
SYS_ST(S0), S1 = SYS_TS(T0), T2 = SYS_ST(S1). 
Finally we compute BLEU (Papineni et al 2002) 
score for every sentence in T2 and T1, using the 
corresponding sentence in T0 as reference. If the 
sentence in T2 has a higher BLEU score than the 
aligned sentence in T1, the corresponding sentences 
in S0 and S1 are selected as candidate paraphrase 
sentence pairs, which are used in the following 
steps of paraphrase extractions. 
3.3 Word Alignments Filtering 
We can construct word alignment between S0 and 
S1 through T0. On the initial corpus of (S0, T0), we 
conduct word alignment with Giza++ (Och and 
Ney, 2000) in both directions and then apply the 
grow-diag-final heuristic (Koehn et al, 2005) for 
symmetrization. Because S1 is generated by 
feeding T0 into the PBMT system SYS_TS, the 
word alignment between T0 and S1 can be acquired 
from the verbose information of the decoder. 
The word alignments of S0 and S1 contain noises 
which are produced by either wrong alignment of 
GIZA++ or translation errors of SYS_TS. To ensure 
the alignment quality, we use some heuristics to 
filter the alignment between S0 and S1: 
1. If two identical words are aligned in S0 and 
S1, then remove all the other links to the two 
words. 
No. LHS RHS 
1 ??/ride   X1   ????/bus ??/ride    X1   ??/bus 
2    ?/at   X1  ?/location   ???/turn left  ???/turn left   ?/at   X1  ?/location 
3 ?/NULL   X1    ?/give    ?/me ?/give    ?/me    X1 
4 ?/from  X1  ?/to  X2  ?/need ??/how long??/time 
?/need   ?/spend  ??/how long  ??/time 
?/from X1?/to X2 
Table 1: Examples of Chinese Paraphrase rules, together with English translations for every word 
981
2. Stop words (including some function words 
and punctuations) can only be aligned to 
either stop words or null. 
Figure 2 illustrates an example of using the 
heuristics to filter alignment. 
3.4 Extracting Paraphrase Rules 
From the word-aligned sentence pairs, we then 
extract a set of rules that are consistent with the 
word alignments. We use the rule extracting 
methods of Chiang (2005). Take the sentence pair 
in Figure 2 as an example, two initial phrase pairs 
PP1 = ?? ? ?? ??? ||| ? ? ?? ????  
and  PP2 = ?? ? ? ?? ??? ? ?? ||| ? 
? ?? ? ? ?? ???? are identified, and 
PP1 is contained by PP2, then we could form the 
rule: 
? X1 ? ?? ? ? ? ?? X1
to  have interest  very feel interest  
4 Paraphrasing the Input Sentences 
The extracted paraphrase rules aim to rewrite the 
input sentences to an MT-favored form which may 
lead to a better translation. However, it is risky to 
directly replace the input sentence with a 
paraphrased sentence, since the errors in automatic 
paraphrase substitution may jeopardize the 
translation result seriously. To avoid such damage, 
for a given input sentence, we first transform all 
paraphrase rules that match the input sentences to 
phrasal paraphrases, and then build a word lattice 
for SMT decoder using the phrasal paraphrases. In 
this case, the decoder can search for the best result 
among all the possible paths. 
The input sentences are first segmented into sub-
sentences by punctuations. Then for each sub-
sentence, the matched paraphrase rules are ranked 
according to: (1) the number of matched words; (2) 
the frequency of the paraphrase rule in the training 
data. Actually, the ranking strategy tends to select 
paraphrase rules that have more matched words 
(therefore less ambiguity) and higher frequency 
(therefore more reliable). 
4.1 Applying Paraphrase Rules 
Given an input sentence S and a paraphrase rule R 
<RLHS, RRHS>, if S matches RLHS, then the matched 
part can be replaced by RRHS. An example for 
applying the paraphrase rules is illustrated in 
Figure 3.  
From Figure 3, we can see that the words of 
position 1~3 are replaced to ??? 10 ? ???. 
Actually, only the words at position 3 and 4 are 
paraphrased to the word ????, other words are 
left unchanged. Therefore, we can use a triple, 
<MIN_RP_TEXT, COVER_START, COVER_LEN> 
(<?? , 3, 1> in this example) to denote the 
paraphrase rule, which means the minimal text to 
replace is ????, and the paraphrasing starts at 
position 3 and covers 1 words. 
In this manner, all the paraphrase rules matched 
for a certain sentence can be converted to the 
format of <MIN_RP_TEXT, COVER_START, 
COVER_LEN>, which can also be considered as 
phrasal paraphrases. Then the methods of building 
phrasal paraphrases into word lattice for SMT 
inputs can be used in our approaches. 
??    ??     [10?]   ????
??     [10?]      ?? 
Rule 
LHS:??/ride  X1 ????/bus 
RHS:??/ride  X1  ??/bus 
Figure 3: Example for Applying Paraphrase Rules 
0         1            2                3
welcome  ride     No.10         bus
ride       No.10        bus 
I  very feel interest that N/A  blue   handbag  
I     to   that   N/A  blue  handbag have interest    
?   ?   ?    ??   ?    ?  ??   ???     ? 
?   ?    ?     ?    ??   ???  ?  ??     ? 
Figure 2: Example for Word Alignment 
Filtration 
I     to   that   N/A  blue  handbag have interest    
?   ?    ?     ?    ??   ???  ?  ??     ? 
I  very feel interest that N/A  blue   handbag  
?   ?   ?    ??   ?    ?  ??   ???      ? 
982
4.2 Construction of Paraphrase Lattice 
Given an input sentence, all the matched 
paraphrase rules are converted to phrasal 
paraphrases first. Then we build the phrasal 
paraphrases into word lattices using the methods 
proposed by Du et al (2010). The construction 
process takes advantage of the correspondence 
between detected phrasal paraphrases and positions 
of the original words in the input sentence, and 
then creates extra edges in the lattices to allow the 
decoder to consider paths involving the paraphrase 
words. An example is illustrated in Figure 4: given 
a sequence of words {w1,?,wN} as the input, two 
phrases ? ={?1,??p} and ? = {?1,?, ?q} are 
detected as paraphrases for P1 = {wx,?, wy} (1 ? x 
? y ? N) and P2 = {wm,?,wn} (1 ? m ? n ? N) 
respectively. The following steps are taken to 
transform them into word lattices: 
1. Transform the original source sentence into 
word lattice. N + 1 nodes (?k, 0 ? k ? N) are 
created, and N edges labeled with wi (1 ? i ? 
N) are generated to connect them 
sequentially. 
2. Generate extra nodes and edges for each of 
the paraphrases. Taking ? as an example, 
firstly, p ? 1 nodes are created, and then p 
edges labeled with ?j (1 ? j ? p) are 
generated to connect node ?x-1, p-1 nodes 
and ?y-1. 
Via step 2, word lattices are generated by adding 
new nodes and edges coming from paraphrases. 
4.3  Weight Estimation 
The weights of new edges in the lattices are 
estimated by an empirical method base on ranking 
positions. Following Du et al (2010), supposing 
that E = {e1,?,ek} are a set of new edges 
constructed from k paraphrase rules, which are 
sorted in a descending order. Then the weight for 
an edge ei is calculated as: 
??e?? ? 1? ? ? ???1 ? ? ? ?? where k is a predefined tradeoff parameter between 
decoding speed and the number of potential 
paraphrases being considered. 
5  Experiments 
5.1  Experimental Data 
In our experiments, we used Moses (Koehn et al, 
2007) as the baseline system which can support 
lattice decoding. The alignment was obtained using 
GIZA++ (Och and Ney, 2003) and then we 
symmetrized the word alignment using the grow-
diag-final heuristic. Parameters were tuned using 
Minimum Error Rate Training (Och, 2003). To 
comprehensively evaluate the proposed methods in 
different domains, two groups of experiments were 
carried out, namely, the oral group (Goral) and the 
news group (Gnews). The experiments were 
conducted in both Chinese-English and English-
Chinese directions for the oral group, and Chinese-
English direction for the news group. The English 
sentences were all tokenized and lowercased, and 
the Chinese sentences were segmented into words 
by Language Technology Platform (LTP) 1 . We 
used SRILM2 for the training of language models 
(5-gram in all the experiments). The metrics for 
automatic evaluation were BLEU 3  and TER 4 
(Snover et al, 2005). 
The detailed statistics of the training data in Goral 
are showed in Table 2. For the bilingual corpus, we 
used the BTEC and PIVOT data of IWSLT 2008, 
HIT corpus 5  and other Chinese LDC (CLDC) 
                                                          
1 http://ir.hit.edu.cn/ltp/ 
2 http://www.speech.sri.com/projects/srilm/ 
3 ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a.pl 
4 http://www.umiacs.umd.edu/~snover/terp/ 
5 The HIT corpus contains the CLDC Olympic corpus (2004-
863-008) and the other HIT corpora available at 
http://mitlab.hit.edu.cn/index.php/resources/29-the-
resource/111-share-bilingual-corpus.html. 
Figure 4: An example of lattice-based 
paraphrases for an input sentence. 
983
corpora, including the Chinese-English Sentence 
Aligned Bilingual Corpus (CLDC-LAC-2003-004) 
and the Chinese-English Parallel Corpora (CLDC-
LAC-2003-006). We trained a Chinese language 
model for the E-C translation on the Chinese part 
of the bi-text. For the English language model of 
C-E translation, an extra corpus named Tanaka was 
used besides the English part of the bilingual 
corpora. For testing and developing, we used six 
Chinese-English development corpora of IWSLT 
2008. The statistics are shown in Table 3.  
In detail, we chose CSTAR03-test and 
IWSLT06-dev as the development set; and used 
IWSLT04-test, IWSLT05-test, IWSLT06-dev and 
IWSLT07-test for testing. For English-Chinese 
evaluation, we used IWSLT English-Chinese MT 
evaluation 2005 as the test set. Due to the lacking 
of development set, we did not tune parameters on 
English-Chinese side, instead, we just used the 
default parameters of Moses. 
In the experiments of the news group, we used 
the Sinorama and FBIS corpora (LDC2005T10 and 
LDC2003E14) for bilingual corpus. After 
tokenization and filtering, this bilingual corpus 
contained 319,694 sentence pairs (7.9M tokens on 
Chinese side and 9.2M tokens on English side). 
We trained a 5-gram language model on the 
English side of the bi-text. The system was tested 
using the Chinese-English MT evaluation sets of 
NIST 2004, NIST 2006 and NIST 2008. For 
development, we used the Chinese-English MT 
evaluation sets of NIST 2002 and NIST 2005. 
Table 4 shows the statistics of test/development 
sets used in the news group. 
5.2 Results 
We extract both Chinese and English rules in Goral, 
and Chinese paraphrase rules in Gnews by 
comparing the results of Forward-Translation and 
Back-Translation as described in Section 3. During 
the extraction, some heuristics are used to ensure 
the quality of paraphrase rules. Take the extraction 
of Chinese paraphrase rules in Goral as a case study. 
Suppose (C0, E0) are the initial bilingual corpus of 
Goral. A Chinese-English and an English-Chinese 
MT system are trained on (C0, E0). We perform 
Back-Translation on E0 (?? ???????? ?? ?? ???????? ?? ??), and 
Forward-Translation on C0 (?? ???????? ?? ??). Suppose e1i and e2i are two aligned sentences in E1 and E2, 
c0i and c1i are the corresponding sentences in C0 
and C1. (c0i, c1i) are selected for the extraction of 
paraphrase rules if two conditions are satisfied: (1) 
BLEU(e2i) ? BLEU(e1i) > ?1, and (2) BLEU(e2i) > 
?2, where BLEU???  is a function for computing 
BLEU score; ?1 and ?2 are thresholds for balancing 
the rules number and the quality of paraphrase 
rules. In our experiment, ?1 and ?2 are empirically 
set to 0.1 and 0.3. 
As a result, we extract 912,625 Chinese and 
1,116,375 English paraphrase rules for Goral, and 
for Gnews the number of Chinese paraphrase rules is 
2,877,960. Then we use the extracted paraphrase 
rules to improve SMT by building word lattices for 
the input sentences. 
The Chinese-English experimental results of 
Goral and Gnews are shown in Table 5 and Table 6, 
respectively. It can be seen that our method 
outperforms the baselines in both oral and news 
domains. Our system gains significant 
improvements of 1.6~3.6 points of BLEU in the 
oral domain, and 0.5~1 points of BLEU in the 
news domain. Figure 5 shows the effect of 
considered paraphrases (k) in the step of building  
Corpus #Sen. pairs #Ch. words #En words
BETC 19,972 174k 190k 
PIVOT 20,000 162k 196k 
HIT 80,868 788k 850k 
CLDC 190,447 1,167k 1,898k 
Tanaka 149,207 - 1,375k 
Table 2: Statistics of training data in Goral 
 Corpus #Sen.  #Ref.  
develop CSTAR03 test set 506 16 IWSLT06 dev set 489 7 
test 
IWSLT04 test set 500 16 
IWSLT05 test set 506 16 
IWSLT06 test set 500 7 
IWSLT07 test set 489 6 
Table 3: Statistics of test/develop sets in Goral 
 Corpus #Sen.  #Ref.  
develop NIST 2002 878 10 NIST 2005 1,082 4 
test 
NIST 2004 1,788 5 
NIST 2006 1,664 4 
NIST 2008 1,357 4 
Table 4: Statistics of test/develop sets in Gnews 
984
word lattices. The result of English-Chinese 
experiments in Goral is shown in Table 7.  
6 Discussion 
We make a detailed analysis on the Chinese-
English translation results that are affected by our 
paraphrase rules. The aim is to investigate what 
kinds of paraphrases have been captured in the 
rules. Firstly the input path is recovered from the 
translation results according to the tracing 
information of the decoder, and therefore we can 
examine which path is selected by the SMT 
decoder from the paraphrase lattice. A human 
annotator is asked to judge whether the recovered 
paraphrase sentence keeps the same meaning as the 
original input. Then the annotator compares the 
baseline translation with the translations proposed 
by our approach. The analysis is carried out on the 
IWSLT 2007 Chinese-English test set, 84 out of 
489 input sentences have been affected by 
paraphrases, and the statistic of human evaluation 
is shown in Table 8.  
It can be seen in Table 8 that the paraphrases 
achieve a relatively high accuracy, 60 (71.4%) 
paraphrased sentences retain the same meaning, 
and the other 24 (28.6%) are incorrect. Among the 
60 correct paraphrases, 36 sentences finally result 
in an improved translation. We further analyze 
these paraphrases and the translation results to 
investigate what kinds of transformation finally 
lead to the translation quality improvement. The 
paraphrase variations can be classified into four 
categories: 
(1) Reordering: The original source sentences 
are reordered to be similar to the order of 
the target language. 
(2) Word substitution: A phrase with multi-
word translations is replaced by a phrase 
with a single-word translation.  
(3) Recovering omitted words: Ellipsis occurs 
frequently in spoken language. Recovering 
the omitted words often leads to a better 
translation. 
(4) Removing redundant words: Mostly, 
translating redundant words may confuse 
the SMT system and would be unnecessary. 
Removing redundant words can mitigate 
this problem. 
44.2?
44.4?
44.6?
44.8?
45.0?
45.2?
45.4?
0 10 20 30 40
BLE
U?s
cor
e?(
%)
Considered?paraprhases?(k)
Figure 5: Effect of considered paraphrases (k) 
on BLEU score
Model BLEU TER iwslt 04 iwslt 05 iwslt 06 iwslt 07 iwslt 04 iwslt 05 iwslt 06 iwslt 07
baseline 0.5353 0.5887 0.2765 0.3977 0.3279 0.2874 0.5559 0.4390 
para. improved 0.5712 0.6107 0.2924 0.4193 0.3055 0.2722 0.5374 0.4217 
Model BLEU TER nist 04 nist 06 nist 08 nist 04 nist 06 nist 08 
baseline 0.2795 0.2389 0.1933 0.6554 0.6515 0.6652 
para. improved 0.2891 0.2485 0.1978 0.6451 0.6407 0.6582 
 model IWSLT 2005  BLEU TER 
 baseline 0.4644 0.4164 
 para. improved  0.4853 0.3883 
trans. 
para. improve comparable worsen total
correct 36 20 4 60 
incorrect 1 9 14 24 
Table 8: Human analysis of the paraphrasing 
results in IWSLT 2007 CE translation 
Table 5: Experimental results of Goral in Chinese-English direction 
Table 6: Experimental results of Gnews in Chinese-English direction 
Table 7: Experimental results of Goral in 
English-Chinese direction 
985
Four examples for category (1), (2), (3) and (4) 
are shown in Table 9, respectively. The numbers in 
the second column indicates the number of the 
sentences affected by the rules, among the 36 
sentences with improved paraphrasing and 
translation. A sentence can be classified into 
multiple categories. Except category (2), the other 
three categories cannot be detected by the previous 
approaches, which verify our statement that our 
rules can capture structured paraphrases on the 
sentence level in addition to the paraphrases on the 
word or phrase level. 
Not all the paraphrased results are correct. 
Sometimes an ill paraphrased sentence can produce 
better translations. Take the first line of Table 9 as 
an example, the paraphrased sentence ???/How 
many ??/cigarettes ??/can ??/duty-free ?
/take ?/NULL? is not a fluent Chinese sentence, 
however, the rearranged word order is closer to 
English, which finally results in a much better 
translation. 
7 Related Work 
Previous studies on improving SMT using 
paraphrase rules focus on hand-crafted rules. 
Nakov (2008) employs six rules for paraphrasing 
the training corpus. Bond et al (2008) use 
grammars to paraphrase the source side of training 
data, covering aspects like word order and minor 
lexical variations (tenses etc.) but not content 
words. The paraphrases are added to the source 
side of the corpus and the corresponding target 
sentences are duplicated. 
A disadvantage for hand-crafted paraphrase 
rules is that it is language dependent. In contrast, 
our method that automatically extracted paraphrase 
rules from bilingual corpus is flexible and suitable 
for any language pairs. 
Our work is similar to Sun et al (2010). Both 
tried to capture the MT-favored structures from 
bilingual corpus. However, a clear difference is 
that Sun et al (2010) captures the structures 
implicitly by training an MT system on (S0, S1) and 
?translates? the SMT input to an MT-favored 
expression. Actually, the rewriting process is 
considered as a black box in Sun et al (2010). In 
this paper, the MT-favored expressions are 
captured explicitly by automatically extracted 
paraphrase rules. The advantages of the paraphrase 
rules are: (1) Our method can explicitly capture the 
structure information in the sentence level, 
enabling global reordering, which is impossible in 
Sun et al (2010). (2) For each rule, we can control 
its quality automatically or manually. 
8 Conclusion 
In this paper, we propose a novel method for 
extracting paraphrase rules by comparing the 
source side of bilingual corpus to the target-to-
source translation of the target side. The acquired 
paraphrase rules are employed to enrich the SMT 
inputs, which target on rewriting the input 
sentences to an MT-favored form. The paraphrase 
rules cover all kinds of paraphrases on the word, 
phrase and sentence levels, enabling structure 
reordering, word or phrase insertion, deletion and 
substitution. Experimental results show that the 
paraphrase rules can improve SMT quality in both 
the oral and news domains. The manual 
investigation on oral translation results indicate 
that the paraphrase rules capture four kinds of MT-
favored transformation to ensure translation quality 
improvement. 
Cate. Num Original Sentence/Translation Paraphrased Sentence/Translation 
(1) 11 
??/cigarette ??/can ??/duty-free ?
/take ??/how much ?/N/A ?  
??/how much ??/cigarettes ??/can ??
/duty-free ?/take ?/N/A ? 
what a cigarette can i take duty-free ? how many cigarettes can i take duty-free  one ? 
(2) 18 
?/you  ?/have  ??/how long  ?/N/A  
??/teaching ??/experience ? 
?/you  ?/have  ??/how much  ??/teaching  
??/experience ? 
you have how long teaching experience ? how much teaching experience you have ? 
(3) 10 ??/need  ??/deposit  ?/N/A ? ?/you  ??/need  ??/deposit  ?/N/A ? you need a deposit ? do you need a deposit ? 
(4) 4 
??/ring ?/fall ?/into ???/washbasin 
?/in ?/N/A ?  
ring off into the washbasin is in . 
??/ring  ?/fall  ?/into  ???/washbasin ?
/N/A ? 
ring off into the washbasin . 
Table 9: Examples for classification of paraphrase rules 
986
Acknowledgement 
This work was supported by National Natural 
Science Foundation of China (NSFC) (61073126, 
61133012), 863 High Technology Program 
(2011AA01A207). 
References  
Francis Bond, Eric Nichols, Darren Scott Appling, and 
Michael Paul. 2008. Improving Statistical Machine 
Translation by Paraphrasing the Training Data. In 
Proceedings of the IWSLT, pages 150?157. 
Chris Callison-Burch, Philipp Koehn, and Miles 
Osborne. 2006. Improved Statistical Machine 
Translation Using Paraphrases. In Proceedings of 
NAACL, pages 17-24. 
David Chiang. 2005. A hierarchical phrase-based model 
for statistical machine translation. In Proceedings of 
ACL, pages 263?270. 
Jinhua Du, Jie Jiang, Andy Way. 2010. Facilitating 
Translation Using Source Language Paraphrase 
Lattices. In Proceedings of EMNLP, pages 420-429. 
Wei He, Shiqi Zhao, Haifeng Wang and Ting Liu. 2011. 
Enriching SMT Training Data via Paraphrasing. In 
Proceedings of IJCNLP, pages 803-810. 
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In 
Proceedings of HLT/NAACL, pages 48?54 
Philipp Koehn. 2004. Statistical significance tests for 
machine translation evaluation. In Proceedings of 
EMNLP, pages 388-395. 
Philipp Koehn, Amittai Axelrod, Alexandra Birch 
Mayne, Chris Callison-Burch, Miles Osborne, and 
David Talbot. 2005. Edinburgh System Description 
for the 2005 IWSLT Speech Translation Evaluation. 
In Proceedings of IWSLT. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran 
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, and Evan Herbst. 2007. Moses: Open 
source toolkit for statistical machine translation. In 
Proceedings of the ACL Demo and Poster Sessions, 
pages 177?180. 
Roland Kuhn, Boxing Chen, George Foster and Evan 
Stratford. 2010. Phrase Clustering for Smoothing TM 
Probabilities-or, How to Extract Paraphrases from 
Phrase Tables. In Proceedings of COLING, pages 
608?616. 
Yuval Marton, Chris Callison-Burch, and Philip Resnik. 
2009. Improved Statistical Machine Translation 
Using Monolingually-Dervied Paraphrases. In 
Proceedings of EMNLP, pages 381-390. 
Aur?lien Max. 2010. Example-Based Paraphrasing for 
Improved Phrase-Based Statistical Machine 
TranslationIn Proceedings of EMNLP, pages 656-
666. 
Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido 
Dagan, Marc Dymetman, Idan Szpektor. 2009. 
Source-Language Entailment Modeling for 
Translation Unknown Terms. In Proceedings of ACL, 
pages 791-799. 
Preslav Nakov. 2008. Improved Statistical Machine 
Translation Using Monolingual Paraphrases. In 
Proceedings of ECAI, pages 338-342. 
Franz Josef Och and Hermann Ney. 2000. Improved 
Statistical Alignment Models. In Proceedings of ACL, 
pages 440-447. 
Franz Josef Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proceedings of 
ACL, pages 160-167. 
Takashi Onishi, Masao Utiyama, Eiichiro Sumita. 2010. 
Paraphrase Lattice for Statistical Machine 
Translation. In Proceedings of ACL, pages 1-5. 
Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing 
Zhu. 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. In Proceedings of 
ACL, pages 311-318. 
Reinhard Rapp. 2009. The Back-translation Score: 
Automatic MT Evaluation at the Sentence Level 
without Reference Translations. In Proceedings of 
ACL-IJCNLP, pages 133-136. 
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, 
John Makhoul, Linnea Micciulla, and Ralph 
Weischedel. 2005. A study of translation error rate 
with targeted human annotation. Technical Report 
LAMP-TR-126, CS-TR-4755, UMIACS-TR-2005-
58, University of Maryland, July, 2005. 
Yanli Sun, Sharon O?Brien, Minako O?Hagan and Fred 
Hollowood. 2010. A Novel Statistical Pre-Processing 
Model for Rule-Based Machine Translation System. 
In Proceedings of EAMT, 8pp. 
987
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1199?1209,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning Semantic Hierarchies via Word Embeddings
Ruiji Fu
?
, Jiang Guo
?
, Bing Qin
?
, Wanxiang Che
?
, Haifeng Wang
?
, Ting Liu
??
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
?
Baidu Inc., Beijing, China
{rjfu, jguo, bqin, car, tliu}@ir.hit.edu.cn
wanghaifeng@baidu.com
Abstract
Semantic hierarchy construction aims to
build structures of concepts linked by
hypernym?hyponym (?is-a?) relations. A
major challenge for this task is the
automatic discovery of such relations.
This paper proposes a novel and effec-
tive method for the construction of se-
mantic hierarchies based on word em-
beddings, which can be used to mea-
sure the semantic relationship between
words. We identify whether a candidate
word pair has hypernym?hyponym rela-
tion by using the word-embedding-based
semantic projections between words and
their hypernyms. Our result, an F-score
of 73.74%, outperforms the state-of-the-
art methods on a manually labeled test
dataset. Moreover, combining our method
with a previous manually-built hierarchy
extension method can further improve F-
score to 80.29%.
1 Introduction
Semantic hierarchies are natural ways to orga-
nize knowledge. They are the main components
of ontologies or semantic thesauri (Miller, 1995;
Suchanek et al, 2008). In the WordNet hierar-
chy, senses are organized according to the ?is-a?
relations. For example, ?dog? and ?canine? are
connected by a directed edge. Here, ?canine? is
called a hypernym of ?dog.? Conversely, ?dog?
is a hyponym of ?canine.? As key sources
of knowledge, semantic thesauri and ontologies
can support many natural language processing
applications. However, these semantic resources
are limited in its scope and domain, and their
manual construction is knowledge intensive and
time consuming. Therefore, many researchers
?
Email correspondence.
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite ???medicinal plant
??medicine
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
Figure 1: An example of semantic hierarchy con-
struction.
have attempted to automatically extract semantic
relations or to construct taxonomies.
A major challenge for this task is the auto-
matic discovery of hypernym-hyponym relations.
Fu et al (2013) propose a distant supervision
method to extract hypernyms for entities from
multiple sources. The output of their model is
a list of hypernyms for a given enity (left pan-
el, Figure 1). However, there usually also exists
hypernym?hyponym relations among these hy-
pernyms. For instance, ??? (plant)? and
???? (Ranunculaceae)? are both hyper-
nyms of the entity ??? (aconit),? and ??
? (plant)? is also a hypernym of ????
(Ranunculaceae).? Given a list of hypernyms
of an entity, our goal in the present work is to
construct a semantic hierarchy of these hypernyms
(right panel, Figure 1).
1
Some previous works extend and refine
manually-built semantic hierarchies by using other
resources (e.g., Wikipedia) (Suchanek et al,
2008). However, the coverage is limited by the
scope of the resources. Several other works relied
heavily on lexical patterns, which would suffer
from deficiency because such patterns can only
cover a small proportion of complex linguistic cir-
cumstances (Hearst, 1992; Snow et al, 2005).
1
In this study, we focus on Chinese semantic hierarchy
construction. The proposed method can be easily adapted to
other languages.
1199
Besides, distributional similarity methods (Kotler-
man et al, 2010; Lenci and Benotto, 2012) are
based on the assumption that a term can only be
used in contexts where its hypernyms can be used
and that a term might be used in any contexts
where its hyponyms are used. However, it is not
always rational. Our previous method based on
web mining (Fu et al, 2013) works well for hy-
pernym extraction of entity names, but it is unsuit-
able for semantic hierarchy construction which in-
volves many words with broad semantics. More-
over, all of these methods do not use the word
semantics effectively.
This paper proposes a novel approach for se-
mantic hierarchy construction based on word em-
beddings. Word embeddings, also known as dis-
tributed word representations, typically represent
words with dense, low-dimensional and real-
valued vectors. Word embeddings have been
empirically shown to preserve linguistic regular-
ities, such as the semantic relationship between
words (Mikolov et al, 2013b). For example,
v(king) ? v(queen) ? v(man) ? v(woman),
where v(w) is the embedding of the word w. We
observe that a similar property also applies to the
hypernym?hyponym relationship (Section 3.3),
which is the main inspiration of the present study.
However, we further observe that hypernym?
hyponym relations are more complicated than a
single offset can represent. To address this chal-
lenge, we propose a more sophisticated and gen-
eral method ? learning a linear projection which
maps words to their hypernyms (Section 3.3.1).
Furthermore, we propose a piecewise linear pro-
jection method based on relation clustering to
better model hypernym?hyponym relations (Sec-
tion 3.3.2). Subsequently, we identify whether an
unknown word pair is a hypernym?hyponym re-
lation using the projections (Section 3.4). To the
best of our knowledge, we are the first to apply
word embeddings to this task.
For evaluation, we manually annotate a dataset
containing 418 Chinese entities and their hyper-
nym hierarchies, which is the first dataset for this
task as far as we know. The experimental results
show that our method achieves an F-score of
73.74% which significantly outperforms the pre-
vious state-of-the-art methods. Moreover, com-
bining our method with the manually-built hier-
archy extension method proposed by Suchanek et
al. (2008) can further improve F-score to 80.29%.
2 Background
As main components of ontologies, semantic hi-
erarchies have been studied by many researchers.
Some have established concept hierarchies based
on manually-built semantic resources such as
WordNet (Miller, 1995). Such hierarchies have
good structures and high accuracy, but their cov-
erage is limited to fine-grained concepts (e.g.,
?Ranunculaceae? is not included in Word-
Net.). We have made similar obsevation that about
a half of hypernym?hyponym relations are absent
in a Chinese semantic thesaurus. Therefore, a
broader range of resources is needed to supple-
ment the manually built resources. In the construc-
tion of the famous ontology YAGO, Suchanek et
al. (2008) link the categories in Wikipedia onto
WordNet. However, the coverage is still limited
by the scope of Wikipedia.
Several other methods are based on lexical
patterns. They use manually or automatically
constructed lexical patterns to mine hypernym?
hyponym relations from text corpora. A hierarchy
can then be built based on these pairwise relations.
The pioneer work by Hearst (1992) has found
out that linking two noun phrases (NPs) via cer-
tain lexical constructions often implies hypernym
relations. For example, NP
1
is a hypernym of NP
2
in the lexical pattern ?such NP
1
as NP
2
.? Snow et
al. (2005) propose to automatically extract large
numbers of lexico-syntactic patterns and subse-
quently detect hypernym relations from a large
newswire corpus. Their method relies on accurate
syntactic parsers, and the quality of the automat-
ically extracted patterns is difficult to guarantee.
Generally speaking, these pattern-based methods
often suffer from low recall or precision because
of the coverage or the quality of the patterns.
The distributional methods assume that the con-
texts of hypernyms are broader than the ones of
their hyponyms. For distributional similarity com-
puting, each word is represented as a semantic
vector composed of the pointwise mutual infor-
mation (PMI) with its contexts. Kotlerman et al
(2010) design a directional distributional measure
to infer hypernym?hyponym relations based on
the standard IR Average Precision evaluation mea-
sure. Lenci and Benotto (2012) propose anoth-
er measure focusing on the contexts that hyper-
nyms do not share with their hyponyms. However,
broader semantics may not always infer broader
contexts. For example, for terms ?Obama? and
1200
?American people?, it is hard to say whose
contexts are broader.
Our previous work (Fu et al, 2013) applies a
web mining method to discover the hypernyms of
Chinese entities from multiple sources. We as-
sume that the hypernyms of an entity co-occur
with it frequently. It works well for named enti-
ties. But for class names (e.g., singers in Hong
Kong, tropical fruits) with wider range of mean-
ings, this assumption may fail.
In this paper, we aim to identify hypernym?
hyponym relations using word embeddings, which
have been shown to preserve good properties for
capturing semantic relationship between words.
3 Method
In this section, we first define the task formally.
Then we elaborate on our proposed method com-
posed of three major steps, namely, word embed-
ding training, projection learning, and hypernym?
hyponym relation identification.
3.1 Task Definition
Given a list of hypernyms of an entity, our goal is
to construct a semantic hierarchy on it (Figure 1).
We represent the hierarchy as a directed graph
G, in which the nodes denote the words, and the
edges denote the hypernym?hyponym relations.
Hypernym-hyponym relations are asymmetric and
transitive when words are unambiguous:
? ?x, y ? L : x
H
??y ? ?(y
H
??x)
? ?x, y, z ? L : (x
H
??z ? z
H
??y)? x
H
??y
Here, L denotes the list of hypernyms. x, y and
z denote the hypernyms in L. We use
H
?? to
represent a hypernym?hyponym relation in this
paper. Actually, x, y and z are unambiguous as
the hypernyms of a certain entity. Therefore, G
should be a directed acyclic graph (DAG).
3.2 Word Embedding Training
Various models for learning word embeddings
have been proposed, including neural net lan-
guage models (Bengio et al, 2003; Mnih and
Hinton, 2008; Mikolov et al, 2013b) and spec-
tral models (Dhillon et al, 2011). More recent-
ly, Mikolov et al (2013a) propose two log-linear
models, namely the Skip-gram and CBOW model,
to efficiently induce word embeddings. These two
models can be trained very efficiently on a large-
scale corpus because of their low time complexity.
No. Examples
1
v(?)? v(??) ? v(?)? v(??)
v(shrimp)? v(prawn) ? v(fish)? v(gold fish)
2
v(??)? v(??) ? v(??)? v(??)
v(laborer)? v(carpenter) ? v(actor)? v(clown)
3
v(??)? v(??) 6? v(?)? v(??)
v(laborer)? v(carpenter) 6? v(fish)? v(gold fish)
Table 1: Embedding offsets on a sample of
hypernym?hyponym word pairs.
Additionally, their experiment results have shown
that the Skip-gram model performs best in identi-
fying semantic relationship among words. There-
fore, we employ the Skip-gram model for estimat-
ing word embeddings in this study.
The Skip-gram model adopts log-linear classi-
fiers to predict context words given the current
word w(t) as input. First, w(t) is projected to its
embedding. Then, log-linear classifiers are em-
ployed, taking the embedding as input and pre-
dict w(t)?s context words within a certain range,
e.g. k words in the left and k words in the
right. After maximizing the log-likelihood over
the entire dataset using stochastic gradient descent
(SGD), the embeddings are learned.
3.3 Projection Learning
Mikolov et al (2013b) observe that word em-
beddings preserve interesting linguistic regulari-
ties, capturing a considerable amount of syntac-
tic/semantic relations. Looking at the well-known
example: v(king) ? v(queen) ? v(man) ?
v(woman), it indicates that the embedding offsets
indeed represent the shared semantic relation be-
tween the two word pairs.
We observe that the same property also ap-
plies to some hypernym?hyponym relations. As
a preliminary experiment, we compute the em-
bedding offsets between some randomly sampled
hypernym?hyponym word pairs and measure their
similarities. The results are shown in Table 1.
The first two examples imply that a word can
also be mapped to its hypernym by utilizing word
embedding offsets. However, the offset from
?carpenter? to ?laborer? is distant from
the one from ?gold fish? to ?fish,? indicat-
ing that hypernym?hyponym relations should be
more complicated than a single vector offset can
represent. To verify this hypothesis, we com-
pute the embedding offsets over all hypernym?
1201
???-????sportsman - footballer ??-???staff - civil servant??-??laborer - gardener??-???seaman - navigator??-??actor - singer ??-??actor - protagonist??-??actor - clown
??-??position - headmaster
??-???actor - matador
??-???laborer - temporary worker ??-??laborer - carpenter ??-???position ? consul general
??-??staff - airline hostess??-???staff - salesclerk??-???staff - conductor?-??chicken - cock?-????sheep - small-tail Han sheep?-??sheep - ram ?-??equus - zebra ?-??shrimp - prawn
?-??dog - police dog?-???rabbit - wool rabbit
??-???dolphin - white-flag dolphin ?-??fish - shark ?-???fish - tropical fish?-??fish - gold fish
?-??crab - sea crab
?-??donkey - wild ass
Figure 2: Clusters of the vector offsets in training data. The figure shows that the vector offsets distribute
in some clusters. The left cluster shows some hypernym?hyponym relations about animals. The right
one shows some relations about people?s occupations.
hyponym word pairs in our training data and vi-
sualize them.
2
Figure 2 shows that the relations
are adequately distributed in the clusters, which
implies that hypernym?hyponym relations in-
deed can be decomposed into more fine-grained
relations. Moreover, the relations about animals
are spatially close, but separate from the relations
about people?s occupations.
To address this challenge, we propose to learn
the hypernym?hyponym relations using projection
matrices.
3.3.1 A Uniform Linear Projection
Intuitively, we assume that all words can be pro-
jected to their hypernyms based on a uniform tran-
sition matrix. That is, given a word x and its hy-
pernym y, there exists a matrix ? so that y = ?x.
For simplicity, we use the same symbols as the
words to represent the embedding vectors. Ob-
taining a consistent exact ? for the projection of
all hypernym?hyponym pairs is difficult. Instead,
we can learn an approximate ? using Equation 1
on the training data, which minimizes the mean-
squared error:
?
?
= arg min
?
1
N
?
(x,y)
? ?x? y ?
2
(1)
where N is the number of (x, y) word pairs in
the training data. This is a typical linear regres-
sion problem. The only difference is that our pre-
dictions are multi-dimensional vectors instead of
scalar values. We use SGD for optimization.
2
Principal Component Analysis (PCA) is applied for di-
mensionality reduction.
3.3.2 Piecewise Linear Projections
A uniform linear projection may still be under-
representative for fitting all of the hypernym?
hyponym word pairs, because the relations are
rather diverse, as shown in Figure 2. To better
model the various kinds of hypernym?hyponym
relations, we apply the idea of piecewise linear re-
gression (Ritzema, 1994) in this study.
Specifically, the input space is first segmented
into several regions. That is, all word pairs (x, y)
in the training data are first clustered into sever-
al groups, where word pairs in each group are
expected to exhibit similar hypernym?hyponym
relations. Each word pair (x, y) is represented
with their vector offsets: y ? x for clustering.
The reasons are twofold: (1) Mikolov?s work has
shown that the vector offsets imply a certain lev-
el of semantic relationship. (2) The vector off-
sets distribute in clusters well, and the word pairs
which are close indeed represent similar relations,
as shown in Figure 2.
Then we learn a separate projection for each
cluster, respectively (Equation 2).
?
?
k
= arg min
?
k
1
N
k
?
(x,y)?C
k
? ?
k
x? y ?
2
(2)
where N
k
is the amount of word pairs in the k
th
cluster C
k
.
We use the k-means algorithm for clustering,
where k is tuned on a development dataset.
3.3.3 Training Data
To learn the projection matrices, we extract train-
ing data from a Chinese semantic thesaurus,
Tongyi Cilin (Extended) (CilinE for short) which
1202
?
?
?
?
?
Root
L ev el 1
L ev el 2
L ev el 3
L ev el 4
L ev el 5
?  ob j ect
??  animal
??  insect
- -
??  dragonf ly
B
i
18
A
06@
?? : ?? 
( dragonf ly  :  animal)
?? : ?? 
( dragonf ly  :  insect)
C ilinEh y perny m-h y pony m pairs
S ense C ode:  B i1 8 A0 6 @
S ense C ode:  B i1 8 A
S ense C ode:  B i1 8
S ense C ode:  B i
S ense C ode:  B
?? : ?? 
( insect :  animal)
Figure 3: Hierarchy of CilinE and an Example of
Training Data Generation
contains 100,093 words (Che et al, 2010).
3
CilinE
is organized as a hierarchy of five levels, in which
the words are linked by hypernym?hyponym
relations (right panel, Figure 3). Each word in
CilinE has one or more sense codes (some words
are polysemous) that indicate its position in the hi-
erarchy.
The senses of words in the first level, such as
?? (object)? and ??? (time),? are very gen-
eral. The fourth level only has sense codes without
real words. Therefore, we extract words in the sec-
ond, third and fifth levels to constitute hypernym?
hyponym pairs (left panel, Figure 3).
Note that mapping one hyponym to multi-
ple hypernyms with the same projection (?x is
unique) is difficult. Therefore, the pairs with the
same hyponym but different hypernyms are ex-
pected to be clustered into separate groups. Fig-
ure 3 shows that the word ?dragonfly? in the
fifth level has two hypernyms: ?insect? in the
third level and ?animal? in the second level.
Hence the relations dragonfly
H
?? insect and
dragonfly
H
?? animal should fall into differ-
ent clusters.
In our implementation, we apply this constraint
by simply dividing the training data into two cat-
egories, namely, direct and indirect. Hypernym-
hyponym word pair (x, y) is classified into the di-
rect category, only if there doesn?t exist another
word z in the training data, which is a hypernym of
x and a hyponym of y. Otherwise, (x, y) is classi-
fied into the indirect category. Then, data in these
two categories are clustered separately.
3
www.ltp-cloud.com/download/
x
y
?k
? 
x'
?l
Figure 4: In this example, ?
k
x is located in the
circle with center y and radius ?. So y is consid-
ered as a hypernym of x. Conversely, y is not a
hypernym of x
?
.
x
y
z
x
y
(a) (b)
z
x
y
Figure 5: (a) If d(?
j
y, x) > d(?
k
x, y), we re-
move the path from y to x; (b) if d(?
j
y, x) >
d(?
k
x, z) and d(?
j
y, x) > d(?
i
z, y), we reverse
the path from y to x.
3.4 Hypernym-hyponym Relation
Identification
Upon obtaining the clusters of training data and
the corresponding projections, we can identify
whether two words have a hypernym?hyponym re-
lation. Given two words x and y, we find cluster
C
k
whose center is closest to the offset y ? x, and
obtain the corresponding projection ?
k
. For y to
be considered a hypernym of x, one of the two
conditions below must hold.
Condition 1: The projection ?
k
puts ?
k
x close
enough to y (Figure 4). Formally, the euclidean
distance between ?
k
x and y: d(?
k
x, y) must be
less than a threshold ?.
d(?
k
x, y) =? ?
k
x? y ?
2
< ? (3)
Condition 2: There exists another word z sat-
isfying x
H
??z and z
H
??y. In this case, we use the
transitivity of hypernym?hyponym relations.
Besides, the final hierarchy should be a DAG
as discussed in Section 3.1. However, the pro-
jection method cannot guarantee that theoretical-
ly, because the projections are learned from pair-
wise hypernym?hyponym relations without the w-
hole hierarchy structure. All pairwise hypernym?
hyponym relation identification methods would
suffer from this problem actually. It is an inter-
esting problem how to construct a globally opti-
1203
mal semantic hierarchy conforming to the form
of a DAG. But this is not the focus of this paper.
So if some conflicts occur, that is, a relation cir-
cle exists, we remove or reverse the weakest path
heuristically (Figure 5). If a circle has only two
nodes, we remove the weakest path. If a circle has
more than two nodes, we reverse the weakest path
to form an indirect hypernym?hyponym relation.
4 Experimental Setup
4.1 Experimental Data
In this work, we learn word embeddings from a
Chinese encyclopedia corpus named Baidubaike
4
,
which contains about 30 million sentences (about
780 million words). The Chinese segmentation
is provided by the open-source Chinese language
processing platform LTP
5
(Che et al, 2010).
Then, we employ the Skip-gram method (Section
3.2) to train word embeddings. Finally we obtain
the embedding vectors of 0.56 million words.
The training data for projection learning is
collected from CilinE (Section 3.3.3). We ob-
tain 15,247 word pairs of hypernym?hyponym
relations (9,288 for direct relations and 5,959 for
indirect relations).
For evaluation, we collect the hypernyms for
418 entities, which are selected randomly from
Baidubaike, following Fu et al (2013). We then
ask two annotators to manually label the seman-
tic hierarchies of the correct hypernyms. The final
data set contains 655 unique hypernyms and 1,391
hypernym?hyponym relations among them. We
randomly split the labeled data into 1/5 for de-
velopment and 4/5 for testing (Table 2). The hi-
erarchies are represented as relations of pairwise
words. We measure the inter-annotator agreement
using the kappa coefficient (Siegel and Castel-
lan Jr, 1988). The kappa value is 0.96, which indi-
cates a good strength of agreement.
4.2 Evaluation Metrics
We use precision, recall, and F-score as our met-
rics to evaluate the performances of the methods.
Since hypernym?hyponym relations and its re-
verse (hyponym?hypernym) have one-to-one cor-
respondence, their performances are equal. For
4
Baidubaike (baike.baidu.com) is one of the largest
Chinese encyclopedias containing more than 7.05 million en-
tries as of September, 2013.
5
www.ltp-cloud.com/demo/
Relation
# of word pairs
Dev. Test
hypernym?hyponym 312 1,079
hyponym?hypernym
?
312 1,079
unrelated 1,044 3,250
Total 1,668 5,408
Table 2: The evaluation data.
?
Since hypernym?
hyponym relations and hyponym?hypernym
relations have one-to-one correspondence, their
numbers are the same.
1 5 
10 15 
20 
0.45 
0.5 
0.55 
0.6 
0.65 
0.7 
0.75 
0.8 
1 10 20 30 40 50 60 Indirect 
F1-
Sco
re 
Direct 
Figure 6: Performance on development data w.r.t.
cluster size.
simplicity, we only report the performance of the
former in the experiments.
5 Results and Analysis
5.1 Varying the Amount of Clusters
We first evaluate the effect of different number of
clusters based on the development data. We vary
the numbers of the clusters both for the direct and
indirect training word pairs.
As shown in Figure 6, the performance of clus-
tering is better than non-clustering (when the clus-
ter number is 1), thus providing evidences that
learning piecewise projections based on clustering
is reasonable. We finally set the numbers of the
clusters of direct and indirect to 20 and 5, respec-
tively, where the best performances are achieved
on the development data.
5.2 Comparison with Previous Work
In this section, we compare the proposed method
with previous methods, including manually-built
hierarchy extension, pairwise relation extraction
1204
P(%) R(%) F(%)
M
Wiki+CilinE
92.41 60.61 73.20
M
Pattern
97.47 21.41 35.11
M
Snow
60.88 25.67 36.11
M
balApinc
54.96 53.38 54.16
M
invCL
49.63 62.84 55.46
M
Fu
87.40 48.19 62.13
M
Emb
80.54 67.99 73.74
M
Emb+CilinE
80.59 72.42 76.29
M
Emb+Wiki+CilinE
79.78 80.81 80.29
Table 3: Comparison of the proposed method with
existing methods in the test set.
Pattern Translation
w?[??|??] h w is a [a kind of] h
w [?]? h w[,] and other h
h [?]?[?] w h[,] called w
h [?] [?]? w h[,] such as w
h [?]??? w h[,] especially w
Table 4: Chinese Hearst-style lexical patterns. The
contents in square brackets are omissible.
based on patterns, word distributions, and web
mining (Section 2). Results are shown in Table 3.
5.2.1 Overall Comparison
M
Wiki+CilinE
refers to the manually-built hierar-
chy extension method of Suchanek et al (2008).
In our experiment, we use the category taxonomy
of Chinese Wikipedia
6
to extend CilinE. Table 3
shows that this method achieves a high precision
but also a low recall, mainly because of the limit-
ed scope of Wikipedia.
M
Pattern
refers to the pattern-based method of
Hearst (1992). We extract hypernym?hyponym
relations in the Baidubaike corpus, which is al-
so used to train word embeddings (Section 4.1).
We use the Chinese Hearst-style patterns (Table
4) proposed by Fu et al (2013), in which w rep-
resents a word, and h represents one of its hy-
pernyms. The result shows that only a small part
of the hypernyms can be extracted based on these
patterns because only a few hypernym relations
are expressed in these fixed patterns, and many are
expressed in highly flexible manners.
In the same corpus, we apply the method
M
Snow
originally proposed by Snow et al (2005).
The same training data for projections learn-
6
dumps.wikimedia.org/zhwiki/20131205/
ing from CilinE (Section 3.3.3) is used as
seed hypernym?hyponym pairs. Lexico-syntactic
patterns are extracted from the Baidubaike corpus
by using the seeds. We then develop a logistic re-
gression classifier based on the patterns to recog-
nize hypernym?hyponym relations. This method
relies on an accurate syntactic parser, and the qual-
ity of the automatically extracted patterns is diffi-
cult to guarantee.
We re-implement two previous distribution-
al methods M
balApinc
(Kotlerman et al, 2010)
and M
invCL
(Lenci and Benotto, 2012) in the
Baidubaike corpus. Each word is represented as a
feature vector in which each dimension is the PMI
value of the word and its context words. We com-
pute a score for each word pair and apply a thresh-
old to identify whether it is a hypernym?hyponym
relation.
M
Fu
refers to our previous web mining
method (Fu et al, 2013). This method mines hy-
pernyms of a given word w from multiple sources
and returns a ranked list of the hypernyms. We
select the hypernyms with scores over a threshold
of each word in the test set for evaluation. This
method assumes that frequent co-occurrence of a
noun or noun phrase n in multiple sources with w
indicate possibility of n being a hypernym of w.
The results presented in Fu et al (2013) show that
the method works well when w is an entity, but
not when w is a word with a common semantic
concept. The main reason may be that there are
relatively more introductory pages about entities
than about common words in the Web.
M
Emb
is the proposed method based on word
embeddings. Table 3 shows that the proposed
method achieves a better recall and F-score than
all of the previous methods do. It can significantly
(p < 0.01) improve the F-score over the state-of-
the-art method M
Wiki+CilinE
.
M
Emb
and M
CilinE
can also be combined. The
combination strategy is to simply merge all pos-
itive results from the two methods together, and
then to infer new relations based on the transitiv-
ity of hypernym?hyponym relations. The F-score
is further improved from 73.74% to 76.29%. Note
that, the combined method achieves a 4.43% re-
call improvement over M
Emb
, but the precision is
almost unchanged. The reason is that the infer-
ence based on the relations identified automatical-
ly may lead to error propagation. For example, the
relation x
H
??y is incorrectly identified by M
Emb
.
1205
P(%) R(%) F(%)
M
Wiki+CilinE
80.39 19.29 31.12
M
Emb+CilinE
71.16 52.80 60.62
M
Emb+Wiki+CilinE
69.13 61.65 65.17
Table 5: Performance on the out-of-CilinE data in
the test set.
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Prec
ision
l l lllllll
l
l
l ll
l MEmb+Wiki+Ci l inEMEmb+Ci l inEMWiki+Ci l inE
Figure 7: Precision-Recall curves on the out-of-
CilinE data in the test set.
When the relation y
H
??z from M
CilinE
is added, it
will cause a new incorrect relation x
H
??z.
Combining M
Emb
with M
Wiki+CilinE
achieves
a 7% F-score improvement over the best baseline
M
Wiki+CilinE
. Therefore, the proposed method
is complementary to the manually-built hierarchy
extension method (Suchanek et al, 2008).
5.2.2 Comparison on the Out-of-CilinE Data
We are greatly interested in the practical perfor-
mance of the proposed method on the hypernym?
hyponym relations outside of CilinE. We say a
word pair is outside of CilinE, as long as there
is one word in the pair not existing in CilinE. In
our test data, about 62% word pairs are outside
of CilinE. Table 5 shows the performances of the
best baseline method and our method on the out-
of-CilinE data. The method exploiting the tax-
onomy in Wikipedia, M
Wiki+CilinE
, achieves the
highest precision but has a low recall. By con-
trast, our method can discover more hypernym?
hyponym relations with some loss of precision,
thereby achieving a more than 29% F-score im-
provement. The combination of these two meth-
ods achieves a further 4.5% F-score improvement
over M
Emb+CilinE
. Generally speaking, the pro-
posed method greatly improves the recall but dam-
ages the precision.
Actually, we can get different precisions and re-
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
( a)  C ilinE
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
( b )  W ik ipedia+ C ilinE
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
( c)  E mb edding
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
( d)  E mb edding+ W ik ipedia+ C ilinE
Figure 8: An example for error analysis. The
red paths refer to the relations between the named
entity and its hypernyms extracted using the web
mining method (Fu et al, 2013). The black paths
with hollow arrows denote the relations identified
by the different methods. The boxes with dotted
borders refer to the concepts which are not linked
to correct positions.
calls by adjusting the threshold ? (Equation 3).
Figure 7 shows that M
Emb+CilinE
achieves a high-
er precision than M
Wiki+CilinE
when their recalls
are the same. When they achieve the same preci-
sion, the recall of M
Emb+CilinE
is higher.
5.3 Error Analysis and Discussion
We analyze error cases after experiments. Some
cases are shown in Figure 8. We can see that
there is only one general relation ??? (plant)?
H
?? ??? (organism)? existing in CilinE. Some
fine-grained relations exist in Wikipedia, but the
coverage is limited. Our method based on
word embeddings can discover more hypernym?
hyponym relations than the previous methods can.
When we combine the methods together, we get
the correct hierarchy.
Figure 8 shows that our method loses the
relation ???? (Aconitum)? H?? ????
(Ranunculaceae).? It is because they are
very semantically similar (their cosine similarity
is 0.9038). Their representations are so close to
each other in the embedding space that we have
not find projections suitable for these pairs. The
1206
error statistics show that when the cosine similari-
ties of word pairs are greater than 0.8, the recall is
only 9.5%. This kind of error accounted for about
10.9% among all the errors in our test set. One
possible solution may be adding more data of this
kind to the training set.
6 Related Work
In addition to the works mentioned in Section 2,
we introduce another set of related studies in this
section.
Evans (2004), Ortega-Mendoza et al (2007),
and Sang (2007) consider web data as a large cor-
pus and use search engines to identify hypernyms
based on the lexical patterns of Hearst (1992).
However, the low quality of the sentences in the
search results negatively influence the precision of
hypernym extraction.
Following the method for discovering patterns
automatically (Snow et al, 2005), McNamee et
al. (2008) apply the same method to extract hy-
pernyms of entities in order to improve the perfor-
mance of a question answering system. Ritter et al
(2009) propose a method based on patterns to find
hypernyms on arbitrary noun phrases. They use
a support vector machine classifier to identify the
correct hypernyms from the candidates that match
the patterns. As our experiments show, pattern-
based methods suffer from low recall because of
the low coverage of patterns.
Besides Kotlerman et al (2010) and Lenci and
Benotto (2012), other researchers also propose di-
rectional distributional similarity methods (Weeds
et al, 2004; Geffet and Dagan, 2005; Bhagat et al,
2007; Szpektor et al, 2007; Clarke, 2009). How-
ever, their basic assumption that a hyponym can
only be used in contexts where its hypernyms can
be used and that a hypernym might be used in all
of the contexts where its hyponyms are used may
not always rational.
Snow et al (2006) provides a global optimiza-
tion scheme for extending WordNet, which is d-
ifferent from the above-mentioned pairwise rela-
tionships identification methods.
Word embeddings have been successfully ap-
plied in many applications, such as in sentiment
analysis (Socher et al, 2011b), paraphrase detec-
tion (Socher et al, 2011a), chunking, and named
entity recognition (Turian et al, 2010; Collobert
et al, 2011). These applications mainly utilize
the representing power of word embeddings to al-
leviate the problem of data sparsity. Mikolov et
al. (2013a) and Mikolov et al (2013b) further ob-
serve that the semantic relationship of words can
be induced by performing simple algebraic oper-
ations with word vectors. Their work indicates
that word embeddings preserve some interesting
linguistic regularities, which might provide sup-
port for many applications. In this paper, we
improve on their work by learning multiple lin-
ear projections in the embedding space, to model
hypernym?hyponym relationships within different
clusters.
7 Conclusion and Future Work
This paper proposes a novel method for seman-
tic hierarchy construction based on word em-
beddings, which are trained using a large-scale
corpus. Using the word embeddings, we learn
the hypernym?hyponym relationship by estimat-
ing projection matrices which map words to their
hypernyms. Further improvements are made us-
ing a cluster-based approach in order to model
the more fine-grained relations. Then we propose
a few simple criteria to identity whether a new
word pair is a hypernym?hyponym relation. Based
on the pairwise hypernym?hyponym relations, we
build semantic hierarchies automatically.
In our experiments, the proposed method signif-
icantly outperforms state-of-the-art methods and
achieves the best F1-score of 73.74% on a manual-
ly labeled test dataset. Further experiments show
that our method is complementary to the previous
manually-built hierarchy extension methods.
For future work, we aim to improve word
embedding learning under the guidance of
hypernym?hyponym relations. By including the
hypernym?hyponym relation constraints while
training word embeddings, we expect to improve
the embeddings such that they become more suit-
able for this task.
Acknowledgments
This work was supported by National Natu-
ral Science Foundation of China (NSFC) via
grant 61133012, 61273321 and the National 863
Leading Technology Research Project via grant
2012AA011102. Special thanks to Shiqi Zhao,
Zhenghua Li, Wei Song and the anonymous re-
viewers for insightful comments and suggestions.
We also thank Xinwei Geng and Hongbo Cai for
their help in the experiments.
1207
References
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137?1155.
Rahul Bhagat, Patrick Pantel, Eduard H Hovy, and Ma-
rina Rey. 2007. Ledir: An unsupervised algorith-
m for learning directionality of inference rules. In
EMNLP-CoNLL, pages 161?170.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp:
A chinese language technology platform. In Coling
2010: Demonstrations, pages 13?16, Beijing, Chi-
na, August.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of
the Workshop on Geometrical Models of Natural
Language Semantics, pages 112?119. Association
for Computational Linguistics.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Paramveer Dhillon, Dean P Foster, and Lyle H Ungar.
2011. Multi-view learning of word embeddings via
cca. In Advances in Neural Information Processing
Systems, pages 199?207.
Richard Evans. 2004. A framework for named entity
recognition in the open domain. Recent Advances in
Natural Language Processing III: Selected Papers
from RANLP 2003, 260:267?274.
Ruiji Fu, Bing Qin, and Ting Liu. 2013. Exploiting
multiple sources for open-domain hypernym discov-
ery. In EMNLP, pages 1224?1234.
Maayan Geffet and Ido Dagan. 2005. The distribution-
al inclusion hypotheses and lexical entailment. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics, pages 107?114.
Association for Computational Linguistics.
Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics-
Volume 2, pages 539?545. Association for Compu-
tational Linguistics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribution-
al similarity for lexical inference. Natural Language
Engineering, 16(4):359?389.
Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of the Sixth International Workshop on
Semantic Evaluation, pages 75?79. Association for
Computational Linguistics.
Paul McNamee, Rion Snow, Patrick Schone, and James
Mayfield. 2008. Learning named entity hyponyms
for question answering. In Proceedings of the
Third International Joint Conference on Natural
Language Processing, pages 799?804.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word rep-
resentations in vector space. arXiv preprint arX-
iv:1301.3781.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL-
HLT, pages 746?751.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Andriy Mnih and Geoffrey E Hinton. 2008. A s-
calable hierarchical distributed language model. In
Advances in neural information processing systems,
pages 1081?1088.
Rosa M Ortega-Mendoza, Luis Villase?nor-Pineda, and
Manuel Montes-y G?omez. 2007. Using lexical
patterns for extracting hyponyms from the web. In
MICAI 2007: Advances in Artificial Intelligence,
pages 904?911. Springer.
Alan Ritter, Stephen Soderland, and Oren Etzioni.
2009. What is this, anyway: Automatic hypernym
discovery. In Proceedings of the 2009 AAAI Spring
Symposium on Learning by Reading and Learning
to Read, pages 88?93.
HP Ritzema. 1994. Drainage principles and
applications.
Erik Tjong Kim Sang. 2007. Extracting hypernym
pairs from the web. In Proceedings of the 45th An-
nual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 165?168. Associa-
tion for Computational Linguistics.
Sidney Siegel and N John Castellan Jr. 1988. Non-
parametric statistics for the behavioral sciences.
McGraw-Hill, New York.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Lawrence K. Saul, Yair Weiss, and
L?eon Bottou, editors, Advances in Neural Informa-
tion Processing Systems 17, pages 1297?1304. MIT
Press, Cambridge, MA.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computation-
al Linguistics, pages 801?808, Sydney, Australia,
July. Association for Computational Linguistics.
1208
Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-
pher D Manning, and Andrew Ng. 2011a. Dynam-
ic pooling and unfolding recursive autoencoders for
paraphrase detection. In Advances in Neural Infor-
mation Processing Systems, pages 801?809.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011b.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161. Association for
Computational Linguistics.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
wikipedia and wordnet. Web Semantics: Sci-
ence, Services and Agents on the World Wide Web,
6(3):203?217.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
456?463, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th internation-
al conference on Computational Linguistics, page
1015. Association for Computational Linguistics.
1209
Accurate and Robust LFG-Based Generation for Chinese
Yuqing Guo
NCLT, School of Computing
Dublin City University
Dublin 9, Ireland
yguo@computing.dcu.ie
Haifeng Wang
Toshiba (China)
Research and Development Center
Beijing, 100738, China
wanghaifeng@rdc.toshiba.com.cn
Josef van Genabith
NCLT, School of Computing
Dublin City University
IBM CAS, Dublin, Ireland
josef@computing.dcu.ie
Abstract
We describe three PCFG-based models for
Chinese sentence realisation from Lexical-
Functional Grammar (LFG) f-structures. Both
the lexicalised model and the history-based
model improve on the accuracy of a simple
wide-coverage PCFG model by adding lexical
and contextual information to weaken inap-
propriate independence assumptions implicit
in the PCFG models. In addition, we pro-
vide techniques for lexical smoothing and rule
smoothing to increase the generation cover-
age. Trained on 15,663 automatically LFG f-
structure annotated sentences of the Penn Chi-
nese treebank and tested on 500 sentences ran-
domly selected from the treebank test set, the
lexicalised model achieves a BLEU score of
0.7265 at 100% coverage, while the history-
based model achieves a BLEU score of 0.7245
also at 100% coverage.
1 Introduction
Sentence generation, or surface realisation can be
described as the problem of producing syntacti-
cally, morphologically, and orthographically cor-
rect sentences from a given abstract semantic /
logical representation according to some linguistic
theory, e.g. Lexical Functional Grammar (LFG),
Head-Driven Phrase Structure Grammar (HPSG),
Combinatory Categorial Grammar (CCG), Tree Ad-
joining Grammar (TAG) etc. Grammars, such as
these, are declarative formulations of the correspon-
dences between semantic and syntactic representa-
tions. Traditionally, grammar rules have been care-
fully handcrafted, such as those used in LinGo (Car-
roll et al, 1999), OpenCCG (White, 2004) and
XLE (Crouch et al, 2007). As handcrafting gram-
mar rules is time-consuming, language-dependent
and domain-specific, recent years have witnessed re-
search on extracting wide-coverage grammars auto-
matically from annotated corpora, for both parsing
and generation. FERGUS (Bangalore and Rambow,
2000) took dependency structures as inputs, and pro-
duced XTAG derivations by a stochastic tree model
automatically acquired from an annotated corpus.
Nakanishi et al (2005) presented log-linear models
for a chart generator using a HPSG grammar ac-
quired from the Penn-II Treebank. From the same
treebank, Cahill and van Genabith (2006) automati-
cally extracted wide-coverage LFG approximations
for a PCFG-based generation model.
In addition to applying statistical techniques to
automatically acquire generation grammars, over the
last decade, there has been a lot of interest in a
generate-and-select paradigm for surface realisation.
The paradigm is characterised by a separation be-
tween generation and selection, in which symbolic
or rule-based methods are used to generate a space
of possible paraphrases, and statistical methods are
used to select one or more outputs from the space.
Starting from Langkilde (2002) who used a n-gram
language model to rank generated output strings, a
substantial number of traditional handcrafted sur-
face realisers have been augmented with sophisti-
cated stochastic rankers (Velldal and Oepen, 2005;
White et al, 2007; Cahill et al, 2007).
It is interesting to note that, while the study of
how the granularity of context-free grammars (CFG)
affects the performance of a parser (e.g. in the form
86
n1:IP
[?=?]
n2:NP
[?SUBJ=?]
n4:NR
[?=?]
L?
JiangZemin
n3:VP
[?=?]
n5:VV
[?=?]
??
interview
n6:NP
[?OBJ=?]
n7:NR
[???ADJUNCT]
I
Thai
n8:NN
[?=?]
on
president
f1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
PRED ????
SUBJ f2
?
?
?
PRED ?L??
NTYPE proper
NUM sg
?
?
?
OBJ f3
?
?
?
?
?
?
?
?
?
?
?
PRED ?on?
NTYPE common
NUM sg
ADJUNCT
?
?
?
?
?
f4
?
?
?
PRED ?I?
NTYPE proper
NUM sg
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
? : N ? F
?(n1)=?(n3)=?(n5)=f1 ?(n2)=?(n4)=f2 ?(n6)=?(n8)=f3 ?(n7)=f4
Figure 1: C- and f-structures with ? links for the sentence ?L???Ion?
of grammar transforms (Johnson, 1998) and lexical-
isation (Collins, 1997)) has attracted substantial at-
tention, to our knowledge, there has been a lot less
research on this subject for surface realisation, a pro-
cess that is generally regarded as the reverse pro-
cess of parsing. Moreover, while most of the re-
search so far has concentrated on English or Euro-
pean languages, we are also interested in generation
for other languages with diverse properties, such as
Chinese which is currently a focus language in pars-
ing (Bikel, 2004; Cao et al, 2007).
In this paper, we investigate three generative
PCFG models for Chinese generation based on
wide-coverage LFG grammars automatically ex-
tracted from the Penn Chinese Treebank (CTB). Our
work is couched in the framework of Lexical Func-
tional Grammar and is implemented in a chart-style
generator. We briefly describe LFG and the basic
generation model in Section 2. We improve the
baseline PCFG model by weakening the indepen-
dence assumptions in two disambiguation models in
Section 3. Section 4 describes the smoothing algo-
rithms adopted for the chart generator and Section 5
gives the experimental details and results.
2 LFG-Based Generation
2.1 Lexical Functional Grammar
Lexical Functional Grammar (Kaplan and Bres-
nan, 1982) is a constraint-based grammar formal-
ism which postulates (minimally) two levels of rep-
resentation: c(onstituent)-structure and f(unctional)-
structure. C-structure takes the form of phrase struc-
ture trees and captures surface grammatical config-
urations. F-structure encodes more abstract gram-
matical functions (GFs) such as SUBJ(ect), OBJ(ect),
ADJUNCT and TOPIC etc., in the form of hierar-
chical attribute-value matrices. C-structures and
f-structures are related by a piecewise correspon-
dence function ? that goes from the nodes of a c-
structure tree into units of f-structure spaces (Ka-
plan, 1995). As illustrated in Figure 1, given a
c-structure node ni, the corresponding f-structure
component fj is ?(ni). Admissible c-structures
are specified by a context-free grammar. The cor-
responding f-structures are derived from functional
annotations attached to the CFG rewriting rules.
(1) a. IP ?? NP VP
[?SUBJ=?] [?=?]
b. VP ?? VV NP
[?=?] [?OBJ=?]
c. NP ?? NR NN
[?ADJ=?] [?=?]
d. NP ?? NR
[?=?]
(1) shows a miniature set of annotated CFG rules
(lexical entries omitted) which generates the c- and
f-structure in Figure 1. In the functional annotations,
(?) refers to the f-structure associated with the local
c-structure node ni, i.e. ?(ni), and (?) refers to the
87
Model Grammar Rule Conditions
PCFG VP[?=?] ? VV[?=?] NP[?OBJ=?] VP[?=?], {PRED, SUBJ, OBJ}
HB-PCFG VP[?=?] ? VV[?=?] NP[?OBJ=?] VP[?=?], {PRED, SUBJ, OBJ}, TOP
LEX-PCFG VP(??)[?=?] ? VV(??)[?=?] NP(on)[?OBJ=?] VP(??)[?=?], {PRED, SUBJ, OBJ}
Table 1: Examples of f-structure annotated CFG rules (from Figure 1) in different models
f-structure associated with the mother (M ) node of
ni, i.e. ?(M(ni)).
2.2 Generation from f-Structures
The generation task in LFG is to determine which
sentences correspond to a specified f-structure,
given a particular grammar, such as (1). Kaplan
and Wedekind (2000) proved that the set of strings
generated by an LFG grammar from fully speci-
fied f-structures is a context-free language. Based
on this theoretical cornerstone, Cahill and van Gen-
abith (2006) presented a PCFG-based chart genera-
tor using wide-coverage LFG approximations auto-
matically extracted from the Penn-II treebank. The
LFG-based statistical generation model defines the
conditional probability P (T |F ), for each candidate
functionally annotated c-structure tree T (which
fully specifies a surface realisation) given an f-
structure F . The generation model searches for the
Tbest that maximises P (T |F ) (Eq. 1). P (T |F ) is
then decomposed as the product of the probabilities
of all the functionally annotated CFG rewriting rules
X ? Y (conditioned on the left hand side (LHS) X
and local features of the corresponding f-structure
?(X)) contributing to the tree T (Eq. 2). The first
line (PCFG) of Table 1 shows the f-structure anno-
tated CFG rule to expand node n3 in Figure 1.
Tbest = argmax
T
P (T |F ) (1)
P (T |F ) =
?
X ? Y in T
Feats = {ai|ai ? ?(X)}
P (X ? Y |X,Feats) (2)
3 Disambiguation Models
The basic generation model presented in (Cahill
and van Genabith, 2006) used simple probabilis-
tic context-free grammars. However, the indepen-
dence assumptions implicit in PCFG models may
not be appropriate to best capture natural language
phenomena. Methodologies such as lexicalisa-
tion (Collins, 1997; Charniak, 2000) and tree trans-
formations (Johnson, 1998), weaken the indepen-
dence assumptions and have been applied success-
fully to parsing and shown significant improvements
over simple PCFGs. In this section we study the ef-
fect of such methods in LFG-based generation for
Chinese.
3.1 A History-Based Model
The history-based (HB) approach which incorpo-
rates more context information has worked well
in parsing (Collins, 1997; Charniak, 2000). Re-
sembling history-based models for parsing, Hogan
et al (2007) presented a history-based generation
model to overcome some of the inappropriate inde-
pendence assumptions in the basic generation model
of (Cahill and van Genabith, 2006). The history-
based model increases the context by simply includ-
ing the parent grammatical function GF of the f-
structure in addition to the local ?-linked feature set
in the conditioning context (Eq. 3). The f-structure
annotated CFG rule expanding n3 in the history-
based model is shown in the second line (HB-PCFG)
of Table 1.1
P (T |F ) =
?
X ? Y in T
Feats = {ai|ai ? ?(X)}
?f (f GF) = ?(X)
P (X ? Y |X,Feats,GF) (3)
The history-based model is motivated by English
data, for example, to generate the appropriate case
for pronouns in subject position and object position,
respectively. Though Chinese does not distinguish
cases, we expect the f-structure parent GF to help
predict grammar rule expansions more accurately in
the tree derivation than the simple PCFG model. We
will investigate how the HB model performs while
migrating it from English to Chinese data.
1The parent grammatical function of the outermost f-
structure is assumed to be a dummy GF TOP.
88
3.2 A Lexicalised Model
Compared to the HB model which includes the par-
ent grammatical function in the conditioning con-
text, lexicalised grammar rules contain more fine-
grained categorial information. To the best of our
knowledge, lexicalised parsers (Bikel, 2004) outper-
form unlexicalised parsers for Chinese. The expec-
tation is that a lexicalised PCFG model also works
better than a simple PCFG model in Chinese gen-
eration, considering e.g. prepositional phrase (PP)
modification in Chinese. Some prepositions indicat-
ing directions can occur either before or after the
main verbs, for instance both (2a) and (2b) are ac-
ceptable in Chinese. However, most PP modifiers
only act as adverbial adjuncts between the subjects
and verbal predicates. For instance ??/to? never
follows a verb as exemplified in the ungrammatical
sentence (3b).
(2) a. ? 4 ?m  ?
this CLS train run to Beijing
?The train is bound for Beijing.?
b. ? 4 ? ? m
this CLS train to Beijing run
(3) a. Ion ??I ?1 ??
Thai president to China make visit
?The Thai president paid a visit to China.?
b. *Ion ?1 ????I
Thai president make visit to China
In order to model phenomena such as these, we
head-lexicalise our grammar by associating each
non-terminal node with the head word2 in the c-
structure tree along the head-projection line. A non-
terminal node is written as X(x), where x is the lex-
ical head of X. The example generation grammar
rule in the lexicalised model is shown in the last line
(LEX-PCFG) of Table 1.
As in CKY chart parsing, generation grammars
are binarised in our chart generator. Thus all gram-
mar rules are either unary of the form X ? H or
binary X ? Y H (or X ? HY ), where H is the
head constituent and Y is the modifier. To handle the
problem of sparse data while estimating rule proba-
bilities, a back-off to baseline model is employed.
As, from a linguistic perspective, it is the modifier
2We use a mechanism similar to (Collins, 1997) but adapted
to Chinese data to find lexical heads in the treebank data.
rather than the head word which plays the main role
in determining word order, a back-off to partial lexi-
calisation on the modifier only is also used for bi-
nary rules. As a result, the probabilities of lexi-
calised unary and binary CFG rules are calculated
as in Eq. (4) and Eq. (5), respectively.
Pbk(H(h)|X(h)) = ?1P (H(h)|X(h))
+?2P (H |X) (4)
Pbk(Y (y)H(h)|X(h)) = ?1P (Y (y)H(h)|X(h))
+?2P (Y (y)H |X) + ?3P (Y H |X) (5)
where
?
i=1
?i = 1
In principle, grammars binarisation from left-to-
right (left-) or from right-to-left (right-) are equiva-
lent to represent the original grammar and the prob-
ability distributions. However the head word is the
final constituent for most phrasal categories in Chi-
nese.3 In lexicalised model, the head word imme-
diately projects to the top level in a left-binary tree,
and as a result, the intermediate NP nodes cannot
be lexicalised with the head word as illustrated in
Figure (2b). By contrast, right-binary rules are lex-
icalised and the head word is percolated from the
bottom of the tree (Figure (2c)). Therefore we adopt
the right binarisation method in our generation algo-
rithm.
4 Chart Generation and Smoothing
Algorithms
4.1 Chart Generation Algorithm
The PCFG-based generation algorithms are imple-
mented in terms of a chart generator (Kay, 1996).
In the generation algorithm, each (sub-)f-structure
indexes a (sub-)chart. Each local chart generates
the most probable trees for the local f-structure in
a bottom-up manner:
? generating lexical edges from the the local GF
PRED and some atomic features representing
function words, mood or aspect etc.
3Except for prepositional phrases, localiser and some verbal
phrases.
89
NP(m)
NR
[???ADJUNCT]
??
Shanghai
NN
[???ADJUNCT]
?
tennis
NN
[???ADJUNCT]
??
masters
NN
[?=?]
m
cup
(a.) the original tree
NP(m)
NP(null)
[?=?]
NP(null)
[?=?]
NR
[???ADJUNCT]
??
Shanghai
NN
[???ADJUNCT]
?
tennis
NN
[???ADJUNCT]
??
masters
NN
[?=?]
m
cup
NP(m)
NR
[???ADJUNCT]
??
Shanghai
NP(m)
[?=?]
NN
[???ADJUNCT]
?
tennis
NP(m)
[?=?]
NN
[???ADJUNCT]
??
masters
NN
[?=?]
m
cup
(b.) left-binarisation (c.) right-binarisation
Figure 2: Lexicalised binary trees
? applying unary rules and binary rules to gener-
ate new edges until no any new edges can be
generated in the current local chart.
? propagating compatible edges to the upper-
level chart.
For efficiency, the generation algorithm does
Viterbi-pruning for each local chart, viz. if two
edges have equivalent categories and lexical cover-
age, only the most probable one is kept.
The generation coverage is impacted on by un-
known words4 and unmatched grammar rules in
chart generation. We present a lexical smoothing
and a rule smoothing strategy in the following sub-
sections.
4.2 Lexical Smoothing
In LFG f-structure, the surface form of the lemma
is represented via lexical rules involving a particular
set of features, e.g. the lemma ?on /president? is
represented as {?PRED=?on ?, ?NTYPE=common,
?NUM=sg}. Particular lexical rules can be cap-
tured in general lexical macros abstracting away
4We use unknown words as a cover term to refer to all words
occurring in the test set but not in the training set.
from particular surface forms to lemmas, e.g. the
lexical macro encapsuling the above lexical rule is
{?PRED=$LEMMA, ?NTYPE=common, ?NUM=sg},
which generally associates to common nouns NN in
the CTB. According to the assumption that unknown
words have a probability distribution similar to ha-
pax legomenon (Baayen and Sproat, 1996), we pre-
dict the part-of-speech of unknown words from in-
frequent words in the training set by automatically
extracting lexical macros corresponding to the par-
ticular set of f-structure features. The probability of
the potential POS tag t associated to a feature set f
is estimated according to Eq. (6).
P (t|f) = count(t, f)?n
i=1 count(ti, f)
(6)
4.3 Rule Smoothing
The coverage of grammar rules increases with the
size of training data and in theory all the rules can
be fully covered by a training set, if it is big enough.
With limited training resources we have to resort to
fuzzy matching of grammar rules. Two smoothing
strategies are carried out at the level of grammar
rules.
90
Mathched Grammar Rule
Nonsmooth VP[?=?] ? VV[?=?] NP[?OBJ=?], {SUBJ, OBJ, PRED}
Feature smooth VP[?=?] ? VV[?=?] NP[?OBJ=?]
Partial match VP ? VV [?OBJ=?], {SUBJ, OBJ, PRED}
Table 2: Smoothing of CFG rules
? Reducing the conditioning f-structure features
during rule matching;
? Applying partial match during rule application.
A node in each unlexicalised grammar rule X ?
Y H includes two parts: constituent category c, such
as IP, NP, VP etc.; functional f-structure annotation
a, such as [?SUBJ=?], [?=?] etc. As a heuristic based
on linguistic experience, we define the order of im-
portance of these elements as follows:
X(c) > H(c) > Y (a) > Y (c) > X(a) > H(a)
(4) IP[?COMP=?] ? NP[?SUBJ=?] VP[?=?]
For the above example rule (4), the importance of
the elements is:
IP > VP > [?SUBJ=?] > NP > [?COMP=?] > [?=?]
The elements can be deleted from the rules in an im-
portance order from low to high.5 The partial rules
adopted in our system ignore the least important 3
elements, viz. the functional annotation of the head
node H(a), the functional annotation on LHS X(a)
and constituent category of the modifier node Y (c).
Examples of the two types of smoothed rules are
shown in Table 2.
5 Experimental Results
Our experiments are carried out on the newly
released Penn Chinese treebank version 6.0
(CTB6) (Xue et al, 2005), excluding the portion of
ACE broadcast news. We follow the recommended
splits (in the list-of-file of CTB6) to divide the
data into test set, development set and training set.
The training set includes 756 files with a total of
15,663 sentences. The CTB trees of the training set
were automatically annotated with LFG f-structure
equations following (Guo et al, 2007). Table 3
shows the number of different grammar rule types
extracted from the training set. From the test files,
5However c and a on the same node can?t be deleted at the
same time.
we randomly select 500 sentences as test data
with minimal sentence length 5 words, maximal
length 80 words, and average length 28.84 words.
The development set alo includes 500 sentences
randomly selected from the development files with
sentence length between 5 and 80 words. The
c-structure trees of the test and development data
were also automatically converted to f-structures as
input to the generator.
Type with features without features
PCFG 22,372 8,548
HB-PCFG 28,487 11,969
LEX-PCFG 325,094 286,468
Table 3: Number of rules in the training set
The generation system is evaluated against the
raw text of the test data in terms of accuracy and cov-
erage. Following (Langkilde, 2002) and other work
on general-purpose generators, we adopt BLEU
score (Papineni et al, 2002), average simple string
accuracy (SSA) and percentage of exactly matched
sentences for accuracy evaluation.6 For coverage
evaluation, we measure the percentage of input f-
structures that generate a sentence.
Table 4 reports the initial experiments on the sim-
ple PCFG, HB-based PCFG and lexicalised PCFG
models. The results in the left column evaluate all
input f-structures, the right column evaluate only
those f-structures which yield a complete sentence.
The results show that the lexicalised model outper-
forms the baseline PCFG model. The HB model is
the most accurate for complete sentences, but with
reduced coverage compared to the other two mod-
els. However the low coverage of sentences com-
pletely generated due to unknown words and un-
matched rules makes the results unusable in prac-
6We are aware of the limitations in fully automatic evalua-
tion metrics, and in an ideal scenario, we would complement the
BLEU and SSA scores by a human evaluation. Unfortunately,
this is beyond the scope of the current paper.
91
All Output Strings Complete Output Sentences
Coverage ExMatch BLEU SSA Coverage ExMatch BLEU SSA
PCFG 100% 7.2% 0.5401 0.6261 36.40% 19.78% 0.7101 0.7687
HB-PCFG 100% 8.60% 0.5474 0.6281 34.80% 24.71% 0.7513 0.8092
LEX-PCFG 100% 9.40% 0.5687 0.6537 37.00% 25.41% 0.7431 0.8024
Table 4: Results without smoothing
All Output Strings Complete Output Sentences
Coverage ExMatch BLEU SSA Coverage ExMatch BLEU SSA
PCFG 100% 11.00% 0.6894 0.7240 94.20% 11.68% 0.7047 0.7388
HB-PCFG 100% 11.80% 0.7108 0.7348 94.00% 12.55% 0.7284 0.7506
LEX-PCFG 100% 14.00% 0.7152 0.7595 94.40% 14.83% 0.7302 0.7754
Table 5: Results with lexical smoothing
Partial match Feature smooth
Complete Sentences Coverage ExMatch BLEU SSA Coverage ExMatch BLEU SSA
PCFG 97.20% 11.32% 0.7022 0.7356 100% 11.20% 0.7021 0.7330
HB-PCFG 96.20% 12.27% 0.7263 0.7458 100% 12.00% 0.7245 0.7413
LEX-PCFG 97.80% 14.31% 0.7265 0.7696 100% 14.20% 0.7265 0.7675
Table 6: Results with lexical and rule smoothing
tice.
Table 5 gives the results with lexical smoothing.
The coverage for complete sentences increases by
nearly 60% absolute for all models. The increased
coverage also improves the overall results evaluated
against all sentences. The HB model performs better
than the simple PCFG model in nearly all respects
and in turn the lexicalised model comprehensively
outperforms the HB model.
The final results with both lexical smoothing and
rule smoothing by two different strategies are tabu-
lated in Table 6. The left column provides the results
of smoothing by partial match and the right column
the results by reducing conditioning f-structure fea-
tures. All results are evaluated for completely gen-
erated sentences only. The feature smoothing re-
sults in a full coverage of 100%, while slightly de-
grading the quality of sentences generated compared
with partial match smoothing. We feel the tradeoff
at the cost of a small decrease in quality is still worth
the full coverage. Throughout the experiments, the
lexicalised model exhibits consistently better perfor-
mance than the unlexicalised models, which proves
our intuition that successful techniques in parsing
also work well in generation.
6 Conclusion and Further Work
We have presented an accurate, robust chart genera-
tor for Chinese based on treebank-based, automati-
cally acquired LFG resources. Our model improves
the baseline provided by (Cahill and van Genabith,
2006): (i) accuracy is increased by creating a lexi-
calised PCFG grammar and enriching conditioning
context with parent f-structure features; and (ii) cov-
erage is increased by providing lexical smoothing
and fuzzy matching techniques for rule smoothing.
The combinational explosion of grammar rules
encountered in the chart generator is similar to that
in parsing. In the current system, we only keep the
most probable realisation for each input f-structure.
An alternative model in line with the generate-and-
select paradigm, would pack all the locally equiva-
lent edges in a forest and re-rank all the realisations
by a separate language model. This might help us to
reduce some errors caused in our current model, for
instance, the generation of function words in fixed
phrases. As shown in ex. (5), the function word
??? is incorrectly generated as ??. This is be-
cause they share the same part-of-speech DEG in
CTB, however ?? has a much higher frequency
than ??? in Chinese text and thus has a higher prob-
ability to be generated.
92
(5) a. ? ? ?
all things DE in
?among all things?
b. *?  ?
all things DE in
Acknowledgments
The research reported in this paper is supported by
Science Foundation Ireland grant 04/IN/I527. Also,
we would like to thank Aoife Cahill for many help-
ful and insightful discussions on the work. And we
gratefully acknowledge the anonymous reviewers.
References
Harald Baayen and Richard Sproat. 1996. Estimat-
ing Lexical Priors for Low-Frequency Morphologi-
cally Ambiguous Forms. Computational Linguistics,
22(2): 155?166.
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a Probabilistic Hierarchical Model for Gen-
eration. Proceedings of the 18th International Con-
ference on Computational Linguistics, pages 42?48.
Saarbru?cken, Germany.
Daniel M. Bikel. 2004. On the Parameter Space of Gen-
erative Lexicalized Statistical Parsing Models. Ph.D.
Thesis of Department of Computer & Information Sci-
ence, University of Pennsylvania.
Aoife Cahill and Josef van Genabith. 2006. Robust
PCFG-Based Generation Using Automatically Ac-
quired LFG Approximations. Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 1033?1040. Syd-
ney, Australia.
Aoife Cahill and Martin Forst and Christian Rohrer.
2007. Stochastic Realisation Ranking for a Free Word
Order Language. Proceedings of the 11th European
Workshop on Natural Language Generation, pages
17?24. Schloss Dagstuhl, Germany.
John Carroll and Ann Copestake and Dan Flickinger and
Victor Poznanski. 1999. An efficient chart genera-
tor for (semi-)lexicalist grammars. Proceedings of the
7th European Workshop on Natural Language Gener-
ation, pages 86?95. Toulouse, France.
Hailong Cao and Yujie Zhang and Hitoshi Isahara. 2007.
Empirical study on Parsing Chinese Based on Collins?
Model. Proceedings of the 10th Conference of the Pa-
cific Association for Computational Linguistics, pages
113?119. Melbourne, Australia.
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. Proceedings of the 1st Annual Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, pages 132?139. Seattle, WA.
Michael Collins. 1997. Three Generative, Lexicalized
Models for Statistical Parsing. Proceedings of the 35th
Annual Meeting of the Association for Computational
Linguistics, pages 16?23. Madrid, Spain.
Dick Crouch and Mary Dalrymple and Ron Kaplan and
Tracy King and John Maxwell and Paula Newman.
2007. XLE Documentation. Palo Alto Research Cen-
ter, CA.
Yuqing Guo and Josef van Genabith and Haifeng Wang.
2007. Treebank-based Acquisition of LFG Resources
for Chinese. Proceedings of LFG07 Conference,
pages 214?232. Stanford, CA, USA.
Deirdre Hogan and Conor Cafferkey and Aoife Cahill
and Josef van Genabith. 2007. Exploiting Multi-Word
Units in History-Based Probabilistic Generation. Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 267?276.
Prague, Czech Republic.
Mark Johnson. 1998. PCFG Models of Linguistic Tree
Representations. Computational Linguistics, 24(4):
613?632. MIT Press, Cambridge, MA,
Ronald M. Kaplan. 1995. The formal architecture of
lexical-functional grammar. Formal Issues in Lexical-
Functional Grammar, pages 7?27. CSLI Publications,
Standford, USA.
Ronald M. Kaplan and Joan Bresnan. 1982. Lexical
Functional Grammar: a Formal System for Grammat-
ical Representation. The Mental Representation of
Grammatical Relations, pages 173-282. MIT Press,
Cambridge, MA.
Ronald M. Kaplan and Jurgen Wedekind. 2000.
LFG Generation Produces Context-free Languages.
Proceedings of the 18th International Conference
on Computational Linguistics, pages 425?431.
Saarbru?cken, Germany.
Martin Kay. 1996. Chart Generation. Proceedings of the
34th Annual Meeting of the Association for Computa-
tional Linguistics, pages 200?204. Santa Cruz, USA.
Irene Langkilde. 2000. Forest-Based Statistical Sen-
tence Generation. Proceedings of 1st Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, pages 170?177. Seattle, WA.
Langkilde, Irene. 2002. An Empirical Verification of
Coverage and Correctness for a General-Purpose Sen-
tence Generator. Proceedings of the Second Interna-
tional Conference on Natural Language Generation,
17?24. New York, USA.
93
Hiroko Nakanishi and Yusuke Nakanishi and Jun?ichi
Tsujii. 2005. Probabilistic Models for Disambigua-
tion of an HPSG-Based Chart Generator. Proceedings
of the 9th International Workshop on Parsing Technol-
ogy, pages 93?102. Vancouver, British Columbia.
Kishore Papineni and Salim Roukos and Todd Ward and
Wei-Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311-318. Philadelphia,
USA.
Erik Velldal and Stephan Oepen. 2005. Maximum en-
tropy models for realization ranking. Proceedings of
the MTSummit ?05.
Michael White. 2004. Reining in CCG Chart Realiza-
tion. Proceedings of the third International Natural
Language Generation Conference. Hampshire, UK.
Michael White and Rajakrishnan Rajkumar and Scott
Martin. 2007. Towards Broad Coverage Surface Re-
alization with CCG. Proceedings of the MT Summit
XI Workshop on Language Generation and Machine
Translation, pages 22?30. Copenhagen, Danmark.
Nianwen Xue and Fei Xia and Fu dong Chiou and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2): 207?238.
94
