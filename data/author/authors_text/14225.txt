Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 317?320,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Time-Efficient Creation of an Accurate Sentence Fusion Corpus
Kathleen McKeown, Sara Rosenthal, Kapil Thadani and Coleman Moore
Columbia University
New York, NY 10027, USA
{kathy,sara,kapil}@cs.columbia.edu, cjm2140@columbia.edu
Abstract
Sentence fusion enables summarization and
question-answering systems to produce out-
put by combining fully formed phrases from
different sentences. Yet there is little data
that can be used to develop and evaluate fu-
sion techniques. In this paper, we present a
methodology for collecting fusions of simi-
lar sentence pairs using Amazon?s Mechani-
cal Turk, selecting the input pairs in a semi-
automated fashion. We evaluate the results
using a novel technique for automatically se-
lecting a representative sentence from multi-
ple responses. Our approach allows for rapid
construction of a high accuracy fusion corpus.
1 Introduction
Summarization and question-answering systems
must transform input text to produce useful output
text, condensing an input document or document set
in the case of summarization and selecting text that
meets the question constraints in the case of question
answering. While many systems use sentence ex-
traction to facilitate the task, this approach risks in-
cluding additional, irrelevant or non-salient informa-
tion in the output, and the original sentence wording
may be inappropriate for the new context in which
it appears. Instead, recent research has investigated
methods for generating new sentences using a tech-
nique called sentence fusion (Barzilay and McKe-
own, 2005; Marsi and Krahmer, 2005; Filippova and
Strube, 2008) where output sentences are generated
by fusing together portions of related sentences.
While algorithms for automated fusion have been
developed, there is no corpus of human-generated
fused sentences available to train and evaluate such
systems. The creation of such a dataset could pro-
vide insight into the kinds of fusions that people
produce. Furthermore, since research in the related
task of sentence compression has benefited from
the availability of training data (Jing, 2000; Knight
and Marcu, 2002; McDonald, 2006; Cohn and La-
pata, 2008), we expect that the creation of this cor-
pus might encourage the development of supervised
learning techniques for automated sentence fusion.
In this work, we present a methodology for cre-
ating such a corpus using Amazon?s Mechanical
Turk1, a widely used online marketplace for crowd-
sourced task completion. Our goal is the generation
of accurate fusions between pairs of sentences that
have some information in common. To ensure that
the task is performed consistently, we abide by the
distinction proposed by Marsi and Krahmer (2005)
between intersection fusion and union fusion. In-
tersection fusion results in a sentence that contains
only the information that the sentences had in com-
mon and is usually shorter than either of the original
sentences. Union fusion, on the other hand, results
in a sentence that contains all information content
from the original two sentences. An example of in-
tersection and union fusion is shown in Figure 1.
We solicit multiple annotations for both union and
intersection tasks separately and leverage the differ-
ent responses to automatically choose a representa-
tive response. Analysis of the responses shows that
our approach yields 95% accuracy on the task of
union fusion. This is a promising first step and indi-
cates that our methodology can be applied towards
efficiently building a highly accurate corpus for sen-
tence fusion.
1https://www.mturk.com
317
1. Palin actually turned against the bridge project only after it
became a national symbol of wasteful spending.
2. Ms. Palin supported the bridge project while running for
governor, and abandoned it after it became a national scandal.
Intersection: Palin turned against the bridge project after it
became a national scandal.
Union: Ms. Palin supported the bridge project while running
for governor, but turned against it when it became a national
scandal and a symbol of wasteful spending.
Figure 1: Examples of intersection and union
2 Related Work
The combination of fragments of sentences on a
common topic has been studied in the domain of sin-
gle document summarization (Jing, 2000; Daume? III
and Marcu, 2002; Xie et al, 2008). In contrast to
these approaches, sentence fusion was introduced to
combine fragments of sentences with common infor-
mation for multi-document summarization (Barzilay
and McKeown, 2005). Automated fusion of sen-
tence pairs has since received attention as an inde-
pendent task (Marsi and Krahmer, 2005; Filippova
and Strube, 2008). Although generic fusion of sen-
tence pairs based on importance does not yield high
agreement when performed by humans (Daume? III
and Marcu, 2004), fusion in the context of a query
has been shown to produce better agreement (Krah-
mer et al, 2008). We examine similar fusion an-
notation tasks in this paper, but we asked workers
to provide two specific types of fusion, intersection
and union, thus avoiding the less specific definition
based on importance. Furthermore, as our goal is
the generation of corpora, our target for evaluation
is accuracy rather than agreement.
This work studies an approach to the automatic
construction of large fusion corpora using workers
through Amazon?s Mechanical Turk service. Previ-
ous studies using this online task marketplace have
shown that the collective judgments of many work-
ers are comparable to those of trained annotators
on labeling tasks (Snow et al, 2008) although these
judgments can be obtained at a fraction of the cost
and effort. However, our task presents an additional
challenge: building a corpus for sentence fusion re-
quires workers to enter free text rather than simply
choose between predefined options; the results are
prone to variation and this makes comparing and ag-
gregating multiple responses problematic.
A. After a decade on the job, Gordon had become an experi-
enced cop.
B. Gordon has a lot of experience in the police force.
Figure 2: An example of sentences that were judged to be
too similar for inclusion in the dataset
3 Collection Methodology
Data collection involved the identification of the
types of sentence pairs that would make suitable
candidates for fusion, the development of a sys-
tem to automatically identify good pairs and manual
filtering of the sentence pairs to remove erroneous
choices. The selected sentence pairs were then pre-
sented to workers on Mechanical Turk in an inter-
face that required them to manually type in a fused
sentence (intersection or union) for each case.
Not all pairs of related sentences are useful for the
fusion task. When sentences are too similar, the re-
sult of fusion is simply one of the input sentences.
For example (Fig. 2), if sentence A contains all the
information in sentence B but not vice versa, then
B is also their intersection while A is their union
and no sentence generation is required. On the other
hand, if the two sentences are too dissimilar, then
no intersection is possible and the union is just the
conjunction of the sentences.
We experimented with different similarity metrics
aimed at identifying pairs of sentences that were in-
appropriate for fusion. The sentences in this study
were drawn from clusters of news articles on the
same event from the Newsblaster summarization
system (McKeown et al, 2002). While these clus-
ters are likely to contain similar sentences, they will
contain many more dissimilar than similar pairs and
thus a metric that emphasizes precision over recall
is important. We computed pairwise similarity be-
tween sentences within each cluster using three stan-
dard metrics: word overlap, n-gram overlap and co-
sine similarity. Bigram overlap yielded the best pre-
cision in our experiments. We empirically arrived at
a lower threshold of .35 to remove dissimilar sen-
tences and an upper threshold of .65 to avoid near-
identical sentences, yielding a false-positive rate of
44.4%. The remaining inappropriate pairs were then
manually filtered. This semi-automated procedure
enabled fast selection of suitable sentence pairs: one
person was able to select 30 pairs an hour yielding
the 300 pairs for the full experiment in ten hours.
318
Responses Intersection Union
All (1500) 0.49 0.88
Representatives (300) 0.54 0.95
Table 1: Union and intersection accuracy
3.1 Using Amazon?s Mechanical Turk
Based on a pilot study with 20 sentence pairs, we
designed an interface for the full study. For inter-
section tasks, the interface posed the question ?How
would you combine the following two sentences into
a single sentence conveying only the information
they have in common??. For union tasks, the ques-
tion was ?How would you combine the following two
sentences into a single sentence that contains ALL of
the information in each??.
We used all 300 pairs of similar sentences for
both union and intersection and chose to collect five
worker responses per pair, given the diversity of
responses that we found in the pilot study. This
yielded a total of 3000 fused sentences with 1500
intersections and 1500 unions.
3.2 Representative Responses
Using multiple workers provides little benefit unless
we are able to harness the collective judgments of
their responses. To this end, we experiment with
a simple technique to select one representative re-
sponse from all responses for a case, hypothesizing
that such a response would have a lower error rate.
We test the hypothesis by comparing the accuracy of
representative responses with the average accuracy
over all responses.
Our strategy for selecting representatives draws
on the common assumption used in human com-
putation that human agreement in independently-
generated labels implies accuracy (von Ahn and
Dabbish, 2004). We approximate agreement be-
tween responses using a simple and transparent
measure for overlap: cosine similarity over stems
weighted by tf-idf where idf values are learned over
the Gigawords corpus2. After comparing all re-
sponses in a pairwise fashion, we need to choose a
representative response. As using the centroid di-
rectly might not be robust to the presence of er-
roneous responses, we first select the pair of re-
sponses with the greatest overlap as candidates and
2LDC Catalog No. LDC2003T05
Errors Intersection Union
Missing clause 2 7
Union/Intersection 46 6
S1/S2 21 8
Additional clause 10 1
Lexical 3 1
Table 2: Errors seen in 30 random cases (150 responses)
then choose the candidate which has the greatest to-
tal overlap with all other responses.
4 Results and Error Analysis
For evaluating accuracy, fused sentences were man-
ually compared to the original sentence pairs. Due to
the time-consuming nature of the evaluation, 50% of
the 300 cases were randomly selected for analysis.
10% were initially analyzed by two of the authors; if
a disagreement occurred, the authors discussed their
differences and came to a unified decision. The re-
maining 40% were then analyzed by one author. In
addition to this high-level analysis, we further ana-
lyzed 10% of the cases to identify the the types of
errors made in fusion as well as the techniques used
and the effect of task difficulty on performance.
The accuracy for intersection and union tasks is
shown in Table 1. For both tasks, accuracy of the se-
lected representatives significantly exceeded the av-
erage response accuracy. In our error analysis, we
found that workers often answered the intersection
task by providing a union, possibly due to a misin-
terpretation of the question. This caused intersection
accuracy to be significantly worse than union. We
analyzed the impact of this error by computing ac-
curacy on the first 30 cases (10%) without this error
and the accuracy for intersection increased 22%.
Error types were categorized as ?missing clause?,
?using union for intersection and vice versa?,
?choosing an input sentence (S1/S2)?, ?additional
clause? and ?lexical error?. Table 2 shows the num-
ber of occurrences of each in 10% of the cases.
We binned the sentence pairs according to
the difficulty of the fusion task for each pair
(easy/medium/hard) and found that performance
was not dependent on difficulty level; accuracy was
relatively similar across bins. We also observed that
workers typically performed fusion by selecting one
sentence as a base and removing clauses or merging
in additional clauses from the other sentence.
319
Figure 3: Number of cases in which x/5 workers pro-
vided accurate responses for fusion
In order to determine the benefit of using many
workers, we studied the number of workers who an-
swered correctly for each case. Figure 3 reveals that
2/5 or more workers (summing across columns) re-
sponded accurately in 99% of union cases and 82%
of intersection cases. The intersection results are
skewed due to the question misinterpretation issue
which, though it was the most common error, was
made by 3/5 workers only 17% of the time. Thus, in
the majority of the cases, accurate fusions can still
be found using the representative method.
5 Conclusion
We presented a methodology to build a fusion cor-
pus which uses semi-automated techniques to select
similar sentence pairs for annotation on Mechanical
Turk3. Additionally, we showed how multiple re-
sponses for each fusion task can be leveraged by au-
tomatically selecting a representative response. Our
approach yielded 95% accuracy for union tasks, and
while intersection fusion accuracy was much lower,
our analysis showed that workers sometimes pro-
vided unions instead of intersections and we sus-
pect that an improved formulation of the question
could lead to better results. Construction of the fu-
sion dataset was relatively fast; it required only ten
hours of labor on the part of a trained undergraduate
and seven days of active time on Mechanical Turk.
Acknowledgements
This material is based on research supported in part
by the U.S. National Science Foundation (NSF) un-
der IIS-05-34871 Any opinions, findings and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not necessarily
reflect the views of the NSF.
3The corpus described in this work is available at
http://www.cs.columbia.edu/?kathy/fusioncorpus
References
Regina Barzilay and Kathleen R. McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297?328.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
COLING, pages 137?144.
Hal Daume? III and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of
ACL, pages 449?456.
Hal Daume? III and Daniel Marcu. 2004. Generic sen-
tence fusion is an ill-defined summarization task. In
Proceedings of the ACL Text Summarization Branches
Out Workshop, pages 96?103.
Katja Filippova and Michael Strube. 2008. Sentence fu-
sion via dependency graph compression. In Proceed-
ings of EMNLP, pages 177?185.
Hongyan Jing. 2000. Sentence reduction for automatic
text summarization. In Proceedings of Applied Natu-
ral Language Processing, pages 310?315.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139(1):91?107.
Emiel Krahmer, Erwin Marsi, and Paul van Pelt. 2008.
Query-based sentence fusion is better defined and
leads to more preferred results than generic sentence
fusion. In Proceedings of ACL, pages 193?196.
Erwin Marsi and Emiel Krahmer. 2005. Explorations in
sentence fusion. In Proceedings of the European Work-
shopon Natural Language Generation, pages 109?117.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL, pages 297?304.
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news on
a daily basis with Columbia?s Newsblaster. In Pro-
ceedings of HLT, pages 280?285.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: Evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of EMNLP, pages
254?263.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of the
SIGCHI conference on Human Factors in Computing
Systems, pages 319?326.
Zhuli Xie, Barbara Di Eugenio, and Peter C. Nel-
son. 2008. From extracting to abstracting: Gener-
ating quasi-abstractive summaries. In Proceedings of
LREC, May.
320
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 763?772,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Age Prediction in Blogs: A Study of Style, Content, and Online
Behavior in Pre- and Post-Social Media Generations
Sara Rosenthal
Department of Computer Science
Columbia University
New York, NY 10027, USA
sara@cs.columbia.edu
Kathleen McKeown
Department of Computer Science
Columbia University
New York, NY 10027, USA
kathy@cs.columbia.edu
Abstract
We investigate whether wording, stylistic
choices, and online behavior can be used
to predict the age category of blog authors.
Our hypothesis is that significant changes
in writing style distinguish pre-social me-
dia bloggers from post-social media blog-
gers. Through experimentation with a
range of years, we found that the birth
dates of students in college at the time
when social media such as AIM, SMS text
messaging, MySpace and Facebook first
became popular, enable accurate age pre-
diction. We also show that internet writing
characteristics are important features for
age prediction, but that lexical content is
also needed to produce significantly more
accurate results. Our best results allow for
81.57% accuracy.
1 Introduction
The evolution of the internet has changed the
way that people communicate. The introduction
of instant messaging, forums, social networking
and blogs has made it possible for people of ev-
ery age to become authors. The users of these
social media platforms have created their own
form of unstructured writing that is best char-
acterized as informal. Even how people com-
municate has dramatically changed, with multi-
tasking increasing and responses generated im-
mediately. We should be able to exploit those
differences to automatically determine from blog
posts whether an author is part of a pre- or post-
social media generation. This problem is called
age prediction and raises two main questions:
? Is there a point in time that proves to be
a significantly better dividing line between
pre and post-social media generations?
? What features of communication most di-
rectly reveal the generation in which a blog-
ger was born?
We hypothesize that the dividing line(s) oc-
cur when people in generation Y1, or the millen-
nial generation, (born anywhere from the mid-
1970s to the early 2000s) were typical college-
aged students (18-22). We focus on this gen-
eration due to the rise of popular social media
technologies such as messaging and online social
networks sites that occurred during that time.
Therefore, we experimented with binary clas-
sification into age groups using all birth dates
from 1975 through 1988, thus including students
from generation Y who were in college during
the emergence of social media technologies. We
find five years where binary classification is sig-
nificantly more accurate than other years: 1977,
1979, and 1982-1984. The appearance of social
media technologies such as AOL Instant Messen-
ger (AIM), weblogs, SMS text messaging, Face-
book and MySpace occurred when people with
these birth dates were in college.
We explore two of these years in more detail,
1979 and 1984, and examine a wide variety of
1http://en.wikipedia.org/wiki/Generation Y
763
features that differ between the pre-social me-
dia and post-social media bloggers. We examine
lexical-content features such as collocations and
part-of-speech collocations, lexical-stylistic fea-
tures such as internet slang and capitalization,
and features representing online behavior such
as time of post and number of friends. We find
that both stylistic and content features have a
significant impact on age prediction and show
that, for unseen blogs, we are able to classify
authors as born before or after 1979 with 80%
accuracy and born before or after 1984 with 82%
accuracy.
In the remainder of this paper, we first dis-
cuss work to date on age prediction for blogs
and then present the features that we extracted,
which is a larger set than previously explored.
We then turn separately to three experiments.
In the first, we implement a prior approach to
show that we can produce a similar outcome. In
the second, we show how the accuracy of age
prediction changes over time and pinpoint when
major changes occur. In the last experiment, we
describe our age prediction experiments in more
detail for the most significant years.
2 Related Work
In previous work, Mackinnon (2006) , used Live-
Journal data to identify a blogger?s age by ex-
amining the mean age of his peer group using
his social network and not just his immediate
friends. They were able to predict the correct
age within +/-5 years at 98% accuracy. This ap-
proach, however, is very different from ours as it
requires access to the age of each of the blogger?s
friends. Our approach uses only a body of text
written by a person along with his blogging be-
havior to determine which age group he is more
closely identified with.
Initial research on predicting age without us-
ing the ages of friends focuses on identifying im-
portant candidate features, including blogging
characteristics (e.g., time of post), text features
(e.g., length of post), and profile information
(e.g., interests) (Burger and Henderson, 2006).
They aimed at binary prediction of age, classify-
ing LiveJournal bloggers as either over or under
18, but were unable to automatically predict age
with more accuracy than a baseline model that
always chose the majority class. In our study on
determining the ideal age split we did not find
18 (bloggers born in 1986 in their dataset) to be
significant.
Prior work by Schler et al (2006) has ex-
amined metadata such as gender and age in
blogger.com bloggers. In contrast to our work,
they examine bloggers based on their age at the
time of the experiment, whether in the 10?s, 20?s
or 30?s age bracket. They identify interesting
changes in content and style features across cat-
egories, in which they include blogging words
(e.g., ?LOL?), all defined by the Linguistic In-
quiry and Word Count (LIWC) (Pennebaker et
al., 2007). They did not use characteristics of
online behavior (e.g., friends). They can distin-
guish between bloggers in the 10?s and in the 30?s
with relatively high accuracy (above 96%) but
many 30s are misclassified as 20s, which results
in a overall accuracy of 76.2%. We re-implement
Schler et al?s work in section 5.1 with similar
findings. Their work shows that ease of classi-
fication is dependent in part on what division
is made between age groups and in turn moti-
vates our decision to study whether the creation
of social media technologies can be used to find
the dividing line(s). Neither Schler et al, nor
we, attempt to determine how a person?s writ-
ing changes over his lifespan (Pennebaker and
Stone, 2003; Robins et al, 2002). Goswami et
al. (2009) add to Schler et al?s approach using
the same data and have a 4% increase in accu-
racy. However, the paper is lacking details and
it is entirely unclear how they were able to do
this with fewer features than Schler et al
In other work, Tam and Martell (2009) at-
tempt to detect age in the NPS chat corpus be-
tween teens and other ages. They use an SVM
classifier with only n-grams as features. They
achieve > 90% accuracy when classifying teens
vs 30s, 40s, 50s, and all adults and achieve at
best 76% when using 3 character gram features
in classifying teens vs 20s. This work shows that
n-grams are useful features for detecting age and
it is difficult to detect differences between con-
secutive groups such as teens and 20s, and this
764
Figure 1: Number of bloggers in 2010 by year of birth
from 1950-1996. A minimal amount of data occurred
in years not shown.
provides evidence for the need to find a good
classification split.
Other researchers have investigated weblogs
for differences in writing style depending on gen-
der identification (Herring and Paolillo, 2006;
Yan and Yan, 2006; Nowson and Oberlander,
2006). Herring et al(2006) found that the typi-
cal gender related features were based on genre
and independent of author gender. Yan et al
(2006) used text categorization and stylistic web
features, such as emoticons, to identify gender
and achieved 60% F-measure. Nowson et al
(2006) employed dictionary and n-gram based
content analysis and achieved 91.5% accuracy
using an SVM classifier. We also use a super-
vised machine learning approach, but classifica-
tion by gender is naturally a binary classification
task, while our work requires determining a nat-
ural dividing point.
3 Data Collection
Our corpus consists of blogs downloaded from
the virtual community LiveJournal. We chose
to use LiveJournal blogs for our corpus because
the website provides an easy-to-use format in
XML for downloading and crawling their site.
In addition, LiveJournal gives bloggers the op-
portunity to post their age on their profile. We
take advantage of this feature by downloading
blogs where the user chooses to publicly provide
this metadata.
We downloaded approximately 24,500 Live-
Journal blogs containing age. We represent age
as the year a person was born and not his age
at the time of the experiment. Since technol-
ogy has different effects in different countries,
we only analyze the blogs of people who have
listed US as their country. It is possible that
text written in a language other than English
is included in our corpus. However, in a man-
ual check of a small portion of text from 500
blogs, we only found English words. Each blog
was written by a unique individual and includes
a user profile and up to 25 recent posts written
between 2000-2010 with the most recent post be-
ing written in 2009-2010. The birth dates of the
bloggers range in years from 1940 to 2000 and
thus, their age ranges from 10 to 70 in 2010. Fig-
ure 1 shows the number of bloggers per age in
our group with birth dates from 1950 to 1996.
The majority of bloggers on LiveJournal were
born between 1978-1989.
4 Methods
We pre-processed the data to add Part-of-
Speech tags (POS) and dependencies (de Marn-
effe et al, 2006) between words using the Stan-
ford Parser (Klein and Manning, 2003a; Klein
and Manning, 2003b). The POS and syntactic
dependencies were only found for approximately
the first 90 words in each sentence. Our classifi-
cation method investigates 17 different features
that fall into three categories: online behavior,
lexical-stylistic and lexical-content. All of the
features we used are explained in Table 1 along
with their trend as age decreases where applica-
ble. Any feature that increased, decreased, or
fluctuated should have some positive impact on
the accuracy of predicting age.
4.1 Online Behavior and Interests
Online behavior features are blog specific, such
as number of comments and friends as described
in Table 1.1. The first feature, interests, is our
only feature that is specific to LiveJournal. In-
terests appear in the LiveJournal user profile,
but are not found on all blog sites. All other
online behavior features are typically available
in any blog.
765
Feature Explanation Example Trend as Age
Decreases
1 Interests Top3 interests provided on the profile page2 disney N/A
2
# of Friends Number of friends the blogger has 45 fluctuates
# of Posts Number of downloadable posts (0-25) 23 decrease
# of Lifetime Posts Number of posts written in total 821 decrease
Time Mode hour (00-23) and day the blogger posts 11/Monday no change
Comments Average number of comments per post 2.64 increase
3
Emoticons number of emoticons1 :) increase
Acronyms number of internet acronyms1 lol increase
Slang number of words that are not found in the dictionary1 wazzup increase
Punctuation number of stand-alone punctuation1 ... increase
Capitalization number of words (with length > 1) that are all CAPS1 YOU increase
Sentence Length average sentence length 40 decrease
Links/Images number of url and image links1 www.site.com fluctuates
4
Collocations Top3 Collocations in the age group. to [] the N/A
Syntax Collocations Top3 Syntax Collocations in the age group. best friends N/A
POS Collocations Top3 Part-of-Speech Collocations in the age group. this [] [] VB N/A
Words Top3 words in the age group his N/A
Table 1: List of all features used during classification divided into three categories (1,2) online behavior and
interests, (3) lexical - content, and (4) lexical - stylistic 1 normalized per sentence per entry, 2 available in
LiveJournal only, 3 pruned from top 200 features to include those that do not occur within +/- 10 position
in any other age group
We extracted the top 200 interests based on
occurrence in the profile page from 1500 random
blogs in three age groups. These age groups are
used solely to illustrate the differences that oc-
cur at different ages and are not used in our
classification experiments. We then pruned the
list of interests by excluding any interest that
occurred within a +/-10 window (based on its
position in the list) in multiple age groups. We
show the top interests in each age group in Ta-
ble 2. For example, ?disney? is the most popu-
lar unique interest in the 18-22 age group with
only 39 other non-unique interests in that age
group occurring more frequently. ?Fanfiction?
is a popular interest in all age groups, but it
is significantly more popular in the 18-22 age
group than in other age groups.
Amongst the other online behavior features,
the number of friends tends to fluctuate but
seems to be higher for older bloggers. The num-
ber of lifetime posts (Figure 2(d)), and posts de-
creases as bloggers get younger which is as one
would expect unless younger people were orders
of magnitude more prolific than older people.
The mode time (Figure 2(b)), refers to the most
18-22 28-32 38-42
disney 39 tori amos 49 polyamory 40
yaoi 40 hiking 55 sca 67
johnny depp 42 women 61 babylon 5 84
rent 44 gaming 62 leather 94
house 45 comic books 67 farscape 103
fanfiction 11 fanfiction 58 fanfiction 138
drawing 10 drawing 25 drawing 65
sci-fi 199 sci-fi 37 sci-fi 21
Table 2: Top interests for three different age groups.
The top half refers to the top 5 interests that are
unique to each age group. The value refers to the
position of the interest in its list
common hour of posting from 00-24 based on
GMT time. We didn?t compute time based on
the time zone because city/state is often not in-
cluded. We found time to not be a useful feature
in this manner and it is difficult to come to any
conclusions from its change as year of birth de-
creases.
4.2 Lexical - Stylistic
The Lexical-Stylistic features in Table 1.2, such
as slang and sentence length, are computed us-
766
Figure 2: Examples of change to features over time (a) Average number of emoticons in a sentence increases
as age decreases (b) The most common time fluctuates until 1982, where it is consistent (c) The number
of links/images in a sentence fluctuates (d) The average number of lifetime posts per year decreases as age
decreases
ing the text from all of the posts written by the
blogger. Other than sentence length, they were
normalized by sentence and post to keep the
numbers consistent between bloggers regardless
of whether the user wrote one or many posts in
his/her blog. The number of emoticons (Figure
2(a)), acronyms, and capital words increased as
bloggers got younger. Slang and punctuation,
which excludes the emoticons and acronyms
counted in the other features, increased as well,
but not as significantly. The length of sentences
decreased as bloggers got younger and the num-
ber of links/images varied across all years as
shown in Figure 2(c).
4.3 Lexical - Content
The last category of features described in Ta-
ble 1.3 consists of collocations and words, which
are content based lexical terms. The top words
are produced using a typical ?bag-of-words? ap-
proach. The top collocations are computed us-
ing a system called Xtract (Smadja, 1993).
We use Xtract to obtain important lexical col-
locations, syntactic collocations, and POS col-
locations as features from our text. Syntac-
tic collocations refer to significant word pairs
that have specific syntactic dependencies such
as subject/verb and verb/object. Due to the
length of time it takes to run this program, we
ran Xtract on 1500 random blogs from each age
group and examined the first 1000 words per
blog. We looked at 1.5 million words in total
and found approximately 2500-2700 words that
were repeated more than 50 times.
We extracted the top 200 words and colloca-
tions sorted by post frequency (pf), which is the
number of posts the term occurred in. Then,
similarly to interests, we pruned each list to
include the features that did not occur within
+/-10 window (based on its position in the list)
within each age group. Prior to settling on these
metrics, we also experimented with other met-
rics such as the number of times the collocation
767
18-22 28-32 38-42
ldquot (?) 101 great 166 may 164
t 152 find 167 old 183
school 172 many 177 house 191
x 173 years 179 world 192
anything 175 week 181 please 198
maybe 179 post 190 - -
because 68 because 80 because 93
him 59 him 85 him 73
Table 3: Top words for three age groups. The top
half refers to the top 5 words that are unique to each
age group. The value refers to the position of the
interest in its list
occurred in total, defined as collocation or term
frequency (tf), the number of blogs the colloca-
tion occurred in, defined as blog frequency (bf),
and variations of TF*IDF (Salton and Buck-
ley, 1988) where we tried using inverse blog fre-
quency and inverse post frequency as the value
for IDF. In addition, we also experimented with
looking at a different number of important words
and collocations ranging from the top 100-300
terms and experimented without pruning. None
of these variations improved accuracy in our
experiments, however, and thus, were dropped
from further experimentation.
Table 3 shows the top words for each age
group; older people tend to use words such as
?house? and ?old? frequently and younger peo-
ple talk about ?school?.
In our analysis of the top collocations, we
found that younger people tend to use first per-
son singular (I,me) in subject position while
older people tend to use first person plural (we)
in subject position, both with a variety of verbs.
5 Experiments and Results
We ran three separate experiments to determine
how well we can predict age: 1. classifying into
three distinct age groups (Schler et al (2006)
experiment), 2. binary classification with the
split at each birth year from 1975-1988 and 3.
Detailed classification on two significant splits
from the second experiment.
We ran all of our experiments in Weka (Hall et
al., 2009) using logistic regression over 10 runs
of 10-fold cross-validation. All values shown are
blogger.com livejournal.com
download
year
2004 2010
# of Blogs 19320 11521
# of Posts1 1.4 million 256,000
# of words1 295 million 50 million
age 13?17 23?27 33?37 18?22 28?32 38?42
size 8240 8086 2994 3518 5549 2454
majority
baseline
43.8% (13-17) 48.2% (22-32)
Table 4: Statistics for Schler et al?s data (blog-
ger.com) vs our data (livejournal.com) 1 is approxi-
mate amount.
the averages of the accuracies from the 10 cross-
validation runs and all results were compared
for statistical significance using the t-test where
applicable.
We use logistic regression as our classifier be-
cause it has been shown that logistic regression
typically has lower asymptotic error than naive
Bayes for multiple classification tasks as well as
for text classification (Ng and Jordan, 2002).
We experimented with an SVM classifier and
found logistic regression to do slightly better.
5.1 Age Groups
The first experiment implements a variation of
the experiment done by Schler et al (2006).
The differences between the two datasets are
shown in Tables 4. The experiment looks at
three age groups containing a 5-year gap be-
tween each group. Intermediate years were not
included to provide clear differentiation between
the groups because many of the blogs have been
active for several years and this will make it less
common for a blogger to have posts that fall into
two age groups (Schler et al, 2006).
We did not use the same age groups as Schler
et al because very few blogs on LiveJournal, in
2010, are in the 13-17 age group. Many early de-
mographic studies (Perseus Development, 2004;
Herring et al, 2004) show teens as the dom-
inant age group in all blogs. However, more
recent studies (Nowson and Oberlander, 2006;
Lenhart et al, 2010) show that less teens blog.
Furthermore, an early study on the LiveJournal
768
Figure 3: Style vs Content: Accuracy from 1975-
1988 for Style (Online-Behavior+Lexical-Stylistic)
vs Content (BOW)
demographic (Kumar et al, 2004) reported that
28.6% of blogs are written by bloggers between
the ages 13-18 whereas based on the current de-
mographic statistics, in 20102, only 6.96% of
blogs are written by that age group and the
number of bloggers in the 31-36 age group in-
creased from 3.9% to 12.08%. We chose the later
age groups because this study is based on blogs
updated in 2009-10 which is 5-6 years later and
thus, the 13-17 age group is now 18-22 and so
on.
We use style-based (lexical-stylistic) and
content-based features (BOW, interests) to
mimic Schler et al?s experiment as closely as
possible and also experimented with adding
online-behavior features. Our experiment with
style-based and content-based features had an
accuracy of 57%. However, when we added
online-behavior, we increased our accuracy to
67%. A more detailed look at the better results
show that our accuracies are consistently 7%
lower than the original work but we have similar
findings; 18-22s are distinguishable from 38-42s
with accuracy of 94.5%, and 18-22s are distin-
guishable from 28-32s with accuracy of 80.5%.
However, many 38-42s are misclassified as 28-
32s with an accuracy of 72.1%, yielding overall
accuracy of 67%. Due to our findings, we believe
that adding online-behavior features to Schler et
al.?s dataset would improve their results as well.
2http://www.livejournal.com/stats.bml
5.2 Social Media and Generation Y
In the first experiment we used the current age
of a blogger based on when he wrote his last
post. However, the age of a person changes;
someone who was in one age group now will be
in a different age group in 5 years. Furthermore,
a blogger?s posts can fall into two categories de-
pending on his age at the time. Therefore, our
second experiment looks at year of birth instead
of age, as that never changes. In contrast to
Schler et al?s experiment, our division does not
introduce a gap between age groups, we do bi-
nary classification, and we use significantly less
data.
We approach age prediction as attempting to
identify a shift in writing style over a 14 year
time span from birth years 1975-1988:
For each year X = 1975-1988:
? get 1500 blogs (?33,000 posts) balanced across
years BEFORE X
? get 1500 blogs (?33,000 posts) balanced across
years IN/AFTER X
? Perform binary classification between blogs BE-
FORE X and IN/AFTER X
The experiment focuses on the range of birth
years of bloggers from 1975-1888 to identify at
what point in time, if any, shift(s) in writing
style occurred amongst college-aged students in
generation Y. We were motivated to examine
these years due to the emergence of social me-
dia technologies during that time. Furthermore,
research by Pew Internet (Zickuhr, 2010) has
found that this generation (defined as 1977-
1992 in their research) uses social networking,
blogs, and instant messaging more than their
elders. The experiment is balanced to ensure
that each birth year is evenly represented. We
balance the data by choosing a blogger consec-
utively from each birth year in the category, re-
peating these sweeps through the category until
we have obtained 1500 blogs. We chose to use
1500 blogs from each group because of process-
ing power, time constraints, and the amount of
blogs needed to reasonably sample the age group
at each split. Due to the extensive running time,
we only examined variations of a combination of
769
Figure 4: Style and Content: Accuracy from 1975-
1988 using BOW, Online Behavior, and Lexical-
Stylistic features
online-behavior, lexical-stylistic, and BOW fea-
tures.
We found accuracy to increase as year of birth
increases in various feature experiments which is
consistent with the trends we found while exam-
ining the distribution of features such as emoti-
cons and lifetime posts in Figure 2. We ex-
perimented with style and content features and
found that both help improve accuracy. Figure 3
shows that content helps more than style, but
style helps more as age decreases. However, as
shown in Figure 4, style and content combined
provided the best results. We found 5 years to
have significant improvement over all prior years
for p ? .0005: 1977, 1979, and 1982-1984.
Generation Y is considered the social me-
dia generation, so we decided to examine how
the creation and/or popularity of social media
technologies compared to the years that had a
change in writing style. We looked at many pop-
ular social media technologies such as weblogs,
messaging, and social networking sites. Figure 5
compares the significant years 1977,1979, and
1982-1984 against when each technology was
created or became popular amongst college aged
students. We find that all the technologies had
an effect on one or more of those years. AIM and
weblogs coincide with the earlier shifts at 1977
and 1979, SMS messaging coincide with both
the earlier and later shifts at 1979 and 1982,
and the social networking sites, MySpace and
Facebook coincide with the later shifts of 1982-
Figure 5: The impact of social media technologies:
The arrows correspond to the years that generation
Yers were college aged students. The highlighted
years represent the significant years. 1Year it be-
came popular (Urmann, 2009)
1984. On the other hand, web forums and Twit-
ter each coincide with only one outlying year
which suggests that either they had less of an
impact on writing style or, in the case of Twit-
ter, the change has not yet been transferred to
other writing forms.
5.3 A Closer Look: 1979 and 1984
Our final experiment provides a more detailed
explanation of the results using various feature
combinations when splitting pre- and post- so-
cial media bloggers by year of birth at two of
the significant years found in the previous sec-
tion; 1979 and 1984. The results for all of the
experiments described are shown in Table 5.
We experimented against two baselines, on-
line behavior and interests. We chose these two
features as baselines because they are both easy
to generate and not lexical in nature. We found
that we were able to exceed the baselines sig-
nificantly using a simple bag-of-words (BOW)
approach. This means the BOW does a better
job of picking topics than interests. We found
that including all 17 features did not do well, but
we were able to get good results using a subset
of the lexical features. We found the best re-
sults to have an accuracy of 79.96% and 81.57%
for 1979 and 1984 respectively using BOW, in-
terests, online behavior, and all lexical-stylistic
features.
In addition, we show accuracy without in-
terests since they are not always available.
770
Experiment 1979 1984
Online-Behavior 59.66 61.61
Interests 70.22 74.61
Lexical-Stylistic 65.382 67.282
Slang+Emoticons+Acronyms 60.572 62.102
Online-Behavior + Lexical-
Stylistic
67.162 71.312
Collocations + Syntax Colloca-
tions
53.471 73.452
POS-Collocations + POS-
Syntax Collocations
55.541 74.002
BOW 75.26 77.76
BOW+Online-Behavior 76.39 79.22
BOW + Online-Behavior +
Lexical-Stylistic
77.45 80.88
BOW + Online-Behavior +
Lexical-Stylistic + Syntax Collo-
cations
74.8 80.36
BOW + Online-Behavior
+ Lexical-Stylistic + POS-
Collocations + POS Syntax
Collocations
74.73 80.54
Online-Behavior + Interests +
Lexical-Stylistic
74.39 77.20
BOW + Online-Behavior + In-
terests + Lexical-Stylistic
79.96 81.57
All Features 71.26 74.072
Table 5: Feature Accuracy. The top portion refers to
the baselines. The best accuracies are shown in bold.
Unless otherwise marked, all accuracies are statisti-
cally significant at p<=.0005 for both baselines. 1
not statistically significant over Online-Behavior and
Interests. 2 not statistically significant over Interests.
BOW, online-behavior, and lexical-stylistic fea-
tures combined did best achieving accuracy of
77.45% and 80.88% in 1979 and 1984 respec-
tively. This indicates that our classification
method could work well on blogs from any web-
site. It is interesting to note that colloca-
tions and POS-collocations were useful, but only
when we use 1984 as the split which implies that
bloggers born in 1984 and later are more homo-
geneous.
6 Conclusion and Future Work
We have shown that it is possible to predict the
age group of a person based on style, content,
and online behavior features with good accu-
racy; these are all features that are available
in any blog. While features representing writ-
ing practices that emerged with social media
(e.g., capitalized words, abbreviations, slang)
do not significantly impact age prediction on
their own, these features have a clear change of
value across time, with post-social media blog-
gers using them more often. We found that
the birth years that had a significant change
in writing style corresponded to the birth dates
of college-aged students at the time of the cre-
ation/popularity of social media technologies,
AIM, SMS text messaging, weblogs, Facebook
and MySpace.
In the future we plan on using age and other
metadata to improve results in larger tasks such
as identifying opinion, persuasion and power
by targeting our approach in those tasks to
the identified age of the person. Another ap-
proach that we will experiment with is the use
of ranking, regression, and/or clustering to cre-
ate meaningful age groups.
7 Acknowledgements
This research was funded by the Office of the
Director of National Intelligence (ODNI), In-
telligence Advanced Research Projects Activity
(IARPA), through the U.S. Army Research Lab.
All statements of fact, opinion or conclusions
contained herein are those of the authors and
should not be construed as representing the of-
ficial views or policies of IARPA, the ODNI or
the U.S. Government.
References
Shlomo Argamon, Moshe Koppel, Jonathan Fine,
and Anat Rachel Shimoni. 2003. Gender, genre,
and writing style in formal written texts. TEXT,
23:321?346.
John D. Burger and John C. Henderson. 2006. An
exploration of observable features related to blog-
ger age. In AAAI Spring Symposia.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In In LREC 2006.
Sumit Goswami, Sudeshna Sarkar, and Mayur
Rustagi. 2009. Stylometric analysis of bloggers?
771
age and gender. In International AAAI Confer-
ence on Weblogs and Social Media.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
Susan C. Herring and John C. Paolillo. 2006. Gen-
der and genre variation in weblogs. Journal of
Sociolinguistics, 10(4):439?459.
Susan C. Herring, L.A. Scheidt, S. Bonus, and
E. Wright. 2004. Bridging the gap: A genre anal-
ysis of weblogs. In Proceedings of the 37th Hawaii
International Conference on System Sciences.
Dan Klein and Christopher D. Manning. 2003a. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 423?430.
Dan Klein and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems, volume 15. MIT Press.
Ravi Kumar, Jasmine Novak, Prabhakar Raghavan,
and Andrew Tomkins. 2004. Structure and evolu-
tion of blogspace. Commun. ACM, 47:35?39, De-
cember.
Amanda Lenhart, Kristen Purcell, Aaron Smith, and
Kathryn Zickuhr. 2010. Social media and young
adults.
Ian Mackinnon. 2006. Age and geographic inferences
of the livejournal social network. In In Statistical
Network Analysis Workshop.
Andrew Y Ng and Michael I Jordan. 2002. On dis-
criminative vs. generative classifiers: A compari-
son of logistic regression and naive bayes. Neural
Information Processing Systems, 2:841?848.
Scott Nowson and Jon Oberlander. 2006. The iden-
tity of bloggers: Openness and gender in personal
weblogs.
James W Pennebaker and Lori D Stone. 2003.
Words of wisdom: language use over the life span.
J Pers Soc Psychol, 85(2):291?301.
J.W. Pennebaker, R.E. Booth, and M.E. Fran-
cis. 2007. Linguistic inquiry and word count:
Liwc2007 ? operator?s manual. Technical report,
LIWC, Austin, TX.
Perseus Development. 2004. The blogging iceberg:
Of 4.12 million hosted weblogs, most little seen
and quickly abandoned. Technical report, Perseus
Development.
R.W. Robins, K. H. Trzesniewski, J.L. Tracy, S.D
Gosling, and J Potter. 2002. Global self-esteem
across the lifespan. Psychology and Aging, 17:423?
434.
Gerard Salton and Christopher Buckley. 1988.
Term-weighting approaches in automatic text re-
trieval. In Information Processing and Manage-
ment, pages 513?523.
J. Schler, M. Koppel, S. Argamon, and J. Pen-
nebaker. 2006. Effects of age and gender on blog-
ging. In AAAI Spring Symposium on Computa-
tional Approaches for Analyzing Weblogs.
Frank Smadja. 1993. Retrieving collocations from
text: Xtract. Computational Linguistics, 19:143?
177.
Jenny Tam and Craig H. Martell. 2009. Age detec-
tion in chat. In Proceedings of the 2009 IEEE In-
ternational Conference on Semantic Computing,
ICSC ?09, pages 33?39, Washington, DC, USA.
IEEE Computer Society.
David H. Urmann. 2009. The history of text mes-
saging.
Xiang Yan and Ling Yan. 2006. Gender classification
of weblog authors. In AAAI Spring Symposium
Series on Computation Approaches to Analyzing
Weblogs, pages 228?230.
Kathryn Zickuhr. 2010. Generations 2010.
772
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 312?320, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 2: Sentiment Analysis in Twitter
Preslav Nakov
QCRI, Qatar Foundation
pnakov@qf.org.qa
Zornitsa Kozareva
USC Information Sciences Institute
kozareva@isi.edu
Alan Ritter
University of Washington
aritter@cs.washington.edu
Sara Rosenthal
Columbia University
sara@cs.columbia.edu
Veselin Stoyanov
JHU HLTCOE
ves@cs.jhu.edu
Theresa Wilson
JHU HLTCOE
taw@jhu.edu
Abstract
In recent years, sentiment analysis in social
media has attracted a lot of research interest
and has been used for a number of applica-
tions. Unfortunately, research has been hin-
dered by the lack of suitable datasets, com-
plicating the comparison between approaches.
To address this issue, we have proposed
SemEval-2013 Task 2: Sentiment Analysis in
Twitter, which included two subtasks: A, an
expression-level subtask, and B, a message-
level subtask. We used crowdsourcing on
Amazon Mechanical Turk to label a large
Twitter training dataset alng with additional
test sets of Twitter and SMS messages for both
subtasks. All datasets used in the evaluation
are released to the research community. The
task attracted significant interest and a total
of 149 submissions from 44 teams. The best-
performing team achieved an F1 of 88.9% and
69% for subtasks A and B, respectively.
1 Introduction
In the past decade, new forms of communication,
such as microblogging and text messaging have
emerged and become ubiquitous. Twitter messages
(tweets) and cell phone messages (SMS) are often
used to share opinions and sentiments about the sur-
rounding world, and the availability of social con-
tent generated on sites such as Twitter creates new
opportunities to automatically study public opinion.
Working with these informal text genres presents
new challenges for natural language processing be-
yond those encountered when working with more
traditional text genres such as newswire.
Tweets and SMS messages are short in length: a
sentence or a headline rather than a document. The
language they use is very informal, with creative
spelling and punctuation, misspellings, slang, new
words, URLs, and genre-specific terminology and
abbreviations, e.g., RT for re-tweet and #hashtags.1
How to handle such challenges so as to automati-
cally mine and understand the opinions and senti-
ments that people are communicating has only very
recently been the subject of research (Jansen et al,
2009; Barbosa and Feng, 2010; Bifet et al, 2011;
Davidov et al, 2010; O?Connor et al, 2010; Pak and
Paroubek, 2010; Tumasjan et al, 2010; Kouloumpis
et al, 2011).
Another aspect of social media data, such as Twit-
ter messages, is that they include rich structured in-
formation about the individuals involved in the com-
munication. For example, Twitter maintains infor-
mation about who follows whom. Re-tweets (re-
shares of a tweet) and tags inside of tweets provide
discourse information. Modeling such structured in-
formation is important because it provides means for
empirically studying social interactions where opin-
ion is conveyed, e.g., we can study the properties of
persuasive language or those associated with influ-
ential users.
Several corpora with detailed opinion and senti-
ment annotation have been made freely available,
e.g., the MPQA corpus (Wiebe et al, 2005) of
newswire text. These corpora have proved very
valuable as resources for learning about the lan-
guage of sentiment in general, but they did not focus
on social media.
1Hashtags are a type of tagging for Twitter messages.
312
Twitter RT @tash jade: That?s really sad, Charlie RT ?Until tonight I never realised how fucked up I was? -
Charlie Sheen #sheenroast
SMS Glad to hear you are coping fine in uni... So, wat interview did you go to? How did it go?
Table 1: Examples of sentences from each corpus that contain subjective phrases.
While some Twitter sentiment datasets have al-
ready been created, they were either small and pro-
prietary, such as the i-sieve corpus (Kouloumpis
et al, 2011), or they were created only for Span-
ish like the TASS corpus2 (Villena-Roma?n et al,
2013), or they relied on noisy labels obtained from
emoticons and hashtags. They further focused on
message-level sentiment, and no Twitter or SMS
corpus with expression-level sentiment annotations
has been made available so far.
Thus, the primary goal of our SemEval-2013 task
2 has been to promote research that will lead to a
better understanding of how sentiment is conveyed
in Tweets and SMS messages. Toward that goal,
we created the SemEval Tweet corpus, which con-
tains Tweets (for both training and testing) and SMS
messages (for testing only) with sentiment expres-
sions annotated with contextual phrase-level polar-
ity as well as an overall message-level polarity. We
used this corpus as a testbed for the system evalua-
tion at SemEval-2013 Task 2.
In the remainder of this paper, we first describe
the task, the dataset creation process, and the evalu-
ation methodology. We then summarize the charac-
teristics of the approaches taken by the participating
systems and we discuss their scores.
2 Task Description
We had two subtasks: an expression-level subtask
and a message-level subtask. Participants could
choose to participate in either or both subtasks. Be-
low we provide short descriptions of the objectives
of these two subtasks.
Subtask A: Contextual Polarity Disambiguation
Given a message containing a marked instance
of a word or a phrase, determine whether that
instance is positive, negative or neutral in that
context. The boundaries for the marked in-
stance were provided: this was a classification
task, not an entity recognition task.
2http://www.daedalus.es/TASS/corpus.php
Subtask B: Message Polarity Classification
Given a message, decide whether it is of
positive, negative, or neutral sentiment. For
messages conveying both a positive and a
negative sentiment, whichever is the stronger
one was to be chosen.
Each participating team was allowed to submit re-
sults for two different systems per subtask: one con-
strained, and one unconstrained. A constrained sys-
tem could only use the provided data for training,
but it could also use other resources such as lexi-
cons obtained elsewhere. An unconstrained system
could use any additional data as part of the training
process; this could be done in a supervised, semi-
supervised, or unsupervised fashion.
Note that constrained/unconstrained refers to the
data used to train a classifier. For example, if other
data (excluding the test data) was used to develop
a sentiment lexicon, and the lexicon was used to
generate features, the system would still be con-
strained. However, if other data (excluding the test
data) was used to develop a sentiment lexicon, and
this lexicon was used to automatically label addi-
tional Tweet/SMS messages and then used with the
original data to train the classifier, then such a sys-
tem would be unconstrained.
3 Dataset Creation
In the following sections we describe the collection
and annotation of the Twitter and SMS datasets.
3.1 Data Collection
Twitter is the most common micro-blogging site on
the Web, and we used it to gather tweets that express
sentiment about popular topics. We first extracted
named entities using a Twitter-tuned NER system
(Ritter et al, 2011) from millions of tweets, which
we collected over a one-year period spanning from
January 2012 to January 2013; we used the public
streaming Twitter API to download tweets.
313
Instructions: Subjective words are ones which convey an opinion. Given a sentence, identify whether it is objective,
positive, negative, or neutral. Then, identify each subjective word or phrase in the context of the sentence and mark
the position of its start and end in the text boxes below. The number above each word indicates its position. The
word/phrase will be generated in the adjacent textbox so that you can confirm that you chose the correct range.
Choose the polarity of the word or phrase by selecting one of the radio buttons: positive, negative, or neutral. If a
sentence is not subjective please select the checkbox indicating that ?There are no subjective words/phrases?. Please
read the examples and invalid responses before beginning if this is your first time answering this hit.
Figure 1: Instructions provided to workers on Mechanical Turk followed by a screenshot.
Average # of Total Phrase Count Vocabulary
Corpus Words Characters Positive Negative Neutral Size
Twitter - Training 25.4 120.0 5,895 3,131 471 20,012
Twitter - Dev 25.5 120.0 648 430 57 4,426
Twitter - Test 25.4 121.2 2,734 1,541 160 11,736
SMS - Test 24.5 95.6 1,071 1,104 159 3,562
Table 2: Statistics for Subtask A.
We then identified popular topics as those named
entities that are frequently mentioned in association
with a specific date (Ritter et al, 2012). Given this
set of automatically identified topics, we gathered
tweets from the same time period which mentioned
the named entities. The testing messages had differ-
ent topics from training and spanned later periods.
To identify messages that express sentiment to-
wards these topics, we filtered the tweets us-
ing SentiWordNet (Baccianella et al, 2010). We
removed messages that contained no sentiment-
bearing words, keeping only those with at least one
word with positive or negative sentiment score that
is greater than 0.3 in SentiWordNet for at least one
sense of the words. Without filtering, we found class
imbalance to be too high.3
Twitter messages are rich in social media features,
including out-of-vocabulary (OOV) words, emoti-
cons, and acronyms; see Table 1. A large portion of
the OOV words are hashtags (e.g., #sheenroast)
and mentions (e.g., @tash jade).
3Filtering based on an existing lexicon does bias the dataset
to some degree; however, note that the text still contains senti-
ment expressions outside those in the lexicon.
Corpus Positive Negative Objective
/ Neutral
Twitter - Training 3,662 1,466 4,600
Twitter - Dev 575 340 739
Twitter - Test 1,573 601 1,640
SMS - Test 492 394 1,208
Table 3: Statistics for Subtask B.
We annotated the same Twitter messages with an-
notations for subtask A and subtask B. However,
the final training and testing datasets overlap only
partially between the two subtasks since we had
to throw away messages with low inter-annotator
agreement, and this differed between the subtasks.
For testing, we also annotated SMS messages, taken
from the NUS SMS corpus4 (Chen and Kan, 2012).
Tables 2 and 3 show statistics about the corpora we
created for subtasks A and B.
4http://wing.comp.nus.edu.sg/SMSCorpus/
314
A B
Lower Avg. Upper Avg.
Twitter - Train 64.7 82.4 90.8 82.7
Twitter - Dev 51.2 74.7 87.8 78.4
Twitter - Test 68.8 83.6 90.9 76.9
SMS - Test 66.5 88.5 81.2 77.6
Table 4: Bounds for datasets in subtasks A and B.
3.2 Annotation Guidelines
The instructions provided to the annotators, along
with an example, are shown in Figure 1. We pro-
vided several additional examples to the annotators,
shown in Table 5.
In addition, we filtered spammers by considering
the following kinds of annotations invalid:
? containing overlapping subjective phrases;
? subjective but without a subjective phrase;
? marking every single word as subjective;
? not having the overall sentiment marked.
3.3 Annotation Process
Our datasets were annotated for sentiment on Me-
chanical Turk. Each sentence was annotated by five
Mechanical Turk workers (Turkers). In order to
qualify for the hits, the Turker had to have an ap-
proval rate greater than 95% and have completed 50
approved hits. Each Turker was paid three cents
per hit. The Turker had to mark all the subjec-
tive words/phrases in the sentence by indicating their
start and end positions and say whether each subjec-
tive word/phrase was positive, negative, or neutral
(subtask A). They also had to indicate the overall
polarity of the sentence (subtask B).
Figure 1 shows the instructions and an exam-
ple provided to the Turkers. The first five rows
of Table 6 show an example of the subjective
words/phrases marked by each of the workers.
For subtask A, we combined the annotations of
each of the workers using intersection as indicated
in the last row of Table 6. A word had to appear
in 2/3 of the annotations in order to be considered
subjective. Similarly, a word had to be labeled with
a particular polarity (positive, negative, or neutral)
2/3 of the time in order to receive that label.
We also experimented with combining annota-
tions by computing the union of the sentences, and
taking the sentence of the worker who annotated the
most hits, but we found that these methods were
not as accurate. Table 4 shows the lower, average,
and upper bounds for all the hits by computing the
bounds for each hit and averaging them together.
This gives a good indication about how well we can
expect the systems to perform. For example, even if
we used the best annotator each time, it would still
not be possible to get perfect accuracy.
For subtask B, the polarity of the entire sentence
was determined based on the majority of the labels.
If there was a tie, the sentence was discarded. In
order to reduce the number of sentences lost, we
combined the objective and the neutral labels, which
Turkers tended to mix up. Table 4 shows the aver-
age bound for subtask B by computing the bounds
for each hit and averaging them together. Since the
polarity is chosen based on the majority, the upper
bound is 100%.
4 Scoring
For both subtasks, the participating systems were
required to perform a three-way classification ? a
particular marked phrase (for subtask A) or an en-
tire message (for subtask B) was to be classified as
positive, negative, or objective. For each system,
we computed a score for predicting positive/negative
phrases/messages vs. the other two classes.
For instance, to compute positive precision, Ppos,
we find the number of phrases/messages that a sys-
tem correctly predicted to be positive, and we divide
that number by the total number of messages it pre-
dicted to be positive. To compute recall, for the pos-
itive class, Rpos, we find the number of messages
correctly predicted to be positive and we divide that
number by the total number of positive messages in
the gold standard.
We then calculate F-score for the positive labels,
the harmonic average of precision and recall as fol-
lows Fpos = 2
PposRpos
Ppos+Rpos
. We carry out a similar
computation to calculate Fneg, which is F1 for neg-
ative messages.
The overall score for each system run is then
given by the average of the F1-scores for the posi-
tive and negative classes: F = (Fpos + Fneg)/2.
315
Authorities are only too aware that Kashgar is 4,000 kilometres (2,500 miles) from Beijing but only a tenth of
the distance from the Pakistani border, and are desperate to ensure instability or militancy does not leak over the
frontiers.
Taiwan-made products stood a good chance of becoming even more competitive thanks to wider access to overseas
markets and lower costs for material imports, he said.
?March appears to be a more reasonable estimate while earlier admission cannot be entirely ruled out,? according
to Chen, also Taiwan?s chief WTO negotiator.
friday evening plans were great, but saturday?s plans didnt go as expected ? i went dancing & it was an ok club,
but terribly crowded :-(
WHY THE HELL DO YOU GUYS ALL HAVE MRS. KENNEDY! SHES A FUCKING DOUCHE
AT&T was okay but whenever they do something nice in the name of customer service it seems like a favor, while
T-Mobile makes that a normal everyday thin
obama should be impeached on TREASON charges. Our Nuclear arsenal was TOP Secret. Till HE told our enemies
what we had. #Coward #Traitor
My graduation speech: ?I?d like to thanks Google, Wikipedia and my computer! :D #iThingteens
Table 5: List of example sentences with annotations that were provided to the annotators. All subjective phrases are
italicized. Positive phrases are in green, negative phrases are in red, and neutral phrases are in blue.
Worker 1 I would love to watch Vampire Diaries :) and some Heroes! Great combination 9/13
Worker 2 I would love to watch Vampire Diaries :) and some Heroes! Great combination 11/13
Worker 3 I would love to watch Vampire Diaries :) and some Heroes! Great combination 10/13
Worker 4 I would love to watch Vampire Diaries :) and some Heroes! Great combination 13/13
Worker 5 I would love to watch Vampire Diaries :) and some Heroes! Great combination 11/13
Intersection I would love to watch Vampire Diaries :) and some Heroes! Great combination
Table 6: Example of a sentence annotated for subjectivity on Mechanical Turk. Words and phrases that were marked as
subjective are italicized and highlighted in bold. The first five rows are annotations provided by Turkers, and the final
row shows their intersection. The final column shows the accuracy for each annotation compared to the intersection.
Note that ignoring Fneutral does not reduce the
task to predicting positive vs. negative labels only
(even though some participants have chosen to do
so) since the gold standard still contains neutral
labels which are to be predicted: Fpos and Fneg
would suffer if these examples are labeled as posi-
tive and/or negative instead of neutral.
We provided participants with a scorer. In addi-
tion to outputting the overall F-score, it produced
a confusion matrix for the three prediction classes
(positive, negative, and objective), and it also vali-
dated the data submission format.
5 Participants and Results
The results for subtask A are shown in Tables 7 and
8 for Twitter and for SMS messages, respectively;
those for subtask B are shown in Table 9 for Twit-
ter and in Table 10 for SMS messages. Systems are
ranked by their scores for the constrained runs; the
ranking based on scores for unconstrained runs is
shown as a subindex.
For both subtasks, there were teams that only sub-
mitted results for the Twitter test set. Some teams
submitted both a constrained and an unconstrained
version (e.g., AVAYA and teragram). As one would
expect, the results on the Twitter test set tended to be
better than those on the SMS test set since the SMS
data was out-of-domain with respect to the training
(Twitter) data.
Moreover, the results for subtask A were signifi-
cantly better than those for subtask B, which shows
that it is a much easier task, probably because there
is less ambiguity at the phrase-level.
5.1 Subtask A: Contextual Polarity
Table 7 shows that subtask A, Twitter, attracted 23
teams, who submitted 21 constrained and 7 uncon-
strained systems. Five teams submitted both a con-
strained and an unconstrained system, and two other
teams submitted constrained systems that are on
the boundary between being constrained and uncon-
strained.
316
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
NRC-Canada 88.93 yes yes
AVAYA 86.98 87.38(1) yes yes
BOUNCE 86.79 yes yes
LVIC-LIMSI 85.70 yes yes
FBM 85.50 yes semi
GU-MLT-LT 85.19 yes yes
UNITOR 84.60 yes yes
USNA 81.31 yes yes
Serendio 80.04 yes yes
ECNUCS 79.48 80.15(2) yes yes
TJP 78.16 yes yes
?columbia-nlp 74.94 yes yes
teragram 74.89(3) yes yes
sielers 74.41 yes yes
KLUE 73.74 yes yes
OPTWIMA 69.17 36.91(6) yes yes
swatcs 67.19 63.86(5) no yes
Kea 63.94 yes yes
senti.ue-en 62.79 71.38(4) yes yes
uottawa 60.20 yes yes
IITB 54.80 yes yes
SenselyticTeam 53.88 yes yes
SU-sentilab 34.73(7) no yes
Majority Baseline 38.10 N/A N/A
Table 7: Results for subtask A on the Twitter dataset. The
? marks a team that includes a task coorganizer, and the
 indicates a system submitted as constrained but which
used additional Tweets or additional sentiment-annotated
text to collect statistics that were then used as a feature.
One system was semi-supervised, and the rest
were supervised. The supervised systems used clas-
sifiers such as SVM (8 systems), Naive Bayes (7 sys-
tems), and Maximum Entropy (3 systems). Other
approaches used include an ensemble of classifiers,
manual rules, and a linear classifier. Two of the sys-
tems chose not to predict neutral as a possible clas-
sification label.
The average F1-measure on the Twitter test set
was 74.1% for constrained systems and 60.5% for
unconstrained ones; this does not mean that using
additional data does not help, it just shows that the
best teams only participated with a constrained sys-
tem. NRC-Canada had the best constrained system
with an F1-measure of 88.9%, and AVAYA had the
best unconstrained one with F1=87.4%.
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
GU-MLT-LT 88.37 yes yes
NRC-Canada 88.00 yes yes
?AVAYA 83.94 85.79(1) yes yes
UNITOR 82.49 yes yes
TJP 81.23 yes yes
LVIC-LIMSI 80.16 yes yes
USNA 79.82 yes yes
ECNUCS 76.69 77.34(2) yes yes
sielers 73.48 yes yes
FBM 72.95 no semi
teragram 72.83 72.83(4) yes yes
KLUE 70.54 yes yes
?columbia-nlp 70.30 yes yes
senti.ue-en 66.09 74.13(3) yes yes
swatcs 66.00 67.68(5) no yes
Kea 63.27 yes yes
uottawa 55.89 yes yes
SU-sentilab 55.38(6) no yes
SenselyticTeam 51.13 yes yes
OPTWIMA 37.32 36.38(7) yes yes
Majority Baseline 31.50 N/A N/A
Table 8: Results for subtask A on the SMS dataset. The
? indicates a late submission, the ? marks a team that
includes a task co-organizer, and the  indicates a sys-
tem submitted as constrained but which used additional
Tweets or additional sentiment-annotated text to collect
statistics that were then used as a feature.
Table 8 shows the results for the SMS test set,
where 20 teams submitted 19 constrained and 7 un-
constrained systems (again, this included two teams
that submitted boundary systems, marked accord-
ingly). The average F-measure on this test set
was 70.8% for constrained systems and 65.7% for
unconstrained systems. The best constrained sys-
tem was that of GU-MLT-LT with an F-measure of
88.4%, and AVAYA had the best unconstrained sys-
tem with an F1 of 85.8%.
5.2 Subtask B: Message Polarity
Table 9 shows that subtask B, Twitter, attracted 38
teams, who submitted 36 constrained and 15 uncon-
strained systems (and two boundary ones).
The average F1-measure was 53.7% for the con-
strained and 54.6% for the unconstrained systems.
317
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
NRC-Canada 69.02 yes yes
GU-MLT-LT 65.27 yes yes
teragram 64.86 64.86(1) yes yes
BOUNCE 63.53 yes yes
KLUE 63.06 yes yes
AMI&ERIC 62.55 61.17(3) yes yes/semi
FBM 61.17 yes yes
AVAYA 60.84 64.06(2) yes yes/semi
SAIL 60.14 61.03(4) yes yes
UT-DB 59.87 yes yes
FBK-irst 59.76 yes yes
nlp.cs.aueb.gr 58.91 yes yes
UNITOR 58.27 59.50(5) yes semi
LVIC-LIMSI 57.14 yes yes
Umigon 56.96 yes yes
NILC USP 56.31 yes yes
DataMining 55.52 yes semi
ECNUCS 55.05 58.42(6) yes yes
nlp.cs.aueb.gr 54.73 yes yes
ASVUniOfLeipzig 54.56 yes yes
SZTE-NLP 54.33 53.10(9) yes yes
CodeX 53.89 yes yes
Oasis 53.84 yes yes
NTNU 53.23 50.71(10) yes yes
UoM 51.81 45.07(15) yes yes
SSA-UO 50.17 yes no
SenselyticTeam 50.10 yes yes
UMCC DLSI (SA) 49.27 48.99(12) yes yes
bwbaugh 48.83 54.37(8) yes yes/semi
senti.ue-en 47.24 47.85(13) yes yes
SU-sentilab 45.75(14) yes yes
OPTWIMA 45.40 54.51(7) yes yes
REACTION 45.01 yes yes
uottawa 42.51 yes yes
IITB 39.80 yes yes
IIRG 34.44 yes yes
sinai 16.28 49.26(11) yes yes
Majority Baseline 29.19 N/A N/A
Table 9: Results for subtask B on the Twitter dataset. The
 indicates a system submitted as constrained but which
used additional Tweets or additional sentiment-annotated
text to collect statistics that were then used as a feature.
These averages are much lower than those for sub-
task A, which indicates that subtask B is harder,
probably because a message can contain parts ex-
pressing both positive and negative sentiment.
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
NRC-Canada 68.46 yes yes
GU-MLT-LT 62.15 yes yes
KLUE 62.03 yes yes
AVAYA 60.00 59.47(1) yes yes/semi
teragram 59.10(2) yes yes
NTNU 57.97 54.55(6) yes yes
CodeX 56.70 yes yes
FBK-irst 54.87 yes yes
AMI&ERIC 53.63 52.62(7) yes yes/semi
ECNUCS 53.21 54.77(5) yes yes
UT-DB 52.46 yes yes
SAIL 51.84 51.98(8) yes yes
UNITOR 51.22 48.88(10) yes semi
SZTE-NLP 51.08 55.46(3) yes yes
SenselyticTeam 51.07 yes yes
NILC USP 50.12 yes yes
REACTION 50.11 yes yes
SU-sentilab 49.57(9) no yes
nlp.cs.aueb.gr 49.41 55.28(4) yes yes
LVIC-LIMSI 49.17 yes yes
FBM 47.40 yes yes
ASVUniOfLeipzig 46.50 yes yes
senti.ue-en 44.65 46.72(12) yes yes
SSA UO 44.39 yes no
UMCC DLSI (SA) 43.39 40.67(14) yes yes
UoM 42.22 35.22(15) yes yes
OPTWIMA 40.98 47.15(11) yes yes
uottawa 40.51 yes yes
bwbaugh 39.73 43.43(13) yes yes/semi
IIRG 22.16 yes yes
Majority Baseline 19.03 N/A N/A
Table 10: Results for subtask B on the SMS dataset. The
 indicates a system submitted as constrained but which
used additional Tweets or additional sentiment-annotated
text to collect statistics that were then used as a feature.
Once again, NRC-Canada had the best con-
strained system with an F1-measure of 69%, fol-
lowed by teragram, which had the best uncon-
strained system with an F1-measure of 64.9%.
As Table 10 shows, the average F1-measure on
the SMS test set was 50.2% for constrained and
50.3% for unconstrained systems. NRC-Canada had
the best constrained system with an F1=68.5%, and
AVAYA had the best unconstrained one with F1-
measure of 59.5%.
318
5.3 Overall
Overall, the results achieved by the best teams were
very strong, especially for the simpler subtask A:
? F1=88.93, NRC-Canada on subtask A, Twitter;
? F1=88.37, GU-MLT-LT on subtask A, SMS;
? F1=69.02, NRC-Canada on subtask B, Twitter;
? F1=68.46, NRC-Canada on subtask B, SMS.
We can see that the strongest team overall was that
of NRC-Canada, which was ranked first on three of
the four conditions; and it was second on subtask A,
SMS. There were two other teams that were strong
across both tasks and on both test sets: GU-MLT-LT
and AVAYA. Three other teams, namely teragram,
BOUNCE and KLUE, were ranked in the top-3 in at
least one subtask and test set.
6 Discussion
We have seen that most participants restricted them-
selves to the provided data and submitted con-
strained systems. Indeed, the best systems for each
of the two subtasks and for each of the two testing
datasets were constrained systems; of course, this
does not mean that additional data would not be use-
ful. Curiously, in some cases where a team submit-
ted a constrained and unconstrained run, the uncon-
strained run actually performed worse.
Not surprisingly, most systems were supervised;
there were only five semi-supervised systems, and
there was only one unsupervised system. One ad-
ditional team declared their system as unsupervised
since it was not making use of the training data; we
still classified it as supervised though since it did use
supervision ? in the form of manual rules.
Most participants predicted all three labels (posi-
tive, negative and neutral), even though some partic-
ipants opted for not predicting neutral, which made
some sense since the final F1-score was averaged
over the positive and the negative predictions only.
The most popular classifiers included SVM, Max-
Ent, linear classifier, Naive Bayes; in some cases,
manual rules or ensembles of classifiers were used.
A variety of features were used, including word-
related (e.g., words, stems, n-grams, word clus-
ters), word-shape (e.g., punctuation, capitalization),
syntactic (e.g., POS tags, dependency relations),
Twitter-specific (e.g., repeated characters, emoti-
cons, URLs, hashtags, slang, abbreviations), and
sentiment-related (e.g., negation); one team also
used discourse relations. Almost all participants re-
lied heavily of various sentiment lexicons, the most
popular ones being MPQA and SentiWordNet, as
well as AFINN and Bing Liu?s Opinion Lexicon;
some participants used their own lexicons ? preex-
isting or built from the provided data.
Given that Twitter messages are noisy, most par-
ticipants did some preprocessing, including tok-
enization, stemming, lemmatization, stopword re-
moval, normalization/removal of URLs, hashtags,
users, slang, emoticons, repeated vowels, punctua-
tion; some even did pronoun resolution.
7 Conclusion
We have described a new task that entered SemEval-
2013: task 2 on Sentiment Analysis on Twitter. The
task has attracted a very high number of participants:
149 submissions from 44 teams.
We believe that the datasets that we have created
as part of the task and which we have released to the
community5 under a Creative Commons Attribution
3.0 Unported License,6 will be found useful by re-
searchers beyond SemEval.
Acknowledgments
The authors would like to thank Kathleen McKeown
for her insight in creating the Amazon Mechanical
Turk annotation task.
Funding for the Amazon Mechanical Turk anno-
tations was provided by the JHU Human Language
Technology Center of Excellence and by the Of-
fice of the Director of National Intelligence (ODNI),
Intelligence Advanced Research Projects Activity
(IARPA), through the U.S. Army Research Lab. All
statements of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of IARPA, the ODNI or the U.S. Government.
5http://www.cs.york.ac.uk/semeval-2013/task2/
6http://creativecommons.org/licenses/by/3.0/
319
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Nicoletta Calzolari (chair), Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios
Piperidis, Mike Rosner, and Daniel Tapias, editors,
Proceedings of the Seventh International Conference
on Language Resources and Evaluation, LREC ?10,
pages 2200?2204, Valletta, Malta.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on Twitter from biased and noisy data.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, COLING ?10,
pages 36?44, Beijing, China.
Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer, and
Ricard Gavalda`. 2011. Detecting sentiment change in
Twitter streaming data. Journal of Machine Learning
Research - Proceedings Track, 17:5?11.
Tao Chen and Min-Yen Kan. 2012. Creating a live, pub-
lic short message service corpus: the NUS SMS cor-
pus. Language Resources and Evaluation, pages 1?
37.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences in
Twitter and Amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ?10, pages 107?116, Upp-
sala, Sweden.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Twitter power: Tweets as elec-
tronic word of mouth. J. Am. Soc. Inf. Sci. Technol.,
60(11):2169?2188.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The Good
the Bad and the OMG! In Lada A. Adamic, Ricardo A.
Baeza-Yates, and Scott Counts, editors, Proceedings of
the Fifth International Conference on Weblogs and So-
cial Media, ICWSM? 11, pages 538?541, Barcelona,
Spain.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In William W. Cohen and
Samuel Gosling, editors, Proceedings of the Fourth
International Conference on Weblogs and Social
Media, ICWSM ?10, Washington, DC, USA.
Alexander Pak and Patrick Paroubek. 2010. Twitter
based system: Using Twitter for disambiguating senti-
ment ambiguous adjectives. In Proceedings of the 5th
International Workshop on Semantic Evaluation, Se-
mEval ?10, pages 436?439, Los Angeles, CA, USA.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an exper-
imental study. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?11, pages 1524?1534, Edinburgh, United
Kingdom.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter. In
Proceedings of the 18th ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD ?12, pages 1104?1112, Beijing, China.
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-
ner, and Isabell M. Welpe. 2010. Predicting elections
with Twitter: What 140 characters reveal about po-
litical sentiment. In William W. Cohen and Samuel
Gosling, editors, Proceedings of the Fourth Inter-
national Conference on Weblogs and Social Media,
ICWSM ?10, pages 178?185, Washington, DC, USA.
The AAAI Press.
Julio Villena-Roma?n, Sara Lana-Serrano, Euge-
nio Mart??nez-Ca?mara, and Jose? Carlos Gonza?lez
Cristo?bal. 2013. TASS - Workshop on Sentiment
Analysis at SEPLN. Procesamiento del Lenguaje
Natural, 50:37?44.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
320
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 478?482, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
Columbia NLP: Sentiment Detection of Subjective Phrases in Social Media
Sara Rosenthal
Department of Computer Science
Columbia University
New York, NY 10027, USA
sara@cs.columbia.edu
Kathleen McKeown
Department of Computer Science
Columbia University
New York, NY 10027, USA
kathy@cs.columbia.edu
Abstract
We present a supervised sentiment detection
system that classifies the polarity of subjec-
tive phrases as positive, negative, or neutral. It
is tailored towards online genres, specifically
Twitter, through the inclusion of dictionaries
developed to capture vocabulary used in on-
line conversations (e.g., slang and emoticons)
as well as stylistic features common to social
media. We show how to incorporate these
new features within a state of the art system
and evaluate it on subtask A in SemEval-2013
Task 2: Sentiment Analysis in Twitter.
1 Introduction
People use social media to write openly about their
personal experiences, likes and dislikes. The follow-
ing sentence from Twitter is a typical example: ?To-
morrow I?m coming back from Barcelona...I don?t
want! :(((?. The ability to detect the sentiment ex-
pressed in social media can be useful for understand-
ing what people think about the restaurants they
visit, the political viewpoints of the day, and the
products they buy. These sentiments can be used
to provided targeted advertising, automatically gen-
erate reviews, and make various predictions, such as
political outcomes.
In this paper we develop a sentiment detection al-
gorithm for social media that classifies the polarity
of sentence phrases as positive, negative, or neutral
and test its performance in Twitter through the par-
ticipation in the expression level task (subtask A)
of the SemEval-2013 Task 2: Sentiment Analysis
in Twitter (Wilson et al, 2013) which the authors
helped organize. To do so, we build on previous
work on sentiment detection algorithms for the more
formal news genre, notably the work of Agarwal et
al (2009), but adapt it for the language of social me-
dia, in particular Twitter. We show that exploiting
lexical-stylistic features and dictionaries geared to-
ward social media are useful in detecting sentiment.
In this rest of this paper, we discuss related work,
including the state of the art sentiment system (Agar-
wal et al, 2009) our method is based on, the lexicons
we used, our method, and experiments and results.
2 Related Work
Several recent papers have explored sentiment anal-
ysis in Twitter. Go et al(2009) and Pak and
Paroubek (2010) classify the sentiment of tweets
containing emoticons using n-grams and POS. Bar-
bosa and Feng (2010) detect sentiment using a po-
larity dictionary that includes web vocabulary and
tweet-specific social media features. Bermingham
and Smeaton (2010) compare polarity detection in
twitter to blogs and movie reviews using lexical fea-
tures. Agarwal et al(2011) perform polarity senti-
ment detection on the entire tweet using features that
are somewhat similar to ours: the DAL, lexical fea-
tures (e.g. POS and n-grams), social media features
(e.g. slang and hashtags) and tree kernel features. In
contrast to this related work, our approach is geared
towards predicting sentiment is at the phrase level as
opposed to the tweet level.
3 Lexicons
Several lexicons are used in our system. We use the
DAL and expand it with WordNet, as it was used in
478
Corpus DAL
NNP
(Post
DAL)
Word
Length-
ening
WordNet Wiktionary Emoticons
Punctuation
& Numbers
Not
Covered
Twitter - Train 42.9% 19.2% 1.4% 10.2% 12.7% 0.3% 1.5% 11.7%
Twitter - Dev 57.3% 13.8% 1.1% 7.1% 12.2% 0.4% 2.7% 5.4%
Twitter - Test 49.9% 15.6% 1.4% 9.6% 12.1% 0.5% 1.6% 9.3%
SMS - Test 60.1% 3.6% 0.6% 7.9% 14.7% 0.6% 1.9% 10.3%
Table 1: Coverage for each of the lexicons in the training and test corpora?s.
the original work (Agarwal et al, 2009), and expand
it further to use Wiktionary and an emoticon lexicon.
We consider proper nouns that are not in the DAL to
be objective. We also shorten words that are length-
ened to see if we can find the shortened version in
the lexicons (e.g. sweeeet? sweet). The coverage
of the lexicons for each corpus is shown in Table 1.
3.1 DAL
The Dictionary of Affect and Language (DAL)
(Whissel, 1989) is an English language dictionary
of 8742 words built to measure the emotional mean-
ing of texts. In addition to using newswire, it was
also built from individual sources such as interviews
on abuse, students? retelling of a story, and adoles-
cent?s descriptions of emotions. It therefore covers a
broad set of words. Each word is given three scores
(pleasantness - also called evaluation (ee), active-
ness (aa), and imagery (ii)) on a scale of 1 (low)
to 3 (high). We compute the polarity of a chunk in
the same manner as the original work (Agarwal et
al., 2009), using the sum of the AE Space Score?s
(|
?
ee2 + aa2|) of each word within the chunk.
3.2 WordNet
The DAL does cover a broad set of words, but we
will still often encounter words that are not included
in the dictionary. Any word that is not in the DAL
and is not a proper noun is accessed in WordNet
(Fellbaum, 1998) 1 and, if it exists, the DAL scores
of the synonyms of its first sense are used in its
place. In addition to the original approach, if there
are no synonyms we look at the hypernym. We then
compute the average scores (ee, aa, and ii) of all the
words and use that as the score for the word.
1We cannot use SentiWordNet because we are interested in
the DAL scores
3.3 Wiktionary
We use Wiktionary, an online dictionary, to supple-
ment the common words that are not found in Word-
Net and the DAL. We first examine all ?form of? re-
lationships for the word such as ?doesnt? is a ?mis-
spelling of? ?doesn?t?, and ?tonite? is an ?alternate
form of? ?tonight?. If no ?form of? relationships ex-
ist, we take all the words in the definitions that have
their own Wiktionary page and look up the scores
for each word in the DAL. (e.g., the verb definition
for LOL (laugh out loud) in Wiktionary is ?To laugh
out loud? with ?laugh? having its own Wiktionary
definition; it is therefore looked up in the DAL and
the score for ?laugh? is used for ?LOL?.) We then
compute the average scores (ee, aa, and ii) of all the
words and use that as the score for the word.
3.4 Emoticon Dictionary
emoticon :) :D <3 :( ;)
definition happy laughter love sad wink
Table 2: Popular emoticons and their definitions
We created a simple lexicon to map common
emoticons to a definition in the DAL. We looked at
over 1000 emoticons gathered from several lists on
the internet2 and computed their frequencies within
a LiveJournal blog corpus. (In the future we would
like to use an external Twitter corpus). We kept
the 192 emoticons that appeared at least once and
mapped each emoticon to a single word definition.
The top 5 emoticons and their definitions are shown
in Table 2. When an emoticon is found in a tweet we
look up its definition in the DAL.
4 Methods
We run our data through several pre-processing steps
to preserve emoticons and expand contractions. We
2www.chatropolis.com, www.piology.org, en.wikipedia.org
479
General Social Media
Feature Example Feature Example
Capital Words Hello Emoticons :)
Out of Vocabulary duh Acronyms LOL
Punctuation . Repeated Questions ???
Repeated Punctuation #@. Exclamation Points !
Punctuation Count 5 Repeated Exclamations !!!!
Question Marks ? Word Lengthening sweeeet
Ellipses ... All Caps HAHA
Avg Word Length 5 Links/Images www.url.com
Table 3: List of lexical-stylistic features and examples.
then pre-process the sentences to add Part-of-Speech
tags (POS) and chunk the sentences using the CRF
tagger and chunker (Phan, 2006a; Phan, 2006b).
The chunker uses three labels, ?B? (beginning), ?I?
(in), and ?O? (out). The ?O? label tends to be ap-
plied to punctuation which one typically wants to
ignore. However, in this context, punctation can be
very important (e.g. exclamation points, and emoti-
cons). Therefore, we append words/phrases tagged
as O to the prior B-I chunk.
We apply the dictionaries to the preprocessed sen-
tences to generate lexical, syntactic, and stylistic
features. All sets of features were reduced using chi-
square in Weka (Hall et al, 2009).
4.1 Lexical and Syntactic Features
We include POS tags and the top 500 n-gram fea-
tures(Agarwal et al, 2009). We experimented with
different amounts of n-grams and found that more
than 500 n-grams reduced performance.
The DAL and other dictionaries are used along
with a negation state machine(Agarwal et al, 2009)
to determine the polarity for each word in the sen-
tence. We include all the features described in the
original system (Agarwal et al, 2009).
4.2 Lexical-Stylistic Features
We include several lexical-stylistic features (see Ta-
ble 3) that can occur in all datasets. We divide these
features into two groups, general: ones that are
common across online and traditional genres, and
social media: one that are far more common in on-
line genres. Examples of general style features are
exclamation points and ellipses. Examples of social
media style features are emoticons and word length-
ening. Word lengthening is a common phenomenon
Figure 1: Percentage of lexical-stylistic features that are
negative (top), neutral (middle), and positive (bottom) in
the Twitter training corpus.
in social media where letters are repeated to indi-
cate emphasis (e.g. sweeeet). It is particularly com-
mon in opinionated words (Brody and Diakopoulos,
2011). The count values of each feature was normal-
ized by the number of words in the phrase.
The percentage of lexical-stylistic features that
are positive/negative/neutral is shown in Figure 1.
For example, emoticons tend to indicate a positive
phrase in Twitter. Each stylistic feature accounts for
less than 2% of the sentence but at least one of the
stylistic features exists in 61% of the Tweets.
We also computed the most frequent emoticons
(<3, :D), acronyms (lol), and punctuation symbols
(#) within a subset of the Twitter training set and
included those as additional features.
5 Experiments and Results
This task was evaluated on the Twitter dataset pro-
vided by Semeval-2013 Task 2, subtask A, which the
authors helped organize. Therefore, a large portion
of time was spent on creating the dataset.
480
Experiment Twitter SMS
Dev Test
Majority 36.3 38.1 31.5
Just DAL 70.1 72.3 67.1
WordNet 72.2 73.6 67.7
Wiktionary 72.8 73.7 68.7
Style 71.5 73.7 69.7
n-grams 75.2 75.7 72.5
WordNet+Style 73.2 74.6 70.1
Dictionaries+Style 74.0 75.0 70.2
Dictionaries+Style+n-grams 75.8 77.6 73.3
Table 4: Experiments using the Twitter corpus. Results
are shown using average F-measure of the positive and
negative class. All experiments include the DAL. The
dictionaries refer to WordNet, Wiktionary, and Emoticon.
Style refers to Lexical-Stylistic features. All results ex-
ceed the majority baseline significantly.
We ran all of our experiments in Weka (Hall et
al., 2009) using Logistic Regression. We also exper-
imented with other learning methods but found that
this worked best. All results are shown using the av-
erage F-measure of the positive and negative class.
We tuned our system for Semeval-2013 Task 2,
subtask A, using the provided development set and
ran it on the provided Twitter and SMS test data.
Our results are shown in Table 4 with all results
being statistically significant over a majority base-
line. We also use the DAL as a baseline to in-
dicate how useful lexical-stylistic features (specifi-
cally those geared towards social media) and the dic-
tionaries are in improving the performance of sen-
timent detection of phrases in online genres in con-
trast to using just the DAL. The results that are statis-
tically significant (computed using the Wilcoxon?s
test, p ? .02) shown in bold. Our best results for
each dataset include all features with an average F-
measure of 77.6% and 73.3% for the Twitter and
SMS test sets respectively resulting in a significant
improvement of more than 5% for each test set over
the DAL baseline.
At the time of submission, we had not experi-
mented with n-grams, and therefore chose the Dic-
tionaries+Style system as our final version for the
official run resulting in a rank of 12/22 (75% F-
measure) for Twitter and 13/19 (70.2% F-measure)
for SMS. Our rank with the best system, which in-
cludes n-grams, would remain the same for Twitter,
but bring our rank up to 10/19 for SMS.
We looked more closely at the impact of our new
features and as one would expect, feature selection
found the general and social media style features
(e.g. emoticons, :(, lol, word lengthening) to be use-
ful in Twitter and SMS data. Using additional online
dictionaries is useful in Twitter and SMS, which is
understandable because they both have poor cover-
age in the DAL and WordNet. In all cases using
n-grams was the most useful which indicates that
context is most important. Using Dictionaries and
Style in addition to n-grams did provide a signifi-
cant improvement in the Twitter test set, but not in
the Twitter Dev and SMS test set.
6 Conclusion and Future Work
We have explored whether social media features,
Wiktionary, and emoticon dictionaries positively im-
pact the accuracy of polarity detection in Twitter and
other online genres. We found that social media re-
lated features can be used to predict sentiment in
Twitter and SMS. In addition, Wiktionary helps im-
prove the word coverage and though it does not pro-
vide a significant improvement over WordNet, it can
be used in place of WordNet. On the other hand, we
found that using the DAL and n-grams alone does al-
most as well as the best system. This is encouraging
as it indicates that content is important and domain
independent sentiment systems can do a good job of
predicting sentiment in social media.
The results of the SMS messages dataset indicate
that even though the online genres are different, the
training data in one online genre can indeed be used
to predict results with reasonable accuracy in the
other online genre. These results show promise for
further work on domain adaptation across different
kinds of social media.
7 Acknowledgements
This research was partially funded by (a) the ODNI,
IARPA, through the U.S. Army Research Lab and
(b) the DARPA DEFT Program. All statements of
fact, opinion or conclusions contained herein are
those of the authors and should not be construed as
representing the official views, policies, or positions
of IARPA, the ODNI, the Department of Defense, or
the U.S. Government.
481
References
Apoorv Agarwal, Fadi Biadsy, and Kathleen R. Mcke-
own. 2009. Contextual phrase-level polarity analysis
using lexical affect scoring and syntactic n-grams. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, EACL ?09, pages 24?32, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analy-
sis of twitter data. In Proceedings of the Workshop
on Language in Social Media (LSM 2011), pages 30?
38, Portland, Oregon, June. Association for Computa-
tional Linguistics.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
In COLING (Posters), pages 36?44.
Adam Bermingham and Alan F. Smeaton. 2010. Clas-
sifying sentiment in microblogs: is brevity an advan-
tage? In Jimmy Huang, Nick Koudas, Gareth J. F.
Jones, Xindong Wu, Kevyn Collins-Thompson, and
Aijun An, editors, CIKM, pages 1833?1836. ACM.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! using word
lengthening to detect sentiment in microblogs. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 562?570,
Edinburgh, Scotland, UK., July. Association for Com-
putational Linguistics.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press, Cambridge,
MA ; London, May.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Processing, pages 1?6.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter as a corpus for sentiment analysis and opinion
mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?10), Valletta, Malta, may. European Lan-
guage Resources Association (ELRA).
Xuan-Hieu Phan. 2006a. Crfchunker: Crf english phrase
chunker.
Xuan-Hieu Phan. 2006b. Crftagger: Crf english phrase
tagger.
C. M. Whissel. 1989. The dictionary of affect in lan-
guage. In R. Plutchik and H. Kellerman, editors, Emo-
tion: theory research and experience, volume 4, Lon-
don. Acad. Press.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan
Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013.
Semeval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the 7th International Workshop on
Semantic Evaluation. Association for Computational
Linguistics.
482
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 73?80,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 9: Sentiment Analysis in Twitter
Sara Rosenthal
Columbia University
sara@cs.columbia.edu
Preslav Nakov
Qatar Computing Research Institute
pnakov@qf.org.qa
Alan Ritter
Carnegie Mellon University
rittera@cs.cmu.edu
Veselin Stoyanov
Johns Hopkins University
ves@cs.jhu.edu
Abstract
We describe the Sentiment Analysis in
Twitter task, ran as part of SemEval-2014.
It is a continuation of the last year?s task
that ran successfully as part of SemEval-
2013. As in 2013, this was the most popu-
lar SemEval task; a total of 46 teams con-
tributed 27 submissions for subtask A (21
teams) and 50 submissions for subtask B
(44 teams). This year, we introduced three
new test sets: (i) regular tweets, (ii) sarcas-
tic tweets, and (iii) LiveJournal sentences.
We further tested on (iv) 2013 tweets, and
(v) 2013 SMS messages. The highest F1-
score on (i) was achieved by NRC-Canada
at 86.63 for subtask A and by TeamX at
70.96 for subtask B.
1 Introduction
In the past decade, new forms of communica-
tion have emerged and have become ubiquitous
through social media. Microblogs (e.g., Twitter),
Weblogs (e.g., LiveJournal) and cell phone mes-
sages (SMS) are often used to share opinions and
sentiments about the surrounding world, and the
availability of social content generated on sites
such as Twitter creates new opportunities to au-
tomatically study public opinion.
Working with these informal text genres
presents new challenges for natural language pro-
cessing beyond those encountered when work-
ing with more traditional text genres such as
newswire. The language in social media is very
informal, with creative spelling and punctuation,
misspellings, slang, new words, URLs, and genre-
specific terminology and abbreviations, e.g., RT
for re-tweet and #hashtags
1
.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1
Hashtags are a type of tagging for Twitter messages.
Moreover, tweets and SMS messages are short:
a sentence or a headline rather than a document.
How to handle such challenges so as to automat-
ically mine and understand people?s opinions and
sentiments has only recently been the subject of
research (Jansen et al., 2009; Barbosa and Feng,
2010; Bifet et al., 2011; Davidov et al., 2010;
O?Connor et al., 2010; Pak and Paroubek, 2010;
Tumasjan et al., 2010; Kouloumpis et al., 2011).
Several corpora with detailed opinion and sen-
timent annotation have been made freely avail-
able, e.g., the MPQA newswire corpus (Wiebe et
al., 2005), the movie reviews corpus (Pang et al.,
2002), or the restaurant and laptop reviews cor-
pora that are part of this year?s SemEval Task 4
(Pontiki et al., 2014). These corpora have proved
very valuable as resources for learning about the
language of sentiment in general, but they do not
focus on tweets. While some Twitter sentiment
datasets were created prior to SemEval-2013, they
were either small and proprietary, such as the i-
sieve corpus (Kouloumpis et al., 2011) or focused
solely on message-level sentiment.
Thus, the primary goal of our SemEval task is
to promote research that will lead to better un-
derstanding of how sentiment is conveyed in So-
cial Media. Toward that goal, we created the Se-
mEval Tweet corpus as part of our inaugural Sen-
timent Analysis in Twitter Task, SemEval-2013
Task 2 (Nakov et al., 2013). It contains tweets
and SMS messages with sentiment expressions an-
notated with contextual phrase-level and message-
level polarity. This year, we extended the corpus
by adding new tweets and LiveJournal sentences.
Another interesting phenomenon that has been
studied in Twitter is the use of the #sarcasm hash-
tag to indicate that a tweet should not be taken lit-
erally (Gonz?alez-Ib?a?nez et al., 2011; Liebrecht et
al., 2013). In fact, sarcasm indicates that the mes-
sage polarity should be flipped. With this in mind,
this year, we also evaluate on sarcastic tweets.
73
In the remainder of this paper, we first describe
the task, the dataset creation process and the eval-
uation methodology. We then summarize the char-
acteristics of the approaches taken by the partici-
pating systems, and we discuss their scores.
2 Task Description
As SemEval-2013 Task 2, we included two sub-
tasks: an expression-level subtask and a message-
level subtask. Participants could choose to partici-
pate in either or both. Below we provide short de-
scriptions of the objectives of these two subtasks.
Subtask A: Contextual Polarity Disambiguation
Given a message containing a marked in-
stance of a word or a phrase, determine
whether that instance is positive, negative or
neutral in that context. The instance bound-
aries were provided: this was a classification
task, not an entity recognition task.
Subtask B: Message Polarity Classification
Given a message, decide whether it is of
positive, negative, or neutral sentiment.
For messages conveying both positive and
negative sentiment, the stronger one is to be
chosen.
Each participating team was allowed to submit
results for two different systems per subtask: one
constrained, and one unconstrained. A constrained
system could only use the provided data for train-
ing, but it could also use other resources such as
lexicons obtained elsewhere. An unconstrained
system could use any additional data as part of
the training process; this could be done in a super-
vised, semi-supervised, or unsupervised fashion.
Note that constrained/unconstrained refers to
the data used to train a classifier. For example,
if other data (excluding the test data) was used to
develop a sentiment lexicon, and the lexicon was
used to generate features, the system would still
be constrained. However, if other data (excluding
the test data) was used to develop a sentiment lexi-
con, and this lexicon was used to automatically la-
bel additional Tweet/SMS messages and then used
with the original data to train the classifier, then
such a system would be considered unconstrained.
3 Datasets
In this section, we describe the process of collect-
ing and annotating the 2014 testing tweets, includ-
ing the sarcastic ones, and LiveJournal sentences.
Corpus Positive Negative Objective
/ Neutral
Twitter2013-train 5,895 3,131 471
Twitter2013-dev 648 430 57
Twitter2013-test 2,734 1,541 160
SMS2013-test 1,071 1,104 159
Twitter2014-test 1,807 578 88
Twitter2014-sarcasm 82 37 5
LiveJournal2014-test 660 511 144
Table 1: Dataset statistics for Subtask A.
3.1 Datasets Used
For training and development, we released the
Twitter train/dev/test datasets from SemEval-2013
task 2, as well as the SMS test set, which uses mes-
sages from the NUS SMS corpus (Chen and Kan,
2013), which we annotated for sentiment in 2013.
We further added a new 2014 Twitter test set,
as well as a small set of tweets that contained
the #sarcasm hashtag to determine how sarcasm
affects the tweet polarity. Finally, we included
sentences from LiveJournal in order to determine
how systems trained on Twitter perform on other
sources. The statistics for each dataset and for
each subtask are shown in Tables 1 and 2.
Corpus Positive Negative Objective
/ Neutral
Twitter2013-train 3,662 1,466 4,600
Twitter2013-dev 575 340 739
Twitter2013-test 1,572 601 1,640
SMS2013-test 492 394 1,207
Twitter2014-test 982 202 669
Twitter2014-sarcasm 33 40 13
LiveJournal2014-test 427 304 411
Table 2: Dataset statistics for Subtask B.
3.2 Annotation
We annotated the new tweets as in 2013: by iden-
tifying tweets from popular topics that contain
sentiment-bearing words by using SentiWordNet
(Baccianella et al., 2010) as a filter. We altered the
annotation task for the sarcastic tweets, displaying
them to the Mechanical Turk annotators without
the #sarcasm hashtag; the Turkers had to deter-
mine whether the tweet is sarcastic on their own.
Moreover, we asked Turkers to indicate the degree
of sarcasm as (a) definitely sarcastic, (b) probably
sarcastic, and (c) not sarcastic.
As in 2013, we combined the annotations using
intersection, where a word had to appear in 2/3
of the annotations to be accepted. An annotated
example from each source is shown in Table 3.
74
Source Example Polarity
Twitter Why would you [still]- wear shorts when it?s this cold?! I [love]+ how Britain see?s a
bit of sun and they?re [like ?OOOH]+ LET?S STRIP!?
positive
SMS [Sorry]- I think tonight [cannot]- and I [not feeling well]- after my rest. negative
LiveJournal [Cool]+ posts , dude ; very [colorful]+ , and [artsy]+ . positive
Twitter Sarcasm [Thanks]+ manager for putting me on the schedule for Sunday negative
Table 3: Example of polarity for each source of messages. The target phrases are marked in [. . .], and
are followed by their polarity; the sentence-level polarity is shown in the last column.
3.3 Tweets Delivery
We did not deliver the annotated tweets to the par-
ticipants directly; instead, we released annotation
indexes, a list of corresponding Twitter IDs, and
a download script that extracts the correspond-
ing tweets via the Twitter API.
2
We provided the
tweets in this manner in order to ensure that Twit-
ter?s terms of service are not violated. Unfor-
tunately, due to this restriction, the task partici-
pants had access to different number of training
tweets depending on when they did the download-
ing. This varied between a minimum of 5,215
tweets and the full set of 10,882 tweets. On av-
erage the teams were able to collect close to 9,000
tweets; for teams that did not participate in 2013,
this was about 8,500. The difference in training
data size did not seem to have had a major impact.
In fact, the top two teams in subtask B (coooolll
and TeamX) trained on less than 8,500 tweets.
4 Scoring
The participating systems were required to per-
form a three-way classification for both subtasks.
A particular marked phrase (for subtask A) or an
entire message (for subtask B) was to be classi-
fied as positive, negative or objective/neutral. We
scored the systems by computing a score for pre-
dicting positive/negative phrases/messages. For
instance, to compute positive precision, p
pos
, we
find the number of phrases/messages that a sys-
tem correctly predicted to be positive, and we di-
vide that number by the total number it predicted
to be positive. To compute positive recall, r
pos
,
we find the number of phrases/messages correctly
predicted to be positive and we divide that number
by the total number of positives in the gold stan-
dard. We then calculate F1-score for the positive
class as follows F
pos
=
2(p
pos
+r
pos
)
p
pos
?r
pos
. We carry
out a similar computation for F
neg
, for the nega-
tive phrases/messages. The overall score is then
F = (F
pos
+ F
neg
)/2.
2
https://dev.twitter.com
We used the two test sets from 2013 and the
three from 2014, which we combined into one test
set and we shuffled to make it hard to guess which
set a sentence came from. This guaranteed that
participants would submit predictions for all five
test sets. It also allowed us to test how well sys-
tems trained on standard tweets generalize to sar-
castic tweets and to LiveJournal sentences, with-
out the participants putting extra efforts into this.
The participants were also not informed about the
source the extra test sets come from.
We provided the participants with a scorer that
outputs the overall score F and a confusion matrix
for each of the five test sets.
5 Participants and Results
The results are shown in Tables 4 and 5, and the
team affiliations are shown in Table 6. Tables 4
and 5 contain results on the two progress test sets
(tweets and SMS messages), which are the official
test sets from the 2013 edition of the task, and on
the three new official 2014 testsets (tweets, tweets
with sarcasm, and LiveJournal). The tables fur-
ther show macro- and micro-averaged results over
the 2014 datasets. There is an index for each re-
sult showing the relative rank of that result within
the respective column. The participating systems
are ranked by their score on the Twitter-2014 test-
set, which is the official ranking for the task; all
remaining rankings are secondary.
As we mentioned above, the participants were
not told that the 2013 test sets would be included
in the big 2014 test set, so that they do not over-
tune their systems on them. However, the 2013
test sets were made available for development, but
it was explicitly forbidden to use them for training.
Still, some participants did not notice this restric-
tion, which resulted in their unusually high scores
on Twitter2013-test; we did our best to identify
all such cases, and we asked the authors to submit
corrected runs. The tables mark such resubmis-
sions accordingly.
75
Most of the submissions were constrained, with
just a few unconstrained: 7 out of 27 for subtask
A, and 8 out of 50 for subtask B. In any case, the
best systems were constrained. Some teams par-
ticipated with both a constrained and an uncon-
strained system, but the unconstrained system was
not always better than the constrained one: some-
times it was worse, sometimes it performed the
same. Thus, we decided to produce a single rank-
ing, including both constrained and unconstrained
systems, where we mark the latter accordingly.
5.1 Subtask A
Table 4 shows the results for subtask A, which at-
tracted 27 submissions from 21 teams. There were
seven unconstrained submissions: five teams sub-
mitted both a constrained and an unconstrained
run, and two teams submitted an unconstrained
run only. The best systems were constrained. All
participating systems outperformed the majority
class baseline by a sizable margin.
5.2 Subtask B
The results for subtask B are shown in Table 5.
The subtask attracted 50 submissions from 44
teams. There were eight unconstrained submis-
sions: six teams submitted both a constrained and
an unconstrained run, and two teams submitted an
unconstrained run only. As for subtask A, the best
systems were constrained. Again, all participating
systems outperformed the majority class baseline;
however, some systems were very close to it.
6 Discussion
Overall, we observed similar trends as in
SemEval-2013 Task 2. Almost all systems used
supervised learning. Most systems were con-
strained, including the best ones in all categories.
As in 2013, we observed several cases of a team
submitting a constrained and an unconstrained run
and the constrained run performing better.
It is unclear why unconstrained systems did not
outperform constrained ones. It could be because
participants did not use enough external data or
because the data they used was too different from
Twitter or from our annotation method. Or it could
be due to our definition of unconstrained, which
labels as unconstrained systems that use additional
tweets directly, but considers unconstrained those
that use additional tweets to build sentiment lexi-
cons and then use these lexicons.
As in 2013, the most popular classifiers were
SVM, MaxEnt, and Naive Bayes. Moreover, two
submissions used deep learning, coooolll (Harbin
Institute of Technology) and ThinkPositive (IBM
Research, Brazil), which were ranked second and
tenth on subtask B, respectively.
The features used were quite varied, includ-
ing word-based (e.g., word and character n-
grams, word shapes, and lemmata), syntactic, and
Twitter-specific such as emoticons and abbrevia-
tions. The participants still relied heavily on lex-
icons of opinion words, the most popular ones
being the same as in 2013: MPQA, SentiWord-
Net and Bing Liu?s opinion lexicon. Popular this
year was also the NRC lexicon (Mohammad et
al., 2013), created by the best-performing team in
2013, which is top-performing this year as well.
Preprocessing of tweets was still a popular tech-
nique. In addition to standard NLP steps such
as tokenization, stemming, lemmatization, stop-
word removal and POS tagging, most teams ap-
plied some kind of Twitter-specific processing
such as substitution/removal of URLs, substitu-
tion of emoticons, word normalization, abbrevi-
ation lookup, and punctuation removal. Finally,
several of the teams used Twitter-tuned NLP tools
such as part of speech and named entity taggers
(Gimpel et al., 2011; Ritter et al., 2011).
The similarity of preprocessing techniques,
NLP tools, classifiers and features used in 2013
and this year is probably partially due to many
teams participating in both years. As Table 6
shows, 18 out of the 46 teams are returning teams.
Comparing the results on the progress Twit-
ter test in 2013 and 2014, we can see that NRC-
Canada, the 2013 winner for subtask A, have
now improved their F1 score from 88.93 to 90.14,
which is the 2014 best score. The best score on the
Progress SMS in 2014 of 89.31 belongs to ECNU;
this is a big jump compared to their 2013 score of
76.69, but it is less compared to the 2013 best of
88.37 achieved by GU-MLT-LT. For subtask B, on
the Twitter progress testset, the 2013 winner NRC-
Canada improves their 2013 result from 69.02 to
70.75, which is the second best in 2014; the win-
ner in 2014, TeamX, achieves 72.12. On the SMS
progress test, the 2013 winner NRC-Canada im-
proves its F1 score from 68.46 to 70.28. Overall,
we see consistent improvements on the progress
testset for both subtasks: 0-1 and 2-3 points abso-
lute for subtasks A and B, respectively.
76
Uncon- 2013: Progress 2014: Official 2014: Average
# System strain.? Tweet SMS Tweet Tweet Live- Macro Micro
sarcasm Journal
1 NRC-Canada 90.14
1
88.03
4
86.63
1
77.13
5
85.49
2
83.08
2
85.61
1
2 SentiKLUE 90.11
2
85.16
8
84.83
2
79.32
3
85.61
1
83.25
1
85.15
2
3 CMUQ-Hybrid
?
88.94
4
87.98
5
84.40
3
76.99
6
84.21
3
81.87
3
84.05
3
4 CMU-Qatar
?
89.85
3
88.08
3
83.45
4
78.07
4
83.89
5
81.80
4
83.56
4
5 ECNU X 87.29
6
89.26
2
82.93
5
73.71
8
81.69
7
79.44
7
81.85
6
6 ECNU 87.28
7
89.31
1
82.67
6
73.71
9
81.67
8
79.35
8
81.75
7
7 Think Positive X 88.06
5
87.65
6
82.05
7
76.74
7
80.90
12
79.90
6
81.15
9
8 Kea
?
84.83
10
84.14
10
81.22
8
65.94
17
81.16
11
76.11
13
80.70
10
9 Lt 3 86.28
8
85.26
7
81.02
9
70.76
13
80.44
13
77.41
11
80.33
13
10 senti.ue 84.05
11
78.72
16
80.54
10
82.75
1
81.90
6
81.73
5
81.47
8
11 LyS 85.69
9
81.44
12
79.92
11
71.67
10
83.95
4
78.51
10
82.21
5
12 UKPDIPF 80.45
15
79.05
14
79.67
12
65.63
18
81.42
9
75.57
14
80.33
11
13 UKPDIPF X 80.45
16
79.05
15
79.67
13
65.63
19
81.42
10
75.57
15
80.33
12
14 TJP 81.13
14
84.41
9
79.30
14
71.20
12
78.27
15
76.26
12
78.39
15
15 SAP-RI 80.32
17
80.26
13
77.26
15
70.64
14
77.68
18
75.19
17
77.32
16
16 senti.ue
?
X 83.80
12
82.93
11
77.07
16
80.02
2
79.70
14
78.93
9
78.83
14
17 SAIL 78.47
18
74.46
20
76.89
17
65.56
20
70.62
22
71.02
21
72.57
21
18 columbia nlp

81.50
13
74.55
19
76.54
18
61.76
22
78.19
16
72.16
19
77.11
18
19 IIT-Patna 76.54
20
75.99
18
76.43
19
71.43
11
77.99
17
75.28
16
77.26
17
20 Citius X 76.59
19
69.31
21
75.21
20
68.40
15
75.82
20
73.14
18
75.38
19
21 Citius 74.71
21
61.44
25
73.03
21
65.18
21
71.64
21
69.95
22
71.90
22
22 IITPatna 70.91
23
77.04
17
72.25
22
66.32
16
76.03
19
71.53
20
74.45
20
23 SU-sentilab 74.34
22
62.58
24
68.26
23
53.31
25
69.53
23
63.70
24
68.59
23
24 Univ. Warwick
?
62.25
26
60.12
26
67.28
24
58.08
24
64.89
25
63.42
25
65.48
25
25 Univ. Warwick
?
X 64.91
25
63.01
23
67.17
25
60.59
23
67.46
24
65.07
23
67.14
24
26 DAEDALUS 67.42
24
63.92
22
60.98
26
45.27
27
61.01
26
55.75
26
60.50
26
27 DAEDALUS X 61.95
27
55.97
27
58.11
27
49.19
26
58.65
27
55.32
27
58.17
27
Majority baseline 38.1 31.5 42.2 39.8 33.4
Table 4: Results for subtask A. The
?
indicates system resubmissions (because they initially trained on
Twitter2013-test), and the

indicates a system that includes a task co-organizer as a team member. The
systems are sorted by their score on the Twitter2014 test dataset; the rankings on the individual datasets
are indicated with a subscript. The last two columns show macro- and micro-averaged results across the
three 2014 test datasets.
Finally, note that for both subtasks, the best sys-
tems on the Twitter-2014 dataset are those that per-
formed best on the 2013 progress Twitter dataset:
NRC-Canada for subtask A, and TeamX (Fuji Xe-
rox Co., Ltd.) for subtask B.
It is interesting to note that the best results
for Twitter2014-test are lower than those for
Twitter2013-test for both subtask A (86.63 vs.
90.14) and subtask B (70.96 vs 72.12). This is
so despite the baselines for Twitter2014-test be-
ing higher than those for Twitter2013-test: 42.2 vs.
38.1 for subtask A, and 34.6 vs. 29.2 for subtask
B. Most likely, having access to Twitter2013-test
at development time, teams have overfitted on it. It
could be also the case that some of the sentiment
dictionaries that were built in 2013 have become
somewhat outdated by 2014.
Finally, note that while some teams such as
NRC-Canada performed well across all test sets,
other such as TeamX, which used a weighting
scheme tuned specifically for class imbalances in
tweets, were only strong on Twitter datasets.
7 Conclusion
We have described the data, the experimental
setup and the results for SemEval-2014 Task 9.
As in 2013, our task was the most popular one at
SemEval-2014, attracting 46 participating teams:
21 in subtask A (27 submissions) and 44 in sub-
task B (50 submissions).
We introduced three new test sets for 2014: an
in-domain Twitter dataset, an out-of-domain Live-
Journal test set, and a dataset of tweets contain-
ing sarcastic content. While the performance on
the LiveJournal test set was mostly comparable
to the in-domain Twitter test set, for most teams
there was a sharp drop in performance for sarcas-
tic tweets, highlighting better handling of sarcas-
tic language as one important direction for future
work in Twitter sentiment analysis.
We plan to run the task again in 2015 with the
inclusion of a new sub-evaluation on detecting sar-
casm with the goal of stimulating research in this
area; we further plan to add one more test domain.
77
Uncon- 2013: Progress 2014: Official 2014: Average
# System strain.? Tweet SMS Tweet Tweet Live- Macro Micro
sarcasm Journal
1 TeamX 72.12
1
57.36
26
70.96
1
56.50
3
69.44
15
65.63
3
69.99
5
2 coooolll 70.40
3
67.68
2
70.14
2
46.66
24
72.90
5
63.23
12
70.51
2
3 RTRGO 69.10
5
67.51
3
69.95
3
47.09
23
72.20
6
63.08
13
70.15
3
4 NRC-Canada 70.75
2
70.28
1
69.85
4
58.16
1
74.84
1
67.62
1
71.37
1
5 TUGAS 65.64
13
62.77
11
69.00
5
52.87
12
69.79
13
63.89
6
68.84
8
6 CISUC KIS
?
67.56
8
65.90
6
67.95
6
55.49
5
74.46
2
65.97
2
70.02
4
7 SAIL 66.80
11
56.98
28
67.77
7
57.26
2
69.34
17
64.79
4
68.06
10
8 SWISS-CHOCOLATE 64.81
18
66.43
5
67.54
8
49.46
16
73.25
4
63.42
10
69.15
6
9 Synalp-Empathic 63.65
23
62.54
12
67.43
9
51.06
15
71.75
9
63.41
11
68.57
9
10 Think Positive X 68.15
7
63.20
9
67.04
10
47.85
21
66.96
24
60.62
18
66.47
15
11 SentiKLUE 69.06
6
67.40
4
67.02
11
43.36
30
73.99
3
61.46
14
68.94
7
12 JOINT FORCES X 66.61
12
62.20
13
66.79
12
45.40
26
70.02
12
60.74
17
67.39
12
13 AMI ERIC 70.09
4
60.29
20
66.55
13
48.19
20
65.32
26
60.02
21
65.58
20
14 AUEB 63.92
21
64.32
8
66.38
14
56.16
4
70.75
11
64.43
5
67.71
11
15 CMU-Qatar
?
65.11
17
62.95
10
65.53
15
40.52
38
65.63
25
57.23
27
64.87
24
16 Lt 3 65.56
14
64.78
7
65.47
16
47.76
22
68.56
20
60.60
19
66.12
17
17 columbia nlp

64.60
19
59.84
21
65.42
17
40.02
40
68.79
19
58.08
25
65.96
19
18 LyS 66.92
10
60.45
19
64.92
18
42.40
33
69.79
14
59.04
22
66.10
18
19 NILC USP 65.39
15
61.35
16
63.94
19
42.06
34
69.02
18
58.34
24
65.21
21
20 senti.ue 67.34
9
59.34
23
63.81
20
55.31
6
71.39
10
63.50
7
66.38
16
21 UKPDIPF 60.65
29
60.56
17
63.77
21
54.59
7
71.92
7
63.43
8
66.53
13
22 UKPDIPF X 60.65
30
60.56
18
63.77
22
54.59
8
71.92
8
63.43
9
66.53
14
23 SU-FMI
?
60.96
28
61.67
15
63.62
23
48.34
19
68.24
21
60.07
20
64.91
23
24 ECNU 62.31
27
59.75
22
63.17
24
51.43
14
69.44
16
61.35
15
65.17
22
25 ECNU X 63.72
22
56.73
29
63.04
25
49.33
17
64.08
31
58.82
23
63.04
27
26 Rapanakis 58.52
32
54.02
35
63.01
26
44.69
27
59.71
37
55.80
31
61.28
32
27 Citius X 63.25
24
58.28
24
62.94
27
46.13
25
64.54
29
57.87
26
63.06
26
28 CMUQ-Hybrid
?
63.22
25
61.75
14
62.71
28
40.95
37
65.14
27
56.27
30
63.00
28
29 Citius 62.53
26
57.69
25
61.92
29
41.00
36
62.40
33
55.11
33
61.51
31
30 KUNLPLab 58.12
33
55.89
31
61.72
30
44.60
28
63.77
32
56.70
29
62.00
29
31 senti.ue
?
X 65.21
16
56.16
30
61.47
31
54.09
9
68.08
22
61.21
16
63.71
25
32 UPV-ELiRF 63.97
20
55.36
33
59.33
32
37.46
42
64.11
30
53.63
37
60.49
33
33 USP Biocom 58.05
34
53.57
36
59.21
33
43.56
29
67.80
23
56.86
28
61.96
30
34 DAEDALUS X 58.94
31
54.96
34
57.64
34
35.26
44
60.99
35
51.30
39
58.26
35
35 IIT-Patna 52.58
40
51.96
37
57.25
35
41.33
35
60.39
36
52.99
38
57.97
36
36 DejaVu 57.43
36
55.57
32
57.02
36
42.46
32
64.69
28
54.72
34
59.46
34
37 GPLSI 57.49
35
46.63
42
56.06
37
53.90
10
57.32
41
55.76
32
56.47
37
38 BUAP 56.85
37
44.27
44
55.76
38
51.52
13
53.94
44
53.74
36
54.97
39
39 SAP-RI 50.18
44
49.00
41
55.47
39
48.64
18
57.86
40
53.99
35
56.17
38
40 UMCC DLSI Sem 51.96
41
50.01
38
55.40
40
42.76
31
53.12
45
50.43
40
54.20
42
41 IBM EG 54.51
38
46.62
43
52.26
41
34.14
46
59.24
38
48.55
43
54.34
41
42 Alberta 53.85
39
49.05
40
52.06
42
40.40
39
52.38
46
48.28
44
51.85
44
43 lsis lif 46.38
46
38.56
47
52.02
43
34.64
45
61.09
34
49.25
41
54.90
40
44 SU-sentilab 50.17
45
49.60
39
49.52
44
31.49
47
55.11
42
45.37
47
51.09
45
45 SINAI 50.59
42
57.34
27
49.50
45
31.15
49
58.33
39
46.33
46
52.26
43
46 IITPatna 50.32
43
40.56
46
48.22
46
36.73
43
54.68
43
46.54
45
50.29
46
47 Univ. Warwick 39.17
48
29.50
49
45.56
47
39.77
41
39.60
49
41.64
48
43.19
48
48 UMCC DLSI Graph 43.24
47
36.66
48
45.49
48
53.15
11
47.81
47
48.82
42
46.56
47
49 Univ. Warwick X 34.23
50
24.63
50
45.11
49
31.40
48
29.34
50
35.28
49
38.88
49
50 DAEDALUS 36.57
49
40.86
45
33.03
50
28.96
50
40.83
48
34.27
50
35.81
50
Majority baseline 29.2 19.0 34.6 27.7 27.2
Table 5: Results for subtask B. The
?
indicates system resubmissions (because they initially trained on
Twitter2013-test), and the

indicates a system that includes a task co-organizer as a team member. The
systems are sorted by their score on the Twitter2014 test dataset; the rankings on the individual datasets
are indicated with a subscript. The last two columns show macro- and micro-averaged results across the
three 2014 test datasets.
In the 2015 edition of the task, we might also
remove the constrained/unconstrained distinction.
Finally, as there are multiple opinions about a
topic in Twitter, we would like to focus on detect-
ing the sentiment trend towards a topic.
Acknowledgements
We would like to thank Kathleen McKeown and
Smaranda Muresan for funding the 2014 Twitter
test sets. We also thank the anonymous reviewers.
78
Subtasks Team Affiliation 2013?
B Alberta University of Alberta
B AMI ERIC AMI Software R&D and Universit?e de Lyon (ERIC LYON 2) yes
B AUEB Athens University of Economics and Business yes
B BUAP Benem?erita Universidad Aut?onoma de Puebla
B CISUC KIS University of Coimbra
A, B Citius University of Santiago de Compostela
A, B CMU-Qatar Carnegie Mellon University, Qatar
A, B CMUQ-Hybrid Carnegie Mellon University, Qatar (different from the above)
A, B columbia nlp Columbia University yes
B cooolll Harbin Institute of Technology
A, B DAEDALUS Daedalus
B DejaVu Indian Institute of Technology, Kanpur
A, B ECNU East China Normal University yes
B GPLSI University of Alicante
B IBM EG IBM Egypt
A, B IITPatna Indian Institute of Technology, Patna
A, B IIT-Patna Indian Institute of Technology, Patna (different from the above)
B JOINT FORCES Zurich University of Applied Sciences
A Kea York University, Toronto yes
B KUNLPLab Koc? University
B lsis lif Aix-Marseille University yes
A, B Lt 3 Ghent University
A, B LyS Universidade da Coru?na
B NILC USP University of S?ao Paulo yes
A, B NRC-Canada National Research Council Canada yes
B Rapanakis Stamatis Rapanakis
B RTRGO Retresco GmbH and University of Gothenburg yes
A, B SAIL Signal Analysis and Interpretation Laboratory yes
A, B SAP-RI SAP Research and Innovation
A, B senti.ue Universidade de
?
Evora yes
A, B SentiKLUE Friedrich-Alexander-Universit?at Erlangen-N?urnberg yes
B SINAI University of Ja?en yes
B SU-FMI Sofia University
A, B SU-sentilab Sabanci University yes
B SWISS-CHOCOLATE ETH Zurich
B Synalp-Empathic University of Lorraine
B TeamX Fuji Xerox Co., Ltd.
A, B Think Positive IBM Research, Brazil
A TJP University of Northumbria at Newcastle Upon Tyne yes
B TUGAS Instituto de Engenharia de Sistemas e Computadores, yes
Investigac??ao e Desenvolvimento em Lisboa
A, B UKPDIPF Ubiquitous Knowledge Processing Lab
B UMCC DLSI Graph Universidad de Matanzas and Univarsidad de Alicante yes
B UMCC DLSI Sem Universidad de Matanzas and Univarsidad de Alicante (different from above) yes
A, B Univ. Warwick University of Warwick
B UPV-ELiRF Universitat Polit`ecnica de Val`encia
B USP Biocom University of S?ao Paulo and Federal University of S?ao Carlos
Table 6: Participating teams, their affiliations, subtasks they have taken part in, and an indication about
whether the team participated in SemEval-2013 Task 2.
79
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. SentiWordNet 3.0: An enhanced
lexical resource for sentiment analysis and opinion
mining. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation,
LREC ?10, Valletta, Malta.
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on Twitter from biased and noisy
data. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 36?44, Beijing, China.
Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer,
and Ricard Gavald`a. 2011. Detecting sentiment
change in Twitter streaming data. Journal of Ma-
chine Learning Research, Proceedings Track, 17:5?
11.
Tao Chen and Min-Yen Kan. 2013. Creating a
live, public short message service corpus: the NUS
SMS corpus. Language Resources and Evaluation,
47(2):299?335.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcasm in Twitter
and Amazon. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing, CoNLL ?10, pages 107?116, Uppsala, Sweden.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: Annotation, features, and experiments.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, ACL-HLT ?11, pages 42?
47, Portland, Oregon, USA.
Roberto Gonz?alez-Ib?a?nez, Smaranda Muresan, and
Nina Wacholder. 2011. Identifying sarcasm in Twit-
ter: a closer look. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Short Pa-
pers, ACL-HLT ?11, pages 581?586, Portland, Ore-
gon, USA.
Bernard Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Twitter power: Tweets as elec-
tronic word of mouth. J. Am. Soc. Inf. Sci. Technol.,
60(11):2169?2188.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The
good the bad and the OMG! In Proceedings of
the Fifth International Conference on Weblogs and
Social Media, ICWSM ?11, Barcelona, Catalonia,
Spain.
Christine Liebrecht, Florian Kunneman, and Antal
Van den Bosch. 2013. The perfect solution for de-
tecting sarcasm in tweets #not. In Proceedings of
the 4th Workshop on Computational Approaches to
Subjectivity, Sentiment and Social Media Analysis,
pages 29?37, Atlanta, Georgia, USA.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-
the-art in sentiment analysis of tweets. In Proceed-
ings of the Seventh international workshop on Se-
mantic Evaluation Exercises, SemEval-2013, pages
321?327, Atlanta, Georgia, USA.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 task 2: Sentiment analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation, SemEval ?13, pages 312?320,
Atlanta, Georgia, USA.
Brendan O?Connor, Ramnath Balasubramanyan, Bryan
Routledge, and Noah Smith. 2010. From tweets
to polls: Linking text sentiment to public opinion
time series. In Proceedings of the Fourth Inter-
national Conference on Weblogs and Social Media,
ICWSM ?10, Washington, DC, USA.
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter based system: Using Twitter for disambiguating
sentiment ambiguous adjectives. In Proceedings of
the 5th International Workshop on Semantic Evalu-
ation, SemEval ?10, pages 436?439, Uppsala, Swe-
den.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing - Volume 10, EMNLP ?02, pages
79?86.
Maria Pontiki, Harris Papageorgiou, Dimitrios Gala-
nis, Ion Androutsopoulos, John Pavlopoulos, and
Suresh Manandhar. 2014. SemEval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation, SemEval ?14, Dublin, Ireland.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?11, pages 1524?1534, Edinburgh,
Scotland, UK.
Andranik Tumasjan, Timm Sprenger, Philipp Sandner,
and Isabell Welpe. 2010. Predicting elections with
Twitter: What 140 characters reveal about politi-
cal sentiment. In Proceedings of the Fourth Inter-
national Conference on Weblogs and Social Media,
ICWSM ?10, Washington, DC, USA.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165?210.
80
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 198?202,
Dublin, Ireland, August 23-24, 2014.
Columbia NLP: Sentiment Detection of Sentences and Subjective Phrases
in Social Media
Sara Rosenthal
Dept. of Computer Science
Columbia University
New York, NY 10027, USA
sara@cs.columbia.edu
Apoorv Agarwal
Dept. of Computer Science
Columbia University
New York, NY 10027, USA
apoorv@cs.columbia.edu
Kathleen McKeown
Dept. of Computer Science
Columbia University
New York, NY 10027, USA
kathy@cs.columbia.edu
Abstract
We present two supervised sentiment de-
tection systems which were used to com-
pete in SemEval-2014 Task 9: Senti-
ment Analysis in Twitter. The first sys-
tem (Rosenthal and McKeown, 2013) clas-
sifies the polarity of subjective phrases as
positive, negative, or neutral. It is tai-
lored towards online genres, specifically
Twitter, through the inclusion of dictionar-
ies developed to capture vocabulary used
in online conversations (e.g., slang and
emoticons) as well as stylistic features
common to social media. The second sys-
tem (Agarwal et al., 2011) classifies entire
tweets as positive, negative, or neutral. It
too includes dictionaries and stylistic fea-
tures developed for social media, several
of which are distinctive from those in the
first system. We use both systems to par-
ticipate in Subtasks A and B of SemEval-
2014 Task 9: Sentiment Analysis in Twit-
ter. We participated for the first time in
Subtask B: Message-Level Sentiment De-
tection by combining the two systems to
achieve improved results compared to ei-
ther system alone.
1 Introduction
In this paper we describe two prior sentiment de-
tection algorithms for social media. Both systems
(Rosenthal and McKeown, 2013; Agarwal et al.,
2011) classify the polarity of sentence phrases and
This work is licensed under a Creative Commons At-
tribution 4.0 International License. Page numbers and pro-
ceedings footer are added by the organizers. License details:
http://creativecommons.org/licenses/by/4.0/
tweets as positive, negative, or neutral. These al-
gorithms were used to participate in the the expres-
sion level task (Subtask A) and message level task
(Subtask B) of the SemEval-2014 Task 9: Senti-
ment Analysis in Twitter (Rosenthal et al., 2014)
which one of the authors helped organize.
We first show improved results compared to our
participation in the prior year in the expression-
level task (Subtask A) by incorporating a new dic-
tionary and new features into the system. Our fo-
cus this year was on Subtask B which we partici-
pated in for the first time. We integrated two sys-
tems to achieve improved results compared to ei-
ther system alone. Our analysis shows that the first
system performs better on recall while the second
system performs better on precision. We used con-
fidence metrics outputted by the systems to deter-
mine which answer should be used. This resulted
in a slight improvement in the Twitter dataset com-
pared to either system alone. In this rest of this
paper, we discuss related work, the methods for
each system, and experiments and results for each
subtask using the data provided by Semeval-2014
Task 9: Sentiment Analysis in Twitter (Rosenthal
et al., 2014).
2 Related Work
Several recent papers have explored sentiment
analysis in Twitter. Go et al (2009) and Pak
and Paroubek (2010) classify the sentiment of
tweets containing emoticons using n-grams and
POS. Barbosa and Feng (2010) detect sentiment
using a polarity dictionary that includes web vo-
cabulary and tweet-specific social media features.
Bermingham and Smeaton (2010) compare polar-
ity detection in twitter to blogs and movie reviews
using lexical features.
Finally, there is a large amount of related work
198
through the participants of Semeval 2013 Task
2, and Semeval 2014 Task9: Sentiment Analysis
in Twitter (Nakov et al., 2013; Rosenthal et al.,
2014). A full list of teams and results can be found
in the task description papers.
3 Phrased-Based Sentiment Detection
We developed a phrase based sentiment detection
system geared towards Social Media by augment-
ing the state of the art system developed by Agar-
wal et al. (2009) to include additional dictionar-
ies such as Wiktionary and new features such as
word lengthening (e.g. helllllloooo) and emoti-
cons (e.g. :)) (Rosenthal and McKeown, 2013).
We initially evaluated our system through our par-
ticipation in the first Sentiment Analysis in Twitter
task (Nakov et al., 2013). We have improved our
system this year by adding a new dictionary and
additional features.
3.1 Lexicons
We assign a prior polarity score to each word by
using the scores provided by the Dictionary of
Affect in Language (DAL) (Whissel, 1989) aug-
mented with WordNet (Fellbaum, 1998) to im-
prove coverage. We additionally augment it with
Wiktionary, emoticon, and acronym dictionaries
to improve coverage in social media (Rosenthal
and McKeown, 2013). The DAL covers 50.1% of
the vocabulary, 16.5% are proper nouns which we
exclude due to their lack of polarity. WordNet cov-
ers 8.7% of the vocabulary and Wiktionary covers
12.5% of the vocabulary. Finally, 3.6% of the vo-
cabulary are emoticons, acronyms, word length-
ening, and forms of punctuation. 8.6% of the vo-
cabulary is not covered which means we find a
prior polarity for 96.4% of the vocabulary. In ad-
dition to these dictionaries we also use SentiWord-
Net (Baccianella et al., 2010) as a new distinct fea-
ture that is used in addition to the prior polarity
computed from the DAL scores.
3.2 Method
We include POS tags and the top n-gram fea-
tures as described in prior work (Agarwal et al.,
2009; Rosenthal and McKeown, 2013). The DAL
and other dictionaries are used along with a nega-
tion state machine (Agarwal et al., 2009) to deter-
mine the polarity for each word in the sentence.
We include all the features described in the orig-
inal system (Agarwal et al., 2009) such as the
Data Set Majority 2013 2014
Twitter Dev 38.14 77.6 81.5
Twitter Test 42.22 N/A 76.54
Twitter Sarcasm 39.81 N/A 61.76
SMS 31.45 73.3 74.55
LiveJournal 33.42 N/A 78.19
Table 1: A comparison between the 2013 and 2014
results for Subtask A using the SemEval Twitter
training corpus. All results exceed the majority
baseline of the positive class significantly.
DAL scores, polar chunk n-grams, and count of
syntactic chunks with their prior polarity based
on the chunks position. Finally, we include sev-
eral lexical-stylistic features that can occur in all
datasets. We divide these features into two groups,
general: ones that are common across online and
traditional genres (e.g. exclamation points), and
social media: one that are far more common in
online genres (e.g. emoticons). The features are
described in further detail in the precursor to this
work (Rosenthal and McKeown, 2013). Feature
selection was performed using chi-square in Weka
(Hall et al., 2009).
In addition we introduce some new features
that were not used in the prior year. SentiWord-
Net (Baccianella et al., 2010) is a sentiment dic-
tionary built upon WordNet that contains scores
for each word where scores > 0 indicate the word
is positive and scores < 0 indicate the word is neg-
ative. We sum the scores for each word in the
phrase and use this as a single polarity feature.
We found that this feature alone gave us a 2% im-
provement over our best results from last year. We
also include some other minor features: tweet and
phrase length and the position of the phrase within
the tweet.
3.3 Experiments and Results
We ran all of our experiments in Weka (Hall et al.,
2009) using Logistic Regression. We also exper-
imented with other learning methods (e.g. SVM
and Naive Bayes) but found that Logistic Regres-
sion worked the same or better than other methods.
All results are shown using the average F-measure
of the positive and negative class. The results are
compared against the majority baseline of the pos-
itive class. We do not use neutral/objective as the
majority class because it is not included in the av-
erage F-score in the Semeval task.
The full results in the participation of SemEval
2014: Sentiment Analysis in Twitter, Subtask A,
199
are shown in Table 1. Our system outperforms the
majority baseline significantly in all classes. Our
submitted system was trained using 3-way clas-
sification (positive/negative/polarity). It included
all the dictionaries from prior years and the top
100 n-grams with feature selection. In addition,
it included SentiWordNet and the other new fea-
tures added in 2014 which provided a 4% increase
compared to our best results during the prior year
(77.6% to 81.5%) and a rank of 10/20 amongst the
constrained systems which used no external data.
Our results on the new test set is 76.54% for a rank
of 14/20. We do not do well in detecting the po-
larity of phrases in sarcastic tweets. This is consis-
tent with the other teams as sarcastic tweets tend to
have their polarity flipped. The improvements to
our system provided a 1% boost in the SMS data
with a rank of 15/20. Finally, in the LiveJournal
dataset we had an F-Score of 78.19% for a rank of
12/20.
4 Message-Level Sentiment Detection
Our message-level system combines two prior sys-
tems to achieve improved results. The first system
inputs an entire tweet as a ?phrase? to the phrase-
level sentiment detection system described in Sec-
tion 3. The second system is described below.
4.1 Lexicons
The second system (Agarwal et al., 2011) makes
use of two dictionaries distinctive from the other
system: 1) an emoticon dictionary and 2) an
acronym dictionary. The emoticon dictionary was
prepared by hand-labeling 170 emoticons listed on
Wikipedia.
1
For example, :) is labeled as positive
whereas :=( is labeled as negative. Each emoticon
is assigned a label from the following set of labels:
Extremely-positive, Extremely-negative, Positive,
Negative, and Neutral. We compile an acronym
dictionary from an on-line resource.
2
The dictio-
nary has translations for 5,184 acronyms. For ex-
ample, lol is translated to laughing out loud.
4.2 Prior Polarity Scoring
A number of our features are based on prior po-
larity of words. As in the phrase-based system we
too build off of prior work (Agarwal et al., 2009)
by using the DAL and augmenting it with Word-
net. However, we do not follow the earlier method
1
http://en.wikipedia.org/wiki/List of emoticons
2
http://www.noslang.com/
but use it as motivation. We consider words with
with a polarity score (using the pleasantness met-
ric from the DAL) of less than 0.5 as negative,
higher than 0.8 as positive and the rest as neutral.
If a word is not directly found in the dictionary, we
retrieve all synonyms from Wordnet. We then look
for each of the synonyms in the DAL. If any syn-
onym is found in the DAL, we assign the original
word the same pleasantness score as its synonym.
If none of the synonyms is present in the DAL, the
word is not associated with any prior polarity. For
the given data we directly found the prior polar-
ity of 50.1% of the words. We find the polarity of
another 8.7% of the words by using WordNet. So
we find prior polarity of about 58.7% of English
language words.
4.3 Features
We propose a set of 50 features. We calculate these
features for the whole tweet and for the last one-
third of the tweet. In total, we get 100 additional
features. Our features may be divided into three
broad categories: ones that are primarily counts
of various features and therefore the value of the
feature is a natural number ? N. Second, we in-
clude features whose value is a real number ? R.
These are primarily features that capture the score
retrieved from DAL. The third category is features
whose values are boolean ? B. These are bag of
words, presence of exclamation marks and capital-
ized text. Each of these broad categories is divided
into two subcategories: Polar features and Non-
polar features. We refer to a feature as polar if we
calculate its prior polarity either by looking it up in
DAL (extended through WordNet) or in the emoti-
con dictionary. All other features which are not
associated with any prior polarity fall in the Non-
polar category. Each of the Polar and Non-polar
features is further subdivided into two categories:
POS and Other. POS refers to features that cap-
ture statistics about parts-of-speech of words and
Other refers to all other types of features.
A more detailed explanation of the system can
be found in Agarwal et al (2011).
4.4 Combined System
Our analysis showed that the first system performs
better on recall while the second system performs
better on precision. We also found that there were
785 tweets in the development set where one sys-
tem got it correct and the other one got it incorrect.
This leaves room for a significant improvement
200
Experiment Twitter SMS LiveJournal
Dev Test Sarcasm
Majority 29.19 34.64 27.73 19.03 27.21
Phrase-Based System 62.09 64.74 40.75 56.86 62.22
Tweet-Level System 62.4 63.73 42.41 60.54 69.44
Combined System 64.6 65.42 40.02 59.84 68.79
Table 2: A comparison between the different systems using the Twitter training corpus provided by the
SemEval task for Subtask B. All results exceed the majority baseline of the positive class significantly.
compared to using each system independently. We
combined the two systems for the evaluation by
using the confidence provided by the phrase-based
system. If the phrase-based system was < 70%
confident we use the message-level system.
4.5 Experiments and Results
This task was evaluated on the Twitter dataset pro-
vided by Semeval-2013 Task 2, Subtask B. All re-
sults are shown using the average F-measure of the
positive and negative class. The full results in the
participation of SemEval 2014: Sentiment Anal-
ysis in Twitter, Subtask B, are shown in Table 2.
All the results outperform the majority baseline of
the more prominent positive polarity class signifi-
cantly. The combined system outperforms the in-
dividual systems for the Twitter development and
test set. It does not outperform the sarcasm test set,
but this may be due to the small size; it contains
only 100 tweets. The Tweet-Level system outper-
forms the phrase-based and combined system for
the LiveJournal and SMS test sets. A closer look at
the results indicated that the phrase-based system
has particular difficulty with the short sentences
which are more common in SMS and LiveJour-
nal. For example, the average number of charac-
ters in a tweet is 120 whereas it is 95.6 in SMS
messages (Nakov et al., 2013). Short sentences
are harder because there are fewer polarity words
which causes the phrase-based system to incor-
rectly pick neutral. In addition, short sentences are
harder because the BOW feature space, which is
huge and already sparse, becomes sparser and in-
dividual features start to over-fit. Part of this prob-
lem is handled by using Senti-features so the space
will be less sparse.
Our ranking in the Twitter 2013 and SMS 2013
development data is 18/50 and 20/50 respectively.
Our rank in the Twitter 2014 test set is 15/50 and
our rank in the LiveJournal test set is 19/50. Based
on our rankings it is clear that our systems are
geared more towards Twitter than other social me-
dia. Finally our ranking in the Sarcasm test set is
41/50. Although this ranking is quite low, it is in
fact encouraging. It indicates that the sarcasm has
switched the polarity of the tweet. In the future we
would like to include a system (e.g. (Gonz?alez-
Ib?a?nez et al., 2011)) that can detect whether the
tweet is sarcastic.
5 Discussion and Future Work
We participated in Semeval-2014 Task 9: Senti-
ment Analysis in Twitter Subtasks A and B. In
Subtask A, we show that adding additional fea-
tures related to location and using SentiWord-
Net gives us improvement compared to our prior
system. In Subtask B, we show that combining
two systems achieves slight improvements over
using either system alone. Combining the two
system achieves greater coverage as the systems
use different emoticon and acronym dictionar-
ies and the phrase-based system uses Wiktionary.
The message-level system is geared toward entire
tweets whereas the phrase-based is geared toward
phrases (even though, in this case we consider the
entire tweet to be a ?phrase?). This is reflective in
several features, such as the position of the target
phrase and the syntactic chunk scores in the phrase
based system and the features related to the last
third of the tweet in the message-level system. In
the future, we?d like to perform an error analysis to
determine the source of our errors and specific ex-
amples of the kind of differences found in the two
systems. Finally, we have found that at times the
scores of the DAL do not line up with polarity in
social media. Therefore, we would like to explore
including more sentiment dictionaries instead of,
or in addition to, the DAL.
6 Acknowledgements
This research was funded by the DARPA DEFT
Program. All statements of fact, opinion or con-
clusions contained herein are those of the authors
and should not be construed as representing the
official views, policies, or positions of the Depart-
ment of Defense, or the U.S. Government.
201
References
Apoorv Agarwal, Fadi Biadsy, and Kathleen R. Mcke-
own. 2009. Contextual phrase-level polarity anal-
ysis using lexical affect scoring and syntactic n-
grams. In Proceedings of the 12th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, EACL ?09, pages 24?32, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-
bow, and Rebecca Passonneau. 2011. Sentiment
analysis of twitter data. In Proceedings of the Work-
shop on Language in Social Media (LSM 2011),
pages 30?38, Portland, Oregon, June. Association
for Computational Linguistics.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan
Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC?10), Valletta, Malta, may. European
Language Resources Association (ELRA).
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on twitter from biased and noisy
data. In COLING (Posters), pages 36?44.
Adam Bermingham and Alan F. Smeaton. 2010. Clas-
sifying sentiment in microblogs: is brevity an advan-
tage? In Jimmy Huang, Nick Koudas, Gareth J. F.
Jones, Xindong Wu, Kevyn Collins-Thompson, and
Aijun An, editors, CIKM, pages 1833?1836. ACM.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press, Cam-
bridge, MA ; London, May.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Processing, pages 1?6.
Roberto Gonz?alez-Ib?a?nez, Smaranda Muresan, and
Nina Wacholder. 2011. Identifying sarcasm in
twitter: A closer look. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies:
Short Papers - Volume 2, HLT ?11, pages 581?586,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312?
320, Atlanta, Georgia, USA, June. Association for
Computational Linguistics.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan
Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC?10), Valletta, Malta, may. European
Language Resources Association (ELRA).
Sara Rosenthal and Kathleen McKeown. 2013.
Columbia nlp: Sentiment detection of subjective
phrases in social media. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 478?482, Atlanta, Georgia, USA, June. Asso-
ciation for Computational Linguistics.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. Semeval-2014 task 9: Sen-
timent analysis in twitter. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval 2014), Dublin, Ireland, August. The COL-
ING 2014 Organizing Committee.
C. M. Whissel. 1989. The dictionary of affect in lan-
guage. In R. Plutchik and H. Kellerman, editors,
Emotion: theory research and experience, volume 4,
London. Acad. Press.
202
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 13?20,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Corpus Creation for New Genres:
A Crowdsourced Approach to PP Attachment
Mukund Jha, Jacob Andreas, Kapil Thadani, Sara Rosenthal and Kathleen McKeown
Department of Computer Science
Columbia University
New York, NY 10027, USA
{mj2472,jda2129}@columbia.edu, {kapil,sara,kathy}@cs.columbia.edu
Abstract
This paper explores the task of building an ac-
curate prepositional phrase attachment corpus
for new genres while avoiding a large invest-
ment in terms of time and money by crowd-
sourcing judgments. We develop and present
a system to extract prepositional phrases and
their potential attachments from ungrammati-
cal and informal sentences and pose the subse-
quent disambiguation tasks as multiple choice
questions to workers from Amazon?s Mechan-
ical Turk service. Our analysis shows that
this two-step approach is capable of producing
reliable annotations on informal and poten-
tially noisy blog text, and this semi-automated
strategy holds promise for similar annotation
projects in new genres.
1 Introduction
Recent decades have seen rapid development in nat-
ural language processing tools for parsing, semantic
role-labeling, machine translation, etc., and much of
this success can be attributed to the study of statisti-
cal techniques and the availability of large annotated
corpora for training. However, the performance of
these systems is heavily dependent on the domain
and genre of their training data, i.e. systems trained
on data from a particular domain tend to perform
poorly when applied to other domains and adap-
tation techniques are not always able to compen-
sate (Dredze et al, 2007). For this reason, achiev-
ing high performance on new domains and genres
frequently necessitates the collection of annotated
training data from those domains and genres, a time-
consuming and frequently expensive process.
This paper examines the problem of collecting
high-quality annotations for new genres with a focus
on time and cost efficiency. We explore the well-
studied but non-trivial task of prepositional phrase
(PP) attachment and describe a semi-automated sys-
tem for identifying accurate attachments in blog
data, which is frequently noisy and difficult to parse.
PP attachment disambiguation involves finding a
correct attachment for a prepositional phrase in a
sentence. For example, in the sentence ?We went to
John?s house on Saturday?, the phrase ?on Satur-
day? attaches to the verb ?went?. In another exam-
ple, ?We went to John?s house on 12th Street?, the
PP ?on 12th street? attaches to the noun ?John?s
house?. This sort of disambiguation requires se-
mantic knowledge about sentences that is difficult
to glean from their surface form, a problem which
is compounded by the informal nature and irregular
vocabulary of blog text.
In this work, we investigate whether crowd-
sourced human judgments are capable of distin-
guishing appropriate attachments. We present a sys-
tem that simplifies the attachment problem and rep-
resents it in a format that can be intuitively tackled
by humans.
Our approach to this task makes use of a heuristic-
based system built on a shallow parser that identi-
fies the likely words or phrases to which a PP can
attach. To subsequently select the correct attach-
ment, we leverage human judgments from multi-
ple untrained annotators (referred to here as work-
ers) through Amazon?s Mechanical Turk 1, an online
marketplace for work. This two-step approach of-
1http://www.mturk.amazon.com
13
fers distinct advantages: the automated system cuts
down the space of potential attachments effectively
with little error, and the disambiguation task can be
reduced to small multiple choice questions which
can be tackled quickly and aggregated reliably.
The remainder of this paper focuses on the PP at-
tachment task over blog text and our analysis of the
resulting aggregate annotations. We note, however,
that this type of semi-automated approach is poten-
tially applicable to any task which can be reliably
decomposed into independent judgments that un-
trained annotators can tackle (e.g., quantifier scop-
ing, conjunction scope). This work is intended as
an initial step towards the development of efficient
hybrid annotation tools that seamlessly incorporate
aggregate human wisdom alongside effective algo-
rithms.
2 Related Work
Identifying PP attachments is an essential task for
building syntactic parse trees. While this task has
been studied using fully-automated systems, many
of them rely on parse tree output for predicting po-
tential attachments (Ratnaparkhi et al, 1994; Yeh
and Vilain, 1998; Stetina and Nagao, 1997; Zavrel
et al, 1997). However, systems that rely on good
parses are unlikely to perform well on new genres
such as blogs and machine translated texts for which
parse tree training data is not readily available.
Furthermore, the predominant dataset for eval-
uating PP attachment is the RRR dataset (Ratna-
parkhi et al, 1994) which consists of PP attach-
ment cases from the Wall Street Journal portion of
the Penn Treebank. Instead of complete sentences,
this dataset consists of sets of the form {V,N1,P,N2}
where {P,N2} is the PP and {V,N1} are the poten-
tial attachments. This simplification of the PP at-
tachment task to a choice between two alternatives
is unrealistic when considering the potential long-
distance attachments encountered in real-world text.
While blogs and other web text, such as discus-
sion forums and emails, have been studied for a va-
riety of tasks such as information extraction (Hong
and Davison, 2009), social networking (Gruhl et
al., 2004), and sentiment analysis (Leshed and
Kaye, 2006), we are not aware of any previous ef-
forts to gather syntactic data (such as PP attach-
ments) in the genre. Syntactic methods such as
POS tagging, parsing and structural disambiguation
are commonly used when analyzing well-structured
text. Including the use of syntactic information
has yielded improvements in accuracy in speech
recognition (Chelba and Jelenik, 1998; Collins et
al., 2005) and machine translation (DeNeefe and
Knight, 2009; Carreras and Collins, 2009). We an-
ticipate that datasets such as ours could be useful for
such tasks as well.
Amazon?s Mechanical Turk (MTurk) has become
very popular for manual annotation tasks and has
been shown to perform equally well over labeling
tasks such as affect recognition, word similarity, rec-
ognizing textual entailment, event temporal order-
ing and word sense disambiguation, when compared
to annotations from experts (Snow et al, 2008).
While these tasks were small in scale and intended to
demonstrate the viability of annotation via MTurk,
it has also proved effective in large-scale tasks in-
cluding the collection of accurate speech transcrip-
tions (Gruenstein et al, 2009). In this paper we ex-
plore a method for corpus building on a large scale
in order to extend annotation into new domains and
genres.
We previously evaluated crowdsourced PP attach-
ment annotation by using MTurk workers to repro-
duce PP attachments from the Wall Street Journal
corpus (Rosenthal et al, 2010). The results demon-
strated that MTurk workers are capable of identi-
fying PP attachments in newswire text, but the ap-
proach used to generate attachment options is de-
pendent on the existing gold-standard parse trees
and cannot be used on corpora where parse trees are
not available. In this paper, we build on the semi-
automated annotation principle while avoiding the
dependency on parsers, allowing us to apply this
technique to the noisy and informal text found in
blogs.
3 System Description
Our system must both identify PPs and generate a
list of potential attachments for each PP in this sec-
tion. Figure 1 illustrates the structure of the system.
First, the system extracts sentences from scraped
blog data. Text is preprocessed by stripping HTML
tags, advertisements, non-Latin and non-printable
14
PPs
PPs
Question 
 Builder
PP Identifier
Chunker
+
Preprocessor
sentences
Chunked 
Chunked 
sentences
point predictor
Attachment
attachments
Potential
Mechanical
     Turk
Questions
forNew domain
data (Blogs)
Figure 1: Overview of question generation system
characters. Emoticon symbols are removed using a
standard list. 2
The cleaned data is then partitioned into sentences
using the NLTK sentence splitter. 3 In order to
compensate for the common occurrence of informal
punctuation and web-specific symbols in blog text,
we replace all punctuation symbols between quo-
tation marks and parentheses with placeholder tags
(e.g. ?QuestionMark?) during the sentence splitting
process and do the same for website names, time
markers and referring phrases (e.g. @John). Ad-
ditionally, we attempt to re-split sentences at ellipsis
boundaries if they are longer than 80 words and dis-
card them if this fails.
As parsers trained on news corpora tend to per-
form poorly on unstructured texts like blogs, we
rely on a chunker to partition sentences into phrases.
Choosing a good chunker is essential to this ap-
proach: around 35% of the cases in which the cor-
rect attachment is not predicted by the system are
due to chunker error. We experimented with differ-
ent chunkers over a random sample of 50 sentences
before selecting a CRF-based chunker (Phan, 2006)
for its robust performance.
The chunker output is initially processed by fus-
ing together chunks in order to ensure that a single
chunk represents a complete attachment point. Two
consecutive NP chunks are fused if the first contains
an element with a possessive part of speech tag (e.g.
John?s book), while particle chunks (PRT) are fused
with the VP chunks that precede them (e.g. pack
up). These chunked sentences are then processed
to identify PPs and potential attachment points for
them, which can then be used to generate questions
2http://www.astro.umd.edu/?marshall/
smileys.html
3http://www.nltk.org
for MTurk workers.
3.1 PP Extraction
PPs can be classified into two broad categories based
on the number of chunks they contain. A simple
PP consists of only two chunks: a preposition and
one noun phrase, while a compound PP has multi-
ple simple PPs attached to its primary noun phrase.
For example, in the sentence ?I just made some last-
minute changes to the latest issue of our newsletter?,
the PP with preposition ?to? can be considered to be
either the simple PP ?to the latest issue? or the com-
pound PP ?to the latest issue of our newsletter?.
We handle compound PPs by breaking them down
into multiple simple PPs; compound PPs can be re-
covered by identifying the attachments of their con-
stituent simple PPs. Our simple PP extraction al-
gorithm identifies PPs as a sequence of chunks that
consist of one or more prepositions terminating in a
noun phrase or gerund.
3.2 Attachment Point Prediction
A PP usually attaches to the noun or verb phrase pre-
ceding it or, in some cases, can modify a following
clause by attaching to the head verb. We build a set
of rules based on this intuition to pick out the poten-
tial attachments in the sentence; these rules are de-
scribed in Table 1. The rules are applied separately
for each PP in a sentence and in the same sequence
as mentioned in the table (except for rule 4, which
is applied while choosing a chunk using any of the
other rules).
15
Rule Example
1 Choose closest NP and VP preceding the PP. I made modifications to our newsletter.
2 Choose next closest VP preceding the PP if the VP selected in (1)
contains a VBG.
He snatched the disk flying away with one hand.
3 Choose first VP following the PP. On his desk he has a photograph.
4 All chunks inside parentheses are skipped, unless the PP falls within
parentheses.
Please refer to the new book (second edition) for
more notes.
5 Choose anything immediately preceding the PP that is not out of
chunk and has not already been picked.
She is full of excitement.
6 If a selected NP contains the word and, expand it into two options,
one with the full expression and one with only the terms following
and.
He is president and chairman of the board.
7 For PPs in chains of the form P-NP-P-NP (PP-PP), choose all the
NPs in the chain preceding the PP and apply all the above rules
considering the whole chain as a single PP.
They found my pictures of them from the concert.
8 If there are fewer than four options after applying the above rules,
also select the VP preceding the last VP selected, the NP preceding
the last NP selected, and the VP following the last VP picked.
Table 1: List of rules for attachment point predictor. In the examples, PPs are denoted by boldfaced text and potential
attachment options are underlined.
4 Experiments
An experimental study was undertaken to test our
hypothesis that we could obtain reliable annotations
on informal genres using MTurk workers. Here we
describe the dataset and our methods.
4.1 Dataset and Interface
We used a corpus of blog posts made on LiveJour-
nal 4 for system development and evaluation. Only
posts from English-speaking countries (i.e. USA,
Canada, UK, Australia and New Zealand) were con-
sidered for this study.
The interface provided to MTurk workers showed
the sentence on a plain background with the PP high-
lighted and a statement prompting them to pick the
phrase in the sentence that the given PP modified.
The question was followed by a list of options. In
addition, we provided MTurk workers the option to
indicate problems with the given PP or the listed op-
tions. Workers could write in the correct attachment
if they determined that it wasn?t present in the list of
options, or the correct PP if the one they were pre-
sented with was malformed. This allowed them to
correct errors made by the chunker and automated
attachment point predictor. In all cases, workers
were forced to pick the best answer among the op-
tions regardless of errors. We also supplied a num-
4http://www.livejournal.com
ber of examples covering both well-formed and er-
roneous cases to aid them in identifying appropriate
attachments.
4.2 Experimental Setup
For our experiment, we randomly selected 1000
questions from the output produced by the system
and provided each question to five different MTurk
workers, thereby obtaining five different judgments
for each PP attachment case. Workers were paid four
cents per question and the average completion time
per task was 48 seconds. In total $225 was spent
on the full study with $200 spent on the workers and
$25 on MTurk fees.The total time taken for the study
was approximately 16 hours.
A pilot study was carried out with 50 sentences
before the full study to test the annotation interface
and experiment with different ways of presenting the
PP and attachment options to workers. During this
study, we observed that while workers were will-
ing to suggest correct answers or PPs when faced
with erroneous questions, they often opted to not
pick any of the options provided unless the question
was well-formed. This was problematic because, in
many cases, expert annotators were able to identify
the most appropriate attachment option. Therefore,
in the final study we forced them to pick the most
suitable option from the given choices before indi-
cating errors and writing in alternatives.
16
Workers in agreement Number of questions Accuracy Coverage
5 (unanimity) 389 97.43% 41.33%
? 4 (majority) 689 94.63% 73.22%
? 3 (majority) 887 88.61% 94.26%
? 2 (plurality) 906 87.75% 96.28%
Total 941 84.48% 100%
Table 2: Accuracy and coverage over agreement thresholds
5 Evaluation corpus
In order to determine if the MTurk results were re-
liable, worker responses had to be validated by hav-
ing expert annotators perform the same task. For
this purpose, two of the authors annotated the 1000
questions used for the experiment independently and
compared their judgments. Disagreements were ob-
served in 127 cases; these were then resolved by a
pool of non-author annotators. If all three annota-
tors on a case disagreed with each other the question
was discarded; this situation occured 43 times. An
additional 16 questions were discarded because they
did not have a valid PP. For example, ?I am painting
with my blanket on today?. Here ?on today? is in-
correctly extracted as a PP because the particle ?on?
is tagged as a preposition. The rest of the analysis
presented in this section was performed on the re-
maining 941 sentences.
The annotators? judgments were compared to the
answers provided by the MTurk workers and, in
the case of disagreement between the experts and
the majority of workers, the sentences were man-
ually inspected to determine the reason. In five
cases, more than one valid attachment was possi-
ble; for example, in the sentence ?The video below is
of my favourite song on the album - A Real Woman?,
the PP ?of my favourite song? could attach to either
the noun phrase ?the video? or the verb ?is? and con-
veys the same meaning. In such cases, both the ex-
perts and the workers were considered to have cho-
sen the correct answer.
In 149 cases, the workers also augmented their
choices by providing corrections to incomplete an-
swers and badly constructed PPs. For example,
the PP ?of the Rings and Mikey? in the sentence
?Samwise from Lord of the Rings and Mikey from
The Goonies are the same actor ?? was corrected to
?of the Rings?. In 34/39 of the cases where the cor-
rect answer was not present in the options provided,
at least one worker indicated correct attachment for
the PP.
5.1 Attachment Prediction Evaluation
We measure the recall for our attachment point pre-
dictor as the number of questions for which the cor-
rect attachment appeared among the generated op-
tions divided by the total number of questions. The
system achieves a recall of 95.85% (902/941 ques-
tions). We observed that in many cases where the
correct attachment point was not predicted, it was
due to a chunker error. For example, in the following
sentence, ?Stop all the clocks , cut off the telephone
, Prevent the dog from barking with a juicy bone...?,
the PP ?from barking? attaches to the verb ?Pre-
vent?; however, due to an error in chunking ?Pre-
vent? is tagged as a noun phrase and hence is not
picked by our system. The correct attachment was
also occasionally missed when the attachment point
was too far from the PP. For example, in the sentence
?Fitting as many people as possible on one sofa and
under many many covers and getting intimate?, the
correct attachment for the PP ?under many many
covers? is the verb ?Fitting? but it is not picked by
our system.
Even though the correct attachment was not al-
ways given, the workers could still provide their own
correct answer. In the first example above, 3/5 work-
ers indicated that the correct attachment was not in
the list of options and wrote it in.
6 Results
Table 2 summarizes the results of the experiment.
We assess both the coverage and reliability of
worker predictions at various levels of worker agree-
ment. This serves as an indicator of the effective-
ness of the MTurk results: the accuracy can be taken
17
Figure 2: The number of questions in which exactly x
workers provided the correct answer
as a general confidence measure for worker predic-
tions; when five workers agree we can be 97.43%
confident in the correctness of their prediction, when
at least four workers agree we can be 94.63% con-
fident, etc. Unanimity indicates that all workers
agreed on an answer, majority indicates that more
than half of workers agreed on an answer, and plu-
rality indicates that two workers agreed on a single
answer, while the remaining three workers each se-
lected different answers. We observe that at high
levels of worker agreement, we get extremely high
accuracy but limited coverage of the data set; as
we decrease our standard for agreement, coverage
increases rapidly while accuracy remains relatively
high.
Figure 2 shows the number of workers providing
the correct answer on a per-question basis. This
illustrates the distribution of worker agreements
across questions. Note that in the majority of cases
(69.2%), at least four workers provided the correct
answer; in only 3.6% of cases were no workers able
to select the correct attachment.
Figure 3 shows the distribution of worker agree-
ments. Unlike Table 2, these figures are not cumu-
lative and include non-plurality two-worker agree-
ments. Note that the number of agreements dis-
cussed in this figure is greater than the 941 evaluated
because in some cases there were multiple agree-
ments on a single question. As an example, three
workers may choose one answer while the remain-
ing two workers choose another; this question then
produces both a three-worker agreement as well as a
two-worker agreement.
Figure 3: The number of cases in which exactly x work-
ers agreed on an answer
No. of options No. of cases Accuracy
< 4 179 86.59%
4 718 84.26%
> 4 44 79.55%
Table 3: Variation in worker performance with the num-
ber of attachment options presented
All questions on which there is agreement also
produce a majority vote, with one exception: the
2/2/1 agreement. Although the correct answer was
selected by one set of two workers in every case of
2/2/1 agreement, this is not particularly useful for
corpus-building as we have no way to identify a pri-
ori which set is correct. Fortunately, 2/2/1 agree-
ments were also quite rare and occurred in only 3%
of cases.
Figure 3 appears to indicate that instances of
agreement between two workers are unlikely to pro-
duce good attachments; they have a an average ac-
curacy of 37.2%. However, this is due in large part
to cases of 3/2 agreement, in which the two workers
in the minority are usually wrong, as well as cases of
2/2/1 agreement which contain at least one incorrect
instance of two-worker agreement. However, if we
only consider cases in which the two-worker agree-
ment forms a plurality (i.e. all other workers dis-
agree amongst themselves), we observe an average
accuracy of 64.3% which is similar to that of cases
of three-worker agreement (67.7%).
We also attempted to study the variation in worker
performance based on the complexity of the task;
specifically looking at how response accuracy var-
ied depending on the number of options that workers
were presented with. Although our system aimed to
18
Figure 4: Variation in accuracy with sentence length.
generate four attachment options per case, fewer op-
tions were produced for small sentences and opening
PPs while additional options were generated in sen-
tences containing PP-NP chains (see Table 1 for the
complete list of rules). Table 3 shows the variation in
accuracy with the number of options provided to the
workers. We might expect that an increased number
of options may be correlated with decreased accu-
racy and the data does indeed seem to suggest this
trend; however, we do not have enough datapoints
for the cases with fewer or more than four options to
verify whether this effect is significant.
We also analyzed the relationship between the
length of the sentence (in terms of number of words)
and the accuracy. Figure 4 indicates that as the
length of the sentence increases, the average accu-
racy decreases. This is not entirely unexpected as
lengthy sentences tend to be more complicated and
therefore harder for human readers to parse.
7 Conclusions and Future Work
We have shown that by working in conjunction
with automated attachment point prediction sys-
tems, MTurk workers are capable of annotating PP
attachment problems with high accuracy, even when
working with unstructured and informal blog text.
This work provides an immediate framework for the
building of PP attachment corpora for new genres
without a dependency on full parsing.
More broadly, the semi-automated framework
outlined in this paper is not limited to the task of
annotating PP attachments; indeed, it is suitable for
almost any syntactic or semantic annotation task
where untrained human workers can be presented
with a limited number of options for selection. By
dividing the desired annotation task into smaller
sub-tasks that can be tackled independently or in a
pipelined manner, we anticipate that more syntac-
tic information can be extracted from unstructured
text in new domains and genres without the sizable
investment of time and money normally associated
with hiring trained linguists to build new corpora.
To this end, we intend to further leverage the advent
of crowdsourcing resources in order to tackle more
sophisticated annotation tasks.
Acknowledgements
The authors would like to thank Kevin Lerman for
his help in formulating the original ideas for this
work. This material is based on research supported
in part by the U.S. National Science Foundation
(NSF) under IIS-05-34871. Any opinions, findings
and conclusions or recommendations expressed in
this material are those of the authors and do not nec-
essarily reflect the views of the NSF.
References
Xavier Carreras and Michael Collins. 2009. Non-
projective parsing for statistical machine translation.
In Proceedings of EMNLP, pages 200?209.
Ciprian Chelba and Frederick Jelenik. 1998. Structured
language modeling for speech recognition. In Pro-
ceedings of NLDB.
Michael Collins, Brian Roark, and Murat Saraclar.
2005. Discriminative syntactic language modeling for
speech recognition. In Proceedings of ACL, pages
507?514.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree adjoining machine translation. In Proceedings of
EMNLP, pages 727?736.
Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuz-
man Ganchev, Joa?o Graca, and Fernando Pereira.
2007. Frustratingly hard domain adaptation for depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL, pages 1051?1055,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Alex Gruenstein, Ian McGraw, and Andrew Sutherland.
2009. A self-transcribing speech corpus: collecting
continuous speech with an online educational game.
In Proceedings of the Speech and Language Technol-
ogy in Education (SLaTE) Workshop.
19
Figure 5: HIT Interface for PP attachment task
Daniel Gruhl, R. Guha, David Liben-Nowell, and An-
drew Tomkins. 2004. Information diffusion through
blogspace. In Proceedings of WWW, pages 491?501.
Liangjie Hong and Brian D. Davison. 2009. A
classification-based approach to question answering in
discussion boards. In Proceedings of SIGIR, pages
171?178.
Gilly Leshed and Joseph ?Jofish? Kaye. 2006. Under-
standing how bloggers feel: recognizing affect in blog
posts. In CHI ?06 extended abstracts on Human fac-
tors in computing systems, pages 1019?1024.
Xuan-Hieu Phan. 2006. CRFChunker: CRF
English phrase chunker. http://crfchunker.
sourceforge.net.
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.
1994. A maximum entropy model for prepositional
phrase attachment. In Proceedings of HLT, pages 250?
255.
Sara Rosenthal, William J. Lipovsky, Kathleen McKe-
own, Kapil Thadani, and Jacob Andreas. 2010. Semi-
automated annotation for prepositional phrase attach-
ment. In Proceedings of LREC, Valletta, Malta.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of EMNLP, pages 254?263.
Jiri Stetina and Makoto Nagao. 1997. Corpus based PP
attachment ambiguity resolution with a semantic dic-
tionary. In Proceedings of the Workshop on Very Large
Corpora, pages 66?80.
Alexander S. Yeh and Marc B. Vilain. 1998. Some prop-
erties of preposition and subordinate conjunction at-
tachments. In Proceedings of COLING, pages 1436?
1442.
Jakub Zavrel, Walter Daelemans, and Jorn Veenstra.
1997. Resolving PP attachment ambiguities with
memory-based learning. In Proceedings of the Work-
shop on Computational Language Learning (CoNLL),
pages 136?144.
Appendix A: Mechanical Turk Interface
Figure 5 shows a screenshot of the interface pro-
vided to the Mechanical Turk workers for the PP at-
tachment task. By default, examples and additional
options are hidden but can be viewed using the links
provided. The screenshot illustrates a case in which
a worker is confronted with an incorrect PP and uses
the additional options to correct it.
20
Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 37?45,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Detecting Influencers in Written Online Conversations
Or Biran1* Sara Rosenthal1* Jacob Andreas1**
Kathleen McKeown1* Owen Rambow2?
1 Department of Computer Science, Columbia University, New York, NY 10027
2 Center for Computational Learning Systems, Columbia University, New York, NY 10027
* {orb, sara, kathy}@cs.columbia.edu
** jda2129@columbia.edu ?rambow@ccls.columbia.edu
Abstract
It has long been established that there is a cor-
relation between the dialog behavior of a par-
ticipant and how influential he or she is per-
ceived to be by other discourse participants.
In this paper we explore the characteristics of
communication that make someone an opinion
leader and develop a machine learning based
approach for the automatic identification of
discourse participants that are likely to be in-
fluencers in online communication. Our ap-
proach relies on identification of three types
of conversational behavior: persuasion, agree-
ment/disagreement, and dialog patterns.
1 Introduction
In any communicative setting where beliefs are ex-
pressed, some are more influential than others. An
influencer can alter the opinions of their audience,
resolve disagreements where no one else can, be rec-
ognized by others as one who makes important con-
tributions, and often continue to influence a group
even when not present. Other conversational par-
ticipants often adopt their ideas and even the words
they use to express their ideas. These forms of per-
sonal influence (Katz and Lazarsfeld, 1955) are part
of what makes someone an opinion leader. In this
paper, we explore the characteristics of communica-
tion that make someone an opinion leader and de-
velop a machine learning based approach for the au-
tomatic identification of discourse participants who
are likely to be influencers in online communication.
Detecting influential people in online conversa-
tional situations has relevance to online advertising
strategies which exploit the power of peer influence
on sites such as Facebook. It has relevance to analy-
sis of political postings, in order to determine which
candidate has more appeal or which campaign strat-
egy is most successful. It is also relevant for design-
ing automatic discourse participants for online dis-
cussions (?chatbots?) as it can provide insight into
effective communication. Despite potential applica-
tions, analysis of influence in online communication
is a new field of study in part because of the rela-
tively recent explosion of social media. Thus, there
is not an established body of theoretical literature in
this area, nor are there established implementations
on which to improve. Given this new direction for
research, our approach draws on theories that have
been developed for identifying influence in spoken
dialog and extends them for online, written dialog.
We hypothesize that an influencer, or an influencer?s
conversational partner, is likely to engage in the fol-
lowing conversational behaviors:
Persuasion: An influencer is more likely to express
personal opinions with follow-up (e.g., justification,
reiteration) in order to convince others.
Agreement/disagreement: A conversational partner
is more likely to agree with an influencer, thus im-
plicitly adopting his opinions.
Dialog Patterns: An influencer is more likely to par-
ticipate in certain patterns of dialog, for example
initiating new topics of conversation, contributing
more to dialog than others, and engendering longer
dialog threads on the same topic.
Our implementation of this approach comprises
a system component for each of these conversa-
tional behaviors. These components in turn provide
37
the features that are the basis of a machine learn-
ing approach for the detection of likely influencers.
We test this approach on two different datasets, one
drawn from Wikipedia discussion threads and the
other drawn from LiveJournal weblogs. Our results
show that the system performs better for detection
of influencer on LiveJournal and that there are in-
teresting differences across genres for detecting the
different forms of conversational behavior.
The paper is structured as follows. After review-
ing related work, we define influence, present our
data and methods. We present a short overview of
the black box components we use for persuasion and
detection of agreement/disagreement, but our focus
is on the development of the influencer system as a
whole and thus we spend most time exploring the
results of experimentation with the system on dif-
ferent data sets, analyzing which components have
most impact. We first review related work.
2 Related Work
It has long been established that there is a correlation
between the conversational behavior of a discourse
participant and how influential he or she is perceived
to be by the other discourse participants (Bales et al,
1951; Scherer, 1979; Brook and Ng, 1986; Ng et al,
1993; Ng et al, 1995). Specifically, factors such as
frequency of contribution, proportion of turns, and
number of successful interruptions have been identi-
fied as being important indicators of influence. Reid
and Ng (2000) explain this correlation by saying that
?conversational turns function as a resource for es-
tablishing influence?: discourse participants can ma-
nipulate the dialog structure in order to gain influ-
ence. This echoes a starker formulation by Bales
(1970): ?To take up time speaking in a small group
is to exercise power over the other members for at
least the duration of the time taken, regardless of the
content.? Simply claiming the conversational floor is
a feat of power. This previous work presents two is-
sues for a study aimed at detecting influence in writ-
ten online conversations.
First, we expect the basic insight ? conversation
as a resource for influence ? to carry over to written
dialog: we expect to be able to detect influence in
written dialog as well. However, some of the charac-
teristics of spoken dialog do not carry over straight-
forwardly to written dialog, most prominently the
important issue of interruptions: there is no interrup-
tion in written dialog. Our work draws on findings
for spoken dialog, but we identify characteristics of
written dialog which are relevant to influence.
Second, the insistence of Bales (1970) that power
is exercised through turn taking ?regardless of con-
tent? may be too strong. Reid and Ng (2000) discuss
experiments which address not just discourse struc-
ture features, but also a content feature which repre-
sents how closely a turn is aligned with the overall
discourse goal of one of two opposing groups (with
opposing opinions on a specific issue) participating
in the conversation. They show that interruptions are
more successful if aligned with the discourse goal.
They propose a model in which such utterances
?lead to participation which in turn predicts social
influence?, so that the correlation between discourse
structure and influence is really a secondary phe-
nomenon. However, transferring such results to
other types of interactions (for example, in which
there are not two well-defined groups) is challeng-
ing. In this study, we therefore examine two types of
features as they relate to influence: content-related
(persuasion and agreement/disagreement), and dis-
course structure-related.
So far, there has been little work in NLP related
to influencers. Quercia et al (2011) look at influ-
encers? language use in Twitter contrasted to other
users? groups and find some significant differences.
However, their analysis and definition relies quite
heavily on the particular nature of social activity
on Twitter. Rienks (2007) discusses detecting influ-
encers in a corpus of conversations. While he fo-
cuses entirely on non-linguistic behavior, he does
look at (verbal) interruptions and topic initiations
which can be seen as corresponding to some of our
Dialog Patterns Language Uses.
3 What is an Influencer?
Our definition of an influencer was collectively for-
mulated by a community of researchers involved in
the IARPA funded project on Socio Cultural Content
in Language (SCIL).
This group defines an influencer to be someone
who:
38
P1 by Arcadian <pc1>There seems to be a much better list at the National Cancer Institute than the one we?ve
got.</pc1><pa1>It ties much better to the actual publication (the same 11 sections, in the same order).</pa1>
I?d like to replace that section in this article. Any objections?
P2 by JFW <pc2><a1>Not a problem.</a1></pc2>Perhaps we can also insert the relative incidence as
published in this month?s wiki Blood journal
P3 by Arcadian I?ve made the update. I?ve included template links to a source that supports looking up
information by ICD-O code.
P4 by Emmanuelm Can Arcadian tell me why he/she included the leukemia classification to this lymphoma
page? It is not even listed in the Wikipedia leukemia page! <pc3>I vote for dividing the WHO classification
into 4 parts in 4 distinct pages: leukemia, lymphoma, histocytic and mastocytic neoplasms.</pc3><pa3>
Remember, Wikipedia is meant to be readable </pa3>by all. Let me know what you think before I delete
the non-lymphoma parts.
P5 by Arcadian Emmanuelm, aren?t you the person who added those other categories on 6 July 2005?
P6 by Emmanuelm <d1>Arcadian, I added only the lymphoma portion of the WHO classification.
You added the leukemias on Dec 29th.</d1>Would you mind moving the leukemia portion to the
leukemia page?
P7 by Emmanuelm <pc4>Oh, and please note that I would be very comfortable with a ?cross-coverage?
of lymphocytic leukemias in both pages.</pc4>My comment is really about myeloid, histiocytic and
mast cell neoplasms who share no real relationship with lymphomas.
P8 by Arcadian <pa5><a2>To simplify the discussion, I have restored that section to your version.
</a2></pa5>You may make any further edits, and <pc6>I will have no objection.</pc6>
P9 by JFW The full list should be on the hematological malignancy page, and the lymphoma part can be here.
<pc7>It would be defendable to list ALL and CLL here.</pc7><pa7>They fall under the lymphoproliferative
disorders.</pa7>
Table 1: Influence Example: A Wikipedia discussion thread displaying Emmanuelm as the influencer. Replies are
indicated by indentation (for example, P2 is a response to P1). All Language Uses are visible in this example: Attempt
to Persuade ({pci, pai}), Claims (pci), Argumentation (pai), Agreement (ai), Disagreement (di), and the five Dialog
Patterns Language Uses (eg. Arcadian has positive Initiative).
1. Has credibility in the group.
2. Persists in attempting to convince others, even if
some disagreement occurs
3. Introduces topics/ideas that others pick up on or
support.
By credibility, we mean someone whose ideas are
adopted by others or whose authority is explicitly
recognized. We hypothesize that this shows up
through agreement by other conversants. By per-
sists, we mean someone who is able to eventually
convince others and often takes the time to do so,
even if it is not quick. This aspect of our definition
corresponds to earlier work in spoken dialog which
shows that frequency of contributions and propor-
tion of turns is a method people use to gain influence
(Reid and Ng, 2000; Bales, 1970). By point 3, we
see that the influencer may be influential even in di-
recting where the conversation goes, discussing top-
ics that are of interest to others. This latter feature
can be measured through the discourse structure of
the interaction. The influencer must be a group par-
ticipant but need not be active in the discussion(s)
where others support/credit him.
The instructions that we provided to annotators
included this definition as well as examples of who
is not an influencer. We told annotators that if some-
one is in a hierarchical power relation (e.g., a boss),
then that person is not an influencer to sub-ordinates
(or, that is not the type of influencer we are look-
ing for). We also included someone with situational
power (e.g., authority to approve other?s actions) or
power in directing the communication (e.g., a mod-
erator) as negative examples.
We also gave positive examples of influencers. In-
fluencers include an active participant who argues
against a disorganized group and resolves a discus-
sion is an influencer, a person who provides an an-
swer to a posted question and the answer is accepted
after discussion, and a person who brings knowledge
to a discussion. We also provided positive and neg-
39
ative examples for some of these cases.
Table 1 shows an example of a dialog where there
is evidence of influence, drawn from a Wikipedia
Talk page. A participant (Arcadian) starts the thread
with a proposal and a request for support from other
participants. The influencer (Emmanuelm) later
joins the conversation arguing against Arcadian?s
proposal. There is a short discussion, and Arcadian
defers to Emmanuelm?s position. This is one piece
of dialog within this group where Emmanuelm may
demonstrate influence. The goal of our system is to
find evidence for situations like this, which suggests
that a person is more likely to be an influencer.
Since we attempt to find local influence (a per-
son who is influential in a particular thread, as op-
posed to influential in general), our notion of influ-
encer is consistent with diverse views on social in-
fluence. It is consistent with the definition of influ-
encer proposed by Gladwell (2001) and Katz (1957):
an exceptionally convincing and influential person,
set apart from everyone else by his or her ability to
spread opinions. While it superficially seems incon-
sistent with Duncan Watts? concept of ?accidental
influentials? (Watts, 2007), that view does not make
the assertion that a person cannot be influential in
a particular situation (in fact, it predicts that some-
one will) - only that one cannot in general identify
people who are always more likely to be influencers.
4 Data and Annotation
Our data set consists of documents from two differ-
ent online sources: weblogs from LiveJournal and
discussion forums from Wikipedia.
LiveJournal is a virtual community in which peo-
ple write about their personal experiences in a we-
blog. A LiveJournal entry is composed of a post
(the top-level content written by the author) and a
set of comments (written by other users and the au-
thor). Every comment structurally descends either
from the post or from another comment.
Each article on Wikipedia has a discussion forum
(called a Talk page) associated with it that is used
to discuss edits for the page. Each forum is com-
posed of a number of threads with explicit topics,
and each thread is composed of a set of posts made
by contributors. The posts in a Wikipedia discussion
thread may or may not structurally descend from
other posts: direct replies to a post typically descend
from it. Other posts can be seen as descending from
the topic of the thread.
For consistency of terms, from here on we refer to
each weblog or discussion forum thread as a thread
and to each post or comment as a post.
We have a total of 333 threads: 245 from Live-
Journal and 88 from Wikipedia. All were annotated
for influencers. The threads were annotated by two
undergraduate students of liberal arts. These stu-
dents had no prior training or linguistic background.
The annotators were given the full definition from
section 3 and asked to list the participants that they
thought were influencers. Each thread may in princi-
ple have any number of influencers, but one or zero
influencers per thread is the common case and the
maximal number of influencers found in our dataset
was two. The inter-annotator agreement on whether
or not a participant is an influencer (given by Co-
hen?s Kappa) is 0.72.
5 Method
Our approach is based on three conversational be-
haviors which are identified by separate system
components described in the following three sec-
tions. Figure 1 shows the pipeline of the Influencer
system and Table 1 displays a Wikipedia discussion
thread where there is evidence of an influencer and
in which we have indicated the conversational be-
haviors as they occur. Motivated by our definition,
each component is concerned with an aspect of the
likely influencer?s discourse behavior:
Persuasion examines the participant?s language to
identify attempts to persuade, such as {pc1, pa1} in
Table 1, which consist of claims (e.g. pc1) made
by the participant and supported by argumentations
(e.g. pa1). It also identifies claims and argumenta-
tions independently of one another (pc4 and pa5).
Agreement/Disagreement examines the other par-
ticipants? language to find how often they agree or
disagree with the participant?s statements. Examples
are a1 and d1 in Table 1.
Dialog Patterns examines how the participant inter-
acts in the discussion structurally, independently of
the content and the language used. An example of
this is Arcadian being the first poster and contribut-
ing the most posts in the thread in Table 1.
40
Figure 1: The influencer pipeline. Solid lines indicate
black-box components, which we only summarize in this
paper. Dashed lines indicate components described here.
Each component contributes a number of Lan-
guage Uses which fall into that category of conver-
sational behavior and these Language Uses are used
directly as features in a supervised machine learn-
ing model to predict whether or not a participant is
an influencer. For example, Dialog Patterns con-
tributes the Language Uses Initiative, Irrelevance,
Incitation, Investment and Interjection.
The Language Uses of the Persuasion and Agree-
ment/Disagreement components are not described in
detail in this paper, and instead are treated as black
boxes (indicated by solid boxes in Figure 1). We
have previously published work on some of these
(Biran and Rambow, 2011; Andreas et al, 2012).
The remainder of this section describes them briefly
and provides the results of evaluations of their per-
formance (in Table 2). The next section describes
the features of the Dialog Patterns component.
5.1 Persuasion
This component identifies three Language Uses: At-
tempt to Persuade, Claims and Argumentation.
We define an attempt to persuade as a set of con-
tributions made by a single participant which may
be made anywhere within the thread, and which are
all concerned with stating and supporting a single
claim. The subject of the claim does not matter:
an opinion may seem trivial, but the argument could
still have the structure of a persuasion.
Our entire data set was annotated for attempts to
persuade. The annotators labeled the text partici-
pating in each instance with either claim, the stated
opinion of which the author is trying to persuade
others or argumentation, an argument or evidence
that supports that claim. An attempt to persuade
must contain exactly one claim and at least one in-
stance of argumentation, like the {claim, argumen-
tation} pairs {pc1, pa1} and {pc3, pj3} in Table 1.
In addition to the complete attempt to persuade
Language Use, we also define the less strict Lan-
guage Uses claims and argumentation, which use
only the subcomponents as stand-alones.
Our work on argumentation, which builds on
Rhetorical Structure Theory (Mann and Thompson,
1988), is described in (Biran and Rambow, 2011).
5.2 Agreement/Disagreement
Agreement and disagreement are two Language
Uses that model others? acceptance of the partici-
pant?s statements. Annotation (Andreas et al, 2012)
is performed on pairs of phrases, {p1, p2}. A phrase
is a substring of a post or comment in a thread. The
annotations are directed since each post or comment
has a time stamp associated with it. This means that
p1 and p2 are not interchangeable. p1 is called the
?target phrase?, and p2 is called the ?subject phrase?.
A person cannot agree with him- or herself, so the
author of p1 and p2 cannot be the same. Each anno-
tation is also labeled with a type: either ?agreement?
or ?disagreement?.
6 Dialog Patterns
The Dialog Patterns component extracts features
based on the structure of the thread. Blogs and dis-
cussion threads have a tree structure, with a blog
post or a topic of discussion as the root and a set of
41
Component Wikipedia LiveJournal
P R F P R F
Attempt 79.1 69.6 74 57.5 48.2 52.4
to persuade
Claims 83.6 74.5 78.8 53.7 13.8 22
Argumentation 23.3 91.7 37.1 30.9 48.9 37.8
Agreement 12 31.9 17.4 20 50 28.6
Disagreement 8.7 9.5 9.1 6.3 14.3 8.7
Table 2: Performance of the black-box Language Uses in
terms of Precision (P), Recall (R), and F-measure(F).
Conversational
Behavior
Language Use
(Feature)
Users
Component A J E
Persuasion Claims 2/6 2/6 2/6
Argumentation Y Y Y
Attempt to Per-
suade
Y Y Y
Agreement/ Agreement 1/1 0/1 0/1
Disagreement Disagreement 1/1 0/1 0/1
Dialog Initiative Y N N
Patterns Irrelevance 2/4 1/2 1/3
Incitation 4 1 3
Interjection 1/9 2/9 4/9
Investment 4/9 2/9 3/9
Table 3: The feature values for each of the partici-
pants, Arcadian (A), JFW (J), and Emmanuelm (E), in
the Wikipedia discussion thread shown in Table 1.
comments or posts which are marked as a reply - ei-
ther to the root or to an earlier post. The hypothesis
behind Dialog Patterns is that influencers have typ-
ical ways in which they participate in a thread and
which are visible from the structure alone.
The Dialog Patterns component contains five sim-
ple Language Uses:
Initiative The participant is or is not the first poster
of the thread.
Irrelevance The percentage of the participant?s
posts that are not replied to by anyone.
Incitation The length of the longest branch of
posts which follows one of the participant?s posts.
Intuitively, the longest discussion started directly by
the participant.
Investment The participant?s percentage of all posts
in the thread.
Interjection The point in the thread, represented
as percentage of posts already posted, at which the
participant enters the discussion.
7 System and Evaluation
The task of the system is to decide for each partici-
pant in a thread whether or not he or she is an influ-
encer in that particular thread. It is realized with a
supervised learning model: we train an SVM with a
small number of features, namely the ten Language
Uses. One of our goals in this work is to evaluate
which Language Uses allow us to more accurately
classify someone as an influencer. Table 3 shows
the full feature set and feature values for the sample
discussion thread in Table 1. We experimented with
a number of different classification methods, includ-
ing bayesian and rule-based models, and found that
SVM produced the best results.
7.1 Evaluation
We evaluated on Wikipedia and LiveJournal sepa-
rately. The data set for each corpus consists of all
participants in all threads for which there was at least
one influencer. We exclude threads for which no in-
fluencer was found, narrowing our task to finding the
influencers where they exist. For each participant X
in each thread Y, the system answers the following
question: Is X an influencer in Y?
We used a stratified 10-fold cross validation of
each data set for evaluation, ensuring that the same
participant (from two different threads) never ap-
peared in both training and test at each fold, to elim-
inate potential bias from fitting to a particular partic-
ipant?s style. The system components were identical
when evaluating both data sets, except for the claims
system which was trained on sentiment-annotated
data from the corpus on which it was evaluated.
Table 4 shows the performance of the full system
and of systems using only one Language Use feature
compared against a baseline which always answers
positively (X is always an influencer in Y). It also
shows the performance for the best system, which
was found for each data set by looking at all possible
combinations of the features. The best system for
the Wikipedia data set is composed of four features:
Claims, Argumentation, Agreement and Investment.
The best LiveJournal system is composed of all five
Dialog Patterns features, Attempt to Persuade and
Argumentation. We found our results to be statis-
42
System Wikipedia LiveJournal
P R F P R F
Baseline: all-
yes
16.2 100 27.9 19.2 19.2 32.2
Full 40.5 80.5 53.9 61.7 82 70.4
Initiative 31.6 31.2 31.4 73.5 72.7 73.1
Irrelevance 21.7 77.9 34 19.2 100 32.2
Incitation 28.3 77.9 41.5 49.5 73.8 59.2
Investment 43 71.4 53.7 50.2 75.4 60.3
Interjection 24.7 88.3 38.6 36.9 91.3 52.5
Agreement 36 46.8 40.7 45.1 82.5 58.3
Disagreement 35.3 70.1 47 19.2 100 32.2
Claims 40 72.7 51.6 54.3 76 63.3
Argumentation 19 98.7 31.8 31.1 85.2 45.6
Attempt 23.7 79.2 36.5 37.4 48.1 42.1
to persuade
Best system 47 80.5 59.3 66.2 84.7 74.3
Table 4: Performance in terms of Precision (P), Recall
(R), and F-measure (F) using the baseline (everyone is an
influencer), all features (full), individual features one at a
time, and the best feature combination for each data set.
tically significant (with the Bonferroni adjustment)
in paired permutation tests between the best system,
the full system and the baseline of each data set.
When we first performed these experiments, we
used all threads in the data set. The performance on
this full set was lower, as shown in Table 5 due to
the presence of threads with no influencers. Threads
in which the annotators could not find a clear influ-
encer tend to be of a different nature: there is either
no clear topic of discussion, or no argument (every-
one is in agreement). We leave the task of distin-
guishing these threads from those which are likely
to have an influencer to future work.
7.2 Evaluating with Perfect Components
In a hierarchical system such as ours, errors can
be attributed to imperfect components or to a bad
choice of features, so it is important to look at the
potential contribution of the components. As an ex-
ample, Table 6 shows the difference between our
Attempt to Persuade system and a hypothetical per-
fect Attempt to Persuade component, simulated by
using the gold annotations, when predicting influ-
encer directly (i.e., a participant is an influencer iff
she makes an attempt to persuade).
Clearly, when predicting influencers, Attempt to
System Wikipedia LiveJournal
P R F P R F
Baseline 13.9 100 24.5 14.2 100 24.9
Full 36.7 79.2 50.2 46.3 79.8 58.6
Best 40.1 76.6 52.7 48.2 81.4 60.6
Table 5: Performance on the data set of all threads, in-
cluding those with no influencers. The ?Best System? is
the system that performed best on the filtered data set.
Data Set Our System Gold Answers
P R F P R F
Wikipedia 23.6 69.4 35.2 23.8 81.6 36.9
LiveJournal 37.5 48.1 42.1 40.7 61.8 49
Table 6: Performance of the Attempt to Persuade compo-
nent in directly predicting influencers. A comparison of
our system and the component?s gold annotation. These
experiments were run on the full data set, which is why
the system results are not exactly those of Table 4.
Persuade is a stronger indicator in LiveJournal than
it is in Wikipedia. However, as shown in Table 2,
our Attempt to Persuade system performs better on
Wikipedia. This situation is reflected in Table 6,
where the lower quality of the system component in
LiveJournal corresponds to a significantly lower per-
formance when applied to the influencer task. These
results demonstrate that Attempt to Persuade is a
good feature: a more precise feature value means
higher predictability of influencer. In the future we
will perform similar analyses for the other features.
8 Discussion
We evaluated our system on two corpora - Live-
Journal and Wikipedia discussions - which differ in
structure, context and discussion topics. As our re-
sults show, they also differ in the way influencers
behave and the way others respond to them. To
illustrate the differences, we contrast the sample
Wikipedia thread (Table 1) with an example from
LiveJournal (Table 7).
It is common in LiveJournal for the blogger to be
an influencer, as is the case in our example thread,
because the topic of the thread is set by the blog-
ger and comments are typically made by her friends.
This fact is reflected in our results: Initiative is a
very strong indicator in LiveJournal, but not so in
43
P1 by poconell <pc1>He really does make good on his promises! </pc1><pa1>Day three in office, and the
Global Gag Rule (A.K.A?The Mexico City Policy?) is gone!</pa1>I was holding my breath, hoping it
wouldn?t be left forgotte. He didn?t wait. <pc2>He can see the danger and risk in this policy, and the damage
it has caused to women and families.</pc2><pc3>I love that man!</pc3>
P2 by thalialunacy <a1>I literally shrieked ?HELL YES!? in my car when I heard. :D:D:D</a1>
P3 by poconell <a2>Yeah, me too</a2>
P4 by lunalovepotter <pc4><a3>He is SO AWESOME!</a3></pc4><pa4>Right down to business, no
ifs, ands, or buts! :D</pa4>
P5 by poconell <pc5>It?s amazing to see him so serious too!</pc5><pa5>This is one tough,
no-nonsense man!</pa5>
P6 by penny sieve My icon says it all :)
P7 by poconell <pc6>And I?m jealous of you with that President!</pc6><pa6>We tried to overthrow
our Prime Minister, but he went crying to the Governor General. </pa6>
Table 7: Influence Example: A LiveJournal discussion thread displaying poconell as the influencer. All the Language
Uses are visible in this example: agreement/disagreement (ai/di), persuasion ({pci, pai}, pci, pai), and dialog patterns
(eg. poconell has positive Initiative). This example is very different from the Wikipedia example in Table 1.
Wikipedia, where the discussion is between a group
of editors, all of whom are equally interested in the
topic. In general, the Dialog Patterns features are
stronger in LiveJournal. We believe this is due to the
fact that the tree structure in LiveJournal is strictly
enforced. In Wikipedia, people do not always reply
directly to the relevant post. Investment is the excep-
tion: it does not make use of the tree structure, and
is therefore an important indicator in Wikipedia.
Attempt to Persuade is useful in LiveJournal (the
influencer poconell makes three attempts to per-
suade in Table 7) but less so in Wikipedia. This is
explained by the precision of the gold system in Ta-
ble 6. Only 23.8% of those who attempt to persuade
in Wikipedia are influencers, compared with 40.7%
in LiveJournal. Attempts to Persuade are more com-
mon in Wikipedia (all participants attempt to per-
suade in Table 1), since people write there specifi-
cally to argue their opinion on how the article should
be edited. Conversely, agreement is a stronger pre-
dictor of influence in Wikipedia than in LiveJournal;
we believe that is because of a similar phenomenon,
that people in LiveJournal (who tend to know each
other) agree with each other more often. Disagree-
ment is not a strong indicator for either corpus which
may say something about influencers in general -
they can be disagreed with as often as anyone else.
9 Conclusion and Future Work
We have studied the relevance of content-related
conversational behavior (persuasion and agree-
ment/disagreement), and discourse structure-related
conversational behavior to detection of influence.
Identifying influencers is a hard task, but we are
able to show good results on the LiveJournal corpus
where we achieve an F-measure of 74.3%. Despite
a lower performance on Wikipedia, we are still able
to significantly outperform the baseline which yields
only 28.2%. Differences in performance between
the two seem to be attributable in part to the more
straightforward dialog structure in LiveJournal.
There are several areas for future work. In our
current work, we train and evaluate separately for
our two corpora. Alternatively, we could investigate
different training and testing combinations: train on
one corpus and evaluate on the other; a mixed cor-
pus for training and testing; genre-independent cri-
teria for developing different systems (e.g. length of
thread). We will also evaluate on new genres (such
as the Enron emails) in order to gain an appreciation
of how different genres of written dialog are.
Acknowledgment
This work has been supported by the Intelligence
Advanced Research Projects Activity (IARPA) via
Army Research Laboratory (ARL) contract num-
ber W911NF-09-C-0141. The U.S. Government is
authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copy-
right annotation thereon.
44
References
Jacob Andreas, Sara Rosenthal, and Kathleen McKe-
own. 2012. Annotating agreement and disagreement
in threaded discussion. In Proceedings of the 8th In-
ternational Conference on Language Resources and
Computation (LREC), Istanbul, Turkey, May.
R. F. Bales, Strodtbeck, Mills F. L., T. M., and M. Rose-
borough. 1951. Channels of communication in small
groups. American Sociological Review, pages 16(4),
461?468.
R. F. Bales. 1970. Personality and interpersonal be-
haviour.
Or Biran and Owen Rambow. 2011. Identifying justifi-
cations in written dialog. In Proceedings of the Fifth
IEEE International Conference on Semantic Comput-
ing.
M.E. Brook and S. H. Ng. 1986. Language and social
influence in small conversational groups. Journal of
Language and Social Psychology, pages 5(3), 201?
210.
Malcolm Gladwell. 2001. The tipping point: how little
things can make a big difference. Abacus.
Elihu Katz and Paul F. Lazarsfeld. 1955. Personal in-
fluence. Free Press, Glencoe, IL. by Elihu Katz and
Paul F. Lazarsfeld. With a foreword by Elmo Roper.
?A report of the Bureau of Applied Social Research,
Columbia University.? Bibliography: p. 381-393.
E. Katz. 1957. The Two-Step Flow of Communication:
An Up-To-Date Report on an Hypothesis. Bobbs-
Merrill Reprint Series in the Social Sciences, S137.
Ardent Media.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
S. H. Ng, D. Bell, and M. Brooke. 1993. Gaining turns
and achieving high in influence ranking in small con-
versational groups. British Journal of Social Psychol-
ogy, pages 32, 265?275.
S. H. Ng, M Brooke, and M. Dunne. 1995. Interruption
and in influence in discussion groups. Journal of Lan-
guage and Social Psychology, pages 14(4),369?381.
Daniele Quercia, Jonathan Ellis, Licia Capra, and Jon
Crowcroft. 2011. In the mood for being influential on
twitter. In SocialCom/PASSAT, pages 307?314. IEEE.
Scott A. Reid and Sik Hung Ng. 2000. Conversation as a
resource for in influence: evidence for prototypical ar-
guments and social identification processes. European
Journal of Social Psychology, pages 30, 83?100.
Rutger Joeri Rienks. 2007. Meetings in smart environ-
ments : implications of progressing technology. Ph.D.
thesis, Enschede, the Netherlands, July.
K. R. Scherer. 1979. Voice and speech correlates of per-
ceived social influence in simulated juries. In H. Giles
and R. St Clair (Eds), Language and social psychol-
ogy, pages 88?120. Oxford: Blackwell.
Duncan Watts. 2007. The accidental influentials. Har-
vard Business Review.
45
