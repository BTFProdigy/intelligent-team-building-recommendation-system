Evaluating Centering-based metrics of coherence for text
structuring using a reliably annotated corpus
Nikiforos Karamanis,? Massimo Poesio,? Chris Mellish,? and Jon Oberlander?
?School of Informatics, University of Edinburgh, UK, {nikiforo,jon}@ed.ac.uk
?Dept. of Computer Science, University of Essex, UK, poesio at essex dot ac dot uk
?Dept. of Computing Science, University of Aberdeen, UK, cmellish@csd.abdn.ac.uk
Abstract
We use a reliably annotated corpus to compare
metrics of coherence based on Centering The-
ory with respect to their potential usefulness for
text structuring in natural language generation.
Previous corpus-based evaluations of the coher-
ence of text according to Centering did not com-
pare the coherence of the chosen text structure
with that of the possible alternatives. A corpus-
based methodology is presented which distin-
guishes between Centering-based metrics taking
these alternatives into account, and represents
therefore a more appropriate way to evaluate
Centering from a text structuring perspective.
1 Motivation
Our research area is descriptive text generation
(O?Donnell et al, 2001; Isard et al, 2003), i.e.
the generation of descriptions of objects, typi-
cally museum artefacts, depicted in a picture.
Text (1), from the gnome corpus (Poesio et al,
2004), is an example of short human-authored
text from this genre:
(1) (a) 144 is a torc. (b) Its present arrangement,
twisted into three rings, may be a modern al-
teration; (c) it should probably be a single ring,
worn around the neck. (d) The terminals are
in the form of goats? heads.
According to Centering Theory (Grosz et al,
1995; Walker et al, 1998a), an important fac-
tor for the felicity of (1) is its entity coherence:
the way centers (discourse entities), such as
the referent of the NPs ?144? in clause (a) and
?its? in clause (b), are introduced and discussed
in subsequent clauses. It is often claimed in
current work on in natural language generation
that the constraints on felicitous text proposed
by the theory are useful to guide text struc-
turing, in combination with other factors (see
(Karamanis, 2003) for an overview). However,
how successful Centering?s constraints are on
their own in generating a felicitous text struc-
ture is an open question, already raised by the
seminal papers of the theory (Brennan et al,
1987; Grosz et al, 1995). In this work, we ex-
plored this question by developing an approach
to text structuring purely based on Centering,
in which the role of other factors is deliberately
ignored.
In accordance with recent work in the emerg-
ing field of text-to-text generation (Barzilay et
al., 2002; Lapata, 2003), we assume that the in-
put to text structuring is a set of clauses. The
output of text structuring is merely an order-
ing of these clauses, rather than the tree-like
structure of database facts often used in tradi-
tional deep generation (Reiter and Dale, 2000).
Our approach is further characterized by two
key insights. The first distinguishing feature is
that we assume a search-based approach to text
structuring (Mellish et al, 1998; Kibble and
Power, 2000; Karamanis and Manurung, 2002)
in which many candidate orderings of clauses
are evaluated according to scores assigned by
a given metric, and the best-scoring ordering
among the candidate solutions is chosen. The
second novel aspect is that our approach is
based on the position that the most straight-
forward way of using Centering for text struc-
turing is by defining a Centering-based metric
of coherence Karamanis (2003). Together, these
two assumptions lead to a view of text planning
in which the constraints of Centering act not
as filters, but as ranking factors, and the text
planner may be forced to choose a sub-optimal
solution.
However, Karamanis (2003) pointed out that
many metrics of coherence can be derived from
the claims of Centering, all of which could be
used for the type of text structuring assumed in
this paper. Hence, a general methodology for
identifying which of these metrics represent the
most promising candidates for text structuring
is required, so that at least some of them can
be compared empirically. This is the second re-
search question that this paper addresses, build-
ing upon previous work on corpus-based evalu-
ations of Centering, and particularly the meth-
ods used by Poesio et al (2004). We use the
gnome corpus (Poesio et al, 2004) as the do-
main of our experiments because it is reliably
annotated with features relevant to Centering
and contains the genre that we are mainly in-
terested in.
To sum up, in this paper we try to iden-
tify the most promising Centering-based metric
for text structuring, and to evaluate how useful
this metric is for that purpose, using corpus-
based methods instead of generally more expen-
sive psycholinguistic techniques. The paper is
structured as follows. After discussing how the
gnome corpus has been used in previous work
to evaluate the coherence of a text according to
Centering we discuss why such evaluations are
not sufficient for text structuring. We continue
by showing how Centering can be used to define
different metrics of coherence which might be
useful to drive a text planner. We then outline
a corpus-based methodology to choose among
these metrics, estimating how well they are ex-
pected to do when used by a text planner. We
conclude by discussing our experiments in which
this methodology is applied using a subset of the
gnome corpus.
2 Evaluating the coherence of a
corpus text according to Centering
In this section we briefly introduce Centering,
as well as the methodology developed in Poesio
et al (2004) to evaluate the coherence of a text
according to Centering.
2.1 Computing CF lists, CPs and CBs
According to Grosz et al (1995), each ?utter-
ance? in a discourse is assigned a list of for-
ward looking centers (CF list) each of which is
?realised? by at least one NP in the utterance.
The members of the CF list are ?ranked? in or-
der of prominence, the first element being the
preferred center CP.
In this paper, we used what we considered to
be the most common definitions of the central
notions of Centering (its ?parameters?). Poe-
sio et al (2004) point out that there are many
definitions of parameters such as ?utterance?,
?ranking? or ?realisation?, and that the setting
of these parameters greatly affects the predic-
tions of the theory;1 however, they found viola-
tions of the Centering constraints with any way
of setting the parameters (for instance, at least
25% of utterances have no CB under any such
setting), so that the questions addressed by our
work arise for all other settings as well.
Following most mainstream work on Center-
ing for English, we assume that an ?utterance?
corresponds to what is annotated as a finite unit
in the gnome corpus.2 The spans of text with
the indexes (a) to (d) in example (1) are exam-
ples. This definition of utterance is not optimal
from the point of view of minimizing Centering
violations (Poesio et al, 2004), but in this way
most utterances are the realization of a single
proposition; i.e., the impact of aggregation is
greatly reduced. Similarly, we use grammatical
function (gf) combined with linear order within
the unit (what Poesio et al (2004) call gfthere-
lin) for CF ranking. In this configuration, the
CP is the referent of the first NP within the unit
that is annotated as a subject for its gf.3
Example (2) shows the relevant annotation
features of unit u210 which corresponds to
utterance (a) in example (1). According to
gftherelin, the CP of (a) is the referent of ne410
?144?.
(2) <unit finite=?finite-yes? id=?u210?>
<ne id="ne410" gf="subj">144</ne>
is
<ne id="ne411" gf="predicate">
a torc</ne> </unit>.
The ranking of the CFs other than the
CP is defined according to the following pref-
erence on their gf (Brennan et al, 1987):
obj>iobj>other. CFs with the same gf are
ranked according to the linear order of the cor-
responding NPs in the utterance. The second
column of Table 1 shows how the utterances in
example (1) are automatically translated by the
scripts developed by Poesio et al (2004) into a
1For example, one could equate ?utterance? with sen-
tence (Strube and Hahn, 1999; Miltsakaki, 2002), use
indirect realisation for the computation of the CF list
(Grosz et al, 1995), rank the CFs according to their
information status (Strube and Hahn, 1999), etc.
2Our definition includes titles which are not always
finite units, but excludes finite relative clauses, the sec-
ond element of coordinated VPs and clause complements
which are often taken as not having their own CF lists
in the literature.
3Or as a post-copular subject in a there-clause.
CF list: cheapness
U {CP, other CFs} CB Transition CBn=CPn?1
(a) {de374, de375} n.a. n.a. n.a.
(b) {de376, de374, de377} de374 retain +
(c) {de374, de379} de374 continue ?
(d) {de380, de381, de382} - nocb +
Table 1: CP, CFs other than CP, CB, nocb or standard (see Table 2) transition and violations of
cheapness (denoted with an asterisk) for each utterance (U) in example (1)
coherence: coherence?:
CBn=CBn?1 CBn 6=CBn?1
or nocb in CFn?1
salience: CBn=CPn continue smooth-shift
salience?: CBn 6=CPn retain rough-shift
Table 2: coherence, salience and the table of standard transitions
sequence of CF lists, each decomposed into the
CP and the CFs other than the CP, according
to the chosen setting of the Centering param-
eters. Note that the CP of (a) is the center
de374 and that the same center is used as the
referent of the other NPs which are annotated
as coreferring with ne410.
Given two subsequent utterances Un?1 and
Un, with CF lists CFn?1 and CFn respectively,
the backward looking center of Un, CBn, is de-
fined as the highest ranked element of CFn?1
which also appears in CFn (Centering?s Con-
straint 3). For instance, the CB of (b) is de374.
The third column of Table 1 shows the CB for
each utterance in (1).4
2.2 Computing transitions
As the fourth column of Table 1 shows, each
utterance, with the exception of (a), is also
marked with a transition from the previous one.
When CFn and CFn?1 do not have any cen-
ters in common, we compute the nocb transi-
tion (Kibble and Power, 2000) (Poesio et als
null transition) for Un (e.g., utterance (d) in
Table 1).5
4In accordance with Centering, no CB is computed
for (a), the first utterance in the sequence.
5In this study we do not take indirect realisation into
account, i.e., we ignore the bridging reference (anno-
tated in the corpus) between the referent of ?it? de374
in (c) and the referent of ?the terminals? de380 in (d),
by virtue of which de374 might be thought as being a
member of the CF list of (d). Poesio et al (2004) showed
that hypothesizing indirect realization eliminates many
violations of entity continuity, the part of Constraint
1 that rules out nocb transitions. However, in this work
we are treating CF lists as an abstract representation
Following again the terminology in Kibble
and Power (2000), we call the requirement that
CBn be the same as CBn?1 the principle of co-
herence and the requirement that CBn be the
same as CPn the principle of salience. Each
of these principles can be satisfied or violated
while their various combinations give rise to the
standard transitions of Centering shown in Ta-
ble 2; Poesio et als scripts compute these vio-
lations.6 We also make note of the preference
between these transitions, known as Centering?s
Rule 2 (Brennan et al, 1987): continue is pre-
ferred to retain, which is preferred to smooth-
shift, which is preferred to rough-shift.
Finally, the scripts determine whether CBn
is the same as CPn?1, known as the principle
of cheapness (Strube and Hahn, 1999). The
last column of Table 1 shows the violations of
cheapness (denoted with an asterisk) in (1).7
2.3 Evaluating the coherence of a text
and text structuring
The statistics about transitions computed as
just discussed can be used to determine the de-
gree to which a text conforms with, or violates,
Centering?s principles. Poesio et al (2004)
found that nocbs account for more than 50%
of the atomic facts the algorithm has to structure, i.e.,
we are assuming that CFs are arguments of such facts;
including indirectly realized entities in CF lists would
violate this assumption.
6If the second utterance in a sequence U2 has a CB,
then it is taken to be either a continue or a retain,
although U1 is not classified as a nocb.
7As for the other two principles, no violation of
cheapness is computed for (a) or when Un is marked as
a nocb.
of the transitions in the gnome corpus in con-
figurations such as the one used in this pa-
per. More generally, a significant percentage of
nocbs (at least 20%) and other ?dispreferred?
transitions was found with all parameter config-
urations tested by Poesio et al (2004) and in-
deed by all previous corpus-based evaluations of
Centering such as Passoneau (1998), Di Eugenio
(1998), Strube and Hahn (1999) among others.
These results led Poesio et al (2004) to the
conclusion that the entity coherence as formal-
ized in Centering should be supplemented with
an account of other coherence inducing factors
to explain what makes texts coherent.
These studies, however, do not investigate
the question that is most important from the
text structuring perspective adopted in this pa-
per: whether there would be alternative ways of
structuring the text that would result in fewer
violations of Centering?s constraints (Kibble,
2001). Consider the nocb utterance (d) in (1).
Simply observing that this transition is ?dispre-
ferred? ignores the fact that every other ordering
of utterances (b) to (d) would result in more
nocbs than those found in (1). Even a text-
structuring algorithm functioning solely on the
basis of the Centering constraints might there-
fore still choose the particular order in (1). In
other words, a metric of text coherence purely
based on Centering principles?trying to mini-
mize the number of nocbs?may be sufficient to
explain why this order of clauses was chosen,
at least in this particular genre, without need
to involve more complex explanations. In the
rest of the paper, we consider several such met-
rics, and use the texts in the gnome corpus to
choose among them. We return to the issue of
coherence (i.e., whether additional coherence-
inducing factors need to be stipulated in addi-
tion to those assumed in Centering) in the Dis-
cussion.
3 Centering-based metrics of
coherence
As said previously, we assume a text structuring
system taking as input a set of utterances rep-
resented in terms of their CF lists. The system
orders these utterances by applying a bias in
favour of the best scoring ordering among the
candidate solutions for the preferred output.8
In this section, we discuss how the Centering
8Additional assumptions for choosing between the or-
derings that are assigned the best score are presented in
the next section.
concepts just described can be used to define
metrics of coherence which might be useful for
text structuring.
The simplest way to define a metric of coher-
ence using notions from Centering is to classify
each ordering of propositions according to the
number of nocbs it contains, and pick the or-
dering with the fewest nocbs. We call this met-
ric M.NOCB, following (Karamanis and Manu-
rung, 2002). Because of its simplicity, M.NOCB
serves as the baseline metric in our experiments.
We consider three more metrics. M.CHEAP
is biased in favour of the ordering with the
fewest violations of cheapness. M.KP sums
up the nocbs and the violations of cheapness,
coherence and salience, preferring the or-
dering with the lowest total cost (Kibble and
Power, 2000). Finally, M.BFP employs the
preferences between standard transitions as ex-
pressed by Rule 2. More specifically, M.BFP
selects the ordering with the highest number
of continues. If there exist several orderings
which have the most continues, the one which
has the most retains is favoured. The number
of smooth-shifts is used only to distinguish
between the orderings that score best for con-
tinues as well as for retains, etc.
In the next section, we present a general
methodology to compare these metrics, using
the actual ordering of clauses in real texts of
a corpus to identify the metric whose behav-
ior mimics more closely the way these actual
orderings were chosen. This methodology was
implemented in a program called the System for
Evaluating Entity Coherence (seec).
4 Exploring the space of possible
orderings
In section 2, we discussed how an ordering of
utterances in a text like (1) can be translated
into a sequence of CF lists, which is the repre-
sentation that the Centering-based metrics op-
erate on. We use the term Basis for Comparison
(BfC) to indicate this sequence of CF lists. In
this section, we discuss how the BfC is used in
our search-oriented evaluation methodology to
calculate a performance measure for each metric
and compare them with each other. In the next
section, we will see how our corpus was used
to identify the most promising Centering-based
metric for a text classifier.
4.1 Computing the classification rate
The performance measure we employ is called
the classification rate of a metric M on a cer-
tain BfC B. The classification rate estimates
the ability of M to produce B as the output of
text structuring according to a specific genera-
tion scenario.
The first step of seec is to search through
the space of possible orderings defined by the
permutations of the CF lists that B consists of,
and to divide the explored search space into sets
of orderings that score better, equal, or worse
than B according to M.
Then, the classification rate is defined accord-
ing to the following generation scenario. We
assume that an ordering has higher chances of
being selected as the output of text structuring
the better it scores for M. This is turn means
that the fewer the members of the set of better
scoring orderings, the better the chances of B
to be the chosen output.
Moreover, we assume that additional factors
play a role in the selection of one of the order-
ings that score the same for M. On average, B
is expected to sit in the middle of the set of
equally scoring orderings with respect to these
additional factors. Hence, half of the orderings
with the same score will have better chances
than B to be selected by M.
The classification rate ? of a metric M on
B expresses the expected percentage of order-
ings with a higher probability of being gener-
ated than B according to the scores assigned
by M and the additional biases assumed by the
generation scenario as follows:
(3) Classification rate:
?(M,B) = Better(M) + Equal(M)2
Better(M) stands for the percentage of order-
ings that score better than B according to M,
whilst Equal(M) is the percentage of order-
ings that score equal to B according to M. If
?(Mx, B) is the classification rate of Mx on B,
and ?(My, B) is the classification rate of My on
B, My is a more suitable candidate than Mx
for generating B if ?(My, B) is smaller than
?(Mx, B).
4.2 Generalising across many BfCs
In order for the experimental results to be re-
liable and generalisable, Mx and My should be
compared on more than one BfC from a corpus
C. In our standard analysis, the BfCs B1, ..., Bm
from C are treated as the random factor in a
repeated measures design since each BfC con-
tributes a score for each metric. Then, the clas-
sification rates for Mx and My on the BfCs are
compared with each other and significance is
tested using the Sign Test. After calculating the
number of BfCs that return a lower classifica-
tion rate for Mx than for My and vice versa, the
Sign Test reports whether the difference in the
number of BfCs is significant, that is, whether
there are significantly more BfCs with a lower
classification rate for Mx than the BfCs with a
lower classification rate for My (or vice versa).9
Finally, we summarise the performance of M
on m BfCs from C in terms of the average clas-
sification rate Y :
(4) Average classification rate:
Y (M,C) = ?(M,B1)+...+?(M,Bm)m
5 Using the gnome corpus for a
search-based comparison of
metrics
We will now discuss how the methodology
discussed above was used to compare the
Centering-based metrics discussed in Section
3, using the original ordering of texts in the
gnome corpus to compute the average classi-
fication rate of each metric.
The gnome corpus contains texts from differ-
ent genres, not all of which are of interest to us.
In order to restrict the scope of the experiment
to the text-type most relevant to our study, we
selected 20 ?museum labels?, i.e., short texts
that describe a concrete artefact, which served
as the input to seec together with the metrics
in section 3.10
5.1 Permutation and search strategy
In specifying the performance of the metrics we
made use of a simple permutation heuristic ex-
ploiting a piece of domain-specific communica-
tion knowledge (Kittredge et al, 1991). Like
Dimitromanolaki and Androutsopoulos (2003),
we noticed that utterances like (a) in exam-
ple (1), should always appear at the beginning
of a felicitous museum label. Hence, we re-
stricted the orderings considered by the seec
9The Sign Test was chosen over its parametric al-
ternatives to test significance because it does not carry
specific assumptions about population distributions and
variance. It is also more appropriate for small samples
like the one used in this study.
10Note that example (1) is characteristic of the genre,
not the length, of the texts in our subcorpus. The num-
ber of CF lists that the BfCs consist of ranges from 4 to
16 (average cardinality: 8.35 CF lists).
Pair M.NOCB p Winner
lower greater ties
M.NOCB vs M.CHEAP 18 2 0 0.000 M.NOCB
M.NOCB vs M.KP 16 2 2 0.001 M.NOCB
M.NOCB vs M.BFP 12 3 5 0.018 M.NOCB
N 20
Table 3: Comparing M.NOCB with M.CHEAP, M.KP and M.BFP in gnome
to those in which the first CF list of B, CF1,
appears in first position.11
For very short texts like (1), which give rise to
a small BfC, the search space of possible order-
ings can be enumerated exhaustively. However,
when B consists of many more CF lists, it is im-
practical to explore the search space in this way.
Elsewhere we show that even in these cases it
is possible to estimate ?(M,B) reliably for the
whole population of orderings using a large ran-
dom sample. In the experiments reported here,
we had to resort to random sampling only once,
for a BfC with 16 CF lists.
5.2 Comparing M.NOCB with other
metrics
The experimental results of the comparisons of
the metrics from section 3, computed using the
methodology in section 4, are reported in Ta-
ble 3.
In this table, the baseline metric M.NOCB is
compared with each of M.CHEAP, M.KP and
M.BFP. The first column of the Table identifies
the comparison in question, e.g. M.NOCB ver-
sus M.CHEAP. The exact number of BfCs for
which the classification rate of M.NOCB is lower
than its competitor for each comparison is re-
ported in the next column of the Table. For ex-
ample, M.NOCB has a lower classification rate
than M.CHEAP for 18 (out of 20) BfCs from
the gnome corpus. M.CHEAP only achieves a
lower classification rate for 2 BfCs, and there
are no ties, i.e. cases where the classification
rate of the two metrics is the same. The p value
returned by the Sign Test for the difference in
the number of BfCs, rounded to the third deci-
mal place, is reported in the fifth column of the
Table. The last column of the Table 3 shows
M.NOCB as the ?winner? of the comparison
with M.CHEAP since it has a lower classifica-
11Thus, we assume that when the set of CF lists serves
as the input to text structuring, CF1 will be identified
as the initial CF list of the ordering to be generated
using annotation features such as the unit type which
distinguishes (a) from the other utterances in (1).
tion rate than its competitor for significantly
more BfCs in the corpus.12
Overall, the Table shows that M.NOCB does
significantly better than the other three metrics
which employ additional Centering concepts.
This result means that there exist proportion-
ally fewer orderings with a higher probability of
being selected than the BfC when M.NOCB is
used to guide the hypothetical text structuring
algorithm instead of the other metrics.
Hence, M.NOCB is the most suitable among
the investigated metrics for structuring the CF
lists in gnome. This in turn indicates that sim-
ply avoiding nocb transitions is more relevant
to text structuring than the combinations of the
other Centering notions that the more compli-
cated metrics make use of. (However, these no-
tions might still be appropriate for other tasks,
such as anaphora resolution.)
6 Discussion: the performance of
M.NOCB
We already saw that Poesio et al (2004) found
that the majority of the recorded transitions in
the configuration of Centering used in this study
are nocbs. However, we also explained in sec-
tion 2.3 that what really matters when trying
to determine whether a text might have been
generated only paying attention to Centering
constraints is the extent to which it would be
possible to ?improve? upon the ordering chosen
in that text, given the information that the text
structuring algorithm had to convey. The av-
erage classification rate of M.NOCB is an esti-
12No winner is reported for a comparison when the p
value returned by the Sign Test is not significant (ns),
i.e. greater than 0.05. Note also that despite conduct-
ing more than one pairwise comparison simultaneously
we refrain from further adjusting the overall threshold
of significance (e.g. according to the Bonferroni method,
typically used for multiple planned comparisons that em-
ploy parametric statistics) since it is assumed that choos-
ing a conservative statistic such as the Sign Test already
provides substantial protection against the possibility of
a type I error.
Pair M.NOCB p Winner
lower greater ties
M.NOCB vs M.CHEAP 110 12 0 0.000 M.NOCB
M.NOCB vs M.KP 103 16 3 0.000 M.NOCB
M.NOCB vs M.BFP 41 31 49 0.121 ns
N 122
Table 4: Comparing M.NOCB with M.CHEAP, M.KP and M.BFP using the novel methodology
in MPIRO
mate of exactly this variable, indicating whether
M.NOCB is likely to arrive at the BfC during
text structuring.
The average classification rate Y for
M.NOCB on the subcorpus of gnome studied
here, for the parameter configuration of Cen-
tering we have assumed, is 19.95%. This means
that on average the BfC is close to the top 20%
of alternative orderings when these orderings
are ranked according to their probability of
being selected as the output of the algorithm.
On the one hand, this result shows that al-
though the ordering of CF lists in the BfC
might not completely minimise the number of
observed nocb transitions, the BfC tends to
be in greater agreement with the preference to
avoid nocbs than most of the alternative or-
derings. In this sense, it appears that the BfC
optimises with respect to the number of poten-
tial nocbs to a certain extent. On the other
hand, this result indicates that there are quite
a few orderings which would appear more likely
to be selected than the BfC.
We believe this finding can be interpreted in
two ways. One possibility is that M.NOCB
needs to be supplemented by other features in
order to explain why the original text was struc-
tured this way. This is the conclusion arrived at
by Poesio et al (2004) and those text structur-
ing practitioners who use notions derived from
Centering in combination with other coherence
constraints in the definitions of their metrics.
There is also a second possibility, however: we
might want to reconsider the assumption that
human text planners are trying to ensure that
each utterance in a text is locally coherent.
They might do all of their planning just on the
basis of Centering constraints, at least in this
genre ?perhaps because of resource limitations?
and simply accept a certain degree of incoher-
ence. Further research on this issue will require
psycholinguistic methods; our analysis never-
theless sheds more light on two previously un-
addressed questions in the corpus-based evalu-
ation of Centering ? a) which of the Centering
notions are most relevant to the text structur-
ing task, and b) to which extent Centering on
its own can be useful for this purpose.
7 Further results
In related work, we applied the methodology
discussed here to a larger set of existing data
(122 BfCs) derived from the MPIRO system
and ordered by a domain expert (Dimitro-
manolaki and Androutsopoulos, 2003). As Ta-
ble 4 shows, the results from MPIRO verify the
ones reported here, especially with respect to
M.KP and M.CHEAP which are overwhelm-
ingly beaten by the baseline in the new do-
main as well. Also note that since M.BFP fails
to overtake M.NOCB in MPIRO, the baseline
can be considered the most promising solution
among the ones investigated in both domains
by applying Occam?s logical principle.
We also tried to account for some additional
constraints on coherence, namely local rhetor-
ical relations, based on some of the assump-
tions in Knott et al (2001), and what Kara-
manis (2003) calls the ?PageFocus? which cor-
responds to the main entity described in a text,
in our example de374. These results, reported
in (Karamanis, 2003), indicate that these con-
straints conflict with Centering as formulated in
this paper, by increasing - instead of reducing
- the classification rate of the metrics. Hence,
it remains unclear to us how to improve upon
M.NOCB.
In our future work, we would like to experi-
ment with more metrics. Moreover, although we
consider the parameter configuration of Center-
ing used here a plausible choice, we intend to ap-
ply our methodology to study different instan-
tiations of the Centering parameters, e.g. by
investigating whether ?indirect realisation? re-
duces the classification rate for M.NOCB com-
pared to ?direct realisation?, etc.
Acknowledgements
Special thanks to James Soutter for writing the
program which translates the output produced by
gnome?s scripts into a format appropriate for seec.
The first author was able to engage in this research
thanks to a scholarship from the Greek State Schol-
arships Foundation (IKY).
References
Regina Barzilay, Noemie Elhadad, and Kath-
leen McKeown. 2002. Inferring strategies
for sentence ordering in multidocument news
summarization. Journal of Artificial Intelli-
gence Research, 17:35?55.
Susan E. Brennan, Marilyn A. Fried-
man [Walker], and Carl J. Pollard. 1987. A
centering approach to pronouns. In Proceed-
ings of ACL 1987, pages 155?162, Stanford,
California.
Barbara Di Eugenio. 1998. Centering in Italian.
In Walker et al (Walker et al, 1998b), pages
115?137.
Aggeliki Dimitromanolaki and Ion Androut-
sopoulos. 2003. Learning to order facts for
discourse planning in natural language gen-
eration. In Proceedings of the 9th European
Workshop on Natural Language Generation,
Budapest, Hungary.
Barbara J. Grosz, Aravind K. Joshi, and Scott
Weinstein. 1995. Centering: A framework
for modeling the local coherence of discourse.
Computational Linguistics, 21(2):203?225.
Amy Isard, Jon Oberlander, Ion Androutsopou-
los, and Colin Matheson. 2003. Speaking the
users? languages. IEEE Intelligent Systems
Magazine, 18(1):40?45.
Nikiforos Karamanis and Hisar Maruli Manu-
rung. 2002. Stochastic text structuring us-
ing the principle of continuity. In Proceedings
of INLG 2002, pages 81?88, Harriman, NY,
USA, July.
Nikiforos Karamanis. 2003. Entity Coherence
for Descriptive Text Structuring. Ph.D. the-
sis, Division of Informatics, University of Ed-
inburgh.
Rodger Kibble and Richard Power. 2000. An
integrated framework for text planning and
pronominalisation. In Proceedings of INLG
2000, pages 77?84, Israel.
Rodger Kibble. 2001. A reformulation of Rule
2 of Centering Theory. Computational Lin-
guistics, 27(4):579?587.
Richard Kittredge, Tanya Korelsky, and Owen
Rambow. 1991. On the need for domain com-
munication knowledge. Computational Intel-
ligence, 7:305?314.
Alistair Knott, Jon Oberlander, Mick
O?Donnell, and Chris Mellish. 2001. Beyond
elaboration: The interaction of relations
and focus in coherent text. In T. Sanders,
J. Schilperoord, and W. Spooren, edi-
tors, Text Representation: Linguistic and
Psycholinguistic Aspects, chapter 7, pages
181?196. John Benjamins.
Mirella Lapata. 2003. Probabilistic text struc-
turing: Experiments with sentence ordering.
In Proceedings of ACL 2003, Saporo, Japan,
July.
Chris Mellish, Alistair Knott, Jon Oberlander,
and Mick O?Donnell. 1998. Experiments us-
ing stochastic search for text planning. In
Proceedings of the 9th International Work-
shop on NLG, pages 98?107, Niagara-on-the-
Lake, Ontario, Canada.
Eleni Miltsakaki. 2002. Towards an aposyn-
thesis of topic continuity and intrasenten-
tial anaphora. Computational Linguistics,
28(3):319?355.
Mick O?Donnell, Chris Mellish, Jon Oberlan-
der, and Alistair Knott. 2001. ILEX: An ar-
chitecture for a dynamic hypertext genera-
tion system. Natural Language Engineering,
7(3):225?250.
Rebecca J. Passoneau. 1998. Interaction of dis-
course structure with explicitness of discourse
anaphoric phrases. In Walker et al (Walker
et al, 1998b), pages 327?358.
Massimo Poesio, Rosemary Stevenson, Barbara
Di Eugenio, and Janet Hitzeman. 2004. Cen-
tering: a parametric theory and its instantia-
tions. Computational Linguistics, 30(3).
Ehud Reiter and Robert Dale. 2000. Building
Natural Language Generation Systems. Cam-
bridge.
Michael Strube and Udo Hahn. 1999. Func-
tional centering: Grounding referential coher-
ence in information structure. Computational
Linguistics, 25(3):309?344.
Marilyn A. Walker, Aravind K. Joshi, and
Ellen F. Prince. 1998a. Centering in nat-
urally occuring discourse: An overview. In
Walker et al (Walker et al, 1998b), pages
1?30.
Marilyn A. Walker, Aravind K. Joshi, and
Ellen F. Prince, editors. 1998b. Centering
Theory in Discourse. Clarendon Press, Ox-
ford.
Computing Locally Coherent Discourses
Ernst Althaus
LORIA
Universite? Henri Poincare?
Vand?uvre-le`s-Nancy, France
althaus@loria.fr
Nikiforos Karamanis
School of Informatics
University of Edinburgh
Edinburgh, UK
N.Karamanis@sms.ed.ac.uk
Alexander Koller
Dept. of Computational Linguistics
Saarland University
Saarbru?cken, Germany
koller@coli.uni-sb.de
Abstract
We present the first algorithm that computes opti-
mal orderings of sentences into a locally coherent
discourse. The algorithm runs very efficiently on a
variety of coherence measures from the literature.
We also show that the discourse ordering problem
is NP-complete and cannot be approximated.
1 Introduction
One central problem in discourse generation and
summarisation is to structure the discourse in a
way that maximises coherence. Coherence is the
property of a good human-authored text that makes
it easier to read and understand than a randomly-
ordered collection of sentences.
Several papers in the recent literature (Mellish et
al., 1998; Barzilay et al, 2002; Karamanis and Ma-
nurung, 2002; Lapata, 2003; Karamanis et al, 2004)
have focused on defining local coherence, which
evaluates the quality of sentence-to-sentence transi-
tions. This is in contrast to theories of global coher-
ence, which can consider relations between larger
chunks of the discourse and e.g. structures them into
a tree (Mann and Thompson, 1988; Marcu, 1997;
Webber et al, 1999). Measures of local coherence
specify which ordering of the sentences makes for
the most coherent discourse, and can be based e.g.
on Centering Theory (Walker et al, 1998; Brennan
et al, 1987; Kibble and Power, 2000; Karamanis
and Manurung, 2002) or on statistical models (Lap-
ata, 2003).
But while formal models of local coherence have
made substantial progress over the past few years,
the question of how to efficiently compute an order-
ing of the sentences in a discourse that maximises
local coherence is still largely unsolved. The fun-
damental problem is that any of the factorial num-
ber of permutations of the sentences could be the
optimal discourse, which makes for a formidable
search space for nontrivial discourses. Mellish et
al. (1998) and Karamanis and Manurung (2002)
present algorithms based on genetic programming,
and Lapata (2003) uses a graph-based heuristic al-
gorithm, but none of them can give any guarantees
about the quality of the computed ordering.
This paper presents the first algorithm that com-
putes optimal locally coherent discourses, and es-
tablishes the complexity of the discourse ordering
problem. We first prove that the discourse order-
ing problem for local coherence measures is equiva-
lent to the Travelling Salesman Problem (TSP). This
means that discourse ordering is NP-complete, i.e.
there are probably no polynomial algorithms for it.
Worse, our result implies that the problem is not
even approximable; any polynomial algorithm will
compute arbitrarily bad solutions on unfortunate in-
puts. Note that all approximation algorithms for the
TSP assume that the underlying cost function is a
metric, which is not the case for the coherence mea-
sures we consider.
Despite this negative result, we show that by ap-
plying modern algorithms for TSP, the discourse or-
dering problem can be solved efficiently enough for
practical applications. We define a branch-and-cut
algorithm based on linear programming, and evalu-
ate it on discourse ordering problems based on the
GNOME corpus (Karamanis, 2003) and the BLLIP
corpus (Lapata, 2003). If the local coherence mea-
sure depends only on the adjacent pairs of sentences
in the discourse, we can order discourses of up to 50
sentences in under a second. If it is allowed to de-
pend on the left-hand context of the sentence pair,
computation is often still efficient, but can become
expensive.
The structure of the paper is as follows. We will
first formally define the discourse ordering problem
and relate our definition to the literature on local co-
herence measures in Section 2. Then we will prove
the equivalence of discourse ordering and TSP (Sec-
tion 3), and present algorithms for solving it in Sec-
tion 4. Section 5 evaluates our algorithms on exam-
ples from the literature. We compare our approach
to various others in Section 6, and then conclude in
Section 7.
2 The Discourse Ordering Problem
We will first give a formal definition of the prob-
lem of computing locally coherent discourses, and
demonstrate how some local coherence measures
from the literature fit into this framework.
2.1 Definitions
We assume that a discourse is made up of discourse
units (depending on the underlying theory, these
could be utterances, sentences, clauses, etc.), which
must be ordered to achieve maximum local coher-
ence. We call the problem of computing the optimal
ordering the discourse ordering problem.
We formalise the problem by assigning a cost to
each unit-to-unit transition, and a cost for the dis-
course to start with a certain unit. Transition costs
may depend on the local context, i.e. a fixed num-
ber of discourse units to the left may influence the
cost of a transition. The optimal ordering is the one
which minimises the sum of the costs.
Definition 1. A d-place transition cost function for
a set U of discourse units is a function cT : Ud ?
R. Intuitively, cT (un|u1, . . . , ud?1) is the cost of
the transition (ud?1, ud) given that the immediately
preceding units were u1, . . . , ud?2.
A d-place initial cost function for U is a function
cI : Ud ? R. Intuitively, cI(u1, . . . , ud) is the
cost for the fact that the discourse starts with the
sequence u1, . . . , ud.
The d-place discourse ordering problem is de-
fined as follows: Given a set U = {u1, . . . , un},
a d-place transition cost function cT and a (d ? 1)-
place initial cost function cI , compute a permutation
pi of {1, . . . , n} such that
cI(upi(1), . . . , upi(d?1))
+
n?d+1?
i=1
cT (upi(i+d?1)|upi(i), . . . , upi(i+d?2))
is minimal.
The notation for the cost functions is suggestive:
The transition cost function has the character of a
conditional probability, which specifies that the cost
of continuing the discourse with the unit ud depends
on the local context u1, . . . , ud?1. This local con-
text is not available for the first d ? 1 units of the
discourse, which is why their costs are summarily
covered by the initial function.
2.2 Centering-Based Cost Functions
One popular class of coherence measures is based
on Centering Theory (CT, (Walker et al, 1998)). We
will briefly sketch its basic notions and then show
how some CT-based coherence measures can be cast
into our framework.
The standard formulation of CT e.g. in (Walker et
al., 1998), calls the discourse units utterances, and
assigns to each utterance ui in the discourse a list
Cf(ui) of forward-looking centres. The members
of Cf(ui) correspond to the referents of the NPs
in ui and are ranked in order of prominence, the
first element being the preferred centre Cp(ui). The
backward-looking centre Cb(ui) of ui is defined as
the highest ranked element of Cf(ui) which also ap-
pears in Cf(ui?1), and serves as the link between
the two subsequent utterances ui?1 and ui. Each
utterance has at most one Cb. If ui and ui?1 have
no forward-looking centres in common, or if ui is
the first utterance in the discourse, then ui does not
have a Cb at all.
Based on these concepts, CT classifies the tran-
sitions between subsequent utterances into differ-
ent types. Table 1 shows the most common clas-
sification into the four types CONTINUE, RETAIN,
SMOOTH-SHIFT, and ROUGH-SHIFT, which are pre-
dicted to be less and less coherent in this order
(Brennan et al, 1987). Kibble and Power (2000)
define three further classes of transitions: COHER-
ENCE and SALIENCE, which are both defined in Ta-
ble 1 as well, and NOCB, the class of transitions
for which Cb(ui) is undefined. Finally, a transition
is considered to satisfy the CHEAPNESS constraint
(Strube and Hahn, 1999) if Cb(ui) = Cp(ui?1).
Table 2 summarises some cost functions from the
literature, in the reconstruction of Karamanis et al
(2004). Each line shows the name of the coherence
measure, the arity d from Definition 1, and the ini-
tial and transition cost functions. To fit the defini-
tions in one line, we use terms of the form fk, which
abbreviate applications of f to the last k arguments
of the cost functions, i.e. f(ud?k+1, . . . , ud).
The most basic coherence measure, M.NOCB
(Karamanis and Manurung, 2002), simply assigns
to each NOCB transition the cost 1 and to every other
transition the cost 0. The definition of cT (u2|u1),
which decodes to nocb(u1, u2), only looks at the
two units in the transition, and no further context.
The initial costs for this coherence measure are al-
ways zero.
The measure M.KP (Kibble and Power, 2000)
sums the value of nocb and the values of three func-
tions which evaluate to 0 if the transition is cheap,
salient, or coherent, and 1 otherwise. This is an in-
stance of the 3-place discourse ordering problem be-
cause COHERENCE depends on Cb(ui?1), which it-
self depends on Cf(ui?2); hence nocoh must take
COHERENCE: COHERENCE?:
Cb(ui) = Cb(ui?1) Cb(ui) 6= Cb(ui?1)
SALIENCE: Cb(ui) = Cp(ui) CONTINUE SMOOTH-SHIFT
SALIENCE?: Cb(ui) 6= Cp(ui) RETAIN ROUGH-SHIFT
Table 1: COHERENCE, SALIENCE and the table of standard transitions
d initial cost cI(u1, . . . , ud?1) transition cost cT (ud|u1, . . . , ud?1)
M.NOCB 2 0 nocb2
M.KP 3 nocb2 + nocheap2 + nosal2 nocb2 + nocheap2 + nosal2 + nocoh3
M.BFP 3 (1? nosal2, nosal2, 0, 0) (cont3, ret3, ss3, rs3)
M.LAPATA 2 ? logP (u1) ? logP (u2|u1)
Table 2: Some cost functions from the literature.
three arguments.
Finally, the measure M.BFP (Brennan et al,
1987) uses a lexicographic ordering on 4-tuples
which indicate whether the transition is a CON-
TINUE, RETAIN, SMOOTH-SHIFT, or ROUGH-
SHIFT. cT and all four functions it is computed from
take three arguments because the classification de-
pends on COHERENCE. As the first transition in the
discourse is coherent by default (it has no Cb), we
can compute cI by distinguishing RETAIN and CON-
TINUE via SALIENCE. The tuple-valued cost func-
tions can be converted to real-valued functions by
choosing a sufficiently large number M and using
the value M3 ? cont + M2 ? ret + M ? ss + rs.
2.3 Probability-Based Cost Functions
A fundamentally different approach to measure dis-
course coherence was proposed by Lapata (2003).
It uses a statistical bigram model that assigns each
pair ui, uk of utterances a probability P (uk|ui) of
appearing in subsequent positions, and each utter-
ance a probability P (ui) of appearing in the initial
position of the discourse. The probabilities are es-
timated on the grounds of syntactic features of the
discourse units. The probability of the entire dis-
course u1 . . . un is the product P (u1) ? P (u2|u1) ?
. . . ? P (un|un?1).
We can transform Lapata?s model straightfor-
wardly into our cost function framework, as shown
under M.LAPATA in Table 2. The discourse that
minimizes the sum of the negative logarithms will
also maximise the product of the probabilities. We
have d = 2 because it is a bigram model in which
the transition probability does not depend on the
previous discourse units.
3 Equivalence of Discourse Ordering and
TSP
Now we show that discourse ordering and the travel-
ling salesman problem are equivalent. In order to do
this, we first redefine discourse ordering as a graph
problem.
d-place discourse ordering problem (dPDOP):
Given a directed graph G = (V,E), a node
s ? V and a function c : V d ? R, compute a
simple directed path P = (s = v0, v1, . . . , vn)
from s through all vertices in V which min-
imises
?n?d+1
i=0 c(vi, vi+1, . . . , vi+d?1). We
write instances of dPDOP as (V,E, s, c).
The nodes v1, . . . , vn correspond to the discourse
units. The cost function c encodes both the initial
and the transition cost functions from Section 2 by
returning the initial cost if its first argument is the
(new) start node s.
Now let?s define the version of the travelling
salesman problem we will use below.
Generalised asymmetric TSP (GATSP): Given a
directed graph G = (V,E), edge weights c :
E ? R, and a partition (V1, . . . , Vk) of the
nodes V , compute the shortest directed cycle
that visits exactly one node of each Vi. We
call such a cycle a tour and write instances of
GATSP as ((V1, . . . , Vk), E, c).
The usual definition of the TSP, in which every
node must be visited exactly once, is the special
case of GATSP where each Vi contains exactly one
node. We call this case asymmetric travelling sales-
man problem, ATSP.
ATSP 2PDOP
 


 
 


Figure 1: Reduction of ATSP to 2PDOP
We will show that ATSP can be reduced to
2PDOP, and that any dPDOP can be reduced to
GATSP.
3.1 Reduction of ATSP to 2PDOP
First, we introduce the reduction of ATSP to
2PDOP, which establishes NP-completeness of
dPDOP for all d > 1. The reduction is approxi-
mation preserving, i.e. if we can find a solution of
2PDOP that is worse than the optimum only by a
factor of  (an -approximation), it translates to a
solution of ATSP that is also an -approximation.
Since it is known that there can be no polynomial al-
gorithms that compute -approximations for general
ATSP, for any  (Cormen et al, 1990), this means
that dPDOP cannot be approximated either (unless
P=NP): Any polynomial algorithm for dPDOP will
compute arbitrarily bad solutions on certain inputs.
The reduction works as follows. Let G =
((V1, . . . , Vk), E, c) be an instance of ATSP, and
V = V1 ? . . . ? Vk. We choose an arbitrary node
v ? V and split it into two nodes vs and vt. We as-
sign all edges with source node v to vs and all edges
with target node v to vt (compare Figure 1). Finally
we make vs the source node of our 2PDOP instance
G?.
For every tour in G, we have a path in G? starting
at vs visiting all other nodes (and ending in vt) with
the same cost by replacing the edge (v, u) out of
v by (vs, u) and the edge (w, v) into v by (w, vt).
Conversely, for every path starting at vs visiting all
nodes, we have an ATSP tour of the same cost, since
all such paths will end in vt (as vt has no outgoing
edges).
An example is shown in Fig. 1. The ATSP in-
stance on the left has the tour (1, 3, 2, 1), indicated
by the solid edges. The node 1 is split into the two
nodes 1s and 1t, and the tour translates to the path
(1s, 3, 2, 1t) in the 2PDOP instance.
3.2 Reduction of dPDOP to GATSP
Conversely, we can encode an instance G =
(V,E, s, c) of dPDOP as an instance G? =
3PDOP GATSP
 
 
 

	 

 

 
	
 

	 

 

 
  
Figure 2: Reduction of dPDOP to GATSP. Edges to
the source node [s, s] are not drawn.
((V ?u)u?V , E
?, c?) of GATSP, in such a way that the
optimal solutions correspond. The cost of traversing
an edge in dPDOP depends on the previous d ? 1
nodes; we compress these costs into ordinary costs
of single edges in the reduction to GATSP.
The GATSP instance has a node [u1, . . . , ud?1]
for every d ? 1-tuple of nodes of V . It has an edge
from [u1, . . . , ud?1] to [u2, . . . , ud?1, ud] iff there
is an edge from ud?1 to ud in G, and it has an edge
from each node into [s, . . . , s]. The idea is to en-
code a path P = (s = u0, u1, . . . , un) in G as
a tour TP in G? that successively visits the nodes
[ui?d+1, . . . ui], i = 0, . . . n, where we assume that
uj = s for all j ? 0 (compare Figure 2).
The cost of TP can be made equal to the cost of P
by making the cost of the edge from [u1, . . . , ud?1]
to [u2, . . . , ud] equal to c(u1, . . . ud). (We set c?(e)
to 0 for all edges e between nodes with first compo-
nent s and for the edges e with target node [sd?1].)
Finally, we define V ?u to be the set of all nodes in G?
with last component u. It is not hard to see that for
any simple path of length n in G, we find a tour TP
in G? with the same cost. Conversely, we can find
for every tour in G? a simple path of length n in G
with the same cost.
Note that the encoding G? will contain many un-
necessary nodes and edges. For instance, all nodes
that have no incoming edges can never be used in a
tour, and can be deleted. We can safely delete such
unnecessary nodes in a post-processing step.
An example is shown in Fig. 2. The 3PDOP
instance on the left has a path (s, 3, 1, 2), which
translates to the path ([s, s], [s, 3], [3, 1], [1, 2]) in
the GATSP instance shown on the right. This path
can be completed by a tour by adding the edge
([1, 2], [s, s]), of cost 0. The tour indeed visits each
V ?u (i.e., each column) exactly once. Nodes with last
component s which are not [s, s] are unreachable
and are not shown.
For the special case of d = 2, the GATSP is sim-
ply an ordinary ATSP. The graphs of both problems
look identical in this case, except that the GATSP
instance has edges of cost 0 from any node to the
source [s].
4 Computing Optimal Orderings
The equivalence of dPDOP and GATSP implies that
we can now bring algorithms from the vast litera-
ture on TSP to bear on the discourse ordering prob-
lem. One straightforward method is to reduce the
GATSP further to ATSP (Noon and Bean, 1993);
for the case d = 2, nothing has to be done. Then
one can solve the reduced ATSP instance; see (Fis-
chetti et al, 2001; Fischetti et al, 2002) for a recent
survey of exact methods.
We choose the alternative of developing a new
algorithm for solving GATSP directly, which uses
standard techniques from combinatorial optimisa-
tion, gives us a better handle on optimising the al-
gorithm for our problem instances, and runs more
efficiently in practice. Our algorithm translates
the GATSP instance into an integer linear pro-
gram (ILP) and uses the branch-and-cut method
(Nemhauser and Wolsey, 1988) to solve it. Integer
linear programs consist of a set of linear equations
and inequalities, and are solved by integer variable
assignments which maximise or minimise a goal
function while satisfying the other conditions.
Let G = (V,E) be a directed graph and S ? V .
We define ?+(S) = {(u, v) ? E | u ? S and v 6?
S} and ??(S) = {(u, v) ? E | u /? S and v ? S},
i.e. ?+(S) and ??(S) are the sets of all incoming
and outgoing edges of S, respectively. We assume
that the graph G has no edges within one partition
Vu, since such edges cannot be used by any solution.
With this assumption, GATSP can be phrased as an
ILP as follows (this formulation is similar to the one
proposed by Laporte et al (1987)):
min
?
e?E
cexe
s.t.
?
e??+(v)
xe =
?
e???(v)
xe ? v ? V (1)
?
e???(Vi)
xe = 1 1 ? i ? n (2)
?
e??+(?i?IVi)
xe ? 1 I ? {1, . . . , n} (3)
xe ? {0, 1}
We have a binary variable xe for each edge e of
the graph. The intention is that xe has value 1 if
e is used in the tour, and 0 otherwise. Thus the
cost of the tour can be written as
?
e?E cexe. The
three conditions enforce the variable assignment to
encode a valid GATSP tour. (1) ensures that all inte-
ger solutions encode a set of cycles. (2) guarantees
that every partition Vi is visited by exactly one cy-
cle. The inequalities (3) say that every subset of the
partitions has an outgoing edge; this makes sure a
solution encodes one cycle, rather than a set of mul-
tiple cycles.
To solve such an ILP using the branch-and-cut
method, we drop the integrality constraints (i.e. we
replace xe ? {0, 1} by 0 ? xe ? 1) and solve
the corresponding linear programming (LP) relax-
ation. If the solution of the LP is integral, we found
the optimal solution. Otherwise we pick a variable
with a fractional value and split the problem into
two subproblems by setting the variable to 0 and 1,
respectively. We solve the subproblems recursively
and disregard a subproblem if its LP bound is worse
than the best known solution.
Since our ILP contains an exponential number of
inequalities of type (3), solving the complete LPs
directly would be too expensive. Instead, we start
with a small subset of these inequalities, and test
(efficiently) whether a solution of the smaller LP
violates an inequality which is not in the current
LP. If so, we add the inequality to the LP, resolve
it, and iterate. Otherwise we found the solution of
the LP with the exponential number of inequalities.
The inequalities we add by need are called cutting
planes; algorithms that find violated cutting planes
are called separation algorithms.
To keep the size of the branch-and-cut tree small,
our algorithm employs some heuristics to find fur-
ther upper bounds. In addition, we improve lower
bound from the LP relaxations by adding further in-
equalities to the LP that are valid for all integral so-
lutions, but can be violated for optimal solutions of
the LP. One major challenge here was to find separa-
tion algorithms for these inequalities. We cannot go
into these details for lack of space, but will discuss
them in a separate paper.
5 Evaluation
We implemented the algorithm and ran it on some
examples to evaluate its practical efficiency. The
runtimes are shown in Tables 3 and 4 for an imple-
mentation using a branch-and-cut ILP solver which
is free for all academic purposes (ILP-FS) and a
commercial branch-and-cut ILP solver (ILP-CS).
Our implementations are based on LEDA 4.4.1
Instance Size ILP-FS ILP-CS
lapata-10 13 0.05 0.05
coffers1 M.NOCB 10 0.04 0.02
cabinet1 M.NOCB 15 0.07 0.01
random (avg) 20 0.09 0.07
random (avg) 40 0.28 0.17
random (avg) 60 1.39 0.40
random (avg) 100 6.17 1.97
Table 3: Some runtimes for d = 2 (in seconds).
(www.algorithmic-solutions.com) for
the data structures and the graph algorithms and
on SCIL 0.8 (www.mpi-sb.mpg.de/SCIL)
for implementing the ILP-based branch-and-cut
algorithm. SCIL can be used with different
branch-and-cut core codes. We used CPLEX
9.0 (www.ilog.com) as commercial core and
SCIP 0.68 (www.zib.de/Optimization/
Software/SCIP/) based on SOPLEX 1.2.2a
(www.zib.de/Optimization/Software/
Soplex/) as the free implementation. Note that
all our implementations are still preliminary. The
software is publicly available (www.mpi-sb.
mpg.de/?althaus/PDOP.html).
We evaluate the implementations on three classes
of inputs. First, we use two discourses from the
GNOME corpus, taken from (Karamanis, 2003), to-
gether with the centering-based cost functions from
Section 2: coffers1, containing 10 discourse units,
and cabinet1, containing 15 discourse units. Sec-
ond, we use twelve discourses from the BLLIP
corpus taken from (Lapata, 2003), together with
M.LAPATA. These discourses are 4 to 13 discourse
units long; the table only shows the instance with
the highest running time. Finally, we generate ran-
dom instances of 2PDOP of size 20?100, and of
3PDOP of size 10, 15, and 20. A random instance is
the complete graph, where c(u1, . . . , ud) is chosen
uniformly at random from {0, . . . , 999}.
The results for the 2-place instances are shown
in Table 3, and the results for the 3-place instances
are shown in Table 4. The numbers are runtimes in
seconds on a Pentium 4 (Xeon) processor with 3.06
GHz. Note that a hypothetical baseline implementa-
tion which naively generates and evaluates all per-
mutations would run over 77 years for a discourse
of length 20, even on a highly optimistic platform
that evaluates one billion permutations per second.
For d = 2, all real-life instances and all random
instances of size up to 50 can be solved in less than
one second, with either implementation. The prob-
lem becomes more challenging for d = 3. Here the
algorithm quickly establishes good LP bounds for
Instance Size ILP-FS ILP-CS
coffers1 M.KP 10 0.05 0.05
coffers1 M.BFP 10 0.08 0.06
cabinet1 M.KP 15 0.40 1.12
cabinet1 M.BFP 15 0.39 0.28
random (avg) 10 1.00 0.42
random (avg) 15 35.1 5.79
random (avg) 20 - 115.8
Table 4: Some runtimes for d = 3 (in seconds).
the real-life instances, and thus the branch-and-cut
trees remain small. The LP bounds for the random
instances are worse, in particular when the number
of units gets larger. In this case, the further opti-
misations in the commercial software make a big
difference in the size of the branch-and-cut tree and
thus in the solution time.
An example output for cabinet1 with M.NOCB
is shown in Fig. 3; we have modified referring ex-
pressions to make the text more readable, and have
marked discourse unit boundaries with ?/? and ex-
pressions that establish local coherence with square
brackets. This is one of many possible optimal so-
lutions, which have cost 2 because of the two NOCB
transitions at the very start of the discourse. Details
on the comparison of different centering-based co-
herence measures are discussed by Karamanis et al
(2004).
6 Comparison to Other Approaches
There are two approaches in the literature that are
similar enough to ours that a closer comparison is
in order.
The first is a family of algorithms for discourse
ordering based on genetic programming (Mellish et
al., 1998; Karamanis and Manurung, 2002). This is
a very flexible and powerful approach, which can be
applied to measures of local coherence that do not
seem to fit in our framework trivially. For exam-
ple, the measure from (Mellish et al, 1998) looks at
the entire discourse up to the current transition for
some of their cost factors. However, our algorithm
is several orders of magnitude faster where a direct
comparison is possible (Manurung, p.c.), and it is
guaranteed to find an optimal ordering. The non-
approximability result for TSP means that a genetic
(or any other) algorithm which is restricted to poly-
nomial runtime could theoretically deliver arbitrar-
ily bad solutions.
Second, the discourse ordering problem we have
discussed in this paper looks very similar to the Ma-
jority Ordering problem that arises in the context
of multi-document summarisation (Barzilay et al,
Both cabinets probably entered England in the early nineteenth century / after the French Revolution caused
the dispersal of so many French collections. / The pair to [this monumental cabinet] still exists in Scotland.
/ The fleurs-de-lis on the top two drawers indicate that [the cabinet] was made for the French King Louis
XIV. / [It] may have served as a royal gift, / as [it] does not appear in inventories of [his] possessions. /
Another medallion inside shows [him] a few years later. / The bronze medallion above [the central door]
was cast from a medal struck in 1661 which shows [the king] at the age of twenty-one. / A panel of marquetry
showing the cockerel of [France] standing triumphant over both the eagle of the Holy Roman Empire and the
lion of Spain and the Spanish Netherlands decorates [the central door]. / In [the Dutch Wars] of 1672 - 1678,
[France] fought simultaneously against the Dutch, Spanish, and Imperial armies, defeating them all. / [The
cabinet] celebrates the Treaty of Nijmegen, which concluded [the war]. / The Sun King?s portrait appears
twice on [this work]. / Two large figures from Greek mythology, Hercules and Hippolyta, Queen of the
Amazons, representatives of strength and bravery in war appear to support [the cabinet]. / The decoration on
[the cabinet] refers to [Louis XIV?s] military victories. / On the drawer above the door, gilt-bronze military
trophies flank a medallion portrait of [the king].
Figure 3: An example output based on M.NOCB.
2002). The difference between the two problems is
that Barzilay et al minimise the sum of all costs
Cij for any pair i, j of discourse units with i < j,
whereas we only sum over the Cij for i = j ? 1.
This makes their problem amenable to the approxi-
mation algorithm by Cohen et al (1999), which al-
lows them to compute a solution that is at least half
as good as the optimum, in polynomial time; i.e.
this problem is strictly easier than TSP or discourse
ordering. However, a Majority Ordering algorithm
is not guaranteed to compute good solutions to the
discourse ordering problem, as Lapata (2003) as-
sumes.
7 Conclusion
We have shown that the problem of ordering clauses
into a discourse that maximises local coherence is
equivalent to the travelling salesman problem: Even
the two-place discourse ordering problem can en-
code ATSP. This means that the problem is NP-
complete and doesn?t even admit polynomial ap-
proximation algorithms (unless P=NP).
On the other hand, we have shown how to encode
the discourse ordering problems of arbitrary arity
d into GATSP. We have demonstrated that mod-
ern branch-and-cut algorithms for GATSP can eas-
ily solve practical discourse ordering problems if
d = 2, and are still usable for many instances with
d = 3. As far as we are aware, this is the first al-
gorithm for discourse ordering that can make any
guarantees about the solution it computes.
Our efficient implementation can benefit genera-
tion and summarisation research in at least two re-
spects. First, we show that computing locally co-
herent orderings of clauses is feasible in practice,
as such coherence measures will probably be ap-
plied on sentences within the same paragraph, i.e.
on problem instances of limited size. Second, our
system should be a useful experimentation tool in
developing new measures of local coherence.
We have focused on local coherence in this paper,
but it seems clear that notions of global coherence,
which go beyond the level of sentence-to-sentence
transitions, capture important aspects of coherence
that a purely local model cannot. However, our al-
gorithm can still be useful as a subroutine in a more
complex system that deals with global coherence
(Marcu, 1997; Mellish et al, 1998). Whether our
methods can be directly applied to the tree struc-
tures that come up in theories of global coherence is
an interesting question for future research.
Acknowledgments. We would like to thank
Mirella Lapata for providing the experimental data
and Andrea Lodi for providing an efficiency base-
line by running his ATSP solver on our inputs. We
are grateful to Malte Gabsdil, Ruli Manurung, Chris
Mellish, Kristina Striegnitz, and our reviewers for
helpful comments and discussions.
References
R. Barzilay, N. Elhadad, and K. R. McKeown.
2002. Inferring strategies for sentence ordering
in multidocument news summarization. Journal
of Artificial Intelligence Research, 17:35?55.
S. Brennan, M. Walker Friedman, and C. Pollard.
1987. A centering approach to pronouns. In
Proc. 25th ACL, pages 155?162, Stanford.
W. Cohen, R. Schapire, and Y. Singer. 1999. Learn-
ing to order things. Journal of Artificial Intelli-
gence Research, 10:243?270.
T. H. Cormen, C. E. Leiserson, and R. L. Rivest.
1990. Introduction to Algorithms. MIT Press,
Cambridge.
M. Fischetti, A. Lodi, and P. Toth. 2001. Solv-
ing real-world ATSP instances by branch-and-
cut. Combinatorial Optimization.
M. Fischetti, A. Lodi, and P. Toth. 2002. Exact
methods for the asymmmetric traveling salesman
problem. In G. Gutin and A. Punnen, editors, The
Traveling Salesman Problem and its Variations.
Kluwer.
N. Karamanis and H. M. Manurung. 2002.
Stochastic text structuring using the principle of
continuity. In Proceedings of INLG-02, pages
81?88, New York.
N. Karamanis, M. Poesio, C. Mellish, and J. Ober-
lander. 2004. Evaluating centering-based met-
rics of coherence for text structuring using a re-
liably annotated corpus. In Proceedings of the
42nd ACL, Barcelona.
N. Karamanis. 2003. Entity Coherence for De-
scriptive Text Structuring. Ph.D. thesis, Division
of Informatics, University of Edinburgh.
R. Kibble and R. Power. 2000. An integrated
framework for text planning and pronominalisa-
tion. In Proc. INLG 2000, pages 77?84, Mitzpe
Ramon.
M. Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proc. 41st
ACL, pages 545?552, Sapporo, Japan.
G. Laporte, H. Mercure, and Y. Nobert. 1987. Gen-
eralized travelling salesman problem through n
sets of nodes: the asymmetrical case. Discrete
Applied Mathematics, 18:185?197.
W. Mann and S. Thompson. 1988. Rhetorical struc-
ture theory: A theory of text organization. Text,
8(3):243?281.
D. Marcu. 1997. From local to global coherence:
A bottom-up approach to text planning. In Pro-
ceedings of the 14th AAAI, pages 629?635.
C. Mellish, A. Knott, J. Oberlander, and
M. O?Donnell. 1998. Experiments using
stochastic search for text planning. In Proc. 9th
INLG, pages 98?107, Niagara-on-the-Lake.
G.L. Nemhauser and L.A. Wolsey. 1988. Integer
and Combinatorial Optimization. John Wiley &
Sons.
C.E. Noon and J.C. Bean. 1993. An efficient trans-
formation of the generalized traveling salesman
problem. Information Systems and Operational
Research, 31(1).
M. Strube and U. Hahn. 1999. Functional center-
ing: Grounding referential coherence in informa-
tion structure. Computational Linguistics, 25(3).
M. Walker, A. Joshi, and E. Prince. 1998. Center-
ing in naturally occuring discourse: An overview.
In M. Walker, A. Joshi, and E. Prince, edi-
tors, Centering Theory in Discourse, pages 1?30.
Clarendon Press, Oxford.
B. Webber, A. Knott, M. Stone, and A. Joshi. 1999.
What are little trees made of: A structural and
presuppositional account using Lexicalized TAG.
In Proc. 36th ACL, pages 151?156, College Park.
Evaluating Centering for Information
Ordering Using Corpora
Nikiforos Karamanis?
University of Cambridge
Chris Mellish??
University of Aberdeen
Massimo Poesio?
University of Essex
Jon Oberlander?
University of Edinburgh
In this article we discuss several metrics of coherence defined using centering theory and
investigate the usefulness of such metrics for information ordering in automatic text generation.
We estimate empirically which is the most promising metric and how useful this metric is using
a general methodology applied on several corpora. Our main result is that the simplest metric
(which relies exclusively on NOCB transitions) sets a robust baseline that cannot be outperformed
by other metrics which make use of additional centering-based features. This baseline can be used
for the development of both text-to-text and concept-to-text generation systems.
1. Introduction
Information ordering (Barzilay and Lee 2004), that is, deciding in which sequence to
present a set of preselected information-bearing items, has received much attention in
recent work in automatic text generation. This is because text generation systems need
to organize the content in a way that makes the output text coherent, that is, easy to read
and understand. The easiest way to exemplify coherence is by arbitrarily reordering the
sentences of a comprehensible text. This process very often gives rise to documents that
do not make sense although the information content is the same before and after the
reordering (Hovy 1988; Marcu 1997; Reiter and Dale 2000).
Entity coherence, which is based on the way the referents of noun phrases (NPs)
relate subsequent clauses in the text, is an important aspect of textual organization.
Since the early 1980s, when it was first introduced, centering theory has been an
influential framework for modelling entity coherence. Seminal papers on centering such
as Brennan, Friedman [Walker], and Pollard (1987, page 160) and Grosz, Joshi, and
Weinstein (1995, page 215) suggest that centering may provide solutions for information
ordering.
Indeed, following the pioneering work of McKeown (1985), recent work on text
generation exploits constraints on entity coherence to organize information (Mellish
et al 1998; Kibble and Power 2000, 2004; O?Donnell et al 2001; Cheng 2002; Lapata
? Computer Laboratory, William Gates Building, Cambridge CB3 0FD, UK.
Nikiforos.Karamanis@cl.cam.ac.uk.
?? Department of Computing Science, King?s College, Aberdeen AB24 3UE, UK.
? Department of Computer Science, Wivenhoe Park, Colchester CO4 3SQ, UK.
? School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK.
Submission received: 15 May 2006; revised submission received: 15 December 2007; accepted for publication:
7 January 2008.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 1
2003; Barzilay and Lee 2004; Barzilay and Lapata 2005, among others). Although these
approaches often make use of heuristics related to centering, the features of entity
coherence they employ are usually defined informally. Additionally, centering-related
features are combined with other coherence-inducing factors in ways that are based
mainly on intuition, leaving many equally plausible options unexplored.
Thus, the answers to the following questions remain unclear: (i) How appropriate
is centering for information ordering in text generation? (ii) Which aspects of centering are
most useful for this purpose? These are the issues we investigate in this paper, which
presents the first systematic evaluation of centering for information ordering. To do this,
we define centering-based metrics of coherence which are compatible with several extant
information ordering approaches. An important insight of our work is that centering
can give rise to many such metrics of coherence. Hence, a general methodology
for identifying which of these metrics represent the most promising candidates for
information ordering is required.
We adopt a corpus-based approach to compare the metrics empirically and
demonstrate the portability and generality of our evaluation methods by experimenting
with several corpora. Our main result is that the simplest metric (which relies
exclusively on NOCB transitions) sets a baseline that cannot be outperformed by
other metrics that make use of additional centering-related features. Thus, we provide
substantial insight into the role of centering as an information ordering constraint and
offer researchers working on text generation a simple, yet robust, baseline to use against
their own information ordering approaches during system development.
The article is structured as follows: In Section 2 we discuss our information ordering
approach in relation to other work on text generation. After a brief introduction
to centering in Section 3, Section 4 demonstrates how we derived centering data
structures from existing corpora. Section 5 discusses how centering can be used to
define various metrics of coherence suitable for information ordering. Then, Section 6
outlines a corpus-based methodology for choosing among these metrics. Section 7
reports on the results of our experiments and Section 8 discusses their implications.
We conclude the paper with directions for future work and a summary of our main
contributions.1
2. Information Ordering
Information ordering has been investigated by substantial recent work in text-to-
text generation (Barzilay, Elhadad, and McKeown 2002; Lapata 2003; Barzilay and
Lee 2004; Barzilay and Lapata 2005; Bollegala, Okazaki, and Ishizuka 2006; Ji and
Pulman 2006; Siddharthan 2006; Soricut and Marcu 2006; Madnani et al 2007,
among others) as well as concept-to-text generation (particularly Kan and McKeown
[2002] and Dimitromanolaki and Androutsopoulos 2003).2 We added to this work
by presenting approaches to information ordering based on a genetic algorithm
(Karamanis and Manurung 2002) and linear programming (Althaus, Karamanis, and
Koller 2004) which can be applied to both concept-to-text and text-to-text generation.
These approaches use a metric of coherence defined using features derived from
1 Earlier versions of this work were presented in Karamanis et al (2004) and Karamanis (2006).
2 Concept-to-text generation is concerned with the automatic generation of text from some underlying
non-linguistic representation. By contrast, the input to text-to-text generation applications is text.
30
Karamanis et al Centering for Information Ordering
centering and will serve as the premises of our investigation of centering in this
article.
Metrics of coherence are used in other work on text generation, too (Mellish et al
1998; Kibble and Power 2000, 2004; Cheng 2002). With the exception of Kibble and
Power?s work, the features of entity coherence used in these metrics are informally
defined using heuristics related to centering. Additionally, the metrics are further
specified by combining these features with other coherence-inducing factors such
as rhetorical relations (Mann and Thompson 1987). However, as acknowledged in
most of this work, these are preliminary computational investigations of the complex
interactions between different types of coherence which leave many other equally
plausible combinations unexplored.
Clearly, one would like to know what centering can achieve on its own before
devising more complicated metrics. To address this question, we define metrics which
are purely centering-based, placing any attempt to specify a more elaborate model of
coherence beyond the scope of this article. This strategy is similar to most work on
centering for text interpretation in which additional constraints on coherence are not
taken into account (the papers in Walker, Joshi, and Prince [1998] are characteristic
examples). This simplification makes it possible to assess for the first time how useful
the employed centering features are for information ordering.
Work on text generation which is solely based on rhetorical relations (Hovy 1988;
Marcu 1997, among others) typically masks entity coherence under the ELABORATION
relation. However, ELABORATION has been characterized as ?the weakest of all
rhetorical relations? (Scott and de Souza 1990, page 60). Knott et al (2001) identified
several theoretical problems all related to ELABORATION and suggested that this relation
be replaced by a theory of entity coherence for text generation. Our work builds on this
suggestion by investigating how appropriate centering is as a theory of entity coherence
for information ordering.
McKeown (1985, pages 60?75) also deployed features of entity coherence to
organize information for text generation. McKeown?s ?constraints on immediate focus?
(which are based on the model of entity coherence that was introduced by Sidner
[1979] and precedes centering) are embedded within the schema-driven approach to
generation which is rather domain-specific (Reiter and Dale 2000). By contrast, our
metrics are general and portable across domains and can be applied within information
ordering approaches which are applicable to both concept-to-text and text-to-text
generation.
3. Centering Overview
This section provides an overview of centering, focusing on the aspects which are most
closely related to our work. Poesio et al (2004) and Walker, Joshi, and Prince (1998)
discuss centering and its relation to other theories of coherence in more detail.
According to Grosz, Joshi, and Weinstein (1995), each utterance Un is assigned a
ranked list of forward looking centers (i.e., discourse entities) denoted as CF(Un). The
members of CF(Un) must be realized by the NPs in Un (Brennan, Friedman [Walker],
and Pollard 1987). The first member of CF(Un) is called the preferred center
CP(Un).
The backward looking center CB(Un) links Un to the previous utterance Un?1.
CB(Un) is defined as the most highly ranked member of CF(Un?1) which also belongs
to CF(Un). CF lists prior to CF(Un?1) are not taken into account for the computation
31
Computational Linguistics Volume 35, Number 1
Table 1
Centering transitions are defined according to whether the backward looking center, CB, is
the same in two subsequent utterances, Un?1 and Un, and whether the CB of the current
utterance, CB(Un), is the same as its preferred center, CP(Un). These identity checks are also
known as the principles of COHERENCE and SALIENCE, the violations of which are denoted
with an asterisk.
COHERENCE: COHERENCE?:
CB(Un)=CB(Un?1) CB(Un) =CB(Un?1)
or CB(Un?1) undef.
SALIENCE: CB(Un)=CP(Un) CONTINUE SMOOTH-SHIFT
SALIENCE?: CB(Un) =CP(Un) RETAIN ROUGH-SHIFT
of CB(Un). The original formulations of centering by Brennan, Friedman [Walker], and
Pollard (1987) and Grosz, Joshi, and Weinstein (1995) lay emphasis on the uniqueness
and the locality of the CB and will serve as the foundations of our work.
The CB and the CP are combined to define transitions across pairs of
adjacent utterances (Table 1). This definition of transitions is based on Brennan,
Friedman [Walker], and Pollard (1987) and has been popular with subsequent work.
There exist several variations, however, the most important of which comes from Grosz,
Joshi, and Weinstein (1995), who define only one SHIFT transition.3
Centering makes two major claims about textual coherence, the first of which
is known as Rule 2. Rule 2 states that CONTINUE is preferred to RETAIN, which
is preferred to SMOOTH-SHIFT, which is preferred to ROUGH-SHIFT. Although the
Rule was introduced within an algorithm for anaphora resolution, Brennan, Friedman
[Walker], and Pollard (1987, page 160) consider it to be relevant to text generation
too. Grosz, Joshi, and Weinstein (1995, page 215) also take Rule 2 to suggest that
text generation systems should attempt to avoid unfavorable transitions such as
SHIFTs.
The second claim, which is implied by the definition of the CB (Poesio et al 2004),
is that CF(Un) should contain at least one member of CF(Un?1). This became known
as the principle of CONTINUITY (Karamanis and Manurung 2002). Although Grosz,
Joshi, and Weinstein and Brennan, Friedman [Walker], and Pollard do not discuss
the effect of violating CONTINUITY, Kibble and Power (2000, Figure 1) define the
additional transition NOCB to account for this case. Different types of NOCB transitions
are introduced by Passoneau (1998) and Poesio et al (2004), among others. Other
researchers, however, consider the NOCB transition to be a type of ROUGH-SHIFT
(Miltsakaki and Kukich 2004).
Kibble (2001) and Beaver (2004) introduced the principles of COHERENCE and
SALIENCE, which correspond to the identity checks used to define the transitions
(see Table 1). To improve the way centering resolves pronominal anaphora, Strube
and Hahn (1999) introduced a fourth principle called CHEAPNESS and defined it as
CB(Un)=CP(Un?1). They also redefined Rule 2 to favor transition pairs which satisfy
3 ?CB(Un?1) undef.? in Table 1 stands for the cases where Un?1 does not have a CB. Instead of classifying
the transition of Un as a CONTINUE or a RETAIN in such cases, the additional transition ESTABLISHMENT
is sometimes used (Kameyama 1998; Poesio et al 2004).
32
Karamanis et al Centering for Information Ordering
CHEAPNESS over those which violate it. This means that CHEAPNESS is given priority
over every other centering principle in Strube and Hahn?s model.
In addition to the variability caused by the numerous definitions of transitions and
the introduction of the various principles, parameters such as ?utterance,? ?ranking,?
and ?realization? can also be specified in several ways giving rise to different
instantiations of centering (Poesio et al 2004). The following section discusses how these
parameters were defined in the corpora we deploy.
4. Experimental Data
We made use of the data of Dimitromanolaki and Androutsopoulos (2003), the GNOME
corpus (Poesio et al 2004), and the two corpora that Barzilay and Lapata (2005)
experimented with. In this section, we discuss how the centering representations we
utilize were derived from each corpus.
4.1 The MPIRO-CF Corpus
Dimitromanolaki and Androutsopoulos (2003, henceforth D&A) derived facts from the
database of the MPIRO concept-to-text generation system (Isard et al 2003), realized
them as sentences, and organized them in sets. Each set consisted of six facts which
were ordered by a domain expert. The orderings produced by this expert were shown
to be very close to those produced by two other archeologists (Karamanis and Mellish
2005b).
Our first corpus, MPIRO-CF, consists of 122 orderings that were made available
to us by D&A. We computed a CF list for each fact in each ordering by applying the
instantiation of centering introduced by Kibble and Power (2000, 2004) for concept-to-
text generation. That is, we took each database fact to correspond to an ?utterance?
and specified the ?realization? parameter using the arguments of each fact as the
members of the corresponding CF list. Table 2 shows the CF lists, the CBs, the
centering transitions, and the violations of CHEAPNESS for the following example from
MPIRO-CF:
(1) (a) This exhibit is an amphora.
(b) This exhibit was decorated by the Painter of Kleofrades.
(c) The Painter of Kleofrades used to decorate big vases.
(d) This exhibit depicts a warrior performing splachnoscopy before leaving for the
battle.
(e) This exhibit is currently displayed in the Martin von Wagner Museum.
(f) The Martin von Wagner Museum is in Germany.
MPIRO facts consist of two arguments, the first of which was specified as the CP
following the definition of ?CF ranking? in O?Donnell et al (2001).4 Notice that the
second argument can often be an entity such as en914 that is realized by a canned phrase
of significant syntactic complexity (a warrior performing splachnoscopy before leaving for
the battle). Moreover, the deployed definition of ?realization? is similar to what Grosz,
4 This is the main difference between our approach and that of Kibble and Power, who allow for more than
one potential CP in their CF lists.
33
Computational Linguistics Volume 35, Number 1
Table 2
The CF list, the CB, NOCB, or centering transition (see Table 1) and violations of CHEAPNESS
(denoted with an asterisk) for each fact in Example (1) from the MPIRO-CF corpus.
Fact CF list: next referent} CB Transition CHEAPNESS
{CP, CBn=CPn?1
(1a) {ex1, amphora} n.a. n.a. n.a.
(1b) {ex1, paint-of-kleofr} ex1 CONTINUE ?
(1c) {paint-of-kleofr, en404} paint-of-kleofr SMOOTH-SHIFT ?
(1d) {ex1, en914} ? NOCB n.a.
(1e) {ex1, wagner-mus} ex1 CONTINUE ?
(1f) {wagner-mus, germany} wagner-mus SMOOTH-SHIFT ?
Joshi, and Weinstein (1995) call ?direct realization,? which ignores potential bridging
relations (Clark 1977) between the members of two subsequent CF lists. These relations
are typically not taken into account for information ordering and were not considered
in any of the deployed corpora.
4.2 The GNOME-LAB Corpus
We also made use of the GNOME corpus (Poesio et al 2004), which contains object
descriptions (museum labels) reliably annotated with features relevant to centering.
The motivation for this study was to examine whether the phenomena observed in
MPIRO-CF (which is arguably somewhat artificial) also manifest in texts from the
same genre written by humans without the constraints imposed by a text generation
system.
Based on the definition of museum labels in Cheng (2002, page 65), we identified
20 such texts in GNOME, which were published in a book and a museum Web site (and
were thus taken to be coherent). The following example is a characteristic text from this
subcorpus (referred to here as GNOME-LAB):
(2) (a) Item 144 is a torc.
(b) Its present arrangement, twisted into three rings, may be a modern alteration;
(c) it should probably be a single ring, worn around the neck.
(d) The terminals are in the form of goats? heads.
The GNOME corpus provides us with reliable annotation of discourse units (i.e.,
clauses and sentences) that can be used for the computation of ?utterance? and of
NPs which introduce entities to the CF list. Each feature was marked up by at
least two annotators and agreement was checked using the ? statistic on part of the
corpus.
In order to avoid deviating too much from the MPIRO application domain, we
computed the CF lists from the units that seemed to correspond more closely to MPIRO
facts. So instead of using sentence for the definition of ?utterance,? we followed most
work on centering for English and computed CF lists from GNOME?s finite units.5 The
5 This definition includes titles which do not always have finite verbs, but excludes finite relative clauses,
the second element of coordinated VPs and clause complements which are often taken as not having their
own CF lists in the centering literature.
34
Karamanis et al Centering for Information Ordering
Table 3
First two members of the CF list, the CB, NOCB, or centering transition (see Table 1) and
violations of CHEAPNESS (denoted with an asterisk) for each finite unit in Example (2) from the
GNOME-LAB corpus.
Unit CF list: next referent} CB Transition CHEAPNESS
{CP, CBn=CPn?1
(2a) {de374, de375} n.a. n.a. n.a.
(2b) {de376, de374, ... } de374 RETAIN ?
(2c) {de374, de379, ... } de374 CONTINUE ?
(2d) {de380, de381, ... } ? NOCB n.a.
text spans with the indexes (a) to (d) in Example (2) are examples of such units. Units
such as (2a) are as simple as the MPIRO-generated sentence (1a), whereas others appear
to be of similar syntactic complexity to (1d). On the other hand, the second sentence in
Example (2) consists of two finite units, namely (b) and (c), and appears to correspond
to higher degrees of aggregation than is typically seen in an MPIRO fact. The texts in
GNOME-LAB consist of 8.35 finite units on average.
Table 3 shows the first two members of the CF list, the CB, the transitions, and the
violations of CHEAPNESS for Example (2). Note that the same entity (i.e., de374) is used
to denote the referent of the NP Item 144 in (2a) and its in (2b), which is annotated as
coreferring with Item 144. All annotated NPs introduce referents to the CF list (which
often contains more entities than in MPIRO), but only direct realization is used for the
computation of the list. This means that, similarly to the MPIRO domain, bridging
relations between, for example, it in (2c) and the terminals in (2d), are not taken into
account.
The members of the CF list were ranked by combining grammatical function with
linear order, which is a robust way of estimating ?CF ranking? in English (Poesio et al
2004). In this instantiation, the CP corresponds to the referent of the first NP within the
unit that is annotated as a subject or as the post-copular NP in a there-clause.
4.3 The NEWS and ACCS Corpora
Barzilay and Lapata (2005) presented a probabilistic approach for information ordering
which is particularly suitable for text-to-text generation and is based on a new
representation called the entity grid. A collection of 200 articles from the North American
News Corpus (NEWS) and 200 narratives of accidents from the National Transportation
Safety Board database (ACCS) was used for training and evaluation. Example (3)
presents a characteristic text from the NEWS corpus:
(3) (a) [The Justice Department]S is conducting [an anti-trust trial]O against [Microsoft
Corp.]X with [evidence]X that [the company]S is increasingly attempting to crush
[competitors]O.
(b) [Microsoft]O is accused of trying to forcefully buy into [markets]X where [its
own products]S are not competitive enough to unseat [established brands]O.
(c) [The case]S revolves around [evidence]O of [Microsoft]S aggressively pressuring
[Netscape]O into merging [browser software]O.
(d) [Microsoft]S claims [its tactics]S are commonplace and good economically.
35
Computational Linguistics Volume 35, Number 1
Table 4
Fragment of the entity grid for Example (3). The grammatical function of the referents in each
sentence is reported using S, O, and X (for subject, object, and other). The symbol ??? is used for
referents which do not occur in the sentence.
Referents
Sentences department trial microsoft evidence ... products brands ...
(3a) S O S X ... ? ? ...
(3b) ? ? O ? ... S O ...
(3c) ? ? S O ... ? ? ...
(3d) ? ? S ? ... ? ? ...
(3e) ? ? ? ? ... ? ? ...
(3f) ? X S ? ... ? ? ...
(e) [The government]S may file [a civil suit]O ruling that [conspiracy]S to curb
[competition]O through [collusion]X is [a violation]O of [the Sherman Act]X.
(f) [Microsoft]S continues to show [increased earnings]O despite [the trial]X.
Barzilay and Lapata automatically annotated their corpora for the grammatical function
of the NPs in each sentence (denoted in the example by the subscripts S, O, and
X for subject, object, and other, respectively) as well as their coreferential relations
(which do not include bridging references). More specifically, they used a parser
(Collins 1997) to determine the constituent structure of the sentences from which the
grammatical function for each NP was derived.6 Coreferential NPs such as Microsoft
Corp. and the company in (3a) were identified using the system of Ng and Cardie
(2002).
The entity grid is a two-dimensional array that captures the distribution of NP
referents across sentences in the text using the aforementioned symbols for their
grammatical role and the symbol ??? for a referent that does not occur in a sentence.
Table 4 illustrates a fragment of the grid for the sentences in Example (3).7
Barzilay and Lapata use the grid to compute models of coherence that are
considerably more elaborate than centering. To derive an appropriate instantiation of
centering for our investigation, we compute a CF list for each grid row using the
referents with the symbols S, O, and X. These referents are ranked according to their
grammatical function and their position in the text. This definition of ?CF ranking? is
similar to the one we use in GNOME-LAB. For instance, department is ranked higher
than microsoft in CF(3a) because the Justice Department is mentioned before Microsoft
Corp. in the text. The derived sequence of CF lists is used to compute the additional
centering data structures shown in Table 5.
The average number of sentences per text is 10.4 in NEWS and 11.5 in ACCS.
As we explain in the next section, our centering-based metrics of coherence can be
6 They also used a small set of patterns to recognize passive verbs and annotate arguments involved in
passive constructions with their underlying grammatical function. This is why Microsoft is marked with
the role O in sentence (3b).
7 If a referent such as microsoft is attested by several NPs in the same sentence, for example, Microsoft
Corp. and the company in (3a), the role with the highest priority (in this case S) is used to represent it.
36
Karamanis et al Centering for Information Ordering
Table 5
First two members of the CF list, the CB, NOCB, or centering transitions (see Table 1) and
violations of CHEAPNESS (denoted with an asterisk) for Example (3) from the NEWS corpus.
Sentence CF list: next referent} CB Transition CHEAPNESS
{CP, CBn=CPn?1
(3a) {department, microsoft, ...} n.a. n.a. n.a.
(3b) {products, microsoft, ...} microsoft RETAIN ?
(3c) {microsoft, case, ...} microsoft CONTINUE ?
(3d) {microsoft, tactics} microsoft CONTINUE ?
(3e) {government, conspiracy, ...} ? NOCB n.a.
(3f) {microsoft, earnings, ... } ? NOCB n.a.
deployed directly on unseen texts, so we treated all texts in NEWS and ACCS as test
data.8
5. Computing Centering-Based Metrics of Coherence
Following our previous work (Karamanis and Manurung 2002; Althaus, Karamanis,
and Koller 2004), the input to information ordering is an unordered set of information-
bearing items represented as CF lists. A set of candidate orderings is produced by
creating different permutations of these lists. A metric of coherence uses features from
centering to compute a score for each candidate ordering and select the highest scoring
ordering as the output.9
A wide range of metrics of coherence can be defined in centering?s terms, simply
on the basis of the work we reviewed in Section 3. To exemplify this, let us first assume
that the ordering in Example (3), which is analyzed as a sequence of CF lists in Table 5,
is a candidate ordering. Table 6 summarizes the NOCBs, the violations of COHERENCE,
SALIENCE, and CHEAPNESS, and the centering transitions for this ordering.10
The candidate ordering contains two NOCBs in sentences (3e) and (3f). Its score
according to M.NOCB, the metric used by Karamanis and Manurung (2002) and
Althaus, Karamanis, and Koller (2004), is 2. Another ordering with fewer NOCBs (should
such an ordering exist) will be preferred over this candidate as the selected output of
information ordering if M.NOCB is used to guide this process. M.NOCB relies only on
CONTINUITY. Because satisfying this principle is a prerequisite for the computation of
every other centering feature, M.NOCB is the simplest possible centering-based metric
and will be used as the baseline in our experiments.
According to Strube and Hahn (1999) the principle of CHEAPNESS is the most
important centering feature for anaphora resolution. We are interested in assessing how
suitable M.CHEAP, a metric which utilizes CHEAPNESS, is for information ordering.
CHEAPNESS is violated twice according to Table 6 so the score of the candidate ordering
8 By contrast, Barzilay and Lapata used 100 texts in each domain to train their models and reserved the
other 100 for testing them.
9 If the best coherence score is assigned to several candidate orderings, then the information ordering
algorithm will choose randomly between them.
10 Principles and transitions will be collectively referred to as ?features? from now on.
37
Computational Linguistics Volume 35, Number 1
Table 6
Violations of CONTINUITY (NOCB), COHERENCE, SALIENCE, and CHEAPNESS and centering
transitions for Example (3), based on the analysis in Table 5. The table reports the sentences
marked with each centering feature: That is, sentences (3e) and (3f) are classified as NOCBs, and
so on.
CONTINUITY? COHERENCE? SALIENCE? CHEAPNESS?
NOCB: CBn = CBn?1: CBn = CPn: CBn = CPn?1:
(3e), (3f) ? (3b) (3b), (3c)
CONTINUE: RETAIN: SMOOTH-SHIFT: ROUGH-SHIFT:
(3c), (3d) (3b) ? ?
according to M.CHEAP is 2.11 If another candidate ordering with fewer violations of
CHEAPNESS exists, it will be chosen as a preferred output according to M.CHEAP.
M.BFP employs the transition preferences of Rule 2 as specified by Brennan,
Friedman [Walker], and Pollard (1987). The first score to be computed by M.BFP is
the sum of CONTINUE transitions, which is 2 for the candidate ordering according to
Table 6. If this ordering is found to score higher than every other candidate ordering for
the number of CONTINUEs, it is selected as the output. If another ordering is found to
have the same number of CONTINUEs, the sum of RETAINs is examined, and so forth for
the other two types of centering transitions.12
M.KP, the metric deployed by Kibble and Power (2000) in their text generation
system, sums up the NOCBs as well as the violations of CHEAPNESS, COHERENCE,
and SALIENCE, preferring the ordering with the lowest total cost. In addition to
the violations of CONTINUITY and CHEAPNESS, the candidate ordering also violates
SALIENCE once, so its score according to M.KP is 5. An alternative ordering with a
lower score (if any) will be preferred by this metric. Although Kibble and Power (2004)
introduced a weighted version of M.KP, the exact weighting of centering?s principles
remains an open question, as argued by Kibble (2001). This is why we decided to
experiment with M.KP instead of its weighted variant.
In the remainder of the paper, we take forward the four metrics motivated in this
section as the most appropriate starting point for experimentation. We would like to
emphasize, however, that these are not the only possible options. Indeed, similarly to
the various ways in which centering?s parameters can be specified, there exist many
other ways of using centering to define metrics of entity coherence for information
ordering. These possibilities arise from the numerous other definitions of centering?s
transitions and the various ways in which transitions and principles can be combined.
These are explored in more detail in Karamanis (2003, Chapter 3), which also provides
a formal definition of the metrics discussed previously.
6. Evaluation Methodology
Because using naturally occurring discourse in psycholinguistic studies to investigate
coherence effects is almost infeasible, computational corpus-based experiments are
11 In order to estimate the effect of CHEAPNESS only, NOCBs are not counted as violations of CHEAPNESS.
12 Following Brennan, Friedman [Walker], and Pollard (1987), NOCBs are not taken into account for the
definition of transitions in M.BFP.
38
Karamanis et al Centering for Information Ordering
often the most viable alternative (Poesio et al 2004; Barzilay and Lee 2004). Corpus-
based evaluation can be usefully employed during system development and may
be later supplemented by less extended evaluation based on human judgments as
suggested by Lapata (2006).
The corpus-based methodology of Karamanis (2003) served as our experimental
framework. This methodology is based on the premise that the original sentence order
(OSO, Barzilay and Lee 2004) observed in a corpus text is more coherent than any other
ordering. If a metric takes an alternative ordering to be more coherent than the OSO, it
has to be penalized.
Karamanis (2003) introduced a performance measure called the classification error
rate which is computed according to the formula: Better(M,OSO)+Equal(M,OSO)/2.
Better(M,OSO) stands for the percentage of orderings that score better than the OSO
according to a metric M, and Equal(M,OSO) is the percentage of orderings that score
equal to the OSO.13 This measure provides an indication of how likely a metric is to lead
to an ordering different from the OSO. When comparing several metrics with each other,
the one with the lowest classification error rate is the most appropriate for ordering
the sentences that the OSO consists of. In other words, the smaller the classification
error rate, the better a metric is expected to perform for information ordering. The
average classification error rate is used to summarize the performance of each metric in
a corpus.
To compute the classification error rate we permute the CF lists of the OSO and
classify each alternative ordering as scoring better, equal, or worse than the OSO
according to M. When the number of CF lists in the OSO is fairly small, it is feasible
to search through all possible orderings. For OSOs consisting of more than 10 CF
lists, the classification error rate for the entire population of orderings can be reliably
estimated using a random sample of one million permutations (Karamanis 2003,
Chapter 5).
7. Results
Table 7 shows the average performance of each metric in the corpora employed in our
experiments. The smallest?that is, best?score in each corpus is printed in boldface.
The table indicates that the baseline M.NOCB performs best in three out of four corpora.
The experimental results of the pairwise comparisons of M.NOCB with each of
M.CHEAP, M.KP, and M.BFP in each corpus are reported in Table 8. The exact number
of texts for which the classification error rate of M.NOCB is lower than its competitor for
each comparison is reported in the columns headed by ?lower.? For instance, M.NOCB
has a lower classification error rate than M.CHEAP for 110 (out of 122) texts from
MPIRO-CF. M.CHEAP achieves a lower classification error rate for just 12 texts, and
there do not exist any ties, that is, cases in which the classification error rate of the two
metrics is the same.
The p value returned by the two-tailed Sign Test for the difference in the number
of texts in each corpus, rounded to the third decimal place, is also reported.14 With
13 Weighting Equal(M,OSO) by 0.5 is based on the assumption that, similarly to tossing a coin, the OSO will
on average do better than half of the orderings that score the same as it does when other coherence
constraints are considered.
14 The Sign Test was chosen over its parametric alternatives to test significance because it does not carry
specific assumptions about population distributions and variance.
39
Computational Linguistics Volume 35, Number 1
Table 7
Average classification error rate for the centering-based metrics in each corpus.
Corpus
Metric MPIRO-CF GNOME-LAB NEWS ACCS Mean
M.NOCB 20.42 19.95 30.90 15.51 21.70
M.BFP 19.91 33.01 37.90 21.20 28.01
M.KP 53.15 58.22 57.70 55.60 56.12
M.CHEAP 81.04 57.23 64.60 76.29 69.79
No. of texts 122 20 200 200
Table 8
Comparing M.NOCB with M.CHEAP, M.KP, and M.BFP in each corpus.
MPIRO-CF GNOME-LAB
M.NOCB M.NOCB
lower greater ties p lower greater ties p
M.CHEAP 110 12 0 <0.001 18 2 0 <0.001
M.KP 103 16 3 <0.001 16 2 2 0.002
M.BFP 42 31 49 0.242 12 3 5 0.036
No. of texts 122 20
NEWS ACCS
M.NOCB M.NOCB
lower greater ties p lower greater ties p
M.CHEAP 155 44 1 <0.001 183 17 0 <0.001
M.KP 131 68 1 <0.001 167 33 0 <0.001
M.BFP 121 71 8 <0.001 100 100 0 1.000
No. of texts 200 200
respect to the exemplified comparison of M.NOCB against M.CHEAP in MPIRO-CF,
the p value is lower than 0.001 after rounding. This in turn means that M.NOCB
returns a better classification error rate for significantly more texts in MPIRO-CF
than M.CHEAP. In other words, M.NOCB outperforms M.CHEAP significantly in this
corpus.
Notably, M.NOCB performs significantly better than its competitor in 10 out of
12 cases.15 In the remaining two comparisons, the difference in performance between
M.NOCB and M.BFP is not significant (p > 0.05). However, this does not constitute
evidence against M.NOCB, the simplest of the investigated metrics. In fact, because
M.BFP fails to outperform the baseline, the latter may be considered as the most
promising solution for information ordering in these cases too by applying Occam?s
razor. Thus, M.NOCB is shown to be the best performing metric across all four
corpora.
15 This result is significant too according to the two-tailed Sign Test (p < 0.05).
40
Karamanis et al Centering for Information Ordering
8. Discussion
Our experiments show that M.NOCB is the most suitable metric for information
ordering among the metrics we experimented with. Despite the differences between our
corpora (in genre, average length, syntactic complexity, number of referents in the CF
list, etc.), M.NOCB proves robust across all four of them. It is also the most appropriate
metric to use in both application areas we relate our corpora to, namely concept-to-text
(MPIRO-CF and GNOME-LAB) as well as text-to-text (NEWS and ACCS) generation.
These results indicate that when purely centering-based metrics are used, simply
avoiding NOCBs is more relevant to information ordering than the combinations of
additional centering features that the other metrics make use of.
In this section, we compare our work with other recent evaluation studies,
including the corpus-based investigation of centering by Poesio et al (2004); discuss
the implications of our findings for text generation; and summarize our contributions.
8.1 Recent Evaluation Studies in Information Ordering
There has been significant recent work on the corpus-based evaluation for information
ordering. In this section, we discuss the methodological differences between our work
and the studies which are most closely related to it.
Barzilay and Lee (2004) introduce a stochastic model for information ordering
which computes the probability of generating the OSO and every alternative ordering.
Then, all orderings are ranked according to this probability and the rank given to the
OSO is retrieved. Several evaluation measures are discussed, the most important of
which is the average OSO rank, that is, the average rank of the OSOs in their corpora.
This measure does not take into account that the OSOs differ in length. However, this
information is necessary to estimate reliably the performance of an information ordering
approach, as we discuss in Karamanis and Mellish (2005a) in more detail.
Barzilay and Lapata (2005) overcome this difficulty by introducing a performance
measure called ranking accuracywhich expresses the percentage of alternative orderings
that are ranked lower than the OSO. In Karamanis?s (2003) terms, ranking accuracy
equals 100% ? Better(M,OSO), assuming that no equally ranking orderings exist.16
Barzilay and Lapata (2005) compare the OSO with just 20 alternative orderings,
often sampled out of several millions. On the other hand, Barzilay and Lee (2004)
enumerate exhaustively each possible ordering, which might become impractical as the
search space grows factorially. We overcame these problems by using a large random
sample for the texts which consist of more than 10 sentences as suggested in Karamanis
(2003, Chapter 5). Equally important is the emphasis we placed on the use of statistical
tests, which were not deployed by either Barzilay and Lee or Barzilay and Lapata.
Lapata (2003) presented a methodology for automatically evaluating generated
orderings on the basis of their distance from observed sentence orderings in a corpus.
A measure of rank correlation (called Kendall?s ?), which was subsequently shown to
correlate reliably with human ratings and reading times (Lapata 2006), was used to
estimate the distance between orderings.
16 Neither Barzilay and Lapata (2005) nor Barzilay and Lee (2004) appear to consider the possibility that two
orderings may be equally ranked.
41
Computational Linguistics Volume 35, Number 1
Whereas ? estimates how close the predictions of a metric are to several original
orderings, we measure how likely a metric is to lead to an ordering different than the
OSO. Taking into account more than one OSO for information ordering is the main
strength of Lapata?s method, but to do this one needs to ask several humans to order the
same set of sentences (Madnani et al 2007). Karamanis and Mellish (2005b) conducted
an experiment in the MPIRO domain using Lapata?s methodology which supplements
the work reported in this article. However, such an approach is less practical for much
larger collections of texts such as NEWS and ACCS. This is presumably the reason why
Barzilay and Lapata (2005) use ranking accuracy instead of ? in their evaluation.
8.2 Previous Corpus-Based Evaluations of Centering
Our work investigates how the coherence score of the OSO compares to the scores
of alternative orderings of the sentences that the OSO consists of. As Kibble (2001,
page 582) noticed, this question is crucial from an information ordering viewpoint, but
was not taken into account by any previous corpus-based study of centering. Grosz,
Joshi, and Weinstein (1995, page 215) also suggested that Rule 2 should be tested by
examining ?alternative multi-utterance sequences that differentially realize the same
content.? We are the first to have pursued this research objective in the evaluation of
centering for information ordering.
Poesio et al (2004) observed that there remained a large number of NOCBs under
every instantiation of centering they tested and concluded that centering is inadequate
as a coherence model.17 However, the frequency of NOCBs does not necessarily provide
adequate indication of how appropriate NOCBs (and centering in general) are for
information ordering. Although over 50% of the transitions in GNOME-LAB are NOCBs,
the average classification error rate of approximately 20% for M.NOCB suggests that the
OSO tends to be in greater agreement with the preference to avoid NOCBs than 80% of
the alternative orderings. Thus, it appears that the observed ordering in the corpus does
optimize with respect to the number of potential NOCBs to a great extent.
8.3 A Simple and Robust Baseline for Text Generation
How likely is M.NOCB to come up with the attested ordering in the corpus (the OSO)
if it is actually used to guide an algorithm that orders the CF lists in our corpora?
The average classification error rates (Table 7) estimate exactly this variable. The
performance of M.NOCB varies across the corpora from about 15.5% (ACCS) to 30.9%
(NEWS). We attribute this variation to the aforesaid differences between the corpora.
Notice, however, that these differences affect all metrics in a similar way, not allowing
for another metric to significantly outperform M.NOCB.
Noticeably, even in ACCS, for which M.NOCB achieves its best performance,
approximately one out of six alternative orderings on average are taken to be more
coherent than the OSO. Given the average number of sentences per text in this corpus
17 We viewed the definition of the centering instantiation as being related to the application domain, as we
explained in Section 4. This is why, unlike Poesio et al, we did not experiment with different
instantiations of centering on the same data.
42
Karamanis et al Centering for Information Ordering
(11.5), this means that several millions of alternative orderings are often taken to be
more coherent than the gold standard.
Barzilay and Lapata (2005) report an average ranking accuracy of 87.3% for their
best sentence ordering method in ACCS. This corresponds to an average classification
error rate of 12.7% (assuming that there are no equally scoring orderings in their
evaluation; see Section 8.1). This is equal to an improvement of just 2.8% over
the performance of our baseline metric (15.5%) using a coherence model which is
substantially more elaborate than centering. However, it is in NEWS (for which
M.NOCB returns its worst performance of 30.9%) that this model shows its real strength,
approximating an average classification error rate of 9.6%, which corresponds to an
improvement of 21.3% over our baseline. We believe that the experiments reported in
this article put the studies of our colleagues in better perspective by providing a reliable
baseline to compare their metrics against.
8.4 Moving Beyond Centering-Based Metrics
Following McKeown (1985), Kibble and Power argue in favor of an integrated approach
for concept-to-text generation in which the same centering features are used at different
stages in the generation pipeline. However, our study suggests that features such as
CHEAPNESS and the centering transitions are not particularly relevant to information
ordering. The poor performance of these features can be explained by the fact that they
were originally introduced to account for pronoun resolution rather than information
ordering. CONTINUITY, on the other hand, captures a fundamental intuition about entity
coherence which constitutes part of several other discourse theories.18
CONTINUITY, however, captures just one aspect of coherence. This explains the
relatively high classification error rates for M.NOCB, which needs to be supplemented
with other coherence-inducing factors in order to be used in practice. This verifies the
premises of researchers such as Kibble and Power who a priori use features derived
from centering in combination with other factors in the definition of their metrics. Our
work should be quite helpful for that effort too, suggesting that M.NOCB is a better
starting point for defining such metrics than M.CHEAP or M.KP.
9. Conclusion
In conclusion, our analysis sheds more light on two previously unaddressed questions
in the corpus-based evaluation of centering: (i) which aspects of centering are most
relevant to information ordering and (ii) to what extent centering on its own can be
useful for this purpose. We have shown that the metric which relies exclusively on NOCB
transitions (M.NOCB) sets a baseline that cannot be outperformed by other coherence
metrics which make use of additional centering features. Although this metric does not
perform well enough to be used on its own, it constitutes a simple, yet robust, baseline
against which more elaborate information ordering approaches can be tested during
system development in both text-to-text and concept-to-text generation.
This work can be extended in numerous ways. For instance, given the abundance
of possible centering-based metrics one may investigate whether a different metric can
18 We thank one anonymous reviewer for suggesting this explanation of our results.
43
Computational Linguistics Volume 35, Number 1
outperform M.NOCB in any corpus or application domain. M.NOCB can also serve as
the starting point for the definition of more informed metrics which will incorporate
additional coherence-inducing factors. Finally, given that we used the instantiation
of centering which seemed to correspond more closely to the targeted application
domains, the extent to which computing the CF list in a different way may affect the
performance of the metrics is another question to explore in future work.
Acknowledgments
Many thanks to Aggeliki Dimitromanolaki,
Mirella Lapata, and Regina Barzilay for their
data; to David Schlangen, Ruli Manurung,
James Soutter, and Le An Ha for
programming solutions; and to Ruth Seal
and two anonymous reviewers for their
comments. Nikiforos Karamanis received
support from the Greek State Scholarships
Foundation (IKY) as a PhD student in
Edinburgh as well as the Rapid Item
Generation project and the BBSRC-funded
FlySlip grant (No 38688) as a postdoc in
Wolverhampton and Cambridge,
respectively.
References
Althaus, Ernst, Nikiforos Karamanis, and
Alexander Koller. 2004. Computing locally
coherent discourses. In Proceedings of ACL
2004, pages 399?406, Barcelona.
Barzilay, Regina, Noemie Elhadad, and
Kathleen McKeown. 2002. Inferring
strategies for sentence ordering in
multidocument news summarization.
Journal of Artificial Intelligence Research,
17:35?55.
Barzilay, Regina and Mirella Lapata. 2005.
Modeling local coherence: An entity-based
approach. In Proceedings of ACL 2005,
pages 141?148, Ann Arbor, MI.
Barzilay, Regina and Lillian Lee. 2004.
Catching the drift: Probabilistic content
models with applications to generation
and summarization. In Proceedings of
HLT-NAACL 2004, pages 113?120,
Boston, MA.
Beaver, David. 2004. The optimization of
discourse anaphora. Linguistics and
Philosophy, 27(1):3?56.
Bollegala, Danushka, Naoaki Okazaki, and
Mitsuru Ishizuka. 2006. A bottom-up
approach to sentence ordering for
multi-document summarization. In
Proceedings of ACL-COLING 2006,
pages 385?392, Sydney.
Brennan, Susan E., Marilyn A.
Friedman [Walker], and Carl J. Pollard.
1987. A centering approach to pronouns.
In Proceedings of ACL 1987, pages 155?162,
Stanford, CA.
Cheng, Hua. 2002. Modelling Aggregation
Motivated Interactions in Descriptive Text
Generation. Ph.D. thesis, Division of
Informatics, University of Edinburgh.
Clark, Herbert. H. 1977. Bridging. In P. N.
Johnson-Laird and P. C. Wason, editors,
Thinking: Readings in Cognitive Science.
Cambridge University Press, Cambridge,
pages 9?27.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of ACL-EACL 1997,
pages 16?23, Madrid.
Dimitromanolaki, Aggeliki and Ion
Androutsopoulos. 2003. Learning to order
facts for discourse planning in natural
language generation. In Proceedings of
ENLG 2003, pages 23?30, Budapest.
Grosz, Barbara J., Aravind K. Joshi, and Scott
Weinstein. 1995. Centering: A framework
for modeling the local coherence of
discourse. Computational Linguistics,
21(2):203?225.
Hovy, Eduard. 1988. Planning coherent
multisentential text. In Proceedings of ACL
1988, pages 163?169, Buffalo, NY.
Isard, Amy, Jon Oberlander, Ion
Androutsopoulos, and Colin Matheson.
2003. Speaking the users? languages. IEEE
Intelligent Systems Magazine, 18(1):40?45.
Ji, Paul and Stephen Pulman. 2006. Sentence
ordering with manifold-based
classification in multi-document
summarization. In Proceedings of EMNLP
2006, pages 526?533, Sydney.
Kameyama, Megumi. 1998. Intrasentential
centering: A case study. In Walker, Joshi,
and Prince 1998, pages 89?122.
Kan, Min-Yen and Kathleen McKeown. 2002.
Corpus-trained text generation for
summarization. In Proceedings of INLG
2002, pages 1?8, Harriman, NY.
Karamanis, N. 2006. Evaluating centering for
information ordering in two new domains.
In Proceedings of NAACL 2006, Companion
Volume, pages 65?68, New York.
Karamanis, N., M. Poesio, C. Mellish, and
J. Oberlander. 2004. Evaluating
centering-based metrics of coherence using
44
Karamanis et al Centering for Information Ordering
a reliably annotated corpus. In Proceedings
of ACL 2004, pages 391?398, Barcelona.
Karamanis, Nikiforos. 2003. Entity Coherence
for Descriptive Text Structuring. Ph.D.
thesis, Division of Informatics, University
of Edinburgh.
Karamanis, Nikiforos and Hisar Maruli
Manurung. 2002. Stochastic text
structuring using the principle of
continuity. In Proceedings of INLG 2002,
pages 81?88, Harriman, NY.
Karamanis, Nikiforos and Chris Mellish.
2005a. A review of recent corpus-based
methods for evaluating information
ordering in text production. In Proceedings
of Corpus Linguistics 2005 Workshop on
Using Corpora for NLG, pages 13?18,
Birmingham.
Karamanis, Nikiforos and Chris Mellish.
2005b. Using a corpus of sentence
orderings defined by many experts to
evaluate metrics of coherence for text
structuring. In Proceedings of ENLG 2005,
pages 174?179, Aberdeen.
Kibble, Rodger. 2001. A reformulation of rule
2 of centering theory. Computational
Linguistics, 27(4):579?587.
Kibble, Rodger and Richard Power. 2000.
An integrated framework for text
planning and pronominalisation. In
Proceedings of INLG 2000, pages 77?84,
Mitzpe Ramon.
Kibble, Rodger and Richard Power. 2004.
Optimizing referential coherence in text
generation. Computational Linguistics,
30(4):401?416.
Knott, Alistair, Jon Oberlander, Mick
O?Donnell, and Chris Mellish. 2001.
Beyond elaboration: The interaction of
relations and focus in coherent text. In
T. Sanders, J. Schilperoord, and
W. Spooren, editors, Text Representation:
Linguistic and Psycholinguistic Aspects.
John Benjamins, Amsterdam, chapter 7,
pages 181?196.
Lapata, Mirella. 2003. Probabilistic text
structuring: Experiments with sentence
ordering. In Proceedings of ACL 2003,
pages 545?552, Sapporo.
Lapata, Mirella. 2006. Automatic evaluation
of information ordering: Kendall?s tau.
Computational Linguistics, 32(4):1?14.
Madnani, Nitin, Rebecca Passonneau,
Necip Fazil Ayan, John Conroy, Bonnie
Dorr, Judith Klavans, Dianne O?Leary, and
Judith Schlesinger. 2007. Measuring
variability in sentence ordering for news
summarization. In Proceedings of ENLG
2007, pages 81?88, Schloss Dagstuhl.
Mann, William C. and Sandra A. Thompson.
1987. Rhetorical structure theory: A theory
of text organisation. Technical Report
RR-87-190, University of Southern
California / Information Sciences Institute.
Marcu, Daniel. 1997. The Rhetorical Parsing,
Summarization and Generation of Natural
Language Texts. Ph.D. thesis, University of
Toronto.
McKeown, Kathleen. 1985. Text Generation:
Using Discourse Strategies and Focus
Constraints to Generate Natural Language
Text. Studies in Natural Language
Processing. Cambridge University Press,
Cambridge.
Mellish, Chris, Alistair Knott, Jon
Oberlander, and Mick O?Donnell. 1998.
Experiments using stochastic search for
text planning. In Proceedings of INLG 1998,
pages 98?107, Niagara-on-the-Lake.
Miltsakaki, Eleni and Karen Kukich. 2004.
Evaluation of text coherence for electronic
essay scoring systems. Natural Language
Engineering, 10(1):25?55.
Ng, Vincent and Claire Cardie. 2002.
Improving machine learning approaches
to coreference resolution. In Proceedings of
ACL 2002, pages 104?111, Philadelphia,
PA.
O?Donnell, Mick, Chris Mellish, Jon
Oberlander, and Alistair Knott. 2001. ILEX:
An architecture for a dynamic hypertext
generation system. Natural Language
Engineering, 7(3):225?250.
Passoneau, Rebecca J. 1998. Interaction
of discourse structure with explicitness
of discourse anaphoric phrases. In
Walker, Joshi, and Prince 1998,
pages 327?358.
Poesio, Massimo, Rosemary Stevenson,
Barbara Di Eugenio, and Janet Hitzeman.
2004. Centering: a parametric theory and
its instantiations. Technical Report
CSM-369, Department of Computer
Science, University of Essex. Extended
version of the paper that appeared in
Computational Linguistics 30(3):309?363,
2004.
Reiter, Ehud and Robert Dale. 2000.
Building Natural Language Generation
Systems. Cambridge University Press,
Cambridge.
Scott, Donia and Clarisse Sieckenius
de Souza. 1990. Getting the message across
in RST-based text generation. In Robert
Dale, Chris Mellish, and Michael Zock,
editors, Current Research in Natural
Language Generation. Academic Press, San
Diego, CA, pages 47?74.
45
Computational Linguistics Volume 35, Number 1
Siddharthan, Advaith. 2006. Syntactic
simplification and text cohesion.
Research on Language and Computation,
4(1):77?109.
Sidner, Candace L. 1979. Towards a
Computational Theory of Definite Anaphora
Comprehension in English. Ph.D. thesis, AI
Laboratory/MIT, Cambridge, MA. Also
available as Technical Report No.
AI-TR-537.
Soricut, Radu and Daniel Marcu. 2006.
Discourse generation using utility-trained
coherence models. In Proceedings of
ACL-COLING 2006 Poster Session,
pages 803?810, Sydney.
Strube, Michael and Udo Hahn. 1999.
Functional centering: Grounding
referential coherence in information
structure. Computational Linguistics,
25(3):309?344.
Walker, Marilyn A., Aravind K. Joshi, and
Ellen F. Prince, editors. 1998. Centering
Theory in Discourse. Clarendon Press,
Oxford.
46
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 65?68,
New York, June 2006. c?2006 Association for Computational Linguistics
Evaluating Centering for Sentence Ordering in Two New Domains
Nikiforos Karamanis
Natural Language and Information Processing Group
Computer Laboratory
University of Cambridge
Nikiforos.Karamanis@cl.cam.ac.uk
Abstract
This paper builds on recent research investigating
sentence ordering in text production by evaluating
the Centering-based metrics of coherence
employed by Karamanis et al (2004) using the
data of Barzilay and Lapata (2005). This is the first
time that Centering is evaluated empirically as a
sentence ordering constraint in several domains,
verifying the results reported in Karamanis et al
1 Introduction
As most literature in text linguistics argues, a
felicitous text should be coherent which means
that the content has to be organised in a way
that makes the text easy to read and comprehend.
The easiest way to demonstrate this claim is
by arbitrarily reordering the sentences that an
understandable text consists of. This process very
often gives rise to documents that do not make sense
although the information content remains the same.
Hence, deciding in which sequence to present a
set of preselected information-bearing items is an
important problem in automatic text production.
Entity coherence, which arises from the way
NP referents relate subsequent sentences in the
text, is an important aspect of textual felicity.
Centering Theory (Grosz et al, 1995) has been
an influential framework for modelling entity
coherence in computational linguistics in the last
two decades. Karamanis et al (2004) were the first
to evaluate Centering-based metrics of coherence
for ordering clauses in a subset of the GNOME
corpus (Poesio et al, 2004) consisting of 20 artefact
descriptions. They introduced a novel experimental
methodology that treats the observed ordering of
clauses in a text as the gold standard, which is
scored by each metric. Then, the metric is penalised
proportionally to the amount of alternative orderings
of the same material that score equally to or better
than the gold standard.
This methodology is very similar to the way
Barzilay and Lapata (2005) evaluate automatically
another model of coherence called the entity grid
using a larger collection of 200 articles from the
North American News Corpus (NEWS) and 200
accident narratives from the National Transportation
Safety Board database (ACCS). The same data and
similar methods were used by Barzilay and Lee
(2004) to compare their probabilistic approach for
ordering sentences with that of Lapata (2003).
This paper discusses how the Centering-based
metrics of coherence employed by Karamanis et al
can be evaluated on the data prepared by Barzilay
and Lapata. This is the first time that Centering
is evaluated empirically as a sentence ordering
constraint in more than one domain, verifying the
results reported in Karamanis et al
The paper also contributes by emphasising the
following methodological point: To conduct our
experiments, we need to produce several alternative
orderings of sentences and compare them with the
gold standard. As the number of possible orderings
grows factorially, enumerating them exhaustively
(as Barzilay and Lee do) becomes impractical.
In this paper, we make use of the methods of
Karamanis (2003) which allow us to explore a
65
Table 1A NP referents
Sentences department trial microsoft ... products brands ...
(a) S O S ... ? ? ...
(b) ? ? O ... S O ...
Table 1B CF list: CHEAPNESS
Sentences {CP, next two referents} CB Transition CBn=CPn?1(a) {department, microsoft, trial, ...} n.a. n.a. n.a.
(b) {products, microsoft, brands, ...} microsoft RETAIN ?
Table 1: (A) Fragment of the entity grid for example (1); (B) CP (i.e. first member of the CF list), next two
referents, CB, transition and violations of CHEAPNESS (denoted with a ?) for the same example.
sufficient number of alternative orderings and return
more reliable results than Barzilay and Lapata,
who used a sample of just 20 randomly produced
orderings (often out of several millions).
2 Materials and methods
2.1 Centering data structures
Example (1) presents the first two sentences of a text
in NEWS (Barzilay and Lapata, Table 2):
(1) (a) [The Justice Department]S is conducting [an anti-trust trial]O against [Microsoft Corp.]X with [evidence]Xthat [the company]S is increasingly attempting to crush[competitors]O . (b) [Microsoft]O is accused of trying toforcefully buy into [markets]X where [its own products]Sare not competitive enough to unseat [established
brands]O . (...)
Barzilay and Lapata automatically annotated their
corpora for the grammatical role of the NPs in
each sentence (denoted in the example by the
subscripts S, O and X for subject, object and
other respectively)1 as well as their coreferential
relations. This information is used as the basis
for the computation of the entity grid: a two-
dimensional array that captures the distribution of
NP referents across sentences in the text using the
aforementioned symbols for their grammatical role
and ??? for a referent that does not occur in a
sentence. Table 1A illustrates a fragment of the grid
for the sentences in example (1).2
Our data transformation script computes the basic
structure of Centering (known as CF list) for each
row of the grid using the referents with the symbols
1Subjects in passive constructions such as ?Microsoft?
in (1b) are marked with O.
2If a referent such as microsoft is attested by several
NPs, e.g. ?Microsoft Corp.? and ?the company? in (1a), the
role with the highest priority (in this case S) is used.
S, O and X (Table 1B). The members of the CF
list are ranked according to their grammatical role
(Brennan et al, 1987) and their position in the grid.3
The derived sequence of CF lists can then be used to
compute other important Centering concepts:
? The CB, i.e. the referent that links the current CF list with
the previous one such as microsoft in (b).
? Transitions (Brennan et al, 1987) and NOCBs, that is,
cases in which two subsequent CF lists do not have any
referent in common.
? Violations of CHEAPNESS (Strube and Hahn, 1999),
COHERENCE and SALIENCE (Kibble and Power, 2000).
2.2 Metrics of coherence
Karamanis (2003) assumes a system which receives
an unordered set of CF lists as its input and uses a
metric to output the highest scoring ordering. He
discusses how Centering can be used to define many
different metrics of coherence which might be useful
for this task. In our experiments we made use of the
four metrics employed in Karamanis et al (2004):
? The baseline metric M.NOCB which simply prefers the
ordering with the fewest NOCBs.
? M.CHEAP which selects the ordering with the fewest
violations of CHEAPNESS.
? M.KP, introduced by Kibble and Power, which sums
up the NOCBs as well as the violations of CHEAPNESS,
COHERENCE and SALIENCE, preferring the ordering with
the lowest total cost.
? M.BFP which employs the transition preferences of
Brennan et al
3The referent department appears in an earlier grid
column than microsoft because ?the Justice Department?
is mentioned before ?Microsoft Corp.? in the text. Since
grid position corresponds to order of mention, the former
can be used to resolve ties between referents with the same
grammatical role in the CF list similarly to the use of the latter
e.g. by Strube and Hahn.
66
NEWS M.NOCB p
corpus lower greater ties
M.CHEAP 155 44 1 <0.000
M.KP 131 68 1 <0.000
M.BFP 121 71 8 <0.000
N of texts 200
Table 2: Comparing M.NOCB with M.CHEAP,
M.KP and M.BFP in the NEWS corpus.
2.3 Experimental methodology
As already mentioned, previous work assumes that
the gold standard ordering (GSO) observed in a text
is more coherent than any other ordering of the
sentences (or the corresponding CF lists) it consists
of. If a metric takes a randomly produced ordering
to be more coherent than the GSO, it has to be
penalised.
Karamanis et al (2004) introduce a measure
called the classication rate which estimates this
penalty as the weighted sum of the percentage
of alternative orderings that score equally to or
better than the GSO.4 When comparing several
metrics with each other, the one with the lowest
classification rate is the most appropriate for
sentence ordering.
Karamanis (2003) argues that computing the
classification rate using a random sample of one
million orderings provides reliable results for the
entire population of orderings. In our experiments,
we used a random sample of that size for GSOs
which consisted of more than 10 sentences. This
allows us to explore a sufficient portion of possible
orderings (without having to exhaustively enumerate
every ordering as Barzilay and Lee do). Arguably,
our experiments also return more reliable results
than those of Barzilay and Lapata who used a sample
of just a few randomly produced orderings.
Since the Centering-based metrics can be directly
deployed on unseen texts without any training, we
treated all texts in NEWS and ACCS as testing data.5
4The classification rate is computed according to the
formula Better(M,GSO) + Equal(M,GSO)/2. Better(M,GSO)
stands for the percentage of orderings that score better than
the GSO according to a metric M, whilst Equal(M,GSO) is the
percentage of orderings that score equal to the GSO.
5By contrast, Barzilay and Lapata used 100 texts in each
domain to train their probabilistic model and 100 to test it. Note
that although they experiment with quite large corpora their
reported results are not verified by statistical tests.
ACCS M.NOCB p
corpus lower greater ties
M.CHEAP 183 17 0 <0.000
M.KP 167 33 0 <0.000
M.BFP 100 100 0 1.000
N of texts 200
Table 3: Comparing M.NOCB with M.CHEAP,
M.KP and M.BFP in the ACCS corpus.
3 Results
The experimental results of the comparisons of the
metrics from section 2.2 are reported in Table 2
for the NEWS corpus and in Table 3 for ACCS.
Following Karamanis et al, the tables compare the
baseline metric M.NOCB with each of M.CHEAP,
M.KP and M.BFP. The exact number of GSOs
for which the classification rate of M.NOCB is
lower than its competitor for each comparison is
reported in the second column of the Table. For
example, M.NOCB has a lower classification rate
than M.CHEAP for 155 (out of 200) GSOs from
NEWS. M.CHEAP achieves a lower classification
rate for just 44 GSOs, while there is a single tie in
which the classification rate of the two metrics is
the same. The p value returned by the two-tailed
sign test for the difference in the number of GSOs,
rounded to the third decimal place, is reported in the
fifth column of Table 2.6
Overall, the Table shows that M.NOCB does
significantly better in NEWS than the other
three metrics which employ additional Centering
concepts. Similarly, M.CHEAP and M.KP are
overwhelmingly beaten by the baseline in ACCS.
Also note that since M.BFP fails to significantly
overtake M.NOCB in ACCS, the baseline can be
considered the most promising solution in that case
too by applying Occam?s razor.
Table 4 shows the results of the evaluation of the
metrics in GNOME from Karamanis et al These
results are strikingly similar to ours despite the much
smaller size of their sample. Hence, M.NOCB is
the most suitable among the investigated metrics for
ordering the CF lists in both NEWS and ACCS in
addition to GNOME.
6The sign test was chosen by Karamanis et al to test
significance because it does not carry specific assumptions
about population distributions and variance.
67
GNOME M.NOCB p
corpus lower greater ties
M.CHEAP 18 2 0 <0.000
M.KP 16 2 2 0.002
M.BFP 12 3 5 0.036
N of texts 20
Table 4: Comparing M.NOCB with M.CHEAP,
M.KP and M.BFP in the GNOME corpus.
4 Discussion
Our experiments have shown that the baseline
M.NOCB performs better than its competitors.
This in turn indicates that simply avoiding NOCB
transitions is more relevant to sentence ordering than
the additional Centering concepts employed by the
other metrics.
But how likely is M.NOCB to come up with the
GSO if it is actually used to guide an algorithm
which orders the CF lists in our corpora? The
average classication rate of M.NOCB is an
estimate of exactly this variable.
The average classification rate for M.NOCB
is 30.90% in NEWS and 15.51% in ACCS.
The previously reported value for GNOME is
19.95%.7 This means that on average M.NOCB
takes approximately 1 out of 3 alternative orderings
in NEWS and 1 out of 6 in ACCS to be more
coherent that the GSO. As already observed by
Karamanis et al, there results suggest that M.NOCB
cannot be put in practical use.
However, the fact that M.NOCB is shown to
overtake its Centering-based competitors across
several corpora means that it is a simple, yet robust,
baseline against which other similar metrics can be
tested. For instance, Barzilay and Lapata report a
ranking accuracy of around 90% for their best grid-
based sentence ordering method, which we take to
correspond to a classification rate of approximately
10% (assuming that there do not exist any equally
scoring alternative orderings). This amounts to an
improvement over M.NOCB of almost 5% in ACCS
and 20% in NEWS.
Given the deficiencies of the evaluation in
Barzilay and Lapata, this comparison can only be
7The variability is presumably due to the different
characteristics of each corpus (which do not prevent M.NOCB
from always beating its competitors).
provisional. In our future work, we intend to directly
evaluate their method using a substantially large
number of alternative orderings and M.NOCB as the
baseline. We will also try to supplement M.NOCB
with other features of coherence to improve its
performance.
Acknowledgments
Many thanks to Regina Barzilay and Mirella Lapata for their
data, to Le An Ha for the data transformation script and to Chris
Mellish, Massimo Poesio and three anonymous reviewers for
comments. Support from the Rapid Item Generation project
(Wolverhampton University) and the BBSRC-funded Flyslip
grant (No 16291) is also acknowledged.
References
Regina Barzilay and Mirella Lapata. 2005. Modeling local
coherence: An entity-based approach. In Proceedings ofACL 2005, pages 141?148.
Regina Barzilay and Lillian Lee. 2004. Catching the drift:
Probabilistic content models with applications to generation
and summarization. In Proceedings of HLT-NAACL 2004,
pages 113?120.
Susan E. Brennan, Marilyn A. Friedman [Walker], and Carl J.
Pollard. 1987. A centering approach to pronouns.
In Proceedings of ACL 1987, pages 155?162, Stanford,
California.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 1995.
Centering: A framework for modeling the local coherence of
discourse. Computational Linguistics, 21(2):203?225.
Nikiforos Karamanis, Massimo Poesio, Chris Mellish, and Jon
Oberlander. 2004. Evaluating centering-based metrics of
coherence using a reliably annotated corpus. In Proceedingsof ACL 2004, pages 391?398, Barcelona, Spain.
Nikiforos Karamanis. 2003. Entity Coherence for DescriptiveText Structuring. Ph.D. thesis, Division of Informatics,
University of Edinburgh.
Rodger Kibble and Richard Power. 2000. An integrated
framework for text planning and pronominalisation. InProceedings of INLG 2000, pages 77?84, Israel.
Mirella Lapata. 2003. Probabilistic text structuring:
Experiments with sentence ordering. In Proceedings of ACL2003, pages 545?552, Saporo, Japan, July.
Massimo Poesio, Rosemary Stevenson, Barbara Di Eugenio,
and Janet Hitzeman. 2004. Centering: a parametric
theory and its instantiations. Computational Linguistics,
30(3):309?363.
Michael Strube and Udo Hahn. 1999. Functional centering:
Grounding referential coherence in information structure.Computational Linguistics, 25(3):309?344.
68
Proceedings of the Fourth International Natural Language Generation Conference, pages 111?113,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Generating Multiple-Choice Test Items from Medical Text:
A Pilot Study
Nikiforos Karamanis
Computer Laboratory
University of Cambridge
CB3 0FD, UK
nk304@cam.ac.uk
Le An Ha and Ruslan Mitkov
Computational Linguistics Research Group
University of Wolverhampton
WV1 1SB, UK
{L.A.Ha, R.Mitkov}@wlv.ac.uk
Abstract
We report the results of a pilot study on generating
Multiple-Choice Test Items from medical text and
discuss the main tasks involved in this process and
how our system was evaluated by domain experts.
1 Introduction
AlthoughMultiple-Choice Test Items (MCTIs) are
used daily for assessment, authoring them is a
laborious task. This gave rise to a relatively new
research area within the emerging field of Text-
to-Text Generation (TTG) called Multiple-Choice
Test Item Generation (MCTIG).1
Mitkov et al (2006) developed a system
which detects the important concepts in a
text automatically and produces MCTIs testing
explicitly conveyed factual knowledge.2 This
differs from most related work in MCTIG such as
Brown et al (2005) and the papers in BEAUNLP-
II (2005) which deploy various NLP techniques to
produce MCTIs for vocabulary assessment, often
using preselected words as the input (see Mitkov
et al for more extensive comparisons).
The approach of Mitkov et al is semi-automatic
since the MCTIs have to be reviewed by domain
experts to assess their usability. They report that
semi-automatic MCTIG can be more than 3 times
quicker than authoring of MCTIs without the aid
of their system.
1TTG, in which surface text is used as the input to
algorithms for text production, contrasts with Concept-
to-Text Generation (better known as Natural Language
Generation) which is concerned with the automatic
production of text from some underlying non-linguistic
representation of information (Reiter and Dale, 2000).
2Mitkov et al used an online textbook on Linguistics as
their source text. Clearly, their approach is not concerned
with concepts or facts derived through inferencing. Neither
does it address the problem of compiling a balanced test from
the generated MCTIs.
Moreover, analysis of MCTIs produced semi-
automatically and used in the classroom reveals
that their educational value is not compromised in
exchange for time and labour savings. In fact, the
semi-automatically produced MCTIs turn out to
fare better than MCTIs produced without the aid
of the system in certain aspects of item quality.
This paper reports the results of a pilot study on
generating MCTIs from medical text which builds
on the work of Mitkov et al
2 Multiple-Choice Test Item Generation
A MCTI such as the one in example (1) typically
consists of a question or stem, the correct answer
or anchor (in our example, ?chronic hepatitis?)
and a list of distractors (options b to d):
(1) Which disease or syndrome may progress to cirrhosis
if it is left untreated?
a) chronic hepatitis
b) hepatic failure
c) hepatic encephalopathy
d) hypersplenism
The MCTI in (1) is based on the following clause
from the source text (called the source clause; see
section 2.3 below):
(2) Chronic hepatitis may progress to cirrhosis if it is left
untreated.
We aim to automatically generate (1) from (2)
using our simple Rapid Item Generation (RIG)
system that combines several components
available off-the-shelf. Based on Mitkov et al, we
saw MCTIG as consisting of at least the following
tasks: a) Parsing b) Key-Term Identification c)
Source Clause Selection d) Transformation to
Stem e) Distractor Selection. These are discussed
in the following sections.
111
2.1 Sentence Parsing
Sentence Parsing is crucial for MCTIG since the
other tasks rely greatly on this information. RIG
employs Charniak?s (1997) parser which appeared
to be quite robust in the medical domain.
2.2 Key-Term Identification
One of our main premises is that an appropriate
MCTI should have a key-term as its anchor
rather than irrelevant concepts. For instance, the
concepts ?chronic hepatitis? and ?cirrhosis? are
quite prominent in the source text that example (2)
comes from, which in turn means that MCTIs
containing these terms should be generated using
appropriate sentences from that text.
RIG uses the UMLS thesaurus3 as a domain
specific resource to compute an initial set of
potential key terms such as ?hepatitis? from the
source text. Similarly to Mitkov et al, the initial
set is enlarged with NPs featuring potential key
terms as their heads and satisfying certain regular
expressions. This step adds terms such as ?acute
hepatitis? (which was not included in the version
of UMLS utilised by our system) to the set.
The tf.idf method (that Mitkov et al did
not find particularly effective) is used to promote
the 30 most prominent potential key terms within
the source text for subsequent processing, ruling
out generic terms such as ?patient? or ?therapy?
which are very frequent within a larger collection
of medical texts (our reference corpus).
2.3 Source Clause Selection
Mitkov et al treat a clause in the source text
as eligible for MCTIG if it contains at least one
key term and is finite as well as of the SV(O)
structure. They acknowledge, however, that this
strategy gives rise to a lot of inappropriate source
clauses, which was the case in our domain too.
To address this problem, we implemented a
module which filters out inappropriate structures
for MCTIG (see Table 1 for examples). This
explains why the number of key terms and MCTIs
varies among texts (Table 2).
A finite main clause which contains an NP
headed by a key term and functioning as a
subject or object with all the subordinate clauses
which depend on it is a source clause eligible
for MCTIG provided that it satisfies our filters.
Example (2) is such an eligible source clause.
3http://www.nlm.nih.gov/research/umls/
Structure Example (key term in italics)
Subordinate clause Although asthma is a lung disease, ...
Negated clause Autoimmune hepatitis should not
be treated with interferon.
Coordinated NP Excessive salt intake causes
hypertension and hypokalemia.
Initial pronoun It associates with hypertension instead.
Table 1: Inappropriate structures for MCTIG.
Experimentation during development showed that
our module improves source clause selection by
around 30% compared to the baseline approach of
Mitkov et al
2.4 Transformation to Stem
Once an appropriate source clause is identified,
it has to be turned to the stem of a MCTI. This
involves getting rid of discourse cues such as
?however? and substituting the NP headed by the
key term such as ?chronic hepatitis? in (1) with a
wh-phrase such as ?which disease or syndrome?.
The wh-phrase is headed by the semantic type of
the key-term derived from UMLS.
RIG utilises a simple transformational
component which produces a stem via minimal
changes in the ordering of the source clause. The
filtering module discussed in the previous section
disregards the clauses in which the key term
functions as a modifier or adjunct. Additionally,
most of the key terms in the eligible source clauses
appear in subject position which in turn means
that wh-fronting and inversion is performed in just
a handful of cases. The following example, again
based on the source clause in (2), is one such case:
(3) To which disease or syndrome may chronic hepatitis
progress if it is left untreated?
2.5 Selection of Appropriate Distractors
MCTIs aim to test the ability of the student
to identify the correct answer among several
distractors. An appropriate distractor is a concept
semantically close to the anchor which, however,
cannot serve as the right answer itself.
RIG computes a set of potential distractors
for a key term using the terms with the same
semantic type in UMLS (rather than WordNet
coordinates employed by Mitkov et al). Then, we
apply a simple measure of distributional similarity
derived from our reference corpus to select the
best scoring distractors. This strategy means that
MCTIs with the same answer feature very similar
distractors.
112
# of # of Usable Usable Items w/out Replaced distractors Total Average Time
Chapter Words Key-terms Items Items post-edited stems per term Time per Item
Asthma 8,843 9 66 42 (64%) 18 (27%) 2.0 140 mins 3 mins 20 secs
Hepatitis 10,259 17 92 49 (53%) 19 (21%) 0.9 150 mins 3 mins 04 secs
Hypertension 12,941 22 121 59 (49%) 15 (12%) 0.8 200 mins 3 mins 23 secs
Total 32,043 40 279 150 (54%) 52 (19%) ? 490 mins 3 mins 16 secs
Table 2: Usability and efficiency of Multiple-Choice Test Item Generation from medical text.
3 Evaluation
RIG is a simple system which often avoids
tough problems such as dealing with key-terms in
syntactic positions that might puzzle the parser or
might be too difficult to question upon. So how
does it actually perform?
Three experts in producing MCTIs for medical
assessment jointly reviewed 279 MCTIs (each
featuring four distractors) generated by the
system. Three chapters from a medical textbook
served as the source texts while a much larger
collection of MEDLINE texts was used as the
reference corpus.
The domain experts regarded a MCTI as
unusable if it could not be used in a test or required
too much revision to do so. The remaining items
were considered to be usable and could be post-
edited by the experts to improve their content and
readability or replace inappropriate distractors.
As Table 2 shows, more than half of the items in
total were judged to be usable. Additionally, about
one fifth of the usable items did not require any
editing. The Table also shows the total number of
key-terms identified in each chapter as well as the
average number of distractors replaced per term.
The last column of Table 2 reports on the
efficiency of MCTIG in our domain. This variable
is calculated by dividing the total time it took
the experts to review all MCTIs by the amount
of usable items which represent the actual end-
product. This is a bit longer than 3 minutes
per usable item across all chapters. Anecdotal
evidence and the experts? own estimations suggest
that it normally takes them at least 10 minutes to
produce an MCTI manually.
Given the distinct domains in which our system
and the one of Mitkov et al were deployed (as
well as the differences between them), a direct
comparison between them could be misleading.
We note, however, that our usability scores are
always higher than their worst score (30%) and
quite close to their best score (57%). The amount
of directly usable items in Mitkov et al was
between just 3.5% and 5%, much lower than
what we achieved. They also report an almost
3-fold improvement in efficiency for computer-
aided MCTIG, which is very similar to our
estimate. These results indicate what our work has
contributed to the state of the art in MCTIG.
In our future work, we aim to address the
following issues: (a) As in Mitkov et al, the
anchor of a MCTI produced by RIG always
corresponds to a key-term. However, the domain
experts pointed out several cases in which it is
better for the key-term to stay in the stem and
for another less prominent concept to serve as the
answer. (b) Students who simply memorise the
input chapter might be able to answer the MCTI if
its surface form is too close to the source clause so
another interesting suggestion was to paraphrase
the stem during MCTIG. (c) We also intend to
introduce greater variability in our process for
distractor selection by investigating several other
measures of semantic similarity.
Acknowledgments
We are grateful to Tony LaDuca, Manfred Straehle and
Robert Galbraith from the National Board of Medical
Examiners (NBME) for their expert-based feedback and to
three anonymous reviewers for their comments.
References
BEAUNLP-II. 2005. Papers on MCTIG by Hoshino and
Nakagawa, Liu et al, and Sumita et al In Proceedings of
the 2nd Workshop on Building Educational Applications
Using NLP.
Jonathan Brown, Gwen Frishkoff, and Maxine Eskenazi.
2005. Automatic question generation for vocabulary
assessment. In Proceedings of HLT-EMNLP 2005, pages
249?254.
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proceedings of AAAI
1997, pages 598?603.
Ruslan Mitkov, Le An Ha, and Nikiforos Karamanis. 2006.
A computer-aided environment for generating multiple-
choice test items. Natural Language Engineering,
12(2):177?194.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge University
Press.
113
Using a Corpus of Sentence Orderings Defined by Many Experts
to Evaluate Metrics of Coherence for Text Structuring
Nikiforos Karamanis
Computational Linguistics Research Group
University of Wolverhampton, UK
N.Karamanis@wlv.ac.uk
Chris Mellish
Department of Computing Science
University of Aberdeen, UK
cmellish@csd.abdn.ac.uk
Abstract
This paper addresses two previously unresolved is-
sues in the automatic evaluation of Text Structuring
(TS) in Natural Language Generation (NLG). First,
we describe how to verify the generality of an exist-
ing collection of sentence orderings defined by one
domain expert using data provided by additional
experts. Second, a general evaluation methodol-
ogy is outlined which investigates the previously
unaddressed possibility that there may exist many
optimal solutions for TS in the employed domain.
This methodology is implemented in a set of ex-
periments which identify the most promising can-
didate for TS among several metrics of coherence
previously suggested in the literature.1
1 Introduction
Research in NLG focused on problems related to TS from
very early on, [McKeown, 1985] being a classic example.
Nowadays, TS continues to be an extremely fruitful field of
diverse active research. In this paper, we assume the so-
called search-based approach to TS [Karamanis et al, 2004]
which employs a metric of coherence to select a text struc-
ture among various alternatives. The TS module is hypothe-
sised to simply order a preselected set of information-bearing
items such as sentences [Barzilay et al, 2002; Lapata, 2003;
Barzilay and Lee, 2004] or database facts [Dimitromanolaki
and Androutsopoulos, 2003; Karamanis et al, 2004].
Empirical work on the evaluation of TS has become in-
creasingly automatic and corpus-based. As pointed out by
[Karamanis, 2003; Barzilay and Lee, 2004] inter alia, using
corpora for automatic evaluation is motivated by the fact that
employing human informants in extended psycholinguistic
experiments is often simply unfeasible. By contrast, large-
scale automatic corpus-based experimentation takes place
much more easily.
[Lapata, 2003] was the first to present an experimental set-
ting which employs the distance between two orderings to es-
timate automatically how close a sentence ordering produced
1Chapter 9 of [Karamanis, 2003] reports the study in more detail.
by her probabilistic TS model stands in comparison to order-
ings provided by several human judges.
[Dimitromanolaki and Androutsopoulos, 2003] derived
sets of facts from the database of MPIRO, an NLG system
that generates short descriptions of museum artefacts [Isard
et al, 2003]. Each set consists of 6 facts each of which cor-
responds to a sentence as shown in Figure 1. The facts in
each set were manually assigned an order to reflect what a
domain expert, i.e. an archaeologist trained in museum la-
belling, considered to be the most natural ordering of the
corresponding sentences. Patterns of ordering facts were au-
tomatically learned from the corpus created by the expert.
Then, a classification-based TS approach was implemented
and evaluated in comparison to the expert?s orderings.
Database fact Sentence
subclass(ex1, amph) ? This exhibit is an amphora.
painted-by(ex1, p-Kleo) ? This exhibit was decorated by
the Painter of Kleofrades.
painter-story(p-Kleo, en4049) ? The Painter of Kleofrades
used to decorate big vases.
exhibit-depicts(ex1, en914) ? This exhibit depicts a warrior performing
splachnoscopy before leaving for the battle.
current-location(ex1, wag-mus) ? This exhibit is currently displayed
in the Martin von Wagner Museum.
museum-country(wag-mus, ger) ? The Martin von Wagner Museum
is in Germany.
Figure 1: MPIRO database facts corresponding to sentences
A subset of the corpus created by the expert in the previous
study (to whom we will henceforth refer as E0) is employed
by [Karamanis et al, 2004] who attempt to distinguish be-
tween many metrics of coherence with respect to their use-
fulness for TS in the same domain. Each human ordering of
facts in the corpus is scored by each of these metrics which
are then penalised proportionally to the amount of alternative
orderings of the same material that are found to score equally
to or better than the human ordering. The few metrics which
manage to outperform two simple baselines in their overall
performance across the corpus emerge as the most suitable
candidates for TS in the investigated domain. This method-
ology is very similar to the way [Barzilay and Lee, 2004]
evaluate their probabilistic TS model in comparison to the
approach of [Lapata, 2003].
Because the data used in the studies of [Dimitromanolaki
and Androutsopoulos, 2003] and [Karamanis et al, 2004]
are based on the insights of just one expert, an obvious un-
resolved question is whether they reflect general strategies
for ordering facts in the domain of interest. This paper ad-
dresses this issue by enhancing the dataset used in the two
studies with orderings provided by three additional experts.
These orderings are then compared with the orders of E0 us-
ing the methodology of [Lapata, 2003]. Since E0 is found
to share a lot of common ground with two of her colleagues
in the ordering task, her reliability is verified, while a fourth
?stand-alone? expert who uses strategies not shared by any
other expert is identified as well.
As in [Lapata, 2003], the same dependent variable which
allows us to estimate how different the orders of E0 are from
the orders of her colleagues is used to evaluate some of the
metrics which perform best in [Karamanis et al, 2004]. As
explained in the next section, in this way we investigate the
previously unaddressed possibility that there may exist many
optimal solutions for TS in our domain. The results of this
additional evaluation experiment are presented and emphasis
is laid on their relation with the previous findings.
Overall, this paper addresses two general issues: a) how to
verify the generality of a dataset defined by one expert using
sentence orderings provided by other experts and b) how to
employ these data for the automatic evaluation of a TS ap-
proach. Given that the methodology discussed in this paper
does not rely on the employed metrics of coherence or the as-
sumed TS approach, our work can be of interest to any NLG
researcher facing these questions.
The next section discusses how the methodology imple-
mented in this study complements the methods of [Karamanis
et al, 2004]. After briefly introducing the employed metrics
of coherence, we describe the data collected for our exper-
iments. Then, we present the employed dependent variable
and formulate our predictions. In the results section, we state
which of these predictions were verified. The paper is con-
cluded with a discussion of the main findings.
2 An additional evaluation test
As [Barzilay et al, 2002] report, different humans often order
sentences in distinct ways. Thus, there might exist more than
one equally good solution for TS, a view shared by almost
all TS researchers, but which has not been accounted for in
the evaluation methodologies of [Karamanis et al, 2004] and
[Barzilay and Lee, 2004].2
Collecting sentence orderings defined by many experts in
our domain enables us to investigate the possibility that there
might exist many good solutions for TS. Then, the measure
of [Lapata, 2003], which estimates how close two orderings
stand, can be employed not only to verify the reliability of E0
but also to compare the orderings preferred by the assumed
TS approach with the orderings of the experts.
However, this evaluation methodology has its limitations
as well. Being engaged in other obligations, the experts nor-
mally have just a limited amount of time to devote to the
2A more detailed discussion of existing corpus-based methods
for evaluating TS appears in [Karamanis and Mellish, 2005].
NLG researcher. Similarly to standard psycholinguistic ex-
periments, consulting these informants is difficult to extend
to a larger corpus like the one used e.g. by [Karamanis et al,
2004] (122 sets of facts).
In this paper, we reach a reasonable compromise by show-
ing how the methodology of [Lapata, 2003] supplements the
evaluation efforts of [Karamanis et al, 2004] using a similar
(yet by necessity smaller) dataset. Clearly, a metric of coher-
ence that has already done well in the previous study, gains
extra bonus by passing this additional test.
3 Metrics of coherence
[Karamanis, 2003] discusses how a few basic notions of co-
herence captured by Centering Theory (CT) can be used to
define a large range of metrics which might be useful for TS
in our domain of interest.3 The metrics employed in the ex-
periments of [Karamanis et al, 2004] include:
M.NOCB which penalises NOCBs, i.e. pairs of adjacent
facts without any arguments in common [Karamanis and
Manurung, 2002]. Because of its simplicity M.NOCB
serves as the first baseline in the experiments of [Kara-
manis et al, 2004].
PF.NOCB, a second baseline, which enhances M.NOCB
with a global constraint on coherence that [Karamanis,
2003] calls the PageFocus (PF).
PF.BFP which is based on PF as well as the original for-
mulation of CT in [Brennan et al, 1987].
PF.KP which makes use of PF as well as the recent re-
formulation of CT in [Kibble and Power, 2000].
[Karamanis et al, 2004] report that PF.NOCB outper-
formed M.NOCB but was overtaken by PF.BFP and PF.KP.
The two metrics beating PF.NOCB were not found to differ
significantly from each other.
This study employs PF.BFP and PF.KP, i.e. two of the best
performing metrics of the experiments in [Karamanis et al,
2004], as well as M.NOCB and PF.NOCB, the two previously
used baselines. An additional random baseline is also defined
following [Lapata, 2003].
4 Data collection
16 sets of facts were randomly selected from the corpus of
[Dimitromanolaki and Androutsopoulos, 2003].4 The sen-
tences that each fact corresponds to and the order defined by
E0 was made available to us as well. We will subsequently
refer to an unordered set of facts (or sentences that the facts
correspond to) as a Testitem.
4.1 Generating the BestOrders for each metric
Following [Karamanis et al, 2004], we envisage a TS ap-
proach in which a metric of coherence M assigns a score to
3Since discussing the metrics in detail is well beyond the scope
of this paper, the reader is referred to Chapter 3 of [Karamanis, 2003]
for more information on this issue.
4These are distinct from, yet very similar to, the sets of facts used
in [Karamanis et al, 2004].
each possible ordering of the input set of facts and selects the
best scoring ordering as the output. When many orderings
score best, M chooses randomly between them. Crucially, our
hypothetical TS component only considers orderings starting
with the subclass fact (e.g. subclass(ex1, amph)
in Figure 1) following the suggestion of [Dimitromanolaki
and Androutsopoulos, 2003]. This gives rise to 5! = 120
orderings to be scored by M for each Testitem.
For the purposes of this experiment, a simple algorithm
was implemented that first produces the 120 possible order-
ings of facts in a Testitem and subsequently ranks them ac-
cording to the scores given by M. The algorithm outputs the
set of BestOrders for the Testitem, i.e. the orderings which
score best according to M. This procedure was repeated for
each metric and all Testitems employed in the experiment.
4.2 Random baseline
Following [Lapata, 2003], a random baseline (RB) was im-
plemented as the lower bound of the analysis. The random
baseline consists of 10 randomly selected orderings for each
Testitem. The orderings are selected irrespective of their
scores for the various metrics.
4.3 Consulting domain experts
Three archaeologists (E1, E2, E3), one male and two females,
between 28 and 45 years of age, all trained in cataloguing
and museum labelling, were recruited from the Department
of Classics at the University of Edinburgh.
Each expert was consulted by the first author in a separate
interview. First, she was presented with a set of six sentences,
each of which corresponded to a database fact and was printed
on a different filecard, as well as with written instructions de-
scribing the ordering task.5 The instructions mention that the
sentences come from a computer program that generates de-
scriptions of artefacts in a virtual museum. The first sentence
for each set was given by the experimenter.6 Then, the expert
was asked to order the remaining five sentences in a coherent
text.
When ordering the sentences, the expert was instructed to
consider which ones should be together and which should
come before another in the text without using hints other than
the sentences themselves. She could revise her ordering at
any time by moving the sentences around. When she was sat-
isfied with the ordering she produced, she was asked to write
next to each sentence its position, and give them to the ex-
perimenter in order to perform the same task with the next
randomly selected set of sentences. The expert was encour-
aged to comment on the difficulty of the task, the strategies
she followed, etc.
5 Dependent variable
Given an unordered set of sentences and two possible order-
ings, a number of measures can be employed to calculate the
5The instructions are given in Appendix D of [Karamanis, 2003]
and are adapted from the ones used in [Barzilay et al, 2002].
6This is the sentence corresponding to the subclass fact.
distance between them. Based on the argumentation in [How-
ell, 2002], [Lapata, 2003] selects Kendall?s ? as the most ap-
propriate measure and this was what we used for our analysis
as well. Kendall?s ? is based on the number of inversions
between the two orderings and is calculated as follows:
(1) ? = 1? 2IPN = 1?
2I
N(N?1)/2
PN stands for the number of pairs of sentences and N is the
number of sentences to be ordered.7 I stands for the number
of inversions, that is, the number of adjacent transpositions
necessary to bring one ordering to another. Kendall?s ? ranges
from ?1 (inverse ranks) to 1 (identical ranks). The higher the
? value, the smaller the distance between the two orderings.
Following [Lapata, 2003], the Tukey test is employed to in-
vestigate significant differences between average ? scores.8
First, the average distance between (the orderings of)9 two
experts e.g. E0 and E1, denoted as T (E0E1), is calculated as
the mean ? value between the ordering of E0 and the order-
ing of E1 taken across all 16 Testitems. Then, we compute
T (EXPEXP ) which expresses the overall average distance
between all expert pairs and serves as the upper bound for the
evaluation of the metrics. Since a total of E experts gives rise
to PE = E(E?1)2 expert pairs, T (EXPEXP ), is computedby summing up the average distances between all expert pairs
and dividing the sum by PE .
While [Lapata, 2003] always appears to single out a unique
best scoring ordering, we often have to deal with many best
scoring orderings. To account for this, we first compute the
average distance between e.g. the ordering of an expert E0
and the BestOrders of a metric M for a given Testitem. In
this way, M is rewarded for a BestOrder that is close to the
expert?s ordering, but penalised for every BestOrder that is
not. Then, the average T (E0M ) between the expert E0 and
the metric M is calculated as their mean distance across all
16 Testitems. Finally, yet most importantly, T (EXPM ) is the
average distance between all experts and M. It is calculated by
summing up the average distances between each expert and M
and dividing the sum by the number of experts. As the next
section explains in more detail, T (EXPM ) is compared with
the upper bound of the evaluation T (EXPEXP ) to estimate
the performance of M in our experiments.
RB is evaluated in a similar way as M using the 10 ran-
domly selected orderings instead of the BestOrders for each
Testitem. T (EXPRB) is the average distance between all ex-
perts and RB and is used as the lower bound of the evaluation.
7In our data, N is always equal to 6.
8Provided that an omnibus ANOVA is significant, the Tukey test
can be used to specify which of the conditions c1, ..., cn measured
by the dependent variable differ significantly. It uses the set of means
m1, ...,mn (corresponding to conditions c1, ..., cn) and the mean
square error of the scores that contribute to these means to calculate
a critical difference between any two means. An observed differ-
ence between any two means is significant if it exceeds the critical
difference.
9Throughout the paper we often refer to e.g. ?the distance be-
tween the orderings of the experts? with the phrase ?the distance
between the experts? for the sake of brevity.
E0E1: ** ** **
0.692 E0E2: ** ** **
0.717 E1E2: ** ** **
0.758 E0E3:
CD at 0.01: 0.338 0.258 E1E3:
CD at 0.05: 0.282 0.300 E2E3:
F(5,75)=14.931, p<0.000 0.192
Table 1: Comparison of distances between the expert pairs
6 Predictions
Despite any potential differences between the experts, one ex-
pects them to share some common ground in the way they or-
der sentences. In this sense, a particularly welcome result for
our purposes is to show that the average distances between
E0 and most of her colleagues are short and not significantly
different from the distances between the other expert pairs,
which in turn indicates that she is not a ?stand-alone? expert.
Moreover, we expect the average distance between the ex-
pert pairs to be significantly smaller than the average distance
between the experts and RB. This is again based on the as-
sumption that even though the experts might not follow com-
pletely identical strategies, they do not operate with absolute
diversity either. Hence, we predict that T (EXPEXP ) will be
significantly greater than T (EXPRB).
Due to the small number of Testitems employed in this
study, it is likely that the metrics do not differ significantly
from each other with respect to their average distance from
the experts. Rather than comparing the metrics directly with
each other (as [Karamanis et al, 2004] do), this study com-
pares them indirectly by examining their behaviour with re-
spect to the upper and the lower bound. For instance, al-
though T (EXPPF.KP ) and T (EXPPF.BFP ) might not be
significantly different from each other, one score could be sig-
nificantly different from T (EXPEXP ) (upper bound) and/or
T (EXPRB) (lower bound) while the other is not.
We identify the best metrics in this study as the ones whose
average distance from the experts (i) is significantly greater
from the lower bound and (ii) does not differ significantly
from the upper bound.10
7 Results
7.1 Distances between the expert pairs
On the first step in our analysis, we computed the T score
for each expert pair, namely T (E0E1), T (E0E2), T (E0E3),
T (E1E2), T (E1E3) and T (E2E3). Then we performed all
15 pairwise comparisons between them using the Tukey test,
the results of which are summarised in Table 1.11
The cells in the Table report the level of significance re-
turned by the Tukey test when the difference between two
10Criterion (ii) can only be applied provided that the average dis-
tance between the experts and at least one metric Mx is found to
be significantly lower than T (EXPEXP ). Then, if the average dis-
tance between the experts and another metric My does not differ
significantly from T (EXPEXP ), My performs better than Mx.
11The Table also reports the result of the omnibus ANOVA, which
is significant: F(5,75)=14.931, p<0.000.
E0E1: ** ** **
0.692 E0E2: ** ** **
0.717 E1E2: ** ** **
0.758 E0RB :
CD at 0.01: 0.242 0.323 E1RB :
CD at 0.05: 0.202 0.347 E2RB :
F(5,75)=18.762, p<0.000 0.352
E0E3:
0.258 E1E3:
0.300 E2E3:
CD at 0.01: 0.219 0.192 E3RB :
CD at 0.05: 0.177 0.302
F(3,45)=1.223, p=0.312
Table 2: Comparison of distances between the experts (E0,
E1, E2, E3) and the random baseline (RB)
distances exceeds the critical difference (CD). Significance
beyond the 0.05 threshold is reported with one asterisk (*),
while significance beyond the 0.01 threshold is reported with
two asterisks (**). A cell remains empty when the difference
between two distances does not exceed the critical difference.
For example, the value of T (E0E1) is 0.692 and the value of
T (E0E3) is 0.258. Since their difference exceeds the CD at
the 0.01 threshold, it is reported to be significant beyond that
level by the Tukey test, as shown in the top cell of the third
column in Table 1.
As the Table shows, the T scores for the distance between
E0 and E1 or E2, i.e. T (E0E1) and T (E0E2), as well as the
T for the distance between E1 and E2, i.e. T (E1E2), are quite
high which indicates that on average the orderings of the three
experts are quite close to each other. Moreover, these T scores
are not significantly different from each other which suggests
that E0, E1 and E2 share quite a lot of common ground in
the ordering task. Hence, E0 is found to give rise to similar
orderings to the ones of E1 and E2.
However, when any of the previous distances is compared
with a distance that involves the orderings of E3 the differ-
ence is significant, as shown by the cells containing two as-
terisks in Table 1. In other words, although the orderings of
E1 and E2 seem to deviate from each other and the orderings
of E0 to more or less the same extent, the orderings of E3
stand much further away from all of them. Hence, there ex-
ists a ?stand-alone? expert among the ones consulted in our
studies, yet this is not E0 but E3.
This finding can be easily explained by the fact that by con-
trast to the other three experts, E3 followed a very schematic
way for ordering sentences. Because the orderings of E3
manifest rather peculiar strategies, at least compared to the or-
derings of E0, E1 and E2, the upper bound of the analysis, i.e.
the average distance between the expert pairs T (EXPEXP ),
is computed without taking into account these orderings:
(2) T (EXPEXP ) = 0.722 = T (E0E1)+T (E0E2)+T (E1E2)3
7.2 Distances between the experts and RB
As the upper part of Table 2 shows, the T score between any
two experts other than E3 is significantly greater than their
distance from RB beyond the 0.01 threshold. Only the dis-
tances between E3 and another expert, shown in the lower
section of Table 2, are not significantly different from the dis-
tance between E3 and RB.
Although this result does not mean that the orders of E3
are similar to the orders of RB,12 it shows that E3 is roughly
as far away from e.g. E0 as she is from RB. By contrast,
E0 stands significantly closer to E1 than to RB, and the same
holds for the other distances in the upper part of the Table.
In accordance with the discussion in the previous section, the
lower bound, i.e. the overall average distance between the
experts (excluding E3) and RB T (EXPRB), is computed as
shown in (3):
(3) T (EXPRB) = 0.341 = T (E0RB)+T (E1RB)+T (E2RB)3
7.3 Distances between the experts and each metric
So far, E3 was identified as an ?stand-alone? expert standing
further away from the other three experts than they stand from
each other. We also identified the distance between E3 and
each expert as similar to her distance from RB.
Similarly, E3 was found to stand further away from the
metrics compared to their distance from the other three ex-
perts.13 This result, gives rise to the set of formulas in (4) for
calculating the overall average distance between the experts
(excluding E3) and each metric.
(4) (4.1): T (EXPPF.BFP ) = 0.629 =
T (E0PF.BFP )+T (E1PF.BFP )+T (E2PF.BFP )
3
(4.2): T (EXPPF.KP ) = 0.571 =
T (E0PF.KP )+T (E1PF.KP )+T (E2PF.KP )
3
(4.3): T (EXPPF.NOCB) = 0.606 =
T (E0PF.NOCB)+T (E1PF.NOCB)+T (E2PF.NOCB)
3
(4.4): T (EXPM.NOCB) = 0.487 =
T (E0M.NOCB)+T (E1M.NOCB)+T (E2M.NOCB)
3
In the next section, we present the concluding analysis for
this study which compares the overall distances in formu-
las (2), (3) and (4) with each other. As we have already
mentioned, T (EXPEXP ) serves as the upper bound of the
analysis whereas T (EXPRB) is the lower bound. The aim
is to specify which scores in (4) are significantly greater than
T (EXPRB), but not significantly lower than T (EXPEXP ).
7.4 Concluding analysis
The results of the comparisons of the scores in (2), (3) and (4)
are shown in Table 3. As the top cell in the last column of
the Table shows, the T score between the experts and RB,
T (EXPRB), is significantly lower than the average distance
between the expert pairs, T (EXPEXP ) at the 0.01 level.
12This could have been argued, if the value of T (E3RB) had been
much closer to 1.
13Due to space restrictions, we cannot report the scores for these
comparisons here. The reader is referred to Table 9.4 on page 175
of Chapter 9 in [Karamanis, 2003].
This result verifies one of our main predictions showing that
the orderings of the experts (modulo E3) stand much closer
to each other compared to their distance from randomly as-
sembled orderings.
As expected, most of the scores that involve the met-
rics are not significantly different from each other, ex-
cept for T (EXPPF.BFP ) which is significantly greater than
T (EXPM.NOCB) at the 0.05 level. Yet, what we are mainly
interested in is how the distance between the experts and each
metric compares with T (EXPEXP ) and T (EXPRB). This
is shown in the first row and the last column of Table 3.
Crucially, T (EXPRB) is significantly lower than
T (EXPPF.BFP ) as well as T (EXPPF.NOCB) and
T (EXPPF.KP ) at the 0.01 level. Notably, even the dis-
tance of the experts from M.NOCB, T (EXPM.NOCB), is
significantly greater than T (EXPRB), albeit at the 0.05
level. These results show that the distance from the experts is
significantly reduced when using the best scoring orderings
of any metric, even M.NOCB, instead of the orderings of
RB. Hence, all metrics score significantly better than RB in
this experiment.
However, simply using M.NOCB to output the best
scoring orders is not enough to yield a distance from
the experts which is comparable to T (EXPEXP ). Al-
though the PF constraint appears to help towards this di-
rection, T (EXPPF.KP ) remains significantly lower than
T (EXPEXP ), whereas T (EXPPF.NOCB) falls only 0.009
points short of CD at the 0.05 threshold. Hence, PF.BFP
is the most robust metric, as the difference between
T (EXPPF.BFP ) and T (EXPEXP ) is clearly not signifi-
cant.
Finally, the difference between T (EXPPF.NOCB) and
T (EXPM.NOCB) is only 0.006 points away from the CD.
This result shows that the distance from the experts is reduced
to a great extent when the best scoring orderings are com-
puted according to PF.NOCB instead of simply M.NOCB.
Hence, this experiment provides additional evidence in favour
of enhancing M.NOCB with the PF constraint of coherence,
as suggested in [Karamanis, 2003].
8 Discussion
A question not addressed by previous studies making use of
a certain collection of orderings of facts is whether the strate-
gies reflected there are specific to E0, the expert who created
the dataset. In this paper, we address this question by enhanc-
ing E0?s dataset with orderings provided by three additional
experts. Then, the distance between E0 and her colleagues
is computed and compared to the distance between the other
expert pairs. The results indicate that E0 shares a lot of com-
mon ground with two of her colleagues in the ordering task
deviating from them as much as they deviate from each other,
while the orderings of a fourth ?stand-alone? expert are found
to manifest rather individualistic strategies.
The same variable used to investigate the distance between
the experts is employed to automatically evaluate the best
scoring orderings of some of the best performing metrics in
[Karamanis et al, 2004]. Despite its limitations due to the
necessarily restricted size of the employed dataset, this eval-
EXPEXP : ** ** **
0.722 EXPPF.BFP : * **
0.629 EXPPF.NOCB : **
0.606 EXPPF.KP : **
CD at 0.01: 0.150 0.571 EXPM.NOCB : *
CD at 0.05: 0.125 0.487 EXPRB :
F(5,75)=19.111, p<0.000 0.341
Table 3: Results of the concluding analysis comparing the distance between the expert pairs (EXPEXP ) with the distance
between the experts and each metric (PF.BFP, PF.NOCB, PF.KP, M.NOCB) and the random baseline (RB)
uation task allows us to explore the previously unaddressed
possibility that there exist many good solutions for TS in the
employed domain.
Out of a much larger set of possibilities, 10 metrics were
evaluated in [Karamanis et al, 2004], only a handful of which
were found to overtake two simple baselines. The additional
test in this study carries on the elimination process by point-
ing out PF.BFP as the single most promising metric to be used
for TS in the explored domain, since this is the metric that
manages to clearly survive both tests.
Equally crucially, our analysis shows that all employed
metrics are superior to a random baseline. Additional evi-
dence in favour of the PF constraint on coherence introduced
in [Karamanis, 2003] is provided as well. The general evalu-
ation methodology as well as the specific results of this study
will be useful for any subsequent attempt to automatically
evaluate a TS approach using a corpus of sentence orderings
defined by many experts.
As [Reiter and Sripada, 2002] suggest, the best way to treat
the results of a corpus-based study is as hypotheses which
eventually need to be integrated with other types of evalua-
tion. Although we followed the ongoing argumentation that
using perceptual experiments to choose between many possi-
ble metrics is unfeasible, our efforts have resulted into a sin-
gle preferred candidate which is much easier to evaluate with
the help of psycholinguistic techniques (instead of having to
deal with a large number of metrics from very early on). This
is indeed our main direction for future work in this domain.
Acknowledgments
We are grateful to Aggeliki Dimitromanolaki for entrusting
us with her data and for helpful clarifications on their use; to
Mirella Lapata for providing us with the scripts for the com-
putation of ? together with her extensive and prompt advice;
to Katerina Kolotourou for her invaluable assistance in re-
cruiting the experts; and to the experts for their participation.
This work took place while the first author was studying at
the University of Edinburgh, supported by the Greek State
Scholarship Foundation (IKY).
References
[Barzilay and Lee, 2004] Regina Barzilay and Lillian Lee. Catch-
ing the drift: Probabilistic content models with applications to
generation and summarization. In Proceedings of HLT-NAACL
2004, pages 113?120, 2004.
[Barzilay et al, 2002] Regina Barzilay, Noemie Elhadad, and
Kathleen McKeown. Inferring strategies for sentence ordering
in multidocument news summarization. Journal of Artificial In-
telligence Research, 17:35?55, 2002.
[Brennan et al, 1987] Susan E. Brennan, Marilyn A. Fried-
man [Walker], and Carl J. Pollard. A centering approach to pro-
nouns. In Proceedings of ACL 1987, pages 155?162, Stanford,
California, 1987.
[Dimitromanolaki and Androutsopoulos, 2003] Aggeliki Dimitro-
manolaki and Ion Androutsopoulos. Learning to order facts for
discourse planning in natural language generation. In Proceed-
ings of the 9th European Workshop on Natural Language Gener-
ation, Budapest, Hungary, 2003.
[Howell, 2002] David C. Howell. Statistical Methods for Psychol-
ogy. Duxbury, Pacific Grove, CA, 5th edition, 2002.
[Isard et al, 2003] Amy Isard, Jon Oberlander, Ion Androutsopou-
los, and Colin Matheson. Speaking the users? languages. IEEE
Intelligent Systems Magazine, 18(1):40?45, 2003.
[Karamanis and Manurung, 2002] Nikiforos Karamanis and
Hisar Maruli Manurung. Stochastic text structuring using the
principle of continuity. In Proceedings of INLG 2002, pages
81?88, Harriman, NY, USA, July 2002.
[Karamanis and Mellish, 2005] Nikiforos Karamanis and Chris
Mellish. A review of recent corpus-based methods for evaluat-
ing text structuring in NLG. 2005. Submitted to Using Corpora
for NLG workshop.
[Karamanis et al, 2004] Nikiforos Karamanis, Chris Mellish, Jon
Oberlander, and Massimo Poesio. A corpus-based methodology
for evaluating metrics of coherence for text structuring. In Pro-
ceedings of INLG04, pages 90?99, Brockenhurst, UK, 2004.
[Karamanis, 2003] Nikiforos Karamanis. Entity Coherence for De-
scriptive Text Structuring. PhD thesis, Division of Informatics,
University of Edinburgh, 2003.
[Kibble and Power, 2000] Rodger Kibble and Richard Power. An
integrated framework for text planning and pronominalisation. In
Proceedings of INLG 2000, pages 77?84, Israel, 2000.
[Lapata, 2003] Mirella Lapata. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of ACL 2003,
pages 545?552, Saporo, Japan, July 2003.
[McKeown, 1985] Kathleen McKeown. Text Generation: Using
Discourse Strategies and Focus Constraints to Generate Natural
Language Text. Studies in Natural Language Processing. Cam-
bridge University Press, 1985.
[Reiter and Sripada, 2002] Ehud Reiter and Somayajulu Sripada.
Should corpora texts be gold standards for NLG? In Proceedings
of INLG 2002, pages 97?104, Harriman, NY, USA, July 2002.
Proceedings of the 12th European Workshop on Natural Language Generation, pages 130?137,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Investigating Content Selection for Language Generation using Machine
Learning
Colin Kelly
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge, UK
Ann Copestake
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge, UK
{colin.kelly,ann.copestake,nikiforos.karamanis}@cl.cam.ac.uk
Nikiforos Karamanis
Department of Computer Science
Trinity College Dublin
Dublin 2
Ireland
Abstract
The content selection component of a nat-
ural language generation system decides
which information should be communi-
cated in its output. We use informa-
tion from reports on the game of cricket.
We first describe a simple factoid-to-text
alignment algorithm then treat content se-
lection as a collective classification prob-
lem and demonstrate that simple ?group-
ing? of statistics at various levels of granu-
larity yields substantially improved results
over a probabilistic baseline. We addi-
tionally show that holding back of specific
types of input data, and linking database
structures with commonality further in-
crease performance.
1 Introduction
Content selection is the task executed by a natu-
ral language generation (NLG) system of decid-
ing, given a knowledge-base, which subset of the
information available should be conveyed in the
generated document (Reiter and Dale, 2000).
Consider the task of generating a cricket match
report, given the scorecard for that match. Such
a scorecard would typically contain a large num-
ber of statistics pertaining to the game as a whole
as well as individual players (e.g. see Figure 1).
Our aim is to identify which statistics should be
selected by the NLG system.
Much work has been done in the field of con-
tent selection, in a diverse range of domains e.g.
weather forecasts (Coch, 1998). Approaches are
usually domain specific and predominantly based
on structured tables of well-defined input data.
Duboue and McKeown (2003) attempted a sta-
tistical approach to content selection using a sub-
stantial corpus of biographical summaries paired
with selected content, where they extracted rules
and patterns linking the two. They then used ma-
chine learning to ascertain what was relevant.
Barzilay and Lapata (2005) extended this ap-
proach but applying it to a sports domain (Amer-
ican football), similarly viewing content selection
as a classification task and additionally taking ac-
count of contextual dependencies between data,
and found that this improved results compared to
a content-agnostic baseline. We aim throughout
to extend and improve upon Barzilay and Lapata?s
methods.
We emphasise that content selection through
statistical machine learning is a relatively new area
? approaches prior to Duboue and McKeown?s are,
in principle, much less portable ? and as such there
is not an enormous body of work to build upon.
This work offers a novel algorithm for data-to-
text alignment, presents a new ?grouping? method
for sharing knowledge across similar but distinct
learning instances and shows that holding back
certain data from the machine learner, and rein-
troducing it later on can improve results.
2 Data Acquisition & Alignment
We first must obtain appropriately aligned cricket
data, for the purposes of machine learning.
Our data comes from the online Wisden al-
manack (Cricinfo, 2007), which we used to down-
load 133 match report/scorecard pairs. We em-
ployed an HTML parser to extract the main text
from the match report webpage, and the match
data-tables from the scorecard webpage. An ex-
ample scorecard can be found in Figure 11.
1Cricket is a bat-and-ball sport, contested by two oppos-
ing teams of eleven players. Each side?s objective is to score
more ?runs? than their opponents. An ?innings? refers to the
collective performance of the batting team, and (usually) ends
when all eleven players have batted.
In Figure 1, in the batting section R stands for ?runs made?,
M for ?minutes played on the field?, B for ?number of balls
faced?. 4s and 6s are set numbers of runs awarded for hit-
ting balls that reach the boundary. SR is the number of runs
per 100 balls. In the bowling section, O stands for ?overs
130
Result India won by 63 runs
India innings (50 overs maximum) R M B 4s 6s SR
SC Ganguly? run out (Silva/Sangakarra?) 9 37 19 2 0 47.36
V Sehwag run out (Fernando) 39 61 40 6 0 97.50
D Mongia b Samaraweera 48 91 63 6 0 76.19
SR Tendulkar c Chandana b Vaas 113 141 102 12 1 110.78
. . .
Extras (lb 6, w 12, nb 7) 25
Total (all out; 50 overs; 223 mins) 304
Fall of wickets 1-32 (Ganguly, 6.5 ov), 2-73 (Sehwag, 11.2 ov), 3-172 (Mongia,
27.4 ov), 4-199 (Dravid, 32.1 ov), . . . , 10-304 (Nehra, 49.6 ov)
Bowling O M R W Econ
WPUJC Vaas 10 1 64 1 6.40 (2w)
DNT Zoysa 10 0 66 1 6.60 (6nb, 2w)
. . .
TT Samaraweera 8 0 39 2 4.87 (2w)
Figure 1: Statistics in a typical cricket scorecard.
2.1 Report Alignment
We use a supervised method to train our data, and
thus need to find all ?links? between the scorecard
and match report. We execute this alignment by
first creating tags with tag attributes according to
the common structure of the scorecards, and tag
values according to the data within a particular
scorecard. We then attempt to automatically align
the values of those tags with factoids, single pieces
of information found in the report.
For example, from Figure 1 the fact that Ten-
dulkar was the fourth player to bat on the first team
is captured by constructing a tag with tag attribute
team1 player4, and tag value ?SR Tendulkar?. The
fact he achieved 113 runs is encapsulated by an-
other tag, with tag attribute as team1 player4 R
and tag value as ?113?. Then if the report con-
tained the phrase ?Tendulkar made 113 off 102
balls? we would hope to match the ?Tendulkar?
factoid with our tag value ?SR Tendulkar?, the
?113? factoid with our tag value ?113? and replace
both factoids with their respective tag attributes, in
this case team1 player4 and team1 player4 R re-
spectively. Similar methods for this problem have
been employed by Barzilay and Lapata (2005) and
Duboue and McKeown (2003).
The basic idea behind our 6-step process for
alignment is that we align those factoids we are
bowled?, M for ?maiden overs?, R for ?runs conceded? and W
for ?wickets taken?. Econ is ?economy rate?, or number of
runs per over.
It is important to note that Figure 1 omits the opposing
team?s innings (comprising new instances of the ?Batting?,
?Fall of Wickets? and ?Bowling? sections), and some addi-
tional statistics found at the bottom of the scorecard.
most certain of first. The main obstacle we face
when aligning is the large incidence of repeated
numbers occurring within the scorecard, as this
would imply we have multiple, different tags all
with the same tag values. It is wholly possible
(and quite typical) that single figures will be re-
peated many times within a single scorecard2.
Therefore it would be advantageous for us to
have some means to differentiate amongst tags,
and hopefully select the correct tag when encoun-
tering a factoid which corresponds to repeated tag
values. Our algorithm is as follows:
Preprocessing We began by converting all ver-
balised numbers to their cardinal equivalents, e.g.
?one?, ?two? to ?1?, ?2?, and selected instances of
?a? into ?1?.
Proper Nouns In the first round of tagging we
attempt to match proper names from the scorecard
with strings within the report. Additionally, we
maintain a list of all players referenced thus far.
Player-Relevant Details Using the list of play-
ers we have accumulated, we search the report for
matches on tag values relating to only those play-
ers. This step was based on the assumption that a
factoid about a specific player is unlikely to appear
unless that player has been named.
Non-Player-Relevant Details The next stage
involves attempting to match factoids to tag values
whose attributes don?t refer to a particular player
e.g., more general match information as well as
team statistics.
2For example in Figure 1 we can see the number 6 appear-
ing four times: twice as the number of 4s for two different
players, once as an lb statistic and once as an nb statistic.
131
Anchor-Based Matching We next use sur-
rounding text anchor-based matching: for exam-
ple, if a sentence contains the string ?he bowled
for 3 overs? we will preferentially attempt to match
the factoid ?3? with tag values from tags which we
know refer to overs.
Remaining Matches The final step acts as our
?catch-all? ? we proceed through all remaining
words in the report and try to match each poten-
tial factoid with the first (if any) tag found whose
tag value is the same.
2.2 Evaluation
The output of our program is the original text with
all aligned figures and strings (factoids) replaced
with their corresponding tag attributes. We can see
an extract from an aligned report in Figure 2 where
we show the aligned factoids in bold, and their cor-
responding tag attributes in italics. We also note at
this point that much of commentary shown does
not in fact appear in the scorecard, and therefore
additional knowledge sources would typically be
required to generate a full match report ? this is
beyond the scope of our paper, but Robin (1995)
attempts to deal with this problem in the domain
of basketball using revision-based techniques for
including additional content.
We asked a domain expert to evaluate five of
our aligned match reports ? he did this by creat-
ing his own ?gold standard? for each report, a list
of aligned tags. Compared to our automatically
aligned tags, we obtained 79.0% average preci-
sion, 75.8% average recall and a mean F of 77.0%.
3 Categorization
We are using the methods of Barzilay and Lapata
(henceforth B&L) as our starting point, so we de-
scribe what we did to emulate and extend them.
3.1 Barzilay and Lapata?s Method
B&L?s corpus was composed of a relational
database of football statistics. Within the database
were multiple tables, which we will refer to as
?categories? (actions within a game, e.g. touch-
downs and fumbles). Each category was com-
posed of ?groups? (the rows within a category ta-
ble), with each row referring to a distinct player,
and each column referring to different types of ac-
tion within that category (?attributes?).
B&L?s technique for the purposes of the ma-
chine learning was to assign a ?1? or ?0? to each
NatWest Series (series), match 9 (team1 player1 R)
India v Sri Lanka (matchtitle)
At Bristol (venue town), July 11 (date) (day/night
(daynight)).
India (team1) won by 63 runs (winmethod).
India (team1) 5 (team1 points) pts.
Toss: India (team1).
The highlight of a meaningless match was a sublime in-
nings from Tendulkar (team1 player4), who resumed
his fleeting love affair with Nevil Road to the delight
of a flag-waving crowd. On India (team1)?s only other
visit to Bristol (venue town), for a World Cup game
in 1999 against Kenya, Tendulkar (team1 player4)
had creamed an unbeaten 140, and this time he drove
with elan to make 113 (team1 player4 R) off just 102
(team1 player4 B) balls with 12 (team1 player4 4s)
fours and a (team1 player4 6s) six.
. . .
Figure 2: Aligned match report extract
row, where a row would receive the value ?1? if
one or more of the entries in the row was ver-
balised in the report. In the context of our data
we could apply a similar division, for example, by
constructing a category entitled ?Batting? with at-
tributes (columns) ?Runs?, ?Balls?, ?Minutes?, ?4s?
and ?6s? etc., and rows corresponding to players.
In this case a group within that category would
correspond to one line of the ?Innings? table in Fig-
ure 1.
We note that B&L were selecting content on a
row basis, while we are aiming to select individual
tag attributes (i.e., specific row/column cell refer-
ences) within the categories, a more difficult task.
We discuss this further in Section 6.
The technique above allows the machine learn-
ing algorithm to be aware that different statistics
are semantically related ? i.e., each group within a
category contains the same ?type? of information.
We therefore think this is a logical starting point
for our work, and we aim to expand upon it.
3.2 Classifying Tags
The key step was deciding upon an appropriate
division of our scorecard into various categories
and the groups for each category in the style of
B&L. As can be seen from Figure 1 our input in-
formation is a mixture of structured (e.g. Bowling,
Batting sections), semi-structured (Fall of Wickets
section) and almost unstructured (Result) informa-
tion. This is somewhat unlike B&L?s data, which
was fully structured in database form. We deal
132
Category Attributes Verb
Batting 9 47.0
Bowling 11 10.2
Fall of Wickets 8 46.4
Match Details 11 75.2
Match Result 8 45.1
Officials 8 6.0
Partnerships 11 75.5
Team Statistics 13 46.2
Table 1: Number of attributes per category with
percent verbalised (Verb)
with this by enforcing a stronger structure ? di-
viding the information into eight of our own ?cat-
egories?, based roughly on the formatting of the
webpages. These are outlined in Table 1.
The first three categories in the table are quite
intuitive and implicit from the respective sections
of the scorecard. There is additional information
in a typical scorecard (not shown in Figure 1),
which we must also categorise. The ?Team Statis-
tics? category contains details about the ?extras?3
scored by each team, as well as the number of
points gained by the team towards that particular
series4. We divide the remaining tag attributes as
follows into three categories: ?Officials? ? persons
participating in the match, other than the teams
(e.g. umpires, referees); ?Match Details? ? infor-
mation that would have been known before the
match was played (e.g. venue, date, season); and
?Match Result? ? data that could only be known
once the match was over (e.g. final result, player
of the match).
Finally we have an additional ?Partnerships?5
category which is given explicitly on a separate
webpage referenced from each scorecard, but is
also implicit from information contained in the
?Fall of Wickets? and ?Batting? sections. We an-
ticipate that this category will help us manage the
issue of data sparsity. For instance, in our domain
we could group partnerships (which could con-
tain a multitude of player combinations and there-
3Additional runs awarded to the batting team for specific
actions executed by the bowling team. There are four types:
No Ball, Wide, Bye, Leg Bye.
4Each cricket game is part of a specific ?series? of games.
e.g. India would receive five points for their win within the
NatWest series.
5A ?partnership? refers to a pair of players who bat to-
gether, and usually comprises information such as the num-
ber of runs scored between them, the number of deliveries
faced and so on.
fore distinct tags) with the various possible binary
combinations of players together for shared learn-
ing. We discuss this further in Section 8.3.
Within 5 of the categories described above, we
are further able to divide the data into ?groups? -
the Batting, Bowling, Fall of Wickets and Partner-
ships categories refer to multiple players and thus
have multiple rows. The Team Statistics category
contains two groups, one for each team. The other
categories merely form one-line tables.
4 Machine Learning
Our task is to establish which tag attributes (and
hence tag values) should be included in the final
match report, and is a multi-label classification
problem. We chose to use BoosTexter (Schapire
and Singer, 2000) as it has been shown to be an
effective classifier (Yang, 1999), and it is one of
the few text classification tools which directly sup-
ports multi-label classification. This is also what
B&L used.
Schapire and Singer?s BoosTexter (2000) uses
?decision stumps?, or single level decision trees
to classify its input data. The predicates of these
stumps are defined, for text, by the presence or
absence of a single term, and, for numerical at-
tributes, whether the attribute exceeds a given
threshold, decided dynamically.
4.1 Running BoosTexter
BoosTexter requires two input files to train a hy-
pothesis, ?Names? and ?Data?.
Names The Names file contains, for each pos-
sible tag attribute, t, across all scorecards, the type
of its corresponding tag value. These are contin-
uous for numbers and text for normal text. From
our 133 scorecards we extracted a total of 61,063
tag values, of which 82.2% were continuous, the
remainder being text.
Data The Data file contains, for each scorecard,
a comma-delimited list of all tag values for a par-
ticular scorecard, with a ??? for unknown values,
followed by a list of the verbalised tag attributes.
Testing We can now run BoosTexter with a
user-defined number of rounds, T , which creates
a hypothesis file. Using this hypothesis file and
a test ?data? file (without the list of verbalised tag
attributes), BoosTexter will give its hypothesized
predictions, a value f for each tag attribute t. The
sign of f determines whether the classifier be-
lieves the tag value corresponding to t is relevant
133
to the test scorecard, while |f | is a measure of the
confidence the classifier has in its assertion.
4.2 Data Sparsity
The very nature of the data means that there are
a large number of tag values which do not occur
in every scorecard ? the average scorecard con-
tained 24 values, yet our ?names? file contained
1193 possible tag attributes. A lot of this was due
to partnership tag attributes which formed 43.6%
of the ?names? entries. This large figure is because
a large number of all possible binary combinations
of players existed in the training data across both
teams6. This implies we will be unable to train for
a significant number of tag attributes as many spe-
cific tag values occur very rarely. Indeed we found
that of 158,669 entries, 97,666 (61.55%) were ?un-
known?.
5 Evaluation Baselines
It is not clear what constitutes a suitable baseline
so we considered multiple options. The issue of
ambiguous reference baselines is not specific to
the cricket domain, as there is no standardized
baseline approach across the prior literature. We
employ ten-fold cross validation throughout.
5.1 Majority Baseline
B&L created a ?majority baseline? whereby they
returned those categories (i.e., tables) which were
verbalised more than half of the time in their
aligned reports.
As explained in Section 3.2 we divided our tag
attributes into 8 categories. We emulated B&L?s
baseline method as follows: For each category, if
any of the tag values within a particular ?group?
was tagged as verbalised, we counted that as a
?vote? for that particular category. We then cal-
culated the total number of ?votes? divided by the
total number of ?groups? within each category. All
categories which had a ratio of 50% or greater
in this calculation were considered to be ?major-
ity categories?. Our baseline Bmaj then consisted
of all tag attributes forming part of those majority
categories. As shown in Table 1 there were only
two categories which exceeded the 50% threshold,
?Match Details? and ?Partnerships?.
We can see that this baseline performs
abysmally. The reason for this poor behaviour is
693 of the possible 2
?10
i=1 i = 110 combinations oc-
curred.
Bmaj ? min max ?
Precision 0.0966 0.0333 0.1583 0.0250
Recall 0.4879 0.2727 0.7895 0.0977
F 0.1603 0.0620 0.2568 0.0384
Table 2: Majority Baseline, Bmaj
that since so many tag attributes contribute to the
categories we are including far too many possibil-
ities in our baseline.
5.2 Probabilistic Baseline
This baseline is based on the premise that those
tag attributes which occur with highest frequency
across the training data refer to those tag values
which will often occur in a typical match report.
To create our baseline set of tag attributes Bprob
we extract the a most frequently verbalised tag at-
tributes across all the training data where a is the
average length of the verbalised tag attribute lists
for each report/scorecard pair.
Bprob ? min max ?
Precision 0.5157 0.2174 0.7391 0.1010
Recall 0.5157 0.1389 0.7647 0.0990
F 0.5100 0.1695 0.6939 0.0852
Table 3: Probabilistic Baseline, Bprob
This baseline achieves a mean F score of 51%,
however the tag attributes being returned are very
inconsistent with a typical match report ? they
correspond in the majority to player names but
not one refers to any other tag attributes relevant
to those players. This renders the output mostly
meaningless in terms of our aim to select content
for an NLG system.
5.3 No-Player Probabilistic Baseline
Taking the above into account we create a base-
line which derives its choice of tag attributes from
match statistics only. This baseline is similar to
the Probabilistic Baseline above, with the excep-
tion that when summing the numbers of tag at-
tributes in the sets we do not consider player-name
tag attributes in our counts. Instead, we extract
the a? most frequent tag attributes, where a? is
the average size of the sets excluding player-name
tag attributes. To finally obtain our baseline set
Bnops we merge our a? most frequent tag attributes
134
with any and all corresponding player-name tag at-
tributes7.
Bnops ? min max ?
Precision 0.4923 0.1765 0.6875 0.0922
Recall 0.3529 0.1111 0.5625 0.0842
F 0.4064 0.1538 0.5946 0.0767
Table 4: No-Player Probabilistic Baseline, Bnops
As can be seen from Table 4, this method suffers
an absolute F-score drop of more than 10% from
the previous method. However if we analyse the
output more closely we can see that although the
accuracy has dropped, the returned tag attributes
are more thematically consistent with the training
data. This is our preferred baseline.
6 Evaluation Paradigm
The main difficulty we encountered arose when
we came to assessing the Precision and Recall fig-
ures as we have yet to decide on what level we are
considering the output of our system to be correct.
We see three possibilities for the level:
Category We could simply count the ?votes?
predicted on a per category basis (as described
in sections 3.1 and 5.1), and evaluate categories
based on the number of votes given for each. We
would expect this to generate very good results as
we are effectively overgrouping, once on a group
basis (grouping together all attributes) and once on
a category basis (unifying all groups within a cate-
gory), but the output would be so general and triv-
ial (effectively stating something to the effect that
?a match report should contain information about
batting, bowling and team statistics?) that it would
be of no use in an NLG system.
Groups Here we compare which ?groups? were
verbalised within each category, and which were
predicted to be verbalised (as we did for the Major-
ity Baseline of Section 5.1). Our implicit grouping
means that we do not have to necessarily return the
correct statistic pertaining to a group since each
group acts as a basket for the statistics contained
within it, and is susceptible to ?false positives?.
This method is most similar to B&L?s.
Tags Since we are trying to establish which tag
attributes should be included rather than which
groups are likely to contain verbalised tag at-
tributes, we could say that even the above method
7e.g., if team1 player4 R is in a? then we would also in-
clude team1 player4 in our final set.
is too liberal in its definition of correctness. Thus
we also evaluate our groups on the basis of their
component parts, i.e., if a particular group of tag
attributes is estimated to be verbalised by Boos-
Texter, then we include all attributes from that
group.
7 Initial Results
Our ?categorized? results are derived from present-
ing BoosTexter with each individual category as
described in Section 3.2, then merging the selected
tag attributes together and evaluating based on the
criteria described above. We then show BoosTex-
ter?s performance ?as is?, by running the program
on the full output of our alignment stage with no
categorization/grouping.
7.1 Categorized ? Groups Level
Our ?Categorized Groups? results can be found in
Figure 3 and Table 5. For each of our tests we vary
the value of T (the number of rounds) to see how
it affects our accuracy.
Here we see we have a maximum F score of
0.7039 for T = 25. This is a very high result,
performing far better than all our baselines, how-
ever we feel the ?basketing? mentioned in Section
6 means that the results are not particularly in-
structive ? instead of specific ?interesting? tag at-
tributes, we return a grouped list of tag attributes,
only some of which are likely to be ?interesting?.
Thus we decide to no longer pursue ?grouping?
as a valid evaluation method, and evaluate all our
methods at the ?tag attribute? level.
Best ? ?
Precision 0.7620 0.7473 0.0320
CG Recall 0.6795 0.6680 0.0322
F 0.7039 0.6897 0.0106
Table 5: Categorized Groups with Best value for
T = 25.
7.2 Categorized ? Tags Level
What is notable here is that, for all values of T
which we ran our tests on (ranging from 1 to
3000), we obtained just one set of results for ?Cat-
egorized Tags?, displayed in Table 6.
This behaviour indicates that the boosting is not
helping to improve the results. Rather, it is repeat-
edly producing the same hypotheses, with vary-
ing confidence levels. The low F score is due to
135
 0.4
 0.5
 0.6
 0.7
 0.8
 1  10  100  1000
T
Unassisted Boosting
Categorized Groups
No Players
Enhanced Categorization
Figure 3: All F scores Results
? min max ?
Precision 0.0880 0.0496 0.1933 0.0223
Recall 0.7872 0.5417 1.0000 0.1096
F 0.1575 0.0924 0.3151 0.0361
Table 6: Categorized Tags Results
the very low Precision value. This method is ef-
fectively a direct application of B&L?s method to
our domain, however because of our strict accu-
racy measurement, it does not perform particularly
well. In fact it is even worse thanBmaj, our worst-
performing baseline. We believe this is because
the Majority Baseline is limited in the breadth of
tags returned, whereas this method returns very
large sets of over 200 tag attributes (due to the
many contributing tag attributes of each category)
while the average size of the training sets is 24.
Ideally we want to strike a balance between
the improved granularity of the Categorized Tags
evaluation (without the low accuracy) with the
excellent performance of the Categorized Groups
evaluation (without the too-broad basketing).
7.3 Unassisted Boosting
Our results are in Table 7 (row UB) and Figure 3.
We can see F values are increasing on the whole,
and that we have nearly reached our Probabilis-
tic Baseline. Inspecting the contents of the sets
returned by BoosTexter, we see they are slightly
more in line with a typical training set, but still suf-
fer from an over-emphasis on player names. We
also believe the high number of rounds required
for our best result (T = 2250) is caused by the
sparsity issue described in Section 4.2.
Best ? ?
Precision 0.4965 0.4730 0.0253
UB Recall 0.4961 0.4723 0.0252
F 0.4907 0.4673 0.0249
Precision 0.4128 0.3976 0.0094
NP Recall 0.4759 0.4633 0.0126
F 0.4367 0.4227 0.0091
Precision 0.4440 0.4318 0.0136
EC Recall 0.5127 0.4753 0.0271
F 0.4703 0.4467 0.0194
Table 7: Unassisted Boosting (UB), No Players
(NP) and Enhanced Categorization (EC). Best val-
ues for T = 2250, 250 and 20 respectively.
8 No-Players & Enhanced
Categorization
We now consider alternative, novel methods for
improving our results.
8.1 Player Exclusion
We have thus far ignored coherency in our data
? for example we want to make sure that player
statistics will be accompanied by their correspond-
ing player name.
One problem so far with our approach has been
that we are effectively double-counting the play-
ers. Our methods inspect which player names
should appear at the same time as finding ap-
propriate match statistics, whereas we believe we
should instead be finding relevant statistics in the
first instance, holding back player names, then in-
cluding only those players to whom the statistics
refer. Thus we restate our task in this way.
This is also sensible as in previous incarnations
the learning algorithm had been learning from the
literal strings of the player names. Although a
player could be more likely to be named for vari-
ous reasons, these reasons would not appear in the
scorecard and we feel the strings are best ignored.
Thus we decide to remove all player names
from the machine learning input, reinstating only
relevant ones once BoosTexter has selected its
chosen tag attributes.
8.2 Player Exclusion Results
As can be seen from Table 7 (row NP) and Figure
3, we have a maximum F value of 0.4367 when
T = 250, and have achieved a 3% absolute in-
crease, over ourBnops baseline, a static implemen-
tation of the above ideas.
136
8.3 Enhanced Categorization
Our final method combines the ideas of Section
8.1 above with the benefits of categorization, and
handles data sparsity issues.
The method is identical to that of Section 3.1,
with two important exceptions: The first is that
we reintroduce player names after the learning, as
above. The second is that instead of just a bi-
nary include/don?t-include decision for each tag
attribute, we offer a list of verbalised tag attributes
to the learner, but anonymising them with respect
to the group in which they appear. This enables
the learner to, given any group, predict which tag
attributes should be returned, independent of the
group in question. This means groups with often-
empty tag values are able to leverage the informa-
tion from groups with usually populated tag val-
ues, hence solving our data-sparsity issues. For
example, this will solve the issue, referenced in
Section 4.2 of a lack of training data for particular
player-combination partnerships.
Having held back the group to which the tag at-
tributes belong, we reintroduce them enabling dis-
covery of the original tag attribute. This offers the
benefits of categorization, but with a finer-grained
approach to the returned sets of tag attributes.
8.4 Enhanced Categorization Results
Our results are in Table 7 (row EC) and Figure
3. We achieved our best F score result of 0.4703
for a relatively low value of T = 20, and we can
clearly see that boosting establishes a reasonable
ruleset after a small number of iterations ? we be-
lieve we have resolved the issue of data sparsity.
The fact that this grouping has improved our re-
sults compared to feeding the information in ?flat?
(as in Section 7.3) emphasises that the construc-
tion and make-up of the categories play a key role
in defining performance.
9 Conclusions & Future Work
This paper has presented an exploration of various
methods which could prove useful when select-
ing content given a partially structured database
of statistics and output text to emulate. We be-
gan by acquiring the necessary domain data, in the
form of scorecards and reports, and employed a
six-step process to align scorecard statistics ver-
balised in the reports. We next categorised our
statistics based on the scorecard format. We es-
tablished three baselines ? one ?unthinking? proba-
bilistic baseline, a ?sensible? probabilistic one, and
another using categorization.
We found that unassisted boosting actually per-
formed worse than our comparable probabilistic
baseline, Bprob, but its output was marginally
more in line with the typical training data. We
explored how categorization affected our results,
and showed that by grouping similar sets of tag
attributes together we achieved a 7.4% improve-
ment over the comparable baseline value, Bnops
(Table 4). We further improved this technique in
a novel way by sharing structural information be-
tween learning instances, and by holding back cer-
tain information from the learner. Our final best F-
value marked a relative 15.7% increase on Bnops.
There are multiple avenues still available for ex-
ploration. One possibility would be to further in-
vestigate the effects of categorization from Section
3.2, for example by varying the size and number of
categories. We would also like to apply our meth-
ods to another domain (e.g. rugby games) to es-
tablish the relative generality of our approach.
Acknowledgments
This paper is based on Colin Kelly?s M.Phil. thesis, written
towards his completion of the University of Cambridge Com-
puter Laboratory?s Computer Speech, Text and Internet Tech-
nology course. Grateful thanks go to the EPSRC for funding.
References
Regina Barzilay and Mirella Lapata. 2005. Collective Con-
tent Selection for Concept-To-Text Generation. In HLT
?05, pages 331?338. Association for Computational Lin-
guistics.
Jose Coch. 1998. Multimeteo: multilingual production of
weather forecasts. ELRA Newsletter, 3(2).
Cricinfo. 2007. Wisden Almanack.
http://cricinfo.com/wisdenalmanack. Retrieved 28
April 2007. Registration required.
Pablo A. Duboue and Kathleen R. McKeown. 2003. Statis-
tical Acquisition of Content Selection Rules for Natural
Language Generation. EMNLP ?03, pages 121?128.
Ehud Reiter and Robert Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University Press.
Jacques Robin. 1995. Revision-based generation of natu-
ral language summaries providing historical background:
corpus-based analysis, design, implementation and evalu-
ation. Ph.D. thesis, Columbia University.
Robert E. Schapire and Yoram Singer. 2000. BoosTexter:
A boosting-based system for text categorization. Machine
Learning, 39(2/3):135?168.
Yiming Yang. 1999. An evaluation of statistical approaches
to text categorization. Information Retrieval, 1(1/2):69?
90.
137
