R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 588 ? 599, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Why Is Zero Marking Important in Korean? 
Sun-Hee Lee1,*, Donna K. Byron2, and Seok Bae Jang3 
1,2 395, Dreese Lab., 2015, Neil Avenue,Columbus, OH 43210 
shlee@ling.ohio-state edu 
dbyron@cse.ohio-state.edu 
3 37th and O Sts., NW, Washington, D.C, 20057 
sbj3@georgetown.edu 
Abstract. This paper argues for the necessity of zero pronoun annotations in 
Korean treebanks and provides an annotation scheme that can be used to de-
velop a gold standard for testing different anaphor resolution algorithms. Rele-
vant issues of pronoun annotation will be discussed by comparing the Penn Ko-
rean Treebank with zero pronoun mark-up and the newly developing Sejong 
Teebank without zero pronoun mark-up. In addition to supportive evidence for 
zero marking, necessary morphosyntactic and semantic features will be sug-
gested for zero annotation in Korean treebanks.   
1   Introduction 
This paper discusses the importance of zero pronoun marking in treebanks and investi-
gates what kind of linguistic features are needed for treebank annotation in order to 
increase the usability of annotated corpora. Zero pronouns refer to empty pronouns 
without phonological realization, which work in a similar manner as English pronouns.  
In the recent decade, there has been remarkable progress in the realm of building 
large corpora in Korean and applying them for linguistic research and natural lan-
guage processing. Based on the broad acknowledgement of the importance of corpus 
and applicative tools, the 21st century Sejong project was launched in 1998 and has 
been developing various database and relevant computational tools including elec-
tronic dictionaries, annotation tools, morphological analyzers, parsers, etc. As a part 
of the Sejong project, the syntactically annotated treebank of Korean has been under 
construction. In addition to the Sejong Treebank (henceforth, ST), the Penn Korean 
Treebank (Han et al[4] henceforth, PKT) has already been released and continues to 
be expanded. These treebanks with abundant linguistic information are expected to 
fulfill a function as informative databases in broad domains of theoretical linguistics 
and computational linguistics such as statistical approaches, machine learning, etc. 
A notable point is that there is a critical difference between annotations of ST and 
PKT with respect to marking zero elements including traces, zero pronouns, etc. The 
most current guidelines of ST specify that zeros are dropped in order to maintain the 
                                                          
*
  This work was supported by the Korea Research Foundation Grant (KRF-2004-037-A00098) 
for the author. 
 Why Is Zero Marking Important in Korean? 589 
consistency and efficiency of the treebank. In contrast, PKT advocates for  
representing zero elements. According to different approaches to zero marking, the 
structure of the following sentence (1a) is differently analyzed as in (1b) and (1c); in 
(1b) ST does not contain any missing subject while PKT marks the missing subject 
and object as pros in (1c)1 
(1) a. ???         12?-?      ?-?-???.  
          eceypam        12 si-ey       pat-ass-supnita.  
          last night        12 o?clock-at   receive-Past-E 
         ?Last night at 12 o?clock, (I/(s)he/they) received (it)?.   
      b. ST:  (VP   (AP    ???/MAG)  
                            (VP  (NP_AJT 12/SN + ?/NNB + ?/JKB) 
                                     (VP ?/VV+?/EP+???/EF.+ ./SF ))) 
      c. PKT :(S   (NP-SUBJ  *pro*) 
                           (VP  (NP-ADV ???/NNC) 
                                   (NP-ADV 12/NNU ?/NNX+?/PAD) 
                                   (VP  (NP-OBJ  *pro*) 
                                           ?/VV+?/EPF+???/EFN))) ./SFN). 
The sentence representation of (1b) does not fully present the subject-predicate re-
lation in contrast with (1c). In this paper, we argue that failure to mark zeros may 
cause a loss of valuable linguistic information such as filler-gap dependencies, argu-
ment-predication relations, semantic and discourse interpretations of sentences, etc. 
The ST style zero-less annotation will impose the burden of zero marking on the post-
annotation tasks, which utilize treebank resources for developing computational tools. 
This, however, is inconsistent with the purpose of developing treebanks. As pointed 
out in Dickinson & Meurers [3], treebanks have major usage for two types of lin-
guists; one is for theoretical linguists who search through the corpora in order to iden-
tify certain linguistic patterns. The other is for computational linguists who use com-
putational technology and develop statistical models from the annotated corpora in 
order to develop parsers and question-answer systems and to extract information such 
as subcategorization frames of predicates, event nouns, complex predicates, etc. In 
general, treebanks are manually or semi-manually annotated by humans. This guaran-
tees more sophisticated representations of sentence structure and reliable mark-ups for 
ambiguous morphosyntactic units. While focusing on the usability of treebanks, we 
propose an argument against dropping zero mark-ups in treebanks and investigate the 
empirical necessity of zero annotation in Korean treebanks.  
In this paper, we will discuss some significant problems of zero-less treebank 
annotation and explain why zero annotation is important in languages like Korean. 
Then we will present a general annotation scheme and features of zero pronouns 
that can be used to develop a gold standard for testing an anaphor resolution algo-
rithm. Adding zero mark-up will solidify accurate syntactic representation and in-
crease the usability of treebanks even though it takes strenuous efforts and time  
for development. 
                                                          
1
  The tagsets of ST and PKT are somewhat different (i.e., MAG represents an adverb, AP, an 
adverbial phrase, and NP-ADV , a nominal functioning as an adverbial modifier in ST). 
590 S.-H. Lee, D.K. Byron, and S.B. Jang 
2   Necessity of Zero Annotation in Korean Treebank 
In contrast with English where a repeated element tends to appear as a pronoun, in 
topic prominent languages like Korean, a repeated element has no surface realization. 
Thus, Korean zero elements are often called zero pronouns. 
(2) a. John met Mary yesterday.       
      b. Kim met her, too.  
(3) a. John-i           eycey          Mary-lul    mannassta.  
          John-Nom   yesterday  Mary-Acc  met 
         ?John met with Mary yesterday. . 
      b. Kim-to       ?       ,mannaassta.   
          Kim also    OBJ    met 
          ?Kim also met (zero=her).? 
The discrepancy between ST and PKT with respect to zero annotation brings us 
two different values with respect to corpus annotation; economy vs. usability. At the 
stage of annotating corpora, excluding all the missing subjects from the Korean tree-
banks may reduce the burden of annotation tasks such as classifying zeros, sorting 
markable zeros, training annotators and maintaining the legitimate level of inter-
annotator agreement. However, at the later stage zero marked treebanks have higher 
usability by higher level processing including anaphor resolution, extracting subcate-
gorization frames of predicates, discourse analysis, etc. 
More specifically, our arguments against zero-less treebanks can be presented as 
follows. First, zero-less treebanks may provide misleading representations with re-
spect to the general patterns of sentence realization. In so-called pro-drop languages 
such as Korean, Japanese, Spanish and Portuguese, basic units of sentence structure, 
such as subjects of matrix clauses, are frequently unrealized. Although missing sub-
ject information in languages like Spanish and Portuguese is recoverable from verb 
morphology, interpretations of missing arguments do not correspond to specific verb 
morphology in Korean. Thus, marking the place of a zero element is an inevitable 
process not only for structural representation but for processing the meaning of a 
sentence. Zero-less treebanks license various VP or S nodes without capturing correct 
argument-predicate relations. For example, the following sentence is simply repre-
sented as VP, which is inconsistent with the subcategorization frame of the main verb. 
(4) ??        ???     ???-?             ???-?  ?-?-???.  
       kunyang camcakho wuntongcang-man naytapo-ko  iss-ess-supnita.  
       just         silently      playground-only    look down-PreP-Past-E  
       ?(I/you/he/she/they) was only looking down the playground just silently.? 
     (VP  (AP ??/MAG)  
            (VP   (AP ???/MAG)  
                      (VP   (NP_OBJ ???/NNG + ?/JX)  
                               (VP   (VP ???/VV + ?/EC)  
                                         (VP ?/VX + ?/EP + ???/EF + ./SF))))) 
According to Hong [6], the rate of subject drop is 57% in spoken Korean, which is 
higher than other elements. In particular, when the subject refers to a nominal entity 
mentioned in the previous utterance, it naturally disappears in speech rather than ap-
 Why Is Zero Marking Important in Korean? 591 
pearing as a pronoun. This suggests that the number of VPs lacking subjects will be 
significantly high in the spoken corpora. We extracted only 100 sentences from the 
ST corpus containing natural spoken conversations and found that 81 sentences are 
represented as VPs or VNPs (predicate nominal phrases). However, it may derive a 
misleading generalization such that canonical sentence patterns in the given corpus 
are VPs or VNPs. In line with this, semantic interpretations of those incomplete VPs 
or VNPs subsume the meaning of the zero pronouns whose antecedents appear in the 
previous utterances. However, zero-less mark-up poses a difficulty in retrieving the 
complete sentential meaning from the given phrasal categories of VPs or VNPs.  
Second, zero-less treebanks make it difficult to extract certain constructions that 
linguists want to identify. For example, one of the most frequently discussed topics in 
Korean grammar is formation of Double Subject Constructions (DSCs), which license 
two subjects. However, zero-less treebanks do not correctly represent Double Subject 
Constructions and represent (5) and (6) differently in spite of their similarity in  
argument realization.  
(5) ??-?                   ?-?         ?       ?-?-???.  
      hayspam-i                mas-i          ssek    choh-ass-supnita.  
      new chestnut-Nom  taste-Nom   quite   good-Past-End 
      ?New chestnuts had pretty good taste.?  
    (S  (NP_SUB ??/NNG + ?/JKS)  
          (S  (NP_SBJ ?/NNG + ?/JKS)  
                (VP  (AP ?/MAG)  
                        (VP ?/VA + ?/EP + ???/EF + ./SF))))  
(6)  ???             ?-?        ?       ?-?-???.  
       yunanhi            mas-i         ssek    choh-ass-supnita.  
       particularly      taste-Nom  quite   good-Past-End 
     ?Particularly, the taste of (it) was pretty good.?  
    (S  (AP  ???/MAG) 
          (S (NP_SBJ ?/NNG + ?/JKS)  
                 (VP  (AP ?/MAG)  
                         (VP ?/VA + ?/EP + ???/EF + ./SF)))) 
According to the analysis of ST, (5) is represented as a DSC that licenses two sub-
jects, hayspam and mas. In contrast, (6) is represented as a complex clause that only 
licenses a single matrix subject, mas and the first zero subject referring to the same 
nominal entity in the preceding phrase has been ignored in the sentential representa-
tion. It is difficult to extract certain syntactic patterns from the zero-less treebanks 
because their structural representations do not reflex the accurate argument-predicate 
realization. It is because they focus on surface realization of arguments instead of 
considering lexical constraints of argument-predicate relations.   
The third critical problem of zero-less treebanks is related to discourse analysis. 
Unrealized arguments are important for tracking the attentional state of a discourse in 
topic-oriented languages like Korean and Japanese. Within the framework of center-
ing theory, e.g. Walker et al [9], Iida [7]), Hong [6], etc. it has been shown that a 
salient entity recoverable by inference from the context is frequently omitted, and 
therefore interpreting these zero pronouns allows one to follow the center of the atten-
tional state. Walker et al [9] applied the centering model, developed for pronoun 
592 S.-H. Lee, D.K. Byron, and S.B. Jang 
resolution in English, to zero pronoun resolution in Japanese. They argue that inter-
pretation of a zero pronoun is determined by discourse factors. This suggests that 
identifying occurrences of zero pronouns and retrieving their antecedents are impor-
tant in developing a computational model of discourse interpretations as well as syn-
tactic and semantic analyses. When it comes to topic information retrieval, the salient 
element under the discussion of the given discourse is realized as a zero. Grammatical 
roles and semantic restrictions provide crucial cues for the interpretations of them. 
However, without specifying the argument positions of these zeros, discourse proc-
essing of the given utterances is impossible. 
3   Relevant Issues of Zero Annotation 
Zero marked treebanks function as useful resource for researchers, especially the 
anaphora resolution community. For developing computational tools of anaphor reso-
lution, it is necessary to determine the distribution of zero pronouns and their link to 
other discourse properties. There has historically been a lack of annotated material 
available to the wider research community that would allow us to investigate these 
questions. Researchers in the past worked mainly with small amounts of hand-
constructed data rather than being able to do large-scale corpus analysis. This lack has 
been recently pointed out by Lee et al [8] evaluating the Penn Korean Treebank (Han 
et. al. [4]), which includes annotations indicating the position of zero pronouns. In 
PKT, annotations of zeros are problematic due to inconsistent mark-up for zero pro-
nouns and structural representation of trees. Inconsistent annotation of zero pronouns 
in PTK brings an imminent issue for developers of Treebanks and other annotated 
language resources; when and how should these unrealized elements be explicitly 
introduced into the linguistic material being developed? Unless these questions are 
resolved, treebanks cannot fulfill their potential as a source of linguistic knowledge 
about zero pronouns. Also, the same question should be taken into consideration by 
other teams developing similar resources in other languages.  
3.1   Argument vs. Adjunct 
Previous authors have pointed out that the antecedents of zero pronouns can often be 
determined by using various grammatical properties such as topicality, agreement, 
tense, and aspect as well as subcategorization information (Walker et al [9]; Iida [7]; 
Hong [6],  etc.). However, in order for these factors to be useful in developing anaph-
ora resolution algorithms, they must be reliably and consistently annotated into the 
source data. Thus, the first crucial step for zero pronoun resolution is identifying the 
exact positions of zero pronouns. Determining the positions of invisible zeros is a 
difficult task. This process needs to refer to the argument realization in a given utter-
ance and the previous utterances of the same discourse unit. The argument realization 
of a sentence is based upon argument structure of a predicate.  
(7) a.  John-i  .       kesil-eyse           swi-ko  iss-ess-ta? 
          John-Nom    living room-in    rest-Pres Prog-Past-E 
         ?John was resting in the living room.? 
      b.  sakwa-lul  mek-ess-ta.  
           apple-Acc  eat-past-E 
          ?(He) ate an apple.? 
 Why Is Zero Marking Important in Korean? 593 
In (7b), the argument structure of mekta ?eat? suggests that the subject is missing in 
a sentence sakwalul mekesse ?ate an apple?. However, do we need to mark the adjunct 
kesileyse  ?in the living room? in (7b)? In the given utterances, it seems to be possible 
for John to have eaten the apple in the living room but it is not necessarily true. The 
combinations of adjunct and predicate are not predictable by using argument structure 
of a predicate. With no specific guideline, identifying missing adjuncts complicates 
the annotation process. Thus, we argue that only missing arguments must be marked. 
As for zero argument annotation, the current annotation in PKT is somewhat prob-
lematic due to unclear distinction of obligatory argument vs. optional argument. Ac-
cording to the guidelines of PKT, only a missing obligatory argument should be anno-
tated as an empty element. Missing optional arguments and adjuncts are not. Thus, in 
PKT, missing subject or object elements were marked as zeros while missing locative 
arguments were not marked when they were omitted. However, the annotation 
method based on an obligatory vs. optional argument may result in the loss of crucial 
information needed at later stage of retrieving an antecedent of a zero element. For 
example, the locative argument, ?the 45th division-in? has not been marked up as a 
zero pronoun in the tagged sentence of (8b)  
(8) a: ?     45   ???  ?     ??-??  ????  ??? ? 
          the   45   division  again what-with   composed  be  
            ?What is the 45th Division composed of ??  
        (S  (NP-SBJ ?/XPF+45/NNU 
                            ??/NNC+?/PAU) 
              (VP (VP (ADVP ?/ADV) 
                              (VP (NP-COMP ??/NPN+??/PAD) 
                                      (VV ??/NNC+?/XSV+?/EAU))) 
                                         ?/VX+??/EFN)  ?/SFN) 
         b: ??          ???-?     ???? . 
              division    head-Nom      exist 
             ?The head division is (there). 
        (S (NP-SBJ ??/NNC 
                            ???/NNC+?/PCA) 
              (ADJP ?/VJ+???/EFN).  /SFN) 
In the given discourse segments, the adjective ?? issta requires a locative argu-
ment which has been treated as an optional argument in PKT. Thus, the information 
of the missing locative has not been represented even though it is crucial for retriev-
ing the meaning of the sentence. Another concern with respect to distinction of argu-
ment and adjunct is that ST classifies only subject and object as arguments and ex-
cludes other case-marked nominals as adjunct2. This classification may cause prob-
lems when zero pronouns are added in their treebanks.  
In identifying missing zero arguments, maintaining consistency is crucial. For this 
task, we can rely on a dictionary containing constant argument structure of predicates. 
Dictionaries with specific argument structure information can be used here, such as 
                                                          
2
  In addition to subject and object, nominals in front of predicates, toyta, anita and quotation 
clause have been included as arguments.  
594 S.-H. Lee, D.K. Byron, and S.B. Jang 
the Yonsei Korean dictionary, where different subcategorization frames are listed 
according to semantically disambiguated senses for each predicate. For correct identi-
fication of a zero pronoun in the given utterance, annotators need to examine the rele-
vant previous utterances in the same discourse unit and determine the exact verb sense 
of the relevant predicate by using the dictionary. In addition, checking inter-annotator 
agreement is also an essential task (Carletta [2]).  
3.2   Language Specific Properties of Korean 
Another notable point is that the developers need to pay attention to language specific 
properties of Korean. There are some notable morphosyntactic properties of Korean 
with respect to zero pronoun annotation. In order to maintain constant annotation of 
zero pronouns, it is important to carefully represent specific features related to zero 
pronouns. In this section, we will discuss specific properties that can be added for 
zero pronoun annotation. It will increase the applicability of the treebank to both 
theoretical research on anaphors and computational modeling of anaphor resolution. 
[1]   CASE MARKING  
In determining an antecedent of a zero pronoun, the existence of topics plays an im-
portant role in Korean. In the previous theoretical literature, it has been commonly 
assumed that the topic marked elements appear at the higher phrasal level than the 
phrasal combination of subject-predicate. At the discourse level, Walker et al [9] and 
Iida [7] provide evidence that topic marked elements function as antecedents of zero 
pronouns in Japanese. The similar property has been also observed with Korean by 
Hong [6]. As seen in the following examples, the sentence-initial topic functions as 
the antecedent of zero pronouns that appear in the latter utterances. This phenomenon 
suggests that the topic marker needs to be differentiated from other postpositions and 
that grammatical topics are to be differentiated from other grammatical arguments 
like subjects and objects.  
(9) a  Seyho-nun apeci-eykey   chingchan-ul pat-ca    ekkay-ka ussukhayci-pnita.  
          Seyho-Top  father-to        praise-Acc     receive-E be proud of 
        ?As for Seyho, he felt pride when he received praise from father.?  
      b. ?  20 ilman-ey  tut-nun   chingchan-ila kippum-un   hankyel tehaysssupnita  
               20days-in    hear-Rel  praise-since    pleasure-Top  far         more  
       ?Since it was the praise (he) heard for the first time in 20 days, his pleasure     
          was much more.?  
      c. ?  emeni-eykey-nun  nul         kkwucilam-kwa   cansoli-man   tulesssupnita.    
              mother-to-Top      always    scolding-and        lecture-only    heard 
       ?From his mother, (he) always heard only scolding and lecture.? 
In general, while the marker nun functions as a topic marker in a sentence initial 
position, it also works as an auxiliary postposition in a non-initial position of a sen-
tence in Korean. The first is classified as a grammatical topic marker while the latter 
is a contrastive topic marker in traditional Korean grammar. However, the current 
annotations of PKT and ST treat topic marker nun as the same auxiliary postposition, 
which is similar to other postpositions man ?only?, to ?also?, and mace ?even?. In par-
ticular, PKT and ST represent a subject NP with a topic marker as the subject, while 
 Why Is Zero Marking Important in Korean? 595 
an object with a topic marker is treated as a scrambled argument out of its canonical 
position. With respect to zero pronouns, the sentence initial topic marker needs to be 
distinctly marked from other postpositions. In addition, we claim that the structural 
position of a topicalized subject needs to be differentiated from a normal subject posi-
tion in parallel with a topicalized object and other element, which leave zero traces in 
their original positions. 
Another problem with case markers is subject marker -eyse, which only combines 
with nominals referring to a group or organization. In Korean, these group nominals 
do not take the nominative case i/ka but the case marker -eyse as in (12)  
(10) wuli hakkyo-eyse  wusung-ul         hayssta.  
        our school-Nom    winning-Acc     did 
        ?Our school won.? 
Although -eyse has been treated as a nominative case marker in traditional Korean 
grammar, both PKT and ST do not treat -eyse as a nominative marker. Instead, group 
or organization nominals with the case marker-eyse  are analyzed as NP adverbial 
phrases. This, however, mistakenly licenses a zero subject in the following example 
of PKT even though the subject with case marker -eyse exists. In order to eliminate 
redundant marking for the zero subjects, it is better to analyze the case marker, -eyse 
as a nominative case marker in Korean. 
(11) 2 ??-??          ??       ???-?               ???-?      ?-?? 
        2 taytay-eyse         etten        mwucenmang-ul     wunyonha-ko iss-ci? 
        2 squadron-Nom   which      radio network-Acc  use-PresP-Q 
       ?What kind of radio network is the 2nd squadron using??             
       (S  (NP-ADV  2/NNU 
                             ??/NNC+??/PAD) 
       (S     (NP-SBJ *pro*) 
                (VP (VP (ADVP ?/ADV) 
                                (VP (NP-OBJ ??/DAN 
                                                      ???/NNC+?/PCA) 
                                        (VV ??/NNC+?/XSV+?/EAU))) 
                                ?/VX+?/EFN) ?/SFN) 
[2]   SUBJECTLESS CONSTRUCTIONS  
Unlike English having expletive pronouns it or there, certain predicate constructions 
do not license subject positions at all in Korean. Some examples are presented in 
(12),which include incomplete predicates with few inflectional forms.  
(12) -ey tayhaye, ?regarding on?-ey kwanhaye, ?about? -ey uyhaye, ?in terms of?  
        -lo inhayse  ?due to~?, -wa tepwule  ?with ~? etc.  
In addition, some modal auxiliary verbs like sangkita, poita, toyta, etc. do not license 
subject positions and have already been classified as subjectless constructions in Ko-
rean grammar. While ST treats these modal verbs to be included in the preceding 
verbal clusters, PKT separates them from the preceding verbs and assigns zero sub-
jects for these verbs. Thus, the PKT approach redundantly assigns zero subjects for 
subjectless predicates.  
596 S.-H. Lee, D.K. Byron, and S.B. Jang 
[3]   VERBAL MORPHOLOGY OF SPEECH ACT  
As for zero pronoun resolution, verbal suffixes representing speech act can be a useful 
source. Thus, we argue for adding these morphosyntactic features in treebanks. It has 
been well known that in Korean certain speech acts such as declaration, request, ques-
tion, promise, etc. are associated with verb morphology; five different types of verbal 
inflections are used to indicate declaratives, interrogatives, imperatives, propositives, 
and exclamatives. Information of a missing subject can be retrieved from verbal mor-
phology. For example, the imperative verbal endings suggest that a missing subject 
refers to the hearer while promising verbal endings imply that a missing subject is the 
speaker. Thus, the missing subjects of the following examples are respectively inter-
preted as I, you and we based on the verbal suffixes representing a particular speech act.    
(13) a. ?   ka-llay. (Question) 
                 go-Q 
           ?Do (you) want to go?? 
        b. ?   ka-llay. (Declaration) 
                  go-will 
            ?(I) will go.?   
        c. ?    ka-ca. (Request) 
                   go-let?s 
             ?Let?s go.? 
Verbal endings of speech acts can be used to enhance the process of determining 
an antecedent of a zero pronoun subject. In the current annotations of PKT and ST, 
verbal suffixes do not subclassify the final endings. We argue that annotating the five 
classes of verbal suffixes differently will facilitate application of anaphor resolution 
algorithms on treebanks.   
[4]   WH-PRONOUN TAGGING  
Wh-pronouns in Korean include nwuka ?who?, mwues ?what?, encey ?when?, etise 
?where?, way ?why?, ettehkey ?how?, etc. Unfortunately, wh-pronouns are not dis-
tinctly tagged from other pronouns in the PKT and the ST. The information of wh-
pronouns can be useful for resolving the meaning of zero pronouns in the next an-
swering utterance As seen in (14b), a fragment directly related to a wh-pronoun nec-
essarily appears in the answering utterance while non-wh-elements previously men-
tioned are easily dropped.  This is because pairs of wh-question-answer tend to have 
the same predicates with the same argument structure. Therefore, answering utter-
ances of the wh-questions generally contain zero pronouns, whose antecedents appear 
in the preceding questioning utterances. 
(14) A:  John-i            Min-ul         mwe-la-ko               mitko  iss-ni?  
              John-Nom     Min-Acc      what-Co-Comp       believe being-Q 
             ?What does John believe Min to be?? 
        B:  ?          ?             kyoswu-la-ko          mitko   iss-nuntey.  
             SUBJ    OBJ         professor-Co-Comp   believe  being-END 
            ?(He) believes (her) to be a professor.?   
 Why Is Zero Marking Important in Korean? 597 
4   New Annotation Scheme of Korean Zero Pronouns  
Once zeros are identified by argument structure information of predicates and the 
previous utterances in the given discourse, the additional reference information can be 
added in treebanks to support anaphor resolution. Zeros in Korean can be classified 
into different classes according to properties of their reference. Anaphor resolution 
algorithms can be applied for certain types of pronouns. For example, in order to 
retrieve the meaning of a zero pronoun referring to a nominal entity in the previous 
utterance, the resolution algorithm will search nominal entities that appear in the pre-
vious utterance by making a list of antecedent candidates and selecting the most ap-
propriate candidate. In contrast, the searching algorithm does not need to apply for a 
zero element referring to an indefinite entity as in (15). 
(15) ?   holangi    kwul-ey      ka-ya    holangi-lul      capnunta.  
                  tiger         den-to         go-E     tiger-Acc        catch 
 (Lit.)    ?One should go to the tiger?s den in order to catch a tiger.  
     (Trans.)?Don?t hesitate but pursue what you need to do.? 
According to reference relation between a zero pronoun and its antecedent, zero 
pronouns in Korean can be divided into three classes as in Table 1; discourse ana-
phoric zeros, deictic and indexical zeros, and indefinite zeros. 
In the given classification, discourse anaphoric zeros take their reference from an-
tecedents in the previous utterances in the given discourse. This class is the main one 
that anaphor resolution systems aim to handle. The discourse anaphoric zeros can be 
divided into three subclasses according to the semantic properties of their antecedents. 
Table 1. Classification of Korean Zero Pronouns 
     Individual Entities 
Propositions 
 
Discourse Anaphoric Zeros 
Eventualities  
Deictic and Indexical Zeros 
Indefinite Zeros 
The first subclass of discourse anaphoric zeros refers to individual domain entities, 
the second, eventualities, and the third, propositions. The zeros of individual entities 
refer to entities that were introduced into the discourse via noun phrases. Most exam-
ples presented in the previous sections correspond to this class. The zeros of proposi-
tions refer to propositions introduced in the previous utterance as in (16). 
 (16) A: 108 yentay   cihwipwu-nun    hyencay eti-ey wichihako issnun-ka? 
             108 regiment   headquarter-TOP  now  where-at locate   being-Q 
             ?Where is the headquarter of the 108th regiment located?? 
        B:  ?1      ?2      molukeyss-supnita.  
              SUBJ   OBJ     not know-END 
             ?I don?t know.?    
      (?1 = ?B?, ?2= ?Where the headquarter of the 108th regiment is located ?) 
598 S.-H. Lee, D.K. Byron, and S.B. Jang 
The third class of zero anaphors referring to eventualities, i.e. action and event as 
in (17) (Asher [1]).    
 (17) A: Mary-ka              cip-ey -   ka-ko          sipheha-ci  anha.  
             Mary-Nom          home-to go-E           want-END  don?t 
            ?Children don?t want to go home.? 
        B: na-to      ?          silhe.       
             I-also                  hate 
            ?I also hate to go home.?       ( ? = the action of going home) 
The second class of zero pronouns includes deictic and indexical zeros that directly 
refer to entities that can be determined in the given spatiotemporal context, which 
generally include a speaker and an addressee. The third class includes indefinite zeros 
referring to general people, which corresponds to they, one, and you in English.  
Given the classification of zero pronouns, different coding systems can be provided 
for each class for annotating these elements. According to different classes of zeros, the 
resolution process varies. Zero anaphors of discourse anaphoric entities will be marked 
the same as their antecedents in the previous utterances. Anaphor resolution algorithms 
determine the antecedent of a zero anaphor by searching through the antecedent candi-
dates in different orders. Deictic and indexical zeros are dependent on discourse partici-
pants. In general, a zero anaphor can also refer to the speaker or the hearer. Overlapping 
mark-up for these zeros need to be allowed although resolution mechanisms for deictic 
and indexical zeros are different from those for anaphors. Indefinite zeros need to be 
marked but anaphor resolution algorithms do not need to be applied to them. 
5   Conclusion 
In this paper, we discussed why zero marking is necessary for Korean treebanks and 
how invisible zeros can be consistently marked in annotated corpora like treebanks. The 
importance of zero mark-up in Korean treebanks has been discussed with respect to 
correct linguistic analysis and efficient application of computational process. We also 
claimed that only missing arguments are marked as zeros and a dictionary like Yonsei 
Dictionary with full specification of argument-predicate relations can be a useful source 
for the annotation task. By examining PKT and the newly developing ST, we deter-
mined four linguistic features that are useful for anaphor resolution in Korean; case 
marking, subjectless construction, verb morphology of speech acts and wh-pronoun 
tagging. In addition, we provided a new annotation scheme that can be utilized for anno-
tating treebanks and testing anaphor resolution algorithms with annotated corpora.  
References  
1. Asher, N.:. Reference to Abstract Objects in Discourse. Kluwer Academic Publishers. 
(1993). 
2. Carletta, J. Assessing Agreement on Classification Tasks: the Kappa Statistic, Computa-
tional Linguistics 22(2) (1996) 249-254. 
3. Dickinson, M. and Meurers, D.: Detecting Inconsistencies in Treebanks in Proceedings of 
the Second Workshop on Treebanks and Linguistics Theories.(TLT 2003)..V?xj?.  
Sweden. (2003) 
 Why Is Zero Marking Important in Korean? 599 
4. Han, C-H., Han, N-R., Ko, E-S.and Palmer, M.: Development and Evaluation of a Korean 
Treebank and Its Application to NLP.in Proceedings of the 3rd International Conference on 
Language Resources and Evaluation (LREC).(2002) 
5. Han, N-R.: Korean Null Pronouns: Classification and Annotation in Proceedings of the 
ACL 2004 Workshop on Discourse Annotation. (2004) 33-40. 
6. Hong M.: Centering Theory and Argument Deletion in Spoken Korean. The Korean Jour-
nal  Cognitive Science. Vol. 11-1 (2000) 9-24. 
7. Iida, M.: Discourse Coherence and Shifting Centers in Japanese texts in Walker, M., Joshi 
A.K., Prince E.F. (Eds.) Centering Theory in Discourse. Oxford University Press, Oxford: 
UK..(1998) 161-182. 
8. Lee, S., Byron, D., and Gegg-Harrison, W.: Annotations of Zero Pronoun Resolution in 
Korean Using the Penn Korean Treebank in the 3rd Worksop on Treebanks and Linguistics 
Theories (TLT 2004). T?bingen. Germany. (2004)  75-88. 
9. Walker, M., Iida, M., .Cotes, S.: Japanese Discourse and the Process of Centering in Com-
putational Linguistics, Vol. 20-2.: (1994.) 193-232 
10. 10. Dictionary   Yonsei Korean Dictionary. (1999) Dong-A Publishing Co. 
11. Guidelines of the Sejong Treebank. Korea University 
Proceedings of the NAACL HLT 2010 Sixth Web as Corpus Workshop, pages 8?16,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Building a Korean Web Corpus for Analyzing Learner Language
Markus Dickinson
Indiana University
md7@indiana.edu
Ross Israel
Indiana University
raisrael@indiana.edu
Sun-Hee Lee
Wellesley College
slee6@wellesley.edu
Abstract
Post-positional particles are a significant
source of errors for learners of Korean. Fol-
lowing methodology that has proven effective
in handling English preposition errors, we are
beginning the process of building a machine
learner for particle error detection in L2 Ko-
rean writing. As a first step, however, we must
acquire data, and thus we present a method-
ology for constructing large-scale corpora of
Korean from the Web, exploring the feasibil-
ity of building corpora appropriate for a given
topic and grammatical construction.
1 Introduction
Applications for assisting second language learners
can be extremely useful when they make learners
more aware of the non-native characteristics in their
writing (Amaral and Meurers, 2006). Certain con-
structions, such as English prepositions, are difficult
to characterize by grammar rules and thus are well-
suited for machine learning approaches (Tetreault
and Chodorow, 2008; De Felice and Pulman, 2008).
Machine learning techniques are relatively portable
to new languages, but new languages bring issues in
terms of defining the language learning problem and
in terms of acquiring appropriate data for training a
machine learner.
We focus in this paper mainly on acquiring data
for training a machine learning system. In partic-
ular, we are interested in situations where the task
is constant?e.g., detecting grammatical errors in
particles?but the domain might fluctuate. This is
the case when a learner is asked to write an essay on
a prompt (e.g., ?What do you hope to do in life??),
and the prompts may vary by student, by semester,
by instructor, etc. By isolating a particular domain,
we can hope for greater degrees of accuracy; see,
for example, the high accuracies for domain-specific
grammar correction in Lee and Seneff (2006).
In this situation, we face the challenge of obtain-
ing data which is appropriate both for: a) the topic
the learners are writing about, and b) the linguistic
construction of interest, i.e., containing enough rel-
evant instances. In the ideal case, one could build
a corpus directly for the types of learner data to
analyze. Luckily, using the web as a data source
can provide such specialized corpora (Baroni and
Bernardini, 2004), in addition to larger, more gen-
eral corpora (Sharoff, 2006). A crucial question,
though, is how one goes about designing the right
web corpus for analyzing learner language (see, e.g.,
Sharoff, 2006, for other contexts)
The area of difficulty for language learners which
we focus on is that of Korean post-positional parti-
cles, akin to English prepositions (Lee et al, 2009;
Ko et al, 2004). Korean is an important language
to develop NLP techniques for (see, e.g., discussion
in Dickinson et al, 2008), presenting a variety of
features which are less prevalent in many Western
languages, such as agglutinative morphology, a rich
system of case marking, and relatively free word or-
der. Obtaining data is important in the general case,
as non-English languages tend to lack resources.
The correct usage of Korean particles relies on
knowing lexical, syntactic, semantic, and discourse
information (Lee et al, 2005), which makes this
challenging for both learners and machines (cf. En-
8
glish determiners in Han et al, 2006). The only
other approach we know of, a parser-based one, had
very low precision (Dickinson and Lee, 2009). A
secondary contribution of this work is thus defin-
ing the particle error detection problem for a ma-
chine learner. It is important that the data represent
the relationships between specific lexical items: in
the comparable English case, for example, interest
is usually found with in: interest in/*with learning.
The basic framework we employ is to train a ma-
chine learner on correct Korean data and then apply
this system to learner text, to predict correct parti-
cle usage, which may differ from the learner?s (cf.
Tetreault and Chodorow, 2008). After describing the
grammatical properties of particles in section 2, we
turn to the general approach for obtaining relevant
web data in section 3, reporting basic statistics for
our corpora in section 4. We outline the machine
learing set-up in section 5 and present initial results
in section 6. These results help evaluate the best way
to build specialized corpora for learner language.
2 Korean particles
Similar to English prepositions, Korean postposi-
tional particles add specific meanings or grammat-
ical functions to nominals. However, a particle can-
not stand alone in Korean and needs to be attached
to the preceding nominal. More importantly, par-
ticles indicate a wide range of linguistic functions,
specifying grammatical functions, e.g., subject and
object; semantic roles; and discourse functions. In
(1), for instance, ka marks both the subject (func-
tion) and agent (semantic role), eykey the dative and
beneficiary; and so forth.1
(1) Sumi-ka
Sumi-SBJ
John-eykey
John-to
chayk-ul
book-OBJ
ilhke-yo
read-polite
?Sumi reads a book to John.?
Particles can also combine with nominals to form
modifiers, adding meanings of time, location, instru-
ment, possession, and so forth, as shown in (2). Note
in this case that the marker ul/lul has multiple uses.2
1We use the Yale Romanization scheme for writing Korean.
2Ul/lul, un/nun, etc. only differ phonologically.
(2) Sumi-ka
Sumi-SBJ
John-uy
John-GEN
cip-eyse
house-LOC
ku-lul
he-OBJ
twu
two
sikan-ul
hours-OBJ
kitaly-ess-ta.
wait-PAST-END
?Sumi waited for John for (the whole) two hours in
his house.?
There are also particles associated with discourse
meanings. For example, in (3) the topic marker nun
is used to indicate old information or a discourse-
salient entity, while the delimiter to implies that
there is someone else Sumi likes. In this paper, we
focus on syntactic/semantic particle usage for nom-
inals, planning to extend to other cases in the future.
(3) Sumi-nun
Sumi-TOP
John-to
John-also
cohahay.
like
?Sumi likes John also.?
Due to these complex linguistic properties, parti-
cles are one of the most difficult topics for Korean
language learners. In (4b), for instance, a learner
might replace a subject particle (as in (4a)) with an
object (Dickinson et al, 2008). Ko et al (2004) re-
port that particle errors were the second most fre-
quent error in a study across different levels of Ko-
rean learners, and errors persist across levels (see
also Lee et al, 2009).
(4) a. Sumi-nun
Sumi-TOP
chayk-i
book-SBJ
philyohay-yo
need-polite
?Sumi needs a book.?
b. *Sumi-nun
Sumi-TOP
chayk-ul
book-OBJ
philyohay-yo
need-polite
?Sumi needs a book.?
3 Approach
3.1 Acquiring training data
Due to the lexical relationships involved, machine
learning has proven to be a good method for sim-
ilar NLP problems like detecting errors in En-
glish preposition use. For example Tetreault and
Chodorow (2008) use a maximum entropy classifier
to build a model of correct preposition usage, with
7 million instances in their training set, and Lee and
Knutsson (2008) use memory-based learning, with
10 million sentences in their training set. In expand-
ing the paradigm to other languages, one problem
9
is a dearth of data. It seems like a large data set is
essential for moving forward.
For Korean, there are at least two corpora pub-
licly available right now, the Penn Korean Treebank
(Han et al, 2002), with hundreds of thousands of
words, and the Sejong Corpus (a.k.a., The Korean
National Corpus, The National Institute of Korean
Language, 2007), with tens of millions of words.
While we plan to include the Sejong corpus in fu-
ture data, there are several reasons we pursue a dif-
ferent tack here. First, not every language has such
resources, and we want to work towards a language-
independent platform of data acquisition. Secondly,
these corpora may not be a good model for the kinds
of topics learners write about. For example, news
texts are typically written more formally than learner
writing. We want to explore ways to quickly build
topic-specific corpora, and Web as Corpus (WaC)
technology gives us tools to do this.3
3.2 Web as Corpus
To build web corpora, we use BootCat (Baroni and
Bernardini, 2004). The process is an iterative algo-
rithm to bootstrap corpora, starting with various seed
terms. The procedure is as follows:
1. Select initial seeds (terms).
2. Combine seeds randomly.
3. Run Google/Yahoo queries.
4. Retrieve corpus.
5. Extract new seeds via corpus comparison.
6. Repeat steps #2-#5.
For non-ASCII languages, one needs to check
the encoding of webpages in order to convert the
text into UTF-8 for output, as has been done for,
e.g., Japanese (e.g., Erjavec et al, 2008; Baroni and
Ueyama, 2004). Using a UTF-8 version of Boot-
Cat, we modified the system by using a simple Perl
module (Encode::Guess) to look for the EUC-
KR encoding of most Korean webpages and switch
it to UTF-8. The pages already in UTF-8 do not need
to be changed.
3.3 Obtaining data
A crucial first step in constructing a web corpus is
the selection of appropriate seed terms for construct-
ing the corpus (e.g., Sharoff, 2006; Ueyama, 2006).
3Tetreault and Chodorow (2009) use the web to derive
learner errors; our work, however, tries to obtain correct data.
In our particular case, this begins the question of
how one builds a corpus which models native Ko-
rean and which provides appropriate data for the task
of particle error detection. The data should be genre-
appropriate and contain enough instances of the par-
ticles learners know and used in ways they are ex-
pected to use them (e.g., as temporal modifiers). A
large corpus will likely satisfy these criteria, but has
the potential to contain distracting information. In
Korean, for example, less formal writing often omits
particles, thereby biasing a machine learner towards
under-guessing particles. Likewise, a topic with dif-
ferent typical arguments than the one in question
may mislead the machine. We compare the effec-
tiveness of corpora built in different ways in training
a machine learner.
3.3.1 A general corpus
To construct a general corpus, we identify words
likely to be in a learner?s lexicon, using a list of 50
nouns for beginning Korean students for seeds. This
includes basic vocabulary entries like the words for
mother, father, cat, dog, student, teacher, etc.
3.3.2 A focused corpus
Since we often know what domain4 learner es-
says are written about, we experiment with building
a more topic-appropriate corpus. Accordingly, we
select a smaller set of 10 seed terms based on the
range of topics covered in our test corpus (see sec-
tion 6.1), shown in figure 1. As a first trial, we select
terms that are, like the aforementioned general cor-
pus seeds, level-appropriate for learners of Korean.
han-kwuk ?Korea? sa-lam ?person(s)?
han-kwuk-e ?Korean (lg.)? chin-kwu ?friend?
kyey-cel ?season? ga-jok ?family?
hayng-pok ?happiness? wun-tong ?exercise?
ye-hayng ?travel? mo-im ?gathering?
Figure 1: Seed terms for the focused corpus
3.3.3 A second focused corpus
There are several issues with the quality of data
we obtain from our focused terms. From an ini-
tial observation (see section 4.1), the difficulty stems
in part from the simplicity of the seed terms above,
4By domain, we refer to the subject of a discourse.
10
leading to, for example, actual Korean learner data.
To avoid some of this noise, we use a second set of
seed terms, representing relevant words in the same
domains, but of a more advanced nature, i.e., topic-
appropriate words that may be outside of a typical
learner?s lexicon. Our hypothesis is that this is more
likely to lead to native, quality Korean. For each
one of the simple words above, we posit two more
advanced words, as given in figure 2.
kyo-sa ?teacher? in-kan ?human?
phyung-ka ?evaluation? cik-cang ?workplace?
pen-yuk ?translation? wu-ceng ?friendship?
mwun-hak ?literature? sin-loy ?trust?
ci-kwu ?earth? cwu-min ?resident?
swun-hwan ?circulation? kwan-kye ?relation?
myeng-sang ?meditation? co-cik ?organization?
phyeng-hwa ?peace? sik-i-yo-pep ?diet?
tham-hem ?exploration? yen-mal ?end of a year?
cwun-pi ?preparation? hayng-sa ?event?
Figure 2: Seed terms for the second focused corpus
3.4 Web corpus parameters
One can create corpora of varying size and general-
ity, by varying the parameters given to BootCaT. We
examine three parameters here.
Number of seeds The first way to vary the type
and size of corpus obtained is by varying the number
of seed terms. The exact words given to BootCaT af-
fect the domain of the resulting corpus, and utilizintg
a larger set of seeds leads to more potential to create
a bigger corpus. With 50 seed terms, for example,
there are 19,600 possible 3-tuples, while there are
only 120 possible 3-tuples for 10 seed terms, limit-
ing the relevant pages that can be returned.
For the general (G) corpus, we use: G1) all 50
seed terms, G2) 5 sets of 10 seeds, the result of split-
ting the 50 seeds randomly into 5 buckets, and G3)
5 sets of 20 seeds, which expand the 10-seed sets in
G2 by randomly selecting 10 other terms from the
remaining 40 seeds. This breakdown into 11 sets (1
G1, 5 G2, 5 G3) allows us to examine the effect of
using different amounts of general terms and facili-
tates easy comparison with the first focused corpus,
which has only 10 seed terms.
For the first focused (F1) corpus, we use: F11) the
10 seed terms, and F12) 5 sets of 20 seeds, obtained
by combining F11 with each seed set from G2. This
second group provides an opportunity to examine
what happens when augmenting the focused seeds
with more general terms; as such, this is a first step
towards larger corpora which retain some focus. For
the second focused corpus (F2), we simply use the
set of 20 seeds. We have 7 sets here (1 F11, 5 F12, 1
F2), giving us a total of 18 seed term sets at this step.
Tuple length One can also experiment with tuple
length in BootCat. The shorter the tuple, the more
webpages that can potentially be returned, as short
tuples are likely to occur in several pages (e.g., com-
pare the number of pages that all of person happi-
ness season occur in vs. person happiness season
exercise travel). On the other hand, longer tuples are
more likely truly relevant to the type of data of inter-
est, more likely to lead to well-formed language. We
experiment with tuples of different lengths, namely
3 and 5. With 2 different tuple lengths and 18 seed
sets, we now have 36 sets.
Number of queries We still need to specify how
many queries to send to the search engine. The max-
imum number is determined by the number of seeds
and the tuple size. For 3-word tuples with 10 seed
terms, for instance, there are 10 items to choose 3
objects from:
(10
3
)
= 10!3!(10?3)! = 120 possibilities.
Using all combinations is feasible for small seed
sets, but becomes infeasible for larger seed sets, e.g.,
(50
5
)
= 2, 118, 760 possibilities. To reduce this, we
opt for the following: for 3-word tuples, we generate
120 queries for all cases and 240 queries for the con-
ditions with 20 and 50 seeds. Similarly, for 5-word
tuples, we generate the maximum 252 queries with
10 seeds, and both 252 and 504 for the other condi-
tions. With the previous 36 sets (12 of which have
10 seed terms), evenly split between 3 and 5-word
tuples, we now have 60 total corpora, as in table 1.
# of seeds
tuple # of General F1 F2
len. queries 10 20 50 10 20 20
3 120 5 5 1 1 5 1
240 n/a 5 1 n/a 5 1
5 252 5 5 1 1 5 1
504 n/a 5 1 n/a 5 1
Table 1: Number of corpora based on parameters
11
Other possibilities There are other ways to in-
crease the size of a web corpus using BootCaT. First,
one can increase the number of returned pages for a
particular query. We set the limit at 20, as anything
higher will more likely result in non-relevant data
for the focused corpora and/or duplicate documents.
Secondly, one can perform iterations of search-
ing, extracting new seed terms with every iteration.
Again, the concern is that by iterating away from the
initial seeds, a corpus could begin to lose focus. We
are considering both extensions for the future.
Language check One other constraint we use is to
specify the particular language of interest, namely
that we want Korean pages. This parameter is set
using the language option when collecting URLs.
We note that a fair amount of English, Chinese, and
Japanese appears in these pages, and we are cur-
rently developing our own Korean filter.
4 Corpus statistics
To gauge the properties of size, genre, and degree of
particle usage in the corpora, independent of appli-
cation, basic statistics of the different web corpora
are given in table 2, where we average over multiple
corpora for conditions with 5 corpora.5
There are a few points to understand in the table.
First, it is hard to count true words in Korean, as
compounds are frequent, and particles have a de-
batable status. From a theory-neutral perspective,
we count ejels, which are tokens occurring between
white spaces. Secondly, we need to know about the
number of particles and number of nominals, i.e.,
words which could potentially bear particles, as our
machine learning paradigm considers any nominal a
test case for possible particle attachment. We use a
POS tagger (Han and Palmer, 2004) for this.
Some significant trends emerge when comparing
the corpora in the table. First of all, longer queries
(length 5) result in not only more returned unique
webpages, but also longer webpages on average than
shorter queries (length 3). This effect is most dra-
matic for the F2 corpora. The F2 corpora also exhibit
a higher ratio of particles to nominals than the other
web corpora, which means there will be more pos-
5For the 252 5-tuple 20 seed General corpora, we average
over four corpora, due to POS tagging failure on the fifth corpus.
itive examples in the training data for the machine
learner based on the F2 corpora.
4.1 Qualitative evaluation
In tandem with the basic statistics, it is also impor-
tant to gauge the quality of the Korean data from
a more qualitative perspective. Thus, we examined
the 120 3-tuple F1 corpus and discovered a number
of problems with the data.
First, there are issues concerning collecting data
which is not pure Korean. We find data extracted
from Chinese travel sites, where there is a mixture of
non-standard foreign words and unnatural-sounding
translated words in Korean. Ironically, we also find
learner data of Korean in our search for correct Ko-
rean data. Secondly, there are topics which, while
exhibiting valid forms of Korean, are too far afield
from what we expect learners to know, including re-
ligious sites with rare expressions; poems, which
commonly drop particles; gambling sites; and so
forth. Finally, there are cases of ungrammatical uses
of Korean, which are used in specific contexts not
appropriate for our purposes. These include newspa-
per titles, lists of personal names and addresses, and
incomplete phrases from advertisements and chats.
In these cases, we tend to find less particles.
Based on these properties, we developed the
aforementioned second focused corpus with more
advanced Korean words and examined the 240 3-
tuple F2 corpus. The F2 seeds allow us to capture a
greater percentage of well-formed data, namely data
from news articles, encyclopedic texts, and blogs
about more serious topics such as politics, literature,
and economics. While some of this data might be
above learners? heads, it is, for the most part, well-
formed native-like Korean. Also, the inclusion of
learner data has been dramatically reduced. How-
ever, some of the same problems from the F1 corpus
persist, namely the inclusion of poetry, newspaper
titles, religious text, and non-Korean data.
Based on this qualitative analysis, it is clear that
we need to filter out more data than is currently be-
ing filtered, in order to obtain valid Korean of a type
which uses a sufficient number of particles in gram-
matical ways. In the future, we plan on restrict-
ing the genre, filtering based on the number of rare
words (e.g., religious words), and using a trigram
language model to check the validity.
12
Ejel Particles Nominals
Corpus Seeds Len. Queries URLs Total Avg. Total Avg. Total Avg.
Gen. 10 3 120 1096.2 1,140,394.6 1044.8 363,145.6 331.5 915,025 838.7
5 252 1388.2 2,430,346.4 1779.9 839,005.8 618.9 1,929,266.0 1415.3
20 3 120 1375.2 1,671,549.2 1222.1 540,918 394.9 1,350,976.6 988.6
3 240 2492.4 2,735,201.6 1099.4 889,089 357.3 2,195,703 882.4
5 252 1989.6 4,533,642.4 2356 1,359,137.2 724.5 3,180,560.6 1701.5
5 504 3487 7,463,776 2193.5 2,515,235.8 741.6 5,795,455.8 1709.7
50 3 120 1533 1,720,261 1122.1 584,065 380.9 1,339,308 873.6
3 240 2868 3,170,043 1105.3 1,049,975 366.1 2,506,995 874.1
5 252 1899.5 4,380,684.2 2397.6 1,501,358.7 821.5 3,523,746.2 1934.6
5 504 5636 5,735,859 1017.7 1,773,596 314.6 4,448,815 789.3
F1 10 3 120 1315 628,819 478.1 172,415 131.1 510,620 388.3
5 252 1577 1,364,885 865.4 436,985 277.1 1,069,898 678.4
20 3 120 1462.6 1,093,772.4 747.7 331,457.8 226.8 885,157.2 604.9
240 2637.2 1,962,741.8 745.2 595,570.6 226.1 1,585,730.4 602.1
5 252 2757.6 2,015,077.8 730.8 616,163.8 223.4 1,621,306.2 588
504 4734 3,093,140.4 652.9 754,610 159.8 1,993,104.4 422.1
F2 20 3 120 1417 1,054,925 744.5 358,297 252.9 829,416 585.3
240 2769 1,898,383 685.6 655,757 236.8 1,469,623 530.7
5 252 1727 4,510,742 2611.9 1,348,240 780.7 2,790,667 1615.9
504 2680 6,916,574 2580.8 2,077,171 775.1 4,380,571 1634.5
Table 2: Basic statistics of different web corpora
Note that one might consider building even larger
corpora from the start and using the filtering step to
winnow down the corpus for a particular application,
such as particle error detection. However, while re-
moving ungrammatical Korean is a process of re-
moving noise, identifying whether a corpus is about
traveling, for example, is a content-based decision.
Given that this is what a search engine is designed
to do, we prefer filtering based only on grammatical
and genre properties.
5 Classification
We describe the classification paradigm used to de-
termine how effective each corpus is for detecting
correct particle usage; evaluation is in section 6.
5.1 Machine learning paradigm
Based on the parallel between Korean particles and
English prepositions, we use preposition error de-
tection as a starting point for developing a classifier.
For prepositions, Tetreault and Chodorow (2008) ex-
tract 25 features to guess the correct preposition (out
of 34 selected prepositions), including features cap-
turing the lexical and grammatical context (e.g., the
words and POS tags in a two-word window around
the preposition) and features capturing various rel-
evant selectional properties (e.g., the head verb and
noun of the preceding VP and NP).
We are currently using TiMBL (Daelemans et al,
2007) for development purposes, as it provides a
range of options for testing. Given that learner
data needs to be processed instantaneously and that
memory-based learning can take a long time to clas-
sify, we will revisit this choice in the future.
5.2 Defining features
5.2.1 Relevant properties of Korean
As discussed in section 2, Korean has major dif-
ferences from English, leading to different features.
First, the base word order of Korean is SOV, which
means that the following verb and following noun
could determine how the current word functions.
However, since Korean allows for freer word order
than English, we do not want to completely disre-
gard the previous noun or verb, either.
Secondly, the composition of words is different
than English. Words contain a stem and an arbitrary
number of suffixes, which may be derivational mor-
13
phemes as well as particles, meaning that we must
consider sub-word features, i.e., segment words into
their component morphemes.
Finally, particles have more functions than prepo-
sitions, requiring a potentially richer space of fea-
tures. Case marking, for example, is even more de-
pendent upon the word?s grammatical function in
a sentence. In order to ensure that our system can
correctly handle all of the typical relations between
words without failing on less frequent constructions,
we need (large amounts of) appropriate data.
5.2.2 Feature set
To begin with, we segment and POS tag the text,
using a hybrid (trigram + rule-based) morphological
tagger for Korean (Han and Palmer, 2004). This seg-
mentation phase means that we can define subword
features and isolate the particles in question. For our
features, we break each word into: a) its stem and b)
its combined affixes (excluding particles), and each
of these components has its own POS, possibly a
combined tag (e.g., EPF+EFN), with tags from the
Penn Korean Treebank (Han et al, 2002).
The feature vector uses a five word window that
includes the target word and two words on either
side for context. Each word is broken down into four
features: stem, affixes, stem POS, and affixes POS.
Given the importance of surrounding noun and verbs
for attachment in Korean, we have features for the
preceding as well as the following noun and verb.
For the noun/verb features, only the stem is used, as
this is largely a semantically-based property.
In terms of defining a class, if the target word?s
affixes contain a particle, it is removed and used as
the basis for the class; otherwise the class is NONE.
We also remove particles in the context affixes, as
we cannot rely on surrounding learner particles.
As an example, consider predicting the particle
for the word Yenge (?English?) in (5a). We gener-
ate the instance in (5b). The first five lines refer
to the previous two words, the target word, and the
following two words, each split into stem and suf-
fixes along with their POS tags, and with particles
removed. The sixth line contains the stems of the
preceding and following noun and verb, and finally,
there is the class (YES/NO).
(5) a. Mikwuk-eyse
America-in
sal-myense
live-while
Yenge-man-ul
English-only-OBJ
cip-eyse
home-at
ss-ess-eyo.
use-Past-Decl
?While living in America, (I/she/he) used only
English at home.?
b. Mikwuk NPR NONE NONE
sal VV myense ECS
Yenge NPR NONE NONE
cip NNC NONE NONE
ss VV ess+eyo EPF+EFN
sal Mikwuk ss cip
YES
For the purposes of evaluating the different cor-
pora, we keep the task simple and only guess YES
or NO for the existence of a particle. We envision
this as a first pass, where the specific particle can
be guessed later. This is also a practical task, in
that learners can benefit from accurate feedback on
knowing whether or not a particle is needed.
6 Evaluation
We evaluate the web corpora for the task of predict-
ing particle usage, after describing the test corpus.
6.1 Learner Corpus
To evaluate, we use a corpus of learner Korean made
up of essays from college students (Lee et al, 2009).
The corpus is divided according to student level (be-
ginner, intermediate) and student background (her-
itage, non-heritage),6 and is hand-annotated for par-
ticle errors. We expect beginners to be less accurate
than intermediates and non-heritage less accurate
than heritage learners. To pick a middle ground, the
current research has been conducted on non-heritage
intermediate learners. The test corpus covers a range
of common language classroom topics such as Ko-
rean language, Korea, friends, family, and traveling.
We run our system on raw learner data, i.e, un-
segmented and with spelling and spacing errors in-
cluded. As mentioned in section 5.2.2, we use a POS
tagger to segment the words into morphemes, a cru-
cial step for particle error detection.7
6Heritage learners have had exposure to Korean at a young
age, such as growing up with Korean spoken at home.
7In the case of segmentation errors, we cannot possibly get
the particle correct. We are currently investigating this issue.
14
Seeds Len. Quer. P R F
Gen. 10 3 120 81.54% 76.21% 78.77%
5 252 82.98% 77.77% 80.28%
20 3 120 81.56% 77.26% 79.33%
3 240 82.89% 78.37% 80.55%
5 252 83.79% 78.17% 80.87%
5 504 84.30% 79.44% 81.79%
50 3 120 82.97% 77.97% 80.39%
3 240 83.62% 80.46% 82.00%
5 252 82.57% 78.45% 80.44%
5 504 84.25% 78.69% 81.36%
F1 10 3 120 81.41% 74.67% 77.88%
5 252 83.82% 77.09% 80.30%
20 3 120 82.23% 76.40% 79.20%
240 82.57% 77.19% 79.78%
5 252 83.62% 77.97% 80.68%
504 81.86% 75.88% 78.73%
F2 20 3 120 81.63% 76.44% 78.93%
240 82.57% 78.45% 80.44%
5 252 84.21% 80.62% 82.37%
504 83.87% 81.51% 82.67%
Table 3: Results of guessing particle existence, training
with different corpora
The non-heritage intermediate (NHI) corpus gives
us 3198 words, with 1288 particles and 1836 nom-
inals. That is, about 70% of the nominals in the
learner corpus are followed by a particle. This is a
much higher average than in the 252 5-tuple F2 cor-
pus, which exhibits the highest average of all of the
web corpora at about 48% ( 7811616 ; see table 2).
6.2 Results
We use the default settings for TiMBL for all the re-
sults we report here. Though we have obtained 4-5%
higher F-scores using different settings, the compar-
isons between corpora are the important measure for
the current task. The results are given in table 3.
The best results were achieved when training
on the 5-tuple F2 corpora, leading to F-scores of
82.37% and 82.67% for the 252 tuple and 504 tu-
ple corpora, respectively. This finding reinforces our
hypothesis that more advanced seed terms result in
more reliable Korean data, while staying within the
domain of the test corpus. Both longer tuple lengths
and greater amounts of queries have an effect on the
reliability of the resulting corpora. Specificaly, 5-
tuple corpora produce better results than similar 3-
tuple corpora, and corpora with double the amount
of queries of n-length perform better than smaller
comparable corpora. Although larger corpora tend
to do better, it is important to note that there is not
a clear relationship. The general 50/5/252 corpus,
for instance, is similarly-sized to the F2 focused
20/5/252 corpus, with over 4 million ejels (see ta-
ble 2). The focused corpus?based on fewer yet
more relevant seed terms?has 2% better F-score.
7 Summary and Outlook
In this paper, we have examined different ways to
build web corpora for analyzing learner language
to support the detection of errors in Korean parti-
cles. This type of investigation is most useful for
lesser-resourced languages, where the error detec-
tion task stays constant, but the topic changes fre-
quently. In order to develop a framework for testing
web corpora, we have also begun developing a ma-
chine learning system for detecting particle errors.
The current web data, as we have demonstrated, is
not perfect, and thus we need to continue improving
that. One approach will be to filter out clearly non-
Korean data, as suggested in section 4.1. We may
also explore instance sampling (e.g., Wunsch et al,
2009) to remove many of the non-particle nominal
(negative) instances, which will reduce the differ-
ence between the ratios of negative-to-positive in-
stances of the web and learner corpora. We still feel
that there is room for improvement in our seed term
selection, and plan on constructing specific web cor-
pora for each topic covered in the learner corpus.
We will also consider adding currently available cor-
pora, such as the Sejong Corpus (The National Insti-
tute of Korean Language, 2007), to our web data.
With better data, we can work on improving the
machine learning system. This includes optimizing
the set of features, the parameter settings, and the
choice of machine learning algorithm. Once the sys-
tem has been optimized, we will need to test the re-
sults on a wider range of learner data.
Acknowledgments
We would like to thank Marco Baroni and Jan
Pomika?lek for kindly providing a UTF-8 version of
BootCat; Chong Min Lee for help with the POS tag-
ger, provided by Chung-Hye Han; and Joel Tetreault
for useful discussion.
15
References
Amaral, Luiz and Detmar Meurers (2006). Where
does ICALL Fit into Foreign Language Teach-
ing? Talk given at CALICO Conference. May
19, 2006. University of Hawaii.
Baroni, Marco and Silvia Bernardini (2004). Boot-
CaT: Bootstrapping Corpora and Terms from the
Web. In Proceedings of LREC 2004. pp. 1313?
1316.
Baroni, Marco and Motoko Ueyama (2004). Re-
trieving Japanese specialized terms and corpora
from the World Wide Web. In Proceedings of
KONVENS 2004.
Daelemans, Walter, Jakub Zavrel, Ko van der Sloot,
Antal van den Bosch, Timbl Tilburg and Memory
based Learner (2007). TiMBL: Tilburg Memory-
Based Learner - version 6.1 - Reference Guide.
De Felice, Rachele and Stephen Pulman (2008). A
classifier-baed approach to preposition and deter-
miner error correction in L2 English. In Proceed-
ings of COLING-08. Manchester.
Dickinson, Markus, Soojeong Eom, Yunkyoung
Kang, Chong Min Lee and Rebecca Sachs (2008).
A Balancing Act: How can intelligent computer-
generated feedback be provided in learner-to-
learner interactions. Computer Assisted Language
Learning 21(5), 369?382.
Dickinson, Markus and Chong Min Lee (2009).
Modifying Corpus Annotation to Support the
Analysis of Learner Language. CALICO Journal
26(3).
Erjavec, Irena Srdanovic`, Tomaz Erjavec and Adam
Kilgarriff (2008). A Web Corpus and Word
Sketches for Japanese. Information and Media
Technologies 3(3), 529?551.
Han, Chung-Hye, Na-Rare Han, Eon-Suk Ko and
Martha Palmer (2002). Development and Eval-
uation of a Korean Treebank and its Application
to NLP. In Proceedings of LREC-02.
Han, Chung-Hye and Martha Palmer (2004). A Mor-
phological Tagger for Korean: Statistical Tag-
ging Combined with Corpus-Based Morphologi-
cal Rule Application. Machine Translation 18(4),
275?297.
Han, Na-Rae, Martin Chodorow and Claudia Lea-
cock (2006). Detecting Errors in English Arti-
cle Usage by Non-Native Speakers. Natural Lan-
guage Engineering 12(2).
Ko, S., M. Kim, J. Kim, S. Seo, H. Chung and S. Han
(2004). An analysis of Korean learner corpora
and errors. Hanguk Publishing Co.
Lee, John and Ola Knutsson (2008). The Role of
PP Attachment in Preposition Generation. In Pro-
ceedings of CICLing 2008. Haifa, Israel.
Lee, John and Stephanie Seneff (2006). Auto-
matic Grammar Correction for Second-Language
Learners. In INTERSPEECH 2006. Pittsburgh,
pp. 1978?1981.
Lee, Sun-Hee, Donna K. Byron and Seok Bae Jang
(2005). Why is Zero Marking Important in Ko-
rean? In Proceedings of IJCNLP-05. Jeju Island,
Korea.
Lee, Sun-Hee, Seok Bae Jang and Sang kyu Seo
(2009). Annotation of Korean Learner Corpora
for Particle Error Detection. CALICO Journal
26(3).
Sharoff, Serge (2006). Creating General-Purpose
Corpora Using Automated Search Engine
Queries. In WaCky! Working papers on the Web
as Corpus. Gedit.
Tetreault, Joel and Martin Chodorow (2008). The
Ups and Downs of Preposition Error Detection
in ESL Writing. In Proceedings of COLING-08.
Manchester.
Tetreault, Joel and Martin Chodorow (2009). Exam-
ining the Use of Region Web Counts for ESL Er-
ror Detection. In Web as Corpus Workshop (WAC-
5). San Sebastian, Spain.
The National Institute of Korean Language (2007).
The Sejong Corpus.
Ueyama, Motoko (2006). Evaluation of Japanese
Web-based Reference Corpora: Effects of Seed
Selection and Time Interval. In WaCky! Working
papers on the Web as Corpus. Gedit.
Wunsch, Holger, Sandra Ku?bler and Rachael
Cantrell (2009). Instance Sampling Methods for
Pronoun Resolution. In Proceedings of RANLP
2009. Borovets, Bulgaria.
16
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 162?165,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Annotating Korean Demonstratives 
    
 
Sun-Hee Lee 
Wellesley College 
Wellesley, USA 
slee6@wellesley.edu 
Jae-young Song 
Yonsei University 
Seoul, Korea 
jysong@yonsei.ac.kr 
  
 
 
Abstract 
This paper presents preliminary work on a 
corpus-based study of Korean demonstratives. 
Through the development of an annotation 
scheme and the use of spoken and written 
corpora, we aim to determine different func-
tions of demonstratives and to examine their 
distributional properties. Our corpus study 
adopts similar features of annotation used in 
Botley and McEnery (2001) and provides 
some linguistic hypotheses on grammatical 
functions of Korean demonstratives to be fur-
ther explored.  
1 Introduction 
Korean demonstratives are known to have two 
different functions: anaphoric and deictic refer-
ence. Anaphoric demonstratives refer to objects, 
individuals, events, situations, or propositions in 
the given linguistic context. Deictic demonstra-
tives refer to physical objects, individuals, or 
positions (or regions) in the given situational 
context. Deictic variations commonly signal the 
speaker?s physical distance from specified items. 
Previous literature on Korean demonstratives has 
focused on deictic functions in spoken Korean, 
but a comprehensive approach to their diverse 
linguistic functions is still lacking. This study 
examines distinct usages of Korean demonstra-
tives in a spoken and a written corpus through 
the annotation of relevant linguistic features. Our 
annotation scheme and features are expected to 
help clarify grammatical functions of Korean 
demonstratives, as well as other anaphoric ex-
pressions.    
  English demonstratives show a binary distinc-
tion that depends on physical distance; there is a 
distinction between proximal forms (this, these, 
this N, these Ns) and distal forms (that, those, 
that N, those Ns). In contrast, demonstratives in 
languages like Korean and Japanese show a 
three-way distinction: proximal forms, speaker-
centered distal forms, and speaker- and hearer-
centered distal forms. For example, deictic de-
monstrative i refers to a proximal object relative 
to the speaker, ku refers to a distant object that is 
close to the hearer, and ce refers to a distant ob-
ject that is far from both the speaker and the 
hearer. Thus, distinct usage of ce and ku is asso-
ciated with how the speaker allocates the deictic 
center and contextual space, i.e., the speaker-
centered space vs. the speaker- and the hearer?
centered space. In contrast with deictic usage, 
previous studies (Chang, 1980; Chang, 1984) 
assumed that anaphoric demonstratives show 
only a two-way distinction between proximal 
forms i and distal forms ku. However, it is still 
controversial as to whether the boundaries be-
tween anaphora and deixis are clear cut. With 
our annotation scheme, we aim to capture the 
linguistic properties contributing to interpreta-
tions of demonstratives in Korean. In particular, 
we aim to determine whether different registers 
or genres contribute to different functions of de-
monstratives by comparing their usage in a spo-
ken corpus and a written corpus.   
In consideration of a future comparative anal-
ysis with English demonstratives, we have de-
signed our annotation scheme by adopting Botley 
and McEnery?s (2001) paradigmatic set of dis-
tinctive features for English demonstratives. 
However, the detailed annotation features have 
been revised according to language specific fea-
tures of Korean.  
2 Corpus Study 
For data extraction, we used two Sejong tagged 
corpora including a 20,343 eojeol spoken corpus 
and 21,023 eojeol written corpus.
1
 Each corpus is 
                                                 
1
 The term eojeol refers to a unit set off by spaces and cor-
responds to a word unit in English. 
162
composed of four conversations/texts with ap-
proximately 5000 eojeol. The subcorpora of the 
spoken corpus are everyday conversations with-
out assigned topics and those of the written cor-
pus are three newspaper articles and part of a 
novel.    
Compared to English, Korean demonstratives 
include more complex grammatical categories 
with morphological relations. The demonstrative 
forms i, ku, and ce combine with other words or 
morphemes and form complex words including 
nominals (e.g., i-kes: this+thing ?this?), adverbs 
(e.g., ce-lehkey: that+way ?that way?), adjectives 
(e.g., ku-lehata: it+is ?is so?) and other lexical 
categories. Thus, it is difficult to determine if 
they all belong to the same category of demon-
stratives in Korean. In this study, demonstratives 
are restricted to words that contain i, ku, and ce 
maintaining a distinct referentialfunction of 
pointing. The selected demonstratives include 
adnouns ( i N ?this N?,  ku N ?that N?, ce ?that 
N?), pronouns ( i-es/i-ke ?this?, ku-kes/ ku-ke ?it?, 
ce-kes/ceke ?that?, i-tul ?these?, ku-tul ?they? ce-
tul), and locative pronouns ( yeki ?here?, keki 
?there?, ceki ?over there?). Although those forms 
have different lexical categories, strong similari-
ties exist within the same morphological families, 
which we will refer to as i type, ku type, and ce 
type demonstratives. Our annotation work aims 
to extract a generalization of the fundamental 
usage of the three different types and to use that 
generalization for developing further research on 
various morphological variants containing i, ku, 
and ce.  
2.1 The Annotation Scheme 
In order to mark referential functions of Korean 
demonstratives, we first adopt Halliday and Ha-
san?s (1976) classification of the different refer-
ence functions of demonstratives: exophoric vs. 
endophoric usage. We further divide exophora 
into deixis and background. While the former 
refers to a physical object or an individual (or 
location) in the situational context, the latter re-
fers to certain shared information between the 
speaker and the hearer.   
    
(1)      Reference 
 
situational (exophoric)  textual (endophoric)  
 
 deictic      background     anaphoric   cataphoric  
           (shared knowledge)       
 
Six distinct features include ?Lexical Category of 
a Demonstrative?, ?Endophoricity?, ?Exopho-
ricity?, ?Syntactic Category of an Antecedent?, 
?Phoric Type?, and ?Semantic Function of an 
Antecedent?. The first five features are adopted 
from five features in Botley and McEnery?s 
(2001) annotation work on English demonstra-
tives.
2
  The last feature (semantic function) has 
been added for future work annotating semantic 
information that facilitates anaphor resolution 
processes.   
Lexical categories of Korean demonstratives 
in this study include four parts of speech: adnoun, 
pronoun, locative pronoun (functioning also as 
an adverb), and exclamatory expressions. While 
the first three categories show referential func-
tions, the exclamatory expressions do not have 
reference. Instead, they are used as expressions 
conveying the speaker?s emotion or state, e.g., 
embarrassment, confusion, hedging.
 
We do not, 
however, exclude the possibility of linguistic 
connectivity between demonstrative and excla-
matory forms. For instance, the distal demonstra-
tive form ce tends to be used as a hedging ex-
pression in Korean. Our study includes exclama-
tory usage as an annotation feature.  
Endophoricity refers to two different func-
tions: anaphoric vs. cataphoric. Exophoricity re-
fers to context based vs. deixis. According to 
Halliday and Hasan?s classification in (1), de-
monstratives with referential function show two 
major usages: endophoric and exophoric. The 
first type takes its antecedent within the given 
text; the latter, within the given situation. Dis-
tinction between an anaphor and a cataphor de-
pends on the position of the antecedent. When an 
endophor follows its antecedent, it is an anaphor; 
the other case is a cataphor. Demonstratives may 
have different types of antecedents syntactically. 
The corresponding values include nominals (in-
cluding N or NP), clausals (including V, A, VP, 
                                                 
2
  As one of the reviewers pointed out, our study has some 
limitations as it only refers to two previous studies, Halliday 
and Hasan (1976) and Botley and McEnery (2001). Al-
though we are aware of the other fundamental work includ-
ing demonstratives in a broader range of referential expres-
sions such as Gundel et al (1993), Prince (1981), Nissim et 
al. (2004), etc., we choose to focus on Korean demonstra-
tives because their exact grammatical functions have not 
been comprehensively studied in existing literature. In addi-
tion, developing a broader classification system for referen-
tial expressions in Korean is a challenging task from both 
theoretical and empirical perspectives; linguistic analyses of 
Korean nominal expressions must deal with controversial 
issues such as definiteness without articles, zero elements 
functioning as anaphors, unsystematic morphological mark-
ing of plurality and genericity, etc.     
163
AP, etc.), and sentential elements (S or Ss for 
more than two sentences).
3
  
The feature semantic function of an antece-
dent includes values of nominal entities, events, 
and propositions. This feature will be expanded 
into specified values such as event, process, state, 
and circumstances in our future study. Phoric 
type has been adopted from Botley and McEnery 
(2001) and refers to two distinct relations: refer-
ence and substitution. According to Halliday and 
Hasan, substitution is a relation between linguis-
tic forms, whereas reference is a relation between 
meanings. The values of phoric type also include 
non-phoric such as exophora whose antecedents 
exist outside the text.  
 The annotation features and values we use 
are summarized in Table 1.  
Feature Value1  Value2 Value 3 Value4 
Lexical  
Category (L) 
AN 
(adnoun) 
PR 
(Pronoun) 
LPR 
(Locative 
pronoun) 
EX 
(Excla-
mation) 
Endophorici-
ty (O) 
A 
(anaphor) 
C 
(cataphor) 
 
 
 
 
Exophoricity 
(X) 
T 
(situation-
al) 
D 
(deictic) 
 
 
 
Syntactic 
Function (F)  
NO 
(nominals) 
CL 
(clausal) 
S 
(sentential) 
 
 
Semantic 
Function (M) 
N 
(entities) 
E (event) 
P  
(proposi-
tions) 
 
Phoric Type 
(H) 
R 
(reference) 
U 
(Substitu-
tion) 
K 
(non-phoric) 
 
Table 1 Annotation Features and Possible Values 
 
The initial results of inter-annotator agreement 
between two trained annotators are promising.  
Cohen?s Kappa is 0.76 for the average agreement 
of six high level categories and it increases fol-
lowing a discussion period (K = 0.83, K=2)
4
. 
3 Results 
We identified 1,235 demonstratives in our pilot 
study. The distributions of demonstratives were 
significantly different between the spoken and 
                                                 
3
 Although the syntactic category of an antecedent can be 
differentiated in a more sophisticated way using phrasal 
categories such as NP, VP, AdvP, etc. (as well as lexical 
categories), this will render the annotation process nearly 
impossible unless one uses a corpus with syntactic annota-
tion, such as treebanks. Thus, we use simplified syntactic 
information such as nominal, clausal, and sentential.  
4
  The agreement rate was calculated for each six high level 
categories separately and then averaged. The syntactic func-
tion has the lowest agreement rate even after the discussion 
(K=0.76). This is due to complex properties of Korean de-
monstratives with unclear boundaries between exclamatory 
expressions and other lexical categories.   
the written corpora. Table 2 shows the raw fre-
quencies in the spoken and the written corpora 
for each combination of feature and value out-
lined in Table 1. The raw frequencies are sup-
plemented with the log likelihood in order to 
show the significance for frequency differences 
in the two corpora in Table 2. Each demonstra-
tive is followed by a two-character code sepa-
rated by underscore. The first character denotes 
the feature and the second the value. For exam-
ple, the first item kulen ?that (kind of)? whose 
lexical category (L) is adnoun (AN) mostly ap-
peared in the spoken corpus and not in the writ-
ten corpus.
5
 
 
Feature                      S            W             LL 
kulen_L_AN 183 14 177.7  
kulen_H_R 178 14 171.3  
kulen_O_A 163 14 152.4  
kuke(s)_L_PR         202 38 128.5  
kuke(s)_H_R 187 38 112.5  
ku_L_EX   114 9 109.6  
i_O_A  6 105 104.0  
kuke(s)_O_A 172 38 97.0  
kulen_F_NO 69 2 82.4  
ike(s)_H_K 68 3 75.7 
ike(s)_X_D 63 2 74.3   
Table 2 Frequency of Demonstrative Features  
 
Whereas 931 demonstratives appeared in the 
spoken corpus, only 304 appeared in the written 
corpus. The distributions of three different types 
of demonstratives are listed in Table 3. 
 
Types 
Total 
Frequency 
Written Spoken 
Freq. % Freq. % 
i  398 176 56 222 44 
ku  773 128 17 645 83 
ce  64 0 0 64 100 
Total 1235 304 25 931 75 
Table 3 Distribution of Three Demonstrative Types 
 
The spoken corpus and the written corpus show 
different preferences for i, ku, and ce types.  
 
Written: i  (58%)  > ku (42%)  > ce (0%) 
Spoken: ku (69%) > i (24%)  > ce (7%) 
 
Whereas ku demonstratives are preferred to cor-
responding i demonstratives in the spoken corpus, 
i demonstratives are preferred in the written cor-
                                                 
5
 In Table 2, the log likelihood scores show that the usage of 
kulen is significantly different in the spoken and the written 
corpus. The log-likelihood scores in Table 2 are significant 
at a 99 percent confidence level with 1 degree of freedom if 
they are greater than 6.6. We only show a partial frequency 
list here due to the space limitations.  
 
164
pus. This fact is associated with the linguistic 
function of ku that represents a speaker?s desire 
to anchor interpersonal involvement with the 
hearer by actively inviting the hearer?s voluntary 
understanding of the target referent. In contrast, i 
demonstratives imply that the speaker (writer) 
intends to incorporate the hearer (reader) within 
the proximal cognitive distance. In terms of an-
notation features, our findings are summarized as 
follows.   
Lexical category: In both the written and 
spoken corpora, adnominal demonstratives are 
more frequently used than pronouns or locative 
pronouns. Demonstrative forms used as intensifi-
ers, hedges, or personal habitual noise have been 
marked as exclamatives. Annotators have found 
that it is often difficult to clearly distinguish 
them from adnominal demonstratives.   
Endophoricity: Our written corpus does not 
include any cataphors, whereas the spoken cor-
pus shows 61 cases (cf. 523 anaphors). This fact 
seems to be related to the speaker?s discourse 
strategy of intending to call the discourse partici-
pants? attention by placing an endophoric ele-
ment before its antecedent.  
Exophoricity: Exophoric usage of demonstr-
atives in the written corpus is very limited. Only 
17 cases were found (6 deixis vs. 11 context-
based). In the spoken corpus, exophoric usages 
occur more frequently across three types of de-
monstratives. The deictic usage dominates the 
context-based usage (151 deixis vs. 79 context-
based). As noted in previous literature, ce de-
monstratives mainly appear in deictic context, 
where its antecedent is visible or exists in the 
given situation. There seems to be a constraint of 
deictic usage of ce involving physical existence 
or visibility (or cognitive awareness) of an entity 
in addition to distance. This hypothesis needs to 
be further investigated with additional data.  
Syntactic and Semantic Function: All three 
types of i, ku, and ce demonstratives refer to no-
minal entities as their antecedents. Although i 
and ku demonstratives are also used to refer to 
clausals and sentential elements, only a few ex-
amples of ce replace clausal or sentential ele-
ments. Another notable point is that i and ku de-
monstratives refer to clausal or sentential ele-
ments (corresponding to events or propositions) 
more frequently than nominal entities in both 
spoken and written corpora. 59% of the antece-
dents of i demonstratives (56% for ku type) in 
the written corpus are clausals or sentential ele-
ments, whereas 53% of the antecedents of i type 
(69% for ku type) are in the spoken corpus. This 
result needs to be tested on a larger corpus in our 
future study.  
Phoric Type: In our annotated corpus, we 
only found referential examples, not substitu-
tional cases. Exophoric examples are marked as 
non-phoric. In the written corpus, referential de-
monstratives are predominant (285 cases) and a 
small number of non-phoric cases are observed 
(18 cases). In the spoken corpus, referential de-
monstratives are more frequent (590 cases), whe-
reas non-phoric cases have been more observed 
than in the written corpus (198 cases).  
3 Conclusion 
In this paper we presented a corpus-based study 
on Korean demonstratives. Six annotation fea-
tures were used to mark complex linguistic func-
tions of demonstratives. Using spoken and writ-
ten corpora, we compared different usages of 
Korean demonstratives and showed that their 
usages are different depending on the registers of 
spoken and written Korean.    
In spite of the deictic functions of demonstra-
tives highlighted in previous research, our study 
indicates that endophoric usage is more predo-
minant. This hypothesis, as well as others in this 
study, will be tested with a large corpus in our 
future work. We also plan to incorporate more 
sophisticated exploitation on semantic types of 
antecedents. This information will be useful for 
resolving the meaning of anaphoric demonstra-
tives. 
References 
Botley, Simon and Tony McEnery. 2001. Demonstra-
tives in English. Journal of English Linguistics, 
29(1): 7-33. 
Chang, Kyung-Hee. 1980. Semantic Analysis of De-
monstrative i, ku, ce. Ehakyenku,16(2):167-0184.  
Chang, Seok-Jin 1984. Cisiwa Coung. Hangul, 186: 
115-149.  
Gundel, Jaeanette, Nancy Hedberg, and Ron Zachars-
ki. 1993. Cognitive Status and the Form of Re-
ferring Expressions in Discourse. Language, 
69(2):274-307.  
Halliday, M.A.K. and Ruqaiya Hasan. 1976. Cohe-
sion in English. London: Longman.  
Min, Kyung Mo. 2008. A Study on Reference Items in 
Korean. Ph.D. Dissertation. Yonsei University.  
Poesio, Massimo. 2004. The MATE/GNOME 
Scheme for Anaphoric Annotation, Revisited. In 
Proceedings of SIGDIAL. Boston.   
Prince, Ellen. 1981. Toward a Taxonomy of Given-
New Information. Radical Pragmatics: 223-255. 
Academic Press. New York.  
165
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 81?86,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Developing Methodology for Korean Particle Error Detection
Markus Dickinson
Indiana University
md7@indiana.edu
Ross Israel
Indiana University
raisrael@indiana.edu
Sun-Hee Lee
Wellesley College
slee6@wellesley.edu
Abstract
We further work on detecting errors in post-
positional particle usage by learners of Korean
by improving the training data and develop-
ing a complete pipeline of particle selection.
We improve the data by filtering non-Korean
data and sampling instances to better match
the particle distribution. Our evaluation shows
that, while the data selection is effective, there
is much work to be done with preprocessing
and system optimization.
1 Introduction
A growing area of research in analyzing learner lan-
guage is to detect errors in function words, namely
categories such as prepositions and articles (see Lea-
cock et al, 2010, and references therein). This work
has mostly been for English, and there are issues,
such as greater morphological complexity, in mov-
ing to other languages (see, e.g., de Ilarraza et al,
2008; Dickinson et al, 2010). Our goal is to build a
machine learning system for detecting errors in post-
positional particles in Korean, a significant source of
learner errors (Ko et al, 2004; Lee et al, 2009b).
Korean postpositional particles are morphemes
that attach to a preceding nominal to indicate a range
of linguistic functions, including grammatical func-
tions, e.g., subject and object; semantic roles; and
discourse functions. In (1), for instance, ka marks
the subject (function) and agent (semantic role).1
Similar to English prepositions, particles can also
have modifier functions, adding meanings of time,
location, instrument, possession, and so forth.
1We use the Yale Romanization scheme for writing Korean.
(1) Sumi-ka
Sumi-SBJ
John-uy
John-GEN
cip-eyse
house-LOC
ku-lul
he-OBJ
twu
two
sikan-ul
hours-OBJ
kitaly-ess-ta.
wait-PAST-END
?Sumi waited for John for (the whole) two hours in
his house.?
We treat the task of particle error detection as
one of particle selection, and we use machine learn-
ing because it has proven effective in similar tasks
for other languages (e.g., Chodorow et al, 2007;
Oyama, 2010). Training on a corpus of well-formed
Korean, we predict which particle should appear af-
ter a given nominal; if this is different from the
learner?s, we have detected an error. Using a ma-
chine learner has the advantage of being able to per-
form well without a researcher having to specify
rules, especially with the complex set of linguistic
relationships motivating particle selection.2
We build from Dickinson et al (2010) in two
main ways: first, we implement a presence-selection
pipeline that has proven effective for English prepo-
sition error detection (cf. Gamon et al, 2008). As
the task is understudied, the work is preliminary, but
it nonetheless is able to highlight the primary ar-
eas of focus for future work. Secondly, we improve
upon the training data, in particular doing a better
job of selecting relevant instances for the machine
learner. Obtaining better-quality training data is a
major issue for machine learning applied to learner
language, as the domain of writing is different from
news-heavy training domains (Gamon, 2010).
2See Dickinson and Lee (2009); de Ilarraza et al (2008);
Oyama (2010) for related work in other languages.
81
2 Particle error detection
2.1 Pre-processing
Korean is an agglutinative language: Korean words
(referred to as ecels) are usually composed of a
root with a number of functional affixes. We thus
first segment and POS tag the text, for both train-
ing and testing, using a hybrid (trigram + rule-
based) morphological tagger for Korean (Han and
Palmer, 2004). The tagger is designed for native
language and is not optimized to make guesses for
ill-formed input. While the POS tags assigned to the
learner corpus are thus often incorrect (see Lee et al,
2009a), there is the more primary problem of seg-
mentation, as discussed in more detail in section 4.
2.2 Machine learning
We use the Maximum Entropy Toolkit (Le, 2004)
for machine learning. Training on a corpus of well-
formed Korean, we predict which particle should ap-
pear after a given nominal; if this is different from
what the learner used, we have detected an error. It
is important that the data represent the relationships
between specific lexical items: in the comparable
English case, for example, interest is usually found
with in: interest in/*with learning.
Treating the ends of nominal elements as possible
particle slots, we break classification into two steps:
1) Is there a particle? (Yes/No); and 2) What is the
exact particle? Using two steps eases the task of ac-
tual particle prediction: with a successful classifica-
tion of negative and positive instances, there is no
need to handle nominals that have no particle in step
2. To evaluate our parameters for obtaining the most
relevant instances, we keep the task simple and per-
form only step 1, as this step provides information
about the usability of the training data. For actual
system performance, we evaluate both steps.
In selecting features for Korean, we have to ac-
count for relatively free word order (Chung et al,
2010). We follow our previous work (Dickinson
et al, 2010) in our feature choices, using a five-
word window that includes the target stem and two
words on either side for context (see also Tetreault
and Chodorow, 2008). Each word is broken down
into: stem, affixes, stem POS, and affixes POS. We
also have features for the preceding and following
noun and verb, thereby approximating relevant se-
lectional properties. Although these are relatively
shallow features, they provide enough lexical and
grammatical context to help select better or worse
training data (section 3) and to provide a basis for a
preliminary system (section 4).
3 Obtaining the most relevant instances
We need well-formed Korean data in order to train
a machine learner. To acquire this, we use web-
based corpora, as this allows us to find data similar
to learner language, and using web as corpus (WaC)
tools allows us to adjust parameters for new data
(Dickinson et al, 2010). However, the methodology
outlined in Dickinson et al (2010) can be improved
in at least three ways, outlined next.
3.1 Using sub-corpora
Web corpora can be built by searching for a set of
seed terms, extracting documents with those terms
(Baroni and Bernardini, 2004). One way to improve
such corpora is to use better seeds, namely, those
which are: 1) domain-appropriate (e.g., about trav-
eling), and 2) of an appropriate level. In Dickinson
et al (2010), we show that basic terms result in poor
quality Korean, but slightly more advanced terms on
the same topics result in better-formed data.
Rather than use all of the seed terms to create a
single corpus, we divide the seed terms into 13 sep-
arate sets, based on the individual topics from our
learner corpus. The sub-corpora are then combined
to create a cohesive corpus covering all the topics.
For example, we use 10 Travel words to build a
subcorpus, 10 Learning Korean words for a differ-
ent subcorpus, and so forth. This means that terms
appropriate for one topic are not mixed with terms
for a different topic, ensuring more coherent web
documents. Otherwise, we might obtain a Health
Management word, such as pyengwen (?hospital?),
mixed with a Generation Gap word, such as kaltung
(?conflict?)?in this case, leading to webpages on
war, a topic not represented in our learner corpus.
3.2 Filtering
One difficulty with our web corpora is that some of
them have large amounts of other languages along
with Korean. The keywords are in the corpora, but
there is additional text, often in Chinese, English, or
Japanese. These types of pages are unreliable for
82
our purposes, as they may not exhibit natural Ko-
rean. By using a simple filter, we check whether a
majority of the characters in a webpage are indeed
from the Korean writing system, and remove pages
beneath a certain threshold.
3.3 Instance sampling
Particles are often dropped in colloquial and even
written Korean, whereas learners are more often
required to use them. It is not always the case
that the web pages contain the same ratio of par-
ticles as learners are expected to use. To alleviate
this over-weighting of having no particle attached
to a noun, we propose to downsample our corpora
for the machine learning experiments, by remov-
ing a randomly-selected proportion of (negative) in-
stances. Instance sampling has been effective for
other NLP tasks, e.g., anaphora resolution (Wunsch
et al, 2009), when the number of negative instances
is much greater than the positive ones. In our web
corpora, nouns have a greater than 50% chance of
having no particle; in section 3.4, we thus downsam-
ple to varying amounts of negative instances from
about 45% to as little as 10% of the total corpus.
3.4 Training data selection
In Dickinson et al (2010), we used a Korean learner
data set from Lee et al (2009b) for development. It
contains 3198 ecels, 1842 of which are nominals,
and 1271 (?70%) of those have particles. We use
this same corpus for development, to evaluate filter-
ing and down-sampling. Evaluating on (yes/no) par-
ticle presence, in tables 1 and 2, recall is the percent-
age of positive instances we correctly find and pre-
cision is the percentage of instances that we classify
as positive that actually are. A baseline of always
guessing a particle gives 100% recall, 69% preci-
sion, and 81.7% F-score.
Table 1 shows the results of the MaxEnt system
for step 1, using training data built for the topics in
the data with filter thresholds of 50%, 70%, 90%,
and 100%?i.e., requiring that percentage of Korean
characters?as well as the unfiltered corpus. The
best F-score is with the filter set at 90%, despite the
size of the filtered corpus being smaller than the full
corpus. Accordingly, we use the 90% filter on our
training corpus for the experiments described below.
Threshold 100% 90% 70% 50% Full
Ecel 67k 9.6m 10.3m 11.1m 12.7m
Instances 37k 5.8m 6.3m 7.1m 8.4m
Accuracy 74.75 81.11 74.64 80.29 80.46
Precision 80.03 86.14 79.65 85.41 85.56
Recall 84.50 86.55 84.97 86.15 86.23
F-score 82.20 86.34 82.22 85.78 85.89
Table 1: Step 1 (particle presence) results with filters
The results for instance sampling are given in ta-
ble 2. We experiment with positive to negative sam-
pling ratios of 1.3/1 (?43% negative instances), 2/1
(?33%), 4/1 (?20%), and 10/1 (?10%). We select
the 90% filter, 1.3/1 downsampling settings and ap-
ply them to the training corpus (section 3.1) for all
experiments below.
P/N ratio 10/1 4/1 2/1 1.3/1 1/1.05
Instances 3.1m 3.5m 4.3m 5m 5.8m
Accuracy 74.75 77.85 80.23 81.59 81.11
Precision 73.38 76.72 80.75 84.26 86.14
Recall 99.53 97.48 93.71 90.17 86.55
F-score 84.47 85.86 86.74 87.12 86.34
Table 2: Step 1 (presence) results with instance sampling
One goal has been to improve the web as corpus
corpus methodology for training a machine learning
system. The results in tables 1 and 2 reinforce our
earlier finding that size is not necessarily the most
important variable in determining the usefulness or
overall quality of data collected from the web for
NLP tasks (Dickinson et al, 2010). Indeed, the cor-
pus producing best results (90% filter, 1.3:1 down-
sampling) is more than 3 million instances smaller
than the unfiltered, unsampled corpus.
4 Initial system evaluation
We have obtained an annotated corpus of 25 essays
from heritage intermediate learners,3 with 299 sen-
tences and 2515 ecels (2676 ecels after correcting
spacing errors). There are 1138 nominals, with 93
particle errors (5 added particles, 35 omissions, 53
substitutions)?in other words, less than 10% of par-
ticles are errors. There are 979 particles after cor-
rection. We focus on 38 particles that intermediate
3Heritage learners have had exposure to Korean at a young
age, such as growing up with Korean spoken at home.
83
students can be reasonably expected to use. A parti-
cle is one of three types (cf. Nam and Ko, 2005): 1)
case markers, 2) adverbials (cf. prepositions), and
3) auxiliary particles.4
Table 3 gives the results for the entire system on
the test corpus, with separate results for each cat-
egory of particle, (Case, Adv., and Aux.) as well
as the concatenation of the three (All). The ac-
curacy presented here is in terms of only the par-
ticle in question, as opposed to the full form of
root+particle(s). Step 2 is presented in 2 ways: Clas-
sified, meaning that all of the instances classified as
needing a particle by step 1 are processed, or Gold,
in which we rely on the annotation to determine par-
ticle presence. It is not surprising, then, that Gold
experiments are more accurate than Classified ex-
periments, due to step 1 errors and also preprocess-
ing issues, discussed next.
Step 1 Step 2
Data # Classified Gold
Case 504 95.83% 71.23% 72.22%
Adv. 205 82.43% 30.24% 32.68%
Aux. 207 89.37% 31.41% 35.74%
All 916 91.37% 53.05% 55.13%
Table 3: Accuracy for step 1 (particle presence) & step 2
(particle selection), with number (#) of instances
Preprocessing For the particles we examine, there
are 135 mis-segmented nominals. The problem is
more conspicuous if we look at the entire corpus:
the tagger identifies 1547 nominal roots, but there
are only 1138. Some are errors in segmentation, i.e.,
mis-identifying the proper root of the ecel, and some
are problems with tagging the root, e.g., a nominal
mistagged as a verb. Table 4 provides results divided
by cases with only correctly pre-processed ecels and
where the target ecel has been mis-handled by the
tagger. This checks whether the system particle is
correct, ignoring whether the whole form is correct;
if full-form accuracy is considered, we have no way
to get the 135 inaccurate cases correct.
Error detection While our goal now is to estab-
lish a starting point, the ultimate, on-going goal of
4Full corpus details will be made available at: http://
cl.indiana.edu/?particles/.
Step 1 Step 2
Data # Classified Gold
Accurate 781 94.24% 55.95% 58.13%
Inaccurate 135 74.81% 36.29% 38.51%
Table 4: Overall accuracy divided by accurate and inac-
curate preprocessing
Case Adv. Aux. All
Precision 28.82% 7.69% 5.51% 15.45%
Recall 87.50% 100% 77.78% 88.00%
Table 5: Error detection (using Gold step 1)
this work is to develop a robust system for automati-
cally detecting errors in learner data. Thus, it is nec-
essary to measure our performance at actually find-
ing the erroneous instances extracted from our test
corpus. Table 5 provides results for step 2 in terms
of our ability to detect erroneous instances. We re-
port precision and recall, calculated as in figure 1.
From the set of erroneous instances:
True Positive (TP) ML class 6= student class
False Negative (FN) ML class = student class
From the set of correct instances:
False Positive (FP) ML class 6= student class
True Negative (TN) ML class = student class
Precision (P) TPTP+FP
Recall (R) TPTP+FN
Figure 1: Precision and recall for error detection
4.1 Discussion and Outlook
One striking aspect about the results in table 3 is the
gap in accuracy between case particles and the other
two categories, particularly in step 2. This points at
a need to develop independent systems for each type
of particle, each relying on different types of linguis-
tic information. Auxiliary particles, for example, in-
clude topic particles which?similar to English arti-
cles (Han et al, 2006)?require discourse informa-
tion to get correct. Still, as case particles comprise
more than half of all particles in our corpus, the sys-
tem is already potentially useful to learners.
Comparing the rows in table 4, the dramatic drop
in accuracy when moving to inaccurately-processed
84
cases shows a clear need for preprocessing adapted
to learner data. While it is disconcerting that nearly
15% (135/916) of the cases have no chance of re-
sulting in a correct full form, the results indicate that
we can obtain reliable accuracy (cf. 94.24%) for pre-
dicting particle presence across all types of particles,
assuming good morphological tagging.
From table 5, it is apparent that we are overguess-
ing errors; recall that only 10% of particles are er-
roneous, whereas we more often guess a different
particle. While this tendency results in high recall,
a tool for learners should have higher precision, so
that correct usage is not flagged. However, this is
a first attempt at error detection, and simply know-
ing that precision is low means we can take steps
to solve this deficiency. Our training data may have
too many possible classes in it, and we have not yet
accounted for phonological alternations; e.g. if the
system guesses ul when lul is correct, we count a
miss, even though they are different realizations of
the same morpheme.
To try and alleviate the over-prediction of errors,
we have begun to explore implementing a confi-
dence filter. As a first pass, we use a simple fil-
ter that compares the probability of the best parti-
cle to the probability of the particle the learner pro-
vided; the absolute difference in probabilities must
be above a certain threshold. Table 6 provides the er-
ror detection results for each type of particle, incor-
porating confidence filters of 10%, 20%, 30%, 40%,
50%, and 60%. The results show that increasing the
threshold at which we accept the classifier?s answer
can significantly increase precision, at the cost of re-
call. As noted above, higher precision is desirable,
so we plan on further developing this confidence fil-
ter. We may also include heuristic-based filters, such
as the ones implemented in Criterion (see Leacock
et al, 2010), as well as a language model approach
(Gamon et al, 2008).
Finally, we are currently working on improving
the POS tagger, testing other taggers in the pro-
cess, and developing optimal feature sets for differ-
ent kinds of particles.
Acknowledgments
We would like to thank the IU CL discussion group
and Joel Tetreault for feedback at various points.
Adv Aux Case All
10
% P 10.0% 6.3% 29.9% 16.3%
R 100% 77.8% 67.8% 73.3%
20
% P 13.5% 7.8% 32.6% 18.0%
R 100% 77.8% 50.0% 60.0%
30
% P 20.0% 8.3% 36.1% 20.8%
R 100% 66.7% 39.3% 50.7%
40
% P 19.4% 14.3% 48.6% 26.9%
R 60.0% 66.7% 30.4% 38.7%
50
% P 23.1% 16.7% 57.9% 32.1%
R 30.0% 44.4% 19.6% 24.0%
60
% P 40.0% 26.7% 72.3% 45.2%
R 20.0% 44.4% 14.3% 18.7%
Table 6: Error detection with confidence filters
References
Marco Baroni and Silvia Bernardini. 2004. Bootcat:
Bootstrapping corpora and terms from the web. In
Proceedings of LREC 2004, pages 1313?1316.
Martin Chodorow, Joel Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involv-
ing prepositions. In Proceedings of the 4th ACL-
SIGSEM Workshop on Prepositions, pages 25?30.
Prague.
Tagyoung Chung, Matt Post, and Daniel Gildea.
2010. Factors affecting the accuracy of korean
parsing. In Proceedings of the NAACL HLT
2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 49?57.
Los Angeles, CA, USA.
Arantza D??az de Ilarraza, Koldo Gojenola, and
Maite Oronoz. 2008. Detecting erroneous uses
of complex postpositions in an agglutinative lan-
guage. In Proceedings of COLING-08. Manch-
ester.
Markus Dickinson, Ross Israel, and Sun-Hee Lee.
2010. Building a korean web corpus for analyz-
ing learner language. In Proceedings of the 6th
Workshop on the Web as Corpus (WAC-6). Los
Angeles.
Markus Dickinson and Chong Min Lee. 2009. Mod-
ifying corpus annotation to support the analysis of
learner language. CALICO Journal, 26(3).
Michael Gamon. 2010. Using mostly native data
85
to correct errors in learners? writing. In Human
Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the
Association for Computational Linguistics, pages
163?171. Los Angeles, California.
Michael Gamon, Jianfeng Gao, Chris Brockett,
Alexander Klementiev, William Dolan, Dmitriy
Belenko, and Lucy Vanderwende. 2008. Using
contextual speller techniques and language mod-
eling for esl error correction. In Proceedings of
IJCNLP. Hyderabad, India.
Chung-Hye Han and Martha Palmer. 2004. A mor-
phological tagger for korean: Statistical tagging
combined with corpus-based morphological rule
application. Machine Translation, 18(4):275?
297.
Na-Rae Han, Martin Chodorow, and Claudia Lea-
cock. 2006. Detecting errors in english article us-
age by non-native speakers. Natural Language
Engineering, 12(2).
S. Ko, M. Kim, J. Kim, S. Seo, H. Chung, and
S. Han. 2004. An analysis of Korean learner cor-
pora and errors. Hanguk Publishing Co.
Zhang Le. 2004. Maximum Entropy Mod-
eling Toolkit for Python and C++. URL
http://homepages.inf.ed.ac.uk/
s0450736/maxent_toolkit.html.
Claudia Leacock, Martin Chodorow, Michael Ga-
mon, and Joel Tetreault. 2010. Automated Gram-
matical Error Detection for Language Learners.
Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool.
Chong Min Lee, Soojeong Eom, and Markus Dick-
inson. 2009a. Towards analyzing korean learner
particles. Talk given at CALICO ?09 Pre-
Conference Workshop on Automatic Analysis of
Learner Language. Tempe, AZ.
Sun-Hee Lee, Seok Bae Jang, and Sang kyu Seo.
2009b. Annotation of korean learner corpora for
particle error detection. CALICO Journal, 26(3).
Ki-shim Nam and Yong-kun Ko. 2005. Korean
Grammar (phyocwun kwuke mwunpeplon). Top
Publisher, Seoul.
Hiromi Oyama. 2010. Automatic error detection
method for japanese particles. Polyglossia, 18.
Joel Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in esl
writing. In Proceedings of COLING-08. Manch-
ester.
Holger Wunsch, Sandra Ku?bler, and Rachael
Cantrell. 2009. Instance sampling methods for
pronoun resolution. In Proceedings of RANLP
2009. Borovets, Bulgaria.
86
