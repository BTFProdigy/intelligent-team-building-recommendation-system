Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 540?545,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Fully Unsupervised Word Segmentation with BVE and MDL
Daniel Hewlett and Paul Cohen
Department of Computer Science
University of Arizona
Tucson, AZ 85721
{dhewlett,cohen}@cs.arizona.edu
Abstract
Several results in the word segmentation liter-
ature suggest that description length provides
a useful estimate of segmentation quality in
fully unsupervised settings. However, since
the space of potential segmentations grows ex-
ponentially with the length of the corpus, no
tractable algorithm follows directly from the
Minimum Description Length (MDL) princi-
ple. Therefore, it is necessary to generate
a set of candidate segmentations and select
between them according to the MDL princi-
ple. We evaluate several algorithms for gen-
erating these candidate segmentations on a
range of natural language corpora, and show
that the Bootstrapped Voting Experts algo-
rithm consistently outperforms other methods
when paired with MDL.
1 Introduction
The goal of unsupervised word segmentation is to
discover correct word boundaries in natural lan-
guage corpora where explicit boundaries are absent.
Often, unsupervised word segmentation algorithms
rely heavily on parameterization to produce the cor-
rect segmentation for a given language. The goal
of fully unsupervised word segmentation, then, is to
recover the correct boundaries for arbitrary natural
language corpora without explicit human parameter-
ization. This means that a fully unsupervised algo-
rithm would have to set its own parameters based
only on the corpus provided to it.
In principle, this goal can be achieved by creat-
ing a function that measures the quality of a seg-
mentation in a language-independent way, and ap-
plying this function to all possible segmentations of
the corpora to select the best one. Evidence from the
word segmentation literature suggests that descrip-
tion length provides a good approximation to this
segmentation quality function. We discuss the Min-
imum Description Length (MDL) principle in more
detail in the next section. Unfortunately, evaluating
all possible segmentations is intractable, since a cor-
pus of length n has 2n?1 possible segmentations. As
a result, MDL methods have to rely on an efficient
algorithm to generate a relatively small number of
candidate segmentations to choose between. It is
an empirical question which algorithm will generate
the most effective set of candidate segmentations.
In this work, we compare a variety of unsupervised
word segmentation algorithms operating in conjunc-
tion with MDL for fully unsupervised segmentation,
and find that the Bootstrapped Voting Experts (BVE)
algorithm generally achieves the best performance.
2 Minimum Description Length
At a formal level, a segmentation algorithm is a
function SEGMENT(c, ?) that maps a corpus c and
a vector of parameters ? ? ? to one of the
possible segmentations of that corpus. The goal
of fully unsupervised segmentation is to reduce
SEGMENT(c, ?) to SEGMENT(c) by removing the
need for a human to specify a particular ?. One way
to achieve this goal is to generate a set of candidate
segmentations by evaluating the algorithm for mul-
tiple values of ?, and then choose the segmentation
that minimizes some cost function. Thus, we can
define SEGMENT(c) in terms of SEGMENT(c, ?):
SEGMENT(c) = argmin
???
COST(SEGMENT(c, ?))
(1)
540
Now, selecting the best segmentation is treated as a
model selection problem, where each segmentation
provides a different model of the corpus. Intuitively,
a general approach is to choose the simplest model
that explains the data, a principle known as Occam?s
Razor. In information theory, this intuitive princi-
ple of simplicity or parsimony has been formalized
as the Minimum Description Length (MDL) princi-
ple, which states that the most likely model of the
data is the one that requires the fewest bits to en-
code (Rissanen, 1983). The number of bits required
to represent a model is called its description length.
Previous work applying the MDL principle to seg-
mentation (Yu, 2000; Argamon et al, 2004; Zhikov
et al, 2010) is motivated by the observation that ev-
ery segmentation of a corpus implicitly defines a lex-
icon, or set of words.
More formally, the segmented corpus S is a list
of words s1s2 . . . sN . L(S), the lexicon implicitly
defined by S, is simply the set of unique words in S.
The description length of S can then be broken into
two components, the description length of the lex-
icon and the description length of the corpus given
the lexicon. If we consider S as being generated
by sampling words from a probability distribution
over words in the lexicon, the number of bits re-
quired to represent each word si in S is simply its
surprisal, ? logP (si). The information cost of the
corpus given the lexicon is then computed by sum-
ming the surprisal of each word si in the corpus:
CODE(S|L(S)) = ?
?N
i=1
logP (si) (2)
To properly compute the description length of the
segmentation, we must also include the cost of the
lexicon. Adding in the description length of the lex-
icon forces a trade-off between the lexicon size and
the size of the compressed corpus. For purposes of
the description length calculation, the lexicon is sim-
ply treated as a separate corpus consisting of char-
acters rather than words. The description length can
then be computed in the usual manner, by summing
the surprisal of each character in each word in the
lexicon:
CODE(L(S)) = ?
?
w?L(S)
?
k?w
logP (k) (3)
where k ? w refers to the characters in word w
in the lexicon. As noted by Zhikov et al (Zhikov
et al, 2010), an additional term is needed for the
information required to encode the parameters of the
lexicon model. This quantity is normally estimated
by (k/2) log n, where k is the degrees of freedom in
the model and n is the length of the data (Rissanen,
1983). Substituting the appropriate values for the
lexicon model yields:
|L(S)| ? 1
2
? logN (4)
The full description length calculation is simply the
sum of three terms shown in 2, 3, and 4. From this
definition, it follows that a low description length
will be achieved by a segmentation that defines a
small lexicon, which nonetheless reduces the corpus
to a short series of mostly high-frequency words.
3 Generating Candidate Segmentations
Recent unsupervised MDL algorithms rely on
heuristic methods to generate candidate segmenta-
tions. Yu (2000) makes simplifying assumptions
about the nature of the lexicon, and then performs an
Expectation-Maximization (EM) search over this re-
duced hypothesis space. Zhikov et al (2010) present
an algorithm called EntropyMDL that generates a
candidate segmentation based on branching entropy,
and then iteratively refines the segmentation in an
attempt to greedily minimize description length.
We selected three entropy-based algorithms for
generating candidate segmentations, because such
algorithms do not depend on the details of any par-
ticular language. By ?unsupervised,? we mean op-
erating on a single unbroken sequence of characters
without any boundary information; Excluded from
consideration are a class of algorithms that are semi-
supervised because they require sentence boundaries
to be provided. Such algorithms include MBDP-1
(Brent, 1999), HDP (Goldwater et al, 2009), and
WordEnds (Fleck, 2008), each of which is discussed
in Section 5.
3.1 Phoneme to Morpheme
Tanaka-Ishii and Jin (2006) developed Phoneme to
Morpheme (PtM) to implement ideas originally de-
veloped by Harris (1955). Harris noticed that if
one proceeds incrementally through a sequence of
phonemes and asks speakers of the language to
count the letters that could appear next in the se-
quence (today called the successor count), the points
where the number increases often correspond to
morpheme boundaries. Tanaka-Ishii and Jin cor-
541
rectly recognized that this idea was an early ver-
sion of branching entropy, given by HB(seq) =
?
?
c?S P (c|seq) logP (c|seq), where S is the set
of successors to seq. They designed their PtM algo-
rithm based on branching entropy in both directions,
and it was able to achieve scores near the state of the
art on word segmentation in phonetically-encoded
English and Chinese. PtM posits a boundary when-
ever the increase in the branching entropy exceeds
a threshold. This threshold provides an adjustable
parameter for PtM, which we exploit to generate 41
candidate segmentations by trying every threshold in
the range [0.0, 2.0], in steps of 0.05.
3.2 Voting Experts
The Voting Experts (VE) algorithm (Cohen and
Adams, 2001) is based on the premise that words
may be identified by an information theoretic signa-
ture: Entropy within a word is relatively low, en-
tropy at word boundaries is relatively high. The
name Voting Experts refers to the ?experts? that vote
on possible boundary locations. VE has two ex-
perts: One votes to place boundaries after sequences
that have low internal entropy (surprisal), given by
HI(seq) = ? logP (seq), the other votes after se-
quences that have high branching entropy. All se-
quences are evaluated locally, within a sliding win-
dow, so the algorithm is very efficient. A boundary
is generated whenever the vote total at a given loca-
tion exceeds a threshold, and in some cases only if
the vote total is a local maximum. VE thus has three
parameters that can be manipulated to generate po-
tential segmentations: Window size, threshold, and
local maximum. Pairing VE with MDL was first ex-
amined by Hewlett and Cohen (2009). We generated
a set of 104 segmentations by trying every viable
threshold and local max setting for each window size
between 2 and 9.
3.3 Bootstrapped Voting Experts
The Bootstrapped Voting Experts (BVE) algorithm
(Hewlett and Cohen, 2009) is an extension to VE.
BVE works by segmenting the corpus repeatedly,
with each new segmentation incorporating knowl-
edge gained from previous segmentations. As with
many bootstrapping methods, three essential com-
ponents are required: some initial seed knowledge,
a way to represent knowledge, and a way to lever-
age that knowledge to improve future performance.
For BVE, the seed knowledge consists of a high-
precision segmentation generated by VE. After this
seed segmentation, BVE segments the corpus re-
peatedly, lowering the vote threshold with each iter-
ation. Knowledge gained from prior segmentations
is represented in a data structure called the knowl-
edge trie. During voting, this knowledge trie pro-
vides statistics for a third expert that places votes in
contexts where boundaries were most frequently ob-
served during the previous iteration. Each iteration
of BVE provides a candidate segmentation, and ex-
ecuting BVE for window sizes 2-8 and both local
max settings generated a total of 126 segmentations.
4 Experiments
There are two ways to evaluate the quality of a seg-
mentation algorithm in the MDL framework. The
first is to directly measure the quantity of the seg-
mentation chosen by MDL. For word segmentation,
this is typically done by computing the F-score,
where F = (2 ? Precision ? Recall)/(Precision +
Recall), for both boundaries (BF) and words (WF)
found by the algorithm. The second is to com-
pare the minimal description length among the can-
didates to the true description length of the corpus.
4.1 Results
We chose a diverse set of natural language cor-
pora, including some widely-used corpora to facil-
itate comparison. For each corpus, we generated a
set of candidate segmentations with PtM, VE, and
BVE, as described in the previous section. From
each set of candidates, results for the segmentation
with minimal description length are presented in the
tables below. Where possible, results for other algo-
rithms are presented in italics, with semi-supervised
algorithms set apart. Source code for all algorithms
evaluated here, as well as data files for all corpora,
are available online1.
One of the most commonly-used benchmark cor-
pora for unsupervised word segmentation is the
BR87 corpus. This corpus is a phonemic encod-
ing of the Bernstein Ratner corpus (Bernstein Rat-
ner, 1987) from the CHILDES database of child-
directed speech (MacWhinney, 2000). The perfor-
1http://code.google.com/p/voting-experts
542
mance of the algorithms on BR87 is shown in Ta-
ble 1 below. As with all experiments in this work,
the input was presented as one continuous sequence
of characters with no word or sentence boundaries.
Published results for two unsupervised algorithms,
the MDL-based algorithm of Yu (2000) and the
EntropyMDL (EMDL) algorithm of Zhikov et al
(2010), on this widely-used benchmark corpus are
shown in italics. Set apart in the table are pub-
lished results for three semi-supervised algorithms,
MBDP-1 (Brent, 1999), HDP (Goldwater, 2007),
and WordEnds (Fleck, 2008), described in Section
5. These algorithms operate on a version of the cor-
pus that includes sentence boundaries.
Algorithm BP BR BF WP WR WF
PtM+MDL 0.861 0.897 0.879 0.676 0.704 0.690
VE+MDL 0.875 0.803 0.838 0.614 0.563 0.587
BVE+MDL 0.949 0.879 0.913 0.793 0.734 0.762
Yu 0.722 0.724 0.723 NR NR NR
EMDL NR NR 0.907 NR NR 0.750
MBDP-1 0.803 0.843 0.823 0.670 0.694 0.682
HDP 0.903 0.808 0.852 0.752 0.696 0.723
WordEnds 0.946 0.737 0.829 NR NR 0.707
Table 1: Results for the BR87 corpus.
Results for one corpus, the first 50,000 charac-
ters of George Orwell?s 1984, have been reported
in nearly every VE-related paper. It thus provides
a good opportunity to compare to the other VE-
derived algorithms: Hierarchical Voting Experts ?
3 Experts (Miller and Stoytchev, 2008) and Markov
Experts (Cheng and Mitzenmacher, 2005). Table 2
shows the results for candidate algorithms as well as
the two other VE-derived algorithms, HVE-3E and
ME.
Algorithm BP BR BF WP WR WF
PtM+MDL 0.694 0.833 0.758 0.421 0.505 0.459
VE+MDL 0.788 0.774 0.781 0.498 0.489 0.493
BVE+MDL 0.841 0.828 0.834 0.585 0.577 0.581
HVE-3E 0.796 0.771 0.784 0.512 0.496 0.504
ME 0.809 0.787 0.798 NR 0.542 NR
Table 2: Results for the first 50,000 characters of 1984.
Chinese and Thai are both commonly written
without spaces between words, though some punc-
tuation is often included. Because of this, these
languages provide an excellent real-world challenge
for unsupervised segmentation. The results shown
in Table 3 were obtained using the first 100,000
words of the Chinese Gigaword corpus (Huang,
2007), written in Chinese characters. The word
boundaries specified in the Chinese Gigaword Cor-
pus were used as a gold standard. Table 4 shows
results for a roughly 100,000 word subset of a cor-
pus of Thai novels written in the Thai script, taken
from a recent Thai word segmentation competition,
InterBEST 2009. Working with a similar but much
larger corpus of Thai text, Zhikov et al were able
to achieve slightly better performance (BF=0.934,
WF=0.822).
Algorithm BP BR BF WP WR WF
PtM+MDL 0.894 0.610 0.725 0.571 0.390 0.463
VE+MDL 0.871 0.847 0.859 0.657 0.639 0.648
BVE+MDL 0.834 0.914 0.872 0.654 0.717 0.684
Table 3: Results for a corpus of orthographic Chinese.
Algorithm BP BR BF WP WR WF
PtM+MDL 0.863 0.934 0.897 0.702 0.760 0.730
VE+MDL 0.916 0.837 0.874 0.702 0.642 0.671
BVE+MDL 0.889 0.969 0.927 0.767 0.836 0.800
Table 4: Results for a corpus of orthographic Thai.
The Switchboard corpus (Godfrey and Holli-
man, 1993) was created by transcribing sponta-
neous speech, namely telephone conversations be-
tween English speakers. Results in Table 5 are for
a roughly 64,000 word section of the corpus, tran-
scribed orthographically.
Algorithm BP BR BF WP WR WF
PtM+MDL 0.761 0.837 0.797 0.499 0.549 0.523
VE+MDL 0.779 0.855 0.815 0.530 0.582 0.555
BVE+MDL 0.890 0.818 0.853 0.644 0.592 0.617
Yu 0.674 0.665 0.669 NR NR NR
WordEnds 0.900 0.755 0.821 NR NR 0.663
HDP 0.731 0.924 0.816 NR NR 0.636
Table 5: Results for a subset of the Switchboard corpus.
4.2 Description Length
Table 6 shows the best description length achieved
by each algorithm for each of the test corpora. In
most cases, BVE compressed the corpus more than
VE, which in turn achieved better compression than
PtM. In Chinese, the two VE-algorithms were able
to compress the corpus beyond the gold standard
543
size, which may mean that these algorithms are
sometimes finding repeated units larger than words,
such as phrases.
Algorithm BR87 Orwell SWB CGW Thai
PtM+MDL 3.43e5 6.10e5 8.79e5 1.80e6 1,23e6
VE+MDL 3.41e5 5.75e5 8.24e5 1.54e6 1.23e6
BVE+MDL 3.13e5 5.29e5 7.64e5 1.56e6 1.13e6
Gold Standard 2.99e5 5.07e5 7.06e5 1.62e6 1.11e6
Table 6: Best description length achieved by each algo-
rithm compared to the actual description length of the
corpus.
5 Related Work
The algorithms described in Section 3 are all rela-
tively recent algorithms based on entropy. Many al-
gorithms for computational morphology make use
of concepts similar to branching entropy, such as
successor count. The HubMorph algorithm (John-
son and Martin, 2003) adds all known words to a
trie and then performs DFA minimization (Hopcroft
and Ullman, 1979) to convert the trie to a finite state
machine. In this DFA, it searches for sequences of
states (stretched hubs) with low branching factor in-
ternally and high branching factor at the boundaries,
which is analogous to the chunk signature that drives
VE and BVE, as well as the role of branching en-
tropy in PtM.
MDL is analogous to Bayesian inference, where
the information cost of the model CODE(M) acts
as the prior distribution over models P (M), and
CODE(D|M), the information cost of the data given
the model, acts as the likelihood function P (D|M).
Thus, Bayesian word segmentation methods may
be considered related as well. Indeed, one of the
early Bayesian methods, MBDP-1 (Brent, 1999)
was adapted from an earlier MDL-based method.
Venkataraman (2001) simplified MBDP-1, relaxed
some of its assumptions while preserving the same
level of performance. Recently, Bayesian methods
with more sophisticated language models have been
developed, including one that models language gen-
eration as a hierarchical Dirichlet process (HDP),
in order to incorporate the effects of syntax into
word segmentation (Goldwater et al, 2009). An-
other recent algorithm, WordEnds, generalizes in-
formation about the distribution of characters near
word boundaries to improve segmentation (Fleck,
2008), which is analogous to the role of the knowl-
edge trie in BVE.
6 Discussion
For the five corpora tested above, BVE achieved
the best performance in conjunction with MDL, and
also achieved the lowest description length. We have
shown that the combination of BVE and MDL pro-
vides an effective approach to unsupervised word
segmentation, and that it can equal or surpass semi-
supervised algorithms such as MBDP-1, HDP, and
WordEnds in some cases.
All of the languages tested here have relatively
few morphemes per word. One area for future work
is a full investigation of the performance of these al-
gorithms in polysynthetic languages such as Inukti-
tut, where each word contains many morphemes. It
is likely that in such languages, the algorithms will
find morphs rather than words.
Acknowledgements
This work was supported by the Office of Naval Re-
search under contract ONR N00141010117. Any
opinions, findings, and conclusions or recommen-
dations expressed in this publication are those of the
authors and do not necessarily reflect the views of
the ONR.
References
Shlomo Argamon, Navot Akiva, Amihood Amir, and
Oren Kapah. 2004. Efficient Unsupervised Recur-
sive Word Segmentation Using Minimum Description
Length. In Proceedings of the 20th International Con-
ference on Computational Linguistics, Morristown,
NJ, USA. Association for Computational Linguistics.
Nan Bernstein Ratner, 1987. The phonology of parent-
child speech, pages 159?174. Erlbaum, Hillsdale, NJ.
Michael R. Brent. 1999. An Efficient, Probabilistically
Sound Algorithm for Segmentation and Word Discov-
ery. Machine Learning, (34):71?105.
Jimming Cheng and Michael Mitzenmacher. 2005. The
Markov Expert for Finding Episodes in Time Series.
In Proceedings of the Data Compression Conference,
pages 454?454. IEEE.
Paul Cohen and Niall Adams. 2001. An algorithm
for segmenting categorical time series into meaning-
ful episodes. In Proceedings of the Fourth Symposium
on Intelligent Data Analysis.
544
Margaret M. Fleck. 2008. Lexicalized phonotactic word
segmentation. In Proceedings of The 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 130?138,
Columbus, Ohio, USA. Association for Computational
Linguistics.
John J. Godfrey and Ed Holliman. 1993. Switchboard- 1
Transcripts.
Sharon Goldwater, Thomas L Griffiths, and Mark John-
son. 2009. A Bayesian Framework for Word Segmen-
tation: Exploring the Effects of Context. Cognition,
112(1):21?54.
Sharon Goldwater. 2007. Nonparametric Bayesian mod-
els of lexical acquisition. Ph.D. dissertation, Brown
University.
Zellig S. Harris. 1955. From Phoneme to Morpheme.
Language, 31(2):190?222.
Daniel Hewlett and Paul Cohen. 2009. Bootstrap Voting
Experts. In Proceedings of the Twenty-first Interna-
tional Joint Conference on Artificial Intelligence.
J. E. Hopcroft and J. D. Ullman. 1979. Introduction
to Automata Theory, Languages, and Computation.
Addison-Wesley.
Chu-Ren Huang. 2007. Tagged Chinese Gigaword
(Catalog LDC2007T03). Linguistic Data Consortium,
Philadephia.
Howard Johnson and Joel Martin. 2003. Unsupervised
learning of morphology for English and Inuktitut. Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology (HLT-
NAACL 2003), pages 43?45.
Brian MacWhinney. 2000. The CHILDES project: Tools
for analyzing talk. Lawrence Erlbaum Associates,
Mahwah, NJ, 3rd editio edition.
Matthew Miller and Alexander Stoytchev. 2008. Hierar-
chical Voting Experts: An Unsupervised Algorithm for
Hierarchical Sequence Segmentation. In Proceedings
of the 7th IEEE International Conference on Develop-
ment and Learning, pages 186?191.
Jorma Rissanen. 1983. A Universal Prior for Integers
and Estimation by Minimum Description Length. The
Annals of Statistics, 11(2):416?431.
Kumiko Tanaka-Ishii and Zhihui Jin. 2006. From
Phoneme to Morpheme: Another Verification Using
a Corpus. In Proceedings of the 21st International
Conference on Computer Processing of Oriental Lan-
guages, pages 234?244.
Anand Venkataraman. 2001. A procedure for unsuper-
vised lexicon learning. In Proceedings of the Eigh-
teenth International Conference on Machine Learning.
Hua Yu. 2000. Unsupervised Word Induction using
MDL Criterion. In Proceedings of the International
Symposium of Chinese Spoken Language Processing,
Beijing, China.
Valentin Zhikov, Hiroya Takamura, and Manabu Oku-
mura. 2010. An Efficient Algorithm for Unsuper-
vised Word Segmentation with Branching Entropy and
MDL. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 832?842, Cambridge, MA. MIT Press.
545
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 121?131,
Dublin, Ireland, August 23-24 2014.
Extracting Latent Attributes from Video Scenes Using Text as
Background Knowledge
Anh Tran, Mihai Surdeanu, Paul Cohen
University of Arizona
{trananh, msurdeanu, prcohen}@email.arizona.edu
Abstract
We explore the novel task of identify-
ing latent attributes in video scenes, such
as the mental states of actors, using
only large text collections as background
knowledge and minimal information about
the videos, such as activity and actor types.
We formalize the task and a measure of
merit that accounts for the semantic re-
latedness of mental state terms. We de-
velop and test several largely unsupervised
information extraction models that iden-
tify the mental states of human partici-
pants in video scenes. We show that these
models produce complementary informa-
tion and their combination significantly
outperforms the individual models as well
as other baseline methods.
1 Introduction
?Labeling a narrowly avoided vehicular
manslaughter as approach(car, person) is
missing something.?
1
The recognition of ac-
tivities, participants, and objects in videos has
advanced considerably in recent years (Li et al.,
2010; Poppe, 2010; Weinland et al., 2011; Yang
and Ramanan, 2011; Ng et al., 2012). However,
identifying latent attributes of scenes, such as the
mental states of human participants, has not been
addressed. Latent attributes matter: If a video
surveillance system detects one person chasing
another, the response from law enforcement
should be radically different if the people are
happy (e.g., children playing) or afraid and angry
(e.g., a person running from an assailant).
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
1
James Donlon, former manager of DARPA?s Mind?s Eye
program, personal communication.
Attributes that are latent in visual representa-
tions are often explicit in textual representations.
This suggests a novel method for inferring latent
attributes: Use explicit features of videos to query
text corpora, and from the resulting texts extract
attributes that are latent in the videos, such as men-
tal states. The contributions of this work are:
1: We formalize the novel task of latent attribute
identification from video scenes, focusing on the
identification of actors? mental states. The input
for the task is contextual information about the
scene, such as detections about the activity (e.g.,
chase) and actor types (e.g., policeman or child),
and the output is a distribution over mental state
labels. We show that gold standard annotations
for this task can be reliably generated using crowd
sourcing. We define a novel evaluation measure,
called constrained weighted similarity-aligned F
1
score, that accounts for both the differences be-
tween mental state distributions and the seman-
tic relatedness of mental state terms (e.g., partial
credit is given for irate when the target is angry).
2: We propose several robust and largely unsuper-
vised information extraction (IE) models for iden-
tifying the mental state labels of human partici-
pants in a scene, given solely the activity and actor
types: a lexical semantic (LS) model that extracts
mental state labels that are highly similar to the
context of the scene in a latent, conceptual vector
space; and an information retrieval (IR) model that
identifies labels commonly appearing in sentences
related to the explicit scene context. We show that
these models are complementary and their combi-
nation performs better than either model, alone.
3: Furthermore, we show that an event-centric
model that focuses on the mental state labels of
the participants in the relevant event (identified us-
ing syntactic patterns and coreference resolution)
outperforms the above shallower models.
121
2 Related Work
As far as we know, the task proposed here is novel.
We can, however, review work relevant to each
part of the problem and our solution. Mental
state inference is often formulated as a classifica-
tion problem, where the goal is to predict target
mental state labels based on low-level sensory in-
put data. Most solutions try to learn classification
models based on large amounts of training data,
while some require human engineering of domain
knowledge. Hidden Markov Models (HMMs) and
Dynamic Bayesian Networks (DBNs) are popular
representations because they can model the tem-
poral evolution of mental states. For instance, the
mental states of students can be inferred from un-
intentional body gestures using a DBN (Abbasi et
al., 2009). Likewise, an HMM can also be used
to model the emotional states of humans (Liu and
Wang, 2011). Some solutions combine HMMs
and DBNs in a Bayesian inference framework to
yield a multi-layer representation that can do real-
time inference of complex mental and emotional
states (El Kaliouby and Robinson, 2004; Baltru-
saitis et al., 2011). Our work differs from these
approaches in several ways: It is mostly unsuper-
vised, multi-modal, and requires little training.
Relevant video processing technology includes
object detection (e.g., (Felzenszwalb et al., 2008)),
person detection, and pose detection (e.g., (Yang
and Ramanan, 2011)). Many tracking algo-
rithms have been developed, such as group track-
ing (McKenna et al., 2000), tracking by learn-
ing appearances (Ramanan et al., 2007), and
tracking in 3D space (Giebel et al., 2004; Brau
et al., 2013). For human action recognition,
current state-of-the-art techniques are capable of
achieving near perfect performance on the com-
monly used KTH Actions dataset (Schuldt et al.,
2004) and high performance rates on other more
challenging datasets (O?Hara and Draper, 2012;
Sadanand and Corso, 2012).
To extract mental state information from texts,
one might use any or all of the technologies of
natural language processing, so a complete review
of relevant technologies is impossible, here. Of
immediate relevance is the work of de Marneffe
et al. (2010), which identified the latent meaning
behind scalar adjectives (e.g., which ages people
have in mind when talking about ?little kids?).
The authors learned these meanings by extract-
ing scalars, such as children?s ages, that were
commonly collocated with phrases, such as ?lit-
tle kids,? in web documents. Mohtarami et al.
(2011) tried to infer yes/no answers from indirect
yes/no question-answer pairs (IQAPs) by predict-
ing the uncertainty of sentiment adjectives in in-
direct answers. Their method employs antonyms,
synonyms, word sense disambiguation as well as
the semantic association between the sentiment
adjectives that appear in the IQAP to assign a de-
gree of certainty to each answer. Sokolova and La-
palme (2011) further showed how to learn a model
for predicting the opinions of users based on their
written contents, such as reviews and product de-
scriptions, on the Web. Gabbard et al. (2011)
found that coreference resolution can significantly
improve the recall rate of relations extraction with-
out much expense to the precision rate.
Our work builds on these efforts by combining
information retrieval, lexical semantics, and event
extraction to extract latent scene attributes.
3 Data
For the experiments in this paper, we focus solely
on videos containing chase scenes. Chases often
invoke clear mental state inferences, and depend-
ing on context can suggest very different mental
state distributions for the actors involved.
3.1 Video Corpus
We compiled a video dataset of 26 chase videos
found on the Web. Of these, five involve police
officers, seven involve children, four show sports-
related scenes, and twelve describe different chase
scenarios involving civilian adults (two videos in-
volve children playing sports). The average dura-
tion of the dataset is 8.8 seconds with a range of
[4, 18]. Most videos involve a single chaser and a
single chasee (a person being chased) while a few
have several chasers and/or chasees.
For each video, we used Amazon Mechanical
Turk (MTurk) to identify both the actors and their
mental states. Each worker was asked to view a
video in its entirety before answering some ques-
tions about the scene. We give no prior training to
the workers. The questions were carefully phrased
to apply to all participants of a particular role, for
example all chasers (if there are more than one).
We also ask obvious validation questions about the
participants in each role (e.g., are the chasers run-
ning towards the camera?) and use the answers to
these questions to filter out poor responses. In gen-
122
eral, we found that most responses were good and
only a few incomplete submissions were rejected.
In the first experiment, we asked MTurk work-
ers to select the actor types and various other de-
tections from a predefined list of tags. This label-
ing task is a proxy for a computer vision detection
system that functions at a human level of perfor-
mance. Indeed, we restricted the actor type labels
to a set that can be reasonably expected from auto-
matic detection algorithms: person, police officer,
child, and (non-human) object. For instance, po-
lice officers often wear distinctive color uniforms
that can be learned using the Felzenszwalb detec-
tor (Felzenszwalb et al., 2008), whereas children
can be reliably differentiated by their heights un-
der a 3D-tracking model (Brau et al., 2013). Each
video was annotated by three different workers
and the union of their annotations is produced.
The overall accuracy of the annotation was excel-
lent. The MTurk workers correctly identified the
important actors in every video.
Next, we collected a gold standard list of mental
state labels for each video by asking MTurk work-
ers to identify all applicable mental state adjec-
tives for the actors involved. We used a text-box
to allow for free-form input. Studies have shown
that people of different cultures can perceive emo-
tions very differently, and having forced choice
options cannot always capture their true percep-
tion (Gendron et al., 2014). Therefore, we did not
restrict the response of the workers in any way.
Workers could abstain from answering if they felt
the video was too ambiguous. Each video was
evaluated by ten different workers. We converted
each term provided to the closest adjective form
if possible. Terms with no equivalent adjective
forms were left in place. On rare occasions, work-
ers provided sentence descriptions despite being
asked for single-word adjectives. These sentences
were either removed, or collapsed into a single
word if appropriate. The overall quality of the an-
notations was good and generally followed com-
mon intuition. Asides from the frequently used
terms, we also received some colorful (yet infor-
mative) descriptions, like incredulous and vindic-
tive. In general, chases involving police scenar-
ios often contained violent and angry states while
chases involving children received more cheerful
labels. There were unexpected descriptions, such
as annoy for a playful chase between two children.
Upon review of the video, we agreed that one child
did indeed look annoyed. Thus, the resulting de-
scriptions were subjective, but very few were hard
to rationalize. By aggregating the answers from
the workers, we generated a gold standard distri-
bution of mental state terms for each video.
2
3.2 Text Corpus
The text corpus used for our models is the En-
glish Gigaword 5th Edition corpus
3
, made avail-
able by the Linguistics Data Consortium and in-
dexed by Lucene
4
. It is a comprehensive archive
of newswire text data (approximately 26 GB), ac-
quired over several years. It is in this corpus that
we expect to find mental state terms cued by con-
textual information from videos.
4 Neighborhood Models
We developed several individual models based on
the neighborhood paradigm, that is, the hypoth-
esis that relevant mental state labels will appear
?near? text cued by the visual features of a scene.
The models take as input the context extracted
from a video scene, defined simply as a list of ?ac-
tivity and actor-type? tuples (e.g., (chase, police)).
Multiple actor types will result in multiple tuples
for a video. The actors can be either a person, a
policeman, a child, or a (non-human) object. If
the detections describe the actor as both a person
and a child, or a person and a policeman, we auto-
matically remove the person label as it is a Word-
Net (Miller, 1995) hypernym of both child and po-
liceman. For each human actor type, we further
increase our coverage by retrieving the synonym
set (synset) of its most frequent sense (i.e., sense
#1) from WordNet. For example, a chase involv-
ing a policeman would generate the following tu-
ples: (chase, policeman) and (chase, officer).
We call these query tuples because they are used
to query text for sentences that ? if all goes well ?
will contain relevant mental state labels.
Given query tuples, our models use an initial
seed set of 160 mental state adjectives to produce
a single distribution over mental state labels, re-
ferred to as the response distribution, for each
video. The seed set is compiled from popular
mental and emotional state dictionaries, includ-
ing the Profile of Mood States (POMS) (McNair
et al., 1971) and Plutchik?s wheel of emotion. We
2
All videos and annotations are available at:
http://trananh.github.io/vlsa
3
Linguistics Data Consortium catalog no. LDC2011T07
4
Apache Lucene: http://lucene.apache.org
123
Source Example Mental State Labels
POMS
alert, annoyed, energetic, exhausted, helpful,
sad, terrified, unworthy, weary, etc.
Plutchik
angry, disgusted, fearful, joyful/joyous,
sad, surprised, trusting, etc.
Others
agitated, competitive, cynical, disappointed,
excited, giddy, happy, inebriated, violent, etc.
Table 1: The initial seed set contains 160 mental
state labels, compiled from different sources like
the popular Profile of Mood States dictionary and
Plutchik?s wheel of emotion.
also included frequently used labels gathered from
synsets found in WordNet (see Table 1 for exam-
ples). Note that the gold standard annotations pro-
duced by MTurk workers (Sec. 3) was not a source
for this set, nor was it restricted to these terms.
4.1 Back-off Interpolation in Vector Space
Our first model uses the recurrent neural net-
work language model (RNNLM) of Mikolov et
al. (2013) to project both mental state labels and
query tuples into a latent conceptual space. Simi-
larity is then trivially computed as the cosine sim-
ilarity between these vectors. In all of our experi-
ments, we used a RNNLM computed over the Gi-
gaword corpus with 600-dimensional vectors.
For this vector space (vec) model, we separate
the query tuples into different levels of back-off
context. The first level includes the set of activ-
ity types as singleton context tuples, e.g., (chase),
while the second level includes all (activity, actor)
context tuples. Hence, each query tuple will yield
two different context tuples, one for each back-off
level. For each context tuple with multiple terms,
such as (chase, policeman), we find the vector rep-
resentation for the context by aggregating the vec-
tors representing the search terms:
vec(chase, policeman) = vec(chase) +
vec(policeman) .
The vector representation for a singleton con-
text tuple is just the vector of the single search
term. We then calculate the distance of each men-
tal state labelm to the normalized vector represen-
tation of the context tuple by computing the cosine
similarity score between the two vectors:
cos(?
m
) =
vec(m) ? vec(context tuple)
||vec(m)|| ||vec(context tuple)||
.
The hypothesis here is that mental state labels
that are related to the search context will have a
RNNLM vector that is closer to the context tuple
vector, resulting in a high cosine similarity score.
Because the number of latent dimensions is rela-
tively small (when compared to vocabulary size),
cosine similarity scores in this latent space tend to
be close. To further separate these scores, we raise
them to an exponential power:
score(m) = e
cos(?
m
)+1
? 1 .
The processing of each context tuple yields 160
different scores, one for each mental state label.
We normalize these scores to form a single distri-
bution of scores for each context tuple. The distri-
butions are then integrated into a single distribu-
tion representative of the complete activity as fol-
lows: (a) the distributions at each context back-off
level are averaged to generate a single distribution
per level ? for the second level (which includes
activity and actor types), it means distributions for
all (activity, actor) tuples are averaged, whereas
the first level only has a single distribution from
the singleton activity tuple (chase); and (b) distri-
butions for the different levels are linearly interpo-
lated, similar to the back-off strategy of (Collins,
1997). Let e
1
and e
2
represent the weights of some
mental state label m from the average distribution
at the first and second level, respectively. Then the
interpolated distribution score e for m is:
e = ?e
1
+ (1? ?)e
2
.
Compiling the distribution scores for each m
produces the final distribution representing the ac-
tivity modeled. We prune this final distribution by
taking the top ranked items that make up some ?
proportion of the distribution. We delay the dis-
cussion of how ? is tuned to Section 6. The final
pruned distribution is normalized to produce the
response distribution.
4.2 Sentence Co-occurrence with Deleted
Interpolation
Our second model, the sent model, extracts mental
state labels based on the likelihood that they ap-
pear in sentences cued by query tuples. For each
tuple, we estimate the conditional probability that
we will see a mental state label m in a sentence,
where m is from the seed set, given that we al-
ready observed the desired activity and actor type
in the same sentence: P (m|activity, actor). In this
case, we refer to the sentence length as the neigh-
borhood window. Furthermore, all terms must ap-
pear as the correct part-of-speech (POS): m must
124
appear as an adjective or verb, the activity as a
verb, and the actor as a noun. (Mental state adjec-
tives are allowed to appear as verbs because some
are often mis-tagged as verbs; e.g., agitated, deter-
mined, welcoming.) We used Stanford?s CoreNLP
toolkit for tokenization and POS tagging.
5
Note that this probability is similar to a trigram
probability in POS tagging, except the triples need
not form an ordered sequence but must appear in
the same sentence and under the correct POS tag.
Unfortunately, we cannot always compute this tri-
gram probability directly from the corpus because
there might be too few instances of each trigram
to compute a probability reliably. As is common,
we instead estimate it as a linear interpolation of
unigrams, bigrams, and trigrams. We define the
maximum likelihood probabilities
?
P , derived from
relative frequencies f , for the unigrams, bigrams,
and trigrams as follows:
?
P (m) =
f(m)
N
?
P (m|activity) =
f(m, activity)
f(activity)
?
P (m|activity, actor) =
f(m, activity, actor)
f(activity, actor)
for all mental state labels m, activities, and actor
types in our queries. N is the total number of to-
kens in the corpus. The aforementioned POS re-
quirement is enforced: f(m) is the number of oc-
currences of m as an adjective or verb. We define
?
P = 0 if the corresponding numerator and denom-
inator are zero. The desired trigram probability is
then estimated as:
P (m|activity, actor) = ?
1
?
P (m) +
?
2
?
P (m|activity) + ?
3
?
P (m|activity, actor) .
As ?
1
+?
2
+?
3
= 1, P represents a probability
distribution. We use the deleted interpolation algo-
rithm (Brants, 2000) to estimate one set of lambda
values for the model, based on all trigrams.
For each query tuple generated in a video, 160
different trigrams are computed, one for each men-
tal state label in the seed set, resulting in 160 con-
ditional probability scores. We normalize these
scores into a single distribution ? the mental state
distribution for that query tuple. We then combine
5
http://nlp.stanford.edu/software/
corenlp.shtml.
all resulting distributions, one from each query tu-
ple, and take the average to produce a single dis-
tribution over mental state labels for the video. As
before, we prune this distribution by taking the
top-ranked items that cover a large fraction ? of
total probability. The pruned distribution is renor-
malized to yield the final response distribution.
4.3 Event-centric with Deleted Interpolation
The sent model has two limitations. On one hand,
it is too sparse: the single sentence neighborhood
window is too small to reliably estimate the fre-
quencies of trigrams for the probabilities of men-
tal state terms. On the other hand, it may be too
lenient, as it extracts all mental state mentions ap-
pearing in the same sentence with the activity, or
event, under consideration, regardless if they ap-
ply to this event or not. We address these limita-
tions next with an event-centric model (event).
Intuitively, the event model focuses on the men-
tal state labels of event participants. Formally,
these mental state terms are extracted as follows:
1: We identify event participants (or actors). We
do this by analyzing the syntactic dependencies of
sentences containing the target verb (e.g., chase)
to find the subject and object. In most cases, the
nominal subject of the verb chase is the chaser and
the direct object is the person being chased. We
implemented additional patterns to model passive
voice and other exceptions. We used Stanford?s
CoreNLP toolkit for syntactic dependency parsing
and the downstream coreference resolution.
2: Once the phrases that point to actors are iden-
tified, we identify all mentions of these actors in
the entire document by traversing the coreference
chains containing the phrases extracted in the pre-
vious step. The sentences traversed in the chains
define the neighborhood area for this model.
3: Lastly, we identify the mental state terms of
event participants using a second set of syntac-
tic patterns. First, we inspect several copulative
verbs, such as to be and feel, and extract men-
tal state labels from these structures if the corre-
sponding subject is one of the mentions detected
above. Second, we search for mental states along
adjectival modifier relations, where the head is an
actor mention. For all patterns, we make sure to
filter for only mental state complements belong-
ing to the initial seed list. The same POS restric-
tion as in the other models also applies. We incre-
ment the joint frequency f for the n-gram once for
125
each neighborhood that properly contain all search
terms from the n-gram in the correct POS.
The event model addresses both limitations of
the sent model: it avoids the lenient extraction of
mental state labels by focusing on labels associ-
ated with event participants; it addresses sparsity
by considering all mentions of event participants
in a document.
To understand the impact of this model, we
compare it against two additional baselines. The
first baseline investigates the importance of focus-
ing on mental state terms associated with event
participants. This model, called coref, implements
the first two steps of the above algorithm, but in-
stead of extracting only mental state terms associ-
ated with event actors (last step), it considers all
mentions appearing anywhere in the coreference
neighborhood. That is, all unique sentences tra-
versed by the relevant coreference chains are first
pieced together to define a single neighborhood for
a given document; then the relative joint frequen-
cies of n-grams are computed by incrementing f
once for each neighborhood that contains all terms
with correct POS tags.
The second baseline analyzes the importance of
coreference resolution to our problem. This model
is similar to sent, with the modification that it in-
creases the size of the neighborhood window to in-
clude the immediate neighbors of target sentences
that contain activity labels. We call this the win-n
model: The window around a target verb contains
2n + 1 sentences. We build the context neigh-
borhood by concatenating all target sentences and
their windows together for a given document. This
defines a single neighborhood for each document.
This contrasts with the sent model, in which the
neighborhood is defined for each sentence con-
taining the activity label in the document, resulting
in several possible neighborhoods in a document.
The joint frequency f for each n-gram ? where
n > 1 ? is computed similarly with the coref
model: it is incremented once for each neighbor-
hood that contains all the terms from the n-gram
in the correct POS. Frequencies for unigrams are
computed similar to sent.
As before, 160 different trigrams are generated
for each query tuple, one for each mental state la-
bel in the seed set, resulting in 160 conditional
probability scores. We similarly combine these
scores and generate a single pruned distribution as
the response for each of the model above.
G (irate, 0.8), (afraid, 0.2)
R
1
(angry, 0.6), (mad, 0.4)
R
2
(irate, 0.2), (afraid, 0.8)
R
3
(mad, 0.4), (irate, 0.4), (scared, 0.2)
Table 2: We show an example gold standard dis-
tribution G and several candidate response distri-
butions to be matched against G. Here, R
3
best
matches the shape and meaning of G, because
(irate, mad) and (afraid, scared) are close syn-
onyms. R
2
appears to match G semantically, but
matches its shape poorly. R
1
misses one of the
mental state labels, afraid, but contains labels that
are semantically close to the weightiest term in G.
4.4 Ensemble Model
We combined the results from the event and
vec models to produce an ensemble model (ens)
which, for a mental state label m, returns the aver-
age of m?s scores according to the response distri-
butions of the two individual models.
5 Evaluation Measures
LetR denote the response distribution over mental
state labels produced for a single video by one of
the models described in the previous section, and
let G denote the gold standard distribution pro-
duced for the same video by MTurk workers. If
R is similar to G then our models produce simi-
lar mental state terms as the workers. There are
many ways to compare distributions (e.g., KL dis-
tance, chi-square statistics) but these give bad re-
sults when distributions are sparse. More impor-
tantly, for our purposes, the measures that compare
the shapes of distributions do not allow semantic
comparisons at the level of distribution elements.
Suppose R assigns high scores to angry and mad,
only, while G assigns a high score to happy, only.
Clearly, R is wrong. But if insteadG had assigned
a high score to irate, only, then R would be more
right than wrong because, at the level of the indi-
vidual elements, angry and mad are similar to irate
but not similar to happy.
We describe a series of measures, starting with
the familiar F
1
score, and discuss their applicabil-
ity. To illustrate the effectiveness of each measure,
we will use the examples shown in Table 2.
5.1 F
1
Score
The F
1
score measures the similarity between two
sets of elements, R and G. F
1
= 1 when R = G
126
and F
1
= 0 when R and G share no elements. F
1
is the harmonic mean of precision and recall:
precision =
|R ?G|
|R|
, recall =
|R ?G|
|G|
,
(1)
F
1
= 2 ?
precision ? recall
precision+ recall
. (2)
The F
1
score penalizes the responses in Table 3
that include semantically similar labels to those in
G, and fails to reflect the weights of the labels in
G and R.
5.2 Similarity-Aligned F
1
Score
Although the standard F
1
does not immediately fit
our needs, it is a good starting point. We can in-
corporate the semantic similarity of distribution el-
ements by generalizing the formulas for precision
and recall as follows:
precision =
1
|R|
?
r?R
max
g?G
?(r, g) ,
recall =
1
|G|
?
g?G
max
r?R
?(r, g) ,
(3)
where ? ? [0, 1] is a function that yields the simi-
larity between two elements. The standard F
1
has:
?(r, g) =
{
1 , if r = g
0 , otherwise
,
but clearly ? can be defined to take values pro-
portional to the similarity of r and g. We can
choose from a wide range of semantic similarity
and relatedness measures that are based on Word-
Net (Pedersen et al., 2004). The recent RNNLM
of Mikolov opens the door to even more similar-
ity measures based on vector space representations
of words (Mikolov et al., 2013). After experi-
mentations, we settled on one proposed by Hirst
and St-Onge (1998). It represents two lexicalized
concepts as semantically close if their WordNet
synsets are connected by a path that is not too
long and that ?does not change direction too of-
ten? (Hirst and St-Onge, 1998). We chose this
metric because it has a finite range, accommodates
numerous POS pairs, and works well in practice.
Given the generalized precision and recall for-
mulas in Eq 3, our similarity-aligned (SA) F
1
score can be computed in the usual way, as the
harmonic mean of precision and recall (Eq 2).
SA-F
1
is inspired by the Constrained Entity-
Aligned F-Measure (CEAF) metric proposed
F
1
SA-F
1
CWSA-F
1
p r f
1
p r f
1
p r f
1
R
1
0 0 0 1 .5
2
3
1 .8 .89
R
2
1 1 1 1 1 1 .4 .4 .4
R
3
1
3
.5 .4 1 1 1 1 1 1
Table 3: The precision (p), recall (r), and F
1
(f
1
) scores under various evaluation models are
presented for the examples from Table 2. Sup-
pose that ?(irate, angry) = ?(irate,mad) =
?(afraid, scared) = 1, with ? of any two identi-
cal strings being 1, and ? of all other pairs are 0.
by (Luo, 2005) for coreference resolution. CEAF
computes an optimal one-to-one mapping between
subsets of reference and system entities before it
computes recall, precision and F. Similarly, SA-F
1
finds optimal mappings between the labels of the
two sets based on ? (this is what the max terms in
Eq 3 do). Table 3 shows that SA-F
1
correctly re-
wards the use of synonyms. The high scores given
to R
2
, however, indicate that it does not measure
the similarity between distribution shapes.
5.3 Constrained Weighted Similarity-Aligned
F
1
Score
Let R(r) and G(r) be the probabilities of label
r in the R and G distributions, respectively. Let
?
?
S
(`) denote the best similarity score achievable
when comparing elements from set S to ` us-
ing the similarity function ?. That is, ?
?
S
(`) =
max
e?S
?(`, e). We can easily weight ?
?
S
(`) by
the probability of `. For example, we might re-
define precision as
?
r?R
R(r) ??
?
G
(r). However,
this would not account for the probability of r in
the gold standard distribution, G.
An analogy might help here: Suppose we have
an unknown ?mystery bag? of 100 colored pen-
cils that we will try to match with a ?response
bag? of pencils. If we fill our response bag with
100 crimson pencils, while the mystery bag con-
tains only 25 crimson pencils, then our precision
score should get points only for the first 25 pen-
cils, while the remaining 75 in the response bag
should not be rewarded. For recall, the reward
given for each color in the mystery bag is capped
by the number of pencils of that color in the re-
sponse bag. The analogy is complete when we
consider that crimson pencils should perhaps be
partially rewarded when matched by cardinal, rose
or cerise pencils. In other words, a similarity mea-
127
sure should account for an accumulated mass of
synonyms. Let M
S
(`) denote the subset of terms
from S that have the best similarity score to `:
M
S
(`) = {e | ?(`, e) = ?
?
S
(`), ?e ? S} .
We define new forms of precision and recall as:
p =
?
r?R
min
?
?
R(r),
?
e?M
G
(r)
G(e)
?
?
?
?
G
(r) ,
r =
?
g?G
min
?
?
G(g),
?
e?M
R
(g)
R(e)
?
?
?
?
R
(g) .
(4)
The resulting constrained weighted similarity-
aligned (CWSA) F
1
score is the harmonic mean
of these new precision and recall scores. Table 3
shows that CWSA-F
1
yields the most intuitive
evaluation of the response distributions, down-
weighting R
2
in favor of R
3
and R
1
.
6 Experimental Procedure
As described in Section 3, MTurk workers anno-
tated 26 videos by identifying the actor types and
mental state labels for each video. The actor types
become query tuples of the form (activity, actor)
and the mental state labels are compiled into one
probability distribution over labels for each video,
designated G. The query tuples were provided to
our neighborhood models (Sec. 4), which returned
a response distribution over mental state labels for
each video, designated R.
We selected four videos of the 26 to calibrate
the prune parameters ? and the interpolation pa-
rameters ? (Sec. 4). One of these videos contains
children, one has police involvement, and two con-
tain adults. We asked additional MTurk workers to
annotate these videos, yielding an independent set
of annotations to be used solely for calibration.
The experimental question is, how well does G
match R for each video?
7 Results & Discussions
We report the average performance of our mod-
els along with two additional baseline methods in
Table 4. The na??ve baseline method unif simply
binds R to the initial seed set of 160 mental state
labels with uniform probability, while the stronger
freq baseline uses the occurrence frequency dis-
tribution of the labels from the Gigaword corpus
(note that only occurrences tagged as adjectives or
F
1
CWSA-F
1
p r f
1
p r f
1
unif .107 .750 .187 .284 .289 .286
freq .107 .750 .187 .362 .352 .355
sent .194 .293 .227 .366 .376 .368
vec .226 .145 .175 .399 .392 .393
coref .264 .251 .253 .382 .461 .416
event .231 .303 .256 .446 .488 .463
ens .259 .296 .274 .488 .517 .500
Table 4: The average evaluation performance
across 26 different chase videos are shown against
2 different baselines for all proposed models. Bold
font indicates the best score in a given column.
verbs were counted). All average improvements
of the ensemble model over the baseline models
are significant (p < 0.01). Significance tests were
one-tailed and were based on nonparametric boot-
strap resampling with 10, 000 iterations.
Using the classical F
1
measure, the coref model
scored highest on precision, while the ensemble
method did best on F
1
. Not surprisingly, no model
can top the baseline methods on recall as both
baselines use the entire seed set of 160 terms.
Even so, the average recall for the baselines were
only .750, which means that the initial seed set did
not include words that were used by the MTurk an-
notators. As we?ve mentioned, the classical F
1
is
misleading because it does not credit synonyms.
For example, in one movie, one of our models
was rewarded once for matching the label angry
and penalized six times for also reporting irate,
enraged, raging, upset, furious, and mad. Fre-
quently, our models were penalized for using the
terms scared and afraid instead of fearful.
Under the CWSA-F
1
evaluation measure,
which correctly accounts for both synonyms and
label probabilities, our ensemble model performed
best. The average CWSA-F
1
score of the ensem-
ble model improves upon the simple uniform base-
line unif by almost 75%, and over the stronger
freq baseline by over 40%. The ensemble method
also outperforms each individual method in all
measured scores. These improvements were also
found to be significant. This strongly suggests
that the vec and event models are complementary,
and not entirely redundant. Furthermore, Table 4
shows that the event model performs considerably
better than coref. This result emphasizes the im-
portance of focusing on the mental state labels of
event participants rather than considering all men-
tal state terms collocated in the same sentence with
an actor or action verb.
128
Models CWSA-F1 Versus coref p-value
win-0 0.388682 ?0.027512 0.0067
win-1 0.415328 ?0.000866 0.4629
win-2 0.399777 ?0.016417 0.0311
win-3 0.392832 ?0.023362 0.0029
Table 5: The average CWSA-F
1
scores for the
win-n model with different window parameters are
shown in comparison to the coref model. The
coref model outperformed all tested configura-
tions, though the difference is not significant for
n = 1. The p-value based on the average differ-
ences were obtained using one-tailed nonparamet-
ric bootstrap resampling with 10, 000 iterations.
Table 5 explores the effectiveness of corefer-
ence resolution in expanding the neighborhood
area. The coref model outperformed the simple
windowing method under every tested configura-
tion. However, the improvement over windowing
with n = 1 is not significant. This can be ex-
plained by fact that immediately neighboring sen-
tences are more likely to be related. Moreover,
since newswire articles tend to be short, the neigh-
borhoods generated by win-1 tend to be similar to
those generated by coref. In general, coref does
not do worse than a simple windowing method and
has the bonus advantage of providing references to
the actors of interest for downstream processes.
In Table 6, we show the performance results
based on the types of chase scenarios happening in
the videos. The average scores under the uniform
baseline unif for chase videos involving children
and sporting events are lower than for police and
other chases. This suggests that our seed set of
160 mental state labels is biased towards the latter
types of events, and is not as fit to describe chases
involving children.
On average, videos involving police officers
show the biggest improvement in the CWSA-F
1
scores over the unif baseline (+0.2693), whereas
videos involving children received the lowest gain
(+0.1517). We believe this is the effect of the
Gigaword text corpus, which is a comprehensive
archive of newswire text, and thus is heavily bi-
ased towards high-speed and violent chases in-
volving the police. The Gigaword corpus is not
the place to find children happily chasing each
other. Similarly, sports-related chases, which are
also news-worthy, have a higher gain than chil-
dren?s videos on average.
Categories Unif Ensemble Gain
children 0.2082 0.3599 +0.1517
police 0.3313 0.6006 +0.2693
sports 0.2318 0.4126 +0.1808
others 0.3157 0.5457 +0.2300
Table 6: The average CWSA-F
1
scores for the en-
semble model are shown in comparison to the uni-
form baseline method, categorize by video types.
8 Conclusion and Future Work
We introduced the novel task of identifying latent
attributes in video scenes, specifically the men-
tal states of actors in chase scenes. We showed
that these attributes can be identified by using ex-
plicit features of videos to query text corpora, and
from the resulting texts extract attributes that are
latent in the videos. We presented several largely
unsupervised methods for identifying distributions
of actors? mental states in video scenes. We de-
fined a similarity measure, CWSA-F
1
, for com-
paring distributions of mental state labels that ac-
counts for both semantic relatedness of the labels
and their probabilities in the corresponding distri-
butions. We showed that very little information
from videos is needed to produce good results that
significantly outperform baseline methods.
In the future, we plan to add more detection
types. Additional contextual information from
videos (e.g., scene locations) should help improve
performance, especially on tougher videos (e.g.,
videos involving children chases). Moreover, we
believe that the initial seed set of mental state la-
bels can be learned simultaneously with the ex-
traction patterns of the event model using a mutual
bootstrapping method, similar to that of (Riloff
and Jones, 1999).
Currently, our experiments assume one distri-
bution of mental state labels for each video. They
do not distinguish between the mental states of the
chaser and chasee, while in reality these partici-
pants may be in very different states of mind. Our
event model is capable of making this distinction
and we will test its performance on this task in the
future. We also plan to test the effectiveness of our
models with actual computer vision detectors. As
a first approximation, we will simulate the noisy
nature of detectors by degrading the quality of an-
notated data. Using artificial noise on ground-truth
data, we can simulate the performance of real de-
tectors and test the robustness of our models.
129
References
Abdul Rehman Abbasi, Matthew N. Dailey, Nitin V.
Afzulpurkar, and Takeaki Uno. 2009. Student men-
tal state inference from unintentional body gestures
using dynamic Bayesian networks. Journal on Mul-
timodal User Interfaces, 3(1-2):21?31, December.
Tadas Baltrusaitis, Daniel McDuff, Ntombikayise
Banda, Marwa Mahmoud, Rana el Kaliouby, Peter
Robinson, and Rosalind Picard. 2011. Real-time
inference of mental states from facial expressions
and upper body gestures. In Face and Gesture 2011,
pages 909?914. IEEE, March.
Thorsten Brants. 2000. TnT: A statistical part-of-
speech tagger. In Proceedings of the sixth confer-
ence on Applied natural language processing, pages
224?231, Morristown, NJ, USA. Association for
Computational Linguistics.
Ernesto Brau, Jinyan Guan, Kyle Simek, Luca Del
Pero, Colin Reimer Dawson, and Kobus Barnard.
2013. Bayesian 3D Tracking from monocular video.
In The IEEE International Conference on Computer
Vision (ICCV), December.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th annual meeting on Association for Computa-
tional Linguistics -, pages 16?23, Morristown, NJ,
USA. Association for Computational Linguistics.
R. El Kaliouby and P. Robinson. 2004. Real-Time In-
ference of Complex Mental States from Facial Ex-
pressions and Head Gestures. In 2004 Conference
on Computer Vision and Pattern Recognition Work-
shop, pages 154?154. IEEE.
Pedro Felzenszwalb, David McAllester, and Deva Ra-
manan. 2008. A discriminatively trained, multi-
scale, deformable part model. In 2008 IEEE Confer-
ence on Computer Vision and Pattern Recognition,
pages 1?8. IEEE, June.
Ryan Gabbard, Marjorie Freedman, and
RM Weischedel. 2011. Coreference for learn-
ing to extract relations: yes, Virginia, coreference
matters. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies: short papers -
Volume 2, pages 288?293.
Maria Gendron, Debi Roberson, Jacoba Marieta
van der Vyver, and Lisa Feldman Barrett. 2014.
Cultural relativity in perceiving emotion from vo-
calizations. Psychological science, 25(4):911?20,
April.
J Giebel, DM Gavrila, and C Schn?orr. 2004. A
bayesian framework for multi-cue 3d object track-
ing. In Computer Vision-ECCV 2004, pages 241?
252.
Graeme Hirst and D St-Onge. 1998. Lexical chains as
representations of context for the detection and cor-
rection of malapropisms. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical Database
(Language, Speech, and Communication), pages
305?332. The MIT Press.
LJ Li, Hao Su, L Fei-Fei, and EP Xing. 2010. Ob-
ject bank: A high-level image representation for
scene classification & semantic feature sparsifica-
tion. In Advances in Neural Information Processing
Systems.
Zhilei Liu and Shangfei Wang. 2011. Emotion recog-
nition using hidden Markov models from facial tem-
perature sequence. In ACII?11 Proceedings of the
4th international conference on Affective computing
and intelligent interaction - Volume Part II, pages
240?247.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of the confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing - HLT
?05, pages 25?32, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
MC De Marneffe, CD Manning, and Christopher Potts.
2010. ?Was it good? It was provocative.? Learning
the meaning of scalar adjectives. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics, pages 167?176.
Stephen J. McKenna, Sumer Jabri, Zoran Duric, Azriel
Rosenfeld, and Harry Wechsler. 2000. Tracking
Groups of People. Computer Vision and Image Un-
derstanding, 80(1):42?56, October.
D M McNair, M Lorr, and L F Droppleman. 1971.
Profile of Mood States (POMS).
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781, pages 1?12.
George A. Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):39?41, November.
Mitra Mohtarami, Hadi Amiri, Man Lan, and
Chew Lim Tan. 2011. Predicting the uncertainty
of sentiment adjectives in indirect answers. In Pro-
ceedings of the 20th ACM international conference
on Information and knowledge management - CIKM
?11, page 2485, New York, New York, USA. ACM
Press.
CB Ng, YH Tay, and BM Goi. 2012. Recognizing hu-
man gender in computer vision: a survey. PRICAI
2012: Trends in Artificial Intelligence, 7458:335?
346.
S O?Hara and B. A. Draper. 2012. Scalable action
recognition with a subspace forest. In 2012 IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 1210?1217. IEEE, June.
130
Ted Pedersen, S Patwardhan, and J Michelizzi. 2004.
WordNet::Similarity: measuring the relatedness of
concepts. In Proceedings of the Nineteenth Na-
tional Conference on Artificial Intelligence (AAAI-
04), pages 1024?1025, San Jose, CA.
Ronald Poppe. 2010. A survey on vision-based human
action recognition. Image and Vision Computing,
28(6):976?990, June.
Deva Ramanan, David a Forsyth, and Andrew Zisser-
man. 2007. Tracking people by learning their ap-
pearance. IEEE transactions on pattern analysis
and machine intelligence, 29(1):65?81, January.
E Riloff and R Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of the sixteenth national conference
on Artificial intelligence (AAAI-1999), pages 474?
479.
S. Sadanand and J. J. Corso. 2012. Action bank: A
high-level representation of activity in video. In
2012 IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 1234?1241. IEEE, June.
C Schuldt, I Laptev, and B Caputo. 2004. Recognizing
human actions: a local SVM approach. In Proceed-
ings of the 17th International Conference on Pattern
Recognition, 2004. ICPR 2004., pages 32?36 Vol.3.
IEEE.
M. Sokolova and G. Lapalme. 2011. Learning opin-
ions in user-generated web content. Natural Lan-
guage Engineering, 17(04):541?567, March.
Daniel Weinland, Remi Ronfard, and Edmond Boyer.
2011. A survey of vision-based methods for action
representation, segmentation and recognition. Com-
puter Vision and Image Understanding, 115(2):224?
241, February.
Yi Yang and Deva Ramanan. 2011. Articulated pose
estimation with flexible mixtures-of-parts. In CVPR
2011, pages 1385?1392. IEEE, June.
131
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 39?47,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Word Segmentation as General Chunking
Daniel Hewlett and Paul Cohen
Department of Computer Science
University of Arizona
Tucson, AZ 85721
{dhewlett,cohen}@cs.arizona.edu
Abstract
During language acquisition, children learn
to segment speech into phonemes, syllables,
morphemes, and words. We examine word
segmentation specifically, and explore the
possibility that children might have general-
purpose chunking mechanisms to perform
word segmentation. The Voting Experts (VE)
and Bootstrapped Voting Experts (BVE) algo-
rithms serve as computational models of this
chunking ability. VE finds chunks by search-
ing for a particular information-theoretic sig-
nature: low internal entropy and high bound-
ary entropy. BVE adds to VE the abil-
ity to incorporate information about word
boundaries previously found by the algorithm
into future segmentations. We evaluate the
general chunking model on phonemically-
encoded corpora of child-directed speech, and
show that it is consistent with empirical results
in the developmental literature. We argue that
it offers a parsimonious alternative to special-
purpose linguistic models.
1 Introduction
The ability to extract words from fluent speech ap-
pears as early as the seventh month in human de-
velopment (Jusczyk et al, 1999). Models of this
ability have emerged from such diverse fields as lin-
guistics, psychology and computer science. Many
of these models make unrealistic assumptions about
child language learning, or rely on supervision, or
are specific to speech or language. Here we present
an alternative: a general unsupervised model of
chunking that performs very well on word segmen-
tation tasks. We will examine the Voting Experts,
Bootstrapped Voting Experts, and Phoneme to Mor-
pheme algorithms in Section 2. Each searches for a
general, information-theoretic signature of chunks.
Each can operate in either a fully unsupervised set-
ting, where the input is a single continuous se-
quence of phonemes, or a semi-supervised setting,
where the input is a sequence of sentences. In Sec-
tion 4, we evaluate these general chunking methods
on phonetically-encoded corpora of child-directed
speech, and compare them to a representative set of
computational models of early word segmentation.
Section 4.4 presents evidence that words optimize
the information-theoretic signature of chunks. Sec-
tion 5 discusses segmentation methods in light of
what is known about the segmentation abilities of
children.
2 General Chunking
The Voting Experts algorithm (Cohen and Adams,
2001) defines the chunk operationally as a sequence
with the property that elements within the sequence
predict one another but do not predict elements out-
side the sequence. In information-theoretic terms,
chunks have low entropy internally and high entropy
at their boundaries. Voting Experts (VE) is a lo-
cal, greedy algorithm that works by sliding a rel-
atively small window along a relatively long input
sequence, calculating the internal and boundary en-
tropies of sequences within the window.
The name Voting Experts refers to the two ?ex-
perts? that vote on possible boundary locations:
One expert votes to place boundaries after se-
quences that have low internal entropy (also called
surprisal), given by HI(seq) = ? logP (seq).
The other places votes after sequences that have
39
high branching entropy, given by HB(seq) =
?
?
c?S P (c|seq) logP (c|seq), where S is the set
of successors to seq. In a modified version of VE,
a third expert ?looks backward? and computes the
branching entropy at locations before, rather than af-
ter, seq.
The statistics required to calculate HI and HB are
stored efficiently using an n-gram trie, which is typ-
ically constructed in a single pass over the corpus.
The trie depth is 1 greater than the size of the slid-
ing window. Importantly, all statistics in the trie are
normalized so as to be expressed in standard devia-
tion units. This allows statistics from sequences of
different lengths to be compared.
The sliding window is then passed over the cor-
pus, and each expert votes once per window for the
boundary location that best matches that expert?s cri-
teria. After voting is complete, the algorithm yields
an array of vote counts, each element of which is
the number of times some expert voted to segment
at that location. The result of voting on the string
thisisacat could be represented in the follow-
ing way, where the number between each letter is
the number of votes that location received, as in
t0h0i1s3i1s4a4c1a0t.
With the final vote totals in place, the boundaries
are placed at locations where the number of votes
exceeds a chosen threshold. For further details of the
Voting Experts algorithm see Cohen et al (2007),
and also Miller and Stoytchev (2008).
2.1 Generality of the Chunk Signature
The information-theoretic properties of chunks upon
which VE depends are present in every non-random
sequence, of which sequences of speech sounds are
only one example. Cohen et al (2007) explored
word segmentation in a variety of languages, as well
as segmenting sequences of robot actions. Hewlett
and Cohen (2010) demonstrated high performance
for a version of VE that segmented sequences of ut-
terances between a human teacher and an AI stu-
dent. Miller and Stoytchev (2008) applied VE in a
kind of bootstrapping procedure to perform a vision
task similar to OCR: first to chunk columns of pix-
els into letters, then to chunk sequences of these dis-
covered letters into words. Of particular relevance to
the present discussion are the results of Miller et al
(2009), who showed that VE was able to segment a
continuous audio speech stream into phonemes. The
input in that experiment was generated to mimic the
input presented to infants by Saffran et al (1996),
and was discretized for VE with a Self-Organizing
Map (Kohonen, 1988).
2.2 Similar Chunk Signatures
Harris (1955) noticed that if one proceeds incremen-
tally through a sequence of letters and asks speakers
of the language to list all the letters that could ap-
pear next in the sequence (today called the succes-
sor count), the points where the number increases
often correspond to morpheme boundaries. Tanaka-
Ishii and Jin (2006) correctly recognized that this
idea was an early version of branching entropy, one
of the experts in VE, and they developed an algo-
rithm called Phoneme to Morpheme (PtM) around it.
PtM calculates branching entropy in both directions,
but it does not use internal entropy, as VE does. It
detects change-points in the absolute branching en-
tropy rather than local maxima in the standardized
entropy. PtM achieved scores similar to those of VE
on word segmentation in phonetically-encoded En-
glish and Chinese.
Within the morphology domain, Johnson and
Martin?s HubMorph algorithm (2003) constructs a
trie from a set of words, and then converts it into
a DFA by the process of minimization. HubMorph
searches for stretched hubs in this DFA, which are
sequences of states in the DFA that have a low
branching factor internally, and high branching fac-
tor at the edges (shown in Figure 1). This is a nearly
identical chunk signature to that of VE, only with
successor/predecessor count approximating branch-
ing entropy. The generality of this idea was not lost
on Johnson and Martin, either: Speaking with re-
spect to the morphology problem, Johnson and Mar-
tin close by saying ?We believe that hub-automata
will be the basis of a general solution for Indo-
European languages as well as for Inuktitut.? 1
2.3 Chunking and Bootstrapping
Bootstrapped Voting Experts (BVE) is an exten-
sion to VE that incorporates knowledge gained from
prior segmentation attempts when segmenting new
input, a process known as bootstrapping. This
1Inuktitut is a polysynthetic Inuit language known for its
highly complex morphology.
40
Figure 1: The DFA signature of a hub (top) and stretched
hub in the HubMorph algorithm. Figure from Johnson
and Martin (2003).
knowledge does not consist in the memorization of
whole words (chunks), but rather in statistics de-
scribing the beginnings and endings of chunks. In
the word segmentation domain, these statistics ef-
fectively correspond to phonotactic constraints that
are inferred from hypothesized segmentations. In-
ferred boundaries are stored in a data structure called
a knowledge trie (shown in Figure 2), which is es-
sentially a generalized prefix or suffix trie.
a
3
t
3
t
3
h
2
s
1
o
1
root
. . .
a
3
t
3
t
3
h
2
s
1
o
1
root
#
3
#
3
o
1
n
1
#
1
e
3
n
1
Figure 2: A portion of the knowledge trie built from
#the#cat#sat#on#the#mat#. Numbers within
each node are frequency counts.
BVE was tested on a phonemically-encoded cor-
pus of child-directed speech and achieved a higher
level of performance than any other unsupervised al-
gorithm (Hewlett and Cohen, 2009). We reproduce
these results in Section 4.
3 Computational Models of Word
Segmentation
While many algorithms exist for solving the word
segmentation problem, few have been proposed
specifically as computational models of word seg-
mentation in language acquisition. One of the most
widely cited is MBDP-1 (Model-Based Dynamic
Programming) by Brent (1999). Brent describes
three features that an algorithm should have to qual-
ify as an algorithm that ?children could use for seg-
mentation and word discovery during language ac-
quisition.? Algorithms should learn in a completely
unsupervised fashion, should segment incrementally
(i.e., segment each utterance before considering the
next one), and should not have any built-in knowl-
edge about specific natural languages (Brent, 1999).
However, the word segmentation paradigm Brent
describes as ?completely unsupervised? is actually
semi-supervised, because the boundaries at the be-
ginning and end of each utterance are known to
be true boundaries. A fully unsupervised paradigm
would include no boundary information at all, mean-
ing that the input is, or is treated as, a continuous se-
quences of phonemes. The MBDP-1 algorithm was
not designed for operation in this continuous condi-
tion, as it relies on having at least some true bound-
ary information to generalize.
MBDP-1 achieves a robust form of bootstrapping
through the use of Bayesian maximum-likelihood
estimation of the parameters of a language model.
More recent algorithms in the same tradition, includ-
ing the refined MBDP-1 of Venkataraman (2001),
the WordEnds algorithm of Fleck (2008), and the
Hierarchical Dirichlet Process (HDP) algorithm of
Goldwater (2007), share this limitation. However,
infants are able to discover words in a single stream
of continuous speech, as shown by the seminal series
of studies by Saffran et al (1996; 1998; 2003). In
these studies, Saffran et al show that both adults and
8-month-old infants quickly learn to extract words
of a simple artificial language from a continuous
speech stream containing no pauses.
The general chunking algorithms VE, BVE, and
PtM work in either condition. The unsupervised,
continuous condition is the norm (Cohen et al,
2007; Hewlett and Cohen, 2009; Tanaka-Ishii and
Jin, 2006) but these algorithms are easily adapted
to the semi-supervised, incremental condition. Re-
call that these methods make one pass over the entire
corpus to gather statistics, and then make a second
pass to segment the corpus, thus violating Brent?s re-
quirement of incremental segmentation. To adhere
to the incremental requirement, the algorithms sim-
ply must segment each sentence as it is seen, and
then update their trie(s) with statistics from that sen-
tence. While VE and PtM have no natural way to
store true boundary information, and so cannot ben-
41
efit from the supervision inherent in the incremental
paradigm, BVE has the knowledge trie which serves
exactly this purpose. In the incremental paradigm,
BVE simply adds each segmented sentence to the
knowledge trie, which will inform the segmentation
of future sentences. This way it learns from its own
decisions as well as the ground truth boundaries sur-
rounding each utterance, much like MBDP-1 does.
BVE and VE were first tested in the incremental
paradigm by Hewlett and Cohen (2009), though only
on sentences from a literary corpus, George Orwell?s
1984.
4 Evaluation of Computational Models
In this section, we evaluate the general chunking al-
gorithms VE, BVE, and PtM in both the continu-
ous, unsupervised paradigm of Saffran et al (1996)
and the incremental, semi-supervised paradigm as-
sumed by bootstrapping algorithms like MBDP-1.
We briefly describe the artificial input used by Saf-
fran et al, and then turn to the broader problem
of word segmentation in natural languages by eval-
uating against corpora drawn from the CHILDES
database (MacWhinney and Snow, 1985).
We evaluate segmentation quality at two levels:
boundaries and words. At the boundary level, we
compute the Boundary Precision (BP), which is sim-
ply the percentage of induced boundaries that were
correct, and Boundary Recall (BR), which is the
percentage of true boundaries that were recovered
by the algorithm. These measures are commonly
combined into a single metric, the Boundary F-
score (BF), which is the harmonic mean of BP and
BR: BF = (2 ? BP ? BR)/(BP + BR). Gener-
ally, higher BF scores correlate with finding cor-
rect chunks more frequently, but for completeness
we also compute the Word Precision (WP), which is
the percentage of induced words that were correct,
and the Word Recall (WR), which is the percent-
age of true words that were recovered exactly by the
algorithm. These measures can naturally be com-
bined into a single F-score, the Word F-score (WF):
WF = (2?WP?WR)/(WP + WR).
4.1 Artificial Language Results
To simulate the input children heard during Saf-
fran et al?s 1996 experiment, we generated a corpus
of 400 words, each chosen from the four artificial
words from that experiment (dapiku, tilado,
burobi, and pagotu). As in the original study,
the only condition imposed on the random sequence
was that no word would appear twice in succession.
VE, BVE, and PtM all achieve a boundary F-score
of 1.0 whether the input is syllabified or considered
simply as a stream of phonemes, suggesting that a
child equipped with a chunking ability similar to VE
could succeed even without syllabification.
4.2 CHILDES: Phonemes
To evaluate these algorithms on data that is closer
to the language children hear, we used corpora
of child-directed speech taken from the CHILDES
database (MacWhinney and Snow, 1985). Two cor-
pora have been examined repeatedly in prior stud-
ies: the Bernstein Ratner corpus (Bernstein Rat-
ner, 1987), abbreviated BR87, used by Brent (1999),
Venkataraman (2001), Fleck (2008), and Goldwater
et al (2009), and the Brown corpus (Brown, 1973),
used by Gambell and Yang (2006).
Before segmentation, all corpora were encoded
into a phonemic representation, to better simulate
the segmentation problem facing children. The
BR87 corpus has a traditional phonemic encoding
created by Brent (1999), which facilitates compar-
ison with other published results. Otherwise, the
corpora are translated into a phonemic representa-
tion using the CMU Pronouncing Dictionary, with
unknown words discarded.
The BR87 corpus consists of speech from nine
different mothers to their children, who had an av-
erage age of 18 months (Brent, 1999). BR87 con-
sists of 9790 utterances, with a total of 36441 words,
yielding an average of 3.72 words per utterance. We
evaluate word segmentation models against BR87 in
two different paradigms, the incremental paradigm
discussed above and an unconstrained paradigm.
Many of the results in the literature do not constrain
the number of times algorithms can process the cor-
pus, meaning that algorithms generally process the
entire corpus once to gather statistics, and then at
least one more time to actually segment it. Results
of VE and other algorithms in this unconstrained set-
ting are presented below in Table 1. In this test, the
general chunking algorithms were given one contin-
uous corpus with no boundaries, while the results for
42
bootstrapping algorithms were reported in a semi-
supervised condition.
Algorithm BP BR BF WP WR WF
PtM 0.861 0.897 0.879 0.676 0.704 0.690
VE 0.875 0.803 0.838 0.614 0.563 0.587
BVE 0.949 0.879 0.913 0.793 0.734 0.762
MBDP-1 0.803 0.843 0.823 0.670 0.694 0.682
HDP 0.903 0.808 0.852 0.752 0.696 0.723
WordEnds 0.946 0.737 0.829 NR NR 0.707
Table 1: Results for the BR87 corpus with unconstrained
processing of the corpus. Algorithms in italics are semi-
supervised.
In the incremental setting, the corpus is treated as
a series of utterances and the algorithm must seg-
ment each one before moving on to the next. This is
designed to better simulate the learning process, as a
child would normally listen to a series of utterances
produced by adults, analyzing each one in turn. To
perform this test, we used the incremental versions
of PtM, VE, and BVE described in Section 3, and
compared them with MBDP-1 on the BR87 corpus.
Each point in Figure 3 shows the boundary F-score
of each algorithm on the last 500 utterances. Note
that VE and PtM do not benefit from the informa-
tion about boundaries at the beginnings and endings
of utterances, yet they achieve levels of performance
not very inferior to MBDP-1 and BVE, which do
leverage true boundary information.
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.5 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5
Bo
un
dar
y F
-Sc
ore
 (B
F)
Thousands of Utterances
VE
MBDP-1
BVE
PtM
Figure 3: Results for three chunking algorithms and
MBDP-1 on BR87 in the incremental paradigm.
We also produced a phonemic encoding of the
BR87 and Bloom73 (Bloom, 1973) corpora from
CHILDES with the CMU pronouncing dictionary,
which encodes stress information (primary, sec-
ondary, or unstressed) on phonemes that serve as
syllable nuclei. Stress information is known to be
a useful factor in word segmentation, and infants
appear to be sensitive to stress patterns by as early
as 8 months of age (Jusczyk et al, 1999). Results
with these corpora are shown below in Figures 4 and
5. For each of the general chunking algorithms, a
window size of 4 was used, meaning decisions were
made in a highly local manner. Even so, BVE out-
performs MBDP-1 in this arguably more realistic
setting, while VE and PtM rival it or even surpass
it. Note that the quite different results shown in Fig-
ure 3 and Figure 4 are for the same corpus, under
two different phonemic encodings, illustrating the
importance of accurately representing the input chil-
dren receive.
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
0.5 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5
Bo
un
dar
y F
-Sc
ore
 (B
F)
Thousands of Utterances
VE
MBDP-1
BVE
PtM
Figure 4: Results for chunking algorithms and MBDP-1
on BR87 (CMU) in the incremental paradigm.
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
0.5 1 1.5 2 2.5
Bo
un
dar
y F
-Sc
ore
 (B
F)
Thousands of Utterances
VE
MBDP-1
BVE
PtM
Figure 5: Results for chunking algorithms and MBDP-1
on Bloom73 (CMU) in the incremental paradigm.
4.3 CHILDES: Syllables
In many empirical studies of word segmentation in
children, especially after Saffran et al (1996), the
problem is treated as though syllables were the ba-
sic units of the stream to be segmented, rather than
phonemes. If we assume children can syllabify their
43
phonemic representation, and that word boundaries
only occur at syllable boundaries, then word seg-
mentation becomes a very different, and potentially
much easier, problem. This must be the case, as the
process of syllabification removes a high percent-
age of the potential boundary locations, and all of
the locations it removes would be incorrect choices.
Table 2 supports this argument. In the CHILDES
corpora examined here, over 85% of the words di-
rected to the child are monosyllabic. This means that
the trivial All-Locations baseline, which segments
at every possible location, achieves an F-measure of
0.913 when working with syllabic input, compared
to only 0.524 for phonemic input.
Gambell and Yang (2006) present an algorithm
for word segmentation that achieves a boundary F-
score of 0.946 on correctly syllabified input. In or-
der to achieve this level of performance, Gambell
and Yang use a form of bootstrapping combined
with a rule called the ?Unique Stress Constraint,?
or USC, which simply requires that each word con-
tain exactly one stressed syllable. Gambell and Yang
developed this algorithm partially as a response to
a hypothesis put forward by Saffran et al (1996)
to explain their own experimental results. Saffran
et al concluded that young infants can attend to
the transitional probabilities between syllables, and
posit word boundaries where transitional probability
(TP) is low. The TP from syllable X to syllable Y is
simply given by:
P (Y |X) = frequency of XY/frequency of X (1)
While TP is sufficient to explain the results of Saf-
fran et al?s 1996 study, it performs very poorly on
actual child-directed speech, regardless of whether
the probabilities are calculated between phonemes
(Brent, 1999) or syllables. Because of the dramatic
performance gains shown by the addition of USC
in testing, as well as the poor performance of TP,
Gambell and Yang conclude that the USC is required
for word segmentation and thus is a likely candidate
for inclusion in Universal Grammar (Gambell and
Yang, 2006).
However, as the results in Table 2 show, VE is
capable of slightly superior performance on syllable
input, without assuming any prior constraints on syl-
lable stress distribution. Moreover, the performance
of both algorithms is also only a few points above
Algorithm BP BR BF
TP 0.416 0.233 0.298
TP + USC 0.735 0.712 0.723
Bootstrapping + USC 0.959 0.934 0.946
Voting Experts 0.918 0.992 0.953
All Points 0.839 1.000 0.913
Table 2: Performance of various algorithms on the Brown
corpus from CHILDES. Other than VE and All Points,
values are taken from (Gambell and Yang, 2006).
the baseline of segmenting at every possible bound-
ary location (i.e., at every syllable). These results
show the limitations of simple statistics like TP, but
also show that segmenting a sequence of syllables is
a simple problem for more powerful statistical algo-
rithms like VE. The fact that a very high percentage
of the words found by VE have one stressed syllable
suggest that a rule like the USC could be emergent
rather than innate.
4.4 Optimality of the VE Chunk Signature
It is one thing to find chunks in sequences, another
to have a theory or model of chunks. The question
addressed in this section is whether the chunk sig-
nature ? low internal entropy and high boundary en-
tropy ? is merely a good detector of chunk bound-
aries, or whether it characterizes chunks, them-
selves. Is the chunk signature merely a good detec-
tor of word boundaries, or are words those objects
that maximize the signal from the signature? One
way to answer the question is to define a ?chunki-
ness score? and show that words maximize the score
while other objects do not.
The chunkiness score is:
Ch(s) =
Hf (s) +Hb(s)
2
? logPr(s) (2)
It is just the average of the forward and backward
boundary entropies, which our theory says should
be high at true boundaries, minus the internal en-
tropy between the boundaries, which should be low.
Ch(s) can be calculated for any segment of any se-
quence for which we can build a trie.
Our prediction is that words have higher chunk-
iness scores than other objects. Given a sequence,
such as the letters in this sentence, we can generate
other objects by segmenting the sequence in every
44
possible way (there are 2n?1 of these for a sequence
of length n). Every segmentation will produce some
chunks, each of which will have a chunkiness score.
For each 5-word sequence (usually between 18
and 27 characters long) in the Bloom73 corpus from
CHILDES, we generated all possible chunks and
ranked them by their chunkiness. The average rank
of true words was the 98.7th percentile of the distri-
bution of chunkiness. It appears that syntax is the
primary reason that true chunks do not rank higher:
When the word-order in the training corpus is scram-
bled, the rank of true words is the 99.6th percentile
of the chunkiness distribution. These early results,
based on a corpus of child-directed speech, strongly
suggest that words are objects that maximize chunk-
iness. Keep in mind that the chunkiness score knows
nothing of words: The probabilities and entropies on
which it is based are estimated from continuous se-
quences that contain no boundaries. It is therefore
not obvious or necessary that the objects that maxi-
mize chunkiness scores should be words. It might be
that letters, or phones, or morphemes, or syllables,
or something altogether novel maximize chunkiness
scores. However, empirically, the chunkiest objects
in the corpus are words.
5 Discussion
Whether segmentation is performed on phonemic or
syllabic sequences, and whether it is unsupervised or
provided information such as utterance boundaries
and pauses, information-theoretic algorithms such
as VE, PtM and especially BVE perform segmen-
tation very well. The performance of VE on BR87
is on par with other state-of-the-art semi-supervised
segmentation algorithms such as WordEnds (Fleck,
2008) and HDP (Goldwater et al, 2009). The
performance of BVE on corpora of child-directed
speech is unmatched in the unconstrained case, to
the best of our knowledge.
These results suggest that BVE provides a sin-
gle, general chunking ability that that accounts for
word segmentation in both scenarios, and potentially
a wide variety of other cognitive tasks as well. We
now consider other properties of BVE that are es-
pecially relevant to natural language learning. Over
time, BVE?s knowledge trie comes to represent the
distribution of phoneme sequences that begin and
end words it has found. We now discuss how this
knowledge trie models phonotactic constraints, and
ultimately becomes an emergent lexicon.
5.1 Phonotactic Constraints
Every language has a set of constraints on how
phonemes can combine together into syllables,
called phonotactic constraints. These constraints af-
fect the distribution of phonemes found at the be-
ginnings and ends of words. For example, words
in English never begin with /ts/, because it is not a
valid syllable onset in English. Knowledge of these
constraints allows a language learner to simplify the
segmentation problem by eliminating many possi-
ble segmentations, as demonstrated in Section 4.3.
This approach has inspired algorithms in the litera-
ture, such as WordEnds (Fleck, 2008), which builds
a statistical model of phoneme distributions at the
beginnings and ends of words. BVE also learns a
model of phonotactics at word boundaries by keep-
ing similar statistics in its knowledge trie, but can
do so in a fully unsupervised setting by inferring its
own set of high-precision word boundaries with the
chunk signature.
5.2 An Emergent Lexicon
VE does not represent explicitly a ?lexicon? of
chunks that it has discovered. VE produces chunks
when applied to a sequence, but its internal data
structures do not represent the chunks it has dis-
covered explicitly. By contrast, BVE stores bound-
ary information in the knowledge trie and refines it
over time. Simply by storing the beginnings and
endings of segments, the knowledge trie comes to
store sequences like #cat#, where # represents a
word boundary. The set of such bounded sequences
constitutes an emergent lexicon. After segmenting
a corpus of child-directed speech, the ten most fre-
quent words of this lexicon are you, the, that, what,
is, it, this, what?s, to, and look. Of the 100 most
frequent words, 93 are correct. The 7 errors include
splitting off morphemes such as ing, and merging
frequently co-occurring word pairs such as do you.
6 Implications for Cognitive Science
Recently, researchers have begun to empirically as-
sess the degree to which segmentation algorithms
accurately model human performance. In particular,
45
Frank et al (2010) compared the segmentation pre-
dictions made by TP and a Bayesian Lexical model
against the segmentation performance of adults, and
found that the predictions of the Bayesian model
were a better match for the human data. As men-
tioned in Section 4.3, computational evaluation has
demonstrated repeatedly that TP provides a poor
model of segmentation ability in natural language.
Any of the entropic chunking methods investigated
here can explain the artificial language results moti-
vating TP, as well as the segmentation of natural lan-
guage, which argues for their inclusion in future em-
pirical investigations of human segmentation ability.
6.1 Innate Knowledge
The word segmentation problem provides a reveal-
ing case study of the relationship between nativism
and statistical learning. The initial statistical pro-
posals, such as TP, were too simple to explain the
phenomenon. However, robust statistical methods
were eventually developed that perform the linguis-
tic task successfully. With statistical learning mod-
els in place that perform as well as (or better than)
models based on innate knowledge, the argument for
an impoverished stimulus becomes difficult to main-
tain, and thus the need for a nativist explanation is
removed.
Importantly, it should be noted that the success
of a statistical learning method is not an argument
that nothing is innate in the domain of word segmen-
tation, but simply that it is the learning procedure,
rather than any specific linguistic knowledge, that is
innate. The position that a statistical segmentation
ability is innate is bolstered by speech segmentation
experiments with cotton-top tamarins (Hauser et al,
2001) that have yielded similar results to Saffran?s
experiments with human infants, suggesting that the
ability may be present in the common ancestor of
humans and cotton-top tamarins.
Further evidence for a domain-general chunking
ability can be found in experiments where human
subjects proved capable of discovering chunks in
a single continuous sequence of non-linguistic in-
puts. Saffran et al (1999) found that adults and 8-
month-old infants were able to segment sequences
of tones at the level of performance previously estab-
lished for syllable sequences (Saffran et al, 1996).
Hunt and Aslin (1998) measured the reaction time
of adults when responding to a single continuous
sequence of light patterns, and found that subjects
quickly learned to exploit predictive subsequences
with quicker reactions, while delaying reaction at
subsequence boundaries where prediction was un-
certain. In both of these results, as well as the word
segmentation experiments of Saffran et al, humans
learned to segment the sequences quickly, usually
within minutes, just as general chunking algorithms
quickly reach high levels of performance.
7 Conclusion
We have shown that a domain-independent theory of
chunking can be applied effectively to the problem
of word segmentation, and can explain the ability of
children to segment a continuous sequence, which
other computational models examined here do not
attempt to explain. The human ability to segment
continuous sequences extends to non-linguistic do-
mains as well, which further strengthens the gen-
eral chunking account, as these chunking algorithms
have been successfully applied to a diverse array of
non-linguistic sequences. In particular, BVE com-
bines the power of the information-theoretic chunk
signature with a bootstrapping capability to achieve
high levels of performance in both the continuous
and incremental paradigms.
8 Future Work
Within the CHILDES corpus, our results have only
been demonstrated for English, which leaves open
the possibility that other languages may present
a more serious segmentation problem. In En-
glish, where many words in child-directed speech
are mono-morphemic, the difference between find-
ing words and finding morphs is small. In some
languages, ignoring the word/morph distinction is
likely to be a more costly assumption, especially
for highly agglutinative or even polysynthetic lan-
guages. One possibility that merits further explo-
ration is that, in such languages, morphs rather than
words are the units that optimize chunkiness.
Acknowledgements
This work was supported by the Office of Naval Re-
search under contract ONR N00141010117. Any views
expressed in this publication are solely those of the au-
thors and do not necessarily reflect the views of the ONR.
46
References
Richard N. Aslin, Jenny R. Saffran, and Elissa L. New-
port. 1998. Computation of Conditional Probability
Statistics by 8-Month-Old Infants. Psychological Sci-
ence, 9(4):321?324.
Nan Bernstein Ratner, 1987. The phonology of parent-
child speech, pages 159?174. Erlbaum, Hillsdale, NJ.
Lois Bloom. 1973. One Word at a Time. Mouton, Paris.
Michael R. Brent. 1999. An Efficient, Probabilistically
Sound Algorithm for Segmentation and Word Discov-
ery. Machine Learning, (34):71?105.
Roger Brown. 1973. A first language: The early stages.
Harvard University, Cambridge, MA.
Paul Cohen and Niall Adams. 2001. An algorithm
for segmenting categorical time series into meaning-
ful episodes. In Proceedings of the Fourth Symposium
on Intelligent Data Analysis.
Paul Cohen, Niall Adams, and Brent Heeringa. 2007.
Voting Experts: An Unsupervised Algorithm for
Segmenting Sequences. Intelligent Data Analysis,
11(6):607?625.
Margaret M. Fleck. 2008. Lexicalized phonotactic word
segmentation. In Proceedings of The 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 130?138,
Columbus, Ohio, USA. Association for Computational
Linguistics.
Michael C Frank, Harry Tily, Inbal Arnon, and Sharon
Goldwater. 2010. Beyond Transitional Probabilities :
Human Learners Impose a Parsimony Bias in Statisti-
cal Word Segmentation. In Proceedings of the 32nd
Annual Meeting of the Cognitive Science Society.
Timothy Gambell and Charles Yang. 2006. Statistics
Learning and Universal Grammar: Modeling Word
Segmentation. In Workshop on Psycho-computational
Models of Human Language.
Sharon Goldwater, Thomas L Griffiths, and Mark John-
son. 2009. A Bayesian Framework for Word Segmen-
tation: Exploring the Effects of Context. Cognition,
112(1):21?54.
Sharon Goldwater. 2007. Nonparametric Bayesian mod-
els of lexical acquisition. Ph.D. dissertation, Brown
University.
Zellig S. Harris. 1955. From Phoneme to Morpheme.
Language, 31(2):190?222.
Marc D. Hauser, Elissa L. Newport, and Richard N.
Aslin. 2001. Segmentation of the speech stream in a
non-human primate: statistical learning in cotton-top
tamarins. Cognition, 78(3):B53?64.
Daniel Hewlett and Paul Cohen. 2009. Bootstrap Voting
Experts. In Proceedings of the Twenty-first Interna-
tional Joint Conference on Artificial Intelligence.
Daniel Hewlett and Paul Cohen. 2010. Artificial General
Segmentation. In The Third Conference on Artificial
General Intelligence.
Ruskin H. Hunt and Richard N. Aslin. 1998. Statisti-
cal learning of visuomotor sequences: Implicit acqui-
sition of sub-patterns. In Proceedings of the Twentieth
Annual Conference of the Cognitive Science Society,
Mahwah, NJ. Lawrence Erlbaum Associates.
Howard Johnson and Joel Martin. 2003. Unsupervised
learning of morphology for English and Inuktitut. Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology (HLT-
NAACL 2003), pages 43?45.
Peter W. Jusczyk, Derek M. Houston, and Mary New-
some. 1999. The Beginnings of Word Segmentation
in English-Learning Infants. Cognitive Psychology,
39(3-4):159?207.
Teuvo Kohonen. 1988. Self-organized formation of topo-
logically correct feature maps.
Brian MacWhinney and Catherine E Snow. 1985. The
child language data exchange system (CHILDES).
Journal of Child Language.
Matthew Miller and Alexander Stoytchev. 2008. Hierar-
chical Voting Experts: An Unsupervised Algorithm for
Hierarchical Sequence Segmentation. In Proceedings
of the 7th IEEE International Conference on Develop-
ment and Learning, pages 186?191.
Matthew Miller, Peter Wong, and Alexander Stoytchev.
2009. Unsupervised Segmentation of Audio Speech
Using the Voting Experts Algorithm. Proceedings of
the 2nd Conference on Artificial General Intelligence
(AGI 2009).
Jenny R. Saffran and Erik D. Thiessen. 2003. Pattern
induction by infant language learners. Developmental
Psychology, 39(3):484?494.
Jenny R. Saffran, Richard N. Aslin, and Elissa L. New-
port. 1996. Statistical Learning by 8-Month-Old In-
fants. Science, 274(December):926?928.
Jenny R. Saffran, Elizabeth K Johnson, Richard N. Aslin,
and Elissa L. Newport. 1999. Statistical learning of
tone sequences by human infants and adults. Cogni-
tion, 70(1):27?52.
Kumiko Tanaka-Ishii and Zhihui Jin. 2006. From
Phoneme to Morpheme: Another Verification Using
a Corpus. In Proceedings of the 21st International
Conference on Computer Processing of Oriental Lan-
guages, pages 234?244.
Anand Venkataraman. 2001. A procedure for unsuper-
vised lexicon learning. In Proceedings of the Eigh-
teenth International Conference on Machine Learning.
47
