Using Syntactic Information to Extract Relevant Terms for Multi-Document
Summarization
Enrique Amigo? Julio Gonzalo V??ctor Peinado Anselmo Pen?as Felisa Verdejo
Departamento de Lenguajes y Sistemas Informa?ticos
Universidad Nacional de Educacio?n a Distancia
c/ Juan del Rosal, 16 - 28040 Madrid - Spain
http://nlp.uned.es
Abstract
The identification of the key concepts in a set of
documents is a useful source of information for
several information access applications. We are
interested in its application to multi-document
summarization, both for the automatic genera-
tion of summaries and for interactive summa-
rization systems.
In this paper, we study whether the syntactic po-
sition of terms in the texts can be used to predict
which terms are good candidates as key con-
cepts. Our experiments show that a) distance
to the verb is highly correlated with the proba-
bility of a term being part of a key concept; b)
subject modifiers are the best syntactic locations
to find relevant terms; and c) in the task of auto-
matically finding key terms, the combination of
statistical term weights with shallow syntactic
information gives better results than statistical
measures alone.
1 Introduction
The fundamental question addressed in this article
is: can syntactic information be used to find the
key concepts of a set of documents? We will pro-
vide empirical answers to this question in a multi-
document summarization environment.
The identification of key terms out of a set of doc-
uments is a common problem in information access
applications and, in particular, in text summariza-
tion: a fragment containing one or more key con-
cepts can be a good candidate to be part of a sum-
mary.
In single-document summarization, key terms are
usually obtained from the document title or head-
ing (Edmundson, 1969; Preston, 1994; Kupiec et
al., 1995). In multi-document summarization, how-
ever, some processing is needed to identify key con-
cepts (Lin and Hovy, 2002; Kraaij et al, 2002;
Schlesinger et al, 2002). Most approaches are
based on statistical criteria.
Criteria to elaborate a manual summary depend,
by and large, on the user interpretation of both the
information need and the content of documents.
This is why this task has also been attempted from
an interactive perspective (Boguraev et al, 1998;
Buyukkokten et al, 1999; Neff and Cooper, 1999;
Jones et al, 2002; Leuski et al, 2003). A standard
feature of such interactive summarization assistants
is that they offer a list of relevant terms (automati-
cally extracted from the documents) which the user
may select to decide or refine the focus of the sum-
mary.
Our hypothesis is that the key concepts of a doc-
ument set will tend to appear in certain syntactic
functions along the sentences and clauses of the
texts. To confirm this hypothesis, we have used
a test bed with manually produced summaries to
study:
? which are the most likely syntactic functions
for the key concepts manually identified in the
document sets.
? whether this information can be used to auto-
matically extract the relevant terms from a set
of documents, as compared to standard statis-
tical term weights.
Our reference corpus is a set of 72 lists of key
concepts, manually elaborated by 9 subjects on
8 different topics, with 100 documents per topic.
It was built to study Information Synthesis tasks
(Amigo et al, 2004) and it is, to the best of
our knowledge, the multi-document summarization
testbed with a largest number of documents per
topic. This feature enables us to obtain reliable
statistics on term occurrences and prominent syn-
tactic functions.
The paper is organized as follows: in Section 2
we review the main approaches to the evaluation
of automatically extracted key concepts for summa-
rization. In Section 3 we describe the creation of the
reference corpus. In Section 4 we study the correla-
tion between key concepts and syntactic function in
texts, and in Section 5 we discuss the experimental
results of syntactic function as a predictor to extract
key concepts. Finally, in Section 6 we draw some
conclusions.
2 Evaluation of automatically extracted
key concepts
It is necessary, in the context of an interactive sum-
marization system, to measure the quality of the
terms suggested by the system, i.e., to what extent
they are related to the key topics of the document
set.
(Lin and Hovy, 1997) compared different strate-
gies to generate lists of relevant terms for summa-
rization using Topic Signatures. The evaluation was
extrinsic, comparing the quality of the summaries
generated by a system using different term lists as
input. The results, however, cannot be directly ex-
trapolated to interactive summarization systems, be-
cause the evaluation does not consider how informa-
tive terms are for a user.
From an interactive point of view, the evaluation
of term extraction approaches can be done, at least,
in two ways:
? Evaluating the summaries produced in the in-
teractive summarization process. This option
is difficult to implement (how do we evaluate
a human produced summary? What is the ref-
erence gold standard?) and, in any case, it is
too costly: every alternative approach would
require at least a few additional subjects per-
forming the summarization task.
? Comparing automatically generated term lists
with manually generated lists of key concepts.
For instance, (Jones et al, 2002) describes a
process of supervised learning of key concepts
from a training corpus of manually generated
lists of phrases associated to a single docu-
ment.
We will, therefore, use the second approach,
evaluating the quality of automatically generated
term lists by comparing them to lists of key con-
cepts which are generated by human subjects after a
multi-document summarization process.
3 Test bed: the ISCORPUS
We have created a reference test bed, the ISCOR-
PUS1 (Amigo et al, 2004) which contains 72 man-
ually generated reports summarizing the relevant in-
formation for a given topic contained in a large doc-
ument set.
For the creation of the corpus, nine subjects per-
formed a complex multi-document summarization
1Available at http://nlp.uned.es/ISCORPUS.
task for eight different topics and one hundred rele-
vant documents per topic. After creating each topic-
oriented summary, subjects were asked to make a
list of relevant concepts for the topic, in two cate-
gories: relevant entities (people, organizations, etc.)
and relevant factors (such as ?ethnic conflicts? as
the origin of a civil war) which play a key role in
the topic being summarized.
These are the relevant details of the ISCORPUS
test bed:
3.1 Document collection and topic set
We have used the Spanish CLEF 2001-2003 news
collection testbed (Peters et al, 2002), and selected
the eight topics with the largest number of docu-
ments manually judged as relevant from the CLEF
assessment pools. All the selected CLEF topics
have more than one hundred documents judged as
relevant by the CLEF assessors; for homogeneity,
we have restricted the task to the first 100 docu-
ments for each topic (using a chronological order).
This set of eight CLEF topics was found to have
two differentiated subsets: in six topics, it is neces-
sary to study how a situation evolves in time: the
importance of every event related to the topic can
only be established in relation with the others. The
invasion of Haiti by UN and USA troops is an ex-
ample of such kind of topics. We refer to them as
?Topic Tracking? (TT) topics, because they are suit-
able for such a task. The other two questions, how-
ever, resemble ?Information Extraction? (IE) tasks:
essentially, the user has to detect and describe in-
stances of a generic event (for instance, cases of
hunger strikes and campaigns against racism in Eu-
rope in this case); hence we will refer to them as IE
summaries.
3.2 Generation of manual summaries
Nine subjects between 25 and 35 years-old were re-
cruited for the manual generation of summaries. All
subjects were given an in-place detailed description
of the task, in order to minimize divergent interpre-
tations. They were told they had to generate sum-
maries with a maximum of information about ev-
ery topic within a 50 sentence space limit, using a
maximum of 30 minutes per topic. The 50 sentence
limit can be temporarily exceeded and, once the 30
minutes have expired, the user can still remove sen-
tences from the summary until the sentence limit is
reached back.
3.3 Manual identification of key concepts
After summarizing every topic, the following ques-
tionnaire was filled in by users:
? Who are the main people involved in the topic?
? What are the main organizations participating in the topic?
? What are the key factors in the topic?
Users provided free-text answers to these ques-
tions, with their freshly generated summary at hand.
We did not provide any suggestions or constraints
at this point, except that a maximum of eight slots
were available per question (i.e., a maximum of
8X3 = 24 key concepts per topic, per user).
This is, for instance, the answer of one user for
a topic about the invasion of Haiti by UN and USA
troops:
People Organizations
Jean Bertrand Aristide ONU (UN)
Clinton EEUU (USA)
Raoul Cedras OEA (OAS)
Philippe Biambi
Michel Josep Francois
Factors
militares golpistas (coup attempting soldiers)
golpe militar (coup attempt)
restaurar la democracia (reinstatement of democracy)
Finally, a single list of key concepts is generated
for each topic, joining all the answers given by the
nine subjects. These lists of key concepts constitute
the gold standard for all the experiments described
below.
3.4 Shallow parsing of documents
Documents are processed with a robust shallow
parser based in finite automata. The parser splits
sentences in chunks and assigns a label to every
chunk. The set of labels is:
? [N]: noun phrases, which correspond to
names or adjectives preceded by a determiner,
punctuation sign, or beginning of a sentence.
? [V]: verb forms.
? [Mod]: adverbial and prepositional phrases,
made up of noun phrases introduced by an ad-
verb or preposition. Note that this is the mech-
anism to express NP modifiers in Spanish (as
compared to English, where noun compound-
ing is equally frequent).
? [Sub]: words introducing new subordinate
clauses within a sentence (que, cuando, mien-
tras, etc.).
? [P]: Punctuation marks.
This is an example output of the chunker:
Previamente [Mod] ,[P]el presidente Bill Clinton [N] hab??a di-
cho [V] que [Sub] tenemos [V] la obligacion [N] de cambiar la
pol??tica estadounidense [Mod] que [Sub] no ha funcionado [V] en
Hait?? [Mod].[P]
Although the precision of the parser is limited,
the results are good enough for the statistical mea-
sures used in our experiments.
4 Distribution of key concepts in syntactic
structures
We have extracted empirical data to answer these
questions:
? Is the probability of finding a key concept cor-
related with the distance to the verb in a sen-
tence or clause?
? Is the probability of finding a key concept in a
noun phrase correlated with the syntactic func-
tion of the phrase (subject, object, etc.)?
? Within a noun phrase, where is it more likely
to find key concepts: in the noun phrase head,
or in the modifiers?
We have used certain properties of Spanish syn-
tax (such as being an SVO language) to decide
which noun phrases play a subject function, which
are the head and modifiers of a noun phrase, etc. For
instance, NP modifiers usually appear after the NP
head in Spanish, and the specification of a concept
is usually made from left to right.
4.1 Distribution of key concepts with verb
distance
Figure 1 shows, for every topic, the probability of
finding a word from the manual list of key con-
cepts in fixed distances from the verb of a sen-
tence. Stop words are not considered for computing
word distance. The broader line represents the aver-
age across topics, and the horizontal dashed line is
the average probability across all positions, i.e., the
probability that a word chosen at random belongs to
the list of key concepts.
The plot shows some clear tendencies in the data:
the probability gets higher when we get close to the
verb, falls abruptly after the verb, and then grows
steadily again. For TT topics, the probability of
finding relevant concepts immediately before the
verb is 56% larger than the average (0.39 before the
verb, versus 0.25 in any position). This is true not
only as an average, but also for all individual TT
topics. This can be an extremely valuable result: it
shows a direct correlation between the position of a
term in a sentence and the importance of the term
in the topic. Of course, this direct distance to the
verb should be adapted for languages with different
syntactic properties, and should be validated for dif-
ferent domains.
The behavior of TT and IE topics is substantially
different. IE topics have smaller probabilities over-
all, because there are less key concepts common to
all documents. For instance, if the topic is ?cases of
hunger strikes?, there is little in common between
Figure 1: Probability of finding key concepts at fixed distances from verb
all cases of hunger strikes found in the collection;
each case has its own relevant people and organiza-
tions, for instance. Users try to make abstraction of
individual cases to write key concepts, and then the
number of key concepts is smaller. The tendency
to have larger probabilities just before the verb and
smaller probabilities just after the verb, however,
can also be observed for IE topics.
Figure 2: Probability of finding key concepts in sub-
ject NPs versus other NPs
4.2 Key Concepts and Noun Phrase Syntactic
Function
We wanted also to confirm that it is more likely to
find a key concept in a subject noun phrase than
in general NPs. For this, we have split compound
sentences in chunks, separating subordinate clauses
([Sub] type chunks). Then we have extracted se-
quences with the pattern [N][Mod]*. We assume
that the sentence subject is a sequence [N][Mod]*
occurring immediately before the verb. For in-
stance:
El presidente [N] en funciones [Mod] de
Hait?? [Mod] ha afirmado [V] que [Sub]...
The rest of [N] and [Mod] chunks are consid-
ered as part of the sentence verb phrase. In a ma-
jority of cases, these assumptions lead to a correct
identification of the sentence subject. We do not
capture, however, subjects of subordinate sentences
or subjects appearing after the verb.
Figure 2 shows how the probability of finding a
key concept is always larger in sentence subjects.
This result supports the assumption in (Boguraev
et al, 1998), where noun phrases receive a higher
weight, as representative terms, if they are syntactic
subjects.
4.3 Distribution of key concepts within noun
phrases
Figure 3: Probability of finding key concepts in NP
head versus NP modifiers
For this analysis, we assume that, in
[N][Mod]* sequences identified as subjects,
[N] is the head and [Mod]* are the modifiers.
Figure 3 shows that the probability of finding a
key concept in the NP modifiers is always higher
than in the head (except for topic TT3, where it is
equal). This is not intuitive a priori; an examination
of the data reveals that the most characteristic con-
cepts for a topic tend to be in the complements: for
instance, in ?the president of Haiti?, ?Haiti? carries
more domain information than ?president?. This
seems to be the most common case in our news
collection. Of course, it cannot be guaranteed that
these results will hold in other domains.
5 Automatic Selection of Key Terms
We have shown that there is indeed a correlation be-
tween syntactic information and the possibility of
finding a key concept. Now, we want to explore
whether this syntactic information can effectively
be used for the automatic extraction of key concepts.
The problem of extracting key concepts for sum-
marization involves two related issues: a) What
kinds of terms should be considered as candidates?
and b) What is the optimal weighting criteria for
them?
There are several possible answers to the first
question. Previous work includes using noun
phrases (Boguraev et al, 1998; Jones et al, 2002),
words (Buyukkokten et al, 1999), n-grams (Leuski
et al, 2003; Lin and Hovy, 1997) or proper
nouns, multi-word terms and abbreviations (Neff
and Cooper, 1999).
Here we will focus, however, in finding appro-
priate weighting schemes on the set of candidate
terms. The most common approach in interactive
single-document summarization is using tf.idf mea-
sures (Jones et al, 2002; Buyukkokten et al, 1999;
Neff and Cooper, 1999), which favour terms which
are frequent in a document and infrequent across
the collection. In the iNeast system (Leuski et al,
2003), the identification of relevant terms is ori-
ented towards multi-document summarization, and
they use a likelihood ratio (Dunning, 1993) which
favours terms which are representative of the set of
documents as opposed to the full collection.
Other sources of information that have been used
as complementary measures consider, for instance,
the number of references of a concept (Boguraev
et al, 1998), its localization (Jones et al, 2002)
or the distribution of the term along the document
(Buyukkokten et al, 1999; Boguraev et al, 1998).
5.1 Experimental setup
A technical difficulty is that the key concepts in-
troduced by the users are intellectual elaborations,
which result in complex expressions which might
even not be present (literally) in the documents.
Hence, we will concentrate on extracting lists of
terms, checking whether these terms are part of
some key concept. We will assume that, once key
terms are found, it is possible to generate full nomi-
nal expressions using, for instance, phrase browsing
strategies (Pen?as et al, 2002).
We will then compare different weighting criteria
to select key terms, using two evaluation measures:
a recall measure saying how well manually selected
key concepts are covered by the automatically gen-
erated term list; and a noise measure counting the
number of terms which do not belong to any key
concept. An optimal list will reach maximum recall
with a minimum of noise. Formally:
R =
|Cl|
|C|
Noise = |Ln|
where C is the set of key concepts manually se-
lected by users; L is a (ranked) list of terms gen-
erated by some weighting schema; Ln is the subset
of terms in L which do not belong to any key con-
cept; and Cl is the subset of key concepts which are
represented by at least one term in the ranked list L.
Here is a (fictitious) example of how R and
Noise are computed:
C = {Haiti, reinstatement of democracy, UN and USA troops}
L = {Haiti, soldiers, UN, USA, attempt}
?
Cl = {Haiti, UN and USA troops} R = 2/3
Ln = {soldiers,attempt} Noise = 2
We will compare the following weighting strate-
gies:
TF The frequency of a word in the set of documents
is taken as a baseline measure.
Likelihood ratio This is taken from (Leuski et al,
2003) and used as a reference measure. We
have implemented the procedure described in
(Rayson and Garside, 2000) using unigrams
only.
OKAPImod We have also considered a measure
derived from Okapi and used in (Robertson et
al., 1992). We have adapted the measure to
consider the set of 100 documents as one single
document.
TFSYNTAX Using our first experimental result,
TFSYNTAX computes the weight of a term
as the number of times it appears preceding a
verb.
Figure 4: Comparison of weighting schemes to ex-
tract relevant terms
5.2 Results
Figure 4 draws Recall/Noise curves for all weight-
ing criteria. They all give similar results except our
TFSYNTAX measure, which performs better than
the others for TT topics. Note that the TFSYN-
TAX measure only considers 10% of the vocabu-
lary, which are the words immediately preceding
verbs in the texts.
In order to check whether this result is consistent
across topics (and not only the effect on an average)
we have compared recall for term lists of size 50 for
individual topics. We have selected 50 as a number
which is large enough to reach a good coverage and
permit additional filtering in an interactive summa-
rization process, such as the iNeast terminological
clustering described in (Leuski et al, 2003).
Figure 5 shows these results by topic. TFSYN-
TAX performs consistently better for all topics ex-
cept one of the IE topics, where the maximum like-
lihood measure is slightly better.
Apart from the fact that TFSYNTAX performs
better than all other methods, it is worth noticing
that sophisticated weighting mechanisms, such as
Okapi and the likelihood ratio, do not behave bet-
ter than a simple frequency count (TF).
6 Conclusions
The automatic extraction of relevant concepts for
a set of related documents is a part of many mod-
els of automatic or interactive summarization. In
this paper, we have analyzed the distribution of rel-
evant concepts across different syntactic functions,
and we have measured the usefulness of detecting
key terms to extract relevant concepts.
Our results suggest that the distribution of key
concepts in sentences is not uniform, having a max-
imum in positions immediately preceding the sen-
tence main verb, in noun phrases acting as subjects
and, more specifically, in the complements (rather
than the head) of noun phrases acting as subjects.
This evidence has been collected using a Spanish
news collection, and should be corroborated outside
the news domain and also adapted to be used for non
SVO languages.
We have also obtained empirical evidence that
statistical weights to select key terms can be im-
proved if we restrict candidate words to those which
precede the verb in some sentence. The combi-
nation of statistical measures and syntactic criteria
overcomes pure statistical weights, at least for TT
topics, where there is certain consistency in the key
concepts across documents.
Acknowledgments
This research has been partially supported by a re-
search grant of the Spanish Government (project
Hermes) and a research grant from UNED. We are
indebted to J. Cigarra?n who calculated the Okapi
weights used in this work.
References
E. Amigo, J. Gonzalo, V. Peinado, A. Pen?as, and
F. Verdejo. 2004. Information synthesis: an em-
pirical study. In Proceedings of the 42th Annual
Meeting of the ACL, Barcelona, July.
B. Boguraev, C. Kennedy, R. Bellamy, S. Brawer,
Y. Wong, and J. Swartz. 1998. Dynamic Presen-
tation of Document Content for Rapid On-line
Skimming. In Proceedings of the AAAI Spring
Figure 5: Comparison of weighting schemes by topic
1998 Symposium on Intelligent Text Summariza-
tion, Stanford, CA.
O. Buyukkokten, H. Garc??a-Molina, and
A. Paepcke. 1999. Seeing the Whole in
Parts: Text Summarization for Web Browsing
on Handheld Devices. In Proceedings of 10th
International WWW Conference.
T. Dunning. 1993. Accurate Methods for the Statis-
tics of Surprise and Coincidence. Computational
Linguistics, 19(1):61?74.
H. P. Edmundson. 1969. New Methods in Auto-
matic Extracting. Journal of the Association for
Computing Machinery, 16(2):264?285.
S. Jones, S. Lundy, and G. W. Paynter. 2002. In-
teractive Document Summarization Using Auto-
matically Extracted Keyphrases. In Proceedings
of the 35th Hawaii International Conference on
System Sciences, Big Island, Hawaii.
W. Kraaij, M. Spitters, and A. Hulth. 2002.
Headline Extraction based on a Combination of
Uni- and Multi-Document Summarization Tech-
niques. In Proceedings of the DUC 2002 Work-
shop on Multi-Document Summarization Evalua-
tion, Philadelphia, PA, July.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A train-
able document summarizer. In Proceedings of SI-
GIR?95.
A. Leuski, C. Y. Lin, and S. Stubblebine. 2003.
iNEATS: Interactive Multidocument Summariza-
tion. In Proceedings of the 4lst Annual Meeting
of the ACL (ACL 2003), Sapporo, Japan.
C.-Y. Lin and E.H. Hovy. 1997. Identifying Top-
ics by Position. In Proceedings of the 5th Con-
ference on Applied Natural Language Processing
(ANLP), Washington, DC.
C. Lin and E. Hovy. 2002. NeATS in DUC
2002. In Proceedings of the DUC 2002 Work-
shop on Multi-Document Summarization Evalu-
ation, Philadelphia, PA, July.
M. S. Neff and J. W. Cooper. 1999. ASHRAM: Ac-
tive Summarization and Markup. In Proceedings
of HICSS-32: Understanding Digital Documents.
A. Pen?as, F. Verdejo, and J. Gonzalo. 2002. Ter-
minology Retrieval: Towards a Synergy be-
tween Thesaurus and Free Text Searching. In IB-
ERAMIA 2002, pages 684?693, Sevilla, Spain.
C. Peters, M. Braschler, J. Gonzalo, and M. Kluck,
editors. 2002. Evaluation of Cross-Language
Information Retrieval Systems, volume 2406 of
Lecture Notes in Computer Science. Springer-
Verlag, Berlin-Heidelberg-New York.
S. Preston, K.and Williams. 1994. Managing the
Information Overload. Physics in Business, June.
P. Rayson and R. Garside. 2000. Comparing Cor-
pora Using Frequency Profiling. In Proceedings
of the workshop on Comparing Corpora, pages
1?6, Honk Kong.
S. E. Robertson, S. Walker, M. Hancock-Beaulieu,
A. Gull, and M. Lau. 1992. Okapi at TREC. In
Text REtrieval Conference, pages 21?30.
J. D. Schlesinger, M. E. Okurowski, J. M. Conroy,
D. P. O?Leary, A. Taylor, J. Hobbs, and H. Wil-
son. 2002. Understanding Machine Performance
in the Context of Human Performance for Multi-
Document Summarization. In Proceedings of the
DUC 2002 Workshop on Multi-Document Sum-
marization Evaluation, Philadelphia, PA, July.
c? 2003 Association for Computational Linguistics
Automatic Association of Web Directories
with Word Senses
Celina Santamar??a? Julio Gonzalo? Felisa Verdejo?
UNED, Madrid UNED, Madrid UNED, Madrid
We describe an algorithm that combines lexical information (from WordNet 1.7) with Web di-
rectories (from the Open Directory Project) to associate word senses with such directories. Such
associations can be used as rich characterizations to acquire sense-tagged corpora automatically,
cluster topically related senses, and detect sense specializations. The algorithm is evaluated for
the 29 nouns (147 senses) used in the Senseval 2 competition, obtaining 148 (word sense, Web
directory) associations covering 88% of the domain-specific word senses in the test data with 86%
accuracy. The richness of Web directories as sense characterizations is evaluated in a supervised
word sense disambiguation task using the Senseval 2 test suite. The results indicate that, when
the directory/word sense association is correct, the samples automatically acquired from the Web
directories are nearly as valid for training as the original Senseval 2 training instances. The re-
sults support our hypothesis that Web directories are a rich source of lexical information: cleaner,
more reliable, and more structured than the full Web as a corpus.
1. Introduction
Combining the size and diversity of the textual material on the World Wide Web
with the power and efficiency of current search engines is an attractive possibility for
acquiring lexical information and corpora. A widespread example is spell-checking:
Many Web users routinely use search engines to assess which is the ?correct? (i.e. with
more hits in the Web) spelling of words. Among NLP researchers, Web search engines
have already been used as a point of departure for extraction of parallel corpora,
automatic acquisition of sense-tagged corpora, and extraction of lexical information.
Extraction of parallel corpora. In Resnik (1999), Nie, Simard, and Foster (2001),
Ma and Liberman (1999), and Resnik and Smith (2002), the Web is harvested in search
of pages that are available in two languages, with the aim of building parallel corpora
for any pair of target languages. This is a very promising technique, as many machine
translation (MT) and cross-language information retrieval (CLIR) strategies rely on
the existence of parallel corpora, which are still a scarce resource. Such Web-mined
parallel corpora have proved to be useful, for instance, in the context of the CLEF
(Cross-Language Evaluation Forum) CLIR competition, in which many participants
use such parallel corpora (provided by the University of Montreal) to improve the
performance of their systems (Peters et al 2002).
Automatic acquisition of sense-tagged corpora. The description of a word sense
can be used to build rich queries in such a way that the occurrences of the word in
the documents retrieved are, with some probability, associated with the desired sense.
If the probability is high enough, it is then possible to acquire sense-tagged corpora
? ETS Ingenier??a Informa?tica de la UNED, c/ Juan del Rosal, 16, Ciudad Universitaria, 28040 Madrid,
Spain. E-mail: {celina,julio,felisa}@lsi.uned.es
486
Computational Linguistics Volume 29, Number 3
in a fully automatic fashion. Again, this is an exciting possibility that would solve the
current bottleneck of supervised word sense disambiguation (WSD) methods (namely,
that sense-tagged corpora are very costly to acquire).
One example of this kind of technique is Mihalcea and Moldovan (1999), in which
a precision of 91% is reported over a set of 20 words with 120 senses. In spite of the
high accuracy obtained, such methodology did not perform well in the comparative
evaluation reported in Agirre and Mart??nez (2000), perhaps indicating that examples
obtained from the Web may have topical biases (depending on the word), and that
further refinement is required. For instance, a technique that behaves well with a small
set of words might fail in the common cases in which a new sense is predominant
on the Web (e.g., oasis or nirvana as music groups, tiger as a golfer, jaguar as a car
brand).
Extraction of lexical information. In Agirre et al (2000), search engines and the
Web are used to assign Web documents to WordNet concepts. The resulting sets of
documents are then processed to build topic signatures, that is, sets of words with
weights that enrich the description of a concept. In Grefenstette (1999), the number of
hits in Web search engines is used as a source of evidence to select optimal translations
for multiword expressions. For instance, apple juice is selected as a better translation
than apple sap for the German ApfelSaft because apple juice hits a thousand times more
documents in AltaVista. Finally, in Joho and Sanderson (2000) and Fujii and Ishikawa
(1999), the Web is used as a resource to provide descriptive phrases or definitions for
technical terms.
A common problem to all the above applications is how to detect and filter out
all the noisy material on the Web, and how to characterize the rest (Kilgarriff 2001b).
Our starting hypotheses is that Web directories (e.g., Yahoo, AltaVista or Google
directories, the Open Directory Project [ODP]), in which documents are mostly manu-
ally classified in hierarchical topical clusters, are an optimal source for acquiring lexical
information; their size is not comparable to the full Web, but they are still enormous
sources of semistructured, semifiltered information waiting to be mined.
In this article, we describe an algorithm for assigning Web directories (from the
Open Directory Project ?http://dmoz.org?) as characterizations for word senses in
WordNet 1.7 noun synsets (Miller 1990). For instance, let us consider the noun circuit,
which has six senses in WordNet 1.7. These senses are grouped in synsets, together
with their synonym terms, and linked to broader (more general) synsets via hyper-
nymy relations:
6 senses of circuit
Sense 1: {circuit, electrical circuit, electric circuit} => {electrical device}
Sense 2: {tour, circuit} => {journey, journeying}
Sense 3: {circuit} => {path, route, itinerary}
Sense 4: {circuit (judicial division)} => {group, grouping}
Sense 5: {racing circuit, circuit} => {racetrack, racecourse, raceway, track}
Sense 6: {lap, circle, circuit} => {locomotion, travel}
Our algorithm associates circuit 1 (electric circuit) with ODP directories such as
business/industries/electronics and electrical/contract manufacturers
487
Santamar??a, Gonzalo, and Verdejo Association of Web Directories with Word Senses
whereas circuit 5 (racing circuit) is tagged with directories such as
sports/motorsports/auto racing/tracks
sports/equestrian/racing/tracks
sports/motorsports/auto racing/formula one
Every ODP directory has an associated URL, which contains a description of the
directory and a number of Web sites that have been manually listed as pertaining to the
directory topic, accompanied by brief descriptions of each site. This information is com-
pleted with a list of subdirectories, each containing more Web sites and subdirectories.
Finally, some directories also have pointers to the same category in other languages.
For instance, the Web page for the directory sports/motorsports/auto racing/tracks can be
seen in Figure 1. This directory contains links and descriptions for 846 Web sites orga-
nized in 12 subdirectories, a link to a related directory (sports/motorsports/karting/tracks)
and a link to the same category in French.
The association of word senses with Web directories is related to the assignment
of domain labels to WordNet synsets as described in Magnini and Cavaglia (2000), in
which WordNet is (manually) enriched with domain categories from the Dewey Dec-
imal Classification (DDC). Some clear differences between the two are that directories
from the ODP are assigned automatically, are richer and deeper and, more importantly,
Figure 1
Contents of an ODP Web directory associated with circuit 5 (racing circuit).
488
Computational Linguistics Volume 29, Number 3
come with a large amount of associated information directly retrievable from the Web.
DDC categories, on the other hand, are a stable domain characterization compared to
Web directories.
As WordNet and ODP are both hierarchical structures, connecting them is also
related to research in mapping thesauruses for digital libraries, ontologies, and data
structures in compatible databases. A salient feature of our task is, however, that we
do not intend to map both structures, as they are of a quite different nature (lexicalized
English concepts versus topics on the Web). Our goal is rather to associate individual
items in a many-to-many fashion. A word sense may be characterized with several
Web directories, and a Web directory may be suitable for many word senses.
The most direct applications of word sense/Web directory associations are
? Clustering of senses with identical or very similar categories.
? Refinement of senses into specialized variants (e.g., equestrian circuit and
formula one circuit as specializations of racing circuit in the example above).
? Extraction of sense-tagged corpora from the Web sites listed under the
appropriate directories.
In Section 2 we describe the proposed algorithm. In Section 3, we evaluate the
precision and recall of the algorithm for the set of nouns used in the Senseval 2 WSD
competition. In Section 4, we make a preliminary experiment using the material from
ODP directories as training corpora for a supervised WSD system. In section 5, we
present the results of applying the algorithm to most WordNet 1.7 nouns. Finally, in
Section 6 we draw some conclusions.
2. Algorithm
Overall, the system takes a WordNet 1.7 noun as input, generates and submits a set
of queries into the ODP, filters the information obtained from the search engine, and
returns a set of ODP directories classified as (1) pseudo?domain labels for some word
sense, (2) noise, and (3) salient noise (i.e., directories that are not suitable for any sense
in WordNet but could reveal and characterize a new relevant sense of the noun). In
case (1), the WordNet sense ? ODP directory association also receives a probability
score. A detailed description of the algorithm steps follows.
2.1 Querying ODP Structure
For every sense wi of the noun w, a query qi is generated, including w as compulsory
term, the synonyms and direct hypernyms of wi as optional terms, and the synonyms
of other senses of w as negated (forbidden) terms. These queries are submitted to ODP,
and a set of directories is retrieved. For instance, for circuit, the following queries are
generated and sent to the ODP search engine:1
q1= [+circuit "electrical circuit" "electric circuit" "electrical device" -tour
-"racing circuit" -lap -circle]
q2= [+circuit tour journey journeying -"electrical circuit" -"electric circuit"
-"electrical device" -"racing circuit" -lap -circle]
1 In ODP queries, compulsory terms are denoted by + and forbidden terms by ?.
489
Santamar??a, Gonzalo, and Verdejo Association of Web Directories with Word Senses
q3= [+circuit path route itinerary -"electrical circuit" -"electric circuit"
-"electrical device" -tour -"racing circuit" -lap -circle ]
q4= [+circuit group grouping -"electrical circuit" -"electric circuit"
-"electrical device" -tour -"racing circuit" -lap -circle]
q5= [+circuit "racing circuit" racetrack racecourse raceway track -"electrical circuit"
-"electric circuit" -"electrical device" -tour -lap -circle]
q6= [+circuit lap circle locomotion travel -"electrical circuit" -"electric circuit"
-"electrical device" -tour -"racing circuit" -lap -circle]
2.2 Representing Retrieved Directory Descriptions
For every directory d, a list of words l(d) is obtained removing stopwords and pre-
serving all content words in the directory path. For instance, one of the directories
produced by the circuit queries is
d = business/industries/electronics and electrical/contract manufacturers
which is characterized by the following word list:
l(d) = [business, industries, electronics, electrical, contract, manufacturers]
2.3 Representing WordNet Senses
For every sense wj, a list l(wj) of words is made with
? all nouns in the hypernym chain of maximal length 6
? all hyponyms
? all meronyms, holonyms, and coordinate terms
of wj in WordNet. l(wj) is used as a description of the sense wj. For instance, circuit 1
receives the following description:
l(circuit1) = [electrical circuit, electric circuit, electrical device, bridge,
bridge circuit, Wheatstone bridge, bridged-T, closed circuit, loop, parallel circuit,
shunt circuit, computer circuit, gate, logic gate, AND circuit, AND gate, NAND circuit,
NAND gate, OR circuit, OR gate, X-OR circuit, XOR circuit, XOR gate, integrated circuit,
(..)
instrumentality, instrumentation, artifact, artefact, object, physical object, entity]
2.4 Sense/Directory Comparisons
For every sense description l(wj), a comparison is made with the terms in the directory
description l(d). This comparison is based on the hypothesis that the terms in an
appropriate directory for a word sense will have some correlation with the sense
description via WordNet semantic relations. In other words, our assumption is that the
path to the directory in the ODP topical structure will have some degree of overlapping
with the hyponymy path to the word sense in the WordNet hierarchical structure.
For this comparison, we simply count the number of co-occurrences between
words in l(wj) and words in l(d). Repeated terms are not discarded, as repetition
is correlated with stronger associations. Other, better-grounded comparisons, such as
the cosine between l(wj) and l(d), were empirically discarded because of the small size
and small amount of overlapping of the average vectors.
490
Computational Linguistics Volume 29, Number 3
2.5 Candidate Sense/Directory Associations
The association vector v(d, w) has as many components as senses for w in WordNet 1.7;
the ith component, v(d, w)i represents the number of matches between the directory
l(d) and the sense descriptor l(wj). For instance, the association vector of
business/industries/electronics and electrical/contract manufacturers
with circuit is
v(d, circuit) = (6, 0, 0, 0, 0, 0)
that is, six coincidences for sense 1 (the electric circuit sense), which has the associated
vector shown in the previous section (which includes five occurrences of electrical and
one occurrence of electronic). The rest of the sense descriptions have no coincidences
with the directory description.
v(d, w) is the basis for making candidate assignments of suitable senses for direc-
tory d: If one of the components v(d, w)j is not null, we assign the sense wj to the
directory d. If all components are null, the directory is provisionally classified as noise
or new sense. If more than one component is not null, the senses i with maximal
v(d, w)i are all considered candidates. These candidate assignments are confirmed or
discarded after passing a number of filters and receiving a confidence score C(d, wj),
both of which are described below.
2.6 Filters
Filters are simple heuristics that contribute to a more accurate classification of the
relations predicted by the co-ocurrence vector v(d, w). We are currently using two
filters: One differentiates nouns and noun modifiers to prevent wrong associations,
and another detects sense specializations.
2.6.1 Modifiers. Frequently, the ODP search engine retrieves directories in which the
noun to be searched, w, has as a noun modifier role. Such cases usually produce
erroneous associations. For instance, the directory
library/sciences/animals & wildlife/mammals/tamarins/golden lion tamarin
is erroneously associated with the mammal sense of lion, which is here a modifier for
tamarin.
Modifiers are detected with a set of simple patterns, as the syntactic properties of
descriptions in directories are quite simple. In particular, we discard most cases using
the structure of the ODP hierarchy, as in this case. The filter analyzes the structure of
the directory, detects that the parent category of golden lion tamarin is tamarin, therefore
assumes that golden lion tamarin is a specialization of tamarin, and assigns the directory
to a suitable sense of tamarin (tamarin 1 in WordNet).
An additional filter (weaker than the previous one) discards compounds according
to the position (the searched noun precedes another noun), as in
personal/kids/arts & entertainment/movies/animals/lion king
This directory could be associated with lion 1 because it contains the word animal, but
the assignment is rejected because of the modifier filter. In general, on such occasions
the searched noun plays a modifier role (as adjective or noun); discarding all such
cases favors precision over recall. In this case, the label is classified as noise.
491
Santamar??a, Gonzalo, and Verdejo Association of Web Directories with Word Senses
2.6.2 Sense Specializations (Hyponyms). A retrieved directory might be appropriate
as a characterization of a sense specialization for some of the word senses being con-
sidered; our algorithm tries to detect such cases, creating a hyponym of the sense and
characterizing the directory with the hyponym.
The filter identifies a directory as a candidate hyponym if it contains explicitly a
modifier w pattern (where w is the noun being searched). This filter detects explicit
specializations, such as office chair as a hyponym of chair 1, or fox family channel as a
hyponym of channel 7, but fails to identify, for instance, memorial day as a hyponym
of holiday.
If the candidate hyponym, as a compound, is not present in WordNet, then it is
incorporated and described with the directory. If it is already present in WordNet, an
additional checking of the hyponymy relation is made. For instance, the directory
business/industries/electronics and electrical/components/integrated circuits
is assigned to the WordNet entry integrated circuit, because integrated circuit is already
a hyponym of circuit in WordNet.
2.7 Confidence Score
Finally, a confidence score C(d, wj) for every potential association (d, wj) is calculated
using four empirical criteria:
1. Checking whether d was directly retrieved for the query associated to wj.
2. Checking whether the system associates d with one or more senses of the
word w.
3. Checking the number of coincidences between l(d) and l(wj).
4. Comparing the previous number with the number of coincidences
between l(d) and the other sense descriptions {l(w)i, i = j}.
The confidence score is a linear combination of these factors, weighted according to
an empirical estimation of their relevance:
C(d, wj) =
4
?
i=1
?iCi(d, wj)
where
C1(d, wj) =
{
1, if query(wj) retrieves d
0, otherwise
C2(d, wj) = 1 ?
k
n
C3(d, wj) =
?
?
?
1, if vj ? 5
(vj + 5)/10, if 1 < vj ? 4
0.5, if vj = 1
C4(d, wj) =
vj ? maxi=j(vi),
?n
i=1 vi
where v is the association vector v(d, w), n the number of senses, k the number of senses
for which vj is non-null, and ?i are coefficients empirically adjusted to (?1,?2,?3,?4) =
492
Computational Linguistics Volume 29, Number 3
(0.1, 0.15, 0.4, 0.35). The value of C(d, wj) ranges between 0 and 1 (all Ci range between
0 and 1, and the sum of the linear coefficients ?i is 1). Note that C2 cannot reach 1 (but
can get asymptotically close to 1), and note also that C4 cannot take negative values,
because, as (d, wj) is a candidate association, vj is maximal in v(d, w), and therefore
vj ? maxi=j(vi) ranges between 0 and vj.
Let us see an example of how this confidence measure works, calculating C(d, wj)
for the directory
d = business/industries/electronics and electrical/contract manufacturers
with circuit 1 (electric circuit):
? C1. This directory has been retrieved from the query
q1= [+circuit "electrical circuit" "electric circuit" "electrical device"
-tour -"racing circuit" -lap -circle]
corresponding to circuit 1, which agrees with the association made by the
system. Hence C1 = 1.
? C2. The association vector v(d, w) = (6, 0, 0, 0, 0, 0) presents only one
non-null coordinate; therefore C2 = 1 ? 16 = 0.83. Note that, in general,
this factor prevents C from reaching the upper bound 1.
? C3. As v1 = 6, C3 = 1. This factor increases along with the number of
coincidences between the sense and directory characterizations.
? C4. As all other components of v are null, the highest value of the
components different from sense 1 is also null (maxi=j(vi) = 0); therefore,
C4 = 1. This factor measures the strength of the association (d, w1)
compared with the other possibilities. It decreases when v(d, w) includes
more than one non-null coordinate, and their values are similar.
? C. Finally, applying the ?i coefficients, we obtain C(d, circuit 1) = 0.975.
The confidence score can be used to set a threshold for accepting/discarding associ-
ations. A higher threshold should produce a lower number of highly precise associa-
tions; a lower threshold would produce more associations with less accuracy. For the
evaluation below, we have retained all directories, regardless of their confidence score,
in order to assess how well this empirical measure correlates with correct and useful
assignments.
An example of the results produced by the algorithm can be seen in Table 1. The
system assigns directories to senses 1, 2, and 5 of circuit (six, two, and three directories,
respectively). Some of them are shown in the table, together with a sense specialization,
integrated circuit, for sense 1 (electrical circuit). Senses 3, 4, and 6, which did not receive
any directory association, do not appear to have domain specificity, but are instead
general terms.
3. Evaluation
We have analyzed the results of the algorithm for the set of nouns in the Senseval 2
WSD English lexical sample test bed (Kilgarriff 2001a). The Senseval campaigns (Ed-
monds and Cotton 2001; Kilgarriff and Palmer 2000) are devoted to the comparative
evaluation of word sense disambiguation systems in many languages. In the Senseval
2 lexical sample task, a large number of instances (occurrences in context extracted
493
Santamar??a, Gonzalo, and Verdejo Association of Web Directories with Word Senses
Table 1
Results of the association algorithm for circuit.
circuit 1 (electrical circuit)
ODP directories C
business/industries/electronics and electrical/contract manufacturers 0.98
manufacturers/printed circuit boards/fabrication 0.88
computers/cad/electronic design automation 0.78
...
sense specializations (hyponyms)
business/industries/electronics and electrical/components/integrated circuits 0.98
circuit 2 (tour, journey around a particular area)
ODP directories
sports/cycling/travel/travelogues/europe/france 0.58
regional/asia/nepal/travel and tourism/travel guides 0.66
circuit 5 (racing circuit)
ODP directories
sports/motorsports/auto racing/stock cars/drivers and teams 0.78
sports/motorsports/auto racing/tracks 0.82
sports/motorsports/auto racing/driving schools 0.78
from corpora) for a fixed set of words had to be tagged with the appropriate sense
by the participating WSD systems. For English, the sense inventory was a prerelease
of WordNet 1.7, and two sets of manually tagged instances were made available: A
first set was intended for training supervised systems, and a second set for evaluation
of all systems attempting the task. Altogether, the Senseval 2 lexical sample test bed
is one of the most widely used resources for studying and comparing word sense
disambiguation approaches.
For our evaluation, we have considered the fraction of the Senseval 2 test bed that
deals with English nouns: 29 polysemous nouns with a total of 147 word senses.
We have applied the algorithm to this set of nouns and examined the results in
terms of coverage and quality of the sense/directory associations. Coverage measures
how many senses can be characterized with directories, assuming that every domain-
specific sense should receive at least one directory. Quality is measured in terms of
precision (are the assignments correct?), relevance (are the assignments useful?), and
confidence (does the confidence score correlate well with precision and relevance of
the associations?).
3.1 Coverage
Table 2 shows the 148 directories retrieved by our algorithm, an average of 1.0 directo-
ries per sense. The directories, however, are not evenly distributed among senses, cov-
ering only 43 different senses with unique directories and 28 specialized (hyponym)
senses. In addition, 9 senses are identified as part of potential clusters (i.e., having
nonunique directories).
In order to measure the real coverage of the system, we have to estimate how
many word senses in the Senseval 2 sample are susceptible to receiving a domain
label. For instance, sense in common sense is not associated with any particular topic
or domain, whereas sense in word sense can be associated with linguistics or language-
related topics.
The decision as to whether or not a word sense might receive a domain label is
not always a simple, binary one. Hence we have manually tagged all word senses
494
Computational Linguistics Volume 29, Number 3
Table 2
Coverage of nouns in the Senseval 2 test bed.
Senseval 2
Nouns
Number of
Senses
Number of
Directories
Number of Labeled
Senses
Number of
Hyponyms
art 4 6 1 1
authority 7 4 2 1
bar 13 3 3 0
bum 4 0 0 0
chair 4 4 1 0
channel 7 5 1 1
child 4 12 2 0
church 3 24 2 4
circuit 6 11 3 1
day 10 15 1 14
detention 2 1 1 0
dyke 2 1 1 0
facility 5 10 3 0
fatigue 4 0 0 0
feeling 6 2 1 0
grip 7 3 2 0
hearth 3 5 2 0
holiday 2 2 2 0
lady 3 0 0 0
material 5 9 2 3
mouth 8 0 0 0
nation 4 4 1 1
nature 5 0 0 0
post 8 14 5 0
restraint 6 4 3 0
sense 5 0 0 0
spade 3 3 1 1
stress 5 5 2 1
yew 2 1 1 0
Total 147 148 43 28
with two criteria (with each tagging performed by a different human annotator): a
strict one (only word senses that can clearly receive a domain label are marked as
positive) and a loose one (only word senses that are completely generic are marked as
negative). The strict judgment gave 59 domain-specific senses in the sample; the loose
judgment gave 71.
With these manual judgments, the coverage of the algorithm is between 73% (loose
judgment) and 88% (strict judgment). This coverage can be increased by
? Propagating a directory/word sense association to all members of the
WordNet synset to which the word sense belongs.
? Propagating directories via hyponymy chains, as in Magnini and
Cavaglia (2000).
3.2 Quality
We have used three criteria to evaluate the directory/sense associations produced:
? Precision. Is the directory correct (suitable) for the word sense or not?
495
Santamar??a, Gonzalo, and Verdejo Association of Web Directories with Word Senses
? Relevance. Is the directory useful for characterizing the word sense?
? Confidence. How well is the confidence value C(d, wj) correlated with
the precision and relevance of the associations?
3.2.1 Precision. An assignment (d, wj) is considered correct (d is suitable for wj) unless
1. d adjusts better to some other sense wi. For instance, the association of
regional/north america/united states/government/agencies/independent/
federal labor relations authority
as a hyponym of
authority4 : assurance, self -assurance, confidence , self -confidence , authority ,
sureness
is considered an error, as the directory would be better suited for a
hyponym of sense 5 (authority as administrative unit).
2. The terms in l(d) are contradictory to the definition of the word sense or
are better suited for a sense that is not listed in the dictionary. This is the
case of
arts/music/bands and artists/offspring
which is erroneously assigned to child 2: human offspring of any age.
The results of this manual evaluation can be seen in Table 3. The overall precision is
86%.
Regarding potential topical clusters (directories associated with more than one
sense of the same word), these are considered correct if (1) the associated directory
is correct for all the senses in the cluster and (2) the occurrences of the word on the
Web page associated with the directory can be loosely assigned to any of the cluster
senses. Twelve out of the 13 clusters extracted are correct according to this criterion.
3.2.2 Confidence Measures. Table 4 shows the distribution of directories according
to the confidence measure. Eighty-four percent of the directories have a confidence C
over 0.7, and 41% over 0.8. This skewed distribution is consistent with the algorithm
filters, which are designed to favor precision rather than recall.
Table 5 shows the distribution of errors in levels of confidence. The percentage
of errors in directories with a confidence level below .6 is 25%. This error percentage
Table 3
Precision over Senseval 2 nouns.
Directories Associated
with WordNet Senses
Number of
Directories Number of Correct Number of Errors
Unique sense 148 127 21
Potential clustering 13 12 1
Total 161 139 (86%) 22 (14%)
496
Computational Linguistics Volume 29, Number 3
Table 4
Confidence distribution.
Confidence C ? 0.7 0.7 < C ? 0.8 0.8 < C
Number of directories 24 63 61
Table 5
Correlation between confidence and correctness.
Confidence Number of Directories Percentage of Errors
C ? 0.7 24 25%
0.7 < C ? 0.8 63 19%
C > 0.8 61 5%
Total 148 14%
decreases with increasing levels of confidence, down to 5% for associations with C
over .8. Table 5 indicates that the confidence value, which is assigned heuristically, is
indeed correlated with precision.
3.2.3 Relevance. Besides correctness of the associations, we want to measure the use-
fulness of the directories: How well can they be used to characterize the associated
word senses? How much information do they provide about the word senses?
We have performed a manual, qualitative classification of the directories extracted
as irrelevant, mildly relevant, or very relevant. An irrelevant directory is compatible
with the word sense but does not provide any useful characterization; a mildly rele-
vant directory illustrates the word sense, but not centrally or in some particular aspect
or domain. A very relevant directory provides a rich characterization per se and can
be considered a domain label for the word sense.
An example of a very relevant directory is
business/industries/electronics and electrical/components/integrated circuit
associated as hyponym of circuit 1 (electrical circuit) with a confidence of 98%. An
example of mildly relevant association is
regional/north america/united states/texas/../society and culture/religion
associated with church 1 (Christian church) with a 73% confidence. Obviously, Texas is
not correlated with church, but the directory contains a lot of material (for instance,
the Web page of the Northcrest Community Church and many others) that might
be used, for instance, to acquire topical signatures for the concept. Hence the mildly
relevant judgment. Finally, an example of an irrelevant association is
regional/north america/united states/new york/localities/utica
associated with art 1 (fine art) with a confidence of 66% (the directory contains a section
on Arts at Utica, which would be considered mildly relevant if pointed to explicitly
by the label). For the purposes of measuring relevance, all the directories that were
judged as incorrect are counted as irrelevant.
497
Santamar??a, Gonzalo, and Verdejo Association of Web Directories with Word Senses
Table 6
Relevance of the directories in the test set.
Relevance Irrelevant MildlyRelevant
Highly
Relevant
C ? 0.7 7 4 13
0.7 < C ? 0.8 13 12 38
0.8 < C 3 9 49
Total 23 (15%) 25 (17%) 100 (67%)
The overall relevance figures, and the correlation of relevance with the confidence
value, can be seen in Table 6. Sixty-seven percent of the directories are highly relevant
to characterize word senses, which is an encouraging result. Also, the set of irrele-
vant directories (15%) is almost identical to the set of erroneous directories (with just
one addition), indicating that (almost) all directories that are correct can be used to
characterize word senses to some extent.
4. Example Application: Automatic Acquisition of Sense-Tagged Corpora
Each ODP directory contains links to related subdirectories and to a large number of
Web sites that have been manually classified there. Every link to a Web site includes
the name of the site and a short description. For instance, under
business/industries/electronics and electrical/components/integrated circuit
we find over 30 descriptions, such as ??Multilink Technology corporation: Manufacture of
integrated circuits, modules, and boards for use in both data and telecommunications??. In
order to perform a first experiment on extraction of sense-tagged corpora, we have
used only such descriptions (without exploring the associated Web sites) to build a
sense-tagged corpus for Senseval 2 nouns.
Notice that we are not using the contents of the Web sites that belong to a directory,
but only the manually added descriptions of Web sites in the directory. Using the Web
sites themselves is also an attractive possibility that would produce a much larger
corpus at the expense of lower precision.
The extraction is straightforward: When a word sense wi has an associated direc-
tory d, we scan the site descriptions on the ODP page that corresponds to the directory
d and extract all contexts in which w occurs, assuming that in all of them w is used in
the sense i. Some examples of the training material for circuit can be seen in Table 7.
On average, these examples are shorter than Senseval 2 training instances.
The goal is to compare the performance of a supervised word sense disambigua-
tion system using Senseval 2 training data (hand made for the competition) to that
using the sense-tagged corpus from ODP (automatically extracted). We have chosen
the Duluth system (Pedersen 2001) to perform the comparison. The Duluth system is
a freely available supervised WSD system that participated in the Senseval 2 compe-
tition. As we are not concerned with absolute performance, we simply adopted the
first of the many available versions of the system (Duluth 1).
An obstacle to performing such comparative evaluation is that, as expected, our
algorithm assigns ODP directories only to a fraction of all word senses, partly because
not every sense is domain-specific, and partly because of lack of coverage. In order to
498
Computational Linguistics Volume 29, Number 3
Table 7
Examples of training material for circuit.
circuit 1 (electrical circuit)
Electromechanical products for brand name firms; offers printed circuit boards (..)
Offers surface mount, thru-hole, and flex circuit assembly, in circuit and functional (..)
circuit 2 (tour, journey around a particular area)
The Tour du Mont-Blanc is a circuit of 322km based in the northern French Alps.
A virtual tour of the circuit by Raimon Bach.
circuit 5 (racing circuit)
The Circuit is a smooth 536 yards of racing for Hot Rod and Stock Car?s at the East of (..)
(..) History of the circuit and its banked track and news of Formula 1 (..)
circumvent this problem, we have considered only the subset of 10 Senseval nouns for
which our system tags at least two senses: bar, child, circuit, facility, grip, holiday, material,
post, restraint, and stress. We have then projected the Senseval 2 training corpus, and
the test material, onto the annotations for the word senses already in our ODP-based
material. Hence we will evaluate the quality of the training material obtained from
Web directories, not the coverage of the approach.
Table 8 shows the training material obtained for that subset of Senseval 2 nouns.
A total of 66 directories are used as a source of training instances, of which 17% are
incorrect and will presumably incorporate noise into the training. Table 9 compares the
training material for the word senses in this sample, and the results of the supervised
WSD algorithm with the Senseval and the ODP training instances.
We have measured the performance of the system in terms of Senseval recall: the
number of correctly disambiguated instances over the total number of test instances.
Overall, using the Senseval training set gives .73 recall, and training with the automat-
ically extracted ODP instances gives .58 (21% worse). A decrease of 21% is significant
but nevertheless encouraging, because the Senseval training set is the gold standard
for the Senseval test set: It is larger than the ODP set (773 versus 547 instances in this
subset), well balanced, built with redundant manual annotations, and part of the same
corpus as the test set.
Table 8
Training material obtained for the WSD experiment.
Word
Senses
Number of Directories
per Sense
Number of Incorrect
Directories
Number of Training
Instances
bar 1,10 1,1 0,0 1,1
child 1,2 3,9 0,0 3,80
circuit 1,2,5 6,2,3 0,0,0 229,2,5
facility 1,4 4,5 0,0 4,18
grip 2,7 2,1 0,1 17,6
holiday 1,2 1,1 0,1 5,17
material 1,4 6,3 2,1 63,10
post 2,3,4,7,8 1,5,1,4,3 1,1,1,0,3 2,7,1,9,3
restraint 1,4,6 2,1,1 0,0,0 2,2,2
stress 1,2 1,4 0,0 8,50
Total 66 11 547
499
Santamar??a, Gonzalo, and Verdejo Association of Web Directories with Word Senses
Table 9
Results of supervised WSD.
Word
Senses
Number of
instances
Senseval
Training
Number of
instances
ODP
Training
Number of Test
Instances
Recall
Senseval
Training
Recall
ODP
Training
bar 1,10 127,11 1,1 62,6 .91 .50
child 1,2 39,78 3,80 35,27 .57 .44
circuit 1,2,5 67,6,7 229,2,5 23,2,8 .70 .70
facility 1,4 26,61 4,18 15,28 .79 .67
grip 2,7 6,1 17,6 4,0 1.00 1.00
holiday 1,2 4,57 5,17 26,2 .96 .96
material 1,4 65,7 63,10 30,9 .79 .79
post 2,3,4,7,8 1,64,20,11,7 2,7,1,9,3 2,25,13,12,4 .45 .25
restraint 1,4,6 17,32,11 2,2,2 8,14,4 .65 .50
stress 1,2 3,45 8,50 1,19 .95 .95
Total 773 547 379 .73 .58
The most similar experiment in the literature is Agirre and Mart??nez (2000), in
which the sense-tagged instances obtained using a high-performance Web-mining al-
gorithm (Mihalcea and Moldovan 1999) performed hardly better than a random base-
line as WSD training instances. A difference between the two experiments is that
Agirre et al do not limit their experiments to the fraction of the test set for which
they have automatically extracted training samples; hence a direct comparison of the
results is not possible.
A detailed examination of the results indicates that the difference in performance
is related to the smaller number of training instances rather than to the quality of
individual instances:
? In all four cases in which ODP provides a comparable?or
larger?number of training instances (circuit, grip, material, stress), ODP
training equals hand-tagged training. In one additional case (holiday), the
number of ODP instances is smaller, but still the recall is the same. For
the other five words, the number of ODP instances is substantially
smaller and the recall is worse.
? Remarkably, incorrect directories harm recall substantially only for post,
which accumulates six erroneous associations (out of 11 errors). The
other five errors (in material 1, 4, holiday 2, grip 7) do not affect the final
recall for these words. There are two possible reasons for this behavior:
? Erroneous directories tend to be less productive in terms of
training instances. Indeed, this fact could be incorporated as an
additional filter for candidate directories. This is the case, for
instance, of material 1, for which correct directories provide
much more training material than the incorrect one.
? Erroneous directories are more frequent with rare (less frequent)
word senses. This is correlated with a smaller number of test
instances (hence the influence on average recall is lower) and
also of training instances (and then the reference, hand-tagged
material does not provide good training data either). This is the
500
Computational Linguistics Volume 29, Number 3
case of grip 7 or holiday 2, which have zero and two test
instances, respectively.
Overall, our results suggest that directory-based instances, in spite of being shorter
and automatically extracted, are not substantially worse for supervised WSD than the
hand-tagged material provided by the Senseval organization. The limitation of the
approach is currently the low coverage of word senses and the amount of training
samples. Two strategies may help in overcoming such limitations: first, propagating
directories via synonymy (attaching directories to synsets rather than word senses)
and semantic relationships (propagating directories via hyponymy relations); second,
retrieving instances not only from the ODP page describing the directory contents, but
from the Web pages listed in the directory.
The only fundamental limitation of our approach for the automatic extraction
of annotated examples is the fact that directories are closely related to topics and
domains, and therefore word senses that do not pertain to any domain cannot receive
directories and training instances from them. Still, the approach can be very useful
for language engineering applications in which only domain disambiguation (versus
sense disambiguation) is required, such as information retrieval (Gonzalo et al 1998)
and content-based user modeling (Magnini and Strapparava 2000).
5. Massive Processing of WordNet Nouns
We have applied the association algorithm to all noncompound nouns in WordNet
without nonalphabetic characters (e.g., sea lion and 10 are not included in the bulk
processing). The results can be seen in Table 10. Overall, the system associates at least
one directory with 13,375 nouns (28% of the candidate set).
The most direct way of propagating directories in the WordNet structure is ex-
tending sense/directory associations to synset/directory relations (i.e., if a word sense
receives a directory, then all word senses in the same synset receive the same direc-
tory). For instance, cable 2 (transmission line) receives the following directories:
business/industries/electronics and electrical
business/industries/electronics and electrical/hardware/connectors and terminals
business/industries/electronics and electrical/contract manufacturers
As cable 2 is part of the synset {cable 2, line 9, transmission line 1}, line 9 and transmission
line 1 inherit the three directories.
With this (quite conservative) strategy, the number of characterized nouns and
word senses almost doubles: 24,558 nouns and 27,383 senses, covering 34% of the can-
Table 10
Massive association of ODP directories with WordNet 1.7 nouns.
With Propagation
Candidate nouns 51,168
Candidate senses 73,612
Associated directories 29,291
Characterized nouns 13,375 24,558
Characterized senses 14,483 27,383
Hyponyms 1,800
501
Santamar??a, Gonzalo, and Verdejo Association of Web Directories with Word Senses
didate nouns plus 7,027 multiword terms that were not in the candidate set. The results
of this massive processing, together with the results for the Senseval 2 test (including
training material) are available for public inspection at ?http://nlp.uned.es/ODP?.
6. Conclusions
Our algorithm is able to associate ODP directories with WordNet senses with 86%
accuracy over the Senseval 2 test, and with coverage between 73% and 88% of the
domain-specific senses. Such associations can be used as rich characterizations for
word senses: as a source of information to cluster senses according to their topical
relatedness, to extract topic signatures, to acquire sense-tagged corpora, etc. The only
intrinsic limitation of the approach is that Web directories are not appropriate for
characterizing general word senses (versus domain-specific ones). If such characteri-
zation is necessary for a particular natural language application, the method should
be complemented by other means of acquiring lexical information.
In the supervised WSD experiment we have carried out, the results suggest that the
characterization of word senses with Web directories provides cleaner data, without
further sophisticated filtering, than a direct use of the full Web. Indeed the WSD
results using training material from ODP directories gives better results than could be
expected from previous cross-validations of training and test WSD materials.
Our ongoing work is extending the algorithm?which works independently for
every input word?to combine and propagate sense/directory associations over the
entire WordNet. The initial coverage of WordNet nouns is 34%, but we hope to improve
this figure by taking advantage of the WordNet structure.
Perhaps the main conclusion of our work is that Web directories are a much more
structured and reliable corpus than the whole Web. In spite of being manually su-
pervised, Web directories offer immense structured corpora that deserve our attention
as sources of linguistic information. In particular, listing word sense/ODP directory
associations has the additional advantage, compared to other Web-mining approaches,
of providing a wealth of lexical information in a very condensed manner.
Acknowledgments
This work has been partially supported by
the Spanish government through project
Hermes (TIC2000-0335-C03-01).
References
Agirre, E., O. Ansa, E. Hovy, and
D. Mart??nez. 2000. Enriching very large
ontologies using the WWW. In Proceedings
of the Ontology Learning Workshop, Berlin.
Agirre, E. and D. Mart??nez. 2000. Exploring
automatic word sense disambiguation
with decision lists and the Web. In
Proceedings of the COLING Workshop on
Semantic Annotation and Intelligent Content,
Luxembourg.
Edmonds, P. and S. Cotton. 2001.
Senseval-2: Overview. In Proceedings of
Senseval 2. Association for Computational
Linguistics, New Brunswick, NJ.
Fujii, A. and T. Ishikawa. 1999. Utilizing the
World Wide Web as an encyclopedia:
Extracting term descriptions from
semi-structured texts. In Proceedings of
ACL-99. Association for Computational
Linguistics, New Brunswick, NJ.
Gonzalo, J., F. Verdejo, I. Chugur, and
J. Cigarra?n. 1998. Indexing with Wordnet
synsets can improve text retrieval. In
COLING/ACL?98 Workshop on Usage of
WordNet in Natural Language Processing
Systems. Association for Computational
Linguistics, New Brunswick, NJ.
Grefenstette, G. 1999. The WWW as a
resource for example-based MT tasks. In
Proceedings of ASLIB-99, London.
Joho, H. and M. Sanderson. 2000. Retrieving
descriptive phrases from large amounts of
free text. In Proceedings of the 9th ACM
CIKM Conference, McLean, VA.
Kilgarriff, A. 2001a. English lexical sample
task description. In Proceedings of Senseval
2. Association for Computational
Linguistics, New Brunswick, NJ.
Kilgarriff, A. 2001b. Web as corpus. In
502
Computational Linguistics Volume 29, Number 3
Proceedings of Corpus Linguistics 2001,
Lancaster, England.
Kilgarriff, A. and M. Palmer. 2000.
Introduction to the special issue on
Senseval. Computers and the Humanities,
34(1?2).
Ma, X. and M. Liberman. 1999. Bits: A
method for bilingual text search over the
Web. In Proceedings of the Machine
Translation Summit VII, Singapore.
Magnini, B. and G. Cavaglia. 2000.
Integrating subject field codes into
WordNet. In Proceedings of LREC-2000,
Second International Conference on Language
Resources and Evaluation, Athens.
Magnini, B. and C. Strapparava. 2000.
Experiments in word domain
disambiguation for parallel texts. In
ACL-2000 Workshop on Word Sense and
Multilinguality. Association for
Computational Linguistics, New
Brunswick, NJ.
Mihalcea, R. and D. Moldovan. 1999a. An
automatic method for generating sense
tagged corpora. In Proceedings of AAAI ?99,
Orlando, FL, July, pages 461?466.
Miller, G. 1990. Wordnet: An on-line lexical
database. Special issue. International
Journal of Lexicography, 3(4).
Nie, Jian-Yun, Michel Simard, and George
Foster. 2001. Multilingual information
retrieval based on parallel texts from the
Web. In Carol Peters, Editor,
Cross-Language Information Retrieval and
Evaluation: Workshop of Cross-Language
Evaluation Forum (CLEF 2000), Lisbon,
Portugal, September 21?22, 2000, Revised
Papers. Lecture Notes in Computer
Science 2069. Berlin, Springer-Verlag,
pages 188?200.
Pedersen, T. 2001. Machine Learning with
lexical features: The Duluth approach to
Senseval-2. In Proceedings of Senseval-2.
Association for Computational
Linguistics, New Brunswick, NJ.
Peters, C., M. Braschler, J. Gonzalo, and
M. Kluck, editors. 2002. Evaluation of
Cross-Language Information Retrieval
Systems. Lecture Notes in Computer
Science 2406. Springer-Verlag.
Resnik, P. 1999. Mining the Web for
bilingual text. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics, College Park,
MD.
Resnik, P. and N. Smith. 2002. The Web as a
parallel corpus. Technical Report
UMIACS-TR-2002, University of
Maryland.
An Empirical Study of Information Synthesis Tasks
Enrique Amigo? Julio Gonzalo V??ctor Peinado Anselmo Pen?as Felisa Verdejo
Departamento de Lenguajes y Sistemas Informa?ticos
Universidad Nacional de Educacio?n a Distancia
c/Juan del Rosal, 16 - 28040 Madrid - Spain
{enrique,julio,victor,anselmo,felisa}@lsi.uned.es
Abstract
This paper describes an empirical study of the ?In-
formation Synthesis? task, defined as the process of
(given a complex information need) extracting, or-
ganizing and inter-relating the pieces of information
contained in a set of relevant documents, in order to
obtain a comprehensive, non redundant report that
satisfies the information need.
Two main results are presented: a) the creation
of an Information Synthesis testbed with 72 reports
manually generated by nine subjects for eight com-
plex topics with 100 relevant documents each; and
b) an empirical comparison of similarity metrics be-
tween reports, under the hypothesis that the best
metric is the one that best distinguishes between
manual and automatically generated reports. A met-
ric based on key concepts overlap gives better re-
sults than metrics based on n-gram overlap (such as
ROUGE) or sentence overlap.
1 Introduction
A classical Information Retrieval (IR) system helps
the user finding relevant documents in a given text
collection. In most occasions, however, this is only
the first step towards fulfilling an information need.
The next steps consist of extracting, organizing and
relating the relevant pieces of information, in or-
der to obtain a comprehensive, non redundant report
that satisfies the information need.
In this paper, we will refer to this process as In-
formation Synthesis. It is normally understood as
an (intellectually challenging) human task, and per-
haps the Google Answer Service1 is the best gen-
eral purpose illustration of how it works. In this ser-
vice, users send complex queries which cannot be
answered simply by inspecting the first two or three
documents returned by a search engine. These are a
couple of real, representative examples:
a) I?m looking for information concerning the history of text
compression both before and with computers.
1http://answers.google.com
b) Provide an analysis on the future of web browsers, if
any.
Answers to such complex information needs are
provided by experts which, commonly, search the
Internet, select the best sources, and assemble the
most relevant pieces of information into a report,
organizing the most important facts and providing
additional web hyperlinks for further reading. This
Information Synthesis task is understood, in Google
Answers, as a human task for which a search engine
only provides the initial starting point. Our mid-
term goal is to develop computer assistants that help
users to accomplish Information Synthesis tasks.
From a Computational Linguistics point of view,
Information Synthesis can be seen as a kind of
topic-oriented, informative multi-document sum-
marization, where the goal is to produce a single
text as a compressed version of a set of documents
with a minimum loss of relevant information. Un-
like indicative summaries (which help to determine
whether a document is relevant to a particular topic),
informative summaries must be helpful to answer,
for instance, factual questions about the topic. In
the remainder of the paper, we will use the term
?reports? to refer to the summaries produced in an
Information Synthesis task, in order to distinguish
them from other kinds of summaries.
Topic-oriented multi-document summarization
has already been studied in other evaluation ini-
tiatives which provide testbeds to compare alterna-
tive approaches (Over, 2003; Goldstein et al, 2000;
Radev et al, 2000). Unfortunately, those stud-
ies have been restricted to very small summaries
(around 100 words) and small document sets (10-
20 documents). These are relevant summarization
tasks, but hardly representative of the Information
Synthesis problem we are focusing on.
The first goal of our work has been, therefore,
to create a suitable testbed that permits qualitative
and quantitative studies on the information synthe-
sis task. Section 2 describes the creation of such a
testbed, which includes the manual generation of 72
reports by nine different subjects across 8 complex
topics with 100 relevant documents per topic.
Using this testbed, our second goal has been to
compare alternative similarity metrics for the Infor-
mation Synthesis task. A good similarity metric
provides a way of evaluating Information Synthe-
sis systems (comparing their output with manually
generated reports), and should also shed some light
on the common properties of manually generated re-
ports. Our working hypothesis is that the best metric
will best distinguish between manual and automati-
cally generated reports.
We have compared several similarity metrics, in-
cluding a few baseline measures (based on docu-
ment, sentence and vocabulary overlap) and a state-
of-the-art measure to evaluate summarization sys-
tems, ROUGE (Lin and Hovy, 2003). We also intro-
duce another proximity measure based on key con-
cept overlap, which turns out to be substantially bet-
ter than ROUGE for a relevant class of topics.
Section 3 describes these metrics and the experi-
mental design to compare them; in Section 4, we an-
alyze the outcome of the experiment, and Section 5
discusses related work. Finally, Section 6 draws the
main conclusions of this work.
2 Creation of an Information Synthesis
testbed
We refer to Information Synthesis as the process
of generating a topic-oriented report from a non-
trivial amount of relevant, possibly interrelated doc-
uments. The first goal of our work is the generation
of a testbed (ISCORPUS) with manually produced
reports that serve as a starting point for further em-
pirical studies and evaluation of information synthe-
sis systems. This section describes how this testbed
has been built.
2.1 Document collection and topic set
The testbed must have a certain number of features
which, altogether, differentiate the task from current
multi-document summarization evaluations:
Complex information needs. Being Informa-
tion Synthesis a step which immediately follows a
document retrieval process, it seems natural to start
with standard IR topics as used in evaluation con-
ferences such as TREC2, CLEF3 or NTCIR4. The
title/description/narrative topics commonly used in
such evaluation exercises are specially well suited
for an Information Synthesis task: they are complex
2http://trec.nist.gov
3http://www.clef-campaign.org
4http://research.nii.ac.jp/ntcir/
and well defined, unlike, for instance, typical web
queries.
We have selected the Spanish CLEF 2001-2003
news collection testbed (Peters et al, 2002), be-
cause Spanish is the native language of the subjects
recruited for the manual generation of reports. Out
of the CLEF topic set, we have chosen the eight
topics with the largest number of documents man-
ually judged as relevant from the assessment pools.
We have slightly reworded the topics to change the
document retrieval focus (?Find documents that...?)
into an information synthesis wording (?Generate a
report about...?). Table 1 shows the eight selected
topics.
C042: Generate a report about the invasion of Haiti by UN/US
soldiers.
C045: Generate a report about the main negotiators of the
Middle East peace treaty between Israel and Jordan, giving
detailed information on the treaty.
C047: What are the reasons for the military intervention of
Russia in Chechnya?
C048: Reasons for the withdrawal of United Nations (UN)
peace- keeping forces from Bosnia.
C050: Generate a report about the uprising of Indians in
Chiapas (Mexico).
C085: Generate a report about the operation ?Turquoise?, the
French humanitarian program in Rwanda.
C056: Generate a report about campaigns against racism in
Europe.
C080: Generate a report about hunger strikes attempted in
order to attract attention to a cause.
Table 1: Topic set
This set of eight CLEF topics has two differenti-
ated subsets: in a majority of cases (first six topics),
it is necessary to study how a situation evolves in
time; the importance of every event related to the
topic can only be established in relation with the
others. The invasion of Haiti by UN and USA troops
(C042) is an example of such a topic. We will refer
to them as ?Topic Tracking? (TT) reports, because
they resemble the kind of topics used in such task.
The last two questions (56 and 80), however, re-
semble Information Extraction tasks: essentially,
the user has to detect and describe instances of
a generic event (cases of hunger strikes and cam-
paigns against racism in Europe); hence we will re-
fer to them as ?IE? reports.
Topic tracking reports need a more elaborated
treatment of the information in the documents, and
therefore are more interesting from the point of view
of Information Synthesis. We have, however, de-
cided to keep the two IE topics; first, because they
also reflect a realistic synthesis task; and second, be-
cause they can provide contrastive information as
compared to TT reports.
Large document sets. All the selected CLEF
topics have more than one hundred documents
judged as relevant by the CLEF assessors. For ho-
mogeneity, we have restricted the task to the first
100 documents for each topic (using a chronologi-
cal order).
Complex reports. The elaboration of a com-
prehensive report requires more space than is al-
lowed in current multi-document summarization ex-
periences. We have established a maximum of fifty
sentences per summary, i.e., half a sentence per doc-
ument. This limit satisfies three conditions: a) it
is large enough to contain the essential information
about the topic, b) it requires a substantial compres-
sion effort from the user, and c) it avoids defaulting
to a ?first sentence? strategy by lazy (or tired) users,
because this strategy would double the maximum
size allowed.
We decided that the report generation would be
an extractive task, which consists of selecting sen-
tences from the documents. Obviously, a realistic
information synthesis process also involves rewrit-
ing and elaboration of the texts contained in the doc-
uments. Keeping the task extractive has, however,
two major advantages: first, it permits a direct com-
parison to automatic systems, which will typically
be extractive; and second, it is a simpler task which
produces less fatigue.
2.2 Generation of manual reports
Nine subjects between 25 and 35 years-old were re-
cruited for the manual generation of reports. All
of them self-reported university degrees and a large
experience using search engines and performing in-
formation searches.
All subjects were given an in-place detailed de-
scription of the task in order to minimize divergent
interpretations. They were told that, in a first step,
they had to generate reports with a maximum of in-
formation about every topic within the fifty sentence
space limit. In a second step, which would take
place six months afterwards, they would be exam-
ined from each of the eight topics. The only docu-
mentation allowed during the exam would be the re-
ports generated in the first phase of the experiment.
Subjects scoring best would be rewarded.
These instructions had two practical effects: first,
the competitive setup was an extra motivation for
achieving better results. And second, users tried to
take advantage of all available space, and thus most
reports were close to the fifty sentences limit. The
time limit per topic was set to 30 minutes, which is
tight for the information synthesis task, but prevents
the effects of fatigue.
We implemented an interface to facilitate the gen-
eration of extractive reports. The system displays a
list with the titles of relevant documents in chrono-
logical order. Clicking on a title displays the full
document, where the user can select any sentence(s)
and add them to the final report. A different frame
displays the selected sentences (also in chronolog-
ical order), together with one bar indicating the re-
maining time and another bar indicating the remain-
ing space. The 50 sentence limit can be temporarily
exceeded and, when the 30 minute limit has been
reached, the user can still remove sentences from
the report until the sentence limit is reached back.
2.3 Questionnaires
After summarizing every topic, the following ques-
tionnaire was filled in by every user:
? Who are the main people involved in the topic?
? What are the main organizations participating in the
topic?
? What are the key factors in the topic?
Users provided free-text answers to these ques-
tions, with their freshly generated summary at hand.
We did not provide any suggestions or constraints
at this point, except that a maximum of eight slots
were available per question (i.e. a maximum of
8X3 = 24 key concepts per topic, per user).
This is, for instance, the answer of one user for
the topic 42 about the invasion of Haiti by UN and
USA troops in 1994:
People Organizations
Jean Bertrand Aristide ONU (UN)
Clinton EEUU (USA)
Raoul Cedras OEA (OAS)
Philippe Biambi
Michel Josep Francois
Factors
militares golpistas (coup attempting soldiers)
golpe militar (coup attempt)
restaurar la democracia (reinstatement of democracy)
Finally, a single list of key concepts is gener-
ated for each topic, joining all the different answers.
Redundant concepts (e.g. ?war? and ?conflict?)
were inspected and collapsed by hand. These lists
of key concepts constitute the gold standard for the
similarity metric described in Section 3.2.5.
Besides identifying key concepts, users also filled
in the following questionnaire:
? Were you familiarized with the topic?
? Was it hard for you to elaborate the report?
? Did you miss the possibility of introducing annotations
or rewriting parts of the report by hand?
? Do you consider that you generated a good report?
? Are you tired?
Out of the answers provided by users, the most
remarkable facts are that:
? only in 6% of the cases the user missed ?a lot?
the possibility of rewriting/adding comments
to the topic. The fact that reports are made ex-
tractively did not seem to be a significant prob-
lem for our users.
? in 73% of the cases, the user was quite or very
satisfied about his summary.
These are indications that the practical con-
straints imposed on the task (time limit and extrac-
tive nature of the summaries) do not necessarily
compromise the representativeness of the testbed.
The time limit is very tight, but the temporal ar-
rangement of documents and their highly redundant
nature facilitates skipping repetitive material (some
pieces of news are discarded just by looking at the
title, without examining the content).
2.4 Generation of baseline reports
We have automatically generated baseline reports in
two steps:
? For every topic, we have produced 30 tentative
baseline reports using DUC style criteria:
? 18 summaries consist only of picking the
first sentence out of each document in 18
different document subsets. The subsets
are formed using different strategies, e.g.
the most relevant documents for the query
(according to the Inquery search engine),
one document per day, the first or last 50
documents in chronological order, etc.
? The other 12 summaries consist of a)
picking the first n sentences out of a set
of selected documents (with different val-
ues for n and different sets of documents)
and b) taking the full content of a few doc-
uments. In both cases, document sets are
formed with similar criteria as above.
? Out of these 30 baseline reports, we have se-
lected the 10 reports which have the highest
sentence overlap with the manual summaries.
The second step increases the quality of the base-
lines, making the task of differentiating manual and
baseline reports more challenging.
3 Comparison of similarity metrics
Formal aspects of a summary (or report), such
as legibility, grammatical correctness, informative-
ness, etc., can only be evaluated manually. How-
ever, automatic evaluation metrics can play a useful
role in the evaluation of how well the information
from the original sources is preserved (Mani, 2001).
Previous studies have shown that it is feasible to
evaluate the output of summarization systems au-
tomatically (Lin and Hovy, 2003). The process is
based in similarity metrics between texts. The first
step is to establish a (manual) reference summary,
and then the automatically generated summaries are
ranked according to their similarity to the reference
summary.
The challenge is, then, to define an appropriate
proximity metric for reports generated in the infor-
mation synthesis task.
3.1 How to compare similarity metrics without
human judgments? The QARLA
estimation
In tasks such as Machine Translation and Summa-
rization, the quality of a proximity metric is mea-
sured in terms of the correlation between the rank-
ing produced by the metric, and a reference ranking
produced by human judges. An optimal similarity
metric should produce the same ranking as human
judges.
In our case, acquiring human judgments about
the quality of the baseline reports is too costly, and
probably cannot be done reliably: a fine-grained
evaluation of 50-sentence reports summarizing sets
of 100 documents is a very complex task, which
would probably produce different rankings from
different judges.
We believe there is a cheaper and more robust
way of comparing similarity metrics without using
human assessments. We assume a simple hypothe-
sis: the best metric should be the one that best dis-
criminates between manual and automatically gen-
erated reports. In other words, a similarity metric
that cannot distinguish manual and automatic re-
ports cannot be a good metric. Then, all we need
is an estimation of how well a similarity metric sep-
arates manual and automatic reports. We propose
to use the probability that, given any manual report
Mref , any other manual report M is closer to Mref
than any other automatic report A:
QARLA(sim) = P (sim(M,Mref ) > sim(A,Mref ))
where M,Mref ?M, A ? A
where M is the set of manually generated re-
ports, A is the set of automatically generated re-
ports, and ?sim? is the similarity metric being eval-
uated.
We refer to this value as the QARLA5 estimation.
QARLA has two interesting features:
? No human assessments are needed to compute
QARLA. Only a set of manually produced
summaries and a set of automatic summaries,
for each topic considered. This reduces the
cost of creating the testbed and, in addition,
eliminates the possible bias introduced by hu-
man judges.
? It is easy to collect enough data to achieve sta-
tistically significant results. For instance, our
testbed provides 720 combinations per topic
to estimate QARLA probability (we have
nine manual plus ten automatic summaries per
topic).
A good QARLA value does not guarantee that
a similarity metric will produce the same rankings
as human judges, but a good similarity metric must
have a good QARLA value: it is unlikely that
a measure that cannot distinguish between manual
and automatic summaries can still produce high-
quality rankings of automatic summaries by com-
parison to manual reference summaries.
3.2 Similarity metrics
We have compared five different metrics using the
QARLA estimation. The first three are meant as
baselines; the fourth is the standard similarity met-
ric used to evaluate summaries (ROUGE); and the
last one, introduced in this paper, is based on the
overlapping of key concepts.
3.2.1 Baseline 1: Document co-selection metric
The following metric estimates the similarity of two
reports from the set of documents which are repre-
sented in both reports (i.e. at least one sentence in
each report belongs to the document).
DocSim(Mr,M) =
|Doc(Mr) ?Doc(M)|
|Doc(Mr)|
where Mr is the reference report, M a second re-
port and Doc(Mr), Doc(M) are the documents to
which the sentences in Mr,M belong to.
5Quality criterion for reports evaluation metrics
3.2.2 Baselines 2 and 3: Sentence co-selection
The more sentences in common between two re-
ports, the more similar their content will be. We can
measure Recall (how many sentences from the ref-
erence report are also in the contrastive report) and
Precision (how many sentences from the contrastive
report are also in the reference report):
SentenceSimR(Mr,M) =
|S(Mr) ? S(M)|
|S(Mr)|
SentenceSimP (Mr,M) =
|S(Mr) ? S(M)|
|S(M)|
where S(Mr), S(M) are the sets of sentences in
the reports Mr (reference) and M (contrastive).
3.2.3 Baseline 4: Perplexity
A language model is a probability distribution over
word sequences obtained from some training cor-
pora (see e.g. (Manning and Schutze, 1999)). Per-
plexity is a measure of the degree of surprise of a
text or corpus given a language model. In our case,
we build a language model LM(Mr) for the refer-
ence report Mr, and measure the perplexity of the
contrastive report M as compared to that language
model:
PerplexitySim(Mr,M) =
1
Perp(LM(Mr),M)
We have used the Good-Turing discount algo-
rithm to compute the language models (Clarkson
and Rosenfeld, 1997). Note that this is also a base-
line metric, because it only measures whether the
content of the contrastive report is compatible with
the reference report, but it does not consider the cov-
erage: a single sentence from the reference report
will have a low perplexity, even if it covers only a
small fraction of the whole report. This problem
is mitigated by the fact that we are comparing re-
ports of approximately the same size and without
repeated sentences.
3.2.4 ROUGE metric
The distance between two summaries can be estab-
lished as a function of their vocabulary (unigrams)
and how this vocabulary is used (n-grams). From
this point of view, some of the measures used in the
evaluation of Machine Translation systems, such as
BLEU (Papineni et al, 2002), have been imported
into the summarization task. BLEU is based in the
precision and n-gram co-ocurrence between an au-
tomatic translation and a reference manual transla-
tion.
(Lin and Hovy, 2003) tried to apply BLEU as
a measure to evaluate summaries, but the results
were not as good as in Machine Translation. In-
deed, some of the characteristics that define a good
translation are not related with the features of a good
summary; then Lin and Hovy proposed a recall-
based variation of BLEU, known as ROUGE. The
idea is the same: the quality of a proposed sum-
mary can be calculated as a function of the n-grams
in common between the units of a model summary.
The units can be sentences or discourse units:
ROUGEn =
?
C?{MU}
?
n-gram?C Countm
?
C?{MU}
?
n-gram?C Count
where MU is the set of model units, Countm is
the maximum number of n-grams co-ocurring in a
peer summary and a model unit, and Count is the
number of n-grams in the model unit. It has been
established that unigram and bigram based metrics
permit to create a ranking of automatic summaries
better (more similar to a human-produced ranking)
than n-grams with n > 2.
For our experiment, we have only considered un-
igrams (lemmatized words, excluding stop words),
which gives good results with standard summaries
(Lin and Hovy, 2003).
3.2.5 Key concepts metric
Two summaries generated by different subjects may
differ in the documents that contribute to the sum-
mary, in the sentences that are chosen, and even in
the information that they provide. In our Informa-
tion Synthesis settings, where topics are complex
and the number of documents to summarize is large,
it is likely to expect that similarity measures based
on document, sentence or n-gram overlap do not
give large similarity values between pairs of man-
ually generated summaries.
Our hypothesis is that two manual reports, even if
they differ in their information content, will have the
same (or very similar) key concepts; if this is true,
comparing the key concepts of two reports can be a
better similarity measure than the previous ones.
In order to measure the overlap of key concepts
between two reports, we create a vector ~kc for every
report, such that every element in the vector repre-
sents the frequency of a key concept in the report in
relation to the size of the report:
kc(M)i =
freq(Ci,M)
|words(M)|
being freq(Ci,M) the number of times the
key concept Ci appears in the report M , and
|words(M)| the number of words in the report.
The key concept similarity NICOS (Nuclear In-
formative Concept Similarity) between two reports
M and Mr can then be defined as the inverse of the
Euclidean distance between their associated concept
vectors:
NICOS(M,Mr) =
1
| ~kc(Mr)? ~kc(M)|
In our experiment, the dimensions of kc vectors
correspond to the list of key concepts provided by
our test subjects (see Section 2.3). This list is our
gold standard for every topic.
4 Experimental results
Figure 1 shows, for every topic (horizontal axis),
the QARLA estimation obtained for each similarity
metric, i.e., the probability of a manual report being
closer to other manual report than to an automatic
report. Table 2 shows the average QARLA measure
across all topics.
Metric TT topics IE topics
Perplexity 0.19 0.60
DocSim 0.20 0.34
SentenceSimR 0.29 0.52
SentenceSimP 0.38 0.57
ROUGE 0.54 0.53
NICOS 0.77 0.52
Table 2: Average QARLA
For the six TT topics, the key concept similarity
NICOS performs 43% better than ROUGE, and all
baselines give poor results (all their QARLA proba-
bilities are below chance, QARLA < 0.5). A non-
parametric Wilcoxon sign test confirms that the dif-
ference between NICOS and ROUGE is highly sig-
nificant (p < 0.005). This is an indication that the
Information Synthesis task, as we have defined it,
should not be studied as a standard summarization
problem. It also confirms our hypothesis that key
concepts tend to be stable across different users, and
may help to generate the reports.
The behavior of the two Information Extraction
(IE) topics is substantially different from TT topics.
While the ROUGE measure remains stable (0.53
versus 0.54), the key concept similarity is much
worse with IE topics (0.52 versus 0.77). On the
other hand, all baselines improve, and some of them
(SentenceSim precision and perplexity) give better
results than both ROUGE and NICOS.
Of course, no reliable conclusion can be obtained
from only two IE topics. But the observed differ-
ences suggest that TT and IE may need different
approaches, both to the automatic generation of re-
ports and to their evaluation.
Figure 1: Comparison of similarity metrics by topic
One possible reason for this different behavior is
that IE topics do not have a set of consistent key
concepts; every case of a hunger strike, for instance,
involves different people, organizations and places.
The average number of different key concepts is
18.7 for TT topics and 28.5 for IE topics, a differ-
ence that reveals less agreement between subjects,
supporting this argument.
5 Related work
Besides the measures included in our experiment,
there are other criteria to compare summaries which
could as well be tested for Information Synthesis:
Annotation of relevant sentences in a corpus.
(Khandelwal et al, 2001) propose a task, called
?Temporal Summarization?, that combines summa-
rization and topic tracking. The paper describes the
creation of an evaluation corpus in which the most
relevant sentences in a set of related news were an-
notated. Summaries are evaluated with a measure
called ?novel recall?, based in sentences selected by
a summarization system and sentences manually as-
sociated to events in the corpus. The agreement rate
between subjects in the identification of key events
and the sentence annotation does not correspond
with the agreement between reports that we have
obtained in our experiments. There are, at least, two
reasons to explain this:
? (Khandelwal et al, 2001) work on an average
of 43 documents, half the size of the topics in
our corpus.
? Although there are topics in both experiments,
the information needs in our testbed are more
complex (e.g. motivations for the invasion of
Chechnya)
Factoids. One of the problems in the evalua-
tion of summaries is the versatility of human lan-
guage. Two different summaries may contain the
same information. In (Halteren and Teufel, 2003),
the content of summaries is manually represented,
decomposing sentences in factoids or simple facts.
They also annotate the composition, generalization
and implication relations between extracted fac-
toids. The resulting measure is different from un-
igram based similarity. The main problem of fac-
toids, as compared to other metrics, is that they re-
quire a costly manual processing of the summaries
to be evaluated.
6 Conclusions
In this paper, we have reported an empirical study
of the ?Information Synthesis? task, defined as the
process of (given a complex information need) ex-
tracting, organizing and relating the pieces of infor-
mation contained in a set of relevant documents, in
order to obtain a comprehensive, non redundant re-
port that satisfies the information need.
We have obtained two main results:
? The creation of an Information Synthesis
testbed (ISCORPUS) with 72 reports manually
generated by 9 subjects for 8 complex topics
with 100 relevant documents each.
? The empirical comparison of candidate metrics
to estimate the similarity between reports.
Our empirical comparison uses a quantitative cri-
terion (the QARLA estimation) based on the hy-
pothesis that a good similarity metric will be able to
distinguish between manual and automatic reports.
According to this measure, we have found evidence
that the Information Synthesis task is not a standard
multi-document summarization problem: state-of-
the-art similarity metrics for summaries do not per-
form equally well with the reports in our testbed.
Our most interesting finding is that manually
generated reports tend to have the same key con-
cepts: a similarity metric based on overlapping key
concepts (NICOS) gives significantly better results
than metrics based on language models, n-gram co-
ocurrence and sentence overlapping. This is an in-
dication that detecting relevant key concepts is a
promising strategy in the process of generating re-
ports.
Our results, however, has also some intrinsic lim-
itations. Firstly, manually generated summaries are
extractive, which is good for comparison purposes,
but does not faithfully reflect a natural process of
human information synthesis. Another weakness is
the maximum time allowed per report: 30 minutes
seems too little to examine 100 documents and ex-
tract a decent report, but allowing more time would
have caused an excessive fatigue to users. Our vol-
unteers, however, reported a medium to high satis-
faction with the results of their work, and in some
occasions finished their task without reaching the
time limit.
ISCORPUS is available at:
http://nlp.uned.es/ISCORPUS
Acknowledgments
This research has been partially supported by a
grant of the Spanish Government, project HERMES
(TIC-2000-0335-C03-01). We are indebted to E.
Hovy for his comments on an earlier version of
this paper, and C. Y. Lin for his assistance with the
ROUGE measure. Thanks also to our volunteers for
their valuable cooperation.
References
P. Clarkson and R. Rosenfeld. 1997. Statistical
language modeling using the CMU-Cambridge
toolkit. In Proceeding of Eurospeech ?97,
Rhodes, Greece.
J. Goldstein, V. O. Mittal, J. G. Carbonell, and
J. P. Callan. 2000. Creating and Evaluating
Multi-Document Sentence Extract Summaries.
In Proceedings of Ninth International Confer-
ences on Information Knowledge Management
(CIKM?00), pages 165?172, McLean, VA.
H. V. Halteren and S. Teufel. 2003. Examin-
ing the Consensus between Human Summaries:
Initial Experiments with Factoids Analysis. In
HLT/NAACL-2003 Workshop on Automatic Sum-
marization, Edmonton, Canada.
V. Khandelwal, R. Gupta, and J. Allan. 2001. An
Evaluation Corpus for Temporal Summarization.
In Proceedings of the First International Confer-
ence on Human Language Technology Research
(HLT 2001), Tolouse, France.
C. Lin and E. H. Hovy. 2003. Automatic Evalua-
tion of Summaries Using N-gram Co-ocurrence
Statistics. In Proceeding of the 2003 Language
Technology Conference (HLT-NAACL 2003), Ed-
monton, Canada.
I. Mani. 2001. Automatic Summarization, vol-
ume 3 of Natural Language Processing. John
Benjamins Publishing Company, Amster-
dam/Philadelphia.
C. D. Manning and H. Schutze. 1999. Foundations
of statistical natural language processing. MIT
Press, Cambridge Mass.
P. Over. 2003. Introduction to DUC-2003: An In-
trinsic Evaluation of Generic News Text Summa-
rization Systems. In Proceedings of Workshop on
Automatic Summarization (DUC 2003).
K. Papineni, S. Roukos, T. Ward, and W. Zhu.
2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311?
318, Philadelphia.
C. Peters, M. Braschler, J. Gonzalo, and M. Kluck,
editors. 2002. Evaluation of Cross-Language
Information Retrieval Systems, volume 2406 of
Lecture Notes in Computer Science. Springer-
Verlag, Berlin-Heidelberg-New York.
D. R. Radev, J. Hongyan, and M. Budzikowska.
2000. Centroid-Based Summarization of Mul-
tiple Documents: Sentence Extraction, Utility-
Based Evaluation, and User Studies. In Proceed-
ings of the Workshop on Automatic Summariza-
tion at the 6th Applied Natural Language Pro-
cessing Conference and the 1st Conference of the
North American Chapter of the Association for
Computational Linguistics, Seattle, WA, April.
Proceedings of the 43rd Annual Meeting of the ACL, pages 280?289,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
QARLA:A Framework for the Evaluation of Text Summarization Systems
Enrique Amigo?, Julio Gonzalo, Anselmo Pen?as, Felisa Verdejo
Departamento de Lenguajes y Sistemas Informa?ticos
Universidad Nacional de Educacio?n a Distancia
c/Juan del Rosal, 16 - 28040 Madrid - Spain
{enrique,julio,anselmo,felisa}@lsi.uned.es
Abstract
This paper presents a probabilistic
framework, QARLA, for the evaluation
of text summarisation systems. The in-
put of the framework is a set of man-
ual (reference) summaries, a set of base-
line (automatic) summaries and a set of
similarity metrics between summaries.
It provides i) a measure to evaluate the
quality of any set of similarity metrics,
ii) a measure to evaluate the quality of
a summary using an optimal set of simi-
larity metrics, and iii) a measure to eval-
uate whether the set of baseline sum-
maries is reliable or may produce biased
results.
Compared to previous approaches, our
framework is able to combine different
metrics and evaluate the quality of a set
of metrics without any a-priori weight-
ing of their relative importance. We pro-
vide quantitative evidence about the ef-
fectiveness of the approach to improve
the automatic evaluation of text sum-
marisation systems by combining sev-
eral similarity metrics.
1 Introduction
The quality of an automatic summary can be es-
tablished mainly with two approaches:
Human assessments: The output of a number of
summarisation systems is compared by hu-
man judges, using some set of evaluation
guidelines.
Proximity to a gold standard: The best auto-
matic summary is the one that is closest to
some reference summary made by humans.
Using human assessments has some clear ad-
vantages: the results of the evaluation are inter-
pretable, and we can trace what a system is do-
ing well, and what is doing poorly. But it also
has a couple of serious drawbacks: i) different hu-
man assessors reach different conclusions, and ii)
the outcome of a comparative evaluation exercise
is not directly reusable for new techniques, i.e., a
summarisation strategy developed after the com-
parative exercise cannot be evaluated without ad-
ditional human assessments made from scratch.
Proximity to a gold standard, on the other hand,
is a criterion that can be automated (see Section 6),
with the advantages of i) being objective, and ii)
once gold standard summaries are built for a com-
parative evaluation of systems, the resulting test-
bed can iteratively be used to refine text summari-
sation techniques and re-evaluate them automati-
cally.
This second approach, however, requires solv-
ing a number of non-trivial issues. For instance,
(i) How can we know whether an evaluation met-
ric is good enough for automatic evaluation?, (ii)
different users produce different summaries, all of
them equally good as gold standards, (iii) if we
have several metrics which test different features
of a summary, how can we combine them into an
optimal test?, (iv) how do we know if our test bed
280
Figure 1: Illustration of some of the restrictions on Q,K
is reliable, or the evaluation outcome may change
by adding, for instance, additional gold standards?
In this paper, we introduce a probabilistic
framework, QARLA, that addresses such issues.
Given a set of manual summaries and another set
of baseline summaries per task, together with a set
of similarity metrics, QARLA provides quantita-
tive measures to (i) select and combine the best
(independent) metrics (KING measure), (ii) apply
the best set of metrics to evaluate automatic sum-
maries (QUEEN measure), and (iii) test whether
evaluating with that test-bed is reliable (JACK
measure).
2 Formal constraints on any evaluation
framework based on similarity metrics
We are looking for a framework to evaluate au-
tomatic summarisation systems objectively using
similarity metrics to compare summaries. The in-
put of the framework is:
? A summarisation task (e.g. topic oriented, in-
formative multi-document summarisation on
a given domain/corpus).
? A set T of test cases (e.g. topic/document set
pairs for the example above)
? A set of summaries M produced by humans
(models), and a set of automatic summaries
A (peers), for every test case.
? A set X of similarity metrics to compare
summaries.
An evaluation framework should include, at
least:
? A measure QM,X(a) ? [0, 1] that estimates
the quality of an automatic summary a, us-
ing the similarity metrics in X to compare
the summary with the models in M . With
Q, we can compare the quality of automatic
summaries.
? A measure KM,A(X) ? [0, 1] that estimates
the suitability of a set of similarity metrics X
for our evaluation purposes. With K, we can
choose the best similarity metrics.
Our main assumption is that all manual sum-
maries are equally optimal and, while they are
likely to be different, the best similarity metric is
the one that identifies and uses the features that are
common to all manual summaries, grouping and
separating them from the automatic summaries.
With these assumption in mind, it is useful to
think of some formal restrictions that any evalua-
tion framework Q,K must hold. We will consider
the following ones (see illustrations in Figure 1):
(1) Given two automatic summaries a, a? and a
similarity measure x, if a is more distant to all
manual summaries than a?, then a cannot be better
281
than a?. Formally: ?m ? M.x(a,m) < x(a?,m) ?
QM,x(a) ? QM,x(a?)
(2) A similarity metric x is better when it is able
to group manual summaries more closely, while
keeping them more distant from automatic sum-
maries: (?m,m? ? M.x(m,m?) > x?(m,m?) ? ?m ?
M,a ? Ax(a,m) < x?(a,m)) ? KM,A(x) > KM,A(x?)
(3) If x is a perfect similarity metric, the quality of
a manual summary cannot be zero: KM,A(x) = 1 ?
?m ?M.QM,x(m) > 0
(4) The quality of a similarity metric or a summary
should not be dependent on scale issues. In gen-
eral, if x? = f(x) with f being a growing mono-
tonic function, then KM,A(x) = KM,A(x?) and
QM,x(a) = QM,x?(a) .
(5) The quality of a similarity metric should
not be sensitive to repeated elements in A, i.e.
KM,A?{a}(x) = KM,A?{a,a}(x).
(6) A random metric x should have KM,A(x) = 0.
(7) A non-informative (constant) metric x should
have KM,A(x) = 0.
3 QARLA evaluation framework
3.1 QUEEN: Estimation of the quality of an
automatic summary
We are now looking for a function QM,x(a) that
estimates the quality of an automatic summary a ?
A, given a set of models M and a similarity metric
x.
An obvious first attempt would be to compute
the average similarity of a to all model summaries
in M in a test sample. But such a measure depends
on scale properties: metrics producing larger sim-
ilarity values will produce larger Q values; and,
depending on the scale properties of x, this cannot
be solved just by scaling the final Q value.
A probabilistic measure that solves this problem
and satisfies all the stated formal constraints is:
QUEENx,M (a) ? P (x(a,m) ? x(m?,m??))
which defines the quality of an automatic sum-
mary a as the probability over triples of manual
summaries m,m?,m?? that a is closer to a model
than the other two models to each other. This mea-
sure draws from the way in which some formal re-
strictions on Q are stated (by comparing similarity
values), and is inspired in the QARLA criterion
introduced in (Amigo et al, 2004).
Figure 2: Summaries quality in a similarity metric
space
Figure 2 illustrates some of the features of the
QUEEN estimation:
? Peers which are very far from the set of
models all receive QUEEN = 0. In other
words, QUEEN does not distinguish between
very poor automatic summarisation strate-
gies. While this feature reduces granularity
of the ranking produced by QUEEN, we find
it desirable, because in such situations, the
values returned by a similarity measure are
probably meaningless.
? The value of QUEEN is maximised for the
peers that ?merge? with the models. For
QUEEN values between 0.5 and 1, peers are
effectively merged with the models.
? An ideal metric (that puts all models to-
gether) would give QUEEN(m) = 1 for all
models, and QUEEN(a) = 0 for all peers
that are not put together with the models.
This is a reasonable boundary condition say-
ing that, if we can distinguish between mod-
els and peers perfectly, then all peers are
poor emulations of human summarising be-
haviour.
3.2 Generalisation of QUEEN to metric sets
It is desirable, however, to have the possibility of
evaluating summaries with respect to several met-
rics together. Let us imagine, for instance, that
the best metric turns out to be a ROUGE (Lin and
Hovy, 2003a) variant that only considers unigrams
to compute similarity. Now consider a summary
282
which has almost the same vocabulary as a hu-
man summary, but with a random scrambling of
the words which makes it unreadable. Even if the
unigram measure is the best hint of similarity to
human performance, in this case it would produce
a high similarity value, while any measure based
on 2-grams, 3-grams or on any simple syntactic
property would detect that the summary is useless.
The issue is, therefore, how to find informative
metrics, and then how to combine them into an op-
timal single quality estimation for automatic sum-
maries. The most immediate way of combining
metrics is via some weighted linear combination.
But our example suggests that this is not the op-
timal way: the unigram measure would take the
higher weight, and therefore it would assign a fair
amount of credit to a summary that can be strongly
rejected with other criteria.
Alternatively, we can assume that a summary is
better if it is closer to the model summaries ac-
cording to all metrics. We can formalise this idea
by introducing a universal quantifier on the vari-
able x in the QUEEN formula. In other words,
QUEENX,M (a) can be defined as the probability,
measured over M ?M ?M , that for every metric
in X the automatic summary a is closer to a model
than two models to each other.
QUEENX,M (a) ? P (?x ? X.x(a,m) ? x(m?,m??))
We can think of the generalised QUEEN mea-
sure as a way of using a set of tests (every simi-
larity metric in X) to falsify the hypothesis that a
given summary a is a model. If, for every compar-
ison of similarities between a,m,m?,m??, there is
at least one test that a does not pass, then a is re-
jected as a model.
This generalised measure is not affected by the
scale properties of every individual metric, i.e. it
does not require metric normalisation and it is not
affected by metric weighting. In addition, it still
satisfies the properties enumerated for its single-
metric counterpart.
Of course, the quality ranking provided by
QUEEN is meaningless if the similarity metric x
does not capture the essential features of the mod-
els. Therefore, we need to estimate the quality of
similarity metrics in order to use QUEEN effec-
tively.
3.3 KING: estimation of the quality of a
similarity metric
Now we need a measure KM,A(x) that estimates
the quality of a similarity metric x to evaluate
automatic summaries (peers) by comparison to
human-produced models.
In order to build a suitable K estimation, we
will again start from the hypothesis that the best
metric is the one that best characterises human
summaries as opposed to automatic summaries.
Such a metric should identify human summaries
as closer to each other, and more distant to peers
(second constraint in Section 2). By analogy with
QUEEN, we can try (for a single metric):
KM,A(x) ? P (x(a,m) < x(m
?,m??)) =
1? (QUEENx,M (a))
which is the probability that two models are
closer to each other than a third model to a peer,
and has smaller values when the average QUEEN
value of peers decreases. The generalisation of K
to metric sets would be simply:
KM,A(X) ? 1? (QUEENX,M (a)))
This measure, however, does not satisfy formal
conditions 3 and 5. Condition 3 is violated be-
cause, given a limited set of models, the K mea-
sure grows with a large number of metrics in X ,
eventually reaching K = 1 (perfect metric set).
But in this situation, QUEEN(m) becomes 0 for
all models, because there will always exist a met-
ric that breaks the universal quantifier condition
over x.
We have to look, then, for an alternative for-
mulation for K. The best K should minimise
QUEEN(a), but having the quality of the models
as a reference. A direct formulation can be:
KM,A(X) = P (QUEEN(m) > QUEEN(a))
According to this formula, the quality of a met-
ric set X is the probability that the quality of a
283
model is higher than the quality of a peer ac-
cording to this metric set. This formula satisfies
all formal conditions except 5 (KM,A?{a}(x) =
KM,A?{a,a}(x)), because it is sensitive to repeated
peers. If we add a large set of identical (or very
similar peers), K will be biased towards this set.
We can define a suitable K that satisfies condi-
tion 5 if we apply a universal quantifier on a. This
is what we call the KING measure:
KINGM,A(X) ?
P (?a ? A.QUEENM,X(m) > QUEENM,X(a))
KING is the probability that a model is better
than any peer in a test sample. In terms of a qual-
ity ranking, it is the probability that a model gets a
better ranking than all peers in a test sample. Note
that KING satisfies all restrictions because it uses
QUEEN as a quality estimation for summaries; if
QUEEN is substituted for a different quality mea-
sure, some of the properties might not hold any
longer.
Figure 3: Metrics quality representation
Figure 3 illustrates the behaviour of the KING
measure in boundary conditions. The left-
most figure represents a similarity metric which
mixes models and peers randomly. Therefore,
P (QUEEN(m) > QUEEN(a)) ? 0.5. As there
are seven automatic summaries, KING = P (?a ?
A,QUEEN(m) > QUEEN(a)) ? 0.57 ? 0
The rightmost figure represents a metric which
is able to group models and separate them from
peers. In this case, QUEEN(a) = 0 for all peers,
and then KING(x) = 1.
3.4 JACK:Reliability of the peers set
Once we detect a difference in quality between
two summarisation systems, the question is now
whether this result is reliable. Would we get the
same results using a different test set (different ex-
amples, different human summarisers (models) or
different baseline systems)?
The first step is obviously to apply statistical
significance tests to the results. But even if they
give a positive result, it might be insufficient. The
problem is that the estimation of the probabilities
in KING,QUEEN assumes that the sample sets
M,A are not biased. If M,A are biased, the re-
sults can be statistically significant and yet un-
reliable. The set of examples and the behaviour
of human summarisers (models) should be some-
how controlled either for homogeneity (if the in-
tended profile of examples and/or users is narrow)
or representativity (if it is wide). But how to know
whether the set of automatic summaries is repre-
sentative and therefore is not penalising certain au-
tomatic summarisation strategies?
Our goal is, therefore, to have some estimation
JACK(X,M,A) of the reliability of the test set to
compute reliable QUEEN,KING measures. We
can think of three reasonable criteria for this es-
timation:
1. All other things being equal, if the elements
of A are more heterogeneous, we are enhanc-
ing the representativeness of A (we have a
more diverse set of (independent) automatic
summarization strategies represented), and
therefore the reliability of the results should
be higher. Reversely, if all automatic sum-
marisers employ similar strategies, we may
end up with a biased set of peers.
2. All other things being equal, if the elements
of A are closer to the model summaries in M ,
the reliability of the results should be higher.
3. Adding items to A should not reduce its reli-
ability.
A possible formulation for JACK which satis-
fies that criteria is:
JACK(X,M,A) ? P (?a, a? ? A.QUEEN(a) >
0 ? QUEEN(a?) > 0 ? ?x ? X.x(a, a?) ? x(a,m))
i.e. the probability over all model summaries m
of finding a couple of automatic summaries a, a?
284
which are closer to each other than to m according
to all metrics.
This measure satisfies all three constraints: it
can be enlarged by increasing the similarity of the
peers to the models (the x(m,a) factor in the in-
equality) or decreasing the similarity between au-
tomatic summaries (the x(a, a?) factor in the in-
equality). Finally, adding elements to A can only
increase the chances of finding a pair of automatic
summaries satisfying the condition in JACK.
Figure 4: JACK values
Figure 4 illustrates how JACK works: in the
leftmost part of the figure, peers are grouped to-
gether and far from the models, giving a low JACK
value. In the rightmost part of the figure, peers are
distributed around the set of models, closely sur-
rounding them, receiving a high JACK value.
4 A Case of Study
In order to test the behaviour of our evaluation
framework, we have applied it to the ISCORPUS
described in (Amigo et al, 2004). The ISCOR-
PUS was built to study an Information Synthesis
task, where a (large) set of relevant documents has
to be studied to give a brief, well-organised answer
to a complex need for information. This corpus
comprises:
? Eight topics extracted from the CLEF Span-
ish Information Retrieval test set, slightly re-
worded to move from a document retrieval
task (find documents about hunger strikes
in...) into an Information Synthesis task
(make a report about major causes of hunger
strikes in...).
? One hundred relevant documents per topic
taken from the CLEF EFE 1994 Spanish
newswire collection.
? M : Manual extractive summaries for every
topic made by 9 different users, with a 50-
sentence upper limit (half the number of rel-
evant documents).
? A: 30 automatic reports for every topic made
with baseline strategies. The 10 reports with
highest sentence overlap with the manual
summaries were selected as a way to increase
the quality of the baseline set.
We have considered the following similarity
metrics:
ROUGESim: ROUGE is a standard measure
to evaluate summarisation systems based on
n-gram recall. We have used ROUGE-1
(only unigrams with lemmatization and stop
word removal), which gives good results with
standard summaries (Lin and Hovy, 2003a).
ROUGE can be turned into a similarity met-
ric ROUGESim simply by considering only
one model when computing its value.
SentencePrecision: Given a reference and a con-
trastive summary, the number of fragments of
the contrastive summary which are also in the
reference summary, in relation to the size of
the reference summary.
SentenceRecall: Given a reference and a con-
trastive summary, the number of fragments of
the reference summary which are also in the
contrastive summary, in relation to the size of
the contrastive summary.
DocSim: The number of documents used to select
fragments in both summaries, in relation to
the size of the contrastive summary.
VectModelSim: Derived from the Euclidean dis-
tance between vectors of relative word fre-
quencies representing both summaries.
NICOS (key concept overlap): Same as Vect-
ModelSim, but using key-concepts (manually
identified by the human summarisers after
producing the summary) instead of all non-
empty words.
285
TruncatedVectModeln: Same as VectModelSim,
but using only the n more frequent terms
in the reference summary. We have used
10 variants of this measure with n =
1, 8, 64, 512.
4.1 Quality of Similarity Metric Sets
Figure 5 shows the quality (KING values averaged
over the eight ISCORPUS topics) of every individ-
ual metric. The rightmost part of the figure also
shows the quality of two metric sets:
? The first one ({ROUGESim, VectModelSim,
TruncVectModel.1}) is the metric set that
maximises KING, using only similarity met-
rics that do not require manual annotation
(i.e. excluding NICOS) or can only be ap-
plied to extractive summaries (i.e. DocSim,
SentenceRecall and SentencePrecision).
? The second one ({ TruncVectModel.1, ROU-
GESim, DocSim, VectModelSim }) is the best
combination considering all metrics.
The best result of individual metrics is obtained
by ROUGESim (0.39). All other individual met-
rics give scores below 0.31. Both metric sets, on
the other, are better than ROUGESim alone, con-
firming that metric combination is feasible to im-
prove system evaluation. The quality of the best
metric set (0.47) is 21% better than ROUGESim.
4.2 Reliability of the test set
The 30 automatic summaries (baselines) per topic
were built with four different classes of strategies:
i) picking up the first sentence from assorted sub-
sets of documents, ii) picking up first and second
sentences from assorted documents, iii) picking
up first, second or third sentences from assorted
documents, and iv) picking up whole documents
with different algorithms to determine which are
the most representative documents.
Figure 6 shows the reliability (JACK) of every
subset, and the reliability of the whole set of au-
tomatic summaries, computed with the best met-
ric set. Note that the individual subsets are all
below 0.2, while the reliability of the full set of
peers goes up to 0.57. That means that the con-
dition in JACK is satisfied for more than half of
the models. This value would probably be higher
if state-of-the-art summarisation techniques were
represented in the set of peers.
5 Testing the predictive power of the
framework
The QARLA probabilistic framework is designed
to evaluate automatic summarisation systems and,
at the same time, similarity metrics conceived as
well to evaluate summarisation systems. There-
fore, testing the validity of the QARLA proposal
implies some kind of meta-meta-evaluation, some-
thing which seems difficult to design or even to
define.
It is relatively simple, however, to perform some
simple cross-checkings on the ISCORPUS data to
verify that the qualitative information described
above is reasonable. This is the test we have im-
plemented:
If we remove a model m from M , and pretend it
is the output of an automatic summariser, we can
evaluate the peers set A and the new peer m using
M ? = M\{m} as the new model set. If the evalu-
ation metric is good, the quality of the new peer m
should be superior to all other peers inA. What we
have to check, then, is whether the average quality
of a human summariser on all test cases (8 topics
in ISCORPUS) is superior to the average quality
of any automatic summariser. We have 9 human
subjects in the ISCORPUS test bed; therefore, we
can repeat this test nine times.
With this criterion, we can compare our quality
measure Q with state-of-the-art evaluation mea-
sures such as ROUGE variants. Table 1 shows
the results of applying this test on ROUGE-
1, ROUGE-2, ROUGE-3, ROUGE-4 (as state-
of-the-art references) and QUEEN(ROUGESim),
QUEEN(Best Metric Combination) as representa-
tives of the QARLA framework. Even if the test is
very limited by the number of topics, it confirms
the potential of the framework, with the highest
KING metric combination doubling the perfor-
mance of the best ROUGE measure (6/9 versus 3/9
correct detections).
286
Figure 5: Quality of similarity metrics
Figure 6: Reliability of ISCORPUS peer sets
Evaluation criterion human summarisers ranked first
ROUGE-1 3/9
ROUGE-2 2/9
ROUGE-3 1/9
ROUGE-4 1/9
QUEEN(ROUGESim) 4/9
QUEEN(Best Metric Combination) 6/9
Table 1: Results of the test of identifying the manual summariser
287
6 Related work and discussion
6.1 Application of similarity metrics to
evaluate summaries
Both in Text Summarisation and Machine Trans-
lation, the automatic evaluation of systems con-
sists of computing some similarity metric between
the system output and a human model summary.
Systems are then ranked in order of decreasing
similarity to the gold standard. When there are
more than one reference items, similarity is calcu-
lated over a pseudo-summary extracted from every
model. BLEU (Papineni et al, 2001) and ROUGE
(Lin and Hovy, 2003a) are the standard similar-
ity metrics used in Machine Translation and Text
Summarisation. Generating a pseudo-summary
from every model, the results of a evaluation met-
ric might depend on the scale properties of the
metric regarding different models; our QUEEN
measure, however, does not depend on scales.
Another problem of the direct application of a
single evaluation metric to rank systems is how to
combine different metrics. The only way to do
this is by designing an algebraic combination of
the individual metrics into a new combined met-
ric, i.e. by deciding the weight of each individual
metric beforehand. In our framework, however, it
is not necessary to prescribe how similarity met-
rics should be combined, not even to know which
ones are individually better indicators.
6.2 Meta-evaluation of similarity metrics
The question of how to know which similar-
ity metric is best to evaluate automatic sum-
maries/translations has been addressed by
? comparing the quality of automatic items
with the quality of manual references (Culy
and Riehemann, 2003; Lin and Hovy,
2003b). If the metric does not identify that
the manual references are better, then it is not
good enough for evaluation purposes.
? measuring the correlation between the values
given by different metrics (Coughlin, 2003).
? measuring the correlation between the rank-
ings generated by each metric and rank-
ings generated by human assessors. (Joseph
P. Turian and Melamed, 2003; Lin and Hovy,
2003a).
The methodology which is closest to our frame-
work is ORANGE (Lin, 2004), which evaluates a
similarity metric using the average ranks obtained
by reference items within a baseline set. As in
our framework, ORANGE performs an automatic
meta-evaluation, there is no need for human as-
sessments, and it does not depend on the scale
properties of the metric being evaluated (because
changes of scale preserve rankings). The OR-
ANGE approach is, indeed, closely related to the
original QARLA measure introduced in (Amigo et
al., 2004).
Our KING,QUEEN, JACK framework, how-
ever, has a number of advantages over ORANGE:
? It is able to combine different metrics, and
evaluate the quality of metric sets, without
any a-priori weighting of their relative impor-
tance.
? It is not sensitive to repeated (or very similar)
baseline elements.
? It provides a mechanism, JACK, to check
whether a set X,M,A of metrics, manual
and baseline items is reliable enough to pro-
duce a stable evaluation of automatic sum-
marisation systems.
Probably the most significant improvement over
ORANGE is the ability of KING,QUEEN, JACK
to combine automatically the information of dif-
ferent metrics. We believe that a comprehensive
automatic evaluation of a summary must neces-
sarily capture different aspects of the problem with
different metrics, and that the results of every indi-
vidual metric should not be combined in any pre-
scribed algebraic way (such as a linear weighted
combination). Our framework satisfies this con-
dition. An advantage of ORANGE, however, is
that it does not require a large number of gold stan-
dards to reach stability, as in the case of QARLA.
Finally, it is interesting to compare the rankings
produced by QARLA with the output of human
assessments, even if the philosophy of QARLA
is not considering human assessments as the gold
standard for evaluation. Our initial tests on DUC
288
Figure 7: KING vs Pearson correlation with manual rankings in DUC for 1024 metrics combinations
test beds are very promising, reaching Pearson
correlations of 0.9 and 0.95 between human as-
sessments and QUEEN values for DUC 2004 tasks
2 and 5 (Over and Yen, 2004), using metric sets
with highest KING values. The figure 7 shows
how Pearson correlation grows up with higher
KING values for 1024 metric combinations.
Acknowledgments
We are indebted to Ed Hovy, Donna Harman, Paul
Over, Hoa Dang and Chin-Yew Lin for their in-
spiring and generous feedback at different stages
in the development of QARLA. We are also in-
debted to NIST for hosting Enrique Amigo? as a
visitor and for providing the DUC test beds. This
work has been partially supported by the Spanish
government, project R2D2 (TIC-2003-7180).
References
E. Amigo, V. Peinado, J. Gonzalo, A. Pen?as, and
F. Verdejo. 2004. An empirical study of informa-
tion synthesis task. In Proceedings of the 42th An-
nual Meeting of the Association for Computational
Linguistics (ACL), Barcelona, July.
Deborah Coughlin. 2003. Correlating Automated and
Human Assessments of Machine Translation Qual-
ity. In In Proceedings of MT Summit IX, New Or-
leans,LA.
Christopher Culy and Susanne Riehemann. 2003. The
Limits of N-Gram Translation Evaluation Metrics.
In Proceedings of MT Summit IX, New Orleans,LA.
Luke Shen Joseph P. Turian and I. Dan Melamed.
2003. Evaluation of Machine Translation and its
Evaluation. In In Proceedings of MT Summit IX,
New Orleans,LA.
C. Lin and E. H. Hovy. 2003a. Automatic Evaluation
of Summaries Using N-gram Co-ocurrence Statis-
tics. In Proceeding of 2003 Language Technology
Conference (HLT-NAACL 2003).
Chin-Yew Lin and Eduard Hovy. 2003b. The Poten-
tial and Limitations of Automatic Sentence Extrac-
tion for Summarization. In Dragomir Radev and Si-
mone Teufel, editors, HLT-NAACL 2003 Workshop:
Text Summarization (DUC03), Edmonton, Alberta,
Canada, May 31 - June 1. Association for Computa-
tional Linguistics.
C. Lin. 2004. Orange: a Method for Evaluating Au-
tomatic Metrics for Machine Translation. In Pro-
ceedings of the 36th Annual Conference on Compu-
tational Linguisticsion for Computational Linguis-
tics (Coling?04), Geneva, August.
P. Over and J. Yen. 2004. An introduction to duc 2004
intrinsic evaluation of generic new text summariza-
tion systems. In Proceedings of DUC 2004 Docu-
ment Understanding Workshop, Boston.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311?318, Philadelphia, jul.
289
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 306?314,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
The Contribution of Linguistic Features to Automatic Machine
Translation Evaluation
Enrique Amigo?1 Jesu?s Gime?nez2 Julio Gonzalo 1 Felisa Verdejo1
1UNED, Madrid
{enrique,julio,felisa}@lsi.uned.es
2UPC, Barcelona
jgimenez@lsi.upc.edu
Abstract
A number of approaches to Automatic
MT Evaluation based on deep linguistic
knowledge have been suggested. How-
ever, n-gram based metrics are still to-
day the dominant approach. The main
reason is that the advantages of employ-
ing deeper linguistic information have not
been clarified yet. In this work, we pro-
pose a novel approach for meta-evaluation
of MT evaluation metrics, since correla-
tion cofficient against human judges do
not reveal details about the advantages and
disadvantages of particular metrics. We
then use this approach to investigate the
benefits of introducing linguistic features
into evaluation metrics. Overall, our ex-
periments show that (i) both lexical and
linguistic metrics present complementary
advantages and (ii) combining both kinds
of metrics yields the most robust meta-
evaluation performance.
1 Introduction
Automatic evaluation methods based on similarity
to human references have substantially accelerated
the development cycle of many NLP tasks, such
as Machine Translation, Automatic Summariza-
tion, Sentence Compression and Language Gen-
eration. These automatic evaluation metrics allow
developers to optimize their systems without the
need for expensive human assessments for each
of their possible system configurations. However,
estimating the system output quality according to
its similarity to human references is not a trivial
task. The main problem is that many NLP tasks
are open/subjective; therefore, different humans
may generate different outputs, all of them equally
valid. Thus, language variability is an issue.
In order to tackle language variability in the
context of Machine Translation, a considerable ef-
fort has also been made to include deeper linguis-
tic information in automatic evaluation metrics,
both syntactic and semantic (see Section 2 for de-
tails). However, the most commonly used metrics
are still based on n-gram matching. The reason is
that the advantages of employing higher linguistic
processing levels have not been clarified yet.
The main goal of our work is to analyze to what
extent deep linguistic features can contribute to the
automatic evaluation of translation quality. For
that purpose, we compare ? using four different
test beds ? the performance of 16 n-gram based
metrics, 48 linguistic metrics and one combined
metric from the state of the art.
Analyzing the reliability of evaluation met-
rics requires meta-evaluation criteria. In this re-
spect, we identify important drawbacks of the
standard meta-evaluation methods based on cor-
relation with human judgements. In order to
overcome these drawbacks, we then introduce six
novel meta-evaluation criteria which represent dif-
ferent metric reliability dimensions. Our analysis
indicates that: (i) both lexical and linguistic met-
rics have complementary advantages and different
drawbacks; (ii) combining both kinds of metrics
is a more effective and robust evaluation method
across all meta-evaluation criteria.
In addition, we also perform a qualitative analy-
sis of one hundred sentences that were incorrectly
evaluated by state-of-the-art metrics. The analysis
confirms that deep linguistic techniques are neces-
sary to avoid the most common types of error.
Section 2 examines the state of the art Section 3
describes the test beds and metrics considered in
our experiments. In Section 4 the correlation be-
tween human assessors and metrics is computed,
with a discussion of its drawbacks. In Section 5
different quality aspects of metrics are analysed.
Conclusions are drawn in the last section.
306
2 Previous Work on Machine
Translation Meta-Evaluation
Insofar as automatic evaluation metrics for ma-
chine translation have been proposed, different
meta-evaluation frameworks have been gradually
introduced. For instance, Papineni et al (2001)
introduced the BLEU metric and evaluated its re-
liability in terms of Pearson correlation with hu-
man assessments for adequacy and fluency judge-
ments. With the aim of overcoming some of the
deficiencies of BLEU, Doddington (2002) intro-
duced the NIST metric. Metric reliability was
also estimated in terms of correlation with human
assessments, but over different document sources
and for a varying number of references and seg-
ment sizes. Melamed et al (2003) argued, at the
time of introducing the GTM metric, that Pearson
correlation coefficients can be affected by scale
properties, and suggested, in order to avoid this
effect, to use the non-parametric Spearman corre-
lation coefficients instead.
Lin and Och (2004) experimented, unlike pre-
vious works, with a wide set of metrics, including
NIST, WER (Nie?en et al, 2000), PER (Tillmann
et al, 1997), and variants of ROUGE, BLEU and
GTM. They computed both Pearson and Spearman
correlation, obtaining similar results in both cases.
In a different work, Banerjee and Lavie (2005) ar-
gued that the measured reliability of metrics can
be due to averaging effects but might not be robust
across translations. In order to address this issue,
they computed the translation-by-translation cor-
relation with human judgements (i.e., correlation
at the segment level).
All that metrics were based on n-gram over-
lap. But there is also extensive research fo-
cused on including linguistic knowledge in met-
rics (Owczarzak et al, 2006; Reeder et al, 2001;
Liu and Gildea, 2005; Amigo? et al, 2006; Mehay
and Brew, 2007; Gime?nez and Ma`rquez, 2007;
Owczarzak et al, 2007; Popovic and Ney, 2007;
Gime?nez and Ma`rquez, 2008b) among others. In
all these cases, metrics were also evaluated by
means of correlation with human judgements.
In a different research line, several authors
have suggested approaching automatic evalua-
tion through the combination of individual metric
scores. Among the most relevant let us cite re-
search by Kulesza and Shieber (2004), Albrecht
and Hwa (2007). But finding optimal metric
combinations requires a meta-evaluation criterion.
Most approaches again rely on correlation with
human judgements. However, some of them mea-
sured the reliability of metric combinations in
terms of their ability to discriminate between hu-
man translations and automatic ones (human like-
ness) (Amigo? et al, 2005). .
In this work, we present a novel approach to
meta-evaluation which is distinguished by the use
of additional easily interpretable meta-evaluation
criteria oriented to measure different aspects of
metric reliability. We then apply this approach to
find out about the advantages and challenges of in-
cluding linguistic features in meta-evaluation cri-
teria.
3 Metrics and Test Beds
3.1 Metric Set
For our study, we have compiled a rich set of met-
ric variants at three linguistic levels: lexical, syn-
tactic, and semantic. In all cases, translation qual-
ity is measured by comparing automatic transla-
tions against a set of human references.
At the lexical level, we have included several
standard metrics, based on different similarity as-
sumptions: edit distance (WER, PER and TER),
lexical precision (BLEU and NIST), lexical recall
(ROUGE), and F-measure (GTM and METEOR). At
the syntactic level, we have used several families
of metrics based on dependency parsing (DP) and
constituency trees (CP). At the semantic level, we
have included three different families which op-
erate using named entities (NE), semantic roles
(SR), and discourse representations (DR). A de-
tailed description of these metrics can be found in
(Gime?nez and Ma`rquez, 2007).
Finally, we have also considered ULC, which
is a very simple approach to metric combina-
tion based on the unnormalized arithmetic mean
of metric scores, as described by Gime?nez and
Ma`rquez (2008a). ULC considers a subset of met-
rics which operate at several linguistic levels. This
approach has proven very effective in recent eval-
uation campaigns. Metric computation has been
carried out using the IQMT Framework for Auto-
matic MT Evaluation (Gime?nez, 2007)1. The sim-
plicity of this approach (with no training of the
metric weighting scheme) ensures that the poten-
tial advantages detected in our experiments are not
due to overfitting effects.
1http://www.lsi.upc.edu/?nlp/IQMT
307
2004 2005
AE CE AE CE
#references 5 5 5 4
#systemsassessed 5 10 5+1 5
#casesassessed 347 447 266 272
Table 1: NIST 2004/2005 MT Evaluation Cam-
paigns. Test bed description
3.2 Test Beds
We use the test beds from the 2004 and 2005
NIST MT Evaluation Campaigns (Le and Przy-
bocki, 2005)2. Both campaigns include two dif-
ferent translations exercises: Arabic-to-English
(?AE?) and Chinese-to-English (?CE?). Human as-
sessments of adequacy and fluency, on a 1-5 scale,
are available for a subset of sentences, each eval-
uated by two different human judges. A brief nu-
merical description of these test beds is available
in Table 1. The corpus AE05 includes, apart from
five automatic systems, one human-aided system
that is only used in our last experiment.
4 Correlation with Human Judgements
4.1 Correlation at the Segment vs. System
Levels
Let us first analyze the correlation with human
judgements for linguistic vs. n-gram based met-
rics. Figure 1 shows the correlation obtained by
each automatic evaluation metric at system level
(horizontal axis) versus segment level (vertical
axis) in our test beds. Linguistic metrics are rep-
resented by grey plots, and black plots represent
metrics based on n-gram overlap.
The most remarkable aspect is that there exists
a certain trade-off between correlation at segment
versus system level. In fact, this graph produces
a negative Pearson correlation coefficient between
system and segment levels of 0.44. In other words,
depending on how the correlation is computed,
the relative predictive power of metrics can swap.
Therefore, we need additional meta-evaluation cri-
teria in order to clarify the behavior of linguistic
metrics as compared to n-gram based metrics.
However, there are some exceptions. Some
metrics achieve high correlation at both levels.
The first one is ULC (the circle in the plot), which
combines both kind of metrics in a heuristic way
(see Section 3.1). The metric nearest to ULC is
2http://www.nist.gov/speech/tests/mt
Figure 1: Averaged Pearson correlation at system
vs. segment level over all test beds.
DP-Or-?, which computes lexical overlapping but
on dependency relationships. These results are a
first evidence of the advantages of combining met-
rics at several linguistic processing levels.
4.2 Drawbacks of Correlation-based
Meta-evaluation
Although correlation with human judgements is
considered the standard meta-evaluation criterion,
it presents serious drawbacks. With respect to
correlation at system level, the main problem is
that the relative performance of different metrics
changes almost randomly between testbeds. One
of the reasons is that the number of assessed sys-
tems per testbed is usually low, and then correla-
tion has a small number of samples to be estimated
with. Usually, the correlation at system level is
computed over no more than a few systems.
For instance, Table 2 shows the best 10 met-
rics in CE05 according to their correlation with
human judges at the system level, and then the
ranking they obtain in the AE05 testbed. There
are substantial swaps between both rankings. In-
deed, the Pearson correlation of both ranks is only
0.26. This result supports the intuition in (Baner-
jee and Lavie, 2005) that correlation at segment
level is necessary to ensure the reliability of met-
rics in different situations.
However, the correlation values of metrics at
segment level have also drawbacks related to their
interpretability. Most metrics achieve a Pearson
coefficient lower than 0.5. Figure 2 shows two
possible relationships between human and metric
308
Table 2: Metrics rankings according to correlation
with human judgements using CE05 vs. AE05
Figure 2: Human judgements and scores of two
hypothetical metrics with Pearson correlation 0.5
produced scores. Both hypothetical metrics A and
B would achieve a 0.5 correlation. In the case
of Metric A, a high score implies a high human
assessed quality, but not the reverse. This is the
tendency hypothesized by Culy and Riehemann
(2003). In the case of Metric B, the high scored
translations can achieve both low or high quality
according to human judges but low scores ensure
low quality. Therefore, the same Pearson coeffi-
cient may hide very different behaviours. In this
work, we tackle these drawbacks by defining more
specific meta-evaluation criteria.
5 Alternatives to Correlation-based
Meta-evaluation
We have seen that correlation with human judge-
ments has serious limitations for metric evalua-
tion. Therefore, we have focused on other aspects
of metric reliability that have revealed differences
between n-gram and linguistic based metrics:
1. Is the metric able to accurately reveal im-
provements between two systems?
2. Can we trust the metric when it says that a
translation is very good or very bad?
Figure 3: SIP versus SIR
3. Are metrics able to identify good translations
which are dissimilar from the models?
We now discuss each of these aspects sepa-
rately.
5.1 Ability of metrics to Reveal System
Improvements
We now investigate to what extent a significant
system improvement according to the metric im-
plies a significant improvement according to hu-
man assessors, and viceversa. In other words: are
the metrics able to detect any quality improve-
ment? Is a metric score improvement a strong ev-
idence of quality increase? Knowing that a metric
has a 0.8 Pearson correlation at the system level or
0.5 at the segment level does not provide a direct
answer to this question.
In order to tackle this issue, we compare met-
rics versus human assessments in terms of pre-
cision and recall over statistically significant im-
provements within all system pairs in the test
beds. First, Table 3 shows the amount of signif-
icant improvements over human judgements ac-
cording to the Wilcoxon statistical significant test
(? ? 0.025). For instance, the testbed CE2004
consists of 10 systems, i.e. 45 system pairs; from
these, in 40 cases (rightmost column) one of the
systems significantly improves the other.
Now we would like to know, for every metric, if
the pairs which are significantly different accord-
ing to human judges are also the pairs which are
significantly different according to the metric.
Based on these data, we define two meta-
metrics: Significant Improvement Precision (SIP)
and Significant Improvement Recall (SIR). SIP
309
Systems System pairs Sig. imp.
CE2004 10 45 40
AE2004 5 10 8
CE2005 5 10 4
AE2005 5 10 6
Total 25 75 58
Table 3: System pairs with a significant difference
according to human judgements (Wilcoxon test)
(precision) represents the reliability of improve-
ments detected by metrics. SIR (recall) represents
to what extent the metric is able to cover the sig-
nificant improvements detected by humans. Let
Ih be the set of significant improvements detected
by human assessors and Im the set detected by the
metric m. Then:
SIP =
|Ih ? Im|
|Im|
SIR =
|Ih ? Im|
|Ih|
Figure 3 shows the SIR and SIP values obtained
for each metric. Linguistic metrics achieve higher
precision values but at the cost of an important re-
call decrease. Given that linguistic metrics require
matching translation with references at additional
linguistic levels, the significant improvements de-
tected are more reliable (higher precision or SIP),
but at the cost of recall over real significant im-
provements (lower SIR).
This result supports the behaviour predicted in
(Gime?nez and Ma`rquez, 2009). Although linguis-
tic metrics were motivated by the idea of model-
ing linguistic variability, the practical effect is that
current linguistic metrics introduce additional re-
strictions (such as dependency tree overlap, for in-
stance) for accepting automatic translations. Then
they reward precision at the cost of recall in the
evaluation process, and this explains the high cor-
relation with human judgements at system level
with respect to segment level.
All n-gram based metrics achieve SIP and SIR
values between 0.8 and 0.9. This result suggests
that n-gram based metrics are reasonably reliable
for this purpose. Note that the combined met-
ric, ULC (the circle in the figure), achieves re-
sults comparable to n-gram based metrics with
this test3. That is, combining linguistic and n-
gram based metrics preserves the good behavior
of n-gram based metrics in this test.
3Notice that we just have 75 significant improvement
samples, so small differences in SIP or SIR have no relevance
5.2 Reliability of High and Low Metric
Scores
The issue tackled in this section is to what extent
a very low or high score according to the metric
is reliable for detecting extreme cases (very good
or very bad translations). In particular, note that
detecting wrong translations is crucial in order to
analyze the system drawbacks.
In order to define an accuracy measure for the
reliability of very low/high metric scores, it is nec-
essary to define quality thresholds for both the
human assessments and metric scales. Defining
thresholds for manual scores is immediate (e.g.,
lower than 4/10). However, each automatic evalu-
ation metric has its own scale properties. In order
to solve scaling problems we will focus on equiva-
lent rank positions: we associate the ith translation
according to the metric ranking with the quality
value manually assigned to the ith translation in
the manual ranking.
Being Qh(t) and Qm(t) the human and met-
ric assessed quality for the translation t, and being
rankh(t) and rankm(t) the rank of the translation
t according to humans and the metric, the normal-
ized metric assessed quality is:
QNm(t) = Qh(t
?)| (rankh(t
?) = rankm(t))
In order to analyze the reliability of metrics
when identifying wrong or high quality transla-
tions, we look for contradictory results between
the metric and the assessments. In other words,
we look for metric errors in which the quality es-
timated by the metric is low (QNm(t) ? 3) but the
quality assigned by assessors is high (Qh(t) ? 5)
or viceversa (QNm(t) ? 7 and Qh(t) ? 4).
The vertical axis in Figure 4 represents the ra-
tio of errors in the set of low scored translations
according to a given metric. The horizontal axis
represents the ratio of errors over the set of high
scored translations. The first observation is that
all metrics are less reliable when they assign low
scores (which corresponds with the situation A de-
scribed in Section 4.2). For instance, the best met-
ric erroneously assigns a low score in more than
20% of the cases. In general, the linguistic met-
rics do not improve the ability to capture wrong
translations (horizontal axis in the figure). How-
ever, again, the combining metric ULC achieves
the same reliability as the best n-gram based met-
ric.
310
In order to check the robustness of these results,
we computed the correlation of individual metric
failures between test beds, obtaining 0.67 Pearson
for the lowest correlated test bed pair (AE2004 and
CE2005) and 0.88 for the highest correlated pair
(AE2004 and CE2004).
Figure 4: Counter sample ratio for high vs low
metric scored translations
5.2.1 Analysis of Evaluation Samples
In order to shed some light on the reasons for the
automatic evaluation failures when assigning low
scores, we have manually analyzed cases in which
a metric score is low but the quality according to
humans is high (QNm ? 3 and Qh ? 7). We
have studied 100 sentence evaluation cases from
representatives of each metric family including: 1-
PER, BLEU, DP-Or-?, GTM (e = 2), METEOR
and ROUGEL. The evaluation cases have been ex-
tracted from the four test beds. We have identified
four main (non exclusive) failure causes:
Format issues, e.g. ?US ? vs ?United States?).
Elements such as abbreviations, acronyms or num-
bers which do not match the manual translation.
Pseudo-synonym terms, e.g. ?US Scheduled the
Release? vs. ?US set to Release?). ) In most of
these cases, synonymy can only be identified from
the discourse context. Therefore, terminological
resources (e.g., WordNet) are not enough to tackle
this problem.
Non relevant information omissions, e.g.
?Thank you? vs. ?Thank you very much? or
?dollar? vs. ?US dollar?)). The translation
system obviates some information which, in
context, is not considered crucial by the human
assessors. This effect is specially important in
short sentences.
Incorrect structures that change the meaning
while maintaining the same idea (e.g., ?Bush
Praises NASA ?s Mars Mission? vs ? Bush praises
nasa of Mars mission? ).
Note that all of these kinds of failure - except
formatting issues - require deep linguistic process-
ing while n-gram overlap or even synonyms ex-
tracted from a standard ontology are not enough to
deal with them. This conclusion motivates the in-
corporation of linguistic processing into automatic
evaluation metrics.
5.3 Ability to Deal with Translations that are
Dissimilar to References.
The results presented in Section 5.2 indicate that a
high score in metrics tends to be highly related to
truly good translations. This is due to the fact that
a high word overlapping with human references is
a reliable evidence of quality. However, in some
cases the translations to be evaluated are not so
similar to human references.
An example of this appears in the test bed
NIST05AE which includes a human-aided sys-
tem, LinearB (Callison-Burch, 2005). This system
produces correct translations whose words do not
necessarily overlap with references. On the other
hand, a statistics based system tends to produce
incorrect translations with a high level of lexical
overlapping with the set of human references. This
case was reported by Callison-Burch et al (2006)
and later studied by Gime?nez and Ma`rquez (2007).
They found out that lexical metrics fail to pro-
duce reliable evaluation scores. They favor sys-
tems which share the expected reference sublan-
guage (e.g., statistical) and penalize those which
do not (e.g., LinearB).
We can find in our test bed many instances in
which the statistical systems obtain a metric score
similar to the assisted system while achieving a
lower mark according to human assessors. For in-
stance, for the following translations, ROUGEL
assigns a slightly higher score to the output of a
statistical system which contains a lot of grammat-
ical and syntactical failures.
Human assisted system: The Chinese President made un-
precedented criticism of the leaders of Hong Kong after
political failings in the former British colony on Mon-
day . Human assessment=8.5.
Statistical system: Chinese President Hu Jintao today un-
precedented criticism to the leaders of Hong Kong
wake political and financial failure in the former
British colony. Human assessment=3.
311
Figure 5: Maximum translation quality decreasing
over similarly scored translation pairs.
In order to check the metric resistance to be
cheated by translations with high lexical over-
lapping, we estimate the quality decrease that
we could cause if we optimized the human-aided
translations according to the automatic metric. For
this, we consider in each translation case c, the
worse automatic translation t that equals or im-
proves the human-aided translation th according
to the automatic metric m. Formally the averaged
quality decrease is:
Quality decrease(m) =
Avgc(maxt(Qh(th)?Qh(t)|Qm(th) ? Qm(t)))
Figure 5 illustrates the results obtained. All
metrics are suitable to be cheated, assigning sim-
ilar or higher scores to worse translations. How-
ever, linguistic metrics are more resistant. In addi-
tion, the combined metric ULC obtains the best re-
sults, better than both linguistic and n-gram based
metrics. Our conclusion is that including higher
linguistic levels in metrics is relevant to prevent
ungrammatical n-gram matching to achieve simi-
lar scores than grammatical constructions.
5.4 The Oracle System Test
In order to obtain additional evidence about the
usefulness of combining evaluation metrics at dif-
ferent processing levels, let us consider the follow-
ing situation: given a set of reference translations
we want to train a combined system that takes
the most appropriate translation approach for each
text segment. We consider the set of translations
system presented in each competition as the trans-
lation approaches pool. Then, the upper bound on
the quality of the combined system is given by the
Metric OST
maxOST 6.72
ULC 5.79
ROUGEW 5.71
DP-Or-? 5.70
CP-Oc-? 5.70
NIST 5.70
randOST 5.20
minOST 3.67
Table 4: Metrics ranked according to the Oracle
System Test
predictive power of the employed automatic eval-
uation metric. This upper bound is obtained by se-
lecting the highest scored translation t according
to a specific metric m for each translation case c.
The Oracle System Test (OST) consists of com-
puting the averaged human assessed quality Qh
of the selected translations according to human as-
sessors across all cases. Formally:
OST(m) = Avgc(Qh(Argmaxt(Qm(t))|t ? c))
We use the sum of adequacy and fluency, both
in a 1-5 scale, as a global quality measure. Thus,
OST scores are in a 2-10 range. In summary,
the OST represents the best combined system that
could be trained according to a specific automatic
evaluation metric.
Table 4 shows OST values obtained for the best
metrics. In the table we have also included a ran-
dom, a maximum (always pick the best transla-
tion according to humans) and a minimum (al-
ways pick the worse translation according to hu-
man) OST for all 4. The most remarkable result
in Table 4 is that metrics are closer to the random
baseline than to the upperbound (maximum OST).
This result confirms the idea that an improvement
on metric reliability could contribute considerably
to the systems optimization process. However, the
key point is that the combined metric, ULC, im-
proves all the others (5.79 vs. 5.71), indicating
the importance of combining n-gram and linguis-
tic features.
6 Conclusions
Our experiments show that, on one hand, tradi-
tional n-gram based metrics are more or equally
4In all our experiments, the meta-metric values are com-
puted over each test bed independently before averaging in
order to assign equal relevance to the four possible contexts
(test beds)
312
reliable for estimating the translation quality at the
segment level, for predicting significant improve-
ment between systems and for detecting poor and
excellent translations.
On the other hand, linguistically motivated met-
rics improve n-gram metrics in two ways: (i) they
achieve higher correlation with human judgements
at system level and (ii) they are more resistant to
reward poor translations with high word overlap-
ping with references.
The underlying phenomenon is that, rather
than managing the linguistics variability, linguis-
tic based metrics introduce additional restrictions
for assigning high scores. This effect decreases
the recall over significant system improvements
achieved by n-gram based metrics and does not
solve the problem of detecting wrong translations.
Linguistic metrics, however, are more difficult to
cheat.
In general, the greatest pitfall of metrics is the
low reliability of low metric values. Our qualita-
tive analysis of evaluated sentences has shown that
deeper linguistic techniques are necessary to over-
come the important surface differences between
acceptable automatic translations and human ref-
erences.
But our key finding is that combining both kinds
of metrics gives top performance according to ev-
ery meta-evaluation criteria. In addition, our Com-
bined System Test shows that, when training a
combined translation system, using metrics at sev-
eral linguistic processing levels improves substan-
tially the use of individual metrics.
In summary, our results motivate: (i) work-
ing on new linguistic metrics for overcoming the
barrier of linguistic variability and (ii) perform-
ing new metric combining schemes based on lin-
ear regression over human judgements (Kulesza
and Shieber, 2004), training models over hu-
man/machine discrimination (Albrecht and Hwa,
2007) or non parametric methods based on refer-
ence to reference distances (Amigo? et al, 2005).
Acknowledgments
This work has been partially supported by the
Spanish Government, project INES/Text-Mess.
We are indebted to the three ACL anonymous re-
viewers which provided detailed suggestions to
improve our work.
References
Joshua Albrecht and Rebecca Hwa. 2007. Regression
for Sentence-Level MT Evaluation with Pseudo Ref-
erences. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 296?303.
Enrique Amigo?, Julio Gonzalo, Anselmo Pe nas, and
Felisa Verdejo. 2005. QARLA: a Framework for
the Evaluation of Automatic Summarization. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
280?289.
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and
Llu??s Ma`rquez. 2006. MT Evaluation: Human-
Like vs. Human Acceptable. In Proceedings of
the Joint 21st International Conference on Com-
putational Linguistics and the 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING-ACL), pages 17?24.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for MT and/or Summariza-
tion.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the Role of BLEU in
Machine Translation Research. In Proceedings of
11th Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL).
Chris Callison-Burch. 2005. Linear B system descrip-
tion for the 2005 NIST MT evaluation exercise. In
Proceedings of the NIST 2005 Machine Translation
Evaluation Workshop.
Christopher Culy and Susanne Z. Riehemann. 2003.
The Limits of N-gram Translation Evaluation Met-
rics. In Proceedings of MT-SUMMIT IX, pages 1?8.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd In-
ternational Conference on Human Language Tech-
nology, pages 138?145.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Linguis-
tic Features for Automatic Evaluation of Heteroge-
neous MT Systems. In Proceedings of the ACL
Workshop on Statistical Machine Translation, pages
256?264.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008a. Hetero-
geneous Automatic MT Evaluation Through Non-
Parametric Metric Combinations. In Proceedings of
the Third International Joint Conference on Natural
Language Processing (IJCNLP), pages 319?326.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008b. On the Ro-
bustness of Linguistic Features for Automatic MT
Evaluation. (Under submission).
313
Jesu?s Gime?nez and Llu??s Ma`rquez. 2009. On the Ro-
bustness of Syntactic and Semantic Features for Au-
tomatic MT Evaluation. In Proceedings of the 4th
Workshop on Statistical Machine Translation (EACL
2009).
Jesu?s Gime?nez. 2007. IQMT v 2.0. Technical Manual
(LSI-07-29-R). Technical report, TALP Research
Center. LSI Department. http://www.lsi.
upc.edu/?nlp/IQMT/IQMT.v2.1.pdf.
Alex Kulesza and Stuart M. Shieber. 2004. A learn-
ing approach to improving sentence-level MT evalu-
ation. In Proceedings of the 10th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation (TMI), pages 75?84.
Audrey Le and Mark Przybocki. 2005. NIST 2005 ma-
chine translation evaluation official results. In Offi-
cial release of automatic evaluation scores for all
submissions, August.
Chin-Yew Lin and Franz Josef Och. 2004. Automatic
Evaluation of Machine Translation Quality Using
Longest Common Subsequence and Skip-Bigram
Statics. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization,
pages 25?32.
Dennis Mehay and Chris Brew. 2007. BLEUATRE:
Flattening Syntactic Dependencies for MT Evalu-
ation. In Proceedings of the 11th Conference on
Theoretical and Methodological Issues in Machine
Translation (TMI).
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Transla-
tion. In Proceedings of the Joint Conference on Hu-
man Language Technology and the North American
Chapter of the Association for Computational Lin-
guistics (HLT-NAACL).
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and
Hermann Ney. 2000. An Evaluation Tool for Ma-
chine Translation: Fast Evaluation for MT Research.
In Proceedings of the 2nd International Conference
on Language Resources and Evaluation (LREC).
Karolina Owczarzak, Declan Groves, Josef Van Gen-
abith, and Andy Way. 2006. Contextual Bitext-
Derived Paraphrases in Automatic MT Evaluation.
In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas
(AMTA), pages 148?155.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007. Labelled Dependencies in Machine
Translation Evaluation. In Proceedings of the ACL
Workshop on Statistical Machine Translation, pages
104?111.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation, RC22176. Technical
report, IBM T.J. Watson Research Center.
Maja Popovic and Hermann Ney. 2007. Word Error
Rates: Decomposition over POS classes and Appli-
cations for Error Analysis. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, pages 48?55, Prague, Czech Republic, June.
Association for Computational Linguistics.
Florence Reeder, Keith Miller, Jennifer Doyon, and
John White. 2001. The Naming of Things and
the Confusion of Tongues: an MT Metric. In Pro-
ceedings of the Workshop on MT Evaluation ?Who
did what to whom?? at Machine Translation Summit
VIII, pages 55?59.
Christoph Tillmann, Stefan Vogel, Hermann Ney,
A. Zubiaga, and H. Sawaf. 1997. Accelerated DP
based Search for Statistical Translation. In Proceed-
ings of European Conference on Speech Communi-
cation and Technology.
314
Sense clusters for Information Retrieval: 
Evidence from Semcor and the EuroWordNet  InterLingual 
Index 
Julio Gonzalo, Irina Chugur and FeHsa Verdejo 
Departamento de Lenguajes y Sistemas Inform~iticos 
Universidad Nacional de Educacidn a Distancia (UNED) 
{julio, irina, felisa}@ieec .uned. es 
Abst rac t  
We examine three different types of sense 
clustering criteria with an Information 
Retrieval application in mind: methods 
based on the wordnet structure (such 
as generMization, cousins, sisters...); eo- 
occurrence of senses obtained from Sere- 
cot; and equivalent translations of senses 
in other languages via the EuroWordNet 
InterLingual Index (ILI). We conclude 
that a) different NLP  applications de- 
mand not only different sense granulari- 
ties but different (possibly overlapped) 
sense clusterings, b) co-occurrence of 
senses in Semcor provide strong evidence 
for Information Retrieval clusters, un- 
like methods based on wordnet structure 
and systematic polysemy, e) parallel pol- 
ysemy in three or more languages via 
the ILI, besides providing sense clusters 
for MT and CLIR, is strongly correlated 
with co-occurring senses in Semcor, and 
thus can be useful for Information Re- 
trieval as well. 
1 I n t roduct ion  
WordNet (Miller et al, 1990) and EuroWordNet 
(Vossen, 1998), as most large-coverage electronic 
dictionaries and semantic networks, are not de- 
signed for a specific Natural Language Process- 
ing (NLP) application. It is commonly assumed 
that sense distinctions in these lex~cal databases 
are too fine-grained for a majority of applications. 
In particular, we have used EuroWordNet in a 
Cross-Language Text Retrieval (CLTR) applica- 
tion (Verdejo et al, 2000) and a number of CLTR 
experiments (Gonzalo et al, 1999; Vossen et al, 
1999), confirming that it is crucial to apply certain 
sense clusters to Wordnet (WN) and EuroWord- 
Net (EWN)  to take real advantage of them in 
Information Retrieval applications. Potentially, 
a semantic network such as WN/EWN can help 
distinguishing different word senses for retrieval, 
enhancing precision, and identifying synonymic 
or conceptually related terms, enhancing recall. 
But not all sense distinctions in a lexical database 
are meaningful for Information Retrieval. For in- 
stance, the following sense distinctions are super- 
fluous in an information retrieval application, as 
the different senses do not lead to different opics 
or different kinds of documents: 
Behav iour  
1. Manner of acting or conducting oneselJ 
2. (psychology) the aggreaate of the responses or 
reaction or movements made by an organism in 
any situation 
3. Behavioural attributes 
Bet  
1. The act of gambling 
2. The money risked on a gamble 
Band 
8. Instrumentalists not including string players 
9. A group of musicians playing popular music 
for dancing 
Bother  
1. Smth. or someone who causes trouble, a source 
of unhappiness 
2. An angry disturbance 
But sense clustering have been generally as- 
sociated with identifying Systematic Polysemy 
rules, taking into account lexicographic arguments 
rather than potential applications. In (Peters et 
al., 1998; Peters and Peters, 2000) the EuroWord- 
Net structure, together with systematic polysemy, 
is used to group senses (sisters, auto-hyponymy, 
cousins, twins). This work is linked to the find- 
ing of systematic polysemy classes in (Buitelaar, 
1998; Buitelaar, 2000; Tomuro, 1998) and others. 
While identifying systematic polysemy might be 
a key issue for and adequate lexico-semantic spec- 
10 
ification, systematic relatedness does not always 
mean sense proximity. In particular, such rules 
do not necessarily predict a similar behavior of 
the clustered senses in an NLP application. For in- 
stance, the animal/food systematic polysemy does 
not lead to good sense clusters neither for Machine 
Translation between English and Sp~nlsh, nor for 
Information Retrieval . In Spanish it is common 
to give different names to an animal in a zoological 
sense or in a food sense. For instance, it is nec- 
essary to distinguish animal/food senses offish in 
order to translate into pez or pes~do, depending 
on the context. And for Information Retrieval, 
the animal sense will appear in documents about, 
say, zoology, while the food sense will appear in 
documents about cooking. Therefore, while the 
animal/food rule is useful for lexical representa- 
tion and prediction of sense extensions in English, 
it cannot be used to cluster senses in MT or IR. 
In (Vossen et al, 1999) we performed a concept- 
based IR experiment where using the ILI with 
clusters was slightly worse than using the ILI with- 
out the clusters. While clustering the EWN Inter- 
lingual Index records on the basis of systematic 
polysemy proved useful to provide better inter- 
languages connectivity in the EWN database, this 
result supports the idea that systematic polysemy, 
per se, is not an indication of potential IR clusters. 
However, we do not claim that all systematic 
polysemy patterns are useless for IR. It is prob- 
ably reasonable to classify different systematic 
polysemy rules according to whether they pro- 
duce I/t clusters or not. Some, already identi- 
fied, patterns of regular polysemy, such as con- 
tainer/quantity or music/dance (Peters and Pe- 
ters, 2000) yield adequate IR clusters. Other 
patterns, such as animal/food, plant/food, aui- 
real/skin, language/people tend to produce clus- 
ters that are not valid for IR. This classification 
of polysemy patterns is, to our opinion, strongly 
related with the black and white dot operators in- 
troduced in (Buitelaar, 1998). The black operator 
was reserved for polysemy patterns including sets 
of senses that may co-occur in the same word in- 
stance (e.g. book as written work or as physical 
object), and white operator is reserved for poly- 
semy patterns for senses that never co-occur in 
the same word instance (e.g. window as physi- 
cal object or as computer frame): Unfortunately, 
the distinction between black and white operators 
classes has not been applied yet -to our knowl- 
edge - to the set of polysemous classes defined in 
Buitelaar's thesis. 
But, in many cases, even useful polysemy 
rules fail to extract pairs of systematically re- 
lated senses in WN/BWN, because the hypernym 
branches that they pertain to do not obey none of 
the described systematic polysemy classes/types. 
Take the following example: 
sack: 
1. The act of terminating someone's employment 
TERMINATION, END, CONCLUSION 
2. a bag made of paper or plastic for holding 
customer purchases -.h BAG 
3. unwai.~ed loose-fitting dress hanging straight 
from the shoulders --~ DRESS, FROCK 
4. hanging bed of canvas or rope netting -4 BED 
5. a woman's full loose hip-length jacket -~ 
JACKET 
6. dry white wine from SW Europe ~ WHITE 
WINE 
7. quantity contained in a sack ~ CONTAINER- 
FUL 
8. pocket. --~ ENCLOSED SPACE 
sack 2 (bag of paper for customer's pur- 
chases) and sack 7 (quantity contained in a 
sack) are related by systematic polysemy as con- 
tainer/containerful. Similarly, sack 8 (pocket) 
should be related to some sense with the mean- 
ing of quantity. Nevertheless, ack 8, whose hy- 
pernym is "enclosed space", cannot be retained in 
the same way that the former pair of senses, in 
spite of identical semantic relationship. System- 
atic polysemy cannot predict, as well, a poten- 
tial IR duster with senses 3 and 5 (both meaning 
types of clothing and thus likely to appear in shn- 
ilar contexts). Senses 3 and 5 indicate, also, that 
clustering might also depend on the application 
domain: they can be clustered in a genetic search, 
but they should be distinguished if the search is 
performed in a clothing domain. 
It is interesting to note, finally, that different 
clustering criteria not only lead to different gran- 
ularities, but they can produce tangled clusters, 
as in  
Onion:  
1. Pungent bulb -+ VEGETABLE ~ FOOD 
2. Bulbuos plant having hollow leaves cultivated 
worldwide for its rounded edible bulb --~ ALLIA- 
CEOUS PLANT ~ PLANT 
3. Edible bulb of an onion plant ~ BULB 
PLANT ORGAN 
The plant/food rule successfully relates senses 
2 and 1, while for Information Retrieval the inter- 
esting cluster is for senses 2 and 3, (both botanical 
terms). 
11 
Our hypothesis i , therefore, that we cannot as- 
sume general clustering criteria; different NLP ap- 
plications require different clustering criteria that 
are difficult to reconcile in a single clustering ap- 
proach. Our work on clustering is centered on 
identifying sense-distinctions that could be rel- 
evant from an Information Retriev'~t and Cross 
Language Information Retrieval point of view. 
Next section describes a clustering strategy that 
adequates to the Information Retrieval criterion: 
cluster senses if they tend to co-occur in the same 
Semcor documents. 
In Section 3, we study a different clustering ceil- 
teflon, based on equivalent ranslations for two 
or  more senses in other wordnets fi'om the Eu- 
roWordNet database. This is a direct criterion 
to duster senses in Machine Translation or Cross- 
Language Text Retrieval. Then we measure the 
overlap between both criteria, to conclude that the 
EWN InterLingual Index is also a valuable source 
of evidence for Information Retrieval clusters. 
2 Cluster evidence from Semcor 
One of our goals within the EuroWordNet and 
ITEM projects was to provide sense clusterings 
for WordNet (and, in general,for the EuroWord- 
Net InterLingual Index, (Gonzalo et al, 1999) 
that leave only the sense distinctions in wordnets 
that indicate different (semantic) indexing units 
for Information Retrieval. Our first lexicographic 
examination of WordNet sense distinctions and 
ciustefings following criteria based on the wordnet 
hierarchy did not produce clear criteria to classify 
senses emi-automatically according to this ~ re- 
quirement. As we mentioned before, the clusters 
applied on the EWN InterLingual Index which re- 
lied solely on hierarchical information in Word- 
net, produced a slight decrease of retrieval per- 
formauce in an experiment using 1LI records as 
indexing units. 
Thus, we decided to stick to our only clear-cut 
criterion: cluster senses if they are likely to co- 
occur in the same document. The fact that the 
same sense combination occurs in several seman- 
tically tagged documents hould provide strong 
evidence for clustering. Fortunately, we had the 
Semcor corpus of semantically-tagged documents 
to start with. 
For example, the first two senses of "breath" 
co-occur in several Semcor documents: 
Breath  
1. (the air that is inhaled or exhaled in respira- 
tion) 
2. (the act of exhaling) 
This co-occurrence indicates that this sense dis- 
tinction ~ not help to discriminate different doc- 
ument contexts. While in this particular exam- 
ple there is a clear relation between senses (sense 
1 is involved in the action specified in sense 2), 
it seems extremely difficult to find general clus- 
tering techniques based on Word.Net hierarchy to 
capture all potential IR clusters. 
We have scanned Semcor in search of sets of 
(two or more) senses that co-occur frequently 
enough. In practice, we started with a thresh- 
old of at least 2 documents (out of 171) with 
the co-occurring senses in a similar distribution. 
We did not use the original Semcor files, but the 
IR-Semcor partition (Gonzalo et al, 1998) that 
splits multi-text documents into coherent retrieval 
chunks. We completed this list of candidates to 
cluster with pairs of senses that only co-occur once 
but belong to any "cousin" combination (Peters et 
al., 1998). Finally, we obtained 507 sets of sense 
combinations (producing above 650 sense pairs) 
for which Semcor provides positive evidence for 
clustering. A manual verification of 50 of such 
clusters howed that above 70% of them were use- 
ful. We also noticed that raising the threshold 
(the number of documents in which the senses co- 
occur), the error rate decreases quickly. 
Then we worked with this set of positive IR clus- 
ters, trying to identify a set of common features 
that could be used to cluster the rest of WN/EWN 
senses. However, it seems extremely difficult to 
find any single criterion, common to all clusters. 
For instance, if we consider a) number of vari- 
ants in common between the synsets correspond- 
ing to the candidate senses; b) number of words 
in common between the glosses; and c) common 
hypernyms, we find that any combination of val- 
ues for these three features is likely to be found 
among the set of clusters inferred from Semcor. 
For example: 
fact 
I. a piece of information about circurastances 
that ezist or events that have occurred; "first you 
must collect all the facts of the case" 
~. a statement or assertion of verified infor- 
mation about something that is the case or has 
happened; "he supported his argument with an 
impressive array of facts" 
Number of documents in which they co-occur: 13 
a) number of variants in common: 1 out of 1 
b) (content) words in common between flosses: 
yes 
c) common hypernyms: no 
12 
door  
1. door -(a swinging or sliding barrier that will 
close the entrance to a room or building; "he 
knocked on the door"; "he slammed the door as 
he left") 2. doorway, door, entree, entry, portal, 
room access'- (the space in a wall through which 
you enter or leave a room or building; the space 
that a door can close; "he stuck his head in the 
doorway") Number of documents in which they 
co-occur: 11 
a} number of variants in common: I out of 6 
b} (content} words in common between glosses: 
yes (also XPOS: enter/entrance} 
c} common hypernyras: yes 
way 
1. manner, mode, style, way, fashion - (a 
manner of performance; "a manner of living"; 
"a way of life") 2. means, way - (how a result 
is obtained or an end is achieved; "a means 
of communication"; "the true way to success") 
Number of documents in which they co-occur: 9 
a) number of variants in common: I out of 6 
b) (content) words in common between glosses: 
no 
c) common hypernyms: no 
The next logical step is to use this positive ev- 
idence, combined with negative co-occurrence ev- 
idence, in training some machine learning system 
that can successfully capture the regularities hid- 
den to our manual inspection. In principle, a bi- 
nary classification task would be easy to capture 
by decision trees or similar techniques. 
Therefore, we have also extracted from Sea-  
cot combinations of senses that appear frequently 
enough in Semcor, but never appear together in 
the same document. The threshold was set in, 
at least, 8 occurrences of each sense in Semcor, 
resulting in more than 500 negative clusters. A 
manual verification of 50 of these negative clusters 
showed that about 80% of them were acceptable 
for Information Retrieval as senses that should 
be distinguished. Together with the positive evi- 
dence, we have more than 1100 training cases for 
a binary classifier. Our plan is to apply this clas- 
sifter to the whole EWN InterLingual Index, and 
then perform precision/recall tests in the environ- 
ment described in (Gonzalo et al, 1998; Gonzalo 
et al, 1999). 
3 C lus ter  ev idence  f rom the  I L I  
When translated into a target language, sense dis- 
tinctions of a word may be lexicalized. For in- 
stance, the English term spring is translated into 
Spanish as primavera in its "season" sense, into 
muelle in its "metal device" sense, or as flaente 
in its "fountain" sense. For an English-Spanish 
Machine Translation system, it is crucial to dis- 
tinguish these three senses of spring. But it is 
also frequent hat two or more senses of a word 
are translated into the same word, for one or more 
languages. For instance, child as "human offspring 
(son or daughter) of any age" and child as "young 
male person" are both translated into "nifio" in 
Spanish, into "enfant" in French, and into "kind" 
in German. We will use the term "parallel poly- 
semy" to refer to this situation in the rest of this 
article. 
Obviously, a Machine Translation system does 
not need to distinguish these two senses. But it is 
also tempting to hypothesize that the existence of 
parallel polysemy in two or more target languages 
may indicate that the two senses are close enough 
to be clustered in more applications. Indeed, in 
(Resnik and Yarowsky, 1999) this criterion is pro- 
posed to determine which word senses hould be 
retained or discarded in a testbed for automatic 
Word Sense Disambiguation systems. 
In particular, our goal has been to test whether 
two or more senses of a word are likely to be clus- 
tered, for IR purposes, if they have parallel pol- 
ysemy in a certain number of languages via the 
EuroWorclNet InterLingual Indez. If the answer 
is positive, then the InterLingual Index, with eight 
languages interconnected, would be a rich source 
of information to provide IR clusters. In EWN, 
each monolingual database is linked, via Cross- 
Language quivalence relations, to the InterLin- 
gual Index (ILI) which is the superset of all con- 
cepts occurring in all languages. The ILI permits 
finding equivalent synsets between any pair of lan- 
guages included in the database. For instance, 
senses 1 and 2 of child are translated into Span- 
ish, French and German as follows: 
Chi ld 
child 1 -r  {child, kid} - (a human offspring (son 
or daughter) of any age; "they had three children"; 
"they were able to send their kids to college") 
child 2 --~ {male child, boy, child} - (a young 
male person; "the baby was a boy"; "she made the 
boy brush his teeth every night") 
Spanish: 
{child, kid} EQ-SYNONYM {ni~o, cr~o, menor} 
13 
{male child, boy, child} EQ-SYNON'YM {nino} 
lwencch: 
{child, kid} EQ-SYNONYM{ en.fant, mineur} 
{male child, boy, child} EQ-SYNONYM{en\]ant) 
German: 
{child, kid} EQ-SYNONYM {kind} 
{mate child, boy, child} EQ-SYNOrCYM {kind, 
spross} 
Note that child I and child ~ have parallel trans- 
lations in all three languages: Sp~mish (nifio), 
French (enfant) and German (kind). In this case, 
this criterion successfully detects a \])air of senses 
that could be clustered for Information Retrieval 
purposes. 
In order to test the general validity of this cri- 
terion, we have followed these steps: 
Select a set of nouns for a full manual study. 
We have chosen the set of 22 nouns used 
in the first SENSEVAL competition (Kilgar- 
rift and Palmer, 2000). This set satisfied 
our requirements of size (small enough for an 
exhaustive manual revision), reasonable de- 
gree of polysemy, and unbiased for our test- 
ing purposes (the criteria to select these 22 
nouns was obviously independent of our ex- 
periment). We had to reduce the original 
set to 20 nouns (corresponding to 73 EWN 
senses), as the other two nouns were polyse- 
mous in the Hector database used for SEN- 
SEVAL, but monosemous in WordNet 1.5 and 
EuroWordNet. As target languages we chose 
Spanish, French, Dutch and German. 
Extract the candidate senses that satisfy the 
parallel polysemy criterion, in three variants: 
- Experiment 1: sets of senses that have 
parallel translations in at least two out 
of the four target languages. 
- Experiment 2: sets of senses that have 
parallel translations in at least one out 
of the four target languages. This is a 
softer constraint that produces a super- 
set of the sense clusters Obtained in Ex- 
periment 1. 
-Experhnent 3: sets of senses whose 
synsets are mapped into the same target 
synset for at least one of the target lan- 
guages. This criterion cannot be tested 
on plain multilingual dictionaries, only 
on EWN-like semantic databases. 
? Check out manually whether the dusters pro- 
duced in Experiments 1-3 are valid for Infor- 
mation Retrieval. At this step, the validity of 
clusters was checked by a human judge. Un- 
fortunately, we did not have the chance yet to 
attest the validity of these judgments using 
more judges and extracting inter-annotator 
agreement rates. We could compare anno- 
tations only on a small ~action of cases (15 
sense pairs), which we use to make the crite- 
rion "valid for Itt" precise nough for reliable 
annotation. The results are reported in sec- 
tions 3.2-3.4 for the different experiments. 
? Identify all possible lexicographic reasons be- 
hind a parallel polysemy, taking advantage of
the previous study. This is reported in the 
next section. 
? Check how many clusters obtained from Sem- 
cor also satisfy the parallel translation crite- 
rion, to have an idea of the overlap between 
both (section 3.5). 
? Finally, study whether the results have a de- 
pendency on possible incompleteness or in- 
adequacy of the InterLingual I.udex (section 
3.6). 
3.1 Typo logy  of  para l le l  po lysemy 
Parallel polysemy can also be a sign of some sys- 
tematic relation between the senses. As it is said 
in (Seto, 1996), ~(..) There often is a one-to- 
one correspondence b tween different languages in 
their lexiealization behaviour towards metonyrny, 
in other words, metonymically related word senses 
are often translated by the same word in other lan- 
guages". 
But the reasons for parallel polysemy are not 
limited only to systematic polysemy. In the case 
of the EWN database, we have distinguished the 
following causes: 
1. There is a series of mechanisms of meaning 
extension, if not universal, at least, common 
to several languages: 
(a) General lzat ion/special l  =ation For 
example, the following two senses for 
band: 
English: band; French: groupe; Ger- 
man: Band, Mnsicgruppe 
1. Instrumentalists not including string 
players 
2. A group of musicians playing popular 
music for dancing 
14 
(b) 
(c) 
(d) 
Sense 1 is a specialization ofSense 2, and 
this pattern is repeated in French and 
German. 
Metonymic  relat ions.  Some of them 
form already will known systematic pol- 
ysemy patterns. As for applicability to 
IR, we should be capable to discrimi- 
nate regular polysemy rules that provide 
valid IR clusters from those that contain 
senses that can not be interpreted simul- 
taneously within a same document. Ex- 
amples include: 
English: glass; Spanish: vaso 
1. container 
2. quantity 
which is a valid IR cluster, and 
English: rabbit; 
l~rench: lapin 
1.mammal 
2.meat 
Spanish: conejo; 
which should be distinguished for IR. 
Metaphors .  This kind of semantic re- 
lation usually does not produce good IR 
clusters, because senses related by means 
of metaphor usually belong to different 
semantic fields and, consequently, tend 
to occur in distinct documents. For ex- 
ample: 
English: giant; Spanish: coloso; French: 
colosse; Dutch: kolossus 
1.a person of exceptional importance and 
reputation 
2.someone who is abnormally large 
Semant ic  caique or loan transla- 
tion. A (probably metaphorical) sense 
extension is copied in other languages. 
It also can produce undesirable clusters 
for Ilt, because the original relation be- 
tween two senses involved can be based 
on a metaphor. For example: 
English: window; Spanish: ventana; 
Dutch: venster. 
1.an opening in the wall of a building to 
admit light and air 
2.a rectangular pert of a computer screen 
that is a display different of the rest of 
the screen 
The original computer sense for window 
is also adopted in Spanish and German 
2. 
for the corresponding words ventana nd 
venster. 
In certain occasions, the particularities of 
how the wordnets have been built semi- 
automatically ead to a mimesis of the WN1.5 
senses and, consequently, to parallel poly- 
semy in several anguages. These sense dis- 
tinctious are not incorrect, but perhaps would 
be different if every monolingual wordnet had 
been constructed without WN 1.5 as a refer- 
ence for semi-automatic extraction of seman- 
tic relations. An example: 
Behaviottr: 
1. Manner of acting or conducting oneself 
(Spanish: compertamiento, conducta; 
French: comportement, conduite) 
2. (psychology) the aggregate of the responses 
or reaction or movements made by an organ- 
ism in any situation 
(Spanish: comportamiento, conducta; 
French: comportement) 
3. Beehavioural ttributes 
(Spanish: comportamiento, conducta; 
French: comportement) 
The question is what classes of parallel poly- 
semy are dominant in EWN, and then whether 
parallel polysemy can be taken as a strong indi- 
cation of a potential IR cluster. A preliminary 
answer to this question is reported in the next 
sections. 
3.2 Exper iment 1 
Here we selected all sense combinations, in our 20 
English nouns test set, that had parallel transla- 
tions in at least two of the four target languages 
considered (Spanish, French, Dutch and German). 
We found 10 clusters: 6 were appropriate for In- 
formation Retrieval, 3 were judged inappropriate, 
and one was due to an error in the database: 
Val id I t t  c lusters 
Band 1,2: something elongated, worn around 
the body or one of the limbs / a strip or stripe of 
a contrasting color or material (mapped into two 
different syusets in Spani.~h and French) 
band 2,5: a strip or stripe of a contrasting 
color or material/a stripe of a contrasting color 
(mapped into different syusets in Spanish and 
French; only one translation into Dutch.) 
band 8,9: instrumentalists not including string 
players / a group of musicians playing popular 
15 
music for dancing (linked to the s~mae synset in 
German and in Dutch) 
behaviour 1,2,3: manner of acting or con- 
ducting oneself /(psychology) the aggregate of 
the responses or reaction or movements made 
by an organism in any situation / bchavioural 
attributes (two senses are sisters, and in general 
the distinction is not easy to understand; in two 
cases the Dutch synset is the same, and there is 
no Dutch translation for the other. In Sp~nigh 
there are three synsets that mimic the English 
ones). 
Bet 1,~: act of gambling/money risked 
(metonymy relation, translated iaato different 
synsets in Spanish and French. One or both 
translations missing for the other languages) 
ezcess 3,4: surplusage / overabundance (differ- 
ent synsets in Spanish and French, one or both 
translations missing in the other languages). 
inappropriate c lusters  
giant 5,6: a person off exceptional importance 
/ someone who is abnormally large (metaphoric 
relation; linked to the same syuset in Dutch, and 
to different synsets in Spanish and French) 
giant 5,7: a person of ezceptional importance / 
a very large person (metaphoric relation; linked 
to different synsets in Dutch and German) 
rabbit 1,2: mammal / meat (systematic poly- 
semy; linked to different syusets in Spanish, Ger- 
man and French). 
E r roneous  cluster 
steer 1,2: castrated bull/ hint, indication off 
potential opportunity. Both are translated into 
"buey" in Spanish and into "stierkalf ~in Dutch. 
Only the "castrated bull" --~ "buey" link is appro- 
priate. 
3.3 Exper iment  2 
If we take all clusters that have a parallel transla- 
tion in at least one target language (rather than 
two target languages as in Experiment 1), we ob- 
tain a larger subset of 27 clusters. The 17 new 
clusters have the following distribution: 
? 9 valid clusters, such as bother 1,2 (something 
that causes trouble / angry disturbance). 
? 3 inappropriate clusters that relate 
homonyms, such as band 2,7 (strip or stripe 
of a contrasting color or material/unofHcial 
association of people). 
? 4 inappropriate clusters that group 
metonymieally related senses, such as 
sanction 2,3 (penalty/authorization). 
? I inappropriate cluster based on a metaphor: 
steering 2,3 (act of steering and holding the 
course/guiding, uidance) 
On the overall, we have 15 valid clusters, 11 
inappropriate, and one error. The percentage of 
useful predictions i  56%, only slightly worse than 
for the tighter constraint of experiment 1. It is 
worth noticing that: 
1. The parallel translation criterion obtained 27 
potential clusters for 20 nouns, nearly one 
and a half cluster per noun. The criterion 
is very productive! 
2. The percentage of incorrect clusters (41%) 
is high enough to suggest hat parallel poly- 
semy cannot be taken as a golden rule to clus- 
ter close senses, at least with the languages 
studied. Even 3 of the negative cases were 
homonyms, totally unrelated senses. Perhaps 
the general WSD clustering criterion pro- 
posed in (Resnik and Yarowsky, 1999) needs 
to be revised for a specific application such 
as IR. For instance, they argue that dusters 
based on parallel polysemy "would eliminate 
many distinctions that are arguably better 
treated as regular polysemy'. But we have 
seen that regular polysemy may lead to sense 
distinctions that are important o keep in an 
Information Retrieval application. On the 
other hand, the results reported in (Resnik 
and Yarowsky, 1999) suggest hat we would 
obtain better clusters if the parallel polysemy 
criteria is tested on more distant languages, 
such as Japanese or Basque to test English 
sense distinctions. 
3.4 Exper iment  3 
In this experiment, which cannot be done with a 
multilingual dictionary, we looked for sense dis- 
tinctions that are translated into the same synset 
for some target language. This is a direct evidence 
of sense relatedness (both senses point to the same 
concept in the target language), although the rela- 
tion may be complex (for instance, one of the two 
senses might be translated as an EQ-HYPONYM). 
We found 9 clusters atisfying the criterion, all 
of them for linlcq to the Dutch wordnet. 5 sense 
combinations are valid IR clusters. Three com- 
binations turned out to be inappropriate for the 
16 
# words = 20 
# senses = 73 
Exp. 1 
Exp. 2 
Exp. 3 
It1 clusters not IR clusters incorrect Total 
6 (60%) 3 (30%) 1 10 
15 (56%) 11 (41%) 1 27 
5 (56%) 3 (33%) 1 9 
Table 1: Adequacy of clusters based on parallel polysemy for Information Retrieval 
needs of 1R (accident 1,2: chance / misfortune; 
steering 2,3: the act of steering and holding the 
course / guiding, guidance; giant 5,6: a person 
of exceptional importance / someone who is ab- 
normally large). Finally, the erroneous cluster for 
steerl (castrated bull) and steer2 (hint, an indica- 
tion of potential opportunity) reappeared again. 
The results for the three experiments are sum- 
mazized in Table 1. It seems that the parallel 
polysemy criteria on the ILI can be a very rich 
source of information to cluster senses for IR, but 
it is as well obvious that it needs to be refined or 
manually revised to obtain high quality clusters. 
3.5 Overlapping of  criteria f rom Semcor  
to ILI 
To complete evidence for correlation between 
Semcor-based clusters and ILI-based clusters, we 
studied two subsets of Semcor-based clusters to 
check if they matched the parallel polysemy crite- 
ria on the ILI. The first set were the 11 sense com- 
binatious with a co-occurrence frequency above 7 
in Semcor. 10 out of 11 (91%) also hold the most 
restrictive criterion used in Experiment 1, again 
indicating a strong correlation between both cri- 
teria. Then we augmented the set of sense com- 
binations to 50 - with co-occurrence frequencies 
above 2-. This time, 27 clusters matched the cri- 
terion in Experiment 2 (54%). As the evidence 
for Semcor clustering decreases, the criterion of 
parallel translations i also less reliable, again in- 
dicating a correlation between both. 
3.6 Adequacy  of the IL I  to get  
tr-n~lat ion clusters 
Clustering methods based on the criterion of par- 
allel translation depend, to a great extent, on 
the adequacy and quality of the lexical resources 
used. How many ILI clusters had we obtained in 
an EWN database with total coverage and com- 
pletely error-free? 
Our experiments, though limited, are a first in- 
dication of the utility of EWN for this task: 
? Analyzing 73 WN senses corresponding to 20 
nouns used in the SENSEVAL, we found 2 er- 
roneons equivalence links in the Spanish and 
Dutch wordnets. Taking into account hat 
EWN was built by semi-automatic means, 
this seems a low error rate. 
Only 16 senses out of 73 have equivalence 
links in the 4 selected wordnets. 19 senses 
have equivalence \]ink,q in 3 languages, 21 
senses in 2 languages, 9 in only one language 
and 6 have no equivalence links in any of the 
selected worduets. The lack of equivalence 
links sometimes can be explained by the lack 
of lexicalized terms for a certain WN concept. 
For example, float2 (a drink with ice-cream 
floating in it) is not lexicalized in Spanish, so 
we should not expect an equivalence link for 
this sense in the Spanish wordnet. In many 
other cases though, the lack of the equiva- 
lence links is due to incompleteness in the 
database. 
Each monolingual wordnet reflects, to a 
large extent, the kind of Machine-Readable 
resources used to build it. The Span- 
ish wordnet was built mainly from bilin- 
gual dictionaries and therefore is closer to 
the Wn 1.5 structure. The French word- 
net departed from an ontology-like database, 
and thus some non-lexicaliT.ed expressions 
are still present (for instance, float ~ has 
soda_avec_un_boule_de_glace as French equiv- 
alent). The Dutch wordnet departed from a 
lexical database rich in semantic information, 
thus it departs more from the Wordnet struc- 
ture, has a richer connectivity and complex 
links into the InterLingual Index, etc. Cross- 
Language quivalent relations are not, there- 
fore, totally homogeneous in EWN. 
On the overall, however, the ILI seems per- 
fectly suitable for automatic applications regard- 
ing multilingual sense mappings. In particular, 
the fine-grainedness of Wordnet and EuroWord- 
Net, in spite of its lack of popularity among NLP 
researchers, may be an advantage for NLP appli- 
cations, as it may suit different clusterings for dif- 
ferent application requirements. 
17 
4 Conc lus ions  
We examined three different ypes of sense clus- 
tering criteria with an Information Retrieval ap- 
plication in mind: methods based on the word- 
net structure (such as generalization, cousins, sis- 
ters...); co-occurrence of senses obtained from 
Semcor; and equivalent translations of senses in 
other languages via the EuroWordNet InterLin- 
gual Index (ILI). We conclude that a) different 
NLP  applications demand not only different sense 
granularities but different (possibly overlapped) 
sense elusterings, b) co-occurrence of senses in 
Semcor provide strong evidence for Information 
Retrieval clusters, unlike methods based on word- 
net structure and systematic polysemy, c) parallel 
polysemy in two or more languages via the ILI, be- 
sides providing sense clusters for MT and CLIR, is 
correlated with coocurring senses in Semcor, and 
thus can be useful to obtain IR dusters as well. 
Both approaches to IR clusters fbr WN/EWN 
(evidence from Semcor and from the ILl) seem 
very promising. The positive and negative evi- 
dence from SeIncor (above 500 clusters each) can 
possibly be used in a Machine Learning approach 
to find additional dusters for the rem~inlng sense 
distinctions without enough evidence from Sere- 
cot. The parallel polysemy criteria, over EWN,  is 
highly productive (more than one candidate per 
word in our experiments), although a more diverse 
set of languages would probably produce a higher 
rate of valid clusters. 
References 
P. Bnitelaar. 1998. CoreLex: systematic polysemy 
and underspecification. Ph.D. thesis, Depart- 
ment of Computer Science, Brandeis University, 
Boston. 
P. Buitelaar. 2000. Reducing lexical semantic 
complexity with systematic polysemous classes 
and underspecillcation. In Proceedings off 
ANLP'2000. 
J. Gonzalo, M. F. Verdejo, I. Chugur, and 
J. CigarrAn. 1.998. Indexing with Wordnet 
syusets can improve text retrievzd. In Proceed- 
ings off the COLING/ACL Workshop on Us- 
age off WordNet in Natural Language Processing 
? Systems. 
J. Gonzalo, A. Pefias, and F. Verdejo. 1999. Lex- 
ical ambiguity and information retrieval revis- 
ited. In Proceedings off EMNLP/VLC'99 Con- 
ff e lq~ nce. 
A. Kilgarriff and M. Palmer. 2000. Special issue 
on senseval. Computers and the Humanities, 
34(1-2). 
G. Miller, C. Beckwith, D. Fellbaum, D. Gross, 
and K. Miller. 1990. Five papers on Wordnet, 
CSL report 43. Technical report, Cognitive Sci- 
ence Laboratory, Princeton University. 
W. Peters and I. Peters. 2000. Automatic sense 
clustering in EuroWordnet. In Proceedings off 
LREC'2000. 
W. Peters, I. Peters, and P. Vossen. 1998. Lex- 
icalized systematic polysemy in EuroWordNet. 
In Proceedings of the First International Con- 
fference on Language Resources and Evaluation. 
P. Resnik and D. Yarowsky. 1999. Distinguishing 
systems and distinguishing senses: New evalu- 
ation methods for word sense disambiguation. 
Natural Language Engineering. 
K. Seto. 1996. On the cognitive triangle: the re- 
lation of metaphor, metonymy and synecdoque. 
In A. Burkhardt and N. Norrich, editors, Tropic 
TrutK De Gruyter. 
N. Tomuro. 1998. Semi-automatic induction of 
systematic polysemy from wordnet. In Proceed- 
ings off COLING/A CL'98 workshop on the use 
of wordnet in NLP applications. 
F. Verdejo, J. Gonzalo, A. Pefias, F. L6pez, and 
D. Fermlndez. 2000. Evaluating wordnets in 
cross-language text retrieval: the item multilin- 
gual search engine. In Proceedings LREC'2000. 
P. Vossen, W. Peters, and J. Gonzalo. 1999. To- 
wards a universal index of meaning. In Proceed- 
ings off SIGLEX'g9. 
P. Vossen. 1998. Euro WordNet: a multilin- 
9ual database with lexical semantic networks. 
Kluwer Academic Publishers. 
18 
Polysemy and Sense Proximity in the Senseval-2 Test Suite.
Irina Chugur
irina@lsi.uned.es
Julio Gonzalo
julio@lsi.uned.es
Departamento de Lenguajes y Sistemas Informa?ticos
Universidad Nacional de Educacio?n a Distancia (UNED)
Madrid, Spain
Felisa Verdejo
felisa@lsi.uned.es
Abstract
We report on an empirical study of sense
relations in the Senseval-2 test suite. We
apply and extend the method described
in (Resnik and Yarowsky, 1999), estimat-
ing proximity of sense pairs from the evi-
dence collected from native-speaker trans-
lations of 508 contexts across 4 Indoeu-
ropean languages representing 3 language
families. A control set composed of 65
contexts has also been annotated in 12
languages (including 2 non-Indoeuropean
languages) in order to estimate the corre-
lation between parallel polysemy and lan-
guage family distance. A new parame-
ter, sense stability, is introduced to assess
the homogeneity of each individual sense
definition. Finally, we combine the sense
proximity estimation with a classification
of semantic relations between senses.
1 Introduction
Our goal is to characterize sense inventories, both
qualitatively and quantitatively, so that the following
questions can be answered:
  Given a pair of senses of the same word, are
they related? If so, in what way and how
closely?
  How well are individual senses defined? For
each sense, how homogeneous are its examples
of use? How coarse is its definition? Should it
be split into subsenses?
  How do these issues affect the evaluation of
automatic Word Sense Disambiguation (WSD)
systems using the sense inventory? What
penalty should be assigned to a WSD system
that confuses two senses, i.e. how much should
it be penalized according to how close these
senses are? Can the sense inventory be im-
proved for evaluation purposes, for instance,
splitting senses into finer-grained distinctions
or collapsing close senses into coarser clusters?
In particular, we are interested in characterizing
WordNet 1.7 as sense inventory for the Senseval-2
WSD comparative evaluation. Unlike conventional
dictionaries, WordNet does not group senses of the
same word in a hierarchical structure; every sense
belongs to a synset, and can only be related to other
senses via conceptual relations (rather than sense re-
lations). Conceptual relations can be used to de-
fine measures of semantic distance (Sussna, 1993;
Agirre and Rigau, 1996; Resnik, 1995), but topic
relatedness is not well captured by wordnet rela-
tions, and this is a fundamental parameter to es-
timate sense similarity in many NLP applications
(Gonzalo et al, 2000).
The issue of estimating semantic distance be-
tween senses of a polysemous word has been pre-
viously addressed in (Resnik and Yarowsky, 1997;
Resnik and Yarowsky, 1999). They propose a mea-
sure of semantic distance based on the likelihood of
the sense distinction being lexicalized in some target
language. The measure was tested using statistics
collected from native-speaker translations of 222
polysemous contexts across 12 languages. The re-
sults obtained showed that monolingual sense dis-
                     July 2002, pp. 32-39.  Association for Computational Linguistics.
                 Disambiguation: Recent Successes and Future Directions, Philadelphia,
                             Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense
tinctions at most levels of granularity can be effec-
tively captured by translations into some set of sec-
ond languages, especially as language family dis-
tance increases. The distance matrices obtained
reproduced faithfully the hierarchical arrangement
of senses provided by the Hector database used in
Senseval-1 (Kilgarriff and Palmer, 2000).
In order to characterize the Senseval-2 Wordnet
1.7 subset, we have adopted such methodology, ex-
tending it to capture also individual sense homo-
geneity, and comparing both quantitative measures
with a coarse, qualitative classification of semantic
relations between senses of a word.
In Section 2 we introduce the quantitative mea-
sures of sense relatedness (as defined by Resnik &
Yarowsky) and sense stability (as an extension to it).
In Section 3, we describe the qualitative classifica-
tion that will be confronted to such measures. In
Sections 4 and 5, we describe the experiment design
and discuss the results obtained. Finally, we draw
some conclusions.
2 Estimating sense relatedness and sense
stability
In order to characterize a sense repository and eval-
uate the quality and nature of its sense distinc-
tions, two aspects of sense granularity should be ad-
dressed:
  Are there sense distinctions that are too close
to be useful in WSD applications, or even to be
clearly distinguished by humans? In general,
what is the semantic distance between senses
of a given word?
  Are there sense definitions that are too coarse-
grained, vague or confusing? If so, should they
be split into finer-grain senses?
Our goal is to give a quantitative characterization
of both aspects for the Senseval-2 test suite. Such
measures would enable a finer scoring of WSD sys-
tems, and would provide new criteria to compare this
test suite with data in forthcoming Senseval evalua-
tions.
The first question can be answered with a quanti-
tative estimate of sense proximity. We will use the
cross-linguistic measure of sense distance proposed
in (Resnik and Yarowsky, 1999), where sense relat-
edness between two meanings of a given word, 
and  , is estimated as the average probability of re-
ceiving the same translation across a set of instances
and a set of languages:
	
same lexicalization 
 

  Word Sense Disambiguation based on
Term to Term Similarity in a Context Space
Javier Artiles
Dpto. Lenguajes y
Sistemas Informa?ticos
UNED, Spain
javart@bec.uned.es
Anselmo Pen?as
Dpto. Lenguajes y
Sistemas Informa?ticos
UNED, Spain
anselmo@lsi.uned.es
Felisa Verdejo
Dpto. Lenguajes y
Sistemas Informa?ticos
UNED, Spain
felisa@lsi.uned.es
Abstract
This paper describes the exemplar based ap-
proach presented by UNED at Senseval-3. In-
stead of representing contexts as bags of terms
and defining a similarity measure between con-
texts, we propose to represent terms as bags
of contexts and define a similarity measure be-
tween terms. Thus, words, lemmas and senses
are represented in the same space (the context
space), and similarity measures can be defined
between them. New contexts are transformed
into this representation in order to calculate
their similarity to the candidate senses. We
show how standard similarity measures obtain
better results in this framework. A new similar-
ity measure in the context space is proposed for
selecting the senses and performing disambigua-
tion. Results of this approach at Senseval-3 are
here reported.
1 Introduction
Word Sense Disambiguation (WSD) is the task
of deciding the appropriate sense for a partic-
ular use of a polysemous word, given its tex-
tual or discursive context. A previous non triv-
ial step is to determine the inventory of mean-
ings potentially attributable to that word. For
this reason, WSD in Senseval is reformulated as
a classification problem where a dictionary be-
comes the class inventory. The disambiguation
process, then, consists in assigning one or more
of these classes to the ambiguous word in the
given context. The Senseval evaluation forum
provides a controlled framework where different
WSD systems can be tested and compared.
Corpus-based methods have offered encour-
aging results in the last years. This kind of
methods profits from statistics on a training
corpus, and Machine Learning (ML) algorithms
to produce a classifier. Learning algorithms
can be divided in two main categories: Super-
vised (where the correct answer for each piece of
training is provided) and Unsupervised (where
the training data is given without any answer
indication). Tests at Senseval-3 are made in
various languages for which two main tasks are
proposed: an all-words task and a lexical sam-
ple task. Participants have available a training
corpus, a set of test examples and a sense inven-
tory in each language. The training corpora are
available in a labelled and a unlabelled format;
the former is mainly for supervised systems and
the latter mainly for the unsupervised ones.
Several supervised ML algorithms have been
applied to WSD (Ide and Ve?ronis, 1998), (Es-
cudero et al, 2000): Decision Lists, Neural Net-
works, Bayesian classifiers, Boosting, Exemplar-
based learning, etc. We report here the
exemplar-based approach developed by UNED
and tested at the Senseval-3 competition in the
lexical sample tasks for English, Spanish, Cata-
lan and Italian.
After this brief introduction, Sections 2 and
3 are devoted, respectively, to the training data
and the processing performed over these data.
Section 4 characterizes the UNED WSD system.
First, we describe the general approach based on
the representation of words, lemmas and senses
in a Context Space. Then, we show how results
are improved by applying standard similarity
measures as cosine in this Context Space. Once
the representation framework is established, we
define the criteria underlying the final similar-
ity measure used at Senseval-3, and we com-
pare it with the previous similarity measures.
Section 5 reports the official results obtained at
the Senseval-3 Lexical Sample tasks for English,
Spanish, Italian and Catalan. Finally, we con-
clude and point out some future work.
2 Data
Each Lexical Sample Task has a relatively large
training set with disambiguated examples. The
test examples set has approximately a half of
the number of the examples in the training data.
Each example offers an ambiguous word and its
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
surrounding context, where the average context
window varies from language to language. Each
training example gives one or more semantic la-
bels for the ambiguous word corresponding to
the correct sense in that context.
Senseval-3 provided the training data and the
test data in XML format. The XML tagging
conventions provides an excellent ground for the
corpora processing, allowing a simple way for
the data browsing and transformation. How-
ever, some of the XML well-formedness con-
straints are not completely satisfied. For exam-
ple, there is no XML declaration and no root
element in the English Lexical Sample docu-
ments. Once these shortcomings are fixed any
XML parser can normally read and process the
data.
Despite the similarity in the structure of the
different corpora at the lexical sample task
in different languages, we had found a het-
erogeneous vocabulary both in the XML tags
and the attributes, forcing to develop ?ad hoc?
parsers for each language. We missed a common
and public document type definition for all the
tasks.
Sense codification is another field where dif-
ferent solutions had been taken. In the English
corpus nouns and adjectives are annotated using
the WordNet 1.7.1. classification1 (Fellbaum,
1998), while the verbs are based on Wordsmyth2
(Scott, 1997). In the Catalan and Spanish tasks
the sense inventory gives a more coarse-grained
classification than WordNet. Both tasks have
provided a dictionary with additional informa-
tion as examples, typical collocations and the
equivalent synsets at WordNet 1.5. Finally, the
Italian sense inventory is based on the Multi-
Wordnet dictionary3 (Pianta et al, 2002). Un-
like the other mentioned languages , the Italian
task doesn?t provide a separate file with the dic-
tionary.
Besides the training data provided by Sen-
seval, we have used the SemCor (Miller et al,
1993) collection in which every word is already
tagged in its part of speech, sense and synset of
WordNet.
3 Preprocessing
A tokenized version of the Catalan, Spanish and
Italian corpora has been provided. In this ver-
sion every word is tagged with its lemma and
1http://www.cogsci.princeton.edu/ wn/
2http://www.wordsmyth.net
3http://multiwordnet.itc.it/
part of speech tag. This information has been
manually annotated by human assessors both in
the Catalan and the Spanish corpora. The Ital-
ian corpus has been processed automatically by
the TnT POStagger4 (Brants, 2000) including
similar tags.
The English data lacked of this information,
leading us to apply the TreeTagger5 (Schmid,
1994) tool to the training and test data as a
previous step to the disambiguation process.
Since the SemCor collection is already tagged,
the preprocessing consisted in the segmentation
of texts by the paragraph tag, obtaining 5382
different fragments. Each paragraph of Semcor
has been used as a separate training example
for the English lexical sample task. We applied
the mapping provided by Senseval to represent
verbs according to the verb inventory used in
Senseval-3.
4 Approach
The supervised UNED WSD system is an ex-
emplar based classifier that performs the disam-
biguation task measuring the similarity between
a new instance and the representation of some
labelled examples. However, instead of repre-
senting contexts as bags of terms and defining
a similarity measure between the new context
and the training contexts, we propose a rep-
resentation of terms as bags of contexts and
the definition of a similarity measure between
terms. Thus, words, lemmas and senses are
represented in the same space, where similar-
ity measures can be defined between them. We
call this space the Context Space. A new disam-
biguation context (bag of words) is transformed
into the Context Space by the inner product,
becoming a kind of abstract term suitable to be
compared with singular senses that are repre-
sented in the same Context Space.
4.1 Representation
The training corpus is represented in the usual
two-dimension matrix A as shown in Figure 1,
where
? c1, ..., cN is the set of examples or con-
texts in the training corpus. Contexts are
treated as bags of words or lemmas.
? lem1, ..., lemT is the set of different words
or lemmas in all the training contexts.
4http://www.coli.uni-sb.de/ thorsten/tnt/
5http://www.ims.uni-stuttgart.de/projekte/corplex/
TreeTagger/
? wi,j is the weight for lemi in the training
context cj .
A new instance q, represented with the vec-
tor of weights (w1q, ..., wiq, ..., wTq), is trans-
formed into a vector in the context space ~q =
(q1, ..., qj , ..., qN ), where ~q is given by the usual
inner product ~q = q ? A (Figure 1):
qj =
T?
i=1
wiqwij
Figure 1: Representation of terms in the Con-
text Space, and transformation of new in-
stances.
If vectors cj (columns of matrix A) and vector
q (original test context) are normalized to have
a length equal to 1, then qj become the cosine
between vectors q and cj . More formally,
~q = q.A = (cos(q, c1), ..., cos(q, cj), ..., cos(q, cN ))
where
cos(q, cj) =
T?
i=1
wiq
?q?
wij
?cj?
and
?x? =
?
?
i
x2i
At this point, both senses and the representa-
tion of the new instance ~q are represented in the
same context space (Figure 2) and a similarity
measure can be defined between them:
sim( ~senik, ~q)
where senik is the k candidate sense for the
ambiguous lemma lemi. Each component j of
~senik is set to 1 if lemma lemi is used with sense
senik in the training context j, and 0 otherwise.
Figure 2: Similarity in the Context Space.
For a new context of the ambiguous lemma
lemi, the candidate sense with higher similarity
is selected:
argmaxk sim( ~senik, ~q)
4.2 Bag of words versus bag of contexts
Table 1 shows experimental results over the En-
glish Lexical Sample test of Senseval-3. Sys-
tem has been trained with the Senseval-3 data
and the SemCor collection. The Senseval train-
ing data has been lemmatized and tagged with
TreeTagger. Only nouns and adjectives have
been considered in their canonical form.
Three different weights wij have been tested:
? Co-occurrence: wij and wiq are set to {0,1}
depending on whether lemi is present or
not in context cj and in the new instance q
respectively. After the inner product q ? A,
the components qj of ~q get the number of
co-occurrences of different lemmas in both
q and the training context cj .
? Term Frequency: wij is set to tfij , the num-
ber of occurrences of lemi in the context cj .
? tf.idf : wij = (1 + log(tfij)) ? (log( Ndfi )),
a standard tf.idf weight where dfi is the
number of contexts that contain lemi.
These weights have been normalized ( wij||cj ||)
and so, the inner product q?A generates a vector
~q of cosines as described above, where qj is the
cosine between q and context cj .
Two similarity measures have been compared.
The first one (maximum) is a similarity of q as
bag of words with the training contexts of sense
sen. The second one (cosine) is the similarity
of sense sen with ~q in the context space:
? Maximum: sim( ~sen, ~q) =
= MaxNj=1 (senj ? qj) =
= Max{j/sen?cj}qj =
= Max{j/sen?cj}cos(q, cj)
Weight Similarity Nouns Adjectives Verbs Total
Co-occurrences Maximum 60.76% 35.85% 60.75% 59.75%
(normalized) Cosine 59.99% 55.97% 63.88% 61.78%
Term frequency Maximum 56.83% 50.31% 56.85% 56.58%
(normalized) Cosine 60.76% 53.46% 63.83% 62.01%
tf.idf Maximum 59.82% 48.43% 59.94% 59.42%
(normalized) Cosine 60.27% 53.46% 64.29% 62.01%
Most frequent
(baseline) 54.01% 54.08% 56.45% 55.23%
Table 1: Bag of words versus bag of contexts, precision-recall
Similarity with sense sen is the high-
est similarity (cosine) between q (as bag of
words) and each of the training contexts
(as bag of words) for sense sen.
? Cosine: sim( ~sen, ~q) = cos( ~sen, ~q) =
=
?
{j/sen?cj}
senj
|| ~sen|| ?
cos(q,cj)
||~q||
Similarity with sense sen is the co-
sine in the Context Space between ~q and
~sen
Table 1 shows that almost all the results are
improved when the similarity measure (cosine)
is applied in the Context Space. The exception
is the consideration of co-ocurrences to disam-
biguate nouns. This exception led us to explore
an alternative similarity measure aimed to im-
prove results over nouns. The following sections
describe this new similarity measure and the cri-
teria underlying it.
4.3 Criteria for the similarity measure
Co-occurrences behave quite good to disam-
biguate nouns as it has been shown in the exper-
iment above. However, the consideration of co-
occurrences in the Context Space permits acu-
mulative measures: Instead of selecting the can-
didate sense associated to the training context
with the maximum number of co-occurrences,
we can consider the co-occurences of q with all
the contexts. The weights and the similarity
function has been set out satisfying the follow-
ing criteria:
1. Select the sense senk assigned to more
training contexts ci that have the maxi-
mum number of co-occurrences with the
test context q. For example, if sense sen1
has two training contexts with the highest
number of co-occurrences and sense sen2
has only one with the same number of co-
occurrences, sen1 must receive a higher
value than sen2.
2. Try to avoid label inconsistencies in the
training corpus. There are some training
examples where the same ambiguous word
is used with the same meaning but tagged
with different sense by human assessors.
Table 2 shows an example of this kind of
inconsistencies.
4.4 Similarity measure
We assign the weights wij and wiq to have ~q a
vector of co-occurrences, where qj is the number
of different nouns and adjectives that co-occurr
in q and the training context cj . In this way, wij
is set to 1 if lemi is present in the context cj .
Otherwise wij is set to 0. Analogously for the
new instance q, wiq is set to 1 if lemi is present
in q and it is set to 0 otherwise.
According to the second criterium, if there
is only one context c1 with the higher num-
ber of co-occurrences with q, then we reduce
the value of this context by reducing artifi-
cially its number of co-occurrences: Being c2
a context with the second higher number of co-
occurrences with q, then we assign to the first
context c1 the number of co-occurrences of con-
text c2.
After this slight modification of ~q we imple-
ment the similarity measure between ~q and a
sense senk according to the first criterium:
sim( ~sen, ~q) =
N?
j=1
senj ? N
qj
Finally, for a new context of lemi we select
the candidate sense that gives more value to the
similarity measure:
argmaxk sim( ~senk, ~q)
<answer instance=?grano.n.1? senseid=?grano.4?/>
<previous> La Federacin Nacional de Cafeteros de Colombia explic que el nuevo valor fue estable-
cido con base en el menor de los precios de reintegro mnimo de grano del pas de los ltimos tres das,
y que fue de 1,3220 dlares la libra, que fue el que alcanz hoy en Nueva York, y tambin en la tasa rep-
resentativa del mercado para esta misma fecha (1.873,77 pesos por dlar). </previous> <target>
El precio interno del caf colombiano permaneci sin modificacin hasta el 10 de noviembre de 1999,
cuando las autoridades cafetaleras retomaron el denominado ?sistema de ajuste automtico?, que
tiene como referencia la cotizacin del <head>grano</head> nacional en los mercados interna-
cionales. </target>
<answer instance=?grano.n.9? senseid=?grano.3?/>
<previous> La carga qued para maana en 376.875 pesos (193,41 dlares) frente a los 375.000 pesos
(192,44 dlares) que rigi hasta hoy. </previous> <target> El reajuste al alza fue adoptado por
el Comit de Precios de la Federacin que fijar el precio interno diariamente a partir de este lunes
tomando en cuenta la cotizacin del <head>grano</head> en el mercado de Nueva York y la tasa
de cambio del da, que para hoy fueron de 1,2613 dlares libra y1.948,60 pesos por dlar </target>
Table 2: Example of inconsistencies in human annotation
Weight Similarity Nouns Adjectives Verbs Total
Co-occurrences Without criterium 2 65.6% 45.9% 62.5% 63.3%
(not normalized) With criterium 2 66.5% 45.9% 63.4% 64.1%
Table 3: Precision-recall for the new similarity measure
Table 3 shows experimental results over the
English Lexical Sample test under the same con-
ditions than experiments in Table 1.
Comparing results in both tables we observe
that the new similarity measure only behaves
better for the disambiguation of nouns. How-
ever, the difference is big enough to improve
overall results. The application of the second
criterium (try to avoid label inconsistencies)
also improves the results as shown in Tables 3
and 4. Table 4 shows the effect of applying this
second criterium to all the languages we have
participated in. With the exception of Cata-
lan, all results are improved slightly (about 1%)
after the filtering of singular labelled contexts.
Although it is a regular behavior, this improve-
ment is not statistically significative.
With Without
Criterium 2 Criterium 2
Spanish 81.8% 80.9%
Catalan 81.8% 82.0%
English 64.1% 63.3%
Italian 49.8% 49.3%
Table 4: Incidence of Criterium 2, precision-
recall
5 Results at Senseval-3
The results submited to Senseval-3 were gener-
ated with the system described in Section 4.4.
Since one sense is assigned to every test con-
text, precison and recall have equal values. Ta-
ble 4 shows official results for the Lexical Sam-
ple Task at Senseval-3 in the four languages we
have participated in: Spanish, Catalan, English
and Italian.
Fine Coarse Baseline
grained grained (most frequent)
Spanish 81.8% - 67%
Catalan 81.8% - 66%
English 64.1% 72% 55%
Italian 49.8% - -
Table 5: Official results at Senseval-3, precision-
recall
Differences between languages are quite re-
markable and show the system dependence on
the training corpora and the sense inventory.
In the English task, 16 test instances have
a correct sense not present in the training cor-
pus. Since we don?t use the dictionary informa-
tion our system was unable to deal with none of
them. In the same way, 68 test instances have
been tagged as ?Unasignable? sense and again
the system was unable to detect none of them.
6 Conclusion and work in progress
We have shown the exemplar-based WSD sys-
tem developed by UNED for the Senseval-3 lexi-
cal sample tasks. The general approach is based
on the definition of a context space that be-
comes a flexible tool to prove quite different
similarity measures between training contexts
and new instances. We have shown that stan-
dard similarity measures improve their results
applied inside this context space. We have es-
tablished some criteria to instantiate this gen-
eral approach and the resulting system has been
evaluated at Senseval-3. The new similarity
measure improves the disambiguation of nouns
and obtains better overall results. The work in
progress includes:
? the study of new criteria to lead us to al-
ternative measures,
? the development of particular disambigua-
tion strategies for verbs, nouns and adjec-
tives,
? the inclusion of the dictionary information,
and
? the consideration of WordNet semantic re-
lationships to extend the training corpus.
Acknowledgements
Special thanks to Julio Gonzalo for the lending
of linguistic resources, and to V??ctor Peinado for
his demonstrated sensibility.
This work has been supported by the Spanish
Ministry of Science and Technology through the
following projects:
? Hermes (TIC-2000-0335-C03-01)
? Syembra (TIC-2003-07158-C04-02)
? R2D2 (TIC 2003-07158-104-01)
References
Thorsten Brants. 2000. Tnt - a statistical part-
of-speech tagger. In In Proceedings of the
Sixth Applied Natural Language Processing
Conference ANLP-2000.
G. Escudero, L. Ma`rquez, and G. Rigau. 2000.
A comparison between supervised learning al-
gorithms for word sense disambiguation. In
In Proceedings of the 4th Computational Nat-
ural Language Learning Workshop, CoNLL.
Christiane Fellbaum, editor. 1998. WordNet
An Electronic Lexical Database. The MIT
Press.
N. Ide and J. Ve?ronis. 1998. Introduction to the
special issue on word sense disambiguation:
The state of the art. Computational Linguis-
tics.
G. Miller, C. Leacock, T. Randee, and
R. Bunker. 1993. A semantic concordance.
In In Procedings of the 3rd DARPA Work-
shop on Human Language Technology.
E. Pianta, L. Bentivogli, and C. Girardi. 2002.
Multiwordnet: developing an aligned mul-
tilingual database. In In Proceedings of
the First International Conference on Global
WordNet.
Helmut Schmid. 1994. Probabilistic part-of-
speech tagging using decision trees. In Inter-
national Conference on New Methods in Lan-
guage Processing.
M. Scott. 1997. Wordsmith tools lexical analy-
sis software for data driven learning and re-
search. Technical report, The University of
Liverpool.
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 49?56, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Evaluating DUC 2004 Tasks with the QARLA Framework
Enrique Amigo?, Julio Gonzalo, Anselmo Pen?as, Felisa Verdejo
Departamento de Lenguajes y Sistemas Informa?ticos
Universidad Nacional de Educacio?n a Distancia
c/Juan del Rosal, 16 - 28040 Madrid - Spain
{enrique,julio,anselmo,felisa}@lsi.uned.es
Abstract
This papers reports the application of
the QARLA evaluation framework to the
DUC 2004 testbed (tasks 2 and 5). Our
experiment addresses two issues: how
well QARLA evaluation measures corre-
late with human judgements, and what ad-
ditional insights can be provided by the
QARLA framework to the DUC evalua-
tion exercises.
1 Introduction
QARLA (Amigo? et al, 2005) is a framework that
uses similarity to models as a building block for
the evaluation of automatic summarisation systems.
The input of QARLA is a summarisation task, a set
of test cases, a set of similarity metrics, and sets of
models and automatic summaries (peers) for each
test case. With such a testbed, QARLA provides:
? A measure, QUEEN, which combines assorted
similarity metrics to estimate the quality of au-
tomatic summarisers.
? A measure, KING, to select the best combina-
tion of similarity metrics.
? An estimation, JACK, of the reliability of the
testbed for evaluation purposes.
The QARLA framework does not rely on human
judges. It is interesting, however, to find out how
well an evaluation using QARLA correlates with hu-
man judges, and whether QARLA can provide ad-
ditional insights into an evaluation based on human
assessments.
In this paper, we apply the QARLA framework
(QUEEN, KING and JACK measures) to the out-
put of two different evaluation exercises: DUC 2004
tasks 2 and 5 (Over and Yen, 2004). Task 2 re-
quires short (one-hundred word) summaries for as-
sorted document sets; Task 5 consists of generating
a short summary in response to a ?Who is? question.
In Section 2, we summarise the QARLA evalua-
tion framework; in Section 3, we describe the sim-
ilarity metrics used in the experiments. Section 4
discusses the results of the QARLA framework us-
ing such metrics on the DUC testbeds. Finally, Sec-
tion 5 draws some conclusions.
2 The QARLA evaluation framework
QARLA uses similarity to models for the evalua-
tion of automatic summarisation systems. Here we
summarise its main features; the reader may refer to
(Amigo? et al, 2005) for details.
The input of the framework is:
? A summarisation task (e.g. topic oriented, in-
formative multi-document summarisation on a
given domain/corpus).
? A set T of test cases (e.g. topic/document set
pairs for the example above)
? A set of summaries M produced by humans
(models), and a set of automatic summaries A
(peers), for every test case.
? A set X of similarity metrics to compare sum-
maries.
With this input, QARLA provides three main
measures that we describe below.
49
2.1 QUEEN : Estimating the quality of an
automatic summary
QUEEN operates under the assumption that a sum-
mary is better if it is closer to the model summaries
according to all metrics; it is defined as the probabil-
ity, measured onM ?M ?M , that for every metric
in X the automatic summary a is closer to a model
than two models to each other:
QUEENX,M (a) ? P (?x ? X.x(a,m) ? x(m
?,m??))
where a is the automatic summary being eval-
uated, ?m,m?,m??? are three models in M , and
x(a,m) stands for the similarity ofm to a. QUEEN
is stated as a probability, and therefore its range of
values is [0, 1].
We can think of the QUEEN measure as using a
set of tests (every similarity metric in X) to falsify
the hypothesis that a given summary a is a model.
Given ?a,m,m?,m???, we test x(a,m) ? x(m?,m??)
for each metric x. a is accepted as a model only if
it passes the test for every metric. QUEEN(a) is,
then, the probability of acceptance for a in the sam-
ple space M ?M ?M .
This measure has some interesting properties: (i)
it is able to combine different similarity metrics
into a single evaluation measure; (ii) it is not af-
fected by the scale properties of individual metrics,
i.e. it does not require metric normalisation and
it is not affected by metric weighting. (iii) Peers
which are very far from the set of models all receive
QUEEN=0. In other words, QUEEN does not distin-
guish between very poor summarisation strategies.
(iv) The value of QUEEN is maximised for peers
that ?merge? with the models under all metrics inX .
(v) The universal quantifier on the metric parameter
x implies that adding redundant metrics do not bias
the result of QUEEN.
Now the question is: which similarity metrics
are adequate to evaluate summaries? Imagine that
we use a similarity metric based on sentence co-
selection; it might happen that humans do not agree
on which sentences to select, and therefore emulat-
ing their sentence selection behaviour is both easy
(nobody agrees with each other) and useless. We
need to take into account which are the features that
human summaries do share, and evaluate according
to them. This is provided by the KING measure.
2.2 KING: estimating the quality of similarity
metrics
The measure KINGM,A(X) estimates the quality of
a set of similarity metrics X using a set of models
M and a set of peers A. KING is defined as the
probability that a model has higher QUEEN value
than any peer in a test sample. Formally:
KINGM,A(X) ?
P (?a ? A,QUEENM,X(m) > QUEENM,X(a))
For example, an ideal metric -that puts all models
together-would give QUEEN(m) = 1 for all mod-
els, and QUEEN(a) = 0 for all peers which are not
put together with the models, obtaining KING = 1.
KING satisfies several interesting properties: (i)
KING does not depend on the scale properties of the
metric; (ii) Adding repeated or very similar peers
do not alter the KING measure, which avoids one
way of biasing the measure. (iii) the KING value of
random and constant metrics is zero or close to zero.
2.3 JACK: reliability of the peer set
Once we detect a difference in quality between two
summarisation systems, the question is now whether
this result is reliable. Would we get the same results
using a different test set (different examples, differ-
ent human summarisers (models) or different base-
line systems)?
The first step is obviously to apply statistical sig-
nificance tests to the results. But even if they give a
positive result, it might be insufficient. The problem
is that the estimation of the probabilities in KING
assumes that the sample sets M,A are not biased.
If M,A are biased, the results can be statistically
significant and yet unreliable. The set of examples
and the behaviour of human summarisers (models)
should be somehow controlled either for homogene-
ity (if the intended profile of examples and/or users
is narrow) or representativity (if it is wide). But how
to know whether the set of automatic summaries is
representative and therefore is not penalising certain
automatic summarisation strategies?
This is addressed by the JACK measure:
50
JACK(X,M,A) ? P (?a, a? ? A|
?x ? X.x(a, a?) ? x(a,m) ? x(a?, a) ? x(a?,m)?
QUEEN(a) > 0 ? QUEEN(a?) > 0)
i.e. the probability over all model summariesm of
finding a couple of automatic summaries a, a? which
are closer to m than to each other according to all
metrics. This measure satisfies three desirable prop-
erties: (i) it can be enlarged by increasing the sim-
ilarity of the peers to the models (the x(m,a) fac-
tor in the inequalities), i.e. enhancing the quality of
the peer set; (ii) it can also be enlarged by decreas-
ing the similarity between automatic summaries (the
x(a, a?) factor in the inequality), i.e. augmenting the
diversity of (independent) automatic summarisation
strategies represented in the test bed; (iii) adding el-
ements to A cannot diminish the JACK value, be-
cause of the existential quantifier on a, a?.
3 Selection of similarity metrics
Each different similarity metric characterises differ-
ent features of a summary. Our first objective is
to select the best set of metrics, that is, the metrics
which best characterise the human summaries (mod-
els) as opposed to automatic summaries. The second
objective is to obtain as much information as possi-
ble about the behaviour of automatic summaries.
In this Section, we begin by describing a set of
59 metrics used as a starting point. Some of them
provide overlapping information; the second step is
then to select a subset of metrics that minimises re-
dundancy and, at the same time, maximises quality
(KING values). Finally, we analyse the characteris-
tics of the selected metrics.
3.1 Similarity metrics
For this work, we have considered the following
similarity metrics:
ROUGE based metrics (R): ROUGE (Lin and
Hovy, 2003) estimates the quality of an au-
tomatic summary on the basis of the n-gram
coverage related to a set of human summaries
(models). Although ROUGE is an evaluation
metric, we can adapt it to behave as a sim-
ilarity metric between pairs of summaries if
we consider only one model in the computa-
tion. There are different kinds of ROUGE met-
rics such as ROUGE-W, ROUGE-L, ROUGE-
1, ROUGE-2, ROUGE-3, ROUGE-4, etc. (Lin,
2004b). Each of these metrics has been ap-
plied over summaries with three preprocessing
options: with stemming and stopword removal
(type c); only with stopwords removal (type b);
or without any kind of preprocessing (type a).
All these combinations give 24 similarity met-
rics based on ROUGE.
Inverted ROUGE based metrics (Rpre): ROUGE
metrics are recall oriented. If we reverse the di-
rection of the similarity computation, we obtain
precision oriented metrics (i.e. Rpre(a, b) =
R(b, a)). In this way, we generate another 24
metrics based on inverted ROUGE.
TruncatedVectModel (TVMn): This family of met-
rics compares the distribution of the n most
relevant terms from original documents in the
summaries. The process is the following: (1)
obtaining the n most frequent lemmas ignoring
stopwords; (2) generating a vector with the rel-
ative frequency of each term in the summary;
(3) calculating the similarity between two vec-
tors as the inverse of the Euclidean distance.
We have used 9 variants of this measure with
n = 1, 4, 8, 16, 32, 64, 128, 256, 512.
AveragedSentencelengthSim (AVLS): This is a very
simple metric that compares the average length
of the sentences in two summaries. It can be
useful to compare the degree of abstraction of
the summaries.
GRAMSIM: This similarity metric compares the
distribution of the part-of-speech tags in the
two summaries. The processing is the follow-
ing: (1) part-of-speech tagging of summaries
using TreeTagger ; (2) generation of a vector
with the tags frequency for each summary; (3)
calculation of the similarity between two vec-
tors as the inverse of the Euclidean distance.
This similarity metric is not content oriented,
but syntax-oriented.
51
Figure 1: Similarity Metric Clusters
3.2 Clustering similarity metrics
From the set of metrics described above we have 57
(24+24+9) content oriented metrics, plus two met-
rics based on stylistic features (AVLS and GRAM-
SIM). However, the 57 metrics characterising sum-
mary contents are highly redundant. Thus, cluster-
ing similar metrics seems desirable.
We perform an automatic clustering process us-
ing the following notion of proximity between two
metric sets:
sim(X,X ?) ? Prob[H(X) ? H(X ?)]
where H(X) ? ?x ? X.x(a,m) ? x(m?,m??)
Two metrics sets are similar, according to the for-
mula, if they behave similarly with respect to the
QUEEN condition (H predicate in the formula),
i.e. the probability that the two sets of metrics dis-
criminate the same automatic summaries when they
are compared to the same pair of models.
Figure 1 shows the clustering of similarity met-
rics for the DUC 2004 Task 2. The number of clus-
ters was fixed in 10. After the clustering process, the
48 ROUGE metrics are grouped in 7 sets, and the 9
TVM metrics are grouped in 3 sets. In each clus-
ter, the metric with highest KING has been marked
in boldface. Note that the ROUGE-c metrics (with
stemming) with highest KING are those based on re-
call whereas the ROUGE-a/b metrics (without stem-
ming) are those based on precision. Regarding TVM
clusters, the metrics with highest KING in each clus-
ter are those based on a higher number of terms.
Finally, we select the metric with highest KING
in each group, obtaining the 10 most representative
metrics.
3.3 Best evaluation metric: KING values
Figure 2 shows the KING values for the selected
similarity metrics, which represent how every metric
characterises model summaries as opposed to auto-
matic summaries. These are the main results:
? The last column shows the best metric set,
considering all possible metric combinations.
In both DUC tasks, the best combination is
{Rpre-W-1.2.b,TVM.512. This metric set gets
better KING values than any individual metric
in isolation (17% better than the second best for
task 2, and 23% better for task 5). This is an in-
teresting result confirming that we can improve
our ability to characterise human summaries
just by combining standard similarity metrics
in the QARLA framework. Note also that both
metrics in the best set are content-oriented.
? Rpre-W.1.2.b (inverted ROUGE measure, us-
ing non-contiguous word sequences, remov-
ing stopwords, without stemming) obtains the
highest individual KING for task 2, and is one
of the best in task 5, confirming that ROUGE-
based metrics are a robust way of evaluating
summaries, and indicating that non-contiguous
word sequences can be more useful for evalua-
tion purposes than n-grams.
52
Figure 2: Similarity Metric quality
? TVM metrics get higher values when consid-
ering more terms (TVM.512), confirming that
comparing with just a few terms (e.g. TVM.4)
is not informative enough.
? Overall, KING values are higher for task
5, suggesting that there is more agreement
between human summaries in topic-oriented
tasks.
3.4 Reliability of the results
The JACK measure estimates the reliability of
QARLA results, and is correlated with the diversity
of automatic summarisation strategies included in
the testbed. In principle, the larger the number of au-
tomatic summaries, the higher the JACK values we
should obtain. The important point is to determine
when JACK values tend to stabilise; at this point, it
is not useful to add more automatic summaries with-
out introducing new summarisation strategies.
Figure 3 shows how JACKRpre-W,TVM.512 values
grow when adding automatic summaries. For more
than 10 systems, JACK values grow slower in both
tasks. Absolute JACK values are higher in Task 2
than in task 5, indicating that systems tend to pro-
duce more similar summaries in Task 5 (perhaps be-
cause it is a topic-oriented task). This result suggests
that we should incorporate more diverse summarisa-
tion strategies in Task 5 to enhance the reliability of
the testbed for evaluation purposes with QARLA.
4 Evaluation of automatic summarisers:
QUEEN values
The QUEEN measure provides two kinds of infor-
mation to compare automatic summarisation sys-
tems: which are the best systems -according to the
best metric set-, and which are the individual fea-
tures of every automatic summariser -according to
individual similarity metrics-.
4.1 System ranking
The best metric combination for both tasks was
{Rpre-W,TVM.512}; therefore, our global system
evaluation uses this combination of content-oriented
metrics. Figure 4 shows the QUEEN{Rpre-W,TVM.512}
values for each participating system in DUC 2004,
also including the model summaries. As expected,
model summaries obtain the highest QUEEN values
in both DUC tasks, with a significant distance with
respect to the automatic summaries.
4.2 Correlation with human judgements
The manual ranking generated in DUC is based on a
set of human-produced evaluation criteria, whereas
the QARLA framework gives more weight to the as-
pects that characterise model summaries as opposed
to automatic summaries. It is interesting, however,
to find out whether both evaluation methodologies
are correlated. Indeed, this is the case: the Pearson
correlation between manual and QUEEN rankings is
0.92 for the Task 2 and 0.96 for the Task 5.
Of course, QUEEN values depend on the chosen
metric set X; it is also interesting to check whether
53
Figure 3: JACK vs. Number of Automatic Summaries
Figure 4: QUEEN system ranking for the best metric set (A-H are models)
Figure 5: Correlation Between DUC and QARLA results
54
Figure 6: QUEEN values over GRAMSIM
metrics with higher KING values lead to QUEEN
rankings more similar to human judgements. Fig-
ure 5 shows the Pearson correlation between man-
ual and QUEEN rankings for 1024 metric combina-
tions with different KING values. The figure con-
firms that higher KING values are associated with
rankings closer to human judgements.
4.3 Stylistic features
The best metric combination leaves out similarity
metrics based on stylistic features. It is interesting,
however, to see how automatic summaries behave
with respect to this kind of features. Perhaps the
most remarkable fact about stylistic similarities is
that, in the case of the GRAMSIM metric, task 2
and task 5 exhibit a rather different behaviour (see
Figure 6). In task 2, systems merge with the models,
while in task 5 the QUEEN values of the systems
are inferior to the models. This suggests that there
is some stylistic component in models that systems
are not capturing in the topic-oriented task.
5 Related work
The methodology which is closest to our frame-
work is ORANGE (Lin, 2004a), which evaluates a
similarity metric using the average ranks obtained
by reference items within a baseline set. As in
our framework, ORANGE performs an automatic
meta-evaluation, there is no need for human assess-
ments, and it does not depend on the scale properties
of the metric being evaluated (because changes of
scale preserve rankings). The ORANGE approach
is, indeed, intimately related to the original QARLA
measure introduced in (Amigo et al, 2004).
There are several approaches to the automatic
evaluation of summarisation and Machine Transla-
tion systems (Culy and Riehemann, 2003; Coughlin,
2003). Probably the most significant improvement
over ORANGE is the ability to combine automati-
cally the information of different metrics. Our im-
pression is that a comprehensive automatic evalua-
tion of a summary must necessarily capture different
aspects of the problem with different metrics, and
that the results of every individual checking (metric)
should not be combined in any prescribed algebraic
way (such as a linear weighted combination). Our
framework satisfies this condition.
ORANGE, however, has also an advantage over
the QARLA framework, namely that it can be used
for evaluation metrics which are not based on sim-
ilarity between model/peer pairs. For instance,
ROUGE can be applied directly in the ORANGE
framework without any reformulation.
6 Conclusions
The application of the QARLA evaluation frame-
work to the DUC testbed provides some useful in-
sights into the problem of evaluating text summari-
sation systems:
? The results show that a combination of simi-
larity metrics behaves better than any metric in
isolation. The best metric set is {Rpre-W, TVM.512},
a combination of content-oriented metrics. Un-
55
surprisingly, stylistic similarity is less useful
for evaluation purposes.
? The evaluation provided by QARLA correlates
well with the rankings provided by DUC hu-
man judges. For both tasks, metric sets with
higher KING values slightly outperforms the
best ROUGE evaluation measure.
? QARLA measures show that DUC tasks 2 and
5 are quite different in nature. In Task 5, human
summaries are more similar, and the automatic
summarisation strategies evaluated are less di-
verse.
Acknowledgements
We are indebted to Ed Hovy, Donna Harman, Paul
Over, Hoa Dang and Chin-Yew Lin for their inspir-
ing and generous feedback at different stages in the
development of QARLA. We are also indebted to
NIST for hosting Enrique Amigo? as a visitor and
for providing the DUC test beds. This work has
been partially supported by the Spanish government,
project R2D2 (TIC-2003-7180).
References
E. Amigo?, J. Gonzalo, A. Pen?as, and F. Verdejo. 2005.
QARLA: a Framework for the Evaluation of Text
Summarization Systems. In Proceedings of the 43rd
Annual Meeting of the Association for Computational
Linguistics (ACL 2005).
E. Amigo, V. Peinado, J. Gonzalo, A. Pen?as, and
F. Verdejo. 2004. An Empirical Study of Information
Synthesis Tasks. In Proceedings of the 42th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), Barcelona, July.
Deborah Coughlin. 2003. Correlating Automated and
Human Assessments of Machine Translation Quality.
In In Proceedings of MT Summit IX, New Orleans,LA.
Christopher Culy and Susanne Riehemann. 2003. The
Limits of N-Gram Translation Evaluation Metrics. In
Proceedings of MT Summit IX, New Orleans,LA.
C. Lin and E. H. Hovy. 2003. Automatic Evaluation of
Summaries Using N-gram Co-ocurrence Statistics. In
Proceeding of 2003 Language Technology Conference
(HLT-NAACL 2003).
C. Lin. 2004a. Orange: a Method for Evaluating Au-
tomatic Metrics for Machine Translation. In Pro-
ceedings of the 36th Annual Conference on Compu-
tational Linguisticsion for Computational Linguistics
(Coling?04), Geneva, August.
Chin-Yew Lin. 2004b. Rouge: A Package for Auto-
matic Evaluation of Summaries. In Marie-Francine
Moens and Stan Szpakowicz, editors, Text Summariza-
tion Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
P. Over and J. Yen. 2004. An introduction to DUC 2004
Intrinsic Evaluation of Generic New Text Summariza-
tion Systems. In Proceedings of DUC 2004 Document
Understanding Workshop, Boston.
56
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 89?94,
Prague, June 2007. c?2007 Association for Computational Linguistics
Experiments of UNED at the Third
Recognising Textual Entailment Challenge
A?lvaro Rodrigo, Anselmo Pen?as, Jesu?s Herrera, Felisa Verdejo
Departmento de Lenguajes y Sistemas Informa?ticos
Universidad Nacional de Educacio?n a Distancia
Madrid, Spain
{alvarory, anselmo, jesus.herrera, felisa}@lsi.uned.es
Abstract
This paper describes the experiments devel-
oped and the results obtained in the partic-
ipation of UNED in the Third Recognising
Textual Entailment (RTE) Challenge. The
experiments are focused on the study of the
effect of named entities in the recognition
of textual entailment. While Named Entity
Recognition (NER) provides remarkable re-
sults (accuracy over 70%) for RTE on QA
task, IE task requires more sophisticated
treatment of named entities such as the iden-
tification of relations between them.
1 Introduction
The systems presented to the Third Recognizing
Textual Entailment Challenge are based on the one
presented to the Second RTE Challenge (Herrera
et al, 2006b) and the ones presented to the An-
swer Validation Exercise (AVE) 2006 (Rodrigo et
al., 2007).
Since a high quantity of pairs of RTE-3 collec-
tions contain named entities (82.6% of the hypothe-
ses in the test collection contain at least one named
entity), the objective of this work is to study the ef-
fect of named entity recognition on textual entail-
ment in the framework of the Third RTE Challenge.
In short, the techniques involved in the experi-
ments in order to reach these objectives are:
? Lexical overlapping between ngrams of text
and hypothesis.
? Entailment between named entities.
? Branch overlapping between dependency trees
of text and hypothesis.
In section 2, the main components of the systems
are described in detail. Section 3 describes the infor-
mation our systems use for the entailment decision.
The description of the two runs submitted are given
in Section 4. The results obtained and its analysis are
described in Section 5. Section 6 shows a discussion
of the results. Finally, some conclusions and future
work are given.
2 Systems Description
The proposed systems are based on surface tech-
niques of lexical and syntactic analysis considering
each task (Information Extraction, Information Re-
trieval, Question Answering and Text Summariza-
tion) of the RTE Challenge independently.
The systems accept pairs of text snippets (text and
hypothesis) at the input and give a boolean value at
the output: YES if the text entails the hypothesis and
NO otherwise. This value is obtained by the appli-
cation of the learned model by a SVM classifier.
The main components of the systems are the fol-
lowing:
2.1 Linguistic processing
Firstly, each text-hypothesis pair is preprocessed in
order to obtain the following information for the en-
tailment decision:
? POS: a Part of Speech Tagging is performed in
order to obtain lemmas for both text and hy-
pothesis using the Freeling POS tagger (Car-
reras et al, 2004).
89
<t>...Iraq invaded Kuwait on <TIMEX>August 2 1990</TIMEX>...</t>
<h>Iraq invaded Kuwait in <NUMEX>1990</NUMEX></h>
Figure 1: Example of an error when disambiguating the named entity type.
<t>...Chernobyl accident began on
<ENTITY>Saturday April 26 1986</ENTITY>...</t>
<h>The Chernobyl disaster was in <ENTITY>1986</ENTITY></h>
Figure 2: Example of a pair that justifies the process of entailment.
<pair id=??5?? entailment=??NO?? task=??IE?? length=??short??>
<t>The Communist Party USA was a small Maoist political party
which was founded in 1965 by members of the Communist Party around
Michael Laski who took the side of China in the Sino-Soviet split.
</t>
<h>Michael Laski was an opponent of China.</h>
</pair>
<pair id=??7?? entailment=??NO?? task=??IE?? length=??short??>
<t>Sandra Goudie was first elected to Parliament in the 2002
elections, narrowly winning the seat of Coromandel by defeating
Labour candidate Max Purnell and pushing incumbent Green MP
Jeanette Fitzsimons into third place.</t>
<h>Sandra Goudie was defeated by Max Purnell.</h>
</pair>
<pair id=??8?? entailment=??NO?? task=??IE?? length=??short??>
<t>Ms. Minton left Australia in 1961 to pursue her studies in
London.</t>
<h>Ms. Minton was born in Australia.</h>
</pair>
Figure 3: IE pairs with entailment between named entities but not between named entities relations.
90
? NER: the Freeling Named Entity Recogniser is
also applied to recover the information needed
by the named entity entailment module that is
described in the following section. Numeric ex-
pressions, proper nouns and temporal expres-
sions of each text and hypothesis are tagged.
? Dependency analysis: a dependency tree of
each text and hypothesis is obtained using Lin?s
Minipar (Lin, 1998).
2.2 Entailment between named entities
Once the named entities of the hypothesis and the
text are detected, the next step is to determine the
entailment relations between the named entities in
the text and the named entities in the hypothesis. In
(Rodrigo et al, 2007) the following entailment rela-
tions between named entities were defined:
1. A Proper Noun E1 entails a Proper Noun E2 if
the text string of E1 contains the text string of
E2.
2. A Time Expression T1 entails a Time Expres-
sion T2 if the time range of T1 is included in
the time range of T2.
3. A numeric expression N1 entails a numeric ex-
pression N2 if the range associated to N2 en-
closes the range of N1.
Some characters change in different expressions
of the same named entity as, for example, in a proper
noun with different wordings (e.g. Yasser, Yaser,
Yasir). To detect the entailment in these situations,
when the previous process fails, we implemented a
modified entailment decision process taking into ac-
count the edit distance of Levenshtein (Levensthein,
1966). Thus, if two named entities differ in less than
20%, then we assume that exists an entailment rela-
tion between these named entities.
However, this definition of named entities entail-
ment does not support errors due to wrong named
entities classification as we can see in Figure 1. The
expression 1990 represents a year but it is recog-
nised as a numeric expression in the hypothesis.
However the same expression is recognised as a tem-
poral expression in the text and, therefore, the ex-
pression in the hypothesis cannot be entailed by it
according to the named entities entailment definition
above.
We quantified the effect of these errors in recog-
nising textual entailment. For this purpose, we de-
veloped the following two settings:
1. A system based in dependency analysis and
WordNet (Herrera et al, 2006b) that uses the
categorization given by the NER tool, where
the entailment relations between named entities
are the previously ones defined.
2. The same system based on dependency analysis
and WordNet but not using the categorization
given by the NER tool. All named entities de-
tected receive the same tag and a named entity
E1 entails a named entity E2 if the text string
of E1 contains the text string of E2 (see Figure
2).
We checked the performance of these two settings
over the test corpus set of the Second RTE Chal-
lenge. The results obtained, using the accuracy mea-
sure that is the fraction of correct responses accord-
ing to (Dagan et al, 2006), are shown in table 1. The
table shows that with an easier and a more robust
processing (NER without classification) the perfor-
mance is not only maintained, but it is even slightly
higher.
This fact led us to ignore the named entity catego-
rization given by the tool and assume that text and
hypothesis are related and close texts where same
expressions must receive same categories, without
the need of classification. Thus, all detected named
entities receive the same tag and we consider that a
named entity E1 entails a named entity E2 if the text
string of E1 contains the text string of E2.
Table 1: Entailment between numeric expressions.
Accuracy
Setting 1 0.610
Setting 2 0.614
2.3 Sentence level matching
A tree matching module, which searches for match-
ing branches into the hypotheses? dependency trees,
is used. There is a potential matching branch per
leaf. A branch from the hypothesis is considered
91
a ?matching branch? only if all its nodes from the
root to the leaf are involved in a lexical entailment
(Herrera et al, 2006a). In this way, the subtree con-
formed by all the matching branches from a hypoth-
esis? dependency tree is included in the respective
text?s dependency tree, giving an idea of tree inclu-
sion.
We assumed that the larger is the included sub-
tree of the hypothesis? dependency tree, the more
semantically similar are the text and the hypothesis.
Thus, the existence or absence of an entailment rela-
tion from a text to its respective hypothesis considers
the portion of the hypothesis? tree that is included in
the text?s tree.
3 Entailment decision
A SVM classifier was applied in order to train a
model from the development corpus. The model was
trained with a set of features obtained from the pro-
cessing described above. The features we have used
and the training strategies were the following:
3.1 Features
We prepared the following features to feed the SVM
model:
1. Percentage of nodes of the hypothesis? de-
pendency tree pertaining to matching branches
according to section 2.3 considering, respec-
tively:
? Lexical entailment between the words of
the snippets involved.
? Lexical entailment between the lemmas of
the snippets involved.
2. Percentage of words of the hypothesis in the
text (treated as bags of words).
3. Percentage of unigrams (lemmas) of the hy-
pothesis in the text (treated as bags of lemmas).
4. Percentage of bigrams (lemmas) of the hypoth-
esis in the text (treated as bags of lemmas).
5. Percentage of trigrams (lemmas) of the hypoth-
esis in the text (treated as bags of lemmas).
6. A boolean value indicating if there is or not any
named entity in the hypothesis that is not en-
tailed by one or more named entities in the text
according to the named entity entailment deci-
sion described in section 2.2.
Table 2: Experiments with separate training over the
development corpus using cross validation.
Accuracy with Accuracy with
the same model a different model
for all tasks for each task
Setting 1 0.64 0.67
Setting 2 0.62 0.66
Table 3: Experiments with separate training over the
test corpus.
Accuracy with Accuracy with
the same model a different model
for all tasks for each task
Setting 1 0.59 0.62
Setting 2 0.60 0.64
Table 4: Results for run 1 and run 2.
Accuracy
run 1 run 2
IE 52.50% 53.50%
IR 67% 67%
QA 72% 72%
SUM 58% 60%
Overall 62.38% 63.12%
3.2 Training
About the decision of how to perform the training
in our SVM models, we wanted to study the effect
of training a unique model compared to training one
different model per task.
For this purpose we used the following two set-
tings:
1. A SVM model that uses features 2, 3, 4 and 5
from section 3.1.
2. A SVM model that uses features 2, 3, 4, 5 and
6 from section 3.1.
Each setting was training using cross validation
over the development set of the Third RTE Chal-
lenge in two different ways:
1. Training a unique model for all pairs.
92
2. Training one model for each task. Each model
is trained with only pairs from the same task
that the model will predict.
The results obtained in the experiments are shown
in table 2. As we can see in the table, with the train-
ing of one model for each task results are slightly
better, increasing performance of both settings. Tak-
ing into account these results, we took the decision
of using a different training for each task in the runs
submitted.
Our decision was confirmed after the runs submis-
sion to RTE-3 Challenge with new experiments over
the RTE-3 test corpus, using the RTE-3 development
corpus as training (see table 3 for results).
4 Runs Submitted
Two different runs were submitted to the Third RTE
Challenge. Each run was trained using the method
described in section 3.2 with the following subset of
the features described in section 3.1:
? Run 1 was obtained using the features 2, 3,
4 and 5 from section 3.1. These features ob-
tained good results for pairs from the QA task,
as we can see in (Rodrigo et al, 2007), and
we wanted to check their performance in other
tasks.
? Run 2 was obtained using the following fea-
tures for each task:
? IE: features 2, 3, 4, 5 and 6 from section
3.1. These ones were the features that ob-
tained the best results for IE pairs in our
experiments over the development set.
? IR: features 2, 3, 4 and 5 from section 3.1.
These ones were the features that obtained
best results for IR pairs in our experiments
over the development set.
? QA: feature 6 from section 3.1. We chose
this feature, which had obtained an ac-
curacy over 70% in previous experiments
over the development set in QA pairs, to
study the effect of named entities in QA
pairs.
? SUM: features 1, 2 and 3 from section 3.1.
We selected these features to show the im-
portance of dependency analysis in SUM
pairs as it is shown in section 6.
5 Results
Accuracy was applied as the main measure to the
participating systems.
The results obtained over the test corpus for the
two runs submitted are shown in table 4.
As we can see in both runs, different accuracy val-
ues are obtained depending on the task. The best re-
sult is obtained in pairs from QA with a 72% accu-
racy in the two runs, although two different systems
are applied. This result pushes us to use this system
for Answer Validation (Pen?as et al, 2007). Results
in run 2, which uses a different setting for each task,
are slightly better than results in run 1, but only in
IE and SUM. However, results are too close to ac-
cept a confirmation of our initial intuition that pairs
from different tasks could need not only a different
training, but also the use of different approaches for
the entailment decision.
6 Discussion
In run 2 we used NER for IE and QA, the two tasks
with the higher percentage of pairs with at least one
named entity in the hypothesis (98.5% in IE and
97% in QA).
Our previous work about the use of named enti-
ties in textual entailment (Rodrigo et al, 2007) sug-
gested that NER permitted to obtain good results.
However, after the RTE-3 experience, we found that
the use of NER does not improve results in all tasks,
but only in QA in a solid way with the previous
work.
We performed a qualitative study over the IE pairs
showing that, as it can be expected, in pairs from IE
the relations between named entities are more im-
portant that named entities themselves.
Figure 3 shows some examples where all named
entities are entailed but not the relation between
them. In pair 5 both Michael Laski and China are
entailed but the relation between them is took the
side of in the text, and was an opponent of in the
hypothesis. The same problem appears in the other
pairs with the relation left instead was born in (pair
8) or passive voice instead active voice (pair 7).
Comparing run 1 and run 2, dependency analysis
shows its usefulness in SUM pairs, where texts and
hypotheses have a higher syntactic parallelism than
in pairs from other tasks. This statement is shown
93
Table 5: Percentage of hypothesis nodes in matching
branches.
Percentage
SUM 75,505%
IE 7,353%
IR 6,422%
QA 8,496%
in table 5 where the percentage of hypothesis nodes
pertaining to matching branches in the dependency
tree is much higher in SUM pairs than in the rest of
tasks.
This syntactic parallelism seems to be the respon-
sible for the 2% increasing between the first and the
second run in SUM pairs.
7 Conclusions and future work
The experiments have been focused on the study of
the importance of considering entailment between
named entities in the recognition of textual entail-
ment, and the use of a separate training for each task.
As we have seen, both approaches increase slightly
the accuracy of the proposed systems. As we have
also shown, different approaches for each task could
also increase the system performance.
Future work is focused on improving the perfor-
mance in IE pairs taking into account relations be-
tween named entities.
Acknowledgments
This work has been partially supported by the Span-
ish Ministry of Science and Technology within the
project R2D2-SyEMBRA (TIC-2003-07158-C04-
02), the Regional Government of Madrid under the
Research Network MAVIR (S-0505/TIC-0267), the
Education Council of the Regional Government of
Madrid and the European Social Fund.
References
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
FreeLing: An Open-Source Suite of Language An-
alyzers.. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC04). Lisbon, Portugal, 2004.
I. Dagan, O. Glickman and B. Magnini. 2006. The
PASCAL Recognising Textual Entailment Challenge.
In Quin?onero-Candela et al, editors, MLCW 2005,
LNAI Volume 3944, Jan 2006, Pages 177 - 190.
J. Herrera, A. Pen?as and F. Verdejo. 2006a. Textual En-
tailment Recognition Based on Dependency Analysis
and WordNet. In Quin?onero-Candela et al, editors,
MLCW 2005, LNAI Volume 3944, Jan 2006, Pages
231-239.
J. Herrera, A. Pen?as, A?. Rodrigo and F. Verdejo. 2006b.
UNED at PASCAL RTE-2 Challenge. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, Venice, Italy.
V. I. Levensthein. 1966. Binary Codes Capable of Cor-
recting Deletions, Insertions and Reversals. In Soviet
Physics - Doklady, volume 10, pages 707710, 1966.
D. Lin. 1998. Dependency-based Evaluation of MINI-
PAR. Workshop on the Evaluation of Parsing Systems,
Granada, Spain, May, 1998.
A. Pen?as, A?. Rodrigo, V. Sama and F. Verdejo. 2007.
Overview of the Answer Validation Exercise 2006. In
Lecture Notes in Computer Science. In press.
A?. Rodrigo, A. Pen?as, J. Herrera and F. Verdejo. 2007.
The Effect of Entity Recognition on Answer Validation.
In Lecture Notes in Computer Science. In press.
94
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 455?466,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Corroborating Text Evaluation Results with Heterogeneous Measures
Enrique Amigo? ? Julio Gonzalo ? Jesu?s Gime?nez ? Felisa Verdejo?
? UNED, Madrid
{enrique,julio,felisa}@lsi.uned.es
? UPC, Barcelona
{jgimenez}@lsi.upc.edu
Abstract
Automatically produced texts (e.g. transla-
tions or summaries) are usually evaluated with
n-gram based measures such as BLEU or
ROUGE, while the wide set of more sophisti-
cated measures that have been proposed in the
last years remains largely ignored for practical
purposes. In this paper we first present an in-
depth analysis of the state of the art in order
to clarify this issue. After this, we formalize
and verify empirically a set of properties that
every text evaluation measure based on simi-
larity to human-produced references satisfies.
These properties imply that corroborating sys-
tem improvements with additional measures
always increases the overall reliability of the
evaluation process. In addition, the greater the
heterogeneity of the measures (which is mea-
surable) the higher their combined reliability.
These results support the use of heterogeneous
measures in order to consolidate text evalua-
tion results.
1 Introduction
The automatic evaluation of textual outputs is a
core issue in many Natural Language Processing
(NLP) tasks such as Natural Language Generation,
Machine Translation (MT) and Automatic Sum-
marization (AS). State-of-the-art automatic evalu-
ation methods all operate by rewarding similari-
ties between automatically-produced candidate out-
puts and manually-produced reference solutions, so-
called human references or models.
Over the last decade, a wide variety of measures,
based on different quality assumptions, have been
proposed. Recent work suggests exploiting exter-
nal knowledge sources and/or deep linguistic an-
notation, and measure combination (see Section 2).
However, original measures based on lexical match-
ing, such as BLEU (Papineni et al, 2001a) and
ROUGE (Lin, 2004) are still preferred as de facto
standards in MT and AS, respectively. There are,
in our opinion, two main reasons behind this fact.
First, the use of a common measure certainly allows
researchers to carry out objective comparisons be-
tween their work and other published results. Sec-
ond, the advantages of novel measures are not easy
to demonstrate in terms of correlation with human
judgements.
Our goal is not to answer which is the most re-
liable metric or to propose yet another novel mea-
sure. Rather than this, we first analyze in depth the
state of the art, concluding that it is not easy to de-
termine the reliability of a measure. In absence of a
clear proof of the advantages of novel measures, sys-
tem developers naturally tend to prefer well-known
standard measures. Second, we formalize and check
empirically two intrinsic properties that any evalua-
tion measure based on similarity to human-produced
references satisfies. Assuming that a measure satis-
fies a set of basic formal constraints, these properties
imply that corroborating a system comparison with
additional measures always increases the overall re-
liability of the evaluation process, even when the
added measures have a low correlation with human
judgements. In most papers, evaluation results are
corroborated with similar n-gram based measures
(eg. BLEU and ROUGE). However, according to
our second property, the greater the heterogeneity of
455
the measures (which is measurable) the higher their
reliability. The practical implication is that, corrob-
orating evaluation results with measures based on
higher linguistic levels increases the heterogeneity,
and therefore, the reliability of evaluation results.
2 State of the Art
2.1 Individual measures
Among NLP disciplines, MT probably has the
widest set of automatic evaluation measures. The
dominant approach to automatic MT evaluation is,
today, based on lexical metrics (also called n-gram
based metrics). These metrics work by rewarding
lexical similarity between candidate translations and
a set of manually-produced reference translations.
Lexical metrics can be classified according to how
they compute similarity. Some are based on edit dis-
tance, e.g., WER (Nie?en et al, 2000), PER (Till-
mann et al, 1997), and TER (Snover et al, 2006).
Other metrics are based on computing lexical preci-
sion, e.g., BLEU (Papineni et al, 2001b) and NIST
(Doddington, 2002), lexical recall, e.g., ROUGE
(Lin and Och, 2004a) and CDER (Leusch et al,
2006), or a balance between the two, e.g., GTM
(Melamed et al, 2003; Turian et al, 2003b), ME-
TEOR (Banerjee and Lavie, 2005), BLANC (Lita et
al., 2005), SIA (Liu and Gildea, 2006), MAXSIM
(Chan and Ng, 2008), and Ol (Gime?nez, 2008).
The lexical measure BLEU has been criticized in
many ways. Some drawbacks of BLEU are the lack
of interpretability (Turian et al, 2003a), the fact that
it is not necessary to increase BLEU to improve sys-
tems (Callison-burch and Osborne, 2006), the over-
scoring of statistical MT systems (Le and Przybocki,
2005), the low reliability over rich morphology lan-
guages (Homola et al, 2009), or even the fact that a
poor system translation of a book can obtain higher
BLEU results than a manually produced translation
(Culy and Riehemann, 2003).
The reaction to these criticisms has been focused
on the development of more sophisticated measures
in which candidate and reference translations are
automatically annotated and compared at different
linguistic levels. Some of the features employed
include parts of speech (Popovic and Ney, 2007;
Gime?nez and Ma`rquez, 2007), syntactic dependen-
cies (Liu and Gildea, 2005; Gime?nez and Ma`rquez,
2007; Owczarzak et al, 2007a; Owczarzak et al,
2007b; Owczarzak et al, 2008; Chan and Ng,
2008; Kahn et al, 2009), CCG parsing (Mehay and
Brew, 2007), syntactic constituents (Liu and Gildea,
2005; Gime?nez and Ma`rquez, 2007), named entities
(Reeder et al, 2001; Gime?nez and Ma`rquez, 2007),
semantic roles (Gime?nez and Ma`rquez, 2007), dis-
course representations (Gime?nez, 2008), and textual
entailment features (Pado? et al, 2009). In general,
when a higher linguistic level is incorporated, lin-
guistic features at lower levels are preserved.
The proposals for summarization evaluation are
less numerous. Some proposals for AS tasks are
based on syntactic units (Tratz and Hovy, 2008), de-
pendency triples (Owczarzak, 2009) or convolution
kernels (Hirao et al, 2005) which reported some re-
liability improvement over ROUGE in terms of cor-
relation with human judgements.
In general, however, it is not easy to determine
clearly the contribution of deeper linguistic knowl-
edge in those proposals. In the case of MT, im-
provements versus BLEU have been reported (Liu
and Gildea, 2005; Kahn et al, 2009), but not over
a more elaborated metric such as METEOR (Mehay
and Brew, 2007; Chan and Ng, 2008). Besides, con-
troversial results on their performance at sentence vs
system level have been reported in shared evaluation
tasks (Callison-Burch et al, 2008; Callison-Burch et
al., 2009; Callison-Burch et al, 2010).
2.2 Combined measures
Several researchers have suggested integrating het-
erogeneous measures. Some of them optimize the
measure combination function according to the met-
ric?s ability to emulate the behavior of human as-
sessors (i.e., correlation with human assessments).
For instance, using linear combinations (Pado? et al,
2009; Liu and Gildea, 2007; Gime?nez and Ma`rquez,
2008), Decision Trees (Akiba et al, 2001; Quirk,
2004), regression based algorithms (Paul et al,
2007; Albrecht and Hwa, 2007a; Albrecht and Hwa,
2007b) or a variety of supervised machine learn-
ing algorithms(Quirk et al, 2005; Corston-Oliver et
al., 2001; Kulesza and Shieber, 2004; Gamon et al,
2005; Amigo? et al, 2005).
Some of these works report evidence on the con-
tribution of combining heterogeneous measures. For
instance, Albrecht and Hwa included syntax-based
456
measures together with lexical measures, outper-
forming other combination schemes (Albrecht and
Hwa, 2007a; Albrecht and Hwa, 2007b). Liu and
Gildea, after examining the contribution of each
component metric, found that ?metrics showing dif-
ferent properties of a sentence are more likely to
make a good combined metric?(Liu and Gildea,
2007). Akiba et al, which combined multiple edit-
distance features based on lexical, morphosyntac-
tic and lexical semantic information, observed that
their approach improved single editing distance for
several data sets (Akiba et al, 2001). More evi-
dence was provided by Corston and Oliver. They
showed that results on the task of discriminating be-
tween manual and automatic translations improve
when combining linguistic and n-gram based fea-
tures. In addition, they showed that this mixed com-
bination improved over the combination of linguistic
or n-gram based measures alone (Corston-Oliver et
al., 2001). (Pado? et al, 2009) reported a reliability
improvement by including measures based on tex-
tual entailment in the set. In (Gime?nez and Ma`rquez,
2008), a simple arithmetic mean of scores for com-
bining measures at different linguistic levels was ap-
plied with remarkable results in recent shared evalu-
ation tasks (Callison-Burch et al, 2010).
2.3 Meta-evaluation criteria
Meta-evaluation methods have been gradually intro-
duced together with evaluation measures. For in-
stance, Papineni et al (2001b) evaluated the reliabil-
ity of the BLEU metric according to its ability to em-
ulate human assessors, as measured in terms of Pear-
son correlation with human assessments of adequacy
and fluency at the document level. The measure
NIST (Doddington, 2002) was meta-evaluated also
in terms of correlation with human assessments, but
over different document sources and for a varying
number of references and segment sizes. Melamed
et al (2003) argued, at the time of introducing the
GTM metric, that Pearson correlation coefficients
can be affected by scale properties. They suggested
using the non-parametric Spearman correlation co-
efficients instead. Lin and Och meta-evaluated
ROUGE over both Pearson and Spearman correla-
tion over a wide set of metrics, including NIST,
WER, PER, and variants of ROUGE, BLEU and
GTM. They obtained similar results in both cases
(Lin and Och, 2004a). Banerjee and Lavie (2005)
argued that the reliability of metrics at the document
level can be due to averaging effects but might not
be robust across sentence translations. In order to
address this issue, they computed the translation-by-
translation correlation with human assessments (i.e.,
correlation at the sentence level).
However, correlation with human judgements is
not enough to determine the reliability of measures.
First, correlation at sentence level (unlike correla-
tion at system level) tends to be low and difficult to
interpret. Second, correlation at system and segment
levels can produce contradictory results. In (Amigo?
et al, 2009) it is observed that higher linguistic lev-
els in measures increases the correlation with human
judgements at the system level at the cost of corre-
lation at the segment level. As far as we know, a
clear explanation for these phenomena has not been
provided yet.
Third, a high correlation at system level does
not ensure a high reliability. Culy and Rieheman
observed that, although BLEU can achieve a high
correlation at system level in some test suites, it
over-scores a poor automatic translation of ?Tom
Sawyer? against a human produced translation (Culy
and Riehemann, 2003). This meta-evaluation crite-
rion based on the ability to discern between man-
ual and automatic translations have been referred to
as human likeness (Amigo? et al, 2006), in contrast
to correlation with human judgements which is re-
ferred to as human acceptability. Examples of meta-
measures based on this criterion are ORANGE (Lin
and Och, 2004b) and KING (Amigo? et al, 2005).
In addition, many of the approaches to metric com-
bination described in Section 2.2 take human like-
ness as the optimization criterion (Corston-Oliver
et al, 2001; Kulesza and Shieber, 2004; Gamon et
al., 2005). The main advantage of meta-evaluation
based on human likeness is that, since human as-
sessments are not required, metrics can be evaluated
over larger test beds. However, the meta-evaluation
in terms of human likeness is difficult to interpret.
2.4 The use of evaluation measures
In general, the state of the art includes a wide set
of results that show the drawbacks of n-gram based
measures as BLEU, and a wide set of proposals for
new single and combined measures which are meta-
457
evaluated in terms of human acceptability (i.e., their
ability to emulate human judges, typically measured
in terms of correlation with human judgements) or
human-likeness (i.e., their ability to discern between
automatic and human translations) (Amigo? et al,
2006). However, the original measures BLEU and
ROUGE are still preferred.
We believe that one of the reasons is the lack of
an in-depth study on to what extent providing ad-
ditional evaluation results with other metrics con-
tributes to the reliability of such results. The state of
the art suggests that the use of heterogeneous mea-
sures can improve the evaluation reliability. How-
ever, as far as we know, there is no comprehen-
sive analysis on the contribution of novel measures
when corroborating evaluation results with addi-
tional measures.
3 Similarity Based Evaluation Measures
In general, automatic evaluation measures applied
in tasks like MT or AS are similarity measures be-
tween system outputs and human references. These
measures are related with precision, recall or overlap
over specific types of linguistic units. For instance,
ROUGE measures n-gram recall. Other measures
that work at higher linguistic levels apply precision,
recall or overlap of linguistic components such as
dependency relations, grammatical categories, se-
mantic roles, etc.
In order to delimit our hypothesis, let us first de-
fine what is a similarity measure in this context. Un-
fortunately, as far as we know, there is no formal
concept covering the properties of current evaluation
similarity measures. A close concept is that of ?met-
ric? or ?distance function?. But, actually, measures
such as ROUGE or BLEU are not proper ?metrics?,
because they do not satisfy the symmetry and the tri-
angle inequality properties. Therefore, we need a
new definition.
Being ? the universe of system outputs s and
gold-standards g, we assume that a similarity mea-
sure, in our context, is a function x : ?2 ?? < such
that there exists a decomposition function f : ? ??
{e1..en} (e.g., words or other linguistic units or
relationships) satisfying the following constraints:
(i) maximum similarity is achieved only when then
the decomposition of the system output resembles
exactly the gold-standard decomposition; and (ii)
growing overlap or removing non overlapped ele-
ments implies growing x. Formally, if x ranges from
0 to 1:
f(s) = f(g)? x(s, g) = 1
(f(s) = f(s?) ? {e ? f(g) \ f(s?)})? x(s, g) > x(s?, g)
(f(s) = f(s?)? {e ? f(s?) \ f(g)})? x(s, g) > x(s?, g)
For instance, a random function and the reversal
of a similarity funtion (f ?(s) = 1f(s) ) do not satisfythese constraints. While the F measure over Pre-
cision and Recall satisfies these constraints1, pre-
cision and recall in isolation do not satisfy all of
them: maximum recall can be achieved without re-
sembling the goldstandard text decomposition; and
maximum precision can be achieved with only a few
overlapped elements.
BLEU (Papineni et al, 2001a) computes the n-
gram precision while the metric ROUGE (Lin and
Och, 2004a) computes the n-gram recall. How-
ever, in general, both metrics satisfy all the con-
straints, given that BLEU includes a brevity penalty
and ROUGE penalizes or limits the system output
length. The measure METEOR creates an align-
ment between the two strings (Banerjee and Lavie,
2005). This overlap-based measure satisfies also the
previous constraints. Measures based on edit dis-
tance over n-grams (Tillmann et al, 1997; Nie?en
et al, 2000) or other linguistic units (Akiba et al,
2001; Popovic and Ney, 2007) match also our def-
inition of similarity measure. The editing distance
is minimum when the two compared text are equal.
The more the evaluated text contains elements from
the gold-standard the more the editing distance is re-
duced (higher similarity). The word ordering can be
also expressed in terms of a decomposition function.
A similar reasoning applies to every relevant mea-
sure in the state-of-the art.
4 Data Sets and Measures
4.1 Data sets
In this paper, we provide empirical results for
MT and AS. For MT, we use the data sets from
the Arabic-to-English (AE) and Chinese-to-English
(CE) NIST MT Evaluation campaigns in 2004 and
1There is an exception. In an extreme case, when recall is
zero, removing non overlapped elements does not modify the F
measure.
458
AE2004 CE2004 AE2005 CE2005
#human-references 5 5 5 4
#systems 5 10 7 10
#system-outputs-assessed 5 10 6 5
#system-outputs 1,353 1,788 1,056 1,082
#outputs-assessed per-system 347 447 266 272
Table 1: Description of the test beds from 2004 and 2005 NIST MT evaluation campaigns used in the experiments
throughout the paper.
DUC 2005 DUC 2006
#human-references 3-4 3-4
#systems 32 35
#system-outputs-assessed 32 35
#system-outputs 50 50
#outputs-assessed per-system 50 50
Table 2: Description of the test beds from 2005 and 2006 DUC evaluation campaigns used in the experiments through-
out the paper.
20052. Both include two translations exercises: for
the 2005 campaign we contacted each participant
individually and asked for permission to use their
data3. In our experiments, we take the sum of ad-
equacy and fluency, both in a 1-5 scale, as a global
measure of quality (LDC, 2005). Thus, human as-
sessments are in a 2-10 scale. For AS, we have used
the AS test suites developed in the DUC 2005 and
DUC 2006 evaluation campaigns4. This AS task
was to generate a question focused summary of 250
words from a set of 25-50 documents to a complex
question. Summaries were evaluated according to
several criteria. Here, we will consider the respon-
siveness judgements, in which the quality score was
an integer between 1 and 5. See Tables 1 and 2 for a
brief quantitative description of these test beds.
2http://www.nist.gov/speech/tests/mt
3We are grateful to a number of groups and companies who
responded positively: University of Southern California Infor-
mation Sciences Institute (ISI), University of Maryland (UMD),
Johns Hopkins University & University of Cambridge (JHU-
CU), IBM, University of Edinburgh, University of Aachen
(RWTH), National Research Council of Canada (NRC), Chi-
nese Academy of Sciences Institute of Computing Technology
(ICT), Instituto Trentino di Cultura - Centro per la Ricerca Sci-
entifica e Tecnologica(ITC-IRST), MITRE.
4http://duc.nist.gov/
4.2 Measures
As for evaluation measures, for MT we have used a
rich set of 64 measures provided within the ASIYA
Toolkit (Gime?nez and Ma`rquez, 2010)5. This in-
cludes measures operating at different linguistic lev-
els: lexical, syntactic, and semantic. At the lexical
level this set includes variants of 8 measures em-
ployed in the state of the art: BLEU, NIST, GTM,
METEOR, ROUGE, WER, PER and TER. In addi-
tion, we have included a basic measure Ol that com-
putes the lexical overlap without considering word
ordering. All these measures have similar granular-
ity. They use n-grams of a varying length as the ba-
sic unit with additional information provided by lin-
guistic tools. The underlying similarity criteria in-
clude precision, recall, overlap, or edit rate, and the
decomposition functions include words, dependency
tree nodes (DP HWC, DP-Or, etc.), constituency
parsing (CP-STM), discourse roles (DR-Or), seman-
tic roles (SR-Or), named entities, etc. Further details
on the measure set may be found in the ASIYA tech-
nical manual (Gime?nez and Ma`rquez, 2010).
According to our computations, our measures
cover high and low correlations at both levels. Cor-
relation at system level spans between 0.63 and 0.95.
Correlations at sentence level ranges from 0.18 up to
0.54. We will discriminate between two subsets of
5http://www.lsi.upc.edu/?nlp/Asiya
459
measures. The first one includes those that decom-
pose the text into words, n-grams, stems or lexical
semantic tags. This set includes BLEU, ROUGE,
NIST, GTM, PER and WER families. We will re-
fer to them as ?lexical? measures. The second set
are those that consider deeper linguistic levels such
as parts of speech, syntactic dependencies, syntactic
constituents, etc. We will refer to them as ?linguis-
tic? measures.
In the case of automatic summarization (AS), we
have employed the standard variants of ROUGE
(Lin, 2004). These 7 measures are ROUGE-{1..4},
ROUGE-SU, ROUGE-L and ROUGE-W. In addi-
tion we have included the reversed precision version
for each variant and the F measure of both. Notice
that the original ROUGE measures are oriented to
recall. In total, we have 21 measures for the sum-
marization task. All of them are based on n-gram
overlap.
5 Additive reliability
As discussed in Section 2, a number of recent pub-
lications address the problem of measure combi-
nation with successful results, specially when het-
erogeneous measures are combined. The following
property clarifies this issue and justifies the use of
heterogeneous measures when corroborating evalu-
ation results. It asserts that the reliability of system
improvements always increases when the evaluation
result is corroborated by an additional similarity
measure, regardless of the correlation achieved by
the additional measure in isolation.
For the sake of clarity, in the rest of the paper,
we will denote the similarity x(s, g) between sys-
tem output s and human reference g by x(s). The
quality of a system output s will be referred to as
Q(s). Let us define the reliability R(X) of a mea-
sure set as the probability of a real improvement (as
measured by human judges) when a score improve-
ment is observed simultaneously for all measures in
the set X. :
R(X) ? P (Q(s) ? Q(s?)|x(s) ? x(s?) ?x ? X)
According to this definition, we may not be able
to predict the quality of any system output (i.e. a
translation) with a highly reliable measure set, but
we can ensure a system improvement when all mea-
sures corroborate the result. Then the additive relia-
bility property can be stated as:
R(X ? {x}) ? R(X)
We could think of violating this property by
adding, for instance, a measure consisting of a ran-
dom function (x?(s) = rand(0..1)) or a reversal of
the original measure (x?(s) = 1/x(s)). These kind
of measures, however, would not satisfy the con-
straints defined in Section 3.
This property is based on the idea that similar-
ity with human references according to any aspect
should not imply statistically a quality decrease. Al-
though our test suites includes measures with low
correlation at segment and system level, we can con-
firm empirically that all of them satisfy this property.
We have developed the following experiment:
taking all possible measure pairs in the test suites,
we have compared their reliability as a set versus the
maximal reliability of any of them (by computing
the difference R(X)?max(R(x1), R(x2)). Figure
1 shows the obtained distribution of this difference
for our MT and AS test suites. Remarkably, in al-
most every case this difference is positive.
This result has a key implication: Corroborating
evaluation results with a new measure, even when
it has lower correlation with human judgements, in-
creases the reliability of results. Therefore, if the
correlation with judgements is not determinant, the
question is now what factor determines the contri-
bution of the new measures. According to the fol-
lowing property, this factor is the heterogeneity of
measures.
6 Heterogeneity
This property states that the reliability of any mea-
sure combination is lower bounded by the hetero-
geneity of the measure set. In other words, a single
measure can be more or less reliable, but a system
improvement according to all measures in an het-
erogeneous set is reliable.
Let us define the heterogeneity H(X) of a set of
measures X as, given two system outputs s and s?
such that g 6= s 6= s? 6= g (g is the reference
text), the probability that there exist two measures
that contradict each other. That is:
H(X) ? P (?x, x? ? X.x(s) > x(s?) ? x?(s) < x?(s?))
460
Figure 1: Additive reliability for metric pairs.
Thus, given a set X of measures, the property
states that there exists a strict growing function F
such that:
R(X) ? F (H(X)) and H(X) = 1? R(X) = 1
In other words, the more the similarity measures
tend to contradict each other, the more a unanimous
improvement over all similarity measures is reliable.
Clearly, the harder it is that measures agree, the more
meaningful it is when they do.
The first part is derived from the Additive Re-
liability property. Intuitively, any individual mea-
sure has zero heterogeneity. Increasing the hetero-
geneity implies joining measures or measure sets
progressively. According to the Additive Reliabil-
ity property, this joining implies a reliability in-
crease. Therefore, the higher the heterogeneity, the
higher the minimum Reliability achieved by the cor-
responding measure sets.
The second part is derived from the Heterogeneity
definition. If H(X) = 1 then, for any distinct pair
of outputs that differ from the reference, there exist
at least two measures in the set contradicting each
other. That is, H(X) = 1 implies that:
?s 6= s? 6= g(?x, x? ? X.x(s) > x(s?) ? x?(s) < x?(s?))
Therefore, if one output improves the other ac-
cording to all measures, then the output must be
equal than the reference.
?(?x, x? ? X.x(s) > x(s?) ? x?(s) < x?(s?))?
Figure 2: Heterogeneity vs. reliability in MT test suites.
?(g 6= s 6= s? 6= g)? g = s ? g = s?
According to the first constraint of similarity mea-
sures, a text that is equal to the reference achieves
the maximum score:
g = s? f(g) = g(s)? ?x.x(s) ? x(s?)
Finally, if we assume that the reference (human pro-
duced texts) has a maximum quality, then it will
have equal or higher quality than the other output.
g = s? Q(s) ? Q(s?)
Therefore, the reliability of the measure set is maxi-
mal. In summary, if H(X) = 1 then:
R(X) = P (Q(s) ? Q(s?)|x(s) ? x(s?) ?x ? X) =
= P (Q(s) ? Q(s?)|s = g) = 1
Figures 2 and 3 show the relationship between the
heterogeneity of randomly selected measure sets and
their reliability for the MT and summarization test
suites. As the figures show, the higher the hetero-
geneity, the higher the reliability of the measure set.
The results in AS are less pronounced due to the re-
dundancy in ROUGE measure.
Notice that the heterogeneity property does not
necessarily imply a high correlation between reli-
ability and heterogeneity. For instance, an ideal
single measure would have zero heterogeneity and
461
Figure 3: Heterogeneity vs. reliability in summarization
test suites.
achieve maximum reliability, appearing in the top
left area. The property rather brings us to the fol-
lowing situation: let us suppose that we have a set
of single measures available which achieve a certain
range of reliability. We can improve our system ac-
cording to any of these measures. Without human
assessments, we do not know what is the most re-
liable measure. But if we combine them, increas-
ing the heterogeneity, the minimal reliability of the
selected measures will be higher. This implies that
combining heterogeneous measures (e.g. at high lin-
guistic levels) that do not achieve high correlation
in isolation, is better than corroborating results with
any individual measure alone, such as ROUGE and
BLEU, which is the common practice in the state of
the art.
The main drawback of this property is that in-
creasing the heterogeneity implies a sensitivity re-
duction. For instance, if H(X) = 0.9, then only
for 10% of output pairs in the corpus there exists
an improvement according to all measures. In other
words, unanimous evaluation results from heteroge-
neous measures are reliable but harder to achieve for
the system developer. The next section investigates
on this issue.
Finally, Figure 4 shows that linguistic measures
increase the heterogeneity of measure sets. We have
generated sets of metrics of size 1 to 10 made up
by lexical or lexical and linguistic metrics. As the
figure shows, in the second case, the measure sets
achieve a higher heterogeneity.
Figure 4: Heterogeneity of lexical measures vs. lexical
and linguistic measures.
7 Score thresholds vs. Additive Reliability
According to the previous properties, corroborating
evaluation results with several measures increases
the reliability of evaluation results at the cost of sen-
sitivity. On the other hand, increasing the score
threshold of a single measure should have a similar
effect. Which is then the best methodology to im-
prove reliability? In this section we provide exper-
imental evidence on the relationship between both
ways of increasing reliability: we have found that,
corroborating evaluation results over single texts
with additional measures is more reliable than re-
quiring higher score differences according to any in-
dividual measure in the set. More specifically, we
have found that the reliability of a measure set is
higher than the reliability of each of the individual
measures at a similar level of sensitivity.
Formally, we define the sensitivity S(X) of a met-
ric set X as the probability of finding a score im-
provement within text pairs with a real (i.e. human
assessed) quality improvement:
S(X) = P (x(s) ? x(s?)?x ? X|Q(s) ? Q(s?))
Being Rth(x) and Sth(x) the reliability and sen-
sitivity of a single measure x for a certain increase
score threshold th:
462
Figure 5: Heterogeneity vs. reliability Gain for MT test
suites.
Rth(x) = P (Q(s) ? Q(s?)|x(s)? x(s?) ? th)
Sth(x) = P (x(s)? x(s?) ? th|Q(s) ? Q(s?))
The property that we want to check is that, at the
same sensitivity level, combining measures is more
reliable than increasing the score threshold of single
measures:
S(X) = Sth(x).x ? X ?? R(X) ? Rth(x)
Note that if we had a perfect measure xp such that
R(xp) = S(xp) = 1, then combining this measure
with a low reliability measure xl would produce a
lower sensitivity, but the maximal reliability would
be preserved.
In order to confirm empirically this property, we
have developed the following experiment: (i) We
compute the reliability and sensitivity of randomly
chosen measure sets over single text pairs. We have
generated sets of 2,3,5,10,20 and 40 measures. In
the case of summarization corpora we have com-
bined up to 20 measures. In addition, we com-
pute also the heterogeneity H(X) of each measure
set; (ii) Experimenting with different values for the
threshold th, we compute the reliability of single
measures for all potential sensitivity levels; (iii) For
each measure set, we compare the reliability of the
measure set versus the reliability of single measures
at the same sensitivity level. We will refer to this as
the Reliability Gain:
Figure 6: Heterogeneity vs. reliability Gain for MT test
suites.
Reliability Gain =
R(X)?max{Rth(x)/x ? X ? Sth(x) = S(X)}
If there are several reliability values with the same
sensitivity for a given single measures, we choose
the highest reliability value for the single measure.
Figures 5 and 6 illustrate the results for the MT
and AS corpora. The horizontal axis represents the
Heterogeneity of measure sets, while the vertical
axis represents the reliability gain. Remarkably, the
reliability gain is positive for all cases in our test
suites. The maximum reliability gain is 0.34 in the
case of MT and 0.08 for AS (note that summariza-
tion measures are more redundant in our corpora).
In both test suites, the largest information gains are
obtained with highly heterogeneous measure sets.
In summary, given comparable measures in terms
of reliability, corroborating evaluation results with
several measures is more effective than optimizing
systems according to the best measure in the set.
This empirical property provides an additional ev-
idence in favour of the use of heterogeneous mea-
sures and, in particular, of the use of linguistic mea-
sures in combination with standard lexical measures.
8 Conclusions
In this paper, we have analyzed the state of the art in
order to clarify why novel text evaluation measures
463
are not exploited by the community. Our first con-
clusion is that it is not easy to determine the reliabil-
ity of measures, which is highly corpus-dependent
and often contradictory when comparing correlation
with human judgements at segment vs. system lev-
els.
In order to tackle this issue, we have studied a
number of properties that suggest the convenience of
using heterogeneous measures to corroborate eval-
uation results. According to these properties, we
can ensure that, even when if we can not determine
the reliability of individual measures, corroborating
a system improvement with additional measures al-
ways increases the reliability of the results. In ad-
dition, the more heterogeneous the measures em-
ployed (which is measurable), the higher the relia-
bility of the results. But perhaps the most impor-
tant practical finding is that the reliability at similar
sensitivity levels by corroborating evaluation results
with several measures is always higher than improv-
ing systems according to any of the combined mea-
sures in isolation.
These properties point to the practical advantages
of considering linguistic knowledge (beyond lexi-
cal information) in measures, even if they do not
achieve a high correlation with human judgements.
Our experiments show that linguistic knowledge in-
creases the heterogeneity of measure sets, which
in turn increases the reliability of evaluation results
when corroborating system comparisons with sev-
eral measures.
Acknowledgements
This work has been partially funded by the Spanish
Government (Holopedia, TIN2010-21128-C02 and
OpenMT-2, TIN2009-14675-C03) and the Euro-
pean Community?s Seventh Framework Programme
(FP7/2007-2013) under grant agreement number
247762 (FAUST project, FP7-ICT-2009-4-247762).
References
Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita.
2001. Using Multiple Edit Distances to Automatically
Rank Machine Translation Output. In Proceedings of
Machine Translation Summit VIII, pages 15?20.
Joshua Albrecht and Rebecca Hwa. 2007a. A Re-
examination of Machine Learning Approaches for
Sentence-Level MT Evaluation. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 880?887.
Joshua Albrecht and Rebecca Hwa. 2007b. Regression
for Sentence-Level MT Evaluation with Pseudo Refer-
ences. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 296?303.
Enrique Amigo?, Julio Gonzalo, Anselmo Pe nas, and Fe-
lisa Verdejo. 2005. QARLA: a Framework for the
Evaluation of Automatic Summarization. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 280?289.
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and Llu??s
Ma`rquez. 2006. MT Evaluation: Human-Like vs. Hu-
man Acceptable. In Proceedings of the Joint 21st In-
ternational Conference on Computational Linguistics
and the 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL), pages 17?
24.
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and Fe-
lisa Verdejo. 2009. The contribution of linguis-
tic features to automatic machine translation evalua-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 1 - Volume 1, ACL ?09,
pages 306?314, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization.
Chris Callison-burch and Miles Osborne. 2006. Re-
evaluating the role of bleu in machine translation re-
search. In In EACL, pages 249?256.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 70?106.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009 work-
shop on statistical machine translation. In Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53. Revised August 2010.
464
Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM:
A maximum similarity metric for machine translation
evaluation. In Proceedings of ACL-08: HLT, pages
55?62.
Simon Corston-Oliver, Michael Gamon, and Chris
Brockett. 2001. A Machine Learning Approach to the
Automatic Evaluation of Machine Translation. In Pro-
ceedings of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 140?147.
Christopher Culy and Susanne Z. Riehemann. 2003. The
Limits of N-gram Translation Evaluation Metrics. In
Proceedings of MT-SUMMIT IX, pages 1?8.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd Inter-
national Conference on Human Language Technology,
pages 138?145.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-Level MT evaluation without refer-
ence translations: beyond language modeling. In Pro-
ceedings of EAMT, pages 103?111.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Linguistic
Features for Automatic Evaluation of Heterogeneous
MT Systems. In Proceedings of the ACL Workshop on
Statistical Machine Translation, pages 256?264.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008. Hetero-
geneous Automatic MT Evaluation Through Non-
Parametric Metric Combinations. In Proceedings of
the Third International Joint Conference on Natural
Language Processing (IJCNLP), pages 319?326.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathemat-
ical Linguistics, 1(94):77?86.
Jesu?s Gime?nez. 2008. Empirical Machine Transla-
tion and its Evaluation. Ph.D. thesis, Universitat
Polite`cnica de Catalunya.
Tsutomu Hirao, Manabu Okumura, and Hideki Isozaki.
2005. Kernel-based approach for automatic evaluation
of natural language generation technologies: Applica-
tion to automatic summarization. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 145?152, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
Petr Homola, Vladislav Kubon?, and Pavel Pecina. 2009.
A simple automatic mt evaluation metric. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, StatMT ?09, pages 33?36, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Jeremy G. Kahn, Matthew Snover, and Mari Ostendorf.
2009. Expected Dependency Pair Match: Predicting
translation quality with expected syntactic structure.
Machine Translation.
Alex Kulesza and Stuart M. Shieber. 2004. A learning
approach to improving sentence-level MT evaluation.
In Proceedings of the 10th International Conference
on Theoretical and Methodological Issues in Machine
Translation (TMI), pages 75?84.
LDC. 2005. Linguistic Data Annotation Spec-
ification: Assessment of Adequacy and Flu-
ency in Translations. Revision 1.5. Tech-
nical report, Linguistic Data Consortium.
http://www.ldc.upenn.edu/Projects/
TIDES/Translation/TransAssess04.pdf.
Audrey Le and Mark Przybocki. 2005. NIST 2005 ma-
chine translation evaluation official results. In Official
release of automatic evaluation scores for all submis-
sions, August.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006.
CDER: Efficient MT Evaluation Using Block Move-
ments. In Proceedings of 11th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics (EACL), pages 241?248.
Chin-Yew Lin and Franz Josef Och. 2004a. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statics. In Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics (ACL).
Chin-Yew Lin and Franz Josef Och. 2004b. ORANGE: a
Method for Evaluating Automatic Evaluation Metrics
for Machine Translation. In Proceedings of the 20th
International Conference on Computational Linguis-
tics (COLING).
Chin-Yew Lin. 2004. Rouge: A Package for Auto-
matic Evaluation of Summaries. In Marie-Francine
Moens and Stan Szpakowicz, editors, Text Summariza-
tion Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
Lucian Vlad Lita, Monica Rogati, and Alon Lavie. 2005.
BLANC: Learning Evaluation Metrics for MT. In
Proceedings of the Joint Conference on Human Lan-
guage Technology and Empirical Methods in Natural
Language Processing (HLT-EMNLP), pages 740?747.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of ACL Workshop on Intrinsic and Extrinsic Eval-
uation Measures for MT and/or Summarization, pages
25?32.
Ding Liu and Daniel Gildea. 2006. Stochastic Iter-
ative Alignment for Machine Translation Evaluation.
In Proceedings of the Joint 21st International Confer-
ence on Computational Linguistics and the 44th An-
nual Meeting of the Association for Computational
Linguistics (COLING-ACL), pages 539?546.
465
Ding Liu and Daniel Gildea. 2007. Source-Language
Features and Maximum Correlation Training for Ma-
chine Translation Evaluation. In Proceedings of the
2007 Meeting of the North American Chapter of the
Association for Computational Linguistics (NAACL),
pages 41?48.
Dennis Mehay and Chris Brew. 2007. BLEUATRE:
Flattening Syntactic Dependencies for MT Evaluation.
In Proceedings of the 11th Conference on Theoreti-
cal and Methodological Issues in Machine Translation
(TMI).
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Translation. In
Proceedings of the Joint Conference on Human Lan-
guage Technology and the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL).
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and Her-
mann Ney. 2000. An Evaluation Tool for Machine
Translation: Fast Evaluation for MT Research. In Pro-
ceedings of the 2nd International Conference on Lan-
guage Resources and Evaluation (LREC).
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007a. Dependency-Based Automatic Evalua-
tion for Machine Translation. In Proceedings of SSST,
NAACL-HLT/AMTA Workshop on Syntax and Struc-
ture in Statistical Translation, pages 80?87.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2007b. Labelled Dependencies in Machine Transla-
tion Evaluation. In Proceedings of the ACL Workshop
on Statistical Machine Translation, pages 104?111.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2008. Evaluating machine translation with lfg depen-
dencies. Machine Translation, 21(2):95?119.
Karolina Owczarzak. 2009. Depeval(summ):
dependency-based evaluation for automatic sum-
maries. In ACL-IJCNLP ?09: Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1, pages
190?198, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Sebastian Pado?, Michael Galley, Dan Jurafsky, and
Christopher D. Manning. 2009. Robust machine
translation evaluation with entailment features. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 297?305.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001a.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311?318, Philadelphia, jul.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001b. Bleu: a method for automatic evalu-
ation of machine translation, RC22176. Technical re-
port, IBM T.J. Watson Research Center.
Michael Paul, Andrew Finch, and Eiichiro Sumita. 2007.
Reducing Human Assessments of Machine Transla-
tion Quality to Binary Classifiers. In Proceedings of
the 11th Conference on Theoretical and Methodologi-
cal Issues in Machine Translation (TMI).
Maja Popovic and Hermann Ney. 2007. Word Error
Rates: Decomposition over POS classes and Applica-
tions for Error Analysis. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
48?55, Prague, Czech Republic, June. Association for
Computational Linguistics.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency Treelet Translation: Syntactically Informed
Phrasal SMT. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 271?279.
Chris Quirk. 2004. Training a Sentence-Level Machine
Translation Confidence Metric. In Proceedings of the
4th International Conference on Language Resources
and Evaluation (LREC), pages 825?828.
Florence Reeder, Keith Miller, Jennifer Doyon, and John
White. 2001. The Naming of Things and the Confu-
sion of Tongues: an MT Metric. In Proceedings of
the Workshop on MT Evaluation ?Who did what to
whom?? at Machine Translation Summit VIII, pages
55?59.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas
(AMTA), pages 223?231.
Christoph Tillmann, Stefan Vogel, Hermann Ney, A. Zu-
biaga, and H. Sawaf. 1997. Accelerated DP based
Search for Statistical Translation. In Proceedings of
European Conference on Speech Communication and
Technology.
Stephen Tratz and Eduard Hovy. 2008. Summarization
evaluation using transformed basic elements. In In
Proceedings of TAC-08. Gaithersburg, Maryland.
Joseph Turian, Luke Shen, and I. Dan Melamed. 2003a.
Evaluation of machine translation and its evaluation.
In In Proceedings of MT Summit IX, pages 386?393.
Joseph P. Turian, Luke Shen, and I. Dan Melamed.
2003b. Evaluation of Machine Translation and its
Evaluation. In Proceedings of MT SUMMIT IX.
466
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 454?460,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UNED: Evaluating Text Similarity Measures without Human Assessments?
Enrique Amigo? ? Julio Gonzalo ? Jesu?s Gime?nez ? Felisa Verdejo?
? UNED, Madrid
{enrique,julio,felisa}@lsi.uned.es
? Google, Dublin
jesgim@gmail.com
Abstract
This paper describes the participation of
UNED NLP group in the SEMEVAL 2012 Se-
mantic Textual Similarity task. Our contribu-
tion consists of an unsupervised method, Het-
erogeneity Based Ranking (HBR), to combine
similarity measures. Our runs focus on com-
bining standard similarity measures for Ma-
chine Translation. The Pearson correlation
achieved is outperformed by other systems,
due to the limitation of MT evaluation mea-
sures in the context of this task. However,
the combination of system outputs that partici-
pated in the campaign produces three interest-
ing results: (i) Combining all systems without
considering any kind of human assessments
achieve a similar performance than the best
peers in all test corpora, (ii) combining the 40
less reliable peers in the evaluation campaign
achieves similar results; and (iii) the correla-
tion between peers and HBR predicts, with a
0.94 correlation, the performance of measures
according to human assessments.
1 Introduction
Imagine that we are interested in developing com-
putable measures that estimate the semantic simi-
larity between two sentences. This is the focus of
the STS workshop in which this paper is presented.
In order to optimize the approaches, the organizers
?This work has been partially funded by the Madrid gov-
ernment, grant MA2VICMR (S-2009/TIC- 1542), the Span-
ish government, grant Holopedia (TIN2010-21128-C02-01) and
the European Community?s Seventh Framework Programme
(FP7/ 2007-2013) under grant agreement nr. 288024 (LiMo-
SINe project).
provide a training corpus with human assessments.
The participants must improve their approaches and
select three runs to participate. Unfortunately, we
can not ensure that systems will behave similarly
in both the training and test corpora. For instance,
some Pearson correlations between system achieve-
ments across test corpora in this competition are:
0.61 (MSRpar-MSRvid), 0.34 (MSRvid-SMTeur),
or 0.49 (MSRpar-SMTeur). Therefore, we cannot
expect a high correlation between the system per-
formance in a specific corpus and the test corpora
employed in the competition.
Now, imagine that we have a magic box that,
given a set of similarity measures, is able to pre-
dict which measures will obtain the highest corre-
lation with human assessments without actually re-
quiring those assessments. For instance, suppose
that putting all system outputs in the magic box, we
obtain a 0.94 Pearson correlation between the pre-
diction and the system achievements according to
human assessments, as in Figure 1. The horizontal
axis represents the magic box ouput, and the vertical
axis represents the achievement in the competition.
Each dot represents one system. In this case, we
could decide which system or system combination
to employ for a certain test set.
Is there something like this magic box? The
answer is yes. Indeed, what Figure 1 shows is
the results of an unsupervised method to combine
measures, the Heterogeneity Based Ranking (HBR).
This method is grounded on a generalization of the
heterogeneity property of text evaluation measures
proposed in (Amigo? et al, 2011), which states that
the more a set of measures is heterogeneous, the
454
Figure 1: Correspondence between the magic box infor-
mation and the (unknown) correlation with human assess-
ments, considering all runs in the evaluation campaign.
more a score increase according to all the mea-
sures is reliable. In brief, the HBR method consists
of computing the heterogeneity of the set of mea-
sures (systems) for which a similarity instance (pair
of texts) improves each of the rest of similarity in-
stances in comparison. The result is that HBR tends
to achieve a similar or higher correlation with human
assessments than the single measures. In order to
select the most appropriate single measure, we can
meta-evaluate measures in terms of correlation with
HBR, which is what the previous figure showed.
We participated in the STS evaluation campaign
employing HBR over automatic text evaluation mea-
sures (e.g. ROUGE (Lin, 2004)), which are not actu-
ally designed for this specific problem. For this rea-
son our results were suboptimal. However, accord-
ing to our experiments this method seem highly use-
ful for combining and evaluating current systems.
In this paper, we describe the HBR method and we
present experiments employing the rest of partici-
pant methods as similarity measures.
2 Definitions
2.1 Similarity measures
In (Amigo? et al, 2011) a novel definition of sim-
ilarity is proposed in the context of automatic text
evaluation measures. Here we extend the definition
for text similarity problems in general.
Being ? the universe of texts d, we assume that
a similarity measure, is a function x : ?2 ??
< such that there exists a decomposition function
f : ? ?? {e1..en} (e.g., words or other linguis-
tic units or relationships) satisfying the following
constraints; (i) maximum similarity is achieved only
when the text decomposition resembles exactly the
other text; (ii) adding one element from the second
text increases the similarity; and (iii) removing one
element that does not appear in the second text also
increases the similarity.
f(d1) = f(d2)? x(d1, d2) = 1
(f(d1) = f(d?1) ? {e ? f(d2) \ f(d1)})
? x(d?1, d2) > x(d1, d2)
(f(d1) = f(d?1)? {e ? f(d1) \ f(d2)})
? x(d?1, d2) > x(d1, d2)
According to this definition, a random function,
or the inverse of a similarity function (e.g. 1x(d1d2) ),
do not satisfy the similarity constraints, and there-
fore cannot be considered as similarity measures.
However, this definition covers any kind of overlap-
ping or precision/recall measure over words, syntac-
tic structures or semantic units, which is the case of
most systems here.
Our definition assumes that measures are granu-
lated: they decompose text in a certain amount of
elements (e.g. words, grammatical tags, etc.) which
are the basic representation and comparison units to
estimate textual similarity.
2.2 Heterogeneity
Heterogeneity (Amigo? et al, 2011) represents to
what extent a set of measures differ from each other.
Let us refer to a pair of texts i = (i1, i2) with a
certain degree of similarity to be computed as a sim-
ilarity instance. Then we estimate the Heterogene-
ity H(X ) of a set of similarity measures X as the
probability over similarity instances i = (i1, i2) and
j = (j1, j2) between distinct texts, that there exist
two measures in X that contradict each other. For-
mally:
H(X ) ? Pi1 6=i2
j1 6=j2
(?x, x? ? X|x(i) > x(j) ? x?(j) < x?(i))
where x(i) stands for the similarity, according to
measure x, between the texts i1, i2.
455
3 Proposal: Heterogeneity-Based
Similarity Ranking
The heterogeneity property of text evaluation mea-
sures (in fact, text similarity measures to human ref-
erences) introduced in (Amigo? et al, 2011) states
that the quality difference between two texts is lower
bounded by the heterogeneity of the set of evalua-
tion measures that corroborate the quality increase.
Based on this, we define the Heterogeneity Principle
which is applied to text similarity in general as: the
probability of a real similarity increase between ran-
dom text pairs is correlated with the Heterogeneity
of the set of measures that corroborate this increase:
P (h(i) ? h(j)) ? H({x|x(i) ? x(j)})
where h(i) is the similarity between i1, i2 accord-
ing to human assessments (gold standard). In addi-
tion, the probability is maximal if the heterogeneity
is maximal:
H({x|x(i) ? x(j)}) = 1? P (h(i) ? h(j)) = 1
The first part is derived from the fact that increas-
ing Heterogeneity requires additional diverse mea-
sures corroborating the similarity increase. The di-
rect relationship is the result of assuming that a sim-
ilarity increase according to any aspect is always a
positive evidence of true similarity. In other words,
a positive match between two texts according to any
feature can never be a negative evidence of similar-
ity.
As for the second part, if the heterogeneity of a
measure set is maximal, then the condition of the
heterogeneity definition holds for any pair of dis-
tinct documents (i1 6= i2 and j1 6= j2). Given that
all measures corroborate the similarity increase, the
heterogeneity condition does not hold. Then, the
compared texts in (i1, i2) are not different. There-
fore, we can ensure that P (h(i) ? h(j)) = 1.
The proposal in this paper consists of rank-
ing similarity instances by estimating, for each in-
stance i, the average probability of its texts (i1, i2)
being closer to each other than texts in a different
instance j:
R(i) = Avgj(P (h(i) ? h(j)))
Applying the heterogeneity principle we can esti-
mate this as:
HBRX (i) = Avgj(H({x|x(i) ? x(j)}))
We refer to this ranking function as the Heterogene-
ity Based Ranking (HBR). It satisfies three crucial
properties for a measure combining function:
1. HBR is independent from measure scales and
it does not require relative weighting schemes
between measures. Formally, being f any strict
growing function:
HBRx1..xn(i) = HBRx1..f(xn)(i)
2. HBR is not sensitive to redundant measures:
HBRx1..xn(i) = HBRx1..xn,xn(i)
3. Given a large enough set of similarity
instances, HBR is not sensitive to non-
informative measures. Being xr a random
function such that P (xr(i) > xr(j)) = 12 ,
then:
HBRx1..xn(i) ? HBRx1..xn,xr(i)
The first two properties are trivially satisfied: the
? operator in H and the score comparisons are not af-
fected by redundant measures nor their scales prop-
erties. Regarding the third property, the heterogene-
ity of a set of measures plus a random function xr
is:
H(X ? {xr}) ?
Pi1 6=i2
j1 6=j2
(?x, x? ? X ? {xr}|x(i) > x(j) ? x?(j) < x?(i)) =
H(X ) + (1?H(X )) ?
1
2
=
H(X ) + 1
2
That is, the heterogeneity grows proportionally
when including a random function. Assuming that
the random function corroborates the similarity in-
crease in a half of cases, the result is a proportional
relationship between HBR and HBR with the addi-
tional measure. Note that we need to assume a large
enough amount of data to avoid random effects.
456
4 Official Runs
We have applied the HBR method with excellent
results in different tasks such as Machine Transla-
tion and Summarization evaluation measures, Infor-
mation Retrieval and Document Clustering. How-
ever, we had not previously applied our method to
semantic similarity. Therefore, we decided to ap-
ply directly automatic evaluation measures for Ma-
chine Translation as single similarity measures to be
combined by means of HBR. We have used 64 auto-
matic evaluation measures provided by the ASIYA
Toolkit (Gime?nez and Ma`rquez, 2010)1. This set in-
cludes measures operating at different linguistic lev-
els (lexical, syntactic, and semantic) and includes all
popular measures (BLEU, NIST, GTM, METEOR,
ROUGE, etc.) The similarity formal constraints in
this set of measures is preserved by considering lex-
ical overlap when the target linguistic elements (i.e.
named entities) do not appear in the texts.
We participated with three runs. The first one con-
sisted of selecting the best measure according to hu-
man assessments in the training corpus. It was the
INIST measure (Doddington, 2002). The second run
consisted of selecting the best 34 measures in the
training corpus and combining them with HBR, and
the last run consisted of combining all evaluation
measures with HBR. The heterogeneity of measures
was computed over 1000 samples of similarity in-
stance pairs (pairs of sentences pairs) extracted from
the five test sets. Similarity instances were ranked
over each test set independently.
In essence, the main contribution of these runs is
to corroborate that Machine Translation evaluation
measures are not enough to solve this task. Our runs
appear at the Mean Rank positions 42, 28 and 77.
Apart of this, our results corroborate our main hy-
pothesis: without considering human assessment or
any kind of supervised tunning, combining the mea-
sures with HBR resembles the best measure (INIST)
in the combined measure set. However, when in-
cluding all measures the evaluation result decreases
(rank 77). The reason is that some Machine Trans-
lation evaluation measures do not represent a posi-
tive evidence of semantic similarity in this corpus.
Therefore, the HBR assumptions are not satisfied
and the final correlation achieved is lower. In sum-
1http://www.lsi.upc.edu/ nlp/Asiya
mary, our approach is suitable if we can ensure that
all measures (systems) combined are at least a posi-
tive (high or low) evidence of semantic similarity.
But let us focus on the HBR behavior when com-
bining participant measures, which are specifically
designed to address this problem.
5 Experiment with Participant Systems
5.1 Combining System Outputs
We can confirm empirically in the official results
that all participants runs are positive evidence of se-
mantic similarity. That is, they achieve a correlation
with human assessments higher than 0. Therefore,
the conditions to apply HBR are satisfied. Our goal
now is to resemble the best performance without ac-
cessing human assessments neither from the training
nor the test corpora. Figure 2 illustrates the Pear-
son correlation (averaged across test sets) achieved
by single measures (participants) and all peers com-
bined in an unsupervised manner by HBR (black
column). As the figure shows, HBR results are com-
parable with the best systems appearing in the ninth
position. In addition, Figure 4 shows the differences
over particular test sets between HBR and the best
system. The figure shows that there are not con-
sistent differences between these approaches across
test beds.
The next question is why HBR is not able to im-
prove the best system. Our intuition is that, in this
test set, average quality systems do not contribute
with additional information. That is, the similarity
aspects that the average quality systems are able to
capture are also captured by the best system.
However, the best system within the combined set
is not a theoretical upper bound for HBR. We can
prove it with the following experiment. We apply
HBR considering only the 40 less predictive systems
in the set (the rest of measures are not considered
when computing HBR). Then we compare the re-
sults of HBR regarding the considered single sys-
tems. As Figure 3 shows, HBR improves substan-
tially all single systems achieving the same result
than when combining all systems (0.61). The rea-
son is that all these systems are positive evidences
but they consider partial similarity aspects. But the
most important issue here is that combining the 40
less predictive systems in the evaluation campaign
457
Figure 2: Measures (runs) and HBR sorted by average correlation with human assessments.
Figure 3: 40 less predictive measures (runs) and HBR
sorted by average correlation with human assessments.
is enough to achieve high final scores. This means
that the drawback of these measures as a whole is
not what information is employed but how this in-
formation is scaled and combined. This drawback is
solved by the HBR approach.
In summary, the main conclusion that we can ex-
tract from these results is that, in the absence of hu-
man assessments, HBR ensures a high performance
without the risk derived from employing potentially
biased training corpora or measures based on partial
similarity aspects.
6 An Unsupervised Meta-evaluation
Method
But HBR has an important drawback: its computa-
tional cost, which isO(n4 ?m), being n the number
Figure 4: Average correlation with human assessments
for the best runs and HBR.
of texts involved in the computation and m the num-
ber of measures. The reason is that computing H is
quadratic with the number of texts, and the method
requires to compute H for every pair of texts. In
addition, HBR does not improve the best systems.
However, HBR can be employed as an unsuper-
vised evaluation method. For this, it is enough to
compute the Pearson correlation between runs and
HBR. This is what Figure 1 showed at the beginning
of this article. For each dot (participant run), the
horizontal axis represent the correlation with HBR
(magic box) and the vertical axis represent the cor-
relation with human assessments. This graph has a
Pearson correlation of 0.94 between both variables.
In other words, without accessing human assess-
ments, this method is able to predict the quality of
458
Figure 5: Predicting the quality of measures over a single test set.
textual similarity system with a 0.94 of accuracy in
this test bed.
In this point, we have two options for optimiz-
ing systems. First, we can optimize measures ac-
cording to the results achieved in an annotated train-
ing corpus. The other option consists of considering
the correlation with HBR in the test corpus. In or-
der to compare both approaches we have developed
the following experiment. Given a test corpus t, we
compute the correlation between system scores in t
versus a training corpus t?. This approach emulates
the scenario of training systems over a (training) set
and evaluating over a different (test) set. We also
compute the correlation between system scores in
all corpora vs. the scores in t. Finally, we compute
the correlation between system scores in t and our
predictor in t (which is the correlation system/HBR
across similarity instances in t). This approach em-
ulates the use of HBR as unsupervised optimization
method.
Figure 5 shows the results. The horizontal axis
represents the test set t. The black columns rep-
resent the prediction over HBR in the correspond-
ing test set. The grey columns represent the predic-
tion by using the average correlation across test sets.
The light grey columns represents the prediction us-
ing the correlation with humans in other single test
set. Given that there are five test sets, the figure in-
cludes four grey columns for each test set. The fig-
ure clearly shows the superiority of HBR as measure
quality predictor, even when it does not employ hu-
man assessments.
7 Conclusions
The Heterogeneity Based Ranking provides a mech-
anism to combine similarity measures (systems)
without considering human assessments. Interest-
ingly, the combined measure always improves or
achieves similar results than the best single measure
in the set. The main drawback is its computational
cost. However, the correlation between single mea-
sures and HBR predicts with a high confidence the
accuracy of measures regarding human assessments.
Therefore, HBR is a very useful tool when optimiz-
ing systems, specially when a representative training
corpus is not available. In addition, our results shed
some light on the contribution of measures to the
task. According to our experiments, the less reliable
measures as a whole can produce reliable results if
they are combined according to HBR.
The HBR software is available at
http://nlp.uned.es/?enrique/
References
Enrique Amigo?, Julio Gonzalo, Jesus Gimenez, and Fe-
lisa Verdejo. 2011. Corroborating text evaluation re-
sults with heterogeneous measures. In Proceedings of
the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 455?466, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd Inter-
459
national Conference on Human Language Technology,
pages 138?145.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathemat-
ical Linguistics, (94):77?86.
Chin-Yew Lin. 2004. Rouge: A Package for Auto-
matic Evaluation of Summaries. In Marie-Francine
Moens and Stan Szpakowicz, editors, Text Summariza-
tion Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
460
Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 36?43,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
The Heterogeneity Principle in Evaluation Measures for Automatic
Summarization?
Enrique Amigo? Julio Gonzalo Felisa Verdejo
UNED, Madrid
{enrique,julio,felisa}@lsi.uned.es
Abstract
The development of summarization systems
requires reliable similarity (evaluation) mea-
sures that compare system outputs with hu-
man references. A reliable measure should
have correspondence with human judgements.
However, the reliability of measures depends
on the test collection in which the measure
is meta-evaluated; for this reason, it has not
yet been possible to reliably establish which
are the best evaluation measures for automatic
summarization. In this paper, we propose
an unsupervised method called Heterogeneity-
Based Ranking (HBR) that combines summa-
rization evaluation measures without requiring
human assessments. Our empirical results in-
dicate that HBR achieves a similar correspon-
dence with human assessments than the best
single measure for every observed corpus. In
addition, HBR results are more robust across
topics than single measures.
1 Introduction
In general, automatic evaluation metrics for summa-
rization are similarity measures that compare system
outputs with human references. The typical develop-
ment cycle of a summarization system begins with
selecting the most predictive metric. For this, evalu-
ation metrics are compared to each other in terms
?This work has been partially funded by the Madrid gov-
ernment, grant MA2VICMR (S-2009/TIC- 1542), the Spanish
Government, grant Holopedia (TIN2010-21128-C02-01) and
the European Community?s Seventh Framework Programme
(FP7/ 2007-2013) under grant agreement nr. 288024 (LiMo-
SINe project).
of correlation with human judgements. The sec-
ond step consists of tuning the summarization sys-
tem (typically in several iterations) in order to maxi-
mize the scores according to the selected evaluation
measure.
There is a wide set of available measures beyond
the standard ROUGE: for instance, those comparing
basic linguistic elements (Hovy et al, 2005), depen-
dency triples (Owczarzak, 2009) or convolution ker-
nels (Hirao et al, 2005) which reported some relia-
bility improvement with respect to ROUGE in terms
of correlation with human judgements. However,
in practice ROUGE is still the preferred metric of
choice. The main reason is that the superiority of a
measure with respect to other is not easy to demon-
strate: the variability of results across corpora, ref-
erence judgements (Pyramid vs responsiveness) and
correlation criteria (system vs. summary level) is
substantial. In the absence of a clear quality crite-
rion, the de-facto standard is usually the most rea-
sonable choice.
In this paper we rethink the development cy-
cle of summarization systems. Given that the best
measure changes across evaluation scenarios, we
propose using multiple automatic evaluation mea-
sures, together with an unsupervised method to com-
bine measures called Heterogeneity Based Rank-
ing (HBR). This method is grounded on the gen-
eral Heterogeneity property proposed in (Amigo? et
al., 2011), which states that the more a measure
set is heterogeneous, the more a score increase ac-
cording to all the measures simultaneously is reli-
able. In brief, the HBR method consists of com-
puting the heterogeneity of measures for which a
36
system-produced summary improves each of the rest
of summaries in comparison.
Our empirical results indicate that HBR achieves
a similar correspondence with human assessments
than the best single measure for every observed cor-
pus. In addition, HBR results are more robust across
topics than single measures.
2 Definitions
We consider here the definition of similarity mea-
sure proposed in (Amigo? et al, 2011):
Being ? the universe of system outputs (sum-
maries) s and gold-standards (human references) g,
we assume that a similarity measure is a function
x : ?2 ?? < such that there exists a decompo-
sition function f : ? ?? {e1..en} (e.g., words
or other linguistic units or relationships) satisfying
the following constraints; (i) maximum similarity is
achieved only when the summary decomposition re-
sembles exactly the gold standard; (ii) adding one
element from the gold standard increases the simi-
larity; and (iii) removing one element that does not
appear in the gold standard also increases the simi-
larity. Formally:
f(s) = f(g)?? x(s, g) = 1
(f(s) = f(s?) ? {eg ? f(g) \ f(s)}) =?
x(s, g) > x(s?, g)
(f(s) = f(s?)? {e?g ? f(s) \ f(g)}) =?
x(s, g) > x(s?, g)
This definition excludes random functions, or the
inverse of any similarity function (e.g. 1f(s) ). It
covers, however, any overlapping or precision/recall
measure over words, n-grams, syntactic structures or
any kind of semantic unit. In the rest of the paper,
given that the gold standard g in summary evaluation
is usually fixed, we will simplify the notation saying
that x(s, g) ? x(s).
We consider also the definition of heterogeneity
of a measure set proposed in (Amigo? et al, 2011):
The heterogeneity H(X ) of a set of measures X is
defined as, given two summaries s and s? such that
g 6= s 6= s? 6= g (g is the reference text), the proba-
bility that there exists two measures that contradict
each other.
H(X ) ?
Ps,s? 6=g(?x, x
? ? X/x(s) > x(s?) ? x?(s) < x?(s?))
3 Proposal
The proposal in this paper is grounded on the hetero-
geneity property of evaluation measures introduced
in (Amigo? et al, 2011). This property establishes
a relationship between heterogeneity and reliability
of measures. However, this work does not provide
any method to evaluate and rank summaries given a
set of available automatic evaluation measures. We
now reformulate the heterogeneity property in order
to define a method to combine measures and rank
systems.
3.1 Heterogeneity Property Reformulation
The heterogeneity property of evaluation measures
introduced in (Amigo? et al, 2011) states that, as-
suming that measures are based on similarity to hu-
man references, the real quality difference between
two texts is lower bounded by the heterogeneity of
measures that corroborate the quality increase. We
reformulate this property in the following way:
Given a set of automatic evaluation measures
based on similarity to human references, the prob-
ability of a quality increase in summaries is corre-
lated with the heterogeneity of the set of measures
that corroborate this increase:
P (Q(s) ? Q(s?)) ? H({x|x(s) ? x(s?)})
where Q(s) is the quality of the summary s accord-
ing to human assessments. In addition, the proba-
bility is maximal if the heterogeneity is maximal:
H({x|x(s) ? x(s?)}) = 1? P (Q(s) ? Q(s?)) = 1
The first part is derived from the fact that
increasing heterogeneity requires additional di-
verse measures corroborating the similarity increase
(H({x|x(s) ? x(s?)}))). The correlation is the re-
sult of assuming that a similarity increase accord-
ing to any aspect is always a positive evidence of
true similarity to human references. In other words,
37
a positive match between the automatic summary
and the human references, according to any feature,
should never be a negative evidence of quality.
As for the second part, if the heterogeneity of a
measure set X is maximal, then the condition of
the heterogeneity definition (?x, x? ? X .x(s) >
x(s?) ? x?(s) < x?(s?)) holds for any pair of sum-
maries that are different from the human references.
Given that all measures in X corroborate the simi-
larity increase (X = {x|x(s) ? x(s?)}), the hetero-
geneity condition does not hold. Then, at least one
of the evaluated summaries is not different from the
human reference and we can ensure that P (Q(s) ?
Q(s?)) = 1.
3.2 The Heterogeneity Based Ranking
The main goal in summarization evaluation is rank-
ing systems according to their quality. This can be
seen as estimating, for each system-produced sum-
mary s, the average probability of being ?better?
than other summaries:
Rank(s) = Avgs?(P (Q(s) ? Q(s
?)))
Applying the reformulated heterogeneity property
we can estimate this as:
HBRX (s) = Avgs?(H({x|x(s) ? x(s
?)}))
We refer to this ranking function as the Heterogene-
ity Based Ranking (HBR). It satisfies three crucial
properties for a measure combining function. Note
that, assuming that any similarity measure over hu-
man references represents a positive evidence of
quality, the measure combining function must be
at least robust with respect to redundant or random
measures:
1. HBR is independent from measure scales and
it does not require relative weighting schemes
between measures. Formally, being f any strict
growing function:
HBRx1..xn(s) = HBRx1..f(xn)(s)
2. HBR is not sensitive to redundant measures:
HBRx1..xn(s) = HBRx1..xn,xn(s)
3. Given a large enough set of similarity
instances, HBR is not sensitive to non-
informative measures. In other words, being
xr a random function such that P (xr(s) >
xr(s?)) = 12 , then:
HBRx1..xn(s) ? HBRx1..xn,xr(s)
The first two properties are trivially satisfied: the
? operator in H and the score comparisons are not af-
fected by redundant measures nor their scale proper-
ties. Regarding the third property, the Heterogeneity
of a set of measures plus a random function xr is:
H(X ? {xr}) ?
Ps,s?(?x, x
? ? X?{xr}|x(s) > x(s
?)?x?(s) < x?(s?)) =
H(X ) + (1?H(X )) ?
1
2
=
H(X ) + 1
2
That is, the Heterogeneity grows proportionally
when including a random function. Assuming that
the random function corroborates the similarity in-
crease in a half of cases, the result is a proportional
relationship between HBR and HBR with the addi-
tional measure. Note that we need to assume a large
enough amount of data to avoid random effects.
4 Experimental Setting
4.1 Test Bed
We have used the AS test collections used in the
DUC 2005 and DUC 2006 evaluation campaigns1
(Dang, 2005; Dang, 2006). The task was to gener-
ate a question focused summary of 250 words from a
set of 25-50 documents to a complex question. Sum-
maries were evaluated according to several criteria.
Here, we will consider the responsiveness judge-
ments, in which the quality score was an integer be-
tween 1 and 5. See Table 1 for a brief numerical
description of these test beds.
In order to check the measure combining method,
we have employed standard variants of ROUGE
(Lin, 2004), including the reversed precision version
for each variant 2. We have considered also the F
1http://duc.nist.gov/
2Note that the original ROUGE measures are oriented to re-
call
38
DUC 2005 DUC 2006
#human-references 3-4 3-4
#systems 32 35
#system-outputs-assessed 32 35
#system-outputs 50 50
#outputs-assessed per-system 50 50
Table 1: Test collections from 2005 and 2006 DUC evaluation campaigns used in our experiments.
measure between recall and precision oriented mea-
sures. Finally, our measure set includes also BE or
Basic Elements (Hovy et al, 2006).
4.2 Meta-evaluation criterion
The traditional way of meta-evaluating measures
consists of computing the Pearson correlation be-
tween measure scores and quality human assess-
ments. But the main goal of automatic evaluation
metrics is not exactly to predict the real quality of
systems; rather than this, their core mission is de-
tecting system outputs that improve the baseline sys-
tem in each development cycle. Therefore, the issue
is to what extent a quality increase between two sys-
tem outputs is reflected by the output ranking pro-
duced by the measure.
According to this perspective, we propose meta-
evaluating measures in terms of an extended version
of AUC (Area Under the Curve). AUC can be seen
as the probability of observing a score increase when
observing a real quality increase between two sys-
tem outputs (Fawcett, 2006).
AUC(x) = P (x(s) > x(s?)|Q(s) > Q(s?))
In order to customize this measure to our scenario,
two special cases must be handled:
(i) For cases in which both summaries obtain the
same value, we assume that the measure rewards
each instance with equal probability. That is, if
x(s) = x(s?),P (x(s) > x(s?)|Q(s) > Q(s?)) = 12 .
(ii) Given that in the AS evaluation scenarios there
are multiple quality levels, we still apply the same
probabilistic AUC definition, considering pairs of
summaries in which one of them achieves more
quality than the other according to human assessors.
Figure 1: Correlation between probability of quality in-
crease and Heterogeneity of measures that corroborate
the increase
5 Experiments
5.1 Measure Heterogeneity vs. Quality
Increase
We hypothesize that the probability of a real similar-
ity increase to human references (as stated by human
assessments) is directly related to the heterogeneity
of the set of measures that confirm such increase. In
order to verify whether this principle holds in prac-
tice, we need to measure the correlation between
both variables. Therefore, we compute, for each pair
of summaries in the same topic the heterogeneity of
the set of measures that corroborate a score increase
between both:
H({x ? X |x(s) ? x(s?)})
The Heterogeneity has been estimated by counting
cases over 10,000 samples (pairs of summaries) in
both corpora.
Then, we have sorted each pair ?s, s?? according
to its related heterogeneity. We have divided the re-
sulting rank in 100 intervals of the same size. For
39
Figure 2: AUC comparison between HBR and single measures in DUC 2005 and DUC 2006 corpora.
each interval, we have computed the average hetero-
geneity of the set and the probability of real quality
increase (P (Q(s) ? Q(s?))).
Figure 1 displays the results. Note that the direct
relation between both variables is clear: a key for
predicting a real quality increase is how heteroge-
neous is the set of measures corroborating it.
5.2 HBR vs. Single Measures
In the following experiment, we compute HBR and
we compare the resulting AUC with that of single
measures. The heterogeneity of measures is esti-
mated over samples in both corpora (DUC 2005 and
DUC 2006), and HBR ranking is computed to rank
summaries for each topic. For the meta-evaluation,
the AUC probability is computed over summary
pairs from the same topic.
Figure 2 shows the resulting AUC values of sin-
gle measures and HBR. The black bar represents the
HBR approach. The light grey bars are ROUGE
measures oriented to precision. The dark grey bars
include ROUGE variants oriented to recall and F,
and the measure BE. As the Figure shows, recall-
based measures achieve in general higher AUC val-
ues than precision-oriented measures. The HBR
measure combination appears near the top. It is im-
proved by some measures such as ROUGE SU4 R,
although the difference is not statistically significant
(p = 0.36 for a t-test between ROUGE SU4 R and
HBR, for instance). HBR improves the 10 worst
single measures with statistical significance (p <
0.025).
5.3 Robustness
The next question is why using HBR instead of the
?best? measure (ROUGE-SU4-R in this case). As
we mentioned, the reliability of measures can vary
across scenarios. For instance, in DUC scenarios
most systems are extractive, and exploit the maxi-
mum size allowed in the evaluation campaign guide-
lines. Therefore, the precision over long n-grams is
not crucial, given that the grammaticality of sum-
maries is ensured. In this scenario the recall over
words or short n-grams over human references is a
clear signal of quality. But we can not ensure that
these characteristics will be kept in other corpora, or
even when evaluating new kind of summarizers with
the same corpora.
Our hypothesis is that, given that HBR resembles
the best measure without using human assessments,
it should have a more stable performance in situa-
tions where the best measure changes.
In order to check empirically this assertion, we
have investigated the lower bound performance of
measures in our test collections. First, we have
ranked measures for each topic according to their
AUC values; Then, we have computed, for every
measure, its rank regarding the rest of measures
(scaled from 0 to 1). Finally, we average each mea-
sure across the 10% of topics in which the measure
40
Figure 3: Average rank of measures over the 10% of topics with lowest results for the measure.
gets the worst ranks. Figure 3 shows the results: the
worst performance of HBR across topics is better
than the worst performance of any single measure.
This confirms that the combination of measures us-
ing HBR is indeed more robust than any measure in
isolation.
5.4 Consistent vs. Inconsistent Topics
The Heterogeneity property is grounded on the as-
sumption that any similarity criteria represents a
positive evidence of similarity to human references.
In general, we can assert that this assumption holds
over a large enough random set of texts. However,
depending on the distribution of summaries in the
corpus, this assumption may not always hold. For
instance, we can assume that, given all possible sum-
maries, improving the word precision with respect to
the gold standard can never be a negative evidence
of quality. However, for a certain topic, it could hap-
pen that the worst summaries are also the shortest,
and have high precision and low recall. In this case,
precision-based similarity could be correlated with
negative quality. Let us refer to these as inconsis-
tent topics vs. consistent topics. In terms of AUC,
a measure represents a negative evidence of quality
when AUC is lower than 0.5. Our test collections
contain 100 topics, out of which 25 are inconsis-
tent (i.e., at least one measure achieves AUC values
lower than 0.5) and 75 are consistent with respect to
our measure set (all measures achieve AUC values
higher than 0.5).
Figure ?? illustrates the AUC achieved by mea-
sures when inconsistent topics are excluded. As with
the full set of topics, recall-based measures achieve
higher AUC values than precision-based measures;
but, in this case, HBR appears at the top of the rank-
ing. This result illustrates that (i) HBR behaves par-
ticularly well when our assumptions on similarity
measures hold in the corpus; and that (ii) in prac-
tice, there may be topics for which our assumptions
do not hold.
6 Conclusions
In this paper, we have confirmed that the heterogene-
ity of a set of summary evaluation measures is cor-
related with the probability of finding a real quality
improvement when all measures corroborate it. The
HBR measure combination method is based on this
principle, which is grounded on the assumption that
any similarity increase with respect to human refer-
ences is a positive signal of quality.
Our empirical results indicate that the Hetero-
geneity Based Ranking achieves a reliability simi-
lar to the best single measure in the set. In addi-
41
Figure 4: AUC comparison between HBR and single measures in corpora DUC2005 and DUC 2006 over topics in
which all measures achieve AUC bigger than 0.5.
tion, HBR results are more robust across topics than
single measures. Our experiments also suggest that
HBR behaves particularly well when the assump-
tions of the heterogeneity property holds in the cor-
pus. These assumptions are conditioned by the dis-
tribution of summaries in the corpus (in particular,
on the amount and variability of the summaries that
are compared with human references), and in prac-
tice 25% of the topics in our test collections do not
satisfy them for our set of measures.
The HBR (Heterogeneity Based Ranking) method
proposed in this paper does not represent the ?best
automatic evaluation measure?. Rather than this, it
promotes the development of new measures. What
HBR does is solving ?or at least palliating? the prob-
lem of reliability variance of measures across test
beds. According to our analysis, our practical rec-
ommendations for system refinement are:
1. Compile an heterogenous set of measures, cov-
ering multiple linguistic aspects (such as n-
gram precision, recall, basic linguistic struc-
tures, etc.).
2. Considering the summarization scenario, dis-
card measures that might not always represent
a positive evidence of quality. For instance,
if very short summaries are allowed (e.g. one
word) and they are very frequent in the set of
system outputs to be compared to each other,
precision oriented measures may violate HBR
assumptions.
3. Evaluate automatically your new summariza-
tion approach within this corpus according to
the HBR method.
Our priority for future work is now developing
a reference benchmark containing an heterogenous
set of summaries, human references and measures
satisfying the heterogeneity assumptions and cover-
ing multiple summarization scenarios where differ-
ent measures play different roles.
The HBR software is available at
http://nlp.uned.es/?enrique/
References
Enrique Amigo?, Julio Gonzalo, Jesus Gimenez, and Fe-
lisa Verdejo. 2011. Corroborating text evaluation re-
sults with heterogeneous measures. In Proceedings of
the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 455?466, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Hoa Trang Dang. 2005. Overview of DUC 2005. In Pro-
ceedings of the 2005 Document Understanding Work-
shop.
Hoa Trang Dang. 2006. Overview of DUC 2006. In Pro-
ceedings of the 2006 Document Understanding Work-
shop.
42
Tom Fawcett. 2006. An introduction to roc analysis.
Pattern Recogn. Lett., 27:861?874, June.
Tsutomu Hirao, Manabu Okumura, and Hideki Isozaki.
2005. Kernel-based approach for automatic evaluation
of natural language generation technologies: Applica-
tion to automatic summarization. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 145?152, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
Eduard Hovy, Chin-Yew Lin, and Liang Zhou. 2005.
Evaluating DUC 2005 using Basic Elements. Proceed-
ings of Document Understanding Conference (DUC).
Vancouver, B.C., Canada.
Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Junichi
Fukumoto. 2006. Automated Summarization Evalu-
ation with Basic Elements. In Proceedings of the 5th
International Conference on Language Resources and
Evaluation (LREC), pages 899?902.
Chin-Yew Lin. 2004. Rouge: A Package for Auto-
matic Evaluation of Summaries. In Marie-Francine
Moens and Stan Szpakowicz, editors, Text Summariza-
tion Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
Karolina Owczarzak. 2009. Depeval(summ):
dependency-based evaluation for automatic sum-
maries. In ACL-IJCNLP ?09: Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1, pages
190?198, Morristown, NJ, USA. Association for Com-
putational Linguistics.
43
Proceedings of the The 1st Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 21?28,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Events are Not Simple: Identity, Non-Identity, and Quasi-Identity   Eduard Hovy Language Technology Institute Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213, USA hovy@cmu.edu  
Teruko Mitamura Language Technology Institute Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213, USA teruko@cs.cmu.edu  
Felisa Verdejo E.T.S.I. Inform?tica, UNED C/ Juan del Rosal, 16 (Ciudad Universitaria) 28040 Madrid, Spain felisa@lsi.uned.es                  Jun Araki Andrew Philpot                   Language Technology Institute Information Sciences Institute                   Carnegie Mellon University University of Southern California                 5000 Forbes Avenue                  Pittsburgh, PA 15213, USA 4676 Admiralty Way Marina del Rey, CA 90292, USA        junaraki@cs.cmu.edu philpot@isi.edu  Abstract1 Despite considerable theoretical and computa-tional work on coreference, deciding when two entities or events are identical is very difficult.  In a project to build corpora containing corefer-ence links between events, we have identified three levels of event identity (full, partial, and none). Event coreference annotation on two cor-pora was performed to validate the findings.    1 The Problem of Identity Last year we had HLT in Montreal, and this year we did it in Atlanta.   Does the ?did it? refer to the same conference or a different one?  The two conferences are not iden-tical, of course, but they are also not totally unre-lated?else the ?did it? would not be interpretable.   When creating text, we treat instances of entities and events as if they are fixed, well-described, and well-understood.  When we say ?that boat over there? or ?Mary?s wedding next month?, we as-sume the reader creates a mental representation of the referent, and we proceed to refer to it without further thought.   However, as has been often noted in theoretical studies of semantics, this assumption is very prob-lematic (Mill, 1872; Frege 1892; Guarino, 1999).  Entities and (even more so) events are complex composite phenomena in the world, and they un-dergo change.                                                              1 This work was supported by grants from DARPA and NSF, as well as by funding that supported Prof. M. Felisa Vedejo from UNED Madrid. 
Since nobody has complete knowledge, the au-thor?s mental image of the entity or event in ques-tion might differ from the reader?s, and from the truth.  Specifically, the properties the author as-sumes for the event or entity might not be the ones the reader assumes. This difference has deep con-sequences for the treatment of the semantic mean-ing of a text.  In particular, it fundamentally affects how one must perform coreference among entities or events.   As discussed in Section 6, events have been the focus of study in both Linguistics and NLP (Chen and Ji, 2009; Bejan and Harabagiu, 2008, 2010; Humphreys et al, 1997).  Determining when two event mentions in text corefer is, however, an un-solved problem2.  Past work in NLP has avoided some of the more complex problems by consider-ing only certain types of coreference, or by simply ignoring the major problems.  The results have been partial, or inconsistent, annotations.   In this paper we describe our approach to the problem of coreference among events.  In order to build a corpus containing event coreference links that is annotated with high enough inter-annotator agreement to be useful for machine learning, it has proven necessary to create a model of event identi-ty that is more elaborate than is usually assumed in the NLP literature, and to formulate quite specific definitions for its central concepts.                                                               2 In this work, we mean both events and states when we say ?event?.  A state refers to a fixed, or regularly changing, con-figuration of entities in the world, such as ?it is hot? or ?he is running?.  An event occurs when there is a change of state in the world, such as ?he stops running? or ?the plane took off?. 
21
 Event coreference is the problem of determin-ing when two mentions in a text refer to the ?same? event. Whether or not the event actually occurred in reality is a separate issue; a text can describe people flying around on dragons or broomsticks.  While the events might be actual occurrences, hy-pothesized or desired ones, etc., they exist in the text as Discourse Elements (DEs), and this is what we consider in this work. Each DE is referred to (explicitly or implicitly) in the text by a mention, for example ?destroy?, ?the attack?, ?that event?, or ?it?. But it is often unclear whether two mentions refer to the same DE or to closely related ones, or to something alto-gether different. The following example illustrates two principal problems of event coreference:  While Turkish troops have been fighting_E.1 a Kurdish faction in northern Iraq, two other Kurdish groups have been battling_E.2 each other. A radio station operated_E.3 by the Kurdistan Democratic Party said_E.4 the party's forces attacked_E.5 positions of the Patriotic Union of Kurdistan on Monday in the Kurdish re-gion's capital Irbil. The Voice of Iraqi Kurdistan radio, moni-tored_E.6 by the British Broadcasting Corp., said_E.7 more than 80 Patriotic Union fight-ers were killed_E.8 and at least 150 wound-ed_E.9. The fighting_E.10 was also reported_E.11 by a senior Patriotic Union official, Kusret Rasul Ali, who said_E.12 PUK forces re-pelled_E.13 a large KDP attack_E.14. ? Ali claimed_E.16 that 300 KDP fighters were killed_E.17 or wounded_E.18 and only 11 Patriotic Union members died_E.19. Problem 1: Partial event overlap.  Event E.2, ?battling each other?, refers to an ongoing series of skirmishes between two Kurdish groups, the KDP and the PUK.  Since one of these battles, where the KDP attacked positions of the PUK, is E.5, it is natural to say that E.2 and E.5 corefer.  However, E.2 clearly denotes other battles as well, and there-fore E.5 and E.2 cannot fully corefer.  In another example, event E.8 refers to the killing of a num-ber of soldiers as part of this fight E.5, and event E.9 to the wounding of others.  Both events E.8 
and E.9 constitute an intrinsic part of the attack E.5, and hence corefer to it, but are each only part of E.5, and hence neither can fully corefer to it.   Problem 2: Inconsistent reporting.  This news fragment contains two reports of the fight: E.5 and E.10.  Since E.10 describes E.5 from the perspec-tive of a senior PUK official, it should corefer to E.5.  But where the KDP?s report claims more than 80 PUK fighters killed (event E.8, part of E.5), the PUK official said that only 11 PUK members died (event E.19, part of E.10).  Without taking into account the fact that the two killing events are re-ports made by different speakers, it would not be possible to recognize them as coreferent.   Examples of partial event overlap and incon-sistent reporting are common in text, and occur as various types.  In our work, we formally recognize partial event overlap, calling it partial event identi-ty, which permits different degrees and types of event coreference.  This approach simplifies the coreference problem and highlights various inter-event relationships that facilitates grouping events into ?families? that support further analysis and combination with other NLP system components.   In this paper, we introduce the idea that there are three degrees of event identity: fully identical, qua-si-identical, and fully independent (not identical).  Full identity reflects in full coreference and quasi-identity in partial coreference.  Fully independent events are singletons.  Our claims in this paper are:  ? Events, being complex phenomena, can corefer fully (identity) or partially (quasi-identity).  ? Event coreference annotation is considera-bly clarified when partial coreference is allowed.  ? A relatively small fixed set of types of quasi-identity suffices to describe most of them.  ? Different domains and genres highlight different subsets of these quasi-identity types.   ? Different auxiliary knowledge sources and texts are relevant for different types. 2 Types of Full and Partial Identity Def: Two mentions fully corefer if their activi-ty/event/state DE is identical in all respects, as far as one can tell from their occurrence in the text.  (In particular, their agents, location, and time are identical or compatible.)  One can distinguish sev-eral types of identity, as spelled out below.  
22
Def: Two mentions partially corefer if activi-ty/event/state DE is quasi-identical: most aspects are the same, but some additional information is provided for one or the other that is not shared. There are two principal types of quasi-identity, as defined below.  Otherwise, two mentions do not corefer.  2.1 Full Identity  Mention1 is identical to mention2 iff there is no semantic (meaning) difference between them. Just one DE, and exactly the same aspects of the DE, are understood from both mentions in their con-texts. It is possible to replace the one mention with the other without any semantic change (though some small syntactic changes might be required to ensure grammaticality). Note that mention2 may contain less detail than mention1 and remain iden-tical, if it carries over information from mention1 that is understood / inherited from the context.  However, when mention2 provides more or new information not contained in mention1 or naturally inferred for it, then the two are no longer identical. Usually, exact identity is rare within a single text, but may occur more often across texts.  We identi-fy the following types:  1. Lexical identity: The two mentions use exactly the same senses of the same word(s), in-cluding derivational words (e.g., ?destroy?, ?de-struction?). 2. Synonym: One mention?s word is a syno-nym of the other?s word.  3. Wide-reading: One mention is a synonym of the wide reading of the other (defined below, under Quasi-identity:Scriptal).  For example, in ?the attack(E1) took place yesterday.  The bomb-ing(E2) killed four people?, E1 and E2 are fully coreferent only when ?bombing? is read in its wide sense that denotes the whole attack, not the narrow sense that denotes just the actual exploding of the bomb.   4. Paraphrase: One mention is a paraphrase of the other.  Here some syntactic differences may occur.  Some examples are active/passive trans-formation (?she gave him the book? / ?he was giv-en the book by her?), shifts of perspective that do not add or lose information (?he went to Boston? / ?he came to Boston?), etc.  No extra semantic in-formation is provided in one mention or the other.    
5. Pronoun: One mention refers deictically to the DE, as in (?the party? / ?that event?), (?the election [went well]? / ?it [went well]?).   2.2  Quasi-identity  Mention1 is quasi- (partially) identical to mention2 iff they refer to the ?same? DE but one mention includes information that is not contained in the other, not counting information understood/inhe-rited from the context.  They are semantically not fully identical, though the core part of the two mentions is.  One mention can replace the other, but some information will be changed, added, or lost.  (This is the typical case between possible coreferent mentions within a document.)   We distinguish between two core types of partial identity: Membership and Subevent.  The essential difference between the two is which aspects of the two events in question differ.  Member-of obtains when we have two instances of the same event that differ in some particulars, such as time and loca-tion and [some] participants (agents, patients, etc).  In contrast, Subevent obtains when we have differ-ent events that occur at more or less the same place and time with the same cast of participants.   Membership: Mention1 is a set of similar DEs (multiple instances of the same kind of event), like several birthday parties, and mention2 is one or more of them.  More precisely, we say that an event B is a member of A if: (i) A is a set of mul-tiple instances of the same type of event (and hence its mention usually pluralized); (ii) B?s DE(s) is one or more (but not all) of them; (iii) ei-ther or both the time and the place of B?s DE(s) and (some of) A?s DEs are different.  For example, in ?I attended three parties(E1) last month.  The first one(E2) was the best?, E2 is a member of E1.  The relation that links the single instance to the set is member-of.  Subevent: The DE of mention1 is a script (a ste-reotypical sequence of events, performed by an agent in pursuit of a given goal, such as eating at a restaurant, executing a bombing, running for elec-tion), and mention2 is one of the actions/events executed as part of that script (say, paying the waiter, or detonating the bomb, or making a cam-paign speech).  More precisely, we say that an event B is a subevent of an event A if: (i) A is a complex sequence of activities, mostly performed by the same (or compatible) agent; (ii) B is one of 
23
these activities; and (iii) B occurs at the same time and place as A.  Here A acts as a kind of collector event.  Often, the whole script is named by the key event of the script (for example, in ?he planned the explosion?, the ?explosion? signifies the whole script, including planning, planting the bomb, the detonation, etc.; but the actual detonation event itself can also be called ?the explosion?).  We call the interpretation of the mention that refers to the whole script its wide reading, and the interpreta-tion that refers to just the key subevent the narrow reading.  It is important not to confuse the two; a wide reading and a narrow reading of a word can-not corefer3. The relation that links the narrow reading DE to the wide one is sub-to.   Several aspects of the events in question provide key information to differentiate between members and subevents:   1. Time: When the time of occurrence of mention1 is temporally ?close enough? to the time of occurrence of mention2, then it is likely that one is a Subevent of the other.  More precisely, we say that an event B is a subevent of event A if: (i) A and B are both events; (ii) the mentions of A and B both refer to the same overall DE; and (iii) the time of occurrence of B is contained in the time of oc-currence of A. But if (i) and (ii) hold but not (iii), and A is a set of events (plural), then B is a mem-ber of A.  (In (Humphreys et al, 1997), any varia-tion in time automatically results in a decision of non-coreference.)   2. Space/location: The location of mention1 is spatially ?close enough? to the location of men-tion2.  More precisely, we say that an event B is a subevent of event A if: (i) A and B are both events; (ii) the mentions of A and B both refer to the same overall DE; and (iii) the location of oc-currence of B is contained in, or overlaps with, or abuts the location of occurrence of A.  But if (i) and (ii) hold but not (iii), and A is a set of events (plural), then B is a member of A. 
                                                            3 For example, in ?James perpetrated the shooting. He was arrested for the attack?, ?shooting? is used in its wide sense and here is coreferent with ?attack?, since it applies to a whole sequence of events.  In contrast, ?James perpetrated the shoot-ing.  He is the one who actually pulled the trigger?, ?shooting? is used in its narrow sense to mean just the single act.  Typi-cally, a word with two readings can corefer (i.e., be lexically or synonymically identical to) another in the same reading only. 
3.  Event participants: Mention1 and men-tion2 refer to the same DE but differ in the overall cast of participants involved.  In these cases, the member relation obtains, and can be differentiated into subtypes, since participants of events can dif-fer in several ways.  For example, if: (i) the men-tions of events A and B refer to the same overall DE; and (ii) the participants (agents, patients, etc.) of mention2 are a subset of the participants of mention1, as in ?the crowd demonstrated on the square. Susan and Mary were in it?, then event B is a participant-member of event A.  In another ex-ample, event B is a participant-instance-member of event A if: (i) the mentions of events A and B refer to the same overall DE; and (ii) one or more of the participants (agents, patients, etc.) of men-tion2 is/are an instance of the participants of men-tion1, as in ?a firebrand addressed the crowd on the square. Joe spoke for an hour?, where Joe is the firebrand.  There are other ways in which two mentions may refer to the same DE but differ from one an-other.  Usually these differences are not semantic but reflect an orientation or perspective difference.  For example, one mention may include the speak-er?s evaluation/opinion, while the other is neutral, as in ?He sang the silly song.  He embarrassed himself?, or the spatial orientation of the speaker, as in ?she went to New York? / ?she came to New York?.  We treat these cases as fully coreferent.   Sometimes it is very difficult to know whether two mentions are bidirectionally implied, meaning that the two must corefer, or whether they are only quasi-identical (i.e., one entails the other but not vice versa).  For example, in ?he had a heart at-tack? / ?he died?, the two mentions are not identi-cal because one can have a heart attack and not die from it. In contrast, ?he had a fatal heart attack? / ?he died from a heart attack? are identical.  In ?she was elected President? / ?she took office as Presi-dent?, it is more difficult to decide. Does being elected automatically entail taking office?  In some political systems it may, and in others it may not.  When in doubt, we treat the case as only quasi-identical.  Thus, comparing to examples from Full-Identity: Paraphrase, the following are only quasi-identical because of additional information: ?she sold the book? / ?she sold Peter the book?; ?she sold Peter the book? / ?Peter got [not bought] the book from her?. 
24
Quasi-identity has been considered in corefer-ence before in (Hasler et al, 2006) but not as ex-tensively, and in (Recasens and Hovy, 2010a; 2011) but applied only to entities.  When applied to events, the issue becomes more complex.  3 Two Problems  3.1 Domain and Reporting Events  As described above, inconsistent reporting occurs when a DE stated in reported text contains signifi-cant differences from the author?s description of the same DE.   To handle such cases we have found it necessary to additionally identify communication events, which we call Reportings, during annotation be-cause they provide a context in which a DE is stat-ed.  We identify two principal types of Reporting verbs: locutionary verbs ?say?, ?report?, ?an-nounce?, etc.) and Speech Acts (?condemn?, ?promise?, ?support?, ?blame?, etc.).  Where the former verbs signal merely a telling, the latter verbs both say and thereby do something.  For ex-ample in the following paragraph, ?admitted? and ?say? are communication events:  Memon admitted_R.7,in-sayR.3 his in-volvement in activities_E.8,in-sayR.3 in-volving an explosives-laden van near the president's motorcade, police said_R.3?.  Sometimes the same event can participate in-side two reporting events, as in   ?The LA Times lauded_R.1 the decision_E.2,in-sayR.1,in-sayR.3, which the NY Times lampooned_R.3. Though an added annotation burden, the link from a DE to a reporting event allows the analyst or learning system to discount apparent contradictory aspects of the DE and make more accurate identity decisions.     3.2 Unclear Semantics of Events  Sometimes it is difficult to determine the exact relationships between events since their semantics is unclear.  In the following, is E.45 coreferent to E.44, or only partially?  If so, how?  Amnesty International has accused both sides of violating_E.44 international humanitarian law by targeting_E.45 civilian areas, and ... 
We decided that E.44 is not fully coreferent with E.45, since violating is not the same as targeting.  Also, E.45 is not a subevent of E.44 since ?violat-ing? is not a script with a well-defined series of steps, does not trigger ?targeting?, and does not occur before ?targeting?.  Rather, targeting is a certain form or example of violation/violating. (It might be easier if the sentence were: ?... of violat-ing international humanitarian law by targeting civilian areas and the human rights group, by kill-ing civilians, and by....?.  As such E.45 could be interpreted as a member of E.44, interpreting the latter as a series of violations.)   4 Annotation  To validate these ideas we have been annotating newspaper texts within the context of a large pro-ject on automated deep reading of text. This pro-ject combines Information Extraction, parsing, and various forms of inference to analyze a small num-ber of texts and to then answer questions about them.  The inability of current text analysis engines to handle event coreference has been a stumbling block in the project.  By creating a corpus of texts annotated for coreference we are working to enable machine learning systems to learn which features are relevant for coreference and then ultimately to perform such coreference as well.  We are annotating two corpora: 1. The Intelligence Community (IC) Corpus contains texts in the Violent Events domain (bombings, killings, wars, etc.).  Given the relative scarcity of the partial coreference subtypes, we annotated only instances of full coreference, Subevent, and Member relations.  To handle Subevents one needs an unambiguous definition of the scripts in the domain.  Fortunately this domain offers a manageable set of events (our event ontol-ogy comprises approximately 50 terms) with a subevent structure that is not overly complex but still realistic.  We did not find the need to exceed three layers of scriptal granularity, as in  campaign > {bombing, attack} > {blast, kill, wound}.  2. The Biography (Bio) Corpus contains texts describing the lives of famous people. Typically, these texts are written when the person dies or has some notable achievement.  Given the complexi-ties of description of artistic and other creative achievements, we restrict our corpus to achieve-
25
ments in politics, science, sports, and other more factual endeavors.  More important than scriptal granularity in this domain is temporal sequencing.  We obtained and modified a version of the An-CoraPipe entity coreference annotation interface (Bertran et al, 2010) that was kindly given us by the AnCora team at the University of Barcelona.  We implemented criteria and an automated method for automatically identifying domain and reporting events.  We also created a tool to check and dis-play the results of annotation, and technology to deliver various agreement scores.  Using different sets of annotators (from 3 to 6 people per text), we have completed a corpus of 100 texts in the IC domain and are in process of annotating the Bio corpus. Our various types of full and partial coreference and the associated an-notation guidelines were developed and refined over the first third of these documents.   Table 1 shows statistics and inter-annotator agreement for the remaining 65 articles.  The aver-age number of domain and reporting events per article is 41.2.  We use Fleiss?s kappa since we have more than two annotators per article.  The (rather low) score for member coreference is not really reliable given the small number of instances.     Avg no per article Agreement (Fleiss?s kappa) Full coreference relations Member coreference relations Subevent coreference relations 19.5 0.620 2.7 0.213 7.2 0.467 Table 1: Annotation statistics and agreement. 5 Validation and Use To validate the conceptualization and definitions of full and partial identity relations, we report in (Araki et al, 2013) a study that determines correla-tions between the Member and Subevent relation instances and a variety of syntactic and lexico-semantic features.  The utility of these features to support automated event coreference is reported in the same paper.   We are now developing a flexible recursive pro-cedure that integrates coreference of events and of their pertinent participants (including locations and times).  This procedure employs inference in addi-tion to feature-based classification to compensate for the shortcomings of each method alone.   
6 Relevant Past Work The problem of identity has been addressed by scholars since antiquity.  In the intensional ap-proach (for example, De Saussure, 1896) a concept is defined as a set of attributes (differentiae), that serve to distinguish it from other concepts; two concepts are identical iff all their attributes and values are.  In the extensional approach (Frege, 1982) a concept can be defined as the set of all in-stances of that concept; two concepts are identical when their two extensional sets are.    Given the impossibility of either approach to support practical work, AI scholars have devoted some attention to so-called Identity Criteria.  Gua-rino (1999) outlines several ?dimensions? along which entities can remain identical or change un-der transformations; for example, a glass before and after it is crushed is identical with respect to its matter but not its shape; the ACL now and one hundred years hence is (probably) identical as an organization but not in its membership.   There has not been much theoretical work on semantic identity in the NLP community.  But there has been a considerable amount of work on the problem of coreference. Focusing on entity coreference are (McCarthy and Lehnert, 1995; Cu-lotta et al, 2007; Ng, 2007; Ng, 2009; Finkel and Manning, 2008; Ng, 2009).  Focusing on event coreference are (Humphries et al, 1997; Chen and Zi, 2009; Bejan and Harabagiu, 2008; 2010).   Anaphora and bridging reference are discussed in (Poesio and Artstein, 2005; 2007). Relevant to events is the TIME-ML corpus (Mani and Pustejovsky, 2004; Pustejovsky et al, 2003), which provides a specification notation for events and temporal expressions. Several corpora contain annotations for entity coreference, including the Prague Dependency Treebank (Ku?ov? and Haji?ov?. 2004), the ACE corpus (Walker et al, 2006), and OntoNotes (Pra-dhan et al, 2007).  Most similar to our work is that of (Hasler et al, 2006). In that study, coreferential events and their arguments (also coreference between the argu-ments) were annotated for the terrorism/security domain, considering five event categories (attack, defend, injure, die, contact), and five event clusters (Bukavu bombing, Peru hostages, Tajikistan hos-tages, Israel suicide bombing and China-Taiwan 
26
hijacking). They also annotated information about the kind of coreferential link, such as identity / synonymy / generalization / specialization / other.   Our work takes further the ideas of (Hasler et al, 2006) and (Recasens et al, 2011) in elaborating the types of full and partial identity, as they are manifest in event coreference.   7 Conclusion The problem of entity and event identity, and hence coreference, is challenging.  We provide a definition of identity and two principal types of quasi-identity, with differentiation based on differ-ences in location, time, and participants.  We hope that these ideas help to clarify the problem and im-prove inter-annotator agreement. Acknowledgments Our grateful thanks goes to Prof. Antonia Mart? and her team for their extensive work on the modi-fications of the AnCoraPipe annotation interface. References  Araki, J., T, Mitamura, and E.H. Hovy. 2013. Identity and Quasi-Identity Relations for Event Coreference. Unpublished manuscript. Bejan, C.A. and S. Harabagiu. 2008. A Linguistic Re-source for Discovering Event Structures and Resolv-ing Event Coreference. Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC 08). Bejan, C.A. and S. Harabagiu. 2010. Unsupervised Event Coreference Resolution with Rich Linguistic Features. Proceedings of the 48th conference of the Association for Computational Linguistics (ACL 10). Bertran, M., O. Borrega, M.A. Mart?, and M. Taul?, 2010. AnCoraPipe: A New Tool for Corpora Annota-tion. Working paper 1: TEXT-MESS 2.0 (Text-Knowledge 2.0). Available at http://clic.ub.edu/files/AnCoraPipe_0.pdf  Chen, Z. and H. Ji. 2009. Graph-based Event Corefer-ence Resolution. Proceedings of the ACL-IJCNLP 09 workshop on TextGraphs-4: Graph-based Methods for Natural Language Processing. Culotta, A., M. Wick, and A. McCallum. 2007. First-order probabilistic models for coreference resolution. Proceedings of the HLT/NAACL conference.  
De Saussure, F. 1896. Course in General Linguistics. Open Court Classics. Finkel, J.R. and C.D. Manning. 2008. Enforcing transi-tivity in coreference resolution. Proceedings of the ACL-HLT conference, pp. 45?48.  Florian, R., J F Pitrelli, S Roukos, I Zitouni. 2010. Im-proving Mention Detection Robustness to Noisy In-put.  Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). Frege, G. 1892. On Sense and Reference. Reprinted in P. Geach and M. Black (eds.) Translations from the Philosophical Writings of Gottlob Frege. Oxford: Blackwell, 1960. Guarino, N. 1999. The Role of Identity Conditions in Ontology Design. In C. Freksa and D.M. Mark (eds.), Spatial Information Theory: Cognitive and Computa-tional Foundations of Geographic Information Sci-ence. Proceedings of International Conference COSIT '99.  Springer Verlag. Hasler, L., C. Orasan, and K. Naumann. 2006. NPs for Events: Experiments in Coreference Annotation. Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-06), pp. 1167?1172.  Hasler, L. and C. Orasan. 2009. Do Coreferential Ar-guments make Event Mentions Coreferential? Pro-ceedings of the 7th Discourse Anaphora and Anaphor Resolution Colloquium (DAARC 09), pp 151?163. Humphreys, K., R. Gaizauskas and S. Azzam. 1997.  Event Coreference for Information Extraction. Pro-ceedings of the ACL conference Workshop on Opera-tional Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts (ANARESOLU-TION 97).  Ku?ov?, L. and E. Haji?ov?. 2004. Coreferential rela-tions in the Prague Dependency Treebank. Proceed-ings of the DAARC workshop, pp. 97?102. Mani, I. and J. Pustejovsky. 2004. Temporal Discourse Models for Narrative Structure. Proceedings of the ACL 2004 Workshop on Discourse Annotation.  McCarthy, J.F. and W. Lehnert. 1995. Using Decision trees for Coreference Resolution. Proceedings of the IJCAI conference.  Mill, J.S. 1872. A System of Logic, definitive 8th edi-tion. 1949 reprint, London: Longmans, Green and Company. Ng, V. 2007. Shallow Semantics for Coreference Reso-lution. Proceedings of the IJCAI conference. 
27
Ng, V. 2009. Graph-cut-based Anaphoricity Determina- tion for Coreference Resolution. Proceedings of the NAACL-HLT conference, pp. 575?583. Poesio, M. and R. Artstein. 2005. The reliability of ana-phoric annotation, reconsidered: Taking ambiguity into account. Proceedings of the ACL Workshop on Frontiers in Corpus Annotation II. Poesio, M. and R. Artstein. 2008. Anaphoric annotation in the ARRAU corpus. Proceedings of the LREC conference. Pradhan, S., E.H. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel 2007. OntoNotes: A Unified Relational Semantic Representation. Interna-tional Journal of Semantic Computing 1(4), pp. 405?420.  Pustejovsky, J., J. Casta?o, R. Ingria, R. Saur?, R. Gai-zauskas, A. Setzer and G. Katz. 2003. TimeML: Ro-bust Specification of Event and Temporal Expressions in Text. Proceedings of IWCS-5, Fifth International Workshop on Computational Seman-tics. Recasens, M. and E.H. Hovy. 2010a. Coreference Reso-lution across Corpora: Languages, Coding Schemes, and Preprocessing Information. Proceedings of the Association of Computational Linguistics conference (ACL 10).  Recasens, M. and E.H. Hovy. 2010b. BLANC: Imple-menting the Rand Index for Coreference Evaluation.  Journal of Natural Language Engineering 16(5).  Recasens, M., E.H. Hovy, and M.A. Mart?. 2011. Identi-ty, Non-identity, and Near-identity: Addressing the Complexity of Coreference.  Lingua.   Taul?, M., M.A. Mart?. and M. Recasens. 2008. An-Cora: Multilevel Annotated Corpora for Catalan and Spanish. Proceedings of the LREC 08 conference, pp. 96?101.  Walker, C., S. Strassel, J. Medero 2006. The ACE 2005 multilingual training corpus.  Linguistic Data Con-sortium, University of Pennsylvania, Philadelphia. 
28
