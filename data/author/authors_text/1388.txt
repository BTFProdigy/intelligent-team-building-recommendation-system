Generating and Visualizing a Soccer Knowledge Base
Paul Buitelaar, Thomas Eigner, Greg Gul-
rajani, Alexander Schutz, Melanie Siegel,
Nicolas Weber
Language Technology Lab, DFKI GmbH
Saarbr?cken, Germany
{paulb,siegel}@dfki.de
Philipp Cimiano, G?nter Ladwig,
Matthias Mantel, Honggang Zhu
Institute AIFB, University of Karlsruhe
Karlsruhe, Germany
cimiano@aifb.uni-karlsruhe.de
Abstract
This demo abstract describes the SmartWeb
Ontology-based Annotation system (SOBA).
A key feature of SOBA is that all informa-
tion is extracted and stored with respect to
the SmartWeb Integrated Ontology
(SWIntO). In this way, other components of
the systems, which use the same ontology,
can access this information in a straightfor-
ward way. We will show how information
extracted by SOBA is visualized within its
original context, thus enhancing the browsing
experience of the end user.
1 Introduction
SmartWeb1 is a multi-modal dialog system,
which derives answers from unstructured re-
sources such as the Web, from automatically ac-
quired knowledge bases and from web services.
In this paper we describe the current status of
the SmartWeb Ontology-Based Annotation
(SOBA) system. SOBA automatically populates
a knowledge base by information extraction from
soccer match reports as available on the web.
The extracted information is defined with respect
to SWIntO, the underlying SmartWeb Integrated
Ontology (Oberle et al, in preparation) in order
to be smoothly integrated into the system.
The ability to extract information and describe
it ontologically is a basic requirement for more
complex processing tasks such as reasoning and
discourse analysis (for related work on ontology-
based information extraction see e.g. Maedche et
al., 2002; Lopez and Motta, 2004; M?ller et al,
2004; Nirenburg and Raskin, 2004).
1 http://www.smartweb-projekt.de/start_en.html
2 System Overview
The SOBA system consists of a web crawler,
linguistic annotation components and a compo-
nent for the transformation of linguistic annota-
tions into an ontology-based representation.
The web crawler acts as a monitor on relevant
web domains (i.e. the FIFA2 and UEFA3 web
sites), automatically downloads relevant
documents from them and sends them to a
linguistic annotation web service.
Linguistic annotation and information
extraction is based on the Heart-of-Gold (HoG)
architecture (Callmeier et al 2004), which
provides a uniform and flexible infrastructure for
building multilingual applications that use
semantics- and XML-based natural language
processing components.
The linguistically annotated documents are
further processed by the transformation
component, which generates a knowledge base
of soccer-related entities (players, teams, etc.)
and events (matches, goals, etc.) by mapping
annotated entities or events to ontology classes
and their properties.
Finally, an automatic hyperlinking component
is used for the visualization of extracted entities
and events. This component is based on the
VieWs system, which was developed
independently of SmartWeb (Buitelaar et al,
2005). In what follows we describe the different
components of the system in detail.
2.1 Web Crawler
The crawler enables the automatic creation of a
football corpus, which is kept up-to-date on a
daily basis. The crawler data is compiled from
texts, semi-structured data and copies of original
2 http://fifaworldcup.yahoo.com/
3 http://www.uefa.com/
123
HTML documents. For each football match, the
data source contains a sheet of semi-structured
data with tables of players, goals, referees, etc.
Textual data comprise of match reports as well as
news articles.
The crawler is able to extract data from two
different sources: FIFA and UEFA. Semi-
structured data, news articles and match reports
covering the WorldCup2006 are identified and
collected from the FIFA website. Match reports
and news articles are extracted from the UEFA
website. The extracted data are labeled by IDs
that match the filename. The IDs are derived
from the corresponding URL and are thus
unique.
The crawler is invoked continuously each day
with the same configuration, extracting only data
which is not yet contained in the corpus. In order
to distinguish between available new data and
data already present in the corpus, the URLs of
all available data from the website are matched
against the IDs of the already extracted data.
2.2 Linguistic Annotation and Information
Extraction
As mentioned before, linguistic annotation in the
system is based on the HoG architecture, which
provides a uniform and flexible infrastructure for
building multilingual applications that use
semantics- and XML-based natural language
processing components.
For the annotation of soccer game reports, we
extended the rule set of the SProUT (Drozdzyn-
ski et al 2004) named-entity recognition compo-
nent in HoG with gazetteers, part-of-speech and
morphological information. SProUT combines
finite-state techniques and unification-based al-
gorithms. Structures to be extracted are ordered
in a type hierarchy, which we extended with soc-
cer-specific rules and output types.
SProUT has basic grammars for the annotation
of persons, locations, numerals and date and time
expressions. On top of this, we implemented
rules for soccer-specific entities, such as actors in
soccer (trainer, player, referee ?), teams, games
and tournaments. Using these, we further imple-
mented rules for soccer-specific events, such as
player activities (shots, headers ?), game events
(goal, card ?) and game results. A soccer-
specific gazetteer contains soccer-specific enti-
ties and names and is supplemented to the gen-
eral named-entity gazetteer.
As an example, consider the linguistic annota-
tion for the following German sentence from one
of the soccer game reports:
Guido Buchwald wurde 1990 in Italien Welt-
meister (Guido Buchwald became world cham-
pion in 1990 in Italy)
<FS type="player_action">
<F name="GAME_EVENT">
<FS type="world champion"/>
<F name="ACTION_TIME">
<FS type="1990"/>
<F name="ACTION_LOCATION">
<FS type="Italy"/>
<F name="AGENT">
<FS type="player">
<F name="SURNAME">
<FS type="Buchwald"/>
<F name="GIVEN_NAME">
<FS type="Guido"/>
2.3 Knowledge Base Generation
The SmartWeb SportEventOntology (a subset of
SWIntO) contains about 400 direct classes onto
which named-entities and other, more complex
structures are mapped. The mapping is repre-
sented in a declarative fashion specifying how
the feature-based structures produced by SProUT
are mapped into structures which are compatible
with the underlying ontology. Further, the newly
extracted information is also interpreted in the
context of additional information about the
match in question.
This additional information is obtained by
wrapping the semi-structured data on relevant
soccer matches, which is also mapped to the on-
tology. The information obtained in this way
about the match in question can then be used as
contextual background with respect to which the
newly extracted information is interpreted.
The feature structure for player as displayed
above will be translated into the following F-
Logic (Kifer et al 1995) statements, which are
then automatically translated to RDF and fed to
the visualization component:
soba#player124:sportevent#FootballPlayer
[sportevent#impersonatedBy ->
soba#Guido_BUCHWALD].
soba#Guido_BUCHWALD:dolce#"natural-person"
[dolce#"HAS-DENOMINATION" ->
soba#Guido_BUCHWALD_Denomination].
soba#Guido_BUCHWALD_Denomination":dolce#"
natural-person-denomination"
[dolce#LASTNAME -> "Buchwald";
dolce#FIRSTNAME -> "Guido"].
124
2.4 Knowledge Base Visualization
The generated knowledge base is visualized by
way of automatically inserted hyperlink menus
for soccer-related named-entities such as players
and teams. The visualization component is based
on the VIeWs4 system. VIeWs allows the user to
simply browse a web site as usual, but is addi-
tionally supported by the automatic hyperlinking
system that adds additional information from a
(generated) knowledge base.
For some examples of this see the included
figures below, which show extracted information
for the Panama team (i.e. all of the football play-
ers in this team in Figure 1) and for the player
Roberto Brown (i.e. his team and events in which
he participated in Figure 2).
3 Implementation
All components are implemented in Java 1.5 and
are installed as web applications on a Tomcat
web server. SOAP web services are used for
communication between components so that the
system can be installed in a centralized as well as
decentralized manner. Data communication is
handled by XML-based exchange formats. Due
to a high degree of flexibility of components,
only a simple configuration over environment
variables is needed.
4 Conclusions and Future Work
We presented an ontology-based approach to
information extraction in the soccer domain that
aims at the automatic generation of a knowledge
base from match reports and the subsequent
visualization of the extracted information
through automatic hyperlinking. We argue that
such an approach is innovative and enhances the
user experience.
Future work includes the extraction of more
complex events, for which deep linguistic analy-
sis and/or semantic inference over the ontology
and knowledge base is required. For this purpose
we will use an HPSG-based parser that is avail-
able within the HoG architecture (Callmeier,
2000) and combine this with a semantic infer-
ence approach based on discourse analysis
(Cimiano et al, 2005).
4 http://views.dfki.de
Acknowledgements
This research has been supported by grants for
the projects SmartWeb (by the German Ministry
of Education and Research: 01 IMD01 A) and
VIeWs (by the Saarland Ministry of Economic
Affairs).
References
Paul Buitelaar, Thomas Eigner, Stefania Racioppa
Semantic Navigation with VIeWs In: Proc. of the
Workshop on User Aspects of the Semantic Web at
the European Semantic Web Conference, Herak-
lion, Greece, May 2005.
Callmeier, Ulrich (2000). PET ? A platform for ex-
perimentation with efficient HPSG processing
techniques. In: Natural Language Engineering, 6
(1) UK: Cambridge University Press pp. 99?108.
Callmeier, Ulrich, Eisele, Andreas, Sch?fer, Ulrich
and Melanie Siegel. 2004. The DeepThought Core
Architecture Framework In Proceedings of LREC
04, Lisbon, Portugal, pages 1205-1208.
Cimiano, Philipp, Saric, Jasmin and Uwe Reyle.
2005. Ontology-driven discourse analysis for in-
formation extraction, Data Knowledge Engineering
55(1).
Drozdzynski, Witold, Hans-Ulrich Krieger, Jakub
Piskorski, Ulrich Sch?fer, and Feiyu Xu. 2004.
Shallow processing with unification and typed fea-
ture structures ? foundations and applications.
K?nstliche Intelligenz, 1:17-23.
Kifer, M., Lausen, G. and J.Wu. 1995. Logical Foun-
dations of Object-Oriented and Frame-Based Lan-
guages. Journal of the ACM 42, pp. 741-843.
Lopez, V. and E. Motta. 2004. Ontology-driven Ques-
tion Answering in AquaLog In Proceedings of 9th
International Conference on applications of natural
language to information systems.
Maedche, Alexander, G?nter Neumann and Steffen
Staab. 2002. Bootstrapping an Ontology-Based In-
formation Extraction System. In: Studies in Fuzzi-
ness and Soft Computing, editor J. Kacprzyk. Intel-
ligent Exploration of the Web, Springer.
M?ller HM, Kenny EE and PW Sternberg. 2004.
Textpresso: An ontology-based information re-
trieval and extraction system for biological litera-
ture. PLoS Biol 2: e309.
Nirenburg, Sergei and Viktor Raskin. 2004. Ontologi-
cal Semantics. MIT Press.
Oberle et al The SmartWeb Integrated Ontology
SWIntO, in preparation.
125
Figure 2: Generated hyperlink on ?Roberto Brown? with extracted information on his
team and events in which he participated
Figure 1: Generated hyperlink on ?Panama? with extracted information on this team
126
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 888?895,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Automatic Acquisition of Ranked Qualia Structures from the Web1
Philipp Cimiano
Inst. AIFB, University of Karlsruhe
Englerstr. 11, D-76131 Karlsruhe
cimiano@aifb.uni-karlsruhe.de
Johanna Wenderoth
Inst. AIFB, University of Karlsruhe
Englerstr. 11, D-76131 Karlsruhe
jowenderoth@googlemail.com
Abstract
This paper presents an approach for the au-
tomatic acquisition of qualia structures for
nouns from the Web and thus opens the pos-
sibility to explore the impact of qualia struc-
tures for natural language processing at a
larger scale. The approach builds on ear-
lier work based on the idea of matching spe-
cific lexico-syntactic patterns conveying a
certain semantic relation on the World Wide
Web using standard search engines. In our
approach, the qualia elements are actually
ranked for each qualia role with respect to
some measure. The specific contribution of
the paper lies in the extensive analysis and
quantitative comparison of different mea-
sures for ranking the qualia elements. Fur-
ther, for the first time, we present a quan-
titative evaluation of such an approach for
learning qualia structures with respect to a
handcrafted gold standard.
1 Introduction
Qualia structures have been originally introduced
by (Pustejovsky, 1991) and are used for a variety
of purposes in natural language processing (NLP),
such as for the analysis of compounds (Johnston and
Busa, 1996) as well as co-composition and coercion
(Pustejovsky, 1991), but also for bridging reference
resolution (Bos et al, 1995). Further, it has also
1The work reported in this paper has been supported by the
X-Media project, funded by the European Commission under
EC grant number IST-FP6-026978 as well by the SmartWeb
project, funded by the German Ministry of Research. Thanks
to all our colleagues for helping to evaluate the approach.
been argued that qualia structures and lexical seman-
tic relations in general have applications in informa-
tion retrieval (Voorhees, 1994; Pustejovsky et al,
1993). One major bottleneck however is that cur-
rently qualia structures need to be created by hand,
which is probably also the reason why there are al-
most no practical NLP systems using qualia struc-
tures, but a lot of systems relying on publicly avail-
able resources such as WordNet (Fellbaum, 1998)
or FrameNet (Baker et al, 1998) as source of lex-
ical/world knowledge. The work described in this
paper addresses this issue and presents an approach
to automatically learning qualia structures for nouns
from the Web. The approach is inspired in recent
work on using the Web to identify instances of a re-
lation of interest such as in (Markert et al, 2003) and
(Etzioni et al, 2005). These approaches rely on a
combination of the usage of lexico-syntactic pattens
conveying a certain relation of interest as described
in (Hearst, 1992) with the idea of using the web as a
big corpus (cf. (Kilgariff and Grefenstette, 2003)).
Our approach directly builds on our previous work
(Cimiano and Wenderoth, 2005) an adheres to the
principled idea of learning ranked qualia structures.
In fact, a ranking of qualia elements is useful as it
helps to determine a cut-off point and as a reliabil-
ity indicator for lexicographers inspecting the qualia
structures. In contrast to our previous work, the fo-
cus of this paper lies in analyzing different measures
for ranking the qualia elements in the automatically
acquired qualia structures. We also introduce ad-
ditional patterns for the agentive role which make
use of wildcard operators. Further, we present a
gold standard for qualia structures created for the 30
words used in the evaluation of Yamada and Bald-
win (Yamada and Baldwin, 2004). The evaluation
888
presented here is thus much more extensive than our
previous one (Cimiano and Wenderoth, 2005), in
which only 7 words were used. We present a quanti-
tative evaluation of our approach and a comparison
of the different ranking measures with respect to this
gold standard. Finally, we also provide an evaluation
in which test persons were asked to inspect and rate
the learned qualia structures a posteriori. The paper
is structured as follows: Section 2 introduces qualia
structures for the sake of completeness and describes
the specific structures we aim to acquire. Section
3 describes our approach in detail, while Section 4
discusses the ranking measures used. Section 5 then
presents the gold standard as well as the qualitative
evaluation of our approach. Before concluding, we
discuss related work in Section 6.
2 Qualia Structures
In the Generative Lexicon (GL) framework (Puste-
jovsky, 1991), Pustejovsky reused Aristotle?s basic
factors (i.e. the material, agentive, formal and final
causes) for the description of the meaning of lexi-
cal elements. In fact, he introduced so called qualia
structures by which the meaning of a lexical ele-
ment is described in terms of four roles: Constitutive
(describing physical properties of an object, i.e. its
weight, material as well as parts and components),
Agentive (describing factors involved in the bringing
about of an object, i.e. its creator or the causal chain
leading to its creation), Formal (describing proper-
ties which distinguish an object within a larger do-
main, i.e. orientation, magnitude, shape and dimen-
sionality), and Telic (describing the purpose or func-
tion of an object).
Most of the qualia structures used in (Pustejovsky,
1991) however seem to have a more restricted inter-
pretation. In fact, in most examples the Constitutive
role seems to describe the parts or components of an
object, while the Agentive role is typically described
by a verb denoting an action which typically brings
the object in question into existence. The Formal
role normally consists in typing information about
the object, i.e. its hypernym. In our approach, we
aim to acquire qualia structures according to this re-
stricted interpretation.
3 Automatically Acquiring Qualia
Structures
Our approach to learning qualia structures from the
Web is on the one hand based on the assumption
that instances of a certain semantic relation can be
acquired by matching certain lexico-syntactic pat-
terns more or less reliably conveying the relation
of interest in line with the seminal work of Hearst
(Hearst, 1992), who defined patterns conveying hy-
ponym/hypernym relations. However, it is well
known that Hearst-style patterns occur rarely, such
that matching these patterns on the Web in order
to alleviate the problem of data sparseness seems a
promising solution. In fact, in our case we are not
only looking for the hypernym relation (comparable
to the Formal-role) but for similar patterns convey-
ing a Constitutive, Telic or Agentive relation. Our
approach consists of 5 phases; for each qualia term
(the word we want to find the qualia structure for)
we:
1. generate for each qualia role a set of so called
clues, i.e. search engine queries indicating the
relation of interest,
2. download the snippets (abstracts) of the 50 first
web search engine results matching the generated
clues,
3. part-of-speech-tag the downloaded snippets,
4. match patterns in the form of regular expressions
conveying the qualia role of interest, and
5. weight and rank the returned qualia elements ac-
cording to some measure.
The patterns in our pattern library are actually
tuples (p, c) where p is a regular expression de-
fined over part-of-speech tags and c a function c :
string ? string called the clue. Given a nomi-
nal n and a clue c, the query c(n) is sent to the web
search engine and the abstracts of the first m docu-
ments matching this query are downloaded. Then
the snippets are processed to find matches of the
pattern p. For example, given the clue f(x) =
?such as p(x)?? and the qualia term computer we
would download m abstracts matching the query
f(computer), i.e. ?such as computers?. Hereby p(x)
is a function returning the plural form of x. We im-
plemented this function as a lookup in a lexicon in
which plural nouns are mapped to their base form.
With the use of such clues, we thus download a num-
889
ber of snippets returned by the web search engine in
which a corresponding regular expression will prob-
ably be matched, thus restricting the linguistic anal-
ysis to a few promising pages. The downloaded ab-
stracts are then part-of-speech tagged using QTag
(Tufis and Mason, 1998). Then we match the corre-
sponding pattern p in the downloaded snippets thus
yielding candidate qualia elements as output. The
qualia elements are then ranked according to some
measure (compare Section 4), resulting in what we
call Ranked Qualia Structures (RQSs). The clues
and patterns used for the different roles can be found
in Tables 1 - 4. In the specification of the clues, the
function a(x) returns the appropriate indefinite arti-
cle ? ?a? or ?an? ? or no article at all for the noun x.
The use of an indefinite article or no article at all ac-
counts for the distinction between countable nouns
(e.g. such as knife) and mass nouns (e.g. water).
The choice between using the articles ?a?, ?an? or
no article at all is determined by issuing appropriate
queries to the web search engine and choosing the
article leading to the highest number of results. The
corresponding patterns are then matched in the 50
snippets returned by the search engine for each clue,
thus leading to up to 50 potential qualia elements per
clue and pattern2. The patterns are actually defined
over part-of-speech tags. We indicate POS-tags in
square brackets. However, for the sake of simplic-
ity, we largely omit the POS-tags for the lexical ele-
ments in the patterns described in Tables 1 - 4. Note
that we use traditional regular expression operators
such as ? (sequence), + (sequence with at least one
element) | (alternative) and ? (option). In general,
we define a noun phrase (NP) by the following reg-
ular expression: NP:=[DT]? ([JJ])+? [NN(S?)])+3,
where the head is the underlined expression, which
is lemmatized and considered as a candidate qualia
element. For all the patterns described in this sec-
tion, the underlined part corresponds to the extracted
qualia element. In the patterns for the formal role
(compare Table 1), NPQT is a noun phrase with the
qualia term as head, whereas NPF is a noun phrase
with the potential qualia element as head. For the
constitutive role patterns, we use a noun phrase vari-
2For the constitutive role these can be even more due to the
fact that we consider enumerations.
3Though Qtag uses another part-of-speech tagset, we rely on
the well-known Penn Treebank tagset for presentation purposes.
Clue Pattern
Singular
?a(x) x is a kind of ? NPQT is a kind of NPF
?a(x) x is? NPQT is a kind of NPF
?a(x) x and other? NPQT (,)? and other NPF
?a(x) x or other? NPQT (,)? or other NPF
Plural
?such as p(x)? NPF such as NPQT
?p(x) and other? NPQT (,)? and other NPF
?p(x) or other? NPQT (,)? or other NPF
?especially p(x)? NPF (,)? especially NPQT
?including p(x)? NPF (,)? including NPQT
Table 1: Clues and Patterns for the Formal role
ant NP? defined by the regular expression NP?:=
(NP of[IN])? NP (, NP)* ((,)? (and|or) NP)?, which
allows to extract enumerations of constituents (com-
pare Table 2). It is important to mention that in the
case of expressions such as ?a car comprises a fixed
number of basic components?, ?data mining com-
prises a range of data analysis techniques?, ?books
consist of a series of dots?, or ?a conversation is
made up of a series of observable interpersonal ex-
changes?, only the NP after the preposition ?of? is
taken into account as qualia element. The Telic Role
is in principle acquired in the same way as the For-
mal and Constitutive roles with the exception that
the qualia element is not only the head of a noun
phrase, but also a verb or a verb followed by a noun
phrase. Table 3 gives the corresponding clues and
patterns. In particular, the returned candidate qualia
elements are the lemmatized underlined expressions
in PURP:=[VB] NP | NP | be[VBD]. Finally, con-
cerning the clues and patterns for the agentive role
shown in Table 4, it is interesting to emphasize the
usage of the adjectives ?new? and ?complete?. These
adjectives are used in the patterns to increase the ex-
pectation for the occurrence of a creation verb. Ac-
cording to our experiments, these patterns are in-
deed more reliable in finding appropriate qualia ele-
ments than the alternative version without the adjec-
tives ?new? and ?complete?. Note that in all patterns,
the participle (VBD) is always reduced to base form
(VB) via a lexicon lookup. In general, the patterns
have been crafted by hand, testing and refining them
in an iterative process, paying attention to maximize
their coverage but also accuracy. In the future, we
plan to exploit an approach to automatically learn
the patterns.
890
Clue Pattern
Singular
?a(x) x is made up of ? NPQT is made up of NP?C
?a(x) x is made of? NPQT is made of NP?C
?a(x) x comprises? NPQT comprises (of)? NP?C
?a(x) x consists of? NPQT consists of NP?C
Plural
?p(x) are made up of ? NPQT is made up of NP?C
?p(x) are made of? NPQT are made of NP?C
?p(x) comprise? NPQT comprise (of)? NP?C
?p(x) consist of? NPQT consist of NP?C
Table 2: Clues and Patterns for the Constitutive Role
Clue Pattern
Singular
?purpose of a(x) x is? purpose of (a|an) x is (to)? PURP
?a(x) is used to? (a|an) x is used to PURP
Plural
?purpose of p(x) is? purpose of p(x) is (to)? PURP
?p(x) are used to? p(x) are used to PURP
Table 3: Clues and Patterns for the Telic Role
4 Ranking Measures
In order to rank the different qualia elements of a
given qualia structure, we rely on a certain ranking
measure. In our experiments, we analyze four differ-
ent ranking measures. On the one hand, we explore
measures which use the Web to calculate the corre-
lation strength between a qualia term and its qualia
elements. These measures are Web-based versions
of the Jaccard coefficient (Web-Jac), the Pointwise
Mutual Information (Web-PMI) and the conditional
probability (Web-P). We also present a version of
the conditional probability which does not use the
Web but merely relies on the counts of each qualia
element as produced by the lexico-syntactic patterns
(P-measure). We describe these measures in the fol-
lowing.
4.1 Web-based Jaccard Measure (Web-Jac)
Our web-based Jaccard (Web-Jac) measure relies on
the web search engine to calculate the number of
documents in which x and y co-occur close to each
other, divided by the number of documents each one
occurs, i.e.
Web-Jac(x, y) :=
Hits(x ? y)
Hits(x) + Hits(y) ? Hits(x AND y)
So here we are relying on the wildcard operator ?*?
provided by the Google search engine API4. Though
4In fact, for the experiments described in this paper we rely
on the Google API.
Clue Pattern
Singular
?to * a(x) new x? to [RB]? [VB] a? new x
?to * a(x) complete x? to [RB]? [VB] a? complete x
?a(x) new has been *? a? new x has been [VBD]
?a(x) complete x has been *? a? complete has been [VBD]
Plural
?to * new p(x)? to [RB]? [VB] new p(x)
?to * complete p(x)? to [RB]? [VB] complete p(x)
Table 4: Clues and Patterns for the Agentive Role
the specific function of the ?*? operator as imple-
mented by Google is actually unknown, the behavior
is similar to the formerly available Altavista NEAR
operator5.
4.2 Web-based Pointwise Mutual Information
(Web-PMI)
In line with Magnini et al (Magnini et al, 2001),
we define a PMI-based measure as follows:
Web ? PMI(x, y) := log2
Hits(x AND y) MaxPages
Hits(y) Hits(y)
where maxPages is an approximation for the maxi-
mum number of English web pages6.
4.3 Web-based Conditional Probability
(Web-P)
The conditional probability P (x|y) is essentially
the probability that x is true given that y is true, i.e.
Web-P(x, y) := P (x|y) = P (x,y)P (y) =
Hits(x NEAR y)
Hits(y)
whereby Hits(x NEAR y) is calculated as
mentioned above using the ?*? operator. In contrast
to the measures described above, this one is asym-
metric so that order indeed matters. Given a qualia
term qt as well as a qualia element qe we actually
calculate Web-P(qe,qt) for a specific qualia role.
4.4 Conditional Probability (P)
The non web-based conditional probability essen-
tially differs from the Web-based conditional prob-
ability in that we only rely on the qualia elements
5Initial experiments indeed showed that counting pages in
which the two terms occur near each other in contrast to count-
ing pages in which they merely co-occur improved the results
of the Jaccard measure by about 15%.
6We determine this number experimentally as the number of
web pages containing the words ?the? and ?and?.
891
matched. On the basis of these, we then calculate
the probability of a certain qualia element given a
certain role on the basis of its frequency of appear-
ance with respect to the total number of qualia ele-
ments derived for this role, i.e. we simply calculate
P (qe|qr, qt) on the basis of the derived occurrences,
where qt is a given qualia term, qr is the specific
qualia role and qe is a qualia element.
5 Evaluation
In this section, we first of all describe our evaluation
measures. Then we describe the creation of the gold
standard. Further, we present the results of the com-
parison of the different ranking measures with re-
spect to the gold standard. Finally, we present an ?a
posteriori? evaluation showing that the qualia struc-
tures learned are indeed reasonable.
5.1 Evaluation Measures
As our focus is to compare the different measures
described above, we need to evaluate their corre-
sponding rankings of the qualia elements for each
qualia structure. This is a similar case to evaluat-
ing the ranking of documents within information re-
trieval systems. In fact, as done in standard infor-
mation retrieval research, our aim is to determine
for each ranking the precision/recall trade-off when
considering more or less of the items starting from
the top of the ranked list. Thus, we evaluate our ap-
proach calculating precision at standard recall levels
as typically done in information retrieval research
(compare (Baeza-Yates and Ribeiro-Neto, 1999)).
Hereby the 11 standard recall levels are 0%, 10%,
20%, 30%, 40%, 50%, 60%, 70%, 80%, 90% and
100%. Further, precision at these standard recall
levels is calculated by interpolating recall as fol-
lows: P (rj) = maxrj?r?rj+1P (r), where, j ?
{0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1}. This
way we can compare the precision over standard re-
call figures for the different rankings, thus observing
which measure leads to the better precision/recall
trade-off.
In addition, in order to provide one single value
to compare, we also calculate the F-Measure cor-
responding to the best precision/recall trade-off for
each ranking measure. This F-Measure thus corre-
sponds to the best cut-off point we can find for the
items in the ranked list. In fact, we use the well-
known F1 measure corresponding to the harmonic
mean between recall and precision:
F1 := maxj
2 P (rj) rj
P (rj) + rj
As a baseline, we compare our results to a naive
strategy without any ranking, i.e. we calculate the
F-Measure for all the items in the (unranked) list of
qualia elements. Consequently, for the rankings to
be useful, they need to yield higher F-Measures than
this naive baseline.
5.2 Gold Standard
The gold standard was created for the 30 words used
already in the experiments described in (Yamada and
Baldwin, 2004): accounting, beef, book, car, cash,
clinic, complexity, counter, county, delegation, door,
estimate, executive, food, gaze, imagination, inves-
tigation, juice, knife, letter, maturity, novel, phone,
prisoner, profession, review, register, speech, sun-
shine, table. These words were distributed more or
less uniformly between 30 participants of our exper-
iment, making sure that three qualia structures for
each word were created by three different subjects.
The participants, who were all non-linguistics, re-
ceived a short instruction in the form of a short pre-
sentation explaining what qualia structures are, the
aims of the experiment as well as their specific task.
They were also shown some examples for qualia
structures for words not considered in our experi-
ments. Further, they were asked to provide between
5 and 10 qualia elements for each qualia role. The
participants completed the test via e-mail. As a first
interesting observation, it is worth mentioning that
the participants only delivered 3-5 qualia elements
on average depending on the role in question. This
shows already that participants had trouble in find-
ing different qualia elements for a given qualia role.
We calculate the agreement for the task of specify-
ing qualia structures for a particular term and role as
the averaged pairwise agreement between the qualia
elements delivered by the three subjects, henceforth
S1, S2 and S3 as:
Agr :=
|S1?S2|
|S1?S2|
+ |S1?S3||S1?S3| +
|S2?S3|
|S2?S3|
3
Averaging over all the roles and words, we get an
average agreement of 11.8%, i.e. our human test
892
subjects coincide in slightly more than every 10th
qualia element. This is certainly a very low agree-
ment and certainly hints at the fact that the task con-
sidered is certainly difficult. The agreement was
lowest (7.29%) for the telic role.
A further interesting observation is that the lowest
agreement is yielded for more abstract words, while
the agreement for very concrete words is reasonable.
For example, the five words with the highest agree-
ment are indeed concrete things: knife (31%), cash
(29%), juice (21%), car (20%) and door (19%). The
words with an agreement below 5% are gaze, pris-
oner, accounting, maturity, complexity and delega-
tion. In particular, our test subjects had substantial
difficulties in finding the purpose of such abstract
words. In fact, the agreement on the telic role is be-
low 5% for more than half of the words.
In general, this shows that any automatic ap-
proach towards learning qualia structures faces se-
vere limits. For sure, we can not expect the results
of an automatic evaluation to be very high. For ex-
ample, for the telic role of ?clinic?, one test subject
specified the qualia element ?cure?, while another
one specified ?cure disease?, thus leading to a dis-
agreement in spite of the obvious agreement at the
semantic level. In this line, the average agreement
reported above has in fact to be regarded as a lower
bound for the actual agreement. Of course, our ap-
proach to calculating agreement is too strict, but in
absence of a clear and computable definition of se-
mantic agreement, it will suffice for the purposes of
this paper.
5.3 Gold Standard Evaluation
We ran experiments calculating the qualia structure
for each of the 30 words, ranking the resulting qualia
elements for each qualia structure using the different
measures described in Section 4.
Figure 1 shows the best F-Measure correspond-
ing to a cut-off leading to an optimal precision/recall
trade-off. We see that the P -measure performs best,
while the Web-P measure and the Web-Jac measure
follow at about 0.05 and 0.2 points distance. The
PMI-based measure indeed leads to the worst F-
Measure values.
Indeed, the P -measure delivered the best results
for the formal and agentive roles, while for the con-
stitutive and telic roles the Web-Jac measure per-
Figure 1: Average F1 measure for the different rank-
ing measures
formed best. The reason why PMI performs so badly
is the fact that it favors too specific results which
are unlikely to occur as such in the gold standard.
For example, while the conditional probability ranks
highest: explore, help illustrate, illustrate and en-
rich for the telic role of novel, the PMI-based mea-
sure ranks highest: explore great themes, illustrate
theological points, convey truth, teach reading skills
and illustrate concepts. A series of significance tests
(paired Student?s t-test at an ?-level of 0.05) showed
that the three best performing measures (P , Web-
P and Web-Jaccard) show no real difference among
them, while all three show significant difference to
the Web-PMI measure. A second series of signif-
icance tests (again paired Student?s t-test at an ?-
level of 0.05) showed that all ranking measures in-
deed significantly outperform the baseline, which
shows that our rankings are indeed reasonable. In-
terestingly, there seems to be an interesting positive
correlation between the F-Measure and the human
agreement. For example, for the best performing
ranking measure, i.e. the P -measure, we get an av-
erage F-Measure of 21% for words with an agree-
ment over 5%, while we get an F-Measure of 9%
for words with an agreement below 5%. The rea-
son here probably is that those words and qualia ele-
ments for which people are more confident also have
a higher frequency of appearance on the Web.
5.4 A posteriori Evaluation
In order to check whether the automatically learned
qualia structures are reasonable from an intuitive
point of view, we also performed an a posteriori
893
evaluation in the lines of (Cimiano and Wenderoth,
2005). In this experiment, we presented the top 10
ranked qualia elements for each qualia role for 10
randomly selected words to the different test per-
sons. Here we only used the P -measure for rank-
ing as it performed best in our previous evaluation
with regard to the gold standard. In order to ver-
ify that our sample is not biased, we checked that
the F-Measure yielded by our 10 randomly selected
words (17.7%) does not differ substantially from the
overall average F-Measure (17.1%) to be sure that
we have chosen words from all F-Measure ranges.
In particular, we asked different test subjects which
also participated in the creation of the gold standard
to rate the qualia elements with respect to their ap-
propriateness for the qualia term using a scale from
0 to 3, whereby 0 means ?wrong?, 1 ?not totally
wrong?, 2 ?acceptable? and 3 ?totally correct?. The
participants confirmed that it was easier to validate
existing qualia structures than to create them from
scratch, which already corroborates the usefulness
of our automatic approach. The qualia structure for
each of the 10 randomly selected words was vali-
dated independently by three test persons. In fact,
in what follows we always report results averaged
for three test subjects. Figure 2 shows the average
values for different roles. We observe that the con-
stitutive role yields the best results, followed by the
formal, telic and agentive roles (in this order). In
general, all results are above 2, which shows that
the qualia structures produced are indeed acceptable.
Though we do not present these results in more de-
tail due to space limitations, it is also interesting to
mention that the F-Measure calculated with respect
to the gold standard was in general highly correlated
with the values assigned by the human test subjects
in this a posteriori validation.
6 Related Work
Instead of matching Hearst-style patterns (Hearst,
1992) in a large text collection, some researchers
have recently turned to the Web to match these pat-
terns such as in (Markert et al, 2003) or (Etzioni et
al., 2005). Our approach goes further in that it not
only learns typing, superconcept or instance-of rela-
tions, but also Constitutive, Telic and Agentive rela-
tions.
Figure 2: Average ratings for each qualia role
There also exist approaches specifically aiming at
learning qualia elements from corpora based on ma-
chine learning techniques. Claveau et al (Claveau
et al, 2003) for example use Inductive Logic Pro-
gramming to learn if a given verb is a qualia ele-
ment or not. However, their approach does no go
as far as learning the complete qualia structure for a
lexical element as in our approach. Further, in their
approach they do not distinguish between different
qualia roles and restrict themselves to verbs as po-
tential fillers of qualia roles.
Yamada and Baldwin (Yamada and Baldwin, 2004)
present an approach to learning Telic and Agentive
relations from corpora analyzing two different ap-
proaches: one relying on matching certain lexico-
syntactic patterns as in the work presented here, but
also a second approach consisting in training a max-
imum entropy model classifier. The patterns used
by (Yamada and Baldwin, 2004) differ substantially
from the ones used in this paper, which is mainly
due to the fact that search engines do not provide
support for regular expressions and thus instantiat-
ing a pattern as ?V[+ing] Noun? is impossible in our
approach as the verbs are unknown a priori.
Poesio and Almuhareb (Poesio and Almuhareb,
2005) present a machine learning based approach to
classifying attributes into the six categories: qual-
ity, part, related-object, activity, related-agent and
non-attribute.
7 Conclusion
We have presented an approach to automatically
learning qualia structures from the Web. Such an
approach is especially interesting either for lexicog-
894
raphers aiming at constructing lexicons, but even
more for natural language processing systems re-
lying on deep lexical knowledge as represented by
qualia structures. In particular, we have focused
on learning ranked qualia structures which allow
to find an ideal cut-off point to increase the preci-
sion/recall trade-off of the learned structures. We
have abstracted from the issue of finding the appro-
priate cut-off, leaving this for future work. In partic-
ular, we have evaluated different ranking measures
for this purpose, showing that all of the analyzed
measures (Web-P, Web-Jaccard, Web-PMI and the
conditional probability) significantly outperformed
a baseline using no ranking measure. Overall, the
plain conditional probability P (not calculated over
the Web) as well as the conditional probability cal-
culated over the Web (Web-P) delivered the best re-
sults, while the PMI-based ranking measure yielded
the worst results. In general, our main aim has been
to show that, though the task of automatically learn-
ing qualia structures is indeed very difficult as shown
by our low human agreement, reasonable structures
can indeed be learned with a pattern-based approach
as presented in this paper. Further work will aim
at inducing the patterns automatically given some
seed examples, but also at using the automatically
learned structures within NLP applications. The cre-
ated qualia structure gold standard is available for
the community7.
References
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Modern In-
formation Retrieval. Addison-Wesley.
C.F. Baker, C.J. Fillmore, and J.B. Lowe. 1998. The
Berkeley FrameNet Project. In Proceedings of COL-
ING/ACL?98, pages 86?90.
J. Bos, P. Buitelaar, and M. Mineur. 1995. Bridging as
coercive accomodation. In Working Notes of the Edin-
burgh Conference on Computational Logic and Natu-
ral Language Processing (CLNLP-95).
P. Cimiano and J. Wenderoth. 2005. Learning qualia
structures from the web. In Proceedings of the ACL
Workshop on Deep Lexical Acquisition, pages 28?37.
V. Claveau, P. Sebillot, C. Fabre, and P. Bouillon. 2003.
Learning semantic lexicons from a part-of-speech and
semantically tagged corpus using inductive logic pro-
gramming. Journal of Machine Learning Research,
(4):493?525.
7See http://www.cimiano.de/qualia.
O. Etzioni, M. Cafarella, D. Downey, A-M. Popescu,
T. Shaked, S. Soderland, D.S. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: An experimental study. Artificial Intelligence,
165(1):91?134.
C. Fellbaum. 1998. WordNet, an electronic lexical
database. MIT Press.
M.A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COL-
ING?92, pages 539?545.
M. Johnston and F. Busa. 1996. Qualia structure and the
compositional interpretation of compounds. In Pro-
ceedings of the ACL SIGLEX workshop on breadth and
depth of semantic lexicons.
A. Kilgariff and G. Grefenstette, editors. 2003. Special
Issue on the Web as Corpus of the Journal of Compu-
tational Linguistics, volume 29(3). MIT Press.
B. Magnini, M. Negri, R. Prevete, and H. Tanev. 2001.
Is it the right answer?: exploiting web redundancy for
answer validation. In Proceedings of the 40th Annual
Meeting of the ACL, pages 425?432.
K. Markert, N. Modjeska, and M. Nissim. 2003. Us-
ing the web for nominal anaphora resolution. In Pro-
ceedings of the EACL Workshop on the Computational
Treatment of Anaphora.
M. Poesio and A. Almuhareb. 2005. Identifying concept
attributes using a classifier. In Proceedings of the ACL
Workshop on Deep Lexical Acquisition, pages 18?27.
J. Pustejovsky, P. Anick, and S. Bergler. 1993. Lexi-
cal semantic techniques for corpus analysis. Compu-
tational Lingustics, Special Issue on Using Large Cor-
pora II, 19(2):331?358.
J. Pustejovsky. 1991. The generative lexicon. Computa-
tional Linguistics, 17(4):209?441.
D. Tufis and O. Mason. 1998. Tagging Romanian
Texts: a Case Study for QTAG, a Language Indepen-
dent Probabilistic Tagger. In Proceedings of LREC,
pages 589?96.
E.M. Voorhees. 1994. Query expansion using lexical-
semantic relations. In Proceedings of the 17th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 61?69.
I. Yamada and T. Baldwin. 2004. Automatic discovery
of telic and agentive roles from corpus data. In Pro-
ceedings of the the 18th Pacific Asia Conference on
Language, Information and Computation (PACLIC).
895
Ontology-based linguistic annotation
Philipp Cimiano, Siegfried Handschuh
Institute AIFB
University of Karlsruhe
 
cimiano,handschuh  @aifb.uni-karlsruhe.de
Abstract
We propose an ontology-based framework
for linguistic annotation of written texts.
We argue that linguistic annotation can be
actually considered a special case of se-
mantic annotation with regard to an on-
tology such as pursued within the con-
text of the Semantic Web. Furthermore,
we present CREAM, a semantic annota-
tion framework, as well as its concrete im-
plementation OntoMat and show how they
can be used for the purpose of linguistic
annotation. We demonstrate the value of
our framework by applying it to the an-
notation of anaphoric relations in written
texts.
1 Introduction
Linguistic annotation is crucial for the develop-
ment and evaluation of natural language processing
(NLP) tools. In particular machine-learning based
approaches to part-of-speech tagging, word sense
disambiguation, information extraction or anaphora
resolution - just to name a few - rely on corpora an-
notated with the corresponding phenomenon to be
trained and tested on. In this paper, we argue that
linguistic annotation can to some extent be consid-
ered a special case of semantic annotation with re-
gard to an ontology. Part-of-Speech (POS) annota-
tion for example can be seen as the task of choos-
ing the appropriate tag for a word from an ontol-
ogy of word categories (compare for example the
Penn Treebank POS tagset as described in (Marcus
et al, 1993)). The annotation of word senses such
as used by machine-learning based word sense dis-
ambiguation (WSD) tools corresponds to the task of
selecting the correct semantic class or concept for a
word from an underlying ontology such as WordNet
(Resnik, 1997). Annotation by template filling such
as used to train machine-learning based information
extraction (IE) systems as (Ciravegna, 2001) can be
seen as the task of finding and marking all the at-
tributes of a given ontological concept in a text. An
ontological concept in this sense can be a launching
event, a management succession event or a person
together with attributes such as name, affiliation, po-
sition, etc. The annotation of anaphoric or bridging
relations is actually the task of identifying the se-
mantic relation between two linguistic expressions
representing a certain ontological concept.
Most linguistic annotation tools make use of schema
specifying what can actually be annotated. These
schema can in fact be understood as a formal rep-
resentation of the conceptualization underlying the
annotation task. Ontologies are formal specifica-
tions of a conceptualization (Gruber, 1993) so that
it seems straightforward to formalize annotation
schemes as ontologies and make use of semantic an-
notation tools such as OntoMat (Handschuh et al,
2001) for the purpose of linguistic annotation.
The structure of this paper is as follows: Section 2
presents the ontology-based framework for linguis-
tic annotation, and section 3 shows how the frame-
work can be applied to the annotation of anaphoric
relations. Section 4 presents CREAM, a semantic
annotation framework for the Semantic Web as well
as its concrete implementation OntoMat. Finally,
section 5 discusses related work, and section 6 con-
cludes the paper.
2 The Ontology-based linguistic
annotation framework
An ontology is a formal specification of a conceptu-
alization (Gruber, 1993). A conceptualization can be
understood as an abstract representation of the world
or domain we want to model for a certain purpose.
The ontological model underlying this work is basi-
cally the one in (Bozsak et al, 2002). According to
this model, an ontology is defined as follows:
Definition 1 (Ontology)
An ontology is a structure   	


consisting of (i) two disjoint sets  and  called
concept identifiers and relation identifiers respec-
tively, (ii) a partial order 
	 on  called concept
hierarchy or taxonomy, (iii) a function Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 28?37,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Automatically Learning Qualia Structures from the Web
Philipp Cimiano & Johanna Wenderoth
Institute AIFB
University of Karlsruhe
Abstract
Qualia Structures have many applications
within computational linguistics, but currently
there are no corresponding lexical resources
such as WordNet or FrameNet. This paper
presents an approach to automatically learn
qualia structures for nominals from the World
Wide Web and thus opens the possibility to ex-
plore the impact of qualia structures for natural
language processing at a larger scale. Further-
more, our approach can be also used support a
lexicographer in the task of manually creating
a lexicon of qualia structures. The approach is
based on the idea of matching certain lexico-
syntactic patterns conveying a certain seman-
tic relation on the World Wide Web using stan-
dard search engines. We evaluate our approach
qualitatively by comparing our automatically
learned qualia structures with the ones from the
literature, but also quantitatively by presenting
results of a human evaluation.
1 Introduction
Qualia Structures have been originally introduced by
(Pustejovsky, 1991) and are used for a variety of purposes
in Natural Language processing such as the analysis of
compounds (Johnston and Busa, 1996), co-composition
and coercion (Pustejovsky, 1991) as well as for bridging
reference resolution (Bos et al, 1995). Further, it has also
been argued that qualia structures and lexical semantic
relations in general have applications in information re-
trieval (Voorhees, 1994; Pustejovsky et al, 1993). One
major bottleneck however is that currently Qualia Struc-
tures need to be created by hand, which is probably also
the reason why there are no practical system using qualia
structures, but a lot of systems using globally available re-
sources such as WordNet (Fellbaum, 1998) or FrameNet1
1http://framenet.icsi.berkeley.edu/
as source of lexical/world knowledge. The work de-
scribed in this paper addresses this issue and presents
an approach to automatically learning qualia structures
for nominals from the Web. The approach is inspired
in recent work on using the Web to identify instances
of a relation of interest such as in (Markert et al, 2003)
and (Cimiano and Staab, 2004). These approaches are
in essence a combination of the usage of lexico-syntactic
pattens conveying a certain relation of interest such as in
(Hearst, 1992), (Charniak and Berland, 1999), (Iwanska
et al, 2000) or (Poesio et al, 2002) with the idea of us-
ing the web as a big corpus (Resnik and Smith, 2003),
(Grefenstette, 1999), (Keller et al, 2002).
The idea of learning Qualia Structures from the Web is
not only a very practical, it is in fact a principled one.
While single lexicographers creating qualia structures -
or lexicon entries in general - might take very subjective
decisions, the structures learned from the Web do not mir-
ror the view of a single person, but of the whole world as
represented on the World Wide Web. Thus, an approach
learning qualia structures from the Web is in principle
more reliable than letting lexicographers craft lexical en-
tries on their own. Obviously, on the other hand, using
an automatic web based approach yields also a lot of in-
appropriate results which are due to 1) errors produced
by the linguistic analysis (e.g. part-of-speech tagging), 2)
idiosyncrasies of ranking algorithms of search machines,
3) the fact that the Web or in particular search engines
are to a great extent commercially biased, 4) the fact that
people also publish erroneous information on the Web,
and 5) lexical ambiguities. Because of these reasons our
aim is in fact not to replace lexicographers, but to support
them in the task of creating qualia structures on the basis
of the automatically learned qualia structures. The pa-
per is structured as follows: Section 2 introduces qualia
structures and describes the specific qualia structures we
aim to acquire. Section 3 describes our approach in detail
and section 4 presents a quantitative and qualitative eval-
uation of our approach. Before concluding, we discuss
some related work in Section 5.
28
2 Qualia Structures
According to Aristotle, there are four basic factors or
causes by which the nature of an object can be described
(cf. (Kronlid, 2003)):
  the material cause, i.e. the material an object is
made of
  the agentive cause, i.e. the source of movement, cre-
ation or change
  the formal cause, i.e. its form or type
  the final cause, i.e. its purpose, intention or aim
In his Generative Lexicon (GL) framework (Puste-
jovsky, 1991) reused Aristotle?s basic factors for the de-
scription of the meaning of lexical elements. In fact he in-
troduced so called Qualia Structures by which the mean-
ing of a lexical element is described in terms of four roles:
  Constitutive: describing physical properties of an
object, i.e. its weight, material as well as parts and
components
  Agentive: describing factors involved in the bringing
about of an object, i.e. its creator or the causal chain
leading to its creation
  Formal: describing that properties which distinguish
an object in a larger domain, i.e. orientation, magni-
tude, shape and dimensionality
  Telic: describing the purpose or function of an object
Most of the qualia structures used in (Pustejovsky,
1991) however seem to have a more restricted interpre-
tation. In fact, in most examples the Constitutive role
seems to describe the parts or components of an object,
while the Agentive role is typically described by a verb
denoting an action which typically brings the object in
question into existence. The Formal role normally con-
sists in typing information about the object, i.e. its hyper-
nym or superconcept. Finally, the Telic role describes the
purpose or function of an object either by a verb or nom-
inal phrase. The qualia structure for knife for example
could look as follows (cf. (Johnston and Busa, 1996)):
Formal: artifact tool
Constitutive: blade,handle,...
Telic: cut act
Agentive: make act
Our understanding of Qualia Structure is in line with this
restricted interpretation of the qualia roles. Our aim is to
automatically acquire Qualia Structures from the Web for
nominals, looking for (i) nominals describing the type of
the object, (ii) verbs defining its agentive role, (iii) nomi-
nals describing its parts or components and (iv) nouns or
verbs describing its intended purpose.
3 Approach
Our approach to learning qualia structures from the
Web is on the one hand based on the assumption that
instances of a certain semantic relation can be learned by
matching certain lexico-syntactic patterns more or less
reliably conveying the relation of interest in line with
the seminal work of (Hearst, 1992), who defined the
following patterns conveying a hypernym relation:
(1)  such as  , 
	 , ..., 
 (and  or)

 2
(2) such  as  , 
	 , ... 
 (and  or) 

(3)  , 
	 , ..., 
 (and  or) other 
(4)  , (including  especially)  , 
	 , ..., 

(and  or) 

According to Hearst, from such patterns we can derive
that for all

 
	
 Proceedings of the 8th International Conference on Computational Semantics, pages 272?276,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Flexible Semantic Composition with DUDES
Philipp Cimiano
Web Information Systems Group, TU Delft
1 DUDES
In this paper we present a novel formalism for semantic construction called
DUDES (Dependency-based Underspecified Discourse REpresentation
Structures). The DUDES formalism has been designed to overcome the
rigidity of semantic composition based on the lambda calculus (where the
order of application is typically fixed) and provides some flexibility with
respect to the direction of the dependence and with respect to the order
of application of arguments. In this short paper we present the DUDES
formalism and work through a simple example. DUDES bears some resem-
blance to the work on ?-DRT [2] and LUDs [1] as well as with the work
of Copestake et al [4] and represents a generalization of the formalism in-
troduced in [3]. A detailed discussion of the relation to these formalisms is
clearly out of the scope of this paper. DUDES are characterized by three
main facts. First, they represent semantic dependencies and are thus in-
herently suitable for a dependency-based grammar formalism assuming that
syntactic dependencies correspond to semantic dependencies (though the
correspondence might be ?inverted?). Second, they explicitly encode scope
relations and are thus able to yield underspecified representations as output
(in contrast to the linear logic approach for LFG [5] where different scopings
correspond to different derivations). Third, there is one single operation for
semantic composition which is, to some extent, order-independent (in con-
trast to traditional lambda-based formalisms) as well as flexible with respect
to the direction of the syntactic dependency. As the name suggests, DUDES
builds on DRT [6] and in particular on UDRT [7] in the sense that it relies on
labeled DRSs and dominance relations between these to talk about scope.
First of all, we now first formally introduce DUDES:
Definition 1 (DUDES)
A DUDES is a 7-tuple (m, l, t, U,A, S,C) consisting of
272
- A main discourse referent m and a distinguished label l
- The type t of the semantic structure (after inserting all arguments)
- A set U of UDRS components. These UDRS components are in essence
labeled DRSs [6].
- A set of argument quadruples (l, v, rel, type) consisting of i) a label l (the main label
of a DUDE inserted as argument, ii) the main variable v of the argument DUDE,
iii) a grammatical relation and iv) a semantic type.
- A set S of scope relations between labels introduced in U .
- A set C of constraints on the arguments, e.g. including ontological constraints or
selectional restrictions etc.
We now give the semantic representation of the entries for our running
example: ?John likes every nice woman.?:
John:
j l ??e, t?, t?
l:
j
John(j)
like:
e > t
>: , ?:
e:like(x,y)
(l
1
, x, subj, ??e, t?, t?), (l
2
, y, obj, ??e, t?, t?)
? ? scope(l
1
), l
1
? >,? ? scope(l
2
), l
2
? >
every:
x l ??e, t?, t?
l:
l
1
:
x
? l
2
:
(l?,x,spec,?e, t?)
l
?
? l
1
nice:
x l ?e, t?
l:
nice(x)
(l
?
, x,mod, ?e, t?)
l ? l
?
woman:
w l ?e, t?
l:
woman(w)
Further, we introduce the semantic composition operation for DUDES
along a dependency tree, distinguishing tree cases:
Definition 2 (Semantic Composition for DUDES)
Let (?
1
, ?
2
) be an edge in some DAG (dependency tree, LTAG derivation tree or F-
Structure DAG). Assume the edge is labeled with r (a grammatical function) and
the semantics of the vertices ?
1
and ?
2
are given by DUDEs as follows: ?
1
:=
(m
1
, l
1
, t
1
, U
1
, A
1
, S
1
, C
1
) and ?
2
= (m
2
, l
2
, t
2
, U
2
, A
2
, S
2
, C
2
). Then the result of applying
?
2
to ?
1
is the DUDE ?
?
= ?
1
(?
2
) = (m
?
, l
?
, t
?
, U
?
, A
?
, S, C
?
), where we need to distinguish
the following cases:
if (l, v, r, t
2
) ? A
1
if (l, v, r, t
1
) ? A
2
(t
1
= t
2
) if (l, v, r, t
1
) ? A
2
(t
1
6= t
2
)
(Complementation) (Modification) (Inversion)
m
?
:= m
1
m
?
:= m
1
m
?
:= m
2
t
?
:= t
1
t
?
:= t
1
t
?
:= t
2
U
?
:= U
1
? U
2
U
?
:= U
1
? U
2
U
?
:= U
1
? U
2
A
?
:= A
1
\{(l, v, r, t
2
)} A
?
:= A
1
A
?
:= A
2
\{(l, v, r, t
1
)}
S
?
:= S
1
? S
2
S
?
:= S
1
? S
2
S
?
:= S
1
? S
2
C
?
:= C
1
? C
2
C
?
:= C
1
? C
2
C
?
:= C
1
? C
2
v ? m
2
, l ? l
2
v ? m
1
(? m
2
), l ? l
1
v ? m
1
, l ? l
1
where ??? is the operation of unification between variables.
Concerning the order of application, from the definition of the seman-
tic composition operator it follows that complements and modifiers can be
applied in any order, but inversions have to be carried out at the end as
273
they change the mother DUDES and would thus inhibit the application of
the complements and the modifiers. In the following section we show how
the semantic composition operation defined above applies to a concrete ex-
ample. We will also discuss that our operations still work if (some of) the
dependencies are inverted.
2 A Worked Example
We will consider the two fol-
lowing (possible) analyses for
the sentence: ?John likes
every nice woman.?, corre-
sponding to the NP analysis
(a) and DP analysis (b), re-
spectively.
a)
like
subj
y
y
y
y
y
y
y
y
obj
E
E
E
E
E
E
E
E
john woman
spec
y
y
y
y
y
y
y
y
mod
E
E
E
E
E
E
E
E
every nice
b)
like
subj
y
y
y
y
y
y
y
y
obj
E
E
E
E
E
E
E
E
john every
spec
woman
mod
nice
2.1 Complementation
Given the dependency analysis in a) to the
right, as a result of applying our seman-
tic composition operator for the comple-
mentation case we get a DUDES where
the argument has been correctly inserted,
the DRS components and the scope condi-
tions have been merged and one argument
has been removed. Note that this was
possible because i) the edge was labeled
with the appropriate grammatical relation
?subj? and ii) the types of ?
2
and of the ar-
gument match (both are of type ??e, t?, t?).
The resulting DUDES for [[John likes]] is
shown in b) to the right. (In case of DRS
conditions which are not complex, we as-
sume that the functions ?res? and ?scope?
are resolved to the identity function.)
a)
e > t
>: , ?:
e:like(x,y)
(l
1
, x, subj, ??e, t?, t?), (l
2
, y, obj, ??e, t?, t?)
? ? scope(l
1
) ? >,? ? scope(l
2
) ? >
subj
j l ??e, t?, t?
l:
j
John(j)
b)
e > t
>: , ?:
e:like(j,y)
l
1
:
j
John(j)
(l
2
, y, obj, ??e, t?, t?)
? ? l
1
? >,? ? scope(l
2
) ? >
2.2 Specification and Modification
The two possible dependency analyses for determiner/noun constructions
give rise to two configurations, corresponding to a) and b) below, for the se-
mantic composition operator. In both cases, independent of the fact whether
274
the determiner is the dependent or the head, we get that first the semantic
representation of the adjective is applied to the one of the noun (as the mod-
ifier has to be applied before the inversion is carried out in configuration a),
thus yielding the two configurations in c) and d).
a)
w l ?e, t?
l:
woman(w)
spec
y
y
y
y
y
y
y
y
mod
E
E
E
E
E
E
E
E
x l ??e, t?, t?
l
1
:
x
? l
2
:
(l?,x,spec,?e, t?)
l
?
? l
1
x l ?e, t?
l:
nice(x)
(l
?
, x,mod, ?e, t?)
l ? l
?
b)
x l ??e, t?, t?
l
1
:
x
? l
2
:
(l?,x,spec,?e, t?)
l
?
? l
1
spec
w l ?e, t?
l:
woman(w)
mod
x l ?e, t?
l:
nice(x)
(l
?
, x,mod, ?e, t?)
l ? l
?
c)
w l ?e, t?
l:
woman(w)
,
l?:
nice(w)
l
?
? l
spec
x l ??e, t?, t?
l
1
:
x
?
l
2
:
(l?,x,spec,?e, t?)
l
?
? l
1
d)
x l ??e, t?, t?
l
1
:
x
?
l
2
:
(l?,x,spec,?e, t?)
l
?
? l
1
spec
w l ?e, t?
l:
woman(w)
,
l?:
nice(w)
l
?
? l
In case c) we have a case of inversion, while in case d) we have a case
of complementation. Overall, in both cases we yield the following DUDES:
[[every nice woman]]=
w l ??e, t?, t?
l
1
:
w
? l
2
: , l:
woman(w)
, l?:
nice(w)
l
?
? l, l ? l
1
2.3 Result
After a further semantic composition step (case complementation), applying
[[every nice woman]] (from Sec. 2.2) to [[John likes]] (from Sec. 2.1), we
yield as resulting UDRS:
275
e > t
>: , ?:
e:like(j,w)
, l
1
:
j
John(j)
l:
l
2
:
w
? l
3
:
, l?:
woman(w)
,
l?:
nice(w)
? ? l
1
? >,? ? l
3
, l ? >, l
?
? l
2
,l
??
? l
?
e
j
John(j)
w
?
woman(w)
nice(w)
e:like(j,w)
References
[1] J. Bos, B. Gamba?ck, C. Lieske, Y. Mori, M. Pinkal, and K. Worm. Compositional
semantics in verbmobil. In Proceedings of COLING?96, 1996.
[2] Johan Bos, Elsbeth Mastenbroek, Scott Mcglashan, Sebastian Millies, and Manfred
Pinkal. A compositional drs-based formalism for nlp applications. In Proceedings of
the International Workshop on Computational Semantics, 1994.
[3] P. Cimiano, A. Frank, and U. Reyle. UDRT-based semantics construction for LTAG
? and what it tells us about the role of adjunction in LTAG ?. In Proceedings of the
7th International Workshop on Computational Semantics, pages 41?52, 2007.
[4] Ann Copestake, Alex Lascarides, and Dan Flickinger. An algebra for semantic con-
struction in constraint-based grammars. In Proceedings of ACL?01, 2001.
[5] Mary Dalrymple, John Lamping, Fernando C. N. Pereira, and Vijay Saraswat. Linear
logic for meaning assembly. revised version of the (overview) paper in Proc. of the
Workshop on Computational Logic for Natural Language Processing, Edinburgh, UK.
1995., 2002.
[6] H. Kamp and U. Reyle. From Discourse to Logic. Kluwer, 1993.
[7] Uwe Reyle. Dealing with ambiguities by underspecification: Construction, represen-
tation and deduction. Journal of Semantics, 10(2):123?179, 1993.
276
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1732?1740,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Orthonormal Explicit Topic Analysis for Cross-lingual Document Matching
John Philip McCrae
University Bielefeld
Inspiration 1
Bielefeld, Germany
Philipp Cimiano
University Bielefeld
Inspiration 1
Bielefeld, Germany
{jmccrae,cimiano,rklinger}@cit-ec.uni-bielefeld.de
Roman Klinger
University Bielefeld
Inspiration 1
Bielefeld, Germany
Abstract
Cross-lingual topic modelling has applications
in machine translation, word sense disam-
biguation and terminology alignment. Multi-
lingual extensions of approaches based on la-
tent (LSI), generative (LDA, PLSI) as well as
explicit (ESA) topic modelling can induce an
interlingual topic space allowing documents
in different languages to be mapped into the
same space and thus to be compared across
languages. In this paper, we present a novel
approach that combines latent and explicit
topic modelling approaches in the sense that
it builds on a set of explicitly defined top-
ics, but then computes latent relations between
these. Thus, the method combines the ben-
efits of both explicit and latent topic mod-
elling approaches. We show that on a cross-
lingual mate retrieval task, our model signif-
icantly outperforms LDA, LSI, and ESA, as
well as a baseline that translates every word in
a document into the target language.
1 Introduction
Cross-lingual document matching is the task of,
given a query document in some source language,
estimating the similarity to a document in some tar-
get language. This task has important applications in
machine translation (Palmer et al, 1998; Tam et al,
2007), word sense disambiguation (Li et al, 2010)
and ontology alignment (Spiliopoulos et al, 2007).
An approach that has become quite popular in re-
cent years for cross-lingual document matching is
Explicit Semantics Analysis (ESA, Gabrilovich and
Markovitch (2007)) and its cross-lingual extension
CL-ESA (Sorg and Cimiano, 2008). ESA indexes
documents by mapping them into a topic space de-
fined by their similarity to predefined explicit top-
ics ? generally articles from an encyclopaedia ? in
such a way that there is a one-to-one correspondence
between topics and encyclopedic entries. CL-ESA
extends this to the multilingual case by exploiting
a background document collection that is aligned
across languages, such as Wikipedia. A feature of
ESA and its extension CL-ESA is that, in contrast to
latent (e.g. LSI, Deerwester et al (1990)) or genera-
tive topic models (such as LDA, Blei et al (2003)),
it requires no training and, nevertheless, has been
demonstrated to outperform LSI and LDA on cross-
lingual retrieval tasks (Cimiano et al, 2009).
A key choice in Explicit Semantic Analysis is the
document space that will act as the topic space. The
standard choice is to regard all articles from a back-
ground document collection ? Wikipedia articles are
a typical choice ? as the topic space. However, it
is crucial to ensure that these topics cover the se-
mantic space evenly and completely. In this pa-
per, we present an alternative approach where we
remap the semantic space defined by the topics in
such a manner that it is orthonormal. In this way,
each document is mapped to a topic that is distinct
from all other topics. Such a mapping can be con-
sidered as equivalent to a variant of Latent Seman-
tic Indexing (LSI) with the main difference that our
model exploits the matrix that maps topic vectors
back into document space, which is normally dis-
carded in LSI-based approaches. We dub our model
ONETA (OrthoNormal Explicit Topic Analysis) and
empirically show that on a cross-lingual retrieval
1732
task it outperforms ESA, LSI, and Latent Dirichlet
Allocation (LDA) as well as a baseline consisting of
translating each word into the target language, thus
reducing the task to a standard monolingual match-
ing task. In particular, we quantify the effect of dif-
ferent approximation techniques for computing the
orthonormal basis and investigate the effect of vari-
ous methods for the normalization of frequency vec-
tors.
The structure of the paper is as follows: we situate
our work in the general context of related work on
topic models for cross-lingual document matching
in Section 2. We present our model in Section 3 and
present our experimental results and discuss these
results in Section 4.
2 Related Work
The idea of applying topic models that map docu-
ments into an interlingual topic space seems a quite
natural and principled approach to tackle several
tasks including the cross-lingual document retrieval
problem.
Topic modelling is the process of finding a rep-
resentation of a document d in a lower dimensional
space RK where each dimension corresponds to one
topic that abstracts from specific words and thus al-
lows us to detect deeper semantic similarities be-
tween documents beyond the computation of the
pure overlap in terms of words.
Three main variants of document models have
been mainly considered for cross-lingual document
matching:
Latent methods such as Latent Semantic Indexing
(LSI, Deerwester et al (1990)) induce a de-
composition of the term-document matrix in
a way that reduces the dimensionality of the
documents, while minimizing the error in re-
constructing the training data. For example,
in Latent Semantic Indexing, a term-document
matrix is approximated by a partial singu-
lar value decomposition, or in Non-Negative
Matrix Factorization (NMF, Lee and Seung
(1999)) by two smaller non-negative matrices.
If we append comparable or equivalent doc-
uments in multiple languages together before
computing the decomposition as proposed by
Dumais et al (1997) then the topic model is
essentially cross-lingual allowing to compare
documents in different languages once they
have been mapped into the topic space.
Probabilistic or generative methods instead at-
tempt to induce a (topic) model that has the
highest likelihood of generating the documents
actually observed during training. As with la-
tent methods, these topics are thus interlin-
gual and can generate words/terms in differ-
ent languages. Prominent representatives of
this type of method are Probabilistic Latent Se-
mantic Indexing (PLSI, Hofmann (1999)) or
Latent Dirichlet Allocation (LDA, Blei et al
(2003)), both of which can be straightforwardly
extended to the cross-lingual case (Mimno et
al., 2009).
Explicit topic models make the assumption that
topics are explicitly given instead of being in-
duced from training data. Typically, a back-
ground document collection is assumed to be
given whereby each document in this corpus
corresponds to one topic. A mapping from doc-
ument to topic space is calculated by comput-
ing the similarity of the document to every doc-
ument in the topic space. A prominent exam-
ple for this kind of topic modelling approach is
Explicit Semantic Analysis (ESA, Gabrilovich
and Markovitch (2007)).
Both latent and generative topic models attempt to
find topics from the data and it has been found that
in some cases they are equivalent (Ding et al, 2006).
However, this approach suffers from the problem
that the topics might be artifacts of the training data
rather than coherent semantic topics. In contrast, ex-
plicit topic methods can use a set of topics that are
chosen to be well-suited to the domain. The princi-
ple drawback of this is that the method for choosing
such explicit topics by selecting documents is com-
paratively crude. In general, these topics may be
overlapping and poorly distributed over the seman-
tic topic space. By comparison, our method takes the
advantage of the pre-specified topics of explicit topic
models, but incorporates a training step to learn la-
tent relations between these topics.
1733
3 Orthonormal explicit topic analysis
Our approach follows Explicit Semantic Analysis in
the sense that it assumes the availability of a back-
ground document collection B = {b1, b2, ..., bN}
consisting of textual representations. The map-
ping into the explicit topic space is defined by a
language-specific function ? that maps documents
into RN such that the jth value in the vector is given
by some association measure ?j(d) for each back-
ground document bj . Typical choices for this associ-
ation measure ? are the sum of the TF-IDF scores or
an information retrieval relevance scoring function
such as BM-25 (Sorg and Cimiano, 2010).
For the case of TF-IDF, the value of the j-th ele-
ment of the topic vector is given by:
?j(d) =
????
tf-idf(bj)
T ????tf-idf(d)
Thus, the mapping function can be represented as
the product of a TF-IDF vector of document dmulti-
plied by anW?N matrix, X, each element of which
contains the TF-IDF value of word i in document bj :
?(d) =
?
?
?
?
????
tf-idf(b1)T
...
????
tf-idf(bN )T
?
?
?
?
????
tf-idf(d) = XT ?
????
tf-idf(d)
For simplicity, we shall assume from this point on
that all vectors are already converted to a TF-IDF
or similar numeric vector form. In order to com-
pute the similarity between two documents di and
dj , typically the cosine-function (or the normalized
dot product) between the vectors ?(di) and ?(dj) is
computed as follows:
sim(di, dj) = cos(?(di),?(dj)) =
?(di)T?(dj)
||?(di)||||?(dj)||
If we represent the above using our above defined
W ?N matrix X then we get:
sim(di, dj) = cos(X
Tdi,X
Tdj) =
dTi XX
Tdj
||XTdi||||XTdj ||
The key challenge with ESA is choosing a good
background document collection B = {b1, ..., bN}.
A simple minimal criterion for a good background
document collection is that each document in this
collection should be maximally similar to itself and
less similar to any other document:
?i 6= j 1 = sim(bj , bj) > sim(bi, bj) ? 0
While this criterion is trivially satisfied if we have
no duplicate documents in our collection, our intu-
ition is that we should choose a background collec-
tion that maximizes the slack margin of this inequal-
ity, i.e. |sim(bj , bj) ? sim(bi, bj)|. We can see that
maximizing this margin for all i,j is the same as
minimizing the semantic overlap of the background
documents, which is given as follows:
overlap(B) =
?
i = 1, . . . , N
j = 1, . . . , N
i 6= j
sim(bi, bj)
We first note that we can, without loss of general-
ity, normalize our background documents such that
||Xbj || = 1 for all j, and in this case we can re-
define the semantic overlap as the following matrix
expression1
overlap(X) = ||XTXXTX? I||1
It is trivial to verify that this equation has a mini-
mum when XTXXTX = I. This is the case when
the topics are orthonormal:
(XTbi)T(XTbj) = 0 if i 6= j
(XTbi)T(XTbi) = 1
Unfortunately, this is not typically the case as the
documents have significant word overlap as well as
semantic overlap. Our goal is thus to apply a suitable
transformation to X with the goal of ensuring that
the orthogonality property holds.
Assuming that this transformation of X is done
by multiplication with some other matrix A, we can
define the learning problem as finding that matrix A
such that:
(AXTX)T(AXTX) = I
1||A||p =
?
i,j |aij |
p is the p-norm. ||A||F =
?
||A||2 is
the Frobenius norm.
1734
If we have the case that W ? N and that the rank
of X is N , then XTX is invertible and thus A =
(XTX)?1 is the solution to this problem.2
We define the projection function of a document
d, represented as a normalized term frequency vec-
tor, as follows:
?ONETA(d) = (X
TX)?1XTd
For the cross-lingual case we assume that we have
two sets of background documents of equal size,
B1 = {b11, . . . , b
1
N}, B
2 = {b21, . . . , b
2
N} in lan-
guages l1 and l2, respectively and that these doc-
uments are aligned such that for every index i, b1i
and b2i are documents on the same topic in each
language. Using this we can construct a projec-
tion function for each language which maps into the
same topic space. Thus, as in CL-ESA, we obtain
the cross-lingual similarity between a document di
in language l1 and a document dj in language l2 as
follows:
sim(di, dj) = cos(?
l1
ONETA(di),?
l2
ONETA(dj))
We note here that we assume that ? could be rep-
resented as a symmetric inner product of two vec-
tors. However, for many common choices of asso-
ciation measures, including BM25, this is not the
case. In this case the expression XTX can be re-
placed with a kernel matrix specifying the associ-
ation of each background document to each other
background document.
3.1 Relationship to Latent Semantic Indexing
In this section we briefly clarify the relationship be-
tween our method ONETA and Latent Semantic In-
dexing. Latent Semantic Indexing defines a map-
ping from a document represented as a term fre-
quency vector to a vector in RK . This transforma-
tion is defined by means of calculating the singu-
lar value decomposition (SVD) of the matrix X as
above, namely
2In the case that the matrix is not invertible we can in-
stead solve ||XTXA ? I||F , which has a minimum at A =
V??1UT where XTX = U?VT is the singular value de-
composition of XTX.
As usual we do not in fact compute the inverse for our exper-
iments, but instead the LU Decomposition and solve by Gaus-
sian elimination at test time.
X = U?VT
Where ? is diagonal and U V are the eigenvec-
tors of XXT and XTX., respectively. Let ?K de-
note the K ? K submatrix containing the largest
eigenvalues, and UK ,VK denote the corresponding
eigenvectors. Thus LSI can be defined as:
?LSI(d) = ?
?1
K UKd
With regards to orthonormalized topics, we see
that using the SVD, we can simply derive the fol-
lowing:
(XTX)?1XT = V??1UT
When we set K = N and thus choose the maxi-
mum number of topics, ONETA is equivalent to LSI
modulo the fact that it multiplies the resulting topic
vector by V, thus projecting back into document
space, i.e. into explicit topics.
In practice, both methods differ significantly in
that the approximations they make are quite differ-
ent. Furthermore, in the case that W  N and X
has n non-zeroes, the calculation of the SVD is of
complexity O(nN + WN2) and requires O(WN)
bytes of memory. In contrast, ONETA requires com-
putation time ofO(Na) for a > 2, which is the com-
plexity of the matrix inversion algorithm3, and only
O(n+N2) bytes of memory.
3.2 Approximations
The computation of the inverse has a complexity
that, using current practical algorithms, is approxi-
mately cubic and as such the time spent calculating
the inverse can grow very quickly. There are sev-
eral methods for obtaining an approximate inverse.
The most commonly used are based on the SVD or
eigendecomposition of the matrix. As XTX is sym-
metric positive definite, it holds that:
XTX = U?UT
Where U are the eigenvectors of XTX and ? is
a diagonal matrix of the eigenvalues. With UK ,?K
3Algorithms with a = 2.3727 are known but practical algo-
rithms have a = 2.807 or a = 3 (Coppersmith and Winograd,
1990)
1735
as the first K eigenvalues and eigenvectors, respec-
tively, we have:
(XTX)?1 ' UK?
?1
K U
T
K (1)
We call this the orthonormal eigenapproxima-
tion or ON-Eigen. The complexity of calculating
(XTX)?1XT from this is O(N2K + Nn), where
n is the number of non-zeros in X.
Similarly, using the formula derived in the previ-
ous section we can derive an approximation of the
full model as follows:
(XTX)?1XT ' UK?
?1
K V
T
K (2)
We call this approximation Explicit LSI as it first
maps into the latent topic space and then into the
explicit topic space.
We can consider another approximation by notic-
ing that X is typically very sparse and moreover
some rows of X have significantly fewer non-zeroes
than others (these rows are for terms with low fre-
quency). Thus, if we take the first N1 columns (doc-
uments) in X, it is possible to rearrange the rows
of X with the result that there is some W1 such
that rows with index greater than W1 have only ze-
roes in the columns up to N1. In other words, we
take a subset of N1 documents and enumerate the
words in such a way that the terms occurring in the
first N1 documents are enumerated 1, . . . ,W1. Let
N2 = N ? N1, W2 = W ?W1. The result of this
row permutation does not affect the value of XTX
and we can write the matrix X as:
X =
(
A B
0 C
)
where A is a W1 ? N1 matrix representing term
frequencies in the first N1 documents, B is a W1 ?
N2 matrix containing term frequencies in the re-
maining documents for terms that are also found in
the firstN1 documents, and C is aW2?N2 contain-
ing the frequency of all terms not found in the first
N1 documents.
Application of the well-known divide-and-
conquer formula (Bernstein, 2005, p. 159) for
matrix inversion yields the following easily verifi-
able matrix identity, given that we can find C? such
that C?C = I.
(
(ATA)?1AT ?(ATA)?1ATBC?
0 C?
)(
A B
0 C
)
= I
(3)
We denote the above equation using a matrix L
as LTX = I. We note that L 6= (XTX)?1X,
but for any document vector d that is representable
as a linear combination of the background doc-
ument set (i.e., columns of X) we have that
Ld = (XTX)?1XTd and in this sense L '
(XTX)?1XT.
We further relax the assumption so that we only
need to find a C? such that C?C ' I. For this,
we first observe that C is very sparse as it contains
only terms not contained in the first N1 documents
and we notice that very sparse matrices tend to be
approximately orthogonal, hence suggesting that it
should be very easy to find a left-inverse of C. The
following lemma formalizes this intuition:
Lemma: If C is a W ? N matrix with M non-
zeros, distributed randomly and uniformly across the
matrix, and all the non-zeros are 1, then DCTC has
an expected value on each non-diagonal value of MN2
and a diagonal value of 1 if D is the diagonal matrix
whose values are given by ||ci||?2, the square of the
norm of the corresponding column of C.
Proof: We simply observe that if D? = DCTC,
then the (i, j)th element of D? is given by
dij =
cTi cj
||ci||2
If i 6= j then the cTi cj is the number of non-zeroes
overlapping in the ith and jth column of C and under
a uniform distribution we expect this to be M
2
N3 . Sim-
ilarly, we expect the column norm to be MN such that
the overall expectation is MN2 . The diagonal value is
clearly equal to 1.
As long as C is very sparse, we can use the fol-
lowing approximation, which can be calculated in
O(M) operations, where M is the number of non-
zeroes.
C? '
?
?
?
||c1||?2 0
. . .
0 ||cN2 ||
?2
?
?
?CT
We call this method L-Solve. The complexity
of calculating a left-inverse by this method is of
1736
Document
Normalization
Frequency Normalization No Yes
TF 0.31 0.78
Relative 0.23 0.42
TFIDF 0.21 0.63
SQRT 0.28 0.66
Table 1: Effect of Term Frequency and Document Nor-
malization on Top-1 Precision
order O(Na1 ), being much more efficient than the
eigenvalue methods. However, it is potentially more
error-prone as it requires that a left-inverse of C ex-
ists. On real data this might be violated if we do not
have linear independence of the rows of C, for ex-
ample if W2 < N2 or if we have even one document
which has only words that are also contained in the
first N1 documents and hence there is a row in C
that consists of zeros only. This can be solved by
removing documents from the collection until C is
row-wise linear independent.4
3.3 Normalization
A key factor in the effectiveness of topic-based
methods is the appropriate normalization of the el-
ements of the document matrix X. This is even
more relevant for orthonormal topics as the matrix
inversion procedure can be very sensitive to small
changes in the matrix. In this context, we con-
sider two forms of normalization, term and docu-
ment normalization, which can also be considered
as row/column normalizations of X.
A straightforward approach to normalization is to
normalize each column of X to obtain a matrix as
follows:
X? =
(
x1
||x1||
. . .
xN
||xN ||
)
If we calculate X?TX? = Y then we get that the
(i, j)-th element of Y is:
yij =
xTi xj
||xi||||xj ||
4In the experiments in the next section we discarded 4.2% of
documents at N1 = 1000 and 47% of documents at N1 = 5000
l
l
l
l
l
l
l
l l l
l l
l l l l
l
l
l l
100 200 300 400 500
0.0
0.2
0.4
0.6
0.8
Approximation rate
Pre
cisio
n
l l
l
l
l
l l
l
l l l
l l
l l
l
l
l
?
ON?EigenL?SolveExplicit LSILSIESA
Figure 1: Effect on Top-1 Precision by various approxi-
mation method
Thus, the diagonal of Y consists of ones only and
due to the Cauchy-Schwarz inequality we have that
|yij | ? 1, with the result that the matrix Y is al-
ready close to I. Formally, we can use this to state
a bound on ||X?TX? ? I||F , but in practice it means
that the orthonormalizing matrix has more small or
zero values.
A further option for normalization is to consider
some form of term frequency normalization. For
term frequency normalization, we use TF (tfwn),
Relative ( tfwnFw ), TFIDF (tfwn log(
N
dfw
)), and SQRT
( tfwn?
Fw
). Here, tfwn is the term frequency of word w
in document n, Fw is the total frequency of word
w in the corpus, and dfw is the number of docu-
ments containing the words w. The first three of
these normalizations have been chosen as they are
widely used in the literature. The SQRT normaliza-
tion has been shown to be effective for explicit topic
methods in previous experiments not reported here.
4 Experiments and Results
For evaluation, we consider a cross-lingual mate re-
trieval task from English/Spanish on the basis of
Wikipedia as aligned corpus. The goal is to, for each
document of a test set, retrieve the aligned document
or mate. For each test document, on the basis of
1737
Method Top-1 Prec. Top-5 Prec. Top-10 Prec. MRR Time Memory
ONETA L-Solve (N1 = 1000) 0.290 0.501 0.596 0.390 73s 354MB
ONETA L-Solve (N1 = 2000) 0.328 0.531 0.600 0.423 2m18s 508MB
ONETA L-Solve (N1 = 3000) 0.462 0.662 0.716 0.551 4m12s 718MB
ONETA L-Solve (N1 = 4000) 0.599 0.755 0.781 0.667 7m44s 996MB
ONETA L-Solve (N1 = 5000) 0.695 0.817 0.843 0.750 12m28s 1.30GB
ONETA L-Solve (N1 = 6000) 0.773 0.883 0.905 0.824 18m40s 1.69GB
ONETA L-Solve (N1 = 7000) 0.841 0.928 0.937 0.881 26m31s 2.14GB
ONETA L-Solve (N1 = 8000) 0.896 0.961 0.968 0.927 37m39s 2.65GB
ONETA L-Solve (N1 = 9000) 0.924 0.981 0.987 0.950 52m52s 3.22GB
ONETA (No Approximation) 0.929 0.987 0.990 0.956 57m10s 3.42GB
Word Translation 0.751 0.884 0.916 0.812 n/a n/a
ESA (SQRT Normalization) 0.498 0.769 0.835 0.621 72s 284MB
LDA (K=1000) 0.287 0.568 0.659 0.417 4h12m 8.4GB
LSI (K=4000) 0.615 0.756 0.783 0.676 13h51m 19.7GB
ONETA + Word Translation 0.932 0.987 0.993 0.958 n/a n/a
Table 2: Result on large-scale mate-finding studies for English to Spanish matching
the similarity of the query document to all indexed
documents, we compute the value ranki indicating
at which position the mate of the ith document oc-
curs. We use two metrics: Top-k Precision, defined
as the percentage of documents for which the mate is
retrieved among the first k elements, and Minimum
Reciprocal Rank, defined as
MRR =
?
i?test
1
ranki
For our experiments, we first extracted a subset
of documents (every 20th) from Wikipedia, filtering
this set down to only those that have aligned pages
in both English and Spanish with a minimum length
of 100 words. This gives us 10,369 aligned doc-
uments in total, which form the background docu-
ment collection B. We split this data into a training
and test set of 9,332 and 1,037 documents, respec-
tively. We then removed all words whose total fre-
quencies were below 50. This resulted in corpus of
6.7 millions words in English and 4.2 million words
in Spanish.
Normalization Methods: In order to investigate
the impact of different normalization methods, we
ran small-scale experiments using the first 500 doc-
uments from our dataset to train ONETA and then
evaluate the resulting models on the mate-finding
task on 100 unseen documents. The results are pre-
sented in Table 1, which shows the Top-1 Precision
for the different normalization methods. We see that
the effect of applying document normalization in
all cases improves the quality of the overall result.
Surprisingly, we do not see the same result for fre-
quency normalization yielding the best result for the
case where we do no normalization at all5 . In the re-
maining experiments we thus employ document nor-
malization and no term frequency normalization.
ApproximationMethods: In order to evaluate the
different approximation methods, we experimen-
tally compare 4 different approximation methods:
standard LSI, ON-Eigen (Equation 1), Explicit LSI
(Equation 2), L-Solve (Equation 3) on the same
small-scale corpus. For convenience we plot an ap-
proximation rate which is either K or N1 depending
on method; at K = 500 and N1 = 500, these ap-
proximations become exact. This is shown in Figure
1. We also observe the effects of approximation and
see that the performance increases steadily as we
increase the computational factor. We see that the
orthonormal eigenvector (Equation 1) method and
the L-solve (Equation 3) method are clearly simi-
lar in approximation quality. We see that the explicit
LSI method (Equation 2) and the LSI method both
perform significantly worse for most of the approxi-
5A likely explanation for this is that low frequency terms are
less evenly distributed and the effect of calculating the matrix
inverse magnifies the noise from the low frequency terms
1738
mation amounts. Explicit LSI is worse than the other
approximations as it first maps the test documents
into a K-dimensional LSI topic space, before map-
ping back into theN -dimensional explicit space. As
expected this performs worse than standard LSI for
all but high values of K as there is significant error
in both mappings. We also see that the (CL-)ESA
baseline, which is very low due to the small number
of documents, is improved upon by even the least ap-
proximation of orthonormalization. In the remain-
ing of this section, we report results using the L-
Solve method as it has a very good performance and
is computationally less expensive than ON-Eigen.
Evaluation and Comparison: We compare
ONETA using the L-Solve method with N1 values
from 1000 to 9000 topics with (CL-)ESA (using
SQRT normalization), LDA (using 1000 topics)
and LSI (using 4000 topics). We choose the largest
topic count for LSI and LDA we could to provide
the best possible comparison. For LSI, the choice of
K was determined on the basis of operating system
memory limits, while for LDA we experimented
with higher values for K without any performance
improvement, likely due to overfitting. We also
stress that for L-Solve ONETA, N1 is not the topic
count but an approximation rate of the mapping. In
all settings we use N topics as with standard ESA,
and so should not be considered directly comparable
to the K values of these methods.
We also compare to a baseline system that re-
lies on word-by-word translation, where we use the
most likely single translation of a word as given by a
phrase table generated by the Moses system (Koehn
et al, 2007) on the EuroParl corpus (Koehn, 2005).
Top 1, Top 5 and Top 10 Precision as well as Mean
Reciprocal Rank are reported in Table 2.
Interestingly, even for a small number of docu-
ments (e.g., N1 = 6000) our results improve both
the word-translation baseline as well as all other
topic models, ESA, LDA and LSI in particular. We
note that at this level the method is still efficiently
computable and calculating the inverse in practice
takes less time than training the Moses system. The
significance for results (N1 ? 7000) have been
tested by means of a bootstrap resampling signifi-
cance test, finding out that our results significantly
improve on the translation base line at a 99% level.
Further, we consider a straightforward combina-
tion of our method with the translation system con-
sisting of appending the topic vectors and the trans-
lation frequency vectors, weighted by the relative
average norms of the vectors. We see that in this
case the translations continue to improve the perfor-
mance of the system (albeit not significantly), sug-
gesting a clear potential for this system to help in im-
proving machine translation results. While we have
presented results for English and Spanish here, simi-
lar results were obtained for the German and French
case but are not presented here due to space limita-
tions.
In Table 2 we also include the user time and peak
resident memory of each of these processes, mea-
sured on an 8 Core Intel Xeon 2.50 GHz server.
We do not include the results for Word Translation
as many hours were spent learning a phrase table,
which includes translations for many phrases not in
the test set. We see that the ONETA method signif-
icantly outperforms LSI and LDA in terms of speed
and memory consumption. This is in line with the
theoretical calculations presented earlier where we
argued that inverting the N ?N dense matrix XTX
when W  N is computationally lighter than find-
ing an eigendecomposition of the W ? W sparse
matrix XXT. In addition, as we do not multiply
(XTX)?1 and XT, we do not need to allocate a
large W ? K matrix in memory as with LSI and
LDA.
The implementations of ESA, ONETA,
LSI and LDA used as well as the data
for the experiments are available at
http://github.com/jmccrae/oneta.
5 Conclusion
We have presented a novel method for cross-lingual
topic modelling, which combines the strengths of
explicit and latent topic models and have demon-
strated its application to cross-lingual document
matching. We have in particular shown that the
method outperforms widely used topic models such
as Explicit Semantic Analysis (ESA), Latent Seman-
tic Indexing (LSI) and Latent Dirichlet Allocation
(LDA). Further, we have shown that it outperforms
a simple baseline relying on word-by-word transla-
tion of the query document into the target language,
1739
while the induction of the model takes less time
than training the machine translation system from a
parallel corpus. We have also presented an effec-
tive approximation method, i.e. L-Solve, which sig-
nificantly reduces the computational cost associated
with computing the topic models.
Acknowledgements
This work was funded by the Monnet Project
and the Portdial Project under the EC Sev-
enth Framework Programme, Grants No.
248458 and 296170. Roman Klinger has been
funded by the ?Its OWL? project (?Intelli-
gent Technical Systems Ostwestfalen-Lippe?,
http://www.its-owl.de/), a leading-edge
cluster of the German Ministry of Education and
Research.
References
Dennis S Bernstein. 2005. Matrix mathematics, 2nd Edi-
tion. Princeton University Press Princeton.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
Sorg, and Steffen Staab. 2009. Explicit versus la-
tent concept models for cross-language information re-
trieval. In IJCAI, volume 9, pages 1513?1518.
Don Coppersmith and Shmuel Winograd. 1990. Matrix
multiplication via arithmetic progressions. Journal of
symbolic computation, 9(3):251?280.
Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391?407.
Chris Ding, Tao Li, and Wei Peng. 2006. NMF and
PLSI: equivalence and a hybrid algorithm. In Pro-
ceedings of the 29th annual international ACM SIGIR,
pages 641?642. ACM.
Susan T Dumais, Todd A Letsche, Michael L Littman,
and Thomas K Landauer. 1997. Automatic cross-
language retrieval using latent semantic indexing. In
AAAI spring symposium on cross-language text and
speech retrieval, volume 15, page 21.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using Wikipedia-based ex-
plicit semantic analysis. In Proceedings of the 20th In-
ternational Joint Conference on Artificial Intelligence,
volume 6, page 12.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual interna-
tional ACM SIGIR conference, pages 50?57. ACM.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, et al 2007. Moses: Open source toolkit for sta-
tistical machine translation. In Proceedings of the 45th
Annual Meeting of the ACL, pages 177?180. Associa-
tion for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In MT summit, volume 5.
Daniel D Lee and H Sebastian Seung. 1999. Learning
the parts of objects by non-negative matrix factoriza-
tion. Nature, 401(6755):788?791.
Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010.
Topic models for word sense disambiguation and
token-based idiom detection. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, pages 1138?1147. Association for
Computational Linguistics.
David Mimno, Hanna M Wallach, Jason Naradowsky,
David A Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 880?889. Association for
Computational Linguistics.
Martha Palmer, Owen Rambow, and Alexis Nasr. 1998.
Rapid prototyping of domain-specific machine trans-
lation systems. In Machine Translation and the Infor-
mation Soup, pages 95?102. Springer.
Philipp Sorg and Philipp Cimiano. 2008. Cross-lingual
information retrieval with explicit semantic analysis.
In Proceedings of the Cross-language Evaluation Fo-
rum 2008.
Philipp Sorg and Philipp Cimiano. 2010. An experi-
mental comparison of explicit semantic analysis im-
plementations for cross-language retrieval. In Natural
Language Processing and Information Systems, pages
36?48. Springer.
Vassilis Spiliopoulos, George A Vouros, and Vangelis
Karkaletsis. 2007. Mapping ontologies elements us-
ing features in a latent space. In IEEE/WIC/ACM
International Conference on Web Intelligence, pages
457?460. IEEE.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for statistical machine
translation. Machine Translation, 21(4):187?207.
1740
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 848?854,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Bidirectional Inter-dependencies of Subjective Expressions and
Targets and their Value for a Joint Model
Roman Klinger and Philipp Cimiano
Semantic Computing Group
Cognitive Interaction Technology ? Center of Excellence (CIT-EC)
Bielefeld University
33615 Bielefeld, Germany
{rklinger,cimiano}@cit-ec.uni-bielefeld.de
Abstract
Opinion mining is often regarded as a clas-
sification or segmentation task, involving
the prediction of i) subjective expressions,
ii) their target and iii) their polarity. In-
tuitively, these three variables are bidirec-
tionally interdependent, but most work has
either attempted to predict them in isolation
or proposing pipeline-based approaches
that cannot model the bidirectional interac-
tion between these variables. Towards bet-
ter understanding the interaction between
these variables, we propose a model that
allows for analyzing the relation of target
and subjective phrases in both directions,
thus providing an upper bound for the im-
pact of a joint model in comparison to a
pipeline model. We report results on two
public datasets (cameras and cars), show-
ing that our model outperforms state-of-
the-art models, as well as on a new dataset
consisting of Twitter posts.
1 Introduction
Sentiment analysis or opinion mining is the task of
identifying subjective statements about products,
their polarity (e. g. positive, negative or neutral)
in addition to the particular aspect or feature of
the entity that is under discussion, i. e., the so-
called target. Opinion analysis is thus typically
approached as a classification (Ta?ckstro?m and Mc-
Donald, 2011; Sayeed et al, 2012; Pang and Lee,
2004) or segmentation (Choi et al, 2010; Johans-
son and Moschitti, 2011; Yang and Cardie, 2012)
task by which fragments of the input are classi-
fied or labelled as representing a subjective phrase
(Yang and Cardie, 2012), a polarity or a target (Hu
and Liu, 2004; Li et al, 2010; Popescu and Etzioni,
2005; Jakob and Gurevych, 2010). As an example,
the sentence ?I like the low weight of the camera.?
contains a subjective term ?like?, and the target
?low weight?, which can be classified as a positive
statement.
While the three key variables (subjective phrase,
polarity and target) intuitively influence each other
bidirectionally, most work in the area of opinion
mining has concentrated on either predicting one
of these variables in isolation (e. g. subjective ex-
pressions by Yang and Cardie (2012)) or modeling
the dependencies uni-directionally in a pipeline ar-
chitecture, e. g. predicting targets on the basis of
perfect and complete knowledge about subjective
terms (Jakob and Gurevych, 2010). However, such
pipeline models do not allow for inclusion of bidi-
rectional interactions between the key variables. In
this paper, we propose a model that can include
bidirectional dependencies, attempting to answer
the following questions which so far have not been
addressed but provide the basis for a joint model:
? What is the impact of the performance loss
of a non-perfect subjective term extraction in
comparison to perfect knowledge?
? Further, how does perfect knowledge about
targets influence the prediction of subjective
terms?
? How is the latter affected if the knowledge
about targets is imperfect, i. e. predicted by a
learned model?
We study these questions using imperatively de-
fined factor graphs (IDFs, McCallum et al (2008),
McCallum et al (2009)) to show how these bi-
directional dependencies can be modeled in an ar-
chitecture which allows for further steps towards
joint inference. IDFs are a convenient way to define
probabilistic graphical models that make structured
predictions based on complex dependencies.
848
2 A Model for the Extraction of Target
Phrases and Subjective Expressions
This section gives a brief introduction to impera-
tively defined factor graphs and then introduces our
model.
2.1 Imperatively Defined Factor Graphs
A factor graph (Kschischang et al, 2001) is a bi-
partite graph over factors and variables. Let factor
graph G define a probability distribution over a
set of output variables y conditioned on input vari-
ables x. A factor ?i computes a scalar value over
the subset of variables xi and yi that are neighbors
of ?i in the graph. Often this real-valued function
is defined as the exponential of an inner product
over sufficient statistics {fik(xi,yi)} and parame-
ters {?ik}, where k ? [1,Ki] and Ki is the number
of parameters for factor ?i.
A factor template Tj consists of parameters
{?jk}, sufficient statistic functions {fjk}, and a
description of an arbitrary relationship between
variables, yielding a set of tuples {(xj ,yj)}. For
each of these tuples, the factor template instan-
tiates a factor that shares {?jk} and {fjk} with
all other instantiations of Tj . Let T be the set of
factor templates and Z(x) be the partition func-
tion for normalization. The probability distri-
bution can then be written as p(y|x) = 1Z(x)?
Tj?T
?
(xi,yi)?Tj exp
(?Kj
k=1 ?jkfjk(xi,yi)
)
.
FACTORIE1 (McCallum et al, 2008; McCallum
et al, 2009) is an implementation of imperatively
defined factor graphs in the context of Markov
1http://factorie.cs.umass.edu
better than CCD shift systems
POS=JJR
W=better
POS-W=better JJR
ONE-EDGE-POS=JJR
ONE-EDGE-W=better
ONE-EDGE-POS-W=better JJR
ONE-EDGE-POS-SEQ=JJR
BOTH-POS=JJR
BOTH-W=better
BOTH-POS-W=better JJR
BOTH-POS-POS-SEQ=JJR
POS=NN
W=shift
W=systems
POS-W=shift NN
POS-W=systems NNS
POS-SEQ=NN-NNS
NO-CLOSE-NOUN
ONE-EDGE-POS=NN
ONE-EDGE-POS=NNS
ONE-EDGE-W=shift
ONE-EDGE-W=sensors
BOTH-POS=NN
BOTH-POS=NNS
. . .
subjective target
sin
gle
sp
an
in
te
rs
pa
n
Figure 1: Example for features extracted for target
and subjective expressions (text snippet taken from
the camera data set (Kessler et al, 2010)). IOB-like
features are merged for simplicity in this depiction.
chain Monte Carlo (MCMC) inference, a common
approach for inference in very large graph struc-
tures (Culotta and McCallum, 2006; Richardson
and Domingos, 2006; Milch et al, 2006). The
term imperative is used to denote that actual code
in an imperative programming language is writ-
ten to describe templates and the relationship of
tuples they yield. This flexibility is beneficial for
modeling inter-dependencies as well as designing
information flow in joint models.
2.2 Model
Our model is similar to a semi-Markov conditional
random field (Sarawagi and Cohen, 2004). It pre-
dicts the offsets for target mentions and subjective
phrases and can use the information of each other
during inference. In contrast to a linear chain con-
ditional random field (Lafferty et al, 2001), this al-
lows for taking distant dependencies of unobserved
variables into account and simplifies the design of
features measuring characteristics of multi-token
phrases. The relevant variables, i. e. target and sub-
jective phrase, are modelled via complex span vari-
ables of the form s = (l, r, c) with a left and right
offset l and r, and a class c ? {target, subjective}.
These offsets denote the span on a token sequence
t = (t1, . . . , tn).
We use two different templates to define factors
between variables: a single span template and an
inter-span template. The single span template de-
fines factors with scores based on features of the
tokens in the span and its vicinity. In our model,
all features are boolean. As token-based features
we use the POS tag, the lower-case representation
of the token as well as both in combination. The
actual span representation consists of these features
prefixed with ?I? for all tokens in the span, with ?B?
for the token at the beginning of the span, and with
?E? for the token at the end of the span. In addition,
the sequence of POS tags of all tokens in the span
is included as a feature.
The inter-span template takes three characteris-
tics of spans into account: Firstly, we measure if
a potential target span contains a noun which is
the closest noun to a subjective expression. Sec-
ondly, we measure for each span if a span of the
other class is in the same sentence. A third fea-
ture indicates whether there is only one edge in the
dependency graph between the tokens contained
in spans of a different class. These features are
to a great extent inspired by Jakob and Gurevych
849
(2010). For parsing, we use the Stanford parser
(Klein and Manning, 2003).
The features described so far, however, cannot
differentiate between a possible aspect mention
which is a target of a subjective expression and
one which is not. Therefore, the features of the
inter-span template are actually built by taking the
cross-product of the three described characteristics
with all single-span features. Spans which are not
in the context of a span of a different class are rep-
resented by a ?negated? feature (namely No-Close-
Noun, No-Single-Edge, and Not-Both-In-Sentence).
The example in Figure 1 shows features for two
spans which are in context of each other. All of
these features representing the text are taken into
account for each class, i. e., target and subjective
expression.
Inference is performed via Markov Chain Monte
Carlo (MCMC) sampling. In each sampling step,
only the variables which actually change need to
be evaluated, and therefore the sampler directs the
process of unrolling the templates to factors. These
world changes are necessary to find the maximum
a posteriori (MAP) configuration as well as learn-
ing the parameters of the model. For each token
in the sequence, a span of length one of each class
is proposed if no span containing the token exists.
For each existing span, it is proposed to change
its label, shorten or extend it by one token if pos-
sible (all at the beginning and at the end of the
span, respectively). Finally, a span can be removed
completely.
In order to learn the parameters of our model, we
apply SampleRank (Wick et al, 2011). A crucial
component in the framework is the objective func-
tion which gives feedback about the quality of a
sample proposal during training. We use the follow-
ing objective function f(t) to evaluate a proposed
span t:
f(t) = max
g?s
o(t,g)
|g| ? ? ? p(t,g) ,
where s is the set of all spans in the gold standard.
Further, the function o calculates the overlap in
terms of tokens of two spans and the function p
returns the number of tokens in t that are not con-
tained in g, i. e., those which are outside the overlap
(both functions taking into account the class of the
span). Thus, the first part of the objective function
represents the fraction of correctly proposed con-
tiguous tokens, while the second part penalizes a
span for containing too many tokens that are out-
side the best span. Here, ? is a parameter which
controls the penalty.
3 Results and Discussion
3.1 Experimental Setting
We report results on the J.D. Power and Associates
Sentiment Corpora2, an annotated data set of blog
posts in the car and in the camera domain (Kessler
et al, 2010). From the rich annotation set, we
use subjective terms and entity mentions which
are in relation to them as targets. We do not con-
sider comitter, negator, neutralizer,
comparison, opo, or descriptor annota-
tions to be subjective expressions. Results on these
data sets are compared to Jakob and Gurevych
(2010).
In addition, we report results on a Twitter data
set3 for the first time (Spina et al, 2012). Here,
we use a Twitter-specific tokenizer and POS tag-
ger4 (Owoputi et al, 2013) instead of the Stanford
parser. Hence, the single-edge-based feature de-
scribed in Section 2.2 is not used for this dataset. A
short summary of the datasets is given in Table 1.
As evaluation metric we use the F1 measure, the
harmonic mean between precision and recall. True
positive spans are evaluated in a perfect match and
approximate match mode, where the latter regards
a span as positive if one token within it is included
in a corresponding span in the gold standard. In this
case, other predicted spans matching the same gold
span do not count as false positives. In the objective
function, ? is set to 0.01 to prefer spans which are
longer than the gold phrase over predicting no span.
Four different experiments are performed (all
via 10-fold cross validation): First, predicting sub-
jectivity expressions followed by predicting targets
while making use of the previous prediction. Sec-
2http://verbs.colorado.edu/jdpacorpus/
3http://nlp.uned.es/?damiano/datasets/
entityProfiling_ORM_Twitter.html
4In version 0.3, http://www.ark.cs.cmu.edu/
TweetNLP/
Car Camera Twitter
Texts 457 178 9238
Targets 11966 4516 1418
Subjectives 15056 5128 1519
Table 1: Statistics of the data sets.
850
 0
 0.2
 0.4
 0.6
 0.8
 1
pred. S. ? T. pred. T. ? S. Gold S. ? T. Gold T. ? S. Jakob 2010
F 1
Target-F1 PartialSubjective-F1 Partial
Target-F1Subjective-F1
 0.53
 0.44
 0.65 0.61  0.65
 0.71
 0.48
 0.32
 0.58
 1.00
 0.50 0.54
 0.60
 1.00
 0.65
 1.00
Figure 2: Results for the workflow of first predicting subjective phrases, then targets (pred. S.? T.), and
vice versa (pred. T.? S.), as well as in comparison to having perfect information for the first step for the
camera data set.
 0
 0.2
 0.4
 0.6
 0.8
 1
pred. S. ? T. pred. T. ? S. Gold S. ? T. Gold T. ? S. Jakob 2010
F 1
Target-F1 PartialSubjective-F1 Partial
Target-F1Subjective-F1
 0.51
 0.43
 0.62 0.64  0.69
 0.74
 0.43
 0.33
 0.55
 1.00
 0.50 0.56
 0.66
 1.00
 0.70
 1.00
Figure 3: Results for the car data set.
ond, predicting targets followed by predicting sub-
jective expressions. Third, assuming perfect knowl-
edge of subjective expressions when predicting tar-
gets, and fourth, assuming perfect knowledge of
targets in predicting subjective expressions. This
provides us with the information how good a pre-
diction can be with perfect knowledge of the other
variable as well as an estimate of how good the
prediction can be without any previous knowledge.
3.2 Results
Figures 2, 3 and 4 show the results for the four
different settings compared to the results by Jakob
and Gurevych (2010) for cars and cameras. The
darker bars correspond to perfect match, the lighter
ones to the increase when taking partial matches
into account. In the following we only discuss the
perfect match.
Comparing the results (for the car and camera
data sets, Figure 2 and 3) for subjectivity predic-
tion, one can observe a limited performance when
targets are not known (0.54F1 for the camera set,
0.56F1 for the car set), an upper bound with per-
fect target information is much higher (0.65F1,
0.7F1). When first predicting targets followed by
subjective term prediction, we obtain results of
0.6F1 and 0.66F1. The results for target predic-
tion are much lower when not knowing subjec-
tive expressions in advance (0.32F1, 0.33F1), and
clearly increase with predicted subjective expres-
sions (0.48F1, 0.43F1) and outperform previous
results when compared to Jakob and Gurevych
(2010) (0.58F1, 0.55F1 in comparison to their
0.5F1 on both sets).
The results for the Twitter data set show the same
characteristics (in Figure 4). However, they are
generally much lower. In addition, the difference
between exact and partial match evaluation modes
851
 0
 0.2
 0.4
 0.6
 0.8
 1
pred. S. ? T. pred. T. ? S. Gold S. ? T. Gold T. ? S.
F 1
Target-F1 PartialSubjective-F1 Partial
Target-F1Subjective-F1
 0.42
 0.32
 0.67
 0.40  0.41
 0.60
 0.26
 0.13
 0.40
 1.00
 0.22  0.28
 1.00
 0.35
Figure 4: Results for the Twitter data set.
 0
 0.2
 0.4
 0.6
 0.8
 1
Sentence Edge Noun All
F 1  0.48
 0.57  0.52
 0.65
 0.41
 0.48  0.42
 0.58
(a) Camera Data Set, given subjective terms.
 0
 0.2
 0.4
 0.6
 0.8
 1
Sentence Edge Noun All
F 1
 0.68
 0.55
 0.17
 0.71
 0.62
 0.51
 0.17
 0.65
(b) Camera Data Set, given target terms.
Figure 5: Evaluation of the impact of different features.
is higher. This is due to the existence of many more
phrases spanning multiple tokens.
Exemplarily, the impact of the three features in
the inter-span templates for the camera data set is
depicted in Figure 5 for (a) given subjective terms
(b) given targets, respectively. Detecting the clos-
est noun is mainly of importance for target iden-
tification and only to a minor extent for detecting
subjective phrases. A short path in the dependency
graph and detecting if both phrases are in the same
sentence have a high positive impact for both sub-
jective and target phrases.
3.3 Conclusion and Discussion
The experiments in this paper show that target
phrases and subjective terms are clearly interde-
pendent. However, the impact of knowledge about
one type of entity for the prediction of the other
type of entity has been shown to be asymmetric.
The results clearly suggest that the impact of sub-
jective terms on target terms is higher than the other
way round. Therefore, if a pipeline architecture is
chosen, this order is to be preferred. However, the
results with perfect knowledge of the counterpart
entity show (in both directions) that the entities
influence each other positively. Therefore, the chal-
lenge of extracting subjective expressions and their
targets is a great candidate for applying supervised,
joint inference.
Acknowledgments
Roman Klinger has been funded by the ?It?s
OWL? project (?Intelligent Technical Systems
Ostwestfalen-Lippe?, http://www.its-owl.
de/), a leading-edge cluster of the German Min-
istry of Education and Research. We thank the
information extraction and synthesis laboratory
(IESL) at the University of Massachusetts Amherst
for their support.
852
References
Yoonjung Choi, Seongchan Kim, and Sung-Hyon
Myaeng. 2010. Detecting Opinions and their Opin-
ion Targets in NTCIR-8. Proceedings of NTCIR8
Workshop Meeting, pages 249?254.
A. Culotta and A. McCallum. 2006. Tractable Learn-
ing and Inference with High-Order Representations.
In ICML Workshop on Open Problems in Statistical
Relational Learning.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168?177,
New York, NY, USA. ACM.
Niklas Jakob and Iryna Gurevych. 2010. Extracting
opinion targets in a single- and cross-domain set-
ting with conditional random fields. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1035?1045,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Richard Johansson and Alessandro Moschitti. 2011.
Extracting opinion expressions and their polarities:
exploration of pipelines and joint models. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies: short papers ? Volume 2, pages
101?106, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Jason S. Kessler, Miriam Eckert, Lyndsie Clark, and
Nicolas Nicolov. 2010. The 2010 ICWSM JDPA
Sentment Corpus for the Automotive Domain. In
4th International AAAI Conference on Weblogs and
Social Media Data Workshop Challenge (ICWSM-
DWC 2010).
D. Klein and Ch. D. Manning. 2003. Fast exact in-
ference with a factored model for natural language
parsing. In Advances in Neural Information Process-
ing Systems 16 [Neural Information Processing Sys-
tems.
F.R. Kschischang, B.J. Frey, and H.-A. Loeliger. 2001.
Factor graphs and the sum-product algorithm. Infor-
mation Theory, IEEE Trans on Information Theory,
47(2):498?519.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In International Conference on Ma-
chine Learning, pages 282?289.
Fangtao Li, Minlie Huang, and Xiaoyan Zhu. 2010.
Sentiment analysis with global topics and local de-
pendency. In Proceedings of the Twenty-Fourth
AAAI Conference on Artificial Intelligence, pages
1371?1376, Atlanta, Georgia, USA.
A. McCallum, K. Rohanimanesh, M. Wick, K. Schultz,
and Sameer Singh. 2008. FACTORIE: Efficient
Probabilistic Programming via Imperative Declara-
tions of Structure, Inference and Learning. In NIPS
Workshop on Probabilistic Programming.
Andrew McCallum, Karl Schultz, and Sameer Singh.
2009. FACTORIE: Probabilistic programming via
imperatively defined factor graphs. In Neural Infor-
mation Processing Systems (NIPS).
B. Milch, B. Marthi, and S. Russell. 2006. BLOG:
Relational Modeling with Unknown Objects. Ph.D.
thesis, University of California, Berkeley.
O. Owoputi, B. OConnor, Ch. Dyer, K. Gimpely,
N. Schneider, and N. A. Smith. 2013. Improved
part-of-speech tagging for online conversational text
with word clusters. In The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: Sentiment analysis using subjectivity sum-
marization based on minimum cuts. In Proceedings
of the 42nd Meeting of the Association for Compu-
tational Linguistics, Main Volume, pages 271?278,
Barcelona, Spain, July.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of Human Language Technology Con-
ference and Conference on Empirical Methods in
Natural Language Processing, pages 339?346, Van-
couver, British Columbia, Canada, October. Associ-
ation for Computational Linguistics.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine Learning, 62(1-2):107?136.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Advances in Neural Information Pro-
cessing Systems 17 [Neural Information Processing
Systems.
Asad Sayeed, Jordan Boyd-Graber, Bryan Rusk, and
Amy Weinberg. 2012. Grammatical structures for
word-level sentiment detection. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 667?
676, Montre?al, Canada, June. Association for Com-
putational Linguistics.
D. Spina, E. Meij, A. Oghina, M. T. Bui, M. Breuss,
and M. de Rijke. 2012. A Corpus for Entity Pro-
filing in Microblog Posts. In LREC Workshop on
Information Access Technologies for Online Reputa-
tion Management.
Oscar Ta?ckstro?m and Ryan McDonald. 2011. Semi-
supervised latent variable models for sentence-level
sentiment analysis. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
853
569?574, Portland, Oregon, USA, June. Association
for Computational Linguistics.
M. Wick, K. Rohanimanesh, K. Bellare, A. Culotta,
and A. McCallum. 2011. SampleRank: Training
factor graphs with atomic gradients. In Interna-
tional Conference on Machine Learning.
Bishan Yang and Claire Cardie. 2012. Extracting opin-
ion expressions with semi-markov conditional ran-
dom fields. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1335?1345, Stroudsburg, PA, USA.
Association for Computational Linguistics.
854
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 119?122,
Dublin, Ireland, August 23-24, 2014.
Bielefeld SC: Orthonormal Topic Modelling for Grammar Induction
John P. M
c
Crae
CITEC, Bielefeld University
Inspiration 1
Bielefeld, Germany
jmccrae@cit-ec.uni-bielefeld.de
Philipp Cimiano
CITEC, Bielefeld University
Inspiration 1
Bielefeld, Germany
cimiano@cit-ec.uni-bielefeld.de
Abstract
In this paper, we consider the application
of topic modelling to the task of induct-
ing grammar rules. In particular, we look
at the use of a recently developed method
called orthonormal explicit topic analysis,
which combines explicit and latent models
of semantics. Although, it remains unclear
how topic model may be applied to the
case of grammar induction, we show that
it is not impossible and that this may allow
the capture of subtle semantic distinctions
that are not captured by other methods.
1 Introduction
Grammar induction is the task of inducing high-
level rules for application of grammars in spoken
dialogue systems. In practice, we can extract rel-
evant rules and the task of grammar induction re-
duces to finding similar rules between two strings.
As these strings are not necessarily similar in sur-
face form, what we really wish to calculate is
the semantic similarity between these strings. As
such, we could think of applying a semantic anal-
ysis method. As such we attempt to apply topic
modelling, that is methods such as Latent Dirich-
let Allocation (Blei et al., 2003), Latent Seman-
tic Analysis (Deerwester et al., 1990) or Explicit
Semantic Analysis (Gabrilovich and Markovitch,
2007). In particular we build on the recent work
to unify latent and explicit methods by means of
orthonormal explicit topics.
In topic modelling the key choice is the docu-
ment space that will act as the corpus and hence
topic space. The standard choice is to regard all
articles from a background document collection
? Wikipedia articles are a typical choice ? as the
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
topic space. However, it is crucial to ensure that
these topics cover the semantic space evenly and
completely. Following McCrae et al. (McCrae et
al., 2013) we remap the semantic space defined by
the topics in such a manner that it is orthonormal.
In this way, each document is mapped to a topic
that is distinct from all other topics.
The structure of the paper is as follows: we de-
scribe our method in three parts, first the method
in section 2, followed by approximation method in
section 3, the normalization methods in section 4
and finally the application to grammar induction
in section 5, we finish with some conclusions in
section 6.
2 Orthonormal explicit topic analysis
ONETA (McCrae et al., 2013, Orthonormal ex-
plicit topic analysis) follows Explicit Semantic
Analysis in the sense that it assumes the avail-
ability of a background document collection B =
{b
1
, b
2
, ..., b
N
} consisting of textual representa-
tions. The mapping into the explicit topic space
is defined by a language-specific function ? that
maps documents into R
N
such that the j
th
value in
the vector is given by some association measure
?
j
(d) for each background document b
j
. Typical
choices for this association measure ? are the sum
of the TF-IDF scores or an information retrieval
relevance scoring function such as BM-25 (Sorg
and Cimiano, 2010).
For the case of TF-IDF, the value of the j-th
element of the topic vector is given by:
?
j
(d) =
????
tf-idf(b
j
)
T
????
tf-idf(d)
Thus, the mapping function can be represented
as the product of a TF-IDF vector of document d
multiplied by a word-by-document (W ?N ) TF-
IDF matrix, which we denote as a X:
1
1T
denotes the matrix transpose as usual
119
?(d) =
?
?
?
????
tf-idf(b
1
)
T
.
.
.
????
tf-idf(b
N
)
T
?
?
?
????
tf-idf(d) = X
T
?
????
tf-idf(d)
For simplicity, we shall assume from this point
on that all vectors are already converted to a TF-
IDF or similar numeric vector form.
In order to compute the similarity between two
documents d
i
and d
j
, typically the cosine-function
(or the normalized dot product) between the vec-
tors ?(d
i
) and ?(d
j
) is computed as follows:
sim(d
i
, d
j
) = cos(?(d
i
),?(d
j
)) =
?(d
i
)
T
?(d
j
)
||?(d
i
)||||?(d
j
)||
sim(d
i
, d
j
) = cos(X
T
d
i
,X
T
d
j
) =
d
T
i
XX
T
d
j
||X
T
d
i
||||X
T
d
j
||
The key challenge with topic modelling is
choosing a good background document collection
B = {b
1
, ..., b
N
}. A simple minimal criterion
for a good background document collection is that
each document in this collection should be maxi-
mally similar to itself and less similar to any other
document:
?i 6= j 1 = sim(b
j
, b
j
) > sim(b
i
, b
j
) ? 0
As shown in McCrae et al. (2013), this property
is satisfied by the following projection:
?
ONETA
(d) = (X
T
X)
?1
X
T
d
And hence the similarity between two docu-
ments can be calculated as:
sim(d
i
, d
j
) = cos(?
ONETA
(d
i
),?
ONETA
(d
j
))
3 Approximations
ONETA relies on the computation of a matrix in-
verse, which has a complexity that, using current
practical algorithms, is approximately cubic and
as such the time spent calculating the inverse can
grow very quickly.
We notice that X is typically very sparse and
moreover some rows ofX have significantly fewer
non-zeroes than others (these rows are for terms
with low frequency). Thus, if we take the first N
1
columns (documents) in X, it is possible to re-
arrange the rows of X with the result that there
is some W
1
such that rows with index greater
than W
1
have only zeroes in the columns up to
N
1
. In other words, we take a subset of N
1
doc-
uments and enumerate the words in such a way
that the terms occurring in the first N
1
documents
are enumerated 1, . . . ,W
1
. Let N
2
= N ? N
1
,
W
2
= W ?W
1
. The result of this row permuta-
tion does not affect the value of X
T
X and we can
write the matrix X as:
X =
(
A B
0 C
)
where A is a W
1
? N
1
matrix representing
term frequencies in the first N
1
documents, B is a
W
1
?N
2
matrix containing term frequencies in the
remaining documents for terms that are also found
in the first N
1
documents, and C is a W
2
? N
2
containing the frequency of all terms not found in
the first N
1
documents.
Application of the well-known divide-and-
conquer formula (Bernstein, 2005, p. 159) for ma-
trix inversion yields the following easily verifiable
matrix identity, given that we can find C
?
such that
C
?
C = I.
(
(A
T
A)
?1
A
T
?(A
T
A)
?1
A
T
BC
?
0 C
?
)(
A B
0 C
)
= I
(1)
The inverse C
?
is approximated by the Jacobi
Preconditioner, J, of C
T
C:
C
?
' JC
T
(2)
=
?
?
?
||c
1
||
?2
0
.
.
.
0 ||c
N
2
||
?2
?
?
?
C
T
4 Normalization
A key factor in the effectiveness of topic-based
methods is the appropriate normalization of the el-
ements of the document matrix X. This is even
more relevant for orthonormal topics as the matrix
inversion procedure can be very sensitive to small
changes in the matrix. In this context, we con-
sider two forms of normalization, term and docu-
ment normalization, which can also be considered
as row/column normalizations of X.
A straightforward approach to normalization is
to normalize each column of X to obtain a matrix
as follows:
120
X?
=
(
x
1
||x
1
||
. . .
x
N
||x
N
||
)
If we calculate X
?
T
X
?
= Y then we get that the
(i, j)-th element of Y is:
y
ij
=
x
T
i
x
j
||x
i
||||x
j
||
Thus, the diagonal of Y consists of ones only and
due to the Cauchy-Schwarz inequality we have
that |y
ij
| ? 1, with the result that the matrix Y
is already close to I. Formally, we can use this
to state a bound on ||X
?
T
X
?
? I||
F
, but in prac-
tice it means that the orthonormalizing matrix has
more small or zero values. Previous experiments
have indicated that in general term normalization
such as TF-IDF is not as effective as using the di-
rect term frequency in ONETA, so we do not apply
term normalization.
5 Application to grammar induction
The application to grammar induction is simply
carried out by taking the rules and creating a sin-
gle ground instance. That is if we have a rule of
the form
LEAVING FROM <CITY>
We would replace the instance of <CITY> with
a known terminal for this rule, e.g.,
leaving from Berlin
This reduces the task to that of string simi-
larity which can be processed by means of any
string similarity function, for example such as the
ONETA function described above. As such the
procedure is as follows:
1. Ground the input grammar rule to an English
string d
2. Ground each candidate matching rule to an
English string d
i
3. Calculate for each d
i
, the similarity
sim
ONETA
(d, d
i
)
4. Add the rule to the grammar class with the
highest similarity
This approach has the obvious drawback that it
removes all information about the valence of the
rule, however the effect of this loss of information
remains unclear.
For application, we used 20,000 Wikipedia ar-
ticles, filtered to contain only those of over 100
words, giving us a corpus of 15.6 million tokens.
We applied ONETA using document normaliza-
tion but no term normalization and the valueN
1
=
5000. These parameters were chosen based on the
best results in previous experiments.
6 Conclusions
The results show that such a naive approach is
not directly applicable to the case of grammar in-
duction, however we believe that it is possible
that the subtle semantic similarities captured by
topic modelling may yet prove useful for gram-
mar induction. However it is clear from the pre-
sented results that the use of a topic model alone
does not suffice to solve this task. We notice that
from the data many of the distinctions rely on
antonyms and stop words, especially distinctions
such as ?to?/?from?, which are not captured by a
topic model as topic models generally ignore stop
words, and generally consider antonyms to be in
the same topic, as they frequently occur together
in text. The question of when semantic similarity
such as provided by topic modelling is applicable
remains an open question.
References
Dennis S Bernstein. 2005. Matrix mathematics, 2nd
Edition. Princeton University Press Princeton.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391?407.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using Wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence, volume 6, page 12.
John P. McCrae, Philipp Cimiano, and Roman Klinger.
2013. Orthonormal explicit topic analysis for cross-
lingual document matching. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, pages 1732?1740.
121
Philipp Sorg and Philipp Cimiano. 2010. An experi-
mental comparison of explicit semantic analysis im-
plementations for cross-language retrieval. In Natu-
ral Language Processing and Information Systems,
pages 36?48. Springer.
122
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 116?125,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Combining statistical and semantic approaches to the translation of
ontologies and taxonomies
John McCrae
AG Semantic Computing
Universita?t Bielefeld
Bielefeld, Germany
jmccrae@cit-ec.uni-bielefeld.de
Mauricio Espinoza
Universidad de Cuenca
Cuenca, Ecuador
mauricio.espinoza@ucuenca.edu.ec
Elena Montiel-Ponsoda, Guadalupe Aguado-de-Cea
Ontology Engineering Group
Universidad Polite?cnica de Madrid
Madrid, Spain
{emontiel, lupe}@fi.upm.es
Philipp Cimiano
AG Semantic Computing
Universita?t Bielefeld
Bielefeld, Germany
cimiano@cit-ec.uni-bielefeld.de
Abstract
Ontologies and taxonomies are widely used to
organize concepts providing the basis for ac-
tivities such as indexing, and as background
knowledge for NLP tasks. As such, trans-
lation of these resources would prove use-
ful to adapt these systems to new languages.
However, we show that the nature of these
resources is significantly different from the
?free-text? paradigm used to train most sta-
tistical machine translation systems. In par-
ticular, we see significant differences in the
linguistic nature of these resources and such
resources have rich additional semantics. We
demonstrate that as a result of these linguistic
differences, standard SMT methods, in partic-
ular evaluation metrics, can produce poor per-
formance. We then look to the task of leverag-
ing these semantics for translation, which we
approach in three ways: by adapting the trans-
lation system to the domain of the resource;
by examining if semantics can help to predict
the syntactic structure used in translation; and
by evaluating if we can use existing translated
taxonomies to disambiguate translations. We
present some early results from these experi-
ments, which shed light on the degree of suc-
cess we may have with each approach.
1 Introduction
Taxonomies and ontologies are data structures that
organise conceptual information by establishing re-
lations among concepts, hierarchical and partitive
relations being the most important ones. Nowadays,
ontologies have a wide range of uses in many do-
mains, for example, finance (International Account-
ing Standards Board, 2007), bio-medicine (Col-
lier et al, 2008) (Ashburner et al, 2000) and li-
braries (Mischo, 1982). These resources normally
attach labels in natural language to the concepts and
relations that define their structure, and these la-
bels can be used for a number of purposes, such
as providing user interface localization (McCrae et
al., 2010), multilingual data access (Declerck et al,
2010), information extraction (Mu?ller et al, 2004)
and natural language generation (Bontcheva, 2005).
It seems natural that for applications that use such
ontologies and taxonomies, translation of the natu-
ral language descriptions associated with them is re-
quired in order to adapt these methods to new lan-
guages. Currently, there has been some work on
this in the context of ontology localisation, such
as Espinoza et al (2008) and (2009), Cimiano et
al. (2010), Fu et al (2010) and Navigli and Pen-
zetto (2010). However, this work has focused on the
case in which exact or partial translations are found
in other similar resources such as bilingual lexica.
Instead, in this paper we look at how we may gain an
adequate translation using statistical machine trans-
lation approaches that also utilise the semantic in-
formation beyond the label or term describing the
concept, that is relations among the concepts in the
ontology, as well as the attributes or properties that
describe concepts, as will be explained in more de-
tail in section 2.
Current work in machine translation has shown
that word sense disambiguation can play an im-
portant role by using the surrounding words as
context to disambiguate terms (Carpuat and Wu,
2007) (Apidianaki, 2009). Such techniques have
116
been extrapolated to the translation of taxonomies
and ontologies, in which the ?context? of a taxon-
omy or ontology label corresponds to the ontology
structure that surrounds the label in question. This
structure, which is made up of the lexical informa-
tion provided by labels and the semantic informa-
tion provided by the ontology structure, defines the
sense of the concept and can be exploited in the dis-
ambiguation process (Espinoza et al, 2008).
2 Definition of Taxonomy and Ontology
Translation
2.1 Formal Definition
We define a taxonomy as a set of concepts, C, with
equivalence (synonymy) links, S, subsumption (hy-
pernymy) links, H , and a labelling function l that
maps each concept to a single label from a language
??. Formally we define a taxonomy, T , as a set of
tuples (C, S,H, l) such that S ? P(C ? C) and
H ? P(C ? C) and l is a function in C ? ??. We
also require that S is a transitive, symmetric and re-
flexive relation, and H is transitive. While we note
here that this abstraction does not come close to cap-
turing the full expressive power of many ontologies
(or even taxonomies), it is sufficient for this paper to
focus on the use of only equivalence and subsump-
tion relationships for translation.
2.2 Analysis of ontology labels
Another important issue to note here is that the
kind of language used within ontologies and tax-
onomies is significantly different from that found
within free text. In particular, we observe that the
terms used to designate concepts are frequently just
noun phrases and are significantly shorter than a
usual sentence. In the case of the relations between
concepts (dubbed object properties) and attributes
of concepts (data type properties), these are occa-
sionally labelled by means of verbal phrases. We
demonstrate this by looking at three widely used on-
tologies/taxonomies.
1. Friend of a friend: The Friend of a Friend
(FOAF) ontology is used to describe social
networks on the Semantic Web (Brickley and
Miller, 2010). It is a small taxonomy with very
short labels. Labels for concepts are compound
words made up of up to three words.
2. Gene Ontology: The Gene Ontology (Ash-
burner et al, 2000) is a very large database of
terminology related to genetics. We note that
while some of the terms are technical and do
not require translation, e.g., ESCRT-I, the ma-
jority do, e.g., cytokinesis by cell plate forma-
tion.
3. IFRS 2009: The IFRS taxonomy (International
Accounting Standards Board, 2007) is used for
providing electronic financial reports for audit-
ing. The terms contained within this taxon-
omy are frequently long and are entirely noun
phrases.
We applied tokenization and manual phrase anal-
ysis to the labels in these resources and the results
are summarized in table 1. As can be observed,
the variety of types of labels we may come across
when linguistically analysing and translating ontol-
ogy and taxonomy labels is quite large. We can
identify the two following properties that may influ-
ence the translation process of taxonomy and ontol-
ogy labels. Firstly, the length of terms ranges from
single words to highly complex compound phrases,
but is still generally shorter than a sentence. Sec-
ondly, terms are frequently about highly specialized
domains of knowledge.
For properties in the ontology we also identify
terms which consist of:
? Noun phrases identifying concepts.
? Verbal phrases that are only made up of the
verb with an optional preposition.
? Complex verbal phrases that include the predi-
cate.
? Noun phrases that indicate possession of a par-
ticular characteristic (e.g., interest meaning X
has an interest in Y).
3 Creation of a corpus for taxonomy and
ontology translation
For the purpose of training systems to work on the
translation of ontologies and taxonomies, it is nec-
essary to create a corpus that has similar linguistic
structure to that found in ontologies and taxonomies.
We used the titles of Wikipedia1 for the following
1http://www.wikipedia.org
117
Size Mean tokens per label Noun Phrases Verb Phrases
FOAF 79 1.57 94.9% 8.9%
Gene Ontology 33795 4.45 100.0% 0.0%
IFRS 2009 2757 8.39 100.0% 0.0%
Table 1: Lexical Analysis of labels
Link Direct Fragment Broken
German 487372 484314 1735 1323
Spanish 347953 346941 330 682
Table 2: Number of translation for pages in Wikipedia
reasons:
? Links to articles in different languages can be
viewed as translations of the page titles.
? The titles of articles have similar properties to
the ontologies labels mentioned above with an
average of 2.46 tokens.
? There are a very large number of labels. In fact
we found that there were 5,941,8902 articles of
which 3,515,640 were content pages (i.e., not
special pages such as category pages)
We included non-content pages (in particular, cat-
egory pages) in the corpus as they were generally
useful for translation, especially the titles of cat-
egory pages. In table 2 we see the number of
translations, which we further grouped according to
whether they actually corresponded to pages in the
other languages, as it is also possible that the trans-
lations links pointed to subsections of an article or
to missing pages.
Wikipedia also includes redirect links that allow
for alternative titles to be mapped to a given con-
cept. These can be useful as they contain synonyms,
but also introduce a lot more noise into the corpus
as they also include misspelled and foreign terms.
To evaluate the effectiveness of including these data
for creating a machine translation corpus, we took
a random sample of 100 pages which at least one
page redirects to (there are 1,244,647 of these pages
in total). We found that these pages had a total
of 242 extra titles from the redirect page of which
2All statistics are based on the dump on 17th March 2011
204 (84.3%) where true synonyms, 19 (7.9%) were
misspellings, 8 (3.3%) were foreign names for con-
cepts (e.g., the French name for ?Zeebrugge?), and
11 (4.5%) were unrelated. As such, we conclude
that these extra titles were useful for constructing the
corpus, increasing the size of the corpus by approx-
imately 50% across all languages. There are sev-
eral advantages to deriving a corpus fromWikipedia,
for example it is possible to provide some hierarchi-
cal links by the use of the category that a page be-
longs to, such as has been performed by the DBpedia
project (Auer et al, 2007).
4 Evaluation metrics for taxonomy and
ontology translation
Given the linguistic differences in taxonomy and
ontology labels, it seems necessary to investigate
the effectiveness of various metrics for the evalua-
tion of translation quality. There are a number of
metrics that are widely used for evaluating trans-
lation. Here we will focus on some of the most
widely used, namely BLEU (Papineni et al, 2002),
NIST (Doddington, 2002), METEOR (Banerjee and
Lavie, 2005) and WER (McCowan et al, 2004).
However, it is not clear which of these methods cor-
relate best with human evaluation, particularly for
the ontologies with short labels. To evaluate this
we collected a mixture of ontologies with short la-
bels on the topics of human diseases, agriculture,
geometry and project management, producing 437
labels. These were translated with web transla-
tion services from English to Spanish, in particu-
lar Google Translate3, Yahoo! BabelFish4 and SDL
FreeTranslation5. Having obtained translations for
each label in the ontology we calculated the evalua-
tion scores using the four metrics mentioned above.
We found that the source ontologies had an average
3http://translate.google.com
4http://babelfish.yahoo.com
5http://www.freetranslation.com
118
BLEU NIST METEOR WER
Evaluator 1,
Fluency 0.108 0.036 0.134 0.122
Evaluator 1,
Adequacy 0.209 0.214 0.303 0.169
Evaluator 2,
Fluency 0.183 0.062 0.266 0.164
Evaluator 2,
Adequacy 0.177 0.111 0.251 0.194
Evaluator 3,
Fluency 0.151 0.067 0.210 0.204
Evaluator 3,
Adequacy 0.143 0.129 0.221 0.120
Table 3: Correlation between manual evaluation results
and automatic evaluation scores
label length of 2.45 tokens and the translations gen-
erated had an average length of 2.16 tokens. We then
created a data set by mixing the translations from the
web translation services with a number of transla-
tions from the source ontologies, to act as a control.
We then gave these translations to 3 evaluators, who
scored them for adequacy and fluency as described
in Koehn (2010). Finally, we calculated the Pearson
correlation coefficient between the automatic scores
and the manual scores obtained. These are presented
in table 3 and figure 1.
As we can see from these results, one metric,
namely METEOR, seems to perform best in evaluat-
ing the quality of the translations. In fact this is not
surprising as there is a clear mathematical deficiency
that both NIST and BLEU have for evaluating trans-
lations for very short labels like the ones we have
here. To illustrate this, we recall the formulation of
BLEU as given in (Papineni et al, 2002):
BLEU = BP ? exp(
N
?
n=1
wn log pn)
WhereBP is a brevity penalty, wn a weight value
and pn represents the n-gram precision, indicating
how many times a particular n-gram in the source
text is found among the target translations. We note,
however, that for very short labels it is highly likely
that pn will be zero. This creates a significant issue,
as from the equation above, if any of the values of pn
are zero, the overall score, BLEU, will also be zero.
Figure 1: Correlation between manual evaluation results
and automatic evaluation scores
For the results above we chose N = 2, and cor-
rected for single-word labels. However, the scores
were still significantly worse, similar problems af-
fect the NIST metric. As such, for the taxonomy
and ontology translation task we do not recommend
using BLEU or NIST as an evaluation metric. We
note that METEOR is a more sophisticated method
than WER and, as expected, performs better.
5 Approaches for taxonomy and ontology
translation
5.1 Domain adaptation
It is generally the case that many ontologies and tax-
onomies focus on only a very specific domain, thus
it seems likely that adaptation of translation systems
by use of an in-domain corpus may improve trans-
lation quality. This is particularly valid in the case
of ontologies which frequently contain ?subject? an-
notations6 for not only the whole data structure but
often individual elements. To demonstrate this we
tried to translate the IFRS 2009 taxonomy using
the Moses Decoder (Koehn et al, 2007), which we
trained on the EuroParl corpus (Koehn, 2005), trans-
lating from Spanish to English. As the IFRS taxon-
omy is on the topic of finance and accounting, we
6For example from the Dublin Core vocabulary: see http:
//dublincore.org/
119
Baseline With domain adaptation
WER? 0.135 0.138
METEOR 0.324 0.335
NIST 1.229 1.278
BLEU 0.090 0.116
Table 4: Results of domain-adapted translation. ?Lower
WER scores are better
chose all terms from our Wikipedia corpus which
belonged to categories containing the words: ?fi-
nance?, ?financial?, ?accounting?, ?accountancy?,
?bank?, ?banking?, ?economy?, ?economic?, ?in-
vestment?, ?insurance?and ?actuarial? and as such
we had a domain corpus of approximately 5000
terms. We then proceeded to recompute the phrase
table using the methodology as described in Wu et
al, (2008), computing the probabilities as follows for
some weighting factor 0 < ? < 1:
p(e|f) = ?p1(e|f) + (1? ?)pd(e|f)
Where p1 is the EuroParl trained probability and pd
the scores on our domain subset. The evaluation for
these metrics is given in table 4. As can be seen
with the exception of the WER metric, the domain
adaption does seem to help in translation, which cor-
roborates the results obtained by other authors.
5.2 Syntactic Analysis
One key question to figure out is: if we have a se-
mantic model can this be used to predict the syntac-
tic structure of the translation to a significant degree?
As an example of this we consider the taxonomic
term ?statement?, which is translated by Google
Translate7 to German as ?Erkla?rung?, whereas the
term ?annual statement? is translated as ?Jahresab-
schluss?. However, if the taxonomy contains a sub-
sumption (hypernymy) relationship between these
terms we can deduce that the translation ?Erkla?rung?
is not correct and the translation ?Abschluss? should
be preferred. We chose to evaluate this idea on the
IFRS taxonomy as the labels it contains are much
longer and more structured than some of the other
resources. Furthermore, in this taxonomy the origi-
nal English labels have been translated into ten lan-
guages, so that it is already a multilingual resource
7Translations results obtained 8th March 2011
P (syn|s) P (syn|p) P (syn|n)
English 0.147 0.012 0.001
Dutch 0.137 0.011 0.001
German 0.125 0.007 0.001
Spanish 0.126 0.012 0.001
Table 5: Probability of syntactic relationship given a se-
mantic relationship in IFRS labels
that can be used as gold standard. Regarding the
syntax of labels, it is often the case that one term is
derived from another by addition of a complemen-
tary phrase. For example the following terms all ex-
ist in the taxonomy:
1. Minimum finance lease payments receivable
2. Minimum finance lease payments receivable, at
present value
3. Minimum finance lease payments receivable, at
present value, end of period not later than one
year
4. Minimum finance lease payments receivable, at
present value, end of period later than one year
and not later than five years
A high-quality translation of these terms would
ideally preserve this same syntactic structure in the
target language.We attempt to answer how useful
ontological structure is by trying to deduce if there
is a semantic relationship between terms then is it
more likely that there is a syntactic relationship. We
started by simplifying the idea of syntactic depen-
dency to the following: we say that two terms are
syntactically related if one label is a sub-string of
another, so that in the example above the first label
is syntactically related to the other three and the sec-
ond is related to the last two. For English, we found
that there were 3744 syntactically related terms ac-
cording to this criteria, corresponding to 0.1% of all
label pairs within the taxonomy, for all languages.
For ontology structure we used the number of rela-
tions indicated in the taxonomy, of which there are
1070 indicating a subsumption relationship and 987
indicating a partitive relationship8. This means that
8IFRS includes links for calculating certain values, i.e., that
?Total Assets? is a sum of values such as ?Total Assets in Prop-
120
e ? f P (synf |syne, s) P (synf |syne, p) P (synf |syne, n)
English ? Spanish 0.813 ? 0.059 0.750 ? 0.205 0.835 ? 0.013
English ? German 0.835 ? 0.062 0.417 ? 0.212 0.790 ? 0.013
English ? Dutch 0.875 ? 0.063 0.833 ? 0.226 0.898 ? 0.013
Average 0.841 ? 0.035 0.665 ? 0.101 0.841 ? 0.008
Table 6: Probability of cross-lingual preservation of syntax given semantic relationship in IFRS. Note here s refers to
the source language and t to the target language. Error values are 95% of standard deviation.
0.08% of label pairs were semantically related. We
then examined if the semantic relation could predict
whether there was a syntactic relationship between
the terms in a single language. We define Ns as the
number of label pairs with a subsumption relation-
ship and similarly define Np, Nn and Nsyn for parti-
tive, semantically unrelated and syntactically related
pairs. We also define Ns?syn, Np?syn and Nn?syn
for label pairs with both subsumption, partitive or no
semantic relation and a syntactic relationships. As
such we define the following values
P (syn|s) = Ns?syn
Ns
Similarly we define P (syn|p) and P (syn|n) and
present these values in table 5 for four languages.
As we can see from these results, it seems that
both subsumption and partitive relationships are
strongly indicative of syntactic relationships as we
might expect. The second question is: is it more
likely that we see a syntactic dependency in trans-
lation if we have a semantic relationship, i.e., is the
syntax more likely to be preserved if these terms are
semantically related. We define Nsyne as the value
of Nsyn for a language e, e.g., Nsynen is the num-
ber of syntactically related English label pairs in the
taxonomy. As each label has exactly one transla-
tion we can also define Nsyne?synf?s as the number
of concepts whose labels are syntactically related in
both language e and f and are semantically related
by a subsumption relationship; similarly we define
Nsyne?synf?p and Nsyne?synf?n. Hence we can de-
fine
P (synf |syne, s) =
Nsynf?syne?s
Nsyne?s
erty, Plant and Equipment?, we view such a relationship as se-
mantically indicative that one term is part of another, i.e., as
partitive or meronymic
And similarly define P (synf |syne, p) and
P (synf |syne, n). We calculated these values on
the IFRS taxonomies, the results of which are
represented in table 6.
The partitive data was very sparse, due to the fact
that only 15 concepts in the source taxonomy had a
partitive relationship and were syntactically related,
so we cannot draw any strong conclusions from it.
For the subsumption relationship we have a clearer
result and in fact averaged across all language pairs
we found that the likelihood of the syntax being pre-
served in the translation was nearly exactly the same
for semantically related and semantically unrelated
concepts. From this result we can conclude that
the probability of syntax given either subsumptive or
partitive relationship is not very large, at least from
the reduced syntactic model we used here. While
our model reduces syntax to n-gram overlap, we
believe that if there was a stronger correlation us-
ing a more sophisticated syntactic model, we would
still see some noticable effect here as we did mono-
lingually. We also note that we applied this to only
one taxonomy and it is possible that the result may
be different in a different resource. Furthermore,
we note there is a strong relationship between se-
mantics and syntax in a mono-lingual context and
as such adaption of a language model to incorporate
this bias may improve the translation of ontologies
and taxonomies.
5.3 Comparison of ontology structure
Our third intuition in approaching ontology trans-
lation is that the comparison of ontology or taxon-
omy structures containing source and target labels
may help in the disambiguation process of transla-
tion candidates. A prerequisite in this sense is the
availability of equivalent (or similar) ontology struc-
tures to be compared.
121
Figure 2: Two approaches to translate ontology labels.
From a technical point of view, we consider the
translation task as a word sense disambiguation task.
We identify two methods for comparing ontology
structures, which are illustrated in Figure 2.
The first method relies on a multilingual resource,
i.e., a multilingual ontology or taxonomy. The on-
tology represented on the left-hand side of the fig-
ure consists of several monolingual conceptualiza-
tions related to each other by means of an inter-
lingual index, as is the case in the EuroWordNet lex-
icon (Vossen, 1999). For example, if the original
label is chair for seat in English, several translations
for it are obtained in Spanish such as: silla (for seat),
ca?tedra (for university position), presidente (for per-
son leading a meeting). Each of these correspond
to a sense in the English WordNet, and hence each
translation selects a hierachical structure with En-
glish labels. The next step is to compare the input
structure of the original ontology containing chair
against the three different structures in English rep-
resenting the several senses of chair and obtain the
corresponding label in Spanish.
The second method relies on a monolingual re-
source, i.e., on monolingual ontologies in the tar-
get language, which means that we need to compare
structures documented with labels in different lan-
guages. As such we obtain a separate translated on-
tologies for each combination of label translations
suggested by the baseline system. Selecting the cor-
rect translations is then clearly a hard optimization
problem.
For the time being, we have only experimented
with the first approach using EuroWordNet. Sev-
eral solutions have been proposed in the context of
ontology matching in a monolingual scenario (see
(Shvaiko and Euzenat, 2005) or (Giunchiglia et al,
2006)). The ranking method we use to compare
structures relies on an equivalence probability mea-
sure between two candidate structures, as proposed
in (Trillo et al, 2007).
We assume that we have a taxonomy or ontology
entity o1 and we wish to deduce if it is similar to
another taxonomy or ontology entity o2 from a ref-
erence taxonomy or ontology (i.e., EuroWordNet) in
the same language. We shall make a simplifying as-
sumption that each ontology entity is associated with
a unique label, e.g., lo1 . As such we wish to deduce
if o1 represents the same concept as o2 and hence if
lo2 is a translation for lo1 . Our model relies on the
Vector Space Model (Raghavan and Wong, 1986)
to calculate the similarity between different labels,
which essentially involves calculating a vector from
the bag of words contained within each labels and
then calculating the cosine similarity between these
vectors. We shall denotes this as v(o1, o2). We then
use four main features in the calculation of the sim-
ilarity
? The VSM-similarity between the labels of enti-
ties, o1, o2.
? The VSM-similarity between any glosses (de-
scriptions) that may exist in the source or refer-
ence taxonomy/ontology.
? The hypernym similarity given to a fixed depth
d, given that set of hypernyms of an entity oi is
given as a set
hO(oi) = {h|(oi, h) ? H}
Then we calculate the similarity for d > 1 re-
cursively as
122
sh(o1, o2, d) =
?
h1?hO(o1),h2?hO(o2) ?(h1, h2, d)
|hO(o1)||hO(o2)|
?(h1, h2, d) = ?v(h1, h2)+(1??)sh(h1, h2, d?1)
And for d = 1 it is given as
sh(o1, o2, 1) =
?
h1?hO(o1),h2?hO(o2) v(h1, h2)
|hO(o1)||hO(o2)|
? The hyponym similarity, calculated as the hy-
pernym similarity but using the hyponym set
given by
HO(oi) = {h|(h, oi) ? H}
We then incorporate these factors into a vector x
and calculate the similarity of two entities as
s(o1, o2) = wTx
Where w is a weight vector of non-negative reals
and satisfies ||w|| = 1, which we set manually.
We then applied this to the FOAF ontol-
ogy (Brickley and Miller, 2010), which was manu-
ally translated to give us a reference translation. Af-
ter that, we collected a set of candidate translations
obtained by using the web translation resources ref-
erenced in section 3, along with additional candi-
dates found in our multilingual resource. Finally,
we used EuroWordNet (Vossen, 1999) as the refer-
ence taxonomy and ranked the translations accord-
ing to the score given by the metric above. In table
7, we present the results where our system selected
the candidate translation with the highest similarity
to our source ontology entity. In the case that we
could not find a reference translation we split the la-
bel into tokens and found the translation by select-
ing the best token. We compared these results to a
baseline method that selected one of the reference
translations at random.
These results are in all cases significantly stronger
than the baseline results showing that by compar-
ing the structure of ontology elements it is possible
to significantly improve the quality of translation.
These results are encouraging and we believe that
more research is needed in this sense. In particular,
we would like to investigate the benefits of perform-
ing a cross-lingual ontology alignment in which we
measure the semantic similarity of terms in different
languages.
Baseline Best Translation
WER? 0.725 0.617
METEOR 0.089 0.157
NIST 0.070 0.139
BLEU 0.103 0.187
Table 7: Results of selecting translation by structural
comparison. ?Lower WER scores are better
6 Conclusion
In this paper we presented the problem of ontology
and taxonomy translation as a special case of ma-
chine translation that has certain extra characteris-
tics. Our examination of the problem showed that
the main two differences are the presence of struc-
tured semantics and shorter, hence more ambiguous,
labels. We demonstrated that as a result of this lin-
guistic nature, some machine translation metrics do
not perform as well as they do in free-text trans-
lations. We then presented the results of early in-
vestigations into how we may use the special fea-
tures of taxonomy and ontology translation to im-
prove quality of translation. The first of these was
domain adaptation, which in line with other authors
is useful for texts in a particular domain. We also in-
vestigated the possibility of using the link between
syntactic similarity and semantic similarity to help,
however although we find that mono-lingually there
was a strong correspondence between syntax and se-
mantics, this result did not seem to extend well to a
cross-lingual setting. As such we believe there may
only be slight benefits of using techniques, however
further investigation is needed. Finally, we looked at
using word sense disambiguation by comparing the
structure of the input ontology to that of an already
translated reference ontology. We found this method
to be very effective in choosing the best translations.
However it is dependent on the existence of a mul-
tilingual resource that already has such terms. As
such, we view the topic of taxonomy and ontology
translation as an interesting sub-problem of machine
translation and believe there is still much fruitful
work to be done to obtain a system that can cor-
rectly leverage the semantics present in these data
structures in a way that improves translation quality.
123
References
Marianna Apidianaki. 2009. Data-driven semantic anal-
ysis for multilingual WSD and lexical selection in
translation. In Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics (EACL).
Michael Ashburner, Catherine Ball, Judith Blake, David
Botstein, Heather Butler, J. Michael Cherry, Allan
Davis, et al 2000. Gene ontology: tool for the uni-
fication of biology. The Gene Ontology Consortium.
Nature genetics, 25(1):25?29.
So?ren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives. 2007.
Dbpedia: A nucleus for a web of open data. The Se-
mantic Web, 4825:722?735.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. Intrinsic and Ex-
trinsic Evaluation Measures for Machine Translation
and/or Summarization, page 65.
Kalina Bontcheva. 2005. Generating tailored textual
summaries from ontologies. In The Semantic Web:
Research and Applications, pages 531?545. Springer.
Dan Brickley and Libby Miller, 2010. FOAF Vocabulary
Specification 0.98. Accessed 3 December 2010.
Marine Carpuat and Dekai Wu. 2007. Improving Sta-
tistical Machine Translation using Word Sense Disam-
biguation. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL 2007).
Philipp Cimiano, Elena Montiel-Ponsoda, Paul Buite-
laar, Mauricio Espinoza, and Asuncio?n Go?mez-Pe?rez.
2010. A note on ontology localization. Journal of Ap-
plied Ontology (JAO), 5:127?137.
Nigel Collier, Son Doan, Ai Kawazoe, Reiko Matsuda
Goodwin, Mike Conway, Yoshio Tateno, Quoc-Hung
Ngo, Dinh Dien, Asanee Kawtrakul, Koichi Takeuchi,
Mika Shigematsu, and Kiyosu Taniguchi. 2008. Bio-
Caster: detecting public health rumors with a Web-
based text mining system. Oxford Bioinformatics,
24(24):2940?2941.
Thierry Declerck, Hans-Ullrich Krieger, Susan Marie
Thomas, Paul Buitelaar, Sean O?Riain, Tobias Wun-
ner, Gilles Maguet, John McCrae, Dennis Spohr, and
Elena Montiel-Ponsoda. 2010. Ontology-based Mul-
tilingual Access to Financial Reports for Sharing Busi-
ness Knowledge across Europe. In Jo?zsef Roo?z and
Ja?nos Ivanyos, editors, Internal Financial Control As-
sessment Applying Multilingual Ontology Framework,
pages 67?76. HVG Press Kft.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, pages 138?145. Morgan Kaufmann Publish-
ers Inc.
Mauricio Espinoza, Asuncio?n Go?mez-Pe?rez, and Ed-
uardo Mena. 2008. Enriching an Ontology with
Multilingual Information. In Proceedings of the 5th
Annual of the European Semantic Web Conference
(ESWC08), pages 333?347.
Mauricio Espinoza, Elena Montiel-Ponsoda, and
Asuncio?n Go?mez-Pe?rez. 2009. Ontology Local-
ization. In Proceedings of the 5th International
Conference on Knowledge Capture (KCAP09), pages
33?40.
Bo Fu, Rob Brennan, and Declan O?Sullivan. 2010.
Cross-Lingual Ontology Mapping and Its Use on the
Multilingual Semantic Web. In Proceedings of the
1st Workshop on the Multilingual Semantic Web, at
the 19th International World Wide Web Conference
(WWW 2010).
Fausto Giunchiglia, Pavel Shvaiko, and Mikalai Yatske-
vich. 2006. Discovering missing background knowl-
edge in ontology matching. In Proceeding of the 17th
European Conference on Artificial Intelligence, pages
382?386.
International Accounting Standards Board, 2007. Inter-
national Financial Reporting Standards 2007 (includ-
ing International Accounting Standards (IAS) and In-
terpretations as at 1 January 2007).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, et al 2007. Moses: Open source toolkit for sta-
tistical machine translation. In Proceedings of the 45th
Annual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 177?180.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
Tenth Machine Translation Summit.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Iain McCowan, Darren Moore, John Dines, Daniel
Gatica-Perez, Mike Flynn, Pierre Wellner, and Herve?
Bourlard. 2004. On the use of information retrieval
measures for speech recognition evaluation. Technical
report, IDIAP.
John McCrae, Jesu?s Campana, and Philipp Cimiano.
2010. CLOVA: An Architecture for Cross-Language
Semantic Data Querying. In Proceedings of the First
Mutlilingual Semantic Web Workshop.
William Mischo. 1982. Library of Congress Subject
Headings. Cataloging & Classification Quarterly,
1(2):105?124.
124
Hans-Michael Mu?ller, Eimear E Kenny, and Paul W
Sternberg. 2004. Textpresso: An ontology-based in-
formation retrieval and extraction system for biologi-
cal literature. PLoS Biol, 2(11):e309.
Roberto Navigli and Simone Paolo Ponzetto. 2010. Ba-
belnet: Building a very large multilingual semantic
network. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 216?225.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th annual meeting on association for computational
linguistics, pages 311?318. Association for Computa-
tional Linguistics.
V.Vijay Raghavan and S.K.M. Wong. 1986. A criti-
cal analysis of vector space model for information re-
trieval. Journal of the American Society for Informa-
tion Science, 37(5):279?287.
Pavel Shvaiko and Jerome Euzenat. 2005. A survey of
schema-based matching approaches. Journal on Data
Semantics IV, pages 146?171.
Fabian Suchanek, Gjergji Kasneci, and Gerhard Weikum.
2007. Yago: a core of semantic knowledge. In Pro-
ceedings of the 16th international conference on World
Wide Web, pages 697?706.
Raquel Trillo, Jorge Gracia, Mauricio Espinoza, and Ed-
uardo Mena. 2007. Discovering the semantics of user
keywords. Journal of Universal Computer Science,
13(12):1908?1935.
Piek Vossen. 1999. EuroWordNet a multilingual
database with lexical semantic networks. Computa-
tional Linguistics, 25(4).
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In
Proceedings of the 22nd International Conference
on Computational Linguistics-Volume 1, pages 993?
1000. Association for Computational Linguistics.
125
Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 40?49,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Representing and resolving ambiguities
in ontology-based question answering
Christina Unger
Cognitive Interaction Technology ? Center of Excellence (CITEC),
Universit?t Bielefeld, Germany
{cunger|cimiano}@cit-ec.uni-bielefeld.de
Philipp Cimiano
Abstract
Ambiguities are ubiquitous in natural lan-
guage and pose a major challenge for the au-
tomatic interpretation of natural language ex-
pressions. In this paper we focus on differ-
ent types of lexical ambiguities that play a role
in the context of ontology-based question an-
swering, and explore strategies for capturing
and resolving them. We show that by employ-
ing underspecification techniques and by us-
ing ontological reasoning in order to filter out
inconsistent interpretations as early as possi-
ble, the overall number of interpretations can
be effectively reduced by 44 %.
1 Introduction
Ambiguities are ubiquitous in natural language.
They pose a key challenge for the automatic inter-
pretation of natural language expressions and have
been recognized as a central issue in question an-
swering (e.g. in (Burger et al, 2001)). In gen-
eral, ambiguities comprise all cases in which nat-
ural language expressions (simple or complex) can
have more than one meaning. These cases roughly
fall into two classes: They either concern structural
properties of an expression, e.g. different parses due
to alternative preposition or modifier attachments
and different quantifier scopings, or they concern al-
ternative meanings of lexical items. It is these lat-
ter ambiguities, ambiguities with respect to lexical
meaning, that we are interested in. More specifi-
cally, we will look at ambiguities in the context of
ontology-based interpretation of natural language.
The meaning of a natural language expression in
the context of ontology-based interpretation is the
ontology concept that this expression verbalizes. For
example, the expression city can refer to a class
geo:city (where geo is the namespace of the corre-
sponding ontology), and the expression inhabitants
can refer to a property geo:population. The cor-
respondence between natural language expressions
and ontology concepts need not be one-to-one. On
the one hand side, different natural language expres-
sions can refer to a single ontology concept, e.g.
flows through, crosses through and traverses could
be three ways of expressing an ontological property
geo:flowsThrough. On the other hand, one natu-
ral language expression can refer to different ontol-
ogy concepts. For example, the verb has is vague
with respect to the relation it expresses ? it could
map to geo:flowsThrough (in the case of rivers)
as well as geo:inState (in the case of cities). Such
mismatches between the linguistic meaning of an
expression, i.e. the user?s conceptual model, and the
conceptual model in the ontology give rise to a num-
ber of ambiguities. We will give a detailed overview
of those ambiguities in Section 3, after introducing
preliminaries in Section 2.
For a question answering system, there are mainly
two ways to resolve ambiguities: by interactive clar-
ification and by means of background knowledge
and the context with respect to which a question is
asked and answered. The former is, for example,
pursued by the question answering system FREyA
(Damljanovic et al, 2010). The latter is incorporated
in some recent work in machine learning. For exam-
ple, (Kate & Mooney, 2007) investigate the task of
40
learning a semantic parser from a corpus whith sen-
tences annotated with multiple, alternative interpre-
tations, and (Zettlemoyer & Collins, 2009) explore
an unsupervised algorithm for learning mappings
from natural language sentences to logical forms,
with context accounted for by hidden variables in a
perceptron.
In ontology-based question answering, context as
well as domain knowledge is provided by the ontol-
ogy. In this paper we explore how a given ontology
can be exploited for ambiguity resolution. We will
consider two strategies in Section 4. The first one
consists in simply enumerating all possible interpre-
tations. Since this is not efficient (and maybe not
even feasible), we will use underspecification tech-
niques for representing ambiguities in a much more
compact way and then present a strategy for resolv-
ing ambiguities by means of ontological reasoning,
so that the number of interpretations that have to be
considered in the end is relatively small and does not
comprise inconsistent and therefore undesired inter-
pretations. We will summarize with quantitative re-
sults in Section 5.
2 Preliminaries
All examples throughout the paper will be based on
Raymond Mooney?s GeoBase1 dataset and the DB-
pedia question set published in the context of the
1st Workshop on Question Answering Over Linked
Data (QALD-1)2. The former is a relatively small
and well-organized domain, while the latter is con-
siderably larger and much more heterogenous. It is
interesting to note that ontological ambiguituies turn
out to be very wide-spread even in a small and ho-
mogenuous domain like GeoBase (see Section 3 for
specific results).
For specifying entries of a grammar that a ques-
tion answering system might work with, we will use
the general and principled linguistic representations
that our question answering system Pythia3 (Unger
et al, 2010) relies on, as they are suitable for dealing
with a wide range of natural language phenomena.
Syntactic representations will be trees from Lexi-
calized Tree Adjoining Grammar (LTAG (Schabes,
1cs.utexas.edu/users/ml/nldata/geoquery.html
2http://www.sc.cit-ec.uni-bielefeld.de/qald-1
3http://www.sc.cit-ec.uni-bielefeld.de/pythia
1990)). The syntactic representation of a lexical
item is a tree constituting an extended projection of
that item, spanning all of its syntactic and semantic
arguments. Argument slots are nodes marked with a
down arrow (?), for which trees with the same root
category can be substituted. For example, the tree
for a transitive verb like borders looks as follows:
1. S
DP1 ? VP
V
borders
DP2 ?
The domain of the verb thus spans a whole sentence,
containing its two nominal arguments ? one in sub-
ject position and one in object position. The corre-
sponding nodes, DP1 and DP2, are slots for which
any DP-tree can be substituted. For example, substi-
tuting the two trees in 2 for subject and object DP,
respectively, yields the tree in 3.
2. (a) DP
DET
no
NP
state
(b) DP
Hawaii
3. S
DP
DET
no
NP
state
VP
V
borders
DP
Hawaii
As semantic representations we take DUDEs
(Cimiano, 2009), representations similar to struc-
tures from Underspecified Discourse Representation
Theory (UDRT (Reyle, 1993)), extended with some
additional information that allows for flexible mean-
ing composition in parallel to the construction of
LTAG trees. The DUDE for the verb to border, for
example, would be the following (in a slightly sim-
plified version):
geo:borders (x, y)
(DP1, x), (DP2, y)
41
It provides the predicate geo:borders correspond-
ing to the intended concept in the ontology. This
correspondence is ensured by using the vocabulary
of the ontology, i.e. by using the URI4 of the con-
cept instead of a more generic predicate. The pre-
fix geo specifies the namespace, in this case the one
of the GeoBase ontology. Furthermore, the seman-
tic representation contains information about which
substitution nodes in the syntactic structure provide
the semantic arguments x and y. That is, the seman-
tic referent provided by the meaning of the tree sub-
stituted for DP1 corresponds to the first argument x
of the semantic predicate, while the semantic refer-
ent provided by the meaning of the tree substituted
for DP2 corresponds to the second argument y. The
uppermost row of the box contains the referent that
is introduced by the expression. For example, the
DUDE for Hawaii (paired with the tree in 2b) would
be the following:
h
geo:name (h,?hawaii?)
It introduces a referent h which is related to the lit-
eral ?hawaii? by means of the relation geo:name.
As it does not have any arguments, the third row
is empty. The bottom-most row, empty in both
DUDEs, is for selectional restrictions of predicates;
we will see those in Section 4.
Parallel to substituting the DP-tree in 2b for the
DP1-slot in 1, the DUDE for Hawaii is combined
with the DUDE for borders, amounting to the satu-
ration of the argument (DP2, y) by unifying the vari-
ables h and y, yielding the following DUDE:
h
geo:borders (x, h)
geo:name (h,?hawaii?)
(DP1, x)
Substituting the subject argument no state involves
quantifier representations which we will gloss over
as they do not play a role in this paper. At this point
4URI stands for Uniform Resource Identifier. URIs uniquely
identify resources on the Web. For an overview, see, e.g.,
http://www.w3.org/Addressing/.
it suffices to say that we implement the treatment of
quantifier scope in UDRT without modifications.
Once a meaning representation for a question is
built, it is translated into a SPARQL query, which
can then be evaluated with respect to a given dataset.
Not a lot hinges on the exact choice of the for-
malisms; we could as well have chosen any other
syntactic and semantic formalism that allows the in-
corporation of underspecification mechanisms. The
same holds for the use of SPARQL as formal query
language. The reason for choosing SPARQL is that
it is the standard query language for the Seman-
tic Web5; we therefore feel safe in relying on the
reader?s familiarity with SPARQL and use SPARQL
queries without further explanation.
3 Types of ambiguities
As described in the introduction above, a central task
in ontology-based interpretation is the mapping of
a natural language expression to an ontology con-
cept. And this mapping gives rise to several different
cases of ambiguities.
First, ambiguities can arise due to homonymy of a
natural language expression, i.e. an expression that
has several lexical meanings, where each of these
meanings can be mapped to one ontology concept
unambiguously. The ambiguity is inherent to the ex-
pression and is independent of any domain or ontol-
ogy. This is what in linguistic contexts is called a
lexical ambiguity. A classical example is the noun
bank, which can mean a financial institution, a kind
of seating, the edge of a river, and a range of other
disjoint, non-overlapping alternatives. An example
in the geographical domain is New York. It can mean
either New York city, in this case it would be mapped
to the ontological entity geo:new york city, or
New York state, in this case it would be mapped to
the entity geo:new york. Ambiguous names are ac-
tually the only case of such ambiguities that occur in
the GeoBase dataset.
Another kind of ambiguities is due to mismatches
between a user?s concept of the meaning of an
expression and the modelling of this meaning
in the ontology. For example, if the ontology
modelling is more fine-grained than the meaning
5For the W3C reference, see
http://www.w3.org/TR/rdf-sparql-query/.
42
of a natural language expression, then an expres-
sion with one meaning can be mapped to several
ontology concepts. These concepts could differ
extensionally as well as intensionally. An example
is the above mentioned expression starring, that
an ontology engineer could want to comprise only
leading roles or also include supporting roles. If
he decides to model this distinction and introduces
two properties, then the ontological model is
more fine-grained than the meaning of the natural
language expression, which could be seen as corre-
sponding to the union of both ontology properties.
Another example is the expression inhabitants
in question 4, which can be mapped either to
<http://dbpedia.org/property/population>
or to <http://dbpedia.org/ontology/popula-
tionUrban>. For most cities, both alternatives give
a result, but they differ slightly, as one captures only
the core urban area while the other also includes
the outskirts. For some city, even only one of them
might be specified in the dataset.
4. Which cities have more than two million inhab-
itants?
Such ambiguities occur in larger datasets like DB-
pedia with a wide range of common nouns and tran-
sitive verbs. In the QALD-1 training questions for
DBpedia, for example, at least 16 % of the questions
contain expressions that do not have a unique onto-
logical correspondent.
Another source for ambiguities is the large num-
ber of vague and context-dependent expressions in
natural language. While it is not possible to pin-
point such expressions to a fully specified lexical
meaning, a question answering system needs to map
them to one (or more) specific concept(s) in the on-
tology. Often there are several mapping possibili-
ties, sometimes depending on the linguistic context
of the expression.
An example for context-dependent expressions
in the geographical domain is the adjective big: it
refers to size (of a city or a state) either with respect
to population or with respect to area. For the ques-
tion 5a, for example, two queries could be intended
? one refering to population and one refering to area.
They are given in 5b and 5c.
5. (a) What is the biggest city?
(b) SELECT ?s WHERE {
?s a geo:city .
?s geo:population ?p . }
ORDER BY DESC ?p LIMIT 1
(c) SELECT ?s WHERE {
?s a geo:city .
?s geo:area ?a . }
ORDER BY DESC ?a LIMIT 1
Without further clarification ? either by means of
a clarification dialog with the user (e.g. employed
by FREyA (Damljanovic et al, 2010)) or an ex-
plicit disambiguation as in What is the biggest city
by area? ? both interpretations are possible and ade-
quate. That is, the adjective big introduces two map-
ping alternatives that both lead to a consistent inter-
pretation.
A slightly different example are vague expres-
sions. Consider the questions 6a and 7a. The
verb has refers either to the object property
flowsThrough, when relating states and rivers, or
to the object property inState, when relating states
and cities. The corresponding queries are given in
6b and 7b.
6. (a) Which state has the most rivers?
(b) SELECT COUNT(?s) AS ?n WHERE {
?s a geo:state .
?r a geo:river .
?r geo:flowsThrough ?s. }
ORDER BY DESC ?n LIMIT 1
7. (a) Which state has the most cities?
(b) SELECT COUNT(?s) AS ?n WHERE {
?s a geo:state .
?c a geo:city .
?c geo:inState ?s. }
ORDER BY DESC ?n LIMIT 1
In contrast to the example of big above, these two
interpretations, flowsThrough and inState, are
exclusive alternatives: only one of them is admis-
sible, depending on the linguistic context. This
is due to the sortal restrictions of those proper-
ties: flowsThrough only allows rivers as domain,
whereas inState only allows cities as domain.
This kind of ambiguities are very frequent, as a
lot of user questions contain semantically light ex-
pressions, e.g. the copula verb be, the verb have,
43
and prepositions like of, in and with (cf. (Cimiano
& Minock, 2009)) ? expressions which are vague
and do not specify the exact relation they are de-
noting. In the 880 user questions that Mooney pro-
vides, there are 1278 occurences of the light expres-
sions is/are, has/have, with, in, and of, in addition
to 151 ocurrences of the context-dependent expres-
sions big, small, and major.
4 Capturing and resolving ambiguities
When constructing a semantic representation and
a formal query, all possible alternative meanings
have to be considered. We will look at two strate-
gies to do so: simply enumerating all interpretations
(constructing a different semantic representation and
query for every possible interpretation), and under-
specification (constructing only one underspecified
representation that subsumes all different interpreta-
tions).
4.1 Enumeration
Consider the example of a lexically ambiguous
question in 8a. It contains two ambiguous expres-
sions: New York can refer either to the city or the
state, and big can refer to size either with respect
to area or with respect to population. This leads to
four possible interpretations of the questions, given
in 8b?8e.
8. (a) How big is New York?
(b) SELECT ?a WHERE {
geo:new york city geo:area ?a . }
(c) SELECT ?p WHERE {
geo:new york city geo:population ?p.}
(d) SELECT ?a WHERE {
geo:new york geo:area ?a . }
(e) SELECT ?p WHERE {
geo:new york geo:population ?p . }
Since the question in 8a can indeed have all four in-
terpretations, all of them should be captured. The
enumeration strategy amounts to constructing all
four queries. In order to do so, we specify two
lexical entries for New York and two lexical en-
tries for the adjective big ? one for each reading.
For big, these two entries are given in 9 and 10.
The syntactic tree is the same for both, while the
semantic representations differ: one refers to the
property geo:area and one refers to the property
geo:population.
9. N
ADJ
big
N?
a
geo:area (x, a)
(N, x)
10. N
ADJ
big
N?
p
geo:population (x, p)
(N, x)
When parsing the question How big is New York,
both entries for big are found during lexical lookup,
and analogously two entries for New York are found.
The interpretation process will use all of them and
therefore construct four queries, 8b?8e.
Vague and context-dependent expressions can be
treated similarly. The verb to have, for example,
can map either to the property flowsThrough, in
the case of rivers, or to the property inState, in
the case of cities. Now we could simply spec-
ify two lexical entries to have ? one using the
meaning flowsThrough and one using the mean-
ing inState. However, contrary to lexical ambigu-
ities, these are not real alternatives in the sense that
both lead to consistent readings. The former is only
possible if the relevant argument is a river, the lat-
ter is only relevant if the relevant argument is a city.
So in order not to derive inconsistent interpretations,
we need to capture the sortal restrictions attached to
such exclusive alternatives. This will be discussed
in the next section.
4.2 Adding sortal restrictions
A straightforward way to capture ambiguities con-
sists in enumerating all possible interpretations
and thus in constructing all corresponding formal
queries. We did this by specifying a separate lex-
ical entry for every interpretation. The only diffi-
culty that arises is that we have to capture the sor-
tal restrictions that come with some natural language
expressions. In order to do so, we add sortal restric-
tions to our semantic representation format.
Sortal restrictions will be of the general form
variable?class. For example, the sortal restriction
that instances of the variable x must belong to the
class river in our domain would be represented as
x?geo:river. Such sortal restrictions are added as
44
a list to our DUDEs. For example, for the verb has
we specify two lexical entries. One maps has to
the property flowThrough, specifying the sortal re-
striction that the first argument of this property must
belong to the class river. This entry looks as fol-
lows:
S
DP1 ? VP
V
has
DP2 ?
geo:flowsThrough (y, x)
(DP1, x), (DP2, y)
x?geo:river
The other lexical entry for has consists of the
same syntactic tree and a semantic representation
that maps has to the property inState and contains
the restriction that the first argument of this property
must belong to the class city. It looks as follows:
S
DP1 ? VP
V
has
DP2 ?
geo:inState (y, x)
(DP1, x), (DP2, y)
x?geo:city
When a question containg the verb has, like 11a,
is parsed, both interpretations for has are found dur-
ing lexical lookup and two semantic representations
are constructed, both containing a sortal restriction.
When translating the semantic representations into a
formal query, the sortal restriction is simply added as
a condition. For 11a, the two corresponding queries
are given in 11b (mapping has to flowsThrough)
and 11c (mapping has inState). The contribution
of the sortal restriction is boxed.
11. (a) Which state has the most rivers?
(b) SELECT COUNT(?r) as ?c WHERE {
?s a geo:state .
?r a geo:river .
?r geo:flowsThrough ?s .
?r a geo:river . }
ORDER BY ?c DESC LIMIT 1
(c) SELECT COUNT(?r) as ?c WHERE {
?s a geo:state .
?r a geo:river .
?r geo:inState ?s .
?r a geo:city . }
ORDER BY ?c DESC LIMIT 1
In the first case, 11b, the sortal restriction adds a
redundant condition and will have no effect. We can
say that the sortal restriction is satisfied. In the sec-
ond case, in 11c, however, the sortal restriction adds
a condition that is inconsistent with the other condi-
tions, assuming that the classes river and city are
properly specified as disjoint. The query will there-
fore not yield any results, as no instantiiation of r
can be found that belongs to both classes. That is,
in the context of rivers only the interpretation using
flowsThrough leads to results.
Actually, the sortal restriction in 11c is al-
ready implicitly specified in the ontological relation
inState: there is no river that is related to a state
with this property. However, this is not necessar-
ily the case and there are indeed queries where the
sortal restriction has to be included explicitly. One
example is the interpretation of the adjective major
in noun phrases like major city and major state. Al-
though with respect to the geographical domain ma-
jor always expresses the property of having a pop-
ulation greater than a certain threshold, this thresh-
old differs for cities and states: major with respect
to cities is interpreted as having a population greater
than, say, 150 000, while major with respect to states
is interpreted as having a population greater than,
say, 10 000 000. Treating major as ambiguous be-
tween those two readings without specifying a sortal
restriction would lead to two readings for the noun
phrase major city, sketched in 12. Both would yield
non-empty results and there is no way to tell which
one is the correct one.
12. (a) SELECT ?c WHERE {
?c a geo:city .
?c geo:population ?p .
FILTER ( ?p > 150000 ) }
(b) SELECT ?c WHERE {
?c a geo:city .
?c geo:population ?p .
FILTER ( ?p > 10000000 ) }
Specifying sortal restrictions, on the other hand,
would add the boxed material in 13, thereby caus-
ing the wrong reading in 13b to return no results.
13. (a) SELECT ?c WHERE {
?c a geo:city .
?c geo:population ?p .
45
FILTER ( ?p > 150000 ) .
?c a geo:city . }
(b) SELECT ?c WHERE {
?c a geo:city .
?c geo:population ?p .
FILTER ( ?p > 10000000 ) .
?c a geo:state . }
The enumeration strategy thus relies on a conflict
that results in queries which return no result. Un-
wanted interpretations are thereby filtered out auto-
matically. But two problems arise here. The first
one is that we have no way to distinguish between
queries that return no result due to an inconsistency
introduced by a sortal restriction, and queries that
return no result, because there is none, as in the case
of Which states border Hawaii?. The second prob-
lem concerns the number of readings that are con-
structed. In view of the large number of ambiguities,
even in the restricted geographical domain we used,
user questions easily lead to 20 or 30 different pos-
sible interpretations. In cases in which several natu-
ral language terms can be mapped to many different
ontological concepts, this number rises. Enumerat-
ing all alternative interpretations is therefore not ef-
ficient. A more practical alternative is to construct
one underspecified representation instead and then
infer a specific interpretation in a given context. We
will explore this strategy in the next section.
4.3 Underspecification
In the following, we will explore a strategy for rep-
resenting and resolving ambiguities that uses under-
specification and ontological reasoning in order to
keep the number of constructed interpretations to a
minimum. For a general overview of underspecifica-
tion formalisms and their applicability to linguistic
phenomena see (Bunt, 2007).
In order not to construct a different query for
every interpretation, we do not any longer specify
separate lexical entries for each mapping but rather
combine them by using an underspecified semantic
representation. In the case of has, for example, we
do not specify two lexical entries ? one with a se-
mantic representation using flowsThrough and one
entry with a representation using inState ? but in-
stead specify only one lexical entry with a represen-
tation using a metavariable, and additionally specify
which properties this metavariable stands for under
which conditions.
So first we extend DUDEs such that they now can
contain metavariables, and instead of a list of sor-
tal restrictions contain a list of metavariable speci-
fications, i.e. possible instantiations of a metavari-
able given that certain sortal restrictions are satis-
fied, where sortal restrictions can concern any of the
property?s arguments. Metavariable specifications
take the following general form:
P ? p1 (x = class1, . . . , y = class2)
| p2 (x = class3, . . . , y = class4)
| . . .
| pn (x = classi, . . . , y = classj)
This expresses that some metavariable P stands for
a property p1 if the types of the arguments x, . . . , y
are equal to or a subset of class1,. . . ,class2, and
stands for some other property if the types of the
arguments correspond to some other classes. For
example, as interpretation of has, we would chose
a metavariable P with a specification stating that P
stands for the property flowsThrough if the first ar-
gument belongs to class river, and stands for the
property inState if the first argument belongs to
the class city. Thus, the lexical entry for has would
contain the following underspecified semantic repre-
sentation.
14. Lexical meaning of ?has?:
P (y, x)
(DP1, x), (DP2, y)
P ? geo:flowsThrough (y = geo:river)
| geo:inState (y = geo:city)
Now this underspecified semantic representation has
to be specified in order to lead to a SPARQL query
that can be evaluated w.r.t. the knowledge base.
That means, in the course of interpretation we need
to determine which class an instantiation of y be-
longs to and accordingly substitute P by the prop-
erty flowsThrough or inState. In the following
section, we sketch a way of exploiting the ontology
to this end.
46
4.4 Reducing alternatives with ontological
reasoning
In order to filter out interpretations that are inconsis-
tent as early as possible and thereby reduce the num-
ber of interpretations during the course of a deriva-
tion, we check whether the type information of a
variable that is unified is consistent with the sor-
tal restrictions connected to the metavariables. This
check is performed at every relevant step in a deriva-
tion, so that inconsistent readings are not allowed to
percolate and multiply. Let us demonstrate this strat-
egy by means of the example Which state has the
biggest city?.
In order to build the noun phrase the biggest
city, the meaning representation of the superlative
biggest, given in 15, is combined with that of the
noun city, which simply contributes the predication
geo:city (y), by means of unification.
15.
z
Q (y, z)
(N, y)
Q ? geo:area(y = geo:city unionsq geo:state)
| geo:population(y = geo:city unionsq geo:state)
The exact details of combining meaning represen-
tations do not matter here. What we want to fo-
cus on is the metavariable Q that biggest introduces.
When combining 15 with the meaning of city, we
can check whether the type information connected
to the unified referent y is compatible with the do-
main restrictions of Q?s interpretations. One way
to do this is by integrating an OWL reasoner and
checking the satisfiability of
geo:city u (geo:city unionsq geo:state)
(for both interpretations of Q, as the restrictions on
y are the same). Since this is indeed satisfiable,
both interpretations are possible, thus cannot be dis-
carded, and the resulting meaning representation of
the biggest city is the following:
y z
geo:city(y)
Q (y, z)
max(z)
Q ? geo:area(y = geo:city unionsq geo:state)
| geo:population(y = geo:city unionsq geo:state)
This is desired, as the ambiguity of biggest is a lexi-
cal ambiguity that could only be resolved by the user
specifying which reading s/he intended.
In a next step, the above representation is com-
bined with the semantic representation of the verb
has, given in 14. Now the type information of the
unified variable y has to be checked for compati-
bility with instantiations of an additional metavari-
able, P . The OWL reasoner would therefore have to
check the satisfiability of the following two expres-
sions:
16. (a) geo:city u geo:river
(b) geo:city u geo:city
While 16b succeeds trivially, 16a fails, assuming
that the two classes geo:river and geo:city are
specified as disjoint in the ontology. Therefore
the instantiation of P as geo:flowsThrough is
not consistent and can be discarded, leading to the
following combined meaning representation, where
P is replaced by its only remaining instantiation
geo:inState:
y z
geo:city(y)
geo:inState (y, x)
Q (y, z)
(DP1, x)
Q ? geo:area(y = geo:city unionsq geo:state)
| geo:population(y = geo:city unionsq geo:state)
Finally, this meaning representation is com-
bined with the meaning representation of which
state, which simply contributes the predication
geo:state (x). As the unified variable x does not
occur in any metavariable specification, nothing fur-
ther needs to be checked. The final meaning repre-
sentation thus leaves one metavariable with two pos-
sible instantiations and will lead to the following two
corresponding SPARQL queries:
47
17. (a) SELECT ?x WHERE {
?x a geo:city .
?y a geo:state.
?x geo:population ?z .
?x geo:inState ?y . }
ORDER BY DESC(?z) LIMIT 1
(b) SELECT ?x WHERE {
?x a geo:city .
?y a geo:state.
?x geo:area ?z .
?x geo:inState ?y . }
ORDER BY DESC(?z) LIMIT 1
Note that if the ambiguity of the metavariable
P were not resolved, we would have ended up
with four SPARQL queries, where two of them use
the relation geo:flowsThrough and therefore yield
empty results. So in this case, we reduced the num-
ber of constructed queries by half by discarding in-
consistent readings. We therefore solved the prob-
lems mentioned at the end of 4.2: The number of
constructed queries is reduced, and since we discard
inconsistent readings, null answers can only be due
to the lack of data in the knowledge base but not can-
not anymore be due to inconsistencies in the gener-
ated queries.
5 Implementation and results
In order to see that the possibility of reducing the
number of interpretations during a derivation does
not only exist in a small number of cases, we ap-
plied Pythia to Mooney?s 880 user questions, imple-
menting the underspecification strategy in 4.3 and
the reduction strategy in 4.4. Since Pythia does not
yet integrate a reasoner, it approximates satisfiabil-
ity checks by means of SPARQL queries. When-
ever meaning representations are combined, it ag-
gregates type information for the unified variable,
together with selectional information connected to
the occuring metavariables, and uses both to con-
struct a SPARQL query. This query is then evalu-
ated against the underlying knowledge base. If the
query returns results, the interpetations are taken to
be compatible, if it does not return results, the in-
terpretations are taken to be incompatible and the
according instantiation possibility of the metavari-
able is discarded. Note that those SPARQL queries
are only an approximation for the OWL expressions
used in 4.4. Furthermore, the results they return are
only an approximation of satisfiability, as the reason
for not returning results does not necessarily need to
be unsatisfiability of the construction but could also
be due the absence of data in the knowledge base.
In order to overcome these shortcomings, we plan to
integrate a full-fledged OWL reasoner in the future.
Out of the 880 user questions, 624 can be parsed
by Pythia (for an evaluation on this dataset and rea-
sons for failing with the remaining 256 questions,
see (Unger & Cimiano, 2011)). Implementing the
enumeration strategy, i.e. not using disambiguation
mechanisms, there was a total of 3180 constructed
queries. With a mechanism for removing scope am-
biguities by means of simulating a linear scope pref-
erence, a total of 2936 queries was built. Addi-
tionally using the underspecification and resolution
strategies described in the previous section, by ex-
ploiting the ontology with respect to which natural
language expressions are interpreted in order to dis-
card inconsistent interpretations as early as possible
in the course of a derivation, the number of total
queries was further reduced to 2100. This amounts
to a reduction of the overall number of queries by
44 %. The average and maximum number of queries
per question are summarized in the following table.
Avg. # queries Max. # queries
Enumeration 5.1 96
Linear scope 4.7 (-8%) 46 (-52%)
Reasoning 3.4 (-44%) 24 (-75%)
6 Conclusion
We investigated ambiguities arising from mis-
matches between a natural language expressions?
lexical meaning and its conceptual modelling in an
ontology. Employing ontological reasoning for dis-
ambiguation allowed us to significantly reduce the
number of constructed interpretations: the average
number of constructed queries per question can be
reduced by 44 %, the maximum number of queries
per question can be reduced even by 75 %.
48
References
Bunt, H.: Semantic Underspecification: Which Tech-
nique For What Purpose? In: Computing Meaning,
vol. 83, pp. 55?85. Springer Netherlands (2007)
Cimiano, P.: Flexible semantic composition with
DUDES. In: Proceedings of the 8th International Con-
ference on Computational Semantics (IWCS). Tilburg
(2009)
Unger, C., Hieber, F., Cimiano, P.: Generating LTAG
grammars from a lexicon-ontology interface. In: S.
Bangalore, R. Frank, and M. Romero (eds.): 10th In-
ternational Workshop on Tree Adjoining Grammars
and Related Formalisms (TAG+10), Yale University
(2010)
Unger, C., Cimiano, P.: Pythia: Compositional mean-
ing construction for ontology-based question answer-
ing on the Semantic Web. In: Proceedings of the 16th
International Conference on Applications of Natural
Language to Information Systems (NLDB) (2011)
Schabes, Y.: Mathematical and Computational Aspects
of Lexicalized Grammars. Ph. D. thesis, University of
Pennsylvania (1990)
Reyle, U.: Dealing with ambiguities by underspecifica-
tion: Construction, representation and deduction. Jour-
nal of Semantics 10, 123?179 (1993)
Kamp, H., Reyle, U.: From Discourse to Logic. Kluwer,
Dordrecht (1993)
Cimiano, P., Minock, M.: Natural Language Interfaces:
What?s the Problem? ? A Data-driven Quantitative
Analysis. In: Proceedings of the International Confer-
ence on Applications of Natural Language to Informa-
tion Systems (NLDB), pp. 192?206 (2009)
Damljanovic, D., Agatonovic, M., Cunningham, H.:
Natural Language Interfaces to Ontologies: Combin-
ing Syntactic Analysis and Ontology-based Lookup
through the User Interaction. In: Proceedings of the
7th Extended Semantic Web Conference, Springer
Verlag (2010)
Zettlemoyer, L., Collins, M.: Learning Context-
dependent Mappings from Sentences to Logical Form.
In: Proceedings of the Joint Conference of the As-
sociation for Computational Linguistics and Interna-
tional Joint Conference on Natural Language Process-
ing (ACL-IJCNLP), pp. 976?984 (2009)
Burger, J., Cardie, C., Chaudhri, V., Gaizauskas,
R., Israel, D., Jacquemin, C., Lin, C.-Y., Maio-
rano, S., Miller, G., Moldovan, D., Ogden,
B., Prager, J., Riloff, E., Singhal, A., Shrihari,
R., Strzalkowski, T., Voorhees, E., Weischedel,
R.: Issues, tasks, and program structures to
roadmap research in question & answering (Q & A).
http://www-nlpir.nist.gov/projects/duc/
papers/qa.Roadmap-paper v2.doc (2001)
Kate, R., Mooney, R.: Learning Language Semantics
from Ambiguous Supervision. In: Proceedings of the
22nd Conference on Artificial Intelligence (AAAI-07),
pp. 895?900 (2007)
49
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 1?2,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Up from Limited Dialog Systems!
Giuseppe Riccardi
University of Trento
via Sommarive, 14
38050, Trento, Italy
riccardi@disi.unitn.it
Philipp Cimiano
Bielefeld University
Universita?tsstra?e 21?23
33615, Bielefeld, Germany
cimiano@cit-ec.uni-bielefeld.de
Alexandros Potamianos
Technical University of Crete
73100, Chania
Crete, Greece
potam@telecom.tuc.gr
Christina Unger
Bielefeld University
Universita?tsstra?e 21?23
33615, Bielefeld, Germany
cunger@cit-ec.uni-bielefeld.de
Abstract
In the last two decades, information-seeking
spoken dialog systems (SDS) have moved
from research prototypes to real-life commer-
cial applications. Still, dialog systems are lim-
ited by the scale, complexity of the task and
coverage of knowledge required by problem-
solving machines or mobile personal assis-
tants. Future spoken interaction are required
to be multilingual, understand and act on large
scale knowledge bases in all its forms (from
structured to unstructured). The Web re-
search community have striven to build large
scale and open multilingual resources (e.g.
Wikipedia) and knowledge bases (e.g. Yago).
We argue that a) it is crucial to leverage
this massive amount of Web lightly structured
knowledge and b) the scale issue can be ad-
dressed collaboratively and design open stan-
dards to make tools and resources available to
the whole speech and language community.
1 Introduction
In the last two decades, interactive spoken dialog
systems (SDS) have moved from research proto-
types to real-life commercial applications (Tur and
De Mori, 2011). Generally, SDS are built for a
specific task (e.g. call routing) with ad-hoc lim-
ited knowledge base and for a predefined target lan-
guage. However, one major limitation in commer-
cial SDS prototyping is that they are not easily and
quickly extensible and portable to new domains or
languages. Such porting requirements range from
defining (or extending) a domain ontology to hand-
crafting a new grammar or training stochastic mod-
els for speech recognition and understanding. These
are the research and engineering goals motivating
the PortDial project whose objectives include the
engagement of the whole technical community. In
the PortDial project we would like to engage re-
searchers in building resources that may be gener-
ated via top-down processes (grammars), bottom-up
processes (statistical models) or via a fusion of both.
In this position paper we want to address the crit-
ical limitations of SDS systems: a) poor ability to
cover the knowledge space and its interface to the
SDS components (speech recognition, language un-
derstanding and dialog manager) and b) collabora-
tively design open standards to make tools and re-
sources available to the whole speech and language
community.
2 Exploiting top-down knowledge
There are at least three main kinds of structured
knowledge sources that SDS modules may ex-
ploit: ontologies, grammars, and lexica. Ontolo-
gies explicitly model background knowledge about
a certain domain. In the last years, many free
and open collaboratively created resources have
emerged, including large multi-lingual corpora such
as Wikipedia, and broad-coverage ontologies, e.g.
as part of the Linked Data Cloud (Bizer et al, 2009),
either created manually or extracted automatically
from existing data (such as DBpedia or Yago). How-
ever, while also lexica such as Wiktionary are avail-
able today on the Web, ontologies typically lack in-
formation about linguistic realization. For this rea-
son, ontologies available on the Web are not di-
rectly exploitable by dialog systems. Linguistic in-
1
formation is commonly captured in grammars, that
are either hand-crafted or created by means of ma-
chine learning techniques. In order to be able to
generate high-quality grammars with as little man-
ual effort as possible, we aim at (semi) automat-
ing the knowledge-based generation of lexica and
grammars. To achieve this, it is crucial to lever-
age Web resources for enriching ontologies with
lexical and linguistic information, i.e. informa-
tion about how ontological concepts are lexicalized
in different languages, capturing in particular lex-
ical and syntactic variation (Unger et al, 2010).
This knowledge-centered grammar generation pro-
cess may be merged with methods for automatically
inferring structure from lightly annotated corpus, in-
cluding data harvested from the Web, in a bottom-
up fashion (Tur and De Mori, 2011). For a dialog
system to be able to exploit ontologies, lexica and
grammars, these three resources need to be tightly
aligned, i.e. they need to share domain-relevant vo-
cabulary. For this alignment, we propose to build on
Semantic Web standards, mainly in order to support
the incorporation of already existing data, to share
resources for SDS engineering, and facilitate collab-
orative knowledge engineering. From a larger per-
spective, such an approach has the potential of cre-
ating SDS resources (ontologies, lexica and gram-
mars) that are strongly aligned with each other as
well as with other resources available on the Web,
thus fostering the creation of an eco-system of linked
resources that can be reused to facilitate the process
of engineering and porting a dialog system to new
domains and languages.
3 Collaboratively building and sharing
knowledge
Today the lack of reusable linguistic resources and
annotated data hinders the rapid development of
spoken dialog systems in industry and academia
alike. Despite progress in standardization of the
format of SDS grammars and semantic represen-
tations, the data proper has to be hand-crafted for
new applications and languages with little or no au-
tomation available. We argue above that language
engineering technology is now mature to help cre-
ate such linguistic resources automatically or semi-
automatically using data that is either harvested
from the web or via community crowdsourcing us-
ing the ?collective wisdom of expert crowds?. Al-
though providing linguistic resources and tools for
cost-effective SDS development is important and
relevant, a data pool that is not openly sharable and
continuously enriched fails its purpose. It is thus im-
portant to guarantee the sustainability of the linguis-
tic SDS resources engineered via a community that
both uses and actively develops the data pool. To-
wards this end, we envision both a free and premium
data exchange targeting non-commercial users that
can maintain and enrich the free version of the data
pool, and commercial speech services developers
that can contribute to the premium data pool via
an electronic marketplace. This is the model we
are launching within the EC-funded PortDial project
and aiming at involving the research community at
large and existing communities for sharing linguistic
resources, such as METANET and METASHARE1.
We believe that the creation of sharable SDS data
and linguistic resources for both academic and com-
mercial use will lead to the democratization of spo-
ken dialog systems development, reduce the barrier
to entry for new developers, as well as lead to im-
proved technologies for authoring speech services.
References
C. Bizer, T. Heath, and T. Berners-Lee. 2009. Linked
data-the story so far. International Journal on Seman-
tic Web and Information Systems, 14:9.
G. Tur and R. De Mori, editors. 2011. Spoken Language
Understanding: Systems for Extracting Semantic In-
formation from Speech. Wiley.
Christina Unger, Felix Hieber, and Philipp Cimiano.
2010. Generating LTAG grammars from a lexicon-
ontology interface. In Srinivas Bangalore, Robert
Frank, and Maribel Romero, editors, Proceedings of
the 10th International Workshop on Tree Adjoining
Grammars and Related Formalisms (TAG+10), pages
61?68, 06/2010.
1http://www.meta-net.eu/meta-share
2
Proceedings of the 14th European Workshop on Natural Language Generation, pages 10?19,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Exploiting ontology lexica for
generating natural language texts from RDF data
Philipp Cimiano, Janna Lu?ker, David Nagel, Christina Unger
Semantic Computing Group
Cognitive Interaction Technology ? Center of Excellence (CITEC),
Bielefeld University, Germany
Abstract
The increasing amount of machine-
readable data available in the context
of the Semantic Web creates a need
for methods that transform such data
into human-comprehensible text. In
this paper we develop and evaluate a
Natural Language Generation (NLG)
system that converts RDF data into
natural language text based on an on-
tology and an associated ontology lex-
icon. While it follows a classical NLG
pipeline, it diverges from most cur-
rent NLG systems in that it exploits
an ontology lexicon in order to capture
context-specific lexicalisations of ontol-
ogy concepts, and combines the use of
such a lexicon with the choice of lexical
items and syntactic structures based on
statistical information extracted from a
domain-specific corpus. We apply the
developed approach to the cooking do-
main, providing both an ontology and
an ontology lexicon in lemon format.
Finally, we evaluate fluency and ade-
quacy of the generated recipes with re-
spect to two target audiences: cooking
novices and advanced cooks.
1 Introduction
The goal of the Semantic Web is to en-
rich the current web by a layer of machine-
readable and machine-understandable con-
tent (Berners-Lee et al, 2001). In recent years,
the growth of data published on the web ac-
cording to Semantic Web formalisms and data
models (e.g. RDF(S) and OWL) has been
exponential, leading to more than 30 billion
RDF triples1 available as part of the Linked
1http://www4.wiwiss.fu-berlin.de/lodcloud/
state/
Open Data cloud, which contains a wide range
of factual knowledge that is very interesting
to many applications and for many purposes.
However, due to the fact that it is available as
RDF, it is not directly accessible to humans.
Thus, natural language generation from RDF
data has recently become an important topic
for research, leading to the development of var-
ious systems generating natural language text
from knowledge bases (Bouayad-Agha et al,
2012a; Mellish and Sun, 2006; Sun and Mel-
lish, 2007; Wilcock and Jokinen, 2003) as well
as corresponding shared tasks (Banik et al,
2012; Bouayad-Agha et al, 2012b).
Natural language generation (NLG) from
knowledge bases requires knowledge about
how the concepts in the underlying ontology?
individuals, classes and relations?are realised
linguistically. For this purpose, lemon, a lex-
icon model for ontologies, has been devel-
oped (McCrae et al, 2011). One of the use
cases of lemon is to support natural language
generation systems that take as input a knowl-
edge base structured with respect to a given
ontology. In this paper, we present a system
that relies on lemon lexica for selecting suit-
able lexicalisations of a given concept, showing
how ontology lexica can be exploited in a stan-
dard generation architecture.
We apply our system to the domain of
cooking, generating natural language texts for
recipes modeled as RDF data based on a cook-
ing ontology. Our system relies on a large text
corpus of cooking recipes that is used to ex-
tract frequency information for single terms
and n-grams as well as syntactic trees, which
are then used in the selection process for lex-
icalisation and surface realisation. Addition-
ally, we provide a manually created lemon lex-
icon for the underlying ontology that was en-
riched with inflectional variants derived from
10
Wiktionary. The lexicon also includes con-
textual information regarding which lexicalisa-
tions to prefer depending on the target group,
and thereby allows our system to personalize
the output to different groups of users. We
demonstrate the flexibility of our system by
showing that it can be easily tuned to gen-
erate recipe descriptions both for novices and
for advanced cooks and that this adaptation is
clearly recognized by users.
The remainder of this paper is structured
as follows. In Section 2 we describe the re-
sources we created and employed, in particular
a domain ontology, a corresponding ontology
lexicon enriching ontology concepts with lexi-
cal information, and a parsed domain corpus.
In Section 3 we describe the architecture of
the system, in particular the use of a corpus
for selecting appropriate syntactic structures
and surface realisations of concepts. Then we
present the results of an extensive user study
in Section 4, compare our approach to related
work in Section 5 and finally give an outlook
on future work in Section 6.
2 Resources
2.1 Domain ontology and lexicon
In order to be able to model cooking recipes
as RDF data, we created a domain ontology
in which recipes are modeled comprising the
following information (for a similar modeling
see (Ribeiro et al, 2006)):
? An indication of the number of people
that it serves.
? A set of ingredients used in the recipe.
? An ordered list of steps involving a certain
action (e.g. cutting) on a set of ingredi-
ents. Each action in turn allows one or
many modifiers (e.g. to indicate cutting
granularity).
? Interim ingredients that are produced as
the result of some step and can be reused
later in another step.
An excerpt from the RDF recipe for mar-
ble cake is given in Figure 1. It shows two
steps, one for mixing the ingredients butter,
flour and egg, using a bowl, thereby creating
1 :Marmorkuchen a :Nachspeise;
2
3 :hasStep [ a :Step ;
4 :hasStepNumber 7??xsd:integer ;
5 :hasAction action:mischen ;
6 :hasMixType prop:vermengen ;
7 :hasIngredient
8 [ a ingredient:Butter ;
9 :hasAmount amount:Gramm ;
10 :hasValue "300" ],
11 [ a ingredient:Mehl ;
12 :hasAmount amount:Gramm ;
13 :hasValue "375" ],
14 [ a ingredient:Ei ;
15 :hasAmount amount:Stueck
16 :hasValue "5" ] ;
17 :hasIndirectIngredient
18 tool:Schuessel ;
19 :creates tool:Marmorkuchen_Interim_1
20 ] ;
21
22 :hasStep [ a :Step ;
23 :hasStepNumber 8??xsd:integer ;
24 :hasAction action:backen ;
25 :isPassive "true "??xsd:boolean ;
26 :hasTimeUnit prop:Minute ;
27 :hasTimeValue 45.0?? xsd:double ;
28 :hasIngredient
29 tool:Marmorkuchen_Interim_1 ;
30 :hasIndirectIngredient
31 tool:Backofen
32 ] .
Figure 1: An excerpt from the RDF recipe for
marble cake.
the dough as an interim object, and a subse-
quent one in which this interim object is being
baked in the oven for 45 minutes.
In general, each step comprises:
? A step number indicating the order in a
list of steps.
? An associated action indicating the type
of action performed in the step, e.g. to
fold in.
? One or more ingredients used in the ac-
tion. This is either an ingredient from the
ingredient list of the recipe, or an object
that was created as a result of some other
step.
? A passivity flag indicating whether a step
does not require an active action by the
cook, e.g. Let the cake cool for 1 hour.
? Further modifiers such as mixType indi-
cating the way in which the ingredients
11
are mixed (e.g. beating or folding), tem-
poral modifiers specifying a time unit and
time value (e.g. 45 minutes). These mod-
ifiers later affect the grouping of steps and
their lexicalisation.
? A flag indicating whether this is a key step
within the recipe, for example a step that
requires particular care and thus should
get emphasis in the verbalization, like
Quickly fry the meat!
Overall, the ontology comprises 54 different
action types that we used to manually model
37 recipes. Further, we created a lemon lexi-
con specifying how the different actions and in-
gredients specified in the ontology are verbal-
ized in German. In total the lexicon contains
1,530 lexical entries, on average 1.13 lexical
variants for each ingredient and 1.96 variants
for each action.
Figure 2 gives an example entry for the
verb schneiden (to cut), specifying its part
of speech, two form variants, the infinitive
and the past participle, and a semantic ref-
erence to the ontology action of cutting. Fig-
ure 3 gives an excerpt from the lexical entry
for tranchieren (to carve), which refers to the
same cutting action but is restricted to cases
where the ingredient is of type meat, modelled
using a logical condition that can be issued
as a query to the knowledge base. This verb
would therefore only be used in the context of
technical registers, i.e. with advanced cooks
as target group.
After having manually created lexical en-
tries with their base forms, we automatically
enrich them with inflectional forms extracted
from Wiktionary, as already indicated in Fig-
ure 2.
The ontology, the RDF recipes as well
as the ontology lexicon can be accessed
at http://www.sc.cit-ec.uni-bielefeld.
de/natural-language-generation.
Although the manual creation of lemon lex-
ica is feasible for small domains (and sup-
ported by tools such as lemon source (McCrae
et al, 2012)), it does not scale to larger do-
mains without a significant amount of effort.
Therefore corpus-based methods for the semi-
automatic creation of ontology lexica are cur-
rently developed, see (Walter et al, 2013).
1 :schneiden a lemon:LexicalEntry ;
2 lexinfo:partOfSpeech lexinfo:verb ;
3
4 lemon:canonicalForm [
5 lemon:writtenRep "schneiden"@de ;
6 lexinfo:tense lexinfo:present ;
7 lexinfo:mood lexinfo:infinitive
8 ];
9 lemon:otherForm [
10 lemon:writtenRep "geschnitten"@de ;
11 lexinfo:verbFormMood
12 lexinfo:participle ;
13 lexinfo:aspect lexinfo:perfective
14 ];
15
16 lemon:sense
17 [ lemon:reference action:schneiden ].
Figure 2: Lexical entry for the verb schneiden,
denoting a cutting action.
1 :tranchieren a lemon:LexicalEntry ;
2 lexinfo:partOfSpeech lexinfo:verb ;
3
4 lemon:canonicalForm [
5 lemon:writtenRep "tranchieren"@de ];
6
7 lemon:sense
8 [ lemon:reference action:schneiden;
9 lemon:condition [ lemon:value
10 "exists ?x :
11 :hasIngredient (?x,?y),
12 :Step(?x),
13 ingredient:Fleisch (?y)" ];
14 lemon:context
15 isocat:technicalRegister ] .
Figure 3: Lexical entry for the verb
tranchieren, denoting a cutting action re-
stricted to meat and marked as a technical
term.
2.2 Domain corpus
In order to build a domain corpus, we crawled
the recipe collection website http://www.
chefkoch.de, which at that point contained
more than 215 000 recipes with a total amount
of 1.9 million sentences. We extracted the
recipe text as well as the list of ingredients and
the specified level of difficulty ? easy, normal
and complicated.
The extracted text was tokenized using the
unsupervised method described by Schmid
(Schmid, 2000), and for each recipe an n-gram
index (considering 2, 3 and 4-grams) for both
the recipe text and the ingredient list was con-
structed. Furthermore, 65 000 sentences were
parsed using the Stanford parser, trained on
12
the German TIGER corpus, also enriching the
training data of the parser with fragments de-
rived from the ontology lexicon in order to en-
sure that the lexical entries in the ontology
lexicon are actually covered. This resulted in
20 000 different phrase structure trees where
the leafs were replaced by lists of all terms oc-
curring at that position in the parse tree. Both
trees and leaf terms were stored together with
the number of their occurrences. Leaf terms
were additionally annotated with lexical senses
by comparing them to the already created lex-
ical entries and thus connecting them to on-
tology concepts.
3 System architecture
Our system implements a classical NLG
pipeline comprising the following three
steps (Reiter and Dale, 2000):
? Document planning
? Microplanning
? Surface realisation
Document planning in our case is quite
straightforward as the recipes already com-
prise exactly the information that needs to be
verbalized. In the following we present the two
remaining steps in more detail, followed by a
brief description of how the text generation is
parametrized with respect to the target group
(novices or experts).
3.1 Microplanning
Following Reiter & Dale (Reiter and Dale,
2000), microplanning comprises three steps:
aggregation, referring expression generation,
and lexicalisation.
Aggregation Aggregation serves to collapse
information using grouping rules in order to
avoid redundancies and repetitions. In our
case, the main goal of aggregation is to group
steps of recipes, deciding which steps should
be verbalized within the same sentences and
which ones should be separated, based on the
following hand-crafted rules:
? Steps are grouped if
? they have the same step number, or
? the actions associated with the steps
are the same, or
? the same ingredient is processed in
subsequent actions, e.g. peeling and
chopping onions.
? Steps that are marked as important in the
ontology can only be grouped with other
important steps.
? If the grouping of steps would result in too
many ingredients to still form a readable
sentence, the steps are not grouped. Cur-
rently we consider more than six ingredi-
ents to be too many, as there are hardly
any trees in the corpus that could gener-
ate corresponding sentences.
? If there is a big enough time difference
between two steps, as e.g. between baking
a cake for 60 minutes and then decorating
it, the steps are not grouped.
Each of these rules contributes to a numeri-
cal value indicating the probability with which
steps will be grouped. The use of the rules is
also controlled by a system parameter ?length
that can be set to a value between 0 and 1,
where 0 gives a strong preference to short sen-
tences, while 1 always favors longer sentences.
Referring expression generation The
generation of referring expressions is also rule-
based and mainly concerns ingredients, as ac-
tions are commonly verbalized as verbs and
tools (such as bowls and the oven) usually
do not re-occur often enough. In deciding
whether to generate a pronoun, the following
rule is used: A re-occurring ingredient is re-
placed by a pronoun if there is no other ingre-
dient mentioned in the previous sentence that
has the same number and gender. A system
parameter ?pronoun can be set to determine the
relative frequency of pronouns to be generated.
If an ingredient is not replaced by a pro-
noun, then one of the following expressions is
generated:
? A full noun phrase based on the verbal-
ization given in the ontology lexicon, e.g.
two eggs.
? A definite expression describing a super-
category of the given ingredient. The
super-category is extracted from the on-
tology and its verbalization from the on-
13
tology lexicon. For instance, if the ingre-
dient in question is pork, the expression
meat would be generated.
? A zero anaphora, i.e. an empty referring
expression, as in Bake for 60 minutes or
Simmer until done.
The use of those variants is regulated by a sys-
tem parameter ?pronoun, where a high value
forces the use of abstract expressions and zero
anaphora, while a low value prefers the use
of exact ingredient names. In future work
the decision of which referring expression to
use should be decided on the basis of gen-
eral principles, such as uniqueness of the refer-
ent, avoidance of unnecessary and inappropri-
ate modifiers, brevity, and preference for sim-
ple lexical items, see, e.g., (Reiter and Dale,
1992).
An exception to the above rules are interim
ingredients, whose realisation is determined as
follows. If there is a lexical entry for the in-
terim, it is used for verbalization. If there is
no lexical entry, then the name of the main
ingredient used in the creation of the interim
is used. Furthermore, we define and exploit
manually specified meaning postulates to cre-
ate names for specific, common interims. For
example dough is used if the interim is gener-
ated from flour and at least one of the ingre-
dients butter, sugar, egg or backing powder.
Lexicalisation In order to lexicalise actions
and ingredients, the ontology lexicon is con-
sulted. Especially for actions, the lexicon con-
tains several lexical variants, usually accompa-
nied by a restriction that specifies the context
in which the lexicalisation is appropriate. For
example the action to cut can be lexicalised
in German as hacken (to chop) if the specified
granularity is rough, as bla?ttrig schneiden (to
thinly slice) if the specified granularity is fine,
or tranchieren (to carve) in case the ingredient
is of type meat.
The conditions under which a lexicalisa-
tion can be used felicitously are given in the
lexicon as logical expressions, as exemplified
in Figure 3 above, which are translated into
SPARQL queries that can be used to check
whether the condition is satisfied with respect
to the recipe database.
In addition, we rely on statistics derived
from our domain corpus in order to choose a
lexicalisation in case the conditions of more
than one lexical variant are fulfilled, by pre-
ferring terms and term combinations with a
higher frequency in the domain corpus. Again,
the system implements a parameter, ?variance,
that regulates how much overall lexical vari-
ability is desired. This, however, should be
used with care, as choosing variants that are
less frequent in the corpus could easily lead to
strange or inappropriate verbalizations.
3.2 Surface realisation
The input to the surface realisation compo-
nent is a list of concepts (spanning one or more
recipe steps) together with appropriate lexical-
isations as selected by the lexicalisation com-
ponent. The task of the surface realiser then
is to find an appropriate syntactic tree from
the parsed corpus that can be used to realise
the involved concepts. An example of such a
parse tree with annotated leaf probabilities is
shown in Figure 4.
All trees retrieved from the index are
weighted to identify the best fitting tree com-
bining the following measures: i) the normal-
ized probability of the syntax tree in the do-
main corpus, ii) a comparison of the part-of-
speech tag, synonyms and the lexical sense of a
given lexicalisation with those of the terms in
the retrieved tree, iii) the node distances of re-
lated words inside each tree, and iv) an n-gram
score for each resulting sentence. These scores
are added up and weighted w.r.t. the size of
n, such that, for example, 4-grams have more
influence on the score than 3-grams. Also,
sentences with unbalanced measure, i.e. that
score very well w.r.t. one measure but very
poorly w.r.t. another one, are penalized.
3.3 Personalization
On the basis of conditions on the context of use
provided in the ontology lexicon, it is possible
to distinguish lexicalisations that are suitable
for experts from lexical variants that are suit-
able for novices. Thus, texts can be generated
either containing a high amount of technical
terms, in case the user has a high proficiency
level, or avoiding technical terms at all, in case
the user is a novice. Furthermore, the com-
plexity of texts can be varied by adjusting the
14
S (0.005)
VP
VVINF
schlagen (0.33)
wu?rfeln (0.22)
stellen (0.13)
. . .
ADJD
steif (0.32)
fein (0.18)
kalt (0.08)
. . .
NP
NN
Sahne (0.20)
Eiwei? (0.09)
Zwiebel (0.07)
. . .
ART
Die (0.60)
Das (0.18)
Den (0.21)
. . .
Figure 4: Example of a parse tree extracted
from the corpus, annotated with leaf proba-
bilities
sentence length and the number of adjectives
used. We used this as an additional parameter
?context for tailoring texts to their target group,
preferring complex structures in expert texts
and simple structures in texts for novices. The
influence of this parameter is tested as part of
the user study described in the next section.
Personalization thus has been implemented
at the level of microplanning. In addition,
personalization is possible on the level of text
planning. For example, experts often require
less detailed descriptions of actions, such that
they can be summarized in one step, while
they need to be broken down into several steps
for beginners. This will be subject of future
work.
4 Evaluation
The system was evaluated in an online study
with 93 participants?mainly students re-
cruited via email or Facebook. The major-
ity of the participants (70%) were between 18
and 34 years old; the native tongue of almost
all participants (95%) was German. About
half of the participants regarded themselves as
novices, while the other half regarded them-
selves as advanced cooks.
For each participant, 20 recipes were ran-
domly selected and split into two groups. For
ten recipes, test subjects were asked to rate
the fluency and adequacy of the automatically
generated text along the categories very good,
good, sufficient and insufficient. The other ten
recipes were used to compare the effect of pa-
rameters of the generation system and thus
were presented in two different versions, vary-
ing the sentence length and complexity as well
as the level of proficiency. Participants were
asked to rate texts as being appropriate for
novices or for advanced cooks.
The parameters that were varied in our ex-
perimental setting are the following:
? ?context: The context of the used terms, in
particular novice or advanced.
? ?pronoun: Amount of proper nouns, where
a high value prefers pronouns over proper
nouns, while a low value generates only
proper nouns.
? ?variance: Amount of repetitions, where
low values lead to always using the same
term, whereas high values lead to fewer
repetitions.
? ?length: Length of the created sentences,
where a low value creates short sentences,
and high values merge short sentences
into longer ones.
The values of these parameters that were
used in the different configurations are sum-
marized in Table 1. The parameter ?pronoun is
not varied but set to a fixed value that yields
a satisfactory generation of referring expres-
sions, as texts with smaller or higher values
tend to sound artificial or incomprehensible.
?context ?pronoun ?variance ?length
Standard novice 0.5 0.5 0.5
Novice vs novice 0.5 0.5 0.3
Advanced advanced 0.5 0.5 0.7
Simple vs novice 0.5 0.0 0.3
Complex novice 0.5 1.0 0.7
Table 1: The used parameter sets
Fluency and adequacy of the generated
texts Each participant was asked to rate flu-
ency and adequacy of ten automatically gen-
erated texts. The results are given in Figures
5 and 6. The fluency of the majority of gen-
erated texts (85.8%) were perceived as very
good or good, whereas only 1% of the generated
texts were rated as insufficient. Similarly, the
adequacy of 92.5% of the generated texts were
rated as very good or good, and again only 1%
of the generated texts were rated as insuffi-
cient. There was no significant difference be-
tween judgments of novices and experts; nei-
ther did the category of the recipe (main or
15
side dish, dessert, etc.) have any influence.
Overall, these results clearly show that the
quality of the texts generated by our system
is high.
Figure 5: Results for text fluency
Figure 6: Results for text adequacy
Error analysis The most frequent errors
found in the generated texts can be grouped
into the following categories:
? Content (39.4%): Errors in document
planning (e.g. due to the ontology miss-
ing details about tools, such as for cut-
ting cookies, or the recipe missing infor-
mation about the amount of ingredients)
or aggregation (e.g. sentences with highly
related content were not aggregated), as
well as sentence repetitions.
? Language (29.4%): Errors in the re-
ferring expression generation or lexicali-
sation steps (e.g. wrong use of function
words like as well) and grammar errors
(e.g. wrong use of definite or indefinite
determiners).
? Other (31.3%): Some users specified
that they would prefer another ordering
of the involved steps, or that they lack
knowledge of particular terms. Also short
sentences with exclamation marks are of-
ten perceived as impolite.
Influence of parameter settings We set
up the following hypotheses, validating them
by means of a ?2-test by comparing answers
across two conditions corresponding to differ-
ent parameter settings. We regarded a p-value
of 0.05 as sufficient to reject the corresponding
null hypothesis.
H1 Users prefer longer sentences: Re-
jecting the null hypothesis that users rate
texts with longer sentences and texts with
shorter sentences in the same way (p-
value: 3 ? 10?5).
H2 Texts for professionals are regarded
as not suitable for novices: Reject-
ing the null hypothesis that texts gen-
erated for professionals are regarded as
many times as suitable for novices as for
professionals (p-value: 2 ? 10?7).
H3 Beginners prefer texts generated
for novices: The null hypothesis that
novices equally prefer texts targeted to
novices and texts targeted to experts
could not be rejected.
H4 Advanced cooks prefer texts gener-
ated for advanced cooks: Rejecting
the null hypothesis that advanced cooks
equally prefer texts targeted to novices
and texts targeted to experts (p-value:
0.0005).
The confirmation of H1 shows that users per-
ceive a difference in sentence length and pre-
fer texts with longer sentences, probably due
to perceived higher fluency. The confirmation
of H2 and H4, on the other hand, corrobo-
rates the successful adaptation of the gener-
ated texts to specific target groups, showing
16
that texts generated for professionals are in-
deed perceived as being generated for profes-
sionals, and that such texts are preferred by
advanced cooks. The rejection of H3 might
be caused by the fact that recipes for ad-
vanced cooks include some but actually not
many technical terms and are therefore also
comprehensible for novices.
5 Related work
There have been different approaches to
natural language generation, ranging from
template-based to statistical architectures.
While early NLG systems were mainly based
on manually created rules (Bourbeau et al,
1990; Reiter et al, 1992), later approaches
started applying statistical methods to the
subtasks involved in generation (Belz, 2005),
focusing on scalability and easy portability
and often relying on overgeneration and sub-
sequent ranking of generation possibilities.
Personalization has been a concern in both
strands of research. PEBA-II (Milosavljevic
et al, 1996), for example, generates target-
group-specific texts for novice and experts
users from taxonomical information, relying
on a phrasal lexicon that is similar in spirit to
our ontology lexicon. Statistical approaches
such as (Isard et al, 2006), on the other
hand, use text corpora to generate personal-
ized texts.
Our approach is hybrid in the sense that it
enriches a classical rule-based approach with
statistical data in the microplanning and reali-
sation steps, thus being comparable to systems
like HALogen (Langkilde and Knight, 1998)
and pCRU (Belz, 2008). The main difference
is that it uses Semantic Web data as base.
Since the emergence of the Semantic Web
there has been a strong interest in NLG
from Semantic Web data, especially for pro-
viding users with natural language access to
structured data. Work in this area com-
prises verbalization of ontologies as well as
RDF knowledge bases; for an overview see
(Bouayad-Agha et al, to appear). Of par-
ticular interest in the context of our work is
NaturalOWL (Galanis and Androutsopoulos,
2007), a system that produces descriptions of
entities and classes relying on linguistic anno-
tations of domain data in RDF format, similar
to our exploitation of ontology lexica. We thus
share with NaturalOWL the use of linguis-
tic resources encoded using standard Semantic
Web formats. The main difference is that the
annotations used by NaturalOWL comprise
not only lexical information but also micro-
plans for sentence planning, which in our case
are derived statistically and represented out-
side the lexicon. Separating lexical informa-
tion and sentence plans makes it easier to use
the same lexicon for generating different forms
of texts, either with respect to specific target
groups or stylistic variants.
6 Conclusion and future work
We have presented a principled natural lan-
guage generation architecture that follows a
classical NLG architecture but exploits an on-
tology lexicon as well as statistical information
derived from a domain corpus in the lexicali-
sation and surface realisation steps. The sys-
tem has been implemented and adapted to the
task of generating cooking recipe texts on the
basis of RDF representations of recipes. In an
evaluation with 93 participants we have shown
that the system is indeed effective and gener-
ates natural language texts that are perceived
as fluent and adequate. A particular feature
of the system is that it can personalize the
generation to particular target groups, in our
case cooking novices and advanced cooks. The
information about which lexicalisation to pre-
fer depending on the target group is included
in the ontology lexicon. In fact, the ontology
lexicon is the main driver of the generation
process, as it also guides the search for ap-
propriate parse trees. It thus is a central and
crucial component of the architecture.
While the system has been adapted to the
particulars of the cooking domain, especially
concerning the generation of referring expres-
sions, the architecture of the system is fairly
general and in principle the system could be
adapted to any domain by replacing the on-
tology, the corresponding ontology lexicon and
by providing a suitable domain corpus. This
flexibility is in our view a clear strength of our
system architecture.
A further characteristic of our system is the
consistent use of standards, i.e. OWL for
the ontology, RDF for the actual data to be
17
verbalized, SPARQL for modelling contextual
conditions under which a certain lexicalisa-
tion is to be used, and the lemon format for
the representation of the lexicon-ontology in-
terface. One important goal for future work
will be to clearly understand which knowledge
an ontology lexicon has to include in order
to optimally support NLG. To this end, we
intend to test the system on other domains,
and at the same time invite other researchers
to test their systems on our data, available
at http://www.sc.cit-ec.uni-bielefeld.
de/natural-language-generation.
Acknowledgment
This work was partially funded within the EU
project PortDial (FP7-296170).
References
E. Banik, C. Gardent, D. Scott, N. Dinesh, and
F. Liang. 2012. KBGen: text generation from
knowledge bases as a new shared task. In Proc.
Seventh International Natural Language Gener-
ation Conference (INLG 2012), pages 141?145.
A. Belz. 2005. Statistical generation: Three meth-
ods compared and evaluated. In Proc. 10th Eu-
ropean Workshop on Natural Language Genera-
tion (ENLG ?05), pages 15?23.
A. Belz. 2008. Automatic generation of weather
forecast texts using comprehensive probabilistic
generation-space models. Natural Language En-
gineering, 14(4):431?455.
T. Berners-Lee, J. Hendler, and O. Lassila. 2001.
The Semantic Web. Scientific American Maga-
zine.
N. Bouayad-Agha, G. Casamayor, S. Mille,
M. Rospocher, H. Saggion, L. Serafini, and
L. Wanner. 2012a. From ontology to NL: Gener-
ation of multilingual user-oriented environmen-
tal reports. In Proc. 17th International Confer-
ence on Applications of Natural Language Pro-
cessing to Information Systems (NLDB 2012),
pages 216?221.
N. Bouayad-Agha, G. Casamayor, L. Wanner, and
C. Mellish. 2012b. Content selection from Se-
mantic Web data. In Proc. Seventh Interna-
tional Natural Language Generation Conference
(INLG 2012), pages 146?149.
N. Bouayad-Agha, G. Casamayor, and L. Wanner.
to appear. Natural Language Generation in the
context of the Semantic Web. Semantic Web
Journal.
L. Bourbeau, D. Carcagno, E. Goldberg, R. Kit-
tredge, and A. Polgue`re. 1990. Bilingual gener-
ation of weather forecasts in an operations en-
vironment. In Proc. 13th International Con-
ference on Computational Linguistics (COLING
1990), pages 318?320.
D. Galanis and I. Androutsopoulos. 2007. Gen-
erating multilingual descriptions from linguis-
tically annotated OWL ontologies: the Nat-
uralOWL system. In Proc. 11th European
Workshop on Natural Language Generation
(ENLG ?07), pages 143?146.
A. Isard, C. Brockmann, and J. Oberlander. 2006.
Individuality and alignment in generated dia-
logues. In Proc. Fourth International Natural
Language Generation Conference (INLG 2006),
pages 25?32.
I. Langkilde and K. Knight. 1998. Generation that
exploits corpus-based statistical knowledge. In
Proc. 17th International Conference on Compu-
tational Linguistics (COLING ?98), pages 704?
710.
J. McCrae, D. Spohr, and P. Cimiano. 2011.
Linking lexical resources and ontologies on the
semantic web with lemon. In Proc. 8th Ex-
tended Semantic Web Conference on The Se-
mantic Web: Research and Applications (ESWC
2011), pages 245?259.
J. McCrae, E. Montiel-Ponsoda, and P. Cimiano.
2012. Collaborative semantic editing of linked
data lexica. In Proceedings of the 2012 Inter-
national Conference on Language Resource and
Evaluation.
C. Mellish and X. Sun. 2006. The Semantic Web
as a linguistic resource: Opportunities for nat-
ural language generation. Knowl.-Based Syst.,
19(5):298?303.
M. Milosavljevic, A. Tulloch, and R. Dale. 1996.
Text generation in a dynamic hypertext environ-
ment. In Proc. 19th Australian Computer Sci-
ence Conference, pages 417?426.
E. Reiter and R. Dale. 1992. A fast algorithm for
the generation of referring expressions.
E. Reiter and R. Dale. 2000. Building natural
language generation systems. Cambridge Uni-
versity Press.
E. Reiter, C. Mellish, and J. Levine. 1992. Au-
tomatic generation of on-line documentation in
the IDAS project. In Proc. Third Conference on
Applied Natural Language Processing (ANLP),
pages 64?71.
R. Ribeiro, F. Batista, J.P. Pardal, N.J. Mamede,
and H.S. Pinto. 2006. Cooking an ontology. In
Proceedings of the 12th international conference
on Artificial Intelligence: methodology, Systems,
18
and Applications, AIMSA?06, pages 213?221.
Springer.
Helmut Schmid. 2000. Unsupervised learning of
period disambiguation for tokenisation. Techni-
cal report, IMS-CL, University of Stuttgart.
X. Sun and C. Mellish. 2007. An experiment on
?free generation? from single RDF triples. In
Proc. 11th European Workshop on Natural Lan-
guage Generation (ENLG ?07), pages 105?108.
S. Walter, C. Unger, and P. Cimiano. 2013. A
corpus-based approach for the induction of on-
tology lexica. In Proceedings of the 18th Inter-
national Conference on the Application of Nat-
ural Language to Information Systems (NLDB
2013).
G. Wilcock and K. Jokinen. 2003. Generating re-
sponses and explanations from RDF/XML and
DAML+OIL. In Knowledge and Reasoning in
Practical Dialogue Systems, IJCAI 2003 Work-
shop, pages 58?63.
19
Proc. of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL 2014, pages 30?37,
Gothenburg, Sweden, April 26 2014.
c
?2014 Association for Computational Linguistics
A multimodal corpus for the evaluation of computational models for
(grounded) language acquisition
Judith Gaspers
a
, Maximilian Panzner
a
, Andre Lemme
b
, Philipp Cimiano
a
,
Katharina J. Rohlfing
c
, Sebastian Wrede
b
a
Semantic Computing Group, CITEC, Bielefeld University, Germany
{jgaspers|mpanzner|cimiano}@cit-ec.uni-bielefeld.de
b
Research Institute for Cognition and Robotics (CoR-Lab), Bielefeld University, Germany
{alemme|swrede}@cor-lab.uni-bielefeld.de
c
Emergentist Semantics Group, CITEC, Bielefeld University, Germany
kjr@uni-bielefeld.de
Abstract
This paper describes the design and ac-
quisition of a German multimodal cor-
pus for the development and evaluation of
computational models for (grounded) lan-
guage acquisition and algorithms enabling
corresponding capabilities in robots. The
corpus contains parallel data from multi-
ple speakers/actors, including speech, vi-
sual data from different perspectives and
body posture data. The corpus is designed
to support the development and evalua-
tion of models learning rather complex
grounded linguistic structures, e.g. syn-
tactic patterns, from sub-symbolic input.
It provides moreover a valuable resource
for evaluating algorithms addressing sev-
eral other learning processes, e.g. concept
formation or acquisition of manipulation
skills. The corpus will be made available
to the public.
1 Introduction
Children acquire linguistic structures through ex-
posure to (spoken) language in a rich context and
environment. The semantics of language may be
learned by establishing connections between lin-
guistic structures and corresponding structures in
the environment, i.e. in different domains such
as the visual one (Harnad, 1990). Both with re-
spect to modeling language acquisition in chil-
dren and with respect to enabling corresponding
language acquisition capabilities in robots, which
may ideally be also grounded in their environment,
it is hence of great interest to explore i) how lin-
guistic structures of different levels of complexity,
e.g. words or grammatical phrases, can be derived
from speech input, ii) how structured representa-
tions for entities observed in the environment can
be derived, e.g. how concepts and structured rep-
resentations of actions can be formed, and iii) how
connections can be established between structured
representations derived from different domains. In
order to gain insights concerning the mechanisms
at play during language acquisition (LA), which
enable children to solve these learning tasks, mod-
els are needed which ideally cover several learning
tasks. For instance, they may cover the acquisition
of both words and grammatical rules as well as the
acquisition of their grounded meanings. Comple-
mentarily, data resources are needed which enable
the design and evaluation of these models by pro-
viding suitable parallel data.
Aiming to provide a basis for the development
and evaluation of LA models addressing the ac-
quisition of rather complex and grounded linguis-
tic structures, i.e. syntactic patterns, from sub-
symbolic input, we designed a German multi-
modal input corpus. The corpus consists of data of
multiple speakers/actors who performed actions in
front of a robot and described these actions while
executing them. Subjects were recorded, i.e. par-
allel data of speech, stereo vision (including the
view-perspective of the ?infant?/robot) and body
postures were gathered. The resulting data hence
allow grounding of linguistic structures in both
vision and body postures. Among others, learn-
ing processes that may be evaluated using the cor-
pus include: acquisition of several linguistic struc-
tures, acquisition of visual structures, concept for-
mation, acquisition of generalized patterns which
abstract over different speakers and actors, estab-
lishment of correspondences between structures
30
from different domains, acquisition of manipula-
tion skills, and development of appropriate models
for the representations of actions.
This paper is organized as follows. Next, we
will provide background information concerning
computational models of LA. In Section 3, we
will then describe the corpus design and acqui-
sition, including the desired properties of the
collected data, corresponding experimental set-
tings and technical implementation. We will then
present the resulting data set and subsequently
conclude..
2 Background
To date, several models addressing LA learning
tasks have been proposed and evaluated using dif-
ferent copora. Yet, these models typically focus on
a subset or certain aspects of the LA learning tasks
mentioned in the previous section, often assuming
other learning tasks, e.g. those of lower complex-
ity, as already solved by the learner. For instance,
models addressing the acquisition of grammatical
constructions and their meaning (Kwiatkowski et
al., 2012; Alishahi and Stevenson, 2008; Gaspers
and Cimiano, in press; Chang and Maia, 2001)
typically learn from symbolic input. In particu-
lar, assuming that the child is already able to seg-
ment a speech signal into a stream of words and to
extract structured representations from the visual
context, such models typically explore learning
from sequences of words and symbolic descrip-
tions of the non-linguistic context. Models ad-
dressing the acquisition of word-like units directly
from a speech signal (R?as?anen, 2011; R?as?anen et
al., 2009) have also been explored. These, how-
ever, typically do not address learning of more
complex linguistic structures/constructions.
Taken together, lexical acquisition from speech
and syntactic acquisition have been mainly stud-
ied independently of each other, often assuming
that syntactic acquisition follows from knowledge
of words. However, learning processes might ac-
tually be interleaved, and top-down learning pro-
cesses may play an important role in LA. For
instance, with respect to computational learn-
ing from symbolic input, it has been shown that
knowledge of syntax can facilitate word learning
(Yu, 2006). Children may, for instance, also make
use of syntactic cues during speech segmentation
and/or word learning, but models addressing lexi-
cal acquisition from speech have to date mainly ig-
nored syntax (R?as?anen, 2012). Models addressing
the acquisition of syntactic patterns directly from
speech provide a basis for exploring to what extent
learning mechanisms might be interleaved in early
LA. Moreover, they allow to investigate the pos-
sible role of several top-down learning processes
which have to date been little explored.
Several corpora comprising interactions of chil-
dren with their caregivers have been collected. A
large such resource is the CHILDES data base
(MacWhinney, 2000), which contains transcribed
speech. Data from CHILDES have been often
used to evaluate models learning from symbolic
input, in particular models for syntactic acquisi-
tion from sequences of words; additional accom-
panying symbolic context representations have
been often created (semi?)automatically. More-
over, multimodal corpora containing caregiver-
child interactions have been recorded and anno-
tated (Bj?orkenstam and Wirn, 2013; Yu et al.,
2008), thus also allowing to study the role of social
interaction and extra-linguistic cues in language
learning. By contrast, in this work we aim to pro-
vide a basis for developing and evaluating models
which address the acquisition of syntactic patterns
from speech. Hence, allowing to derive general-
ized patterns, linguistic units as well as the objects
and actions they refer to have to re-appear in the
data several times. Thus, in line with the CARE-
GIVER corpus (Altosaar et al., 2010) we did not
record caregiver-child interactions but attempted
to approximate speech used by caregivers with re-
spect to the learning task(s) at hand. However,
the focus of the CAREGIVER corpus is on mod-
els learning word-like units from speech. Thus,
a number of keywords were spoken in different
carrier sentences; speech is accompanied by only
limited non-linguistic context information in the
corpus. In contrast to CAREGIVER, we did not
restrict language use directly and recorded paral-
lel context information from different modalities,
focusing not only on the acquisition of word-like
units from speech and word-to-object mapping but
moreover on the acquisition of simple syntactic
patterns and mapping language to actions.
3 Corpus design and acquisition
In this section, we will first describe the desired
properties of the corpus. Subsequently, we will
present the corresponding experimental settings,
used stimuli and procedure, the technical imple-
31
mentation of the robot behavior and the data ac-
quisition as well as the resulting corpus.
3.1 Desired properties
Our goal was to design a corpus comprising multi-
modal data which supports the evaluation of com-
putational models addressing several LA learn-
ing tasks, and in particular the acquisition of
grounded syntactic patterns from sub-symbolic in-
put only as well as the development of compo-
nents supporting the acquisition of language by
robots. Thus, the main focus was to design the
corpus in such a way that the data acquisition
scenario was simplified enough to allow solving
the task of learning grounded syntactic patterns
from sub-symbolic input with the resulting data
set (which of course contains much less data when
compared to the innumerable natural language ex-
amples children receive when acquiring language
over several years). In particular, since the ac-
quisition of rather complex structures should be
enabled using sub-symbolic information, several
(repeated) examples for contained structures were
needed, allowing the formation of generalized rep-
resentations. Thus, we opted for a rather sim-
ple scenario. Specifically, the following properties
were taken into account:
? Rather few objects and actions were included
that could moreover be differentiated rather
easily from a visual point of view. How-
ever, in order to reflect differences between
actions, these differed i) with respect to
the number of their referents as well as ii)
with respect to their specificity to certain
objects. In particular, we included actions
which could be performed on different sub-
sets of the objects, ranging from specificity
to one certain object to being executed with
all of the objects.
? Objects and actions reappeared several times,
yielding several examples for each of them.
Repeated appearance is an essential aspect,
since the formation of generalized represen-
tations starting from continuous input re-
quires several observations in order to allow
abstraction over observed examples/different
actors and speakers.
? The scenario was designed such that it en-
couraged human subjects to use rather simple
syntactic patterns/short sentences. Yet, lan-
guage use was in principle unrestricted in or-
der to acquire rather natural data and to cap-
ture speaker-dependent differences. This also
reflects the input children receive in that par-
ents use rather simple language when talking
to children.
? Data were gathered from several human sub-
jects in order to allow for the evaluation of
generalization over different speakers (with
different acoustic properties and different
language use, e.g. different words for ob-
jects, different syntactic patterns with differ-
ent complexity, etc) as well as over different
actors in case of actions, since children inter-
act with different people and are able to solve
this task. Moreover, generalization to dif-
ferent speakers/actors is also important with
respect to learning in artificial agents which
should preferably not be operable by a single
person only.
? Parallel data were gathered in which ob-
jects and actions were explicitly named when
they were used. This is an important as-
pect because the corpus should allow learn-
ing connections between vision, i.e. objects
and actions, and speech (segments) referring
to these objects/actions, i.e. (sequences of
words) and syntactic patterns. It reflects the
input children receive in that caregivers also
explain/show objects directly to their chil-
dren and may show them how to use ob-
jects/perform actions in front of them (Rolf
et al., 2009; Schillingmann et al., 2009).
We opted for the collection of parallel data con-
cerning vision and body postures for human tutors.
Hence, the corpus allows grounding of linguistic
structures in both vision and body postures. In-
cluding body postures moreover allows the eval-
uation of algorithms showing manipulation skills
which is of interest with respect to learning in
robots.
We used stereo vision to allow computational
learners to reliably track object movement and in-
teraction using both visual and depth information.
With respect to vision, four cameras with two dif-
ferent perspectives were used: two static external
cameras as well as the robot?s two internal moving
cameras. The latter basically mimics the ?infant?
view, i.e. while the external cameras were static,
32
the robot moved its eyes (and thus the cameras)
and focused on the tutor?s hand performing the ac-
tions, thus reflecting how a child may focus her/his
attention to the important aspects of a scene/a per-
formance of her/his caregiver.
3.2 Participants
A total of 27 adult human subjects participated in
data collection (7 male, 20 female, mean age: 26).
Subjects were paid for their participation.
3.3 Experimental setting
Human subjects performed pre-defined actions
and simultaneously described their performances
in front of the robot iCub (Metta et al., 2008); Fig.
1 depicts a human subject interacting with iCub.
While interacting with iCub, human subjects? be-
Figure 1: A human subject interacting with iCub.
havior was recorded. In particular, the following
data were recorded simultaneously:
? Speech/Audio (via a headset microphone)
? Vision/Video, static perspective (via two
cameras, allowing for stereo vision)
? iCub-Vision/Video, iCub?s (attentive) per-
spective (via iCub?s two internal cameras,
again allowing for stereo vision)
? Body postures (via a Kinect).
An experimental sketch showing the experimental
setting including the positions of the human sub-
ject and iCub, as well as camera and Kinect po-
sitions, is illustrated in Fig. 2. As can be seen,
the human subject was placed directly opposite to
iCub. The two external cameras and the Kincet
were placed slightly sloped opposite to the sub-
ject. Subjects were instructed about which actions
should be performed via a computer screen which
was operated by an experimentator.
In order to encourage subjects to perform the tu-
toring task rather naturally, i.e. just like they were
Figure 2: Experimental sketch.
interacting with a human (child), iCub provided
feedback (Nagai and Rohlfing, 2009; Fischer et
al., 2011). In particular, a gazing behavior was
implemented to make the robot appear attentively
following the tutoring.
3.4 Stimuli
Data were gathered in the framework of a toy
cooking scenario. In particular, subjects prepared
several dishes in front of iCub using toy objects.
Specifically, 21 toy objects were chosen such that
Figure 3: Utilized objects.
they were rather easy to differentiate with respect
to color and/or form. The chosen objects were:
pizza, pita bread, plate, bowl, spaghetti, pepper,
vinegar, red pepper, lettuce leafs, tomato, onion,
cucumber, cheese, toast, salami, chillies, egg, an-
chovy, cutting board, knife, and mushrooms. The
objects are depicted in Fig. 3. Moreover, six dif-
ferent actions were chosen which could be exe-
cuted using these objects. Again, the goal was
to support rather easy identification visually (with
respect to their trajectories). The chosen actions
were: showing an object, cutting an object (egg
or tomato) into two pieces (with knife), placing
an object onto another one (plate, pizza, cutting
board, toast), putting an object into another one
(bowl, pita bread), pour vinegar, and strew pep-
per. Thus, most actions were object-specific to a
33
certain degree, i.e. they were to be executed with
a certain subset of the objects each. The show ac-
tion was to be executed using each of the objects.
Furthermore, 20 different dishes, i.e. preparation
processes each consisting of a sequence of actions,
were created (four dishes including salad, pizza,
pita bread, spaghetti and sandwich/toast, respec-
tively). This was done in order to gather rather
fluent/consistent courses of action and rather flu-
ent communication in case of descriptions. For in-
stance, one sequence for preparing a salad started
as follows: showing bowl, showing lettuce leafs,
putting lettuce leafs into bowl, showing cutting
board, showing knife, showing tomato, putting
tomato onto cutting board, cutting tomato into two
pieces, putting tomato pieces into bowl, etc.
3.5 Procedure
Subjects first prepared one dish while not being
recorded in order to get familiar with the task.
They were instructed to perform presented actions
and to describe their performance simultaneously.
Moreover, they were asked to name objects and
actions explicitly, since a goal of the corpus is to
allow learning connections between speech, vision
and body postures. Subjects were not asked to
use particular words or phrases, but were free to
make own choices. For instance, when being ex-
posed to a picture of the pita bread, they were sup-
posed to explicitly name the pita bread. Yet, they
were free to choose a suitable word (or sequence
of words), e.g. ?Pita?, ?Pitatasche?, ?Teigtasche?,
?D?onertasche?, ?Brottasche?, etc.
Actions to be performed were presented to the
subjects via a computer screen; either one action
was presented or ? in most cases ? two actions
were presented at once to be executed one after
another. In most cases two actions were presented
in order to gain more fluent communications and
courses of action. In no case more than two ac-
tions were presented together because we wanted
subjects to focus on performance and not on re-
membering a certain course of action. Actions
were presented only in the form of pictures in or-
der to elicit rather natural language use. In par-
ticular, as mentioned previously, subjects could
choose freely how to name objects and actions. An
example for a screen/picture showing two actions
to be performed one after another is presented in
Fig. 4. An experimentator operated the screen,
i.e. guided the subjects through the sequences of
Figure 4: Example screen showing the actions
show red pepper and put red pepper into bowl.
actions. Subjects participated for approximately
one hour; only subject?s actual performances were
recorded, yielding approximately 20?30 minutes
of usable material per subject.
3.6 Robot behavior
As mentioned previously, a gazing behavior was
implemented to make the robot appear attentively
following the tutoring. In particular, the robot?s
gaze followed a subject?s presentation of an action
by gazing at her/his right wrist. At times when
subjects did not move their hands (to present ac-
tions) the robot was looking around, i.e. it gazed
at random targets. In the following, the implemen-
tation of the robot behavior will be described in
more detail.
The experimental setup shown in Fig. 2 allows the
system to observe a person in front of the robot
iCub. While the presentation task was performed
by the person, the robot was supposed to gaze at
the right wrist of this person. Via the Kinect data
it was possible to acquire the body posture of the
robot?s interaction partner. We extracted the loca-
tion of the wrist and represented the Cartesian po-
sition in the coordinate system of the robot. This
position was then used as the target to generate
the head and eye movements. The movement was
executed by the iKinGaze module available in the
iCub software repository (Pattacini, 2010).
Next to this ?tracking? behavior of the robot we
also used a ?background? behavior. The ?back-
ground? behavior then drew randomly new tar-
gets x
targ
(in meter) from the uniform distribution
? ? [?1.5,?1, 5] ? [?0.2, 0.2] ? [0.2, 0, 4] in
front of the robot. After convergence to the tar-
get the behavior waited for t = 3 seconds before
a new target was drawn. The switch from ?back-
ground? behavior to ?tracking? behavior was trig-
gered if new targets arrived from the Kinect-based
tracking component. This behavior stayed active
34
as long as targets were received. If no targets were
arriving during t = 2 sec. after the gazing con-
verged on the last target, the ?background? behav-
ior took over. Due to the difference in distance
between targets, the motion duration was different
as well. Therefore, time delays were added to the
target generation, which resulted in a more natural
behavior of the robot gazing.
4 Acquired data
In order to record synchronized data from the
external sensors, the robot system and the ex-
perimental control software, we utilized a ded-
icated framework for the acquisition of multi-
modal human-robot interaction data sets (Wienke
et al., 2012). The framework and the underly-
ing technology (Wienke and Wrede, 2011) allows
to directly capture the network communication of
robotics software components. Through this ap-
proach, system-internal data from the iCub such as
its proprioception and stereo cameras images can
be synchronously captured and transformed into
an RETF
1
-described log-file format with explicit
time and type information. Moreover, additional
recording devices such as the Kinect sensors, the
external pair of stereo cams or the audio input
from a close-talk microphone are captured directly
with this system and stored persistently. An exam-
ple of the acquired parallel data is provided by Fig.
5 while Table 6 summarizes the technical aspects
of the acquired data.
The applied framework also supports the auto-
matic export and conversion of synchronized parts
of the multimodal data set to common formats
used by other 3rd party tools such as the annota-
tion tool ELAN (Sloetjes and Wittenburg, 2008)
used for ground truth annotation of the acquired
corpus. In this experiment, we additionally cap-
tured the logical state of the experiment control
software which allowed us to efficiently post-
process the raw data and, e.g., automatically pro-
vide cropped video files containing only single ut-
terances. A logical state corresponds to the image
seen at the screen by a human subject at a certain
time, showing the action(s) to be performed.
The acquired corpus contains in total 11.45
hours / approx. 2.3 TB of multimodal input data
recorded in 27 trials. Each trial was recorded in
about 1 hour of wallclock time and cropped to 20?
30 minutes of effective parallel data. While in 5
1
Robot Engineering Task-Force, cf. http://retf.info/
a)
b)
c) d)
Figure 5: Example of acquired parallel data com-
prising a) visual data from two static cameras, b)
visual data from two cameras contained in the
robot?s eyes, c) audio and d) body posture data
recorded by the Kinect. In this example the sub-
ject is preparing a sandwich, and currently strew-
ing pepper onto it.
cases not all of the parallel data streams are avail-
able due to difficulties with the robot and the wire-
less microphones, we decided to leave this data
in the corpus to evaluate machine learning pro-
cesses addressing learning from one or a subset of
the modalities only, e.g. blind segmentation of a
speech stream.
From the data logs, we exported audio (in AAC
format) and the 4 synchronized video (with H.264
encoding) files (MP4 container format) for each
trial with an additional ELAN project file for an-
notation. This annotation is currently carried out;
a screenshot of acquired data and corresponding
annotations in ELAN is depicted in Fig. 7. It
comprises annotation of errors, as well as start-
ing and end points for both presented actions and
spoken utterances. In particular, in case of speech
word transcriptions are added, while in case of vi-
sion actions are annotated in the form of predicate
logic formulas. Hence, once the corpus is prepro-
cessed, it is also suitable for the evaluation of mod-
els learning from symbolic input with respect to
data from one or more domains. For instance, one
could explore the acquisition of syntactic patterns
from speech by providing parallel visual context
35
# Device Description Data type Frequency Dimension Throughput
1 Cam 1 Scene video rst.vision.Image ? 30 Hz 640? 480? 3 ? 28 MB/s
2 Cam 2 Scene video rst.vision.Image ? 30 Hz 640? 480? 3 ? 28 MB/s
3 Mic 1 Speech rst.audition.SoundChunk ? 50 kHz 1-2 ? 0.5 MB/s
4 iCub Cam 1 Ego left bottle/yarp::sig::Image ? 30 Hz 320? 240? 3 ? 7 MB/s
5 iCub Cam 2 Ego right bottle/yarp::sig::Image ? 30 Hz 320? 240? 3 ? 7 MB/s
6 Kinect Body posture TrackedPosture3DFloat
2
? 30 Hz 36 ? 6 kB/s
7 Control Logical state string ? 0.05 Hz - ? 5 B/s
Figure 6: Description of acquired data streams, type specifications, average frequency, data dimension
and throughput as measured during recording.
information either in sub-symbolic form or in the
form of predicate logic formulas.
Figure 7: Example of acquired data and corre-
sponding annotations in ELAN.
Word transcriptions for utterances for the whole
data set are not yet available. According to the ex-
perimentators? impressions, most subjects indeed
used, as desired, rather short sentences. Further-
more, a few subjects tried to vary their linguis-
tic descriptions, i.e. to use different sentences
for each description. Thus, the corpus appears
to cover not only several examples of rather sim-
ple linguistic constructions with variations across
speakers, but moreover input examples with a
rather large degree of linguistic variation for a sin-
gle speaker, hence providing examples of more
challenging data.
We will make the corpus available to the public
once post-processing is completely finished.
5 Conclusion
In this paper, we have described the design and
acquisition of a German multimodal data set for
the development and evaluation of grounded lan-
guage acquisition models and algorithms enabling
corresponding abilities in robots. The corpus con-
tains parallel data including speech, visual data
from four different cameras with different per-
spectives and body posture data from multiple
speakers/actors. Among others, learning pro-
cesses that may be evaluated using the corpus in-
clude: acquisition of several linguistic structures,
acquisition of visual structures, concept forma-
tion, acquisition of generalized patterns which ab-
stract over different speakers and actors, establish-
ment of correspondences between structures from
different domains and acquisition of manipulation
skills.
Acknowledgments
We are deeply grateful to Jan Moringen, Michael
G?otting and Stefan Kr?uger for providing techni-
cal support. We wish to thank Luci Filinger,
Christina Lehwalder, Anne Nemeth and Frederike
Strunz for support in data collection and annota-
tion. This work has been funded by the German
Research Foundation DFG within the Collabora-
tive Research Center 673 Alignment in Communi-
cation and the Center of Excellence Cognitive In-
teraction Technology. Andre Lemme is funded by
FP7 under GA. No. 248311-AMARSi.
References
Afra Alishahi and Suzanne Stevenson. 2008. A com-
putational model of early argument structure acqui-
sition. Cognitive Science, 32(5):789?834.
Toomas Altosaar, Louis ten Bosch, Guillaume Aimetti,
Christos Koniaris, Kris Demuynck, and Henk
van den Heuvel. 2010. A speech corpus for mod-
eling language acquisition: Caregiver. In Proceed-
36
ings of the International Conference on Language
Resources and Evaluation.
Kristina Nilsson Bj?orkenstam and Mats Wirn. 2013.
Multimodal annotation of parent-child interaction
in a free-play setting. In Proceedings of the Thir-
teenth International Conference on Intelligent Vir-
tual Agents.
Nancy C. Chang and Tiago V. Maia. 2001. Learn-
ing grammatical constructions. In Proceedings of
the 23rd Cognitive Science Society Conference.
Kerstin Fischer, Kilian Foth, Katharina J. Rohlfing,
and Britta Wrede. 2011. Mindful tutors: Linguis-
tic choice and action demonstration in speech to in-
fants and to a simulated robot. Interaction Studies,
12(1):134?161.
Judith Gaspers and Philipp Cimiano. in press. A
computational model for the item-based induction of
construction networks. Cognitive Science.
Stevan Harnad. 1990. The symbol grounding problem.
Physica D: Nonlinear Phenomena, 42(1-3):335?
346.
Tom Kwiatkowski, Sharon Goldwater, Luke Zettle-
moyer, and Mark Steedman. 2012. A probabilis-
tic model of syntactic and semantic acquisition from
child-directed utterances and their meanings. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics.
Brian MacWhinney. 2000. The CHILDES Project:
Tools for analyzing talk. Mahwah, NJ.
Giorgio Metta, Giulio Sandini, David Vernon, Lorenzo
Natale, and Francesco Nori. 2008. The iCub hu-
manoid robot: an open platform for research in em-
bodied cognition. In Proceedings of the 8th Work-
shop on Performance Metrics for Intelligent Sys-
tems, pages 50?56, New York, NY. ACM.
Yukie Nagai and Katharina J. Rohlfing. 2009. Com-
putational analysis of motionese toward scaffolding
robot action learning. IEEE Transactions on Au-
tonomous Mental Development, 1:44?54.
Ugo Pattacini. 2010. Modular Cartesian Controllers
for Humanoid Robots: Design and Implementation
on the iCub. Ph.D. thesis, RBCS, Istituto Italiano di
Tecnologia, Genova.
Okko R?as?anen, Unto K. Laine, and Toomas Altosaar.
2009. Computational language acquisition by sta-
tistical bottom-up processing. In Proceedings Inter-
speech.
Okko R?as?anen. 2011. A computational model of word
segmentation from continuous speech using transi-
tional probabilities of atomic acoustic events. Cog-
nition, 120:149176.
Okko R?as?anen. 2012. Computational modeling of
phonetic and lexical learning in early language ac-
quisition: existing models and future directions.
Speech Communication, 54:975?997.
Matthias Rolf, Marc Hanheide, and Katharina J. Rohlf-
ing. 2009. Attention via synchrony. making use of
multimodal cues in social learning. IEEE Transac-
tions on Autonomous Mental Development, 1:55?67.
Lars Schillingmann, Britta Wrede, and Katharina J.
Rohlfing. 2009. A computational model of acous-
tic packaging. IEEE Transactions on Autonomous
Mental Development, 1:226?237.
Han Sloetjes and Peter Wittenburg. 2008. Annota-
tion by category: Elan and iso dcr. In Proceed-
ings of the International Conference on Language
Resources and Evaluation.
Johannes Wienke and Sebastian Wrede. 2011. A Mid-
dleware for Collaborative Research in Experimen-
tal Robotics. In IEEE/SICE International Sympo-
sium on System Integration (SII2011), Kyoto, Japan.
IEEE.
Johannes Wienke, David Klotz, and Sebastian Wrede.
2012. A Framework for the Acquisition of Multi-
modal Human-Robot Interaction Data Sets with a
Whole-System Perspective. In LREC Workshop on
Multimodal Corpora for Machine Learning: How
should multimodal corpora deal with the situation?,
Istanbul, Turkey.
Chen Yu, Linda B. Smith, and Alfredo F. Pereira. 2008.
Grounding word learning in multimodal sensorimo-
tor interaction. In Proceedings of the 30th Annual
Conference of the Cognitive Science Society.
Chen Yu. 2006. Learning syntax-semantics mappings
to bootstrap word learning. In Proceedings of the
28th Annual Conference of the Cognitive Science
Society (2006) Key: citeulike:5276016.
37
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 42?49,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
An Impact Analysis of Features in a Classification Approach to
Irony Detection in Product Reviews
Konstantin Buschmeier, Philipp Cimiano and Roman Klinger
Semantic Computing Group
Cognitive Interaction Technology ? Center of Excellence (CIT-EC)
Bielefeld University
33615 Bielefeld, Germany
kbuschme@techfak.uni-bielefeld.de
{rklinger,cimiano}@cit-ec.uni-bielefeld.de
Abstract
Irony is an important device in human com-
munication, both in everyday spoken con-
versations as well as in written texts includ-
ing books, websites, chats, reviews, and
Twitter messages among others. Specific
cases of irony and sarcasm have been stud-
ied in different contexts but, to the best of
our knowledge, only recently the first pub-
licly available corpus including annotations
about whether a text is ironic or not has
been published by Filatova (2012). How-
ever, no baseline for classification of ironic
or sarcastic reviews has been provided.
With this paper, we aim at closing this gap.
We formulate the problem as a supervised
classification task and evaluate different
classifiers, reaching an F
1
-measure of up to
74 % using logistic regression. We analyze
the impact of a number of features which
have been proposed in previous research as
well as combinations of them.
1 Introduction
Irony is often understood as ?the use of words that
mean the opposite of what you really think espe-
cially in order to be funny? or ?a situation that
is strange or funny because things happen in a
way that seems to be the opposite? of what is ex-
pected.
1
Many dictionaries make this difference
between verbal irony and situational irony (British
Dictionary, 2014; New Oxford American Dictio-
nary, 2014; Merriam Webster Dictionary, 2014).
1
as defined in the Merriam Webster Dictionary
(2014), http://www.merriam-webster.com/
dictionary/irony
The German Duden (2014) mentions sarcasm as
synonym to irony, while the comprehension of sar-
casm as a special case of irony might be more
common. For instance, the Merriam Webster Dic-
tionary (2014) defines sarcasm as ?a sharp and
often satirical or ironic utterance designed to cut or
give pain?.
2
Irony is a frequent phenomenon within human
communication, occurring both in spoken and writ-
ten discourse including books, websites, fora, chats,
Twitter messages, Facebook posts, news articles
and product reviews. Even for humans it is some-
times difficult to recognize irony. Irony markers
are thus often used in human communication, sup-
porting the correct interpretation (Attardo, 2000).
The automatic identification of ironic formulations
in written text is a very challenging as well as im-
portant task as shown by the comment
3
?Read the book!?
which in the context of a movie review could be
regarded as ironic and as conveying the fact that the
film was far worse compared to the book. Another
example is taken from a review for the book ?Great
Expectations? by Charles Dickens:
4
?i would recomend this book to friends
who have insomnia or those who i abso-
lutely despise.?
The standard approach of recommending X implies
that X is worthwhile is clearly not valid in the given
context as the author is stating that she disliked the
book.
2
http://www.merriam-webster.com/
dictionary/sarcasm, accessed April 28, 2014
3
Example from Lee (2009).
4
http://www.amazon.com/review/
R86RAMEBZSB11, access date March 10, 2014
42
In real world applications of sentiment analysis,
large data sets are automatically classified into pos-
itive statements or negative statements and such
output is used to generate summaries of the sen-
timent about a product. In order to increase the
accurateness of such systems, ironic or sarcastic
statements need to be identified in order to infer
the actual communicative intention of the author.
In this paper, we are concerned with approaches
for the automatic detection of irony in texts, which
is an important task in a variety of applications,
including the automatic interpretation of text-based
chats, computer interaction or sentiment analysis
and opinion mining. In the latter case, the detec-
tion is of outmost importance in order to correctly
assign a polarity score to an aspect of a reviewed
product or a person mentioned in a Twitter mes-
sage. In addition, the automatic detection of irony
or sarcasm in text requires an operational definition
and has therefore the potential to contribute to a
deeper understanding of the linguistic properties
of irony and sarcasm as linguistic phenomena and
their corpus based evaluation and verification.
The rest of this paper is structured as follows:
We introduce the background and theories on irony
in Section 1.1 and discuss previous work in the area
of automatically recognizing irony in Section 1.2.
In the methods part in Section 2, we present our
set of features (Section 2.1) and the classifiers we
take into account (Section 2.2). In Section 3, we
discuss the data set used in this work in more detail
(Section 3.1), present our experimental setting (Sec-
tion 3.2) and show the evaluation of our approach
(Section 3.3). We conclude with a discussion and
summary (Section 4) and with an outlook on possi-
ble future work (Section 5).
1.1 Background
Irony is an important and frequent device in human
communication that is used to convey an attitude
or evaluation towards the propositional content of a
message, typically in a humorous fashion (Abrams,
1957, p. 165?168). Between the age of six (Nakas-
sis and Snedeker, 2002) and eight years (Creusere,
2007), children are able to recognize ironic utter-
ances or at least notice that something in the sit-
uation is not common (Glenwright and Pexman,
2007). The principle of inferability (Kreuz, 1996)
states that figurative language is used if the speaker
is confident that the addressee will interpret the
utterance and infer the communicative intention
of the speaker/author correctly. It has been shown
that irony is ubiquitous, with 8 % of the utterances
exchanged between interlocutors that are familiar
with each other being ironic (Gibbs, 2007).
Utsumi (1996) claim that an ironic utterance can
only occur in an ironic environment, whose pres-
ence the utterance implicitly communicates. Given
the formal definition it is possible to computation-
ally resolve if an utterance is ironic using first-order
predicate logic and situation calculus. Different the-
ories such as the echoic account (Wilson and Sper-
ber, 1992), the Pretense Theory (Clark and Gerrig,
1984) or the Allusional Pretense Theory (Kumon-
Nakamura et al., 1995) have challenged the un-
derstanding that an ironic utterance typically con-
veys the opposite of its literal propositional content.
However, in spite of the fact that the attributive
nature of irony is widely accepted (see Wilson and
Sperber (2012)), no formal or operational definition
of irony is available as of today.
1.2 Previous Work
Corpora providing annotations as to whether ex-
pressions are ironic or not are scarce. Kreuz and
Caucci (2007) have automatically generated such
a corpus exploiting Google Book search
5
. They
collected excerpts containing the phrase ?said sar-
castically?, removed that phrase and performed a
regression analysis on the remaining text, exploit-
ing the number of words as well as the occurrence
of adjectives, adverbs, interjections, exclamation
and question marks as features.
Tsur et al. (2010) present a system to identify
sarcasm in Amazon product reviews exploiting fea-
tures such as sentence length, punctuation marks,
the total number of completely capitalized words
and automatically generated patterns which are
based on the occurrence frequency of different
terms (following the approach by Davidov and
Rappoport (2006)). Unfortunately, their corpus
is not publicly available. Carvalho et al. (2009) use
eight patterns to identify ironic utterances in com-
ments on articles from a Portuguese online newspa-
per. These patterns contain positive predicates and
utilize punctuation, interjections, positive words,
emoticons, or onomatopoeia and acronyms for
laughing as well as some Portuguese-specific pat-
terns considering the verb-morphology. Gonz?alez-
Ib?a?nez et al. (2011) differentiate between sarcastic
and positive or negative Twitter messages. They
5
http://books.google.de/
43
exploit lexical features like unigrams, punctuation,
interjections and dictionary-based as well as prag-
matic features including references to other users
in addition to emoticons. Reyes et al. (2012) distin-
guish ironic and non-ironic Twitter messages based
on features at different levels of linguistic analysis
including quantifiers of sentence complexity, struc-
tural, morphosyntactic and semantic ambiguity, po-
larity, unexpectedness, and emotional activation,
imagery, and pleasantness of words. Tepperman
et al. (2006) performed experiments to recognize
sarcasm in spoken language, specifically in the ex-
pression ?yeah right?, using spectral, contextual
and prosodic cues. On the one hand, their results
show that it is possible to identify sarcasm based on
spectral and contextual features and, on the other
hand, they confirm that prosody is insufficient to
reliably detect sarcasm (Rockwell, 2005, p. 118).
Very recently, Filatova (2012) published a prod-
uct review corpus from Amazon, being annotated
with Amazon Mechanical Turk. It contains 437
ironic and 817 non-ironic reviews. A more de-
tailed description of this resource can be found in
Section 3.1. To our knowledge, no automatic classi-
fication approach has been evaluated on this corpus.
We therefore contribute a text classification system
including the previously mentioned features. Our
results serve as a strong baseline on this corpus as
well as an ?executable review? of previous work.
6
2 Methods
We model the task of irony detection as a super-
vised classification problem in which a review is
categorized as being ironic or non-ironic. We inves-
tigate different classifiers and focus on the impact
analysis of different features by investigating what
effect their elimination has on the performance of
the approach. In the following, we describe the
features used and the set of classifiers compared.
2.1 Features
To estimate if a review is ironic or not, we measure
a set of features. Following the idea that irony is
expressing the opposite of its literal content, we
take into account the imbalance between the over-
all (prior) polarity of words in the review and the
star-rating (as proposed by Davidov et al. (2010)).
We assume the imbalance to hold if the star-rating
6
The system as implemented to perform the described
experiments is made available at https://github.com/
kbuschme/irony-detection/
is positive (i. e., 4 or 5 stars) but the majority of
words is negative, and, vice versa, if the star-rating
is negative (i. e., 1 or 2 stars) but occurs with a
majority of positive words. We refer to this feature
as Imbalance. The polarity of words is determined
based on a dictionary consisting of about 6,800
words with their polarity (Hu and Liu, 2004).
7
The feature Hyperbole (Gibbs, 2007) indicates
the occurrence of a sequence of three positive or
negative words in a row. Similarly, the feature
Quotes indicates that up to two consecutive adjec-
tives or nouns in quotation marks have a positive
or negative polarity.
The feature Pos/Neg&Punctuation indicates that
a span of up to four words contains at least one
positive (negative) but no negative (positive) word
and ends with at least two exclamation marks or a
sequence of a question mark and an exclamation
mark (Carvalho et al., 2009). Analogously, the fea-
ture Pos/Neg&Ellipsis indicates that such a positive
or negative span ends with an ellipsis (?. . . ?). El-
lipsis and Punctuation indicates that an ellipsis is
followed by multiple exclamation marks or a com-
bination of an exclamation and a question mark.
The Punctuation feature conveys the presence of
an ellipses as well as multiple question or excla-
mation marks or a combination of the latter two.
The Interjection feature indicates the occurrence of
terms like ?wow? and ?huh?, and Laughter mea-
sures onomatopoeia (?haha?) as well as acronyms
for grin or laughter (?*g*?, ?lol?). In addition, the
feature Emoticon indicates the occurrence of an
emoticon. In order to capture a range of emotions,
it combines a variety of emoticons such as happy,
laughing, winking, surprised, dissatisfied, sad, cry-
ing, and sticking tongue out. In addition, we use
each occurring word as a feature (bag-of-words).
All together, we have 21,773 features. The num-
ber of specific features (i. e., without bag-of-words)
alone is 29.
2.2 Classifiers
In order to perform the classification based on the
features mentioned above, we explore a set of stan-
dard classifiers typically used in text classification
research. We employ the open source machine
learning library scikit-learn (Pedregosa et al., 2011)
for Python.
7
Note that examples can show that this is not always the
case. Funny or odd products ironically receive a positive star-
rating. However, this feature may be a strong indicator for
irony.
44
We use a support vector machine (SVM, Cortes
and Vapnik (1995)) with a linear kernel in the im-
plementation provided by libSVM (Fan et al., 2005;
Chang and Lin, 2011). The na??ve Bayes classifier is
employed with a multinomial prior (Zhang, 2004;
Manning et al., 2008). This classifier might suffer
from the issue of over-counting correlated features,
such that we compare it to the logistic regression
classifier as well (Yu et al., 2011).
Finally, we use a decision tree (Breiman et al.,
1984; Hastie et al., 2009) and a random forest clas-
sifier (Breiman, 2001).
3 Experiments and Results
3.1 Data Set
The data set by Filatova (2012) consists of 1,254
Amazon reviews, of which 437 are ironic, i. e.,
contain situational irony or verbal irony, and
817 are non-ironic. It has been acquired using
the crowd sourcing platform Amazon Mechanical
Turk
8
. Note that Filatova (2012) interprets sarcasm
as being verbal irony.
In a first step, the workers were asked to find
pairs of reviews on the same product so that one
of the reviews is ironic while the other one is not.
They were then asked to submit the ID of both
reviews, and, in the case of an ironic review, to
provide the fragment conveying the irony.
In a second step, each collected review was an-
notated by five additional workers and remained
in the corpus if three of the five new annotators
concurred with the initial category, i. e., ironic or
non-ironic. The corpus contains 21,744 distinct
tokens
9
, of which 5,336 occur exclusively in ironic
reviews, 9,468 exclusively in non-ironic reviews,
and the remaining 6,940 tokens occur in both ironic
and non-ironic reviews. Thus, all ironic reviews
comprise a total of 12,276 distinct tokens, whereas
a total of 16,408 distinct tokens constitute all non-
ironic reviews. On average, a single review consists
of 271.9 tokens, a single ironic review of an aver-
age of 261.4 and a single non-ironic review of an
average of 277.5 tokens. The distribution of ironic
and non-ironic reviews for the different star-ratings
is shown in Table 2. Note that this might be a result
of the specific annotation procedure applied by the
8
https://www.mturk.com/mturk/, accessed on
March 10, 2014
9
Using the TreeBankWordTokenizer as implemented in the
Natural Language Toolkit (NLTK) (http://www.nltk.
org/)
annotators to search for ironic reviews. Neverthe-
less, this motivates a simple baseline system which
just takes one feature into account: the numbers of
stars assigned to the respective review (?Star-rating
only?).
3.2 Experimental Settings
We run experiments for three baselines: The star-
rating baseline relies only on the number of stars
assigned in the review as a feature. The bag-of-
words baseline exploits only the unigrams in the
text as features. The sentiment word count only
uses the information whether the number of posi-
tive words in the text is larger than the number of
negative words.
We emphasize that the first baseline is only of
limited applicability as it requires the explicit avail-
ability of a star-rating. The second baseline relies
on standard text classification features that are not
specific for the task. The third baseline relies on a
classical feature used in sentiment analysis, but is
not specific for irony detection.
We refer to the feature set ?All? encompassing
all features described in Section 2.1, including bag-
of-words and the set ?Specific Features?.
In order to understand the impact of a specific
feature A, we run three sets of experiments:
? Using all features with the exception of A.
? Using all specific features with the exception
of A.
? Using A as the only feature.
In addition to evaluating each single feature as
described above, we evaluate the set of positive and
negative instantiations of features when using the
sentiment dictionary. The ?Positive set? and ?Neg-
ative set? take into account the respective subsets
of all specific features.
Each experiment is performed in a 10-fold cross-
validation setting on document level. We report
recall, precision and F
1
-measure for each of the
classifiers.
3.3 Evaluation
Table 1 shows the results for the three baselines and
different feature set combinations, all for the differ-
ent classifiers. The star-rating as a feature alone is a
very strong indicator for irony. However, this result
is of limited usefulness as it only regards reviews
of a specific rating as ironic, namely results with
45
Linear SVM Logistic Regression Decision Tree Random Forest Naive Bayes
Feature set R. P. F
1
R. P. F
1
R. P. F
1
R. P. F
1
R. P. F
1
Star-rating only 66.7 78.4 71.7 66.7 78.4 71.7 66.7 78.4 71.7 66.7 78.4 71.7 66.7 78.4 71.7
BOW only 61.8 67.2 64.1 63.3 76.0 68.8 53.8 53.4 53.4 21.7 70.4 32.9 48.1 77.4 59.1
Sentiment Word Count 57.3 59.4 58.1 57.3 59.4 58.1 57.3 59.4 58.1 57.3 59.4 58.1 0.0 100.0 0.0
All + Star-rating 69.0 74.4 71.3 68.9 81.7 74.4 71.7 73.2 72.2 34.0 85.0 48.2 55.3 79.7 65.0
All (= Sp. Features + BOW) 61.3 68.0 64.3 62.2 75.2 67.8 55.0 59.8 56.9 24.1 73.2 35.3 50.9 77.3 61.2
All ? Imbalance 62.4 67.1 64.4 62.5 75.0 67.9 53.0 54.3 53.3 22.3 75.9 33.8 47.8 75.8 58.4
All ? Hyperbole 61.3 68.0 64.3 62.2 75.2 67.8 57.1 61.5 58.9 22.3 79.6 34.4 50.9 77.3 61.2
All ? Quotes 61.3 68.0 64.3 62.8 75.1 68.2 57.2 61.7 59.1 25.9 76.8 38.5 50.6 77.0 60.9
All ? Pos/Neg&Punctuation 61.5 67.9 64.4 62.4 75.2 68.0 56.7 60.1 58.0 21.8 77.8 33.5 50.9 77.3 61.2
All ? Pos/Neg&Ellipsis 61.0 67.4 63.8 63.0 75.1 68.3 57.6 60.5 58.8 29.0 79.2 42.2 50.4 76.6 60.7
All ? Ellipsis and Punctuation 61.3 68.0 64.3 62.4 75.2 68.0 55.1 59.7 56.9 24.6 73.6 36.2 50.9 77.3 61.2
All ? Punctuation 61.8 67.9 64.5 62.5 74.9 67.8 56.1 61.2 58.3 28.6 78.1 41.5 50.2 76.7 60.6
All ? Injections 61.3 68.0 64.3 62.2 75.0 67.8 56.1 61.8 58.5 24.1 75.2 35.6 50.9 77.3 61.2
All ? Laughter 61.3 68.2 64.4 62.4 75.3 68.0 56.6 60.9 58.2 24.0 79.3 36.5 50.9 77.3 61.2
All ? Emoticons 61.3 68.2 64.4 62.6 75.3 68.1 57.7 60.2 58.6 24.3 76.5 36.7 50.9 77.3 61.2
All ? Negative set 61.0 68.0 64.1 62.3 74.7 67.7 59.0 61.1 59.7 25.4 76.8 37.6 50.2 76.6 60.5
All ? Positive set 62.6 67.3 64.6 62.5 75.7 68.2 53.7 55.1 54.2 20.5 67.7 31.1 47.8 75.8 58.4
Sp. Features 37.5 77.2 50.2 38.2 77.5 50.8 38.3 76.0 50.6 38.3 74.8 50.2 34.3 80.5 47.7
Sp. Features ? Imbalance 9.3 50.4 15.4 11.0 54.1 18.1 11.3 48.5 18.1 12.9 47.4 20.0 5.9 55.8 10.3
Sp. Features ? Hyperbole 37.5 77.4 50.3 38.2 77.5 50.8 38.3 76.7 50.7 38.8 76.4 51.2 34.3 80.9 47.8
Sp. Features ? Quotes 37.7 76.9 50.3 38.0 78.1 50.7 37.8 75.6 50.1 38.3 73.6 50.0 34.3 80.5 47.7
Sp. Features ? Pos/Neg&Punctuation 37.7 77.9 50.5 37.8 77.6 50.5 37.1 74.5 49.2 38.2 73.8 49.9 33.3 80.2 46.7
Sp. Features ? Pos/Neg&Ellipsis 37.7 77.3 50.4 38.1 78.2 50.9 37.9 76.2 50.4 39.1 72.3 50.3 34.5 79.7 47.8
Sp. Features ? Ellipsis and Punctuation 37.8 76.9 50.3 37.8 76.9 50.3 38.3 75.8 50.6 39.0 72.5 50.5 34.5 80.2 47.9
Sp. Features ? Punctuation 37.1 79.7 50.3 37.6 78.7 50.6 37.0 76.7 49.6 38.4 75.4 50.5 32.6 78.9 45.6
Sp. Features ? Interjections 37.7 76.9 50.3 37.9 77.5 50.6 38.1 76.1 50.4 38.7 75.2 50.7 34.3 80.5 47.7
Sp. Features ? Laughter 37.8 77.3 50.5 38.0 77.7 50.7 37.3 75.5 49.6 37.5 73.4 49.4 34.5 81.2 48.0
Sp. Features ? Emoticons 37.3 78.2 50.2 38.2 77.5 50.8 38.0 75.4 50.2 38.7 75.0 50.7 33.4 80.7 46.8
Sp. Features ? Positive set 10.5 48.7 17.1 11.0 56.3 18.1 9.9 49.3 16.3 12.3 50.8 19.5 6.3 64.8 11.0
Sp. Features ? Negative set 37.7 78.2 50.6 38.0 78.7 50.9 38.2 75.1 50.3 37.6 72.0 48.9 34.9 79.8 48.3
Imbalance only 36.9 81.4 50.4 36.9 81.4 50.4 36.9 81.4 50.4 36.9 81.4 50.4 0.0 100.0 0.0
Hyperbole only 0.0 80.0 0.0 0.0 90.0 0.0 0.0 80.0 0.0 0.2 55.0 0.4 0.0 100.0 0.0
Quotes only 3.9 45.5 7.0 0.9 67.0 1.7 4.0 43.8 7.0 2.5 52.2 4.5 0.0 100.0 0.0
Pos/Neg&Punctuation only 0.9 90.0 1.8 0.5 90.0 0.9 0.0 90.0 0.0 0.4 90.0 0.8 0.9 90.0 1.8
Pos/Neg&Ellipsis only 6.8 59.0 12.1 6.8 59.0 12.1 6.8 59.0 12.1 6.8 59.0 12.1 0.0 100.0 0.0
Ellipsis and Punctuation only 0.9 90.0 1.7 0.4 90.0 0.8 0.9 90.0 1.7 0.9 90.0 1.7 0.0 100.0 0.0
Punctuation only 5.4 64.6 9.8 5.4 64.6 9.8 3.3 60.8 6.2 4.0 60.8 7.5 4.7 64.6 8.6
Interjections only 0.5 75.8 0.9 0.3 82.5 0.5 0.5 75.8 0.9 1.4 74.2 2.7 0.0 100.0 0.0
Laughter only 0.0 100.0 0.0 0.0 100.0 0.0 0.0 100.0 0.0 0.0 80.0 0.0 0.0 100.0 0.0
Emoticons only 0.0 100.0 0.0 0.0 100.0 0.0 0.0 100.0 0.0 0.0 100.0 0.0 0.0 80.0 0.0
Positive set only 36.9 81.4 50.4 36.9 81.1 50.4 37.1 80.5 50.5 37.3 79.3 50.5 32.4 80.7 45.6
Negative set only 8.2 54.5 14.1 7.3 48.8 12.5 8.8 49.4 14.8 9.0 49.9 15.2 0.0 80.0 0.0
Table 1: Comparison of different classification methods using different feature sets. ?All? refers to the
features described in Section 2 including bag-of-words (?BOW?). ?Sp. Features? are ?All? without
?BOW?.
a positive rating by the author, as explained by Ta-
ble 2, which shows the more real-world compatible
result of a rich feature set in addition. Obviously,
the depicted distribution is very similar to the dis-
tribution of the manually annotated data set, which
can obviously not be achieved by the star-rating
feature alone.
The best result is achieved by using the star-
rating together with bag-of-words and specific fea-
tures with a logistic regression approach (leading
to an F
1
-measure of 74 %). The SVM and decision
tree have a comparable performance on the task,
which is albeit lower compared to the performance
of the logistic regression approach.
Using the task-agnostic pure bag-of-words ap-
proach leads to a performance of 68.8 % for logistic
regression; this classifier has the property of deal-
ing well with correlated features and the additional
specific features cannot contribute positively to the
result. Similarly, the F
1
-measure of 64.1 % pro-
duced by the SVM cannot be increased by includ-
ing additional features. In contrast, a positive im-
pact of additional features can be observed for the
decision tree in the case that specific features are
combined with bag-of-word-based features, reach-
ing close to 59 % F
1
in comparison to 53.4 % F
1
for bag-of-words alone.
It would be desirable to have a model only or
mainly based on the problem-specific features, as
this leads to a much more compact and therefore ef-
46
ficient representation than taking all words into ac-
count. In addition, the model would be easier to un-
derstand. By exploiting task-specific features alone,
the performance reaches at most an F
1
-measure of
50.9 %, which shows that task-agnostic features
such as unigram features are needed. A significant
drop in performance when leaving out a feature
or feature set can be observed for the Imbalance
feature and the Positive set. Both these feature sets
take into account the star-rating.
The task-specific features alone yield high preci-
sion results at the expense of a very low recall. This
clearly shows that task-specific features should
be used with standard, task-independent features
(the bag-of-words). The most helpful task-specific
features are: Imbalance, Positive set, Quotes and
Pos/Neg&Ellipses.
4 Discussion and Summary
The best performance is achieved with very corpus-
specific features taking into account meta-data
from Amazon, namely the product rating of the
reviewer. This leads to an F
1
-measure of 74 %.
However, we could not show a competitive perfor-
mance with more problem-specific features (lead-
ing to 51 % F
1
) or in combination with bag-of-
word-based features (leading to 68 % F
1
).
The baseline only predicting based on the star-
rating itself is highly competitive, however, not
applicable to texts without meta-data and of lim-
ited use due to its naturally highly biased outcome
towards positive reviews being non-ironic and neg-
ative reviews being ironic. Our results show that
the best results are achieved via meta-data and it re-
mains an open research task to develop comparably
good approaches only based on text features.
It should be noted that the corpus used in this
Distribution
Corpus Predicted
Rating ironic non-ironic ironic non-ironic
5 114 605 126 593
4 14 96 17 93
3 20 35 14 41
2 27 17 17 27
1 262 64 192 134
1?5 437 817 366 888
Table 2: Frequencies for the different star-ratings
of a review, as annotated, and according to the
logistic regression classifier with the feature set
?All ? Imbalance?.
work is not a random sample from all reviews avail-
able in a specific group of products. We actually
assume ironic reviews to be much more sparse
when sampling equally distributed. The evaluation
should be seen from the angle of the application
scenario: For instance, in a discovery setting in
which the task is to retrieve examples for ironic
reviews, a highly precise system would be desir-
able. In a setting in which only a small number
of reviews should be used for opinion mining, the
polarity of a text would be discovered taking the
classifier?s result into account ? therefore a sys-
tem with high precision and high recall would be
needed.
5 Future Work
As discussed at the end of the last section, a study
on the distribution of irony in the entirety of avail-
able reviews is needed to better shape the structure
and characteristics of an irony or sarcasm detection
system. This could be approached by perform-
ing a random sample from reviews and annotation,
though this would lead to a substantial amount of
annotation work in comparison to the directed se-
lection procedure used in the corpus by Filatova
(2012).
Future research should focus on the development
of approaches analyzing the vocabulary used in the
review in a deeper fashion. Our impression is that
many sarcastic and ironic reviews use words and
phrases which are non-typical for the specific do-
main or product class. Such out-of-domain vocabu-
lary can be detected with text similarity approaches.
Preliminary experiments taking into account the av-
erage cosine similarity of a review to be classified
to a large set of reviews from the same product class
have been of limited success. We propose that fu-
ture research should focus on analyzing the specific
vocabulary and develop semantic similarity mea-
sures which we assume to be more promising than
approaches taking into account lexical approaches
only.
Most work has been performed on text sets from
one source like Twitter, books, reviews, etc. Some
of the proposed features mentioned in this paper
or previous publications are probably transferable
between text sources. However, this still needs
to be proven and further development might be
necessary to actually provide automated domain
adaption for the area of irony and sarcasm detection.
We assume that not only the vocabulary changes
47
(as known in other domain adaptation tasks) but
actually the linguistic structure might change.
Finally, it should be noted that the corpus is actu-
ally a mixture of ironic and sarcastic reviews. Irony
and sarcasm are not fully exchangeable and can be
assumed to have different properties. Further inves-
tigations and analyses regarding the characteristics
that can be transferred are necessary.
Acknowledgements
Roman Klinger has been funded by the ?It?s
OWL? project (?Intelligent Technical Systems
Ostwestfalen-Lippe?, http://www.its-owl.
de/), a leading-edge cluster of the German Min-
istry of Education and Research. We thank the
reviewers for their valuable comments. We thank
Christina Unger for proof-reading the manuscript
and helpful comments.
References
Meyer Howard Abrams. 1957. A Glossary of Literary
Terms. Cengage Learning Emea, 9th edition.
Salvatore Attardo. 2000. Irony markers and functions:
Towards a goal-oriented theory of irony and its pro-
cessing. Rask: Internationalt Tidsskrift for Sprog og
Kommunikation, 12:3?20.
Leo Breiman, Jerome H. Friedman, Richard A. Olshen,
and Charles J. Stone. 1984. Classification and Re-
gression Trees. Wadsworth, Belmont, California.
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5?32.
British Dictionary. 2014. MacMillan Publishers. On-
line: http://www.macmillandictionary.
com/dictionary/british/irony. ac-
cessed April 28, 2014.
Paula Carvalho, Lu??s Sarmento, M?ario J. Silva, and
Eug?enio de Oliveira. 2009. Clues for detecting
irony in user-generated contents: oh. . . !! it?s ?so
easy? ;-). In Proceedings of the 1st international
CIKM workshop on Topic-sentiment analysis for
mass opinion, TSA ?09, pages 53?56, New York,
NY, USA. ACM.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology, 2(3).
Herbert H. Clark and Richard J. Gerrig. 1984. On the
pretense theory of irony. Journal of Experimental
Psychology: General, 113(1):121?126.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?
297.
Marlena A. Creusere. 2007. A developmental test
of theoretical perspective on the understanding of
verbal irony: Children?s recognition of allusion and
pragmatic insincerity. In Raymond W. Jr. Gibbs and
Herbert L. Colston, editors, Irony in Language and
Thought: A Cognitive Science Reader, chapter 18,
pages 409?424. Lawrence Erlbaum Associates, 1st
edition.
Dmitry Davidov and Ari Rappoport. 2006. Efficient
unsupervised discovery of word categories using
symmetric patterns and high frequency words. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 297?304, Sydney, Australia, July. Association
for Computational Linguistics.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the
Fourteenth Conference on Computational Natural
Language Learning, CoNLL ?10, pages 107?116,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Duden. 2014. Duden Verlag. Online: http://www.
duden.de/rechtschreibung/Ironie. ac-
cessed April 28, 2014.
Rong-En Fan, Pai-Hsuen Chen, and Chih-Jen Lin.
2005. Working set selection using second order
information for training support vector machines.
Jounral of Machine Learning Reasearch, 6:1889?
1918.
Elena Filatova. 2012. Irony and sarcasm: Corpus gen-
eration and analysis using crowdsourcing. In Nico-
letta Calzolari, Khalid Choukri, Thierry Declerck,
Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eighth International Conference on
Language Resources and Evaluation (LREC-2012),
pages 392?398, Istanbul, Turkey, May. European
Language Resources Association (ELRA).
Raymond W. Jr. Gibbs. 2007. Irony in talk among
friends. In Raymond W. Jr. Gibbs and Herbert L.
Colston, editors, Irony in Language and Thought:
A Cognitive Science Reader, chapter 15, pages
339?360. Lawrence Erlbaum Associates, 1st edition,
May.
Melanie Harris Glenwright and Penny M. Pexman.
2007. Children?s perceptions of the social func-
tions of verbal irony. In Raymond W. Jr. Gibbs and
Herbert L. Colston, editors, Irony in Language and
Thought: A Cognitive Science Reader, chapter 20,
pages 447?464. Lawrence Erlbaum Associates, 1st
edition.
Roberto Gonz?alez-Ib?a?nez, Smaranda Muresan, and
Nina Wacholder. 2011. Identifying sarcasm in twit-
ter: a closer look. In Proceedings of the 49th Annual
48
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: short pa-
pers - Volume 2, HLT ?11, pages 581?586, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Trevor Hastie, Robert Tibshirani, and Jerome H. Fried-
man. 2009. Elements of Statistical Learning.
Springer, 2nd edition.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Roger J. Kreuz and Gina M. Caucci. 2007. Lexical
influences on the perception of sarcasm. In Proceed-
ings of the Workshop on Computational Approaches
to Figurative Language, pages 1?4, Rochester, New
York, April. Association for Computational Linguis-
tics.
Roger J. Kreuz. 1996. The use of verbal irony:
Cues and constraints. In Jeffery S. Mio and Al-
bert N. Katz, editors, Metaphor: Implications and
Applications, pages 23?38, Mahwah, NJ, October.
Lawrence Erlbaum Associates.
Sachi Kumon-Nakamura, Sam Glucksberg, and Mary
Brown. 1995. How about another piece of pie: The
allusional pretense theory of discourse irony. Jour-
nal of Experimental Psychology: General, 124(1):3?
21, Mar 01. Last updated - 2013-02-23.
Lillian Lee. 2009. A tempest or, on the flood of inter-
est in: sentiment analysis, opinion mining, and the
computational treatment of subjective language. Tu-
torial at ICWSM, May.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to Information
Retrieval. Cambridge University Press.
Merriam Webster Dictionary. 2014. Merriam Webster
Inc. Online: www.merriam-webster.com/
dictionary/irony. accessed April 28, 2014.
Constantine Nakassis and Jesse Snedeker. 2002. Be-
yond sarcasm: Intonation and context as relational
cues in children?s recognition of irony. In A. Green-
hill, M. Hughs, H. Littlefield, and H. Walsh, editors,
Proceedings of the Twenty-sixth Boston University
Conference on Language Development, Somerville,
MA, July. Cascadilla Press.
New Oxford American Dictionary. 2014. Ox-
ford University Press. Online: http:
//www.oxforddictionaries.com/us/
definition/american_english/ironic.
accessed April 28, 2014.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and
?
Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Antonio Reyes, Paolo Rosso, and Davide Buscaldi.
2012. From humor recognition to irony detection:
The figurative language of social media. Data &
Knowledge Engineering, 74:1?12.
Patricia Rockwell. 2005. Sarcasm on television talk
shows: Determining speaker intent through verbal
and nonverbal cues. In Anita V. Clark, editor, Psy-
chology of Moods, chapter 6, pages 109?122. Nova
Science Pubishers Inc.
Joseph Tepperman, David Traum, and Shrikanth S.
Narayanan. 2006. ?yeah right?: Sarcasm recogni-
tion for spoken dialogue systems. In Proceedings
of InterSpeech, pages 1838?1841, Pittsburgh, PA,
September.
Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010.
ICWSM ? A Great Catchy Name: Semi-Supervised
Recognition of Sarcastic Sentences in Online Prod-
uct Reviews. In Proceedings of the Fourth Interna-
tional AAAI Conference on Weblogs and Social Me-
dia, pages 162?169. The AAAI Press.
Akira Utsumi. 1996. A unified theory of irony and its
computational formalization. In Proceedings of the
16th conference on Computational linguistics - Vol-
ume 2, COLING ?96, pages 962?967, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Deirdre Wilson and Dan Sperber. 1992. On verbal
irony. Lingua, 87:53?76.
Deirdre Wilson and Dan Sperber, 2012. Explaining
Irony, chapter 6, pages 123?145. Cambridge Uni-
versity Press, 1st edition, April.
Hsiang-Fu. Yu, Fang-Lan Huang, and Chih-Jen Lin.
2011. Dual coordinate descent methods for logistic
regression and maximum entropy. Machine Learn-
ing, 85(1?2):41?75, October.
Harry Zhang. 2004. The optimality of naive bayes. In
Valerie Barr and Zdravko Markov, editors, Proceed-
ings of the Seventeenth International Florida Artifi-
cial Intelligence Research Society (FLAIRS) Confer-
ence, pages 3?9. AAAI Press.
49
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 118?127,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Towards Gene Recognition from Rare and
Ambiguous Abbreviations using a Filtering Approach
Matthias Hartung
?
, Roman Klinger
?
, Matthias Zwick
?
and Philipp Cimiano
?
?
Semantic Computing Group
Cognitive Interaction Technology ?
Center of Excellence (CIT-EC)
Bielefeld University
33615 Bielefeld, Germany
{mhartung,rklinger,cimiano}
@cit-ec.uni-bielefeld.de
?
Research Networking
Boehringer Ingelheim Pharma GmbH
Birkendorfer Str. 65
88397 Biberach, Germany
matthias.zwick
@boehringer-ingelheim.com
Abstract
Retrieving information about highly am-
biguous gene/protein homonyms is a chal-
lenge, in particular where their non-protein
meanings are more frequent than their pro-
tein meaning (e. g., SAH or HF). Due to
their limited coverage in common bench-
marking data sets, the performance of exist-
ing gene/protein recognition tools on these
problematic cases is hard to assess.
We uniformly sample a corpus of eight am-
biguous gene/protein abbreviations from
MEDLINEr and provide manual annota-
tions for each mention of these abbrevia-
tions.
1
Based on this resource, we show
that available gene recognition tools such
as conditional random fields (CRF) trained
on BioCreative 2 NER data or GNAT tend
to underperform on this phenomenon.
We propose to extend existing gene recog-
nition approaches by combining a CRF
and a support vector machine. In a cross-
entity evaluation and without taking any
entity-specific information into account,
our model achieves a gain of 6 points
F
1
-Measure over our best baseline which
checks for the occurrence of a long form
of the abbreviation and more than 9 points
over all existing tools investigated.
1 Introduction
In pharmaceutical research, a common task is to
gather all relevant information about a gene, e. g.,
from published articles or abstracts. The task of rec-
ognizing the mentions of genes or proteins can be
understood as the classification problem to decide
1
The annotated corpus is available for future research at
http://dx.doi.org/10.4119/unibi/2673424.
whether the entity of interest denotes a gene/protein
or something else. For highly ambiguous short
names, this task can be particularly challenging.
Consider, for instance, the gene acyl-CoA syn-
thetase medium-chain family member 3 which has
synonyms protein SA homolog or SA hypertension-
associated homolog, among others, with abbrevia-
tions ACSM3, and SAH.
2
Standard thesaurus-based
search engines would retrieve results where SAH
denotes the gene/protein of interest, but also oc-
currences in which it denotes other proteins (e. g.,
ATX1 antioxidant protein 1 homolog
3
) or entities
from semantic classes other than genes/proteins
(e. g., the symptom sub-arachnoid hemorrhage).
For an abbreviation such as SAH, the use as de-
noting a symptom or another semantic class dif-
ferent from genes/proteins is more frequent by a
factor of 70 compared to protein-denoting men-
tions according to our corpus analysis, such that
the retrieval precision for acyl-CoA synthetase by
the occurrence of the synonym SAH is only about
0.01, which is totally unacceptable for practical
applications.
In this paper, we discuss the specific challenge
of recognizing such highly ambiguous abbrevia-
tions. We consider eight entities and show that
common corpora for gene/protein recognition are
of limited value for their investigation. The abbre-
viations we consider are SAH, MOX, PLS, CLU,
CLI, HF, AHR and COPD (cf. Table 1). Based
on a sample from MEDLINE
4
, we show that these
names do actually occur in biomedical text, but
are underrepresented in corpora typically used for
benchmarking and developing gene/protein recog-
nition approaches.
2
http://www.ncbi.nlm.nih.gov/gene/6296
3
http://www.ncbi.nlm.nih.gov/gene/
443451
4
http://www.nlm.nih.gov/pubs/
factsheets/medline.html
118
Synonym Other names Other meaning EntrezGene ID
SAH acyl-CoA synthetase medium-chain family
member 3; ACSM3
subarachnoid hemorrhage;
S-Adenosyl-L-homocysteine hydrolase
6296
MOX monooxygenase, DBH-like 1 moxifloxacin; methylparaoxon 26002
PLS POLARIS partial least squares; primary lateral sclerosis 3770598
CLU clusterin; CLI covalent linkage unit 1191
CLI clusterin; CLU clindamycin 1191
HF complement factor H; CFH high frequency; heart failure; Hartree-Fock 3075
AHR aryl hydrocarbon receptor; bHLHe76 airway hyperreactivity 196
COPD archain 1; ARCN1; coatomer protein
complex, subunit delta
Chronic Obstructive Pulmonary Disease 22819; 372
Table 1: The eight synonyms for genes/proteins which are subject of analysis in this paper and their long
names together with frequent other meanings.
We propose a machine learning-based filtering
approach to detect whether a mention in question
actually denotes a gene/protein or not and show
that for the eight highly ambiguous abbreviations
that we consider, the performance of our approach
in terms of F
1
measure is higher than for a state-of-
the-art tagger based on conditional random fields
(CRF), a freely available dictionary-based approach
and an abbreviation resolver. We evaluate differ-
ent parameters and their impact in our filtering
approach and discuss the results. Note that this
approach does not take any information about the
specific abbreviation into account and can therefore
be expected to generalize to names not considered
in our corpus.
The main contributions of this paper are:
(i) We consider the problem of recognizing
highly ambiguous abbreviations that fre-
quently do not denote proteins as a task that
has so far attracted only limited attention.
(ii) We show that the recognition of such ambigu-
ous mentions is important as their string rep-
resentation is frequent in collections such as
MEDLINE.
(iii) We show, however, that this set of ambiguous
names is underrepresented in corpora com-
monly used for system design and develop-
ment. Such corpora do not provide a suffi-
cient data basis for studying the phenomenon
or for training systems that appropriately han-
dle such ambiguous abbreviation. We con-
tribute a manually annotated corpus of 2174
occurrences of ambiguous abbreviations.
(iv) We propose a filtering method for classifying
ambiguous abbreviations as denoting a pro-
tein or not. We show that this method has a
positive impact on the overall performance of
named entity recognition systems.
2 Related Work
The task of gene/protein recognition consists in
the classification of terms as actually denoting a
gene/protein or not. The task is typically either
tackled by using machine learning or dictionary-
based approaches. Machine learning approaches
rely on appropriate features describing the local
context of the term to be classified and induce a
model to perform the classification from training
data. Conditional random fields have shown to
yield very good results on the task (Klinger et al.,
2007; Leaman and Gonzalez, 2008; Kuo et al.,
2007; Settles, 2005).
Dictionary-based approaches rely on an explicit
dictionary of gene/protein names that are matched
in text. Such systems are common in practice due
to the low overhead required to adapt and maintain
the system, essentially only requiring to extend the
dictionary. Examples of commercial systems are
ProMiner (Fluck et al., 2007) or I2E (Bandy et al.,
2009); a popular free system is made available by
Hakenberg et al. (2011).
Such dictionary-based systems typically incorpo-
rate rules for filtering false positives. For instance,
in ProMiner (Hanisch et al., 2003), ambiguous syn-
onyms are only accepted based on external dictio-
naries and matches in the context. Abbreviations
are only accepted if a long form matches all parts of
the abbreviation in the context (following Schwartz
and Hearst (2003)). Similarly, Hakenberg et al.
(2008) discuss global disambiguation on the doc-
ument level, such that all mentions of a string in
one abstract are uniformly accepted as denoting an
entity or not.
A slightly different approach is taken by the web-
service GeneE
5
(Schuemie et al., 2010): Entering a
query as a gene/protein in the search field generates
5
http://biosemantics.org/geneE
119
MEDLINE BioCreative2 GENIA
Protein # Tokens % tagged # Tokens % of genes # Tokens % of genes
SAH 30019 6.1 % 2 0 % 0
MOX 16007 13.1 % 0 0
PLS 11918 25.9 % 0 0
CLU 1077 29.1 % 0 0
CLI 1957 4.8 % 4 0 % 0
HF 42563 7.9 % 8 62.5 % 4 0 %
AHR 21525 75.7 % 12 91.7 % 0
COPD 44125 0.6 % 6 0 % 0
Table 2: Coverage of ambiguous abbreviations in MEDLINE, BioCreative2 and GENIA corpora. The
percentage of tokens tagged as a gene/protein in MEDLINE (% tagged) is determined with a conditional
random field in the configuration described by Klinger et al. (2007), but without dictionary-based features
to foster the usage of contextual features). The percentages of genes/proteins (% of genes) in BC2 and
GENIA are based on the annotations in these corpora.
a query to e. g. PubMedr
6
with the goal to limit
the number of false positives.
Previous to the common application of CRFs,
other machine learning methods have been popu-
lar as well for the task of entity recognition. For
instance, Mitsumori et al. (2005) and Bickel et al.
(2004) use a support vector machine (SVM) with
part-of-speech information and dictionary-based
features, amongst others. Zhou et al. (2005) use an
ensemble of different classifiers for recognition.
In contrast to this application of a classifier
to solve the recognition task entirely, other ap-
proaches (including the one in this paper) aim at
filtering specifically ambiguous entities from a pre-
viously defined set of challenging terms. For in-
stance, Al-mubaid (2006) utilize a word-based clas-
sifier and a mutual information-based feature selec-
tion to achieve a highly discriminating list of terms
which is applied for filtering candidates.
Similarly to our approach, Tsuruoka and Tsujii
(2003) use a classifier, in their case a na??ve Bayes
approach, to learn which entities to filter from
the candidates generated by a dictionary-based ap-
proach. They use word based features in the con-
text including the candidate itself. Therefore, the
approach is focused on specific entities.
Gaudan et al. (2005) use an SVM and a dictio-
nary of long forms of abbreviations to assign them
a specific meaning, taking contextual information
into account. However, their machine learning ap-
proach is trained on each possible sense of an ab-
breviation. In contrast, our approach consists in
deciding if a term is used as a protein or not. Fur-
ther, we do not train to detect specific, previously
given senses.
6
http://www.ncbi.nlm.nih.gov/pubmed/
Xu et al. (2007) apply text similarity measures to
decide about specific meanings of mentions. They
focus on the disambiguation between different en-
tities. A corpus for word sense disambiguation is
automatically built based on MeSH annotations by
Jimeno-Yepes et al. (2011). Okazaki et al. (2010)
build a sense inventory by automatically applying
patterns on MEDLINE and use this in a logistic
regression approach.
Approaches are typically evaluated on freely
available resources like the BioCreative Gene Men-
tion Task Corpus, to which we refer as BC2 (Smith
et al., 2008), or the GENIA Corpus (Kim et al.,
2003). When it comes to identifying particular pro-
teins by linking the protein in question to some
protein in an external database ? a task we do
not address in this paper ? the BioCreative Gene
Normalization Task Corpus is a common resource
(Morgan et al., 2008).
In contrast to these previous approaches, our
method is not tailored to a particular set of entities
or meanings, as the training methodology abstracts
from specific entities. The model, in fact, knows
nothing about the abbreviations to be classified and
does not use their surface form as a feature, such
that it can be applied to any unseen gene/protein
term. This leads to a simpler model that is applica-
ble to a wide range of gene/protein term candidates.
Our cross-entity evaluation regime clearly corrobo-
rates this.
3 Data
We focus on eight ambiguous abbreviations of
gene/protein names. As shown in Table 2, these
homonyms occur relatively frequently in MEDLINE
but are underrepresented in the BioCreative 2 entity
120
Protein Pos. Inst. Neg. Inst. Total
SAH 5 349 354
MOX 62 221 283
PLS 1 206 207
CLU 235 30 265
CLI 11 211 222
HF 2 353 355
AHR 53 80 133
COPD 0 250 250
Table 3: Number of instances per protein in the
annotated data set and their positive/negative distri-
bution
recognition data set and the GENIA corpus which
are both commonly used for developing and evalu-
ating gene recognition approaches. We compiled
a corpus from MEDLINE by randomly sampling
100 abstracts for each of the eight abbreviations (81
for MOX) such that each abstract contains at least
one mention of the respective abbreviation. One
of the authors manually annotated the mentions
of the eight abbreviations under consideration to
be a gene/protein entity or not. These annotations
were validated by another author. Both annotators
disagreed in only 2% of the cases. The numbers
of annotations, including their distribution over
positive and negative instances, are summarized
in Table 3. The corpus is made publicly available
at http://dx.doi.org/10.4119/unibi/
2673424 (Hartung and Zwick, 2014).
In order to alleviate the imbalance of positive
and negative examples in the data, additional pos-
itive examples have been gathered by manually
searching PubMed
7
. At this point, special attention
has been paid to extract only instances denoting the
correct gene/protein corresponding to the full long
name, as we are interested in assessing the impact
of examples of a particularly high quality. This
process yields 69 additional instances for AHR
(distributed over 11 abstracts), 7 instances (3 ab-
stracts) for HF, 14 instances (2 abstracts) for PLS
and 15 instances (7 abstracts) for SAH. For the
other gene/proteins in our dataset, no additional
positive instances of this kind could be retrieved
using PubMed. In the following, this process will
be referred to as manual instance generation. This
additional data is used for training only.
7
http://www.ncbi.nlm.nih.gov/pubmed
4 Gene Recognition by Filtering
We frame gene/protein recognition from ambigu-
ous abbreviations as a filtering task in which a set
of candidate tokens is classified into entities and
non-entities. In this paper, we assume the candi-
dates to be generated by a simple dictionary-based
approach taking into account all tokens that match
the abbreviation under consideration.
4.1 Filtering Strategies
We consider the following filtering approaches:
? SVM classifies the occurring terms based on a
binary support vector machine.
? CRF classifies the occurring terms based on
a conditional random field (configured as de-
scribed by Klinger et al. (2007)) trained on the
concatenation of BC2 data and our newly gen-
erated corpus. This setting thus corresponds
to state-of-the-art performance on the task.
? CRF?SVM considers the candidate an entity
if both the standard CRF and the SVM from
the previous steps yield a positive prediction.
? HRCRF?SVM is the same as the previous
step, but the output of the CRF is optimized
towards high recall by joining the recognition
of entities of the five most likely Viterbi paths.
? CRF?SVM is similar to the first setting, but
the output of the CRF is taken into account as
a feature in the SVM.
4.2 Features for Classification
Our classifier uses local contextual and global fea-
tures. Local features focus on the immediate con-
text of an instance, whereas global features encode
abstract-level information. Throughout the follow-
ing discussion, t
i
denotes a token at position i that
corresponds to a particular abbreviation to be classi-
fied in an abstract A. Note that we blind the actual
representation of the entity to be able to generalize
to all genes/proteins, not being limited to the ones
contained in our corpus.
4.2.1 Local Information
The feature templates context-left and context-right
collect the tokens immediately surrounding an ab-
breviation in a window of size 6 (left) and 4 (right)
in a bag-of-words-like feature generation. Addi-
tionally, the two tokens from the immediate context
on each side are combined into bigrams.
The template abbreviation generates features if
t
i
occurs in brackets. It takes into account the min-
imal Levenshtein distance (ld, Levenshtein (1966))
121
between all long forms L of the abbreviation (as
retrieved from EntrezGene) in comparison to each
string on the left of t
i
(up to a length of seven,
denoted by t
k:i
as the concatenation of tokens
t
k
, . . . , t
i
). Therefore, the similarity value sim(t
i
)
taken into account is given by
sim(t
i
) = max
l?L;k?[1:7]
1?
ld(t
k:i?1
, l)
max(|t
i
|, |l|)
,
where the denominator is a normalization term.
The features used are generated by cumulative bin-
ning of sim(t
i
).
The feature tagger
local
takes the prediction of the
CRF for t
i
into account. Note that this feature is
only used in the CRF?SVM setting.
4.2.2 Global Information
The feature template unigrams considers each word
in A as a feature. There is no normalization or
frequency weighting. Stopwords are ignored
8
. Oc-
currences of the same string as t
i
are blinded.
The feature tagger
global
collects all tokens in A
other than t
i
that are tagged as an entity by the CRF.
In addition, the cardinality of these entities in A is
taken into account by cumulative binning.
The feature long form holds if one of the long
forms previously defined to correspond with the ab-
breviation occurs in the text (in arbitrary position).
Besides using all features, we perform a greedy
search for the best feature set by wrapping the best
model configuration. A detailed discussion of the
feature selection process follows in Section 5.3.
4.2.3 Feature Propagation
Inspired by the ?one sense per discourse? heuristic
commonly adopted in word sense disambiguation
(Gale et al., 1992), we apply two feature combi-
nation strategies. In the following, n denotes the
number of occurrences of the abbreviation in an
abstract.
In the setting propagation
all
, n ? 1 identical
linked instances are added for each occurrence.
Each new instance consists of the disjunction of
the feature vectors of all occurrences. Based on
the intuition that the first mention of an abbrevia-
tion might carry particularly valuable information,
propagation
first
introduces one additional linked in-
stance for each occurrence, in which the feature
vector is joined with the first occurrence.
8
Using the stopword list at http://www.ncbi.nlm.
nih.gov/books/NBK3827/table/pubmedhelp.
T43/, last accessed on March 25, 2014
Setting P R F
1
SVM 0.81 0.45 0.58
CRF?SVM 0.99 0.26 0.41
HRCRF?SVM 0.95 0.27 0.42
CRF?SVM 0.83 0.49 0.62
CRF?SVM+FS 0.97 0.74 0.84
GNAT 0.73 0.45 0.56
CRF 0.55 0.43 0.48
AcroTagger 0.92 0.63 0.75
Long form 0.98 0.65 0.78
lex 0.18 1.00 0.32
Table 4: Overall micro-averaged results over eight
genes/proteins. For comparison, we show the re-
sults of a default run of GNAT (Hakenberg et al.,
2011), a CRF trained on BC2 data (Klinger et al.,
2007), AcroTagger (Gaudan et al., 2005), and a
simple approach of accepting every token of the
respective string as a gene/protein entity (lex). Fea-
ture selection is denoted with +FS.
In both settings, all original and linked instances
are used for training, while during testing, original
instances are classified by majority voting on their
linked instances. For propagation
all
, this results in
classifying each occurrence identically.
5 Experimental Evaluation
5.1 Experimental Setting
We perform a cross-entity evaluation, in which we
train the support vector machine (SVM) on the ab-
stracts of 7 genes/proteins from our corpus and test
on the abstracts for the remaining entities, i. e., the
model is evaluated only on tokens representing en-
tities which have never been seen labeled during
training. The CRFs are trained analogously with
the difference that the respective set used for train-
ing is augmented with the BioCreative 2 Training
data. The average numbers of precision, recall and
F
1
measure are reported.
As a baseline, we report the results of a simple
lexicon-based approach assuming that all tokens
denote an entity in all their occurrences (lex). In ad-
dition, the baseline of accepting an abbreviation as
gene/protein if the long form occurs in the same ab-
stract is reported (Long form). Moreover, we com-
pare our results with the publicly available toolkit
GNAT (Hakenberg et al., 2011)
9
and the CRF ap-
9
The gene normalization functionality of GNAT is not
taken into account here. We acknowledge that this comparison
122
proach as described in Section 4. In addition, we
take into account the AcroTagger
10
that resolves
abbreviations to their most likely long form which
we manually map to denoting a gene/protein or not.
5.2 Results
5.2.1 Overall results
In Table 4, we summarize the results of the recogni-
tion strategies introduced in Section 4. The lexical
baseline clearly proves that a simple approach with-
out any filtering is not practical. GNAT adapts well
to ambiguous short names and turns out as a com-
petitive baseline, achieving an average precision of
0.73. In contrast, the filtering capacity of a stan-
dard CRF is, at best, mediocre. The long form
baseline is very competitive with an F
1
measure of
0.78 and a close-to-perfect precision. The results of
AcroTagger are similar to this long form baseline.
We observe that the SVM outperforms the CRF
in terms of precision and recall (by 10 percentage
points in F
1
). Despite not being fully satisfactory
either, these results indicate that global features
which are not implemented in the CRF are of im-
portance. This is confirmed by the CRF?SVM
setting, where CRF and SVM are stacked: This fil-
tering procedure achieves the best precision across
all models and baselines, whereas the recall is still
limited. Despite being designed for exactly this
purpose, the HRCRF?SVM combination can only
marginally alleviate this problem, and only at the
expense of a drop in precision.
The best trade-off between precision and recall
is offered by the CRF?SVM combination. This
setting is not only superior to all other variants of
combining a CRF with an SVM, but outperforms
GNAT by 6 points in F
1
score, while being inferior
to the long form baseline. However, performing
feature selection on this best model using a wrapper
approach (CRF?SVM+FS) leads to the overall
best result of F
1
= 0.84, outperforming all other
approaches and all baselines.
5.2.2 Individual results
Table 5 summarizes the performance of all filter-
ing strategies broken down into individual entities.
Best results are achieved for AHR, MOX and CLU.
COPD forms a special case as no examples for the
might be seen as slightly inappropriate as the focus of GNAT
is different.
10
ftp://ftp.ebi.ac.uk/pub/software/
textmining/abbreviation_resolution/, ac-
cessed April 23, 2014
occurrence as a gene/protein are in the data; how-
ever the results show that the system can handle
such a special distribution.
SVM and CRF are mostly outperformed by a
combination of both strategies (except for CLI and
HF), which shows that local and global features
are highly complementary in general. Complemen-
tary cases generally favor the CRF?SVM strategy,
except for PLS, where stacking is more effective.
In SAH, the pure CRF model is superior to all
combinations of CRF and SVM. Apparently, the
global information as contributed by the SVM is
less effective than local contextual features as avail-
able to the CRF in these cases. In SAH and CLI,
moreover, the best performance is obtained by the
AcroTagger.
5.2.3 Impact of instance generation
All results reported in Tables 4 and 5 refer to con-
figurations in which additional training instances
have been created by manual instance generation.
The impact of this method is analyzed in Table 6.
The first column reports the performance of our
models on the randomly sampled training data. In
order to obtain the results in the second column,
manual instance generation has been applied.
The results show that all our recognition mod-
els generally benefit from additional information
that helps to overcome the skewed class distribu-
tion of the training data. Despite their relatively
small quantity and uneven distribution across the
gene/protein classes, including additional exter-
nal instances yields a strong boost in all mod-
els. The largest difference is observed in SVM
(?F
1
= +0.2) and CRF?SVM (?F
1
= +0.16).
Importantly, these improvements include both pre-
cision and recall.
5.3 Feature Selection
The best feature set (cf. CRF?SVM+FS in Ta-
ble 4) is determined by a greedy search using a
wrapper approach on the best model configuration
CRF?SVM. The results are depicted in Table 7.
In each iteration, the table shows the best feature
set detected in the previous iteration and the results
for each individual feature when being added to
this set. In each step, the best individual feature
is kept for the next iteration. The feature analysis
starts from the long form feature as strong base-
line. The added features are, in that order, context,
tagger
global
, and propagation
all
.
Overall, feature selection yields a considerable
123
AHR CLI CLU COPD
Setting P R F
1
P R F
1
P R F
1
P R F
1
SVM 1.00 0.72 0.84 0.30 0.27 0.29 1.00 0.41 0.58 0.00 1.00 0.00
CRF?SVM 1.00 0.70 0.82 0.00 0.00 0.00 1.00 0.15 0.26 1.00 1.00 1.00
HRCRF?SVM 1.00 0.70 0.82 1.00 0.00 0.00 1.00 0.16 0.28 1.00 1.00 1.00
CRF?SVM 0.96 0.83 0.89 0.30 0.27 0.29 1.00 0.40 0.57 0.00 1.00 0.00
CRF?SVM+FS 0.93 0.98 0.95 0.50 0.09 0.15 0.99 0.84 0.91 1.00 1.00 1.00
GNAT 0.74 0.66 0.70 1.00 0.18 0.31 0.97 0.52 0.68 1.00 1.00 1.00
CRF 0.52 0.98 0.68 0.00 0.00 0.00 1.00 0.20 0.33 0.00 1.00 0.00
AcroTagger 1.00 0.60 0.75 1.00 0.82 0.90 1.00 0.00 0.00 1.00 1.00 1.00
Long form 1.00 0.96 0.98 1.00 0.09 0.17 0.99 0.80 0.88 1.00 1.00 1.00
lex 0.40 1.00 0.57 0.05 1.00 0.09 0.89 1.00 0.94 0.00 1.00 0.00
HF MOX PLS SAH
Setting P R F
1
P R F
1
P R F
1
P R F
1
SVM 0.25 1.00 0.40 0.87 0.44 0.58 0.14 1.00 0.25 0.00 0.00 0.00
CRF?SVM 1.00 0.00 0.00 1.00 0.39 0.56 1.00 1.00 1.00 1.00 0.00 0.00
HRCRF?SVM 1.00 0.00 0.00 1.00 0.39 0.56 0.20 1.00 0.33 1.00 0.00 0.00
CRF?SVM 0.25 1.00 0.40 0.91 0.63 0.74 0.50 1.00 0.67 1.00 0.00 0.00
CRF?SVM+FS 1.00 0.00 0.00 1.00 0.37 0.54 0.00 0.00 0.00 1.00 0.00 0.00
GNAT 1.00 0.00 0.00 0.38 0.08 0.14 0.00 0.00 0.00 0.00 0.00 0.0
CRF 0.00 0.00 0.00 0.43 0.90 0.59 0.14 1.00 0.25 1.00 0.50 0.67
AcroTagger 0.33 1.00 0.50 1.00 0.00 0.00 1.00 0.00 0.00 1.00 0.60 0.75
Long form 1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00
lex 0.01 1.00 0.02 0.22 1.00 0.36 0.00 1.00 0.01 0.01 1.00 0.03
Table 5: Results for the eight genes/proteins and results for our different recognition schemes.
randomly sampled +instance generation
P R F
1
?P ?R ?F
1
SVM 0.73 0.25 0.38 +0.08 +0.20 +0.20
CRF?SVM 1.00 0.17 0.29 -0.01 +0.09 +0.13
HRCRF?SVM 0.97 0.18 0.30 -0.02 +0.09 +0.12
CRF?SVM 0.79 0.32 0.46 +0.05 +0.17 +0.16
CRF?SVM+FS 0.99 0.60 0.75 -0.02 +0.14 +0.09
Table 6: Impact of increasing the randomly sampled training set by adding manually curated additional
positive instances (+instance generation), measured in terms of the increase in precision, recall and F
1
(?P, ?R, ?F
1
).
boost in recall, while precision remains almost con-
stant. Surprisingly, the unigrams feature has a par-
ticularly strong negative impact on overall perfor-
mance.
While the global information contributed by the
CRF turns out very valuable, accounting for most
of the improvement in recall, local tagger informa-
tion is widely superseded by other features. Like-
wise, the abbreviation feature does not provide any
added value to the model beyond what is known
from the long form feature.
Comparing the different feature propagation
strategies, we observe that propagation
all
outper-
forms propagation
first
.
5.4 Discussion
Our experiments show that the phenomena inves-
tigated pose a challenge to all gene recognition
paradigms currently available in the literature, i. e.,
dictionary-based, machine-learning-based (e. g. us-
ing a CRF), and classification-based filtering.
Our results indicate that stacking different meth-
ods suffers from a low recall in early steps of the
workflow. Instead, a greedy approach that consid-
ers all occurrences of an abbreviation as input to
a filtering approach yields the best performance.
Incorporating information from a CRF as features
into a SVM outperforms all baselines at very high
levels of precision; however, the recall still leaves
room for improvement.
124
Iter. Feature Set P R F
1
?F
1
1 long form 0.98 0.65 0.78
+propagation
1
st
0.98 0.65 0.78 +0.00
+propagation
all
0.98 0.65 0.78 +0.00
+tagger
local
0.72 0.81 0.76 -0.02
+tagger
global
0.55 0.79 0.65 -0.13
+context 0.98 0.67 0.79 +0.01
+abbreviation 0.98 0.65 0.78 +0.00
+unigrams 0.71 0.43 0.53 -0.25
2 long form
+context 0.98 0.67 0.79
+propagation
1
st
0.98 0.67 0.79 +0.00
+propagation
all
0.96 0.70 0.81 +0.02
+tagger
local
0.98 0.70 0.82 +0.03
+tagger
global
0.97 0.72 0.83 +0.04
+abbreviation 0.98 0.67 0.80 +0.01
+unigrams 0.77 0.39 0.52 -0.27
3 long form
+context
+tagger
global
0.97 0.72 0.83
+propagation
1
st
0.97 0.71 0.82 -0.01
+propagation
all
0.97 0.74 0.84 +0.01
+tagger
local
0.97 0.72 0.82 -0.01
+abbreviation 0.97 0.72 0.82 -0.01
+unigrams 0.77 0.44 0.56 -0.27
4 long form
+context
+tagger
global
+propagation
all
0.97 0.74 0.84
+tagger
local
0.90 0.66 0.76 -0.08
+abbreviation 0.97 0.74 0.84 -0.00
+unigrams 0.80 0.49 0.61 -0.23
Table 7: Greedy search for best feature combina-
tion in CRF?SVM (incl. additional positives).
In a feature selection study, we were able to show
a largely positive overall impact of features that
extend local contextual information as commonly
applied by state-of-the-art CRF approaches. This
ranges from larger context windows for collecting
contextual information over abstract-level features
to feature propagation strategies. However, feature
selection is not equally effective in all individual
classes (cf. Table 5).
The benefits due to feature propagation indi-
cate that several instances of the same abbreviation
in one abstract should not be considered indepen-
dently of one another, although we could not verify
the intuition that the first mention of an abbrevia-
tion introduces particularly valuable information
for classification.
Overall, our results seem encouraging as the ma-
chinery and the features used are in general suc-
cessful in determining whether an abbreviation ac-
tually denotes a gene/protein or not. The best pre-
cision/recall balance is obtained by adding CRF
information as features into the classifier.
As we have shown in the cross-entity experi-
ment setting, the system is capable of generalizing
to other unseen entities. For a productive system,
we assume our workflow to be applied to specific
abbreviations such that the performance on other
entities (and therefore on other corpora) is not sub-
stantially influenced.
6 Conclusions and Outlook
The work reported in this paper was motivated from
the practical need for an effective filtering method
for recognizing genes/proteins from highly ambigu-
ous abbreviations. To the best of our knowledge,
this is the first approach to tackle gene/protein
recognition from ambiguous abbreviations in a
systematic manner without being specific for the
particular instances of ambiguous gene/protein
homonyms considered.
The proposed method has been proven to allow
for an improvement in recognition performance
when added to an existing NER workflow. Despite
being restricted to eight entities so far, our approach
has been evaluated in a strict cross-entity manner,
which suggests sufficient generalization power to
be extended to other genes as well.
In future work, we plan to extend the data set
to prove the generalizability on a larger scale and
on an independent test set. Furthermore, an inclu-
sion of the features presented in this paper into the
CRF will be evaluated. Moreover, assessing the
impact of the global features that turned out benefi-
cial in this paper on other gene/protein inventories
seems an interesting path to explore. Finally, we
will investigate the prospects of our approach in an
actual black-box evaluation setting for information
retrieval.
Acknowledgements
Roman Klinger has been funded by the ?It?s
OWL? project (?Intelligent Technical Systems
Ostwestfalen-Lippe?, http://www.its-owl.
de/), a leading-edge cluster of the German Min-
istry of Education and Research. We thank J?org
Hakenberg and Philippe Thomas for their support
in performing the baseline results with GNAT. Ad-
ditionally, we thank the reviewers of this paper for
their very helpful comments.
125
References
Hisham Al-mubaid. 2006. Biomedical term disam-
biguation: An application to gene-protein name dis-
ambiguation. In In IEEE Proceedings of ITNG06.
Judith Bandy, David Milward, and Sarah McQuay.
2009. Mining protein-protein interactions from pub-
lished literature using linguamatics i2e. Methods
Mol Biol, 563:3?13.
Steffen Bickel, Ulf Brefeld, Lukas Faulstich, J?org Hak-
enberg, Ulf Leser, Conrad Plake, and Tobias Schef-
fer. 2004. A support vector machine classifier for
gene name recognition. In In Proceedings of the
EMBO Workshop: A Critical Assessment of Text
Mining Methods in Molecular Biology.
Juliane Fluck, Heinz Theodor Mevissen, Marius Os-
ter, and Martin Hofmann-Apitius. 2007. ProMiner:
Recognition of Human Gene and Protein Names
using regularly updated Dictionaries. In Proceed-
ings of the Second BioCreative Challenge Evalua-
tion Workshop, pages 149?151, Madrid, Spain.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the Workshop on Speech and Natural
Language, pages 233?237, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Sylvain Gaudan, Harald Kirsch, and Dietrich Rebholz-
Schuhmann. 2005. Resolving abbreviations to their
senses in medline. Bioinformatics, 21(18):3658?
3664.
J?org Hakenberg, Conrad Plake, Robert Leaman,
Michael Schroeder, and Graciela Gonzalez. 2008.
Inter-species normalization of gene mentions with
GNAT. Bioinformatics, 24(16):i126?i132, Aug.
J?org Hakenberg, Martin Gerner, Maximilian Haeus-
sler, Ills Solt, Conrad Plake, Michael Schroeder,
Graciela Gonzalez, Goran Nenadic, and Casey M.
Bergman. 2011. The GNAT library for local and
remote gene mention normalization. Bioinformatics,
27(19):2769?2771, Oct.
Daniel Hanisch, Juliane Fluck, Heinz-Theodor Mevis-
sen, and Ralf Zimmer. 2003. Playing biology?s
name game: identifying protein names in scientific
text. Pac Symp Biocomput, pages 403?414.
Matthias Hartung and Matthias Zwick. 2014. A cor-
pus for the development of gene/protein recognition
from rare and ambiguous abbreviations. Bielefeld
University. doi:10.4119/unibi/2673424.
Antonio J Jimeno-Yepes, Bridget T McInnes, and
Alan R Aronson. 2011. Exploiting mesh indexing
in medline to generate a data set for word sense dis-
ambiguation. BMC bioinformatics, 12(1):223.
J-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. Ge-
nia corpus?semantically annotated corpus for bio-
textmining. Bioinformatics, 19 Suppl 1:i180?i182.
Roman Klinger, Christoph M. Friedrich, Juliane Fluck,
and Martin Hofmann-Apitius. 2007. Named
Entity Recognition with Combinations of Condi-
tional Random Fields. In Proceedings of the Sec-
ond BioCreative Challenge Evaluation Workshop,
Madrid, Spain, April.
Cheng-Ju Kuo, Yu-Ming Chang, Han-Shen Huang,
Kuan-Ting Lin, Bo-Hou Yang, Yu-Shi Lin, Chun-
Nan Hsu, and I-Fang Chung. 2007. Rich feature
set, unication of bidirectional parsing and dictionary
filtering for high f-score gene mention tagging. In
Proceedings of the Second BioCreative Challenge
Evaluation Workshop, Madrid, Spain, April.
Robert Leaman and Graciela Gonzalez. 2008. Ban-
ner: An executable survey of advances in biomed-
ical named entity recognition. In Russ B. Altman,
A. Keith Dunker, Lawrence Hunter, Tiffany Murray,
and Teri E. Klein, editors, Pacific Symposium on Bio-
computing, pages 652?663. World Scientific.
Vladimir I. Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions, and reversals. So-
viet Physics Doklady, 10:707?710.
Tomohiro Mitsumori, Sevrani Fation, Masaki Mu-
rata, Kouichi Doi, and Hirohumi Doi. 2005.
Gene/protein name recognition based on support
vector machine using dictionary as features. BMC
Bioinformatics, 6 Suppl 1:S8.
Alexander A. Morgan, Zhiyong Lu, Xinglong Wang,
Aaron M. Cohen, Juliane Fluck, Patrick Ruch, Anna
Divoli, Katrin Fundel, Robert Leaman, Jrg Haken-
berg, Chengjie Sun, Heng-hui Liu, Rafael Torres,
Michael Krauthammer, William W. Lau, Hongfang
Liu, Chun-Nan Hsu, Martijn Schuemie, K Bretonnel
Cohen, and Lynette Hirschman. 2008. Overview of
biocreative ii gene normalization. Genome Biol, 9
Suppl 2:S3.
Naoaki Okazaki, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2010. Building a high-quality sense inventory
for improved abbreviation disambiguation. Bioinfor-
matics, 26(9):1246?1253, May.
Martijn J. Schuemie, Ning Kang, Maarten L. Hekkel-
man, and Jan A. Kors. 2010. Genee: gene and pro-
tein query expansion with disambiguation. Bioinfor-
matics, 26(1):147?148, Jan.
Ariel S. Schwartz and Marti A. Hearst. 2003. A simple
algorithm for identifying abbreviation definitions in
biomedical text. Pac Symp Biocomput, pages 451?
462.
Burr Settles. 2005. Abner: an open source tool for au-
tomatically tagging genes, proteins and other entity
names in text. Bioinformatics, 21(14):3191?3192,
Jul.
Larry Smith, Lorraine K. Tanabe, Rie Johnson nee J.
Ando, Cheng-Ju J. Kuo, I-Fang F. Chung, Chun-
Nan N. Hsu, Yu-Shi S. Lin, Roman Klinger,
126
Christoph M. Friedrich, Kuzman Ganchev, Man-
abu Torii, Hongfang Liu, Barry Haddow, Craig A.
Struble, Richard J. Povinelli, Andreas Vlachos,
William A. Baumgartner, Lawrence Hunter, Bob
Carpenter, Richard Tzong-Han T. Tsai, Hong-Jie J.
Dai, Feng Liu, Yifei Chen, Chengjie Sun, Sophia Ka-
trenko, Pieter Adriaans, Christian Blaschke, Rafael
Torres, Mariana Neves, Preslav Nakov, Anna Divoli,
Manuel Ma?na L?opez, Jacinto Mata, and W. John
Wilbur. 2008. Overview of BioCreative II gene
mention recognition. Genome biology, 9 Suppl
2(Suppl 2):S2+.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2003. Boost-
ing precision and recall of dictionary-based pro-
tein name recognition. In Proceedings of the ACL
2003 Workshop on Natural Language Processing in
Biomedicine, pages 41?48, Sapporo, Japan, July. As-
sociation for Computational Linguistics.
Hua Xu, Jung-Wei Fan, George Hripcsak, Eneida A
Mendonc?a, Marianthi Markatou, and Carol Fried-
man. 2007. Gene symbol disambiguation us-
ing knowledge-based profiles. Bioinformatics,
23(8):1015?1022.
GuoDong Zhou, Dan Shen, Jie Zhang, Jian Su, and
SoonHeng Tan. 2005. Recognition of protein/gene
names from text using an ensemble of classifiers.
BMC Bioinformatics, 6 Suppl 1:S7.
127
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 198?209,
Dublin, Ireland, August 23, 2014.
Modelling the Semantics of Adjectives in the Ontology-Lexicon Interface
John P. M
c
Crae
Universit?at Bielefeld
Bielefeld
Germany
jmccrae@cit-ec.uni-bielefeld.de
Christina Unger
Universit?at Bielefeld
Bielefeld
Germany
cunger@cit-ec.uni-bielefeld.de
Francesca Quattri
Hong Kong Polytechnic University
Hong Kong
francesca.quattri@connect.polyu.hk
Philipp Cimiano
Universit?at Bielefeld
Bielefeld
Germany
cimiano@cit-ec.uni-bielefeld.de
Abstract
The modelling of the semantics of adjectives is notoriously challenging. We consider this prob-
lem in the context of the so called ontology-lexicon interface, which attempts to capture the
semantics of words by reference to an ontology in description logics or some other, typically
first-order, logical formalism. The use of first order logic (hence also description logics), while
effective for nouns and verbs, breaks down in the case of adjectives. We argue that this is primar-
ily due to a lack of logical expressivity in the underlying ontology languages. In particular, be-
yond the straightforward intersective adjectives, there exist gradable adjectives, requiring fuzzy
or non-monotonic semantics, as well as operator adjectives, requiring second-order logic for
modelling. We consider how we can extend the ontology-lexicon interface as realized by extant
models such as lemon in the face of the issues mentioned above, in particular those arising in the
context of modelling the ontological semantics of adjectives. We show howmore complex logical
formalisms that are required to capture the ontological semantics of adjectives can be backward
engineered into OWL-based modelling by means of pseudo-classes. We discuss the implications
of this modelling in the context of application to ontology-based question answering.
1 Introduction
Ontology-lexicon models, such as lemon (Lexicon Model for Ontologies) (M
c
Crae et al., 2012) model
the semantics of open class words by capturing their semantics with respect to the semantic vocabulary
defined in a given ontology. Such ontology-lexica are built around the separation of a lexical layer, de-
scribing how a word or phrase acts syntactically and morphologically, and a semantic layer describing
how the meaning of a word is expressed in a formal logical model, such as OWL (Web Ontology Lan-
guage) (Deborah L. M
c
Guinness and others, 2004). As such, the modelling is based around a lexical
entry which describes the morphology and syntax of a word, and is linked by means of a lexical sense
to an ontology entity defined in a given ontology described in formal logic. It has been shown that this
principle known as semantics by reference (Buitelaar, 2010) is an effective model that can support the
task of developing question answering systems (Unger and Cimiano, 2011) and natural language gen-
eration (Cimiano et al., 2013) over backends based on Semantic Web data models. The Pythia system,
which builds on the lemon formalism to declaratively capture the lexicon-ontology interface, for exam-
ple, has been instantiated to the case of answering questions from DBpedia (Unger and Cimiano, 2011).
However, as has been shown by the Question Answering over Linked Data (Lopez et al., 2013, QALD)
benchmarking campaigns, there are many questions that can be asked over this database that require a
deeper representation of the semantics of words, adjectives in particular. For example, questions such
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
198
as (1a) require understanding of the semantics of ?high? in a manner that goes beyond the expressivity of
OWL. The formalization of this question as an executable query formulated with respect to the SPARQL
query language is provided in (1b). In particular, the interpretation of this question involves the formal
interpretation of the word ?high? as relating to the property dbo:elevation, including ordering and
subset selection operations.
1. (a) What is the highest mountain in Australia?
(b) SELECT DISTINCT ?uri WHERE {
?uri rdf:type dbo:Mountain .
?uri dbo:locatedInArea res:Australia .
?uri dbo:elevation ?elevation .
} ORDER BY DESC(?elevation) LIMIT 1
In the above query, we select an entity denoted by the query variable ?uri that has the properties
that i) the entity?s type is a mountain, ii) it is located in Australia, and iii) it has an elevation bound to
the variable ?elevation. We then sort the query in descending order by the value of the elevation
and limit so the query returns only the first result, in effect choosing the largest value in the data set.
It has been claimed that first-order logic and thus by extension description logics, such as OWL, ?fail
decidedly when it comes to adjectives? (Bankston, 2003). In fact, we largely agree that the semantics of
many adjectives are difficult or impossible to describe in first-order logic. However, from the point of
view of the ontology-lexicon interface, the logical expressivity of the ontology is not a limiting factor. In
fact, due to the separation of the lexical and ontology layers in a model such as lemon, it is possible to
express the meaning of words without worrying about the formalism used in the ontology. To this extent,
we will first demonstrate that adjectives are in general a case where the use of description logics (DL)
breaks down, and for which more sophisticated logical formalisms must be applied. We then consider
to what extent this can be handled in the context of the ontology-lexicon, and introduce pseudo-classes,
that is OWL classes with annotations, which we use to express the semantics of adjectives in a manner
that would allow reasoning with fuzzy, high-order models. To this extent, we base our models on the
previously introduced design patterns (M
c
Crae and Unger, 2014) for modelling ontology-lexica. Finally,
we show how these semantics can be helpful in practical applications of question answering over the
DBpedia knowledge base.
2 Classification of adjectives
There are a number of classifications of adjectives. First we will start with the most fundamental dis-
tinction between attributive and predicative usage, that is the use of adjectives in noun phrases (?X is a
A N?) versus as objects of the copula (?X is A?). It should be noted that there are many adjectives for
which only predicative or attributive usage is allowed, as shown in (3a) and (3).
2. (a) Clinton is a former president.
(b)
?
Clinton is former.
3. (a) The baby is awake.
(b)
?
The awake baby.
One of the principle classifications of the semantics of adjectives (for example (Partee, 2003; Bouillon
and Viegas, 1999; Morzycki, 2013b)) is based on the meaning of adjective noun compounds relative to
the meaning of the single words that form the compound. This classification is as follows (where ?
denotes entailment).
Intersective (X is a A N ? X is A ? X is a N ) Such adjectives work as if they were another noun
and indicate that the compound noun phrase is a member of class denoted by the noun and the class
denoted by the adjective. For example, in the phrase ?Belgian violinist? it refers to a person in the
class intersection Belgian ? V iolinist(X), and hence we can infer that a ?Belgian violinist? is a
subclass of a ?Belgian?. Furthermore, we could conclude that if the same person were a surgeon,
he/she would also be a ?Belgian Surgeon?.
199
Subsective (X is a AN ? X is a N , but X is a A N ?? X is A) Such adjectives acquire their specific
meaning in combination with the noun the modify. For example, a ?skilful violinist? is certainly in
the class V iolinist(X) but the described person is ?skilful as a violinist?, but not skilful in general,
e.g. as a surgeon.
Privative (X is a A N ?? X is a N ) These adjectives modify the meaning of a noun phrase to create a
noun phrase that is potentially incompatible with the original meaning. For example, a ?fake gun?
is not a member of the class of guns.
Another important distinction is whether adjectives are gradable, i.e. whether a comparative or su-
perlative statement with these adjectives makes sense. For example, adjectives such as ?big? or ?tall? can
express relationships such as ?X is bigger than Y ?. However it is not possible to say that one individ-
ual is ?more former?. Most gradable adjectives are subsective (e. g.?a big mouse? is not ?a big animal?
(Morzycki, 2013a)).
Finally, we consider operator or property-modifying adjectives. They can be understood along the
lines of privative adjectives but differ in that they represent operators that modify some property in the
qualia structure (Pustejovsky, 1991) of the class. For instance, we may express the adjective ?former? in
lambda calculus as a function that takes a class C as input and returns the class of entities that were a
member of C to some prior time point t (Partee, 2003):
?C[?x?tC(x, t) ? t < now]
Such adjectives have not only a difference in semantic meaning but can also frequently have syntactic
impact, for example in adjective ordering restrictions, as they may be reordered with only semantic
impact (Teodorescu, 2006), e.g.,
4. (a) A big red car.
(b)
?
A red big car.
5. (a) A famous former actor.
(b) A former famous actor.
Finally, we define object-relational adjectives as those adjectives which have a meaning that expresses
a relationship between two individuals or events
1
, for example:
6. He is related to her.
7. She is similar to her brother.
8. This is useful for something.
3 Representation of adjectives in the ontology-lexicon interface
In general it is assumed that adjectives form frames with exactly one argument except for extra arguments
provided by adjuncts, typically prepositional phrases. Most adjectives are thus associated with a pred-
icative frame, which much like the standard noun predicate frame (X is a N ) is stereotyped in English
as:
X is A
The attributive usage of an adjective is associate to a stereotypical frame where the N? argument is
not semantically bound, but can instead be obtained by syntactic unification to a noun predicate frame:
X is A N?
As such, when we encounter the attributive usage of an adjective such as in 9, we understand this as
the realization of two frames, given in 10.
9. Juan is a Spanish researcher.
10. (a) Juan is a researcher.
(b) Juan is a Spanish N?
Note that we do not provide modelling for adjectives where the meaning is unique for a particular
noun phrase, such as ?polar bear?, which we would capture as a normal noun phrase with meaning ursus
maritimus.
1
Our definition of relational here is borrowed from the idea of relational nouns (De Bruin and Scha, 1988) as a word that requires
an argument. Our definition is also different from the one for ?relational adjectives? as proposed by (Morzycki, 2013a).
200
Figure 1: Modelling of an intersective adjective ?Belgian? in lemon
3.1 Intersective adjectives
Intersective adjectives are the most straightforward class, as in many cases they can be modelled es-
sentially as a noun or verb (e.g. deverbal adjectives such as ?broken?). Intersective adjectives take one
argument and can thus be modelled as unary predicates in first-order logic or classes in OWL, as de-
scribed by M
c
Crae and Unger (2014). For practical modelling examples, we will use the lemon model,
since it is the most prominent implementation of the ontology-lexicon interface.
The primary mechanism of modelling the syntax-semantics interface in the context of lemon is by
means of assigning a frame as a syntactic behaviour of an entry and giving it syntactic arguments, which
can then be linked to the lexical sense, which stands proxy for a true semantic frame in the ontology. For
example, the modelling of an adjective such as ?Belgian? can be achieved as follows (depicted in Figure
1)
2
.
lexicon:belgian a lemon:LexicalEntry ;
lemon:canonicalForm belgian:Lemma ;
lemon:synBehavior belgian:AttrFrame ,
belgian:PredFrame ;
lemon:sense belgian:Sense .
belgian:Lemma lemon:writtenRep "Belgian"@eng .
belgian:AttrFrame lexinfo:attributiveArg belgian:AttrSynArg .
belgian:PredFrame lexinfo:copulativeArg belgian:PredSynArg .
belgian:sense lemon:reference [ a owl:Restriction ;
owl:onProperty dbpedia:nationality ;
owl:hasValue dbpedia:Belgium ] ;
lemon:isA belgian:AttrSynArg , belgian:PredSynArg .
In this example, the word ?Belgian? is associated with a lemma with representation ?Belgian?, two
frame objects and a lexical sense. The frame objects describe the attributive and predicative usage,
and are associated with an attributive and copulative argument respectively. The sense links the word
to the anonymous ontological class for objects that have ?Belgium? as the value of their ?national-
ity? property and furthermore the arguments of each frame are linked to the sense in order to estab-
lish a correspondence between the ontology class and the syntactic frames. Note that here we use
the external vocabulary defined in the LexInfo ontology (Cimiano et al., 2011) to define the mean-
ing of the arguments of the frame as the attributive argument, corresponding to the frame stereo-
type ?X is A N?? and the copulative argument for the frame stereotype ?X is A?. Furthermore, the
2
We assume that the namespaces are defined for the lexicon as lexicon, e.g., http://www.example.org/lexicon
and for the entry, e.g., belgian is http://www.example.org/lexicon/belgian#. Other namespaces are as-
sumed to be as usual.
201
class of Belgians is not named in our reference ontology DBpedia, so we introduce an anonymous
class with the axiomatization, i.e. ? nationality .Belgium. It is in fact common that the referent of
an adjective is not named in an ontology. An obvious choice is thus to model denominal adjec-
tives as classes of the form ? prop.Value, where Value is an individual that represents the seman-
tics of the noun from which the adjective was derived. This modelling is so common that it has al-
ready been encoded as two design patterns, called IntersectiveObjectPropertyAdjective
and IntersectiveDatatypePropertyAdjective (see (M
c
Crae and Unger, 2014)). Simi-
larly, most deverbal adjectives refer to an event, and as such a common modelling is of the form
? theme
?1
.EventClass. For example, ?vandalized? may be ? theme
?1
.VandalismEvent.
3.2 Gradable adjectives and relevant observables
Gradable adjectives have a number of properties which differentiate them from intersective adjectives:
? They occur in comparative constructions, in English with either ?-er? or ?more? (Kennedy and Mc-
Nally, 1999), e.g. ?smaller? and ?more frequent?, as opposed to intersectives such as ?*less geologi-
cal? and ?*more wooden?.
? Gradable adjectives can be defined as ?scalar?, since their value can ideally be measured on a scale
of set degrees
? They have a context-dependent truth-conditional variability, meaning that their positive form is un-
derstood in relation to the class of the object modified by the adjective. For example, an ?expensive
watch? has a different price scale to an ?expensive bottle of water?.
? They are frequently fuzzy (or vague) (Kennedy, 2007).
? There may be a minimum or maximum of the adjective?s scale, which can be determined by, for
example, whether they can modified by adverbs such as ?completely? or ?utterly?.
As such, we define gradable adjectives relative to a particular property. These adjectives are also
called ?observable? (Bennett, 2006)
3
as they are related to some observable or measurable property, e.g.
size in the case of ?big?. However, a specification of the observable property is clearly not sufficient to
differentiate between the meaning of antonyms such as big and small. Thus, we introduce the notions of
covariance and contravariance, which specify whether the comparative form indicates a higher property
value for the subject or the object. In this sense ?big? is covariant with size, as bigger things have a
higher size value, and ?small? is contravariant with size.
4
We also introduce a third concept, i.e. the one
of absolute gradability, which expresses the fact that the degree of membership in the denotation of the
adjective is stronger the more it approaches a prototypical or ideal value. A common example of this is
colours, where we may say that some object is redder than another if it is closer to some ideal value of
red (e.g., RGB 0xff0000).
While these notions can handle the comparative structure of the semantics of adjectives, the predicative
and superlative usage of adjectives is complicated by three factors that we will outline below. We notice
that gradable classes are not crisply defined like in the case of many intersective adjectives. In fact, while
we can clearly define all people in the world as ?Belgian? or ?not Belgian?, according to whom holds
a Belgian passport or not, it is not easy to split the world?s population into ?tall? and ?not tall? (This is
known as sorites paradox (Bennett, 2006)). Furthermore, while it may be easy to say that someone with
height 6?6? (198cm) is ?tall?, it is not clear whether someone with height 6? (182cm) is ?tall?, although
compared to an average (different) height for a man, they are ?taller?. As such, one frequently used way
to deal with this class of vague adjectives (and nouns) is via fuzzy logic (Goguen, 1969; Zadeh, 1975;
Zadeh, 1965; Dubois and Prade, 1988; Bennett, 2006). Secondly, we notice that these class boundaries
are non-monotonic, that is that with knowledge of more instances of the relative class we must revise
our class boundaries. This is especially the case for superlatives, as the discovery of a new tallest person
3
Note that in many cases the property is quite abstract such as in ?breakable?.
4
The use of these terms is borrowed from type systems, and resembles the concept of ?converse observables? as introduced by
((Bennett, 2006):42). As stated by the author, adjectives often come in pairs of polar opposites (e. g. conv(tall) = short, and
both refer to the same observable (in this case size). Some observables analogously hold converse relationships with other
observables (e. g. conv(flexibility) = rigidity or conv(tallness) = shortness).
202
in the world would remove the existing tallest person in the world from the class of tallest person in the
world. This non-monotonicity also affects the class boundaries of the gradable class itself. For example,
in the 18th century, the average height of a male was 5?5? (165cm)
5
; as such a male of 6? would have
clearly been considered tall.
It follows from this that each instance added to our ontology might lead to a revision of the class
boundaries of a gradable class, hence leading to the fact that gradable adjectives are fundamentally non-
monotonic. We must also notice that gradability can only be understood relative to the class that we wish
to grade. Thus, while it is a priori unclear whether 6? is tall for a male, it is clear that 6? is tall for a
female given the current average height of a female being about 5?4? (162cm).
We can therefore conclude that gradable adjectives are fuzzy, non-monotonic and context-sensitive, all
of which are incompatible with the description logic used in OWL.
Pseudo-classes in lemonOILS
Currently there are only limited models for representing fuzzy logic in the context of the Web (Zhao and
Boley, 2008). In order to capture the properties of gradable adjectives, we introduce a new model which
we name lemonOILS (The lemon Ontology for the Interpretation of Lexical Semantics)
6
. This ontology
introduces three new classes:
? CovariantScalar, indicating that the adjective is covariant with its bound property
? ContravariantScalar, indicating that the adjective is contravariant with its bound property
? AbsoluteScalar, indicating that the property represents similarity to an absolute value
In addition, the following properties are introduced to enable the description of gradable adjectives.
Note that all these properties are typed as annotation properties in the OWL ontology, so that they do
not interfere with the standard OWL reasoning.
? boundTo indicates the property that a scalar refers to (e.g., ?size? for ?big?)
? threshold specifies a sensible minimal value for which the adjective can be said to hold
? absoluteValue is the ideal value of an absolute scalar
? degree is specified as weak, medium, strong or very strong, corresponding to approxi-
mately 50%, 25%, 5% or 1% of all known individuals
? comparator indicates an object property that is equivalent to the comparison of the adjective
(e.g., an object property biggerThan may be considered a comparator for the adjective class
big)
? measure indicates a unit that can be used as a measure for this adjective, e.g., ?John is 175 cen-
timetres tall?.
Using such classes we can capture the semantics of gradable adjectives syntactically but not formally
within an OWL model. As such, we call these introduced classes pseudo-classes. An example of mod-
elling an adjective such as ?high? is given below (and depicted in Figure 2).
lexicon:high a lemon:LexicalEntry ;
lemon:canonicalForm high:Lemma ;
lemon:synBehavior high:PredFrame ;
lemon:sense high:Sense .
high:Lemma lemon:writtenRep "high"@eng .
high:PredFrame lexinfo:copulativeArg high:PredArg .
high:Sense lemon:reference [
rdfs:subClassOf oils:CovariantScalar ;
oils:boundTo dbpedia:elevation ;
oils:degree oils:strong ] ;
lemon:isA high:PredArg .
5
https://en.wikipedia.org/wiki/Human_height
6
http://lemon-model.net/oils
203
Figure 2: An example of the modelling of ?high? in lemon
As an example of a logic in which these annotations could be interpreted, we consider Markov
Logic (Richardson and Domingos, 2006), which is an extension of first-order logic in which each clause
is given a cost. The process of reasoning is thus transformed into an optimization problem of finding
the extension which minimizes the summed weight of all violated clauses. As such, we can formulate
a gradable adjective based on the number of known instances. For example, we can specify ?big? with
respect to size for some class C as in (11).
11. ?x ? C, y ? C : size(x) > size(y) ? big
C
(x) : ?
?x ? C, y ? C : size(x) < size(y) ? ?big
C
(x) : ?
In this way, the classification of an object into ?big? or ?small? can be defined as follows. For an individual
x ? C, the property big
C
(x) holds if and only if:
|{y ? C, size(y) > size(x)}|? < |{y ? C, size(y) < size(x)}|?
where the values of ? and ? are related to the degree defined in the ontology.
We see that ?big? defined in this way has the three properties outlined above: it is non-monotonic (in
that more individuals may change whether we consider an individual to be ?big? or not), it is fuzzy (given
by the strength of the probability of the proposition big
C
(x)), and it is context-sensitive (as whether an
individual counts as big or not depends on the class C). Furthermore, our definition does not rely on
defining ?big? for a given class, but instead is inferred from some known number of instances of this
class. This eliminates the need to define a threshold for each individual class, or even to define the
predicate big
C
on a per-class basis.
The supervaluation theory and SUMO
Another way to capture the meaning of these vague terms can be achieved by supervaluation semantics.
Through supervaluation theory, the modelling or positioning of sorites vague concepts is grounded in a
judgement or meaning that lies on arbitrary thresholds, but these thresholds are based on a number of
relevant objective measures (Bennett, 2006).
A recent extension of the SUMO ontology (Niles and Pease, 2001, Suggested Upper Merged On-
tology)
7
includes default measurements (currently amounting to 300+) added to the Artifacts,
Devices and Objects enlisted in the ontology (and marked with capitals). The compilation of
defaultMeasurements in SUMO has been just conducted on observables, not on predicates. Given
for instance an Artifact such as Book, the compilation of its default measurements would look like:
;;Book
(defaultMinimumHeight Book (MeasureFn 10 Inch))
(defaultMaximumHeight Book (MeasureFn 11 Inch))
(defaultMinimumLength Book (MeasureFn 5.5 Inch))
(defaultMaximumLength Book (MeasureFn 7 Inch))
(defaultMinimumWidth Book (MeasureFn 1.2 Inch))
(defaultMaximumWidth Book (MeasureFn 5.5 Inch))
7
www.ontologyportal.org
204
The example for Book shows that the default measurements for the observable reflect a standard kind
of book, i.e., one of the most commonly known kinds of the same artifact. As for this case, SUMO
implies Book to be a physical object with a certain length, height and width (and possibly weight). A
weakness here is that the there is no systematic connection between the defaultMinimumHeight
and Height or Width, since these physical properties have been defined in SUMO just in terms of
first-order logic, and have not been assigned default measurements yet. With lemonOILS we can add this
information as follows:
sumo:Book oils:default [
oils:defaultFor sumo:height ;
oils:defaultMin "10in" ;
oils:defaultMax "11in" ] .
Then, if we understand a lexical entry ?high? as referring to a scalar covariant pseudo-class for
sumo:height, it is possible to understand that a ?high? object exceeds the default minimum set es-
tablished for the same object and owns at the same time a value for ?high? which does not go beyond the
established default maximum. A further weakness of this approach is captured by the following example:
12. Avery Johnson is a short basketball player.
Here, we see the difficulty in interpreting the sentence, as Avery Johnson is in fact of average height
(5?10?) but for the class of basketball players he is unusually short. While SUMO has some very specific
listings of subsets for the same Artifact
8
, SUMO does not provide a well-structured subset net for
e. g. Person.As a way to address this bottleneck, we could introduce default values for every subclass
of Person, as well as to introduce default values for the same Artifact in conjunction with a predicate
or adjective (e. g. BigPerson, BulkyPerson). The creation of such ad hoc subclasses is not feasible
in general, as we would have to introduce a new class into the ontology for every combination of an
adjective and a noun. On the other side though, the SUMO default measurements serve the purpose
they were originally conceived for, namely to be an arbitrary, yet computable approximation of physical
measures.
3.3 Operator adjectives
Operator adjectives are those that combine with a noun to modify the meaning of the noun itself. There
are two primary issues with the understanding of the adjective in this manner. Firstly, the reference
of the lexical item does not generally refer to an existing item in the ontology, but rather is novel and
productive, in the sense that it generates a new class. Secondly, the compositional nature of adjective-
noun compounds is no longer simple, as in the cases of intersective and gradable adjectives. This means
that, in order to understand a concept such as a ?fake gun?, we must first derive a class of FakeGuns
from the class of Guns. Thus the modified noun phrase must be an argument of the operator adjective.
To this extent we claim that it is not generally possibly to represent the meaning of an operator adjective
within the context of an OWL ontology. Instead, following Bankston (Bankston, 2003), we claim that the
reference of an operator adjective must be a higher order predicate. If we assume that there are operators
of the form of a function, then the argument of an operator is the attributed noun phrase. As such, we
introduce a frame operator attributive, that has one argument which is the noun. Thus we understand
that the interpretation of ?fake gun? is by means of an operator fake, which is a function that takes
a class and produces a new class, i.e., [fake(Gun)](X). Capturing such an operator lies beyond the
expressivity of first-order logic. To fully capture the semantics of such an operator adjective, formalisms
beyond first-order logic are thus clearly needed.
3.4 Object-relational adjectives
Object-relational adjectives are those that require a second argument, such as ?known?, which can only
be understood as being ?known? to some person, in comparison to ?famous?. Thus, the modelling of the
relational adjective known is quite similar to the semantics of the corresponding verb know. It can be
modelled for instance via the frame ?X is known to Y ? and reference foaf:knows as:
8
For example, some of the subsets Car are: CrewDormCar, GalleryCar, MotorRailcar, FreightCar, BoxCar,
RefrigeratorCar, FiveWellStackCar, and more.
205
lexicon:known a lemon:LexicalEntry ;
lemon:canonicalForm known:Lemma ;
lemon:sense known:Sense ;
lemon:synBehavior known:Frame .
known:Lemma lemon:writtenRep "known"@eng .
known:Frame lexinfo:attributeArg known:Subject ;
lexinfo:prepositionalObject known:Object .
known:Sense lemon:reference foaf:knows ;
lemon:subjOfProp known:Subject ;
lemon:objOfProp known:Object .
known:Object lemon:marker lexicon:to .
4 Adjectives in question answering
In this section we empirically analyze the adequacy of the modelling proposed in this paper with respect
to the QALD-4
9
dataset, a shared dataset for Question Answering over Linked Data. The 250 training
and test questions of the QALD-4 benchmark contain 76 adjectives in total (not counting adjectives in
names such as ?Mean Hamster Software?).
18 of the occurring adjectives do not have a semantic contribution w.r.t. the underlying DBpedia
ontology, or at least none that is separable from the noun, as exemplified in the noun phrases in (13) and
(14).
10
13. (a) [[official website]] = dbo:website
(b) [[national anthem]] = dbo:anthem
14. (a) [[official languages]] = dbo:officialLanguages
(b) [[military conflicts]] = dbo:battle
Otherwise, the most common kinds of adjectives among them are gradable (27) and intersective (13)
adjectives.
All intersective adjectives denote restriction classes that are not explicitely named in DBpedia, in
correspondence with the modelling proposed in Section 3.1 above, for example:
15. (a) [[Danish]] = ?dbo:country .res:Denmark
(b) [[female]] = ?dbo:gender .res:Female
(c) [[Methodist]] = ?dbo:religion .res:Methodism
In some cases these intersectives have a context-dependent and highly ontology-specific meaning,
often tightly interwoven with the meaning of the noun, as in the following examples:
16. (a) [[first president of the United States]] = ?dbo:office . ?1st President of the United States?
(b) [[first season]] = ?dbo:seasonNumber . 1
All gradable adjectives that occur in the QALD-4 question set can be captured in terms of lemonOILS
as CovariantScalar (e.g. ?high?) or ContravariantScalar (e.g. ?young?) (cf. Section 3.2
above), bound to a DBpedia datatype property (e.g. elevation or birthDate). The positive form
of those adjectives only occurs in ?how (much)? questions, denoting the property they are bound to, for
example:
17. (a) [[deep]] = dbo:depth in ?How deep is Lake Placid??
(b) [[tall]] = dbo:height in ?How tall is Michael Jordan??
9
http://www.sc.cit-ec.uni-bielefeld.de/qald/
10
[[?]] stands for ?denotes? and the prefixes dbo and res abbreviate the DBpedia namespaces
http://dbpedia.org/ontology/ and http://dbpedia.org/resource/, respectively.
206
The comparative form denotes the property they are bound to, together with an aggregation operation,
usually a filter invoking a term of comparison that depends on whether the adjective is covariant or
contravariant.
18. (a) [[Which mountains are higher than the Nanga Parbat?]] =
SELECT DISTINCT ?uri WHERE {
res:Nanga_Parbat dbo:elevation ?x .
?uri rdf:type dbo:Mountain .
?uri dbo:elevation ?y .
FILTER (?y > ?x)
}
Finally, the superlative form denotes the property they are bound to, together with an aggregation
operation, usually an ordering with a cut-off of all results except the first one, as exemplified in (19). In
some cases, the superlative property is already encoded in the ontology, e.g., in the case of the property
dbo:highestPlace.
19. [[What is the longest river?]] =
SELECT DISTINCT ?uri WHERE {
?uri rdf:type dbo:River .
?uri dbo:length ?l .
} ORDER BY DESC(?l) OFFSET 0 LIMIT 1
There are three instances of operator adjectives. Examples are ?former?, as in 20, which does not
refer to an element in the DBpedia ontology but is instead a disambiguation clue in the given query, and
?professional?, which refers to the property dbo:occupation, see 21.
20. [[the former Dutch queen Juliana]] = res:Juliana
21. [[professional surfer]] = ?dbo:occupation .res:Surfing
Finally, there were 8 remaining adjectives totalling 15 occurrences, which do not correspond to mean-
ing in an ontology, but instead are part of the discourse structure, each ?same?, ?other?.
5 Related work
The categorization of adjectives in terms of formal semantics goes back to Montague (1970) and
Vendler (1968). However, one of the most significant attempts to assign a formal meaning was car-
ried out in the Mikrokosmos project (Raskin and Nirenburg, 1995). The approach to adjective modelling
in the Mikrokosmos provided one of the first computational implementations of a microtheory of adjec-
tive meaning. The modelling of adjectives presented in this paper is clearly inspired by the modelling
of adjectives adopted in the Mikrokosmos project. In particular, scalar adjectives in the Microkosmos
project are modeled by association with an attribute and a range, e.g., ?big? is described as being >0.75
(i.e., 75% of all known instances) on the size-attribute. Still, these classifications do not clearly
separate meaning and syntax and also require a separate modelling of comparatives and class-specific
meanings for many adjectives.
Amoia and Gardent (2006) handled the problem of adjectives in the context of textual entailment. They
analyzed 15 classes that show the subtle interaction between the semantic class (e.g., ?privative?) and the
issues of attributive/predicative use and gradability. Abdullah and Frost (2005) focused on the modelling
of privative adjectives by arguing that these adjectives modify the underlying set itself in a manner that
is naturally second-order. Similarly, Partee (2003) proposed a limited second-order model by means of
the ?head primary principle? requiring that adjectives are interpreted within their context. Bankston?s
analysis (2003), however, shows that the fundamental nature of many adjectives is higher-order, and pro-
vides a very sophisticated formal representation framework for adjectives. A more thorough discussion
of non-gradable, non-intersective adjectives is given by Morzycki (2013a). Bouillion and Viegas (1999)
consider the case of the French adjective ?vieux? (?old?), which they interprete as selecting two differ-
ent elements in the event structure of an attributed noun, that is whether the state, e.g., ?being a mayor?
for ?mayor?, is considered old or the individual itself. In this way, the introduction of two senses for
?vieux? is avoided, however it remains unclear if such reasoning introduces more complexity than the
207
extra senses. In his analysis of adjectives, Larson (1998) suggests that many adjectives denote properties
of events, rather than of simple heads or nouns (which does not fall very far from the statement, made
above, that relational adjectives denote properties of kinds). Pustejovsky (1992; 1991) and Lenci (2000)
state that lexical and semantic decomposition can be achieved generatively, assigning to each lexical item
a specific qualia structure. For instance, in an expression like:
22. The round, heavy, wooden, inlaid magnifying glass
? ?round? represents the Formal role (giving indications of shape and dimensionality)
? ?heavy? and ?wooden? related to the Constitutive role and indicate the relation between the
object and its parts (e. g. by specifying weight, material, parts and components)
? ?inlaid? is the Agentive role of the lexical item, denoting the factors that have been involved in
the generation of the objects, such as creator, artifact, natural kind, and causal chain
? ?magnifying? describes the Telic role of ?glass?, since it shows its purpose and function
Finally, Peters and Peters (2000) provide one of the few other practical reports on modelling adjectives
with ontologies, in the context of the SIMPLE lexica. This work is primarily focussed on the categoriza-
tion of by means of intensional and extensional properties, rather than due to their logical modelling.
6 Conclusion
In this paper we have proposed an approach to model the semantics of adjectives in the context of the
lexicon-ontology interface with a focus on the ontology-lexicon model lemon. We have argued that the
semantics of adjectives, in particular gradable and privative adjectives, is beyond what can be expressed
in first-order logics, OWL in particular. Instead, capturing the semantics of such adjectives requires
formalisms that are non-monotonic, second-order and can represent fuzzy concepts. We have proposed
an extension of lemon by the lemonOILS vocabulary that adds ?syntactic sugar? that allows us to represent
the semantics of adjectives in a way that abstracts from the actual representational formalism used. This
work has been used in the construction of lexical resources to support a question answering system,
and we found that this framework is sufficient to enable tractable computation of natural language to
SPARQL mapping over at least a small but varied set of test questions used in the QALD evaluation
task. Future work will show whether this model is scalable and applicable to most adjectives as well as
domains and natural languages.
References
Nabil Abdullah and Richard A Frost. 2005. Adjectives: A uniform semantic approach. In Advances in Artificial
Intelligence, pages 330?341. Springer.
Marilisa Amoia and Claire Gardent. 2006. Adjective based inference. In Proceedings of the Workshop KRAQ?06
on Knowledge and Reasoning for Language Processing, pages 20?27. Association for Computational Linguis-
tics.
Paul Bankston. 2003. Modeling nonintersective adjectives using operator logics. The Review of Modern Logic,
9(1-2):9?28.
Brandon Bennett. 2006. A theory of vague adjectives grounded in relevant observables. In John Mylopoulos
Patrick Doherty and Christopher A. Welty, editors, Proceedings of the Tenth International Conference on Prin-
ciples of Knowledge Representation and Reasoning, pages 36?45. AAAI Press.
Pierrette Bouillon and Evelyne Viegas. 1999. The description of adjectives for natural language processing: Theo-
retical and applied perspectives. In Proceedings of Description des Adjectifs pour les Traitements Informatiques.
Traitement Automatique des Langues Naturelles. Citeseer.
Pierrette Bouillon. 1999. The adjective ?vieux?: The point of view of ?generative lexicon?. In Breadth and depth
of semantic lexicons, pages 147?166. Springer.
Paul Buitelaar, 2010. Ontology-based Semantic Lexicons: Mapping between Terms and Object Descriptions,
pages 212?223. Cambridge University Press.
Philipp Cimiano, Paul Buitelaar, John M
c
Crae, and Michael Sintek. 2011. Lexinfo: A declarative model for the
lexicon-ontology interface. Web Semantics: Science, Services and Agents on the World Wide Web, 9(1):29?51.
Philipp Cimiano, Janna L?uker, David Nagel, and Christina Unger. 2013. Exploiting ontology lexica for generating
natural language texts from rdf data. In Proceedings of the 14th European Workshop on Natural Language
Generation, pages 10?19.
208
Jos De Bruin and Remko Scha. 1988. The interpretation of relational nouns. In Proceedings of the 26th annual
meeting on Association for Computational Linguistics, pages 25?32. Association for Computational Linguistics.
Frank Van Harmelen Deborah L. M
c
Guinness et al. 2004. Owl web ontology language overview. W3C recom-
mendation, 10(2004-03):10.
Didier Dubois and Henri Prade. 1988. Possibility theory. Plenum Press, New York.
Joseph H. Goguen. 1969. The logic of inexact concepts. Synthese, 19:325?373.
Christopher Kennedy and Louise McNally. 1999. Deriving the scalar structure of deverbal adjectives. Catalan
Working Papers in Linguistics, 7:125?139.
Christopher Kennedy. 2007. Vagueness and grammar: the semantics of relative and absolute gradable adjectives.
Linguistics and philosophy, 30:1?45.
Richard K. Larson. 1998. Events and modification in nominals. In Devon Strolovitch and Aaron Lawson, editors,
Proceedings from Semantics and Linguistic Theory (SALT) VIII, pages 145?168. CLC Publications, Itaca, New
York.
Alessandro Lenci et al. 2000. Simple work package 2, linguistic specifications, deliverable d2.1.
Vanessa Lopez, Christina Unger, Philipp Cimiano, and Enrico Motta. 2013. Evaluating question answering over
linked data. Web Semantics: Science, Services and Agents on the World Wide Web, 21:3?13.
Louise McNally and Gemma Boleda. 2004. Relational adjectives as properties of kinds. Empirical issues in
formal syntax and semantics, 5:179?196.
Richard Montague. 1970. English as a formal language. In Bruno Visentini et al, editor, Linguaggi nella societa
e nella tecnica, pages 189?224. Milan: Edizioni di Comunit`a.
Marcin Morzycki. 2013a. The lexical semantics of adjectives: More than just scales. Ms., Michigan State
University. Draft of a chapter in Modification, a book in preparation for the Cambridge University Press series
Key Topics in Semantics and Pragmatics.
Marcin Morzycki. 2013b. Modification. Cambridge University Press.
John P. M
c
Crae and Christina Unger. 2014. Design patterns for the ontology-lexicon interface. In Paul Buitelaar
and Philipp Cimiano, editors, Towards the Multilingual Semantic Web: Principles, Methods and Applications.
Springer.
John M
c
Crae, Guadalupe Aguado-de Cea, Paul Buitelaar, Philipp Cimiano, Thierry Declerck, Asunci?on G?omez-
P?erez, Jorge Gracia, Laura Hollink, Elena Montiel-Ponsoda, Dennis Spohr, et al. 2012. Interchanging lexical
resources on the semantic web. Language Resources and Evaluation, 46(4):701?719.
Ian Niles and Adam Pease. 2001. Towards a standard upper ontology.
Barbara H Partee. 2003. Are there privative adjectives. In Conference on the Philosophy of Terry Parsons,
University of Massachusetts, Amherst.
Ivonne Peters and Wim Peters. 2000. The treatment of adjectives in simple: Theoretical observations. In LREC.
James Pustejovsky. 1991. The generative lexicon. Computational linguistics, 17(4):409?441.
James Pustejovsky. 1992. The syntax of event structure. In Bett Levin and Steven Pinker, editors, Lexical &
Conceptual Semantics, pages 47?83. Oxford: Blackwell.
Victor Raskin and Sergei Nirenburg. 1995. Lexical semantics of adjectives. New Mexico State University, Com-
puting Research Laboratory Technical Report, MCCS-95-288.
Matthew Richardson and Pedro Domingos. 2006. Markov logic networks. Machine learning, 62(1-2):107?136.
Alexandra Teodorescu. 2006. Adjective ordering restrictions revisited. In Proceedings of the 25th West Coast
Conference on Formal Linguistics, pages 399?407. Citeseer.
Christina Unger and Philipp Cimiano. 2011. Pythia: Compositional meaning construction for ontology-based
question answering on the semantic web. In Rafael Munoz, editor, Natural Language Processing and Infor-
mation Systems: 16th International Conference on Applications of Natural Language to Information Systems,
NLDB 2011, Alicante, Spain, June 28-30, 2011. Proceedings, volume 6716, pages 153?160. Springer.
Zeno Vendler. 1968. Adjectives and nominalizations. Number 5 in Papers on formal linguistics. Mouton.
Lofti A. Zadeh. 1965. Fuzzy sets. Information and Control, 8:338?353.
Lofti A. Zadeh. 1975. The concept of linguistic variable and its application to approximate reasoningi. Informa-
tion Sciences, 8:199?249.
Jidi Zhao and Harold Boley. 2008. Uncertainty treatment in the rule interchange format: From encoding to
extension. In URSW.
209
Proceedings of Third Workshop on Semantic Web and Information Extraction, pages 25?32,
Dublin, Ireland, 24 August, 2014.
Ontology-based Extraction of Structured Information from Publications
on Preclinical Experiments for Spinal Cord Injury Treatments
Benjamin Paassen
?
, Andreas St?ckel
?
, Raphael Dickfelder
?
, Jan Philip G?pfert
?
,
Tarek Kirchhoffer
?
, Nicole Brazda
?,?
, Hans Werner M?ller
?,?
,
Roman Klinger
?
, Matthias Hartung
?
, Philipp Cimiano
?1
?
Semantic Computing Group, CIT-EC, Bielefeld University, 33615 Bielefeld, Germany
?
Molecular Neurobiology, Neurology, HHU D?sseldorf, 40225 D?sseldorf, Germany
?
Center for Neuronal Regeneration, Life Science Center, 40225 D?sseldorf, Germany
{bpaassen,astoecke,rdickfel,jgoepfert}@techfak.uni-bielefeld.de
tarek.kirchhoffer@cnr.de, {nicole.brazda,hanswerner.mueller}@uni-duesseldorf.de
{rklinger,mhartung,cimiano}@cit-ec.uni-bielefeld.de
Abstract
Preclinical research in the field of central nervous system trauma advances at a fast pace, currently
yielding over 8,000 new publications per year, at an exponentially growing rate. This amount of
published information by far exceeds the capacity of individual scientists to read and understand the
relevant literature. So far, no clinical trial has led to therapeutic approaches which achieve functional
recovery in human patients.
In this paper, we describe a first prototype of an ontology-based information extraction system that
automatically extracts relevant preclinical knowledge about spinal cord injury treatments from nat-
ural language text by recognizing participating entity classes and linking them to each other. The
evaluation on an independent test corpus of manually annotated full text articles shows a macro-
average F
1
measure of 0.74 with precision 0.68 and recall 0.81 on the task of identifying entities
participating in relations.
1 Introduction
Injury to the central nervous system of adult mammals typically results in lasting deficits, like permanent
motor and sensor impairments, due to a lack of profound neural regeneration. Specifically, patients who have
sustained spinal cord injuries (SCI) usually remain partially paralyzed for the rest of their lives. Preclinical
research in the field of central nervous system trauma advances at fast pace, currently yielding over 8,000
new publications per year, at an exponentially growing rate, with a total amount of approximately 160,000
PubMed-listed papers today.
2
However, translational neuroscience faces a strong disproportion between the immense preclinical re-
search effort and the lack of successful clinical trials in SCI therapy: So far, no therapeutic approach has
led to functional recovery in human patients (Filli and Schwab, 2012). As the vast amount of published in-
formation by far exceeds the capacity of individual scientists to read and understand the relevant knowledge
(Lok, 2010), the selection of promising therapeutic interventions for clinical trials is notoriously based on
incomplete information (Prinz et al., 2011; Steward et al., 2012).
Thus, automatic information extraction methods are needed to gather structured, actionable knowledge
from large amounts of unstructured text that describe outcomes of preclinical experiments in the SCI do-
main. Being stored in a database, such knowledge provides a highly valuable resource enabling curators
and researchers to objectively assess the prospective success of experimental therapies in humans, and sup-
ports the cost-effective execution of meta studies based on all previously published data. First steps towards
such a database have already been undertaken by manually extracting the desired information from a limited
number of papers (Brazda et al., 2013), which is not feasible on a large scale, though.
In this paper, we present a first prototype of an automated ontology-based information extraction system
for the acquisition of structured knowledge about experimental SCI therapies. As main contributions, we
point out the highly relational problem structure by describing the entity classes and relations relevant for
1
The first four authors contributed equally.
2
As in this query to the database PubMed (link to http://www.ncbi.nlm.nih.gov/pubmed), as of April 2014.
25
Named Entity Recognition Relation Extraction
Rule-based
Ontology-based
Regular
Expression
Rule-based
recombination
Token
lookup
Candidate
generation
Candidate
filtering
Inp
ut 
PD
F
Ontological
reduction
Relations
Ou
tpu
t
Animal
Injury
Treatment
Resulttex
t e
xtr
act
ion
Figure 1: Workflow of our implementation, from the input PDF document to the generation of the output
relations. Named entity recognition is described in Section 3.1, relation extraction in Section 3.2.
knowledge representation in the domain, and provide a cascaded workflow that is capable of extracting these
relational structures from unstructured text with an average F
1
measure of 0.74.
2 Related Work
Our workflow for acquiring structured information in the domain of spinal cord injury treatments is an
example of ontology-based information extraction systems (Wimalasuriya and Dou, 2010): Large amounts
of unstructured natural language text are processed through a mechanism guided by an ontology, in order to
extract predefined types of information. Our long-term goal is to represent all relevant information on SCI
treatments in structured form, similar to other automatically populated databases in the biomedical domain,
such as STRING-DB for protein-protein interactions (Franceschini et al., 2013), among others.
A strong focus in biomedical information extraction has long been on named entity recognition, for which
machine-learning solutions such as conditional random fields (Lafferty et al., 2001) or dictionary-based
systems (Schuemie et al., 2007; Hanisch et al., 2005; Hakenberg et al., 2011) are available which tackle
the respective problem with decent performance and for specific entity classes such as organisms (Pafilis
et al., 2013) or symptoms (Savova et al., 2010; Jimeno et al., 2008). A detailed overview on named entity
recognition, covering other domains as well, can be found in Nadeau and Sekine (2007).
The use case described in this paper, however, involves a highly relational problem structure in the sense
that individual facts or relations have to be aggregated in order to yield accurate, holistic domain knowledge,
which corresponds most closely to the problem structure encountered in event extraction, as triggered by
the ACE program (Doddington et al., 2004; Ji and Grishman, 2008; Strassel et al., 2008), and the BioNLP
shared task series (Nedellec et al., 2013; Tsujii et al., 2011; Tsujii, 2009). General semantic search engines
in the biomedical domain mainly focus on isolated entities. Relations are typically only taken into account
by co-occurrence on abstract or sentence level. Examples for such search engines include GoPubMed (Doms
and Schroeder, 2005), SCAIView (Hofmann-Apitius et al., 2008), and GeneView (Thomas et al., 2012).
With respect to the extraction methodology, our work is similar to Saggion et al. (2007) and Buitelaar et
al. (2008), in that a combination of gazetteers and extraction rules is derived from the underlying ontology,
in order to adapt the workflow to the domain of interest. A schema in terms of a reporting standard has
recently been proposed by the MIASCI-consortium (Lemmon et al., 2014, Minimum Information About a
Spinal Cord Injury Experiment). To the best of our knowledge, our work is the first attempt at automated
information extraction in the SCI domain.
3 Method and Architecture
An illustration of the proposed workflow is shown in Figure 1. Based on the unstructured information
management architecture (UIMA, Ferrucci and Lally (2004)), full text PDF documents serve as input to the
workflow. Plain text and structural information are extracted from these documents using Apache PDFBox
3
.
The proposed system extracts relations which we define as templates that contain slots, each of which is
to be filled by an instance of a particular entity class (cf. Table 1). At the same time, a particular instance
can be a filler for different slots (cf. Figure 2). We argue that a relational approach is essential to information
extraction in the SCI domain as (i) many instances of entity classes found in the text do not convey relevant
3
Apache PDFBox ? A Java PDF Library http://pdfbox.apache.org/
26
Relation Entity Class Example Method Resource Count
Integer ?42?, ?2k?, ?1,000? R Regular Expressions
Float ?4.23?, ?8.12 ? 10
-8
? R Regular Expressions
Roman Number ?XII?, ?MCLXII? R Regular Expressions
Word Number ?seventy-six" O Word Number List 99
Range ?2-4? R QTY + PARTICLE + QTY
Language Quantifier ?many?, ?all? O Quantifier List 11
Time ?2 h?, ?14 weeks? R QTY + TIME UNIT
Duration ?for 2h? R PARTICLE + TIME
Animal
Organism ?dog?, ?rat?, ?mice? O NCBI Taxonomy 67657
Laboratory Animal ?Long-Evans rats? O Special Laboratory Animals 5
Sex ?male?, ?female? O Gender List 2
Exact Age ?14 weeks old? R TIME + AGE PARTICLE
Age ?adult?, ?juvenile? O Age Expressions 2
Weight ?200 g? R QTY + WEIGHT UNIT
Number ?44?, ?seventy-six? R QTY
Injury
Injury Type ?compression? O Injury Type List 7
Injury Device ?NYU Impactor? O Injury Device List 21
Vertebral Position ?T4?, ?T8-9? R Regular Expressions
Injury Height ?cervical?, ?thoracic? O Injury Height Expressions 4
Treatment
Drug ?EPO?, ?inosine? O MeSH 14000
Delivery ?subcutaneous?, ?i.v.? O Delivery Dictionary 34
Dosage ?14 ml/kg? R QTY + UNIT
Result
Investigation Method ?walking analysis? O Method List 117
Significance ?significant? O Significance Quantifiers 2
Trend ?decreased?, ?improved? O Trend Dictionary 4
p Value ?p < 0.05? R P + QTY 4
Table 1: A detailed list of relations and the entity classes whose instances are valid slot fillers for them.
Examples for instances of each entity class are also shown, as well as the extraction method, and resources
used for extraction. Instances are either extracted from the text using regular expressions (R) or on a
lookup in our ontology database (O). Resources in italics were specifically created for this application,
resources in SMALL CAPITALS are regular expression-based recombinations of other entities. Entity
classes in bold face are required arguments for relation extraction (cf. Section 3.2). The count specifies
the number of elements in the respective resource.
information on their own, but only in combination with other instances (e. g., surgical devices mentioned in
the text are only relevant if used to inflict a spincal cord injury to the animals in an experimental group), and
(ii) a holistic picture of a preclinical experiment can only be captured by aggregating several relations (e. g.,
a certain p value being mentioned in the text implies a particular treatment of one group of animals to be
significantly different from another treatment of a control group).
We take four relations (Animal, Injury, Treatment and Result) into account which capture the semantic
essence of a preclinical experiment: Laboratory animals are injured, then treated and the effect of the treat-
ment is measured. Table 1 provides an overview of all entity classes and relations. The workflow consists
of two steps: Firstly, rule- and ontology-based named entity recognition (NER) is performed (cf. Section
3.1). Secondly, the pool of entities recognized during NER serves as a basis for relation extraction (cf.
Section 3.2).
3.1 Ontology-based Named Entity Recognition
We store ontological information in a relational database as a set of directed graphs, accompanied by a
dictionary for efficient token lookup. Each entity is stored with possible linguistic surface forms (e. g.,
?Wistar rats? as a surface form of the Wistar rat entity from the class Laboratory Animal). Each surface
form s is tokenized (on white space and non-alphanumeric symbols, including transformation to lowercase,
e. g., leading to tokens ?wistar? and ?rats?) and normalized (stemming, removal of special characters and
stop words) resulting in a set of dictionary keys (e. g., ?wistar? and ?rat?). The resources used as content
for the ontology are shown in Table 1. We use specifically crafted resources for our use case
4
as well as the
4
Resources built specifically are made publicly available at http://opensource.cit-ec.de/projects/scie
27
Five adult male guinea pigs weighing 200-250 g.
Animal Animal
Organism: guinea pigs
Weight: 200-250 g
Age: adult
Sex: male
Number: Five (5)
Organism: guinea pigs
Weight: -
Age: adult
Sex: male
Number: 200
Figure 2: Two example instances of the
Animal relation that can be generated
from the same text. Given its entity
class, the number 200 is a valid filler for
the ?number? slot as well as the ?weight?
slot. Both candidates are generated and
ranked according to their probability (cf.
Equation 4). The manually defined con-
straints of p
sem
ensure that 200 cannot
fill both slots at the same time.
NCBI taxonomy
5
and the Medical Subject Headings
6
(MeSH). The process of ontology-based NER consists
of (i) token lookup in the dictionary, (ii) candidate generation, (iii) probabilistic candidate filtering and (iv)
ontological reduction (cf. Figure 1).
Token lookup. For each token t in the document, the corresponding surface form tokens s
t
are retrieved
from the database. A confidence value p
conf
based on the Damerau-Levenshtein-Distance without swaps
(dld, Damerau (1964)) is calculated as
p
conf
(t, s
t
) := max
{
0, 1?min
t
?
?s
t
dld(t
?
, t)
|t
?
|
}
, (1)
where |t| denotes the number of characters in token t. Assuming to find t = ?rat? in the text with the
according surface form s
t
= (?wistar?, ?rats?), p
conf
(t, s
t
) = 1 ?
1
4
= 0.75. Tokens with p
conf
< 0.5 are
discarded.
Candidate generation. A candidate h for matching the surface form tokens s
h
is a list of tokens (t
h
1
, . . . , t
h
n
)
from the text. Candidates are constructed using all possible combinations of matching tokens for each surface
form token (as retrieved above). To keep this tractable, we restrict the search space to combinations with the
proximity d(t
h
k
, t
h
`
) ? 9 for all t
h
k
, t
h
`
? h, where d(u, v) := N
W
(u, v) + 3 ? N
S
(u, v) + 10 ? N
P
(u, v)
models the distance between two tokens u and v in the text withN
W
, N
S
, N
P
denoting the number of words,
sentences and paragraphs between u and v. In our example, a candidate would be h = (?rat?).
Candidate filtering. For a candidate h and the surface form tokens s
h
it refers to, we calculate a total
match probability, taking into account the distance d(u, v) of all tokens in the candidate, the confidence
p
conf
(t
?
, s
h
) that the token actually belongs to the surface form, and the ratio
?
t
?
?h
|t
?
|/
?
t?s
h
|t| of the
surface form tokens covered by the candidate:
p
match
(h, s
h
) =
1
?
t?s
h
|t|
max
t?h
?
t
?
?h
(
p
3
dist
(t, t
?
) ? p
conf
(t
?
, s
h
) ? |t
?
|
)
, (2)
where p
?
dist
(u, v) := exp
(
?
d(u, v)
2
2?
2
)
(3)
models the confidence that two tokens u and v belong together given their distance in the text. In our example
of the candidate h = (?rat?) with the surface form tokens s
h
= (?wistar?, ?rats?) is p
match
(h, s
h
) =
1 ? 0.75 ?
3
6+4
= 0.225. Candidates with p
match
< 0.7 are discarded. The resulting set of all recognized
candidates is denoted with H .
Ontological reduction. As the algorithm ignores the hierarchical information provided by the ontologies,
we may obtain overlapping matches for ontologically related entities. Therefore, in case of overlapping
entities that are related in an ?is a? relationship in the ontology, only the more specific one is kept. Assume
for instance the candidates ?Rattus norvegicus? and ?Rattus norvegicus albus?, where the latter is more
specific and therefore accepted.
3.2 Relation Extraction
We frame relation extraction as a template filling task such that each slot provided by a relation has to be
assigned a filler of the correct entity class. Entity classes for the four relations of interest are shown in
5
Sayers et al. (2012), database limited to vertebrates: http://www.ncbi.nlm.nih.gov/taxonomy/?term=
txid7742[ORGN
6
Lipscomb (2000), excerpt of drugs from Descriptor and Supplemental: https://www.nlm.nih.gov/mesh/
28
Table 1, where required slots are in bold face, whereas all other slots are optional.
The slot filling process is based on testing all combinations of appropriate entities taking into account
their proximity and additional constraints. In more detail, we define the set of all recognized relationsR
?
of
a type ? as
R
?
=
?
?
?
r
?
? P(H)
?
?
?
?
?
?
p
sem
(r
?
)
n
?
?
?
h?r
?
,h6=g(r
?
)
p
match
(h, s
h
) min
t?h,t
?
?g(r
?
)
p
?
?
dist
(t, t
?
) > 0.2
?
?
?
(4)
where P(H) denotes the power set over all candidates H recognized by NER. g(r
?
) returns the filler for
the required slot of r
?
, p
match
and p
dist
are defined as in Section 3.1 and p
sem
implements manually defined
constraints on r
?
: A wrongly typed filler h for one slot of r
?
leads to p
sem
(r
?
) = 0, as does a negative number
in the Number slot of the Animal relation. Animal Numbers larger than 100 or Animal Weights smaller than
1 g or larger than 1 t are punished. All other cases lead to p
sem
(r
?
) = 1. Note that p
match
(h, s
h
) = 1 for
candidates h retrieved by rule-based entity recognition. Further, we set ?
Animal
= ?
Treatment
= 6, ?
Injury
= 10
and ?
Result
= 15.
4 Experiments
4.1 Data Set
Overall 1186
Organism 58
Weight 32
Sex 33
Age 17
Injury Height 35
Injury Type 62
Injury Device 23
Drug 134
Dosage 106
Delivery 70
Investigation Method 129
Trend 219
Significance 137
p Value 131
Table 2: The number of anno-
tations in our evaluation set for
each entity class.
The workflow is evaluated against an independent, manually annotated
corpus of 32 complete papers which contain 1186 separate annotations
of entities, produced by domain experts
7
. Information about relations
is not provided in the corpus. Only entities which participate in the
description of the preclinical experiment are marked. The frequencies
of annotations among the different classes are shown in Table 2.
4.2 Experimental Settings
We evaluate the system with regard to two different tasks: extraction (?Is
the approach able to extract relevant information from the text, without
regard to the exact location of the information??) and annotation (?Is the
system able to annotate relevant information at the correct location as in-
dicated by medical experts??). Furthermore, we distinguish between an
all instances setting, where we consider all instances independently, and
a fillers only setting, where only those annotations in the system output
are considered, that are fillers in a relation (i.e. the fillers only-setting
evaluates a subset of the all instances-setting). The relation extraction
procedure is not evaluated separately. For each setting, we report preci-
sion, recall, and F
1
measure.
Taking the architecture into account, we have the following hypotheses: (i) For the all instances setting we
expect high recall, but low precision. (ii) For the fillers only setting, precision should increase notably. (iii)
Comparing the all entities and the fillers only setting, recall should remain at the same level. We therefore
expect the extraction task to be simpler than the annotation task: For any information to be annotated at
the correct position, it must have been extracted correctly. On the other hand, information that has been
extracted correctly, can still be found at a ?wrong? location in the text. Thus, we expect a drop of precision
and recall when moving from extraction to annotation.
4.3 Results
The results are presented in Table 3: For each relation mentioned in Section 3, and the entity classes partic-
ipating in it, we report precision, recall and F
1
-measure
8
. This is done for all four combinations of setting
and task. For each relation we also provide the macro-average of precision, recall and F
1
-measure over all
entity classes considered in that relation and the overall average.
7
Performed in Prot?g? http://protege.stanford.edu/ with the plug-in Knowtator http://knowtator.
sourceforge.net/ (Ogren, 2006)
8
Note that VertebralPosition and InjuryHeight are merged in the result table, as are Organism and Laboratory Animal and
Age and Exact Age. The Animal Number was excluded from the evaluation as it has not been annotated in our evaluation set.
29
Task Extraction Annotation
Setting All Instances Fillers Only All Instances Fillers Only
Entity Class Prec. Rec. F
1
Prec. Rec. F
1
Prec. Rec. F
1
Prec. Rec. F
1
Overall Average 0.58 0.95 0.72 0.68 0.81 0.74 0.13 0.77 0.22 0.21 0.51 0.30
Animal Average 0.62 0.99 0.76 0.82 0.94 0.87 0.12 0.91 0.21 0.31 0.81 0.44
Organism 0.41 1.00 0.58 0.88 0.90 0.89 0.02 1.00 0.04 0.24 0.66 0.35
Weight 0.20 1.00 0.33 0.52 0.94 0.67 0.08 0.97 0.15 0.49 0.91 0.64
Sex 0.85 0.99 0.91 0.87 0.98 0.92 0.18 0.94 0.30 0.26 0.94 0.41
Age 1.00 0.95 0.97 1.00 0.93 0.96 0.19 0.71 0.30 0.23 0.71 0.35
Injury Average 0.63 0.94 0.76 0.74 0.75 0.75 0.12 0.72 0.21 0.18 0.38 0.24
Injury Height 0.42 0.98 0.59 0.56 0.74 0.64 0.10 0.91 0.18 0.24 0.51 0.33
Injury Type 0.70 0.91 0.79 0.81 0.73 0.77 0.07 0.48 0.12 0.18 0.35 0.24
Injury Device 0.78 0.93 0.85 0.86 0.79 0.82 0.20 0.77 0.32 0.11 0.28 0.16
Treatment Average 0.45 0.91 0.61 0.53 0.78 0.63 0.14 0.72 0.23 0.19 0.54 0.28
Drug 0.10 0.98 0.18 0.24 0.69 0.36 0.01 0.74 0.02 0.10 0.42 0.16
Dosage 1.00 0.81 0.90 1.00 0.76 0.86 0.30 0.52 0.38 0.32 0.46 0.38
Delivery 0.26 0.95 0.41 0.34 0.89 0.49 0.11 0.89 0.20 0.15 0.74 0.25
Result Average 0.59 0.93 0.72 0.60 0.75 0.67 0.13 0.71 0.22 0.15 0.30 0.20
Investigation Method 0.29 0.96 0.45 0.27 0.79 0.40 0.03 0.66 0.06 0.02 0.16 0.04
Trend 0.37 0.91 0.53 0.44 0.78 0.56 0.06 0.63 0.11 0.07 0.27 0.11
Significance 0.70 0.90 0.79 0.70 0.71 0.70 0.17 0.69 0.27 0.22 0.39 0.28
p Value 1.00 0.96 0.98 1.00 0.71 0.83 0.27 0.86 0.41 0.30 0.36 0.33
Table 3: The macro-averaged evaluation results for each class given in precision, recall and F
1
measure.
For the extraction task with all instances setting, recall is close to 100% for all entity classes considered
in the Animal relation. It is 81% for Dosages. The rule-based recognition for Dosages (as for Ages and p
Values) is very precise: All recognized entities have been annotated by medical experts somewhere in the
document. This strong difference between entity classes can be observed in the annotation task and the fillers
only setting as well: The best average performance in F
1
-measure is achieved for entity classes that are part
of the Animal relation. Precision is best for Dosages, Ages and p Values.
The recall for the all instances setting is high in both the extraction and in the annotation task. However,
the number of annotated instances (29,628 annotations in total) is about 25 times higher than the number of
expert annotations, which leads to low precision especially in the annotation task. For the fillers only setting,
the number of annotations decreases dramatically (to 4069 annotations); at the same time, precision improves.
Regarding the comparison of both tasks, precision and recall are both notably lower in the annotation task,
for the all entities setting, as well as for the fillers only setting. The overall recall is lower by 14 percentage
points (pp) in the extraction task and by 26 pp in the annotation task when considering the fillers only setting.
The decrease is most pronounced for Investigation Methods in the annotation task with a drop of 50 pp.
4.4 Discussion
The results are promising for named entity recognition. Recall is close-to-perfect in the extraction task
and acceptable in the annotation task. The results for relation extraction leave space for improvement: An
increase in precision can be observed but the decrease in recall is too substantial. The Animal relation is an
exception, where an increase in F
1
measure is observed for the fillers only setting for nearly all entity classes,
leading to 0.87 F
1
for Animals in the extraction task.
An error analysis revealed that for the fillers only setting, most false positives (55%) are due to the fact
that the medical experts did not annotate all occurrences of the correct entity, but only one or a few. 18% are
due to ambiguities of surface forms (for instance the abbreviation ?it? for ?intrathecal? leads to many false
positives). Regarding false negatives, 41% are due to missing entries in our ontology database and further
26% are caused by wrong treatment of characters (mostly wrong transcriptions of characters from the PDF).
30
5 Conclusion and Outlook
We described the challenge of extracting relational descriptions about preclinical experiments on spinal cord
injury from scientific literature. To tackle that challenge, we introduced a cascaded approach of named
entity recognition, followed by relation extraction. Our results show that the first step can be achieved by
relying strongly on domain-specific ontologies. We show that modeling relations as aggregated entities,
and extracting them using a distance filtering principle combined with domain specific knowledge, yields
promising results, specifically for the Animal relation.
Future work will focus on improving the recognition at the correct position in the text. This is a pre-
requisite to actually tackle and evaluate the relation extraction not only on the basis of detected participating
entities. Therefore, improved relation detection approaches will be implemented which relax the assumption
that relevant entities are found close-by in the text. In addition, we will relax the assumption that different
slots of the annotation are all equally important. Finally, we will address aggregation beyond individual
relations in order to allow for a fully accurate holistic assessment of experimental therapies.
Our system offers a semantic analysis of scientific papers on spinal cord injuries. This lays groundwork
for populating a comprehensive semantic database on preclinical studies of SCI treatment approaches as de-
scribed by Brazda et al. (2013), laying ground and supporting transfer from preclinical to clinical knowledge
in the future.
References
N. Brazda, M. Kruse, F. Kruse, T. Kirchhoffer, R. Klinger, and H.-W. M?ller. 2013. The CNR preclinical database
for knowledge management in spinal cord injury research. Abstracts of the Society of Neurosciences, 148(22).
P. Buitelaar, P. Cimiano, A. Frank, M. Hartung, and S. Racioppa. 2008. Ontology-based information extraction
and integration from heterogeneous data sources. Int. J. Hum.-Comput. Stud., 66(11):759?788.
F. J. Damerau. 1964. A Technique for Computer Detection and Correction of Spelling Errors. Commun. ACM,
7(3):171?176, March.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw, S. Strassel, and R. Weischedel. 2004. The Automatic
Content Extraction (ACE) program: tasks, data, and evaluation. In Proceedings of LREC 2004, pages 837?840.
A. Doms and M. Schroeder. 2005. GoPubMed: exploring PubMed with the Gene Ontology. Nucleic Acids Res,
33(Web Server issue):W783?W786, Jul.
D. Ferrucci and A. Lally. 2004. Building an example application with the Unstructured Information Management
Architecture. IBM Systems Journal, 43(3):455?475.
L. Filli and M. E. Schwab. 2012. The rocky road to translation in spinal cord repair. Ann Neurol, 72(4):491?501.
A. Franceschini, D. Szklarczyk, S. Frankild, M. Kuhn, M. Simonovic, A. Roth, J. Lin, P. Minguez, P. Bork, C. von
Mering, and L. J. Jensen. 2013. STRING v9.1: protein-protein interaction networks, with increased coverage
and integration. Nucleic Acids Res, 41(Database issue):D808?D815, Jan.
J. Hakenberg, M. Gerner, M. Haeussler, I. Solt, C. Plake, M. Schroeder, G. Gonzalez, G. Nenadic, and C. M.
Bergman. 2011. The GNAT library for local and remote gene mention normalization. Bioinformatics,
27(19):2769?2771, Oct.
D. Hanisch, K. Fundel, H.-T. Mevissen, R. Zimmer, and J. Fluck. 2005. ProMiner: rule-based protein and gene
entity recognition. BMC Bioinformatics, 6 Suppl 1:S14.
M. Hofmann-Apitius, J. Fluck, L. Furlong, O. Fornes, C. Kolarik, S. Hanser, M. Boeker, S. Schulz, F. Sanz,
R. Klinger, T. Mevissen, T. Gattermayer, B. Oliva, and C. M. Friedrich. 2008. Knowledge environments
representing molecular entities for the virtual physiological human. Philos Trans A Math Phys Eng Sci,
366(1878):3091?3110, Sep.
H. Ji and R. Grishman. 2008. Refining Event Extraction through Cross-Document Inference. In Proceedings of
ACL-08: HLT, pages 254?262, Columbus, Ohio, June. Association for Computational Linguistics.
A. Jimeno, E. Jimenez-Ruiz, V. Lee, S. Gaudan, R. Berlanga, and D. Rebholz-Schuhmann. 2008. Assessment of
disease named entity recognition on a corpus of annotated sentences. BMC Bioinformatics, 9 Suppl 3:S3.
31
J. Lafferty, A. McCallum, and F. C. N. Pereira. 2001. Conditional Random Fields: Probabilistic Models for
Segmenting and Labeling Sequence Data. In Proceedings of ICML 2001, pages 282?289. Morgan Kaufmann.
V. P. Lemmon, A. R. Ferguson, P. G. Popovich, X.-M. Xu, D. M. Snow, M. Igarashi, C. E. Beattie, J. L. Bixby
et al. 2014. Minimum Information About a Spinal Cord Injury Experiment (MIASCI) ? a proposed reporting
standard for spinal cord injury experiments. Neurotrauma. in press.
C. E. Lipscomb. 2000. Medical Subject Headings (MeSH). Bull Med Libr Assoc, 88(3):265?266, Jul.
C. Lok. 2010. Literature mining: Speed reading. Nature, 463(7280):416?418, Jan.
D. Nadeau and S. Sekine. 2007. A survey of named entity recognition and classification. Lingvisticae Investiga-
tiones, 30(1):3?26.
C. Nedellec, R. Bossy, J.-D. Kim, J. jae Kim, T. Ohta, S. Pyysalo, and P. Zweigenbaum, editors. 2013. Proceedings
of the BioNLP Shared Task 2013 Workshop. Association for Computational Linguistics, Sofia, Bulgaria, August.
P. V. Ogren. 2006. Knowtator: a prot?g? plug-in for annotated corpus construction. In Proceedings NAACL/HLT
2006, pages 273?275, Morristown, NJ, USA. Association for Computational Linguistics.
E. Pafilis, S. P. Frankild, L. Fanini, S. Faulwetter, C. Pavloudi, A. Vasileiadou, C. Arvanitidis, and L. J. Jensen.
2013. The SPECIES and ORGANISMS Resources for Fast and Accurate Identification of Taxonomic Names
in Text. PLoS One, 8(6):e65390.
F. Prinz, T. Schlange, and K. Asadullah. 2011. Believe it or not: how much can we rely on published data on
potential drug targets? Nat Rev Drug Discov, 10(9):712, Sep.
H. Saggion, A. Funk, D. Maynard, and K. Bontcheva. 2007. Ontology-Based Information Extraction for Business
Intelligence. In K. A. et al., editor, The Semantic Web, volume 4825 of Lecture Notes in Computer Science,
pages 843?856. Springer.
G. K. Savova, J. J. Masanz, P. V. Ogren, J. Zheng, S. Sohn, K. C. Kipper-Schuler, and C. G. Chute. 2010. Mayo
clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and
applications. J Am Med Inform Assoc, 17(5):507?513.
E. W. Sayers, T. Barrett, D. A. Benson, E. Bolton, S. H. Bryant, K. Canese, V. Chetvernin, D. M. Church, M. Dicuc-
cio, S. Federhen, M. Feolo, I. M. Fingerman, L. Y. Geer, W. Helmberg, Y. Kapustin, S. Krasnov, D. Landsman,
D. J. Lipman, Z. Lu, T. L. Madden, T. Madej, D. R. Maglott, A. Marchler-Bauer, V. Miller, I. Karsch-Mizrachi,
J. Ostell, A. Panchenko, L. Phan, K. D. Pruitt, G. D. Schuler, E. Sequeira, S. T. Sherry, M. Shumway, K. Sirotkin,
D. Slotta, A. Souvorov, G. Starchenko, T. A. Tatusova, L. Wagner, Y. Wang, W. J. Wilbur, E. Yaschenko, and
J. Ye. 2012. Database resources of the National Center for Biotechnology Information. Nucleic Acids Res,
40(Database issue):D13?D25, Jan.
M. Schuemie, R. Jelier, and J. Kors. 2007. Peregrine: lightweight gene name normalization by dictionary lookup.
In Proceedings of the Biocreative 2 workshop 2007, page 131?140, Madrid, Spain, April.
O. Steward, P. G. Popovich, W. D. Dietrich, and N. Kleitman. 2012. Replication and reproducibility in spinal cord
injury research. Exp Neurol, 233(2):597?605, Feb.
S. Strassel, M. Przybocki, K. Peterson, Z. Song, and K. Maeda. 2008. Linguistic Resources and Evaluation
Techniques for Evaluation of Cross-Document Automatic Content Extraction. In Proceedings of the Language
Resources and Evaluation Conference, pages 2706?2709.
P. Thomas, J. Starlinger, A. Vowinkel, S. Arzt, and U. Leser. 2012. GeneView: a comprehensive semantic search
engine for PubMed. Nucleic Acids Res, 40:W585?W591, Jul.
J. Tsujii, J.-D. Kim, and S. Pyysalo, editors. 2011. Proceedings of BioNLP Shared Task 2011 Workshop. Associa-
tion for Computational Linguistics, Portland, Oregon, USA, June.
J. Tsujii, editor. 2009. Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task. Associa-
tion for Computational Linguistics, Boulder, Colorado, June.
D. C. Wimalasuriya and D. Dou. 2010. Ontology-based information extraction: An introduction and a survey of
current approaches. Journal of Information Science, 36(3):306?323.
32
