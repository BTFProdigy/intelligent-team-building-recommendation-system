Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1290?1301, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Besting the Quiz Master:
Crowdsourcing Incremental Classification Games
Jordan Boyd-Graber
iSchool and UMIACS
University of Maryland
jbg@umiacs.umd.edu
Brianna Satinoff, He He, and Hal Daume? III
Department of Computer Science
University of Maryland
{bsonrisa, hhe, hal}@cs.umd.edu
Abstract
Cost-sensitive classification, where the features
used in machine learning tasks have a cost, has
been explored as a means of balancing knowl-
edge against the expense of incrementally ob-
taining new features. We introduce a setting
where humans engage in classification with
incrementally revealed features: the collegiate
trivia circuit. By providing the community with
a web-based system to practice, we collected
tens of thousands of implicit word-by-word
ratings of how useful features are for eliciting
correct answers. Observing humans? classifi-
cation process, we improve the performance
of a state-of-the art classifier. We also use the
dataset to evaluate a system to compete in the
incremental classification task through a reduc-
tion of reinforcement learning to classification.
Our system learns when to answer a question,
performing better than baselines and most hu-
man players.
1 Introduction
A typical machine learning task takes as input a set
of features and learns a mapping from features to a
label. In such a setting, the objective is to minimize
the error of the mapping from features to labels. We
call this traditional setting, where all of the features
are consumed, rapacious machine learning.1
This not how humans approach the same task.
They do not exhaustively consider every feature. Af-
ter a certain point, a human has made a decision
and no longer needs additional features. Even in-
defatigable computers cannot always exhaustively
consider every feature. This is because the result
1Earlier drafts called this ?batch? machine learning, which
confused the distinction between batch and online learning. We
gladly adopt ?rapacious? to make this distinction clearer and
to cast traditional machine learning?that always examines all
features?as a resource hungry approach.
is time sensitive, such as in interactive systems, or
because processing time is limited by the sheer quan-
tity of data, as in sifting e-mail for spam (Pujara et
al., 2011). In such settings, often the best solution
is incremental: allow a decision to be made without
seeing all of an instance?s features. We discuss the
incremental classification framework in Section 2.
Our understanding of how humans conduct incre-
mental classification is limited. This is because com-
plicating an already difficult annotation task is often
an unwise tradeoff. Instead, we adapt a real world
setting where humans are already engaging (eagerly)
in incremental classification?trivia games?and de-
velop a cheap, easy method for capturing human
incremental classification judgments.
After qualitatively examining how humans con-
duct incremental classification (Section 3), we show
that knowledge of a human?s incremental classifi-
cation process improves state-of-the-art rapacious
classification (Section 4). Having established that
these data contain an interesting signal, we build
Bayesian models that, when embedded in a Markov
decision process, can engage in effective incremental
classification (Section 5), and develop new hierar-
chical models combining local and thematic content
to better capture the underlying content (Section 7).
Finally, we conclude in Section 8 and discuss exten-
sions to other problem areas.
2 Incremental Classification
In this section, we discuss previous approaches that
explore how much effort or resources a classifier
needs to come to a decision, a problem not as thor-
oughly examined as the question of whether the de-
cision is right or not.2 Incremental classification is
2When have an externally interrupted feature stream, the
setting is called ?any time? (Boddy and Dean, 1989; Horsch and
Poole, 1998). Like ?budgeted? algorithms (Wang et al 2010),
these are distinct but related problems.
1290
not equivalent to missing features, which have been
studied at training time (Cesa-Bianchi et al 2011),
test time (Saar-Tsechansky and Provost, 2007), and
in an online setting (Rostamizadeh et al 2011). In
contrast, incremental classification allows the learner
to decide whether to acquire additional features.
A common paradigm for incremental classification
is to view the problem as a Markov decision process
(MDP) (Zubek and Dietterich, 2002). The incremen-
tal classifier can either request an additional feature
or render a classification decision (Chai et al 2004;
Ji and Carin, 2007; Melville et al 2005), choosing
its actions to minimize a known cost function. Here,
we assume that the environment chooses a feature
in contrast to a learner, as in some active learning
settings (Settles, 2011). In Section 5, we use a MDP
to decide whether additional features need to be pro-
cessed in our application of incremental classification
to a trivia game.
2.1 Trivia as Incremental Classification
A real-life setting where humans classify documents
incrementally is quiz bowl, an academic competition
between schools in English-speaking countries; hun-
dreds of teams compete in dozens of tournaments
each year (Jennings, 2006). Note the distinction be-
tween quiz bowl and Jeopardy, a recent application
area (Ferrucci et al 2010). While Jeopardy also uses
signaling devices, these are only usable after a ques-
tion is completed (interrupting Jeopardy?s questions
would make for bad television). Thus, Jeopardy is
rapacious classification followed by a race to see?
among those who know the answer?who can punch
a button first. Moreover, buzzes before the question?s
end are penalized.
Two teams listen to the same question.3 In this
context, a question is a series of clues (features) re-
ferring to the same entity (for an example question,
see Figure 1). We assume a fixed feature ordering
for a test sequence (i.e., you cannot request specific
features). Teams interrupt the question at any point
by ?buzzing in?; if the answer is correct, the team
gets points and the next question is read. Otherwise,
the team loses points and the other team can answer.
3Called a ?starter? (UK) or ?tossup? (US) in the lingo, as it
often is followed by a ?bonus? given to the team that answers the
starter; here we only concern ourselves with tossups answerable
by both teams.
After losing a race for the Senate, this politician edited the Om-
aha World-Herald. This man resigned 3 from one of his posts
when the President sent a letter to Germany protesting the Lusi-
tania 3 sinking, and 3 he advocated 3 coining 3 silver at a 16
3 to 1 33 rate 3 compared to 3 gold. He was the 3 three-time
Democratic 3 Party 333 nominee for 3 President 3 but 333
lost to McKinley twice 33 and then Taft, although he served as
Secretary of State 33 under Woodrow Wilson, 3 and he later
argued 3 against Clarence Darrow 3 in the Scopes 33 Monkey
Trial. For ten points, name this 3 man who famously declared
that ?we shall not be crucified on a Cross of 3 Gold?. 3
Figure 1: Quiz bowl question on William Jennings Bryan,
a late nineteenth century American politician; obscure
clues are at the beginning while more accessible clues are
at the end. Words (excluding stop words) are shaded based
on the number of times the word triggered a buzz from any
player who answered the question (darker means more
buzzes; buzzes contribute to the shading of the previous
five words). Diamonds (3) indicate buzz positions.
The answers to quiz bowl questions are well-
known entities (e.g., scientific laws, people, battles,
books, characters, etc.), so the answer space is rel-
atively limited; there are no open-ended questions
of the form ?why is the sky blue?? However, there
are no multiple choice questions?as there are in
Who Wants to Be a Millionaire (Lam et al 2003)?
or structural constraints?as there are in crossword
puzzles (Littman et al 2002).
Now that we introduced the concepts of questions,
answers, and buzzes, we pause briefly to define them
more formally and explicitly connect to machine
learning. In the sequel, we will refer to: questions,
sequences of words (tokens) associated with a single
answer; features, inputs used for decisions (derived
from the tokens in a question); labels, a question?s
correct response; answers, the responses (either cor-
rect or incorrect) provided; and buzzes, positions in
a question where users halted the stream of features
and gave an answer.
Quiz bowl is not a typical problem domain for natu-
ral language processing; why should we care about it?
First, it is a real-world instance of incremental classi-
fication that happens hundreds of thousands of times
most weekends. Second, it is a classification problem
intricately intertwined with core computational lin-
guistics problems such as anaphora resolution, online
sentence processing, and semantic priming. Finally,
quiz bowl?s inherent fun makes it easy to acquire
human responses, as we describe in the next section.
1291
Number of Tokens Revealed
Ac
cu
rac
y
0.4
0.6
0.8
1.0
40 60 80 100
Total
500
1000
1500
2000
2500
3000
Figure 2: Users plotted based on accuracy vs. the number
of tokens?on average?the user took to give an answer.
Dot size and colour represent the total number of ques-
tions answered. Users that answered questions later in the
question had higher accuracy. However, there were users
that were able to answer questions relatively early without
sacrificing accuracy.
3 Getting a Buzz through Crowdsourcing
We built a corpus with 37,225 quiz bowl questions
with 25,498 distinct labels from 121 tournaments
written for tournaments between 1999 and 2010. We
created a webapp4 that simulates the experience of
playing quiz bowl. Text is incrementally revealed
(at a pace adjustable by the user) until users press
the space bar to ?buzz?. Users then answer, and the
webapp judges correctness using a string matching
algorithm. Players can override the automatic check
if the system mistakenly judged an answer incorrect.
Answers of previous users are displayed after answer-
ing a question; this enhances the sense of community
and keeps users honest (e.g., it?s okay to say that ?wj
bryan? is an acceptable answer for the label ?william
jennings bryan?, but ?asdf? is not). We did not see
examples of nonsense answers from malicious users;
in contrast, users were stricter than we expected, per-
haps because protesting required effort.
To collect a set of labels with many buzzes, we
focused on the 1186 labels with more than four dis-
tinct questions. Thus, we shuffled the labels into a
canonical order shown to all users (e.g., everyone
saw a question on ?Jonathan Swift? and then a ques-
tion on ?William Jennings Bryan?, but because these
labels have many questions the specific questions
4Play online or download the datasets at http://umiacs.
umd.edu/?jbg/qb.
Figure 3: A screenshot of the webapp used to collect data.
Users see a question revealed one word at a time. They
signal buzzes by clicking on the answer button and input
an answer.
were different for each user). Participants were ea-
ger to answer questions; over 7000 questions were
answered in the first day, and over 43000 questions
were answered in two weeks by 461 users.
To represent a ?buzz?, we define a function b(q, f)
(?b? for buzz) as the number of times that feature
f occurred in question q at most five tokens before
a user correctly buzzed on that question.5 Aggre-
gating buzzes across questions (summing over q)
shows different features useful for eliciting a buzz
(Figure 4(a)). Some features coarsely identify the
type of answer sought, e.g., ?author?, ?opera?, ?city?,
?war?, or ?god?. Other features are relational, con-
necting the answer to other clues, e.g., ?namesake?,
?defeated?, ?husband?, or ?wrote?. The set of buzzes
help narrow which words are important for matching
a question to its answer; for an example, see how
the word cloud for all of the buzzes on ?Wuthering
5This window was chosen qualitatively by examining the
patterns of buzzes; this is person-dependent, based on reading
comprehension, reaction time, and what reveal speed the user
chose. We leave explicitly modeling this for future work.
1292
(a) Buzzes over all Questions (b) Wuthering Heights Question Text (c) Buzzes on Wuthering Heights
Figure 4: Word clouds representing all words that were a part of a buzz (a), the original text appearing in seven questions
on the book ?Wuthering Heights? by Emily Bro?nte (b), and the buzzes of users on those questions (c). The buzzes
reflect what users remember about the work and is more focused than the complete question text.
Heights? (Figure 4(c)) is much more focused than the
word cloud for all of the words from the questions
with that label (Figure 4(b)).
4 Buzzes Reveal Useful Features
If we restrict ourselves to a finite set of labels, the
process of answering questions is a multiclass clas-
sification problem. In this section, we show that in-
formation gleaned from humans making a similar de-
cision can help improve rapacious machine learning
classification. This validates that our crowdsourcing
technique is gathering useful information.
We used a state-of-the-art maximum entropy clas-
sification model, MEGAM (Daume? III, 2004), that
accepts a per-class mean prior for feature weights
and applied MEGAM to the 200 most frequent labels
(11,663 questions, a third of the dataset). The prior
mean of the feature weight is a convenient, simple
way to incorporate human feature utility; apart from
the mean, all default options are used.
Specifying the prior requires us to specify a weight
for each pair of label and feature. The weight com-
bines buzz information (described in Section 3) and
tf-idf (Salton, 1968). The tf-idf value is computed by
treating the training set of questions with the same
label as a single document.
Buzzes and tf-idf information were combined into
the prior ? for label a and feature f as ?a,f =
[
?b(a, f) + ?I [b(a, f) > 0] + ?
]
tf-idf(a, f). (1)
We describe our weight strategies in increasing order
of human knowledge. If ?, ?, and ? are zero, this
is a na??ve zero prior. If ? only is nonzero, this is a
linear transformation of features? tf-idf. If only ?
is nonzero, this is a linear transformation of buzzed
Weighting ? ? ? Error
zero - - - 0.37
tf-idf - - 3.5 0.14
buzz-binary 7.1 - - 0.10
buzz-linear - 1.5 - 0.16
buzz-tier - 1.1 0.1 0.09
Table 1: Classification error of a rapacious classifier able
to draw on human incremental classification. The best
weighting scheme for each dataset is in bold. Missing
parameter values (-) mean that the parameter is fixed to
zero for that weighting scheme.
words? tf-idf weights. If only ? is non-zero, num-
ber of buzzes is now a linear multiplier of the tf-idf
weight (buzz-linear). Finally we allow unbuzzed
words to have a separate linear transformation if both
? and ? are non-zero (buzz-tier).
Grid search (width of 0.1) on development set error
was used to set parameters. Table 1 shows test error
for weighting schemes and demonstrates that adding
human information as a prior improves classification
error, leading to a 36% error reduction over tf-idf
alone. While not directly comparable (this classifier
is rapacious, not incremental, and has a predefined
answer space), the average user had an error rate of
16.7%.
5 Building an Incremental Classifier
In the previous section we improved rapacious classi-
fication using humans? incremental classification. A
more interesting problem is how to compete against
humans in incremental classification. While in the
previous section we used human data for a training
set, here we use human data as an evaluation set.
Doing so requires us to formulate an incremental rep-
resentation of the contents of questions and to learn
a strategy to decide when to buzz.
1293
Because this is the first machine learning algo-
rithm for quiz bowl, we attempt to provide reason-
able rapacious baselines and compare against our new
strategies. We believe that our attempts represent a
reasonable explanation of the problem space, but ad-
ditional improvements could improve performance,
as discussed in Section 8.
A common way to represent state-dependent strate-
gies is via a Markov decision process (MDP). The
most salient component of a MDP is the policy, i.e., a
mapping from the state space to an action. In our con-
text, a state is a sequence of (thus far revealed) tokens,
and the action is whether to buzz or not. To learn a
policy, we use a standard reinforcement learning tech-
nique (Langford and Zadrozny, 2005; Abbeel and
Ng, 2004; Syed et al 2008): given a representation
of the state space, learn a classifier that can map from
a state to an action. This is also a common paradigm
for other incremental tasks, e.g., shift-reduce pars-
ing (Nivre, 2008).
Given examples of the correct answer given a con-
figuration of the state space, we can learn a MDP
without explicitly representing the reward function.
In this section, we define our method of defining
actions and our representation of the state space.
5.1 Action Space
We assume that there are only two possible actions:
buzz now or wait. An alternative would be a more
expressive action space (e.g., an action for every pos-
sible answer). However, this conflates the question
of when to buzz with what to answer. Instead, we call
the distinct component that provides what to answer
the content model. We describe an initial content
model in Section 5.2, below, and improve the models
further in Section 7. For the moment, assume that
a content model maintains a posterior distribution
over labels and when needed can provide its best
guess (e.g., given the features seen, the best answer
is ?William Jennings Bryan?).
Given the action space, we need to specify where
examples of state space and action come from. In
the language of classification, we need to provide
(x, y) pairs to learn a mapping x 7? y. The clas-
sifier attempts to learn that action y is (?buzz?) in
all states where the content model gave a correct re-
sponse given state x. Negative examples (?wait?)
are applied to states where the content model gave
a wrong answer. Every token in our training set cor-
responds to a classification example; both states are
prevalent enough that we do not to explicitly need to
address class imbalance. This resembles approaches
that merge different classifiers (Riedel et al 2011) or
attempt to estimate confidence of models (Blatz et al
2004). However, here we use partial observations.
This is a simplification of the problem and corre-
sponds to a strategy of ?buzz as soon as you know the
answer?, ignoring all other factors. While reasonable,
this is not always optimal. For example, if you know
your opponent is unlikely to answer a question, it is
better to wait until you are more confident. Incorrect
answers might also help your opponent, e.g., by elim-
inating an incorrect answer. Moreover, strategies in a
game setting (rather than a single question) are more
complicated. For example, if a right answer is worth
+10 points and the penalty for an incorrect question
is ?5, then a team leading by 15 points on the last
question should never attempt to answer. Investigat-
ing such gameplay strategies would require a ?roll
out? of game states (Tesauro and Galperin, 1996) to
explore the efficacy of such strategies. While inter-
esting, we leave these issues to future work.
We also investigated learning a policy directly
from users? buzzes directly (Abbeel and Ng, 2004),
but this performed poorly because the content model
is incompatible with the players? abilities and the
high variation in players? ability and styles (compare
Figure 2).
5.2 State Space
Recall that our goal is to learn a classifier that maps
states to actions; above, we defined the action space
(the classifier?s output) but not the state space, the
classifier?s input. The straightforward parameteriza-
tion of the state space would be all of the words that
have been revealed. However, such a feature set is
very sparse.
We use three components to form the state space:
what information has been observed, what the content
model believes is the correct answer, how confident
the content model is, and whether the content model?s
confidence is changing. We describe each in more
detail below.
Text In addition to the obvious, sparse parameter-
ization that contains all of the features thus far ob-
1294
served, we also include the total number of tokens
revealed and whether the phrase ?for ten points? has
appeared.6
Guess An additional feature that we used to repre-
sent the state space is the current guess of the content
model; i.e., the argmax of the posterior.
Posterior The posterior feature (Pos for short) cap-
tures the shape of the posterior distribution: the prob-
ability of the current guess (the max of the poste-
rior), the difference between the top two probabilities
and the probabilities associated with the fifteen most
probable labels under the posterior.
Change As features are revealed, there is often
a rapid transition from a state of confusion?when
there are many candidates with no clear best choice?
to a state of clarity with the posterior pointing to only
one probable label. To capture when this happens,
we add a binary feature to reflect when the best guess
has changed when a single feature has been revealed.
Other Features We thought that other features
would be useful. While useful on their own, no
features that we tried were useful when the content
model?s posterior was also used as a feature. Fea-
tures that we attempted to use were: a logistic re-
gression model attempting to capture the probability
that any player would answer (Silver et al 2008), a
regression predicting how many individuals would
buzz in the next n words, the year the question was
written, the category of the question, etc.
5.3 Na??ve Content Model
The action space is only deciding when to answer,
having abdicated responsibility for what to answer.
So where does do the answers come from? We as-
sume that at any point we can ask ?what is the highest
probability label given my current feature observa-
tions?? We call the component of our model that
answers this question the content model.
Our first content model is a na??ve Bayes
model (Lewis, 1998) trained over a text collection.
This generative model assumes labels for questions
come from a multinomial distribution ? ? Dir(?)
6The phrase ?for ten points? (abbreviated FTP) appears in
all quiz bowl questions to signal the question?s last sentence or
clause. It is a signal to answer soon, as the final ?giveaway? clue
is next.
and assumes that label l has a word distribution
?l ? Dir(?). Each question n has a label zn and
its words are generated from ?zn . Given labeled ob-
servations, we use the maximum a posteriori (MAP)
estimate of ?l.
Why use a generative model when a discriminative
classifier could use a richer feature space? The most
important reason is that, by definition, it makes sense
to ask a generative model the probability of a label
given a partial observation; such a question is not
well-formed for discriminative models, which expect
a complete feature set. Another important consid-
eration is that generative models can predict future,
unrevealed features (Chai et al 2004); however, we
do not make use of that capability here.
In addition to providing our answers, the content
model also provides an additional, critically impor-
tant feature for our state space: its posterior (pos
for short) probability. With every revealed feature,
the content model updates its posterior distribution
over labels given that t tokens have been revealed in
question n,
p(zn |w1 . . . wt, ?,?). (2)
To train our na??ve Bayes model, we semi-
automatically associate labels with a Wikipedia page
(correcting mistakes manually) and then form the
MAP estimate of the class multinomial distribution
from the Wikipedia page?s text. We did this for the
1065 labels that had at least three human answers,
excluding ambiguous labels associated with multiple
concepts (e.g., ?europa?, ?steppenwolf?, ?georgia?,
?paris?, and ?v?).
Features were taken to be the 25,000 most frequent
tokens and bigrams7 that were not stop words; fea-
tures were extracted from the Wikipedia text in the
same manner as from the question tokens.8
After demonstrating our ability to learn an incre-
mental classifier using this simple content model, we
extend the content model to capture local context and
correlations between similar labels in Section 7.
7We used NLTK (Loper and Bird, 2002) to filter stop words
and we used a ?2 test to identify bigrams with that rejected the
null hypothesis at the 0.01 level.
8The Dirichlet scaling parameter ? was set to 10,000 given
our relatively large vocabulary (25,000) and to not penalize a
label?s posterior probability if there were unseen features; this
corresponds to a pseudocount of 0.4. ? was set to 1.0.
1295
6 Pitting the Algorithm Against Humans
With a state space and a policy, we now have all the
necessary ingredients to have our algorithm compete
against humans. Classification, which allows us to
learn a policy, was done using the default settings of
LIBLINEAR (Fan et al 2008). To determine where
the algorithm buzzes, we provide a sequence of state
spaces until the policy classifier determines that it is
time to buzz.
We simulate competition by taking the human an-
swers and buzzes as a given and ask our algorithm
(independently) to provide its decision on when to
buzz on a test set. We compare the two buzz positions.
If the algorithm buzzed earlier with the right answer,
we consider it to have ?won? the question; equiva-
lently, if the algorithm buzzed later, we consider it to
have ?lost? that question. Ties are rare (less than 1%
of cases), as the algorithm had significantly different
behavior from humans; in the case where there was a
tie, ties were broken in favor of the machine.
Because we have a large, diverse population an-
swering questions, we need aggregate measures of
human performance to get a comprehensive view of
algorithm performance. We use the following metrics
for each question in the test set:
? best: the earliest anyone buzzed correctly
? median: the first buzz after 50% of human buzzes
? mean: for each recorded buzz compute a reward and
we average over all rewards
We compare the algorithm against baseline strategies:
? rap The rapacious strategy waits until the end of the
question and answers the best answer possible.
? ftp Waiting until when ?for 10 points? is said, then
giving the best answer possible.
? indexn Waiting until the first feature after the nth to-
ken has been processed, then giving the best answer
possible. The indices were chosen as the quartiles
for question length (by convention, most questions
are of similar length).
We compare these baselines against policies that de-
cide when to buzz based on the state.
Recall that the non-oracle algorithms were un-
aware of the true reward function. To best simulate
conventional quiz bowl settings, a correct answer
was +10 and the incorrect answer was ?5. The full
payoff matrix for the computer is shown in Table 2.
Cases where the opponent buzzes first but is wrong
are equivalent to rapacious classification, as there is
no longer any incentive to answer early. Thus we
exclude such situations (Outcomes 3, 5, 6 in Table 2)
from the dataset to focus on the challenge of process-
ing clues incrementally.
Computer Human Payoff
1 first and wrong right ?15
2 ? first and correct ?10
3 first and wrong wrong ?5
4 first and correct ? +10
5 wrong first and wrong +5
6 right first and wrong +15
Table 2: Payoff matrix (from the computer?s perspective)
for when agents ?buzz? during a question. To focus on
incremental classification, we exclude instances where the
human interrupts with an incorrect answer, as after an
opponent eliminates themselves, the answering reduces to
rapacious classification.
Table 3 shows the algorithm did much better when
it had access to the posterior. While incremental
algorithms outperform rapacious baselines, they lose
to humans. Against the median and average players,
they lose between three and four points per question,
and nearly twice that against the best players.
Although the content model is simple, this poor
performance is not from the content model never
producing the correct answer. To see this, we also
computed the optimal actions that could be executed.
We called this strategy the oracle strategy; it was able
to consistently win against its opponents. Thus, while
the content model was able to come up with correct
answers often enough to on average win against oppo-
nents (even the best human players), we were unable
to consistently learn winning policies.
There are two ways to solve this problem: create
deeper, more nuanced policies (or the features that
feed into them) or refine content models that provide
the signal needed for our policies to make sound
decisions. We chose to refine the content model, as
we felt we had added all of the obvious features for
learning effective policies.
7 Expanding the Content Model
When we asked quiz bowlers how they answer ques-
tions, they said that they first determine the category
1296
Strategy Features Mean Best Median Index
Classify
text -8.72 -10.04 -6.50 40.36
+guess -5.71 -8.40 -3.95 66.02
+pos -4.13 -7.56 -2.70 67.97
+change -4.02 -7.41 -2.63 77.33
Oracle text 3.36 0.61 4.35 49.90
all -6.61 -9.03 -4.42 100.19
ftp -5.22 -8.62 -4.23 88.65
Rapacious index30 -7.89 -8.71 -6.41 32.23
Baseline index60 -5.16 -7.56 -3.71 61.90
index90 -5.02 -8.62 -3.50 87.13
Table 3: Performance of strategies against users. The
human scoring columns show the average points per ques-
tion (positive means winning on average, negative means
losing on average) that the algorithm would expect to ac-
cumulate per question versus each human amalgam metric.
The index column notes the average index of the token
when the strategy chose to buzz.
of a question, which substantially narrows the an-
swer space. Ideally, the content model should con-
duct the same calculus?if a question seems to be
about mathematics, all answers related with mathe-
matics should be more likely in the posterior. This
was consistent with our error analysis; many errors
were non-sensical (e.g., answering ?entropy? for ?Jo-
hannes Brahms?, when an answer such as ?Robert
Schumann?, another composer, would be better).
In addition, assuming independence between fea-
tures given a label causes us to ignore potentially
informative multiword expressions such as quota-
tions, titles, or dates. Adding a language model to
our content model allows us to capture some of these
phenomena.
To create a model that jointly models categories
and local context, we propose the following model:
1. Draw a distribution over labels ? ? Dir(?)
2. Draw a background distribution over words ?0 ?
Dir(?0~1)
(a) For each category c of questions, draw a distribution
over words ?c ? Dir(?1?0).
i. For each label l in category c, draw a distribu-
tion over words ?l,c ? Dir(?2?c)
A. For each type v, draw a bigram distribution
?l,c,v ? Dir(?3?l,c)
3. Draw a distribution over labels ? ? Dir(?).
4. For each question with category c and N words, draw
answer l ? Mult(?):
(a) Assume w0 ? START
(b) Draw wn ? Mult(?l,c,wn?1) for n ? {1 . . . N}
This creates a language model over categories, la-
bels, and observed words (we use ?words? loosely, as
bigrams replace some word pairs). By constructing
the word distributions using hierarchical distributions
based on domain and ngrams (a much simpler para-
metric version of more elaborate methods (Wood and
Teh, 2009)), we can share statistical strength across
related contexts. We assume that labels are (only)
associated with their majority category as seen in our
training data and that category assignments are ob-
served. All scaling parameters ? were set to 10,000,
? was 1.0, and the vocabulary was still 25,000.
We used the maximal seating assignment (Wallach,
2008) for propagating counts through the Dirichlet
hierarchy. Thus, if the word v appeared Bl,u,v times
in label l following a preceding word u, Sl,v times in
label l, Tc,v times in category c, andGv times in total,
we estimate the probability of a word v appearing
in label k, category t, and after word u as p(wn =
v | lab = l, cat = c, wn?1 = u;~?) =
Bl,u,v + ?3
Sl,v+?2
Tc,v+?1
Gv+?0/V
G?+?0
Tc,?+?2
Sl,?+?2
Bl,u,? + ?3
, (3)
where we use ? to represent marginalization, e.g.
Tc,? =
?
v? Tc,v? . As with na??ve Bayes, Bayes? rule
provides posterior label probabilities (Equation 2).
We compare the na??ve model with models that
capture more of the content in the text in Table 4;
these results also include intermediate models be-
tween na??ve Bayes and the full content model: ?cat?
(omit 2.a.i.A) and ?bigram? (omit 2.a). These models
perform much better than the na??ve Bayes models
seen in Table 3. They are about even against the
mean and median players and lose four points per
question against top players.
7.1 Qualitative Analysis
In this section, we explore what defects are prevent-
ing the model presented here from competing with
top players, exposing challenges in reinforcement
learning, interpreting pragmatic cues, and large data.
Three examples of failures of the model are in Fig-
ure 5. This model is the best performing model of
the previous section.
Too Slow The first example is a question on Mau-
rice Ravel, a French composer known for Bole?ro. The
question leads off with Ravel?s orchestral version of
1297
Strategy Model Mean Best Median Index
Classify
na??ve -4.02 -7.41 -2.63 77.33
cat -1.69 -5.22 0.12 67.97
bigram -3.80 -7.66 -2.51 78.69
bgrm+cat -0.86 -4.46 0.83 63.42
Oracle
naive 3.36 0.61 4.35 49.90
cat 4.48 1.64 5.47 47.88
bigram 3.58 0.87 4.61 49.34
bgrm+cat 4.67 1.99 5.74 46.49
Table 4: As in Table 3, performance of strategies against
users, but with enhanced content models. Modeling both
bigrams and label categories improves overall perfor-
mance.
Mussorgsky?s piano piece ?Pictures at an Exhibition?.
Based on that evidence, the algorithm considers ?Pic-
tures at an Exhibition? the most likely but does not
yet buzz. When it receives enough information to be
sure about the correct answer, over half of the players
had already buzzed. Correcting this problem would
require a more aggressive strategy, perhaps incorpo-
rating the identity of the opponent or estimating the
difficulty of the question.
Mislead by the Content Model The second ex-
ample is a question on Enrico Fermi, an Italian-
American physicist. The first clues are about mag-
netic fields near a Fermi surface, which causes the
content model to view ?magnetic field? as the most
likely answer. The question?s text, however, has
pragmatic cues ?this man? and ?this Italian? which
would have ruled out the abstract answer ?magnetic
field?. Correcting this would require a model that
jointly models content and bigrams (Hardisty et
al., 2010), has a coreference system as its content
model (Haghighi and Klein, 2007), or determines the
correct question type (Moldovan et al 2000).
Insufficient Data The third example is where our
approach had no chance. The question is a very diffi-
cult question about George Washington, America?s
first president. As a sign of its difficulty, only half
the players answered correctly, and only near the end
of the question. The question concerns lesser known
episodes from Washington?s life, including a mistress
caught in the elements. To the content model, of the
several hypotheses it considers, the closest match
it can find is ?Yasunari Kawabata?, who wrote the
novel Snow Country, whose plot matches some of
these keywords. To answer these types of question,
george washington
Tokens Revealed
0.0
0.2
0.4
0.6
0.8
0 10 20 30 40 50 60
maurice ravel
Tokens Revealed
0.0
0.2
0.4
0.6
0.8
1.0
0 10 20 30 40 50 60 70
enrico fermi
Tokens Revealed
0.0
0.2
0.4
0.6
0.8
1.0
0 20 40 60 80 100
charlemagne yasunari kawabata
generals
chill
mistress
language
magnetic field neutrinomagnetic field
magnetic
paradox
zero
this_man
this_italian
pictures
orchestrated
pictures at an exhibition
maurice ravel
this_french
composer
bolero
Prediction
answer
Observation
feature
Buzz
Posterior Opponent
Figure 5: Three questions where our algorithm performed
poorly. It gets ?Maurice Ravel? (top) right but only after
over half the humans had answered correctly (i.e., the
buzz?s hexagon appears when the cyan line is above 0.6);
on ?Enrico Fermi? (middle) it confuses the correct type
of answer (person vs. concept); on ?George Washington?
(bottom) it lacks information to answer correctly. Lines
represent the current estimate posterior probability of the
answer (red) and the proportion of opponents who have
answered the question correctly (cyan). The label of each
of the three questions is above each chart. Words are in
black with arrows and arrows, and the current argmax
answer is at the bottom of the graph in red. The buzz
location is the hexagon.
1298
the repository used to train the content model would
have to be orders of magnitude larger to be able to
link the disparate clues in the question to a consistent
target. The content model would also benefit from
weighting later (more informative) features higher.
7.2 Assumptions
We have made assumptions to solve a problem that
is subtly different that the game of quiz bowl that
a human would play. Some of these were simpli-
fying assumptions, such as our assumption that the
algorithm has a closed set of possible answers (Sec-
tion 5.3). Even with this advantage, the algorithm is
unable to compete with human players, who choose
answers from an unbounded set. On the other hand,
to focus on incremental classification, we idealized
our human opponents so that they never give incor-
rect answers (Section 6). This causes our estimates
of our performance to be lower than they would be
against real players.
8 Conclusion and Future Work
We make three contributions. First, we introduce a
new setting for exploring the problem of incremental
classification: trivia games. This problem is intrin-
sically interesting because of its varied topics and
competitive elements, has a great quantity of stan-
dardized, machine-readable data, and also has the
boon of being cheaply and easily annotated. We took
advantage of that ease and created a framework for
quickly and efficiently gathering examples of humans
doing incremental classification.
There are other potential uses for the dataset; the
progression of clues from obscure nuggets to could
help determine how ?known? a particular aspect of
an entity is (e.g., that William Jennings Bryant gave
the ?Cross of Gold? speech is better known his resig-
nation after the Lusitania sinking, Figure 1). Which
could be used in educational settings (Smith et al
2008) or summarization (Das and Martins, 2007).
The second contribution shows that humans? incre-
mental classification improves state-of-the-art rapa-
cious classification algorithms. While other frame-
works (Zaidan et al 2008) have been proposed to
incorporate user clues about features, the system de-
scribed here provides analogous features without the
need for explicit post-hoc reflection, has faster anno-
tation throughput, and is much cheaper.
The problem of answering quiz bowl questions is
itself a challenging task that combines issues from
language modeling, large data, coreference, and re-
inforcement learning. While we do not address all
of these problems, our third contribution is a sys-
tem that learns a policy in a MDP for incremental
classification even in very large state spaces; it can
successfully compete with skilled human players.
Incorporating richer content models is one of our
next steps. This would allow us to move beyond the
closed-set model and use a more general coreference
model (Haghighi and Klein, 2007) for identifying
answers and broader corpora for training. In addi-
tion, using larger corpora would allow us to have
more comprehensive doubly-hierarchical language
models (Wood and Teh, 2009). We are also inter-
ested in adding richer models of opponents to the
state space that would adaptively adjust strategies as
it learned more about the strengths and weaknesses
of its opponent (Waugh et al 2011).
Further afield, our presentation of sentences
closely resembles paradigms for cognitive experi-
ments in linguistics (Thibadeau et al 1982) but are
much cheaper to conduct. If online processing ef-
fects (Levy et al 2008; Levy, 2011) could be ob-
served in buzzing behavior; e.g., if a confusingly
worded phrase depresses buzzing probability, it could
help validate cognitively-inspired models of online
sentence processing.
Incremental classification is a natural problem,
both for humans and resource-limited machines.
While our data set is trivial (in a good sense), learn-
ing how humans process data and make decisions
in a cheap, easy crowdsourced application can help
us apply new algorithms to improve performance in
settings where features aren?t free, either because of
computational or annotation cost.
1299
Acknowledgments
We thank the many players who played our online
quiz bowl to provide our data (and hopefully had fun
doing so) and Carlo Angiuli, Arnav Moudgil, and
Jerry Vinokurov for providing access to quiz bowl
questions. This research was supported by NSF grant
#1018625. Jordan Boyd-Graber is also supported by
the Army Research Laboratory through ARL Cooper-
ative Agreement W911NF-09-2-0072. Any opinions,
findings, conclusions, or recommendations expressed
are the authors? and do not necessarily reflect those
of the sponsors.
References
Pieter Abbeel and Andrew Y. Ng. 2004. Apprentice-
ship learning via inverse reinforcement learning. In
Proceedings of International Conference of Machine
Learning.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte,
A. Kulesza, A. Sanchis, and N. Ueffing. 2004. Confi-
dence estimation for machine translation. In Proceed-
ings of the Association for Computational Linguistics.
Mark Boddy and Thomas L. Dean. 1989. Solving time-
dependent planning problems. In International Joint
Conference on Artificial Intelligence, pages 979?984.
Morgan Kaufmann Publishers, August.
Nicolo` Cesa-Bianchi, Shai Shalev-Shwartz, and Ohad
Shamir. 2011. Efficient learning with partially ob-
served attributes. Journal of Machine Learning Re-
search, 12:2857?2878.
Xiaoyong Chai, Lin Deng, Qiang Yang, and Charles X.
Ling. 2004. Test-cost sensitive naive bayes classi-
fication. In IEEE International Conference on Data
Mining.
Dipanjan Das and Andre Martins. 2007. A survey on
automatic text summarization. Engineering and Tech-
nology, 4:192?195.
Hal Daume? III. 2004. Notes on CG and LM-
BFGS optimization of logistic regression. Pa-
per available at http://pub.hal3.name/
?daume04cg-bfgs, implementation available at
http://hal3.name/megam/.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
Fan, David Gondek, Aditya A. Kalyanpur, Adam Lally,
J. William Murdock, Eric Nyberg, John Prager, Nico
Schlaefer, and Chris Welty. 2010. Building Watson:
An Overview of the DeepQA Project. AI Magazine,
31(3).
Aria Haghighi and Dan Klein. 2007. Unsupervised coref-
erence resolution in a nonparametric bayesian model.
In Proceedings of the Association for Computational
Linguistics.
Eric Hardisty, Jordan Boyd-Graber, and Philip Resnik.
2010. Modeling perspective using adaptor grammars.
In Proceedings of Emperical Methods in Natural Lan-
guage Processing.
Michael C. Horsch and David Poole. 1998. An anytime
algorithm for decision making under uncertainty. In
Proceedings of Uncertainty in Artificial Intelligence.
Ken Jennings. 2006. Brainiac: adventures in the curious,
competitive, compulsive world of trivia buffs. Villard.
Shihao Ji and Lawrence Carin. 2007. Cost-sensitive fea-
ture acquisition and classification. Pattern Recognition,
40:1474?1485, May.
Shyong K. Lam, David M. Pennock, Dan Cosley, and
Steve Lawrence. 2003. 1 billion pages = 1 million
dollars? mining the web to play ?who wants to be a mil-
lionaire??. In Proceedings of Uncertainty in Artificial
Intelligence.
John Langford and Bianca Zadrozny. 2005. Relating
reinforcement learning performance to classification
performance. In Proceedings of International Confer-
ence of Machine Learning.
Roger P. Levy, Florencia Reali, and Thomas L. Griffiths.
2008. Modeling the effects of memory on human on-
line sentence processing with particle filters. In Pro-
ceedings of Advances in Neural Information Processing
Systems.
Roger Levy. 2011. Integrating surprisal and uncertain-
input models in online sentence comprehension: formal
techniques and empirical results. In Proceedings of the
Association for Computational Linguistics.
David D. Lewis. 1998. Naive (Bayes) at forty: The inde-
pendence assumption in information retrieval. In Claire
Ne?dellec and Ce?line Rouveirol, editors, Proceedings
of European Conference of Machine Learning, number
1398.
Michael L. Littman, Greg A. Keim, and Noam Shazeer.
2002. A probabilistic approach to solving crossword
puzzles. Artif. Intell., 134(1-2):23?55, January.
Edward Loper and Steven Bird. 2002. NLTK: the natu-
ral language toolkit. In Tools and methodologies for
teaching.
Prem Melville, Maytal Saar-Tsechansky, Foster Provost,
and Raymond J. Mooney. 2005. An expected utility
approach to active feature-value acquisition. In Inter-
national Conference on Data Mining, November.
Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada
Mihalcea, Roxana Girju, Richard Goodrum, and Vasile
1300
Rus. 2000. The structure and performance of an open-
domain question answering system. In Proceedings of
the Association for Computational Linguistics.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Comput. Linguist.,
34(4):513?553, December.
Jay Pujara, Hal Daume III, and Lise Getoor. 2011. Using
classifier cascades for scalable e-mail classification.
In Collaboration, Electronic Messaging, Anti-Abuse
and Spam Conference, ACM International Conference
Proceedings Series.
Sebastian Riedel, David McClosky, Mihai Surdeanu, An-
drew McCallum, and Christopher D. Manning. 2011.
Model combination for event extraction in bionlp 2011.
In Proceedings of the BioNLP Workshop.
Afshin Rostamizadeh, Alekh Agarwal, and Peter L.
Bartlett. 2011. Learning with missing features. In
Proceedings of Uncertainty in Artificial Intelligence.
Maytal Saar-Tsechansky and Foster Provost. 2007. Han-
dling missing values when applying classification mod-
els. Journal of Machine Learning Research, 8:1623?
1657, December.
Gerard. Salton. 1968. Automatic Information Organiza-
tion and Retrieval. McGraw Hill Text.
Burr Settles. 2011. Closing the loop: Fast, interactive
semi-supervised annotation with queries on features
and instances. In Proceedings of Emperical Methods
in Natural Language Processing.
David Silver, Richard S. Sutton, and Martin Mu?ller. 2008.
Sample-based learning and search with permanent and
transient memories. In International Conference on
Machine Learning.
Noah A. Smith, Michael Heilman, and Rebecca Hwa.
2008. Question generation as a competitive under-
graduate course project. In Proceedings of the NSF
Workshop on the Question Generation Shared Task and
Evaluation Challenge.
Umar Syed, Michael Bowling, and Robert E. Schapire.
2008. Apprenticeship learning using linear program-
ming. In Proceedings of International Conference of
Machine Learning.
Gerald Tesauro and Gregory R. Galperin. 1996. On-line
policy improvement using monte-carlo search. In Pro-
ceedings of Advances in Neural Information Processing
Systems.
Robert Thibadeau, Marcel A. Just, and Patricia A. Carpen-
ter. 1982. A model of the time course and content of
reading. Cognitive Science, 6.
Hanna M Wallach. 2008. Structured Topic Models for
Language. Ph.D. thesis, University of Cambridge.
Lidan Wang, Donald Metzler, and Jimmy Lin. 2010.
Ranking Under Temporal Constraints. In Proceedings
of the ACM International Conference on Information
and Knowledge Management.
Kevin Waugh, Brian D. Ziebart, and J. Andrew Bagnell.
2011. Computational rationalization: The inverse equi-
librium problem. In Proceedings of International Con-
ference of Machine Learning.
F. Wood and Y. W. Teh. 2009. A hierarchical nonpara-
metric Bayesian approach to statistical language model
domain adaptation. In Proceedings of Artificial Intelli-
gence and Statistics.
Omar F. Zaidan, Jason Eisner, and Christine Piatko. 2008.
Machine learning with annotator rationales to reduce
annotation cost. In Proceedings of the NIPS*2008
Workshop on Cost Sensitive Learning.
Valentina Bayer Zubek and Thomas G. Dietterich. 2002.
Pruning improves heuristic search for cost-sensitive
learning. In International Conference on Machine
Learning.
1301
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1455?1464,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Dynamic Feature Selection for Dependency Parsing
He He Hal Daume? III
Department of Computer Science
University of Maryland
College Park, MD 20740
{hhe,hal}@cs.umd.edu
Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218
jason@cs.jhu.edu
Abstract
Feature computation and exhaustive search
have significantly restricted the speed of
graph-based dependency parsing. We propose
a faster framework of dynamic feature selec-
tion, where features are added sequentially as
needed, edges are pruned early, and decisions
are made online for each sentence. We model
this as a sequential decision-making problem
and solve it by imitation learning techniques.
We test our method on 7 languages. Our dy-
namic parser can achieve accuracies compara-
ble or even superior to parsers using a full set
of features, while computing fewer than 30%
of the feature templates.
1 Introduction
Graph-based dependency parsing usually consists of
two stages. In the scoring stage, we score all pos-
sible edges (or other small substructures) using a
learned function; in the decoding stage, we use com-
binatorial optimization to find the dependency tree
with the highest total score.
Generally linear edge-scoring functions are used
for speed. But they use a large set of features, de-
rived from feature templates that consider different
conjunctions of the edge?s attributes. As a result,
parsing time is dominated by the scoring stage?
computing edge attributes, using them to instanti-
ate feature templates, and looking up the weights of
the resulting features in a hash table. For example,
McDonald et al (2005a) used on average about 120
first-order feature templates on each edge, built from
attributes such as the edge direction and length, the
two words connected by the edge, and the parts of
speech of these and nearby words.
We therefore ask the question: can we use fewer
features to score the edges, while maintaining the ef-
fect that the true dependency tree still gets a higher
score? Motivated by recent progress on dynamic
feature selection (Benbouzid et al, 2012; He et al,
2012), we propose to add features one group at a
time to the dependency graph, and to use these fea-
tures together with interactions among edges (as de-
termined by intermediate parsing results) to make
hard decisions on some edges before all their fea-
tures have been seen. Our approach has a similar
flavor to cascaded classifiers (Viola and Jones, 2004;
Weiss and Taskar, 2010) in that we make decisions
for each edge at every stage. However, in place of
relatively simple heuristics such as a global relative
pruning threshold, we learn a featurized decision-
making policy of a more complex form. Since each
decision can affect later stages, or later decisions in
the same stage, we model this problem as a sequen-
tial decision-making process and solve it by Dataset
Aggregation (DAgger) (Ross et al, 2011), a recent
iterative imitation learning technique for structured
prediction.
Previous work has made much progress on the
complementary problem: speeding up the decoding
stage by pruning the search space of tree structures.
In Roark and Hollingshead (2008) and Bergsma and
Cherry (2010), pruning decisions are made locally
as a preprocessing step. In the recent vine prun-
ing approach (Rush and Petrov, 2012), significant
speedup is gained by leveraging structured infor-
mation via a coarse-to-fine projective parsing cas-
1455
cade (Charniak et al, 2006). These approaches
do not directly tackle the feature selection problem.
Although pruned edges do not require further fea-
ture computation, the pruning step must itself com-
pute similar high-dimensional features just to de-
cide which edges to prune. For this reason, Rush
and Petrov (2012) restrict the pruning models to a
smaller feature set for time efficiency. We aim to do
feature selection and edge pruning dynamically, bal-
ancing speed and accuracy by using only as many
features as needed.
In this paper, we first explore standard static fea-
ture selection methods for dependency parsing, and
show that even a few feature templates can give de-
cent accuracy (Section 3.2). We then propose a
novel way to dynamically select features for each
edge while keeping the overhead of decision mak-
ing low (Section 4). Our present experiments use the
Maximum Spanning Tree (MST) parsing algorithm
(McDonald et al, 2005a; McDonald and Pereira,
2006). However, our approach applies to other
graph-based dependency parsers as well?including
non-projective parsing, higher-order parsing, or ap-
proximations to higher-order parsing that use stack-
ing (Martins et al, 2008), belief propagation (Smith
and Eisner, 2008), or structured boosting (Wang et
al., 2007).
2 Graph-based Dependency Parsing
In graph-based dependency parsing of an n-word in-
put sentence, we must construct a tree y whose ver-
tices 0, 1, . . . n correspond to the root node (namely
0) and the ordered words of the sentence. Each di-
rected edge of this tree points from a head (parent)
to one of its modifiers (child).
Following a common approach to structured pre-
diction problems, the score of a tree y is defined
as a sum of local scores. That is, s?(y) = ? ??
E?y ?(E) =
?
E?y ? ? ?(E), where E ranges
over small connected subgraphs of y that can be
scored individually. Here ?(E) extracts a high-
dimensional feature vector from E together with the
input sentence, and ? denotes a weight vector that
has typically been learned from data.
The first-order model decomposes the tree into
edges E of the form ?h,m?, where h ? [0, n] and
m ? [1, n] (with h 6= m) are a head token and one
of its modifiers. Finding the best tree requires first
computing ???(E) for each of the n2 possible edges.
Since scoring the edges independently in this way
restricts the parser to a local view of the depen-
dency structure, higher-order models can achieve
better accuracy. For example, in the second-order
model of McDonald and Pereira (2006), each local
subgraph E is a triple that includes the head and
two modifiers of the head, which are adjacent to
each other. Other methods that use triples include
grandparent-parent-child triples (Koo and Collins,
2010), or non-adjacent siblings (Carreras, 2007).
Third-order models (Koo and Collins, 2010) use
quadruples, employing grand-sibling and tri-sibling
information.
The usual inference problem is to find the high-
est scoring tree for the input sentence. Note that in
a valid tree, each token 1, . . . , n must be attached
to exactly one parent (either another token or the
root 0). We can further require the tree to be pro-
jective, meaning that edges are not allowed to cross
each other. It is well known that dynamic program-
ming can be used to find the best projective depen-
dency tree in O(n3) time, much as in CKY, for first-
order models and some higher-order models (Eis-
ner, 1996; McDonald and Pereira, 2006).1 When
the projectivity restriction is lifted, McDonald et al
(2005b) pointed out that the best tree can be found in
O(n2) time using a minimum directed spanning tree
algorithm (Chu and Liu, 1965; Edmonds, 1967; Tar-
jan, 1977), though only for first-order models.2 We
will make use of this fast non-projective algorithm
as a subroutine in early stages of our system.
3 Dynamic Feature Selection
Unlike typical feature selection methods that fix a
subset of selected features and use it throughout test-
ing, in dynamic feature selection we choose features
adaptively for each instance. We briefly introduce
this framework below and motivate our algorithm
from empirical results on MST dependency parsing.
1Although the third-order model of Koo and Collins (2010),
for example, takes O(n4) time.
2The non-projective parsing problem becomes NP-hard for
higher-order models. One approximate solution (McDonald
and Pereira, 2006) works by doing projective parsing and then
rearranging edges.
1456
  
.This time , the firms were ready$ .This time , the firms were ready$ $ This time , the firms were ready .
This time , were ready .$ the firms
add feat.group
projectivedecoding
(a) (b) (c)
.This time , were ready$ the firms(d)(e).This time , the firms were ready$(f)
add feat.group
add feat.group
add f eat.group
Figure 1: Dynamic feature selection for dependency parsing. (a) Start with all possible edges except those filtered
by the length dictionary. (b) ? (e) Add the next group of feature templates and parse using the non-projective parser.
Predicted trees are shown as blue and red edges, where red indicates the edges that we then decide to lock. Dashed
edges are pruned because of having the same child as a locked edge; 2-dot-3-dash edges are pruned because of crossing
with a locked edge; fine-dashed edges are pruned because of forming a cycle with a locked edge; and 2-dot-1-dash
edges are pruned since the root has already been locked with one child. (f) Final projective parsing.
3.1 Sequential Decision Making
Our work is motivated by recent progress on dy-
namic feature selection (Benbouzid et al, 2012; He
et al, 2012; Grubb and Bagnell, 2012), where fea-
tures are added sequentially to a test instance based
on previously acquired features and intermediate
prediction results. This requires sequential decision
making. Abstractly, when the system is in some state
s ? S, it chooses an action a = pi(s) from the ac-
tion setA using its policy pi, and transitions to a new
state s?, inducing some cost. In the specific case of
dynamic feature selection, when the system is in a
given state, it decides whether to add some more
features or to stop and make a prediction based on
the features added so far. Usually the sequential de-
cision making problem is solved by reinforcement
learning (Sutton and Barto, 1998) or imitation learn-
ing (Abbeel and Ng, 2004; Ratliff et al, 2004).
The dynamic feature selection framework has
been successfully applied to supervised classifica-
tion and ranking problems (Benbouzid et al, 2012;
He et al, 2012; Gao and Koller, 2010). Below, we
design a version that avoids overhead in our struc-
tured prediction setting. As there are n2 possible
edges on a sentence of length n, we wish to avoid
the overhead of making many individual decisions
about specific features on specific edges, with each
decision considering the current scores of all other
edges. Instead we will batch the work of dynamic
feature selection into a smaller number of coarse-
grained steps.
3.2 Strategy
To speed up graph-based dependency parsing, we
first investigate time usage in the parsing process
on our development set, section 22 of the Penn
Treebank (PTB) (Marcus et al, 1993). In Fig-
ure 2, we observe that (a) feature computation took
more than 80% of the total time; (b) even though
non-projective decoding time grows quadratically in
terms of the sentence length, in practice it is al-
most negligible compared to the projective decoding
time, with an average of 0.23 ms; (c) the second-
order projective model is significantly slower due
to higher asymptotic complexity in both the scoring
and decoding stages.
At each stage of our algorithm, we need to de-
cide whether to use additional features to refine the
edge scores. As making this decision separately for
each of the n2 possible edges is expensive, we in-
stead propose a version that reduces the number of
decisions needed. We show the process for one short
sentence in Figure 1. The first step is to parse us-
ing the current features. We use the fast first-order
non-projective parser for this purpose, since given
observations (b) and (c), we cannot afford to run
projective parsing multiple times. The single result-
ing tree (blue and red edges in Figure 1) has only
1457
0 10 20 30 40 50 60 70sentence length0
200
400
600
800
1000
1200
1400
mean
time(
ms)
1st-order scoring O(n2)2nd-order scoring O(n3)proj dec O(n3)non-proj dec O(n2)2nd-order proj dec O(n3)
Figure 2: Time comparison of scoring time and decoding
time on English PTB section 22.
n edges, and we use a classifier to decide which
of these edges are reliable enough that we should
?lock? them?i.e., commit to including them in the
final tree. This is the only decision that our policy
pi must make. Locked (red) edges are definitely in
the final tree. We also do constraint propagation: we
rule out all edges that conflict with the locked edges,
barring them from appearing in the final tree.3 Con-
flicts are defined as violation of the projective pars-
ing constraints:
? Each word has exactly one parent
? Edges cannot cross each other4
? The directed graph is non-cyclic
? Only one word is attached to the root
For example, in Figure 1(d), the dashed edges are
removed because they have the same child as one of
the locked (red) edges. The 2-dot-3-dash edge time
? firms is removed because it crosses the locked
edge (comma)? were (whereas we ultimately seek
a projective parse). The fine dashed edge were ?
(period) is removed because it forms a cycle with
were ? (period). In Figure 1(e), the 2-dot-1-dash
edge (root) ? time is removed since we allow the
root to have only one modifier.
3Constraint propagation also automatically locks an edge
when all other edges with the same child have been ruled out.
4A reviewer asks about the cost of finding edges that cross a
locked edge. Naively this is O(n2). But at most n edges will be
locked during the entire algorithm, for a total O(n3) runtime?
the same as one call to projective parsing, and far faster in prac-
tice. With cleverness this can even be reduced to O(n2 logn).
Once constraint propagation has finished, we visit
all edges (gray) whose fate is still unknown, and up-
date their scores in parallel by adding the next group
of features.
As a result, most edges will be locked in or ruled
out without needing to look up all of their features.
Some edges may still remain uncertain even after in-
cluding all features. If so, a final iteration (Figure 1
(f)) uses the slower projective parser to resolve the
status of these maximally uncertain edges. In our
example, the parser does not figure out the correct
parent of time until this final step. This final, accu-
rate parser can use its own set of weighted features,
including higher-order features, as well as the pro-
jectivity constraint. But since it only needs to re-
solve the few uncertain edges, both scoring and de-
coding are fast.
If we wanted our parser to be able to produce non-
projective trees, then we would skip this final step
or have it use a higher-order non-projective parser.
Also, at earlier steps we would not prune edges
crossing the locked edges.
4 Methods
Our goal is to produce a faster dependency parser by
reducing the feature computation time. We assume
that we are given three increasingly accurate but in-
creasingly slow parsers that can be called as sub-
routines: a first-order non-projective parser, a first-
order projective parser, and a second-order projec-
tive parser. In all cases, their feature weights have
already been trained using the full set of features,
and we will not change these weights. In general
we will return the output of one of the projective
parsers. But at early iterations, the non-projective
parser helps us rapidly consider interactions among
edges that may be relevant to our dynamic decisions.
4.1 Feature Template Ranking
We first rank the 268 first-order feature templates by
forward selection. We start with an empty list of fea-
ture templates, and at each step we greedily add the
one whose addition most improves the parsing ac-
curacy on a development set. Since some features
may be slower than others (for example, the ?be-
tween? feature templates require checking all tokens
in-between the head and the modifier), we could in-
1458
0 50 100 150 200 250 300 350 400Number of feature templates used0.5
0.6
0.7
0.8
0.9
Unlab
eleda
ttachm
entsc
ore(U
AS)
1st-order non-proj1st-order proj2nd-order
Figure 3: Forward feature selection result using the non-
projective model on English PTB section 22.
stead select the feature template with the highest ra-
tio of accuracy improvement to runtime. However,
for simplicity we do not consider this: after group-
ing (see below), minor changes of the ranks within a
group have no effect. The accuracy is evaluated by
running the first-order non-projective parser, since
we will use it to make most of the decisions. The
112 second-order feature templates are then ranked
by adding them in a similar greedy fashion (given
that all first-order features have already been added),
evaluating with the second-order projective parser.
We then divide this ordered list of feature tem-
plates into K groups: {T1, T2, . . . , TK}. Our parser
adds an entire group of feature templates at each
step, since adding one template at a time would re-
quire too many decisions and obviate speedups. The
simplest grouping method would be to put an equal
number of feature templates in each group. From
Figure 3 we can see that the accuracy increases sig-
nificantly with the first few templates and gradually
levels off as we add less valuable templates. Thus,
a more cost-efficient method is to split the ranked
list into several groups so that the accuracy increases
by roughly the same amount after each group is
added. In this case, earlier stages are fast because
they tend to have many fewer feature templates than
later stages. For example, for English, we use 7
groups of first-order feature templates and 4 groups
of second-order feature templates. The sequence of
group sizes is 1, 4, 10, 12, 47, 33, 161 and 35, 29, 31,
17 for first- and second-order parsing respectively.
4.2 Sequential Feature Selection
Similar to the length dictionary filter of Rush and
Petrov (2012), for each test sentence, we first de-
terministically remove edges longer than the maxi-
mum length of edges in the training set that have the
same head POS tag, modifier POS tag, and direction.
This simple step prunes around 40% of the non-gold
edges in our Penn Treebank development set (Sec-
tion 6.1) at a cost of less than 0.1% in accuracy.
Given a test sentence of length n, we start with
a complete directed graph G(V, E), where E =
{?h,m? : h ? [0, n], m ? [1, n]}. After the length
dictionary pruning step, we compute T1 for all re-
maining edges to obtain a pruned weighted directed
graph. We predict a parse tree using the features so
far (other features are treated as absent, with value
0). Then for each edge in this intermediate tree, we
use a binary linear classifier to choose between two
actions: A = {lock, add}. The lock action ensures
that ?h,m? appears in the final parse tree by prun-
ing edges that conflict with ?h,m?.5 If the classi-
fier is not confident enough about the parent of m,
it decides to add to gather more information. The
add action computes the next group of features for
?h,m? and all other competing edges with child m.
(Since we classify the edges one at a time, deci-
sions on one edge may affect later edges. To im-
prove efficiency and reduce cascaded error, we sort
the edges in the predicted tree and process them as
above in descending order of their scores.)
Now we can continue with the second iteration of
parsing. Overall, our method runs up to K = K1 +
K2 iterations on a given sentence, where we have
K1 groups of first-order features and K2 groups of
second-order features. We run K1 ? 1 iterations
of non-projective first-order parsing (adding groups
T1, . . . , TK1?1), then 1 iteration of projective first-
order parsing (adding group TK1), and finally K2 it-
erations of projective second-order parsing (adding
groups TK1+1, . . . TK).
Before each iteration, we use the result of the pre-
vious iteration (as explained above) to prune some
edges and add a new group of features to the rest. We
5If the conflicting edge is in the current predicted parse tree
(which can happen because of non-projectivity), we forbid the
model to prune it. Otherwise in rare cases the non-projective
parser at the next stage may fail to find a tree.
1459
then run the relevant parser. Each of the three parsers
has a different set of feature weights, so when we
switch parsers on rounds K1 and K1 + 1, we must
also change the weights of the previously added fea-
tures to those specified by the new parsing model.
In practice, we can stop as soon as the fate of all
edges is known. Also, if no projective parse tree
can be constructed at round K1 using the available
unpruned edges, then we immediately fall back to
returning the non-projective parse tree from round
K1 ? 1. This FAIL case rarely occurs in our experi-
ments (fewer than 1% of sentences).
We report results both for a first-order system
where K2 = 0 (shown in Figure 1 and Algorithm 1)
and for a second-order system where K2 > 0.
Algorithm 1 DynFS(G(V, E), pi)
E ? {?h,m? : |h?m| ? lenDict(h,m)}
Add T1 to all edges in E
y? ? non-projective decoding
for i = 2 to K do
Esort ? sort unlocked edges {E : E ? y?} in
descending order of their scores
for ?h,m? ? Esort do
if pi(?(?h,m?)) == lock then
E ? E \ {{?h?,m? ? E : h? 6= h}
?
{?h?,m?? ? E : crosses ?h,m?} ?
{?h?,m?? ? E : cycle with ?h,m?}}
if h == 0 then
E ? E \ {?0,m?? ? E : m? 6= m}
end if
else
Add Ti to {?h?,m?? ? E : m? == m}
end if
end for
if i == K then
y? ? projective decoding
else if i 6= K or FAIL then
y? ? non-projective decoding
end if
end for
return y?
5 Policy Training
We cast this problem as an imitation learning task
and use Dataset Aggregation (DAgger) Ross et al
(2011) to train the policy iteratively.
5.1 Imitation Learning
In imitation learning (also called apprenticeship
learning) (Abbeel and Ng, 2004; Ratliff et al, 2004),
instead of exploring the environment directed by its
feedback (reward) as in typical reinforcement learn-
ing problems, the learner observes expert demon-
strations and aims to mimic the expert?s behavior.
The expert demonstration can be represented as tra-
jectories of state-action pairs, {(st, at)} where t is
the time step. A typical approach to imitation learn-
ing is to collect supervised data from the expert?s
trajectories to learn a policy (multiclass classifier),
where the input is ?(s), a feature representation of
the current state (we call these policy features to
avoid confusion with the parsing features), and the
output is the predicted action (label) for that state.
In the sequential feature selection framework, it is
hard to directly apply standard reinforcement learn-
ing algorithms, as we cannot assign credit to certain
features until the policy decides to stop and let us
evaluate the prediction result. On the other hand,
knowing the gold parse tree makes it easy to ob-
tain expert demonstrations, which enables imitation
learning.
5.2 DAgger
Since the above approach collects training data only
from the expert?s trajectories, it ignores the fact that
the distribution of states at training time and that at
test time are different. If the learned policy can-
not mimic the expert perfectly, one wrong step may
lead to states never visited by the expert due to cu-
mulative errors. This problem of insufficient explo-
ration can be alleviated by iteratively learning a pol-
icy trained under states visited by both the expert
and the learner (Ross et al, 2011; Daume? III et al,
2009; Ka?a?ria?inen, 2006).
Ross et al (2011) proposed to train the policy iter-
atively and aggregate data collected from the previ-
ous learned policy. Let pi? denote the expert?s policy
and spii denote states visited by executing pii. In its
simplest parameter-free form, in each iteration, we
first run the most recently learned policy pii; then for
each state spii on the trajectory, we collect a training
example (?(spii), pi?(spii)) by labeling the state with
the expert?s action. Intuitively, this step intends to
correct the learner?s mistakes and pull it back to the
1460
expert?s trajectory. Thus we can obtain a policy that
performs well under its own induced state distribu-
tion.
5.3 DAgger for Feature Selection
In our case, the expert?s decision is rather straight-
forward. Replace the policy pi in Algorithm 1 by
an expert. If the edge under consideration is a gold
edge, it executes lock; otherwise, it executes add.
Basically the expert ?cheats? by knowing the true
tree and always making the right decision. On our
PTB dev set, it can get 96.47% accuracy6 with only
2.9% of the first-order features. This is an upper
bound on our performance.
We present the training procedure in Algorithm
2. We begin by partitioning the training set into
N folds. To simulate parsing results at test time,
when collecting examples on T i, similar to cross-
validation, we use parsers trained on T? i = T \ T i.
Also note that we show only one pass over training
sentences in Algorithm 2; however, multiple passes
are possible in practice, especially when the training
data is limited.
Algorithm 2 DAgger(T , pi?)
Split the training sentences T into N folds
T 1, T 2, . . . , T N
Initialize D ? ?, pi1 ? pi?
for i = 1 to N do
for G(V, E) ? T i do
Sample trajectories {(spii , pii(spii))} by
DynFS(G(V, E), pii)
D ? D
?
{(?(s), pi?(s)}
end for
end for
Train policy pii+1 on D
return Best pii evaluated on development set
5.4 Policy Features
Our linear edge classifier uses a feature vector ? that
concatenates all previously acquired parsing fea-
tures together with ?meta-features? that reflect con-
fidence in the edge. The classifier?s weights are fixed
6The imperfect performance is because the accuracy is mea-
sured with respect to the gold parse trees. The expert only
makes optimal pruning decisions but the performance depends
on the pre-trained parser as well.
across iterations, but ?(edge) changes per iteration.
We standardize the edge scores by a sigmoid func-
tion. Let s? denote the normalized score, defined
by s??(?h,m?) = 1/(1 + exp{?s?(?h,m?)}). Our
meta-features for ?h,m? include
? current normalized score, and normalized score
before adding the current feature group
? margin to the highest scoring competing edges,
i.e., s?(w, ?h,m?)?maxh? s?(w, ?h?,m?)
where h? ? [0, n] and h? 6= h
? index of the next feature group to be added
We also tried more complex meta-features, for ex-
ample, mean and variance of the scores of compet-
ing edges, and structured features such as whether
the head of e is locked and how many locked chil-
dren it currently has. It turns out that given all the
parsing features, the margin is the most discrimi-
native meta-feature. When it is present, other meta-
features we added do not help much, Thus we do not
include them in our experiments due to overhead.
6 Experiment
6.1 Setup
We generate dependency structures from the PTB
constituency trees using the head rules of Yamada
and Matsumoto (2003). Following convention, we
use sections 02?21 for training, section 22 for de-
velopment and section 23 for testing. We also re-
port results on six languages from the CoNLL-X
shared task (Buchholz and Marsi, 2006) as sug-
gested in (Rush and Petrov, 2012), which cover a
variety of language families. We follow the stan-
dard training/test split specified in the CoNLL-X
data and tune parameters by cross validation when
training the classifiers (policies). The PTB test data
is tagged by a Stanford part-of-speech (POS) tagger
(Toutanova et al, 2003) trained on sections 02?21.
We use the provided gold POS tags for the CoNLL
test data. All results are evaluated by the unlabeled
attachment score (UAS). For fair comparison with
previous work, punctuation is included when com-
puting parsing accuracy of all CoNLL-X languages
but not English (PTB).
For policy training, we train a linear SVM classi-
fier using Liblinear (Fan et al, 2008). For all lan-
guages, we run DAgger for 20 iterations and se-
1461
Language Method
First-order Second-order
Speedup Cost(%) UAS(D) UAS(F) Speedup Cost(%) UAS(D) UAS(F)
Bulgarian
DYNFS 3.44 34.6 91.1 91.3 4.73 16.3 91.6 92.0
VINEP 3.25 - 90.5 90.7 7.91 - 91.6 92.0
Chinese
DYNFS 2.12 42.7 91.0 91.3 2.36 31.6 91.6 91.9
VINEP 1.02 - 89.3 89.5 2.03 - 90.3 90.5
English
DYNFS 5.58 24.8 91.7 91.9 5.27 49.1 92.5 92.7
VINEP 5.23 - 91.0 91.2 11.88 - 92.2 92.4
German
DYNFS 4.71 21.0 89.2 89.3 6.02 36.6 89.7 89.9
VINEP 3.37 - 89.0 89.2 7.38 - 90.1 90.3
Japanese
DYNFS 4.80 15.6 93.7 93.6 8.49 7.53 93.9 93.9
VINEP 4.60 - 91.7 92.0 14.90 - 92.1 92.0
Portuguese
DYNFS 4.36 32.9 87.3 87.1 6.84 40.4 88.0 88.2
VINEP 4.47 - 90.0 90.1 12.32 - 90.9 91.2
Swedish
DYNFS 3.60 37.8 88.8 89.0 5.04 22.1 89.5 89.8
VINEP 4.64 - 88.3 88.5 13.89 - 89.4 89.7
Table 1: Comparison of speedup and accuracy with the vine pruning cascade approach for six languages. In the setup,
DYNFS means our dynamic feature selection model, VINEP means the vine pruning cascade model, UAS(D) and
UAS(F) refer to the unlabeled attachment score of the dynamic model (D) and the full-feature model (F) respectively.
For each language, the speedup is relative to its corresponding first- or second-order model using the full set of features.
Results for the vine pruning cascade model are taken from Rush and Petrov (2012). The cost is the percentage of
feature templates used per sentence on edges that are not pruned by the dictionary filter.
lect the best policy evaluated on the development set
among the 20 policies obtained from each iteration.
6.2 Baseline Models
We use the publicly available implementation of
MSTParser7 (with modifications to the feature com-
putation) and its default settings, so the feature
weights of the projective and non-projective parsers
are trained by the MIRA algorithm (Crammer and
Singer, 2003; Crammer et al, 2006).
Our feature set contains most features proposed
in the literature (McDonald et al, 2005a; Koo and
Collins, 2010). The basic feature components in-
clude lexical features (token, prefix, suffix), POS
features (coarse and fine), edge length and direction.
The feature templates consists of different conjunc-
tions of these components. Other than features on
the head word and the child word, we include fea-
tures on in-between words and surrounding words as
well. For PTB, our first-order model has 268 feature
templates and 76,287,848 features; the second-order
model has 380 feature templates and 95,796,140 fea-
tures. The accuracy of our full-feature models is
7http://www.seas.upenn.edu/?strctlrn/
MSTParser/MSTParser.html
comparable or superior to previous results.
6.3 Results
0 1 2 3 4 5 6Feature selection stage0.0
0.2
0.4
0.6
0.8
1.0
Time/
Accur
acy/E
dgeP
ercen
tage
runtime %UAS %remaining edge %locked edge %pruned edge %
Figure 4: System dynamics on English PTB section 23.
Time and accuracy are relative to those of the baseline
model using full features. Red (locked), gray (unde-
cided), dashed gray (pruned) lines correspond to edges
shown in Figure 1.
In Table 1, we compare the dynamic parsing mod-
els with the full-feature models and the vine prun-
ing cascade models for first-order and second-order
1462
0 10 20 30 40 50 60 70 80Runtime (s)0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
Unlab
eleda
ttachm
entsc
ore(U
AS)
staticdynamic
Figure 5: Pareto curves for the dynamic and static ap-
proaches on English PTB section 23.
parsing. The speedup for each language is defined as
the speed relative to its full-feature baseline model.
We take results reported by Rush and Petrov (2012)
for the vine pruning model. As speed comparison
for parsing largely relies on implementation, we also
report the percentage of feature templates chosen for
each sentence. The cost column shows the average
number of feature templates computed for each sen-
tence, expressed as a percentage of the number of
feature templates if we had only pruned using the
length dictionary filter.
From the table we notice that our first-order
model?s performance is comparable or superior to
the vine pruning model, both in terms of speedup
and accuracy. In some cases, the model with fewer
features even achieves higher accuracy than the
model with full features. The second-order model,
however, does not work as well. In our experi-
ments, the second-order model is more sensitive to
false negatives, i.e. pruning of gold edges, due to
larger error propagation than the first-order model.
Therefore, to maintain parsing accuracy, the policy
must make high-precision pruning decisions and be-
comes conservative. We could mitigate this by train-
ing the original parsing feature weights in conjunc-
tion with our policy feature weights. In addition,
there is larger overhead during when checking non-
projective edges and cycles.
We demonstrate the dynamics of our system in
Figure 4 on PTB section 23. We show how the run-
time, accuracy, number of locked edges and unde-
cided edges change over the iterations in our first-
order dynamic projective parsing. From iterations
1 to 6, we obtain parsing results from the non-
projective parser; in iteration 7, we run the projective
parser. The plot shows relative numbers (percent-
age) to the baseline model with full features. The
number of remaining edges drops quickly after the
second iteration. From Figure 3, however, we notice
that even with the first feature group which only con-
tains one feature template, the non-projective parser
can almost achieve 50% accuracy. Thus, ideally, our
policy should have locked that many edges after the
first iteration. The learned policy does not imitate
the expert perfectly, either because our policy fea-
tures are not discriminative enough, or because a lin-
ear classifier is not powerful enough for this task.
Finally, to show the advantage of making dynamic
decisions that consider the interaction among edges
on the given input sentence, we compare our results
with a static feature selection approach on PTB sec-
tion 23. The static algorithm does no pruning except
by the length dictionary at the start. In each iteration,
instead of running a fast parser and making deci-
sions online, it simply adds the next group of feature
templates to all edges. By forcing both algorithms
to stop after each stage, we get the Pareto curves
shown in Figure 5. For a given level of high accu-
racy, our dynamic approach (black) is much faster
than its static counterpart (blue).
7 Conclusion
In this paper we present a dynamic feature selec-
tion algorithm for graph-based dependency parsing.
We show that choosing feature templates adaptively
for each edge in the dependency graph greatly re-
duces feature computation time and in some cases
improves parsing accuracy. Our model also makes
it practical to use an even larger feature set, since
features are computed only when needed. In future,
we are interested in training parsers favoring the dy-
namic feature selection setting, for example, parsers
that are robust to missing features, or parsers opti-
mized for different stages.
Acknowledgements
This work was supported by the National Science
Foundation under Grant No. 0964681. We thank the
anonymous reviewers for very helpful comments.
1463
References
P. Abbeel and A. Y. Ng. 2004. Apprenticeship learning
via inverse reinforcement learning. In Proceedings of
ICML.
D. Benbouzid, R. Busa-Fekete, and B. Ke?gl. 2012. Fast
classification using space decision DAGs. In Proceed-
ings of ICML.
S. Bergsma and C. Cherry. 2010. Fast and accurate arc
filtering for dependency parsing. In Proceedings of
COLING.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL.
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph
Austerweil, David Ellis, Isaac Haxton, Catherine Hill,
R. Shrivaths, Jeremy Moore, Michael Pozar, and
Theresa Vu. 2006. Multilevel coarse-to-fine PCFG
parsing. In Proceedings of ACL.
Y. J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951?991.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Hal Daume? III, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine Learn-
ing Journal (MLJ).
J. Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards,
(71B):233?240.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: an exploration. In Proceedings
of COLING.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Tianshi Gao and Daphne Koller. 2010. Active classifi-
cation based on value of classifier. In Proceedings of
NIPS.
Alexander Grubb and J. Andrew Bagnell. 2012.
SpeedBoost: Anytime prediction with uniform near-
optimality. In AISTATS.
He He, Hal Daume? III, and Jason Eisner. 2012. Cost-
sensitive dynamic feature selection. In ICML Infern-
ing Workshop.
Matti Ka?a?ria?inen. 2006. Lower bounds for reduc-
tions. Talk at the Atomic Learning Workshop (TTI-C),
March.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Andre? F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers. In
Proceedings of EMNLP.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL, pages 81?88.
Ryan McDonald, K. Crammer, and Fernando Pereira.
2005a. Online large-margin training of dependency
parsers. In Proceedings of ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-projective dependency parsing
using spanning tree algorithms. In Proc. of EMNLP.
N. Ratliff, D. Bradley, J. A. Bagnell, and J. Chestnutt.
2004. Boosting structured prediction for imitation
learning. In Proceedings of ICML.
B. Roark and K. Hollingshead. 2008. Classifying chart
cells for quadratic complexity context-free inference.
In Proceedings of COLING.
Ste?phane. Ross, Geoffrey J. Gordon, and J. Andrew. Bag-
nell. 2011. A reduction of imitation learning and
structured prediction to no-regret online learning. In
Proceedings of AISTATS.
Alexander Rush and Slav Petrov. 2012. Vine pruning for
efficient multi-pass dependency parsing. In Proceed-
ings of NAACL.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In EMNLP.
Richard S. Sutton and Andrew G. Barto. 1998. Rein-
forcement Learning : An Introduction. MIT Press.
R. E. Tarjan. 1977. Finding optimum branchings. Net-
works, 7(1):25?35.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In NAACL.
Paul Viola and Michael Jones. 2004. Robust feal-time
face detection. International Journal of Computer Vi-
sion, 57:137?154.
Qin Iris Wang, Dekang Lin, and Dale Schuurmans. 2007.
Simple training of dependency parsers via structured
boosting. In Proceedings of IJCAI.
David Weiss and Ben Taskar. 2010. Structured predic-
tion cascades. In Proceedings of AISTATS.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with Support Vector Machines. In Pro-
ceedings of IWPT.
1464
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1342?1352,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Don?t Until the Final Verb Wait:
Reinforcement Learning for Simultaneous Machine Translation
Alvin C. Grissom II
and Jordan Boyd-Graber
Computer Science
University of Colorado
Boulder, CO
Alvin.Grissom@colorado.edu
Jordan.Boyd.Graber@colorado.edu
He He, John Morgan,
and Hal Daume? III
Computer Science and UMIACS
University of Maryland
College Park, MD
{hhe,jjm,hal}@cs.umd.edu
Abstract
We introduce a reinforcement learning-
based approach to simultaneous ma-
chine translation?producing a trans-
lation while receiving input words?
between languages with drastically dif-
ferent word orders: from verb-final lan-
guages (e.g., German) to verb-medial
languages (English). In traditional ma-
chine translation, a translator must
?wait? for source material to appear be-
fore translation begins. We remove this
bottleneck by predicting the final verb
in advance. We use reinforcement learn-
ing to learn when to trust predictions
about unseen, future portions of the
sentence. We also introduce an evalua-
tion metric to measure expeditiousness
and quality. We show that our new
translation model outperforms batch
and monotone translation strategies.
1 Introduction
We introduce a simultaneous machine transla-
tion (MT) system that predicts unseen verbs
and uses reinforcement learning to learn when
to trust these predictions and when to wait for
more input.
Simultaneous translation is producing a par-
tial translation of a sentence before the input
sentence is complete, and is often used in im-
portant diplomatic settings. One of the first
noted uses of human simultaneous interpreta-
tion was the Nuremberg trials after the Sec-
ond World War. Siegfried Ramler (2009), the
Austrian-American who organized the transla-
tion teams, describes the linguistic predictions
and circumlocutions that translators would use
to achieve a tradeoff between translation la-
tency and accuracy. The audio recording tech-
nology used by those interpreters sowed the
seeds of technology-assisted interpretation at
the United Nations (Gaiba, 1998).
Performing real-time translation is especially
difficult when information that comes early in
the target language (the language you?re trans-
lating to) comes late in the source language (the
language you?re translating from). A common
example is when translating from a verb-final
(sov) language (e.g., German or Japanese) to
a verb-medial (svo) language, (e.g., English).
In the example in Figure 1, for instance, the
main verb of the sentence (in bold) appears
at the end of the German sentence. An of-
fline (or ?batch?) translation system waits until
the end of the sentence before translating any-
thing. While this is a reasonable approach, it
has obvious limitations. Real-time, interactive
scenarios?such as online multilingual video
conferences or diplomatic meetings?require
comprehensible partial interpretations before
a sentence ends. Thus, a significant goal in
interpretation is to make the translation as
expeditious as possible.
We present three components for an sov-to-
svo simultaneous mt system: a reinforcement
learning framework that uses predictions to
create expeditious translations (Section 2), a
system to predict how a sentence will end (e.g.,
predicting the main verb; Section 4), and a met-
ric that balances quality and expeditiousness
(Section 3). We combine these components in
a framework that learns when to begin trans-
lating sections of a sentence (Section 5).
Section 6 combines this framework with a
1342
ich bin mit dem Zug nach Ulm gefahren
I am with the train to Ulm traveled
I (. . . . . . waiting. . . . . . ) traveled by train to Ulm
Figure 1: An example of translating from a
verb-final language to English. The verb, in
bold, appears at the end of the sentence, pre-
venting coherent translations until the final
source word is revealed.
translation system that produces simultaneous
translations. We show that our data-driven
system can successfully predict unseen parts
of the sentence, learn when to trust them, and
outperform strong baselines (Section 7).
While some prior research has approached
the problem of simultaneous translation?we re-
view these systems in more detail in Section 8?
no current model learns when to definitively
begin translating chunks of an incomplete sen-
tence. Finally, in Section 9, we discuss the
limitations of our system: it only uses the most
frequent source language verbs, it only applies
to sentences with a single main verb, and it
uses an idealized translation system. However,
these limitations are not insurmountable; we
describe how a more robust system can be as-
sembled from these components.
2 Decision Process for
Simultaneous Translation
Human interpreters learn strategies for their
profession with experience and practice. As
words in the source language are observed, a
translator?human or machine?must decide
whether and how to translate, while, for cer-
tain language pairs, simultaneously predicting
future words. We would like our system to do
the same. To this end, we model simultaneous
mt as a Markov decision process (mdp) and
use reinforcement learning to effectively com-
bine predicting, waiting, and translating into
a coherent strategy.
2.1 States: What is, what is to come
The state s
t
represents the current view of
the world given that we have seen t words of
a source language sentence.
1
The state con-
tains information both about what is known
and what is predicted based on what is known.
1
We use t to evoke a discrete version of time. We
only allow actions after observing a complete source
word.
To compare the system to a human transla-
tor in a decision-making process, the state is
akin to the translator?s cognitive state. At any
given time, we have knowledge (observations)
and beliefs (predictions) with varying degrees
of certainty: that is, the state contains the re-
vealed words x
1:t
of a sentence; the state also
contains predictions about the remainder of
the sentence: we predict the next word in the
sentence and the final verb.
More formally, we have a prediction at time
t of the next source language word that will
appear, n
(t)
t+1
, and for the final verb, v
(t)
. For
example, given the partial observation ?ich
bin mit dem?, the state might contain a pre-
diction that the next word, n
(t)
t+1
, will be ?Zug?
and that the final verb v
(t)
will be ?gefahren?.
We discuss the mechanics of next-word and
verb prediction further in Section 4; for now,
consider these black boxes which, after observ-
ing every new source word x
t
, make predictions
of future words in the source language. This
representation of the state allows for a richer set
of actions, described below, permitting simul-
taneous translations that outpace the source
language input
2
by predicting the future.
2.2 Actions: What our system can do
Given observed and hypothesized input, our
simultaneous translation system must decide
when to translate them. This is expressed
in the form of four actions: our system can
commit to a partial translation, predict the
next word and use it to update the transla-
tion, predict the verb and use it to update the
translation, or wait for more words.
We discuss each of these actions in turn be-
fore describing how they come together to in-
crementally translate an entire sentence:
Wait Waiting is the simplest action. It pro-
duces no output and allows the system to re-
ceive more input, biding its time, so that when
it does choose to translate, the translation is
based on more information.
Commit Committing produces translation
output: given the observed source sentence,
produce the best translation possible.
2
Throughout, ?input? refers to source language in-
put, and ?output? refers to target language translation.
1343
STOP
C
o
m
m
i
t
 Observation  Observation (prediction)  Observation  Observation 
 Observation 
1. Mit dem Zug 2. Mit dem Zug bin
ich
3. Mit dem Zug bin
ich nach
4. Mit dem Zug bin
 ich nach ? gefahren ...
 Observation (prediction) 
5. Mit dem Zug bin ich
nach Ulm
? gefahren ...
6. Mit dem Zug bin ich
nach Ulm gefahren.
Output: I traveled
by train
Output: I traveled
by train
to Ulm
Output: I traveled 
by train
to Ulm.
S
with the 
train
I am
with the 
train
by train
to Ulm
S
by train
Wait Wait
Predict
S
I traveled
by train
with the 
train
to
C
o
m
m
i
t
Wait
Fixed 
output
Commit
s
t
a
t
e
Figure 2: A simultaneous translation from source (German) to target (English). The agent
chooses to wait until after (3). At this point, it is sufficiently confident to predict the final verb
of the sentence (4). Given this additional information, it can now begin translating the sentence
into English, constraining future translations (5). As the rest of the sentence is revealed, the
system can translate the remainder of the sentence.
Next Word The next word action takes
a prediction of the next source word and pro-
duces an updated translation based on that
prediction, i.e., appending the predicted word
to the source sentence and translating the new
sentence.
Verb Our system can also predict the source
sentence?s final verb (the last word in the sen-
tence). When our system takes the verb ac-
tion, it uses its verb prediction to update the
translation using the prediction, by placing it
at the end of the source sentence.
We can recreate a traditional batch trans-
lation system (interpreted temporally) by a
sequence of wait actions until all input is ob-
served, followed by a commit to the complete
translation. Our system can commit to par-
tial translations if it is confident, but producing
a good translation early in the sentence often
depends on missing information.
2.3 Translation Process
Having described the state, its components,
and the possible actions at a state, we present
the process in its entirety. In Figure 2, after
each German word is received, the system ar-
rives at a new state, which consists of the source
input, target translation so far, and predictions
of the unseen words. The translation system
must then take an action given information
about the current state. The action will result
in receiving and translating more source words,
transitioning the system to the next state. In
the example, for the first few source-language
words, the translator lacks the confidence to
produce any output due to insufficient informa-
tion at the state. However, after State 3, the
state shows high confidence in the predicted
verb ?gefahren?. Combined with the German
input it has observed, the system is sufficiently
confident to act on that prediction to produce
English translation.
2.4 Consensus Translations
Three straightforward actions?commit, next
word, and verb?all produce translations.
These rely black box access to a translation
(discussed in detail in Section 6): that is, given
a source language sentence fragment, the trans-
lation system produces a target language sen-
tence fragment.
Because these actions can happen more than
once in a sentence, we must form a single con-
sensus translation from all of the translations
that we might have seen. If we have only one
translation or if translations are identical, form-
ing the consensus translation is trivial. But
how should we resolve conflicting translations?
Any time our system chooses an action that
1344
produces output, the observed input (plus extra
predictions in the case of next-word or verb),
is passed into the translation system. That
system then produces a complete translation
of its input fragment.
Any new words?i.e., words whose target
index is greater than the length of any previ-
ous translation?are appended to the previous
translation.
3
Table 1 shows an example of
forming these consensus translations.
Now that we have defined how states evolve
based on our system?s actions, we need to know
how to select which actions to take. Eventu-
ally, we will formalize this as a learned policy
(Section 5) that maps from states to actions.
First, however, we need to define a reward that
measures how ?good? an action is.
3 Objective: What is a good
simultaneous translation?
Good simultaneous translations must optimize
two objectives that are often at odds, i.e., pro-
ducing translations that are, in the end, accu-
rate, and producing them in pieces that are
presented expeditiously. While there are exist-
ing automated metrics for assessing translation
quality (Papineni et al., 2002; Banerjee and
Lavie, 2005; Snover et al., 2006), these must
be modified to find the necessary compromise
between translation quality and expeditious-
ness. That is, a good metric for simultaneous
translation must achieve a balance between
translating chunks early and translating accu-
rately. All else being equal, maximizing either
goal in isolation is trivial: for accurate transla-
tions, use a batch system and wait until the
sentence is complete, translating it all at once;
for a maximally expeditious translation, cre-
ate monotone translations, translating each
word as it appears, as in Tillmann et al. (1997)
and Pytlik and Yarowsky (2006). The former
is not simultaneous at all; the latter is mere
word-for-word replacement and results in awk-
ward, often unintelligible translations of distant
language pairs.
Once we have predictions, we have an ex-
panded array of possibilities, however. On one
extreme, we can imagine a psychic translator?
3
Using constrained decoding to enforce consistent
translation prefixes would complicate our method but
is an appealing extension.
one that can completely translate an imagined
sentence after one word is uttered?as an un-
obtainable system. On the other extreme is a
standard batch translator, which waits until
it has access to the utterer?s complete sentence
before translating anything.
Again, we argue that a system can improve
on this by predicting unseen parts of the sen-
tence to find a better tradeoff between these
conflicting goals. However, to evaluate and op-
timize such a system, we must measure where
a system falls on the continuum of accuracy
versus expeditiousness.
Consider partial translations in a two-
dimensional space, with time (quantized by
the number of source words seen) increasing
from left to right on the x axis and the bleu
score (including brevity penalty against the
reference length) on the y axis. At each point
in time, the system may add to the consensus
translation, changing the precision (Figure 3).
Like an roc curve, a good system will be high
and to the left, optimizing the area under the
curve: the ideal system would produce points
as high as possible immediately. A translation
which is, in the end, accurate, but which is less
expeditious, would accrue its score more slowly
but outperform a similarly expeditious system
which nevertheless translates poorly.
An idealized psychic system achieves this,
claiming all of the area under the curve, as it
would have a perfect translation instantly, hav-
ing no need of even waiting for future input.
4
A batch system has only a narrow (but tall)
sliver to the right, since it translates nothing
until all of the words are observed.
Formally, let Q be the score function for a
partial translation, x the sequentially revealed
source words x
1
, x
2
, . . . , x
T
from time step 1 to
T , and y the partial translations y
1
, y
2
, . . . , y
T
,
where T is the length of the source language
input. Each incremental translation y
t
has a
bleu-n score with respect to a reference r. We
apply the usual bleu brevity penalty to all the
incremental translations (initially empty) to
4
One could reasonably argue that this is not ideal:
a fluid conversation requires the prosody and timing
between source and target to match exactly. Thus, a
psychic system would provide too much information
too quickly, making information exchange unnatural.
However, we take the information-centric approach:
more information faster is better.
1345
Pos Input Intermediate Consensus
1
2 Er He
1
He
1
3 Er wurde
gestaltet
It
1
was
2
designed
3
He
1
was
2
designed
3
4 It
1
was
2
designed
3
He
1
was
2
designed
3
5 Er wurde
gestern
renoviert
It
1
was
2
renovated
3
yesterday
4
He
1
was
2
designed
3
yesterday
4
Table 1: How intermediate translations are combined into a consensus translation. Incorrect
translations (e.g., ?he? for an inanimate object in position 3) and incorrect predictions (e.g.,
incorrectly predicting the verb gestaltet in position 5) are kept in the consensus translation.
When no translation is made, the consensus translation remains static.
Er ist zum Laden gegangen
He went to 
the store
He
He to the
He to the store
Psychic
Monotone
He went 
to the 
store
Batch
Policy
Prediction
He
He went
He went to 
the store
He to the 
store went
He went 
to  the
Source Sentence
?
Figure 3: Comparison of lbleu (the area under
the curve given by Equation 1) for an impossi-
ble psychic system, a traditional batch system,
a monotone (German word order) system, and
our prediction-based system. By correctly pre-
dicting the verb ?gegangen? (to go), we achieve
a better overall translation more quickly.
obtain latency-bleu (lbleu),
Q(x,y) =
1
T
?
t
bleu(y
t
, r) (1)
+ T ? bleu(y
T
, r)
The lbleu score is a word-by-word inte-
gral across the input source sentence. As each
source word is observed, the system receives a
reward based on the bleu score of the partial
translation. lbleu, then, represents the sum of
these T rewards at each point in the sentence.
The score of a simultaneous translation is the
sum of the scores of all individual segments
that contribute to the overall translation.
We multiply the final bleu score by T to en-
sure good final translations in learned systems
to compensate for the implicit bias toward low
latency.
5
4 Predicting Verbs and Next
Words
The next and verb actions depend on predic-
tions of the sentence?s next word and final verb;
this section describes our process for predict-
ing verbs and next words given a partial source
language sentence.
The prediction of the next word in the source
language sentence is modeled with a left-to-
right language model. This is (na??vely) anal-
ogous to how a human translator might use
his own ?language model? to guess upcoming
words to gain some speed by completing, for
example, collocations before they are uttered.
We use a simple bigram language model for
next-word prediction. We use Heafield et al.
(2013).
For verb prediction, we use a generative
model that combines the prior probability of
a particular verb v, p(v), with the likelihood
of the source context at time t given that
verb (namely, p(x
1:t
| v)), as estimated by a
smoothed Kneser-Ney language model (Kneser
and Ney, 1995). We use Pauls and Klein
(2011). The prior probability p(v) is estimated
by simple relative frequency estimation. The
context, x
1:t
, consists of all words observed.
We model p(x
1:t
| v) with verb-specific n-gram
language models. The predicted verb v
(t)
at
time t is then:
arg max
v
p(v)
t
?
i=1
p(x
i
| v, x
i?n+1:i?1
) (2)
5
One could replace T with a parameter, ?, to bias
towards different kinds of simultaneous translations. As
? ??, we recover batch translation.
1346
where x
i?n+1:i?1
is the n?1-gram context. To
narrow the search space, we consider only the
100 most frequent final verbs, where a ?final
verb? is defined as the sentence-final sequence
of verbs and particles as detected by a German
part-of-speech tagger (Toutanova et al., 2003).
6
5 Learning a Policy
We have a framework (states and actions) for
simultaneous machine translation and a metric
for assessing simultaneous translations. We
now describe the use of reinforcement learning
to learn a policy, a mapping from states to
actions, to maximize lbleu reward.
We use imitation learning (Abbeel and Ng,
2004; Syed et al., 2008): given an optimal se-
quence of actions, learn a generalized policy
that maps states to actions. This can be viewed
as a cost-sensitive classification (Langford and
Zadrozny, 2005): a state is represented as a fea-
ture vector, the loss corresponds to the quality
of the action, and the output of the classifier is
the action that should be taken in that state.
In this section, we explain each of these com-
ponents: generating an optimal policy, repre-
senting states through features, and learning a
policy that can generalize to new sentences.
5.1 Optimal Policies
Because we will eventually learn policies via
a classifier, we must provide training exam-
ples to our classifier. These training exam-
ples come from an oracle policy pi
?
that
demonstrates the optimal sequence?i.e., with
maximal lbleu score?of actions for each se-
quence. Using dynamic programming, we can
determine such actions for a fixed translation
model.
7
From this oracle policy, we generate
training examples for a supervised classifier.
State s
t
is represented as a tuple of the ob-
served words x
1:t
, predicted verb v
(t)
, and the
predicted word n
(t)
t+1
. We represent the state to
a classifier as a feature vector ?(x
1:t
, n
(t)
t+1
, v
(t)
).
6
This has the obvious disadvantage of ignoring mor-
phology and occasionally creating duplicates of common
verbs that have may be associated with multiple parti-
cles; nevertheless, it provides a straightforward verb to
predict.
7
This is possible for the limited class of consensus
translation schemes discussed in Section 2.4.
5.2 Feature Representation
We want a feature representation that will al-
low a classifier to generalize beyond the specific
examples on which it is trained. We use sev-
eral general classes of features: features that
describe the input, features that describe the
possible translations, and features that describe
the quality of the predictions.
Input We include both a bag of words rep-
resentation of the input sentence as well as
the most recent word and bigram to model
word-specific effects. We also use a feature
that encodes the length of the source sentence.
Prediction We include the identity of the
predicted verb and next word as well as their re-
spective probabilities under the language mod-
els that generate the predictions. If the model
is confident in the prediction, the classifier can
learn to more so trust the predictions.
Translation In addition to the state, we in-
clude features derived from the possible actions
the system might take. This includes a bag of
words representation of the target sentence, the
score of the translation (decreasing the score is
undesirable), the score of the current consen-
sus translation, and the difference between the
current and potential translation scores.
5.3 Policy Learning
Our goal is to learn a classifier that can accu-
rately mimic the oracle?s choices on previously
unseen data. However, at test time, when we
run the learned policy classifier, the learned
policy?s state distribution may deviate from
the optimal policy?s state distribution due to
imperfect imitation, arriving in states not on
the oracle?s path. To address this, we use
searn (Daume? III et al., 2009), an iterative
imitation learning algorithm. We learn from
the optimal policy in the first iteration, as in
standard supervised learning; in the following
iterations, we run an interpolated policy
pi
k+1
= pi
k
+ (1? )pi
?
, (3)
with k as the iteration number and  the mixing
probability. We collect examples by asking
the policy to label states on its path. The
interpolated policy will execute the optimal
action with probability 1?  and the learned
1347
policy?s action with probability . In the first
iteration, we have pi
0
= pi
?
.
Mixing in the learned policy allows the
learned policy to slowly change from the oracle
policy. As it trains on these no-longer-perfect
state trajectories, the state distribution at test
time will be more consistent with the states
used in training.
searn learns the policy by training a cost-
sensitive classifier. Besides providing the opti-
mal action, the oracle must also assign a cost
to an action
C(a
t
,x) ? Q(x, pi
?
(x
t
))?Q(x, a
t
(x
t
)), (4)
where a
t
(x
t
) represents the translation outcome
of taking action a
t
. The cost is the regret of
not taking the optimal action.
6 Translation System
The focus of this work is to show that given an
effective batch translation system and predic-
tions, we can learn a policy that will turn this
into a simultaneous translation system. Thus,
to separate translation errors from policy er-
rors, we perform experiments with a nearly
optimal translation system we call an omni-
scient translator.
More realistic translation systems will nat-
urally lower the objective function, often in
ways that make it difficult to show that we can
effectively predict the verbs in verb-final source
languages. For instance, German to English
translation systems often drop the verb; thus,
predicting a verb that will be ignored by the
translation system will not be effective.
The omniscient translator translates a source
sentence correctly once it has been fed the ap-
propriate source words as input. There are
two edge cases: empty input yields an empty
output, while a complete, correct source sen-
tence returns the correct, complete translation.
Intermediate cases?where the input is either
incomplete or incorrect?require using an align-
ment. The omniscient translator assumes as
input a reference translation r, a partial source
language input x
1:t
and a corresponding partial
output y. In addition, the omniscient transla-
tor assumes access to an alignment between r
and x. In practice, we use the hmm aligner (Vo-
gel et al., 1996; Och and Ney, 2003).
We first consider incomplete but correct in-
puts. Let y = ?(x
1:t
) be the translator?s output
given a partial source input x
1:t
with transla-
tion y. Then, ?(x
1:t
) produces all target words
y
j
if there is a source word x
i
in the input
aligned to those words?i.e., (i, j) ? a
x,y
?and
all preceding target words can be translated.
(That translations must be contiguous is a nat-
ural requirement for human recipients of trans-
lations). In the case where y
j
is unaligned, the
closest aligned target word to y
j
that has a
corresponding alignment entry is aligned to x
i
;
then, if x
i
is present in the input, y
j
appears in
the output. Thus, our omniscient translation
system will always produce the correct output
given the correct input.
However, our learned policy can make wrong
predictions, which can produce partial trans-
lations y that do not match the reference.
In this event, an incorrect source word x?
i
produces incorrect target words y?
j
, for all
j : (i, j) ? a
x,y
. These y?
j
are sampled from
the ibm Model 1 lexical probability table mul-
tiplied by the source language model y?
j
?
Mult(?
x?
i
)p
LM
(x?).
8
Thus, even if we predict
the correct verb using a next word action, it
will be in the wrong position and thus gener-
ate a translation from the lexical probabilities.
Since translations based on Model 1 probabil-
ities are generally inaccurate, the omniscient
translator will do very well when given correct
input but will produce very poor translations
otherwise.
7 Experiments
In this section, we describe our experimental
framework and results from our experiments.
From aligned data, we derive an omniscient
translator. We use monolingual data in the
source language to train the verb predictor and
the next word predictor. From these features,
we compute an optimal policy from which we
train a learned policy.
7.1 Data sets
For translation model and policy training, we
use data from the German-English Parallel ?de-
news? corpus of radio broadcast news (Koehn,
2000), which we lower-cased and stripped of
8
If a policy chooses an incorrect unaligned word, it
has no effect on the output. Alignments are position-
specific, so ?wrong? refers to position and type.
1348
punctuation. A total of 48, 601 sentence pairs
are randomly selected for building our system.
Of these, we use 70% (34, 528 pairs) for training
word alignments.
For training the translation policy, we re-
strict ourselves to sentences that end with one
of the 100 most frequent verbs (see Section 4).
This results in a data set of 4401 training sen-
tences and 1832 test sentences from the de-news
data. We did this to narrow the search space
(from thousands of possible, but mostly very
infrequent, verbs).
We used 1 million words of news text from
the Leipzig Wortschatz (Quasthoff et al., 2006)
German corpus to train 5-gram language mod-
els to predict a verb from the 100 most frequent
verbs.
For next-word prediction, we use the 18, 345
most frequent German bigrams from the train-
ing set to provide a set of candidates in a lan-
guage model trained on the same set. We use
frequent bigrams to reduce the computational
cost of finding the completion probability of
the next word.
7.2 Training Policies
In each iteration of searn, we learn a
multi-class classifier to implement the pol-
icy. The specific learning algorithm we use
is arow (Crammer et al., 2013). In the com-
plete version of searn, the cost of each action
is calculated as the highest expected reward
starting at the current state minus the actual
roll-out reward. However, computing the full
roll-out reward is computationally very expen-
sive. We thus use a surrogate binary cost: if
the predicted action is the same as the opti-
mal action, the cost is 0; otherwise, the cost
is 1. We then run searn for five iterations.
Results on the development data indicate that
continuing for more iterations yields no benefit.
7.3 Policy Rewards on Test Set
In Figure 4, we show performance of the opti-
mal policy vis-a`-vis the learned policy, as well
as the two baseline policies: the batch policy
and the monotone policy. The x-axis is the
percentage of the source sentence seen by the
model, and the y-axis is a smoothed average of
the reward as a function of the percentage of
the sentence revealed. The monotone policy?s
performance is close to the optimal policy for
llllllllll l l l l l l l l
l
0.25
0.50
0.75
1.00
1.25
0.00 0.25 0.50 0.75 1.00% of Sentence
Sm
oo
the
d A
ve
ra
ge
l Batch Monotone Optimal Searn
Figure 4: The final reward of policies on Ger-
man data. Our policy outperforms all baselines
by the end of the sentence.
0
2500
5000
7500
10000
0
2500
5000
7500
10000
0
2500
5000
7500
10000
0
2500
5000
7500
10000
Batch
Monotone
Optimal
Searn
COMMIT WAIT NEXTWORD VERBAction
Ac
tio
n C
ou
nt
Policy Actions
Figure 5: Histogram of actions taken by the
policies.
the first half of the sentence, as German and
English have similar word order, though they
diverge toward the end. Our learned policy
outperforms the monotone policy toward the
end and of course outperforms the batch policy
throughout the sentence.
Figure 5 shows counts of actions taken by
each policy. The batch policy always commits
at the end. The monotone policy commits at
each position. Our learned policy has an ac-
tion distribution similar to that of the optimal
policy, but is slightly more cautious.
7.4 What Policies Do
Figure 6 shows a policy that, predicting incor-
rectly, still produces sensible output. The pol-
icy correctly intuits that the person discussed
1349
VE
R
B
federal minister of the 
environment angela merkel 
shown the draft of an 
ecopolitical program
bundesumweltministerin 
merkel hat den entwurf
bundesumweltministerin
INPUT OUTPUT
federal minister of the 
environment angela merkel
federal minister of the 
environment angela merkel 
shown the draft
Merkel
gezeigt
bundesumweltministerin 
merkel hat den entwurf 
eines umweltpolitischen 
programms vorgestellt
C
O
M
M
I
T
N
E
X
T
Figure 6: An imperfect execution of a learned
policy. Despite choosing the wrong verb
?gezeigt? (showed) instead of ?vorgestellt? (pre-
sented), the translation retains the meaning.
is Angela Merkel, who was the environmen-
tal minister at the time, but the policy uses
an incorrectly predicted verb. Because of our
poor translation model (Section 6), it renders
this word as ?shown?, which is a poor transla-
tion. However, it is still comprehensible, and
the overall policy is similar to what a human
would do: intuit the subject of the sentence
from early clues and use a more general verb
to stand in for a more specific one.
8 Related Work
Just as mt was revolutionized by statistical
learning, we suspect that simultaneous mt will
similarly benefit from this paradigm, both from
a systematic system for simultaneous transla-
tion and from a framework for learning how to
incorporate predictions.
Simultaneous translation has been
dominated by rule and parse-based ap-
proaches (Mima et al., 1998a; Ryu et al., 2006).
In contrast, although Verbmobil (Wahlster,
2000) performs incremental translation using a
statistical mt module, its incremental decision-
making module is rule-based. Other recent
approaches in speech-based systems focus on
waiting until a pause to translate (Sakamoto
et al., 2013) or using word alignments (Ryu
et al., 2012) between languages to determine
optimal translation units.
Unlike our work, which focuses on predic-
tion and learning, previous strategies for deal-
ing with sov-to-svo translation use rule-based
methods (Mima et al., 1998b) (for instance,
passivization) to buy time for the translator to
hear more information in a spoken context?or
use phrase table and reordering probabilities to
decide where to translate with less delay (Fu-
jita et al., 2013). Oda et al. (2014) is the
most similar to our work on the translation
side. They frame word segmentation as an
optimization problem, using a greedy search
and dynamic programming to find segmenta-
tion strategies that maximize an evaluation
measure. However, unlike our work, the direc-
tion of translation was from from svo to svo,
obviating the need for verb prediction. Simul-
taneous translation is more straightforward for
languages with compatible word orders, such
as English and Spanish (Fu?gen, 2008).
To our knowledge, the only attempt to
specifically predict verbs or any late-occurring
terms (Matsubara et al., 2000) uses pattern
matching on what would today be considered
a small data set to predict English verbs for
Japanese to English simultaneous mt.
Incorporating verb predictions into the trans-
lation process is a significant component of
our framework, though n-gram models strongly
prefer highly frequent verbs. Verb prediction
might be improved by applying the insights
from psycholinguistics. Ferreira (2000) argues
that verb lemmas are required early in sentence
production?prior to the first noun phrase
argument?and that multiple possible syntac-
tic hypotheses are maintained in parallel as the
sentence is produced. Schriefers et al. (1998)
argues that, in simple German sentences, non-
initial verbs do not need lemma planning at
all. Momma et al. (2014), investigating these
prior claims, argues that the abstract relation-
ship between the internal arguments and verbs
triggers selective verb planning.
9 Conclusion and Future Work
Creating an effective simultaneous translation
system for sov to svo languages requires not
only translating partial sentences, but also ef-
fectively predicting a sentence?s verb. Both
elements of the system require substantial re-
finement before they are usable in a real-world
system.
Replacing our idealized translation system
is the most challenging and most important
next step. Supporting multiple translation hy-
potheses and incremental decoding (Sankaran
1350
et al., 2010) would improve both the efficiency
and effectiveness of our system. Using data
from human translators (Shimizu et al., 2014)
could also add richer strategies for simultane-
ous translation: passive constructions, reorder-
ing, etc.
Verb prediction also can be substantially im-
proved both in its scope in the system and
how we predict verbs. Verb-final languages
also often place verbs at the end of clauses,
and also predicting these verbs would improve
simultaneous translation, enabling its effective
application to a wider range of sentences. In-
stead predicting an exact verb early (which is
very difficult), predicting a semantically close
or a more general verb might yield interpretable
translations.
A natural next step is expanding this work
to other languages, such as Japanese, which not
only has sov word order but also requires tok-
enization and morphological analysis, perhaps
requiring sub-word prediction.
Acknowledgments
We thank the anonymous reviewers, as well as
Yusuke Miyao, Naho Orita, Doug Oard, and
Sudha Rao for their insightful comments. This
work was supported by NSF Grant IIS-1320538.
Boyd-Graber is also partially supported by
NSF Grant CCF-1018625. Daume? III and He
are also partially supported by NSF Grant IIS-
0964681. Any opinions, findings, conclusions,
or recommendations expressed here are those
of the authors and do not necessarily reflect
the view of the sponsor.
References
Pieter Abbeel and Andrew Y. Ng. 2004. Appren-
ticeship learning via inverse reinforcement learn-
ing. In Proceedings of the International Confer-
ence of Machine Learning.
Satanjeev Banerjee and Alon Lavie. 2005. ME-
TEOR: An automatic metric for MT evalua-
tion with improved correlation with human judg-
ments. In Proceedings of the Association for
Computational Linguistics. Association for Com-
putational Linguistics.
Koby Crammer, Alex Kulesza, and Mark Dredze.
2013. Adaptive regularization of weight vectors.
Machine Learning, 91(2):155?187.
Hal Daume? III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Ma-
chine Learning Journal (MLJ).
Fernanda Ferreira. 2000. Syntax in language
production: An approach using tree-adjoining
grammars. Aspects of language production,
pages 291?330.
Christian Fu?gen. 2008. A system for simultane-
ous translation of lectures and speeches. Ph.D.
thesis, KIT-Bibliothek.
Tomoki Fujita, Graham Neubig, Sakriani Sakti,
Tomoki Toda, and Satoshi Nakamura. 2013.
Simple, lexicalized choice of translation timing
for simultaneous speech translation. INTER-
SPEECH.
Francesca Gaiba. 1998. The origins of simultane-
ous interpretation: The Nuremberg Trial. Uni-
versity of Ottawa Press.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable
modified Kneser-Ney language model estima-
tion. In Proceedings of the Association for Com-
putational Linguistics.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for n-gram language model-
ing. In Acoustics, Speech, and Signal Processing,
1995. ICASSP-95., 1995 International Confer-
ence on. IEEE.
Philipp Koehn. 2000. German-english parallel cor-
pus ?de-news?.
John Langford and Bianca Zadrozny. 2005. Relat-
ing reinforcement learning performance to clas-
sification performance. In Proceedings of the In-
ternational Conference of Machine Learning.
Shigeki Matsubara, Keiichi Iwashima, Nobuo
Kawaguchi, Katsuhiko Toyama, and Yasuyoshi
Inagaki. 2000. Simultaneous Japanese-English
interpretation based on early predition of En-
glish verb. In Symposium on Natural Language
Processing.
Hideki Mima, Hitoshi Iida, and Osamu Furuse.
1998a. Simultaneous interpretation utilizing
example-based incremental transfer. In Pro-
ceedings of the 17th international conference on
Computational linguistics-Volume 2, pages 855?
861. Association for Computational Linguistics.
Hideki Mima, Hitoshi Iida, and Osamu Furuse.
1998b. Simultaneous interpretation utilizing
example-based incremental transfer. In Proceed-
ings of the Association for Computational Lin-
guistics.
Shota Momma, Robert Slevc, and Colin Phillips.
2014. The timing of verb selection in english
active and passive sentences.
1351
Franz Josef Och and Hermann Ney. 2003. A
systematic comparison of various statistical
alignment models. Computational Linguistics,
29(1):19?51.
Yusuke Oda, Graham Neubig, Sakriani Sakti,
Tomoki Toda, and Satoshi Nakamura. 2014. Op-
timizing segmentation strategies for simultane-
ous speech translation. In Proceedings of the As-
sociation for Computational Linguistics, June.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the Association for Computational
Linguistics.
Adam Pauls and Dan Klein. 2011. Faster and
smaller n-gram language models. In Proceed-
ings of the Association for Computational Lin-
guistics.
Brock Pytlik and David Yarowsky. 2006. Machine
translation for languages lacking bitext via mul-
tilingual gloss transduction. In 5th Conference
of the Association for Machine Translation in
the Americas (AMTA), August.
Uwe Quasthoff, Matthias Richter, and Christian
Biemann. 2006. Corpus portal for search in
monolingual corpora. In International Language
Resources and Evaluation, pages 1799?1802.
Siegfried Ramler and Paul Berry. 2009. Nuremberg
and Beyond: The Memoirs of Siegfried Ramler
from 20th Century Europe to Hawai?i. Booklines
Hawaii Limited.
Koichiro Ryu, Shigeki Matsubara, and Yasuyoshi
Inagaki. 2006. Simultaneous english-japanese
spoken language translation based on incremen-
tal dependency parsing and transfer. In Proceed-
ings of the Association for Computational Lin-
guistics.
Koichiro Ryu, Shigeki Matsubara, and Yasuyoshi
Inagaki. 2012. Alignment-based translation
unit for simultaneous japanese-english spoken di-
alogue translation. In Innovations in Intelligent
Machines?2, pages 33?44. Springer.
Akiko Sakamoto, Nayuko Watanabe, Satoshi Ka-
matani, and Kazuo Sumita. 2013. Development
of a simultaneous interpretation system for face-
to-face services and its evaluation experiment in
real situation.
Baskaran Sankaran, Ajeet Grewal, and Anoop
Sarkar. 2010. Incremental decoding for phrase-
based statistical machine translation. In Pro-
ceedings of the Joint Fifth Workshop on Statis-
tical Machine Translation.
H Schriefers, E Teruel, and RM Meinshausen.
1998. Producing simple sentences: Results from
picture?word interference experiments. Journal
of Memory and Language, 39(4):609?632.
Hiroaki Shimizu, Graham Neubig, Sakriani Sakti,
Tomoki Toda, and Satoshi Nakamura. 2014.
Collection of a simultaneous translation corpus
for comparative analysis. In International Lan-
guage Resources and Evaluation.
Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
study of translation edit rate with targeted hu-
man annotation. In In Proceedings of Associa-
tion for Machine Translation in the Americas.
Umar Syed, Michael Bowling, and Robert E.
Schapire. 2008. Apprenticeship learning using
linear programming. In Proceedings of the Inter-
national Conference of Machine Learning.
Christoph Tillmann, Stephan Vogel, Hermann Ney,
and Alex Zubiaga. 1997. A dp-based search us-
ing monotone alignments in statistical transla-
tion. In Proceedings of the Association for Com-
putational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D
Manning, and Yoram Singer. 2003. Feature-rich
part-of-speech tagging with a cyclic dependency
network. In Conference of the North American
Chapter of the Association for Computational
Linguistics, pages 173?180.
Stephan Vogel, Hermann Ney, and Christoph Till-
mann. 1996. HMM-based word alignment in
statistical translation. In Proceedings of the 16th
International Conference on Computational Lin-
guistics (COLING).
Wolfgang Wahlster. 2000. Verbmobil: foundations
of speech-to-speech translation. Springer.
1352
