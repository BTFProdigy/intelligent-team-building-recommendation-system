R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 600 ? 611, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
A Phrase-Based Context-Dependent Joint Probability 
Model for Named Entity Translation 
Min Zhang1, Haizhou Li1, Jian Su1, and Hendra Setiawan1,2 
1
 Institute for Infocomm Research,  
21 Heng Mui Keng Terrace, Singapore 119613 
{mzhang, hli, sujian, stuhs}@i2r.a-star.edu.sg 
2
 Department of Computer Science,  
National University of Singapore, Singapore, 117543 
hendrase@comp.nus.edu.sg 
Abstract. We propose a phrase-based context-dependent joint probability 
model for Named Entity (NE) translation. Our proposed model consists of a 
lexical mapping model and a permutation model. Target phrases are generated 
by the context-dependent lexical mapping model, and word reordering is per-
formed by the permutation model at the phrase level. We also present a two-
step search to decode the best result from the models. Our proposed model is 
evaluated on the LDC Chinese-English NE translation corpus. The experiment 
results show that our proposed model is high effective for NE translation.  
1   Introduction 
A Named Entity (NE) is essentially a proper noun phrase. Automatic NE translation is 
an indispensable component of cross-lingual applications such as machine translation 
and cross-lingual information retrieval and extraction.  
NE is translated by a combination of meaning translation and/or phoneme trans-
literation [1]. NE transliteration has been given much attention in the literature. 
Many attempts, including phoneme and grapheme-based methods, various machine 
learning and rule-based algorithms [2,3] and Joint Source-Channel Model (JSCM) 
[4], have been made recently to tackle the issue of NE transliteration. However, 
only a few works have been reported in NE translation. Chen et al [1] proposed a 
frequency-based approach to learn formulation and transformation rules for multi-
lingual Named Entities (NEs). Al-Onaizan and Knight [5] investigated the transla-
tion of Arabic NEs to English using monolingual and bilingual resources. Huang et 
al. [6] described an approach to translate rarely occurring NEs by combining pho-
netic and semantic similarities. In this paper, we pay special attention to the issue of 
NE translation.  
Although NE translation is less sophisticated than machine translation (MT) in gen-
eral, to some extent, the issues in NE translation are similar to those in MT. Its chal-
lenges lie in not only the ambiguity in lexical mapping such as <?(Fu),Deputy> and 
<?(Fu),Vice> in Fig.1 in the next page, but also the position permutation and fertility 
of words. Fig.1 illustrates two excerpts of NE translation from the LDC corpus [7]: 
 A Phrase-Based Context-Dependent Joint Probability Model 601 
(a) Regional office of science and technology for Africa 
 
??(FeiZhou) ??(DiQu) ??(KeJi) ???(BanShiChu) 
 
 
(b) Deputy chief of staff to office of the vice president 
 
   ?(Fu) ??(ZongTong) ???(BanGongShi)?(Fu)??(ZhuRen) 
Fig. 1. Example bitexts with alignment 
where the italic word is the Chinese pinyin transcription. 
Inspired by the JSCM model for NE transliteration [4] and the success of statistical 
phrase-based MT research [8-12], in this paper we propose a phrase-based context-
dependent joint probability model for NE translation. It decomposes the NE transla-
tion problem into two cascaded steps: 
1)  Lexical mapping step, using the phrase-based context-dependent joint prob-
ability model, where the appropriate lexical item in the target language is 
chosen for each lexical item in the source language;  
2)  Reordering step, using the phrase-based n-gram permutation model, where 
the chosen lexical items are re-arranged in a meaningful and grammatical 
order of target language.  
A two-step decoding algorithm is also presented to allow for effective search of the 
best result in each of the steps. 
The layout of the paper is as follows. Section 2 introduces the proposed model. In 
Section 3 and 4, the training and decoding algorithms are discussed. Section 5 reports 
the experimental results. In Section 6, we compare our model with the other relevant 
existing models. Finally, we conclude the study in Section 7. 
2   The Proposed Model 
We present our method by starting with a definition of translation unit in Section 2.1, 
followed by the formulation of the lexical mapping model and the permutation model 
in Section 2.2. 
2.1   Defining Translation Unit 
Phrase level translation models in statistical MT have demonstrated significant im-
provement in translation quality by addressing the problem of local re-ordering across 
language boundaries [8-12]. Thus we also adopt the same concept of phrase used in 
statistical phrase-based MT [9,11,12] as the basic NE translation unit to address the 
problems of word fertility and local re-ordering within phrase.  
Suppose that we have Chinese as the source language 1 1... ...=
J
j Jc c c c and Eng-
lish as the target language 1 1... ...
I
i Ie e e e=  in an NE translation 1 1( , )J Ic e , where 
602 M. Zhang et al 
1
J
jc c?  and 1
I
ie e?  are Chinese and English words respectively. Given a directed 
word alignment A :{ 1 1?J Ic e , 1 1?I Je c }, the set of the bilingual phrase pairs ?  is 
defined as follows: 
2 2
1 11 1
1 2 1 2
( , , )={ ( , ) :
                        { ... }, { ... }:
                          }              
j iJ I
j ic e c e
j j j i i i j i
vice versa
?
? ? ? ? ? ?
?
A
A
                   (1)  
The above definition means that two phrases are considered to be translations of 
each other, if the words are aligned exclusively within the phrase pair, and not to the 
words outside [9,11,12]. The phrases have to be contiguous and a null phrase is not 
allowed. 
Suppose that the NE pair 1 1( , )J Ic e  is segmented into X phrase pairs ( 1Xc% , 1Xe% ) ac-
cording to the phrase pair set ? , where 1
Xe% is reordered so that the phrase alignment 
is in monotone order, i.e., xc% is aligned ?% %x xc e For simplicity, we denote by 
,? =< >% %x x xc e  the xth phrase pair in ( 1Xc% , 1Xe% ) = 1... ...x X? ? ? , ? ? ?x . 
2.2   Lexical Mapping Model and Permutation Model 
Given the phrase pair set ? , an NE pair ( 1Jc , 1Ie ) can be rewritten as ( 1Xc% , 1Xe% ) = 
1... ...x X? ? ? = 1X? . Let us describe a Chinese to English (C2E) bilingual training 
corpus as the output of a generative stochastic process: 
 
 
(1) Initialize queue Qc and  Qe as empty sequences; 
(2) Select a phrase pair ,x x xc e? =< >% %  according to the probability distribu-
tion 11( | )xxp ?? ? , remove x?  from ? ; 
(3) Append the phrase xc%  to Qc and append the phrase xe%  to Qe; 
(4) Repeat steps 2) and 3) until ? is empty; 
(5) Reorder all phrases in Qe according to the probability distribution of the 
permutation model; 
(6) Output Qe and Qc . 
 
As 11( | )xxp ?? ?  is typically obtained from a source-ordered aligned bilingual 
corpus, reordering is needed only for the target language. According to this generative 
story, the joint probability of the NE pair ( 1Jc , 1Ie ) can then be obtained by summing 
the probabilities over all possible ways of generating various sets of ? and all possi-
ble permutations that can arrive at ( 1
J
c , 1
I
e ).  This joint probability can be formulated 
 A Phrase-Based Context-Dependent Joint Probability Model 603 
in Eq.(2). Here we assume that the generation of the set ? and the reordering process 
are modeled by n-order Markov models, and the reordering process is independent of 
the source word position. 
1
1 1 1 1 1
1
1
1
( , )= { ( ) * ( | )}
       {( ( | ))* ( | )}
?
?
?
?
=
?
? ??
?
? ?
%
% %X
J I X I X
X
kx X
x x n k
x
p c e p p e e
p p e e
                     (2) 
1
1 1
1
( | ) ( | )xX
x x n
X
kk X
k k k
x
p e e p e e ?
?
=
? ?% % % %                                                      (3) 
where 
1
% Xkke  stands for one of the permutational sequences of 1% Xe  that can yield 1Ie  
by linearly joining all phrases, i.e., 
11
= % XkI ke e ().  The generative process, as formu-
lated above, does not try to capture how the source NE is mapped into the target NE, 
but rather how the source and target translation units can be generated simultaneously 
in the source order and how the target NE can be constructed by reordering the target 
phrases, 1% Xe .  
In essence, our proposed model consists of two sub-models: a lexical mapping 
model (LMM), characterized by 1( | )xx x np ??? ? , that models the monotonic genera-
tive process of phrase pairs; and a permutation model (PM), characterized by 
1( | )x
x x n
k
k kp e e ?
?
% % , that models the permutation process for reordering of the target 
language. The LMM in this paper is among the first attempts to introduce context-
dependent lexical mapping into statistical MT (Och et al, 2003). The PM here is also 
different from the widely used position-based distortion model in that it models 
phrase connectivity instead of position distortion. Although PM functions as an n-
gram language model, it only models the ordering connectivity between target lan-
guage phrases, i.e., it is not in charge of target word selection. 
Since the proposed model is phrase-based and we use conditional joint probability 
in LMM and use context-dependent n-gram in PM, we call the proposed model a 
phrase-based context-dependent joint probability model. 
3   Training 
Following the modeling strategy discussed above, the training process consists of 
three steps: phrase alignment, reordering of corpus, and learning statistical parameters 
for lexical mapping and permutation models. 
3.1   Acquiring Phrase Pairs 
To reduce vocabulary size and avoid sparseness, we constrain the phrase length to up 
to three words and the lower-frequency phrase pairs are pruned out for accurate 
604 M. Zhang et al 
phrase-alignment1. Given a word alignment corpus which can be obtained by means 
of the publicly available GIZA++ toolkit [15], it is very straightforward to construct 
the phrase-alignment corpus by incrementally traversing the word-aligned NE from 
left to right2. The set of resulting phrase pairs forms a lexical mapping table.  
3.2   Reordering Corpus 
The context-dependent lexical mapping model assumes monotonic alignment in the 
bilingual training corpus. Thus, the phrase aligned corpus needs to be reordered so 
that it is in either source-ordered or target-ordered alignment. We choose to reorder 
the target phrases to follow the source order. Only in this way can we use the lexical 
mapping model to describe the monotonic generative process and leave the reordering 
of target translation units to the permutation model.  
3.3   Training LMM and PM  
According to Eq. (2), the lexical mapping model (LMM) and the permutation 
model (PM) can be interpreted as a kind of n-gram Markov model. The phrase pair is 
the basic token of LMM and the target phrase is the basic token of PM. A bilingual 
corpus aligned in the source language order is used to train LMM, and a target lan-
guage corpus with phrase segmentation in their original word order is used to train 
PM. Given the two corpora, we use the SRILM Toolkit [13] to train the two n-gram 
models. 
4   Decoding 
The proposed modeling framework allows LMM and PM decoding to cascade as in 
Fig.2.  
 
Fig. 2. A cascaded decoding strategy 
The two-step operation is formulated by Eq.(4) and Eq.(5). Here, the probability 
summation as in Eq.(2) is replaced with maximization to reduce the computational 
complexity: 
1
1
1
? arg max{ ( | )}
X
X x
x x n
x
e p ?
?
?
=
= ? ??%                                                 (4) 
                                                          
1
  Koehn et. al. [12] found that that in MT learning phrases longer than three words and learning 
phrases from high-accuracy word-alignment does not have strong impact on performance. 
2
  For the details of the algorithm to acquire phrase alignment from word alignment, please refer 
to the section 2.2 & 3.2 in [9] and the section 3.1 in [12].  
%1?Xe
LMM 
Decoder
PM 
Decoder
1
Jc 1Ie
 A Phrase-Based Context-Dependent Joint Probability Model 605 
1
1
1
? arg max{ ( | )}x
x x n
X
kI
k k
x
e p e e ?
??
=
= ? % %                                                 (5) 
LMM decoding: Given the input 1
Jc , the LMM decoder searches for the most prob-
able phrase pair set ? in the source order using Eq.(4). Since this is a monotone 
search problem, we use a stack decoder [14,18] to arrive at the n-best results. 
PM decoding: Given the translation phrase sequence 1?
Xe% from the LMM decoder, 
the PM decoder searches for the best phrase order that gives the highest n-gram score 
by using Eq.(5) in the search space ? , which is all the !X  permutations of the all 
phrases in 1?
Xe% . This is a non-monotone search problem. 
The PM decoder conducts a time-synchronized search from left to right, where time 
clocking is synchronized over the number of phrases covered by the current partial 
path. To reduce the search space, we prune the partial paths along the way.  Two par-
tial paths are considered identical if they satisfy the following both conditions: 
1) They cover the same set of phrases regardless of the phrase order; 
2) The last n-1 phrases and their ordering are identical, where n is the order 
of the n-gram permutation model. 
For any two identical partial paths, only the path with higher n-gram score is retained. 
According to Eq. (5), the above pruning strategy is risk-free because the two partial 
paths cover the exact same portion of input phrases and the n-gram histories for the 
next input phrases in the two partial paths are also identical. 
It is also noteworthy that the decoder only needs to perform / 2X  expansions as 
after / 2X  expansions, all combinations of / 2X  phrases would have been explored 
already. Therefore, after / 2X  expansions, we only need to combine the correspond-
ing two partial paths to make up the entire input phrases, then select the path with 
highest n-gram score as the best translation output. 
Let us examine the number of paths that the PM decoder has to traverse. The prun-
ing reduces the search space by a factor of !Z , from !
( )!
Z
XP
X
X Z
=
?
 
to
!
! ( )!
Z
XC
X
Z X Z
=
? ?
, where Z is the number of phrases in a partial path. 
Since X ZX
Z
XC C
?
= , the maximum number of paths that we have to traverse is / 2XXC . 
For instance, when 10X = , the permutation decoder traverses 510 252C =  paths 
instead of the 510 30, 240P = in an exhausted search. 
By cascading the translation and permutation steps, we greatly reduce the search 
space. In LMM decoding, the traditional stack decoder for monotone search is very 
fast. In PM decoding, since most of NE is less than 10 phrases, the permutation de-
coder only needs to explore at most 510 252C =  living paths due to our risk-free prun-
ing strategy. 
606 M. Zhang et al 
5   Experiments 
5.1   Experimental Setting and Modeling 
All the experiments are conducted on the LDC Chinese-English NE translation corpus 
[7]. The LDC corpus consists of a large number of Chinese-Latin language NE en-
tries. Table 1 reports the statistics of the entire corpus. Because person and place 
names in this corpus are translated via transliteration, we only extract the categories 
of organization, industry, press, international organization, and others to form a cor-
pus subset for our NE translation experiment, as indicated in bold in Table 1. As the 
corpus is in its beta release, there are still many undesired entries in it. We performed 
a quick proofreading to correct some errors and remove the following types of entries:  
1) The duplicate entry; 
2) The entry of single Chinese or English word;  
3) The entries whose English translation contains two or more non-English words. 
We also segment the Chinese translation into a word sequence. Finally, we obtain a 
corpus of 74,606 unique bilingual entries, which are randomly partitioned into 10 
equal parts for 10-fold cross validation.  
Table 1.  Statistics of the LDC Corpus 
# of Entries  
Category C2E E2C 
Person 486,212 572,213 
Place 276,382 298,993 
Who-is-Who 30,028 36,881 
Organization 30,800 37,145 
Industry 54,747 58,468 
Press 29,757 32,922 
Int?l Org 7,040 7,040 
Others 13,007 14,066 
As indicated in Section 1, although MT is more difficult than NE translation, they 
both have many properties in common, such as lexical mapping ambiguity and permu-
tation/distortion. Therefore, to establish a comparison, we use the publicly available 
statistical MT training and decoding tools, which can represent the state-of-the-art of 
statistical phrase-based MT research, to carry out the same NE translation experiments 
as reference cases. All the experiments conducted in this paper are listed as follow: 
1) IBM method C: word-based IBM Model 4 trained by GIZA++3 [15] and ISI 
Decoder4 [14,16]; 
                                                          
3
 http://www.fjoch.com/ 
4
 http://www.isi.edu/natural-language/software/decoder/manual.html 
 A Phrase-Based Context-Dependent Joint Probability Model 607 
2) IBM method D:   phrase-based IBM Model 4 trained by GIZA++ on phrase-
aligned corpus and ISI Decoder working on phrase-segmented testing corpus. 
3) Koehn method: Koehn et al?s phrase-based model [12] and PHARAOH5 de-
coder6; 
4) Our method: phrase-based bi-gram LMM and bi-gram PM, and our two-step 
decoder. 
To make an accurate comparison, all the above three phrase-based models are 
trained on the same phrase-segmented and aligned corpus, and tested on the same 
phrase-segmented corpus. ISI Decoder carries out a greedy search, and PHARAOH is 
a beam-search stack decoder. To optimize their performances, the two decoders are 
allowed to do unlimited reordering without penalty. We train trigram language mod-
els in the first three experiments and bi-gram models in the forth experiment. 
5.2   NE Translation 
Table 2 and Table 3 report the performance of the four methods on the LDC NE 
translation corpus. The results are interpreted in different scoring measures, which 
allow us to compare the performances from different viewpoints.   
? ACC reports the accuracy of the exact;  
? WER reports the word error rate;  
? PER is the position-independent, or ?bag-of-words? word error rate;  
? BLEU score measures n-gram precision [19] 
? NIST score [20] is a weighted n-gram precision.  
Please note that WER and PER are error rates, the lower numbers represent better 
results. For others, the higher numbers represents the better results. 
Table 2.  E2C NE translation performance (%) 
 IBM 
  method C 
IBM 
  method D 
Koehn  
method 
Our  
method 
ACC 24.5 36.3 47.1 51.5 
WER 51.0 38.5 32.5 26.6 
PER 48.5 36.2 26.8 16.3 
BLEU 29.9 41.8 51.2 56.1 O
p e
n
 
te
s t
 
NIST 7.2 8.6 9.3 10.2 
ACC 51.1 78.9 88.2 90.9 
WER 34.1 12.8 6.3 4.3 
PER 31.5 9.5 4.1 2.7 
BLEU 54.7 80.9 89.1 91.9 
E2
C 
Cl
o s
e d
 
te
s t
 
NIST 11.1 14.2 14.7 14.8 
                                                          
5
 http://www.isi.edu/licensed-sw/pharaoh/ 
6
 http://www.isi.edu/licensed-sw/pharaoh/manual-v1.2.ps 
608 M. Zhang et al 
Table 3.  C2E NE translation performance (%) 
 IBM 
  method C 
IBM 
  method D 
Koehn  
method 
Our  
method 
ACC 13.4 21.8 31.2 36.1 
WER 60.8 45.8 41.3 38.9 
PER 49.6 38.2 32.6 26.6 
BLEU 25.1 49.8 52.9 54.1 o
p e
n  
te
s t
 
NIST 5.94 8.21 8.91 9.25 
ACC 34.3 69.5 79.2 81.3 
WER 48.2 23.6 11.3 9.2 
PER 35.7 14.7 8.7 6.2 
BLEU 42.5 76.2 85.7 88.0 
C2
E 
c l
o s
e d
 te
s t 
NIST 8.7 12.7 13.8 14.4 
Table 2 & 3 show that our method outperforms the other three methods consis-
tently in all cases and by all scores. IBM method D gives better performance than 
IBM method C, simply because it uses phrase as the translation unit instead of single 
word. Koehn et al?s phrase-based model [12] and IBM phrase-based Model 4 used in 
IBM method D are very similar in modeling. They both use context-independent 
lexical mapping model, distortion model and trigram target language model. The 
reason why Koehn method outperforms IBM method D may be due to the different 
decoding strategy. However, we still need further investigation to understand why 
Koehn method outperforms IBM method D significantly. It may also be due to the 
different LM training toolkits used in the two experiments. 
Our method tops the performance among the four experiments. The significant po-
sition-independent word error rate (PER) reduction shows that our context-dependent 
joint probability lexical mapping model is quite effective in target word selection 
compared with the other context-free conditional probability lexical model together 
with target word n-gram language model. 
Table 4. Step by step top-1 performance (%) 
 
 
LMM decoder  
 
LMM+PM decoder  
 
E2C 
 
59.9 
 
51.5 
C2E 40.5 36.1 
Table 4 studies the performance of the decoder by steps. The LMM decoder col-
umn reports the top-1 ?bag-of-words? accuracy of the LMM decoder regardless of 
word order. This is the upper bound of accuracy that the PM decoder can achieve. The 
LMM+PM decoder column shows the combined performance of two steps, where we 
 A Phrase-Based Context-Dependent Joint Probability Model 609 
measure the top-1 LMM+PM accuracy by taking top-1 LMM decoding results as 
input. It is found that the PM decoder is surprisingly effective in that it perfectly reor-
ders 85.9% (51.5/59.9) and 89.1% (36.1 /40.5) target languages in E2C and C2E 
translation respectively. 
All the experiments above recommend that our method is an effective solution for 
NE translation. 
6   Related Work 
Since our method has benefited from the JSCM of Li et al [4] and statistical MT 
research [8-12], let us compare our study with the previous related work. 
The n-gram JSCM was proposed for machine transliteration by Li et al [4]. It cou-
ples the source and channel constraints into a generative model to directly estimate 
the joint probability of source and target algnment using n-gram statistics. It was 
shown that JSCM captures rich contextual information that is present in a bilingual 
corpus to model the monotonic generative process of sequential data. In this point, our 
LMM model is the same as JSCM. The only difference is that in machine translitera-
tion Li et al [4] use phoneme unit as the basic modeling unit and our LMM is phrase-
based.  
In our study, we enhance the LMM with the PM to account for the word reorder-
ing issue in NE translation, so our model is capable of modeling the non-monotone 
problem. In contrast, JSCM only models the monotone problem. 
Both rule-based [1] and statistical model-based [5,6] methods have been proposed 
to address the NE translation problem. The model-based methods mostly are based on 
conditional probability under the noisy-channel framework [8]. Now let?s review the 
different modeling methods: 
1) As far as lexical choice issue is concerned, the noisy-channel model, repre-
sented by IBM Model 1-5 [8], models lexical dependency using a context-free 
conditional probability. Marcu and Wong [10] proposed a phrase-based con-
text-free joint probability model for lexical mapping. In contrast, our LMM 
models lexical dependency using n-order bilingual contextual information.  
2) Another characteristic of our method lies in its modeling and search strat-
egy.  NE translation and MT are usually viewed as a non-monotone search 
problem and it is well-known that a non-monotone search is exponentially 
more complex than a monotone search. Thus, we propose the two separated 
models and the two-step search, so that the lexical mapping issue can be re-
solved by monotone search. This results in a large improvement on transla-
tion selection. 
3) In addition, instead of the position-based distortion model [8-12], we use the 
n-gram permutation model to account for word reordering. A risk-free de-
coder is also proposed for the permutation model.  
One may argue that our proposed model bears a strong resemblance to IBM Model 
1: a position-independent translation model and a language model on target sentence 
without explicit distortion modeling. Let us discuss the major differences between 
them: 
610 M. Zhang et al 
1) Our LMM models the lexical mapping and target word selection using a con-
text-dependent joint probability while IBM Model 1 using a context-
independent conditional probability and a target n-gram language model. 
2) Our LMM carries out the target word selection and our PM only models the 
target word connectivity while the language model in IBM Model 1 performs 
the function of target word selection. 
Alternatively, finite-state automata (FSA) for statistical MT were previous sug-
gested for decoding using contextual information [21,22]. Bangalore and Riccardi 
[21] proposed a phrase-based variable length n-gram model followed by a reordering 
scheme for spoken language translation. However, their re-ordering scheme was not 
evaluated by empirical experiments.  
7   Conclusions 
In this paper, we propose a new model for NE translation. We present the training and 
decoding methods for the proposed model. We also compare the proposed method 
with related work. Empirical experiments show that our method outperforms the pre-
vious methods significantly in all test cases. We conclude that our method works 
more effectively and efficiently in NE translation than previous work does.  
Our method does well in NE translation, which is relatively less sophisticated in 
terms of word distortion. We expect to improve its permutation model by integrating 
a distortion model to account for larger sentence structure and apply to machine trans-
lation study. 
Acknowledgments 
We would like to thank the anonymous reviews for their invaluable suggestions on 
our original manuscript. 
References 
1. Hsin-Hsi Chen, Changhua Yang and Ying Lin. 2003. Learning Formulation and Trans-
formation Rules for Multilingual NEs. Proceedings of the ACL 2003 Workshop on 
MMLNER 
2. K. Knight and J. Graehl. 1998. Machine Transliteration. Computational Linguistics, 24(4) 
3. Jong-Hoon Oh and Key-Sun Choi, 2002. An English-Korean Transliteration Model Using 
Pronunciation and Contextual Rules. Proceedings of COLING 2002 
4. Haizhou Li, Ming Zhang and Jian Su. 2004. A Joint Source-Channel Model for Machine 
Transliteration. Proceedings of the 42th ACL, Barcelona,  160-167 
5. Y. Al-Onaizan and K. Knight, 2002. Translating named entities using monolingual and bi-
lingual resources. Proceedings of the 40th ACL, Philadelphia,  400-408 
6. Fei Huang, S. Vogel and A. Waibel, 2004. Improving NE Translation Combining Phonetic 
and Semantic Similarities. Proceedings of HLT-NAACL-2004 
7. LDC2003E01, 2003. http://www.ldc.upenn.edu/ 
 A Phrase-Based Context-Dependent Joint Probability Model 611 
8. P.F. Brown, S.A.D. Pietra, V.J.D. Pietra and R.L. Mercer.1993. The mathematics of statis-
tical machine translation. Computational Linguistics,19(2):263-313 
9. Richard Zens and Hermann Ney. 2004. Improvements in Phrase-Based Statistical Machine 
Translation. Proceedings of HLT-NAACL-2004 
10. D. Marcu and W. Wong. 2002. A Phrase-based, Joint Probability Model for Statistical 
Machine Translation. Proceedings of EMNLP-2002 
11. Franz Joseh Och, C. Tillmann and H. Ney. 1999. Improved Alignment Models for Statisti-
cal Machine Translation. Proceedings of Joint Workshop on EMNLP and Very Large Cor-
pus: 20-28 
12. P. Koehn, F. J. Och and D. Marcu. 2003. Statistical Phrase-based Translation. Proceedings 
of HLT-2003 
13. A. Stolcke. 2002. SRILM -- An Extensible Language Modeling Toolkit. Proceedings of 
ICSLP-2002, vol. 2, 901-904, Denver. 
14. U. Germann, M. Jahr, K. Knight, D. Marcu and K. Yamada. 2001. Fast Decoding and Op-
timal Decoding for Machine Translation. Proceedings of ACL-2001 
15. Franz Joseh Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical 
Alignment Models. Computational Linguistics, 29(1):19-51 
16. U. Germann. 2003. Greedy Decoding for Statistical Machine Translation in Almost Linear 
Time. Proceedings of HLT-NAACL-2003 
17. Christoph Tillmann and Hermann Ney. 2003. Word Reordering and a Dynamic Program-
ming Beam Search Algorithm for Statistical Machine Translation. Computational Linguis-
tics, 29(1):97-133 
18. R. Schwartz and Y. L. Chow. 1990. The N-best algorithm: An efficient and Exact procedure 
for finding the N most likely sentence hypothesis, Proceedings of ICASSP 1990, 81-84 
19. K. Papineni, S. Roukos, T. Ward and W. J. Zhu. 2001. BLEU: a method for automatic 
evaluation of machine translation. Technical Report RC22176 (W0109-022), IBM Re-
search Report. 
20. G. Doddington. 2002. Automatic evaluation of machine translation quality using n-gram 
co-occurrence statistics. Proceedings of ARPA Workshop on HLT 
21. S. Bangalore and G. Riccardi, 2000, Stochastic Finite State Models for Spoken Language 
Machine Translation, Workshop on Embedded MT System 
22. Stephan Kanthak and Hermann Hey, 2004. FSA: An Efficient and Flexiable C++ Tookkit 
for Finite State Automata Using On-Demand Computation, Proceedings of ACL-2004 
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 712?719,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Ordering Phrases with Function Words
Hendra Setiawan and Min-Yen Kan
School of Computing
National University of Singapore
Singapore 117543
{hendrase,kanmy}@comp.nus.edu.sg
Haizhou Li
Institute for Infocomm Research
21 Heng Mui Keng Terrace
Singapore 119613
hli@i2r.a-star.edu.sg
Abstract
This paper presents a Function Word cen-
tered, Syntax-based (FWS) solution to ad-
dress phrase ordering in the context of
statistical machine translation (SMT). Mo-
tivated by the observation that function
words often encode grammatical relation-
ship among phrases within a sentence, we
propose a probabilistic synchronous gram-
mar to model the ordering of function words
and their left and right arguments. We im-
prove phrase ordering performance by lexi-
calizing the resulting rules in a small number
of cases corresponding to function words.
The experiments show that the FWS ap-
proach consistently outperforms the base-
line system in ordering function words? ar-
guments and improving translation quality
in both perfect and noisy word alignment
scenarios.
1 Introduction
The focus of this paper is on function words, a class
of words with little intrinsic meaning but is vital in
expressing grammatical relationships among words
within a sentence. Such encoded grammatical infor-
mation, often implicit, makes function words piv-
otal in modeling structural divergences, as project-
ing them in different languages often result in long-
range structural changes to the realized sentences.
Just as a foreign language learner often makes
mistakes in using function words, we observe that
current machine translation (MT) systems often per-
form poorly in ordering function words? arguments;
lexically correct translations often end up reordered
incorrectly. Thus, we are interested in modeling
the structural divergence encoded by such function
words. A key finding of our work is that modeling
the ordering of the dependent arguments of function
words results in better translation quality.
Most current systems use statistical knowledge
obtained from corpora in favor of rich natural lan-
guage knowledge. Instead of using syntactic knowl-
edge to determine function words, we approximate
this by equating the most frequent words as func-
tion words. By explicitly modeling phrase ordering
around these frequent words, we aim to capture the
most important and prevalent ordering productions.
2 Related Work
A good translation should be both faithful with ade-
quate lexical choice to the source language and flu-
ent in its word ordering to the target language. In
pursuit of better translation, phrase-based models
(Och and Ney, 2004) have significantly improved the
quality over classical word-based models (Brown et
al., 1993). These multiword phrasal units contribute
to fluency by inherently capturing intra-phrase re-
ordering. However, despite this progress, inter-
phrase reordering (especially long distance ones)
still poses a great challenge to statistical machine
translation (SMT).
The basic phrase reordering model is a simple
unlexicalized, context-insensitive distortion penalty
model (Koehn et al, 2003). This model assumes
little or no structural divergence between language
pairs, preferring the original, translated order by pe-
nalizing reordering. This simple model works well
when properly coupled with a well-trained language
712
model, but is otherwise impoverished without any
lexical evidence to characterize the reordering.
To address this, lexicalized context-sensitive
models incorporate contextual evidence. The local
prediction model (Tillmann and Zhang, 2005) mod-
els structural divergence as the relative position be-
tween the translation of two neighboring phrases.
Other further generalizations of orientation include
the global prediction model (Nagata et al, 2006) and
distortion model (Al-Onaizan and Papineni, 2006).
However, these models are often fully lexicalized
and sensitive to individual phrases. As a result, they
are not robust to unseen phrases. A careful approx-
imation is vital to avoid data sparseness. Proposals
to alleviate this problem include utilizing bilingual
phrase cluster or words at the phrase boundary (Na-
gata et al, 2006) as the phrase identity.
The benefit of introducing lexical evidence with-
out being fully lexicalized has been demonstrated
by a recent state-of-the-art formally syntax-based
model1, Hiero (Chiang, 2005). Hiero performs
phrase ordering by using linked non-terminal sym-
bols in its synchronous CFG production rules cou-
pled with lexical evidence. However, since it is dif-
ficult to specify a well-defined rule, Hiero has to rely
on weak heuristics (i.e., length-based thresholds) to
extract rules. As a result, Hiero produces grammars
of enormous size. Watanabe et al (2006) further
reduces the grammar?s size by enforcing all rules to
comply with Greibach Normal Form.
Taking the lexicalization an intuitive a step for-
ward, we propose a novel, finer-grained solution
which models the content and context information
encoded by function words - approximated by high
frequency words. Inspired by the success of syntax-
based approaches, we propose a synchronous gram-
mar that accommodates gapping production rules,
while focusing on the statistical modeling in rela-
tion to function words. We refer to our approach
as the Function Word-centered Syntax-based ap-
proach (FWS). Our FWS approach is different from
Hiero in two key aspects. First, we use only a
small set of high frequency lexical items to lexi-
calize non-terminals in the grammar. This results
in a much smaller set of rules compared to Hiero,
1Chiang (2005) used the term ?formal? to indicate the use of
synchronous grammar but without linguistic commitment
,\ 4 ? { j? Q? ? { ?\
a form is a coll. of data entry fields on a page
(((
((((
(((
((((
((













PP
PP
PP
PPP
```
```
```
```
``
Figure 1: A Chinese-English sentence pair.
greatly reducing the computational overhead that
arises when moving from phrase-based to syntax-
based approach. Furthermore, by modeling only
high frequency words, we are able to obtain reliable
statistics even in small datasets. Second, as opposed
to Hiero, where phrase ordering is done implicitly
alongside phrase translation and lexical weighting,
we directly model the reordering process using ori-
entation statistics.
The FWS approach is also akin to (Xiong et al,
2006) in using a synchronous grammar as a reorder-
ing constraint. Instead of using Inversion Transduc-
tion Grammar (ITG) (Wu, 1997) directly, we will
discuss an ITG extension to accommodate gapping.
3 Phrase Ordering around Function
Words
We use the following Chinese (c) to English (e)
translation in Fig.1 as an illustration to conduct an
inquiry to the problem. Note that the sentence trans-
lation requires some translations of English words
to be ordered far from their original position in Chi-
nese. Recovering the correct English ordering re-
quires the inversion of the Chinese postpositional
phrase, followed by the inversion of the first smaller
noun phrase, and finally the inversion of the sec-
ond larger noun phrase. Nevertheless, the correct
ordering can be recovered if the position and the se-
mantic roles of the arguments of the boxed function
words were known. Such a function word centered
approach also hinges on knowing the correct phrase
boundaries for the function words? arguments and
which reorderings are given precedence, in case of
conflicts.
We propose modeling these sources of knowl-
edge using a statistical formalism. It includes 1) a
model to capture bilingual orientations of the left
and right arguments of these function words; 2) a
model to approximate correct reordering sequence;
and 3) a model for finding constituent boundaries of
713
the left and right arguments. Assuming that the most
frequent words in a language are function words,
we can apply orientation statistics associated with
these words to reorder their adjacent left and right
neighbors. We follow the notation in (Nagata et
al., 2006) and define the following bilingual ori-
entation values given two neighboring source (Chi-
nese) phrases: Monotone-Adjacent (MA); Reverse-
Adjacent (RA); Monotone-Gap (MG); and Reverse-
Gap (RG). The first clause (monotone, reverse) in-
dicates whether the target language translation order
follows the source order; the second (adjacent, gap)
indicates whether the source phrases are adjacent or
separated by an intervening phrase on the target side.
Table 1 shows the orientation statistics for several
function words. Note that we separate the statistics
for left and right arguments to account for differ-
ences in argument structures: some function words
take a single argument (e.g., prepositions), while
others take two or more (e.g., copulas). To han-
dle other reordering decisions not explicitly encoded
(i.e., lexicalized) in our FWS model, we introduce a
universal token U , to be used as a backoff statistic
when function words are absent.
For example, orientation statistics for 4 (to be)
overwhelmingly suggests that the English transla-
tion of its surrounding phrases is identical to its Chi-
nese ordering. This reflects the fact that the argu-
ments of copulas in both languages are realized in
the same order. The orientation statistics for post-
position ? (on) suggests inversion which captures
the divergence between Chinese postposition to the
English preposition. Similarly, the dominant orien-
tation for particle { (of) suggests the noun-phrase
shift from modified-modifier to modifier-modified,
which is common when translating Chinese noun
phrases to English.
Taking all parts of the model, which we detail
later, together with the knowledge in Table 1, we
demonstrate the steps taken to translate the exam-
ple in Fig. 2. We highlight the function words with
boxed characters and encapsulate content words as
indexed symbols. As shown, orientation statistics
from function words alone are adequate to recover
the English ordering - in practice, content words also
influence the reordering through a language model.
One can think of the FWS approach as a foreign lan-
guage learner with limited knowledge about Chinese
grammar but fairly knowledgable about the role of
Chinese function words.
,\4 ? { j? Q?? { ?\
X1 4 X2 ? { X3 { X4
HHj
? X2
?
9
XXXXXz
X3 { X5
?
)
XXXXXXXz
X4 { X6
?? ?
X1 4 X7
X1 4 X4 { X3 { ? X2
,\ 4 ?\ { j?Q?? { ? 
a form is a coll. of data entry fields on a page
#1
#2
#3 ? ? ? ? ? ? ? ? ?
Figure 2: In Step 1, function words (boxed char-
acters) and content words (indexed symbols) are
identified. Step 2 reorders phrases according to
knowledge embedded in function words. A new in-
dexed symbol is introduced to indicate previously
reordered phrases for conciseness. Step 3 finally
maps Chinese phrases to their English translation.
4 The FWS Model
We first discuss the extension of standard ITG to
accommodate gapping and then detail the statistical
components of the model later.
4.1 Single Gap ITG (SG-ITG)
The FWS model employs a synchronous grammar
to describe the admissible orderings.
The utility of ITG as a reordering constraint for
most language pairs, is well-known both empirically
(Zens and Ney, 2003) and analytically (Wu, 1997),
however ITG?s straight (monotone) and inverted (re-
verse) rules exhibit strong cohesiveness, which is in-
adequate to express orientations that require gaps.
We propose SG-ITG that follows Wellington et al
(2006)?s suggestion to model at most one gap.
We show the rules for SG-ITG below. Rules 1-
3 are identical to those defined in standard ITG, in
which monotone and reverse orderings are repre-
sented by square and angle brackets, respectively.
714
Rank Word unigram MAL RAL MGL RGL MAR RAR MGR RGR
1 { 0.0580 0.45 0.52 0.01 0.02 0.44 0.52 0.01 0.03
2 ? 0.0507 0.85 0.12 0.02 0.01 0.84 0.12 0.02 0.02
3  0.0550 0.99 0.01 0.00 0.00 0.92 0.08 0.00 0.00
4  0.0155 0.87 0.10 0.02 0.00 0.82 0.12 0.05 0.02
5  0.0153 0.84 0.11 0.01 0.04 0.88 0.11 0.01 0.01
6 Z 0.0138 0.95 0.02 0.01 0.01 0.97 0.02 0.01 0.00
7 ? 0.0123 0.73 0.12 0.10 0.04 0.51 0.14 0.14 0.20
8 ,1 0.0114 0.78 0.12 0.03 0.07 0.86 0.05 0.08 0.01
9 ? 0.0099 0.95 0.02 0.02 0.01 0.96 0.01 0.02 0.01
10 R 0.0091 0.87 0.10 0.01 0.02 0.88 0.10 0.01 0.00
21 4 0.0056 0.85 0.11 0.02 0.02 0.85 0.04 0.09 0.02
37 ? 0.0035 0.33 0.65 0.02 0.01 0.31 0.63 0.03 0.03
- U 0.0002 0.76 0.14 0.06 0.05 0.74 0.13 0.07 0.06
Table 1: Orientation statistics and unigram probability of selected frequent Chinese words in the HIT corpus.
Subscripts L/R refers to lexical unit?s orientation with respect to its left/right neighbor. U is the universal
token used in back-off for N = 128. Dominant orientations of each word are in bold.
(1) X ? c/e
(2) X ? [XX] (3) X ? ?XX?
(4) X? [X X] (5) X? ?X X?
(6) X ? [X ?X] (7) X ? ?X ?X?
SG-ITG introduces two new sets of rules: gap-
ping (Rules 4-5) and dovetailing (Rules 6-7) that
deal specifically with gaps. On the RHS of the gap-
ping rules, a diamond symbol () indicates a gap,
while on the LHS, it emits a superscripted symbol
X to indicate a gapped phrase (plain Xs without
superscripts are thus contiguous phrases). Gaps in
X are eventually filled by actual phrases via dove-
tailing (marked with an ? on the RHS).
Fig.3 illustrates gapping and dovetailing rules
using an example where two Chinese adjectival
phrases are translated into a single English subordi-
nate clause. SG-ITG can generate the correct order-
ing by employing gapping followed by dovetailing,
as shown in the following simplified trace:
X1 ? ? 1997{??, V.1  1997 ?
X2 ? ? 1998{??, V.2  1998 ?
X3 ? [X1 ?X2]
? [ 1997{?? Z 1998{??,
V.1  1997 ? V.2  1998 ]
? 1997{??Z1998{??,
V.1 and V.2 that were released in 1997 and 1998
where X1 and X

2 each generate the translation of
their respective Chinese noun phrase using gapping
and X3 generates the English subclause by dovetail-
ing the two gapped phrases together.
Thus far, the grammar is unlexicalized, and does
1997#q{ ?? Z 1998#q{??
V.1 and V.2 that were released in 1997 and 1998.
!!
!!
!!
((((
((((
(((
((
hhhh
hhhh
hhh
hh
PP
PP
PP
P
Figure 3: An example of an alignment that can be
generated only by allowing gaps.
not incorporate any lexical evidence. Now we mod-
ify the grammar to introduce lexicalized function
words to SG-ITG. In practice, we introduce a new
set of lexicalized non-terminal symbols Yi, i ?
{1...N}, to represent the topN most-frequent words
in the vocabulary; the existing unlexicalized X is
now reserved for content words. This difference
does not inherently affect the structure of the gram-
mar, but rather lexicalizes the statistical model.
In this way, although different Yis follow the same
production rules, they are associated with different
statistics. This is reflected in Rules 8-9. Rule 8 emits
the function word; Rule 9 reorders the arguments
around the function word, resembling our orienta-
tion model (see Section 4.2) where a function word
influences the orientation of its left and right argu-
ments. For clarity, we omit notation that denotes
which rules have been applied (monotone, reverse;
gapping, dovetailing).
(8) Yi? c/e (9) X? XYiX
In practice, we replace Rule 9 with its equivalent
2-normal form set of rules (Rules 10-13). Finally,
we introduce rules to handle back-off (Rules 14-16)
and upgrade (Rule 17). These allow SG-ITG to re-
715
vert function words to normal words and vice versa.
(10) R? YiX (11) L? XYi
(12) X? LX (13) X? XR
(14) Yi? X (15) R? X
(16) L? X (17) X? YU
Back-off rules are needed when the grammar has
to reorder two adjacent function words, where one
set of orientation statistics must take precedence
over the other. The example in Fig.1 illustrates such
a case where the orientation of ? (on) and { (of)
compete for influence. In this case, the grammar
chooses to use{ (of) and reverts the function word
? (on) to the unlexicalized form.
The upgrade rule is used for cases where there are
two adjacent phrases, both of which are not function
words. Upgrading allows either phrase to act as a
function word, making use of the universal word?s
orientation statistics to reorder its neighbor.
4.2 Statistical model
We now formulate the FWS model as a statistical
framework. We replace the deterministic rules in our
SG-ITG grammar with probabilistic ones, elevating
it to a stochastic grammar. In particular, we develop
the three sub models (see Section 3) which influence
the choice of production rules for ordering decision.
These models operate on the 2-norm rules, where the
RHS contains one function word and its argument
(except in the case of the phrase boundary model).
We provide the intuition for these models next, but
their actual form will be discussed in the next section
on training.
1) Orientation Model ori(o|H,Yi): This model
captures the preference of a function word Yi to a
particular orientation o ? {MA,RA,MG,RG} in
reordering its H ? {left, right} argument X . The
parameter H determines which set of Yi?s statistics
to use (left or right); the model consults Yi?s left ori-
entation statistic for Rules 11 and 13 where X pre-
cedes Yi, otherwise Yi?s right orientation statistic is
used for Rules 10 and 12.
2) Preference Model pref(Yi): This model ar-
bitrates reordering in the cases where two function
words are adjacent and the backoff rules have to de-
cide which function word takes precedence, revert-
ing the other to the unlexicalized X form. This
model prefers the function word with higher uni-
gram probability to take the precedence.
3) Phrase BoundaryModel pb(X): This model is
a penalty-based model, favoring the resulting align-
ment that conforms to the source constituent bound-
ary. It penalizes Rule 1 if the terminal rule X
emits a Chinese phrase that violates the boundary
(pb = e?1), otherwise it is inactive (pb = 1).
These three sub models act as features alongside
seven other standard SMT features in a log-linear
model, resulting in the following set of features
{f1, . . . , f10}: f1) orientation ori(o|H,Yi); f2)
preference pref(Yi); f3) phrase boundary pb(X);
f4) language model lm(e); f5 ? f6) phrase trans-
lation score ?(e|c) and its inverse ?(c|e); f7 ? f8)
lexical weight lex(e|c) and its inverse lex(c|e); f9)
word penalty wp; and f10) phrase penalty pp.
The translation is then obtained from the most
probable derivation of the stochastic SG-ITG. The
formula for a single derivation is shown in Eq. (18),
where X1, X2, ..., XL is a sequence of rules with
w(Xl) being the weight of each particular rule Xl.
w(Xl) is estimated through a log-linear model, as
in Eq. (19), with all the abovementioned features
where ?j reflects the contribution of each feature fj .
P (X1, ..., XL) =
?L
l=1
w(Xl)(18)
w(Xl) =
?10
j=1
fj(Xl)
?j(19)
5 Training
We train the orientation and preference models from
statistics of a training corpus. To this end, we first
derive the event counts and then compute the rela-
tive frequency of each event. The remaining phrase
boundary model can be modeled by the output of a
standard text chunker, as in practice it is simply a
constituent boundary detection mechanism together
with a penalty scheme.
The events of interest to the orientation model are
(Yi, o) tuples, where o ? {MA,RA,MG,RG} is
an orientation value of a particular function word
Yi. Note that these tuples are not directly observable
from training data. Hence, we need an algorithm to
derive (Yi, o) tuples from a parallel corpus. Since
both left and right statistics share identical training
steps, thus we omit references to them.
The algorithm to derive (Yi, o) involves several
steps. First, we estimate the bi-directional alignment
716
by running GIZA++ and applying the ?grow-diag-
final? heuristic. Then, the algorithm enumerates all
Yi and determines its orientation o with respect to
its argument X to derive (Yi, o). To determine o,
the algorithm inspects the monotonicity (monotone
or reverse) and adjacency (adjacent or gap) between
Yi?s and X?s alignments.
Monotonicity can be determined by looking at the
Yi?s alignment with respect to the most fine-grained
level of X (i.e., word level alignment). However,
such a heuristic may inaccurately suggest gap ori-
entation. Figure 1 illustrates this problem when de-
riving the orientation for the second{ (of). Look-
ing only at the word alignment of its left argument
? (fields) incorrectly suggests a gapped orientation,
where the alignment of j?Q? (data entry) in-
tervened. It is desirable to look at the alignment of
j?Q?? (data entry fields) at the phrase level,
which suggests the correct adjacent orientation in-
stead.
To address this issue, the algorithm uses gap-
ping conservatively by utilizing the consistency con-
straint (Och and Ney, 2004) to suggest phrase level
alignment of X . The algorithm exhaustively grows
consistent blocks containing the most fine-grained
level of X not including Yi. Subsequently, it merges
each hypothetical argument with the Yi?s alignment.
The algorithm decides that Yi has a gapped orienta-
tion only if all merged blocks violate the consistency
constraint, concluding an adjacent orientation other-
wise.
With the event countsC(Yi, o) of tuple (Yi, o), we
estimate the orientation model for Yi and U using
Eqs. (20) and (21). We also estimate the prefer-
ence model with word unigram counts C(Yi) using
Eqs. (22) and (23), where V indicates the vocabu-
lary size.
ori(o|Yi) = C(Yi, o)/C(Yi, ?), i 6 N(20)
ori(o|U) =
?
i>N
C(Yi, o)/
?
i>N
C(Yi, ?)(21)
pref(Yi) = C(Yi)/C(?), i 6 N(22)
pref(U) = 1/(V ?N)
?
i>N
C(Yi)/C(?)(23)
Samples of these statistics are found in Table 1
and have been used in the running examples. For
instance, the statistic ori(RAL|{) = 0.52, which
is the dominant one, suggests that the grammar in-
versely order {(of)?s left argument; while in our
illustration of backoff rules in Fig.1, the grammar
chooses{(of) to take precedence since pref({) >
pref(?).
6 Decoding
We employ a bottom-up CKY parser with a beam
to find the derivation of a Chinese sentence which
maximizes Eq. (18). The English translation is then
obtained by post-processing the best parse.
We set the beam size to 30 in our experiment and
further constrain reordering to occur within a win-
dow of 10 words. Our decoder also prunes entries
that violate the following constraints: 1) each entry
contains at most one gap; 2) any gapped entries must
be dovetailed at the next level higher; 3) an entry
spanning the whole sentence must not contain gaps.
The score of each newly-created entry is derived
from the scores of its parts accordingly. When scor-
ing entries, we treat gapped entries as contiguous
phrases by ignoring the gap symbol and rely on the
orientation model to penalize such entries. This al-
lows a fair score comparison between gapped and
contiguous entries.
7 Experiments
We would like to study how the FWS model affects
1) the ordering of phrases around function words; 2)
the overall translation quality. We achieve this by
evaluating the FWS model against a baseline system
using two metrics, namely, orientation accuracy and
BLEU respectively.
We define the orientation accuracy of a (function)
word as the accuracy of assigning correct orientation
values to both its left and right arguments. We report
the aggregate for the top 1024 most frequent words;
these words cover 90% of the test set.
We devise a series of experiments and run it in two
scenarios - manual and automatic alignment - to as-
sess the effects of using perfect or real-world input.
We utilize the HIT bilingual computer manual cor-
pus, which has been manually aligned, to perform
Chinese-to-English translation (see Table 2). Man-
ual alignment is essential as we need to measure ori-
entation accuracy with respect to a gold standard.
717
Chinese English
train words 145,731 135,032
(7K sentences) vocabulary 5,267 8,064
dev words 13,986 14,638
(1K sentences) untranslatable 486 (3.47%)
test words 27,732 28,490
(2K sentences) untranslatable 935 (3.37%)
Table 2: Statistics for the HIT corpus.
A language model is trained using the SRILM-
Toolkit, and a text chunker (Chen et al, 2006) is ap-
plied to the Chinese sentences in the test and dev
sets to extract the constituent boundaries necessary
for the phrase boundary model. We run minimum er-
ror rate training on dev set using Chiang?s toolkit to
find a set of parameters that optimizes BLEU score.
7.1 Perfect Lexical Choice
Here, the task is simplified to recovering the correct
order of the English sentence from the scrambled
Chinese order. We trained the orientation model us-
ing manual alignment as input. The aforementioned
decoder is used with phrase translation, lexical map-
ping and penalty features turned off.
Table 4 compares orientation accuracy and BLEU
between our FWS model and the baseline. The
baseline (lm+d) employs a language model and
distortion penalty features, emulating the standard
Pharaoh model. We study the behavior of the
FWS model with different numbers of lexicalized
items N . We start with the language model alone
(N=0) and incrementally add the orientation (+ori),
preference (+ori+pref) and phrase boundary models
(+ori+pref+pb).
As shown, the language model alone is rela-
tively weak, assigning the correct orientation in only
62.28% of the cases. A closer inspection reveals that
the lm component aggressively promotes reverse re-
orderings. Including a distortion penalty model (the
baseline) improves the accuracy to 72.55%. This
trend is also apparent for the BLEU score.
When we incorporate the FSW model, including
just the most frequent word (Y1={), we see im-
provement. This model promotes non-monotone re-
ordering conservatively around Y1 (where the dom-
inant statistic suggests reverse ordering). Increasing
the value of N leads to greater improvement. The
most effective improvement is obtained by increas-
pharaoh (dl=5) 22.44 ? 0.94
+ori 23.80 ? 0.98
+ori+pref 23.85 ? 1.00
+ori+pref+pb 23.86 ? 1.08
Table 3: BLEU score with the 95% confidence in-
tervals based on (Zhang and Vogel, 2004). All im-
provement over the baseline (row 1) are statistically
significant under paired bootstrap resampling.
ing N to 128. Additional (marginal) improvement
is obtained at the expense of modeling an additional
900+ lexical items. We see these results as validat-
ing our claim that modeling the top few most fre-
quent words captures most important and prevalent
ordering productions.
Lastly, we study the effect of the pref and pb fea-
tures. The inclusion of both sub models has little af-
fect on orientation accuracy, but it improves BLEU
consistently (although not significantly). This sug-
gests that both models correct the mistakes made by
the ori model while preserving the gain. They are
not as effective as the addition of the basic orienta-
tion model as they only play a role when two lexi-
calized entries are adjacent.
7.2 Full SMT experiments
Here, all knowledge is automatically trained on the
train set, and as a result, the input word alignment
is noisy. As a baseline, we use the state-of-the-art
phrase-based Pharaoh decoder. For a fair compari-
son, we run minimum error rate training for different
distortion limits from 0 to 10 and report the best pa-
rameter (dl=5) as the baseline.
We use the phrase translation table from the base-
line and perform an identical set of experiments as
the perfect lexical choice scenario, except that we
only report the result for N=128, due to space con-
straint. Table 3 reports the resulting BLEU scores.
As shown, the FWS model improves BLEU score
significantly over the baseline. We observe the same
trend as the one in perfect lexical choice scenario
where top 128 most frequent words provides the ma-
jority of improvement. However, the pb features
yields no noticeable improvement unlike in prefect
lexical choice scenario; this is similar to the findings
in (Koehn et al, 2003).
718
N=0 N=1 N=4 N=16 N=64 N=128 N=256 N=1024
O
ri
en
ta
ti
on
A
cc
.
(%
)
lm+d 72.55
+ori 62.28 76.52 76.58 77.38 77.54 78.17 77.76 78.38
+ori+pref 76.66 76.82 77.57 77.74 78.13 77.94 78.54
+ori+pref+pb 76.70 76.85 77.58 77.70 78.20 77.94 78.56
B
L
E
U
lm+d 75.13
+ori 66.54 77.54 77.57 78.22 78.48 78.76 78.58 79.20
+ori+pref 77.60 77.70 78.29 78.65 78.77 78.70 79.30
+ori+pref+pb 77.69 77.80 78.34 78.65 78.93 78.79 79.30
Table 4: Results using perfect aligned input. Here, (lm+d) is the baseline; (+ori), (+ori+pref) and
(+ori+pref+pb) are different FWS configurations. The results of the model (where N is varied) that fea-
tures the largest gain are bold, whereas the highest score is italicized.
8 Conclusion
In this paper, we present a statistical model to cap-
ture the grammatical information encoded in func-
tion words. Formally, we develop the FunctionWord
Syntax-based (FWS) model, a probabilistic syn-
chronous grammar, to encode the orientation statis-
tics of arguments to function words. Our experimen-
tal results shows that the FWS model significantly
improves the state-of-the-art phrase-based model.
We have touched only the surface benefits of mod-
eling function words. In particular, our proposal is
limited to modeling function words in the source
language. We believe that conditioning on both
source and target pair would result in more fine-
grained, accurate orientation statistics.
From our error analysis, we observe that 1) re-
ordering may span several levels and the preference
model does not handle this phenomena well; 2) cor-
rectly reordered phrases with incorrect boundaries
severely affects BLEU score and the phrase bound-
ary model is inadequate to correct the boundaries es-
pecially for cases of long phrase. In future, we hope
to address these issues while maintaining the bene-
fits offered by modeling function words.
References
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical Lower Bounds on
the Complexity of Translational Equivalence. In
ACL/COLING 2006, pp. 977?984.
Christoph Tillman and Tong Zhang. 2005. A Localized
Prediction Model for Statistical Machine Translation.
In ACL 2005, pp. 557?564.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In ACL
2005, pp. 263?270.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3):377?403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase ReorderingModel for Sta-
tistical Machine Translation. In ACL/COLING 2006,
pp. 521?528.
Franz J. Och and Hermann Ney. 2004. The Alignment
Template Approach to Statistical Machine Translation.
Computational Linguistics, 30(4):417?449.
Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,
and Kazuteru Ohashi. 2006. A Clustered Global
Phrase Reordering Model for Statistical Machine
Translation. In ACL/COLING 2006, pp. 713?720.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, Robert L. Mercer 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263?311.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In HLT-NAACL
2003, pp. 127?133.
Richard Zens and Hermann Ney. 2003. A Compara-
tive Study on Reordering Constraints in Statistical Ma-
chine Translation. In ACL 2003.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-Right Target Generation for Hierarchi-
cal Phrase-Based Translation. In ACL/COLING 2006,
pp. 777?784.
Wenliang Chen, Yujie Zhang and Hitoshi Isahara 2006.
An Empirical Study of Chinese Chunking In ACL
2006 Poster Sessions, pp. 97?104.
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion Models for Statistical Machine Translation. In
ACL/COLING 2006, pp. 529?536.
Ying Zhang and Stephan Vogel. 2004. Measuring Confi-
dence Intervals for theMachine Translation Evaluation
Metrics. In TMI 2004.
719
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 324?332,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Topological Ordering of Function Words
in Hierarchical Phrase-based Translation
Hendra Setiawan
1
and Min-Yen Kan
2
and Haizhou Li
3
and Philip Resnik
1
1
University of Maryland Institute for Advanced Computer Studies
2
School of Computing, National University of Singapore
3
Human Language Technology, Institute for Infocomm Research, Singapore
{hendra,resnik}@umiacs.umd.edu,
kanmy@comp.nus.edu.sg, hli@i2r.a-star.edu.sg
Abstract
Hierarchical phrase-based models are at-
tractive because they provide a consis-
tent framework within which to character-
ize both local and long-distance reorder-
ings, but they also make it difcult to
distinguish many implausible reorderings
from those that are linguistically plausi-
ble. Rather than appealing to annotation-
driven syntactic modeling, we address this
problem by observing the inuential role
of function words in determining syntac-
tic structure, and introducing soft con-
straints on function word relationships as
part of a standard log-linear hierarchi-
cal phrase-based model. Experimentation
on Chinese-English and Arabic-English
translation demonstrates that the approach
yields signicant gains in performance.
1 Introduction
Hierarchical phrase-based models (Chiang, 2005;
Chiang, 2007) offer a number of attractive bene-
ts in statistical machine translation (SMT), while
maintaining the strengths of phrase-based systems
(Koehn et al, 2003). The most important of these
is the ability to model long-distance reordering ef-
ciently. To model such a reordering, a hierar-
chical phrase-based system demands no additional
parameters, since long and short distance reorder-
ings are modeled identically using synchronous
context free grammar (SCFG) rules. The same
rule, depending on its topological ordering ? i.e.
its position in the hierarchical structure ? can af-
fect both short and long spans of text. Interest-
ingly, hierarchical phrase-based models provide
this benet without making any linguistic commit-
ments beyond the structure of the model.
However, the system's lack of linguistic com-
mitment is also responsible for one of its great-
est drawbacks. In the absence of linguistic knowl-
edge, the system models linguistic structure using
an SCFG that contains only one type of nontermi-
nal symbol
1
. As a result, the system is susceptible
to the overgeneration problem: the grammar may
suggest more reordering choices than appropriate,
and many of those choices lead to ungrammatical
translations.
Chiang (2005) hypothesized that incorrect re-
ordering choices would often correspond to hier-
archical phrases that violate syntactic boundaries
in the source language, and he explored the use
of a ?constituent feature? intended to reward the
application of hierarchical phrases which respect
source language syntactic categories. Although
this did not yield signicant improvements, Mar-
ton and Resnik (2008) and Chiang et al (2008)
extended this approach by introducing soft syn-
tactic constraints similar to the constituent feature,
but more ne-grained and sensitive to distinctions
among syntactic categories; these led to substan-
tial improvements in performance. Zollman et al
(2006) took a complementary approach, constrain-
ing the application of hierarchical rules to respect
syntactic boundaries in the target language syn-
tax. Whether the focus is on constraints from the
source language or the target language, the main
ingredient in both previous approaches is the idea
of constraining the spans of hierarchical phrases to
respect syntactic boundaries.
In this paper, we pursue a different approach
to improving reordering choices in a hierarchical
phrase-based model. Instead of biasing the model
toward hierarchical phrases whose spans respect
syntactic boundaries, we focus on the topologi-
cal ordering of phrases in the hierarchical struc-
ture. We conjecture that since incorrect reorder-
ing choices correspond to incorrect topological or-
derings, boosting the probability of correct topo-
1
In practice, one additional nonterminal symbol is used in
?glue rules?. This is not relevant in the present discussion.
324
logical ordering choices should improve the sys-
tem. Although related to previous proposals (cor-
rect topological orderings lead to correct spans
and vice versa), our proposal incorporates broader
context and is structurally more aware, since we
look at the topological ordering of a phrase relative
to other phrases, rather than modeling additional
properties of a phrase in isolation. In addition, our
proposal requires no monolingual parsing or lin-
guistically informed syntactic modeling for either
the source or target language.
The key to our approach is the observation that
we can approximate the topological ordering of
hierarchical phrases via the topological ordering
of function words. We introduce a statistical re-
ordering model that we call the pairwise domi-
nance model, which characterizes reorderings of
phrases around a pair of function words. In mod-
eling function words, our model can be viewed as
a successor to the function words-centric reorder-
ing model (Setiawan et al, 2007), expanding on
the previous approach by modeling pairs of func-
tion words rather than individual function words
in isolation.
The rest of the paper is organized as follows. In
Section 2, we briey review hierarchical phrase-
based models. In Section 3, we rst describe the
overgeneration problem in more detail with a con-
crete example, and then motivate our idea of us-
ing the topological ordering of function words to
address the problem. In Section 4, we develop
our idea by introducing the pairwise dominance
model, expressing function word relationships in
terms of what we call the the dominance predi-
cate. In Section 5, we describe an algorithm to es-
timate the parameters of the dominance predicate
from parallel text. In Sections 6 and 7, we describe
our experiments, and in Section 8, we analyze the
output of our system and lay out a possible future
direction. Section 9 discusses the relation of our
approach to prior work and Section 10 wraps up
with our conclusions.
2 Hierarchical Phrase-based System
Formally, a hierarchical phrase-based SMT sys-
tem is based on a weighted synchronous context
free grammar (SCFG) with one type of nonter-
minal symbol. Synchronous rules in hierarchical
phrase-based models take the following form:
X ? ??, ?,?? (1)
where X is the nonterminal symbol and ? and ?
are strings that contain the combination of lexical
items and nonterminals in the source and target
languages, respectively. The ? symbol indicates
that nonterminals in ? and ? are synchronized
through co-indexation; i.e., nonterminals with the
same index are aligned. Nonterminal correspon-
dences are strictly one-to-one, and in practice the
number of nonterminals on the right hand side is
constrained to at most two, which must be sepa-
rated by lexical items.
Each rule is associated with a score that is com-
puted via the following log linear formula:
w(X ? ??, ?,??) =
?
i
f?ii (2)
where fi is a feature describing one particular as-
pect of the rule and ?i is the corresponding weight
of that feature. Given e? and f? as the source
and target phrases associated with the rule, typi-
cal features used are rule's translation probability
Ptrans(f? |e?) and its inverse Ptrans(e?|f?), the lexi-
cal probability Plex(f? |e?) and its inverse Plex(e?|f?).
Systems generally also employ a word penalty, a
phrase penalty, and target language model feature.
(See (Chiang, 2005) for more detailed discussion.)
Our pairwise dominance model will be expressed
as an additional rule-level feature in the model.
Translation of a source sentence e using hier-
archical phrase-based models is formulated as a
search for the most probable derivationD? whose
source side is equal to e:
D? = argmax P (D),where source(D)=e.
D = Xi, i ? 1...|D| is a set of rules following a
certain topological ordering, indicated here by the
use of the superscript.
3 Overgeneration and Topological
Ordering of Function Words
The use of only one type of nonterminal allows a
exible permutation of the topological ordering of
the same set of rules, resulting in a huge number of
possible derivations from a given source sentence.
In that respect, the overgeneration problem is not
new to SMT: Bracketing Transduction Grammar
(BTG) (Wu, 1997) uses a single type of nontermi-
nal and is subject to overgeneration problems, as
well.
2
2
Note, however, that overgeneration in BTG can be
viewed as a feature, not a bug, since the formalism was origi-
325
The problem may be less severe in hierarchi-
cal phrase-based MT than in BTG, since lexical
items on the rules' right hand sides often limit the
span of nonterminals. Nonetheless overgeneration
of reorderings is still problematic, as we illustrate
using the hypothetical Chinese-to-English exam-
ple in Fig. 1.
Suppose we want to translate the Chinese sen-
tence in Fig. 1 into English using the following set
of rules:
1. Xa ? ??Z X1, computers andX1?
2. Xb ? ?X14 X2, X1 are X2?
3. Xc ? ?C? , cell phones ?
4. Xd ? ?X1{? , inventions of X1?
5. Xe ? ???- , the last century ?
Co-indexation of nonterminals on the right hand
side is indicated by subscripts, and for our ex-
amples the label of the nonterminal on the left
hand side is used as the rule's unique identier.
To correctly translate the sentence, a hierarchical
phrase-based system needs to model the subject
noun phrase, object noun phrase and copula con-
structions; these are captured by rulesXa,Xd and
Xb respectively, so this set of rules represents a
hierarchical phrase-based system that can be used
to correctly translate the Chinese sentence. Note
that the Chinese word order is correctly preserved
in the subject (Xa) as well as copula constructions
(Xb), and correctly inverted in the object construc-
tion (Xd).
However, although it can generate the correct
translation in Fig. 2, the grammar has no mech-
anism to prevent the generation of an incorrect
translation like the one illustrated in Fig. 3. If
we contrast the topological ordering of the rules
in Fig. 2 and Fig. 3, we observe that the difference
is small but quite signicant. Using precede sym-
bol (?) to indicate the rst operand immediately
dominates the second operand in the hierarchical
structure, the topological orderings in Fig. 2 and
Fig. 3 are Xa ? Xb ? Xc ? Xd ? Xe and
Xd ? Xa ? Xb ? Xc ? Xe, respectively. The
only difference is the topological ordering of Xd:
in Fig. 2, it appears below most of the other hier-
archical phrases, while in Fig. 3, it appears above
all the other hierarchical phrases.
nally introduced for bilingual analysis rather than generation
of translations.
Modeling the topological ordering of hierarchi-
cal phrases is computationally prohibitive, since
there are literally millions of hierarchical rules in
the system's automatically-learned grammar and
millions of possible ways to order their applica-
tion. To avoid this computational problem and
still model the topological ordering, we propose
to use the topological ordering of function words
as a practical approximation. This is motivated by
the fact that function words tend to carry crucial
syntactic information in sentences, serving as the
?glue? for content-bearing phrases. Moreover, the
positional relationships between function words
and content phrases tends to be xed (e.g., in En-
glish, prepositions invariably precede their object
noun phrase), at least for the languages we have
worked with thus far.
In the Chinese sentence above, there are three
function words involved: the conjunctionZ (and),
the copula 4 (are), and the noun phrase marker
{ (of).3 Using the function words as approximate
representations of the rules in which they appear,
the topological ordering of hierarchical phrases in
Fig. 2 is Z(and) ? 4(are) ? {(of), while that
in Fig. 3 is {(of) ? Z(and) ? 4(are).4 We
can distinguish the correct and incorrect reorder-
ing choices by looking at this simple information.
In the correct reordering choice,{(of) appears at
the lower level of the hierarchy while in the incor-
rect one,{(of) appears at the highest level of the
hierarchy.
4 Pairwise Dominance Model
Our example suggests that we may be able to im-
prove the translation model's sensitivity to correct
versus incorrect reordering choices by modeling
the topological ordering of function words. We do
so by introducing a predicate capturing the domi-
nance relationship in a derivation between pairs of
neighboring function words.
5
Let us dene a predicate d(Y ?, Y ??) that takes
two function words as input and outputs one of
3
We use the term ?noun phrase marker? here in a general
sense, meaning that in this example it helps tell us that the
phrase is part of an NP, not as a technical linguistic term. It
serves in other grammatical roles, as well. Disambiguating
the syntactic roles of function words might be a particularly
useful thing to do in the model we are proposing; this is a
question for future research.
4
Note that for expository purposes, we designed our sim-
ple grammar to ensure that these function words appear in
separate rules.
5
Two function words are considered neighbors iff no other
function word appears between them in the source sentence.
326
? Z C? 4 ?{??-
?
XXXXXz
?????9?? ? ?
arecomputers and cell phones inventions of the last century
Figure 1: A running example of Chinese-to-English translation.
Xa???Z Xb, computers andXb?
???Z Xc4 Xd, computers andXc are Xd?
???ZC?4 Xd, computers and cell phones areXd?
???ZC?4 Xe{? , computers and cell phones are inventions ofXe?
???ZC?4??-{? , computers and cell phones are inventions of the last century?
Figure 2: The derivation that leads to the correct translation
Xd??Xa{? , inventions of Xa?
???Z Xb{? , inventions of computers andXb?
???Z Xc4 Xe{? , inventions of computers andXc are Xe?
???ZC?4 Xe{? , inventions of computers and cell phones areXe?
???ZC?4??-{? , inventions of computers and cell phones are the last century?
Figure 3: The derivation that leads to the incorrect translation
four values: {leftFirst, rightFirst, dontCare, nei-
ther}, where Y ? appears to the left of Y ?? in the
source sentence. The value leftFirst indicates that
in the derivation's topological ordering, Y ? pre-
cedes Y ?? (i.e. Y ? dominates Y ?? in the hierarchi-
cal structure), while rightFirst indicates that Y ??
dominates Y ?. In Fig. 2, d(Y ?, Y ??) = leftFirst
for Y ? = the copula 4 (are) and Y ?? = the noun
phrase marker{ (of).
The dontCare and neither values capture two
additional relationships: dontCare indicates that
the topological ordering of the function words is
exible, and neither indicates that the topologi-
cal ordering of the function words is disjoint. The
former is useful in cases where the hierarchical
phrases suggest the same kind of reordering, and
therefore restricting their topological ordering is
not necessary. This is illustrated in Fig. 2 by the
pairZ(and) and the copula 4(are), where putting
either one above the other does not change the -
nal word order. The latter is useful in cases where
the two function words do not share a same parent.
Formally, this model requires several changes in
the design of the hierarchical phrase-based system.
1. To facilitate topological ordering of function
words, the hierarchical phrases must be sub-
categorized with function words. Taking Xb
in Fig. 2 as a case in point, subcategorization
using function words would yield:
6
Xb(4 ?{) ? Xc4 Xd({) (3)
The subcategorization (indicated by the
information in parentheses following the
nonterminal) propagates the function word
4(are) of Xb to the higher level structure to-
gether with the function word {(of) of Xd.
This propagation process generalizes to other
rules by maintaining the ordering of the func-
tion words according to their appearance in
the source sentence. Note that the subcate-
gorized nonterminals often resemble genuine
syntactic categories, for instance X({) can
frequently be interpreted as a noun phrase.
2. To facilitate the computation of the domi-
nance relationship, the coindexing in syn-
chronized rules (indicated by the ? symbol
in Eq. 1) must be expanded to include infor-
mation not only about the nonterminal corre-
spondences but also about the alignment of
the lexical items. For example, adding lexi-
cal alignment information to rule Xd would
yield:
Xd ? ?X1{2?3, inventions3 of2 X1?
(4)
6
The target language side is concealed for clarity.
327
The computation of the dominance relation-
ship using this alignment information will be
discussed in detail in the next section.
Again takingXb in Fig. 2 as a case in point, the
dominance feature takes the following form:
fdom(Xb) ? dom(d(4,{)|4, {)) (5)
dom(d(YL, YR)|YL, YR)) (6)
where the probability of4 ?{ is estimated ac-
cording to the probability of d(4,{).
In practice, both 4(are) and {(of) may ap-
pear together in one same rule. In such a case, a
dominance score is not calculated since the topo-
logical ordering of the two function words is un-
ambiguous. Hence, in our implementation, a
dominance score is only calculated at the points
where the topological ordering of the hierarchical
phrases needs to be resolved, i.e. the two function
words always come from two different hierarchi-
cal phrases.
5 Parameter Estimation
Learning the dominance model involves extract-
ing d values for every pair of neighboring func-
tion words in the training bitext. Such statistics
are not directly observable in parallel corpora, so
estimation is needed. Our estimation method is
based on two facts: (1) the topological ordering
of hierarchical phrases is tightly coupled with the
span of the hierarchical phrases, and (2) the span
of a hierarchical phrase at a higher level is al-
ways a superset of the span of all other hierarchical
phrases at the lower level of its substructure. Thus,
to establish soft estimates of dominance counts,
we utilize alignment information available in the
rule together with the consistent alignment heuris-
tic (Och and Ney, 2004) traditionally used to guess
phrase alignments.
Specically, we dene the span of a function
word as a maximal, consistent alignment in the
source language that either starts from or ends
with the function word. (Requiring that spans be
maximal ensures their uniqueness.) We will re-
fer to such spans as Maximal Consistent Align-
ments (MCA). Note that each function word has
two such Maximal Consistent Alignments: one
that ends with the function word (MCAR)and an-
other that starts from the function word (MCAL).
Y ? Y ?? left- right- dont- nei-
First First Care ther
Z (and) 4 (are) 0.11 0.16 0.68 0.05
4 (are) { (of) 0.57 0.15 0.06 0.22
Table 1: The distribution of the dominance values
of the function words involved in Fig. 1. The value
with the highest probability is in bold.
Given two function words Y ? and Y ??, with Y ?
preceding Y ??, we dene the value of d by exam-
ining the MCAs of the two function words.
d(Y ?, Y ??) =?
?????
?????
leftFirst, Y ? 6? MCAR(Y ??) ? Y ??? MCAL(Y ?)
rightFirst, Y ?? MCAR(Y ??) ? Y ?? 6? MCAL(Y ?)
dontCare, Y ?? MCAR(Y ??) ? Y ??? MCAL(Y ?)
neither, Y ? 6? MCAR(Y ??) ? Y ?? 6? MCAL(Y ?)
(6)
Fig. 4a illustrates the leftFirst dominance value
where the intersection of the MCAs contains only
the second function word ({(of)). Fig. 4b illus-
trates the dontCare value, where the intersection
contains both function words. Similarly, rightFirst
and neither are represented by an intersection that
contains only Y ?, or by an empty intersection, re-
spectively. Once all the d values are counted, the
pairwise dominance model of neighboring func-
tion words can be estimated simply from counts
using maximum likelihood. Table 1 illustrates es-
timated dominance values that correctly resolve
the topological ordering for our running example.
6 Experimental Setup
We tested the effect of introducing the pairwise
dominance model into hierarchical phrase-based
translation on Chinese-to-English and Arabic-to-
English translation tasks, thus studying its effect
in two languages where the use of function words
differs signicantly. Following Setiawan et al
(2007), we identify function words as the N most
frequent words in the corpus, rather than identify-
ing them according to linguistic criteria; this ap-
proximation removes the need for any additional
language-specic resources. We report results
for N = 32, 64, 128, 256, 512, 1024, 2048.7 For
7
We observe that even N = 2048 represents less than
1.5% and 0.8% of the words in the Chinese and Arabic vo-
cabularies, respectively. The validity of the frequency-based
strategy, relative to linguistically-dened function words, is
discussed in Section 8
328
n
a
n
b
j
j
j
z
j
z
j
the last century
of
innovations
are
cell phones
and
computers
?
Z
C
? 4
?
-
 {

?
j
z
j
z
j
j
j
the last century
of
innovations
are
cell phones
and
computers
?
 Z
C
? 4
?
-
{

?
Figure 4: Illustrations for: a) the leftFirst value,
and b) the dontCare value. Thickly bordered
boxes are MCAs of the function words while solid
circles are the alignment points of the function
words. The gray boxes are the intersections of the
two MCAs.
all experiments, we report performance using the
BLEU score (Papineni et al, 2002), and we assess
statistical signicance using the standard boot-
strapping approach introduced by (Koehn, 2004).
Chinese-to-English experiments. We trained
the system on the NIST MT06 Eval corpus ex-
cluding the UN data (approximately 900K sen-
tence pairs). For the language model, we used a 5-
gram model with modied Kneser-Ney smoothing
(Kneser and Ney, 1995) trained on the English side
of our training data as well as portions of the Giga-
word v2 English corpus. We used the NIST MT03
test set as the development set for optimizing inter-
polation weights using minimum error rate train-
ing (MERT; (Och and Ney, 2002)). We carried out
evaluation of the systems on the NIST 2006 eval-
uation test (MT06) and the NIST 2008 evaluation
test (MT08). We segmented Chinese as a prepro-
cessing step using the Harbin segmenter (Zhao et
al., 2001).
Arabic-to-English experiments. We trained
the system on a subset of 950K sentence pairs
from the NIST MT08 training data, selected by
?subsampling? from the full training data using a
method proposed by Kishore Papineni (personal
communication). The subsampling algorithm se-
lects sentence pairs from the training data in a
way that seeks reasonable representation for all n-
grams appearing in the test set. For the language
model, we used a 5-gram model trained on the En-
glish portion of the whole training data plus por-
tions of the Gigaword v2 corpus. We used the
NIST MT03 test set as the development set for
optimizing the interpolation weights using MERT.
We carried out the evaluation of the systems on the
NIST 2006 evaluation set (MT06) and the NIST
2008 evaluation set (MT08). Arabic source text
was preprocessed by separating clitics, the de-
niteness marker, and the future tense marker from
their stems.
7 Experimental Results
Chinese-to-English experiments. Table 2 sum-
marizes the results of our Chinese-to-English ex-
periments. These results conrm that the pairwise
dominance model can signicantly increase per-
formance as measured by the BLEU score, with a
consistent pattern of results across the MT06 and
MT08 test sets. Modeling N = 32 drops the per-
formance marginally below baseline, suggesting
that perhaps there are not enough words for the
pairwise dominance model to work with. Dou-
bling the number of words (N = 64) produces
a small gain, and dening the pairwise dominance
model using N = 128 most frequent words pro-
duces a statistically signicant 1-point gain over
the baseline (p < 0.01). Larger values of N
yield statistically signicant performance above
the baseline, but without further improvements
over N = 128.
Arabic-to-English experiments. Table 3 sum-
marizes the results of our Arabic-to-English ex-
periments. This set of experiments shows a pat-
tern consistent with what we observed in Chinese-
to-English translation, again generally consistent
across MT06 and MT08 test sets although mod-
eling a small number of lexical items (N = 32)
brings a marginal improvement over the baseline.
In addition, we again nd that the pairwise dom-
inance model with N = 128 produces the most
signicant gain over the baseline in the MT06,
although, interestingly, modeling a much larger
number of lexical items (N = 2048) yields the
strongest improvement for the MT08 test set.
329
MT06 MT08
baseline 30.58 24.08
+dom(N = 32) 30.43 23.91
+dom(N = 64) 30.96 24.45
+dom(N = 128) 31.59 24.91
+dom(N = 256) 31.24 24.26
+dom(N = 512) 31.33 24.39
+dom(N = 1024) 31.22 24.79
+dom(N = 2048) 30.75 23.92
Table 2: Experimental results on Chinese-to-
English translation with the pairwise dominance
model (dom) of different N . The baseline (the
rst line) is the original hierarchical phrase-based
system. Statistically signicant results (p < 0.01)
over the baseline are in bold.
MT06 MT08
baseline 41.56 40.06
+dom(N = 32) 41.66 40.26
+dom(N = 64) 42.03 40.73
+dom(N = 128) 42.66 41.08
+dom(N = 256) 42.28 40.69
+dom(N = 512) 41.97 40.95
+dom(N = 1024) 42.05 40.55
+dom(N = 2048) 42.48 41.47
Table 3: Experimental results on Arabic-to-
English translation with the pairwise dominance
model (dom) of different N . The baseline (the
rst line) is the original hierarchical phrase-based
system. Statistically signicant results over the
baseline (p < 0.01) are in bold.
8 Discussion and Future Work
The results in both sets of experiments show con-
sistently that we have achieved a signicant gains
by modeling the topological ordering of function
words. When we visually inspect and compare
the outputs of our system with those of the base-
line, we observe that improved BLEU score often
corresponds to visible improvements in the sub-
jective translation quality. For example, the trans-
lations for the Chinese sentence ?<1 
?2 :3
?4 ?5 ?6 8?7 8 9 ?10 ?11 ?12
?13?, taken from Chinese MT06 test set, are as
follows (co-indexing subscripts represent recon-
structed word alignments):
? baseline: ?military1 intelligence2 un-
der observation8 in5 u.s.6 air raids7 :3 iran4
to9 how11 long12 ?13 ?
? +dom(N=128): ? military1 survey2 :3 how11
long12 iran4 under8 air strikes7 of the u.s6
can9 hold out10 ?13 ?
In addition to some lexical translation errors
(e.g. ?6 should be translated to U.S. Army),
the baseline system also makes mistakes in re-
ordering. The most obvious, perhaps, is its fail-
ure to capture the wh-movement involving the in-
terrogative word ?11 (how); this should move
to the beginning of the translated clause, consis-
tent with English wh-fronting as opposed to Chi-
nese wh in situ. The pairwise dominance model
helps, since the dominance value between the in-
terrogative word and its previous function word,
the modal verb 9(can) in the baseline system's
output, is neither, rather than rightFirst as in the
better translation.
The fact that performance tends to be best us-
ing a frequency threshold of N = 128 strikes
us as intuitively sensible, given what we know
about word frequency rankings.
8
In English,
for example, the most frequent 128 words in-
clude virtually all common conjunctions, deter-
miners, prepositions, auxiliaries, and comple-
mentizers ? the crucial elements of ?syntactic
glue? that characterize the types of linguistic
phrases and the ordering relationships between
them ? and a very small proportion of con-
tent words. Using Adam Kilgarriff's lemma-
tized frequency list from the British National Cor-
pus, http://www.kilgarriff.co.uk/bnc-readme.html,
the most frequent 128 words in English are heav-
ily dominated by determiners, ?functional? ad-
verbs like not and when, ?particle? adverbs like
up, prepositions, pronouns, and conjunctions, with
some arguably ?functional? auxiliary and light
verbs like be, have, do, give, make, take. Con-
tent words are generally limited to a small number
of frequent verbs like think and want and a very
small handful of frequent nouns. In contrast, ranks
129-256 are heavily dominated by the traditional
content-word categories, i.e. nouns, verbs, adjec-
tives and adverbs, with a small number of left-over
function words such as less frequent conjunctions
while, when, and although.
Consistent with these observations for English,
the empirical results for Chinese suggest that our
8
In fact, we initially simply choseN = 128 for our exper-
imentation, and then did runs with alternative N to conrm
our intuitions.
330
approximation of function words using word fre-
quency is reasonable. Using a list of approxi-
mately 900 linguistically identied function words
in Chinese extracted from (Howard, 2002), we ob-
serve that that the performance drops when in-
creasing N above 128 corresponds to a large in-
crease in the number of non-function words used
in the model. For example, with N = 2048, the
proportion of non-function words is 88%, com-
pared to 60% when N = 128.9
One natural extension of this work, therefore,
would be to tighten up our characterization of
function words, whether statistically, distribution-
ally, or simply using manually created resources
that exist for many languages. As a rst step, we
did a version of the Chinese-English experiment
using the list of approximately 900 genuine func-
tion words, testing on the Chinese MT06 set. Per-
haps surprisingly, translation performance, 30.90
BLEU, was around the level we obtained when
using frequency to approximate function words at
N = 64. However, we observe that many of
the words in the linguistically motivated function
word list are quite infrequent; this suggests that
data sparseness may be an additional factor worth
investigating.
Finally, although we believe there are strong
motivations for focusing on the role of function
words in reordering, there may well be value in
extending the dominance model to include content
categories. Verbs and many nouns have subcat-
egorization properties that may inuence phrase
ordering, for example, and this may turn out to ex-
plain the increase in Arabic-English performance
for N = 2048 using the MT08 test set. More gen-
erally, the approach we are taking can be viewed
as a way of selectively lexicalizing the automati-
cally extracted grammar, and there is a large range
of potentially interesting choices in how such lex-
icalization could be done.
9 Related Work
In the introduction, we discussed Chiang's (2005)
constituency feature, related ideas explored by
Marton and Resnik (2008) and Chiang et al
(2008), and the target-side variation investigated
by Zollman et al (2006). These methods differ
from each other mainly in terms of the specic lin-
9
We plan to do corresponding experimentation and anal-
ysis for Arabic once we identify a suitable list of manually
identied function words.
guistic knowledge being used and on which side
the constraints are applied.
Shen et al (2008) proposed to use lin-
guistic knowledge expressed in terms of a de-
pendency grammar, instead of a syntactic con-
stituency grammar. Villar et al (2008) attempted
to use syntactic constituency on both the source
and target languages in the same spirit as the con-
stituency feature, along with some simple pattern-
based heuristics ? an approach also investigated by
Iglesias et al (2009). Aiming at improving the se-
lection of derivations, Zhou et al (2008) proposed
prior derivation models utilizing syntactic annota-
tion of the source language, which can be seen as
smoothing the probabilities of hierarchical phrase
features.
A key point is that the model we have intro-
duced in this paper does not require the linguistic
supervision needed in most of this prior work. We
estimate the parameters of our model from parallel
text without any linguistic annotation. That said,
we would emphasize that our approach is, in fact,
motivated in linguistic terms by the role of func-
tion words in natural language syntax.
10 Conclusion
We have presented a pairwise dominance model
to address reordering issues that are not handled
particularly well by standard hierarchical phrase-
based modeling. In particular, the minimal lin-
guistic commitment in hierarchical phrase-based
models renders them susceptible to overgenera-
tion of reordering choices. Our proposal han-
dles the overgeneration problem by identifying
hierarchical phrases with function words and by
using function word relationships to incorporate
soft constraints on topological orderings. Our
experimental results demonstrate that introducing
the pairwise dominance model into hierarchical
phrase-based modeling improves performance sig-
nicantly in large-scale Chinese-to-English and
Arabic-to-English translation tasks.
Acknowledgments
This research was supported in part by the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-2-001. Any opinions, ndings, conclusions or
recommendations expressed in this paper are those
of the authors and do not necessarily reect the
view of the sponsors.
331
References
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the
2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 224?233, Honolulu,
Hawaii, October.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL'05), pages
263?270, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jiaying Howard. 2002. A Student Handbook for Chi-
nese Function Words. The Chinese University Press.
Gonzalo Iglesias, Adria de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule ltering by pattern
for efcient hierarchical translation. In Proceedings
of the 12th Conference of the European Chapter of
the Association of Computational Linguistics (to ap-
pear).
R. Kneser and H. Ney. 1995. Improved backing-
off for m-gram language modeling. In Proceed-
ings of IEEE International Conference on Acoustics,
Speech, and Signal Processing95, pages 181?184,
Detroit, MI, May.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 127?133,
Edmonton, Alberta, Canada, May. Association for
Computational Linguistics.
Philipp Koehn. 2004. Statistical signicance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395, Barcelona, Spain,
July.
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In Proceedings of The 46th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1003?
1011, Columbus, Ohio, June.
Franz Josef Och and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 295?302, Philadelphia,
Pennsylvania, USA, July.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li.
2007. Ordering phrases with function words. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 712?
719, Prague, Czech Republic, June.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of The 46th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 577?585, Columbus,
Ohio, June.
David Vilar, Daniel Stein, and Hermann Ney. 2008.
Analysing soft syntax features and heuristics for hi-
erarchical phrase based machine translation. Inter-
national Workshop on Spoken Language Translation
2008, pages 190?197, October.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404, Sep.
Tiejun Zhao, Yajuan Lv, Jianmin Yao, Hao Yu, Muyun
Yang, and Fang Liu. 2001. Increasing accuracy
of chinese segmentation with strategy of multi-step
processing. Journal of Chinese Information Pro-
cessing (Chinese Version), 1:13?18.
Bowen Zhou, Bing Xiang, Xiaodan Zhu, and Yuqing
Gao. 2008. Prior derivation models for formally
syntax-based translation using linguistically syntac-
tic parsing and tree kernels. In Proceedings of
the ACL-08: HLT Second Workshop on Syntax and
Structure in Statistical Translation (SSST-2), pages
19?27, Columbus, Ohio, June.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York City,
June.
332
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 145?149,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
The University of Maryland Statistical Machine Translation System for
the Fourth Workshop on Machine Translation
Chris Dyer??, Hendra Setiawan?, Yuval Marton??, and Philip Resnik??
?UMIACS Laboratory for Computational Linguistics and Information Processing
?Department of Linguistics
University of Maryland, College Park, MD 20742, USA
{redpony,hendra,ymarton,resnik} AT umd.edu
Abstract
This paper describes the techniques we
explored to improve the translation of
news text in the German-English and
Hungarian-English tracks of the WMT09
shared translation task. Beginning with a
convention hierarchical phrase-based sys-
tem, we found benefits for using word seg-
mentation lattices as input, explicit gen-
eration of beginning and end of sentence
markers, minimum Bayes risk decoding,
and incorporation of a feature scoring the
alignment of function words in the hy-
pothesized translation. We also explored
the use of monolingual paraphrases to im-
prove coverage, as well as co-training to
improve the quality of the segmentation
lattices used, but these did not lead to im-
provements.
1 Introduction
For the shared translation task of the Fourth Work-
shop on Machine Translation (WMT09), we fo-
cused on two tasks: German to English and Hun-
garian to English translation. Despite belonging to
different language families, German and Hungar-
ian have three features in common that complicate
translation into English:
1. productive compounding (especially of
nouns),
2. rich inflectional morphology,
3. widespread mid- to long-range word order
differences with respect to English.
Since these phenomena are poorly addressed with
conventional approaches to statistical machine
translation, we chose to work primarily toward
mitigating their negative effects when construct-
ing our systems. This paper is structured as fol-
lows. In Section 2 we describe the baseline model,
Section 3 describes the various strategies we em-
ployed to address the challenges just listed, and
Section 4 summarizes the final translation system.
2 Baseline system
Our translation system makes use of a hierarchical
phrase-based translation model (Chiang, 2007),
which we argue is a strong baseline for these
language pairs. First, such a system makes use
of lexical information when modeling reorder-
ing (Lopez, 2008), which has previously been
shown to be useful in German-to-English trans-
lation (Koehn et al, 2008). Additionally, since
the decoder is based on a CKY parser, it can con-
sider all licensed reorderings of the input in poly-
nomial time, and German and Hungarian may re-
quire quite substantial reordering. Although such
decoders and models have been common for sev-
eral years, there have been no published results for
these language pairs.
The baseline system translates lowercased and
tokenized source sentences into lowercased target
sentences. The features used were the rule transla-
tion relative frequency P (e?|f?), the ?lexical? trans-
lation probabilities Plex(e?|f?) and Plex(f? |e?), a rule
count, a target language word count, the target
(English) language model P (eI1), and a ?pass-
through? penalty for passing a source language
word to the target side.1 The rule feature values
were computed online during decoding using the
suffix array method described by Lopez (2007).
1The ?pass-through? penalty was necessary since the En-
glish language modeling data contained a large amount of
source-language text.
145
2.1 Training and development data
To construct the translation suffix arrays used to
compute the translation grammar, we used the par-
allel training data provided. The preprocessed
training data was filtered for length and aligned
using the GIZA++ implementation of IBM Model
4 (Och and Ney, 2003) in both directions and sym-
metrized using the grow-diag-final-and
heuristic. We trained a 5-gram language model
from the provided English monolingual training
data and the non-Europarl portions of the parallel
training data using modified Kneser-Ney smooth-
ing as implemented in the SRI language modeling
toolkit (Kneser and Ney, 1995; Stolcke, 2002). We
divided the 2008 workshop ?news test? sets into
two halves of approximately 1000 sentences each
and designated one the dev set and the other the
dev-test set.
2.2 Automatic evaluation metric
Since the official evaluation criterion for WMT09
is human sentence ranking, we chose to minimize
a linear combination of two common evaluation
metrics, BLEU and TER (Papineni et al, 2002;
Snover et al, 2006), during system development
and tuning:
TER ? BLEU
2
Although we are not aware of any work demon-
strating that this combination of metrics correlates
better than either individually in sentence ranking,
Yaser Al-Onaizan (personal communication) re-
ports that it correlates well with the human evalua-
tion metric HTER. In this paper, we report uncased
TER and BLEU individually.
2.3 Forest minimum error training
To tune the feature weights of our system, we used
a variant of the minimum error training algorithm
(Och, 2003) that computes the error statistics from
the target sentences from the translation search
space (represented by a packed forest) that are ex-
actly those that are minimally discriminable by
changing the feature weights along a single vector
in the dimensions of the feature space (Macherey
et al, 2008). The loss function we used was the
linear combination of TER and BLEU described in
the previous section.
3 Experimental variations
This section describes the experimental variants
explored.
3.1 Word segmentation lattices
Both German and Hungarian have a large number
of compound words that are created by concate-
nating several morphemes to form a single ortho-
graphic token. To deal with productive compound-
ing, we employ word segmentation lattices, which
are word lattices that encode alternative possible
segmentations of compound words. Doing so en-
ables us to use possibly inaccurate approaches to
guess the segmentation of compound words, al-
lowing the decoder to decide which to use during
translation. This is a further development of our
general source-lattice approach to decoding (Dyer
et al, 2008).
To construct the segmentation lattices, we de-
fine a log-linear model of compound word seg-
mentation inspired by Koehn and Knight (2003),
making use of features including number of mor-
phemes hypothesized, frequency of the segments
as free-standing morphemes in a training corpus,
and letters in each segment. To tune the model
parameters, we selected a set of compound words
from a subset of the German development set,
manually created a linguistically plausible seg-
mentation of these words, and used this to select
the parameters of the log-linear model using a lat-
tice minimum error training algorithm to minimize
WER (Macherey et al, 2008). We reused the same
features and weights to create the Hungarian lat-
tices. For the test data, we created a lattice of ev-
ery possible segmentation of any word 6 charac-
ters or longer and used forward-backward pruning
to prune out low-probability segmentation paths
(Sixtus and Ortmanns, 1999). We then concate-
nated the lattices in each sentence.
Source Condition BLEU TER
German
baseline 20.8 60.7
lattice 21.3 59.9
Hungarian
baseline 11.0 71.1
lattice 12.3 70.4
Table 1: Impact of compound segmentation lat-
tices.
To build the translation model for lattice sys-
tem, we segmented the training data using the one-
best split predicted by the segmentation model,
146
and word aligned this with the English side. This
variant version of the training data was then con-
catenated with the baseline system?s training data.
3.1.1 Co-training of segmentation model
To avoid the necessity of manually creating seg-
mentation examples to train the segmentation
model, we attempted to generate sets of training
examples by selecting the compound splits that
were found along the path chosen by the decoder?s
one-best translation. Unfortunately, the segmen-
tation system generated in this way performed
slightly worse than the one-best baseline and so
we continued to use the parameter settings derived
from the manual segmentation.
3.2 Modeling sentence boundaries
Incorporating an n-gram language model proba-
bility into a CKY-based decoder is challenging.
When a partial hypothesis (also called an ?item?)
has been completed, it has not yet been determined
what strings will eventually occur to the left of
its first word, meaning that the exact computation
must deferred, which makes pruning a challenge.
In typical CKY decoders, the beginning and ends
of the sentence (which often have special charac-
teristics) are not conclusively determined until the
whole sentence has been translated and the proba-
bilities for the beginning and end sentence proba-
bilities can be added. However, by this point it is
often the case that a possibly better sentence be-
ginning has been pruned away. To address this,
we explicitly generate beginning and end sentence
markers as part of the translation process, as sug-
gested by Xiong et al (2008). The results of doing
this are shown in Table 2.
Source Condition BLEU TER
German
baseline 21.3 59.9
+boundary 21.6 60.1
Hungarian
baseline 12.3 70.4
+boundary 12.8 70.4
Table 2: Impact of modeling sentence boundaries.
3.3 Source language paraphrases
In order to deal with the sparsity associated with
a rich source language morphology and limited-
size parallel corpora (bitexts), we experimented
with a novel approach to paraphrasing out-of-
vocabulary (OOV) source language phrases in
our Hungarian-English system, using monolingual
contextual similarity rather than phrase-table piv-
oting (Callison-Burch et al, 2006) or monolin-
gual bitexts (Barzilay and McKeown, 2001; Dolan
et al, 2004). Distributional profiles for source
phrases were represented as context vectors over
a sliding window of size 6, with vectors defined
using log-likelihood ratios (cf. Rapp (1999), Dun-
ning (1993)) but using cosine rather than city-
block distance to measure profile similarity.
The 20 distributionally most similar source
phrases were treated as paraphrases, considering
candidate phrases up to a width of 6 tokens and fil-
tering out paraphrase candidates with cosine simi-
larity to the original of less than 0.6. The two most
likely translations for each paraphrase were added
to the grammar in order to provide mappings to
English for OOV Hungarian phrases.
This attempt at monolingually-derived source-
side paraphrasing did not yield improvements over
baseline. Preliminary analysis suggests that the
approach does well at identifying many content
words in translating extracted paraphrases of OOV
phrases (e.g., a kommunista part vezetaje ? ,
leader of the communist party or a ra tervezett?
until the planned to), but at the cost of more fre-
quently omitting target words in the output.
3.4 Dominance feature
Although our baseline hierarchical system permits
long-range reordering, it lacks a mechanism to
identify the most appropriate reordering for a spe-
cific sentence translation. For example, when the
most appropriate reordering is a long-range one,
our baseline system often also has to consider
shorter-range reorderings as well. In the worst
case, a shorter-range reordering has a high proba-
bility, causing the wrong reordering to be chosen.
Our baseline system lacks the capacity to address
such cases because all the features it employs are
independent of the phrases being moved; these are
modeled only as an unlexicalized generic nonter-
minal symbol.
To address this challenge, we included what we
call a dominance feature in the scoring of hypothe-
sis translations. Briefly, the premise of this feature
is that the function words in the sentence hold the
key reordering information, and therefore function
words are used to model the phrases being moved.
The feature assesses the quality of a reordering by
looking at the phrase alignment between pairs of
147
function words. In our experiments, we treated
the 128 most frequent words in the corpus as func-
tion words, similar to Setiawan et al (2007). Due
to space constraints, we will discuss the details in
another publication. As Table 3 reports, the use of
this feature yields positive results.
Source Condition BLEU TER
German
baseline 21.6 60.1
+dom 22.2 59.8
Hungarian
baseline 12.8 70.4
+dom 12.6 70.0
Table 3: Impact of alignment dominance feature.
3.5 Minimum Bayes risk decoding
Although during minimum error training we as-
sume a decoder that uses the maximum derivation
decision rule, we find benefits to translating using
a minimum risk decision rule on a test set (Kumar
and Byrne, 2004). This seeks the translation E of
the input lattice F that has the least expected loss,
measured by some loss function L:
E? = arg min
E?
EP (E|F)[L(E,E
?)] (1)
= arg min
E?
?
E
P (E|F)L(E,E?) (2)
We approximate the posterior distribution
P (E|F) and the set of possible candidate transla-
tions using the unique 500-best translations of a
source lattice F . If H(E,F) is the decoder?s path
weight, this is:
P (E|F) ? exp?H(E,F)
The optimal value for the free parameter ?must
be experimentally determined and depends on the
ranges of the feature functions and weights used in
the model, as well as the amount and kind of prun-
ing using during decoding.2 For our submission,
we used ? = 1. Since our goal is to minimize
TER?BLEU
2 we used this as the loss function in (2).
Table 4 shows the results on the dev-test set for
MBR decoding.
2If the free parameter ? lies in (1,?) the distribution is
sharpened, if it lies in [0, 1), the distribution is flattened.
Source Decoder BLEU TER
German
Max-D 22.2 59.8
MBR 22.6 59.4
Hungarian
Max-D 12.6 70.0
MBR 12.8 69.8
Table 4: Performance of maximum derivation vs.
MBR decoders.
4 Conclusion
Table 5 summarizes the impact on the dev-test set
of all features included in the University of Mary-
land system submission.
Condition
German Hungarian
BLEU TER BLEU TER
baseline 20.8 60.7 11.0 71.1
+lattices 21.3 59.9 12.3 70.4
+boundary 21.6 60.1 12.8 70.4
+dom 22.2 59.8 12.6 70.0
+MBR 22.6 59.4 12.8 69.8
Table 5: Summary of all features
Acknowledgments
This research was supported in part by the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-2-001, and the Army Research Laboratory.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the view of the
sponsors. Discussions with Chris Callison-Burch
were helpful in carrying out the monolingual para-
phrase work.
References
Regina Barzilay and Kathleen McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In In
Proceedings of ACL-2001.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine trans-
lation using paraphrases. In Proceedings NAACL-
2006.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsu-
pervised construction of large paraphrase corpora:
148
exploiting massively parallel news sources. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics of the Association for
Computational Linguistics, Geneva, Switzerland.
T. Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
Chris Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-08: HLT. Association for Compu-
tational Linguistics, June.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In Proceedings of
IEEE Internation Conference on Acoustics, Speech,
and Signal Processing, pages 181?184.
P. Koehn and K. Knight. 2003. Empirical methods
for compound splitting. In Proceedings of the EACL
2003.
Philipp Koehn, Abhishek Arun, and Hieu Hoang.
2008. Towards better machine translation quality for
the German-English language pairs. In ACL Work-
shop on Statistical Machine Translation.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In Pro-
cessings of HLT-NAACL.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 976?
985.
Adam Lopez. 2008. Tera-scale translation models
via pattern matching. In Proceedings of COLING,
Manchester, UK.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uszkoreit. 2008. Lattice-based minimum
error rate training for statistical machine translation.
In Proceedings of EMNLP, Honolulu, HI.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 160?167, Sapporo, Japan,
July.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual
Meeting of the ACL, pages 311?318.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of the 37th Annual
Conference of the Association for Computational
Linguistics., pages 519?525.
Hendra Setiawan, Min-Yen Kan, and Haizhao Li.
2007. Ordering phrases with function words. In
Proceedings of ACL.
S. Sixtus and S. Ortmanns. 1999. High quality word
graphs using forward-backward pruning. In Pro-
ceedings of ICASSP, Phoenix, AZ.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
study of translation edit rate with targeted human an-
notation. In Proceedings of Association for Machine
Translation in the Americas.
A. Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Intl. Conf. on Spoken Language
Processing.
Deyi Xiong, Min Zhang, Ai Ti Aw, Haitao Mi, Qun
Liu, and Shouxun Lin. 2008. Refinements in BTG-
based statistical machine translation. In Proceed-
ings of IJCNLP 2008.
149
Phrase-Based Statistical Machine Translation:
A Level of Detail Approach
Hendra Setiawan1,2, Haizhou Li1, Min Zhang1, and Beng Chin Ooi2
1 Institute for Infocomm Research,
21 Heng Mui Keng Terrace,
Singapore 119613
{stuhs, hli, mzhang}@i2r.a-star.edu.sg
2 School of Computing,
National University of Singapore,
Singapore 117543
{hendrase, ooibc}@comp.nus.edu.sg
Abstract. The merit of phrase-based statistical machine translation is
often reduced by the complexity to construct it. In this paper, we ad-
dress some issues in phrase-based statistical machine translation, namely:
the size of the phrase translation table, the use of underlying transla-
tion model probability and the length of the phrase unit. We present
Level-Of-Detail (LOD) approach, an agglomerative approach for learn-
ing phrase-level alignment. Our experiments show that LOD approach
significantly improves the performance of the word-based approach. LOD
demonstrates a clear advantage that the phrase translation table grows
only sub-linearly over the maximum phrase length, while having a per-
formance comparable to those of other phrase-based approaches.
1 Introduction
Early approach to statistical machine translation relies on the word-based trans-
lation model to describe the translation process [1]. However, the underlying as-
sumption of word-to-word translation often fails to capture all properties of the
language, i.e. the existence of the phrase where a group of words often function
together as a unit. Many researchers have proposed to move from the word-based
to the phrase-based translation model [2] [3] [4]. A phrase-based approach offers
many advantages as a phrase translation captures word context and local re-
ordering inherently [3]. It has become popular in statistical machine translation
applications.
There are typically two groups of approaches to constructing the phrase-
based model. The first group learns phrase translation directly from the sen-
tence pair. It learns both word and phrase units simultaneously. Although these
approaches appear intuitive, it usually suffers from a prohibitive computational
cost. It might have to consider all possible multi-word sequences as phrase can-
didates and all possible pairings as phrase translations at the same time.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 576?587, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Phrase-Based Statistical Machine Translation: A Level of Detail Approach 577
The second group of approaches learns phrase translations through word-level
alignment: alignment template [2] and projection extension [6], just to name a
few. In general, these approaches take the word-level alignment, a by-product of
the word-based translation model, as their input and then utilize a heuristic mea-
surement to learn the phrase translation. The heuristic measurement contains
all possible configurations of word-level alignment on a phrase translation.
It is noted that the underlying word-level alignment is just an approximation
to the exact alignment. The approximation is reflected by a probability produced
by the word-based translation model. The majority of approaches do not make
use of this probability, whereas it may provide a valuable clue leading to a better
phrase translation from a statistical point of view. Koehn, et. al [8] compared the
representative of both groups and reported that learning phrase translation using
a simple heuristic from word alignment yields a better translation performance
than learning phrase translation directly from the sentence pair.
Many approaches try to learn all phrase translations in one step, either di-
rectly from the sentence pair or through word alignment. As a result, they may
encounter a huge amount of phrase translation candidates at once. Usually, they
limit the maximum phrase length to reduce the choice of candidates. Although
this method is sufficient to satisfy the computational requirement, it comes with
the cost of not finding the good phrases longer than the imposed limit. Addition-
ally, to reduce the candidates, those approaches use a threshold to separate good
phrase translation from the rest. The threshold is ad-hoc and often not capable
of making a clear separation. Therefore, the use of threshold often comes with
the cost of the inclusion of undesired phrase translations and the absence of good
phrase translations in the phrase translation table. The cost may be reflected
from the size of the phrase translation table that often grows almost linearly over
the phrase length limit [6][8]. The growth implies a non-intuitive behavior: two
phrases with different length introduce an equal number of additional entries to
the phrase translation table. As longer phrases occur less often, there should be
fewer entries introduced into the phrase translation table.
We propose an agglomerative approach to learn phrase translations. Our
approach is motivated by the second group, which is to learn phrase translation
through word-alignment, while addressing the common issues: the size of the
phrase translation table, the use of underlying translation model probability
and the length of the phrase unit.
Only a few approaches move away from one-step learning. Melamed [13]
presented an agglomerative approach to learn the phrases progressively from
a parallel corpus by using sub-phrase bigram statistics. Moore [14] proposed
a similar approach which identifies the phrase candidates by parsing the raw
training data. Our idea differs from these approaches in that we look into the
association of the alignments rather than the association of the words to discover
the phrases.
In this paper, we propose the Level of Detail (LOD) approach for learning
of phrase translations in phrase-based statistical machine translation. Section 2
discusses the background and motivation and then formulates the LOD approach
578 H. Setiawan et al
while section 3 describes the learning process in details. Section 4 describes
the experimental results. In this section, we compare LOD with state-of-the-art
word-based approach in translation tasks. Finally, section 5 concludes this paper
by providing some discussion in comparison with other related works.
2 Statistical Machine Translation: A Level of Detail
2.1 Motivation and Background
It is often not intuitive to model the translation of a phrase using the word-based
translation model. First, the literal translation of phrase constituents is often in-
appropriate from a linguistic point of view. The word-based translation model
treats a phrase as a multi-word. One such example is the case where a phrase
appears as an idiom. The translation of an idiom cannot be synthesized from
the literal translation of its constituents but rather from the semantic trans-
lation of the whole. Besides, the literal translation of an idiom detracts from
the intended meaning. In one such example, the literal translation of French
?manger sur le pouce? is ?to eat on the thumb?. This detracts from the correct
translation ?to grab a bite to eat ?. In addition, to produce the correct trans-
lation, the word-based translation model might have to learn that ?manger?
is translated as ?eat? or ?pouce? is translated as ?thumb?. Although it may
serve the translation purpose, it will introduce many non-intuitive entries to the
dictionary.
Second, even if it is possible to translate a phrase verbatim, modeling phrase
translation using the word-based translation model suffers from a disadvantage:
the number of word alignments required to synthesize the phrase translation is
large. It requires four word alignments to model the translation between ?une
minute de silence? and ?one minute of silence?, whereas one phrase alignment
is adequate. The introduction of more alignments also implies the requirement
to estimate more parameters for the translation model. The implication often
comes with the cost of learning wrong word alignments.
Third, a phrase often constitutes some spurious words. The word-based trans-
lation model often has trouble in modeling spurious words, such as function
words. Function words may appear freely in any position and often may not
be translated to any word. We observe that many of these function words ap-
pear inside a phrase. It is beneficial to realize these spurious words inside a
phrase unit so as to improve statistical machine translation performance and
also to remove the necessity to model them explicitly. All these suggest that,
ideally, a phrase translation should be realized as a phrase alignment, where
the lexical correspondence is established on phrase level rather than on its word
constituents.
The discussion above suggests that phrase-based translation is a wise choice.
Practically, as a phrase is not a well defined lexical entry, a mechanism is needed
to judge what constitutes a phrase in the context of statistical machine transla-
tion. In this paper, we advocate an approach to look into the phrase discovery
process at different level of details. The level of detail refers to the size of a
Phrase-Based Statistical Machine Translation: A Level of Detail Approach 579
phrase unit. At its finest level of detail, a phrase translation uses the word-based
translation model where a phrase is modeled through its word constituent. At
a coarser level of detail, a sub-phrase unit is introduced as a sequence of words,
making it a constituent of the phrase. The coarsest level of detail refers to the
status of a phrase where all word constituents converge into a whole unit.
Our Level-Of-Detail (LOD) approach views the problem of phrase-based
translation modeling through a LOD process. It starts from the finest word-
level alignment and transforms the phrase translation into its coarsest level of
detail.
2.2 Formulation
Let < e, f > be a sentence pair of two sequences of words with e as an English
sentence and f as its translation in French1. Let < e?, f? > represents the same
sentence pair but with the phrase as its atomic unit rather than the word. To
generalize the notation, we treat word and phrase unit similarly by considering
a word as a phrase of length one. Therefore, < e, f > hereafter will be referred as
< e?, f? >(0), which represents the finest level of detail, and < e?, f? > as < e?, f? >(N),
which represents the coarsest level of detail. Let each tuple in the sentence pair
of any level of detail n, < e?, f? >(n) be e?(n) = {e?(n)0 , e?
(n)
1 , . . . , e?
(n)
i , . . . , e?
(n)
l(n)} and
f?
(n)
= {f? (n)0 , f?
(n)
1 , . . . , f?
(n)
j , . . . , f?
(n)
m(n)} where e?
(n)
0 ,f?
(n)
0 represent the special token
NULL as suggested in [1] and l(n),m(n) represent the length of the corresponding
sentence. Let T (n) be a set of alignment defined over the sentence pair < e?, f? >(n)
with t(n)ij = [e?
(n)
i , f?
(n)
j ] as its member. The superscript in all notations denotes
the level of detail where 0 represents the finest and N represents the coarsest
level of detail.
LOD algorithm iteratively transforms < e?, f? >(0) to < e?, f? >(N) through
re-alignment of phrases and re-estimation of phrase translation probability. At
n-th iteration, LOD harvests all bi-directional alignments from the sentence pair
< e?, f? >(n). The alignment is obtained by a typical word-based translation model,
such as the IBM model, while treating a sub-phrase at n-th iteration as a word.
We refer to those alignments as B(n), a pool of sub-phrase alignments unique to
the particular iteration. Afterwards, LOD generates all possible phrase alignment
candidates C(n) for a coarser level of detail from these sub-phrase alignments.
A resulting phrase alignment candidate is basically a joining of two adjacent
sub-phrase alignments subject to a certain criterion. It represents the future
coarser level alignment. Up to this point, two sets of alignment are obtained
over< e?, f? >(n): a pool of sub-phrase alignments B(n) at the current level and a
pool of phrase alignment candidates C(n) at a coarser level. From these two sets
of alignments B(n) ?C(n), we would like to derive a new set of alignments T (n+1)
that best describes the training corpus with the re-estimated statistics obtained
at n-th iteration. LOD constructs < e?, f? >(n+1) from the new set of alignment.
Algorithm 1 provides the general overview of LOD algorithm.
1 Subsequently, we will refer e as source sentence and f as target sentence, but the
term does not always reflect the translation direction.
580 H. Setiawan et al
Algorithm 1. An overview of LOD approach in learning phrase translation. The LOD
approach takes a sentence pair at its finest level of detail as its input, learns the phrase-
level alignment iteratively and outputs the same sentence pair at its coarsest level of
detail along with its phrase translation table.
input ?e?, f??(0)
for n = 0 to (N ? 1) do
- Generate bi-directional sub-phrase level alignments B(n) from ?e?, f??(n)
- Identify phrase-level alignment candidates C(n) from B(n)
- Estimate the alignment probability in B(n) and C(n)
- Learn coarser level alignment T (n+1) from B(n) ? C(n) and construct ?e?, f??(n+1)
output ?e?, f??(N) and T (N)
3 Learning Phrase Translation
In this section, we discuss the steps of LOD algorithm in detail. As presented
in Algorithm 1, moving from one level of alignment to its coarser level, LOD
follows four simple steps:
1. Generation of bi-directional sub-phrase level alignments 2
2. Identification of phrase level alignment candidates
3. Estimation of alignment probability
4. Learning coarser level alignment
3.1 Generation of Bi-directional Sub-phrase Level Alignments
LOD follows the common practice to utilize the IBM translation model for learn-
ing the phrase translation. That is to harvest all alignments from both translation
directions. For the sake of clarity, LOD defines the following notation for these
alignments, as follows:
Let ? (n)ef : e?
(n)
i ?? f?
(n)
j be an alignment function represents all alignments
from translating the source English sentence to the target French sentence, and
? (n)fe : f?
(n)
j ?? e?
(n)
i be the reversed translation direction. Then, bi-directional
sub-phrase alignment B(n) includes all possible alignment by both functions:
B(n) = {t(n)ij = [e?
(n)
i , f?
(n)
j ]|(?
(n)
ef (e?
(n)
i ) = f?
(n)
j ) ? (?
(n)
fe (f?
(n)
j ) = e?
(n)
i )}
Let us denote NULL alignments, N (n), a subset of alignments in B(n) in
which the special token NULL is involved.
2 The process starts with word level alignment. A word here is also referred to as a
sub-phrase.
Phrase-Based Statistical Machine Translation: A Level of Detail Approach 581
3.2 Identification of Phrase Alignment Candidates
LOD applies a simple heuristic to identify a phrase alignment candidate. First,
LOD considers every combination of two distinct sub-phrase alignments and as-
sesses its candidacy. Here, we define a phrase alignment candidate < t(n)ij , t
(n)
i?j? >?
C(n) as follows:
Let < t(n)ij , t
(n)
i?j? > be a set of two tuples, where t
(n)
ij ? B(n) and t
(n)
i?j? ? B(n).
Then < t(n)ij , t
(n)
i?j? > is a phrase aligment candidate if and only if
1. not ((i, i?)= 0) or (|i ? i?| = 1)
2. not ((t(n)ij ? N (n)) and (t
(n)
i?j? ? N (n)))
In the definition above, the first clause defines a candidate as a set of two whose
source sub-phrases are adjacent. The second clause forbids the consideration of
two NULL alignments.
As LOD considers only two alignments for each phrase alignment candidate,
it implies that, at the n-th iteration, the length of the longest possible phrase
is bounded by 2n. Apparently, we do not have to examine sub-phrase alignment
trunks of more than two sub-phrases because the iteration process guarantees
LOD to explore phrases of any length given sufficient iteration. This way, the
search space at each iteration can be manageable at each iteration.
3.3 Estimation of Alignment Probability
Joining the alignment set B(n) derived in Section 3.1 and the coarser level align-
ment C(n) derived in Section 3.2, we form a candidate alignment set B(n) ? C(n).
Assuming that there are two alignments x ? B(n), y ? B(n), and a candidate
alignment < x, y >? C(n), we derive the probability p(x) and p(y) from the
statistics as the count of x and y normalized by the number of alignments in the
corpus, and we derive the joint probability p(< x, y >) in a similar way.
If there is a genuine association between the two alignments, x and y, then
we expect that p(< x, y >)  p(x)p(y). If there is no interesting relationship
between x and y, then p(< x, y >) ? p(x)p(y) where we say that x and y are
independent. If x and y are in a complementary relationship, then we expect to
see that p(< x, y >)  p(x)p(y). These statistics allow us to discover a genuine
sub-phrase association.
The probability is estimated by the count of observed events normalized by
the corpus size. Note that the alignment from the IBM translation model is
derived using a Viterbi-like decoding scheme. Each observed event is counted as
one. This is referred to as hard-counting. As the alignment is done according to
probability distribution, another way of counting the event is to use the fractional
count that can be derived from the translation model. We refer to it as soft-
counting.
3.4 Learning a Coarser Level Alignment
From section 3.1 to 3.3, we have prepared all the necessary alignments with their
probability estimates. The next step is to re-align < e?, f? >(n) into < e?, f? >(n+1)
582 H. Setiawan et al
using alignment phrases in B(n) ? C(n) with their newly estimated probability
distribution. The re-alignment is considered as a constrained search process. Let
p(t(n)ij ) be the probability of a phrase alignment t
(n)
ij ? (B(n) ? C(n)) as defined
in Section 3.3, T (n) be the potential new alignment sequence for < e?, f? >(n), we
have the likelihood for T (n) as
log P (< e?, f? >(n) |T (n)) =
?
t(n)ij ?T
(n)
log p(t(n)ij ) (1)
The constrained search is to decode an alignment sequence that produces
the highest likelihood possible in the current iteration, subject to the following
constraints:
1. to preserve the phrase ordering of the source and target languages
2. to preserve the completeness of word or phrase coverage in the sentence pair
3. to ensure the mutual exclusion between alignments (except for the special
NULL tokens)
The constrained search can be formulated as follows:
T (n+1) = argmax
?T (n)
log P (< e?, f? >(n) |T (n)) (2)
In Eq.(2), we have T (n+1) as the best alignment sequence to re-align sentence
pair < e?, f? >(n) to < e?, f? >(n+1) .
The constraints are to ensure that the search leads to a valid alignment re-
sult. The search is essentially a decoding process, which traverses the sentence
pair along the source language and explores all the possible phrase alignments
with the target language. In practice, LOD tries to find a phrase translation
table that maximizes Eq.(2) as formulated in Algorithm 2. As the existing align-
ment for < e?, f? >(n) in the n-th iteration is a valid alignment subject to three
Algorithm 2. A stack decoding algorithm to explore the best alignment path between
source and target languages by considering all alignment candidates in B(n) ? C(n) at
n-th iteration.
1. Initialize a lattice of l(n) slots for l(n) sub-phrase in source language.
2. Starting from i=1, for all phrases in source language ei;
1) Register all the alignments t(n)ij that map source phrases ending with ei,
including ei itself, into slot i in the lattice;
2) Register the probability of alignment p(t(n)ij ) together with
the alignment entry t(n)ij
3) Repeat 1) and 2) until i=l(n)
3. Apply stack decoding [15] process to find the top n-best paths subject to the
three constraints. During the decoding processing, the extension of partial path
is subject to a connectivity test to enforce the three constraints.
4. Output the top best alignment result as the final result.
Phrase-Based Statistical Machine Translation: A Level of Detail Approach 583
constraints, it also serves as one resolution to the search. In the worst case, if the
constrained search can not discover any new alignment other than the existing
one, then the existing alignment in the current iteration will stand through the
next iteration.
In Algorithm 2, we establish the lattice along the source language. In the
case of English to French translation, we follow the phrases in the English order.
However, it can be done along the target language as well since our approach
follows a symmetric many-to-many word alignment strategy.
This step ends with the promotion of all phrase alignment candidates in the
best alignment sequence T (n+1). The promotion includes the merging of the two
sub-phrase alignments and the concerning sub-phrases. The merged unit will be
considered as a unit in the next iteration.
4 Experiments
The objective of our experiments is to validate our LOD approach in ma-
chine translation task. Additionally, we are interested in investigating the fol-
lowing: the effect of soft-counting in probability estimation, and the behav-
ior of LOD approach in every iteration, in terms of the length of the phrase
unit and the size of the phrase translation table. We report all our experi-
ments using BLEU metrics [10]. Furthermore, we report confidence intervals
with 95% statistical significance level of each experiments, as suggested by
Koehn [16].
We validate our approach through several experiments using English and
French language pairs from the Hansard corpus. We restrict the sentence length
to at most 20 words to obtain around 110 thousands sentence pairs. Then we
randomly select around 10 thousands sentence pair as our own testing set. In
total, the French corpus consists of 994,564 words and 29,360 unique words; while
the English corpus consists of 1,055,167 words and 20,138 unique words. Our
experiment is conducted on both English-to-French (e2f) and French-to-English
(f2e) tasks under open testing set-up. We use these available tools: GIZA++3
for word-based IBM 4 model training and ISI ReWrite4 for translation test. For
measuring the BLEU score and deriving the confidence intervals, we use the
publicly available tools5.
4.1 Soft-Counting vs. Hard-Counting
Table 1 summarizes our experiments in analyzing the effect of soft-counting
and hard-counting in the probability estimation on the BLEU score. Case I
demonstrates the BLEU score of the experiment using the underlying transla-
tion model probability or soft-counting, while Case II demonstrates the score of
3 http://www.fjoch.com/
4 http://www.isi.edu/licensed-sw/rewrite-decoder/
5 http://www.nist.gov/speech/tests/mt/resources/scoring.htm and
http://projectile.is.cs.cmu.edu/research/public/tools/bootStrap/tutorial.htm
584 H. Setiawan et al
Table 1. Summary of experiment showing the contribution of using the translation
model probability. The experiments are conducted on English-to-French task. Case I
indicates the BLEU score of the LOD approach using soft-counting whereas Case II
indicates the BLEU score of hard-counting. The value in the column indicates the
BLEU score. The range inside the bracket indicates the confidence intervals with 95%
statistical significance level.
iteration Case I Case II
1 29.60 (29.01-30.14) 28.80 (28.20-29.38)
2 30.72 (30.09-31.29) 30.11 (29.48-30.67)
3 31.52 (30.87-32.06) 30.70 (30.05-31.32)
4 31.93 (31.28-32.50) 30.93 (30.30-31.51)
5 31.90 (31.45-32.68) 31.07 (30.39-31.62)
hard-counting. The experimental results suggest that the use of the underlying
translation model probability is beneficial as it gives consistently higher BLEU
scores in all the iterations. The comparison using paired bootstrap resampling
[16] also confirms the conclusion.
4.2 LOD Behavior over Iteration
Table 2 summarizes the performance of our LOD approach for the first 10 itera-
tions in comparison with the baseline IBM 4 word-based approach. The results
show that the LOD approach produces a significant improvement over IBM 4
consistently. The first iteration yields the biggest improvement. We achieve an
absolute BLEU score improvement of 5.01 for the English-to-French task and
5.48 for the French-to-English task from the first iteration. The subsequent im-
provement is obtained by performing more iterations and capturing longer phrase
translation, however, the improvement gained is less significant compared to that
of the first iteration.
Table 2 also summarizes the maximum phrase length and the behavior of
the phrase translation table: its size and its increment over iteration. It shows
that the phrase length is soft-constrained by the maximum likelihood criterion
in Eq. (2) rather than limited. As iteration goes on, longer phrases are learnt
but their probabilities are less probable than shorter one. Consequently, longer
phrases introduce fewer entries to the phrase translation table. Table 2 captures
the behavior of the phrase translation table. The first iteration contributes the
highest increment of 12.5 % to the phrase translation table while the accumulated
increment of table size up to 10th iteration only contributes 27.5% increment
over the original size. It suggests that as iteration goes and longer phrases are
captured, fewer additional entries are introduced to the phrase translation table.
The results also show the growth of the size of the phrase translation table is
sub-linear and it converges after reasonable number of iterations. This represents
a clear advantage of LOD over other related work [6][8].
Phrase-Based Statistical Machine Translation: A Level of Detail Approach 585
Table 2. Summary of experiments showing the behavior of LOD approach and the
characteristics of the phrase translation table in each iteration. The table shows the
translation performance of the word-based IBM 4 approach and the first 10 iteration of
LOD approach in BLEU score. The value in the columns indicate the BLEU score while
the range inside the bracket represents the confidence intervals with 95% statistical
significance level. The table also shows the trend of the phrase translation table: the
maximum phrase length, its size, and its increase over iterations.
Max Table BLEU with confidence intervals
Iteration Phrase Size Increase
Length e2f f2e
IBM 4 1 216,852 - 24.59 (24.12-25.21) 26.76 (26.15-27.33)
1 2 244,097 27,245 29.60 (29.01-30.14) 32.24 (31.58-32.83)
2 4 258,734 14,637 30.72 (30.09-31.29) 32.93 (32.28-33.57)
3 7 266,209 7,475 31.52 (30.87-32.06) 33.88 (33.22-34.49)
4 7 270,531 4,322 31.93 (31.28-32.50) 34.14 (33.46-34.76)
5 10 271,793 1,262 31.90 (31.45-32.68) 34.26 (33.56-34.93)
6 11 273,589 1,796 32.14 (31.48-32.72) 34.50 (33.78-35.16)
7 12 274,641 1,052 32.09 (31.43-32.68) 34.55 (33.81-35.18)
8 12 275,399 758 32.07 (31.39-32.60) 34.43 (33.71-35.09)
9 13 275,595 196 31.98 (31.32-32.55) 34.65 (33.93-35.29)
10 14 276,508 913 32.22 (31.55-32.79) 34.61 (33.91-35.26)
5 Discussion
In this paper, we propose LOD approach to phrase-based statistical machine
translation. The LOD approach addresses three issues in the phrase-based trans-
lation framework: the size of phrase translation table, the use of underlying
translation model probability and the length of the phrase unit.
In terms of the size of the phrase translation table, our LOD approach
presents a sub-linear growth of the phrase translation table. It demonstrates a
clear advantage over other reported attempts, such as in [6][8] where the phrase
translation table grows almost linearly over the phrase length limit. The LOD
approach manages the phrase translation table size in a systematic way as a
result of the incorporation of maximum likelihood criterion into the phrase dis-
covery process.
In terms of the use of underlying translation model probability, we propose
to use soft-counting instead of hard-counting in the re-estimation processing of
probability estimation. In the projection extension algorithm [6], the phrases are
learnt based on the presence of alignment in certain configurations. In alignment
template[2], two phrases are considered to be translation of each other, if the
word alignments exist within the phrases and not to the words outside. Both
methods are based on hard-counting of translation event. Our experiment results
suggest the use of soft-counting.
586 H. Setiawan et al
In terms of the length of the phrase unit, we move away from the window-like
limit for phrase candidacy [4][9]. The LOD approach is shown to be more flexible
in capturing phrases of different length. It gradually explores longer phrases as
iteration goes, leading any reasonable length given sufficient iteration as long as
they are statistically credible.
It is known that statistical machine translation relies very much on the
training corpus. A larger phrase translation table means more training data
are needed for the translation model to be statistically significant. In this paper,
we successfully introduce the LOD approach to control the process of new phrase
discovery process. The results are encouraging.
References
1. Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert
L. Mercer. 1993. The mathematics of statistical machine translation: parameter
estimation. Computational Linguistics, 19(2), pp. 263-311.
2. Franz Josef Och, Christoph Tillmann, and Hermann Ney. 1999. Improved alignment
models for statistical machine translation. In Proc of the Joint SIGDAT Conference
on Empirical Methods in Natural Language Processing and Very Large Corpora,
pp. 20-28, University of Maryland, College Park, MD, June.
3. Franz Josef Och and Hermann Ney. 2000. A Comparison of alignment models for
statistical machine translation. In Proc of the 18th International Conference of
Computational Linguistics, Saarbruken, Germany, July.
4. Daniel Marcu and William Wong. 2002. A phrase-Based, joint probability model for
statistical machine translation. In Proc. of the Conference on Empirical Methods
in Natural Language Processing, pp. 133-139, Philadelphia, PA, July.
5. Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word
alignment in statistical translation, Proc. of COLING ?96: The 16th International
Conference of Computational Linguistics. pp. 836-841. Copenhagen, Denmark.
6. Christoph Tillmann. 2003. A projection extension algorithm for statistical machine
translation. in Proc. of the Conference on Empirical Methods in Natural Language
Processing, Sapporo, Japan.
7. Ying Zhang, Stephan Vogel, Alex Waibel. 2003. Integrated phrase segmentation
and alignment algorithm for statistical machine translation. in Proc. of the Confer-
ence on Natural Language Processing and Knowledge Engineering, Beijing, China.
8. Philipp Koehn, Franz Josef Och, Daniel Marcu. 2003. Statistical Phrase-based
Translation. In Proc. of the Human Language Technology Conference, pp. 127-
133, Edmonton, Canada, May/June.
9. Ashish Venugopal, Stephan Vogel, Alex Waibel. 2004. Effective phrase translation
extraction from alignment models. in Proc. of 41st Annual Meeting of Association
of Computational Linguistics, pp. 319-326, Sapporo, Japan, July.
10. K. Papineni, S. Roukos, T. Ward and W. J. Zhu. 2001. BLEU: A method for
automatic evaluation of machine translation. Technical Report RC22176 (W0109-
022), IBM Research Report.
11. G. Doddington. 2002. Automatic evaluation of machine translation quality using
N-gram co-occurence statistics. In Proc. of the Conference on Human Language
Technology, pp. 138-135, San Diego, CA, USA.
Phrase-Based Statistical Machine Translation: A Level of Detail Approach 587
12. Richard Zens, Hermann Ney. 2004. Improvements in phrase-Based statistical ma-
chine translation. in Proc. of Conference on Human Language Technology, pp.
257-264, Boston, MA, USA.
13. I. D. Melamed. 1997. Automatic discovery of non-compositional compounds in par-
allel data. In Proc. of 2nd Conference on Empirical Methods in Natural Language
Processing, Provicence, RI.
14. Robert C Moore. 2001. Towards a simple and accurate statistical approach to
learning translation relationships among words. In Proc of Workshop on Data-
driven Machine Translation, 39th Annual Meeting and 10th Conference of the
European Chapter, Association for Computational Linguistics, pp. 79-86, Toulouse,
France.
15. R Schwartz and Y. L. Chow . 1990. The N-best algorithm: An efficient and exact
procedure for finding the N most likely sentence hypothesis. In Proc. of ICASSP
1990, pp. 81-84. Albuquerque, CA.
16. Philipp Koehn. 2004. Statistical significance tests for machine translation evalua-
tion. In Proc. of the 2004 Conference on Empirical Methods in Natural Language
Processing, pp. 388-395.
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 534?544,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Discriminative Word Alignment with a Function Word Reordering Model
Hendra Setiawan
UMIACS
University of Maryland
hendra@umiacs.umd.edu
Chris Dyer
Language Technologies Institute
Carnegie Mellon University
cdyer@cs.cmu.edu
Philip Resnik
Linguistics and UMIACS
University of Maryland
resnik@umd.edu
Abstract
We address the modeling, parameter estima-
tion and search challenges that arise from the
introduction of reordering models that capture
non-local reordering in alignment modeling.
In particular, we introduce several reordering
models that utilize (pairs of) function words
as contexts for alignment reordering. To ad-
dress the parameter estimation challenge, we
propose to estimate these reordering models
from a relatively small amount of manually-
aligned corpora. To address the search chal-
lenge, we devise an iterative local search al-
gorithm that stochastically explores reorder-
ing possibilities. By capturing non-local re-
ordering phenomena, our proposed alignment
model bears a closer resemblance to state-
of-the-art translation model. Empirical re-
sults show significant improvements in align-
ment quality as well as in translation perfor-
mance over baselines in a large-scale Chinese-
English translation task.
1 Introduction
In many Statistical Machine Translation (SMT) sys-
tems, alignment represents an important piece of in-
formation, from which translation rules are learnt.
However, while translation models have evolved
from word-based to syntax-based modeling, the de
facto alignment model remains word-based (Brown
et al, 1993; Vogel et al, 1996). This gap be-
tween alignment modeling and translation modeling
is clearly undesirable as it often generates tensions
that would prevent the extraction of many useful
translation rules (DeNero and Klein, 2007). Recent
work, e.g. by Blunsom et al (2009) and Haghihi et
al. (2009) just to name a few, show that alignment
models that bear closer resemblance to state-of-the-
art translation model consistently yields not only a
better alignment quality but also an improved trans-
lation quality.
In this paper, we follow this recent effort to nar-
row the gap between alignment model and trans-
lation model to improve translation quality. More
concretely, we focus on the reordering component
since we observe that the treatment of reordering re-
mains significantly different when comparing align-
ment versus translation: the reordering component
in state-of-the-art translation models has focused
on long-distance reordering, but its counterpart in
alignment models has remained focused on local
reordering, typically modeling distortion based en-
tirely on positional information. This leaves most
alignment decisions to association-based scores.
Why is employing stronger reordering models
more challenging in alignment than in translation?
One answer can be attributed to the fact that align-
ment points are unobserved in parallel text, thus so
are their reorderings. As such, introducing stronger
reordering often further exacerbates the computa-
tional complexity to do inference over the model.
Some recent alignment models appeal to external
linguistic knowledge, mostly by using monolingual
syntactic parses (Cherry and Lin, 2006; Pauls et al,
2010), which at the same time, provides an approx-
imation of the bilingual syntactic divergences that
drive the reordering. To our knowledge, however,
this approach has been used mainly to constrain re-
ordering possibilities, or to add to the generalization
ability of association-based scores, not to directly
model reordering in the context of alignment.
534
In this paper, we introduce a new approach to im-
proving the modeling of reordering in alignment. In-
stead of relying on monolingual parses, we condi-
tion our reordering model on the behavior of func-
tion words and the phrases that surround them.
Function words are the ?syntactic glue? of sen-
tences, and in fact many syntacticians believe that
functional categories, as opposed to substantive cat-
egories like noun and verb, are primarily responsi-
ble for cross-language syntactic variation (Ouhalla,
1991). Our reordering model can be seen as offering
a reasonable approximation to more fully elaborated
bilingual syntactic modeling, and this approxima-
tion is also highly practical, as it demands no exter-
nal knowledge (other than a list of function words)
and avoids the practical issues associated with the
use of monolingual parses, e.g. whether the mono-
lingual parser is robust enough to produce reliable
output for every sentence in training data.
At a glance, our reordering model enumerates
the function words on both source and target sides,
modeling their reordering relative to their neighbor-
ing phrases, their neighboring function words, and
the sentence boundaries. Because the frequency of
function words is high, we find that by predicting the
reordering of function words accurately, the reorder-
ing of the remaining words improves in accuracy as
well. In total, we introduce six sub-models involving
function words, and these serve as features in a log
linear model. We train model weights discrimina-
tively using Minimum Error Rate Training (MERT)
(Och, 2003), optimizing F-measure.
The parameters of our sub-models are estimated
from manually-aligned corpora, leading the reorder-
ing model more directly toward reproducing human
alignments, rather than maximizing the likelihood
of unaligned training data. This use of manual data
for parameter estimation is a reasonable choice be-
cause these models depend on a small, fixed number
of lexical items that occur frequently in language,
hence only small training corpora are required. In
addition, the availability of manually-aligned cor-
pora has been growing steadily.
The remainder of the paper proceeds as follows.
In Section 2, we provide empirical motivation for
our approach. In Section 3, we discuss six sub-
models based on function word relationships and
how their parameters are estimated; these are com-
????
?
??
?
?
?
???
one
of
few
that
have
dipl. rels.
with
countries
Australia
is
?
??
North Korea
the
1
1 2 3 4 5 6 7 8 9 10 11
2
3
4
5
6
7
8
9
10
11
12
Figure 1: An aligned Chinese-English sentence pair.
bined with additional features in Section 4 to pro-
duce a single discriminative alignment model. Sec-
tion 5 describes a simple decoding algorithm to find
the most probable alignment under the combined
model, Section 6 describes the training of our dis-
criminative model and Section 7 presents experi-
mental results for the model using this algorithm.
We wrap up in Sections 8 and 9 with a discussion
of related work and a summary of our conclusions.
2 Empirical Motivation
Fig. 1 shows an example of a Chinese-English sen-
tence pair together with correct alignment points.
Predicting the alignment for this particular Chinese-
English sentence pair is challenging, since the sig-
nificantly different syntactic structures of these two
languages lead to non-monotone reordering. For ex-
ample, an accurate alignment model should account
for the fact that prepositional phrases in Chinese ap-
pear in a different order than in English, as illus-
trated by the movement of the phrase ????/with
North Korea? from the beginning of the Chinese
noun phrase to the end of the corresponding English.
The central question that concerns us here is how
to define and infer regularities that can be useful
to predict alignment reorderings. The approach we
take here is supported by empirical results from a
pilot study, conducted as an inquiry into the idea of
focusing on function words to model alignment re-
ordering, which we briefly describe.
We took a Chinese-English manually-aligned cor-
pus of approximately 21 thousand sentence pairs,
535
?
??
?
?
?
?
?
? ?
one
of
few
that
have
dipl. rels.
with
countries
Australia
is
?
?
North Korea
the
1
1 2 3 4 5 6 7 8 9 10 11
2
3
4
5
6
7
8
9
10
11
12
Figure 2: The all-monotone phrase pairs, indicated as
rectangular areas in bold, that can be extracted from the
Fig. 1 example.
and divided each sentence pair into all-monotone
phrase pairs. Visually, an all-monotone phrase pair
corresponds to a maximal block in the alignment
matrix for which internal alignment points appear
in monotone order from the top-left corner to the
bottom-right corner. Fig. 2 illustrates seven such
pairs that can be extracted from the example in
Fig. 1. In total, there are 154,517 such phrase pairs
in our manually-aligned corpus.
The alignment configuration internal to all-
monotone phrase pair blocks is, obviously, mono-
tonic, which is a configuration that is effectively
modeled by traditional alignments models. On the
other hand, the reordering between two adjacent
blocks is the focus of our efforts since existing mod-
els are less effective at modeling non-monotonic
alignment configurations. To measure the function
words? potential to predict non-monotone reorder-
ings, we examined the border words where two ad-
jacent blocks meet. In particular, we are interested
in how many adjacent blocks whose border words
are function words.
The results of this pilot study were quite encour-
aging. If we consider only the Chinese side of the
phrase pairs, 88.35% adjacent blocks have function
words as their boundary words. If we consider only
the English side, function words appear at the bor-
ders of 93.91% adjacent blocks. If we consider
both the Chinese and English sides, the percentage
increases to 95.53%. Notice that in Fig. 2, func-
tion words appear at the borders of all adjacent all-
monotone phrase pairs, if both Chinese and English
sides are considered. Clearly with such high cov-
erage, function words are central in predicting non-
monotone reordering in alignment.
3 Reordering with Function Words
The reordering models we describe follow our previ-
ous work using function word models for translation
(Setiawan et al, 2007; Setiawan et al, 2009). The
core hypothesis in this work is that function words
provide robust clues to the reordering patterns of the
phrases surrounding them. To make this insight use-
ful for alignment, we develop features that score the
alignment configuration of the neighboring phrases
of a function word (which functions as an anchor)
using two kinds of information: 1) the relative order-
ing of the phrases with respect to the function word
anchor; and 2) the span of the phrases. This sec-
tion provides a high level overview of our reordering
model, which attempts to leverage this information.
To facilitate subsequent discussions, we introduce
the notion of monolingual function word phrase
FWi, which consists of the tuple (Yi, Li, Ri), where
Yi is the i-th function word and Li,Ri are its left and
right neighboring phrases, respectively. Note that
this notion of ?phrase? is defined only for reorder-
ing purposes in our model, and does not necessar-
ily correspond to a linguistic phrase. We define
such phrases on both sides to cover as many non-
monotone reorderings as possible, as suggested by
the pilot study. To denote the side, we append a sub-
script: FWi,S = (Yi,S , Li,S , Ri,S) refers to a func-
tion word phrase on the source side, and FWi,T =
(Yi,T , Li,T , Ri,T ) to one on the target side. In our
subsequent discussion, we will mainly use FWi,S ,
and we will omit subscripts S or T if they are clear
from context.
The primary objective of our reordering model
is to predict the projection of monolingual func-
tion word phrases from one language to the
other, inferring bilingual function word phrase pairs
FWi,S?T = (Yi,S?T , Li,S?T , Ri,S?T ), which en-
code the two aforementioned pieces of informa-
tion.1 To infer these phrases, we take a probabilis-
1The subscript S ? T denotes the projection direction from
source to target. The subscript for the other direction is T ? S.
536
tic approach. For instance, to estimate the spans of
Li,S?T , Ri,S?T , our reordering model assumes that
any span to the left of Yi,S is a possible Li,S and
any span to the right of Yi,S is a possible Ri,S , de-
ciding which is most probable via features, rather
than committing to particular spans (e.g. as defined
by a monolingual text chunker or parser). We only
enforce one criterion on Li,S?T and Ri,S?T : they
have to be the maximal alignment blocks satisfying
the consistent heuristic (Och and Ney, 2004) that end
or start with Yi,S?T on the source S side respec-
tively.2
To infer these phrases, we decompose Li,S?T
into (o(Li,S?T ), d(FWi?1,S?T ), b(?s?)); sim-
ilarly, Ri,S?T into (o(Ri,S?T ),d(FWi+1,S?T ),
b(?/s?) )). Taking the decomposition of Li,S?T as
a case in point, here o(Li,S?T ) describes the re-
ordering of the left neighbor Li,S?T with respect
to the function word Yi,S?T , while d(FWi?1,S?T )
and b(?s?)) probe the span of Li,S?T , i.e. whether
it goes beyond the preceding function word phrase
pairs FWi?1,S?T and up to the beginning-of-
sentence marker ?s? respectively. The same defini-
tion applies to the decomposition of Ri,S?T , where
FWi+1,S?T is the succeeding function word phrase
pair and ?/s? is the end-of-sentence marker.
3.1 Six (Sub-)Models
To model o(Li,S?T ), o(Ri,S?T ), i.e. the re-
ordering of the neighboring phrases of a func-
tion word, we employ the orientation model in-
troduced by Setiawan et al (2007). Formally,
this model takes the form of probability distribution
Pori(o(Li,S?T ), o(Ri,S?T )|Yi,S?T ), which condi-
tions the reordering on the lexical identity of the
function word alignment (but independent of the lex-
ical identity of its neighboring phrases). In particu-
lar, o maps the reordering into one of the following
four orientation values (borrowed from Nagata et al
(2006)) with respect to the function word: Mono-
tone Adjacent (MA), Monotone Gap (MG), Reverse
Adjacent (RA) and Reverse Gap (RG). The Mono-
tone/Reverse distinction indicates whether the pro-
jected order follows the original order, while the
Adjacent/Gap distinction indicates whether the pro-
2This heuristic is commonly used in learning phrase pairs
from parallel text. The maximality ensures the uniqueness of L
and R.
jections of the function word and the neighboring
phrase are adjacent or separated by an intervening
phrase.
To model d(FWi?1,S?T ), d(FWi+1,S?T ), i.e.
whether Li,S?T and Ri,S?T extend beyond the
neighboring function word phrase pairs, we uti-
lize the pairwise dominance model of Setiawan
et al (2009). Taking d(FWi?1,S?T ) as
a case in point, this model takes the form
Pdom(d(FWi?1,S?T )|Yi?1,S?T , Yi,S?T ), where d
takes one of the following four dominance val-
ues: leftFirst, rightFirst, dontCare, or neither.
We will detail the exact formulation of these val-
ues in the next subsection. However, to provide
intuition, the value of either leftFirst or neither
for d(FWi?1,S?T ) would suggest that the span of
Li,S?T doesn?t extend to Yi?1,S?T ; the further dis-
tinction between leftFirst and neither concerns with
whether the span of Ri?1,S?T extends to FWi,S?T .
To model b(?s?), b(?/s?), i.e. whether the span of
Li,S?T and Ri,S?T extends up to sentence mark-
ers, we introduce the borderwise dominance model.
Formally, this model is similar to the pairwise domi-
nance model, except that we use the sentence bound-
aries as the anchors instead of the neighboring
phrase pairs. This model captures longer distance
dependencies compared to the previous two mod-
els; in the Chinese-English case, in particular, it is
useful to discourage word alignments from crossing
clause or sentence boundaries. The sentence bound-
ary issue is especially important in machine trans-
lation (MT) experimentation, since the Chinese side
of English-Chinese parallel text often includes long
sentences that are composed of several independent
clauses joined together; in such cases, words from
one clause should be discouraged from aligning to
words from other clauses. In Fig. 1, this model is
potentially useful to discourage words from cross-
ing the copula ??/is?.
We define each model for all (pairs of) function
word phrase pairs, forming features over a set of
word alignments (A) between source (S) and target
537
(T ) sentence pair, as follows:
fori =
N
?
i=1
Pori(o(Li), o(Ri)|Yi) (1)
fdom =
N
?
i=2
Pdom(d(FWi?1)|Yi?1, Yi) (2)
fbdom =
N
?
i=1
Pbdom(b (?s?)|?s?, Yi) ?
Pbdom(b (?/s?)|Yi, ?/s?) (3)
where N is the number of function words (of the
source side, in the S ? T case). As the bilingual
function word phrase pairs are uni-directional, we
employ these three models in both directions, i.e.
T ? S as well as S ? T . As a result, there are
six reordering models based on function words.
3.2 Prediction and Parameter Estimation
Given FWi?1,S?T (and all other FW?i?/i,S?T ),
our reordering model has to decompose Li,S?T into
(o(Li,S?T ), d(FWi?1,S?T ), b(?s?)); and Ri,S?T
into (o(Ri,S?T ),d(FWi+1,S?T ), b(?/s?) )) during
prediction and parameter estimation. In prediction
mode (described in Section 5), it has to make the de-
composition on the current state of alignment, while
during parameter estimation, it has to make the
same decomposition on the manually-aligned cor-
pora. Since the process is identical, we proceed with
the discussion in the context of parameter estima-
tion, where the decomposition is performed to col-
lect counts to estimate the parameters of our models.
Orientation model. Using Li,S?T as a case in
point and given (Yi,S?T =sll/tmm, Li,S?T =sl2l1/tm2m1 ,
Ri,S?T =s
l4
l3
/tm4m3)
3
, the value of o(Li,S?T ) in terms
of Monotone/Reverse is:
Monotone/Reverse =
{
M, m2 < m,
R, m < m1.
(4)
while its value in terms of Adjacent/Gap values is:
Adjacent/Gap =
{
A, |m ? m1| ? |m ? m2| = 1,
G, otherwise.
(5)
3We use subscripts to indicate the starting index, and super-
scripts the ending index.
By adjusting the indices, the computation of
o(Ri,S?T ) follows similarly to the procedure above.
Suppose we want to estimate the probability of
Li,S?T =MA for a particular Yi. Note that here, we
are interested in the lexical identity of Yi, thus the
index i is irrelevant. We first gather the counts of the
orientation value for all Li,S?T of Yi in the corpus:
c(o(Li,S?T ) ? {MA, RA, MG, RG}, Yi). Then
Pori(MA|Yi) is estimated as follows:
Pori(MA|Yi) =
c(MA, Yi)
c(Yi)
(6)
where c(Yi) is the frequency of Yi in the corpus. The
estimation of other orientation values as well as the
T ? S version of the model, follows the same pro-
cedure.
Pairwise and Borderwise dominance models.
Given Ri,S?T = sl2l1/t
m2
m1 and Li+1,S?T =
sl4l3/t
m4
m3 , i.e. the spans of the neighbors of a
pair of neighboring function word phrase pairs
(Yi = sl5l5/tm5m5 , Yi+1 = s
l6
l6
/tm6m6), the value of
d(FWi+1,S?T ) is:
=
?
?
?
?
?
?
?
?
?
?
?
leftFirst, l2 ? l6
?
l3 > l5
rightFirst, l2 < l6
?
l3 ? l5
dontCare, l2 ? l6
?
l3 ? l5
neither, l2 < l6
?
l3 > l5
(7)
Note that the neighbors of the sentence markers for
the borderwise models span the whole sentence, thus
value of neither is impossible for these models.
Suppose we want to estimate the probability of Yi
and Yi+1 having a dontCare dominance value. Note
that here we are interested in the lexical identity of
Yi and Yi+1, thus the models are insensitive to the in-
dices. We first gather the counts of the Yi and Yi+1
having the dontCare value c(dontCare, Yi, Yi+1);
then Pdom(dontCare|Yi, Yi+1) is estimated as fol-
lows:
Pdom(dontCare|Yi, Yi+1) =
c(dontCare, Yi, Yi+1)
c(Yi, Yi+1)
(8)
where c(Yi, Yi+1) is the count of Yi appears after
Yi+1 in the training corpus without any other func-
tion word comes in between.
538
4 Alignment Model
To use the function word alignment features de-
scribed in the previous section to predict alignments,
we use a linear model of the following form:
A? = arg max
A?A(S,T )
? ? f(A, S, T ) (9)
where A(S, T ) is the set of all possible alignments
of a source sentence S and target sentence T , and
f(A, S, T ) is a vector of feature functions on A, S,
and T , and ? is a parameter vector.
In addition to the six reordering models, our
model employs several association-based scores that
look at alignments in isolation. These features in-
clude:
1. Normalized log-likelihood ratio (LLR). This
feature represents an association score, derived from
statistical testing statistics. LLR (Dunning, 1993)
has been widely used especially to measure lexical
association. Since the values of LLR are unnormal-
ized, we normalize them on a per-sentence basis, so
that the normalized LLRs of, say, a particular source
word to the target words in a particular sentence sum
up to one.
2. Translation table from IBM model 4. This
feature represents another association score, derived
from a generative model, in particular the word-
based IBM model 4. The use of this feature is
widespread in recent alignment models, since it pro-
vides a relatively accurate initial prediction.
3. Translation table from manually-aligned
corpora. This feature represents a gold-standard as-
sociation score, based on human annotation. While
attractive, this feature suffers from data sparse-
ness issues since the lexical coverage of manually-
aligned corpora, especially over content words, is
very low. To overcome this issue, we design this
feature to have two levels of granularity; as such, a
fine-grained one is applied for function words and
the coarse-grained one for content words.
4. Grow-diag-final alignments bonus. This fea-
ture encourages our alignment model to reuse align-
ment points that are part of the alignments created
by the grow-diag-final heuristic, which we used as
the baseline of our machine translation experiments.
5. Fertility model from IBM model 4. This fea-
ture, which is another by-product of IBM model 4,
measures the probability of a certain word aligning
to zero, one, or two or more words.
6. Null-alignment probability. This bino-
mial feature models preference towards not aligning
words, i.e. aligning to the NULL token. The intu-
ition is to penalize NULL alignments depending on
word class, by assigning lower probability mass to
unaligned content words than to unaligned function
words. In our experiment, we assign feature value
10?3 for a function word aligning to NULL, and
10?5 for a content word aligning to NULL.
Note that with the exception of the alignment
bonus feature (4), all features are uni-directional,
and therefore we employ these features in both di-
rections just as was done for the reordering models.
5 Search
To find A? using the model in Eq. 9, it is neces-
sary to search 2|S|?|T | different alignment config-
urations, and, because of the non-local dependen-
cies in some of our features, it is not possible to use
dynamic programming to perform this search effi-
ciently. We therefore employ an approximate search
for the best alignment. We use a local search pro-
cedure which starts from some alignment (in our
case, a symmetrized Model 4 alignment) and make
local changes to it. Rather than taking a pure hill-
climbing approach which greedily moves to locally
better configurations (Brown et al, 1993), we use
a stochastic search procedure which can move into
lower-scoring states with some probability, similar
to the Monte Carlo techniques used to draw sam-
ples from analytically intractable probability distri-
butions.
5.1 Algorithm
To find A?, our search algorithm starts with an initial
alignment A(1) and iteratively draws a new set by
making a few small changes to the current set. For
each step i = [1, n], with alignment A(i), a set of
neighboring alignments N (A(i)) is induced by ap-
plying small transformations (discussed below) to
the current alignment. The next alignment A(i+1)
539
is sampled from the following distribution:
p(A(i+1)|S, T, A(i)) = exp? ? f(A
(i+1), S, T )
Z(A(i), S, T )
where Z(A(i), S, T ) =
?
A??N (A(i))
exp? ? f(A?, S, T )
In addition to the current ?active? alignment configu-
ration A(i), the algorithm keeps track of the highest
scoring alignment observed so far, Amax. After n
steps, the algorithm returns Amax as its approxima-
tion of A?. In the experiments reported below, we
initialized A(1) with the Model 4 alignments sym-
metrized by using the grow-diag-final-and heuristic
(Koehn et al, 2003).
5.2 Alignment Neighborhoods
We now turn to a discussion of how the alignment
neighborhoods used by our stochastic search algo-
rithm are generated. We define three local transfor-
mation operations that apply to single columns of
the alignment grid (which represent all of the align-
ments to the lth source word), rows, or existing align-
ment points (l, m). Our three neighborhood gener-
ating operators are ALIGN, ALIGNEXCLUSIVE, and
SWAP. The ALIGN operator applies to the lth col-
umn of A and can either add an alignment point
(l, m?) or move an existing one (including to null,
thus deleting it). ALIGNEXCLUSIVE adds an align-
ment point (l, m) and deletes all other points from
row m. Finally, the SWAP operator swaps (l, m) and
(l?, m?), resulting in new alignment points (l, m?)
and (l?, m). We increase the decoder?s mobility
by traversing the target side and applying the same
steps above for each target word. Fig. 3 illustrates
the three operators. By iterating over all columns l
and rows m, the full alignment space A(S, T ) can
be explored.4
To further reduce the search space, an alignment
point (l, m) is only admitted into a neighborhood if
it is found in the high-recall alignment set R(S, T ),
which we define to be the model 4 union alignments
(bidirectional model 4 symmetrized via union) plus
the 5 best alignments according to the log-likelihood
ratio.
4Using only the ALIGN operator, it is possible to explore
the full alignment space; however, using all three operators in-
creases mobility.
(a)
(b)
(c)
l l' l l'
m
m'
m
m'
m
m'
Figure 3: Illustrations for (a) ALIGN, (b) ALIGNEXCLU-
SIVE, and (c) SWAP operators, as applied to align the dot-
ted, smaller circle (l,m) to (l,m?). The left hand side rep-
resents A(i), while the right hand side represents a can-
didate for A(i+1). The solid circles represent the new
alignment points added to A(i+1).
6 Discriminative Training
To set the model parameters ?, we used the min-
imum error rate training (MERT) algorithm (Och,
2003) to maximize the F-measure of the 1-best
alignment of the model on a development set con-
sisting of sentence pairs with manually generated
alignments. The candidate set used by MERT to ap-
proximate the model is simply the set of alignments
{A(1), A(2), . . . , A(n)} encountered in the stochastic
search.
While MERT does not scale to large numbers of
features, the scarcity of manually aligned training
data also means that models with large numbers of
sparse features would be difficult to learn discrimi-
natively, so this limitation is somewhat inherent in
the problem space. Additionally, MERT has sev-
eral advantages that make it particularly useful for
our task. First, we can optimize F-measure of the
alignments directly, which has been shown to corre-
late with translation quality in a downstream system
(Fraser and Marcu, 2007b). Second, we are opti-
mizing the quality of the 1-best alignments under the
model. Since translation pipelines typically use only
a single word alignment, this criterion is appropri-
ate. Finally, and very importantly for us, MERT re-
quires only an approximation of the model?s hypoth-
esis space to carry out optimization. Since we are
using a stochastic search, this is crucial, since sub-
540
sequent evaluations of the same sentence pair (even
with the same weights) may result in a different can-
didate set.
Although MERT is a non-probabilistic optimizer,
we explore the alignment space stochastically. This
is necessary to make sure that the weights we use
correspond to a probability distribution that is not
overly peaked (which would result in a greedy hill-
climbing search) or flat (which would explore the
model space without information from the model).
We found that normalizing the weights by the Eu-
clidean norm resulted in a distribution that was well-
balanced between the two extremes.
7 Experiments
We evaluated our proposed alignment model intrin-
sically on an alignment task and extrinsically on a
large-scale translation task, focusing on Chinese-
English as the language pair. Our training data
consists of manually aligned corpora available from
LDC (LDC2006E93 and LDC2008E57) and un-
aligned corpora, which include FBIS, ISI, HKNews
and Xinhua. In total, the manually aligned corpora
consist of more than 21 thousand sentence pairs,
while the unaligned corpora consist of more than
710 thousand sentence pairs. The manually-aligned
corpora are primarily used for training the reorder-
ing models and for discriminative training purposes.
For translation experiments, we used cdec (Dyer
et al, 2010), a fast implementation of hierarchi-
cal phrase-based translation models (Chiang, 2005),
which represents a state-of-the-art translation sys-
tem.
We constructed the list of function words in En-
glish manually and in Chinese from (Howard, 2002).
Punctuation marks were added to the list, result-
ing in 883 and 359 tokens in the Chinese and En-
glish lists, respectively. For the alignment experi-
ments, we took the first 500 sentence pairs from the
newswire genre of the manually-aligned corpora and
used the first 250 sentences as the development set,
with the remaining 250 as the test set. To ensure
blind experimentation, we excluded these sentence
pairs from the training of the features, including the
reordering models.
7.1 Alignment Quality
We used GIZA++, the implementation of the de-
facto standard IBM alignment model, as our base-
line alignment model. In particular, we used
GIZA++ to align the concatenation of the develop-
ment set, the test set, and the unaligned corpora, with
5, 5, 3 and 3 iterations of model 1, HMM, model
3, and model 4 respectively. Since the IBM model
is asymmetric, we followed the standard practice of
running GIZA++ twice, once in each direction, and
combining the resulting outputs heuristically. We
chose to use the grow-diag-final-and heuristic as it
worked well for hierarchical phrase-based transla-
tion in our early experiments. We recorded the align-
ment quality of the test set as our baseline perfor-
mance.
For our alignment model, we used the same set of
training data. To align the test set, we first tuned
the weights of the features in our discriminative
alignment model using minimum error rate training
(MERT) (Och, 2003) with F?=0.1 as the optimiza-
tion criterion. At each iteration, our aligner outputs
k-best alignments under current set of weights, from
which MERT proceeds to compute the next set of
weights. MERT terminates once the improvement
over the previous iteration is lower than a predefined
value. Once tuned, we ran our aligner on the test set
and measured the quality of the resulting alignment
as the performance of our model.
Model P R F0.5 F0.1
gdfa 70.97 63.83 67.21 64.48
association 73.70 76.85 75.24 76.52
+ori 74.09 78.29 76.13 77.85
+dom 75.06 78.98 76.97 78.57
+bdom 75.41 80.53 77.89 79.99
Table 1: Alignment quality results (F0.1) for our discrim-
inative reordering models with various features (lines 2-
5) versus the baseline IBM word-based Model 4 sym-
metrized using the grow-diag-final-and heuristic. The
balanced F0.5 measure is reported for reference. The best
scores are bolded.
Table 1 reports the results of our experiments,
which are conducted in an incremental fashion pri-
marily to highlight the role of reordering model-
ing. The first line (gdfa) reports the baseline perfor-
541
mance. In the first experiment (association), we em-
ployed only the association-based features described
in Section 4. As shown, we obtain a significant im-
provement over baseline. This result is consistent
with recent literature (Fraser and Marcu, 2007a) that
shows that a discriminatively trained model outper-
forms baseline unsupervised models like GIZA++.
In the second set of experiments, we added the re-
ordering models into our discriminative model one
by one, starting with the orientation models, then
the pairwise dominance model and finally the bor-
derwise dominance model, reported in lines +ori,
+dom and +bdom respectively. As shown, each ad-
ditional reordering model provides a significant ad-
ditional improvement. The best result is obtained by
employing all reordering models. These results em-
pirically confirm our hypothesis that we can improve
alignment quality by employing reordering models
that capture non-local reordering phenomena.
7.2 Translation Quality
For translation experiments, we used the products
from our intrinsic experiments to learn translation
rules for the hierarchical phrase-based decoder, i.e.
the features weights of the +bdom experiment to
align the MT training data using our discriminative
model. For our translation model, we used the stan-
dard features based on the relative frequency counts,
including a 5-gram language model feature trained
on the English portion of the whole training data
plus portions of the Gigaword v2 corpus. Specif-
ically, we tuned the weights of these features via
MERT on the NIST MT06 set and we report the re-
sult on the NIST MT02, MT03, MT04 and MT05
sets.
MT02 MT03 MT04 MT05
gdfa 25.61 32.05 31.80 29.34
this work 26.56 33.79 32.61 30.47
Table 2: The translation performance (BLEU) of hierar-
chical phrase-based translation trained on training data
aligned by IBM model 4 symmetrized with the grow-
diag-final-and heuristic, versus being trained on align-
ments by our discriminative alignment model. Bolded
scores indicate that the improvement is statistically sig-
nificant.
Table 2 shows the result of our translation exper-
iments. In our alignment model, we employed the
whole set of reordering models, i.e. the one reported
in the +bdom line in Table 1. As shown, our dis-
criminative alignment model produces a consistent
and significant improvement over the baseline IBM
model 4 (p < 0.01), ranging between 0.81 and 1.71
BLEU points.
8 Related Work
The focus of our work is to strengthen the reordering
component of alignment modeling. Although the de
facto standard, the IBM models do not generalize
well in practice: the IBM approach employs a series
of reordering models based on the word?s position,
but reordering depends on syntactic context rather
than absolute position in the sentence. Over the
years, there have been many proposals to improve
these reordering models, most notably Vogel et al
(1996), which adds a first-order dependency. Never-
theless, the use of these distortion-based models re-
mains widespread (Marcu and Wong, 2002; Moore,
2004).
Alignment modeling is challenging because it
often has to consider a prohibitively large align-
ment space. Efforts to constrain the space gen-
erally comes from the use of Inversion Transduc-
tion Grammar (ITG) (Wu, 1997). Recent propos-
als that use ITG constraints include (Haghighi et
al., 2009; Blunsom et al, 2009) just to name a few.
More recent models have begun to use linguistically-
motivated constraints, often in combination with
ITG, primarily exploiting monolingual syntactic in-
formation (Burkett et al, 2010; Pauls et al, 2010).
Our reordering model is closely related to the
model proposed by Zhang and Gildea (2005; 2006;
2007a), with respect to conditioning the reordering
predictions on lexical items. These related models
treat their lexical items as latent variables to be es-
timated from training data, while our model uses
a fixed set of lexical items that correspond to the
class of function words. With respect to the focus
on function words, our reordering model is closely
related to the UALIGN system (Hermjakob, 2009).
However, UALIGN uses deep syntactic analysis and
hand-crafted heuristics in its model.
542
9 Conclusions
Languages exhibit regularities of word order that
are preserved when projected to another language.
We use the notion of function words to infer such
regularities, resulting in several reordering models
that are employed as features in a discriminative
alignment model. In particular, our models pre-
dict the reordering of function words by looking
at their dependencies with respect to their neigh-
boring phrases, their neighboring function words,
and the sentence boundaries. By capturing such
long-distance dependencies, our proposed align-
ment model contributes to the effort to unify align-
ment and translation. Our experiments demonstrate
that our alignment approach achieves both its intrin-
sic and extrinsic goals.
Acknowledgements
This research was supported in part by the GALE
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-06-2-001. Any
opinions, findings, conclusions or recommendations
expressed in this paper are those of the authors and
do not necessarily reflect the view of the sponsors.
References
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In ACL, pages 782?790, Sun-
tec, Singapore, August. Association for Computational
Linguistics.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchronized
grammars. In HLT-NAACL, pages 127?135, Los An-
geles, California, June. Association for Computational
Linguistics.
Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discriminative
training. In COLING/ACL, pages 105?112, Sydney,
Australia, July. Association for Computational Lin-
guistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In ACL, pages
263?270, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In ACL,
pages 17?24, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74, March.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In ACL, Up-
psala, Sweden.
Alexander Fraser and Daniel Marcu. 2007a. Getting
the structure right for word alignment: LEAF. In
EMNLP-CoNLL, pages 51?60, Prague, Czech Repub-
lic, June. Association for Computational Linguistics.
Alexander Fraser and Daniel Marcu. 2007b. Measuring
word alignment quality for statistical machine transla-
tion. Computational Linguistics, 33(3):293?303.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
itg models. In ACL, pages 923?931, Suntec, Singa-
pore, August. Association for Computational Linguis-
tics.
Ulf Hermjakob. 2009. Improved word alignment with
statistics and linguistic heuristics. In EMNLP, pages
229?237, Singapore, August. Association for Compu-
tational Linguistics.
Jiaying Howard. 2002. A Student Handbook for Chinese
Function Words. The Chinese University Press.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HTL-NAACL,
pages 127?133, Edmonton, Alberta, Canada, May. As-
sociation for Computational Linguistics.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In EMNLP, July 23.
Robert C. Moore. 2004. Improving ibm word alignment
model 1. In ACL, pages 518?525, Barcelona, Spain,
July.
Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,
and Kazuteru Ohashi. 2006. A clustered global phrase
reordering model for statistical machine translation. In
ACL, pages 713?720, Sydney, Australia, July. Associ-
ation for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL, pages 160?
167.
Jamal Ouhalla. 1991. Functional Categories and Para-
metric Variation. Routledge.
543
Adam Pauls, Dan Klein, David Chiang, and Kevin
Knight. 2010. Unsupervised syntactic alignment with
inversion transduction grammars. In HLT-NAACL,
pages 118?126, Los Angeles, California, June. Asso-
ciation for Computational Linguistics.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li. 2007.
Ordering phrases with function words. In ACL, pages
712?719, Prague, Czech Republic, June. Association
for Computational Linguistics.
Hendra Setiawan, Min Yen Kan, Haizhou Li, and Philip
Resnik. 2009. Topological ordering of function
words in hierarchical phrase-based translation. In
ACL, pages 324?332, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In COLING, pages 836?841, Copenhagen.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404, Sep.
Hao Zhang and Daniel Gildea. 2005. Stochastic lexical-
ized inversion transduction grammar for alignment. In
ACL. The Association for Computer Linguistics.
Hao Zhang and Daniel Gildea. 2006. Inducing word
alignments with bilexical synchronous trees. In ACL.
The Association for Computer Linguistics.
544
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 501?512,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Anchor Graph:
Global Reordering Contexts for Statistical Machine Translation
Hendra Setiawan ?
IBM Research
1101 Kitchawan Road
NY 10598, USA
Bowen Zhou
IBM Research
1101 Kitchawan Road
NY 10598, USA
Bing Xiang ?
Thomson Reuters
3 Times Square
NY 10036, USA
Abstract
Reordering poses one of the greatest chal-
lenges in Statistical Machine Translation re-
search as the key contextual information may
well be beyond the confine of translation units.
We present the ?Anchor Graph? (AG) model
where we use a graph structure to model
global contextual information that is crucial
for reordering. The key ingredient of our AG
model is the edges that capture the relation-
ship between the reordering around a set of
selected translation units, which we refer to as
anchors. As the edges link anchors that may
span multiple translation units at decoding
time, our AG model effectively encodes global
contextual information that is previously ab-
sent. We integrate our proposed model into a
state-of-the-art translation system and demon-
strate the efficacy of our proposal in a large-
scale Chinese-to-English translation task.
1 Introduction
Reordering remains one of the greatest challenges
in Statistical Machine Translation (SMT) research as
the key contextual information may span across mul-
tiple translation units.1 Unfortunately, previous ap-
proaches fall short in capturing such cross-unit con-
textual information that could be critical in reorder-
ing. For example, state-of-the-art translation mod-
els, such as Hiero (Chiang, 2005) or Moses (Koehn
et al, 2007), are good at capturing local reordering
within the confine of a translation unit, but their for-
mulation is approximately a simple unigram model
? This work was done when the authors were with IBM.
1We define translation units as phrases in phrase-based SMT
or as translation rules in syntax-based SMT.
over derivation (a sequence of the application of
translation units) with some aid from target language
models. Moving to a higher order formulation (say
to a bigram model) is highly impractical for several
reasons: 1) it has to deal with a severe sparsity issue
as the size of the unigram model is already huge;
and 2) it has to deal with a spurious ambiguity issue
which allows multiple derivations of a sentence pair
to have radically different model scores.
In this paper, we develop ?Anchor Graph? (AG)
where we use a graph structure to capture global
contexts that are crucial for translation. To circum-
vent the sparsity issue, we design our model to rely
only on contexts from a set of selected translation
units, particularly those that appear frequently with
important reordering patterns. We refer to the units
in this special set as anchors where they act as ver-
tices in the graph. To address the spurious ambigu-
ity issue, we insist on computing the model score for
every anchors in the derivation, including those that
appear inside larger translation units, as such our AG
model gives the same score to the derivations that
share the same reordering pattern.
In AG model, the actual reordering is modeled
by the edges, or more specifically, by the edges? la-
bels where different reordering around the anchors
would correspond to a different label. As detailed
later, we consider two distinct set of labels, namely
dominance and precedence, reflecting the two domi-
nant views about reordering in literature, i.e. the first
one that views reordering as a linear operation over
a sequence and the second one that views reordering
as a recursive operation over nodes in a tree struc-
ture The former is prevalent in phrase-based con-
text, while the latter in hierarchical phrase-based and
501
syntax-based context. More concretely, the domi-
nance looks at the anchors? relative positions in the
translated sentence, while the precedence looks at
the anchors? relative positions in a latent structure,
induced via a novel synchronous grammar: Anchor-
centric, Lexicalized Synchronous Grammar.
From these two sets of labels, we develop two
probabilistic models, namely the dominance and the
orientation models. As the edges of AG link pairs
of anchors that may appear in multiple translation
units, our AG models are able to capture high or-
der contextual information that is previously absent.
Furthermore, the parameters of these models are es-
timated in an unsupervised manner without linguis-
tic supervision. More importantly, our experimental
results demonstrate the efficacy of our proposed AG-
based models, which we integrate into a state-of-the-
art syntax-based translation system, in a large scale
Chinese-to-English translation task. We would like
to emphasize that although we use a syntax-based
translation system in our experiments, in principle,
our approach is applicable to other translation mod-
els as it is agnostic to the translation units.
2 Anchor Graph Model
Formally, an AG consists of {A,L} where A is a
set of vertices that correspond to anchors, while L
is a set of labeled edges that link a pair of anchors.
In principle, our AG model is part of a transla-
tion model that focuses on the reordering within the
source sentence F and its translation E. Thus, we
start by first introducing A into a translation model
(either word-based, phrase-based or syntax-based
model) followed by L. Given an F , A is essentially
a subset of non-overlapping (word or phrase) units
that make up F . As the information related to A is
not observed, we introduce A as a latent variable.
Let P (E,? |F ) be a translation model where ?
corresponds to the alignments between units in F
and E. 2 We introduce A into a translation model,
2Alignment (?) represents an existing latent variable. De-
pending on the translation units, it can be defined at different
level, i.e. word, phrase or hierarchical phrase. As during trans-
lation, we are interested in the anchors that appear inside larger
translation units, we set ? at word level, which information can
be induced for (hierarchical) phrase units by either keeping the
word alignment from the training data inside the units or infer-
ring it via lexical translation probability. We use the former.
as follow:
P (E,? |F ) =
?
?A?
P (E,?,A?|F ) (1)
P (E,?,A?|F ) = P (E,? |A?, F )P (A?) (2)
As there can be many possible subsets of F and
summing over all possibleA is intractable, we make
the following approximation for P (A?) such that we
only need to consider one particular A?: P (A?) =
?(A? = A?) which returns 1 only for A?, otherwise
0. The exact definition of the heuristic will be de-
scribed in Section 7, but in short, we equateA? with
units that appear frequently with important reorder-
ing patterns in training data.
Given an A?, we then introduce the edges of AG
(L) into the equation as follow:
P (E,? |A?, F ) = P (E,?,L|A?, F ) (3)
Note that L is also a latent variable but its values are
derived deterministically from (F,E,?) and A?,
thus no extra summation is present in Eq. 3.
Then, we further simplify Eq. 3 by factorizing it
with respect to each individual edges, as follow:
P (E,?,L|A?, F ) ?
?
?am,an?A?
m<n
P (Lm,n|am, an) (4)
where Lm,n ? L corresponds to the label of an edge
that links am and an.
In principle, Lm,n can take any arbitrary value.
For addressing the reordering challenge, it should
ideally correspond to some aspect of the reordering
around am and an, for example, how the reorder-
ing around am affects the reordering around an. As
mentioned earlier, we choose to associate Lm,n with
the dominance and the precedence relations between
am and an, where the former looks at the relative po-
sitions of the two anchors when they are projected
into a latent tree structure, while the latter looks at
their relative positions when they are projected into
the target sentence. We illustrate the two in Fig. 1.
Furthermore, we assume that dominance and
precedence are independent and develop one model
for each, resulting in the dominance and the orien-
tation models, which we describe in Section 3 and 4
respectively. To make the model more compact, we
502
introduce an additional parameterO that restricts the
maximum order of AG as follows:
?
O?
o=1
|A?|+o?1?
i=0
Po(Li?o,i|ai?o, ai) (5)
Thus, we only consider edges that link two anchors
that are at most O? 1 anchors apart. For O = 1, the
AG model only considers relations between neigh-
boring anchors. Following the standard practice in
the n-gram language modeling, we append O num-
ber of pseudo anchors at the beginning and at the end
of F , which represent the sentence delimiter mark-
ers. We do so in a monotone order.
Figure 1: The illustration of the dominance and the prece-
dence relations. The former looks at the anchors? pro-
jection on a derivation structure. The latter looks at the
anchors? projection on the translated sentence.
3 Dominance Model
This section describes our dominance model where
we equate Lm,n in Eq. 4 with dom(am, an) that ex-
presses to the dominance relation between am and
an in a latent tree structure. Due to reordering, an-
chors can only appear in specific nodes. We first
describe a novel formalism of Anchor-centric, Lexi-
calized Synchronous Grammar (AL-SG), used to in-
duce the tree structure and then discuss the proba-
bilistic formulation of the model. Just to be clear,
we introduce AL-SG mainly to facilitate the compu-
tation of dom(am, an). The actual translation model
at decoding time remains either phrase-based, hier-
archical phrase-based or syntax-based model.
3.1 Anchor-centric, Lexicalized Synchronous
Grammar
Given (F,E,?) and A, Anchor-centric, Lexical-
ized Synchronous Grammar (AL-SG) produces a
tree structure where the nodes are decorated with
anchors-related information. As the name alludes,
the core of AL-SG is anchor-centric constituents
(ACC), which corresponds to nodes, composed from
merging anchors with by either their left, their right
neighboring constituents or both.
More concretely, first of all, we consider a span
on the source sentence F to be a constituent if it is
consistent with the alignment (?). Second of all, we
can construct a larger constituent by merging smaller
constituents given that the larger constituent is also
consistent with the alignment. These two constraints
are similar to the heuristic applied to extract hierar-
chical phrases (Chiang, 2005).
Then, specific to AL-SG, we consider an anchor a
to lexicalize a constituent c, if: a) we can compose c
from at most three smaller constituents: cL, a and cR
where a is the anchor while cL,cR are the (possibly
empty) constituents immediately to the left and to
the right of a; and b) we can create smaller anchors-
centric constituents from concatenating a with cL
and a with cR. If a can lexicalize c, then the node
associated with c would be marked with a. In com-
puting dom(am, an), we look at the constituents that
cover both anchors and check whether the anchors
can lexicalized any of such constituents.
Now, we will describe AL-SG in a formal way.
For simplicity, we use a simple grammar, called In-
version Transduction Grammar (ITG) (Wu, 1997),
although in practice, we handle a more powerful
synchronous grammar. Hence, we proceed to de-
scribe Anchor-centric, Lexicalized ITG (AL-ITG).
An AL-ITG is a quadruple {?,A,V,R} where:
? ? = {(f/e)} is a set of terminal symbols,
which represents all possible units defined over
(F,E,?) where each pair corresponds to a link
in ?. We define ? at the most fine-grained
level (i.e. word-level), as we insist on comput-
ing model score for each anchors even if they
appear inside larger units.
? A ? ? is a set of anchors, which is a subset of
the terminal symbols.
? V = {{P,X, Y } ? {A, ?}} is a set of (possi-
bly lexicalized) nonterminal symbols. P rep-
resents the terminal symbols (?); while X and
Y correspond to the spans that are created from
merging two adjacent constituents. On the tar-
503
Figure 2: An illustration of an aligned Chinese-English sentence pair with one possible AL-ITG derivation obtained
by applying the grammar in a left-to-right fashion. Circles represent alignment points. Black circle represents the
anchor; boxes represent the anchor?s neighbors. In the derivation tree, the anchors are represented by their position
and in bold. For succinctness, we omit the preterminal rules in the tree.
get side, for X , the order of the two children
follows the source order, while for Y , the or-
der follows the inverse. Nonterminal symbols
can be lexicalized with zero or more than one
anchor. We represent a lexicalized constituent
as a nonterminal symbol followed by a bracket
which contains the lexicalizing anchors, e.g.
P (H) where H is the anchors lexicalizing P .
? R is a set of production rules which can be clas-
sified into the following categories:
? Preterminal rules. We propagate the sym-
bol if it corresponds to an anchor.
P (H = f/e)? f/e, if f/e ? A?
P (H = ?)? f/e, otherwise
? Monotone production rules, which reorder
the children in monotone order, denoted
by square brackets (?[?,?]?).
X(H1 ?H2)? [P (H1)P (H2)]
X(H1 ?H2)? [X(H1)P (H2)]
X(H1 ?H2)? [X(H1)X(H2)]
X(H1)? [X(H1)Y (H2)]
X(H2)? [Y (H1)P (H2)]
X(H2)? [Y (H1)X(H2)]
X(?)? [Y (H1)Y (H2)]
? Inverse production rules, which reorder
the children in the inverse order, denoted
by angle brackets (???,???).
Y (H1 ?H2)? ?P (H1)P (H2)?
Y (H1 ?H2)? ?Y (H1)P (H2)?
Y (H1 ?H2)? ?Y (H1)Y (H2)?
Y (H1)? ?Y (H1)X(H2)?
Y (H2)? ?X(H1)P (H2)?
Y (H2)? ?X(H1)Y (H2)?
Y (?)? ?X(H1)X(H2)?
Like ITG, AL-ITG only permits two kind of re-
ordering operations, namely monotone and inverse.
To accommodate the lexicalization, we first assign
a unique nonterminal symbol for each, i.e. X for
monotone reordering and Y for inverse reordering.
Then, we lexicalize Xs and Y s with anchors as long
as they satisfy the constraint that the child shares the
same label as the parent. This constraint guarantees
that the constituents are valid ACCs. It also enables
the anchors to lexicalize long constituents, although
the terminal symbols are defined at word-level.
Fig. 2 illustrates an example Chinese-to-English
translation with a AL-ITG derivation when the
grammar is applied in a left-to-right fashion. Admit-
tedly, AL-ITG (or more generally AL-SG) is suscep-
tible to spurious ambiguity as it produces multiple
derivation trees for a given (F,E,?). Fortunately,
the value of dom(am, an) is identical for all deriva-
tions, since the computation of dom(am, an) relies
504
only on whether am and an can lexicalize at least
one constituent that covers both anchors. Hence,
we only need to look at one derivation to compute
dom(am, an). Generalizing AL-ITG to a more pow-
erful formalism is trivial; we just need to forbid the
propagation for non-binarizeable production rules.
3.2 Probabilistic Model
We read-off the dominance relations dom(am, an)
from D obtained from the application of AL-SG to
(F,E,?). As lexicalization is a bottom-up process,
for reading-off dom(am, an), it is sufficient to look
at the lowest common ancestor (LCA) of both an-
chors; if the anchors cannot lexicalize the LCA, they
won?t be able to lexicalize the constituents larger
than LCA. To be more concrete, let?s consider theD
in Fig. 2. In that D, the LCA of am = yu3/with10
and an = de7/that7 is Y5(7). Then, we check the
anchors that can lexicalize the LCA. Let V (H) be
the LCA, then dom(am, an) ?
(LH) , if am ? H ? an 6? H
(RH) , if am 6? H ? an ? H
(BL) , if am ? H ? an ? H
(BD) , if am 6? H ? an 6? H
The value refers to cases where am and an can
lexicalize V (H) and it is useful to model spans
that share a simple, uniform reordering, i.e. all-
monotone or all-inverse, while the value refers to
the cases where am and an cannot lexicalize V (H)
and it is useful to model spans that involve in a com-
plex reordering. Meanwhile, the and refer to cases
where only one anchor can lexicalize V (H), i.e. am
and an respectively. These values are useful for
modeling cases where the surroundings of the two
anchors exhibit different kind of reordering pattern.
With such definition, the edge labels L in Fig. 2
are indicated in Table 1. Note that in Table 1, we
don?t specify the relations involving pseudo anchors,
although they are crucial.
The final probabilistic formulation of the domi-
nance model is as follows:
?
O?
o=1
|A|+o?1?
i=0
Pdomo(dom(ai?o, ai)|ai?o, ai) (6)
As shown, we allocate a separate model Pdomo for
each separate order (o) where each Pdomo will con-
HHH
HHn
m
1 2 3 4 5
1 = (shi2/is2) - - - - -
2 = (yu3/with10) LH - - - -
3 = (you5/have8) LH BD - - -
4 = (de7/that7) LH RH RH - -
5 = (zhi10/of4) LH RH RH BL -
Table 1: The dominance relations between pairs of an-
chors according to the derivation in Fig. 2.
tribute as one additional feature in the log-linear
model of the translation model. In allocating a sep-
arate model for each o, we conjecture that different
pair of anchors contributes differently depending on
how far the two anchors are.
4 Orientation Model
In this section, we introduce the orientation model
(ori) where we equate Lm,n with the precedence re-
lations between a pair of anchors. Instead of directly
modeling the precedence between the two anchors,
we approximate it by modeling the precedence of
each anchor with its neighboring constituents. For-
mally, we approximate P (Lm,n|am, an) as
PoriR(ori(am,MR(am))|am)?
PoriL(ori(an,ML(an))|an) (7)
where MR(am) is the largest constituent to the right
of the first anchor am, ML(an) the largest con-
stituent to the left of the second anchor an, and ori()
a function that maps the anchor and the neighboring
constituent to a particular orientation.
Plugging Eq. 7 into Eq. 5 results in the following
approximation of P (?|A):
C.
|A|?1?
i=0
{PoriL(ori(ai,ML(ai))|ai)?
PoriR(ori(ai,MR(ai))|ai)}
O (8)
where C is a constant term related to the pseudo an-
chors and O is the maximum order of the AG. In
practice, we can safely ignore both C and O as they
are constant for a given AG. As shown, the orienta-
tion model is simplified into a model that looks at the
reordering of the anchors? neighboring constituents.
The exact definition of ML and MR will be
discussed in Section 5. Their orientation, i.e.
505
oriL(CL, a) and oriR(CR, a) respectively, may take
one of the following four values: (MA), (RA), (MG)
and (RG). The first clause (monotone, reverse) in-
dicates whether the target order follows the source
order; the second (adjacent, gap) indicates whether
the anchor and its neighboring constituent are adja-
cent or separated by an intervening when projected.
5 Parameter Estimation
For each (F,E,?), the training starts with the iden-
tification of the regions in the source sentences as
anchors (A). For our Chinese-English experiments,
we use a simple heuristic that equates anchors (A?)
with constituents whose corresponding word class
belongs to function words-related classes, bearing
a close resemblance to (Setiawan et al, 2007). In
total, we consider 21 part-of-speech tags; some of
which are as follows: VC (copula), DEG, DEG,
DER, DEV (de-related), PU (punctuation), AD (ad-
jectives) and P (prepositions).
5.1 Extracting Events from (F,E,?)
The parameter estimation first involves extracting
two statistics from (F,E,?), namely dom(am, an)
for the dominance model as well as ori(a,ML(a))
and ori(a,MR(a)) for the orientation model. In-
stead of developing a separate algorithm for each,
we describe a unified way to extract these statistics
via the largest neighboring constituents of the an-
chors, i.e. ML(a) and MR(a). This approach en-
ables the dominance model to share the same resid-
ual state information as the orientation model.3
Let am be an anchor and MR(am) be its largest
neighboring constituent to the right. Let an be
an anchor to the left of am and ML(an) be an?s
largest neighboring constituent to the left. Ac-
cording to AL-SG, we say that am dominates an
if ori(am,MR(am)) ? {MA,RA} and an ?
MR(am). By the same token, we say that an dom-
inates am if ori(an,ML(an)) ? {MA,RA} and
am ? ML(an). The constraints on the orientation
reflect the fact that in AL-SG, anchors can only be
propagated through monotone or inverse production
rules, which correspond to the MA and RA respec-
tively. The fact that we are looking at the largest
3The analogy in an n-gram language model is the first n?1
words of the hypothesis that have incomplete history.
neighboring constituents guarantees that if the other
anchor is outside that constituent, then that other an-
chor is never dominated.
More formally, given an aligned sentence pair
? = (F,E,?), let ?(?) be all possible con-
stituents that can be extracted from ?:4
{(f j2j1/e
i2
i1) :?(j, i) ??: ((j1? j? j2) ? (ii? i? i2))
?(?(j1? j? j2) ? ?(ii? i? i2))
Then, let the anchors A be a subset of ?(?).
Given A ? ?(?), let a = (f j2j1/e
i2
i1) ? A be a par-
ticular anchor. And, let CL(a) ? ?(?) be a?s left
neighbors and let CR(a) ? ?(?) be a?s right neigh-
bors, iff:
?CL = (f
j4
j3/e
i4
i3) ? CL(a) : j4 + 1 = j1
?CR = (f
j6
j5/e
i6
i5) ? CR(a) : j2 + 1 = j5
Then, let ML(a) ? CL(a) and MR(a) ? CR(a) be
the largest left and right neighbors according to:
ML(a) = arg max
(f
j4
j3
/e
i4
i3
)?CL(a)
(j4 ? j3)
MR(a) = arg max
(f
j6
j5
/e
i6
i5
)?CR(a)
(j6 ? j5)
Let ML = (f
j4
j3/e
i4
i3) and MR = (f
j6
j5/e
i6
i5).
We then proceed to extract oriL(a,ML(a)) and
oriR(a,MR(a)) respectively as follows:
? MA, if (i4 +1) = i1 for oriL or if (i2 +1) = i5
for oriR
? RA, if (i2 + 1) = i3 for oriL or if (i6 + 1) = i1
for oriR
? MG, if (i4 +1) < i1 for oriL or if (i2 +1) < i5
for oriR
? RG, if (i2 + 1) < i3 for oriL or if (i6 + 1) < i1
for oriR.
Then, we proceed to extract dom(am, an). Given
two anchors am, an where m < n, we define the
4We represent a constituent as a source and target phrase
pair (f j2j1/e
i2
i1
) where the subscript and the superscript indicate
the starting and the ending indices as such f j2j1 denotes a source
phrase that spans from j1 to j2.
506
dominance relation between am and an viaMR(am)
and ML(an). Let am = (f
j2
j1/e
i2
i1), MR(am) =
(f j4j3/e
i4
i3), an = (f
j6
j5/e
i6
i5) and ML(an) = (f
j8
j7/e
i8
i7).
Then, ldom(am, an) is true only if (j4 ? j6)
and oriR(am,MR(am)) ? {MA,RA}. Simi-
larly, rdom(am, an) is true only if (j7 ? j1) and
oriL(an,ML(an)) ? {MA,RA}.
Hence, dom(am, an) is as follows:
? LH, if ldom(am, an) ? ?rdom(am, an)
? RH, if ?ldom(am, an) ? rdom(am, an)
? BL, if ldom(am, an) ? rdom(am, an)
? BD, if ?ldom(am, an) ? ?rdom(am, an)
5.2 Parameterization and Training
After extracting events, we are now ready to train
the models. To estimate them, we train a discrimi-
native classifier for each model and use the normal-
ized posteriors at decoding time as additional feature
scores in SMT?s log-linear framework.
At a high level, we use a rich set of binary fea-
tures ranging from lexical to part-of-speech (POS)
and to syntactic features. Additionally, we augment
the feature set with compound features, e.g. a con-
junction of the source word of the left anchor and the
source word of the right anchor. Although they in-
crease the number of features significantly, we found
that they are empirically beneficial.
Suppose a = (f j2j1 /e
i2
i1), ML(a) = (f
j4
j3 /e
i4
i3) and
MR(a) = (f
j6
j5 /e
i6
i5), then based on the context?s
location, the elementary features employed in our
classifiers can be categorized into:
? anchor-related: (the actual word of f j2j1 ),
(part-of-speech (POS) tag of ), (?s parent in the
parse tree), (ei2i1?s actual target word).
? surrounding: (the previous word / f j1?1j1?1 ), (the
next word / f j2+1j2+1 ), (?s POS tag), (?s POS tag),
(?s parent), (?s parent).
? non-local: (the previous anchor?s source word)
, (the next anchor?s source word), (?s POS tag),
(?s POS tag).
There is a separate set of elementary features for am
and an and we come up with manual combination to
construct compound features.
In training the models, we manually come up with
around 30-50 types of features, which consists of a
combination of elementary and compound features.
Due to space constraints, we will describe the ac-
tual features that we use and the classification per-
formance of our models elsewhere. In total, we
generate around one hundred millions binary fea-
tures from our training data that contains six million
sentence pairs. To reduce the number of features,
we employ the L1-regularization in training to en-
force sparse solutions, using the off-the-shelf LIB-
LINEAR toolkit (Fan et al, 2008). After training,
the number of features in our classifiers decreases to
below 1 million features for each classifier.
6 Decoding
As mentioned earlier, we wish to avoid the spuri-
ous ambiguity issue where different derivations have
radically different scores although they lead to the
same reordering. This section describes our decod-
ing algorithm that avoids spurious ambiguity issue
by incrementally constructing MLs and MRs thus
allowing the computation of the models over partial
hypotheses.
In our experiments, we integrate our dominance
model as well as our orientation model into a syntax-
based SMT system that uses SCFG formalism. In-
tegrating the models into syntax-based SMT sys-
tems is non-trivial, especially since the anchors of-
ten reside within translation rules and the model
doesn?t always decompose naturally with the hy-
pothesis structure. To facilitate that, we need to
first induce the necessary alignment for all transla-
tion units in the hypothesis.
To describe the algorithm, let us consider a cheat-
ing exercise where we have to translate the Chinese
sentence in Fig. 2 with the following set of hierar-
chical phrases:
Xa??Aozhou
1shi2X1,Australia
1 is2X1?
Xb??yu
3 Beihan4X1, X1with
3 North4 Korea?
Xc??you
5bangjiao6, have5dipl.6 rels.?
Xd??X1 de
7shaoshu8 guojia9 zhi10 yi11,
one11of10the few8 countries9 that7X1?
As a case in point, let us consider D = Xa ? Xb
? Xd ? Xc, which will lead to the correct English
507
Target string (w/ source index) Symbol(s) read Op. Stack(s)
(1) Xc have
5 dipl.6 rels. [5][6] S,S,R Xc:[5-6]
(2) Xd one11 of
10 few8 countries9 [11][10] S,S,R [10-11]
that7 Xc
(3) [8][9] S,S,R,R [8-11]
(4) [7] S [8-11][7]
(5) Xc:[5,6] S Xd:[8-11][7][5,6]
(6) Xb Xd with
3 North4 Korea Xd:[8-11][7][5,6] S [8-11][7][5,6]
(7) [3][4] S,S,R,R Xb:[8-11][7][3-6]
(8) Xa Australia
1 is2 Xb [1][2] S,S,R [1-2]
(9) Xb:[8-11][7][3,6] S,A Xa:[1-2][8-11][7][3,6]
Table 2: The application of the shift-reduce parsing algorithm, which corresponds to the following derivation D =
Xa ? Xb ? Xd ? Xc. Anchor is in bold. In column Op., S, R and A refer to shift, reduce and accept operation
respectively.
translation as in Fig. 2. Note that the translation
rules contain internal word alignment, which we as-
sume to have been previously inferred.
The algorithm bears a close resemblance to the
shift-reduce algorithm found in phrase-based decod-
ing (Galley and Manning, 2008; Feng et al, 2010;
Cherry et al, 2012). A stack is used to accumulate
(partial) information about a, ML and MR for each
a ? A in the derivation. This algorithm takes an in-
put stream and applies either the shift or the reduce
operations starting from the beginning until the end
of the stream. The shift operation advances the input
stream by one symbol and push the symbol into the
stack; while the reduce operation applies some rule
to the top-most elements of the stack. The algorithm
terminates at the end of the input stream where the
resulting stack will be propagated to the parent for
the later stage of decoding. In our case, the input
stream is the target string of the rule and the symbol
is the corresponding source index of the elements of
the target string. The reduction rule looks at two in-
dices and merge them if they are adjacent (i.e. has
no intervening phrase). We forbid the application
of the reduction rule to anchors. Table 2 shows the
execution trace of the algorithm for the derivation
described earlier. For conciseness, we assume that
there is only one anchor and that is de7/that7.
As shown, the algorithm starts with an empty
stack. It then projects the source index to the corre-
sponding target word and then enumerates the target
string in a left to right fashion. If it finds a target
word with a source index, it applies the shift oper-
ation, pushing the index to the stack. Unless the
symbol corresponds to an anchor, it tries to apply
the reduce operation. Line (4) indicates the special
treatment to the anchor. If the symbol being read
is a nonterminal, then we push the entire stack that
corresponds to that nonterminal. For example, when
the algorithm reads Xd at line (6), it pushes the en-
tire stack from line (5).
As MLs and MRs are being incremen-
tally constructed, we can immediately com-
pute Pdomo(dom(am, an)|am, an) as soon
as a partial derivation covers both am
and an. For example, we can compute
Pdom1(dom(you5/have8, de7/that7) = ),
Pdom1(dom(de7/that7, zhi10/of4) = ) and
Pdom2(dom(you5/have8, zhi10/of4) = ) at
partial hypothesis Xd ? Xc which corresponds to a
constituent spanning from 5-11.
7 Experiments
Our baseline systems is a state-of-the-art string-to-
dependency system (Shen et al, 2008). The sys-
tem is trained on 10 million parallel sentences that
are available to the Phase 1 of the DARPA BOLT
Chinese-English MT task. The training corpora in-
clude a mixed genre of newswire, weblog, broad-
cast news, broadcast conversation, discussion fo-
rums and comes from various sources such as LDC,
HK Law, HK Hansard and UN data.
In total, our baseline model employs more than
50 features, including from our proposed dominance
and orientation models. In addition to the standard
508
Model
newswire weblog newswire+weblog
BLEU TER Comb BLEU TER Comb BLEU TER Comb
(a) (b) (c) (d) (e) (f) (g) (h) (i)
(1) S2D 37.63 53.17 7.77 27.60 57.19 14.77 33.39 54.97 10.79
(2) +dom1 38.12 52.31 7.10 27.56 56.58 14.51 33.64 54.24 10.30
(3) +dom2 38.31 52.28 6.99 27.66 56.57 14.45 33.78 54.20 10.21
(4) +dom3 38.31 52.52 7.10 28.24 56.56 14.16 34.02 54.33 10.15
(5) +dom4 38.54 52.22 6.84 28.38 56.55 14.08 34.20 54.16 9.98
(6) +dom5 38.17 52.57 7.20 28.67 56.27 13.80 34.16 54.27 10.05
(7) +dom6 38.17 52.52 7.18 28.64 56.22 13.79 34.10 54.18 10.04
(8) +ori 38.52 52.43 6.96 28.26 56.54 14.14 34.15 54.27 10.06
(9) +ori+dom1 38.87 52.05 6.59 28.01 56.48 14.23 34.26 54.03 9.89
(10) +ori+dom2 38.96 51.87 6.45 27.98 56.23 14.12 34.29 53.82 9.77
(11) +ori+dom3 39.19 51.77 6.29 28.19 56.15 13.98 34.52 53.73 9.61
(12) +ori+dom4 39.34 51.77 6.21 28.41 56.17 13.88 34.60 53.69 9.54
(13) +ori+dom5 39.31 51.67 6.18 28.62 56.09 13.74 34.76 53.65 9.45
Table 3: The NIST MT08 results on newswire (nw), weblog (wb) and combined genres. S2D is the baseline string-
to-dependency system (line 1). Lines 2-7 shows the results of the dominance model with O = 1 ? 6. Line 8 shows
result on adding ori to the baseline. Lines 9-13 shows the results of the orientation complemented with the dominance
model with varying O. The best BLEU, TER and Comb on each genre of the first set are in italic while those of the
second set are in bold. For BLEU, higher scores are better, while for TER and Comb, lower scores are better.
features such as translation probabilities, we incor-
porate features that are found useful for developing
a state-of-the-art baseline, such as the provenance
features (Chiang et al, 2011). We use a 6-gram
language model, which was trained on 10 billion
English words from multiple corpora, including the
English side of our parallel corpus plus other cor-
pora such as Gigaword (LDC2011T07) and Google
News. We also train a class-based language model
(Chen, 2009) on two million English sentences se-
lected from the parallel corpus. As for our string-to-
dependency system, we train 3-gram models for left
and right dependencies and unigram for head using
the target side of the parallel corpus. To train our
models, we select a set of 5 million sentence pairs.
For the tuning and development sets, we set
aside 1275 and 1239 sentences selected from
LDC2010E30 corpus. We tune the feature weights
with PRO (Hopkins and May, 2011) to minimize
(TER-BLEU)/2 metric. As for the blind test set,
we report the performance on the NIST MT08 eval-
uation set, which consists of 691 sentences from
newswire and 666 sentences from weblog. We pick
the weights that produce the highest development set
scores to decode the test set.
We perform two sets of experiments. The first set
looks at the contribution of the dominance model
with varying values of o. The second one looks at
the combination of the dominance model and the
orientation model. Table 3 summarizes the experi-
mental results on NIST MT08 sets, categorized by
genres. We report the results on newswire genre in
columns a-c, those on weblog genre in column d-f,
and those on mixed genre in column g-i. The perfor-
mance of our baseline string-to-dependency syntax-
based SMT is shown in the first line.
Lines 2-7 in Table 3 show the results of our first
set of experiments, starting from the result of dom1,
which looks at only at pairs of adjacent anchors, to
the result of dom6, which looks at pairs of anchors
that are at most 5 anchors away. As shown in line
2, our dominance model provides a nice improve-
ment of around 0.5 point over the baseline even if it
only looks at restricted context. Increasing the or-
der of our dominance model provides an additional
gain. However, the gain is more pronounced in the
weblog genre (up to around 1 BLEU point) than in
the newswire genre. We conjecture that this may be
the artifact of our tune set, which comes from the
weblog genre. We stop at dom6 because we observe
509
that the weight of the feature score that corresponds
to the maximum order (o = 6) has a negative sign,
which often indicates a high correlation between the
new features and existing ones.
Lines 8-13 in Table 3 shows the results of our sec-
ond set of experiments. Line 8 shows the result of
adding the orientation model (ori) to the baseline
system. As shown, integrating ori shows a signifi-
cant gain. On top of which, we then integrate dom1
to dom5. We see a very encouraging result as adding
the dominance model increases the performance fur-
ther, consistently over different value of o. This sug-
gests that the dominance model is complementary
to the orientation model. Our best result provides
more than 1 BP improvement and 1 TER reduction
consistently over different genres. We see this result
as confirming our intuition that the global contextual
information provided by our AG model can signifi-
cantly improve the performance of SMT even in a
state-of-the-art system.
8 Related Work
Our work intersects with existing work in many dif-
ferent respects. In this section, we mainly focus on
work related to introducing higher-order contextual
information to reordering model.
In providing global contextual information, our
work is related to a large amount of literature. To
name a few, Zens and Ney (2006) improves the lexi-
calized reordering model of Tillman (2004) by in-
corporating part-of-speech information. Chang et
al. (2009) incorporates contexts from syntactic parse
tree. Bach et al (2009) exploits the dependency in-
formation and Xiong et al (2012) uses the predicate-
argument structure.
Vaswani et al (2011) introduces rule markov
models for a forest-to-string model in which the
number of possible derivations is restricted. More
recently, Durrani et al (2013) and Zhang et al
(2013) cast reordering process as a Markov process.
Similar to these models, our proposed model also
provide context dependencies to the application of
translation rules, however, as they focus on mini-
mal translation units (MTU) where we focus on a
selected set of translation units. (Banchs et al, 2005)
introduces a bigram model for monotone phrase-
based system, but their definition of translation units
is suitable only for language pairs with limited re-
ordering, such as translating Spanish to English.
In equating anchors with the function word class,
our work is closely related to the function word-
centered model of Setiawan et al (2007), especially
the orientation model. Our dominance model is
closely related to the reordering model of Setiawan
et al (2009), except that they only look at pair of ad-
jacent anchors, forming a chain structure instead of
a graph like in our dominance model. Furthermore,
we provide a discriminative treatment to the model
to include a richer set of features including syntac-
tic features. This work can be seen as modeling the
identity of the neighboring of the anchors, similar to
(Setiawan et al, 2013). However, instead of looking
at the words at the borders, we look at whether the
neighboring constituents contain other anchors.
9 Conclusion
We propose the ?Anchor Graph? (AG) model to en-
code global contextual information. A selected set
of translation units, which we call anchors, serves
as the vertices of AG. And as the edges, we model
two types of relations, namely the dominance and
the precedence relations, where the former looks at
the positions of the anchors in the derivation struc-
ture, while the latter looks at the positions of the
anchors in the surface structure, resulting into two
probabilistic models over edge labels. As the mod-
els look at the pairs of anchors that go beyond multi-
ple translation units, our AG model provides global
contextual information.
Our AG model embodies (admittedly crudely)
some basic principles of sentence organization,
namely categorization (in categorizing units into an-
chors and non-anchors), linear order (in modeling
the precedence of anchors) and constituency struc-
ture (in modeling the dominance between anchors).
We are encouraged by the facts that we learn these
principles in an unsupervised way and that we can
achieve a significant improvement over a strong
baseline in a large-scale Chinese-to-English trans-
lation task. In the future, we hope to continue this
line of research, perhaps by learning to identify an-
chors automatically from training data or by using
our models to induce derivations directly from un-
aligned sentence pair.
510
Acknowledgements
We would like to acknowledge the support of
DARPA under Grant HR0011-12-C-0015 for fund-
ing part of this work. The views, opinions, and/or
findings contained in this article/presentation are
those of the author/presenter and should not be inter-
preted as representing the official views or policies,
either expressed or implied, of the DARPA.
References
Nguyen Bach, Qin Gao, and Stephan Vogel. 2009.
Source-side dependency tree reordering models with
subtree movements and constraints. In Proceedings of
the Twelfth Machine Translation Summit (MTSummit-
XII), Ottawa, Canada, August. International Associa-
tion for Machine Translation.
Rafael E. Banchs, Josep M. Crego, Adria` de Gispert, Pa-
trik Lambert, and Jose? B. Marin?o. 2005. Statisti-
cal machine translation of Euparl data by using bilin-
gual n-grams. In Proceedings of the ACL Workshop
on Building and Using Parallel Texts, pages 133?136,
Ann Arbor, Michigan, June. Association for Compu-
tational Linguistics.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative re-
ordering with Chinese grammatical relations features.
In Proceedings of the Third Workshop on Syntax and
Structure in Statistical Translation (SSST-3) at NAACL
HLT 2009, pages 51?59, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Stanley Chen. 2009. Shrinking exponential language
models. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 468?476, Boulder, Colorado,
June. Association for Computational Linguistics.
Colin Cherry, Robert C. Moore, and Chris Quirk. 2012.
On hierarchical re-ordering and permutation parsing
for phrase-based decoding. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
pages 200?209, Montre?al, Canada, June. Association
for Computational Linguistics.
David Chiang, Steve DeNeefe, and Michael Pust. 2011.
Two easy improvements to lexical weighting. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 455?460, Portland, Oregon, USA,
June. Association for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 263?270, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013. Model with minimal translation units, but de-
code with phrases. In Proceedings of the 2013 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 1?11, Atlanta, Georgia, June. As-
sociation for Computational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An
efficient shift-reduce decoding algorithm for phrased-
based machine translation. In Coling 2010: Posters,
pages 285?293, Beijing, China, August. Coling 2010
Organizing Committee.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 848?856, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352?1362, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation, June.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li. 2007.
Ordering phrases with function words. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 712?719, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Hendra Setiawan, Min Yen Kan, Haizhou Li, and Philip
Resnik. 2009. Topological ordering of function words
in hierarchical phrase-based translation. In Proceed-
ings of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint Confer-
ence on Natural Language Processing of the AFNLP,
pages 324?332, Suntec, Singapore, August. Associa-
tion for Computational Linguistics.
Hendra Setiawan, Bowen Zhou, Bing Xiang, and Libin
Shen. 2013. Two-neighbor orientation model with
cross-boundary global contexts. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
511
1264?1274, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, pages 577?585, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In HLT-NAACL
2004: Short Papers, pages 101?104, Boston, Mas-
sachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule markov models for fast tree-to-
string translation. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 856?864,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404, Sep.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Model-
ing the translation of predicate-argument structure for
smt. In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 902?911, Jeju Island, Korea, July.
Association for Computational Linguistics.
Richard Zens and Hermann Ney. 2006. Discrimina-
tive reordering models for statistical machine trans-
lation. In Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL): Proceed-
ings of the Workshop on Statistical Machine Transla-
tion, pages 55?63, New York City, NY, June. Associa-
tion for Computational Linguistics.
Hui Zhang, Kristina Toutanova, Chris Quirk, and Jian-
feng Gao. 2013. Beyond left-to-right: Multiple de-
composition structures for smt. In Proceedings of the
2013 Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 12?21, Atlanta, Geor-
gia, June. Association for Computational Linguistics.
512
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 349?352,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Generalizing Hierarchical Phrase-based Translation
using Rules with Adjacent Nonterminals
Hendra Setiawan and Philip Resnik
UMIACS Laboratory for Computational Linguistics and Information Processing
University of Maryland, College Park, MD 20742, USA
hendra, resnik @umd.edu
Abstract
Hierarchical phrase-based translation (Hiero,
(Chiang, 2005)) provides an attractive frame-
work within which both short- and long-
distance reorderings can be addressed consis-
tently and efciently. However, Hiero is gen-
erally implemented with a constraint prevent-
ing the creation of rules with adjacent nonter-
minals, because such rules introduce compu-
tational and modeling challenges. We intro-
duce methods to address these challenges, and
demonstrate that rules with adjacent nontermi-
nals can improve Hiero's generalization power
and lead to signicant performance gains in
Chinese-English translation.
1 Introduction
Hierarchical phrase-based translation (Hiero, (Chi-
ang, 2005)) has proven to be a very useful com-
promise between syntactically informed and purely
corpus-driven translation. By automatically learn-
ing synchronous grammar rules from parallel text,
Hiero captures short- and long-distance reorderings
consistently and efciently. However, implementa-
tions of Hiero generally forbid adjacent nonterminal
symbols on the source side of hierarchical rules, a
practice we will refer to as the non-adjacent nonter-
minals constraint. The main argument against such
rules is that they cause the system to produce multi-
ple derivations that all lead to the same translation ?
a form of redundancy known as spurious ambiguity.
Spurious ambiguity can lead to drastic reductions in
decoding efciency, and the obvious solutions, such
as reducing beam width, erode translation quality.
In Section 2, we argue that the non-adjacent non-
terminals constraints severely limits Hiero's gener-
alization power, limiting its coverage of important
reordering phenomena. In Section 3, we discuss
the challenges that arise in relaxing this constraint.
In Section 4 we introduce new methods to address
those challenges, and Section 5 validates the ap-
proach empirically.
Improving Hiero via variations on rule prun-
ing and ltering is well explored, e.g., (Chiang,
2005; Chiang et al, 2008; Zollmann and Venugopal,
2006), to name just a few. These proposals dif-
fer from each other mainly in the specic linguis-
tic knowledge being used, and on which side the
constraints are applied. In contrast, we complement
previous work by showing that adding rules to Hiero
can provide benets if done judiciously.
2 Judicious Use of Adjacent Nonterminals
Our motivations largely followMenezes and Quirk's
(2007) discussion of reorderings and generalization.
As a specic example, we will use a Chinese to En-
glish verb phrase (VP) translation (Fig. 1), which
represents one of the most prominent phrase con-
structions in Chinese. Here the construction of the
Chinese VP involves joining a prepositional phrase
(PP) and a smaller verbal phrase (VP-A), with the
preposition at the beginning as a PP marker. In the
translation, the VP-A precedes the PP, a shift from
pre-verbal PP in Chinese to post-verbal in English.
?\??? ???
rank 10th at Eastern division
????
????
?
PPP
PPP
P
PPP
PPP
P
P NP VP-A
PP
VP
?? HH
?? HHHHHH
Figure 1: A Chinese-English verb phrase translation
349
Hiero can correctly translate the example if it
learns any of the following rules from training data:
X??? X1 ???, rank 10th at X1? (1)
X?? ??\?? X1, X1 at Eastern div.? (2)
X??X1 ?\?? X2, X2 X1 Eastern div.? (3)
However, in practice, data sparsity makes the chance
of learning these rules rather slim. For instance,
learning Rule 1 depends on training data containing
instances of the shift with identical wording for the
VP-A, which belongs to an open word class.
If Hiero fails to learn any of the above rules, it
will apply the ?glue rules? S ? ?S X1, S X1? and
S ? ?X, X?. But these glue rules clearly can-
not model the VP-A's movement. In failing to learn
Rules 1-3, Hiero has no choice but to translate VP-A
in a monotone order.
On the other hand, consider the following rules
with adjacent nonterminals on the source side (or XX
rules, for brevity):
X??? X1X2, X2 at X1? (4)
X??X1X2???, rank 10th X1X2? (5)
X??X1X2, X2X1? (6)
Note that although XX rules 4-6 can potentially in-
crease the chance of modeling the pre-verbal to post-
verbal shift, not all of them are benecial to learn.
For instance, Rule 5 models the word order shift but
introduces spurious ambiguity, since the nontermi-
nals are translated in monotone order. Rule 6, which
resembles the inverted rule of the Inversion Trans-
duction Grammar (Wu, 1997), is highly ambigu-
ous because its application has no lexical grounding.
Rule 4 avoids both problems, and is also easier to
learn, since it is lexically anchored by a preposition,
?(at), which we can expect to appear frequently in
training. These observations will motivate us to fo-
cus on rules that model non-monotone reordering of
phrases surrounding a lexical item on the target side.
3 Addressing XX Rule Challenges
The rst challenge created by introducing XX rules
is computational: relaxing the constraint signi-
cantly increases the grammar size. Motivated by
our earlier discussion, we address this by permitting
only rules that model non-monotone reordering, i.e.
those rules whose nonterminals are projected into
the target language in a different word order, leaving
monotone mappings to be handled by the glue rules
as previously. This choice helps keep the search
space more manageable, and also avoids spurious
ambiguity. In addition, we disallow rules in which
nonterminals are adjacent on both the source and tar-
get sides, by imposing the non adjacent nonterminal
constraint on the target side whenever the constraint
is relaxed on the source side. This forces any non-
monotone reorderings to always be grounded in lex-
ical evidence. We refer to the permitted subset of
XX rules as XX-nonmono rules.
The second challenge involves modeling: intro-
ducing XX rules places them in competition with
the existing glue rules. In particular, these two kinds
of rules try to model the same phenomena, namely
the translations of phrases that appear next to each
other. However, they differ in terms of the features
associated with the rules. XX rules will be asso-
ciated with the same features as any other hierar-
chical rules, since they are all learned via an iden-
tical training method. In contrast, glue rules are
introduced into the grammar in an ad hoc manner,
and the only feature associated with them is a ?glue
penalty?. These distinct feature sets makes direct
comparison of scores unreliable. As a result the de-
coder may simply prefer to always select glue rules
because they are associated with fewer features re-
sulting in adjacent phrases always being translated
in a monotone order. To address this issue, we in-
troduce a new model, which we call the target-side
function words orientation-based model, or simply
Porit , which evaluates the application of the two
kinds of rules on the same context, i.e. for our ex-
ample, it is the function word?(at).
4 Target-side Function Words
Orientation-based Model
The Porit model is motivated by the function words
reordering hypothesis (Setiawan et al, 2007), which
suggests that function words encode essential infor-
mation about the (re)ordering of their neighboring
phrases. In contrast to Setiawan et al (2007), who
looked at neighboring contexts for function words
on the source side, we focus here on modeling the
inuence of function words on neighboring phrases
350
on the target side. We argue that this focus better ts
our purpose, since the phrases that we want to model
are the function words' neighbors on the target side,
as illustrated in Fig. 1.
To develop this idea, we rst dene an orit func-
tion that takes a source function word as a refer-
ence point, along with its neighboring phrase on the
target side. The orit function outputs one of the
following orientation values (Nagata et al, 2006):
Monotone-Adjacent (MA); Reverse-Adjacent (RA);
Monotone-Gap (MG); and Reverse-Gap (RG). The
Monotone/Reverse distinction indicates whether the
source order follows the target order. The Ad-
jacent/Gap distinction indicates whether the two
phrases are adjacent or separated by an intervening
phrase on the source side. For example, in Fig. 1,
the value of orit for right neighbor Eastern division
with respect to function word? (at) is MA, since its
corresponding source phrase ?\?? is adjacent
to? (at) and their order is preserved on the English
side. The value for left neighbor rank 10th with re-
spect to? (at) is RG, since ??? is separated
from ? (at) and their order is reversed on the En-
glish side.
More formally, we dene Porit(orit(Y,X)|Y ),
where orit(Y,X) ? {MA,RA,MG,RG} is the ori-
entation of a target phrase X with a source function
word Y as the reference point.1
We estimate the orientation model us-
ing maximum likelihood, which involves
counting and normalizing events of interest:
(Y, o = orit(Y,X)). Specically, we estimate
Porit(o|Y ) = C(Y, o)/C(Y, ?). Collecting training
counts C(Y, o) involves several steps. First, we
run GIZA++ on the training bitext and apply the
?grow-diag-nal? heuristic over the training data
to produce a bi-directional word alignment. Then,
we enumerate all occurrences of Y and determine
orit(Y,X). To ensure uniqueness, we enforce
that neighbor X be the longest possible phrase
that satises the consistency constraint (Och and
Ney, 2004). Determining orit(Y,X) can then be
done in a straightforward manner by looking at the
monotonicity (monotone or reverse) and adjacency
(adjacent or gap) between Y 's and X .
1
In fact, separate models are developed for left and right
neighbors, although for clarity we suppress this distinction
throughout.
MT06 MT08
baseline 30.58 23.59
+itg 29.82 23.21
+XX 30.10 22.86
+XX-nonmono 30.96 24.07
+orit 30.19 23.69
+XX-nonmono+orit 31.49 24.73
Table 1: Experimental results where better than baseline
results are italicized, and statistically signicant better
(p < 0.01) are in bold.
5 Experiments
We evaluated the generalization of Hiero to include
XX rules on a Chinese-to-English translation task.
We treat the N = 128 most frequent words in
the corpus as function words, an approximation that
has worked well in the past and minimized depen-
dence on language-specic resources (Setiawan et
al., 2007). We report BLEU r4n4 and assess signi-
cance using the standard bootstrapping approach.
We trained on the NIST MT06 Eval corpus ex-
cluding the UN data (approximately 900K sentence
pairs), segmenting Chinese using the Harbin seg-
menter (Zhao et al, 2001). Our 5-gram language
model with modied Kneser-Ney smoothing was
trained on the English side of our training data plus
portions of the Gigaword v2 English corpus. We
optimized the feature weights using minimum er-
ror rate training, using the NIST MT03 test set as
the development set. We report the results on the
NIST 2006 evaluation test (MT06) and the NIST
2008 evaluation test (MT08).
Table 1 reports experiments in an incremental
fashion, starting from the baseline model (the orig-
inal Hiero), then adding different sets of rules, and
nally adding the orientation-based model. In our
rst experiments, we investigated the introduction
of three different sets of XX rules. First (+itg),
we simply add the ITG's inverted rule (Rule 6) to
the baseline system in an ad-hoc manner, similar to
the glue rules. This hurts performance consistently
across MT06 and MT08 sets, which we suspect is
a result of ITG rule applications often aggravating
search error. Second (+XX), we permitted general
XX rules. This results in a grammar size increase of
25-26%, ltering out rules irrelevant for the test set,
351
and leads to a signicant performance drop, again
perhaps attributable to search error. When we in-
spected the rules, we observe that the majority of
these rules involve spurious word insertions. Third
(+XX-nonmono), we introduced only XX-nonmono
rules; this produced only a 5% additional rules, and
yielded a marginal but consistent gain.
In a second experiment (+orit), we introduced
the target-side function words orientation-based
model. Note that this experiment is orthogonal to the
rst set, since we introduce no additional rules. Re-
sults are mixed, worse for MT06 but better (with sig-
nicance) for MT08. Here, we suspect the model's
potential has not been fully realized, since Hiero
only considers monotone reordering in unseen cases.
Finally, we combine both the XX-nonmono rules
and the Porit model (+XX-nonmono+orit). The
combination produces a signicant, consistent gain
across all test sets. This result suggests that the ori-
entation model contributes more strongly in unseen
cases when Hiero also considers non-monotone re-
ordering. We interpret this result as a validation
of our hypothesis that carefully relaxing the non-
adjacent constraint improves translation.
6 Discussion and Future Work
To our knowledge, the work reported here is the
rst to relax the non-adjacent nonterminals con-
straint in hierarchical phrase-based models. The re-
sults conrm that judiciously adding rules to a Hiero
grammar, adjusting the modeling accordingly, can
achieve signicant gains.
Although we found that XX-nonmono rules per-
formed better than general XX rules, we believe the
latter may nonetheless prove useful. Manually in-
specting our system's output, we nd that the output
is often shorter than the references, and the missing
words often correspond to function words that are
modeled by those rules. Using XX rules to model
legitimate word insertions is a topic for future work.
Acknowledgments
The authors gratefully acknowledge partial support
from the GALE program of the Defense Advanced
Research Projects Agency, Contract No. HR0011-
06-2-001. Any opinions, ndings, conclusions or
recommendations expressed in this paper are those
of the authors and do not necessarily reect the
views of the sponsors.
References
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 224?233, Honolulu, Hawaii,
October.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL'05), pages 263?270, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
Arul Menezes and Chris Quirk. 2007. Using dependency
order templates to improve generality in translation.
In Proceedings of the Second Workshop on Statistical
Machine Translation, pages 1?8, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,
and Kazuteru Ohashi. 2006. A clustered global phrase
reordering model for statistical machine translation.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 713?720, Sydney, Australia, July. Association
for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li. 2007.
Ordering phrases with function words. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 712?719, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404, Sep.
Tiejun Zhao, Yajuan Lv, Jianmin Yao, Hao Yu, Muyun
Yang, and Fang Liu. 2001. Increasing accuracy of
chinese segmentation with strategy of multi-step pro-
cessing. Journal of Chinese Information Processing
(Chinese Version), 1:13?18.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation, pages 138?141, New York City, June. As-
sociation for Computational Linguistics.
352
Proceedings of NAACL-HLT 2013, pages 335?341,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Discriminative Training of 150 Million Translation Parameters
and Its Application to Pruning
Hendra Setiawan and Bowen Zhou
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598, USA
{hendras, zhou}@us.ibm.com
Abstract
Until recently, the application of discrimina-
tive training to log linear-based statistical ma-
chine translation has been limited to tuning
the weights of a limited number of features or
training features with a limited number of pa-
rameters. In this paper, we propose to scale
up discriminative training of (He and Deng,
2012) to train features with 150 million pa-
rameters, which is one order of magnitude
higher than previously published effort, and
to apply discriminative training to redistribute
probability mass that is lost due to model
pruning. The experimental results confirm the
effectiveness of our proposals on NIST MT06
set over a strong baseline.
1 Introduction
State-of-the-art statistical machine translation sys-
tems based on a log-linear framework are parame-
terized by {?,?}, where the feature weights ? are
discriminatively trained (Och and Ney, 2002; Chi-
ang et al, 2008b; Simianer et al, 2012) by directly
optimizing them against a translation-oriented met-
ric such as BLEU. The feature parameters ? can
be roughly divided into two categories: dense fea-
ture that measures the plausibility of each translation
rule from a particular aspect, e.g., the rule transla-
tion probabilities p(f |e) and p(e|f); and sparse fea-
ture that fires when certain phenomena is observed,
e.g., when a frequent word pair co-occured in a rule.
In contrast to ?, feature parameters in ? are usually
modeled by generative models for dense features, or
by indicator functions for sparse ones. It is therefore
desirable to train the dense features for each rule in a
discriminative fashion to maximize some translation
criterion. The maximum expected BLEU training of
(He and Deng, 2012) is a recent effort towards this
direction, and in this paper, we extend their work
to a scaled-up task of discriminative training of the
features of a strong hierarchical phrase-based model
and confirm its effectiveness empirically.
In this work, we further consider the application
of discriminative training to pruned model. Various
pruning techniques (Johnson et al, 2007; Zens et al,
2012; Eck et al, 2007; Lee et al, 2012; Tomeh et al,
2011) have been proposed recently to filter transla-
tion rules. One common consequence of pruning is
that the probability distribution of many surviving
rules become deficient, i.e.
?
f p(f |e) < 1. In prac-
tice, others have chosen either to leave the pruned
rules as it-is, or simply to re-normalize the proba-
bility mass by distributing the pruned mass to sur-
viving rules proportionally. We argue that both ap-
proaches are suboptimal, and propose a more prin-
cipled method to re-distribute the probability mass,
i.e. using discriminative training with some trans-
lation criterion. Our experimental results demon-
strate that at various pruning levels, our approach
improves performance consistently. Particularly at
the level of 50% of rules being pruned, the discrimi-
natively trained models performs better than the un-
pruned baseline grammar. This shows that discrim-
inative training makes it possible to achieve smaller
models that perform comparably or even better than
the baseline model.
Our contributions in this paper are two-folded:
First of all, we scale up the maximum expected
BLEU training proposed in (He and Deng, 2012) in
a number of ways including using 1) a hierarchical
phrase-based model, 2) a richer feature set, and 3) a
larger training set with a much larger parameter set,
resulting in more than 150 million parameters in the
model being updated, which is one order magnitude
higher than the phrase-based model reported in (He
and Deng, 2012). We are able to show a reasonable
335
improvement over this strong baseline. Secondly,
we combine discriminative training with pruning
techniques to reestimate parameters of pruned gram-
mar. Our approach is shown to alleviate the loss due
to pruning, and sometimes can even outperform the
baseline unpruned grammar.
2 Discriminative Training of ?
Given the entire training data {Fn, En}Nn=1, and cur-
rent parameterization {?,?}, we decode the source
side of training data Fn to produce hypothesis
{E?n}Nn=1. Our goal is to update ? towards ?
? that
maximizes the expected BLEU scores of the entire
training data given the current ?:
U(?)=
?
?E?1...E?N
P??(E?1..E?N |F1..FN )B(E?1..E?N ) (1)
where B(E?1...E?N ) is the BLEU score of the con-
catenated hypothesis of the entire training data, fol-
lowing (He and Deng, 2012).
Eq. 1 summarizes over all possible combina-
tions of E?1...E?N , which is intractable. Hence we
make two simplifying approximations as follows.
First, let the k-best hypotheses of the n-th sen-
tence, E?n =
{
E?1n, ..., E?
K
n
}
, approximate all its
possible translation. In other words, we assume
that
?K
k=1 P? (E?
k
n|Fn) = 1, ?n. Second, let the
sum of sentence-level BLEU approximate the cor-
pus BLEU. We note that corpus BLEU is not strictly
decomposable (Chiang et al, 2008a), however, as
the training data?s size N gets big as in our case, we
expect them to become more positively correlated.
Under these assumptions and the fact that each
sentence is decoded independently, Eq. 1 can be al-
gebraically simplified into:
U(?) =
N?
n=1
K?
k=1
P?(E?
k
n|Fn)B(E?
k
n) (2)
where P?(E?kn|Fn)=P??(E?
k
n|Fn)/
?
?k P??(E?
k
n|Fn).
We detail the process in the Appendix.
To further simplify the problem and relate it with
model pruning, we consider to update a subset of
? ? ? while keeping other parameterization of ?
unchanged, where ? = {?ij = p(ej |fi)} denotes our
parameter set that satisfies
?
j ?ij = 1 and ?ij ? 0.
In experiments, we also consider {?ji = p(fi|ej)}.
To alleviate overfitting, we introduce KL-distance
based reguralization as in (He and Deng, 2012). We
thus arrive at the following objective function:
O(?) = log(U(?))? ? ?KL(?||?0)/N (3)
where ? controls the regularization term?s contribu-
tion, and ?0 represents a prior parameter set, e.g.,
from the conventional maximum likelihood training.
The optimization algorithm is based on the Ex-
tended Baum Welch (EBW) (Gopalakrishnan et al,
1991) as derived by (He and Deng, 2012). The final
update rule is as follow:
??ij =
?
n
?
k ?(n, k, i, j) + U(?)??
0
ij/?+Di?ij
?
n
?
k
?
j ?(n, k, i, j) + U(?)?/?+Di
(4)
where ??ij is the updated parameter, ?(n, k, i, j) =
P?(E?kn|Fn){B(E?
k
n) ? Un(?)}
?
l 1(fn,k,l =
fi, en,k,l = ej); Un(?) =
?K
k=1 P?(E?
k
n|Fn)B(E?
k
n);
Di =
?
n,k,j max(0,??(n, k, i, j)) and ? is the
current feature?s weight.
3 DT is Beneficial for Pruning
Pruning is often a key part in deploying large-scale
SMT systems for many reasons, such as for reduc-
ing runtime memory footprint and for efficiency.
Many pruning techniques have been proposed to as-
sess translation rules and filter rules out if they are
less plausible than others. While different pruning
techniques may use different criterion, they all as-
sume that pruning does not affect the feature func-
tion values of the surviving rules. This assumption
may be suboptimal for some feature functions that
have probabilistic sense since pruning will remove
a portion of the probability mass that is previously
assigned to the pruned rules. To be concrete, for the
rule translation probabilities ?ij under consideration,
the constraint
?
j ?ij = 1 will not hold for all source
rules i after pruning. Previous works typically left
the probability mass as it-is, or simply renormalize
the pruned mass, i.e. ??ij = ?ij/
?
j ?ij .
We argue that applying the DT techniques to a
pruned grammar, as described in Sec. 2, provides
a more principled method to redistribute the mass,
i.e. by quantizing how each rule contributes to the
expected BLEU score in comparison to other com-
peting rules. To empirically verify this, we consider
336
the significance test based pruning (Johnson et al,
2007), though our general idea can be appllied to
any pruning techniques. For our experiments, we
use the significance pruning tool that is available as
part of Moses decoder package (Koehn et al, 2007).
4 Experiments
Our experiments are designed to serve two goals:
1) to show the performance of discriminative train-
ing of feature parameters ? in a large-scale task;
and 2) to show the effectiveness of DT when ap-
plied to pruned grammar. Our baseline system is a
state-of-the-art hierarchical phrase-based system as
described in (Zhou et al, 2008), trained on six mil-
lion parallel sentences corpora that are available to
the DARPA BOLT Chinese-English task. The train-
ing corpora includes a mixed genre of news wire,
broadcast news, web-blog and comes from various
sources such as LDC, HK Hansard and UN data.
In total, there are 50 dense features in our trans-
lation system. In addition to the standard features
which include the rule translation probabilities, we
incorporate features that are found useful for devel-
oping a state-of-the-art baseline, e.g. provenance-
based lexical features (Chiang et al, 2011). We use
a large 6-gram language model, which we train on a
10 billion words monolingual corpus, including the
English side of our parallel corpora plus other cor-
pora such as Gigaword (LDC2011T07) and Google
News. To prevent possible over-fitting, we only kept
the rules that have at most three terminal words (plus
up to two nonterminals) on the source side, resulting
in a grammar with 167 million rules.
Our discriminative training procedure includes
updating both ? and ?, and we follow (He and Deng,
2012) to optimize them in an alternate manner. That
is, when we optimize ? via EBW, we keep ? fixed
and when we optimize ?, we keep ? fixed. We use
PRO (Hopkins and May, 2011) to tune ?.
For discriminative training of ?, we use a subset
of 550 thousands of parallel sentences selected from
the entire training data, mainly to allow for faster ex-
perimental cycle; they mainly come from news and
web-blog domains. For each sentence of this subset,
we generate 500-best of unique hypotheses using the
baseline model. The 1-best and the oracle BLEU
scores for this subset are 40.19 and 47.06 respec-
tively. Following (He and Deng, 2012), we focus on
discriminative training of p(f |e) and p(e|f), which
in practice affects around 150 million of parameters;
hence the title.
For the tuning and development sets, we set
aside 1275 and 1239 sentences respectively from
LDC2010E30 corpus. The tune set is used by PRO
for tuning ? while the dev set is used to decide the
best DT model. As for the blind test set, we re-
port the performance on the NIST MT06 evaluation
set, which consists of 1644 sentences from news and
web-blog domains. Our baseline system?s perfor-
mance on MT06 is 39.91 which is among the best
number ever published so far in the community.
Table 1 compares the key components of our
baseline system with that of (He and Deng, 2012).
As shown, we are working with a stronger system
than (He and Deng, 2012), especially in terms of the
number of parameters under consideration |?|.
He&Deng(2012) This paper
Model phrase-based hierarchical
n-gram lm 3-gram 6-gram
# features 10 50
Max terminal 4 3
|?| 9.2 M 150M
# training data 750K 6M
N for DT 750K 550K
max K-best 100 500
Table 1: Our system compares to He&Deng?s (2012).
4.1 DT of 150 Million Parameters
To ensure the correctness of our implementation,
we show in Fig 2, the first five EBW updates with
? = 0.10. As shown, the utility function log(U(?))
increases monotonically but is countered by the KL
term, resulting in a smaller but consistent increase
of the objective function O(?). This monotonically-
increasing trend of the objective function confirms
the correctness of our implementation since EBW
algorithm is a bound-based technique that ensures
growth transformations between updates.
We then explore the optimal setting for ? which
controls the contribution of the regularization term.
Specifically, we perform grid search, exploring val-
ues of ? from 0.1 to 0.75. For each ? , we run several
iterations of discriminative training where each it-
eration involves one simultaneous update of p(f |e)
337
and p(e|f) according to Eq. 4, followed by one up-
date of ? via PRO (as in (He and Deng, 2012)). In
total, we run 10 such iterations for each ? .
tau=0.01N tau=0.05N tau=0.10N tau=0.25N tau=0.50N tau=0.75N tau=0.100 32.22 32.22 32.22 32.22 32.22 32.22 0 32.221 32.33 32.24 32.39 32.42 32.5 32.34 1 32.392 32.39 32.34 32.63 32.45 32.41 32.33 2 32.633 32.37 32.29 32.54 32.32 32.24 32.45 3 32.544 32.35 32.18 32.41 32.45 32.41 32.38 4 32.415 32.38 32.21 32.45 32.62 32.31 32.08 5 32.456 32.26 32.27 32.68 32.45 32.26 32.28 6 32.687 32.17 32.15 32.45 32.54 32.37 32.15 7 32.458 31.93 32.26 32.29 32.56 32.25 32.31 8 32.299 32.1 32.36 32.25 32.33 32.23 32.54 9 32.2510 32.1 32.29 32.2 32.42 32.29 32.31 10 32.232.39 32.36 32.68 32.62 32.5 32.5433.090.01 0.05 0.1 0.25 0.5 0.750.01 (2) 0.05 (9) 0.10 (6) 0.25 (5) 0.50 (1) 0.75 (9)32.39 32.36 32.68 32.62 32.5 32.5432.22 32.22 32.22 32.22 32.22 32.22
32.2$
32.3$
32.4$
32.5$
32.6$
32.7$
32.8$
0$ 1$ 2$ 3$ 4$ 5$ 6$ 7$
tau=0.10$
32.39$ 32.36$
32.68$ 32.62$
32.5$ 32.54$
32.22$32.2$32.25$
32.3$32.35$
32.4$32.45$
32.5$32.55$
32.6$32.65$
32.7$
0.01$(2)$ 0.05$(9)$ 0.10$(6)$ 0.25$(5)$ 0.50$(1)$ 0.75$(9)$
DT$baseline$
? = 0.01N
Figure 1: The dev set?s BLEU score (y-axis) on different
setting of ? (x-axis). The grey line indicates the baseline
performance on dev set. The number in bracket on the x-
axis indicates the iteration at which the score is obtained.
Across different ? , we find that the first iteration
provides most of the gain while the subsequent iter-
ations provide additional, smaller gain with occas-
sional performance degradation; thus the translation
performance is not always monotonically increasing
over iteration. We report the best score of each ? in
Fig. 1 and at which iteration that score is produced.
As shown in Fig. 1, all settings of ? improve over the
baseline and ? = 0.10 gives the highest gain of 0.45
BLEU score. This improvement is in the same ball-
park as in (He and Deng, 2012) though on a scaled-
up task. We next decode the MT06 using the best
model (i.e. ? = 0.10 at 6-th iteration) observed on
the dev set, and obtained 40.33 BLEU with an im-
provement of around 0.4 BLEU point. We see this
result as confirming the effectiveness of discrimi-
native training but on a larger-scale task, adding to
what was reported by (He and Deng, 2012).
4.2 DT for Significance Pruning
Next, we show the contribution of discriminative
training for model pruning. To do so, we prune the
translation grammar so that its size becomes 50%,
25%, 10% of the original grammar. Respectively,
we delete rules whose significance value below 15,
50 and 500. Table 2 compares the statistics of the
pruned grammars and the unpruned one. In particu-
lar, columns 4 and 5 show the total averaged prob-
ability mass of the remaining rules. This statistics
provides some indication of how deficient the fea-
Figure 2: Objective function (O(??)), the regularization
term (KL(??)) and the unregularized objective function
(log(U(??))) for five EBW updates of updating p(ej |fi)
tures are after pruning. As shown, the total averaged
probability mass after pruning is below 100% and
even lower for the more aggressive pruning.
To show that the deficiency is suboptimal, we con-
siders two baseline systems: models with/without
mass renormalization. We tune a new ? for each
model and use the new ? to decode the dev and test
sets. The results are shown in columns 6 and 9 of
Table 2 where we show the results for the unnor-
malized model in the brackets following the results
for the re-normalized model. The results show that
pruning degrades the performances and that naively
re-normalizing the model provides no significant
changes in performance. Subsequently, we will fo-
cus on the normalized models as the baseline as they
represents the starting points of our EBW iteration.
Next, we run discriminative training that would
reassign the probability mass to the surviving rules.
First, we normalize p(f |e) and p(e|f), so that they
satisfy the sum to one constraint required by the al-
gorithm. Then, we run discriminative training on
these pruned grammars using ? = 0.10 (i.e. the
setting that gives the best performance for the un-
pruned grammar as discussed in Section 4.1). We
report the results in columns 7 and 9 for the dev and
test sets respectively, as well as the gain over the
baseline system in columns 8 and 10.
As shown in Table 2, DT provides a nice im-
provement over the baseline model of no mass re-
assignment. For all pruning levels, DT can compen-
sate the loss associated with pruning. In particular,
at 50% level of pruning, there is a loss about 0.4
338
size |f | |e| p(?|e) p(?|f) dev-set test-set (MT06)
(%) (M) (M) baseline (un) DT (iter) gain baseline (un) DT gain
100 59 50 1.00 1.00 32.22(32.08) 32.68 (6) +0.44 39.91 (39.71) 40.33 +0.42
50 38 35 0.92 0.94 31.84 (32.02) 32.31 (6) +0.57 39.61(39.72) 40.08 +0.47
25 14 14 0.87 0.91 31.39 (31.43) 31.68 (2) +0.29 39.23 (39.17) 39.43 +0.20
10 4 3 0.77 0.84 27.27 (27.10) 27.82 (2) +0.55 36.01 (36.04) 36.43 +0.42
Table 2: The statistics of grammars pruned at various level (column 1), including the number of unique source and
target phrases (columns 2 & 3), total probability mass of the remaining rules for p(f |e) and p(e|f) (columns 4 & 5),
the performance of the pruned model before and after discriminative training as well as the gain on the dev and the
test sets (columns 6 to 11). The iteration at which DT gives the best dev set is indicated by the number enclosed by
bracket in column 7. The baseline performance is in italics, followed by a number in the bracket which refers to the
performance of using unnormalized model. The above-the-baseline performances are in bold.
BLEU point after pruning. With the DT on pruned
model, all pruning losses are reclaimed and the new
pruned model is even better than the unpruned orig-
inal model. This empirical result shows that leaving
probability mass unassigned after pruning is sub-
optimal and that discriminative training provides a
principled way to redistribute the mass.
5 Conclusion
In this paper, we first extend the maximum expected
BLEU training of (He and Deng, 2012) to train
two features of a state-of-the-art hierarchical phrase-
based system, namely: p(f |e) and p(e|f). Com-
pared to (He and Deng, 2012), we apply the algo-
rithm to a strong baseline that is trained on a big-
ger parallel corpora and comes with a richer feature
set. The number of parameters under consideration
amounts to 150 million. Our experiments show that
discriminative training these two features (out of 50)
gives around 0.40 BLEU point improvement, which
is consistent with the conclusion of (He and Deng,
2012) but in a much larger-scale system.
Furthermore, we apply the algorithm to redis-
tribute the probability mass of p(f |e) and p(e|f) that
is commonly lost due to conventional model prun-
ing. Previous techniques either leave the probability
mass as it is or distribute it proportionally among the
surviving rules. We show that our proposal of us-
ing discriminative training to redistribute the mass
empirically performs better, demonstrating the ef-
fectiveness of our proposal.
Appendix
We describe the process to simplify Eq. 1 to Eq. 2,
which is omitted in (He and Deng, 2012). For con-
ciseness, we drop the conditions and write P (E?i|Fi)
as P (E?i). We write Eq. 1 again below as Eq. 5 .
?
?E?1...E?N
N?
i=1
P (E?i|Fi) ?
N?
i=1
B(E?i) (5)
We first focus on the first sentence E1/F1 and ex-
pand the related terms from the equation as follow:
?
?E?1
?
?E?2...E?N
P (E?1)
N?
i=2
P (E?i).
[
B(E?1)+
N?
i=2
B(E?i)
]
Expanding the inner summation, we arrive at:
?
?E?1
P (E?1)B(E?1)
?
?E?2...E?N
N?
i=2
P (E?i) +
?
?E?1
P (E?1)
?
?E?2...E?N
N?
i=2
P (E?i)
N?
i=2
B(E?i)
Due to the that
?K
k=1 P? (E?
K
n |Fn) = 1, we can
equate
?
?E?2...E?N
?N
i=2 P (E?i) and
?
?E?1
P (E?1) to
1. Thus, we arrive at:
?
?E?1
P (E?1)B(E?1) +
?
?E?2...E?N
N?
i=2
P (E?i)
N?
i=2
B(E?i)
Notice that the second term has the same form
as Eq. 5 except that the starting index starts from
the second sentence. The same process can be per-
formed and at the end, thus we can arrive at Eq. 2.
339
Acknowledgments
We thank Xiadong He for helpful discussions. We
would like to acknowledge the support of DARPA
under Grant HR0011-12-C-0015 for funding part of
this work. The views, opinions, and/or findings con-
tained in this article/presentation are those of the au-
thor/presenter and should not be interpreted as rep-
resenting the official views or policies, either ex-
pressed or implied, of the DARPA.
References
David Chiang, Steve DeNeefe, Yee Seng Chan, and
Hwee Tou Ng. 2008a. Decomposability of transla-
tion metrics for improved evaluation and efficient al-
gorithms. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 610?619, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
David Chiang, Yuval Marton, and Philip Resnik. 2008b.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 224?233, Honolulu, Hawaii,
October. Association for Computational Linguistics.
David Chiang, Steve DeNeefe, and Michael Pust. 2011.
Two easy improvements to lexical weighting. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 455?460, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2007.
Translation model pruning via usage statistics for sta-
tistical machine translation. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Companion Volume, Short Papers,
pages 21?24, Rochester, New York, April. Association
for Computational Linguistics.
P. S. Gopalakrishnan, Dimitri Kanevsky, Arthur Na?das,
and David Nahamoo. 1991. An inequality for ratio-
nal functions with applications to some statistical esti-
mation problems. IEEE Transactions on Information
Theory, 37(1):107?113.
Xiaodong He and Li Deng. 2012. Maximum expected
bleu training of phrase and lexicon translation mod-
els. In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 292?301, Jeju Island, Korea, July.
Association for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352?1362, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975, Prague, Czech Republic, June. Association for
Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Seung-Wook Lee, Dongdong Zhang, Mu Li, Ming Zhou,
and Hae-Chang Rim. 2012. Translation model size
reduction for hierarchical phrase-based statistical ma-
chine translation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 291?295, Jeju Is-
land, Korea, July. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 295?302, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in smt. In
Proceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1: Long
Papers), pages 11?21, Jeju Island, Korea, July. Asso-
ciation for Computational Linguistics.
N. Tomeh, M. Turchi, G. Wisniewski, A. Allauzen, and
F. Yvon. 2011. How good are your phrases? assess-
ing phrase quality with single class classification. In
Proceedings of the International Workshop on Spoken
Language Translation, pages 261?268.
Richard Zens, Daisy Stanton, and Peng Xu. 2012. A
systematic comparison of phrase table pruning tech-
niques. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
340
pages 972?983, Jeju Island, Korea, July. Association
for Computational Linguistics.
Bowen Zhou, Bing Xiang, Xiaodan Zhu, and Yuqing
Gao. 2008. Prior derivation models for formally
syntax-based translation using linguistically syntactic
parsing and tree kernels. In Proceedings of the ACL-
08: HLT Second Workshop on Syntax and Structure in
Statistical Translation (SSST-2), pages 19?27, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
341
Proceedings of the ACL 2010 System Demonstrations, pages 7?12,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
cdec: A Decoder, Alignment, and Learning Framework for
Finite-State and Context-Free Translation Models
Chris Dyer
University of Maryland
redpony@umd.edu
Adam Lopez
University of Edinburgh
alopez@inf.ed.ac.uk
Juri Ganitkevitch
Johns Hopkins University
juri@cs.jhu.edu
Jonathan Weese
Johns Hopkins University
jweese@cs.jhu.edu
Ferhan Ture
University of Maryland
fture@cs.umd.edu
Phil Blunsom
Oxford University
pblunsom@comlab.ox.ac.uk
Hendra Setiawan
University of Maryland
hendra@umiacs.umd.edu
Vladimir Eidelman
University of Maryland
vlad@umiacs.umd.edu
Philip Resnik
University of Maryland
resnik@umiacs.umd.edu
Abstract
We present cdec, an open source frame-
work for decoding, aligning with, and
training a number of statistical machine
translation models, including word-based
models, phrase-based models, and models
based on synchronous context-free gram-
mars. Using a single unified internal
representation for translation forests, the
decoder strictly separates model-specific
translation logic from general rescoring,
pruning, and inference algorithms. From
this unified representation, the decoder can
extract not only the 1- or k-best transla-
tions, but also alignments to a reference,
or the quantities necessary to drive dis-
criminative training using gradient-based
or gradient-free optimization techniques.
Its efficient C++ implementation means
that memory use and runtime performance
are significantly better than comparable
decoders.
1 Introduction
The dominant models used in machine transla-
tion and sequence tagging are formally based
on either weighted finite-state transducers (FSTs)
or weighted synchronous context-free grammars
(SCFGs) (Lopez, 2008). Phrase-based models
(Koehn et al, 2003), lexical translation models
(Brown et al, 1993), and finite-state conditional
random fields (Sha and Pereira, 2003) exemplify
the former, and hierarchical phrase-based models
the latter (Chiang, 2007). We introduce a soft-
ware package called cdec that manipulates both
classes in a unified way.1
Although open source decoders for both phrase-
based and hierarchical translation models have
been available for several years (Koehn et al,
2007; Li et al, 2009), their extensibility to new
models and algorithms is limited by two sig-
nificant design flaws that we have avoided with
cdec. First, their implementations tightly couple
the translation, language model integration (which
we call rescoring), and pruning algorithms. This
makes it difficult to explore alternative transla-
tion models without also re-implementing rescor-
ing and pruning logic. In cdec, model-specific
code is only required to construct a translation for-
est (?3). General rescoring (with language models
or other models), pruning, inference, and align-
ment algorithms then apply to the unified data
structure (?4). Hence all model types benefit im-
mediately from new algorithms (for rescoring, in-
ference, etc.); new models can be more easily pro-
totyped; and controlled comparison of models is
made easier.
Second, existing open source decoders were de-
signed with the traditional phrase-based parame-
terization using a very small number of dense fea-
tures (typically less than 10). cdec has been de-
signed from the ground up to support any parame-
terization, from those with a handful of dense fea-
tures up to models with millions of sparse features
(Blunsom et al, 2008; Chiang et al, 2009). Since
the inference algorithms necessary to compute a
training objective (e.g. conditional likelihood or
expected BLEU) and its gradient operate on the
unified data structure (?5), any model type can be
trained using with any of the supported training
1The software is released under the Apache License, ver-
sion 2.0, and is available from http://cdec-decoder.org/ .
7
criteria. The software package includes general
function optimization utilities that can be used for
discriminative training (?6).
These features are implemented without com-
promising on performance. We show experimen-
tally that cdec uses less memory and time than
comparable decoders on a controlled translation
task (?7).
2 Decoder workflow
The decoding pipeline consists of two phases. The
first (Figure 1) transforms input, which may be
represented as a source language sentence, lattice
(Dyer et al, 2008), or context-free forest (Dyer
and Resnik, 2010), into a translation forest that has
been rescored with all applicable models.
In cdec, the only model-specific logic is con-
fined to the first step in the process where an
input string (or lattice, etc.) is transduced into
the unified hypergraph representation. Since the
model-specific code need not worry about integra-
tion with rescoring models, it can be made quite
simple and efficient. Furthermore, prior to lan-
guage model integration (and distortion model in-
tegration, in the case of phrase based translation),
pruning is unnecessary for most kinds of mod-
els, further simplifying the model-specific code.
Once this unscored translation forest has been
generated, any non-coaccessible states (i.e., states
that are not reachable from the goal node) are re-
moved and the resulting structure is rescored with
language models using a user-specified intersec-
tion/pruning strategy (?4) resulting in a rescored
translation forest and completing phase 1.
The second phase of the decoding pipeline (de-
picted in Figure 2) computes a value from the
rescored forest: 1- or k-best derivations, feature
expectations, or intersection with a target language
reference (sentence or lattice). The last option
generates an alignment forest, from which a word
alignment or feature expectations can be extracted.
Most of these values are computed in a time com-
plexity that is linear in the number of edges and
nodes in the translation hypergraph using cdec?s
semiring framework (?5).
2.1 Alignment forests and alignment
Alignment is the process of determining if and
how a translation model generates a ?source, tar-
get? string pair. To compute an alignment under
a translation model, the phase 1 translation hyper-
graph is reinterpreted as a synchronous context-
free grammar and then used to parse the target
sentence.2 This results in an alignment forest,
which is a compact representation of all the deriva-
tions of the sentence pair under the translation
model. From this forest, the Viterbi or maximum a
posteriori word alignment can be generated. This
alignment algorithm is explored in depth by Dyer
(2010). Note that if the phase 1 forest has been
pruned in some way, or the grammar does not de-
rive the sentence pair, the target intersection parse
may fail, meaning that an alignment will not be
recoverable.
3 Translation hypergraphs
Recent research has proposed a unified repre-
sentation for the various translation and tagging
formalisms that is based on weighted logic pro-
gramming (Lopez, 2009). In this view, trans-
lation (or tagging) deductions have the structure
of a context-free forest, or directed hypergraph,
where edges have a single head and 0 or more tail
nodes (Nederhof, 2003). Once a forest has been
constructed representing the possible translations,
general inference algorithms can be applied.
In cdec?s translation hypergraph, a node rep-
resents a contiguous sequence of target language
words. For SCFG models and sequential tag-
ging models, a node also corresponds to a source
span and non-terminal type, but for word-based
and phrase-based models, the relationship to the
source string (or lattice) may be more compli-
cated. In a phrase-based translation hypergraph,
the node will correspond to a source coverage vec-
tor (Koehn et al, 2003). In word-based models, a
single node may derive multiple different source
language coverages since word based models im-
pose no requirements on covering all words in the
input. Figure 3 illustrates two example hyper-
graphs, one generated using a SCFG model and
other from a phrase-based model.
Edges are associated with exactly one syn-
chronous production in the source and target lan-
guage, and alternative translation possibilities are
expressed as alternative edges. Edges are further
annotated with feature values, and are annotated
with the source span vector the edge corresponds
to. An edge?s output label may contain mixtures
of terminal symbol yields and positions indicating
where a child node?s yield should be substituted.
2The parser is smart enough to detect the left-branching
grammars generated by lexical translation and tagging mod-
els, and use a more efficient intersection algorithm.
8
SCFG parser
FST transducer
Tagger
Lexical transducer
Phrase-based 
transducer
Source CFG
Source 
sentence
Source lattice
Unscored 
hypergraph
Input Transducers
Cube pruning
Full intersection
FST rescoring
Translation 
hypergraph
Output
Cube growing
No rescoring
Figure 1: Forest generation workflow (first half of decoding pipeline). The decoder?s configuration
specifies what path is taken from the input (one of the bold ovals) to a unified translation hypergraph.
The highlighted path is the workflow used in the test reported in ?7.
Translation 
hypergraph
Target 
reference
Viterbi extraction
k-best extraction
max-translation 
extraction
feature 
expectations
intersection by 
parsing
Alignment 
hypergraph
feature 
expectations
max posterior 
alignment
Viterbi alignment
Translation outputs Alignment outputs
Figure 2: Output generation workflow (second half of decoding pipeline). Possible output types are
designated with a double box.
In the case of SCFG grammars, the edges corre-
spond simply to rules in the synchronous gram-
mar. For non-SCFG translation models, there are
two kinds of edges. The first have zero tail nodes
(i.e., an arity of 0), and correspond to word or
phrase translation pairs (with all translation op-
tions existing on edges deriving the same head
node), or glue rules that glue phrases together.
For tagging, word-based, and phrase-based mod-
els, these are strictly arranged in a monotone, left-
branching structure.
4 Rescoring with weighted FSTs
The design of cdec separates the creation of a
translation forest from its rescoring with a lan-
guage models or similar models.3 Since the struc-
ture of the unified search space is context free (?3),
we use the logic for language model rescoring de-
scribed by Chiang (2007), although any weighted
intersection algorithm can be applied. The rescor-
3Other rescoring models that depend on sequential con-
text include distance-based reordering models or Markov fea-
tures in tagging models.
ing models need not be explicitly represented as
FSTs?the state space can be inferred.
Although intersection using the Chiang algo-
rithm runs in polynomial time and space, the re-
sulting rescored forest may still be too large to rep-
resent completely. cdec therefore supports three
pruning strategies that can be used during intersec-
tion: full unpruned intersection (useful for tagging
models to incorporate, e.g., Markov features, but
not generally practical for translation), cube prun-
ing, and cube growing (Huang and Chiang, 2007).
5 Semiring framework
Semirings are a useful mathematical abstraction
for dealing with translation forests since many
useful quantities can be computed using a single
linear-time algorithm but with different semirings.
A semiring is a 5-tuple (K,?,?, 0, 1) that indi-
cates the set from which the values will be drawn,
K, a generic addition and multiplication operation,
? and ?, and their identities 0 and 1. Multipli-
cation and addition must be associative. Multi-
plication must distribute over addition, and v ? 0
9
Goal
JJ NN
1 2
a
s
m
a
l
l
l
i
t
t
l
e
h
o
u
s
e
s
h
e
l
l
Goal
010
100 101
110
a
s
m
a
l
l
l
i
t
t
l
e
1
a
1
house
1
shell
1
little
1
small
1
house
1
shell
1
little
1
small
Figure 3: Example unrescored translation hypergraphs generated for the German input ein (a) kleines
(small/little) Haus (house/shell) using a SCFG-based model (left) and phrase-based model with a distor-
tion limit of 1 (right).
must equal 0. Values that can be computed using
the semirings include the number of derivations,
the expected translation length, the entropy of the
translation posterior distribution, and the expected
values of feature functions (Li and Eisner, 2009).
Since semirings are such a useful abstraction,
cdec has been designed to facilitate implementa-
tion of new semirings. Table 1 shows the C++ rep-
resentation used for semirings. Note that because
of our representation, built-in types like double,
int, and bool (together with their default op-
erators) are semirings. Beyond these, the type
prob t is provided which stores the logarithm of
the value it represents, which helps avoid under-
flow and overflow problems that may otherwise
be encountered. A generic first-order expectation
semiring is also provided (Li and Eisner, 2009).
Table 1: Semiring representation. T is a C++ type
name.
Element C++ representation
K T
? T::operator+=
? T::operator*=
0 T()
1 T(1)
Three standard algorithms parameterized with
semirings are provided: INSIDE, OUTSIDE, and
INSIDEOUTSIDE, and the semiring is specified us-
ing C++ generics (templates). Additionally, each
algorithm takes a weight function that maps from
hypergraph edges to a value in K, making it possi-
ble to use many different semirings without alter-
ing the underlying hypergraph.
5.1 Viterbi and k-best extraction
Although Viterbi and k-best extraction algorithms
are often expressed as INSIDE algorithms with
the tropical semiring, cdec provides a separate
derivation extraction framework that makes use of
a < operator (Huang and Chiang, 2005). Thus,
many of the semiring types define not only the el-
ements shown in Table 1 but T::operator< as
well. The k-best extraction algorithm is also pa-
rameterized by an optional predicate that can filter
out derivations at each node, enabling extraction
of only derivations that yield different strings as in
Huang et al (2006).
6 Model training
Two training pipelines are provided with cdec.
The first, called Viterbi envelope semiring train-
ing, VEST, implements the minimum error rate
training (MERT) algorithm, a gradient-free opti-
mization technique capable of maximizing arbi-
trary loss functions (Och, 2003).
6.1 VEST
Rather than computing an error surface using k-
best approximations of the decoder search space,
cdec?s implementation performs inference over
the full hypergraph structure (Kumar et al, 2009).
In particular, by defining a semiring whose values
are sets of line segments, having an addition op-
eration equivalent to union, and a multiplication
operation equivalent to a linear transformation of
the line segments, Och?s line search can be com-
puted simply using the INSIDE algorithm. Since
the translation hypergraphs generated by cdec
may be quite large making inference expensive,
the logic for constructing error surfaces is fac-
tored according to the MapReduce programming
paradigm (Dean and Ghemawat, 2004), enabling
parallelization across a cluster of machines. Im-
plementations of the BLEU and TER loss functions
are provided (Papineni et al, 2002; Snover et al,
2006).
10
6.2 Large-scale discriminative training
In addition to the widely used MERT algo-
rithm, cdec also provides a training pipeline for
discriminatively trained probabilistic translation
models (Blunsom et al, 2008; Blunsom and Os-
borne, 2008). In these models, the translation
model is trained to maximize conditional log like-
lihood of the training data under a specified gram-
mar. Since log likelihood is differentiable with
respect to the feature weights in an exponential
model, it is possible to use gradient-based opti-
mization techniques to train the system, enabling
the parameterization of the model using millions
of sparse features. While this training approach
was originally proposed for SCFG-based transla-
tion models, it can be used to train any model
type in cdec. When used with sequential tagging
models, this pipeline is identical to traditional se-
quential CRF training (Sha and Pereira, 2003).
Both the objective (conditional log likelihood)
and its gradient have the form of a difference in
two quantities: each has one term that is com-
puted over the translation hypergraph which is
subtracted from the result of the same computa-
tion over the alignment hypergraph (refer to Fig-
ures 1 and 2). The conditional log likelihood is
the difference in the log partition of the translation
and alignment hypergraph, and is computed using
the INSIDE algorithm. The gradient with respect
to a particular feature is the difference in this fea-
ture?s expected value in the translation and align-
ment hypergraphs, and can be computed using ei-
ther INSIDEOUTSIDE or the expectation semiring
and INSIDE. Since a translation forest is generated
as an intermediate step in generating an alignment
forest (?2) this computation is straightforward.
Since gradient-based optimization techniques
may require thousands of evaluations to converge,
the batch training pipeline is split into map and
reduce components, facilitating distribution over
very large clusters. Briefly, the cdec is run as the
map function, and sentence pairs are mapped over.
The reduce function aggregates the results and per-
forms the optimization using standard algorithms,
including LBFGS (Liu et al, 1989), RPROP (Ried-
miller and Braun, 1993), and stochastic gradient
descent.
7 Experiments
Table 2 compares the performance of cdec, Hi-
ero, and Joshua 1.3 (running with 1 or 8 threads)
decoding using a hierarchical phrase-based trans-
lation grammar and identical pruning settings.4
Figure 4 shows the cdec configuration and
weights file used for this test.
The workstation used has two 2GHz quad-core
Intel Xenon processors, 32GB RAM, is running
Linux kernel version 2.6.18 and gcc version 4.1.2.
All decoders use SRI?s language model toolkit,
version 1.5.9 (Stolcke, 2002). Joshua was run on
the Sun HotSpot JVM, version 1.6.0 12. A hierar-
chical phrase-based translation grammar was ex-
tracted for the NIST MT03 Chinese-English trans-
lation using a suffix array rule extractor (Lopez,
2007). A non-terminal span limit of 15 was used,
and all decoders were configured to use cube prun-
ing with a limit of 30 candidates at each node and
no further pruning. All decoders produced a BLEU
score between 31.4 and 31.6 (small differences are
accounted for by different tie-breaking behavior
and OOV handling).
Table 2: Memory usage and average per-sentence
running time, in seconds, for decoding a Chinese-
English test set.
Decoder Lang. Time (s) Memory
cdec C++ 0.37 1.0Gb
Joshua (1?) Java 0.98 1.5Gb
Joshua (8?) Java 0.35 2.5Gb
Hiero Python 4.04 1.1Gb
formalism=scfg
grammar=grammar.mt03.scfg.gz
add pass through rules=true
scfg max span limit=15
feature function=LanguageModel \
en.3gram.pruned.lm.gz -o 3
feature function=WordPenalty
intersection strategy=cube pruning
cubepruning pop limit=30
LanguageModel 1.12
WordPenalty -4.26
PhraseModel 0 0.963
PhraseModel 1 0.654
PhraseModel 2 0.773
PassThroughRule -20
Figure 4: Configuration file (above) and feature
weights file (below) used for the decoding test de-
scribed in ?7.
4http://sourceforge.net/projects/joshua/
11
8 Future work
cdec continues to be under active development.
We are taking advantage of its modular design to
study alternative algorithms for language model
integration. Further training pipelines are un-
der development, including minimum risk train-
ing using a linearly decomposable approximation
of BLEU (Li and Eisner, 2009), and MIRA train-
ing (Chiang et al, 2009). All of these will be
made publicly available as the projects progress.
We are also improving support for parallel training
using Hadoop (an open-source implementation of
MapReduce).
Acknowledgements
This work was partially supported by the GALE
program of the Defense Advanced Research
Projects Agency, Contract No. HR0011-06-2-001.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views of the
sponsors. Further support was provided the Euro-
Matrix project funded by the European Commis-
sion (7th Framework Programme). Discussions
with Philipp Koehn, Chris Callison-Burch, Zhifei
Li, Lane Schwarz, and Jimmy Lin were likewise
crucial to the successful execution of this project.
References
P. Blunsom and M. Osborne. 2008. Probalistic inference for
machine translation. In Proc. of EMNLP.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrimina-
tive latent variable model for statistical machine transla-
tion. In Proc. of ACL-HLT.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: parameter estimation. Computational Lin-
guistics, 19(2):263?311.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new
features for statistical machine translation. In Proc. of
NAACL, pages 218?226.
D. Chiang. 2007. Hierarchical phrase-based translation.
Comp. Ling., 33(2):201?228.
J. Dean and S. Ghemawat. 2004. MapReduce: Simplified
data processing on large clusters. In Proc. of the 6th Sym-
posium on Operating System Design and Implementation
(OSDI 2004), pages 137?150.
C. Dyer and P. Resnik. 2010. Context-free reordering, finite-
state translation. In Proc. of HLT-NAACL.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing
word lattice translation. In Proc. of HLT-ACL.
C. Dyer. 2010. Two monolingual parses are better than one
(synchronous parse). In Proc. of HLT-NAACL.
L. Huang and D. Chiang. 2005. Better k-best parsing. In In
Proc. of IWPT, pages 53?64.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Proc. ACL.
L. Huang, K. Knight, and A. Joshi. 2006. A syntax-directed
translator with extended domain of locality. In Proc. of
AMTA.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT/NAACL, pages 48?54.
P. Koehn, H. Hoang, A. B. Mayne, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst.
2007. Moses: Open source toolkit for statistical ma-
chine translation. In Proc. of ACL, Demonstration Ses-
sion, pages 177?180, June.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Efficient
minimum error rate training and minimum B ayes-risk de-
coding for translation hypergraphs and lattices. In Proc.
of ACL, pages 163?171.
Z. Li and J. Eisner. 2009. First- and second-order expectation
semirings with applications to minimum-risk training on
translation forests. In Proc. of EMNLP, pages 40?51.
Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch, S. Khu-
danpur, L. Schwartz, W. N. G. Thornton, J. Weese, and
O. F. Zaidan. 2009. Joshua: an open source toolkit for
parsing-based machine translation. In Proc. of the Fourth
Workshop on Stat. Machine Translation, pages 135?139.
D. C. Liu, J. Nocedal, D. C. Liu, and J. Nocedal. 1989. On
the limited memory BFGS method for large scale opti-
mization. Mathematical Programming B, 45(3):503?528.
A. Lopez. 2007. Hierarchical phrase-based translation with
suffix arrays. In Proc. of EMNLP, pages 976?985.
A. Lopez. 2008. Statistical machine translation. ACM Com-
puting Surveys, 40(3), Aug.
A. Lopez. 2009. Translation as weighted deduction. In Proc.
of EACL, pages 532?540.
M.-J. Nederhof. 2003. Weighted deductive parsing and
Knuth?s algorithm. Comp. Ling., 29(1):135?143, Mar.
F. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. of ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311?318.
M. Riedmiller and H. Braun. 1993. A direct adaptive method
for faster backpropagation learning: The RPROP algo-
rithm. In Proc. of the IEEE international conference on
neural networks, pages 586?591.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of NAACL, pages 134?141.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proc. AMTA.
A. Stolcke. 2002. SRILM ? an extensible language modeling
toolkit. In Intl. Conf. on Spoken Language Processing.
12
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1264?1274,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Two-Neighbor Orientation Model with Cross-Boundary Global Contexts
Hendra Setiawan, Bowen Zhou, Bing Xiang and Libin Shen
IBM T.J.Watson Research Center
1101 Kitchawan Road
Yorktown Heights, NY 10598, USA
{hendras,zhou,bxiang,lshen}@us.ibm.com
Abstract
Long distance reordering remains one of
the greatest challenges in statistical ma-
chine translation research as the key con-
textual information may well be beyond
the confine of translation units. In this
paper, we propose Two-Neighbor Orien-
tation (TNO) model that jointly models
the orientation decisions between anchors
and two neighboring multi-unit chunks
which may cross phrase or rule bound-
aries. We explicitly model the longest
span of such chunks, referred to as Max-
imal Orientation Span, to serve as a
global parameter that constrains under-
lying local decisions. We integrate our
proposed model into a state-of-the-art
string-to-dependency translation system
and demonstrate the efficacy of our pro-
posal in a large-scale Chinese-to-English
translation task. On NIST MT08 set, our
most advanced model brings around +2.0
BLEU and -1.0 TER improvement.
1 Introduction
Long distance reordering remains one of the great-
est challenges in Statistical Machine Translation
(SMT) research. The challenge stems from the
fact that an accurate reordering hinges upon the
model?s ability to make many local and global
reordering decisions accurately. Often, such
reordering decisions require contexts that span
across multiple translation units.1 Unfortunately,
previous approaches fall short in capturing such
cross-unit contextual information that could be
1We define translation units as phrases in phrase-based
SMT, and as translation rules in syntax-based SMT.
critical in reordering. Specifically, the popular dis-
tortion or lexicalized reordering models in phrase-
based SMT focus only on making good local pre-
diction (i.e. predicting the orientation of imme-
diate neighboring translation units), while transla-
tion rules in syntax-based SMT come with a strong
context-free assumption, which model only the re-
ordering within the confine of the rules. In this
paper, we argue that reordering modeling would
greatly benefit from richer cross-boundary contex-
tual information
We introduce a reordering model that incorpo-
rates such contextual information, named the Two-
Neighbor Orientation (TNO) model. We first iden-
tify anchors as regions in the source sentences
around which ambiguous reordering patterns fre-
quently occur and chunks as regions that are con-
sistent with word alignment which may span mul-
tiple translation units at decoding time. Most no-
tably, anchors and chunks in our model may not
necessarily respect the boundaries of translation
units. Then, we jointly model the orientations of
chunks that immediately precede and follow the
anchors (hence, the name ?two-neighbor?) along
with the maximal span of these chunks, to which
we refer as Maximal Orientation Span (MOS).
As we will elaborate further in next sections,
our models provide a stronger mechanism to make
more accurate global reordering decisions for the
following reasons. First of all, we consider the
orientation decisions on both sides of the anchors
simultaneously, in contrast to existing works that
only consider one-sided decisions. In this way, we
hope to upgrade the unigram formulation of exist-
ing reordering models to a higher order formula-
tion. Second of all, we capture the reordering of
chunks that may cross translation units and may
be composed of multiple units, in contrast to ex-
1264
isting works that focus on the reordering between
individual translation units. In effect, MOS acts as
a global reordering parameter that guides or con-
strains the underlying local reordering decisions.
To show the effectiveness of our model, we
integrate our TNO model into a state-of-the-
art syntax-based SMT system, which uses syn-
chronous context-free grammar (SCFG) rules to
jointly model reordering and lexical translation.
The introduction of nonterminals in the SCFG
rules provides some degree of generalization.
However as mentioned earlier, the context-free
assumption ingrained in the syntax-based for-
malism often limits the model?s ability to in-
fluence global reordering decision that involves
cross-boundary contexts. In integrating TNO, we
hope to strengthen syntax-based system?s ability
to make more accurate global reordering deci-
sions.
Our other contribution in this paper is a prac-
tical method for integrating the TNO model into
syntax-based translations. The integration is non-
trivial since the decoding of syntax-based SMT
proceeds in a bottom-up fashion, while our model
is more natural for top-down parsing, thus the
model?s full context sometimes is often available
only at the latest stage of decoding. We implement
an efficient shift-reduce algorithm that facilitates
the accumulation of partial context in a bottom-up
fashion, allowing our model to influence the trans-
lation process even in the absence of full context.
We show the efficacy of our proposal in a large-
scale Chinese-to-English translation task where
the introduction of our TNO model provides a
significant gain over a state-of-the-art string-to-
dependency SMT system (Shen et al, 2008) that
we enhance with additional state-of-the-art fea-
tures. Even though the experimental results car-
ried out in this paper employ SCFG-based SMT
systems, we would like to point out that our mod-
els is applicable to other systems including phrase-
based SMT systems.
The rest of the paper is organized as follows.
In Section 2, we introduce the formulation of our
TNO model. In Section 3, we introduce and moti-
vate the concept of Maximal Orientation Span. In
Section 4, we introduce four variants of the TNO
model with different model complexities. In Sec-
tion 5, we describe the training procedure to esti-
mate the parameters of our models. In Section 6,
we describe our shift-reduce algorithm which inte-
grates our proposed TNO model into syntax-based
SMT. In Section 7, we describe our experiments
and present our results. We wrap up with related
work in Section 8 and conclusion in Section 9.
2 Two-Neighbor Orientation Model
Given an aligned sentence pair ? = (F,E,?), let
?(?) be all possible chunks that can be extracted
from ? according to: 2
{(f j2j1/e
i2
i1) :?j1? j? j2,?i : (j, i)??, ii? i? i2 ?
?i1? i? i2,?j : (j, i)??, ji? j?j2}
Our Two-Neighbor Orientation model (TNO)
designatesA ? ?(?) as anchors and jointly mod-
els the orientation of chunks that appear immedi-
ately to the left and to the right of the anchors as
well as the identities of these chunks. We define
anchors as chunks, around which ambiguous re-
ordering patterns frequently occur. Anchors can
be learnt automatically from the training data or
identified from the linguistic analysis of the source
sentence. In our experiments, we use a simple
heuristics based on part-of-speech tags which will
be described in Section 7.
More concretely, given A ? ?(?), let a =
(f j2j1/e
i2
i1) ? A be a particular anchor. Then, let
CL(a) ? ?(?) be a?s left neighbors and let
CR(a) ? ?(?) be a?s right neighbors, iff:
?CL = (f j4j3/e
i4
i3) ? CL(a) : j4 + 1 = j1 (1)
?CR = (f j6j5/e
i6
i5) ? CR(a) : j2 + 1 = j5 (2)
Given CL(a) and CR(a), let CL = (f j4j3/ei4i3) and
CR = (f j6j5/e
i6
i5) be a particular pair of left and right
neighbors of a = (f j2j1/ei2i1). Then, the orientationof CL and CR are OL(CL, a) and OR(CR, a) re-
spectively and each may take one of the following
four orientation values (similar to (Nagata et al,
2006)):
? Monotone Adjacent (MA), if (i4 + 1) = i1
for OL and if (i2 + 1) = i5 for OR
? Reverse Adjacent (RA), if (i2 + 1) = i3 for
OL and if (i6 + 1) = i1 for OR
? Monotone Gap (MG), if (i4 + 1) < i1 for
OL and if (i2 + 1) < i5 for OR
2We represent a chunk as a source and target phrase pair
(f j2j1/ei2i1 ) where the subscript and the superscript indicate the
starting and the ending indices as such f j2j1 denotes a sourcephrase that spans from j1 to j2.
1265
Figure 1: An aligned Chinese-English sentence pair. Circles
represent alignment points. Black circle represents the an-
chor; boxes represent the anchor?s neighbors.
? Reverse Gap (RG), if (i2 + 1) < i3 for OL
and if (i6 + 1) < i1 for OR. (1)
The first clause (monotone, reverse) indicates
whether the target order of the chunks follows the
source order; the second (adjacent, gap) indicates
whether the chunks are adjacent or separated by an
intervening phrase when projected.
To be more concrete, let us consider an aligned
sentence pair in Fig. 1, which is adapted from
(Chiang, 2005). Suppose there is only one anchor,
i.e. a = (f77 /e77) which corresponds to the word
de(that). By applying Eqs. 1 and 2, we can infer
that a has three left neighbors and four right neigh-
bors, i.e. CL(a) = (f66 /e99), (f65 /e98), (f63 /e118 ) and
CR(a) = (f88 /e55), (f98 /e65), (f108 /e64), (f118 /e63)
respectively. Then, by applying Eq.
1, we can compute the orientation val-
ues of each of these neighbors, which
are OL(CL(a), a) = RG,RA,RA and
OR(CR(a), a) = RG,RA,RA,RA. As shown,
most of the neighbors have Reverse Adjacent
(RA) orientation except for the smallest left and
right neighbors (i.e. (f66 /e99) and (f88 /e55)) which
have Reverse Gap (RG) orientation.
Given the anchors together with its neighboring
chunks and their orientations, the Two-Neighbor
Orientation model takes the following form:
?
a?A
?
CL?CL(a),
CR?CR(a)
PTNO(CL, OL, CR, OR|a; ?) (2)
For conciseness, references that are clear from
context, such the reference to CL and a in
OL(CL, a), are dropped.
3 Maximal Orientation Span
As shown in Eq. 2, the TNO model has to enu-
merate all possible pairing of CL ? CL(a) and
CR ? CR(a). To make the TNO model more
tractable, we simplify the TNO model to consider
only the largest left and right neighbors, referred
to as the Maximal Orientation Span/MOS (M ).
More formally, given a = (f j2j1/ei2i1), the left andthe right MOS of a are:
ML(a) = arg max
(fj4j3 /e
i4
i3 )?CL(a)
(j4 ? j3)
MR(a) = arg max
(fj6j5 /e
i6
i5 )?CR(a)
(j6 ? j5)
Coming back to our example, the left and right
MOS of the anchor are ML(a) = (f63 /e118 ) and
MR(a) = (f118 /e63). In Fig. 1, they are denoted as
the largest boxes delineated by solid lines.
As such, we reformulate Eq. 2 into:
?
a?A
?
CL?CL(a),
CR?CR(a)
PTNO(ML, OL,MR, OR|a; ?).?CL==ML?
CR==MR
(3)
where ? returns 1 if (CL == ML ?CR == MR),
otherwise 0.
Beyond simplifying the computation, the key
benefit of modeling MOS is that it serves as a
global parameter that can guide or constrain un-
derlying local reorderings. As a case in point, let
us consider a cheating exercise where we have to
translate the Chinese sentence in Fig. 1 with the
following set of hierarchical phrases3:
Xa??Aozhou1shi2X1,Australia1 is2X1?
Xb??yu3 Beihan4X1, X1with3 North4 Korea?
Xc??you5bangjiao6, have5dipl.6 rels.?
Xd??X1de7shaoshu8 guojia9 zhi10 yi11,
one11of10the few8 countries9 that7X1?
This set of hierarchical phrases represents a trans-
lation model that has resolved all local ambiguities
(i.e. local reordering and lexical mappings) except
for the spans of the hierarchical phrases. With this
example, we want to show that accurate local de-
cisions (rather obviously) don?t always lead to ac-
curate global reordering and to demonstrate that
explicit MOS modeling can play a crucial role to
address this issue. To do so, we will again focus
on the same anchor de (that).
3We use hierarchical phrase-based translation system as a
case in point, but the merit is generalizable to other systems.
1266
d? ?X1de7shaoshu8 guojia9 zhi10 yi11?, ?one11of10the few8 countries9 that7X1?
a? ??Aozhou1shi2X1?de7shaoshu8 guojia9 zhi10 yi11?,
?one11of10the few8 countries9 that7?Australia1 is2X1??
b? ??Aozhou1shi2 ?yu3 Beihan4X1??de7shaoshu8 guojia9 zhi10 yi11?,
?one11of10the few8 countries9 that7?Australia1 is2?X1with3 North4 Korea???
c? ?d ?aAozhou1shi2 ?byu3 Beihan4 ?cyou5bangjiao6?c?b?ade7shaoshu8 guojia9 zhi10 yi11 ?d ,
?one11of10the few8 countries9 that7?Australia1 is2??have5dipl.6 rels.?with3 North4 Korea???
Table 1: Derivation of Xd ?Xa ?Xb ?Xc that leads to an incorrect translation.
a? ?Aozhou1shi2X1?, ?Australia1 is2X1?
b? ?Aozhou1shi2?yu3Beihan4X1??, ?Australia1 is2?X1with3 North4 Korea??
d? ?Aozhou1shi2?yu3Beihan4?X1de7shaoshu8 guojia9 zhi10 yi11???,
?Australia1 is2??one11of10the few8 countries9 that7X1?with3 North4 Korea??
c? ?aAozhou1shi2?byu3Beihan4 ?d ?cyou5bangjiao6?cde7shaoshu8 guojia9 zhi10 yi11 ?d ?b?a,
Australia1 is2??one11of10the few8 countries9 that7?have5dipl.6??with3 North4 Korea??
Table 2: Derivation of Xa ?Xb ?Xd ?Xc that leads to the correct translation.
As the rule?s identifier, we attach an alphabet
letter to each rule?s left hand side, as such the an-
chor de (that) appears in rule Xd. We also attach
the word indices as the superscript of the source
words and project the indices to the target words
aligned, as such ?have5? suggests that the word
?have? is aligned to the 5-th source word, i.e. you.
Note that to facilitate the projection, the rules must
come with internal word alignment in practice.
Now the indices on the target words in the rules
are different from those in Fig. 1. We will also
extensively use indices in this sense in the sub-
sequent section about decoding. In such a sense,
ML(a) = (f63 /e63) and MR(a) = (f118 /e118 ).
Given the rule set, there are three possible
derivations, i.e. Xd ?Xa ?Xb ?Xc,Xa ?Xb ?
Xd ?Xc, and Xa ?Xd ?Xb ?Xc, where ? in-
dicates that the first operand dominates the second
operand in the derivation tree. The application of
the rules would show that the first derivation will
produce an incorrect reordering while the last two
will produce the correct ones. Here, we would like
to point out that even in this simple example where
all local decisions are made accurate, this ambigu-
ity occurs and it would occur even more so in the
real translation task where local decisions may be
highly inaccurate.
Next, we will show that the MOS-related in-
formation can help to resolve this ambiguity, by
focusing more closely on the first and the second
derivations, which are detailed in Tables 1 and 2.
Particularly, we want to show that the MOS gen-
erated by the incorrect derivation does not match
the MOS learnt from Fig. 1. As shown, at the
end of the derivation, we have all the informa-
tion needed to compute the MOS (i.e. ?) which is
equivalent to that available at training time, i.e. the
source sentence, the complete translation and the
word alignment. Running the same MOS extrac-
tion procedure on both derivations would produce
the right MOS that agrees with the right MOS pre-
viously learnt from Fig. 1, i.e. (f118 /e118 ). How-
ever, that?s not the case for left MOS, which we
underline in Tables 1 and 2. As shown, the incor-
rect derivation produces a left MOS that spans six
words, i.e. (f61 /e61), while the correct derivation
produces a left MOS that spans four words, i.e.
(f63 /e63). Clearly, the MOS of the incorrect deriva-
tion doesn?t agree with the MOS we learnt from
Fig. 1, unlike the MOS of the correct translation.
This suggests that explicit MOS modeling would
provide a mechanism for resolving crucial global
reordering ambiguities that are beyond the ability
of local models.
Additionally, this illustration also shows a case
where MOS acts as a cross-boundary context
which effectively relaxes the context-free assump-
tion of hierarchical phrase-based formalism. In
Tables 1 and 2?s full derivations, we indicate rule
boundaries explicitly by indexing the angle brack-
ets, e.g. ?a indicates the beginning of rule Xa in
the derivation. As the anchor appears in Xd, we
1267
highlight its boundaries in box frames. de (that)?s
MOS respects rule boundaries if and only if all
the words come entirely from Xd?s antecedent or
?d and ?d appears outside of MOS; otherwise it
crosses the rule boundaries. As clearly shown in
Table 2, the left MOS of the correct derivation (un-
derlined) crosses the rule boundary (of Xd) since
?d appears within the MOS.
Going back to the formulation, focusing on
modeling MOS would simplify the formulation of
TNO model from Eq. 2 into:
?
a?A
PTNO(ML, OL,MR, OR|a; ?) (4)
which doesn?t require enumerating of all possible
pairs of CL and CR.
4 Model Decomposition and Variants
To make the model more tractable, we decompose
PTNO in Eq. 4 into the following four factors:
P (MR|a)? P (OR|MR, a)? P (ML|OR,MR, a)
? P (OL|ML, OR,MR, a). Subsequently, we will
refer to them as PMR , POR , PML and POL respec-
tively. Each of these factors will act as an addi-
tional feature in the log-linear framework of our
SMT system. The above decomposition follows
a generative story that starts from generating the
right neighbor first. There are other equally credi-
ble alternatives, but based on empirical results, we
settle with the above.
Next, we present four different variants of the
model (not to be confused with the four factors
above). Each variant has a different probabilistic
conditioning of the factors. We start by making
strong independence assumptions in Model 1 and
then relax them as we progress to Model 4. The
description of the models is as follow:
? Model 1. We assume PML and PMR to be
equal to 1 and POR ? P (OR|a; ?) to be in-
dependent of MR and POL ? P (OL|a; ?) to
be in independent of ML,MR and OR.
? Model 2. On top of Model 1, we
make POL dependent on POR , thus
POL?P (OL|OR, a; ?).
? Model 3. On top of Model 2, we make POR
dependent on MR and POL on MR and ML,
thus POR ? P (OR|MR, a; ?) and POL ?
P (OL|ML, OR,MR; a,?) .
? Model 4. On top of Model 3, we model PMR
and PML as multinomial distributions esti-
mated from training data.
Model 1 represents a model that focuses on
making accurate one-sided decisions, independent
of the decision on the other side. Model 2 is
designed to address the deficiency of Model 1
since Model 1 may assign non-zero probability to
improbable assignment of orientation values, e.g.
Monotone Adjacent for the left neighbor and Re-
verse Adjacent for the right neighbor. Model 2
does so by conditioning POL on OR. In Model 3,
we start incorporating MOS-related information in
predicting OL and OR. In Model 4, we explicitly
model the MOS of each anchor.
5 Training
The TNO model training consists of two differ-
ent training regimes: 1) discriminative for train-
ing POL ,POR ; and 2) generative for training PML ,
PMR . Before describing the specifics, we start by
describing the procedure to extract anchors and
their corresponding MOS from training data, from
which we collect statistics and extract features to
train the model.
For each aligned sentence pair (F,E,?) in the
training data, the training starts with the iden-
tification of the regions in the source sentences
as anchors (A). For our Chinese-English experi-
ments, we use a simple heuristic that equates as
anchors, single-word chunks whose corresponding
word class belongs to closed-word classes, bear-
ing a close resemblance to (Setiawan et al, 2007).
In total, we consider 21 part-of-speech tags; some
of which are as follow: VC (copula), DEG, DEG,
DER, DEV (de-related), PU (punctuation), AD
(adjectives) and P (prepositions).
Next we generate all possible chunks ?(?)
as previously described in Sec. 3. We then de-
fine a functionMinC(?, j1, j2) which returns the
shortest chunk that can span from j1 to j2. If
(f j2j1 /e
i2
i1) ? ?, then MinC returns (f j2j1 /ei2i1).The algorithm to extract MOS takes ? and an
anchor a = (f j2j1 /ei2i1) as input; and outputs thechunk that qualifies as MOS or none. Alg. 1
provides the algorithm to extract the right MOS;
the algorithm to extract the left MOS is identical
to Alg. 1, except that it scans for chunks to the
left of the anchor. In Alg. 1, there are two in-
termediate parameters si and ei which represent
the active search range and should initially be set
to j2 + 1 and |F | respectively. Once we obtain
a,ML(a) andMR(a), we computeOL(ML(a), a)
and OR(MR(a), a) and are ready for training.
1268
To estimate POL and POR , we train discrimi-
native classifiers that predict the orientation val-
ues and use the normalized posteriors at decoding
time as additional feature scores in SMT?s log lin-
ear framework. We train the classifiers on a rich
set of binary features ranging from lexical to part-
of-speech (POS) and to syntactic features.
Algorithm 1: Function MREx
input : a = (f j2j1 /ei2i1), si, ei: int; ?: chunks
output: (f j4j3 /ei4i3) : chunk or ?
(f j4j3 /e
i4
i3) = MinC(?, j2 + 1, ei)
if (j3 == j2 + 1 ? j4 == ei) then
? f j4j3 /e
i4
i3
else
if (j2 + 1 == ei) then
? ?
else
if (ei-2 ? si) then
?MREx(a, si, ei? 1,?)
else
m = d(si+ei)/2e
(f j4j3 /e
i4
i4) = MinC(?, j2 + 1,m)
if (j3 == j2 + 1) then
c = MREx(a,m, ei? 1,?)
if (c == ?) then
? f j4j3 /e
i4
i3
else
? c
end
else
?MREx(a, si,m? 1,?)
end
end
end
end
Suppose a = (f j2j1 /ei2i1), ML(a) = (f j4j3 /ei4i3)
and ML(a) = (f j6j5 /ei6i5), then based on the con-text?s location, the elementary features employed
in our classifiers can be categorized into:
1. anchor-related: slex (the actual word of
f j2j1 ), spos (part-of-speech (POS) tag of
slex), sparent (spos?s parent in the parse
tree), tlex (ei2i1?s actual target word)..
2. surrounding: lslex (the previous word /
f j1?1j1?1 ), rslex (the next word / f j2+1j2+1 ), lspos(lslex?s POS tag), rspos (rslex?s POS
tag), lsparent (lslex?s parent), rsparent
(rslex?s parent).
3. non-local: lanchorslex (the previous
anchor?s word) , ranchorslex (the next an-
chor?s word), lanchorspos (lanchorslex?s
POS tag), ranchorspos (ranchorslex?s
POS tag).
4. MOS-related: mosl int slex (the actual
word of f j3j3 ), mosl ext slex (the actual word
of f j3j3 ), mosl int spos (mosl int slex?sPOS tag), mosl ext spos (mosl ext spos?s
POS tag), mosr int slex (the actual word of
f j3j3 ), mosr ext slex (the actual word of f j3j3 ),
mosr int spos (mosr int slex?s POS tag),
mosr ext spos (mosr ext spos?s POS tag).
For Model 1, we train one classifier each for
POR and POL . For Model 2-4, we train four clas-
sifiers for POL for each value of OR. We use only
the MOS features for Model 3 and 4. Addition-
ally, we augment the feature set with compound
features, e.g. conjunction of the lexical of the an-
chor and the lexical of the left and the right an-
chors. Although they increase the number of fea-
tures significantly, we found that these compound
features are empirically beneficial.
We come up with > 50 types of features, which
consist of a combination of elementary and com-
pound features. In total, we generate hundreds of
millions of such features from the training data.
To keep the number features to a manageable size,
we employ the L1-regularization in training to en-
force sparse solutions, using the off-the-shelf LIB-
LINEAR toolkit (Fan et al, 2008). After training,
the number of features in our classifiers decreases
to below 5 million features for each classifier.
We train PML and PMR via the relative fre-
quency principle. To avoid the sparsity issue, we
represent ML as (mosl int spos,mosl ext spos)
and MR as (mosr int spos,mosr ext spos). We
condition PML and PMR only on spos and the ori-
entation, estimating them as follow:
P (ML|spos, OL) =
N(ML, spos, OL)
N(spos, OL)
P (MR|spos, OR) =
N(MR, spos, OR)
N(spos, OR)
where N returns the count of the events in the
training data.
1269
Target string (w/ source index) Symbol(s) read Op. Stack(s)
(1) Xc have5 dipl.6 rels. [5][6] S,S,R Xc:[5-6]
(2) Xd one11 of10 few8 countries9 [11][10] S,S,R [10-11]
that7 Xc
(3) [8][9] S,S,R,R [8-11]
(4) [7] S [8-11][7]
(5) Xc:[5,6] S Xd:[8-11][7][5,6]
(6) Xb Xd with3 North4 Korea Xd:[8-11][7][5,6] S [8-11][7][5,6]
(7) [3][4] S,S,R,R Xb:[8-11][7][3-6]
(8) Xa Australia1 is2 Xb [1][2] S,S,R [1-2]
(9) Xb:[8-11][7][3,6] S,A Xa:[1-2][8-11][7][3,6]
Table 3: The application of the shift-reduce parsing algorithm, which corresponds to Table 2?s derivation.
6 Decoding
Integrating the TNO Model into syntax-based
SMT systems is non-trivial, especially with the
MOS modeling. The method described in Sec. 3
assumes ? = (F,E,?), thus it is only applicable
at training or at the last stage of decoding. Since
many reordering decisions may have been made
at the earlier stages, the late application of TNO
model would limit the utility of the model. In this
section, we describe an algorithm that facilitates
the incremental construction of MOS and the com-
putation of TNO model on partial derivations.
The algorithm bears a close resemblance to the
shift-reduce algorithm where a stack is used to ac-
cumulate (partial) information about a, ML and
MR for each a ? A in the derivation. This al-
gorithm takes an input stream and applies either
the shift or the reduce operations starting from the
beginning until the end of the stream. The shift op-
eration advances the input stream by one symbol
and push the symbol into the stack; while the re-
duce operation applies some reduction rule to the
topmost elements of the stack. The algorithm ter-
minates at the end of the input stream where the
resulting stack will be propagated to the parent for
the later stage of decoding. In our case, the in-
put stream is the target string of the rule and the
symbol is the corresponding source index of the
elements of the target string. The reduction rule
looks at two indices and merge them if they are
adjacent (i.e. has no intervening phrase). We for-
bid the application of the reduction rule to anchors.
Table 3 shows the execution trace of the algorithm
for the derivation described in Table 2.
As shown, the algorithm starts with an empty
stack. It then projects the source index to the cor-
responding target word and then enumerates the
target string in a left to right fashion. If it finds
a target word with a source index, it applies the
shift operation, pushing the index to the stack. Un-
less the symbol corresponds to an anchor, it tries
to apply the reduce operation. Line (4) indicates
the special treatment to the anchor. If the symbol
read is a nonterminal, then we push the entire stack
that corresponds to that nonterminal. For example,
when the algorithm reads Xd at line (6), it pushes
the entire stack from line (5).
This algorithm facilitates the incremental con-
struction of MOS which may cross rule bound-
aries. For example, at the end of the application of
Xd at line (5), the current left MOS is [5-6]. How-
ever, the algorithm grows it to [3-6] after the appli-
cation of ruleXb at line (7). Furthermore, it allows
us to compute the models from partial hypothesis.
For example, at line (5), we can compute POL by
considering [5,6] as ML to be updated with [3,6]
in line (7). This way, we expect our TNO model
would play a bigger role at decoding time.
Specific to SCFG-based translation, the values
of OL and OR are identical in the partial or in
the full derivations. For example, the orientation
values of de (that)?s left neighbor is always RA.
This statement holds, even though at the end of
Section 2, we stated that de (that)?s left neigh-
bor may have other orientation values, i.e. RG
for CL(a) = (f66 /e99). The formal proof is omit-
ted, but the intuition comes from the fact that the
derivations for SCFG-based translation are sub-
set of ?(?) and that (f66 /e99) will never become
ML forMinC(CL(a), a) respectively (chunk that
spans a and CL). Consequently, for Model 1 and
Model 2, we can obtain the model score earlier in
the decoding process.
1270
7 Experiments
Our baseline systems is a state-of-the-art string-
to-dependency system (Shen et al, 2008). The
system is trained on 10 million parallel sentences
that are available to the Phase 1 of the DARPA
BOLT Chinese-English MT task. The training cor-
pora include a mixed genre of newswire, weblog,
broadcast news, broadcast conversation, discus-
sion forums and comes from various sources such
as LDC, HK Law, HK Hansard and UN data.
In total, our baseline model employs about
40 features, including four from our proposed
Two-Neighbor Orientation model. In addition to
the standard features including the rule transla-
tion probabilities, we incorporate features that are
found useful for developing a state-of-the-art base-
line, such as the provenance features (Chiang et
al., 2011). We use a large 6-gram language model,
which was trained on 10 billion English words
from multiple corpora, including the English side
of our parallel corpus plus other corpora such as
Gigaword (LDC2011T07) and Google News. We
also train a class-based language model (Chen,
2009) on two million English sentences selected
from the parallel corpus. As the backbone of
our string-to-dependency system, we train 3-gram
models for left and right dependencies and un-
igram for head using the target side of the bi-
lingual training data. To train our Two-Neighbor
Orientation model, we select a subset of 5 million
aligned sentence pairs.
For the tuning and development sets, we set
aside 1275 and 1239 sentences selected from
LDC2010E30 corpus. We tune the decoding
weights with PRO (Hopkins and May, 2011) to
maximize BLEU-TER. As for the blind test set,
we report the performance on the NIST MT08
evaluation set, which consists of 691 sentences
from newswire and 666 sentences from weblog.
We pick the weights that produce the highest de-
velopment set scores to decode the test set.
Table 4 summarizes the experimental results on
NIST MT08 newswire and weblog. In column 2,
we report the classification accuracy on a subset of
training data. Note that these numbers are for ref-
erence only and not directly comparable with each
other since the features used in these classifiers
include several gold standard information, such
as the anchors? target words, the anchors? MOS-
related features (Model 3 & 4) and the orientation
of the right MOS (Model 2-4); all of which have
Acc MT08 nw MT08 wbBLEU TER BLEU TER
S2D - 36.77 53.28 26.34 57.41
M1 72.5 37.60 52.70 27.59 56.33
M2 77.4 37.86 52.68 27.74 56.11
M3 84.5 38.02 52.42 28.22 55.82
M4 84.5 38.55 52.41 28.44 56.45
Table 4: The NIST MT08 results on newswire (nw) and we-
blog (wb) genres. S2D is the baseline string-to-dependency
system (line 1), on top of which Two-Neighbor Orientation
Model 1 to 4 are employed (line 2-5). The best TER and
BLEU results on each genre are in bold. For BLEU, higher
scores are better, while for TER, lower scores are better.
to be predicted at decoding time.
In columns 2 and 4, we report the BLEU scores,
while in columns 3 and 5, we report the TER
scores. The performance of our baseline string-
to-dependency syntax-based SMT is shown in the
first line, followed by the performance of our Two-
Neighbor Orientation model starting from Model
1 to Model 4. As shown, the empirical results
confirm our intuition that SMT can greatly benefit
from reordering model that incorporate cross-unit
contextual information.
Model 1 provides most of the gain across the
two genres of around +0.9 to +1.2 BLEU and -0.5
to -1.1 TER. Model 2 which conditions POL on
OR provides an additional +0.2 BLEU improve-
ment on BLEU score consistently across the two
genres. As shown in line 4, we see a stronger
improvement in the inclusion of MOS-related in-
formation as features in Model 3. In newswire,
Model 3 gives an additional +0.4 BLEU and -0.2
TER, while in weblog, it gives a stronger improve-
ment of an additional +0.5 BLEU and -0.3 TER.
The inclusion of explicit MOS modeling in Model
4 gives a significant BLEU score improvement of
+0.5 but no TER improvement in newswire. In
weblog, Model 4 gives a mixed results of +0.2
BLEU score improvement and a hit of +0.6 TER.
We conjecture that the weblog text has a more am-
biguous orientation span that are more challenging
to learn. In total, our TNO model gives an encour-
aging result. Our most advanced model gives sig-
nificant improvement of +1.8 BLEU/-0.8 TER in
newswire domain and +2.1 BLEU/-1.0 TER over
a strong string-to-dependency syntax-based SMT
enhanced with additional state-of-the-art features.
1271
8 Related Work
Our work intersects with existing work in many
different respects. In this section, we mainly focus
on work related to the probabilistic conditioning
of our TNO model and the MOS modeling.
Our TNO model is closely related to the Uni-
gram Orientation Model (UOM) (Tillman, 2004),
which is the de facto reordering model of phrase-
based SMT (Koehn et al, 2007). UOM views
reordering as a process of generating (b, o) in a
left-to-right fashion, where b is the current phrase
pair and o is the orientation of b with the pre-
viously generated phrase pair b?. UOM makes
strong independence assumptions and formulates
the model as P (o|b). Tillmann and Zhang (2007)
proposed a Bigram Orientation Model (BOM) to
include both phrase pairs (b and b?) into the model.
Their original intent is to model P (o, b|o?, b?), but
perhaps due to sparsity concerns, they settle with
P (o|b, b?), dropping the conditioning on the pre-
vious orientation o?. Subsequent improvements
use the P (o|b, b?) formula, for example, for in-
corporating various linguistics feature like part-of-
speech (Zens and Ney, 2006), syntactic (Chang et
al., 2009), dependency information (Bach et al,
2009) and predicate-argument structure (Xiong et
al., 2012). Our TNO model is more faithful to the
BOM?s original formulation.
Our MOS concept is also closely related to hi-
erarchical reordering model (Galley and Manning,
2008) in phrase-based decoding, which computes
o of b with respect to a multi-block unit that may
go beyond b?. They mainly use it to avoid overes-
timating ?discontiguous? orientation but fall short
in modeling the multi-block unit, perhaps due to
data sparsity issue. Our MOS is also closely re-
lated to the efforts of modeling the span of hi-
erarchical phrases in formally syntax-based SMT.
Early works reward/penalize spans that respect the
syntactic parse constituents of an input sentence
(Chiang, 2005), and (Marton and Resnik, 2008).
(Xiong et al, 2009) learn the boundaries from
parsed and aligned training data, while (Xiong et
al., 2010) learn the boundaries from aligned train-
ing data alone. Recent work couples span mod-
eling tightly with reordering decisions, either by
adding an additional feature for each hierarchical
phrase (Chiang et al, 2008; Shen et al, 2009) or
by refining the nonterminal label (Venugopal et
al., 2009; Huang et al, 2010; Zollmann and Vo-
gel, 2011). Common to this work is that the spans
modeled may not correspond to MOS, which may
be suboptimal as discussed in Sec. 3.
In equating anchors with the function word
class, our work, particularly Model 1, is closely
related to the function word-centered model of Se-
tiawan et al (2007) and Setiawan et al (2009).
However, we provide a discriminative treatment
to the model to include a richer set of features in-
cluding the MOS modeling. Our work in incorpo-
rating global context also intersects with existing
work in Preordering Model (PM), e.g. (Niehues
and Kolss, 2009; Costa-jussa` and Fonollosa, 2006;
Genzel, 2010; Visweswariah et al, 2011; Tromble
and Eisner, 2009). The goal of PM is to reorder the
input sentence F into F ? whose order is closer to
the target language order, whereas the goal of our
model is to directly reorder F into the target lan-
guage order. The crucial difference is that we have
to integrate our model into SMT decoder, which is
highly non-trivial.
9 Conclusion
We presented a novel approach to address a kind
of long-distance reordering that requires global
cross-boundary contextual information. Our ap-
proach, which we formulate as a Two-Neighbor
Orientation model, includes the joint modeling of
two orientation decisions and the modeling of the
maximal span of the reordered chunks through the
concept of Maximal Orientation Span. We de-
scribe four versions of the model and implement
an algorithm to integrate our proposed model into
a syntax-based SMT system. Empirical results
confirm our intuition that incorporating cross-
boundaries contextual information improves trans-
lation quality. In a large scale Chinese-to-English
translation task, we achieve a significant improve-
ment over a strong baseline. In the future, we hope
to continue this line of research, perhaps by learn-
ing to identify anchors automatically from training
data, incorporating a richer set of linguistics fea-
tures such as dependency structure and strength-
ening the modeling of Maximal Orientation Span.
Acknowledgements
We would like to acknowledge the support of DARPA un-
der Grant HR0011-12-C-0015 for funding part of this work.
The views, opinions, and/or findings contained in this arti-
cle/presentation are those of the author/ presenter and should
not be interpreted as representing the official views or poli-
cies, either expressed or implied, of the DARPA.
1272
References
Nguyen Bach, Qin Gao, and Stephan Vogel. 2009.
Source-side dependency tree reordering models with
subtree movements and constraints. In Proceed-
ings of the Twelfth Machine Translation Summit
(MTSummit-XII), Ottawa, Canada, August. Interna-
tional Association for Machine Translation.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with Chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation (SSST-3)
at NAACL HLT 2009, pages 51?59, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Stanley Chen. 2009. Shrinking exponential language
models. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 468?476, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the
2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 224?233, Honolulu,
Hawaii, October.
David Chiang, Steve DeNeefe, and Michael Pust.
2011. Two easy improvements to lexical weighting.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 455?460, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages
263?270, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Marta R. Costa-jussa` and Jose? A. R. Fonollosa. 2006.
Statistical machine reordering. In Proceedings of
the 2006 Conference on Empirical Methods in Nat-
ural Language Processing, pages 70?76, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848?856, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine trans-
lation. In Proceedings of the 23rd International
Conference on Computational Linguistics (Coling
2010), pages 376?384, Beijing, China, August. Col-
ing 2010 Organizing Committee.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Zhongqiang Huang, Martin Cmejrek, and Bowen
Zhou. 2010. Soft syntactic constraints for hierar-
chical phrase-based translation using latent syntac-
tic distributions. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 138?147, Cambridge, MA, Octo-
ber. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion, June.
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In Proceedings of The 46th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1003?
1011, Columbus, Ohio, June.
Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,
and Kazuteru Ohashi. 2006. A clustered global
phrase reordering model for statistical machine
translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 713?720, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 206?214, Athens, Greece,
March. Association for Computational Linguistics.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li.
2007. Ordering phrases with function words. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 712?
719, Prague, Czech Republic, June. Association for
Computational Linguistics.
Hendra Setiawan, Min Yen Kan, Haizhou Li, and Philip
Resnik. 2009. Topological ordering of function
words in hierarchical phrase-based translation. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
1273
of the AFNLP, pages 324?332, Suntec, Singapore,
August. Association for Computational Linguistics.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of ACL-08: HLT, pages 577?585,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of lin-
guistic and contextual information for statistical ma-
chine translation. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 72?80, Singapore, August. Asso-
ciation for Computational Linguistics.
Christoph Tillman. 2004. A unigram orienta-
tion model for statistical machine translation. In
HLT-NAACL 2004: Short Papers, pages 101?104,
Boston, Massachusetts, USA, May 2 - May 7. Asso-
ciation for Computational Linguistics.
Christoph Tillmann and Tong Zhang. 2007. A
block bigram prediction model for statistical ma-
chine translation. ACM Transactions on Speech and
Language Processing (TSLP), 4(3).
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 1007?1016,
Singapore, August. Association for Computational
Linguistics.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars:
Softening syntactic constraints to improve statisti-
cal machine translation. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 236?244,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for im-
proved machine translation. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 486?496, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.
2009. A syntax-driven bracketing model for phrase-
based translation. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 315?
323, Suntec, Singapore, August. Association for
Computational Linguistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010.
Learning translation boundaries for phrase-based
decoding. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 136?144, Los Angeles, California,
June. Association for Computational Linguistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Mod-
eling the translation of predicate-argument structure
for smt. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 902?911, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Richard Zens and Hermann Ney. 2006. Discrimina-
tive reordering models for statistical machine trans-
lation. In Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL):
Proceedings of the Workshop on Statistical Machine
Translation, pages 55?63, New York City, NY, June.
Association for Computational Linguistics.
Andreas Zollmann and Stephan Vogel. 2011. A word-
class approach to labeling pscfg rules for machine
translation. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1?11,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
1274
