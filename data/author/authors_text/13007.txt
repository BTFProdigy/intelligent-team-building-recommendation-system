Proceedings of the ACL 2010 Conference Short Papers, pages 241?246,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Generating Entailment Rules from FrameNet
Roni Ben Aharon
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
r.ben.aharon@gmail.com
Idan Szpektor
Yahoo! Research
Haifa, Israel
idan@yahoo-inc.com
Ido Dagan
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
dagan@cs.biu.ac.il
Abstract
Many NLP tasks need accurate knowl-
edge for semantic inference. To this end,
mostly WordNet is utilized. Yet Word-
Net is limited, especially for inference be-
tween predicates. To help filling this gap,
we present an algorithm that generates
inference rules between predicates from
FrameNet. Our experiment shows that the
novel resource is effective and comple-
ments WordNet in terms of rule coverage.
1 Introduction
Many text understanding applications, such as
Question Answering (QA) and Information Ex-
traction (IE), need to infer a target textual mean-
ing from other texts. This need was proposed as a
generic semantic inference task under the Textual
Entailment (TE) paradigm (Dagan et al, 2006).
A fundamental component in semantic infer-
ence is the utilization of knowledge resources.
However, a major obstacle to improving semantic
inference performance is the lack of such knowl-
edge (Bar-Haim et al, 2006; Giampiccolo et al,
2007). We address one prominent type of infer-
ence knowledge known as entailment rules, focus-
ing specifically on rules between predicates, such
as ?cure X ? X recover?.
We aim at highly accurate rule acquisition,
for which utilizing manually constructed sources
seem appropriate. The most widely used manual
resource is WordNet (Fellbaum, 1998). Yet it is in-
complete for generating entailment rules between
predicates (Section 2.1). Hence, other manual re-
sources should also be targeted.
In this work1, we explore how FrameNet
(Baker et al, 1998) could be effectively used for
generating entailment rules between predicates.
1The detailed description of our work can be found in
(Ben Aharon, 2010).
FrameNet is a manually constructed database
based on Frame Semantics. It models the semantic
argument structure of predicates in terms of proto-
typical situations called frames.
Prior work utilized FrameNet?s argument map-
ping capabilities but took entailment relations
from other resources, namely WordNet. We
propose a novel method for generating entail-
ment rules from FrameNet by detecting the entail-
ment relations implied in FrameNet. We utilize
FrameNet?s annotated sentences and relations be-
tween frames to extract both the entailment rela-
tions and their argument mappings.
Our analysis shows that the rules generated by
our algorithm have a reasonable ?per-rule? accu-
racy of about 70%2. We tested the generated rule-
set on an entailment testbed derived from an IE
benchmark and compared it both to WordNet and
to state-of-the-art rule generation from FrameNet.
Our experiment shows that our method outper-
forms prior work. In addition, our rule-set?s per-
formance is comparable to WordNet and it is com-
plementary to WordNet when uniting the two re-
sources. Finally, additional analysis shows that
our rule-set accuracy is 90% in practical use.
2 Background
2.1 Entailment Rules and their Acquisition
To generate entailment rules, two issues should
be addressed: a) identifying the lexical entailment
relations between predicates, e.g. ?cure ? re-
cover?; b) mapping argument positions, e.g. ?cure
X ? X recover?. The main approach for gener-
ating highly accurate rule-sets is to use manually
constructed resources. To this end, most systems
mainly utilize WordNet (Fellbaum, 1998), being
the most prominent lexical resource with broad
coverage of predicates. Furthermore, some of its
2The rule-set is available at: http://www.cs.biu.
ac.il/?nlp/downloads
241
relations capture types of entailment relations, in-
cluding synonymy, hypernymy, morphologically-
derived, entailment and cause.
Yet, WordNet is limited for entailment rule gen-
eration. First, many entailment relations, no-
tably for the WordNet entailment and cause re-
lation types, are missing, e.g. ?elect ? vote?.
Furthermore, WordNet does not include argument
mapping between related predicates. Thus, only
substitutable WordNet relations (synonymy and
hypernymy), for which argument positions are
preserved, could be used to generate entailment
rules. The other non-substitutable relations, e.g.
cause (?kill ? die?) and morphologically-derived
(?meet.v? meeting.n?), cannot be used.
2.2 FrameNet
FrameNet (Baker et al, 1998) is a knowledge-
base of frames, describing prototypical situations.
Frames can be related to each other by inter-frame
relations, e.g. Inheritance, Precedence, Usage and
Perspective.
For each frame, several semantic roles are spec-
ified, called frame elements (FEs), denoting the
participants in the situation described. Each FE
may be labeled as core if it is central to the frame.
For example, some core FEs of the Commerce pay
frame are Buyer and Goods, while a non-core FE
is Place. Each FE may also be labeled with a se-
mantic type, e.g. Sentient, Event, and Time.
A frame includes a list of predicates that can
evoke the described situation, called lexical units
(LUs). LUs are mainly verbs but may also be
nouns or adjectives. For example, the frame Com-
merce pay lists the LUs pay.v and payment.n.
Finally, FrameNet contains annotated sentences
that represent typical LU occurrences in texts.
Each annotation refers to one LU in a specific
frame and the FEs of the frame that occur in the
sentence. An example sentence is ?IBuyer have to
pay the billsMoney?. Each sentence is accompa-
nied by a valence pattern, which provides, among
other info, grammatical functions of the core FEs
with respect to the LU. The valence pattern of the
above sentence is [(Buyer Subj), (Money Obj)].
2.3 Using FrameNet for Semantic Inference
To the best of our knowledge, the only work that
utilized FrameNet for entailment rule generation
is LexPar (Coyne and Rambow, 2009). LexPar
first identifies lexical entailment relations by go-
ing over all LU pairs which are either in the
same frame or whose frames are related by one of
FrameNet?s inter-frame relations. Each candidate
pair is considered entailing if the two LUs are ei-
ther synonyms or in a direct hypernymy relation in
WordNet (providing the vast majority of LexPar?s
relations), or if their related frames are connected
via the Perspective relation in FrameNet.
Then, argument mappings between each entail-
ing LU pair are extracted based on the core FEs
that are shared between the two LUs. The syntac-
tic positions of the shared FEs are taken from the
valence patterns of the LUs. A LexPar rule exam-
ple is presented in Figure 3 (top part).
Since most of LexPar?s entailment relations
are based on WordNet?s relations, LexPar?s rules
could be viewed as an intersection of WordNet and
FrameNet lexical relations, accompanied with ar-
gument mappings taken from FrameNet.
3 Rule Extraction from FrameNet
The above prior work identified lexical entailment
relations mainly from WordNet, which limits the
use of FrameNet in two ways. First, some rela-
tions that appear in FrameNet are missed because
they do not appear in WordNet. Second, unlike
FrameNet, WordNet does not include argument
mappings for its relations. Thus, prior work for
rule generation considered only substitutable rela-
tions from WordNet (synonyms and hypernyms),
not utilizing FrameNet?s capability to map argu-
ments of non-substitutable relations.
Our goal in this paper is to generate entail-
ment rules solely from the information within
FrameNet. We present a novel algorithm for gen-
erating entailment rules from FrameNet, called
FRED (FrameNet Entailment-rule Derivation),
which operates in three steps: a) extracting tem-
plates for each LU; b) detecting lexical entailment
relations between pairs of LUs; c) generating en-
tailment rules by mapping the arguments between
two LUs in each entailing pair.
3.1 Template Extraction
Many LUs in FrameNet are accompanied by an-
notated sentences (Section 2.2). From each sen-
tence of a given LU, we extract one template for
each annotated FE in the sentence. Each tem-
plate includes the LU, one argument correspond-
ing to the target FE and their syntactic relation
in the sentence parse-tree. We focus on extract-
ing unary templates, as they can describe any ar-
242
Figure 1: Template extraction for a sentence con-
taining the LU ?arrest?.
gument mapping by decomposing templates with
several arguments into unary ones (Szpektor and
Dagan, 2008). Figure 1 exemplifies this process.
As a pre-parsing step, all FE phrases in a given
sentence are replaced by their related FE names,
excluding syntactic information such as preposi-
tions or possessives (step (b) in Figure 1). Then,
the sentence is parsed using the Minipar depen-
dency parser (Lin, 1998) (step (c)). Finally, a
path in the parse-tree is extracted between each FE
node and the node of the LU (step (d)). Each ex-
tracted path is converted into a template by replac-
ing the FE node with an argument variable.
We simplify each extracted path by removing
nodes along the path that are not part of the syn-
tactic relation between the LU and the FE, such
as conjunctions and other FE nodes. For example,
?Authorities
subj
?? enter
conj
?? arrest? is simplified
into ?Authorities
subj
?? arrest?.
Some templates originated from different anno-
tated sentences share the same LU and syntactic
structure, but differ in their FEs. Usually, one of
these templates is incorrect, due to erroneous parse
(e.g. ?Suspect
obj
?? arrest? is a correct template, in
contrast to ?Charges
obj
?? arrest?). We thus keep
only the most frequently annotated template out of
the identical templates, assuming it is the correct
one.
3.2 Identifying Lexical Entailment Relations
FrameNet groups LUs in frames and describes re-
lations between frames. However, relations be-
tween LUs are not explicitly defined. We next de-
scribe how we automatically extract several types
of lexical entailment relations between LUs using
two approaches.
In the first approach, LUs in the same frame
that are morphological derivations of each other,
e.g. ?negotiation.n? and ?negotiate.v?, are marked
as paraphrases. We take morphological derivation
information from the CATVAR database (Habash
and Dorr, 2003).
The second approach is based on our observa-
tion that some LUs express the prototypical situ-
ation that their frame describes, which we denote
dominant LUs. For example, the LU ?recover? is
dominant for the Recovery frame. We mark LUs
as dominant if they are morphologically derived
from the frame?s name.
Our assumption is that since dominant LUs ex-
press the frame?s generic meaning, their meaning
is likely to be entailed by the other LUs in this
frame. Consequently, we generate such lexical
rules between any dominant LU and any other LU
in a given frame, e.g. ?heal? recover? and ?con-
valescence? recover? for the Recovery frame.
In addition, we assume that if two frames are
related by some type of entailment relation, their
dominant LUs are also related by the same rela-
tion. Accordingly, we extract entailment relations
between dominant LUs of frames that are con-
nected via the Inheritance, Cause and Perspective
relations, where Inheritance and Cause generate
directional entailment relations (e.g. ?choose ?
decide? and ?cure? recover?, respectively) while
Perspective generates bidirectional paraphrase re-
lations (e.g. ?transfer? receive?).
Finally, we generate the transitive closure of
the set of lexical relations identified by the above
methods. For example, the combination of ?sell?
buy? and ?buy? get? generates ?sell? get?.
3.3 Generating Entailment Rules
The final step in the FRED algorithm generates
lexical syntactic entailment rules from the ex-
tracted templates and lexical entailment relations.
For each identified lexical relation ?left? right?
between two LUs, the set of FEs that are shared by
both LUs is collected. Then, for each shared FE,
we take the list of templates that connect this FE
243
Lexical Relation:
cure? recovery
Templates:
Patient
obj
?? cure (cure Patient)
Affliction
of
?? cure (cure of Affliction)
Patient
gen
?? recovery (Patient?s recovery)
Patient
of
?? recovery (recovery of Patient)
Affliction
from
?? recovery (recovery from Affliction)
Intra-LU Entailment Rules:
Patient
gen
?? recovery?? Patient
of
?? recovery
Inter-LU Entailment Rules:
Patient
obj
?? cure =? Patient
gen
?? recovery
Patient
obj
?? cure =? Patient
of
?? recovery
Affliction
of
?? cure =? Affliction
from
?? recovery
Figure 2: Some entailment rules generated for the
lexical relation ?cure.v? recovery.n?.
Configuration R (%) P (%) F1
No-Rules 13.8 57.7 20.9
LexPar 14.1 42.9 17.4
WordNet 18.3 32.2 17.8
FRED 17.6 55.1 24.6
FRED ?WordNet 21.8 33.3 20.9
Table 1: Macro average Recall (R), Precision (P)
and F1 results for the tested configurations.
to each of the LUs, denoted by T feleft and T
fe
right.
Finally, for each template pair, l ? T feleft and r ?
T feright, the rule ?l ? r? is generated. In addition,
we generate paraphrase rules between the various
templates including the same FE and the same LU.
Figure 2 illustrates this process.
To improve rule quality, we filter out rules that
map FEs of adjunct-like semantic types, such as
Time and Location, since different templates of
such FEs may have different semantic meanings
(e.g. ?Time
before
?? arrive? ?Time
after
?? arrive?).
Thus, it is hard to identify those template pairs that
correctly map these FEs for entailment.
We manually evaluated a random sample of 250
rules from the resulting rule-set, out of which we
judged 69% as correct.
4 Application-based Evaluation
4.1 Experimental Setup
We would like to evaluate the overall utility of our
resource for NLP applications, assessing the cor-
rectness of the actual rule applications performed
in practice, as well as to compare its performance
to related resources. To this end, we follow the ex-
perimental setup presented in (Szpektor and Da-
gan, 2009), which utilized the ACE 2005 event
dataset3 as a testbed for entailment rule-sets. We
briefly describe this setup here.
The task is to extract argument mentions for
26 events, such as Sue and Attack, from the ACE
annotated corpus, using a given tested entailment
rule-set. Each event is represented by a set of
unary seed templates, one for each event argu-
ment. Some seed templates for Attack are ?At-
tacker
subj
??attack? and ?attack
obj
??Target?.
Argument mentions are found in the ACE cor-
pus by matching either the seed templates or tem-
plates entailing them found in the tested rule-set.
We manually added for each event its relevant
WordNet synset-ids and FrameNet frame-ids, so
only rules fitting the event target meaning will be
extracted from the tested rule-sets.
4.2 Tested Configurations
We evaluated several rule-set configurations:
No-Rules The system matches only the seed
templates directly, without any additional rules.
WordNet Rules are generated from WordNet
3.0, using only the synonymy and hypernymy rela-
tions (see Section 2.1). Transitive chaining of re-
lations is allowed (Moldovan and Novischi, 2002).
LexPar Rules are generated from the publicly
available LexPar database. We generated unary
rules from each LexPar rule based on a manually
constructed mapping from FrameNet grammatical
functions to Minipar dependency relations. Fig-
ure 3 presents an example of this procedure.
FRED Rules are generated by our algorithm.
FRED ?WordNet The union of the rule-sets of
FRED and WordNet.
4.3 Results
Each configuration was tested on each ACE event.
We measured recall, precision and F1. Table 1
reports macro averages of the three measures over
the 26 ACE events.
As expected, using No-Rules achieves the high-
est precision and the lowest recall compared to all
other configurations. When adding LexPar rules,
3http://projects.ldc.upenn.edu/ace/
244
LexPar rule:
Lexemes: arrest ?? apprehend
Valencies: [(Authorities Subj), (Suspect Obj), (Offense (for))] =? [(Authorities Subj), (Suspect Obj), (Offense (in))]
Generated unary rules:
X
subj
?? arrest =? X
subj
?? apprehend , arrest
obj
?? Y =? apprehend
obj
?? Y , arrest
for
?? Z =? apprehend
in
?? Z
Figure 3: An example for generation of unary entailment rules from a LexPar rule.
only a slight increase in recall is gained. This
shows that the subset of WordNet rules captured
by LexPar (Section 2.3) might be too small for the
ACE application setting.
When using all WordNet?s substitutable rela-
tions, a substantial relative increase in recall is
achieved (32%). Yet, precision decreases dramat-
ically (relative decrease of 44%), causing an over-
all decrease in F1. Most errors are due to correct
WordNet rules whose LHS is ambiguous. Since
we do not apply a WSD module, these rules are
also incorrectly applied to other senses of the LHS.
While this phenomenon is common to all rule-sets,
WordNet suffers from it the most since it contains
many infrequent word senses.
Our main result is that using FRED?s rule-set,
recall increases significantly, a relative increase
of 27% compared to No-Rules, while precision
hardly decreases. Hence, overall F1 is the high-
est compared to all other configurations (a rela-
tive increase of 17% compared to No-Rules). The
improvement in F1 is statistically significant com-
pared to all other configurations, according to the
two-sided Wilcoxon signed rank test at the level of
0.01 (Wilcoxon, 1945).
FRED preforms significantly better than LexPar
in both recall, precision and F1 (a relative increase
of 25%, 28% and 41% respectively). For example,
LexPar hardly utilizes FrameNet?s argument map-
ping capabilities since most of its rules are based
on a sub-set of WordNet?s substitutable relations.
FRED?s precision is substantially higher than
WordNet. This mostly results from the fact
that FrameNet mainly contains common senses
of predicates while WordNet includes many rare
word senses; which, as said above, harms preci-
sion when WSD is not applied. Error analysis
showed that only 7.5% of incorrect extractions are
due to erronous rules in FRED, while the majority
of errors are due to sense mismatch or syntactic
matching errors of the seed templates ot entailing
templates in texts.
FRED?s Recall is somewhat lower than Word-
Net, since FrameNet is a much smaller resource.
Yet, its rules are mostly complementary to those
from WordNet. This added value is demon-
strated by the 19% recall increase for the union of
FRED and WordNet rule-sets compared to Word-
Net alne. FRED provides mainly argument map-
pings for non-substitutable WordNet relations, e.g.
?attack.n on X ? attack.v X?, but also lexical re-
lations that are missing from WordNet, e.g. ?am-
bush.v? attack.v?.
Overall, our experiment shows that the rule-
base generated by FRED seems an appropri-
ate complementary resource to the widely used
WordNet-based rules in semantic inference and
expansion over predicates. This suggestion is es-
pecially appealing since our rule-set performs well
even when a WSD module is not applied.
5 Conclusions
We presented FRED, a novel algorithm for gener-
ating entailment rules solely from the information
contained in FrameNet. Our experiment showed
that FRED?s rules perform substantially better
than LexPar, the only prior rule-set derived from
FrameNet. In addition, FRED?s rule-set largely
complements the rules generated from WordNet
because it contains argument mappings between
non-substitutable predicates, which are missing
from WordNet, as well as lexical relations that are
not included in WordNet.
In future work we plan to investigate combin-
ing FrameNet and WordNet rule-sets in a transitive
manner, instead of their simple union.
Acknowledgments
This work was partially supported by the Rec-
tor?s research grant of Bar-Ilan University, the
PASCAL-2 Network of Excellence of the Eu-
ropean Community FP7-ICT-2007-1-216886 and
the Israel Science Foundation grant 1112/08.
245
References
Collin Baker, Charles Fillmore, and John Lowe. 1998.
The berkeley framenet project. In Proceedings of
COLING-ACL, Montreal, Canada.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second pascal recognising tex-
tual entailment challenge. In Second PASCAL Chal-
lenge Workshop for Recognizing Textual Entailment.
Roni Ben Aharon. 2010. Generating entailment rules
from framenet. Master?s thesis, Bar-Ilan University.
Robert Coyne and Owen Rambow. 2009. Lexpar: A
freely available english paraphrase lexicon automat-
ically extracted from framenet. In Proceedings of
the Third IEEE International Conference on Seman-
tic Computing.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Lecture Notes in Computer Science,
volume 3944, pages 177?190.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Massachusetts.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recogniz-
ing textual entailment challenge. In Proceedings of
the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing.
Nizar Habash and Bonnie Dorr. 2003. A categorial
variation database for english. In Proceedings of
the North American Association for Computational
Linguistics (NAACL ?03), pages 96?102, Edmonton,
Canada. Association for Computational Linguistics.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalu-
ation of Parsing Systems at LREC.
Dan Moldovan and Adrian Novischi. 2002. Lexical
chains for question answering. In Proceedings of
COLING.
Idan Szpektor and Ido Dagan. 2008. Learning en-
tailment rules for unary templates. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 849?856,
Manchester, UK, August.
Idan Szpektor and Ido Dagan. 2009. Augmenting
wordnet-based inference with argument mapping.
In Proceedings of the 2009 Workshop on Applied
Textual Inference, pages 27?35, Suntec, Singapore,
August.
Frank Wilcoxon. 1945. Individual comparisons by
ranking methods. Biometrics Bulletin, 1(6):80?83.
246
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 185?193,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Topics as Contextual Indicators for Word Choice in SMS Conversations 
 
 
Ute Winter1, Roni Ben-Aharon, Daniel Chernobrov, Ron M Hecht1 
 
1GM Advanced Technical Center, HaManofim Street 11, Herzeliya 46725, Israel 
ute.winter@gm.com, r.ben.aharon@gmail.com, 
 daniel-cher@hotmail.com, ron.hecht@gm.com 
 
 
 
 
Abstract 
SMS dictation by voice is becoming a viable al-
ternative providing a convenient method for 
texting in a variety of environments. Contextual 
knowledge should be used to improve perfor-
mance. We propose to add topic knowledge as 
part of the contextual awareness of both texting 
partners during SMS conversations. Topics can 
be used for speech applications, if the relation 
between the conversed topics and the choice of 
words in SMS dialogs is measurable. In this 
study, we collected an SMS corpus, developed 
a topic annotation scheme, and built a topic hie-
rarchy in a tree structure. We validated our top-
ic assignments and tree structure by the 
Agglomerative Information Bottleneck method, 
which also proved the measurability of the in-
terrelation between topics and wording. To 
quantify this relation we propose a na?ve classi-
fication method based on the calculation of top-
ic distinctive word lists and compare the 
classifiers? topic recognition capabilities for 
SMS dialogs with unigram language models. 
The results demonstrate that the relation be-
tween topic and wording is significant and can 
be integrated into SMS dictation.  
1 Introduction 
One of the largest growth areas in communication 
is the Short Message Service (SMS) or text mes-
saging, as it is more popularly known. SMS grew 
out of what was initially a by-product of the mo-
bile phone industry (Agar, 2003; Goggin, 2006). In 
fact, by 2009 text messaging has become the most 
frequently used communication means among 
teens in the US, supported by the mobile phone 
industry offering unlimited texting plans (Lenhart 
et. al., 2010).  
For many reasons, voice enabled texting has be-
come a desirable alternative in a variety of mobile 
scenarios. The number of speech applications for 
mobile phones including texting by voice is con-
stantly growing. However, the challenges for SMS 
dictation by voice are multifold, from particular 
noise conditions, to the use of vocabulary and do-
main specific language, the dialogical nature of 
text messaging (Thurlow and Poff, 2009), and to 
error correction of imperfect recognition results.  
Achieving a high and robust performance is cru-
cial for the success of the application. For this pur-
pose additional contextual factors can be integrated 
into the recognition process. One possible factor, 
the conversed topic, has influence on the speaker?s 
choice of words. Hence, it is an important contex-
tual factor for the prediction of the speaker?s word-
ing, since it originates in the speaker?s mental 
concepts during a dialog situation, which is the 
nature of texting. 
To date, research on text messaging has primari-
ly examined socio-linguistic phenomena (e.g., 
Thurlow, 2003). With respect to language and 
communication, text messaging is still an under-
examined research area. Thurlow and Poff (2009) 
provide a comprehensive overview of existing lite-
rature about SMS in linguistics. Moreover, there 
exists noteworthy work on SMS text normalization 
(Aw et. al., 2006; Fairon and Paumier, 2006; Cook 
and Stevenson, 2009; Kobus et. al., 2008; Pennell 
and Liu, 2010), for instance for the purpose of Ma-
chine Translation, Text-to-Speech engines or spell 
checking, work on SMS based question answering 
185
services (Kothari, 2009), and work on predefined 
SMS replies in automobiles (Wu et. al., 2010). 
However, conversed topics in the context of SMS 
discourse have not been examined in the literature, 
neither in linguistics nor for any Natural Language 
Processing applications.  
Hence, in this paper we have developed a new 
approach to make topics useful as context know-
ledge for SMS dictation by voice. We describe top-
ic annotation of a novel SMS corpus and study the 
influence which SMS dialog topics may have on 
the choice of words. Based on the results, we are 
able to estimate and initially quantify its impact. 
This research can serve as the basis for developing 
algorithms that use topic knowledge for SMS dic-
tation in speech applications.  
2 Topic Annotation for SMS 
2.1 SMS Corpus in US English 
SMS data was collected from 250 participants who 
conversed with another 900. Participants were dis-
tributed almost evenly across gender, two age 
groups, and four US regions. Participants under 30 
years comprised 48% of the dataset, and partici-
pants over 30 years comprised 52% of the dataset. 
Within each of these two age groups, there were 
equal number of men and women. The demograph-
ic spread contained datasets from participants from 
the various regions in the USA: east coast 19%, 
west coast 24%, central 29%, and south 28%. 
The corpus dataset contains a total number of 
more than 51,000 messages, chosen randomly from 
a significantly larger set of data, for which partici-
pants provided authentic SMS conversations from 
their mobile phones to online SMS backup servic-
es. Besides demographic constraints, all text mes-
sages are part of SMS conversations, each 
composed at least by one message and a textual 
response, to preserve a contextual authentic situa-
tion. A conversation is considered to be ended if a 
time frame of 4 hours elapses without a response. 
The average length of SMS conversations in the 
corpus is between 8-9 messages, distributed over a 
notably higher number of shorter conversions than 
longer dialogs. Altogether the corpus contains 
more than 5800 conversations.  
Personal information of the SMS conversations 
was removed. Nonetheless the corpus itself is cur-
rently not published, because identifying informa-
tion can be indirectly present in SMS dialogs.  
The SMS corpus is semi-automatically norma-
lized following a general guideline to transform 
each texted message into one which could be dic-
tated by the user. For all following research the 
normalized rather than the raw SMS textual utter-
ances are used.  
 Table 1 shows representative examples for text 
normalization.  
  
Raw Normalized 
Yea b workin for 
hospice 
yeah be working for 
hospice 
I am at vetran 
@at@8 am 
I am at Veteran at 
eight ei-em 
Lets go 2 eat Let?s go to eat 
You wanna go to da 
b walk or sumthin? 
You wanna go to the 
bee walk or some-
thing? 
 
Table 1: Text messages in raw and normalized format. 
2.2 Topic Annotation Method 
A key point for usefulness of an annotated corpus 
is the abstraction which maps SMS conversations 
present in the corpus to an abstract model serving 
the research goals (Wallis and Nelson, 2001; Mc 
Enery et. al., 2006). In our research, the corpus 
shall be used to explore to what extent the know-
ledge of one or more discussed topics, for which 
both SMS dialog partners try to make progress, can 
contribute to the performance of a speech recogni-
tion engine, where we expect the engine to be 
based on Statistical Language Models (SLM). 
Consequently, the annotation needs to enable us to 
trace a path from discussed topics to the choice of 
words and phrases in SMS conversations. This ab-
straction leads to our definition of the term topic 
and to guidelines for the annotation which are 
identified to be essential, when incorporating top-
ics into speech recognition.   
Other than an agreement on ?what is being 
talked about?, the definition of topic in linguistics 
is a matter of viewpoint and dispute (Levinson, 
1983; Li and Thompson, 1976; Chafe, 1976; 
Moln?r, 1993; Stutterheim, 1997). Moreover, a 
literature review has not revealed existing topic 
annotations which can be used for our purpose (Mc 
Enery et. al., 2006; Meyer, 2002). Since the inten-
186
tion is to build a task driven, problem oriented an-
notation scheme we further specify a discourse 
topic as observable content or story line which dis-
course partners follow up in an SMS conversation. 
Hence, we understand a topic foremost as an 
attribute of an SMS dialog rather than of a single 
SMS, or of a phrase within the dialog. We assign at 
least one topic to each dialog. Since dialogs can in 
fact contain several distinct topics, we assign all 
explicitly mentioned topics to a conversation and 
mark separately all SMS which belong doubtlessly 
to each topic in the context of the conversation,  
Topics describe the content only, not any other 
level of discourse. The example in figure 1 shows a 
conversation with the topic ?meeting arrangement?. 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: Example of SMS dialog about ?meeting      
arrangement?. 
2.3 Topic Annotation Procedure 
Discourse topics are highly domain dependent in 
their nature and may differ from the SMS domain 
to other domains, even to computer mediated 
communication services, like e-mail, Twitter, or 
Instant Messaging. Because of that, the list of SMS 
relevant topics evolves from the data itself. Addi-
tionally the list of possible topics always remains 
an open tag list, although one can expect recurring 
topics after a while with sparse extension of an 
existing topic list. Hence, the approach for annotat-
ing the SMS corpus must be manual. For this pur-
pose a team of four annotators marked the 
conversations with the help of an annotation tool 
developed specifically for the topic annotation. To 
ensure annotator agreement a linguist verified and 
confirmed the growing topic list and all topic as-
signments in several iterations. Further annotation 
of a larger corpus may be semi-automated based on 
the achieved topic list.  
Assigning topics to a dialog remains intuitive to 
a certain extent, because any mutual understanding 
of the dialog?s content and pragmatic meaning is 
supported by social cues, situation awareness and 
world knowledge of dialog partners (Levinson, 
1983; Lambert and Carberry, 1992). These know-
ledge dimensions need to be reconstituted during 
the annotation process, when assigning a new top-
ic. One criterion is to ask if the topic is distinct 
from other topics with regard to describing pieces 
of our world knowledge dimensions, e.g. scripts 
and events that people repeatedly experience, or 
subjects, they are recurrently dealing with. 
Furthermore, a task driven approach demands to 
determine the level of specialization and detail for 
topics. Even if broad topics, such as ?food? or ?ap-
pointment?, may prove themselves to be distinct 
and meaningful enough for speech recognition, the 
annotation is done to one degree more detailed. 
Each topic is composed by a term and one restric-
tive attribute which divides a major topic into more 
distinctive topics. Thus ?appointment? appears in 
the corpus divided into ?cancel appointment?, ?at-
tending an appointment?, ?meeting arrangements?, 
and other. The advantage of the annotation proce-
dure is twofold; it leads to a list of topics, which 
can be depicted in a tree structure with several le-
vels of specialization, and, even though the annota-
tion is targeted to a special problem, there is 
sufficient information to make the corpus useful 
for a broader range of research. 
3 Corpus Analysis for Topic Usage 
3.1 Properties of Topics 
SMS conversations may follow up on one or more 
topics. Multiple topic conversations may make 
progress on topics even in parallel, either switching 
topics or addressing both within the same SMS. In 
general, we avoid topics which are suspected to 
describe the intention or strategy for the conversa-
tion rather than the content. There are a few excep-
tions, where the topic is implicitly or explicitly 
present in the dialog not only on content level but 
also as driving force for texting, e.g. ?maintain 
friendship/relationship? or ?small talk? (see exam-
ple (2) in figure 2). The border cannot be clearly 
drawn in these cases.  
Two topic assignments require explanation. 
?Small talk? is used for a group of short SMS di-
Hey how is every-
thing going? 
 
Good. Wanna go 
to the lax house? 
 
Maybe, when are u 
planning on going? 
In a little bit 
I'm still at the li-
brary? maybe i'll 
meet u ther Ok sounds good. 
187
alogs, for which one cannot identify a topic. One is 
able to understand the dialog as a short form of 
friendship maintenance though, where both parties 
achieve mutual positive feedback about their cur-
rent situation, e.g. via salutation. Therefore ?small 
talk? is expected to be of interest regarding word 
usage contrary to ?undefined topic?. The latter is 
assigned to all conversations, where we do not 
share enough knowledge about the background and 
situation of the texters to understand and identify 
the topic of the dialog (example (3) in figure 2).   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2: SMS dialogs with (1) multiple topics, (2) 
small talk, and (3) undefined topic.  
 
All in all, the corpus contains 42.1% of dialogs 
with one annotated topic and 46.6% with multiple 
topics. The remaining 11.3% of dialogs are tagged 
as ?undefined?.   
3.2 Building a Topic Tree 
The identification of similar or related topics in our 
corpus allow for grouping them together in specific 
topic clusters, such as ?human relations?, ?tech-
nology?, and ?transportation?, and represent them 
in a tree structure hierarchy. The assignment to a 
topic cluster for each topic is determined by the 
relation between topics, which humans define 
based on their world knowledge and based on the 
semantic meaning of the topic. 
The topic tree hierarchy consists of four levels. 
The nodes in the first two levels build the tree 
structure and represent the topic clusters. Therefore 
they have not been used during the annotation 
process. Only from level three and above the topic 
names are assigned to the corpus and may be 
leaves of the tree. A forth level is used, when third 
level topics are frequently used in SMS dialogs and 
can further be divided into meaningful sub topics. 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Topic tree branch related to ?shopping?. 
 
 
 
 
 
 
 
 
 
 
Figure 4: Topic tree branch for ?positive emotion?. 
3.3 Topic Distribution in SMS Corpus 
87.1% of all text messages are categorized in nine 
preferably conversed topic clusters (see figure 5), 
the remaining messages belong either to SMS di-
alogs, where the topic is labeled as undefined, or to 
miscellaneous, rarely conversed topics, e.g. 
?weather? or ?religious belief?.  
More than 55% of all text messages are moti-
vated by interpersonal and emotional matters. 
About 45% of all text messages deal with ?human 
relations?, mainly including sub topics regarding 
relation maintenance (36% of ?human relations?, 
e.g. ?make promise?, ?make apology?, ?health 
condition?, ?small talk?, a. o.), regarding relations 
with friends (14%), concerning relationship issues 
activities & events 
 
 
travel    recreation    special occasions ? 
 
 
sport activities    going shopping    going out ? 
 
 
buy clothes   buy gift   buy item   going to store 
emotion 
 
 
negative            positive 
 
 
express joy   express love   feeling better ? 
Missed phone call, planned schedule 
 
Texter 1: Hi, sorry I missed your call. I'm 
actually at an appointment right now.  
Texter 1: I will call you about 12:45pm. 
Please answer, so we can finally connect, if 
not I will call after 17:00. 
Texter 2: O.K no problem, call me when 
you're free :) 
Texter 1: The appointment is over, I tried 
calling you but you didn't answer, will talk 
when I'm on my way home 
Texter 2: Thankyou. 
Small talk  
 
Texter 1: What?s up? 
Texter 2: I?m good, u? 
Texter 1: I?m fine, talk to you later 
Texter 2: Sure :) 
Topic undefined 
  
Texter 1: df 
Texter 2: what? 
Texter 1: don?t forget 
Texter 2: Lol :-) I won't 
1 
2 
3 
188
with a partner (11%). The latter 10% converse 
about negative or positive emotions, nearly 50% of 
these dialogs expressing love. SMS dialogs from 
?human relations? contain 9.3 messages per dialog 
in the average, which is significantly more than the 
average of 4-6 messages in all other topic clusters.     
The second most discussed topic is ?activities & 
events? (14% of all messages), such as ?going out? 
(32% of ?activities & events? labeled messages), 
or ?going shopping? (15%). Interestingly, the topic 
of ?appointment & scheduling? is only the third 
most popular, consisting of less than 13% of all 
text messages.  
Figure 5 shows the topic distribution in the cor-
pus with respect to the topic tree?s first hierarchy. 
   
 
 
Figure 5: Topic distribution on first tree level. 
 
Thurlow (2003) has presented a study about the 
communicative intent of US English text messag-
es, describing their functional orientation rather 
than the content. Thurlow?s findings concur in that 
the amount of SMS with relational and intimate 
orientation vs. transactional orientation is similar 
to the amount of SMS with interpersonal and emo-
tional content vs. all other topic clusters.   
Finally, we examine if distribution differences 
depend on the demographic data of the users re-
garding gender, age groups (18-23, 24-28, 29-35, 
36-42) and regions. Users older than 42 years are 
not taken into account because of the limited num-
ber of text messages in the corpus.  
Generally, males and females talk about the 
same topics in SMS conversations through all age 
groups and regions. However, there are still some 
differences between those groups worth mention-
ing and shown in figure 6.  
While interpersonal and emotional text messag-
es together are present in fairly equal quantity for 
both gender groups, females tend to express their 
?emotion? via text messages much more frequently 
than males (12.5% compared to 8.5%); likely on 
the expense of non-emotional ?human relations? 
messages (46.8% for males compared to 41.9%). 
Furthermore, males and females have contradicting 
trends in ?emotion? talk over ages. Females tend to 
express emotions more with age progression, while 
males have the opposite tendency. In both genders, 
the corpus suggests a tradeoff between the topics 
?human relations? and ?emotion?, i.e. age may 
change the portion of one topic on the expense of 
the other one. 
4 Relation between Topic and Wording 
4.1 Automated Validation of Topic Tree 
A human annotation process is highly effective 
due to people?s ability to exploit their mental 
knowledge base and mind concepts, and thus a 
broad range of information sources. However, even 
Figure 6: Topic distribution by gender (males left, females right) and age groups 
189
in a most rigorous procedure errors may occur, 
especially regarding annotation and tree consisten-
cy. Therefore we need to verify the quality of the 
annotation. Additionally, we want to ensure that 
relevant algorithms can trace the interrelation be-
tween topics and the choice of words in SMS.  
In order to verify both requirements, we perform 
an automatic validation by applying a nuance 
(Hecht et al, 2009) of the Agglomerative Informa-
tion Bottleneck (AIB) method (Tishby et al, 1999; 
Slonim and Tishby, 2000). This derivative of the 
AIB is a hierarchical clustering algorithm, and as 
such, it produces a hierarchical topic tree.  
The clustering starts with each lower level topic 
as a singleton. In an iterative process, the two clos-
est topics are merged to form a larger topic, where 
the two closest topics are defined as the ones that 
minimize the AIB functional (Eq. 1). The process 
ends when all topics are merged into a single topic. 
 
? ?? ? ? ? ? ?XYIXXIxxpL ?;?;? ???                           (1) 
X , Y and X?  are the set of topics, set of words 
and clustered set of topics respectively. ? ?BAI ;
 
is 
the mutual information between A  and B .  
 
 
 
Figure 7: Tree branch of the hierarchical clustering of 
topics into groups. 
Intuitively, the function tries to achieve two 
goals simultaneously. It minimizes ? ?XXI ?; which 
can be interpreted as finding the most compact top-
ic representation and at the same time it maximizes 
? ?XYI ?;
 
which can be interpreted as finding the 
most indicative subset of topics. These two goals 
contradict one another. Therefore a tradeoff para-
meter ?  is added. 
Presenting the entire AIB tree is not feasible in 
this paper. In order to provide some intuition, a sub 
tree is shown in figure 7. Briefly, each AIB tree 
branch shows a distribution of topics that is mostly 
in line with the hand crafted topic tree. Even sen-
timents are clustered (negative sentiment for all 
lower level topics in figure 7), a superior achieve-
ment to the manual topic tree, where this is done 
only for ?emotion?. Moreover, it becomes evident 
that the interrelation between topics and wording 
in SMS can likely be captured automatically. 
4.2 Method for Relation Discovery 
Being confident regarding automatic computation, 
we can strive for more and aim to discover the in-
terrelation between topics and wording in detail. 
Any vocabulary used in SMS dialogs can intuitive-
ly be viewed as containing information which 
points to one or a limited group of conversed top-
ics, or as being general vocabulary with respect to 
topic distinctiveness. Such a view point entails 
questions. How can we extract a list of distinctive 
words per topic; words which are dominant in a 
certain topic but subordinate in others respective-
ly? To what extent are topic distinctive words still 
ambiguous and are assigned to more than one top-
ic? And ultimately, can we use topic distinctive 
vocabulary to recognize a list of conversed topics 
for each SMS dialog based on its choice of words? 
Our method evolves from the questions as fol-
lows: First, we categorize the SMS vocabulary into 
topic distinctive vs. general vocabulary by intro-
ducing an algorithm which uses topic information 
as qualitative measurement to extract a list of dis-
tinctive words operating as classifiers for topics. In 
a second step we evaluate for each topic to what 
extent topic distinctive word list classifiers can 
recognize topics in SMS dialogs. Finally we com-
pare the classifiers? topic recognition capabilities 
with unigram language models. We use only the 
nine first level topic clusters to guarantee that the 
amount of available dialogs per topic is sufficient. 
190
4.3 Topic Distinctive Vocabulary 
To categorize the vocabulary we calculate for each 
word wi with at least 4 occurrences in the corpus 
and topic tj the ratio between word frequency in the 
topic and general word frequency in the corpus 
(known as Term Frequency/Collection Frequency 
Measure) normalized by the topic size (Eq. 2):  
 
? ?
?
?
l m
jmli
ji
jicorpus
itj
ji
twcounttwcount
twcount
tsizewfreq
wfreq
twCfTf
),(*),(
),(
)(
1
*
)(
)(
),(?
             (2) 
 
After scores are calculated for all words, we sort 
the words for each topic from their highest to low-
est score. Then we assign a topic dependent thre-
shold for each topic determined by a Receiver 
Operating Characteristic (ROC) analysis as de-
scribed in 4.4. All words above the threshold be-
long to the distinctive word set (DWS) per topic. In 
additionally conducted experiments with the cor-
pus this method has proven to outperform other 
alternatives, such as TF*IDF or Term Discrimina-
tion Models (Salton et. al., 1975). 
 
 
Table 2: Examples of topic distinctive words. 
 
Table 2 illustrates examples of high-scored re-
trieved distinctive words from several topics. It 
becomes evident that words with high scores are 
related to a topic in our intuition or mental con-
cepts. However, frequently used general words, 
such as pronouns, prepositions, and common 
nouns, do not receive high scores, because of their 
vast number of occurrences in other topics, e.g. 
?never?, ?flat?, ?boy?, ?you?, or ?from?. Topics 
that are more descriptive or transactional in their 
orientation, such as ?transportation? or ?finance?, 
generate better content distinctive word sets than 
the ones with relational intent, such as ?emotion?. 
4.4 Topic Recognition by Word Sets 
In order to determine optimal thresholds (see 4.3) 
and to analyze the coverage and distinctiveness of 
the word set , we divide the corpus into a training 
batch (90% of all messages) and a test batch 
(10%). The training batch is used for the calcula-
tion of word scores as described in 4.3. By itera-
tively increasing the score threshold which defines 
a word set, we calculate per iteration the amount of 
dialogs from the test batch containing at least one 
word of the set, for dialogs annotated with the affi-
liated topic as well as for dialogs tagged different-
ly. Consequently, ROC curves are created for all 
topics. This process is performed in a cross valida-
tion manner (10-fold).  
Figure 8 shows the ROC curves for the topics 
?human relations?, ?activities & events?, ?finance 
& property?, and ?food & drinks?, averaged over 
the 10-fold iterations.  
 
 
 
Figure 8: ROC curves for selected topics including best 
and worst performing topics with x axes for false posi-
tive rate (FPR) and y axes for true positive rate (TPR). 
 
These results show that once appropriate thre-
sholds are chosen, relatively small DWS, mostly 
ranging between 60-120 words per set, have the 
capability of achieving a true positive rate (TPR, 
transporta-
tion 
finance & 
property 
emotion 
lane loan loss 
boarding payments xox 
tires printing beyond 
flight sander childish 
wheel cheque love 
license paypal bitching 
roads discount mentally 
battery invoice soo 
plane price stressed 
exit dollars nerves 
191
also known as recall) of 80.3% for topic dialogs 
with an average false positive rate (FPR, also 
known as fall-out) of 26.8%, even with a relatively 
na?ve classification method. Table 3 provides de-
tailed results of TPR and FPR. Topic DWS for 
more descriptive or transactional topics (e.g. 
?transportation?, ?food & drinks?) manage to dis-
tinguish better than relational targeted topics, such 
as ?emotion? and ?human relations?, since words 
like ?love?, ?babe?, or ?thank? are highly related to 
the ?emotion? topic, but also appear in many other 
topics. Hence, these words are increasing the FPR. 
Eventually, the word sets chosen by optimal 
thresholds allow us to quantify topic recognition of 
dialogs. We automatically assign topics to each 
dialog in the corpus according to the described al-
gorithm. Then we compare these topics to the ma-
nually annotated topics and measure recall and 
precision per dialog, denoted (Eq. 3): 
 
topicsmatched
topicsmatchedcorrect
prec
topicsannotated
topicsmatchedcorrect
recall
_#
__#
_#
__#
?
?
            (3) 
   
The average recall and precision rates over all 
dialogs are 73.5% and 44.3%, respectively. Taking 
into account the complexity of the recognition task 
due to the possibility of multiple topic assignment 
for each dialog, the results strengthen the hypothe-
sis of the positively measureable interrelation be-
tween topics and wording. 
4.5 Comparison to Full Vocabulary Models 
Finally, we wish to better understand the impact of 
DWS, in comparison to the general language de-
rived from the topic text, which is motivated by the 
fact that speech applications rely on SLMs. To this 
end, we construct a unigram language model bi-
nary classifier for each topic as baseline and per-
form a 10-fold cross validation classification task, 
to identify whether a given dialog is related to the 
topic or not, using the following formula (Eq. 4), 
where Di is the i
th dialog and Mt is the language 
model of topic t: 
 
?
??
?
?
?
iDw
t
topictopict
ti
topictopict
i
Mwp
MDDtopic
))|((maxarg
)|(maxarg)(
,
,
*
        (4) 
Table 3 summarizes the results of TPR and FPR 
of the two approaches. As expected, the DWS ap-
proach suffers from a higher FPR, due to a lack of 
weights and relative comparisons to other classes. 
Since the differences in FPR between the two me-
thods are not immense, we conclude that our cho-
sen word sets are indeed distinctive, and with 
proper tuning have the potential of achieving better 
results. On the other hand, the DWS approach 
manages to outperform language models in terms 
of TPR. Hence, most of the information needed for 
the identification of dialog topics is provided by 
distinctive words to a significant higher extent as 
by the rest of the vocabulary. 
 
Table 3: True and false positive rates for all topics using   
DWS classification and language models. 
5 Conclusion  
The primary motivation of this study has been to 
estimate and facilitate the potential integration of 
contextual knowledge, in particular topics, into 
SMS dictation by voice. We have identified the 
interrelation between conversed topics and the 
choice of words in SMS dialogs as a key property, 
which needs to be quantified. After creating an 
annotated corpus and developing a classification 
method based on topic distinctive word lists, we 
have presented initial, promising results, which 
encourage further research.  
Our study exposes also some challenges, which 
may not be easy to address. It would be useful to 
have a larger annotated corpus. Fully automated 
annotation of topics seems hardly achievable in 
view of our results. We may therefore rely on 
semi-supervised or unsupervised learning algo-
rithms. Moreover, the study explores the relation 
of topics to single words. It needs to be enhanced 
Topic DWS Language 
models 
TPR FPR TPR FPR 
Activities & events 81.9 34.7 64.1 22.8 
Appoint. & schedule 69.5 31.0 82.6 21.4 
Transportation 78.7 17.3 68.8 9.8 
Finance & property 77.9 17.0 76.5 9.6 
Food & drinks 88.4 11.7 74.1 10.6 
School & work 80.9 22.4 54.3 14.0 
Technology 92.4 28.7 75.5 12.6 
Emotion 80.7 34.4 71.3 12.7 
Human relation 72.2 34.7 69.8 20.8 
 80.3 26.8 70.7 14.9 
192
to phrases, because SMS dictation by voice relies 
on higher order n-gram SLMs.  
In summary, when taking the next step and 
moving towards speech applications, we expect 
performance improvement after making topic 
knowledge useful for SMS dictation.   
References  
Agar, Jon (2003). Constant touch: A global history of 
the mobile phone. Cambridge, UK: Icon Books. 
Aw, AiTi, Zhang, Min, Xiao, Juan & Su, Jian (2006). A 
phrase-based statistical model for SMS text normali-
zation. In Proceedings of COLING/ACL, Sidney, 
AU. 
Chafe, Wallace (1976). Givenness, contrastiveness, de-
finiteness, subjects, topics, and point of view. In Li, 
Charles N. (Ed.), Subject and Topic (pp. 25-55). New 
York: Academic Press. 
Cook, Paul & Stevenson, Suzanne (2009). An unsuper-
vised model for text message normalization. In Pro-
ceedings of the NAACL HLT, Boulder, CO. 
Fairon, C?drick & Paumier, S?bastien (2006). A trans-
lated corpus of 30,000 French SMS. In Proceedings 
of LREC, Genova 
Goggin, Gerard (2006). Cell phone culture: Mobile 
technology in everyday life. New York: Routledge. 
Hecht, Ron M., et. al. (2009). Information Bottleneck 
based age verification. In Proceedings of Interspeech, 
Brighton, UK. 
Kobus, Catherine, Yvon, Francois & Damnati, Geral-
dine (2008). Normalizing SMS: are two metaphors 
better than one? In Proceedings of COLING, Man-
chester, UK. 
Kothari, Govind, Negi, Sumit & Faruquie, Tanveer A. 
(2009). SMS based interface for FAQ retrieval. In 
Proceedings of ACL, Singapore. 
Lambert, Lynn & Carberry, Sandra (1992). Using lin-
guistic, world, and contextual knowledge in a plan 
recognition model of dialogue. In Proceedings of the 
14th International Conference on Computational 
Linguistics. 
Lenhart, Amanda, et. al. (2010). Teens and mobile 
phones. From Pew Research Center 
http://pewinternet.org/Reports/2010/Teens-and-
Mobile-Phones.aspx 
Levinson, Stephen C. (1983). Pragmatics. Cambridge: 
Cambridge University Press. 
Li, Charles N. & Thompson, Sandra A. (1976). Subject 
and topic. A new typology of languages. In Li, 
Charles N. (Ed.), Subject and Topic (pp. 457-490). 
New York: Academic Press. 
McEnery, Tony, Xiao, Richard & Tono, Yukio (2006). 
Corpus-based language studies. An advanced re-
source book. London, New York: Routledge. 
Meyer, Charles F. (2002). English corpus linguistics. 
An introduction. Cambridge: Cambridge University 
Press. 
Moln?r, Valer?a (1993). Zur Pragmatik und Grammatik 
des Topik-Begriffes. In Reis, Marga (Ed.), 
Wortstellung und Informationsstruktur (pp. 155-202). 
T?bingen: Niemeyer. 
Pennell, Deana L. & Liu, Yang (2010). Normalization 
of text messages for text-to-speech. In Proceedings of 
ICASSP, Dallas, TX. 
Salton, Gerard, Wong, Anita & Yang, Chung-Shu 
(1975). A Vector Space Model for automatic index-
ing. In Proceedings of Communications of the ACM,  
18(11), 613?620. 
Slonim, Noam & Tishby, Naftali (2000). Agglomerative 
Information Bottleneck. In Proceedings of NIPS 12. 
Stutterheim, Christiane von (1997). Einige Prinzipien 
des Textaufbaus. Empirische Untersuchungen zur 
Produktion m?ndlicher Texte. T?bingen: Niemeyer. 
Thurlow, Crispin (2003). Generation txt? The sociolin-
guistics of young people's text-messaging. From Dis-
course Analysis Online 
http://extra.shu.ac.uk/daol/articles/v1/n1/a3/thurlow2
002003-01.html 
Thurlow, Crispin & Poff, Michele (2009). The language 
of text messaging. In Herring, Susan C., Stein, Dieter 
& Virtanen, Tuija (Eds.), Handbook of the Pragmat-
ics of CMC. Berlin and New York: Mouton de Gruy-
ter. 
Tishby, Naftali, Pereira, Fernando C. & Bialek, William 
(1999). The Information Bottleneck method. In Pro-
ceedings of 37th annual Allerton conference on 
communication, control and computing, Monticello, 
IL. 
Wallis, Sean & Nelson, Gerald (2001). Knowledge dis-
covery in grammatically analysed corpora. Data Min-
ing and Knowledge Discovery, 5(4), 305-335. 
Wu, Wei, Ju, Yun-Cheng, Li, Xiao & Wang, Ye-Yi 
(2010). Paraphrase detection on SMS messages in 
automobiles. In Proceedings of ICASSP, Dallas, TX.  
 
 
193
