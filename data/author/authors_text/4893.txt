Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 161?169,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Review Sentiment Scoring via a Parse-and-Paraphrase Paradigm 
 
 
Jingjing Liu, Stephanie Seneff 
MIT Computer Science & Artificial Intelligence Laboratory 
32 Vassar Street, Cambridge, MA 02139  
{jingl, seneff}@csail.mit.edu 
 
  
 
Abstract 
 
This paper presents a parse-and-paraphrase pa-
radigm to assess the degrees of sentiment for 
product reviews. Sentiment identification has 
been well studied; however, most previous 
work provides binary polarities only (positive 
and negative), and the polarity of sentiment is 
simply reversed when a negation is detected. 
The extraction of lexical features such as uni-
gram/bigram also complicates the sentiment 
classification task, as linguistic structure such 
as implicit long-distance dependency is often 
disregarded. In this paper, we propose an ap-
proach to extracting adverb-adjective-noun 
phrases based on clause structure obtained by 
parsing sentences into a hierarchical represen-
tation. We also propose a robust general solu-
tion for modeling the contribution of adver-
bials and negation to the score for degree of 
sentiment. In an application involving extract-
ing aspect-based pros and cons from restaurant 
reviews, we obtained a 45% relative improve-
ment in recall through the use of parsing me-
thods, while also improving precision. 
 
1 Introduction 
Online product reviews have provided an exten-
sive collection of free-style texts as well as prod-
uct ratings prepared by general users, which in 
return provide grassroots contributions to users 
interested in a particular product or service as 
assistance. Yet, valuable as they are, free-style 
reviews contain much noisy data and are tedious 
to read through in order to reach an overall con-
clusion. Thus, we conducted this study to auto-
matically process and evaluate product reviews 
in order to generate both numerical evaluation 
and textual summaries of users? opinions, with 
the ultimate goal of adding value to real systems 
such as a restaurant-guide dialogue system. 
Sentiment summarization has been well stu-
died in the past decade (Turney, 2002; Pang et al, 
2002; Dave et al, 2003; Hu and Liu, 2004a, 
2004b; Carenini et al, 2006; Liu et al, 2007). 
The polarity of users? sentiments in each seg-
ment of review texts is extracted, and the polari-
ties of individual sentiments are aggregated 
among all the sentences/segments of texts to give 
a numerical scaling on sentiment orientation.  
 Most of the work done for sentiment analysis 
so far has employed shallow parsing features 
such as part-of-speech tagging. Frequent adjec-
tives and nouns/noun phrases are extracted as 
opinion words and representative product fea-
tures. However, the linguistic structure of the 
sentence is usually not taken into consideration. 
High level linguistic features, if well utilized and 
accurately extracted, can provide much insight 
into the semantic meaning of user opinions and 
contribute to the task of sentiment identification. 
Furthermore, in addition to adjectives and 
nouns, adverbials and negation also play an im-
portant role in determining the degree of the 
orientation level. For example, ?very good? and 
?good? certainly express different degrees of 
positive sentiment. Also, in previous studies, 
when negative expressions are identified, the 
polarity of sentiment in the associated segment 
of text is simply reversed. However, semantic 
expressions are quite different from the absolute 
opposite values in mathematics. For example, 
?not bad? does not express the opposite meaning 
of ?bad?, which would be highly positive. Simp-
ly reversing the polarity of sentiment on the ap-
pearance of negations may result in inaccurate 
interpretation of sentiment expressions. Thus, a 
system which attempts to quantify sentiment 
while ignoring adverbials is missing a significant 
component of the sentiment score, especially if 
the adverbial is a negative word. 
161
Another challenging aspect of negation is 
proper scoping of the negative reference over the 
right constituent, which we argue, can be han-
dled quite well with careful linguistic analysis. 
Take the sentence ?I don?t think the place is very 
clean? as example. A linguistic approach asso-
ciating long-distance elements with semantic 
relations can identify that the negation ?not? 
scopes over the complement clause, thus extract-
ing ?not very clean? instead of ?very clean?.  
Our goal in modeling adverbials is to investi-
gate whether a simple linear correction model 
can capture the polarity contribution of all ad-
verbials. Furthermore, is it also appropriate to 
adjust for multiple adverbs, including negation, 
via a linear additive model? I.e., can ?not very 
good? be modeled as not(very(good))? The fact 
that ?not very good? seems to be less negative 
than ?not good? suggests that such an algorithm 
might work well. From these derivations we have 
developed a model which treats negations in the 
exact same way as modifying adverbs, via an 
accumulative linear offset model. This yields a 
very generic and straightforward solution to 
modeling the strength of sentiment expression. 
In this paper we utilize a parse-and-paraphrase 
paradigm to identify semantically related phrases 
in review texts, taking quantifiers (e.g., modify-
ing adverbs) and qualifiers (e.g., negations) into 
special consideration. The approach makes use 
of a lexicalized probabilistic syntactic grammar 
to identify and extract sets of adverb-adjective-
noun phrases that match review-related patterns. 
Such patterns are constructed based on well-
formed linguistic structure; thus, relevant phrases 
can be extracted reliably. 
We also propose a cumulative linear offset 
model to calculate the degree of sentiment for 
joint adjectives and quantifiers/qualifiers. The 
proposed sentiment prediction model takes mod-
ifying adverbs and negations as universal scales 
on strength of sentiment, and conducts cumula-
tive calculation on the degree of sentiment for 
the associated adjective. With this model, we can 
provide not only qualitative textual summariza-
tion such as ?good food? and ?bad service?, but 
also a numerical scoring of sentiment, i.e., ?how 
good the food is? and ?how bad the service is.? 
2 Related Work  
There have been many studies on sentiment 
classification and opinion summarization (Pang 
and Lee, 2004, 2005; Gamon et al, 2005; Popes-
cu and Etzioni, 2005; Liu et al, 2005; Zhuang et 
al., 2006; Kim and Hovy, 2006). Specifically, 
aspect rating as an interesting topic has also been 
widely studied (Titov and McDonald, 2008a; 
Snyder and Barzilay, 2007; Goldberg and Zhu, 
2006). Recently, Baccianella et. al. (2009) 
conducted a study on multi-facet rating of 
product reviews with special emphasis on how to 
generate vectorial representations of the text by 
means of POS tagging, sentiment analysis, and 
feature selection for ordinal regression learning. 
Titov and McDonald (2008b) proposed a joint 
model of text and aspect ratings which utilizes a 
modified LDA topic model to build topics that 
are representative of ratable aspects, and builds a 
set of sentiment predictors. Branavan et al (2008) 
proposed a method for leveraging unstructured 
annotations in product reviews to infer semantic 
document properties, by clustering user 
annotations into semantic properties and tying 
the induced clusters to hidden topics in the text.  
3 System Overview 
Our review summarization task is to extract sets 
of descriptor-topic pairs (e.g., ?excellent service?) 
from a set of reviews (e.g., for a particular res-
taurant), and to cluster the extracted phrases into 
representative aspects on a set of dimensions 
(e.g., ?food?, ?service? and ?atmosphere?). Dri-
ven by this motivation, we propose a three-stage 
system that automatically processes reviews. A 
block diagram is given in Figure 1.  
 
 
Figure 1.  Framework of review processing. 
 
The first stage is sentence-level data filtering. 
Review data published by general users is often 
in free-style, and a large fraction of the data is 
either ill-formed or not relevant to the task. We 
classify these as out of domain sentences. To fil-
ter out such noisy data, we collect unigram statis-
tics on all the relevant words in the corpus, and 
select high frequency adjectives and nouns. Any 
sentence that contains none of the high-
frequency nouns or adjectives is rejected from 
further analysis. The remaining in-domain sen-
tences are subjected to the second stage, parse 
162
analysis and semantic understanding, for topic 
extraction.  
From the parsable sentences we extract de-
scriptor-topic phrase patterns based on a careful-
ly-designed generation grammar. We then apply 
LM (language model) based topic clustering to 
group the extracted phrases into representative 
aspects. The third stage scores the degree of sen-
timent for adjectives, as well as the strength of 
sentiment for modifying adverbs and negations, 
which further refine the degree of sentiment of 
the associated adjectives. We then run a linear 
additive model to assign a combined sentiment 
score for each extracted phrase. 
The rest of the paper is structured as follows: 
In Section 4, we explain the linguistic analysis. 
In Section 5, we describe the cumulative model 
for assessing the degree of sentiment. Section 6 
provides a systematic evaluation, conducted on 
real data in the restaurant review domain har-
vested from the Web. Section 7 provides a dis-
cussion analyzing the results. Section 8 summa-
rizes the paper as well as pointing to future work. 
4 Linguistic Analysis 
4.1 Parse-and-Paraphrase 
Our linguistic analysis is based on a parse-and-
paraphrase paradigm. Instead of the flat structure 
of a surface string, the parser provides a hierar-
chical representation, which we call a linguistic 
frame (Xu et al, 2008). It preserves linguistic 
structure by encoding different layers of seman-
tic dependencies. The grammar captures syntac-
tic structure through a set of carefully con-
structed context free grammar rules, and employs 
a feature-passing mechanism to enforce long dis-
tance constraints. The grammar is lexicalized, 
and uses a statistical model to rank order compet-
ing hypotheses. It knows explicitly about 9,000 
words, with all unknown words being interpreted 
as nouns. The grammar probability model was 
trained automatically on the corpus of review 
sentences. 
To produce the phrases, a set of generation 
rules is carefully constructed to only extract sets 
of related adverbs, adjectives and nouns. The 
adjective-noun relationships are captured from 
the following linguistic patterns: (1) all adjec-
tives attached directly to a noun in a noun phrase, 
(2) adjectives embedded in a relative clause 
modifying a noun, and (3) adjectives related to 
nouns in a subject-predicate relationship in a 
clause. These patterns are compatible, i.e., if a 
clause contains both a modifying adjective and a 
predicate adjective related to the same noun, two 
adjective-noun pairs are generated by different 
patterns. As in, ?The efficient waitress was none-
theless very courteous.? It is a ?parse-and-
paraphrase-like? paradigm: the paraphrase tries 
to preserve the original words intact, while reor-
dering them and/or duplicating them into mul-
tiple NP units. Since they are based on syntactic 
structure, the generation rules can also be applied 
in any other domain involving opinion mining. 
An example linguistic frame is shown in Fig-
ure 2, which encodes the sentence ?The caesar 
with salmon or chicken is really quite good.? In 
this example, for the adjective ?good?, the near-
by noun ?chicken? would be associated with it if 
only proximity is considered. From the linguistic 
frame, however, we can easily associate ?caesar? 
with ?good? by extracting the head of the topic 
sub-frame and the head of the predicate sub-
frame, which are encoded in the same layer (root 
layer) of the linguistic frame. Also, we can tell 
from the predicate sub-frame that there is an ad-
verb ?quite? modifying the head word ?good?. 
The linguistic frame also encodes an adverb ?re-
ally? in the upstairs layer. A well-constructed 
generation grammar can create customized ad-
verb-adjective-noun phrases such as ?quite good 
caesar? or ?really quite good caesar?. 
{c cstatement 
  :topic {q caesar 
             :quantifier "def" 
             :pred {p with :topic {q salmon 
                                         :pred {p conjunction 
                                           :or {q chicken  }}}} 
  :adv "really" 
   :pred {p adj_complement 
            :pred {p adjective 
                    :adv "quite" 
                :pred {p quality :topic "good" }}}} 
Figure 2.  Linguistic frame for ?The caesar with 
salmon or chicken is really quite good.? 
Interpreting negation in English is not 
straightforward, and it is often impossible to do 
correctly without a deep linguistic analysis. Xu-
ehui Wu (2005) wrote: ?The scope of negation is 
a complex linguistic phenomenon. It is easy to 
perceive but hard to be defined from a syntactic 
point of view. Misunderstanding or ambiguity 
may occur when the negative scope is not un-
derstood clearly and correctly.? The majority 
rule for negation is that it scopes over the re-
mainder of its containing clause, and this works 
well for most cases. For example, Figure 3 shows 
163
the linguistic frame for the sentence ?Their menu 
was a good one that didn?t try to do too much.?  
{c cstatement   
:topic {q menu   :poss "their" } } 
   :complement {q pronoun   :name ?one? 
             :adj_clause {c cstatement 
                           :conjn "that" 
                           :negate "not" 
                           :pred {p try :to_clause  {p do 
                                           :topic {q object 
                                           :adv "too" 
                                           :quant "much" }}}} 
   :pred {p adjective 
                :pred {p quality :topic "good" }}} 
Figure 3.  Linguistic frame for ?Their menu was a 
good one that didn?t try to do too much.? 
Traditional approaches which do not consider 
the linguistic structure would treat the appear-
ance of ?not? as a negation and simply reverse 
the sentiment of the sentence to negative polarity, 
which is wrong as the sentence actually ex-
presses positive opinion for the topic ?menu?. In 
our approach, the negation ?not? is identified as 
under the sub-frame of the complement clause, 
instead of in the same or higher layer of the ad-
jective sub-frame; thus it is considered as unre-
lated to the adjective ?good?. In this way we can 
successfully predict the scope of the reference of 
the negation over the correct constituent of a sen-
tence and create proper association between ne-
gation and its modified words. 
4.2 LM-based Topic Clustering 
To categorize the extracted phrases into repre-
sentative aspects, we automatically group the 
identified topics into a set of clusters based on 
LM probabilities. The LM-based algorithm as-
sumes that topics which are semantically related 
have high probability of co-occurring with simi-
lar descriptive words. For example, ?delicious? 
might co-occur frequently with both ?pizza? and 
?dessert?. By examining the distribution of bi-
gram probability of these topics with correspond-
ing descriptive words, we can group ?pizza? and 
?dessert? into the same cluster of ?food?. 
We select a small set of the most common top-
ics, i.e., topics with the highest frequency counts, 
and put them into an initial set I. Then, for each 
candidate topic  outside set I, we calculate its 
probability given each topic  within the initial 
set I, given by:  
       | 	
  ? |
 ? |	
  
                       ? ,

 ? 
,


   
                     
?


 ? , 
 ? , 	
        (1) 
where A represents the set of all the adjectives in 
the corpus. For each candidate topic  , we 
choose the cluster of the initial topic   with 
which it has the highest probability score.  
There are also cases where a meaningful ad-
jective occurs in the absence of an associated 
topic, e.g., ?It is quite expensive.? We call such 
cases the ?widow-adjective? case. Without hard-
coded ontology matching, it is difficult to identi-
fy ?expensive? as a price-related expression. To 
discover such cases and associate them with re-
lated topics, we propose a ?surrogate topic? 
matching approach based on bigram probability.  
As aforementioned, the linguistic frame orga-
nizes all adjectives into separate clauses. Thus, 
we create a ?surrogate topic? category in the lin-
guistic frames for widow-adjective cases, which 
makes it easy to detect descriptors that are affi-
liated with uninformative topics like the pronoun 
?it?. We then have it generate phrases such as 
?expensive surrogate_topic? and use bigram 
probability statistics to automatically map each 
sufficiently strongly associated adjective to its 
most common topic among our major classes, 
e.g., mapping ?expensive? with its surrogate top-
ic ?price?. Therefore, we can generate sets of 
additional phrases in which the topic is ?halluci-
nated? from the widow-adjective.  
5 Assessment of Sentiment Strength 
5.1 Problem Formulation 
Given the sets of adverb-adjective-noun phrases 
extracted by linguistic analysis, our goal is to 
assign a score for the degree of sentiment to each 
phrase and calculate an average rating for each 
aspect. An example summary is given in Table 1. 
Table 1. Example of review summary. 
Aspect Extracted phrases Rating 
Atmosphere very nice ambiance, 
outdoor patio 4.8 
Food not bad meal, quite authentic food 4.1 
Place not great place, 
very smoky restaurant 2.8 
Price so high bill, high cost, 
not cheap price 2.2 
To calculate the numerical degree of sentiment, 
there are three major problems to solve: 1) how 
to associate numerical scores with textual senti-
ment; 2) whether to calculate sentiment scores 
for adjectives and adverbs jointly or separately; 3) 
164
whether to treat negations as special cases or in 
the same way as modifying adverbs.  
There have been studies on building sentiment 
lexicons to define the strength of sentiment of 
words. Esuli and Sebastiani (2006) constructed a 
lexical resource, SentiWordNet, a WordNet-like 
lexicon emphasizing sentiment orientation of 
words and providing numerical scores of how 
objective, positive and negative these words are. 
However, lexicon-based methods can be tedious 
and inefficient and may not be accurate due to 
the complex cross-relations in dictionaries like 
WordNet. Instead, our primary approach to sen-
timent scoring is to make use of collective data 
such as user ratings. In product reviews collected 
from online forums, the format of a review entry 
often consists of three parts: pros/cons, free-style 
text and user rating. We assume that user rating 
is normally consistent with the tone of the review 
text published by the same user. By associating 
user ratings with each phrase extracted from re-
view texts, we can easily associate numerical 
scores with textual sentiment.  
A simple strategy of rating assignment is to 
take each extracted adverb-adjective pair as a 
composite unit. However, this method is likely to 
lead to a large number of rare combinations, thus 
suffering from sparse data problems. Therefore, 
an interesting question to ask is whether it is 
feasible to assign to each adverb a perturbation 
score, which adjusts the score of the associated 
adjective up or down by a fixed scalar value. 
This approach thus hypothesizes that ?very ex-
pensive? is as much worse than ?expensive? as 
?very romantic? is better than ?romantic?. This 
allows us to pool all instances of a given adverb 
regardless of which adjective it is associated with, 
in order to compute the absolute value of the per-
turbation score for that adverb. Therefore, we 
consider adverbs and adjectives separately when 
calculating the sentiment score, treating each 
modifying adverb as a universal quantifier which 
consistently scales up/down the strength of sen-
timent for the adjectives it modifies. 
Furthermore, instead of treating negation as a 
special case, the universal model works for all 
adverbials. The model hypothesizes that ?not bad? 
is as much better than ?bad? as ?not good? is 
worse than ?good?, i.e., negations push posi-
tive/negative adjectives to the other side of sen-
timent polarity by a universal scale. This again, 
allows us to pool all instances of a given nega-
tion and compute the absolute value of the per-
turbation score for that negation, in the same way 
as dealing with modifying adverbs.  
5.2 Linear Additive Model 
For each adjective, we average all its ratings giv-
en by: 
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 334?342, Prague, June 2007. c?2007 Association for Computational Linguistics
Low-Quality Product Review Detection in Opinion Summarization 
Jingjing Liu 
Nankai University 
Tianjin, China 
v-jingil@microsoft.com 
Yunbo Cao 
Microsoft Research Asia 
Beijing, China 
yucao@microsoft.com 
 
Chin-Yew Lin 
Microsoft Research Asia 
Beijing, China 
cyl@microsoft.com 
Yalou Huang 
Nankai University 
Tianjin, China 
huangyl@nankai.edu.cn 
Ming Zhou 
Microsoft Research Asia 
Beijing, China 
mingzhou@microsoft.com 
Abstract 
Product reviews posted at online shopping 
sites vary greatly in quality. This paper ad-
dresses the problem of detecting low-
quality product reviews. Three types of bi-
ases in the existing evaluation standard of 
product reviews are discovered. To assess 
the quality of product reviews, a set of spe-
cifications for judging the quality of re-
views is first defined. A classification-
based approach is proposed to detect the 
low-quality reviews. We apply the pro-
posed approach to enhance opinion sum-
marization in a two-stage framework. Ex-
perimental results show that the proposed 
approach effectively (1) discriminates low-
quality reviews from high-quality ones and 
(2) enhances the task of opinion summari-
zation by detecting and filtering low-
quality reviews. 
1 Introduction 
In the past few years, there has been an increasing 
interest in mining opinions from product reviews 
(Pang, et al 2002; Liu, et al 2004; Popescu and 
Etzioni, 2005). However, due to the lack of 
editorial and quality control, reviews on products 
vary greatly in quality. Thus, it is crucial to have a 
mechanism capable of assessing the quality of 
reviews and detecting low-quality/noisy reviews.  
Some shopping sites already provide a function 
of assessing the quality of reviews. For example, 
Amazon1 allows users to vote for the helpfulness 
of each review and then ranks the reviews based on 
the accumulated votes. However, according to our 
survey in Section 3, users? votes at Amazon have 
three kinds of biases as follows: (1) imbalance vote 
bias, (2) winner circle bias, and (3) early bird bias. 
Existing studies (Kim et al 2006; Zhang and Va-
radarajan, 2006) used these users? votes for train-
ing ranking models to assess the quality of reviews, 
which therefore are subject to these biases.  
In this paper, we demonstrate the aforemen-
tioned biases and define a standard specification to 
measure the quality of product reviews. We then 
manually annotate a set of ground-truth with real 
world product review data conforming to the speci-
fication.  
To automatically detect low-quality product re-
views, we propose a classification-based approach 
learned from the annotated ground-truth. The pro-
posed approach explores three aspects of product 
reviews, namely informativeness, readability, and 
subjectiveness.  
We apply the proposed approach to opinion 
summarization, a typical opinion mining task. The 
proposed approach enhances the existing work in a 
two-stage framework, where the low-quality re-
view detection is applied right before the summari-
zation stage.  
Experimental results show that the proposed ap-
proach can discriminate low-quality reviews from 
high-quality ones effectively. In addition, the task 
of opinion summarization can be enhanced by de-
tecting and filtering low-quality reviews. 
                                                 
1 http://www.amazon.com 
334
The rest of the paper is organized as follows: 
Section 2 introduces the related work. In Section 3, 
we define the quality of product reviews. In Sec-
tion 4, we present our approach to detecting low-
quality reviews. In Section 5, we empirically verify 
the effectiveness of the proposed approach and its 
use for opinion summarization. Section 6 summa-
rizes our work in this paper and points out the fu-
ture work. 
2 Related Work 
2.1 Evaluating Helpfulness of Reviews 
The problem of evaluating helpfulness of reviews 
(Kim et al 2006), also known as learning utility of 
reviews (Zhang and Varadarajan, 2006), is quite 
similar to our problem of assessing the quality of 
reviews.  
In practice, researchers in this area considered 
the problem as a ranking problem and solved it 
with regression models. In the process of model 
training and testing, they used the ground-truth 
derived from users? votes of helpfulness provided 
by Amazon. As we will show later in Section 3, 
these models all suffered from three types of vot-
ing bias.  
In our work, we avoid using users? votes by de-
veloping a specification on the quality of reviews 
and building a ground-truth according to the speci-
fication.  
2.2 Mining Opinions from Reviews 
One area of research on opinion mining from 
product reviews is to judge whether a review 
expresses a positive or a negative opinion. For 
example, Turney (2006) presented a simple 
unsupervised learning algorithm in judging 
reviews as ?thumbs up? (recommended) or 
?thumbs down? (not recommended). Pang et al
(2002) considered the same problem and presented 
a set of supervised machine learning approaches to 
it. For other work see also Dave et al (2003), Pang 
and Lee (2004, 2005). 
Another area of research on opinion mining is to 
extract and summarize users? opinions from prod-
uct reviews (Hu and Liu, 2004; Liu et al, 2005; 
Popescu and Etzioni, 2005). Typically, a sentence 
or a text segment in the reviews is treated as the 
basic unit. The polarity of users? sentiments on a 
product feature in each unit is extracted. Then the 
aggregation of the polarities of individual senti-
ments is presented to users so that they can have an 
at-a-glance view on how other experienced users 
rated on a certain product. The major weakness in 
the existing studies is that all the reviews, includ-
ing low-quality ones, are taken into consideration 
and treated equally for generating the summary. In 
this paper, we enhance the application by detecting 
and filtering low-quality reviews. In order to 
achieve that, we first define what the quality of 
reviews is. 
3 Quality of Product Reviews 
In this section, we will first show three biases of 
users? votes observed on Amazon, and then present 
our specification on the quality of product reviews. 
3.1 Amazon Ground-truth 
In our study, we use the product reviews on digital 
cameras crawled from Amazon as our data set. The 
data set consists of 23,141 reviews on 946 digital 
cameras. At the Amazon site, users could vote for 
a review with a ?helpful? or ?unhelpful? label. 
Thus, for each review there are two numbers 
indicating the statistics of these two labels, namely 
the number of ?helpful? votes and that of 
?unhelpful? ones. Kim et al(2006) used the 
percentage of ?helpful? votes as the measure of 
evaluating the ?quality of reviews? in their 
experiments. We call the ground-truth based on 
this measure as ?Amazon ground-truth?. 
Certainly, the ground-truth has the advantage of 
convenience. However, we identify three types of 
biases that make the Amazon ground-truth not al-
ways suitable for determining the quality of re-
views. We describe these biases in details in the 
rest of this section. 
3.1.1 Imbalance Vote Bias 
 
Figure 1. Reviews? percentage scores 
At the Amazon site, users tend to value others? 
opinions positively rather than negatively. From 
Figure 1, we can see that a half of the 23,141 
0
2000
4000
6000
8000
10000
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
# 
R
e
vi
e
w
s
Percentage of 'helpful' votes
335
reviews (corresponding to the two bars on the right 
of the figure) have more than 90% ?helpful? votes, 
including 9,100 reviews with 100% ?helpful? 
votes. From an in-depth investigation on these 
highly-voted reviews, we observed that some did 
not really have as good quality as the votes hint. 
For example, in Figure 2, the review about Canon 
PowerShot S500 receives 40 ?helpful? votes out of 
40 votes although it only gives very brief 
description on the product features in its second 
paragraph. We call this type of bias ?imbalance 
vote? bias. 
 
This is my second Canon digital elph camera. Both were great 
cameras. Recently upgraded to the S500. About 6 months later I get 
the dreaded E18 error. I searched the Internet and found numerous 
people having problems. When I determined the problem to be the 
lens not fully extending I decided to give it a tug. It clicked and the 
camera came on, ready to take pictures. Turning it off and on pro-
duced the E18 again. While turning it on I gave it a nice little bump 
on the side (where the USB connector is) and the lens popped out 
on its own. No problems since. 
 It?s a nice compact and light camera and takes great photos and 
videos. Only complaint (other than E18) is the limit of 30-second 
videos on 640x480 mode. I've got a 512MB compact flash card, I 
should be able to take as much footage as I have memory in one 
take. 
Figure 2. An example review 
3.1.2 Winner Circle Bias 
 
Figure 3. Votes of the top-50 ranked reviews 
There also exists a bootstrapping effect of ?hot? 
reviews at the Amazon site. Figure 3 shows the 
?helpful? votes for the top 50 ranked reviews. The 
numbers are averaged over 127 digital cameras 
which have no less than 50 reviews. As shown in 
this figure, the top two reviews hold more than 250 
and 140 votes respectively on average; while the 
numbers of votes held by lower-ranked reviews 
decrease exponentially. This is so-called the 
?winner circle? bias: the more votes a review 
gains, the more default authority it would appear to 
the readers, which in turn will influence the 
objectivity of the readers? votes. Also, the higher 
ranked reviews would attract more eyeballs and 
therefore gain more people?s votes. This mutual 
influence among labelers should be avoided when 
the votes are used as the evaluation standard. 
3.1.3 Early Bird Bias 
 
   
Figure 4. Dependency on publication date 
Publication date can influence the accumulation of 
users? votes. In Figure 4, the n?th publication date 
represents the n?th month after the product is 
released. The number in the figure is averaged over 
all the digital cameras in the data set. We can 
observe a clear trend that the earlier a review is 
posted, the more votes it will get. This is simply 
because reviews posted earlier are exposed to users 
for a longer time. Therefore, some high quality 
reviews may get fewer users? vote because of later 
publication. We call this ?early bird? bias. 
3.2 Specification of Quality 
Besides these aforementioned biases, using the raw 
rating from readers directly also fails to provide a 
clear guideline for what a good review consists of. 
In this section, we provide such a guideline, which 
we name as the specification (SPEC). 
In the SPEC, we define four categories of re-
view quality which represent different values of 
the reviews to users? purchase decision: ?best re-
view?, ?good review?, ?fair review?, and ?bad re-
view?. A generic description of the SPEC is as fol-
lows: 
A best review must be a rather complete and de-
tailed comment on a product. It presents several 
aspects of a product and provides convincing opi-
nions with enough evidence. Usually a best review 
could be taken as the main reference that users on-
ly need to read before making their purchase deci-
sion on a certain product. The first review in Fig-
ure 5 is a best review. It presents several product 
features and provides convincing opinions with 
sufficient evidence. It is also in a good format for 
readers to easily understand. Note that we omit 
some words in the example to save the space. 
0
50
100
150
200
250
300
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49
#V
o
te
s 
h
e
ld
 b
y 
re
vi
e
w
s
Ranking positions of reviews
0
10
20
30
40
50
60
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49
# 
V
o
te
s 
h
e
ld
 b
y 
re
vi
e
w
s
Publication Date
336
A good review is a relatively complete comment 
on a product, but not with as much supporting evi-
dence as necessary. It could be used as a strong 
and influential reference, but not as the only rec-
ommendation. The second review in Figure 5 is 
such an example. 
A fair review contains a very brief description 
on a product. It does not supply detailed evaluation 
on the product, but only comments on some as-
pects of the product. For example, the third review 
in Figure 5 mainly talks about ?the delay between 
pictures?, but less about other aspects of the cam-
era. 
A bad review is usually an incorrect description 
of a product with misleading information. It talks 
little about a specific product but much about some 
general topics (e.g. photography). For example, the 
last review in Figure 5 talks about the topic of ?ge-
neric battery?, but does not specify any digital 
camera. A bad review is an ?unhelpful? review that 
can be ignored.  
 
Best Review: 
I purchased this camera about six months ago after my Kodak 
Easyshare camera completely died on me. I did a little research 
and read only good things about this Canon camera so I decided to 
go with it because it was very reasonably priced (about $200). Not 
only did the camera live up to my expectations, it surpassed them 
by leaps and bounds! Here are the things I have loved about this 
camera: 
 
BATTERY - this camera has the best battery of any digital cam-
era I have ever owned or used. ? 
 
EASY TO USE - I was able to ? 
 
PICTURE QUALITY - all of the pictures I've taken and printed 
out have been great. ? 
 
FEATURES - I love the ability to quickly and easily ? 
 
LCD SCREEN - I was hoping ? 
 
SD MEMORY CARD - I was also looking for a camera that used 
SD memory cards. Mostly because? 
 
I cannot stress how highly I recommend this camera. I will never 
buy another digital camera besides Canon again. And the A610 (as 
well as the A620 - the 7.0MP version) is the best digital camera I've 
ever used. 
Good Review: 
The Sony DSC "P10" Digital Camera is the top pick for CSC. 
Running against cameras like Olympus stylus, Canon Powereshot, 
Sony V1, Nikon, Fuji, and More. The new release of 5.0 mega pix-
els has shot prices for digital cameras up to $1000+. This camera I 
purchased through a Private Dealer cost me $400.86. The Retail 
Price is Running $499.00 to $599.00. Purchase this camera from a 
wholesale dealer for the best price $377.00. Great Photo Even in 
dim light w/o a flash. The p10 is very compact. Can easily fit into 
any pocket. The camera can record 90 minutes of mpeg like a home 
movie. There are a lot of great digital cameras on the market that 
shoot good pictures and video. What makes the p10 the top pick is 
it comes with a rechargeable lithium battery. Many use AA batte-
ries, the digital camera consumes theses AA batteries in about two 
hours time while the unit is on. That can add continuous expense to 
the camera. It's also the best resolution on the market. 6.0 megapix 
is out, though only a few. And the smallest that we found. Also the 
best price for a major brand. 
Fair Review: 
There is nothing wrong with the 2100 except for the very notice-
able delay between pics. The camera's digital processor takes 
about 5 seconds after a photo is snapped to ready itself for the next 
one. Otherwise, the optics, the 3X optical zoom and the 2 megapixel 
resolution are fine for anything from Internet apps to 8" x 10" print 
enlarging. It is competent, not spectacular, but it gets the job done 
at an agreeable price point. 
Bad Review: 
I want to point out that you should never buy a generic battery, 
like the person from San Diego who reviewed the S410 on May 15, 
2004, was recommending. Yes you'd save money, but there have 
been many reports of generic batteries exploding when charged for 
too long. And don't think if your generic battery explodes you can 
sue somebody and win millions. These batteries are made in sweat-
shops in China, India and Korea, and I doubt you can find anybody 
to sue. So play it safe, both for your own sake and the camera's 
sake. If you want a spare, get a real Canon one. 
Figure 5. Example reviews 
3.3 Annotation of Quality 
According to the SPEC defined above, we built a 
ground-truth from the Amazon data set. We 
randomly selected 100 digital cameras and 50 
reviews for each camera. Totally we have 4,909 
reviews since some digital cameras have fewer 
than 50 unique reviews. Then we hired two 
annotators to label the reviews with the SPEC as 
their guideline. As the result, we have two 
independent copies of annotations on 4,909 
reviews, with the labels of ?best?, ?good?, ?fair?, 
and ?bad?. Table 1 shows the confusion matrix 
between the two copies of annotation. The value of 
the kappa statistic (Cohen, 1960) calculated from 
the matrix is 0.8142. This shows that the two 
annotators achieved highly consistent results by 
following the SPEC, although they worked 
independently.  
 Annota-
tion 1 
Annotation 2 
best good fair bad total 
best 294 44 2 0 340 
good 66 639 113 0 818 
fair 0 200 1,472 113 1,785 
bad 1 2 78 1,885 1,966 
total 361 885 1,665 1,998 4,909 
Table 1. Confusion matrix bet. the annotations 
 
In order to examine the difference between our 
annotations and Amazon ground-truth, we evaluate 
the Amazon ground-truth against the annotations, 
337
with the measure of ?error rate of preference pairs? 
(Herbrich et al 1999).  
????????? =
|????????? ?????????? ????? |
|??? ?????????? ?????|
 (1) 
where the ?preference pair? is defined as a pair of 
reviews with a order. For example, a best review 
and a good review correspond to a preference pair 
with the order of ?best review preferring to good 
review?. The ?all preference pairs? are collected 
from one of the annotations (the annotation 1 or 
the annotation 2) by ignoring the pairs from the 
same category. The ?incorrect preference pairs? 
are the preference pairs collected from the Ama-
zon ground-truth but not with the same order as 
that in the all preference pairs. The order of the 
preference pair collected from the Amazon 
ground-truth is evaluated on the basis of the per-
centage score as described in Section 3.1.  
The error rate of preference pairs based on the 
annotation 1 and that based on the annotation 2 are 
0.448 and 0.446, respectively, averaged over 100 
digital cameras. The high error rate of preference 
pairs demonstrates that the Amazon ground-truth 
diverges from the annotations (our ground-truth) 
significantly. 
To discover which kind of ground-truth is more 
reasonable, we ask an additional annotator (the 
third annotator) to compare these two kinds of 
ground-truth. More specifically, we randomly se-
lected 100 preference pairs whose orders the two 
kinds of ground-truth don?t agree on (called incor-
rect preference pairs in the evaluation above). As 
for our ground-truth, we choose the Annotation 1 
in the new test. Then, the third annotator is asked 
to assign a preference order for each selected pair. 
Note that the third annotator is blind to both our 
specification and the existing preference order.  
Last, we evaluate the two kinds of ground-truth 
with the new annotation. Among 100 pairs, our 
ground-truth agrees to the new annotation on 85 
pairs while the Amazon ground-truth agrees to the 
new annotation on 15 pairs. To confirm the result, 
yet another annotator (the fourth annotator) is 
called to repeat the same annotation independently 
as the third one. And we obtain the same statistical 
result (85 vs. 15) although the fourth annotator 
does not agree with the third annotator on some 
pairs. 
In practice, we treat the reviews in the first three 
categories (?best?, ?good? and ?fair?) as high-
quality reviews and those in the ?bad? category as 
low-quality reviews, since our goal is to identify 
low quality reviews that should not be considered 
when creating product review summaries. 
4 Classification of Product Reviews  
We employ a statistical machine learning approach 
to address the problem of detecting low-quality 
products reviews.  
Given a training data set? =  ?? ,?? 1
? , we 
construct a model that can minimize the error in 
prediction of y given x (generalization error). Here 
?? ? ?  and ?? = {???? ??????? , ??? ???????} 
represents a product review and a label, 
respectively. When applied to a new instance x, the 
model predicts the corresponding y and outputs the 
score of the prediction. 
4.1 The Learning Model 
In our study, we focus on differentiating low-
quality product reviews from high-quality ones. 
Thus, we treat the task as a binary classification 
problem.  
We employ SVM (Support Vector Machines) 
(Vapnik, 1995) as the model of classification. 
Given an instance x (product review), SVM assigns 
a score to it based on 
? ? = ??? + ? (2) 
where w denotes a vector of weights and b denotes 
an intercept. The higher the value of f(x) is, the 
higher the quality of the instance x is. In 
classification, the sign of f(x) is used. If it is 
positive, then x is classified into the positive 
category (high-quality reviews), otherwise into the 
negative category (low-quality reviews). 
The construction of SVM needs labeled training 
data (in our case, the categories are ?high-quality 
reviews? and ?low-quality reviews?). Briefly, the 
learning algorithm creates the ?hyper plane? in (2), 
such that the hyper plane separates the positive and 
negative instances in the training data with the 
largest ?margin?.  
4.2 Product Feature Resolution 
Product features (e.g., ?image quality? for digital 
camera) in a review are good indicators of review 
quality. However, different product features may 
refer to the same meaning (e.g., ?battery life? and 
?power?), which will bring redundancy in the 
study. In this paper, we formulize the problem as 
the ?resolution of product features?. Thus, the 
338
problem is reduced to how to determine the equi-
valence of a product feature in different forms.  
In (Hu and Liu, 2004), the matching of different 
product features is mentioned briefly and ad-
dressed by fuzzy matching. However, there exist 
many cases where the method fails to match the 
multiple mentions, e.g., ?battery life? and ?power?, 
because it only considers string similarity. In this 
paper we propose to resolve the problem by leve-
raging two kinds of evidence: one is ?surface string? 
evidence, the other is ?contextual evidence?.  
We use edit distance (Ukkonen, 1985) to com-
pare the similarity between the surface strings of 
two mentions, and use contextual similarity to re-
flect the semantic similarity between two mentions. 
When using contextual similarity, we split all 
the reviews into sentences. For each mention of a 
product feature, we take it as a query and search 
for all the relevant sentences. Then we construct a 
vector for the mention, by taking each unique term 
in the relevant sentences as a dimension of the vec-
tor. The cosine similarity between two vectors of 
mentions is then present to measure the contextual 
similarity between two mentions.  
4.3 Feature Development for Learning 
To detect low-quality reviews, our proposed 
approach explores three aspects of product reviews, 
namely informativeness, subjectiveness, and 
readability. We denote the features employed for 
learning as ?learning features?, discriminative from 
the ?product features? we discussed above. 
4.3.1 Features on Informativeness 
As for informativeness, the resolution of product 
features is employed when we generate the 
learning features as listed below. Pairs mapping to 
the same product feature will be treated as the 
same product feature, when we calculate the 
frequency and the number of product features. We 
apply the approach proposed in (Hu and Liu, 2004) 
to extract product features.  
We also use a list of product names and a list of 
brand names to generate the learning features. Both 
lists can be collected from the Amazon site be-
cause they are relatively stable within a time inter-
val. 
The learning features on the informativeness of 
a review are as follows. 
? Sentence level (SL) 
? The number of sentences in the review 
? The average length of sentences  
? The number of sentences with product features 
? Word level (WL) 
? The number of words in the review 
? The number of products (e.g., DMC-FZ50, 
EX-Z1000) in the review 
? The number of products in the title of a review  
? The number of brand names (e.g., Canon, Sony) 
in the review  
? The number of brand names in the title of a 
review 
? Product feature level (PFL) 
? The number of product features in the review 
? The total frequency of product features in the 
review 
? The average frequency of product features in 
the review 
? The number of product features in the title of a 
review 
?  The total frequency of product features in the 
title of a review 
4.3.2 Features on Readability 
We make use of several features at paragraph level 
which indicate the underlying structure of the 
reviews.  These features include, 
? The number of paragraphs in the review 
? The average length of paragraphs in the review 
? The number of paragraph separators in the re-
view 
Here, we refer to the keywords, such as ?Pros? 
vs. ?Cons? as ?paragraph separators?. The key-
words usually appear at the beginning of para-
graphs for categorizing two contrasting aspects of 
a product. We extract the nouns and noun phrases 
at the beginning of each paragraph from the 4,909 
reviews and use the most frequent 30 pairs of key-
words as paragraph separators. Table 2 provides 
some examples of the extracted separators. 
Separators Separators 
Positive Negative Positive Negative 
Pros Cons The Good The Bad 
Strength Weakness Thumb up Bummer 
PLUSES MINUSES Positive Negative 
Advantages Drawbacks Likes Dislikes 
The  upsides Downsides 
GOOD 
THINGS 
BAD 
THINGS 
Table 2. Examples of paragraph separators 
339
4.3.3 Features on Subjectiveness 
We also take the subjectiveness of reviews into 
consideration. Unlike previous work (Kim et al 
2006; Zhang and Varadarajan, 2006) using shallow 
syntactic information directly, we use a sentiment 
analysis tool (Hu and Liu, 2004) which aggregates 
a set of shallow syntactic information. The tool is a 
classifier capable of determining the sentiment 
polarity of each sentence. We create three learning 
features regarding the subjectiveness of reviews. 
? The percentage of positive sentences in the 
review 
? The percentage of negative sentences in the 
review 
? The percentage of subjective sentences (re-
gardless of positive or negative) in the review 
5 Experiments 
In this section, we describe our experiments with 
the proposed classification-based approach to low-
quality review detection, and its effectiveness on 
the task of opinion summarization. 
5.1 Detecting Low-quality Reviews 
In our proposed approach, the problem of assessing 
quality of reviews is formalized as a binary classi-
fication problem. We conduct experiments by tak-
ing reviews in the categories of ?best?, ?good?, and 
?fair? as high-quality reviews and those in the 
?bad? category as low-quality reviews.  
As for classification model, we utilize the 
SVMLight toolkit (Joachims, 2004). We randomly 
divide the 100 queries of digital cameras into two 
sets, namely a training set of 50 queries and a test 
set of 50 queries. For the two copies of annota-
tions, we use the same division. We use the train-
ing set from ?annotation 1? to train the model and 
apply the model to the test sets from both ?annota-
tion 1? and ?annotation 2?, respectively. Table 3 
reports the accuracies of our approach to review 
classification. The accuracy is defined as the per-
centage of correctly classified reviews. 
We take the approach that utilizes only the cate-
gory of features on sentence level (SL) as the base-
line, and incrementally add other categories of fea-
tures on informativeness, readability and subjec-
tiveness. We can see that both the features on word 
level (WL) and those on product feature level (PFL) 
can improve the performance of classification 
much. The features on readability can still increase 
the accuracy although the contribution is much 
less. The features on subjectiveness, however, 
make no contribution.   
 
Feature Category Annotation1 Annotation2 
Informative-
ness  
SL 73.59% 72.81% 
WL 80.41% 79.15% 
PFL 83.30% 82.37% 
Readability 83.93% 82.91% 
Subjectiveness 83.84% 82.96% 
Table 3. Low-quality reviews detection 
We also conduct a more detailed analysis on 
each individual feature. Two categories of features 
on ?title? and ?brand name? have poor perfor-
mance, which is due to the lack of information in 
the title and the low coverage of brand names in a 
review, respectively. 
5.2 Summarizing Sentiments of Reviews 
One potential application of low-quality review 
detection is the opinion summarization of reviews.  
The process of opinion summarization of re-
views with regards to a query of a product consists 
of the following steps (Liu et al 2005): 
1. From each of the reviews, identify every text 
segment with opinion in the review, and de-
termine the polarities of the opinion segments. 
2. For each product feature, generate a positive 
opinion set and a negative opinion set of opi-
nion segments, denoted as POS(?) 
and NOS(?). 
3. For each product feature, aggregate the num-
bers of segments in POS(?)  andNOS(?) , as 
opinion summarization on the product feature. 
In this process, all the reviews contribute the 
same. However, different reviews do hold different 
authorities. A positive/negative opinion from a 
high-quality review should not have the same 
weight as that from a low-quality review.  
We use a two-stage approach to enhance the re-
liability of summarization. That is, we add a 
process of low-quality review detection before the 
summarization process, so that the summarization 
result is obtained based on the high-quality reviews 
only. We are to demonstrate how much difference 
the proposed two-stage approach can bring into the 
opinion summarization. 
We use the best classification model trained as 
described in Section 5.1 to filter low-quality re-
views, and do summarization on the high-quality 
340
reviews associated to the 50 test queries. We de-
note the proposed approach and the old approach 
as ?two-stage? and ?one-stage?, respectively. Due 
to the limited space, we only give a visual compar-
ison of the two approaches on ?image quality? in 
Figure 6. The upper figure shows the summariza-
tion of positive opinions and the lower figure 
shows that of negative opinions. From the figures 
we can see that the two-stage approach preserves 
fewer text segments as the result of filtering out 
many low-quality product reviews. 
 
 
 
Figure 6. Summarization on ?image quality? 
To show the comparison on more features in a 
compressed space, we give the statistic ratio of 
change between two approaches instead. As for the 
evaluation measure, we define ?RatioOfChange? 
(ROC) on a feature f as, 
 
ROC ? =
Rateone?stage  ? ? Ratetwo?stage (?)
Rateone?stage (?)
 (3) 
 
where Rate *(f) is defined as, 
  
Rate?(?) =
|POS(?)|
|POS(?)| + |NOS(?)|
 (4) 
 
Table 4 shows some statistic results on ROC on 
five product features, namely ?image quality?(IQ), 
?battery?, ?LCD screen? (LCD), ?flash? and ?mov-
ie mode? (MM). The values in the cells are the 
percentage of queries whose ROC is larger/smaller 
than the respective thresholds. We can see that a 
large portion of queries have big changes on the 
values of ROC. This means that the result achieved 
by the two-stage approach is substantially different 
from that achieved by the one-stage approach. 
 
%Query 
RatioOfChange (+) 
>0.30 >0.25 >0.20 >0.15 >0.10 >0.05 
IQ 2% 4% 4% 10% 14% 22% 
Battery 10% 14% 18% 30% 38% 50% 
LCD  12% 18% 20% 22% 24% 28% 
Flash  6% 10% 16% 20% 26% 42% 
MM 6% 8% 8% 12% 18% 26% 
%Query 
RatioOfChange (-) 
<-0.30 <-0.25 <-0.20 <-0.15 <-0.10 <-0.05 
IQ 4% 6% 10% 14% 18% 44% 
Battery 2% 4% 4% 10% 14% 22% 
LCD  4% 4% 8% 12% 22% 28% 
Flash  4% 6% 8% 16% 18% 28% 
MM 8% 10% 16% 18% 34% 42% 
Table 4. RatioOfChange on five features 
There is no standard way to evaluate the quality 
of opinion summarization as it is rather a subjec-
tive problem. In order to demonstrate the impact of 
the two-stage approach, we turn to external author-
itative sources other than Amazon.com as the ob-
jective evaluation reference. We observe that 
CNET2 provides a professional ?editor?s review? 
for many products, which gives a rating in the 
range of 1~10 on product features. 9 digital cam-
eras out of the 50 test queries are found to have the 
editor?s rating on ?image quality? at CNET. We 
use this rating to compare with the results of our 
opinion summarization. We rescale the Rate scores 
obtained by both the one-stage approach and the 
two-stage approach into the range of 1-10 in order 
to perform the comparison.  
Figure 7 provides the visual comparison. We 
can see that the result achieved by the two-stage 
approach has a much better (closer) resemblance to 
CNET rating than one-stage approach does. This 
indicates that our two-stage approach can achieve a 
more consistent summarization result to the profes-
sional evaluations by the editors. Although the 
CNET rating is not the absolute standard for prod-
uct evaluation, it provides a professional yet objec-
tive evaluation of the products. Therefore, the ex-
perimental results demonstrate that our proposed 
approach could achieve more reliable opinion 
summarization which is closer to the generic eval-
uation from authoritative sources. 
 
                                                 
2 http://www.cnet.com 
0
30
60
90
120
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49
N
u
m
b
e
r 
o
f 
su
p
p
o
rt
in
g 
se
n
te
n
ce
s 
(P
o
si
ti
ve
)
QueryID
One-stage Two-stage
0
20
40
60
80
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49
N
u
m
b
e
r 
o
f 
su
p
p
o
rt
in
g 
se
n
te
n
ce
s 
(N
e
ga
ti
ve
)
QueryID
One-stage Two-stage
341
 Figure 7. Comparison with CNET rating 
6 Conclusion 
In this paper, we studied the problem of detecting 
low-quality product reviews. Our contribution can 
be summarized in two-fold: (1) we discovered 
three types of biases in the ground-truth used ex-
tensively in the existing work, and proposed a spe-
cification on the quality of product reviews. The 
three biases that we discovered are imbalance vote 
bias, winner circle bias, and early bird bias. (2) 
Rooting on the new ground-truth (conforming to 
the proposed specification), we proposed a classi-
fication-based approach to low-quality product 
review detection, which yields better performance 
of opinion summarization. 
We hope to explore our future work in several 
areas, such as further consolidating the new 
ground-truth from different points of view and ve-
rifying the effectiveness of low-quality review de-
tection with other applications. 
References 
Jacob Cohen. 1960. A coefficient of agreement for no-
minal scales, Educational and Psychological Mea-
surement 20: 37?46.  
Kushal Dave, Steve Lawrence, and David M. Pennock. 
2003. Mining the peanut gallery: opinion extraction 
and semantic classification of product reviews. 
WWW?03. 
Harris Drucker, Chris J.C., Burges Linda Kaufman, 
Alex Smola and Vladimir Vapnik. 1997. Support 
vector regression machines. Advances in Neural In-
formation Processing Systems.  
Christiane Fellbaum. 1998. WordNet: an Electronic 
Lexical Database, MIT Press. 
Ralf Herbrich, Thore Graepel, and Klaus Obermayer. 
1999. Support Vector Learning for Ordinal Regres-
sion. In Proc. of the 9th International Conference on 
Artificial Neural Networks. 
Minqing Hu and Bing Liu. 2004a. Mining and Summa-
rizing Customer Reviews. KDD?04.  
Minqing Hu and Bing Liu. 2004b. Mining Opinion Fea-
tures in Customer Reviews. AAAI?04. 
Kalervo Jarvelin & Jaana Kekalainen. 2000. IR: evalua-
tion methods for retrieving highly relevant docu-
ments. SIGIR?00.  
Nitin Jindal and Bing Liu. 2006. Identifying Compara-
tive Sentences in Text Documents. SIGIR?06. 
Nitin Jindal and Bing Liu. 2006. Mining comparative 
sentences and relations. AAAI?06. 
Thorsten Joachims. SVMlight -- Support Vector Ma-
chine. http://svmlight.joachims.org/, 2004. 
Soo-Min Kim, Patrick Pantel, Tim Chklovski, Marco 
Pennacchiotti. 2006. Automatically Assessing Re-
view Helpfulness. EMNLP?06. 
Dekang Lin. 1998, Automatic retrieval and clustering of 
similar words. COLING-ACL?98. 
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005. 
Opinion observer: analyzing and comparing opinions 
on the web. WWW ?05.  
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summari-
zation based on minimum cuts. ACL?04. 
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting 
class relationships for sentiment categorization with 
respect to rating scales. ACL?05. 
Bo Pang and Lillian Lee, and S. Vaithyanathan. 2002. 
Thumbs up? sentiment classification using machine 
learning techniques. EMNLP?02.  
Ana-Maria Popescu and O Etzioni. 2005. Extracting 
product    features and opinions from reviews. HLT-
EMNLP?05.  
Peter D. Turney. 2001. Thumbs up or thumbs down?: 
semantic orientation applied to unsupervised classifi-
cation of reviews. ACL?02  
Esko Ukkonen. 1985. Algorithms for approximate string 
matching. Information and Control, pp. 100 ? 118. 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer. 
Zhu Zhang and Balaji Varadarajan. 2006. Utility Scor-
ing of Product Reviews. CIKM?06 
 
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
R
at
in
g 
Sc
o
re
QueryID
One-stage
Two-stage
CNET Ground-truth
342
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 64?72,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Dialogue-Oriented Review Summary Generation for Spoken Dialogue 
Recommendation Systems 
 
 
Jingjing Liu, Stephanie Seneff, Victor Zue 
MIT Computer Science & Artificial Intelligence Laboratory 
32 Vassar Street, Cambridge, MA 02139 
{jingl, seneff, zue}@csail.mit.edu 
 
 
 
Abstract 
In this paper we present an opinion summari-
zation technique in spoken dialogue systems. 
Opinion mining has been well studied for 
years, but very few have considered its appli-
cation in spoken dialogue systems. Review 
summarization, when applied to real dialogue 
systems, is much more complicated than pure 
text-based summarization. We conduct a sys-
tematic study on dialogue-system-oriented 
review analysis and propose a three-level 
framework for a recommendation dialogue 
system. In previous work we have explored a 
linguistic parsing approach to phrase extrac-
tion from reviews. In this paper we will de-
scribe an approach using statistical models 
such as decision trees and SVMs to select the 
most representative phrases from the ex-
tracted phrase set. We will also explain how 
to generate informative yet concise review 
summaries for dialogue purposes. Experimen-
tal results in the restaurant domain show that 
the proposed approach using decision tree al-
gorithms achieves an outperformance of 13% 
compared to SVM models and an improve-
ment of 36% over a heuristic rule baseline. 
Experiments also show that the decision-tree-
based phrase selection model can achieve ra-
ther reliable predictions on the phrase label, 
comparable to human judgment. The pro-
posed statistical approach is based on do-
main-independent learning features and can 
be extended to other domains effectively. 
1 Introduction 
Spoken dialogue systems are presently available 
for many purposes, such as weather inquiry (Zue 
et al, 2000), bus schedules and route guidance 
(Raux et al, 2003), customer service (Gorin et al, 
1997), and train timetable inquiry (Eckert et al, 
1993). These systems have been well developed 
for laboratory research, and some have become 
commercially viable. 
The next generation of intelligent dialogue sys-
tems is expected to go beyond factoid question 
answering and straightforward task fulfillment, by 
providing active assistance and subjective recom-
mendations, thus behaving more like human 
agents. For example, an intelligent dialogue sys-
tem may suggest which airline is a better choice, 
considering cost, flight duration, take-off time, 
available seats, etc.; or suggest which digital cam-
era is the most popular among teenagers or highest 
rated by professional photographers; or which res-
taurant is a perfect spot for a semi-formal business 
meeting or a romantic date. 
Luckily, there are enormous amounts of reviews 
published by general users on the web every day. 
These are perfect resources for providing subjec-
tive recommendations and collective opinions. If 
there exists a systematic framework that harvests 
these reviews from general users, extracts the es-
sence from the reviews and presents it appropriate-
ly in human-computer conversations, then we can 
enable dialogue systems to behave like a human 
shopping assistant, a travel agent, or a local friend 
who tells you where to find the best restaurant.  
Summarization from online reviews, therefore, 
plays an important role for such dialogue systems. 
There have been previous studies on review analy-
sis for text-based summarization systems (Mei et 
al., 2007; Titov and McDonald, 2008a; Branavan 
et al, 2008). Mixture models and topic models are 
used to predict the underlying topics of each doc-
ument and generate a phrase-level summary. An 
aspect rating on each facet is also automatically 
64
learned with statistical models (Snyder and Barzi-
lay, 2007; Titov and McDonald, 2008b; Baccia-
nella et al, 2009). These approaches are all very 
effective, and the review databases generated are 
well presented.  
So the first thought for developing a recom-
mendation dialogue system is to use such a cate-
gorized summary in a table-lookup fashion. For 
example, a dialogue system for restaurant recom-
mendations can look up a summary table as exem-
plified in Table 1, and generate a response 
utterance from each row: ?Restaurant A has good 
service and bad food; restaurant B has good ser-
vice and good food; restaurant C has great service 
and nice atmosphere; restaurant D has poor service 
and reasonable price.?  
 
Restaurant Summary 
A Good service, bad food, 
B Good service, good food 
C Great service, nice atmosphere 
D Poor service, reasonable price 
Table 1. A partial table of categorization-based review 
summaries. 
 
Such a dialogue system is, however, not very 
informative. First of all, there is too much redun-
dancy. Long utterances repeated in the same pat-
tern on the same topic are quite boring, and the 
information density is very low. Second, such a 
summary is too coarse-grained to be helpful. A 
user querying a restaurant recommendation system 
expects more fine-grained information such as 
house specials, wine selections and choices on 
desserts rather than just general ?good food.?  
In contrast to a ?text? summarization system, the 
textual space in a dialogue turn is often very li-
mited. Speech is inherently serial, and it cannot be 
skipped and scanned easily. A dialogue system 
which speaks long diatribes in each single conver-
sation turn would likely not be well received. We 
believe that the best unit of review summary for 
dialogue purposes is short succinct phrases (e.g., 
?amazing sauce? and ?romantic ambiance?) which 
are catalogued into categories to answer high-level 
questions such as ?How is the food at this restau-
rant?? or ?What about the atmosphere?? Also, 
domain-specific phrases are greatly needed to 
generate correct answers to specific queries, such 
as ?Which restaurants have fresh sushi?? and 
?Show me restaurants that serve fantastic cock-
tails.? 
Thus, the generally used correlated topic mod-
els, although very effective in text-based systems, 
are not quite suitable for interactive dialogue sys-
tems. The missing piece is an HCI-oriented (hu-
man computer interaction), fine-grained, 
informative yet concise review summarization. A 
good recommendation system should be able to 
provide sufficient yet specific information to help 
users choose a restaurant, a movie or a consumer 
product. Figure 1 shows an example of a conversa-
tion with a good recommendation system, which 1) 
provides a generalized yet high-density review 
summary in human-friendly dialogue; 2) provides 
both a coarse-grained summary (e.g., ?authentic 
food?) and fine-grained information such as house 
specialties.  
 
U: Are there any French restaurants in Cambridge? 
S: There are five French restaurants in Cambridge. 
Most of them are on Backbay Street. And most of 
them have attentive service. 
U: Show me the ones that have nice ambiance. 
S: Two of them have nice ambiance, Pierrot Bistrot 
Francais and Coriander. 
U: How about the food? 
S: Pierrot Bistrot Francais has authentic food, 
fresh salad, and absolutely delicious spaghetti 
carbonara. The food at Coriander is below aver-
age. It has very greasy chicken tikka masala and 
dry bread.  
Figure 1. Example of a conversation with a good rec-
ommendation dialogue system (?U? denotes the user 
and ?S? denotes the dialogue system. The responses to 
the user queries are produced by our system and the 
actual dialogue was an illustration of system capacities). 
 
Therefore, the task of developing recommenda-
tion dialogue systems is decomposed into three 
problems: 1) how to extract context-related phras-
es, both coarse-grained and fine-grained, from 
online reviews; 2) how to select a representative 
set from the extracted phrases to create an infor-
mative yet concise dialogue-oriented summary 
database; 3) how to generate human-friendly di-
alogue responses from the review summary data-
base.  
To tackle these problems, we propose a three-
level framework. In previous work (Liu and Seneff, 
2009), we explored the first level by proposing a 
linguistic parse-and-paraphrase paradigm for re-
65
view phrase extraction. In this paper, we address 
the second problem: dialogue-oriented review 
summary generation. We propose an automatic 
approach to classifying high/low informative 
phrases using statistical models. Experiments con-
ducted on a restaurant-domain dataset indicate that 
the proposed approach can predict phrase labels 
consistently with human judgment and can gener-
ate high-quality review summaries for dialogue 
purposes.  
The rest of the paper is organized as follows: 
Section 2 gives an overview of the three-level 
framework for recommendation dialogue systems. 
In Section 3, we explain the proposed approach to 
dialogue-oriented review summary generation. 
Section 4 provides a systematic evaluation of the 
proposed approach, and Section 5 gives a further 
discussion on the experimental results. Section 6 
summarizes the paper as well as pointing to future 
work. 
2 System Overview 
The three-level framework of a review-summary-
based recommendation dialogue system is shown 
in Figure 2. The bottom level is linguistic phrase 
extraction. In previous work (Liu and Seneff, 
2009), we employed a probabilistic lexicalized 
grammar to parse review sentences into a hierar-
chical representation, which we call a linguistic 
frame. From the linguistic frames, phrases are ex-
tracted by capturing a set of adjective-noun rela-
tionships. Adverbs and negations conjoined with 
the adjectives are also captured. We also calcu-
lated a numerical score for sentiment strength for 
each adjective and adverb, and further applied a 
cumulative offset model to assign a sentiment 
score to each phrase. 
The approach relies on linguistic features that 
are independent of frequency statistics; therefore it 
can retrieve very rare phrases such as ?very greasy 
chicken tikka masala? and ?absolutely delicious 
spaghetti carbonara?, which are very hard to derive 
from correlated topic models. Experimental results 
showed that the linguistic paradigm outperforms 
existing methods of phrase extraction which em-
ploy shallow parsing features (e.g., part-of-speech). 
The main contribution came from the linguistic 
frame, which preserves linguistic structure of a 
sentence by encoding different layers of semantic 
dependencies. This allows us to employ more so-
phisticated high-level linguistic features (e.g., long 
distance semantic dependencies) for phrase extrac-
tion. 
However, the linguistic approach fails to distin-
guish highly informative and relevant phrases 
from uninformative ones (e.g., ?drunken husband?, 
?whole staff?). To apply these extracted phrases 
within a recommendation dialogue system, we 
have to filter out low quality or irrelevant phrases 
and maintain a concise summary database. This is 
the second level: dialogue-oriented review sum-
mary generation.  
 
 
 
Figure 2. Three-level framework of review-based rec-
ommendation dialogue systems. 
 
The standard of highly informative and relevant 
phrases is a very subjective problem. To gain in-
sights on human judgment on this, the first two 
authors separately labeled a set of review-related 
phrases in a restaurant domain as ?good? and ?bad? 
summary phrases. We surveyed several subjects, 
all of whom indicated that, when querying a dialo-
gue system for information about a restaurant, 
they care much more about special dishes served 
in this restaurant than generic descriptions such as 
?good food.? This knowledge informed the annota-
tion task: to judge whether a phrase delivered by a 
dialogue recommendation system would be help-
66
ful for users to make a decision. Surprisingly, al-
though this is a difficult and subjective problem, 
the judgment from the two annotators is substan-
tially consistent. By examining the annotations we 
observed that phrases such as ?great value? and 
?good quality? are often treated as ?uninformative? 
as they are too common to be representative for a 
particular product, a restaurant or a movie. Phrases 
with neutral sentiment (e.g., ?green beans? and 
?whole staff?) are often considered as uninforma-
tive too. Phrases on specific topics such as house 
specialties (e.g., ?absolutely delicious spaghetti 
carbonara?) are what the annotators care about 
most and are often considered as highly relevant, 
even though they may have only been seen once in 
a large database.  
Driven by these criteria, from each phrase we 
extract a set of statistical features such as uni-
gram/bigram probabilities and sentiment features 
such as sentiment orientation degree of the phrase, 
as well as underlying semantic features (e.g., 
whether the topic of the phrase fits in a domain-
specific ontology). Classification models such as 
SVMs and decision tree algorithms are then 
trained on these features to automatically classify 
high/low informative phrases. Phrases identified 
as ?good? candidates are further pruned and cata-
logued to create concise summaries for dialogue 
purposes. 
After generating the review summary database, 
the third level is to modify the response generation 
component in dialogue systems to create genera-
lized and interactive conversations, as exemplified 
in Figure 1. The utterance from users is piped 
through speech recognition and language under-
standing. The meaning representation is then sent 
to the dialogue management component for re-
view-summary database lookup. A response is 
then generated by the language generation compo-
nent, and a speech utterance is generated by the 
synthesizer and sent back to the user. The dialogue 
system implementation is beyond the scope of this 
paper and will be discussed later in a separate pa-
per. 
3 Dialogue-oriented Review Summary 
Generation 
Given an inquiry from users, the answer from a 
recommendation system should be helpful and 
relevant. So the first task is to identify a phrase as 
?helpful? or not. The task of identifying a phrase as 
informative and relevant, therefore, is defined as a 
classification problem: 
 =  ? ? #? = ?  %#%&%=1          (1) 
where y is the label of a phrase, assigned as ?1? if 
the phrase is highly informative and relevant, and 
?-1? if the phrase is uninformative. #? is the feature 
vector extracted from the phrase, and  ?  is the 
coefficient vector.  
We employ statistical models such as SVMs 
(Joachims, 1998) and decision trees (Quinlan, 
1986) to train the classification model. For model 
learning, we employ a feature set including statis-
tical features, sentiment features and semantic 
features.  
Generally speaking, phrases with neutral senti-
ment are less informative than those with strong 
sentiment, either positive or negative. For example, 
?fried seafood appetizer?, ?baked halibut?, ?elec-
tronic bill? and ?red drink? do not indicate whether 
a restaurant is worth trying, as they did not express 
whether the fried seafood appetizer or the baked 
halibut are good or bad. Therefore, we take the 
sentiment score of each phrase generated from a 
cumulative offset model (Liu and Seneff, 2009) as 
a sentiment feature. Sentiment scores of phrases 
are exemplified in Table 2 (on a scale of 1 to 5).  
 
Phrase Sc. Phrase Sc. 
really welcoming 
atmosphere 
4.8 truly amazing flavor 4.6 
perfect portions  4.4 very tasty meat 4.3 
busy place 3.1 typical Italian restaurant 3.1 
a little bit high 
price 
2.2 pretty bad soup 1.8 
sloppy service 1.8 absolute worst service 1.4 
Table 2. Examples of sentiment scores of phrases. 
 
We also employ a set of statistical features for 
model training, such as the unigram probability of 
the adjective in a phrase, the unigram probability 
of the noun in a phrase, the unigram probability of 
the phrase and the bigram probability of the adjec-
tive-noun pair in a phrase.  
Statistical features, however, fail to reveal the 
underlying semantic meaning of phrases. For ex-
ample, phrases ?greasy chicken tikka masala? and 
?drunken husband? have the same n-gram proba-
bilities in our corpus (a single observation), but 
67
they should certainly not be treated as the same. 
To capture the semantic meanings of phrases, we 
first cluster the topics of phrases into generic se-
mantic categories. The language-model based al-
gorithm is given by: 
 
        '(() | (%) = ? '(() |+) ? '(+|(%)+?.  
                 =  ? '(+ ,())'(+) ?
'(+ ,(%)
'((%)+?.
  
                 =  1'((%)
? 1
'(+) ? '(+, ()) ? '(+, (%)+?.          (2) 
where A represents the set of all the adjectives in 
the corpus. We select a small set of initial topics 
with the highest frequency counts (e.g., ?food?, 
?service? and ?atmosphere?). For each of the other 
topics tc  (e.g., ?chicken?, ?waitress? and ?d?cor?), 
we calculate its similarity with each initial topic (% 
based on the bigram probability statistics. For 
those topics with conditional probability higher 
than a threshold for an initial topic (%, we assign 
them to the cluster of (%. We use this as a semantic 
feature, e.g., whether the topic of a phrase belongs 
to a generic semantic category. Table 3 gives some 
clustering examples. 
 
Category Relevant Topics 
food 
appetizer, beer, bread, fish, fries, ice 
cream, margaritas, menu, pizza, pasta, 
rib, roll, sauce, seafood, sandwich, 
steak, sushi, dessert, cocktail, brunch 
service waiter, staff, management, server, hostess, chef, bartender, waitstaff 
atmosphere d?cor, ambiance, music, vibe, setting, environment, crowd 
price bill, pricing, prices 
Table 3. Topic to semantic category clustering. 
 
This language-model-based method relies on 
bigram probability statistics and can well cluster 
highly frequent topics. Categories such as ?service? 
and ?atmosphere? contain very limited related top-
ics, most of which have high frequencies (e.g., 
?waiter?, ?staff?, ?ambiance? and ?vibe?). The cate-
gory ?food?, however, is very domain-specific and 
contains a very large vocabulary, from generic 
sub-categories such as ?sushi?, ?dessert? and 
?sandwich? as shown in the examples, to specific 
courses such as ?bosc pear bread pudding? and 
?herb roasted vermont pheasant wine cap mu-
shrooms?. These domain-specific topics have very 
low frequencies, yet they are very relevant and 
valuable. But many of them are discarded by the 
clustering. It would be a similar case in other do-
mains. For example, consumer products, movies 
and books all have domain-independent semantic 
categories (e.g., ?price? and ?released date?) and 
domain-specific categories (e.g., technical features 
of consumer products, casts of movies and authors 
of books). 
To recover these context-relevant topics, we 
employ domain context relations such as a con-
text-related ontology. A context-related ontology 
can be constructed from structured web resources 
such as online menus of restaurants, names of ac-
tors and actresses from movie databases, and spe-
cifications of products from online shops. An 
example of a partial online menu of a restaurant is 
shown in Figure 3. From these structured web re-
sources, we can build up a hierarchical ontology, 
based on which a set of semantic features can be 
extracted (e.g., whether a phrase contains a course 
name, or an actress?s name, or a dimension of 
technical features of a consumer product).  
 
Entree 
Roasted Pork Loin Wrapped In Bacon with watermelon and 
red onion salad spicy honey-mustard bbq sauce 
Spicy Halibut And Clam Roast with bacon braised greens, 
white beans and black trumpet mushrooms 
Parmesan and Caramelized Shallot Wrapper Style Ravi-
oli turnip greens and white truffle oil 
Herb Roasted Vermont Pheasant Wine Cap Mushrooms, 
Pearl Onions and Fava Beans 
Dessert 
Chocolate Tasting Plate of white chocolate bombe milk choc-
olate creme brul?e and dark chocolate flourless cake  
White Fruit Tasting Plate of warm apple strudel butterscotch, 
Bosc Pear bread pudding and toasted coconut panna cotta  
 
Entr?e Pork loin, bacon, watermelon, red onion 
salad, honey, mustard, bbq sauce 
Dessert  Chocolate, milk, cr?me brulee, cake 
Figure 3. Example of a partial online menu and an ex-
emplary ontology derived. 
 
After the classification, phrases identified as 
?highly informative and relevant? are clustered 
into different aspects according to the semantic 
category clustering and the hierarchical ontology. 
An average sentiment score for each aspect is then 
calculated:  
+/0(1() =
? 233?41
|41|
     (3) 
68
where 1(  represents the aspect s of entry t (e.g., a 
restaurant, a movie, or a consumer product), 41  
represents the set of phrases in the cluster of as-
pect s, and 23  represents the sentiment score of 
phrase j in the cluster. 
The set of phrases selected for one entry may 
come from several reviews on this single entry, 
and many of them may include the same noun 
(e.g., ?good fish?, ?not bad fish? and ?above-
average fish? for one restaurant). Thus, the next 
step is multi-phrase redundancy resolution. We 
select the phrase with a sentiment score closest to 
the average score of its cluster as the most repre-
sentative phrase on each topic:  
5 = +265%&3?4%(|23 ? +/0(1()|)    (4) 
where +/0(1()  represents the average sentiment 
score of aspect 1, 4%  represents the set of phrases 
on the same topic %  in the cluster 1 , and 23  
represents the sentiment score of phrase 3.  
This sequence of topic categorization, ontology 
construction, phrase pruning and redundancy eli-
mination leads to a summary database, which can 
be utilized for dialogue generation in spoken rec-
ommendation systems. A review summary data-
base entry generated by the proposed approaches 
is exemplified in Figure 4. 
 
{ restaurant "dali restaurant and tapas bar" 
     :atmosphere ( "wonderful evening", "cozy atmos- 
phere", "fun decor", "romantic date" ) 
     :atmosphere_rating "4.1" 
     :food ( "very fresh ingredients",  "tasty fish", 
"creative dishes",  "good sangria" ) 
     :food_rating "3.9"         
     :service ( "fast service" ) 
     :service_rating "3.9"    
     :general ("romantic restaurant","small space" ) 
     :general_rating "3.6"                 } 
Figure 4. Example of a review summary database entry 
generated by the proposed approaches. 
4 Experiments 
In this project, we substantiate the proposed ap-
proach in a restaurant domain for our spoken di-
alogue system (Gruenstein and Seneff, 2007), 
which is a web-based multimodal dialogue system 
allowing users to inquire about information about 
restaurants, museums, subways, etc. We harvested 
a data collection of 137,569 reviews on 24,043 
restaurants in 9 cities in the U.S. from an online 
restaurant evaluation website1. From the dataset, 
857,466 sentences were subjected to parse analysis; 
and a total of 434,372 phrases (114,369 unique 
ones) were extracted from the parsable subset 
(78.6%) of the sentences.  
Most pros/cons consist of well-formatted phras-
es; thus, we select 3,000 phrases extracted from 
pros/cons as training data. To generate a human 
judgment-consistent training set, we manually la-
bel the training samples with ?good? and ?bad? la-
bels. We then randomly select a subset of 3,000 
phrases extracted from review texts as the test set 
and label the phrases. The kappa agreement be-
tween two sets of annotations is 0.73, indicating 
substantial consistency. We use the two annotation 
sets as the ground truth. 
To extract context-related semantic features, we 
collect a large pool of well-formatted menus from 
an online resource2, which contains 16,141 restau-
rant menus. Based on the hierarchical structure of 
these collected menus, we build up a context-
related ontology and extract a set of semantic fea-
tures from the ontology, such as whether the topic 
of a phrase is on category-level (e.g., ?entr?e?, 
?dessert?, ?appetizers?, ?salad?), whether the topic 
is on course-level (e.g., ?Roasted Pork Loin?, ?Spi-
cy Halibut and Clam Roast?), and whether the top-
ic is on ingredient-level (e.g., ?beans?, ?chicken?, 
?mushrooms?, ?scallop?).  
We employ the three types of features as afore-
mentioned to train the SVMs and the decision tree 
models. To select the most valuable features for 
model training, we conducted a set of leave-one-
feature-out experiments for both the SVMs and the 
decision tree models. We found that all the fea-
tures except the adjective unigram probability 
contribute positively to model learning. From fur-
ther data analysis we observed that many phrases 
with popular adjectives have context-unrelated 
nouns, which makes the adjective unigram proba-
bility fail to become a dominant factor for phrase 
relevance. Using the adjective unigram probability 
as a learning feature will mislead the system into 
trusting an adjective that is common but has a poor 
bigram affinity to the noun in the phrase. Thus, we 
eliminate this feature for both the SVMs and the 
decision tree learning. 
                                                          
1 http://www.citysearch.com 
2 http://www.menupages.com  
69
 To evaluate the performance of the classifica-
tion models, we take a set of intuitively motivated 
heuristic rules as the baseline. Figure 5 gives the 
pseudo-code of the heuristic rule algorithm, which 
uses variations of all the features except the uni-
gram probability of adjectives. 
  
If(sentiment score of the phrase exists) 
if(sentiment score is within neutral range) label=-1; 
else    
if(phrase appeared in the training data) 
      if((3<frequency of phrase < 100))   label = 1; 
          else 
            if(frequency of phrase >= 100)   label = -1; 
              else    if(topic belongs to ontology)  label = 1; 
                        else   label = -1; 
       else 
            if(topic belongs to ontology)   label = 1; 
            else   label = -1; 
else 
if(phrase appeared in the training data) 
           if((3<frequency of phrase < 100))    
if(topic belongs to ontology)  label = 1; 
                   else   label = -1; 
           else 
              if(frequency of phrase >= 100)   label = -1; 
              else 
                   if(topic belongs to ontology)  label = 1; 
                   else   if(frequency of noun > 100) label = 1; 
                            else   label = -1; 
    else 
         if(topic belongs to ontology)  label = 1; 
         else     if(frequency of noun > 100)   label = 1; 
                    else   label = -1;                 
Figure 5. Pseudo-code of the heuristic rule algorithm. 
 
The performance of classification by different 
models is shown in Table 4. Although the heuris-
tic rule algorithm is complicated and involves hu-
man knowledge, the statistical models trained by 
SVMs and the decision tree algorithms both out-
perform the baseline significantly. The SVM mod-
el outperforms the baseline by 10.5% and 11.9% 
on the two annotation sets respectively. The deci-
sion tree model outperforms the baseline by 16.4% 
and 23.2% (average relative improvement of 36%), 
and it also outperforms the SVM model by 5.9% 
and 11.3% (average relative improvement of 13%). 
The classification model using the decision tree 
algorithm can achieve a precision of 77.9% and 
74.5% compared with the ground truth, which is 
quite comparable to human judgment (the preci-
sion of one annotation set based on the other is 
74%). This shows that the decision tree model can 
predict phrase labels as reliably as human judg-
ment. 
 
 Baseline SVM Decision tree 
Annotation 1 61.5% 72.0% 77.9% 
Annotation 2 51.3% 63.2% 74.5% 
Table 4. Precision of phrase classification using the 
heuristic rule baseline, the SVM model, and the deci-
sion tree algorithm. 
 
To gain further insight on the contributions of 
each feature set to the decision tree learning, Table 
5 gives the experimental results on leaving each 
feature out of model training. As shown, without 
semantic features, the precision is 70.6% and 65.4% 
on the two annotation sets, lower by 7.3% and 9.1% 
than the case of training the model with all the 
features (77.9% and 74.5%). This shows that the 
semantic features significantly contribute to the 
decision tree learning. 
 
Feature set A1 A2 
all features  77.9% 74.5% 
without bigram probability 
of adjective-noun pair 
56.6%  
(-21.3%) 
63.9%  
(-10.6%) 
without unigram probability 
of the phrase 
57.6%  
(-20.3%) 
64.3%  
(-10.2%) 
without unigram probability 
of the noun 
59.8%  
(-18.1%) 
67.8%  
(-6.7%) 
without sentiment score of 
the phrase 
63.4%  
(-14.5%) 
66.6%  
(-7.9%) 
without underlying semantic 
features  
70.6%  
(-7.3%) 
65.4%  
(-9.1%) 
Table 5. Performance of the decision tree model by 
leaving each feature out of model training (?A1? and 
?A2? represent the annotation set 1 and 2 respectively). 
 
The experimental results also show that the fea-
ture of bigram probability of the adjective-noun 
pair contributes the most to the model learning. 
Without this feature, the precision drops by 21.3% 
and 10.6%, reaching the lowest precision among 
all the leave-one-out experiments. This confirms 
our observation that although a single adjective is 
not dominant, the pair of the adjective and the 
noun that co-occurs with it plays an important role 
in the classification.  
The sentiment of phrases also plays an impor-
tant role. Without sentiment features, the precision 
70
drops to 63.4% and 66.6% respectively on the two 
annotations, decreasing by 14.5% and 7.9%. This 
shows that the sentiment features contribute sig-
nificantly to the classification.  
5 Discussions 
Experimental results show that the decision tree 
algorithm outperforms the SVMs on this particular 
classification problem, and it outperforms the heu-
ristic rule baseline significantly. Thus, although 
the identification of informativeness and relevance 
of phrases is a rather subjective problem, which is 
difficult to predict using only human knowledge, it 
can be well defined by decision trees. Part of the 
reason is that the decision tree algorithm can make 
better use of a combination of Boolean value fea-
tures (e.g., whether a topic belongs to a context-
related ontology) and continuous value features. 
Also, as the phrase classification task is very sub-
jective, it is very similar to a ?hierarchical if-else 
decision problem? in human cognition, where de-
cision tree algorithms can fit well. Figure 6 shows 
a partial simplified decision tree learned from our 
model, which can give an intuitive idea of the de-
cision tree models. 
6 Related Work 
Sentiment classification and opinion mining have 
been well studied for years. Most studies have fo-
cused on text-based systems, such as document-
level sentiment classification and sentence-level 
opinion aggregation (Turney, 2002; Pang et al, 
2002; Dave et al, 2003; Hu and Liu, 2004; Popes-
cu and Etzioni, 2005; Wilson et al, 2005; Zhuang 
et al, 2006; Kim and Hovy, 2006).  
There was a study conducted by Carenini et al 
in 2006, which proposed a combination of a sen-
tence extraction-based approach and a language 
generation-based approach for summarizing eva-
luative arguments. In our work, we utilize a lower-
level phrase-based extraction approach, which uti-
lizes high level linguistic features and syntactic 
structure to capture phrase patterns.  
There was also a study on using reviews to gen-
erate a dictionary of mappings between semantic 
representations and realizations of concepts for 
dialogue systems (Higashinaka et al, 2006; Higa-
shinaka, 2007). They also used the association 
between user ratings and reviews to capture se-
mantic-syntactic structure mappings. A set of fil-
tering rules was manually created to eliminate 
low-quality mappings. In our approach, we use an 
automatic approach to classifying high/low infor-
mative phrases. The learning features are domain-
independent with no hand-crafted rules, and can 
be extended to other domains effortlessly.  
7 Conclusions 
In this paper we proposed a three-level framework 
for review-based recommendation dialogue sys-
tems, including linguistic phrase extraction, dialo-
gue-oriented review summary generation, and 
human-friendly dialogue generation. The contribu-
tions of this paper are three-fold: 1) it identified 
and defined the research goal of utilizing opinion 
summarization for real human-computer conversa-
tion; 2) it formulated an evaluation methodology 
for high-density review summary for dialogue 
purposes; 3) it proposed an approach to automatic 
classification of high/low informative phrases us-
ing a decision tree model. Experimental results 
showed that the decision tree model significantly 
outperforms a heuristic rule baseline and the SVM 
model, and can resolve the phrase classification 
problem comparably to humans consistently. 
Future work will focus on: 1) applying the sen-
timent scoring model to noun/verb sentiment as-
sessment; 2) application of the review summary 
generation approach in other domains and other 
languages; 3) data collection on user engagement 
with our dialogue systems involving review-
summary evaluation. 
 
 
Figure 6. A partial simplified decision tree learned from 
our model. 
  
71
References  
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2009. Multi-facet Rating of Product Reviews. 
In Proceedings of European Conference on Informa-
tion Retrieval. 
S.R.K. Branavan, Harr Chen, Jacob Eisenstein, and 
Regina Barzilay. 2008. Learning document-level 
semantic properties from free-text annotations. In 
Proc. of ACL. 
Giuseppe Carenini, Raymond Ng, and Adam Pauls. 
2006. Multi-Document Summarization of Evaluative 
Text. In Proceedings of the Conference of the Euro-
pean Chapter of the Association for Computational 
Linguistics. 
Kushal Dave, Steve Lawrence, and David M. Pennock. 
2003. Mining the peanut gallery: opinion extraction 
and semantic classification of product reviews. In 
Proceedings of the International Conference on 
World Wide Web. 
W. Eckert, T. Kuhn, H. Niemann, S. Rieck, A. Scheuer, 
and E. G. Schukat-talamazzini. 1993. A Spoken Di-
alogue System for German Intercity Train Timetable 
Inquiries. In Proc. European Conf. on Speech Tech-
nology. 
Alexander Gruenstein and Stephanie Seneff. 2007. Re-
leasing a Multimodal Dialogue System into the 
Wild: User Support Mechanisms. In Proceedings of 
the 8th SIGdial Workshop on Discourse and Dialo-
gue, Antwerp, pages 111-119. 
A. L. Gorin, G. Riccardi and J. H. Wright. 1997. ?How 
may I help you?? Speech Communication, vol. 23, 
pp. 113?127.  
Ryuichiro Higashinaka, Rashmi Prasad and Marilyn 
Walker. 2006. Learning to Generate  
Naturalistic Utterances Using Reviews in Spoken 
Dialogue Systems. In Proceedings of COLING-ACL.  
Ryuichiro Higashinaka, Marilyn Walker and Rashmi 
Prasad. 2007. An Unsupervised Method  
for Learning Generation Dictionaries for Spoken Di-
alogue Systems by Mining User Reviews.  
Journal of ACM Transactions on Speech and Lan-
guage Processing. 
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 2004 
ACM SIGKDD international conference on Know-
ledge Discovery and Data mining. 
S.M. Kim and E.H. Hovy. 2006. Identifying and Ana-
lyzing Judgment Opinions. In Proc. of HLT/NAACL. 
Jingjing Liu and Stephanie Seneff. 2009. Review Sen-
timent Scoring via a Parse-and-Paraphrase Para-
digm. In proceedings of EMNLP. 
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, 
and ChengXiang Zhai. 2007. Topic Sentiment Mix-
ture: Modeling Facets and Opinions in Weblogs. In 
Proc. of WWW. 
Bo Pang, Lillian Lee, and S. Vaithyanathan. 2002. 
Thumbs up? Sentiment classification using machine 
learning techniques. In Proceedings of EMNLP. 
A.M. Popescu and O. Etzioni. 2005. Extracting product 
features and opinions from reviews. In Proceedings 
of EMNLP. 
JR Quinlan, 1986. Induction of decision trees. Machine 
learning, Springer-Netherlands. 
A. Raux, B. Langner, A. Black, and M. Eskenazi. 2003. 
LET'S GO: Improving Spoken Dialog Systems for 
the Elderly and Non-natives. In Proc. Eurospeech. 
Benjamin Snyder and Regina Barzilay. 2007. Multiple 
Aspect Ranking using the Good Grief Algorithm. In 
Proceedings of NAACL-HLT. 
Ivan Titov and Ryan McDonald. 2008a. Modeling On-
line Reviews with Multi-Grain Topic Models. In 
Proc. of WWW. 
Ivan Titov and Ryan McDonald. 2008b. A Joint Model 
of Text and Aspect Ratings for Sentiment Summari-
zation. In Proceedings of the Annual Conference of 
the Association for Computational Linguistics. 
Peter D. Turney. 2002. Thumbs up or thumbs down? 
Sentiment orientation applied to unsupervised classi-
fication of reviews. In Proceedings of the Annual 
Conference of the Association for Computational 
Linguistics. 
T. Joachims. 1998. Text categorization with support 
vector machines: Learning with many relevant fea-
tures. In Proc. of ECML, p. 137?142.  
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing Contextual Polarity in Phrase-Level Senti-
ment Analysis. In Proc. of HLT/EMNLP. 
Victor Zue, Stephanie Seneff, James Glass, Joseph Po-
lifroni, Christine Pao, Timothy J. Hazen, and Lee 
Hetherington. 2000. JUPITER: A Telephone-Based 
Conversational Interface for Weather Information. In 
IEEE Transactions on Speech and Audio Processing, 
Vol. 8 , No. 1.  
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie 
review mining and summarization. In Proceedings of 
the 15th ACM international conference on Informa-
tion and knowledge management. 
72
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 83?86,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Utilizing Review Summarization in a Spoken Recommendation System 
 
 
Jingjing Liu, Stephanie Seneff and Victor Zue 
MIT Computer Science & Artificial Intelligence Laboratory 
Cambridge, U.S.A. 
{jingl,seneff,zue}@csail.mit.edu 
 
  
 
Abstract 
 
In this paper we present a framework for spoken rec-
ommendation systems. To provide reliable recom-
mendations to users, we incorporate a review summa-
rization technique which extracts informative opinion 
summaries from grass-roots users? reviews. The dia-
logue system then utilizes these review summaries to 
support both quality-based opinion inquiry and fea-
ture-specific entity search. We propose a probabilistic 
language generation approach to automatically creat-
ing recommendations in spoken natural language 
from the text-based opinion summaries. A user study 
in the restaurant domain shows that the proposed ap-
proaches can effectively generate reliable and helpful 
recommendations in human-computer conversations. 
 
1 Introduction 
Spoken dialogue systems are presently available 
for many purposes, such as flight reservations 
(Seneff and Polifroni, 2000), telephone calls 
routing (Gorin et al, 1997), and subway lookup 
(Johnston et al, 2002). Recently, we have been 
exploring a next generation of intelligent dia-
logue systems, which can behave like a human 
agent and provide proactive assistance and selec-
tive recommendations (e.g., highly-rated restau-
rants or hotels) to users.  
To enhance dialogue systems with intelligent 
services, we have to let the system ?grow? relia-
ble knowledge and intelligence. Luckily, there 
has recently been an explosive growth in the 
availability of public review sites (e.g., yelp.com, 
tripadvisor.com, etc.) which make a perfect re-
source for gathering collective opinions. In this 
paper, we will explore how to utilize summaries 
of public reviews to automatically generate rec-
ommendation-type conversations in spoken dia-
logue systems. An example of a conversation 
with our recommendation system is shown in 
Figure 1. 
 
U: Are there any American restaurants in Brighton? 
S: There are 61 American restaurants in Brighton. All of 
the rated restaurants have good or excellent ambiance. 
Many are on Washington Street, Commonwealth Avenue 
and Market Street. 
U: Show me the ones that have nice ambiance. 
S: I found 4 entries. Devlin's Restaurant and Bar is per-
fect for romantic date, Stockyard Restaurant has friendly 
atmosphere, Tasca Tapas Restaurant has excellent ambi-
ance, and Wings Express has friendly vibes. 
U: Which one has good martinis? 
S: Devlin's Restaurant and Bar serves American food. It 
has wonderful martinis, good wine, great fish. It is perfect 
for romantic date. Great place. 
Figure 1. A real conversation with our recom-
mendation dialogue system in the restaurant do-
main (?U? is the user and ?S? is the system). 
2 Dialogue Management 
In our previous work (Liu and Seneff, 2009; Liu 
et al, 2010) we proposed an approach to extract-
ing representative phrases and creating aspect 
ratings from public reviews. An example of an 
enhanced database entry in the restaurant domain 
is shown in Figure 2. Here, we use these ?sum-
mary lists? (e.g., ?:food?, ?:atmosphere?) as well 
as aspect ratings (e.g., ?:food_rating?) to address 
two types of recommendation inquires: ?feature-
specific? (e.g., asking for a restaurant that serves 
good martinis or authentic seafood spaghetti), 
and ?quality-based? (e.g., looking for restaurants 
with good food quality or nice ambiance). 
 
{q restaurant 
     :name "devlin?s restaurant and bar" 
     :atmosphere ("romantic date" "elegant decor") 
     :place ("great place") 
     :food ("wonderful martinis" "good wine" "great fish") 
     :atmosphere_rating "4.2" 
     :place_rating "4.2" 
     :food_rating "4.3" 
     :specialty ("martinis" "wine" "fish")     } 
Figure 2. A database entry in our system. 
83
2.1 Feature-specific Entity Search 
To allow the system to identify feature-related 
topics in users? queries, we modify the context-
free grammar in our linguistic parser by includ-
ing feature-specific topics (e.g., nouns in the 
summary lists) as a word class. When a feature-
specific query utterance is submitted by a user 
(as exemplified in Figure 3), our linguistic parser 
will generate a hierarchical structure for the ut-
terance, which encodes the syntactic and seman-
tic structure of the utterance and, especially, 
identifies the feature-related topics. A feature-
specific key-value pair (e.g., ?specialty: marti-
nis?) is then created from the hierarchical parsing 
structure, with which the system can filter the 
database and retrieve the entities that satisfy the 
constraints.  
 
Utterance 
?Are there any restaurants in Brighton that 
have good martinis?? 
 
Key-value 
pairs 
?topic: restaurant,  city: Brighton,  
  specialty: martinis? 
 
Database 
filters 
:specialty = ?martinis?  :city = ?Brighton? 
:entity_type  = ?restaurant? 
 
Figure 3. Procedure of feature-specific search. 
2.2 Quality-based Entity Search 
For quality-based questions, however, similar 
keyword search is problematic, as the quality of 
entities has variants of expressions. The assess-
ment of different degrees of sentiment in various 
expressional words is very subjective, which 
makes the quality-based search a hard problem.  
To identify the strength of sentiment in quali-
ty-based queries, a promising solution is to map 
textual expressions to scalable numerical scores. 
In previous work (Liu and Seneff, 2009), we 
proposed a method for calculating a sentiment 
score for each opinion-expressing adjective or 
adverb (e.g., ?bad?: 1.5, ?good?: 3.5, ?great?: 4.0, 
on a scale of 1 to 5). Here, we make use of these 
sentiment scores and convert the original key-
value pair to numerical values (e.g., ?great food? 
? ?food_rating: 4.0? as exemplified in Figure 
4). In this way, the sentiment expressions can be 
easily converted to scalable numerical key-value 
pairs, which will be used for filtering the data-
base by ?aspect ratings? of entities. As exempli-
fied in Figure 4, all the entities in the required 
range of aspect rating (i.e., ?:food_rating   4.0?) 
can be retrieved (e.g., the entity in Figure 2 with 
?food_rating = 4.3?). 
 
Utterance 
?Show me some american restaurants with 
great food? 
 
Key-value 
pairs 
?topic: restaurant, cuisine: american,  
property: food, quality: great? 
 
Converted 
k-v pairs 
?topic: restaurant, cuisine: american, 
food_rating: 4.0? 
 
Database 
filters 
:food_rating > ?4.0?  :cuisine = ?american? 
:entity_type =  ?restaurant? 
 
Figure 4. Procedure of qualitative entity search. 
3 Probabilistic Language Generation 
After corresponding entities are retrieved from 
the database based on the user?s query, the lan-
guage generation component will create recom-
mendations by expanding the summary lists of 
the retrieved database entries into natural lan-
guage utterances.  
Most spoken dialogue systems use predefined 
templates to generate responses. However, man-
ually defining templates for each specific linguis-
tic pattern is tedious and non-scalable. For ex-
ample, given a restaurant with ?nice jazz music, 
best breakfast spot, great vibes?, three templates 
have to be edited for three different topics (e.g., 
?<restaurant> plays <adjective> music?; ?<res-
taurant> is <adjective> breakfast spot?; ?<restau-
rant> has <adjective> vibes?). To avoid the hu-
man effort involved in the task, corpus-based 
approaches (Oh and Rudnicky, 2000; Rambow et 
al., 2001) have been developed for more efficient 
language generation. In this paper, we propose a 
corpus-based probabilistic approach which can 
automatically learn the linguistic patterns (e.g., 
predicate-topic relationships) from a corpus and 
generate natural sentences by probabilistically 
selecting the best-matching pattern for each top-
ic.  
The proposed approach consists of three stag-
es: 1) plant seed topics in the context-free gram-
mar; 2) identify semantic structures associated 
with the seeds; 3) extract association pairs of lin-
guistic patterns and the seeds, and calculate the 
probability of each association pair.  
First, we extract all the nouns and noun 
phrases that occur in the review summaries as the 
seeds. As aforementioned, our context-free 
grammar can parse each sentence into a hierar-
chical structure. We modify the grammar such 
that, when parsing a sentence which contains one 
of these seed topics, the parser can identify the 
seed as an ?active? topic (e.g., ?vibes?, ?jazz mu-
sic?, and ?breakfast spot?). 
84
The second stage is to automatically identify 
all the linguistic patterns associated with each 
seed. To do so, we use a large corpus as the re-
source pool and parse each sentence in the cor-
pus for linguistic analysis. We modify our parser 
such that, in a preprocessing step, the predicate 
and clause structures that are semantically related 
to the seeds will be assigned with identifiable 
tags. For example, if the subject or the comple-
ment of the clause (or the object of the predicate) 
is an ?active? topic (i.e., a seed), an ?active? tag 
will be automatically assigned to the clause (or 
the predicate). In this way, when examining syn-
tactic hierarchy of each sentence in the corpus, 
the system can encode all the linguistic patterns 
of clauses or predicate-topic relationships associ-
ated with the seeds with ?active? tags.  
Based on these tags, association pairs of ?ac-
tive? linguistic patterns and ?active? topics can 
be extracted automatically. For each seed topic, 
we calculate the probability of its co-occurrence 
with each of its associated patterns by: 
 
    (        |     )  
                        
?                         
    (1) 
 
where       is a seed topic, and          is 
every linguistic pattern associated with      . 
The probability of          for       is the 
percentage of the co-occurrences of          
and       among all the occurrences of       
in the corpus. This is similar to a bigram lan-
guage model. A major difference is that the lin-
guistic pattern is not necessarily the word adja-
cent to the seed. It can be a long distance from 
the seed with strong semantic dependencies, and 
it can be a semantic chunk of multiple words. 
The long distance semantic relationships are cap-
tured by our linguistic parser and its hierarchical 
encoding structure; thus, it is more reliable than 
pure co-occurrence statistics or bigrams. Figure 5 
shows some probabilities learned from a review 
corpus. For example, ?is? has the highest proba-
bility (0.57) among all the predicates that co-
occur with ?breakfast spot?; while ?have? is the 
best-match for ?jazz music?. 
 
Association pair Constituent Prob. 
?at? : ?breakfast spot? PP 0.07 
?is? : ?breakfast spot? Clause 0.57 
?for? : ?breakfast spot? PP 0.14 
?love?  : ?jazz music? VP 0.08 
?have? : ?jazz music? VP 0.23 
?enjoy?: ?jazz music? VP 0.08 
Figure 5.  Partial table of probabilities of associa-
tion pairs (VP: verb phrase; PP: preposition 
phrase).  
Given these probabilities, we can define pat-
tern selection algorithms (e.g., always select the 
pattern with the highest probability for each top-
ic; or rotates among different patterns from high 
to low probabilities), and generate response ut-
terances based on the selected patterns. The only 
domain-dependent part of this approach is the 
selection of the seeds. The other steps all depend 
on generic linguistic structures and are domain-
independent. Thus, this probabilistic method can 
be easily applied to generic domains for custom-
izing language generation. 
4 Experiments 
A web-based multimodal spoken dialogue sys-
tem, CityBrowser (Gruenstein and Seneff, 2007), 
developed in our group, can provide users with 
information about various landmarks such as the 
address of a museum, or the opening hours of a 
restaurant. To evaluate our proposed approaches, 
we enhanced the system with a review-summary 
database generated from a review corpus that we 
harvested from a review publishing web site 
(www.citysearch.com), which contains 137,569 
reviews on 24,043 restaurants.  
We utilize the platform of Amazon Mechani-
cal Turk (AMT) to conduct a series of user stud-
ies. To understand what types of queries the sys-
tem might potentially be handling, we first con-
ducted an AMT task by collecting restaurant in-
quiries from general users. Through this AMT 
task, 250 sentences were collected and a set of 
generic templates encoding the language patterns 
of these sentences was carefully extracted. Then 
10,000 sentences were automatically created 
from these templates for language model training 
for the speech recognizer.  
To evaluate the quality of recommendations, 
we presented the system to real users via custom-
ized AMT API (McGraw et al, 2010) and gave 
each subject a set of assignments to fulfill. Each 
assignment is a scenario of finding a particular 
restaurant, as shown in Figure 6. The user can 
talk to the system via a microphone and ask for 
restaurant recommendations.  
We also gave each user a questionnaire for a 
subjective evaluation and asked them to rate the 
system on different aspects. Through this AMT 
task we collected 58 sessions containing 270 ut-
terances (4.6 utterances per session on average) 
and 34 surveys. The length of the utterances var-
ies significantly, from ?Thank you? to ?Restau-
rants along Brattle Street in Cambridge with nice 
85
cocktails.? The average number of words per 
utterance is 5.3.  
 
 
Figure 6. Interface of our system in an AMT as-
signment. 
 
Among all the 58 sessions, 51 were success-
fully fulfilled, i.e., in 87.9% of the cases the sys-
tem provided helpful recommendations upon the 
user?s request and the user was satisfied with the 
recommendations. Among those seven failed 
cases, one was due to loud background noise, 
two were due to users? operation errors (e.g., 
clicking ?DONE? before finishing the scenario), 
and four were due to recognition performance.  
The user ratings in the 34 questionnaires are 
shown in Figure 7. On a scale of 0 (the center) to 
5 (the edge), the average rating is 3.6 on the eas-
iness of the system, 4.4 on the helpfulness of the 
recommendations, and 4.1 on the naturalness of 
the system response. These numbers indicate that 
the system is very helpful at providing recom-
mendation upon users? inquiries, and the re-
sponse from the system is present in a natural 
way that people could easily understand.  
 
 
Figure 7. Users? ratings from the questionnaires. 
 
The lower rating of ease of use is partially due 
to recognition errors. For example, a user asked 
for ?pancakes?, and the system recommended 
?pizza places? to him. In some audio clips rec-
orded, the background noise is relatively high. 
This may be due to the fact that some AMT 
workers work from home, where it can be noisy.   
5 Conclusions 
In this paper we present a framework for incor-
porating review summarization into spoken rec-
ommendation systems. We proposed a set of en-
tity search methods as well as a probabilistic lan-
guage generation approach to automatically cre-
ate natural recommendations in human-computer 
conversations from review summaries. A user 
study in the restaurant domain shows that the 
proposed approaches can make the dialogue sys-
tem provide reliable recommendations and can 
help general users effectively. 
Future work will focus on: 1) improving the 
system based on users? feedback; and 2) apply-
ing the review-based approaches to dialogue sys-
tems in other domains.  
 
Acknowledgments 
This research is supported by Quanta Computers, 
Inc. through the T-Party project.  
References  
Gorin, A., Riccardi, G., and Wright, J. H. 1997. How 
May I Help You? Speech Communications. Vol. 
23, pp. 113 ? 127.  
Gruenstein, A. and Seneff, S. 2007. Releasing a Mul-
timodal Dialogue System into the Wild: User Sup-
port Mechanisms. In Proc. the 8th SIGdial Work-
shop on Discourse and Dialogue, pp. 111?119.  
Johnston, M., Bangalore, S., Vasireddy, G., Stent, A., 
Ehlen, P., Walker, M., Whittaker, S., Maloor, P. 
2002. MATCH: An Architecture for Multimodal 
Dialogue Systems. In Proc. ACL, pp. 376 ? 383. 
Liu, J. and Seneff, S. 2009. Review sentiment scoring 
via a parse-and-paraphrase paradigm, In Proc. 
EMNLP, Vol. 1.  
Liu, J., Seneff, S. and Zue, V. 2010. Dialogue-
Oriented Review Summary Generation for Spoken 
Dialogue Recommendation Systems. In Proc. 
NAACL-HLT.  
McGraw, I., Lee, C., Hetherington, L., Seneff, S., 
Glass, J. 2010. Collecting Voices from the Cloud. 
In Proc. LREC.  
Oh, A.H. and Rudnicky, A.I. 2000. Stochastic Lan-
guage Generation for Spoken Dialogue Systems. In 
Proc. of ANLP-NAACL, pp. 27-32. 
Rambow, O., Bangalore, S., Walker, M. 2001. Natu-
ral Language Generation in Dialog Systems. In 
Proc. Human language technology research.  
Seneff, S. and Polifroni, J. 2000. Dialogue Manage-
ment in the Mercury Flight Reservation System. In 
Proc. Dialogue Workshop, ANLP-NAACL.  
0
1
2
3
4
5
1
2 3
4
5
6
7
8
9
10
11
12
13
14
15
1617
18
1920
21
22
23
24
25
26
27
28
29
30
31
32
33 34
Ease of use
Helpfulness
Naturalness
86
