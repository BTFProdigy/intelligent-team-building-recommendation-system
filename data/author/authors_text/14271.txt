Proceedings of the 2010 Workshop on Companionable Dialogue Systems, ACL 2010, pages 37?42,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
?How was your day??
S. G. Pulman, J. Boye
University of Oxford
sgp@clg.ox.ac.uk
M. Cavazza, C. Smith
Teesside University
m.o.cavazza@tees.ac.uk
R. S. de la Ca?mara
Telefonica I+D
e.rsai@tid.es
Abstract
We describe a ?How was your day??
(HWYD) Companion whose purpose is to
establish a comforting and supportive rela-
tionship with a user via a conversation on
a variety of work-related topics. The sys-
tem has several fairly novel features aimed
at increasing the naturalness of the interac-
tion: a rapid ?short loop? response primed
by the results of acoustic emotion anal-
ysis, and an ?interruption manager?, en-
abling the user to interrupt lengthy or ap-
parently inappropriate system responses,
prompting a replanning of behaviour on
the part of the system. The ?long loop?
also takes into account the emotional state
of the user, but using more conventional
dialogue management and planning tech-
niques. We describe the architecture and
components of the implemented prototype
HWYD system.
1 Introduction
As the existence of this workshop shows, there is a
good deal of interest in a type of spoken language
dialogue system distinct from the traditional task-
based models used for booking airline tickets and
the like. The purpose of these ?social agent? sys-
tems is to be found in the relationship they can
establish with human users, rather than on the as-
sistance the agent can provide in giving informa-
tion or solving a problem. Designing such agents
provides many significant technical challenges, re-
quiring progress in the integration of linguistic
communication and non-verbal behaviour for af-
fective dialogue (Andre? et al 2004). In this pa-
per, we present the implementation of a Compan-
ion Embodied Conversational Agent (ECA) which
integrates emotion and sentiment detection with
more traditional dialogue components.
2 From Dialogue to Conversation
Most spoken language dialogue systems are ?task-
based?: they aim at getting from the user values for
a fixed number of slots in some template. When
enough values have been found, the filled tem-
plate is sent off to some back-end system so that
the task in question - ordering a pizza, booking a
ticket etc. - can be carried out. However, a so-
cial Companion agent assumes a kind of conver-
sation not necessarily connected to any immediate
task, and which may not follow the conventions
associated with task-driven dialogues, for exam-
ple, the relatively strict turn-taking of task-based
dialogue. In everyday life, many interhuman con-
versations see one of the participants producing
lengthy descriptions of events, without this corre-
sponding to any specific request or overall con-
versational purpose. Our objective was to sup-
port such free conversation, whilst still obtaining
meaningful answers from the agent, in the form of
advice appropriate both to the affective and infor-
mational content of the conversation. In order to
balance the constraints of free conversation with
those of tractability, we have deliberately opted
for a single-domain conversation, in contrast with
both small talk (Bickmore and Cassell, 1999) and
?chatterbot? approaches. Our HWYD domain in-
volves typical events and topics of conversation in
the workplace, ranging from the relatively mun-
dane - meeting colleagues, getting delayed by traf-
fic, project deadlines - to rather more important -
promotions, firings, arguments, office politics - de-
signed to evoke stronger emotions and hence more
affective dialogues.
However, our HWYD Companions retains
some features of a typical task based system, in
that each of these subtopics can be thought of as a
task or information extraction template. Unfilled
slots will drive the dialogue manager to question
the user for possible values. When enough slots
37
are filled, the initiative will be passed to an ?affec-
tive strategy? module, which will generate a longer
response designed to empathise appropriately with
the user over that particular topic.
3 System Overview and Architecture
The HWYD Companion integrates 15 different
software components, covering at least to some
degree all the necessary aspects of multimodal af-
fective input and output: including speech recog-
nition (ASR, using Dragon Naturally Speaking),
emotional speech recognition (AA: the EmoVoice
system (Vogt et al 2008)), turn detection (ATT),
Dialogue Act segmentation and tagging (DAT),
Emotional modelling (EM), Sentiment Analy-
sis (SA) (Moilanen et al 2007), Natural Lan-
guage Understanding (NLU), Dialogue Manage-
ment (DM), user modelling and a knowledge
base (KB/UM), an ?Affective Strategy Module?
(ASM) generating complex system replies, Natu-
ral Language Generation (NLG), Speech Synthe-
sis (TTS), an avatar (ECA), and Multimodal con-
trol of the ECA persona (MFM): gesture and fa-
cial expression, supported by the Haptek anima-
tion toolkit. Clearly the use of Naturally Speaking
imposes on us speaker dependence, since the sys-
tem needs training: in the scenario we have chosen
this is in fact not too unrealistic an assumption, but
this is merely a practical decision - we are not do-
ing research on speech recognition as such in this
project and so want to get as good a recognition
rate as possible.
The software architecture of the prototype re-
lies on the Inamode Framework developed by
Telefnica I+D. Communication between modules
follows a blackboard-like paradigm, in which cen-
tral hubs broadcast any incoming message from
any module to all of the other modules that are
connected to it. Figure 1 below shows the system
architecture, and Figure 2 shows one version of
what is on the screen when the system is running.
4 Emotional Feedback Loops
Recognising and responding appropriately to dif-
ferent emotions is an important aspect of a social
agent. In our HWYD Companion, emotion and
sentiment are used in two ways: firstly, to pro-
vide immediate feedback to a user utterance (given
that there will inevitably be some delay in the re-
sponse from natural language and dialogue pro-
cessing modules) and secondly to inform the more
extended responses given by the system when it
has learned enough about the current sub-topic.
There are two feedback loops: the ?short loop?
(response time < 700 ms) provides an immedi-
ate backchannel, and its main purpose is to main-
tain contact and keep the communication alive and
realistic. This is achieved by matching the non-
verbal response (gesture, facial expression) of the
avatar to the emotional speech parameters detected
by EmoVoice prior to affective fusion (where the
emotion detected from speech and the sentiment
value detected from the corresponding text are
merged: see below), and occasionally including
an appropriate verbal acknowledgement, on a ran-
dom basis to avoid acknowledging all user utter-
ances. The short loop essentially aligns the ECA
response to the user?s attitude, thus showing empa-
thy. (We should also use SA for this, but currently
processing speed is not fast enough).
The ?major loop? (response time < 3000 ms) in-
volves the ECA?s full response to the user utter-
ance in terms of both verbal and non-verbal be-
haviour. There are effectively two sources of sys-
tem output: the dialogue manager engages with
the user to find out what happened during their
work day, and will ask questions, or drop into
clarificatory sub-dialogues, gradually building up
a complex event description along with an assess-
ment of the prevailing emotions of the speaker.
When sufficient information has been gathered,
control is passed to an ?affective strategy module?
which will produce a longer output, typically ad-
vice or warning in response to the user?s recollec-
tion of his daily events.
The system also includes an interruption man-
ager which detects interruption and barge-in by the
user, resulting in the immediate suspension of the
current system utterance, triggering the process-
ing of any content specific to the interrupting ut-
terance, and consequent replanning on the part of
other modules to produce an appropriate response.
Such an interruption is illustrated in Figure 1. The
design of such an interruption manager in a sys-
tem with so many separate modules is quite chal-
lenging, in fact: the system is described further in
Crook et al (2010).
The ECA listens sympathetically to the user?s
account of work difficulties, whilst also reacting to
apparent discrepancies between perceived mood
and the affective content of the recognised events.
In the following example from a real conversation,
38
Figure 1: Overview of the System Architecture illustrating the information flow between modules.
Figure 2: The HWYD Companion in operation: the central portion of the screen traces the dialogue, whilst the right-hand
one displays which current software module is activated.
39
the user announces a company restructuring, but in
a rather cheerful tone. Notice that some of the En-
glish is a little strange: the generator produces a
mixture of genuinely generated output along with
some canned text.
User: I have just learnt that my department is going to be
merged with the Barcelona one (EMOTION FUSION: POS-
ITIVE ACTIVE)
System: You seem a bit too ecstatic! I?m doubtful that your
workload could be maintained.
Every time the Companion has gathered suffi-
cient information it will try to respond more glob-
ally to the situation at hand. In the following ex-
ample (see also Figure 2), after the user has ex-
pressed different sorts of concerns, the Companion
picks on the latest (positive) information to sum-
marise the few positive elements it picked from the
conversation:
User: yes but one good thing anyway we had a new
coffee machine arrived today (EMOTION FUSION: POSI-
TIVE ACTIVE)
System: it?s always reassuring to hear good news! It?s right
to feel good about such a good situation. I am quite fond of
getting new stuff. I?m happy that your mood improves. It?s
worth remembering you should take time to enjoy the good
things when they happen.
5 Emotional Modelling and Fusion
Two different modules analyse the emotional con-
tent of user utterances: an emotional speech recog-
nition system (EmoVoice (Vogt et al 2008) which
returns information indicating both the arousal and
valence of the acoustic properties of the user?s
speech as negative passive, negative-active, neu-
tral, positive-active or positive-passive, and a text-
based Sentiment Analysis module which operates
on the utterance transcript after its recognition by
the ASR module. The SA module operates in
a compositional way and is able to classify lin-
guistic units of any syntactic type: noun phrases,
clauses, sentences etc. It is also able to assign
a ?strength? of the sentiment expressed. In the
current implementation it simply classifies clauses
as either negative, neutral or positive. These two
emotional inputs are then merged by a fusion pro-
cedure, whose purpose is to provide an aggregate
emotional category to be attached to the event de-
scription template produced by the NLU and DM
module. Essentially, the mechanism for affective
fusion consists in overriding the valence category
of EmoVoice with the one obtained by SA every
time the confidence score attached to EmoVoice
is below a preset value (depending on the com-
peting valence categories). Fusion is currently an
underdeveloped module: for example, detecting
mismatches between speech and language emo-
tion and sentiment values could lead to the recog-
nition of irony, sarcasm etc. (Tepperman et al
2006). Saying an intrinsically negative thing in a
positive and cheerful way, or the other way round,
suggests that the speaker is trying for some special
effect.
6 Natural Language Understanding and
Dialogue Management
The task of the NLU module is to recognise a spe-
cific set of events reported by the user within ut-
terances which can be of significant length (> 50
words) and which can be difficult to parse due to
speech recognition errors. This led us to follow an
Information Extraction (IE) approach to dialogue
analysis (see Jo?nsson et al 2004), using shallow
syntactic and semantic processing to find instan-
tiations of event templates. The NLU component
of the HWYD Companion demonstration system
takes the 1-best output from the speech recogniser
(currently: work in progress will take n-best),
which has already been segmented into dialogue-
act sized utterances (by the DAT module which si-
multaneously segments and labels the recogniser
output: see Figure 1). So, for example, a sequence
like ?It was okay there are not many projects at the
moment so it is very quiet would be segmented
into three separate dialogue acts. The utterances
are then part-of-speech tagged and chunked into
Noun Phrase (NP) and Verb Group (VG) units.
VGs consist of a main verb and any auxiliary verbs
or semantically important adverbs. Both of these
stages are carried out by a Hidden Markov Model
trained on the Penn Treebank, although some cus-
tomisation has been carried out for this applica-
tion: relevant vocabulary added and some proba-
bilities re-estimated to reflect properties of the ap-
plication. NP and VG chunks are then classified
into ?Named Entity? classes, some of which are
the usual person, organisation, time etc. but oth-
ers of which are specific to the scenario, as is tradi-
tional in IE: e.g. salient work events, expressions
of emotion, organisational structure etc. Named
Entity classification, in the absence of domain spe-
cific training data, is carried out via hand-written
pattern matching rules and gazetteers. Each chunk
40
is further annotated with features encoding the
head word, stem form, polarity, agreement fea-
tures, relevant modifiers, etc. for later syntac-
tic and semantic processing. The NPs and VGs
are represented as unification grammar categories
containing information about the internal structure
of the constituents.
The next stage applies unification based syn-
tax rules which combine NP and VG chunks into
larger constituents. These rules are of two types:
most are syntactically motivated and are attempt-
ing to build a parse tree from which main gram-
matical relations (subject, object, etc.) can be
recognised. These have coverage of the main syn-
tactic constructs of English. But within the same
formalism we add domain specific Information
Extraction type patterns, looking out for particular
constellations of entities and events relevant to the
HWYD scenario, for example ?argument at work
between X and Y?, or ?meeting with X about Y?.
Processing is non-deterministic and so sentences
will get many analyses. We use a ?shortest path
through the chart heuristic to select an interpre-
tation. This is far from perfect, and we are cur-
rently working on a separate more motivated dis-
ambiguation module.
The final stage of processing before the Dia-
logue Manager takes over is to perform reference
resolution for pronouns and definite NPs. This
module is based partly on the system described
by Kennedy and Boguraev 1996, with the various
weighting factors based on theirs, but designed so
that the weights can be trained given appropriate
data. Currently we are collecting such data and
the present set of weights are taken from Kennedy
and Boguraev but with additional salience given
to the domain-specific named entity classes. Each
referring NP gives rise to a discourse referent, and
these are grouped into coreference classes based
on grammatical, semantic, and salience properties.
The DMmaintains an information state contain-
ing all objects mentioned during the conversation,
and uses this information to decide whether the
objects referred to in the utterance are salient or
not. The DM also uses type information to inter-
pret elliptical answers to questions (System: ?Who
was at the meeting?? User: ?Nigel.?). After the
user?s utterance has been interpreted in its dia-
logue context and the information state has been
updated, the dialogue manager decides on the ap-
propriate response. If a new object has been intro-
duced by the user, the DM adds a goal to its agenda
to talk about that object. For instance, if a new per-
son is mentioned, the DM will ask questions about
the user?s relation to that person, etc.
For each turn of the dialogue, the DM chooses
which topic to pursue next by considering all the
currently un-satisfied goals on the agenda and
heuristically rating them for importance. The
heuristics employed use factors such as recency in
the dialogue history, general importance, and emo-
tional value associated with the goal. We are cur-
rently exploring the use of reinforcement learning
with a reward function based on the results of SA
on the users input to choose goals in a more natural
way. The DM also has the option of invoking the
ASM (described below) to generate an appropri-
ate answer, in the cases where the user says some-
thing highly emotive. Again, this is a decision that
could involve reinforcement learning, and we are
exploring this in our current work.
The joint operation of the NLU and the DM
hence supports a kind of IE or task-specific
template-filling: the content of the user?s utter-
ances, prompted by questions from the DM, pro-
vides the information necessary to fill a template
to the point where the ASM can take over. The
number of templates for domain events is signifi-
cantly higher than in traditional IE or task-based
dialogue systems, however, since the HWYD
Companion currently instantiates more than 30
templates, and will eventually cover around 50.
7 Affective Dialogue Strategies
Once the NLU and DM have a sufficiently in-
stantiated template, which also records emotional
value, it is passed to the ASM. This controls the
generation of longer ECA narrative replies which
aim at influencing the user by providing advice or
reassurance. Our overall framework for influence
is inspired by the work of Bremond 1973. The
narrative is constituted by a set of argumentative
statements which can be based on emotional op-
erators (e.g. show-empathy) or specific commu-
nicative operators. The ASM is based on a Hier-
archical Task Network (HTN) planner (Nau et al
2004), which works through recursive decompo-
sition of a high level task into sub-tasks until we
reach a plan of sub-tasks that can be directly ex-
ecuted. The operators constituting the plan gen-
erated by the HTN implement Bremond?s the-
ory of influence by emphasising the determinants
41
of the event reported by the user. For instance,
various operators can emphasise or play down
the event consequences (emphasise-outcome-
importance, emphasise-outcome-justification,
emphasise-outcome-warning) or comment on
additional factors that may affect the course
of events (commend-enabler, reassure-helper).
The planner uses a set of 25 operators, each of
which can be in addition instantiated to incorpo-
rate specific elements of the event. Overall this
supports the generation of hundreds of signifi-
cantly different influencing strategies.
8 Results and Conclusions
We have described an initial, fully-implemented
prototype of a Companion ECA supporting free
conversation, including affective aspects, over a
variety of everyday work-related topics. The sys-
tem has been demonstrated extensively outside of
its development group and was regularly able to
sustain consistent dialogues with an average du-
ration exceeding 20 minutes. The Companion
ECA recently won the best demonstration prize
at AAMAS 2010,the 9th International Conference
on Autonomous Agents and Multiagent Systems,
Toronto, which is some subjective indication at
least that its behaviour is of some interest outside
of the project which developed it.
However, we have not yet systematically evalu-
ated the ECA, although this task has begun (Webb
et al 2010). The question of evaluation for sys-
tems like this is in fact a rather difficult one, since
unlike task-based systems there is no simple mea-
sure of success. In our current work we aim to
conduct extensive trials with real users and via
interview and questionnaires to get some useful
measure of how natural and ?companionable? the
system is perceived to be.
In other current work we are, as mentioned
above, experimenting with reinforcement learning
where the reward function is based on the emo-
tion and sentiment detected in the user?s input. We
are collecting data via Amazon?s Mechanical Turk
and hope to be able to show how the ECA can de-
velop different ?personalities? depending on how
this reward function is defined. For example, we
could imagine using simulated dialogues to pro-
duce a Companion that was relentlessly cheerful,
producing positive outputs whatever the input. Al-
ternatively, we could produce a ?mirror? Compan-
ion which simply reflected the mood of the user.
We could even produce a ?misery loves company?
Companion which, instead of trying to cheer the
user up when recognising negative sentiment or
emotion, could reply in an equally negative man-
ner.
Acknowledgements
This work was funded by the Companions project
(http://www.companions-project.org) sponsored by the Euro-
pean Commission as part of the Information Society Tech-
nologies (IST) programme under EC grant number IST-FP6-
034434. The EmoVoice system has been used courtesy of
the Multimedia Concepts and Applications Group of the Uni-
versity of Augsburg. Other contributors to the prototype de-
scribed in this paper are Karo Moilanen, and from the COM-
PANIONS consortium: David Benyon, Jay Bradley, Daniel
Charlton, WeiWei Cheng, Morena Danieli, Simon Dobnik,
Carlos Sanchez Fernandez, Debora Field, Mari Carmen Ro-
driguez Gancedo, Jose Relano Gil, Ramon Granell, Jaakko
Hakulinen, Preben Hansen, Sue Harding, Topi Hurtig, Oli
Mival, Roger Moore, Olov Stahl, Markku Turunen, Enrico
Zovato.
References
Andre?, E., Dybkjr, L., Minker, W., and Heisterkamp, P.
(Eds.), 2004, Affective Dialogue Systems Lecture Notes in
Computer Science 3068, Springer.
Bickmore, T., and Cassell, J., 1999. Small Talk and Con-
versational Storytelling in Embodied Interface Agents. Pro-
ceedings of the AAAI Fall Symposium on Narrative Intelli-
gence, pp. 87-92. November 5-7, Cape Cod, MA.
Bremond, C., 1973, Logique du Re?cit, Paris: Editions du
Seuil.
Cavazza, M., Pizzi, D., Charles, F., Vogt, T. And Andre?,
E. 2009, Emotional input for character-based interactive sto-
rytelling. International Joint Conference on Autonomous
Agents and Multi-Agents Systems 2009, pp. 313-320.
Nigel Crook, Cameron Smith, Marc Cavazza, Stephen
Pulman, Roger Moore, Johan Boye, 2010, Handling User In-
terruptions in an Embodied Conversational Agent Proceed-
ings of International Workshop on Interacting with ECAs as
Virtual Characters, AAMAS 2010.
Jo?nsson, A., Ande?n, F., Degerstedt, L., Flycht-Eriksson,
A., Merkel, M., and Norberg, S., 2004, Experiences from
combining dialogue system development with information ex-
traction techniques, in: Mark T. Maybury (Ed), New Direc-
tions in Question Answering, AAAI/MIT Press.
Kennedy and B. Boguraev, 1996, Anaphora for everyone:
Pronominal anaphora resolution without a parser. Proceed-
ings of the 16th International Conference on Computational
Linguistics, Copenhagen, ACL, pp 113-118.
Moilanen, K. and Pulman, S. G. , 2007, Sentiment Compo-
sition, Proceedings of the Recent Advances in Natural Lan-
guage Processing International Conference (RANLP-2007),
pp 378?382.
Nau, D., Ghallab, M., Traverso, P., 2004,Automated Plan-
ning: Theory and Practice, Morgan Kaufmann Publishers
Inc., San Francisco, CA.
J Tepperman, D Traum, and S Narayanan, 2006, ?Yeah
right?: Sarcasm recognition for spoken dialogue systems, In-
terspeech 2006, Pittsburgh, PA, 2006.
Vogt, T., Andre?, E. and Bee, N., 2008 EmoVoice - A frame-
work for online recognition of emotions from voice. In: Pro-
ceedings of Workshop on Perception and Interactive Tech-
nologies for Speech-Based Systems, Springer, Kloster Irsee,
Germany, (June 2008).
Webb, N., D. Benyon, P. Hansen and O. Mival, 2010,
Evaluating Human-Machine Conversation for Appropriate-
ness, in proceedings of the 7th International Conference on
Language Resources and Evaluation (LREC2010), Valletta,
Malta.
42
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 47?50,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
?How was your day?? An architecture for multimodal ECA systems 
 
Ra?l Santos de la 
C?mara 
Telef?nica I+D 
C/ Emilio Vargas 6 
28043 Madrid, Spain 
e.rsai@tid.es 
Markku Turunen 
Univ. of Tampere 
Kanslerinrinne 1 
FI-33014, Finland 
mturunen@ 
cs.uta.fi 
Jaakko Hakulinen 
Univ. of Tampere 
Kanslerinrinne 1 
FI-33014, Finland 
jh@cs.uta.fi 
Debora Field 
Computer Science 
Univ. of  Sheffield 
S1 4DP, UK 
d.field@shef. 
ac.uk 
 
Abstract 
Multimodal conversational dialogue sys-
tems consisting of numerous software 
components create challenges for the un-
derlying software architecture and devel-
opment practices. Typically, such sys-
tems are built on separate, often pre-
existing components developed by dif-
ferent organizations and integrated in a 
highly iterative way. The traditional dia-
logue system pipeline is not flexible 
enough to address the needs of highly in-
teractive systems, which include parallel 
processing of multimodal input and out-
put. We present an architectural solution 
for a multimodal conversational social 
dialogue system. 
1 Introduction 
Multimodal conversational dialogue applica-
tions with embodied conversational agents 
(ECas) are complex software systems consisting 
of multiple software components. They require 
much of architectural solutions and development 
approaches compared to traditional spoken dia-
logue systems. These systems are mostly assem-
bled from separate, often pre-existing compo-
nents developed by different organizations. For 
such systems, the simple pipeline architecture is 
not a viable choice. When multimodal systems 
are built, software architecture should be flexible 
enough to enable the system to support natural 
interaction with features such as continuous and 
timely multimodal feedback and interruptions by 
both participants. Such features require parallel 
processing components and flexible communica-
tion between the components. Furthermore, the 
architecture should provide an open sandbox, 
where the components can be efficiently com-
bined and experimented with during the iterative 
development process. 
The HWYD (?How was your day??) Compan-
ion system is a multimodal virtual companion 
capable of affective social dialogue and for 
which we have developed a custom novel archi-
tecture. The application features an ECA which 
exhibits facial expressions and bodily move-
ments and gestures. The system is rendered on a 
HD screen with the avatar being presented as 
roughly life-size. The user converses with the 
ECA using a wireless microphone. A demonstra-
tion video of the virtual companion in action is 
available online1. 
The application is capable of long social con-
versations about events that take place during a 
user?s working day. The system monitors the 
user?s emotional state on acoustic and linguistic 
levels, generates affective spoken responses, and 
attempts to positively influence the user?s emo-
tional state. The system allows for user initiative, 
it asks questions, makes comments and sugges-
tions, gives warnings, and offers advice. 
2 Communications framework 
The HWYD Companion system architecture em-
ploys Inamode, a loosely coupled multi-hub 
framework which facilitates a loose, non-
hierarchical connection between any number of 
components. Every component in the system is 
connected to a repeating hub which broadcasts 
all messages sent to it to all connected compo-
nents. The hub and the components connected to 
it form a single domain. Facilitators are used to 
forward messages between different domains 
according to filtering rules. During development, 
we have experimented with a number of Facilita-
tors to create efficient and simple domains to 
overcome problems associated with single-hub 
systems. For example, multiple hubs allow the 
                                                
1
 http://www.youtube.com/ 
watch?v=BmDMNguQUmM 
47
reduction of broadcast messages, which is for 
example used in the audio processing pipeline, 
where a dedicated hub allows very rapid message 
broadcast (nearly 100 messages per second are 
exchanged) without compromising the stability 
of the system by flooding the common pipeline. 
For communication between components, a 
lightweight communication protocol is used to 
support components implemented in various 
programming languages. A common XML mes-
sage ?envelope? specifies the basic format of 
message headers as seen in Figure 1. 
 
Figure 1: System message XML format 
. 
Mandatory elements in the envelope (top 
block) are necessary so other modules can iden-
tify the purpose of the message and its contents 
upon a shallow inspection. These include the 
sender component and a unique message id. Ad-
ditional envelope fields elements include: mes-
sage type, turn id, dialogue segment identifier, 
recipient identifier, and a list of message identi-
fiers corresponding to the previous messages in 
the current processing sequence.  
For system-wide and persistent knowledge 
management, a central XML-database allows the 
system to have inter-session and intra-session 
?memory? of past events and dialogues. This da-
tabase (KB) includes information such the user 
and dialogue models, processing status of mod-
ules, and other system-wide information. 
3 Data flow in the architecture 
To maximize the naturalness of the ECA?s inter-
action, the system implements parallel process-
ing paths. It also makes use of a special module, 
the Interruption Manager (IM), to control 
components in situations where regular process-
ing procedure must be deviated from. In addi-
tion, there are ?long? and ?short? processing  se-
quences from user input to system output. Both 
?loops? operate simultaneously. The Main Dia-
logue (?long?) Loop, which is the normal proc-
essing path, is indicated by the bold arrows in 
Fig. 2, and includes all system components ex-
cept the IM. The dotted arrows signal the devia-
tions to this main path that are introduced by the  
Natural Language 
Understanding (NLU)
Acoustic
Analyzer (AA)
Autom
atic Speech
Recognition (ASR)
Acoustic Em
otion Classifier (AEC)
Sentiment
Analyzer (SA)
Dialogue Act 
Tagger (DAT)
Dialogue 
Manager(DM)
Affective Strategy 
(ASM
)
Multimodal Fission 
Manager (MFM)
Text-to-Speech
(TTS)
Avatar
(ECA)
Acoustic Turn-
Taking (ATT)
Interruption 
M
anager(IM
)
Emotional 
Model (EM)
Natural Language 
Generation (NLG)
Knowledge 
Base & UM
 
Figure 2:HWYD Companion main modules 
 
interruption management and feedback loops.  
The system has an activity detector in the input 
subsystem that is active permanently and analy-
ses user input in real-time. If there is a detection 
of user input at the same time as the ECA is talk-
ing, this module triggers a signal that is captured 
by the IM. The IM, which tracks the activity of 
the rest of the modules in the system, has a set of 
heuristics that are examined each time this trig-
gering signal is detected. If any heuristic 
matches, the system decides there has been a 
proper user interruption and decides upon a se-
ries of actions to recover from the interruption. 
4 Module Processing Procedure 
The first stage in the processing is the acoustic 
processing. User speech is processed by the 
Acoustic Analyzer, the Automatic Speech Rec-
ognizer, and the Acoustic Emotion Classifier 
simultaneously for maximum responsiveness. 
The Acoustic Analyzer (AA) extracts low-
level features (pitch, intensity and the probability 
that the input was from voiced speech) from the 
acoustic signal at frequent time intervals (typi-
cally 10 milliseconds). Features are passed to the 
Acoustic Turn-Taking Detector in larger buffers 
(a few hundred milliseconds) together with time-
stamps. AA is implemented in TCL using Snack 
toolkit (http://www.speech.kth.se/snack/). 
The Acoustic Turn-Taking detector (ATT) 
is a Java module, which estimates when the user 
has finished a turn by comparing intensity pause 
lengths and pitch information of user speech to 
configurable empirical thresholds. ATT also de-
cides whether the user has interrupted the system 
48
(?barge-in?), while ignoring shorter backchannel-
ling phrases (Crook et al (2010)). Interruption 
messages are passed to the Interruption Manager. 
ATT receives a message from the ECA module 
when the system starts or stops speaking. 
Dragon Naturally Speaking Automatic 
Speech Recognition (ASR) system is used to 
provide real-time large vocabulary speech recog-
nition. Per-user acoustic adaptation is used to 
improve recognition rates. ASR provides N-best 
lists, confidence scores, and phrase hypotheses. 
The Acoustic Emotion Classifier (AEC) 
component (EmoVoice (Vogt et al (2008)) cate-
gorizes segments of user speech into five va-
lence+arousal categories, also applying a confi-
dence score. The Interruption Manager monitors 
the messages of the AEC to include emotion-
related information into feedback loop messages 
sent to the ECA subsystem. This allows rapid 
reactions to the user mood. 
The Sentiment Analyzer (SA) labels ASR 
output strings with sentiment information at 
word and sentence levels using valence catego-
ries positive, neutral and negative. The SA uses 
the AFFECTiS Sentiment Server, which is a gen-
eral purpose .NET SOAP XML service for 
analysis and scoring of author sentiment. 
The Emotional Model (EM), written in Lisp, 
fuses information from the AEC and SA. It 
stores a globally accessible emotional representa-
tion of the user for other system modules to 
make use of. Affective fusion is rule-based, pre-
fers the SA?s valence information, and outputs 
the same five valence+arousal categories as used 
in the AEC. The EM can also serve as a basis for 
temporal integration (mood representation) as 
part of the affective content of the User Model. It 
also combines the potentially different segmenta-
tions by the ASR and AEC. 
The User Model (UM) stores facts about the 
user as objects and associated attributes. The in-
formation contained in the User Model is used by 
other system modules, in particular by Dialogue 
Manager and Affective Strategy Module. 
The Dialogue Act Tagger and Segmenter 
(DAT), written in C under Linux, uses the ATT 
results to compile all ASR results corresponding 
to each user turn. DAT then segments the com-
bined results into semantic units and labels each 
with a dialogue act (DA) tag (from a subset of 
SWBD-DAMSL (Jurafsky et al (2001)). A Sto-
chastic Machine Learning model combining 
Hidden Markov Model (HMM) and N-grams is 
used in a manner analogous to Mart?nez-
Hinarejos et al (2006). The N-grams yield the 
probability of a possible DA tag given the previ-
ous ones. The Viterbi algorithm is used to find 
the most likely sequence of DA tags.  
The Natural Language Understanding 
(NLU) component, implemented in Prolog, pro-
duces a logical form representing the semantic 
meaning of a user turn. The NLU consists of a 
part-of-speech tagger, a Noun Phrase and Verb 
Group chunker, a named-entity classification 
component (rule-based), and a set of pattern-
matching rules which recognize major gram-
matical relationships (subject, direct object, etc.) 
The resulting shallow-parsed text is further proc-
essed using pattern-matching rules. These recog-
nize configurations of entity and relation relevant 
to the templates needed by the Dialogue Man-
ager, the EM, and the Affective Strategy Module. 
The Dialogue Manager (DM), written in Java 
and Prolog, combines the SA and NLU results, 
decides on the system's next utterance and identi-
fies salient objects for the Affective Strategy 
Module. The DM maintains an information state 
containing information about concepts under dis-
cussion, as well as the system's agenda of current 
conversational goals.  
One of the main features of the HWYD Com-
panion is its ability to positively influence the 
user?s mood through its Affective Strategy 
Module (ASM). This module appraises the 
user?s situation, considering the events reported 
in the user turn and its (bi-modal) affective ele-
ments. From this appraisal, the ASM generates a 
long multi-utterance turn. Each utterance imple-
ments communicative acts constitutive of the 
strategy. ASM generates influence operators 
which are passed to the Natural Language Gen-
eration module. ASM output is triggered when 
the system has learned enough about a particular 
event to warrant affective influence. As input, 
ASM takes information extraction templates de-
scribing events, together with the emotional data 
attached. ASM is a Hierarchical Task Network 
(HTN) Planner implemented in Lisp.  
The Natural Language Generator (NLG), 
written in Lisp, produces linguistic surface forms 
from influence operators produced by the ASM. 
These operators correspond to communicative 
actions taking the form of performatives. NLG 
uses specific rhetorical structures and constructs 
associated with humour, and uses emotional TTS 
expressions through specific lexical choice.  
49
5 Multimodal ECA Control 
Multimodal control of the ECA, which consists 
of a tightly-synchronized naturalistic avatar and 
affective Text-To-Speech (TTS) generation, is 
highly challenging from an architectural view-
point, since the coordinating component needs to 
be properly synchronized with the rest of the sys-
tem, including both the main dialogue loop and 
the feedback and interruption loops. 
The system Avatar is in charge of generating a 
three-dimensional, human-like character to serve 
as the system?s ?face?. The avatar is connected to 
the TTS, and the speech is synchronized with the 
lip movements. The prototype is currently using 
the HaptekTM 3D avatar engine running inside a 
web browser. The Haptek engine provides a talk-
ing head and torso along with a low level API to 
control its interaction with any SAPI-compliant 
TTS subsystem, and also allows some manipula-
tion of the character animation. An intermediate 
layer consisting of a Java applet and Javascript 
code embeds the rendered avatar in a web page 
and provides connectivity with the Multimodal 
Fission Manager. We intend to replace the cur-
rent avatar with a photorealistic avatar under de-
velopment within the project consortium. 
LoquendoTM TTS SAPI synthesizer is used to 
vocalize system turns. The TTS engine works in 
close connection with the ECA software using 
the SAPI interface. TTS includes custom para-
linguistic events for producing expressive 
speech. TTS is based on the concatenative tech-
nique with variable length acoustic units. 
The Multimodal Fission Manager (MFM) con-
trols the Avatar and the TTS engine, enabling the 
system to construct complex communicative acts 
that chain together series of utterances and ges-
tures. It offers FML-standard-based syntax to 
make the avatar perform a series of body and 
facial gestures. 
The system features a template-based input 
mode in which a module can call ECA to per-
form actions without having to build a full FML-
based XML message. This is intended to be used 
in the feedback loops, for example, to convey the 
impression that the ECA is paying attention.  
6 Conclusions 
We have presented an advanced multimodal dia-
logue system that challenges the usual pipeline-
based implementation. To do so, it leverages on 
an architecture that provides the means for a 
flexible component interconnection, that can ac-
comodate the needs of a system using more than 
one processing path for its data. We have shown 
how this has enabled us to implement complex 
behavior such as interrupt and short loop han-
dling. We are currently expanding coverage and 
will carry out an evaluation with real users this 
September. 
Acknowledgements 
This work was funded by Companions, a Euro-
pean Commission Sixth Framework Programme 
Information Society Technologies Integrated 
Project (IST-34434). 
References 
Vogt, T., Andr?, E. and Bee, N. 2008. EmoVoice ? A 
framework for online recognition of emotions from 
voice. In: Proc. Workshop on Perception and In-
teractive Technologies for Speech-Based Systems, 
Springer, Kloster Irsee, Germany. 
Cavazza, M., Smith, C., Charlton, D., Crook, N., 
Boye, J., Pulman, S., Moilanen, K., Pizzi, D., San-
tos de la Camara, R., Turunen, M. 2010 Persuasive 
Dialogue based on a Narrative Theory: an ECA 
Implementation, Proc. 5th Int. Conf. on Persuasive 
Technology (to appear). 
Hern?ndez, A., L?pez, B., Pardo, D., Santos, R., 
Hern?ndez, L., Rela?o Gil, J. and Rodr?guez, M.C. 
2008 Modular definition of multimodal ECA 
communication acts to improve dialogue robust-
ness and depth of intention. In: Heylen, D., Kopp, 
S., Marsella, S., Pelachaud, C., and Vilhj?lmsson, 
H. (Eds.), AAMAS 2008 Workshop on Functional 
Markup Language. 
Crook, N., Smith, C., Cavazza, M., Pulman, S., 
Moore, R., and Boye, J. 2010 Handling User Inter-
ruptions in an Embodied Conversational Agent.  In 
Proc. AAMAS 2010. 
Wagner J., Andr?, E., and Jung, F. 2009 Smart sensor 
integration: A framework for multimodal emotion 
recognition in real-time. In Affective Computing 
and Intelligent Interaction 2009. 
Cavazza, M., Pizzi, D., Charles, F., Vogt, T. Andr?, 
E. 2009 Emotional input for character?based in-
teractive storytelling AAMAS (1) 2009: 313-320. 
Jurafsky, D. Shriberg, E., Biasca, D. 2001 
Switchboard swbd?damsl shallow? discourse?
function annotation coders manual. Tech. Rep. 97
?01, University of Colorado Institute of Cognitive 
Science 
Mart?nez?Hinarejos, C.D., Granell, R., Bened?, J.M. 
2006. Segmented and unsegmented dialogue?act 
annotation with statistical dialogue models. Proc. 
COLING/ACL Sydney, Australia, pp. 563?570. 
50
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 277?280,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
?How was your day?? An affective companion ECA prototype 
 
Marc Cavazza 
School of Computing 
Teesside University 
Middlesbrough TS1 3BA 
m.o.cavazza@tees.ac.uk 
Ra?l Santos de la C?mara 
Telef?nica I+D 
C/ Emilio Vargas 6 
28043 Madrid 
e.rsai@tid.es 
Markku Turunen 
University of Tampere 
Kanslerinrinne 1 
FI-33014 
mturunen@cs.uta.fi 
 
 
Jos? Rela?o Gil 
Telef?nica I+D 
C/ Emilio Vargas 6 
28043 Madrid 
joserg@tid.es 
Jaakko Hakulinen 
University of Tampere 
Kanslerinrinne 1 
FI-33014 
jh@cs.uta.fi 
Nigel Crook 
Oxford University 
Computing Laboratory 
Oxford OX1 3QD 
nigc@comlab.ox.
ac.uk 
Debora Field 
Computer Science 
Sheffield University 
Sheffield S1 4DP 
d.field@shef. 
ac.uk 
 
Abstract 
This paper presents a dialogue system in 
the form of an ECA that acts as a socia-
ble and emotionally intelligent compan-
ion for the user. The system dialogue is 
not task-driven but is social conversation 
in which the user talks about his/her day 
at the office. During conversations the 
system monitors the emotional state of 
the user and uses that information to in-
form its dialogue turns. The system is 
able to respond to spoken interruptions 
by the user, for example, the user can in-
terrupt to correct the system. The system 
is already fully implemented and aspects 
of actual output will be used to illustrate. 
1 Introduction 
Historically, Embodied Conversational Agents 
(ECAs) have been used in research and industry 
make information and complex tasks more ac-
cessible to customers and users. With the rise of 
new technologies in affective dialogue systems, 
we are beginning to see a future in which ECA 
dialogues are not all task-driven, but some will 
be focused on the social aspects of conversation. 
We envisage the development of ECAs that en-
hance the social quality of life of the user, and 
that build deep relationships with their users over 
years of use. Our affective companion ECA is 
our first contribution to this emerging technol-
ogy.  
Our emotionally intelligent dialogue system 
requires a high level of understanding of the 
meaning of the user turns, and so is quite unlike 
a ?chatbot?. Affective aspects of the situation are 
also key in influencing the course of the conver-
sation. The system attempts to balance both se-
mantic and affective understanding in order to 
create an engaging and enjoyable companion for 
social conversation. 
2 Scenario description 
The system conversation is free-ranging and fol-
lows the user?s initiative, rather than being task-
driven. We have for the time being restricted the 
range of conversation topics to things that hap-
pen in the life of someone who works in an of-
fice (the ?How was your day?? (HWYD) sce-
nario). The user ?offloads? about his/her day at 
work by chatting to the ECA, and the ECA tries 
to empathise with and advise the user. The ECA 
asks questions to elicit information from the user, 
makes empathetic and sympathetic comments, 
and offers advice.  
The system is able to process long user turns, 
as well as to generate long system turns (around 
50 words). If the user perhaps dislikes what 
he/she hears, he/she can interrupt the system by 
barging in, and the ECA will respond in a hu-
man-like way. The conversation style is therefore 
considerably different from traditional task-based 
dialogues that consist of short dialogue turns and 
in which the system takes all the initiative.  
The dialogue in Table 1 (actual output from a 
user session) illustrates some of the significant 
features of the system, including long system 
277
turns, user interruptions and emotional analysis 
of user input.  
 
User1 Worst day ever. I arrived late be-
cause of the traffic. 
System1 You have my sympathy. What 
happened next? 
User2 I missed the meeting on project 
responsibilities; got more work 
dumped on me. Just what I don?t 
need. I?m already likely to miss 
the deadline. [ECA looks worried 
when user?s negative tone is de-
tected] 
System2 Sorry to hear that. Who was at the 
meeting? 
User3 Only my team. They gave me a 
set of PowerPoint slides from the 
meeting. I spent the rest of the day 
reading them but my computer 
kept crashing because of  updates. 
System3 Oh dear! It is understandable how 
you must be feeling. It?s right to 
be unhappy about such a bad situ-
ation. Try not to worry about 
missing the deadline because of 
the technical problems. [At this 
point user interrupts ECA by 
beginning turn User4] It?s sad that 
your motivation could be affected 
by them. [System stops speaking] 
User4 No, I?m not worried about the 
technical problems, I?m worried 
about the amount of extra work on 
this project and the deadline. 
System4 What is the project about? 
 
Table 1: Dialogue example 
 
A video demonstration of the system in action 
is available at: 
http://www.youtube.com/watch?v=
BmDMNguQUmM 
3 Architecture 
Figure 1 shows a screen shot taken at run-time of 
actual system output. The ECA is represented on 
a screen as a woman (waist up) who displays 
natural, human-like movements and performs a 
wide range of complex facial expressions, bodily 
movements, and hand and arm gestures. 
The screen also displays a transcript of the 
user and system turns. The user turns shown con-
stitute the output of the Automatic Speech Rec-
ogniser (ASR). The system?s analysis of the 
user?s emotional state is also shown. 
The right-most panel of the screen shows 
graphics which convey real-time information 
about how the dialogue is being processed. It 
presents a streamlined view of the software 
modules that comprise the system. Module activ-
ity is visually represented at run-time by flashing 
colours. This ?glass-box? approach enables de-
tailed observation and analysis of system 
procedure at run-time. 
The system comprises a number of distinct 
modules that are connected using Inamode, a 
hub-based message-passing framework using 
XML formatted messages over plain text sock-
ets. 
The system?s ASR is the NuanceTM dictation 
engine. This is run in parallel with our own a-
coustic analysis pipeline which extracts low level 
(pitch, tone) speech features and also high-level 
features such as emotional characteristics. 
Analysis of the emotions is currently carried out 
 
Figure 1: Screenshot of the prototype interface 
 
278
by EmoVoice (Vogt et al (2008)). The ASR 
output strings are analysed for sentiment by the 
AFFECTiS system (Moilanen and Pulman (2007, 
2009)) and classed as positive, neutral, or nega-
tive. This output is fused with the output from 
EmoVoice to generate a value that represents the 
user?s current emotional state, which is ex-
pressed as a valence+arousal pairing (with five 
possible values). 
The ASR output goes to our own Natural Lan-
guage Understanding (NLU) module which per-
forms syntactic and semantic analysis of user 
utterances and derives noun phrases and verb 
groups and associated arguments. Events rele-
vant to the scenario (e.g., promotions, redundan-
cies, meetings, arguments, etc.) are recognised 
by the NLU and are used to populate an ontology 
(a model of the conversation content).  The sys-
tem is currently able to recognize and respond to 
more than 30 event types.  
The events recognised in a user turn are 
labelled with the output of the Emotion Module 
for that turn; the result is a representation of both 
the semantic and affective information that the 
user might be trying to convey. 
Our own rule-based Dialogue Manager (DM) 
takes the affect-annotated semantic output of the 
NLU, and from that and its model of the conver-
sation content determines the next system turn. It 
will either ask a question about the events that 
occurred in the user?s day, express an opinion on 
the events already described, or make empathetic 
comments. Whenever the system has gained suf-
ficient understanding of a key event in the user?s 
day, it generates a complex long turn that encap-
sulates comfort, opinion, warnings and advice to 
the user. 
These long system turns are generated by our 
own plan-based Affective Strategy Module that 
makes an appraisal of the user?s situation and  
generates an appropriate emotional strategy 
(Cavazza et al (2010)). This strategy?expressed 
as an abstract, conceptual representation?is han-
ded to our own Natural Language Generator 
(NLG) that maps it into a series of linguistic sur-
face forms (usually 4 or 5 sentences). We use a 
style-controllable system using Tree-Furcating 
Grammars (an extension of the Tree-Adjoining 
Grammars formalism (Joshi et al (1997)). This 
ensures the generation of a large set of different 
surface forms from the same semantic input. 
The output of the NLG is passed to a module 
that adds this information to its system turn 
instructions for the ECA. The ECA has been de-
veloped around the HaptekTM toolkit and is con-
trolled using an FML-like language (after 
Hern?ndez et al (2008)). This 2-D embodiment 
produces gestures, facial expressions, and body 
movements that convey the emotional state of 
the ECA. Its movements and expressions enable 
it to visually display interest and enjoyment in 
talking to the user, and to display empathy with 
the user. The speech synthesis module is our own 
emotion-focused extension of the LoquendoTM 
TTS system. It includes paralinguistic elements 
such as exclamations and laughter, and emo-
tional prosody generation for negative and posi-
tive utterances. 
4 Special procedural features 
A significant processing design feature of the 
system is that there are two main processing 
loops from user input to system output; a ?long 
loop? which passes through all the components 
of the system; and a ?short loop? or ?feedback 
loop? which will now be discussed (the proce-
dure already described in Section 3 is the long 
loop procedure). 
4.1 Feedback loop 
The feedback loop (?short loop?) bypasses many 
linguistic components and generates immediate 
reactions to user activity. The main function of 
the short loop is maintain user engagement by 
preventing unnaturally long gaps of ECA inactiv-
ity. The feedback loop engages the acoustic 
analysis components, the TTS, and the ECA. It is 
responsible for the generation of real-time (< 500 
ms) reactions in the ECA in response to the emo-
tional state of the user. It attempts to align  both 
verbal behaviour (backchannelling) and non-
verbal behaviour (facial expressions, gestures, 
and general body language) to the emotions de-
tected during most recent user turn. In order to 
achieve a reasonable level of realism, these sys-
tem reactions to the perceived emotional state of 
the user need to be perceptibly instantaneous. 
Using this short feedback loop that bypasses 
many of the linguistic components ensures this. 
The feedback loop is also occasionally used to 
make sympathetic comments immediately after 
the user stops speaking. These act as acknowl-
edgements of the emotion expressed by the user. 
An example can be seen in the System2 turn of 
the example dialogue in Table 1: 
1.?Sorry to hear that. Who was at the meeting?? 
Here, the first utterance was spoken by the sys-
tem within a few tenths of a second after the end 
279
of the previous user turn (User2). The system 
tried to identify the user?s emotion in the previ-
ous turn and then to behave linguistically and 
visually in an empathetic way. The actual sympa-
thetic utterance was randomly chosen from a set 
of ?negative emotion utterances? (there are also 
?positive? and ?neutral? sets).  
The second half of the system turn in (1) was 
derived by the system?s ?long loop?. It is a ques-
tion which refers to a meeting that the user men-
tioned in the previous turn. This ?meeting? event 
has been heard by the ASR, understood by the 
NLU system, remembered by the DM, and is 
now referred to by an appropriate definite noun 
phrase in the output of the NLG.   
The feedback and main loops run in parallel. 
However, the feedback loop generates its speech 
output almost immediately, giving time for the 
main dialogue loop to complete its more detailed 
analysis of the user?s utterance.  
4.2 Handling user interruptions 
This system has a complex strategy for handling 
situations in which the user interrupts long 
system turns.  The system?s response to ?barge-
in? user interruptions is overseen by the Interrup-
tion Manager (IM), which is alerted by the 
acoustic input modules whenever a genuine user 
interruption (as opposed to, say, a backchannel) 
is detected during a long system utterance. When 
alerted, the IM instructs the ECA to stop speak-
ing when it reaches a natural stopping point in its 
current turn (usually the end of the current 
phrase). The user?s interruption utterance is 
processed by the long loop. Its progress is 
tracked and controlled by the IM, for example, it 
makes sure that the linguistic modules know that 
the current utterance is an interruption, whic 
means it requires special treatment. The DM has 
a range of strategies for system recoveries from 
user interruptions, including different ways of 
continuing, replanning, and aborting. An exam-
ple of a user interruption is shown in Table 1. 
The user interrupts the long system utterance in 
the System3 turn. The system?s response to the 
interruption is to stop the speech output from the 
ECA, abort the long system turn altogether, and 
instead to ask for more details about the project 
that the user has just mentioned during the inter-
ruption. (See (Crook et al (2010))  for a more 
detailed description of the IM.) 
 
 
Acknowledgements 
This work was funded by Companions, a Eu-
ropean Commission Sixth Framework Pro-
gramme Information Society Technologies Inte-
grated Project (IST-34434).  
We would also like to thank the following 
people for their valuable contributions to the 
work presented here: Stephen Pulman, Ramon 
Granell, and Simon Dobnick (Oxford Univer-
sity), Johan Boye (KTH Stockholm), Cameron 
Smith and Daniel Charlton (Teesside Univer-
sity), Roger Moore, WeiWei Cheng and Lei Ye 
(University of Sheffield), Morena Danieli and 
Enrico Zovato (Loquendo). 
References 
Cavazza, M., Smith, C., Charlton, D., Crook, N., 
Boye, J., Pulman, S., Moilanen, K., Pizzi, D., San-
tos de la Camara, R., Turunen, M. 2010 Persuasive 
Dialogue based on a Narrative Theory: an ECA 
Implementation, Proc. of the 5th Int. Conf. on Per-
suasive Technology (Persuasive 2010), to appear 
2010. 
Crook, N., Smith, C., Cavazza, M., Pulman, S., 
Moore, R., and Boye, J. 2010 Handling User Inter-
ruptions in an Embodied Conversational Agent In 
proc. of AAMAS 2010. 
Hern?ndez, A., L?pez, B., Pardo, D., Santos, R., 
Hern?ndez, L., Rela?o Gil, J. and Rodr?guez, M.C. 
(2008) Modular definition of multimodal ECA 
communication acts to improve dialogue robust-
ness and depth of intention. In: Heylen, D., Kopp, 
S., Marsella, S., Pelachaud, C., and Vilhj?lmsson, 
H. (Eds.), AAMAS 2008 Workshop on Functional 
Markup Language.  
Joshi, A.K. & Schabes, Y. (1997) Tree-adjoining 
Grammars. Handbook of formal languages, vol. 3: 
Beyond Words, Springer-Verlag New York, Inc., 
New York, NY, 1997. 
Moilanen, K. and Pulman. S. (2009). Multi-entity 
Sentiment Scoring. Proc. Recent Advances in 
Natural Language Processing (RANLP 2009). 
September 14-16, Borovets, Bulgaria. pp. 258--
263.  
Moilanen, K. and Pulman. S. (2007). Sentiment Com-
position. Proc. Recent Advances in Natural Lan-
guage Processing (RANLP 2007). September 27-
29, Borovets, Bulgaria. pp. 378--382. 
Vogt, T., Andr?, E. and Bee, N. 2008. EmoVoice ? A 
framework for online recognition of emotions 
from voice. Proc. Workshop on Perception and 
Interactive Technologies for Speech-Based Sys-
tems, Springer, Kloster Irsee, Germany, (June 
2008). 
280
