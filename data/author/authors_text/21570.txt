Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1779?1785,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Cross-Lingual Part-of-Speech Tagging through Ambiguous Learning
Guillaume Wisniewski Nicolas P?cheux Souhir Gahbiche-Braham Fran?ois Yvon
Universit? Paris Sud
LIMSI-CNRS
91 403 ORSAY CEDEX, France
{wisniews,pecheux,souhir,yvon}@limsi.fr
Abstract
When Part-of-Speech annotated data is
scarce, e.g. for under-resourced lan-
guages, one can turn to cross-lingual trans-
fer and crawled dictionaries to collect par-
tially supervised data. We cast this prob-
lem in the framework of ambiguous learn-
ing and show how to learn an accurate
history-based model. Experiments on ten
languages show significant improvements
over prior state of the art performance.
1 Introduction
In the past two decades, supervised Machine
Learning techniques have established new perfor-
mance standards for many NLP tasks. Their suc-
cess however crucially depends on the availability
of annotated in-domain data, a not so common sit-
uation. This means that for many application do-
mains and/or less-resourced languages, alternative
ML techniques need to be designed to accommo-
date unannotated or partially annotated data.
Several attempts have recently been made to
mitigate the lack of annotated corpora using par-
allel data pairing a (source) text in a resource-rich
language with its counterpart in a less-resourced
language. By transferring labels from the source
to the target, it becomes possible to obtain noisy,
yet useful, annotations that can be used to train a
model for the target language in a weakly super-
vised manner. This research trend was initiated
by Yarowsky et al. (2001), who consider the trans-
fer of POS and other syntactic information, and
further developed in (Hwa et al., 2005; Ganchev
et al., 2009) for syntactic dependencies, in (Pad?
and Lapata, 2009; Kozhevnikov and Titov, 2013;
van der Plas et al., 2014) for semantic role la-
beling and in (Kim et al., 2012) for named-entity
recognition, to name a few.
Assuming that labels can actually be projected
across languages, these techniques face the issue
of extending standard supervised techniques with
partial and/or uncertain labels in the presence of
alignment noise. In comparison to the early ap-
proach of Yarowsky et al. (2001) in which POS
are directly transferred, subject to heuristic fil-
tering rules, recent works consider the integra-
tion of softer constraints using expectation regu-
larization techniques (Wang and Manning, 2014),
the combination of alignment-based POS transfer
with additional information sources such as dic-
tionaries (Li et al., 2012; T?ckstr?m et al., 2013)
(Section 2), or even the simultaneous use of both
techniques (Ganchev and Das, 2013).
In this paper, we reproduce the weakly super-
vised setting of T?ckstr?m et al. (2013). By re-
casting this setting in the framework of ambiguous
learning (Bordes et al., 2010; Cour et al., 2011)
(Section 3), we propose an alternative learning
methodology and show that it improves the state of
the art performance on a large array of languages
(Section 4). Our analysis of the remaining errors
suggests that in cross-lingual settings, improve-
ments of error rates can have multiple causes and
should be looked at with great care (Section 4.2).
All tools and resources used in this study
are available at http://perso.limsi.fr/
wisniews/ambiguous.
2 Projecting Labels across Aligned
Corpora
Projecting POS information across languages re-
lies on a rather strong assumption that morpho-
syntactic categories in the source language can
be directly related to the categories in the tar-
get language, which might not always be war-
ranted (Evans and Levinson, 2009; Broschart,
2009). The universal reduced POS tagset pro-
posed by Petrov et al. (2012) defines an opera-
tional, albeit rather empirical, ground to perform
this mapping. It is made of the following 12 cat-
egories: NOUN (nouns), VERB (verbs), ADJ (ad-
1779
ar cs de el es fi fr id it sv
% of test covered tokens (type) 83.2 93.2 95.6 97.4 96.7 83.0 98.3 90.5 95.8 95.3
% of test correctly covered token (type) 72.9 94.2 93.7 92.9 93.8 93.6 92.1 89.6 93.6 94.1
avg. number of labels per token (type) 2.1 1.3 1.3 1.3 1.3 1.4 1.3 1.2 1.3 1.3
avg. number of labels per token (type+token) 1.7 1.1 1.1 1.1 1.1 1.2 1.2 1.1 1.1 1.1
% of aligned tokens 53.0 77.8 66.7 69.3 74.0 73.1 64.7 81.6 72.2 79.9
% of token const. violating type const. 2.5 16.0 15.8 21.4 16.9 14.3 16.1 19.3 17.5 13.6
% informative token const. 79.7 27.5 15.7 29.8 21.3 36.0 25.5 16.2 28.2 26.4
Table 1: Interplay between token and type constraints on our training parallel corpora. ?Informative?
token constraints correspond to tokens for which (a) a POS is actually transfered and (b) type constraints
do not disambiguate the label, but type+token constraints do.
jectives), ADV (adverbs), PRON (pronouns), DET
(determiners and articles), ADP (prepositions and
postpositions), NUM (numerals), CONJ (conjunc-
tions), PRT (particles), ?.? (punctuation marks)
and X (a catch-all for other categories). These
labels have been chosen for their stability across
languages and for their usefulness in various mul-
tilingual applications. In the rest of this work, all
annotations are mapped to this universal tagset.
Transfer-based methods have shown to be very
effective, even if projected labels only deliver a
noisy supervision, due to tagging (of the source
language) and other alignment errors (Yarowsky
et al., 2001). While this uncertainty can be ad-
dressed in several ways, recent works have pro-
posed to combine projected labels with monolin-
gual information in order to filter out invalid la-
bel sequences (Das and Petrov, 2011; T?ckstr?m
et al., 2013). In this work we follow T?ckstr?m et
al. (2013) and use two families of constraints:
Token constraints rely on word alignments to
project labels of source words to target words
through alignment links. Table 1 shows that, de-
pendening on the language, only 50?80% of the
target tokens would benefit from label transfer.
Type constraints rely on a tag dictionary to
define the set of possible tags for each word
type. Type constraints reduce the possible la-
bels for a given word and help filtering out cross-
lingual transfer errors (up to 20%, as shown in Ta-
ble 1). As in (T?ckstr?m et al., 2013), we con-
sider two different dictionaries. The first one is
extracted automatically from Wiktionary,
1
us-
ing the method of (Li et al., 2012). The second
tag dictionary is built by using for each word the
two most frequently projected POS labels from
the training data.
2
In contrast to T?ckstr?m et al.
1
http://www.wiktionary.org/
2
This heuristic is similar to the way T?ckstr?m et al.
(2013) we use the intersection
3
of the two type
constraints instead of their union. Table 1 shows
the precision and recall of the resulting constraints
on the test data.
These two information sources are merged ac-
cording to the rules of T?ckstr?m et al. (2013).
These rules assume that type constraints are more
reliable than token constraints and should take
precedence: by default, a given word is associated
to the set of possible tags licensed type constraints;
additionally, when a POS tag can be projected
through alignment and also satisfies the type con-
straints, then it is actually projected, thereby pro-
viding a full (yet noisy) supervision.
As shown in Table 1, token and type con-
straints complement each other effectively and
greatly reduce label ambiguity. However, the
transfer method sketched above associates each
target word with a set of possible labels, of which
only one is true. This situation is less favorable
than standard supervised learning in which one
unique gold label is available for each occurrence.
We describe in the following section how to learn
from this ambiguous supervision information.
3 Modeling Sequences under Ambiguous
Supervision
We use a history-based model (Black et al., 1992)
with a LaSO-like training method (Daum? and
Marcu, 2005). History-based models reduce struc-
tured prediction to a sequence of multi-class clas-
sification problems. The prediction of a complex
structure (here, a sequence of POS tags) is thus
modeled as a sequential decision problem: at each
(2013) filter the tag distribution with a threshold to build the
projected type constraints.
3
If the intersection is empty we use the constraints
from Wiktionary first, if also empty, the projected con-
straints then, and by default the whole tag set.
1780
position in the sequence, a multiclass classifier
is used to make a decision, using features that
describe both the input structure and the history
of past decisions (i.e. the partially annotated se-
quence).
Let x = (x
i
)
n
i=1
denote the observed sequence
and Y be the set of possible labels (in our case
the 12 universal POS tags). Inference consists in
predicting labels one after the other using, for in-
stance, a linear model:
y
?
i
= argmax
y?Y
?w|?(x, i, y, h
i
)? (1)
where ??|?? is the standard dot product operation,
y
?
i
the predicted label for position i, w the weight
vector, h
i
= y
?
1
, ..., y
?
i?1
the history of past de-
cisions and ? a joint feature map. Inference can
therefore be seen as a greedy search in the space
of the # {Y}
n
possible labelings of the input se-
quence. Trading off the global optimality of in-
ference for additional flexibility in the design of
features and long range dependencies between la-
bels has proved useful for many sequence labeling
tasks in NLP (Tsuruoka et al., 2011).
The training procedure, sketched in Algo-
rithm 1, consists in performing inference on each
input sentence and correcting the weight vector
each time a wrong decision is made. Impor-
tantly (Ross and Bagnell, 2010), the history used
during training has to be made of the previous pre-
dicted labels so that the training samples reflect the
fact that the history will be imperfectly known at
test time.
This reduction of sequence labeling to multi-
class classification allows us to learn a sequence
model in an ambiguous setting by building on the
theoretical results of Bordes et al. (2010) and Cour
et al. (2011). The decision about the correctness of
a prediction and the weight updates can be adapted
to the amount of supervision information that is
available.
Full Supervision In a fully supervised setting,
the correct label is known for each word token: a
decision is thus considered wrong when this gold
label is not predicted. In this case, a standard per-
ceptron update is performed:
w
t+1
? w
t
?? (x, i, y
?
i
, h
i
)+? (x, i, y?
i
, h
i
) (2)
where y
?
i
and y?
i
are the predicted and the gold la-
bel, respectively. This update is a stochastic gra-
dient step that increases the score of the gold label
while decreasing the score of the predicted label.
Ambiguous Supervision During training, each
observation i is now associated with a set of possi-
ble labels, denoted by
?
Y
i
. In this case, a decision is
considered wrong when the predicted label is not
in
?
Y
i
and the weight vector is updated as follows:
w
t+1
? w
t
?? (x, i, y
?
i
, h
i
)+
?
y?
i
?
?
Y
i
? (x, i, y?
i
, h
i
)
(3)
Compared to (2), this rule uniformly increases the
scores of all the labels in
?
Y
i
.
It can be shown (Bordes et al., 2010; Cour et
al., 2011), under mild assumptions (namely that
two labels never systematically co-occur in the
supervision information), that the update rule (3)
enables to learn a classifier in an ambiguous set-
ting, as if the gold labels were known. Intuitively,
as long as two labels are not systematically co-
occurring in
?
Y , updates will reinforce the correct
labels more often than the spurious ones; at the
end of training, the highest scoring label should
therefore be the correct one.
Algorithm 1 Training algorithm. In the ambigu-
ous setting,
?
Y
i
contains all possible labels; in the
supervised setting, it only contains the gold label.
w
0
? 0
for t ? J1, T K do
Randomly pick example x,
?
y
h? empty list
for i ? J1, nK do
y
?
i
= argmax
y?Y
?w
t
|?(x, i, y, h
i
)?
if y
?
i
/?
?
Y
i
then
w
t+1
? update(w
t
,x, i,
?
Y
i
, y
?
i
, h
i
)
end if
push(y
?
i
, h)
end for
end for
return
1
T
?
T
t=1
w
t
4 Empirical Study
Datasets Our approach is evaluated on 10 lan-
guages that present very different characteristics
and cover several language families.
4
In all our ex-
periments we use English as the source language.
Parallel sentences
5
are aligned with the standard
4
Resources considered in the related works are not freely
available, which prevents us from presenting a more complete
comparison.
5
All resources and features used in our experiments are
thoroughly documented in the supplementary material.
1781
ar cs de el es fi fr id it sv
HBAL 27.9 10.4 8.8 8.1 8.2 13.3 10.2 11.3 9.1 10.1
Partially observed CRF 33.9 11.6 12.2 10.9 10.7 12.9 11.6 16.3 10.4 11.6
HBSL ? 1.5 5.0 ? 2.4 5.9 3.5 4.8 2.8 3.8
HBAL + matched POS 24.1 7.6 8.0 7.3 7.4 12.2 7.4 9.8 8.3 8.8
(Ganchev and Das, 2013) 49.9 19.3 9.6 9.4 12.8 ? 12.5 ? 10.1 10.8
(T?ckstr?m et al., 2013) ? 18.9 9.5 10.5 10.9 ? 11.6 ? 10.2 11.1
(Li et al., 2012) ? ? 14.2 20.8 13.6 ? ? ? 13.5 13.9
Table 2: Error rate (in %) achieved by the method described in Sec. 3 trained in an ambiguous (HBAL)
or in a supervised setting (HBSL), a partially observed CRF and different state-of-the-art results.
MOSES pipeline, using the intersection heuristic
that only retains the most reliable alignment links.
The English side of the bitext is tagged using a
standard linear CRF trained on the Penn Treebank.
Tags are then transferred to the target language us-
ing the procedure described in Section 2. For each
language, we train a tagger using the method de-
scribed in Section 3 with T = 100 000 iterations
6
using a feature set similar to the one of Li et al.
(2012) and T?ckstr?m et al. (2013). The baseline
system is our reimplementation of the partially
observed CRF model of T?ckstr?m et al. (2013).
Evaluation is carried out on the test sets of tree-
banks for which manual gold tags are known. For
Czech and Greek, we use the CoNLL?07 Shared
Task on Dependency Parsing; for Arabic, the Ara-
bic Treebank; and otherwise the data of the Uni-
versal Dependency Treebank Project (McDonald
et al., 2013). Tagging performance is evaluated
with the standard error rate.
4.1 Results
Table 2 summarizes the performance achieved
by our method trained in the ambiguous setting
(HBAL) and by our re-implementation of the
partially supervised CRF baseline. As an upper
bound, we also report the score of our method
when trained in a supervised (HBSL) settings
considering the training part of the various tree-
banks, when it is available.
7
For the sake of com-
parison, we also list the best scores of previous
studies. Note, however, that a direct comparison
with these results is not completely fair as these
6
Preliminary experiments showed that increasing the
number of iterations T in Algorithm 1 has no significant
impact.
7
In this setting, HBSL implements an averaged percep-
tron, and achieves results that are similar to those obtained
with standard linear CRF.
systems were not trained and evaluated with the
same exact resources (corpora,
8
type constraints,
alignments, etc). Also note that the state-of-the-
art scores have been achieved by different models,
which have been selected based on their scores on
the test set and not on a validation set.
9
Experimental results show that HBAL signif-
icantly outperforms, on all considered languages
but one, the partially observed CRF that was
trained and tested in the same setting.
4.2 Discussion
The performance of our new method still falls
short of the performance of a fully supervised POS
tagger: for instance, in Spanish, full supervision
reduces the error rate by a factor of 4. A fine-
grained error analysis shows that many errors of
HBAL directly result from the fact that, contrary
to the fully supervised learner HBSL, our am-
biguous setting suffers from a train/test mismatch,
which has two main consequences. First, the train
and test sets do not follow exactly the same nor-
malization and tokenization conventions, which is
an obvious source of mistakes. Second, and more
importantly, many errors are caused by systematic
differences between the test tags and the super-
vised tags (i.e. the English side of the bitext and
Wiktionary). While some of these differences
are linguistically well-justified and reflect funda-
mental differences in the language structure and
usage, others seem to be merely due to arbitrary
annotation conventions.
For instance, in Greek, proper names are labeled
8
The test sets are only the same for Czech, Greek and
Swedish.
9
The partially observed CRF is the best model in (T?ck-
str?m et al., 2013) only for German (de), Greek (el) and
Swedish (sv), and uses only type constraints extracted from
Wiktionary.
1782
either as X (when they refer to a foreigner and are
not transliterated) or as NOUN (in all other cases),
while they are always labeled as NOUN in English.
In French and in Greek, contractions of a prepo-
sition and a determiner such as ????? (??? ???,
meaning ?to the?) or ?aux? (?? les? also meaning
?to the?) are labeled as ADP in the Universal De-
pendency Treebank but as DET in Wiktionary
and are usually aligned with a determiner in the
parallel corpora. In the Penn Treebank, quanti-
fiers like ?few? or ?little? are generally used in con-
junction with a determiner (?a few years?, ?a little
parable?, ...) and labeled as ADJ; the correspond-
ing Spanish constructions lack an article (?mucho
tempio?, ?pocos a?os?, ...) and the quantifiers are
therefore labeled as DET. Capturing such subtle
differences is hardly possible without prior knowl-
edge and specifically tailored features.
This annotation mismatch problem is all the
more important in settings like ours, that rely
on several, independently designed, information
sources, which follow contradictory annotation
conventions and for which the mapping to the uni-
versal tagset is actually error-prone (Zhang et al.,
2012). To illustrate this point, we ran three ad-
ditional experiments to assess the impact of the
train/test mismatch.
We first designed a control experiment in which
the type constraints were manually completed
with the gold labels of the most frequent errors of
HBAL. These errors generally concern function
words and can be assumed to result from system-
atic differences in the annotations rather than pre-
diction errors. For instance, for French the type
constraints for ?du?, ?des?, ?au? and ?aux? were cor-
rected from DET to ADP. The resulting model,
denoted ?HBAL + matched POS? in Table 2, sig-
nificantly outperforms HBAL, stressing the diver-
gence in the different annotation conventions.
Additionally, in order to approximate the am-
biguous setting train/test mismatch, we learn two
fully supervised Spanish taggers on the same train-
ing data as HBAL, using two different strategies
to obtain labeled data. We first use HBSL (which
was trained on the treebank) to automatically la-
bel the target side of the parallel corpus. In this
setting, the POS tagger is trained with data from
a different domain, but labeled with the same an-
notation scheme as a the test set. Learning with
this fully supervised data yields an error rate of
4.2% for Spanish, almost twice as much as HBSL,
bringing into light the impact of domain shift. We
then use a generic tagger, FREELING,
10
to label
the training data, this time with possible addi-
tional inconsistent annotations. The correspond-
ing error rate for Spanish was 6.1%, to be com-
pared with the 8.2% achieved by HBAL. The last
two control experiments show that many of the re-
maining labeling errors seem to be due to domain
and convention mismatches rather to the trans-
fer/ambiguous setting, as supervised models also
suffer from very similar conditions.
These observations show that the evaluation of
transfer-based methods suffer from several biases.
Their results must therefore be interpreted with
great care.
5 Conclusion
In this paper, we have presented a novel learning
methodology to learn from ambiguous supervision
information, and used it to train several POS tag-
gers. Using this method, we have been able to
achieve performance that surpasses the best re-
ported results, sometimes by a wide margin. Fur-
ther work will attempt to better analyse these re-
sults, which could be caused by several subtle
differences between HBAL and the baseline sys-
tem. Nonetheless, these experiments confirm that
cross-lingual projection of annotations have the
potential to help in building very efficient POS
taggers with very little monolingual supervision
data. Our analysis of these results also suggests
that, for this task, additional gains might be more
easily obtained by fixing systematic biases intro-
duced by conflicting mappings between tags or
by train/test domain mismatch than by designing
more sophisticated weakly supervised learners.
Acknowledgments
We wish to thank Thomas Lavergne and Alexan-
dre Allauzen for early feedback and for providing
us with the partially observed CRF implementa-
tion. We also thank the anonymous reviewers for
their helpful comments.
References
Ezra Black, Fred Jelinek, John Lafferty, David M.
Magerman, Robert Mercer, and Salim Roukos.
1992. Towards history-based grammars: Using
10
http://nlp.lsi.upc.edu/freeling/
1783
richer models for probabilistic parsing. In Proceed-
ings of the Workshop on Speech and Natural Lan-
guage, HLT ?91, pages 134?139, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Antoine Bordes, Nicolas Usunier, and Jason Weston.
2010. Label ranking under ambiguous supervision
for learning semantic correspondences. In ICML,
pages 103?110.
J?rgen Broschart. 2009. Why Tongan does it differ-
ently: Categorial distinctions in a language without
nouns and verbs. Linguistic Typology, 1:123?166,
10.
Timothee Cour, Ben Sapp, and Ben Taskar. 2011.
Learning from partial labels. Journal of Machine
Learning Research, 12:1501?1536, July.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Volume
1, HLT ?11, pages 600?609, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Hal Daum?, III and Daniel Marcu. 2005. Learning
as search optimization: Approximate large margin
methods for structured prediction. In Proceedings
of the 22Nd International Conference on Machine
Learning, ICML ?05, pages 169?176, New York,
NY, USA. ACM.
Nicholas Evans and Stephen C. Levinson. 2009. The
myth of language universals: Language diversity
and its importance for cognitive science. Behavioral
and Brain Sciences, 32:429?448, 10.
Kuzman Ganchev and Dipanjan Das. 2013. Cross-
lingual discriminative learning of sequence models
with posterior regularization. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1996?2006, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction
via bitext projection constraints. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP:
Volume 1 - Volume 1, ACL ?09, pages 369?377,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Nat. Lang. Eng., 11(3):311?325, September.
Sungchul Kim, Kristina Toutanova, and Hwanjo Yu.
2012. Multilingual named entity recognition using
parallel data and metadata from wikipedia. In Pro-
ceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics: Long Papers
- Volume 1, ACL ?12, pages 694?702, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Mikhail Kozhevnikov and Ivan Titov. 2013. Cross-
lingual transfer of semantic role labeling models.
In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 1190?1200, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Shen Li, Jo?o V. Gra?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 1389?1398, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T?ckstr?m, Claudia Bedini, N?ria
Bertomeu Castell?, and Jungmee Lee. 2013. Uni-
versal dependency annotation for multilingual pars-
ing. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 92?97, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Sebastian Pad? and Mirella Lapata. 2009. Cross-
lingual annotation projection of semantic roles. J.
Artif. Int. Res., 36(1):307?340, September.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Nicoletta Cal-
zolari (Conference Chair), Khalid Choukri, Thierry
Declerck, Mehmet U?gur Do?gan, Bente Maegaard,
Joseph Mariani, Jan Odijk, and Stelios Piperidis, ed-
itors, Proceedings of the Eight International Con-
ference on Language Resources and Evaluation
(LREC?12), Istanbul, Turkey, may. European Lan-
guage Resources Association (ELRA).
St?phane Ross and Drew Bagnell. 2010. Efficient re-
ductions for imitation learning. In AISTATS, pages
661?668.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi
Kazama. 2011. Learning with lookahead: Can
history-based models rival globally optimized mod-
els? In Proceedings of the Fifteenth Conference on
Computational Natural Language Learning, pages
238?246, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Oscar T?ckstr?m, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the Association for Computa-
tional Linguistics, 1:1?12.
1784
Lonneke van der Plas, Marianna Apidianaki, and Chen-
hua Chen. 2014. Global methods for cross-lingual
semantic role and predicate labelling. In Proceed-
ings of COLING 2014, the 25th International Con-
ference on Computational Linguistics: Technical
Papers, pages 1279?1290, Dublin, Ireland, August.
Dublin City University and Association for Compu-
tational Linguistics.
Mengqiu Wang and Christopher D. Manning. 2014.
Cross-lingual projected expectation regularization
for weakly supervised learning. Transactions of the
ACL, 2:55?66, February.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analy-
sis tools via robust projection across aligned cor-
pora. In Proceedings of the First International Con-
ference on Human Language Technology Research,
HLT ?01, pages 1?8, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Yuan Zhang, Roi Reichart, Regina Barzilay, and Amir
Globerson. 2012. Learning to map into a univer-
sal pos tagset. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, EMNLP-CoNLL ?12, pages 1368?
1378, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
1785
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84?89,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The KIT-LIMSI Translation System for WMT 2014
?
Quoc Khanh Do,
?
Teresa Herrmann,
??
Jan Niehues,
?
Alexandre Allauzen,
?
Franc?ois Yvon and
?
Alex Waibel
?
LIMSI-CNRS, Orsay, France
?
Karlsruhe Institute of Technology, Karlsruhe, Germany
?
surname@limsi.fr
?
firstname.surname@kit.edu
Abstract
This paper describes the joined submis-
sion of LIMSI and KIT to the Shared
Translation Task for the German-to-
English direction. The system consists
of a phrase-based translation system us-
ing a pre-reordering approach. The base-
line system already includes several mod-
els like conventional language models on
different word factors and a discriminative
word lexicon. This system is used to gen-
erate a k-best list. In a second step, the
list is reranked using SOUL language and
translation models (Le et al., 2011).
Originally, SOUL translation models were
applied to n-gram-based translation sys-
tems that use tuples as translation units
instead of phrase pairs. In this article,
we describe their integration into the KIT
phrase-based system. Experimental re-
sults show that their use can yield sig-
nificant improvements in terms of BLEU
score.
1 Introduction
This paper describes the KIT-LIMSI system for
the Shared Task of the ACL 2014 Ninth Work-
shop on Statistical Machine Translation. The sys-
tem participates in the German-to-English trans-
lation task. It consists of two main components.
First, a k-best list is generated using a phrase-
based machine translation system. This system
will be described in Section 2. Afterwards, the k-
best list is reranked using SOUL (Structured OUt-
put Layer) models. Thereby, a neural network lan-
guage model (Le et al., 2011), as well as several
translation models (Le et al., 2012a) are used. A
detailed description of these models can be found
in Section 3. While the translation system uses
phrase pairs, the SOUL translation model uses tu-
ples as described in the n-gram approach (Mari?no
et al., 2006). We describe the integration of the
SOUL models into the translation system in Sec-
tion 3.2. Section 4 summarizes the experimen-
tal results and compares two different tuning al-
gorithms: Minimum Error Rate Training (Och,
2003) and k-best Batch Margin Infused Relaxed
Algorithm (Cherry and Foster, 2012).
2 Baseline system
The KIT translation system is an in-house imple-
mentation of the phrase-based approach and in-
cludes a pre-ordering step. This system is fully
described in Vogel (2003).
To train translation models, the provided Eu-
roparl, NC and Common Crawl parallel corpora
are used. The target side of those parallel corpora,
the News Shuffle corpus and the GigaWord cor-
pus are used as monolingual training data for the
different language models. Optimization is done
with Minimum Error Rate Training as described
in Venugopal et al. (2005), using newstest2012
and newstest2013 as development and test data,
respectively.
Compound splitting (Koehn and Knight, 2003)
is performed on the source side (German) of the
corpus before training. Since the web-crawled
Common Crawl corpus is noisy, this corpus is
first filtered using an SVM classifier as described
in Mediani et al. (2011).
The word alignment is generated using the
GIZA++ Toolkit (Och and Ney, 2003). Phrase
extraction and scoring is done using the Moses
toolkit (Koehn et al., 2007). Phrase pair proba-
bilities are computed using modified Kneser-Ney
smoothing (Foster et al., 2006).
We apply short-range reorderings (Rottmann
and Vogel, 2007) and long-range reorder-
ings (Niehues and Kolss, 2009) based on part-of-
speech tags. The POS tags are generated using
the TreeTagger (Schmid, 1994). Rewriting rules
84
based on POS sequences are learnt automatically
to perform source sentence reordering according
to the target language word order. The long-range
reordering rules are further applied to the training
corpus to create reordering lattices to extract the
phrases for the translation model. In addition,
a tree-based reordering model (Herrmann et al.,
2013) trained on syntactic parse trees (Rafferty
and Manning, 2008; Klein and Manning, 2003)
is applied to the source sentence. In addition
to these pre-reordering models, a lexicalized
reordering model (Koehn et al., 2005) is applied
during decoding.
Language models are trained with the SRILM
toolkit (Stolcke, 2002) using modified Kneser-Ney
smoothing (Chen and Goodman, 1996). The sys-
tem uses a 4-gram word-based language model
trained on all monolingual data and an additional
language model trained on automatically selected
data (Moore and Lewis, 2010). The system fur-
ther applies a language model based on 1000 auto-
matically learned word classes using the MKCLS
algorithm (Och, 1999). In addition, a bilingual
language model (Niehues et al., 2011) is used as
well as a discriminative word lexicon (DWL) us-
ing source context to guide the word choices in the
target sentence.
3 SOUL models for statistical machine
translation
Neural networks, working on top of conventional
n-gram back-off language models (BOLMs), have
been introduced in (Bengio et al., 2003; Schwenk,
2007) as a potential means to improve discrete
language models. The SOUL model (Le et al.,
2011) is a specific neural network architecture that
allows us to estimate n-gram models using large
vocabularies, thereby making the training of large
neural network models feasible both for target lan-
guage models and translation models (Le et al.,
2012a).
3.1 SOUL translation models
While the integration of SOUL target language
models is straightforward, SOUL translation mod-
els rely on a specific decomposition of the joint
probability P (s, t) of a sentence pair, where s is a
sequence of I reordered source words (s
1
, ..., s
I
)
1
1
In the context of the n-gram translation model, (s, t) thus
denotes an aligned sentence pair, where the source words are
reordered.
and t contains J target words (t
1
, ..., t
J
). In the
n-gram approach (Mari?no et al., 2006; Crego et
al., 2011), this segmentation is a by-product of
source reordering, and ultimately derives from ini-
tial word and phrase alignments. In this frame-
work, the basic translation units are tuples, which
are analogous to phrase pairs, and represent a
matching u = (s, t) between a source phrase s
and a target phrase t.
Using the n-gram assumption, the joint proba-
bility of a segmented sentence pair using L tupels
decomposes as:
P (s, t) =
L
?
i=1
P (u
i
|u
i?1
, ..., u
i?n+1
) (1)
A first issue with this decomposition is that the
elementary units are bilingual pairs. Therefore,
the underlying vocabulary and hence the number
of parameters can be quite large, even for small
translation tasks. Due to data sparsity issues, such
models are bound to face severe estimation prob-
lems. Another problem with Equation (1) is that
the source and target sides play symmetric roles,
whereas the source side is known, and the tar-
get side must be predicted. To overcome some
of these issues, the n-gram probability in Equa-
tion (1) can be factored by first decomposing tu-
ples in two (source and target) parts, and then de-
composing the source and target parts at the word
level.
Let s
k
i
denote the k
th
word of source part of the
tuple s
i
. Let us consider the example of Figure 1,
s
1
11
corresponds to the source word nobel, s
4
11
to
the source word paix, and similarly t
2
11
is the tar-
get word peace. We finally define h
n?1
(t
k
i
) as the
sequence of the n?1 words preceding t
k
i
in the tar-
get sentence, and h
n?1
(s
k
i
) as the n?1 words pre-
ceding s
k
i
in the reordered source sentence: in Fig-
ure 1, h
3
(t
2
11
) thus refers to the three word context
receive the nobel associated with the target word
peace. Using these notations, Equation 1 can be
rewritten as:
P (s, t) =
L
?
i=1
[
|t
i
|
?
k=1
P
(
t
k
i
|h
n?1
(t
k
i
), h
n?1
(s
1
i+1
)
)
?
|s
i
|
?
k=1
P
(
s
k
i
|h
n?1
(t
1
i
), h
n?1
(s
k
i
)
)
]
(2)
This decomposition relies on the n-gram assump-
tion, this time at the word level. Therefore, this
85
 s?
8
: ? 
 t
?
8
: to 
 s?
9
: recevoir 
 t
?
9
: receive 
 s?
10
: le 
 t
?
10
: the 
 s?
11
: nobel de la paix 
 t
?
11
: nobel peace 
 s?
12
: prix 
 t
?
12
: prize 
 u
8
  u
9
  u
10
  u
11
  u
12
 
s :   .... 
t :   .... 
? recevoir le prix nobel de la paixorg :   ....
....
....
Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org)
French sentence appears at the top of the figure, just above the reordered source s and the target t. The
pair (s, t) decomposes into a sequence of L bilingual units (tuples) u
1
, ..., u
L
. Each tuple u
i
contains a
source and a target phrase: s
i
and t
i
.
model estimates the joint probability of a sentence
pair using two sliding windows of length n, one
for each language; however, the moves of these
windows remain synchronized by the tuple seg-
mentation. Moreover, the context is not limited
to the current phrase, and continues to include
words in adjacent phrases. Equation (2) involves
two terms that will be further denoted as TrgSrc
and Src, respectively P
(
t
k
i
|h
n?1
(t
k
i
), h
n?1
(s
1
i+1
)
)
and P
(
s
k
i
|h
n?1
(t
1
i
), h
n?1
(s
k
i
)
)
. It is worth notic-
ing that the joint probability of a sentence pair
can also be decomposed by considering the fol-
lowing two terms: P
(
s
k
i
|h
n?1
(s
k
i
), h
n?1
(t
1
i+1
)
)
and P
(
t
k
i
|h
n?1
(s
1
i
), h
n?1
(t
k
i
)
)
. These two terms
will be further denoted by SrcTrg and Trg. There-
fore, adding SOUL translation models means that
4 scores are added to the phrase-based systems.
3.2 Integration
During the training step, the SOUL translation
models are trained as described in (Le et al.,
2012a). The main changes concern the inference
step. Given the computational cost of computing
n-gram probabilities with neural network models,
a solution is to resort to a two-pass approach: the
first pass uses a conventional system to produce
a k-best list (the k most likely hypotheses); in
the second pass, probabilities are computed by the
SOUL models for each hypothesis and added as
new features. Then the k-best list is reordered ac-
cording to a combination of all features including
these new features. In the following experiments,
we use 10-gram SOUL models to rescore 300-
best lists. Since the phrase-based system described
in Section 2 uses source reordering, the decoder
was modified in order to generate k-best lists that
contain necessary word alignment information be-
tween the reordered source sentence and its asso-
ciated target hypothesis. The goal is to recover
the information that is illustrated in Figure 1 and
to apply the n-gram decomposition of a sentence
pair.
These (target and bilingual) neural network
models produce scores for each hypothesis in the
k-best list; these new features, along with the fea-
tures from the baseline system, are then provided
to a new phase which runs the traditional Mini-
mum Error Rate Training (MERT ) (Och, 2003), or
a recently proposed k-best Batch Margin Infused
Relaxed Algorithm (KBMIRA ) (Cherry and Fos-
ter, 2012) for tuning purpose. The SOUL mod-
els used for this year?s evaluation are similar to
those described in Allauzen et al. (2013) and Le
et al. (2012b). However, since compared to these
evaluations less parallel data is available for the
German-to-English task, we use smaller vocabu-
laries of about 100K words.
4 Results
We evaluated the SOUL models on the German-
to-English translation task using two systems to
generate the k-best lists. The first system used
all models of the baseline system except the DWL
model and the other one used all models.
Table 1 summarizes experimental results in
terms of BLEU scores when the tuning is per-
formed using KBMIRA. As described in Section
3, the probability of a phrase pair can be decom-
posed into products of words? probabilities in 2
different ways: we can first estimate the probabil-
ity of words in the source phrase given the context,
and then the probability of the target phrase given
its associated source phrase and context words
(see Equation (2)); or inversely we can generate
the target side before the source side. The for-
mer proceeds by adding Src and TrgSrc scores as
86
No DWL DWL
Soul models Dev Test Dev Test
No 26.02 27.02 26.27 27.46
Target 26.30 27.42 26.43 27.85
Translation st 26.46 27.70 26.66 28.04
Translation ts 26.48 27.41 26.61 28.00
All Translation 26.50 27.86 26.70 28.08
All SOUL models 26.62 27.84 26.75 28.10
Table 1: Results using KBMIRA
No DWL DWL
Soul models Dev Test Dev Test
No 26.02 27.02 26.27 27.46
Target 26.18 27.09 26.44 27.54
Translation st 26.36 27.59 26.66 27.80
Translation ts 26.44 27.69 26.63 27.94
All Translation 26.53 27.65 26.69 27.99
All SOUL models 26.47 27.68 26.66 28.01
Table 2: Results using MERT. Results in bold correpond to the submitted system.
2 new features into the k-best list, and the latter by
adding Trg and SrcTrg scores. These 2 methods
correspond respectively to the Translation ts and
Translation st lines in the Table 1. The 4 trans-
lation models may also be added simultaneously
(All Translations). The first line gives baseline
results without SOUL models, while the Target
line shows results in adding only SOUL language
model. The last line (All SOUL models) shows
the results for adding all neural network models
into the baseline systems.
As evident in Table 1, using the SOUL trans-
lation models yields generally better results than
using the SOUL target language model, yielding
about 0.2 BLEU point differences on dev and test
sets. We can therefore assume that the SOUL
translation models provide richer information that,
to some extent, covers that contained in the neural
network language model. Indeed, these 4 trans-
lation models take into account not only lexi-
cal probabilities of translating target words given
source words (or in the inverse order), but also the
probabilities of generating words in the target side
(Trg model) as does a language model, with the
same context length over both source and target
sides. It is therefore not surprising that adding the
SOUL language model along with all translation
models (the last line in the table) does not give sig-
nificant improvement compared to the other con-
figurations. The different ways of using the SOUL
translation models perform very similarly.
Table 2 summarizes the results using MERT in-
stead of KBMIRA. We can observe that using KB-
MIRA results in 0.1 to 0.2 BLEU point improve-
ments compared to MERT. Moreover, this impact
becomes more important when more features are
considered (the last line when all 5 neural net-
work models are added into the baseline systems).
In short, the use of neural network models yields
up to 0.6 BLEU improvement on the DWL sys-
tem, and a 0.8 BLEU gain on the system without
DWL. Unfortunately, the experiments with KB-
MIRA were carried out after the the submission
date. Therefore the submitted system corresponds
to the last line of table 2 indicated in bold.
5 Conclusion
We presented a system with two main features: a
phrase-based translation system which uses pre-
reordering and the integration of SOUL target lan-
guage and translation models. Although the trans-
lation performance of the baseline system is al-
ready very competitive, the rescoring by SOUL
models improve the performance significantly. In
the rescoring step, we used a continuous language
model as well as four continuous translation mod-
87
els. When combining the different SOUL models,
the translation models are observed to be more im-
portant in increasing the translation performance
than the language model. Moreover, we observe a
slight benefit to use KBMIRA instead of the stan-
dard MERT tuning algorithm. It is worth noticing
that using KBMIRA improves the performance
but also reduces the variance of the final results.
As future work, the integration of the SOUL
translation models could be improved in differ-
ent ways. For SOUL translation models, there
is a mismatch between translation units used dur-
ing the training step and those used by the de-
coder. The former are derived using the n-gram-
based approach, while the latter use the conven-
tional phrase extraction heuristic. We assume that
reducing this mismatch could improve the overall
performance. This can be achieved for instance
using forced decoding to infer a segmentation of
the training data into translation units. Then the
SOUL translation models can be trained using
this segmentation. For the SOUL target language
model, in these experiments we only used the En-
glish part of the parallel data for training. Results
may be improved by including all the monolingual
data.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
?
287658 as well as the French Ar-
maments Procurement Agency (DGA) under the
RAPID Rapmat project.
References
Alexandre Allauzen, Nicolas P?echeux, Quoc Khanh
Do, Marco Dinarelli, Thomas Lavergne, Aur?elien
Max, Hai-Son Le, and Franc?ois Yvon. 2013.
Limsi@ wmt13. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 60?
67.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
S.F. Chen and J. Goodman. 1996. An empirical study
of smoothing techniques for language modeling. In
Proceedings of the 34th Annual Meeting on Associa-
tion for Computational Linguistics (ACL ?96), pages
310?318, Santa Cruz, California, USA.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427?436. Association for Computational Lin-
guistics.
Josep M. Crego, Franois Yvon, and Jos B. Mari?no.
2011. N-code: an open-source Bilingual N-gram
SMT Toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49?58.
George F. Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In EMNLP, pages 53?61.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA,
June. Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL
2003.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), Pittsburgh, PA,
USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of ACL 2007, Demonstration Ses-
sion, Prague, Czech Republic.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012a. Continuous space translation models with
neural networks. pages 39?48, Montr?eal, Canada,
June. Association for Computational Linguistics.
Hai-Son Le, Thomas Lavergne, Alexandre Al-
lauzen, Marianna Apidianaki, Li Gong, Aur?elien
Max, Artem Sokolov, Guillaume Wisniewski, and
Franc?ois Yvon. 2012b. Limsi@ wmt?12. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, pages 330?337. Association for
Computational Linguistics.
88
Jos?e B. Mari?no, Rafael E. Banchs, Josep M. Crego,
Adri`a de Gispert, Patrick Lambert, Jos?e A.R. Fonol-
losa, and Marta R. Costa-Juss`a. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation systems for IWSLT
2011. In Proceedings of the Eight Interna-
tional Workshop on Spoken Language Translation
(IWSLT).
R.C. Moore and W. Lewis. 2010. Intelligent selection
of language model training data. In Proceedings of
the ACL 2010 Conference Short Papers, pages 220?
224, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An Efficient Method for De-
termining Bilingual Word Classes. In EACL?99.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160?167. As-
sociation for Computational Linguistics.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation
(TMI), Sk?ovde, Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, United Kingdom.
Holger Schwenk. 2007. Continuous space lan-
guage models. Computer Speech and Language,
21(3):492?518, July.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In International Confer-
ence on Spoken Language Processing, Denver, Col-
orado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluating Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, Michigan, USA.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In International Conference on Natural
Language Processing and Knowledge Engineering,
Beijing, China.
89
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 246?253,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
LIMSI @ WMT?14 Medical Translation Task
Nicolas P
?
echeux
1,2
, Li Gong
1,2
, Quoc Khanh Do
1,2
, Benjamin Marie
2,3
,
Yulia Ivanishcheva
2,4
, Alexandre Allauzen
1,2
, Thomas Lavergne
1,2
,
Jan Niehues
2
, Aur
?
elien Max
1,2
, Franc?ois Yvon
2
Univ. Paris-Sud
1
, LIMSI-CNRS
2
B.P. 133, 91403 Orsay, France
Lingua et Machina
3
, Centre Cochrane franc?ais
4
{firstname.lastname}@limsi.fr
Abstract
This paper describes LIMSI?s submission
to the first medical translation task at
WMT?14. We report results for English-
French on the subtask of sentence trans-
lation from summaries of medical ar-
ticles. Our main submission uses a
combination of NCODE (n-gram-based)
and MOSES (phrase-based) output and
continuous-space language models used in
a post-processing step for each system.
Other characteristics of our submission in-
clude: the use of sampling for building
MOSES? phrase table; the implementation
of the vector space model proposed by
Chen et al. (2013); adaptation of the POS-
tagger used by NCODE to the medical do-
main; and a report of error analysis based
on the typology of Vilar et al. (2006).
1 Introduction
This paper describes LIMSI?s submission to the
first medical translation task at WMT?14. This
task is characterized by high-quality input text
and the availability of large amounts of training
data from the same domain, yielding unusually
high translation performance. This prompted us
to experiment with two systems exploring differ-
ent translation spaces, the n-gram-based NCODE
(?2.1) and an on-the-fly variant of the phrase-
based MOSES (?2.2), and to later combine their
output. Further attempts at improving translation
quality were made by resorting to continuous lan-
guage model rescoring (?2.4), vector space sub-
corpus adaptation (?2.3), and POS-tagging adap-
tation to the medical domain (?3.3). We also per-
formed a small-scale error analysis of the outputs
of some of our systems (?5).
2 System Overview
2.1 NCODE
NCODE implements the bilingual n-gram ap-
proach to SMT (Casacuberta and Vidal, 2004;
Mari?no et al., 2006; Crego and Mari?no, 2006) that
is closely related to the standard phrase-based ap-
proach (Zens et al., 2002). In this framework, the
translation is divided into two steps. To translate
a source sentence f into a target sentence e, the
source sentence is first reordered according to a
set of rewriting rules so as to reproduce the tar-
get word order. This generates a word lattice con-
taining the most promising source permutations,
which is then translated. Since the translation step
is monotonic, the peculiarity of this approach is to
rely on the n-gram assumption to decompose the
joint probability of a sentence pair in a sequence
of bilingual units called tuples.
The best translation is selected by maximizing
a linear combination of feature functions using the
following inference rule:
e
?
= argmax
e,a
K
?
k=1
?
k
f
k
(f , e,a) (1)
where K feature functions (f
k
) are weighted by
a set of coefficients (?
k
) and a denotes the set of
hidden variables corresponding to the reordering
and segmentation of the source sentence. Along
with the n-gram translation models and target n-
gram language models, 13 conventional features
are combined: 4 lexicon models similar to the ones
used in standard phrase-based systems; 6 lexical-
ized reordering models (Tillmann, 2004; Crego et
al., 2011) aimed at predicting the orientation of
the next translation unit; a ?weak? distance-based
distortion model; and finally a word-bonus model
and a tuple-bonus model which compensate for the
system preference for short translations. Features
are estimated during the training phase. Training
source sentences are first reordered so as to match
246
the target word order by unfolding the word align-
ments (Crego and Mari?no, 2006). Tuples are then
extracted in such a way that a unique segmenta-
tion of the bilingual corpus is achieved (Mari?no et
al., 2006) and n-gram translation models are then
estimated over the training corpus composed of tu-
ple sequences made of surface forms or POS tags.
Reordering rules are automatically learned during
the unfolding procedure and are built using part-
of-speech (POS), rather than surface word forms,
to increase their generalization power (Crego and
Mari?no, 2006).
2.2 On-the-fly System (OTF)
We develop an alternative approach implement-
ing an on-the-fly estimation of the parameter of
a standard phrase-based model as in (Le et al.,
2012b), also adding an inverse translation model.
Given an input source file, it is possible to compute
only those statistics which are required to trans-
late the phrases it contains. As in previous works
on on-the-fly model estimation for SMT (Callison-
Burch et al., 2005; Lopez, 2008), we first build
a suffix array for the source corpus. Only a lim-
ited number of translation examples, selected by
deterministic random sampling, are then used by
traversing the suffix array appropriately. A coher-
ent translation probability (Lopez, 2008) (which
also takes into account examples where translation
extraction failed) is then estimated. As we cannot
compute exactly an inverse translation probability
(because sampling is performed independently for
each source phrase), we resort to the following ap-
proximation:
p(
?
f |e?) = min
(
1.0,
p(e?|
?
f)? freq(
?
f)
freq(e?)
)
(2)
where the freq(?) is the number of occurrences of
the given phrase in the whole corpus, and the nu-
merator p(e?|
?
f)?freq(
?
f) represents the predicted
joint count of
?
f and e?. The other models in this
system are the same as in the default configuration
of MOSES.
2.3 Vector Space Model (VSM)
We used the vector space model (VSM) of Chen
et al. (2013) to perform domain adaptation. In
this approach, each phrase pair (
?
f, e?) present in
the phrase table is represented by a C-dimensional
vector of TF-IDF scores, one for each sub-corpus,
where C represents the number of sub-corpora
(see Table 1). Each component w
c
(
?
f, e?) is a stan-
dard TF-IDF weight of each phrase pair for the
c
th
sub-corpus. TF(
?
f, e?) is the raw joint count of
(
?
f, e?) in the sub-corpus; the IDF(
?
f, e?) is the in-
verse document frequency across all sub-corpora.
A similar C-dimensional representation of the
development set is computed as follows: we first
perform word alignment and phrase pairs extrac-
tion. For each extracted phrase pair, we compute
its TF-IDF vector and finally combine all vectors
to obtain the vector for the develompent set:
w
dev
c
=
J
?
j=0
K
?
k=0
count
dev
(
?
f
j
, e?
k
)w
c
(
?
f
j
, e?
k
) (3)
where J and K are the total numbers of source
and target phrases extracted from the development
data, respectively, and count
dev
(
?
f
j
, e?
k
) is the joint
count of phrase pairs (
?
f
j
, e?
k
) found in the devel-
opment set. The similarity score between each
phrase pair?s vector and the development set vec-
tor is added into the phrase table as a VSM fea-
ture. We also replace the joint count with the
marginal count of the source/target phrase to com-
pute an alternative average representation for the
development set, thus adding two VSM additional
features.
2.4 SOUL
Neural networks, working on top of conventional
n-gram back-off language models, have been in-
troduced in (Bengio et al., 2003; Schwenk et al.,
2006) as a potential means to improve discrete
language models. As for our submitted transla-
tion systems to WMT?12 and WMT?13 (Le et al.,
2012b; Allauzen et al., 2013), we take advantage
of the recent proposal of (Le et al., 2011). Using
a specific neural network architecture, the Struc-
tured OUtput Layer (SOUL), it becomes possible
to estimate n-gram models that use large vocab-
ulary, thereby making the training of large neural
network language models feasible both for target
language models and translation models (Le et al.,
2012a). Moreover, the peculiar parameterization
of continuous models allows us to consider longer
dependencies than the one used by conventional
n-gram models (e.g. n = 10 instead of n = 4).
Additionally, continuous models can also be
easily and efficiently adapted as in (Lavergne et
al., 2011). Starting from a previously trained
SOUL model, only a few more training epochs are
247
Corpus Sentences Tokens (en-fr) Description wrd-lm pos-lm
in-domain
COPPA 454 246 10-12M -3 -15
EMEA 324 189 6-7M 26 -1
PATTR-ABSTRACTS 634 616 20-24M 22 21
PATTR-CLAIMS 888 725 32-36M 6 2
PATTR-TITLES 385 829 3-4M 4 -17
UMLS 2 166 612 8-8M term dictionary -7 -22
WIKIPEDIA 8 421 17-18k short titles -5 -13
out-of-domain
NEWSCOMMENTARY 171 277 4-5M 6 16
EUROPARL 1 982 937 54-60M -7 -33
GIGA 9 625 480 260-319M 27 52
all parallel all 17M 397-475M concatenation 33 69
target-lm
medical-data -146M 69 -
wmt13-data -2 536M 49 -
devel/test
DEVEL 500 10-12k khresmoi-summary
LMTEST 3 000 61-69k see Section 3.4
NEWSTEST12 3 003 73-82k from WMT?12
TEST 1 000 21-26k khresmoi-summary
Table 1: Parallel corpora used in this work, along with the number of sentences and the number of English
and French tokens, respectively. Weights (?
k
) from our best NCODE configuration are indicated for each
sub-corpora?s bilingual word language model (wrd-lm) and POS factor language model (pos-lm).
needed on a new corpus in order to adapt the pa-
rameters to the new domain.
3 Data and Systems Preparation
3.1 Corpora
We use all the available (constrained) medical data
extracted using the scripts provided by the orga-
nizers. This resulted in 7 sub-corpora from the
medical domain with distinctive features. As out-
of-domain data, we reuse the data processed for
WMT?13 (Allauzen et al., 2013).
For pre-processing of medical data, we closely
followed (Allauzen et al., 2013) so as to be able to
directly integrate existing translation and language
models, using in-house text processing tools for
tokenization and detokenization steps (D?echelotte
et al., 2008). All systems are built using a
?true case? scheme, but sentences fully capital-
ized (plentiful especially in PATTR-TITLES) are
previously lowercased. Duplicate sentence pairs
are removed, yielding a sentence reduction up to
70% for EMEA. Table 1 summarizes the data used
along with some statistics after the cleaning and
pre-processing steps.
3.2 Language Models
A medical-domain 4-gram language model is built
by concatenating the target side of the paral-
lel data and all the available monolingual data
1
,
with modified Kneser-Ney smoothing (Kneser and
Ney, 1995; Chen and Goodman, 1996), using the
SRILM (Stolcke, 2002) and KENLM (Heafield,
2011) toolkits. Although more similar to term-to-
term dictionaries, UMLS and WIKIPEDIA proved
better to be included in the language model.
The large out-of-domain language model used for
WMT?13 (Allauzen et al., 2013) is additionaly
used (see Table 1).
3.3 Part-of-Speech Tagging
Medical data exhibit many peculiarities, includ-
ing different syntactic constructions and a specific
vocabulary. As standard POS-taggers are known
not to perform very well for this type of texts, we
use a specific model trained on the Penn Treebank
and on medical data from the MedPost project
(Smith et al., 2004). We use Wapiti (Lavergne
et al., 2010), a state-of-the-art CRF implementa-
tion, with a standard feature set. Adaptation is per-
formed as in (Chelba and Acero, 2004) using the
out-of-domain model as a prior when training the
in-domain model on medical data. On a medical
test set, this adaptation leads to a 8 point reduc-
tion of the error rate. A standard model is used for
WMT?13 data. For the French side, due to the lack
of annotaded data for the medical domain, corpora
are tagged using the TreeTagger (Schmid, 1994).
1
Attempting include one language model per sub-corpora
yielded a significant drop in performance.
248
3.4 Proxy Test Set
For this first edition of a Medical Translation Task,
only a very small development set was made avail-
able (DEVEL in Table 1). This made both system
design and tuning challenging. In fact, with such a
small development set, conventional tuning meth-
ods are known to be very unstable and prone to
overfitting, and it would be suboptimal to select
a configuration based on results on the develop-
ment set only.
2
To circumvent this, we artificially
created our own internal test set by randomly se-
lecting 3 000 sentences out from the 30 000 sen-
tences from PATTR-ABSTRACTS having the low-
est perplexity according to 3-gram language mod-
els trained on both sides of the DEVEL set. This
test set, denoted by LMTEST, is however highly
biaised, especially because of the high redundancy
in PATTR-ABSTRACTS, and should be used with
great care when tuning or comparing systems.
3.5 Systems
NCODE We use NCODE with default settings, 3-
gram bilingual translation models on words and 4-
gram bilingual translation factor models on POS,
for each included corpora (see Table 1) and for the
concatenation of them all.
OTF When using our OTF system, all in-
domain and out-of-domain data are concatenated,
respectively. For both corpora, we use a maxi-
mum random sampling size of 1 000 examples and
a maximum phrase length of 15. However, all
sub-corpora but GIGA
3
are used to compute the
vectors for VSM features. Decoding is done with
MOSES
4
(Koehn et al., 2007).
SOUL Given the computational cost of com-
puting n-gram probabilities with neural network
models, we resort to a reranking approach. In
the following experiments, we use 10-gram SOUL
models to rescore 1 000-best lists. SOUL models
provide five new features: a target language model
score and four translation scores (Le et al., 2012a).
We reused the SOUL models trained for our par-
ticipation to WMT?12 (Le et al., 2012b). More-
over, target language models are adapted by run-
ning 6 more epochs on the new medical data.
2
This issue is traditionally solved in Machine Learning by
folded cross-validation, an approach that would be too pro-
hibitive to use here.
3
The GIGA corpus is actually very varied in content.
4
http://www.statmt.org/moses/
System Combination As NCODE and OTF dif-
fer in many aspects and make different errors, we
use system combination techniques to take advan-
tage of their complementarity. This is done by
reranking the concatenation of the 1 000-best lists
of both systems. For each hypothesis within this
list, we use two global features, corresponding
either to the score computed by the correspond-
ing system or 0 otherwise. We then learn rerank-
ing weights using Minimum Error Rate Training
(MERT) (Och, 2003) on the development set for
this combined list, using only these two features
(SysComb-2). In an alternative configuration, we
use the two systems without the SOUL rescoring,
and add instead the five SOUL scores as features in
the system combination reranking (SysComb-7).
Evaluation Metrics All BLEU scores (Pap-
ineni et al., 2002) are computed using cased
multi-bleu with our internal tokenization. Re-
ported results correspond to the average and stan-
dard deviation across 3 optimization runs to bet-
ter account for the optimizer variance (Clark et al.,
2011).
4 Experiments
4.1 Tuning Optimization Method
MERT is usually used to optimize Equation 1.
However, with up to 42 features when using
SOUL, this method is known to become very sen-
sitive to local minima. Table 2 compares MERT,
a batch variant of the Margin Infused Relaxation
Algorithm (MIRA) (Cherry and Foster, 2012) and
PRO (Hopkins and May, 2011) when tuning an
NCODE system. MIRA slightly outperforms PRO
on DEVEL, but seems prone to overfitting. How-
ever this was not possible to detect before the re-
lease of the test set (TEST), and so we use MIRA
in all our experiments.
DEVEL TEST
MERT 47.0? 0.4 44.1? 0.8
MIRA 47.9? 0.0 44.8? 0.1
PRO 47.1? 0.1 45.1? 0.1
Table 2: Impact of the optimization method during
the tuning process on BLEU score, for a baseline
NCODE system.
249
4.2 Importance of the Data Sources
Table 3 shows that using the out-of-domain data
from WMT?13 yields better scores than only using
the provided medical data only. Moreover, com-
bining both data sources drastically boosts perfor-
mance. Table 1 displays the weights (?
k
) given by
NCODE to the different sub-corpora bilingual lan-
guage models. Three corpora seems particulary
useful: EMEA, PATTR-ABSTRACTS and GIGA.
Note that several models are given a negative
weight, but removing them from the model sur-
prisingly results in a drop of performance.
DEVEL TEST
medical 42.2? 0.1 39.6? 0.1
WMT?13 43.0? 0.1 41.0? 0.0
both 48.3? 0.1 45.4? 0.0
Table 3: BLEU scores obtained by NCODE trained
on medical data only, WMT?13 data only, or both.
4.3 Part-of-Speech Tagging
Using the specialized POS-tagging models for
medical data described in Section 3.3 instead of a
standart POS-tagger, a 0.5 BLEU points increase
is observed. Table 4 suggests that a better POS
tagging quality is mainly beneficial to the reorder-
ing mechanism in NCODE, in contrast with the
POS-POS factor models included as features.
Reordering Factor model DEVEL TEST
std std 47.9? 0.0 44.8? 0.1
std spec 47.9? 0.1 45.0? 0.1
spec std 48.4? 0.1 45.3? 0.1
spec spec 48.3? 0.1 45.4? 0.0
Table 4: BLEU results when using a standard POS
tagging (std) or our medical adapted specialized
method (spec), either for the reordering rule mech-
anism (Reordering) or for the POS-POS bilingual
language models features (Factor model).
4.4 Development and Proxy Test Sets
In Table 5, we assess the importance of domain
adaptation via tuning on the development set used
and investigate the benefits of our internal test set.
Best scores are obtained when using the pro-
vided development set in the tuning process. Us-
DEVEL LMTEST NEWSTEST12 TEST
48.3? 0.1 46.8? 0.1 26.2? 0.1 45.4? 0.0
41.8? 0.2 48.9? 0.1 18.5? 0.1 40.1? 0.1
39.8? 0.1 37.4? 0.2 29.0? 0.1 39.0? 0.3
Table 5: Influence of the choice of the develop-
ment set when using our baseline NCODE system.
Each row corresponds to the choice of a develop-
ment set used in the tuning process, indicated by a
surrounded BLEU score.
Table 6: Contrast of our two main systems and
their combination, when adding SOUL language
(LM) and translation (TM) models. Stars indicate
an adapted LM. BLEU results for the best run on
the development set are reported.
DEVEL TEST
NCODE 48.5 45.2
+ SOUL LM 49.4 45.7
+ SOUL LM
?
49.8 45.9
+ SOUL LM + TM 50.1 47.0
+ SOUL LM
?
+ TM 50.1 47.0
OTF 46.6 42.5
+ VSM 46.9 42.8
+ SOUL LM 48.6 44.0
+ SOUL LM
?
48.4 44.2
+ SOUL LM + TM 49.6 44.8
+ SOUL LM
?
+ TM 49.7 44.9
SysComb-2 50.5 46.6
SysComb-7 50.7 46.5
ing NEWSTEST12 as development set unsurpris-
ingly leads to poor results, as no domain adapta-
tion is carried out. However, using LMTEST does
not result in much better TEST score. We also note
a positive correlation between DEVEL and TEST.
From the first three columns, we decided to use the
DEVEL data set as development set for our sub-
mission, which is a posteriori the right choice.
4.5 NCODE vs. OTF
Table 6 contrasts our different approaches. Prelim-
inary experiments suggest that OTF is a compara-
ble but cheaper alternative to a full MOSES sys-
tem.
5
We find a large difference in performance,
5
A control experiment for a full MOSES system (using a
single phrase table) yielded a BLEU score of 45.9 on DEVEL
and 43.2 on TEST, and took 3 more days to complete.
250
extra missing incorrect unknown
word content filler disamb. form style term order word term all
syscomb 4 13 20 47 62 8 18 21 1 11 205
OTF+VSM+SOUL 4 4 31 44 82 6 20 42 3 12 248
Table 7: Results for manual error analysis following (Vilar et al., 2006) for the first 100 test sentences.
NCODE outperforming OTF by 2.8 BLEU points
on the TEST set. VSM does not yield any signifi-
cant improvement, contrarily to the work of Chen
et al. (2013); it may be the case all individual sub-
corpus are equally good (or bad) at approximating
the stylistic preferences of the TEST set.
4.6 Integrating SOUL
Table 6 shows the substantial impact of adding
SOUL models for both baseline systems. With
only the SOUL LM, improvements on the test set
range from 0.5 BLEU points for NCODE system
to 1.2 points for the OTF system. The adaptation
of SOUL LM with the medical data brings an ad-
ditional improvement of about 0.2 BLEU points.
Adding all SOUL translation models yield an
improvement of 1.8 BLEU points for NCODE and
of 2.4 BLEU points with the OTF system using
VSM models. However, the SOUL adaptation step
has then only a modest impact. In future work, we
plan to also adapt the translation models in order
to increase the benefit of using in-domain data.
4.7 System Combination
Table 6 shows that performing the system combi-
nation allows a gain up to 0.6 BLEU points on the
DEVEL set. However this gain does not transfer to
the TEST set, where instead a drop of 0.5 BLEU
is observed. The system combination using SOUL
scores showed the best result over all of our other
systems on the DEVEL set, so we chose this (a
posteriori sub-obtimal) configuration as our main
system submission.
Our system combination strategy chose for DE-
VEL about 50% hypotheses among those produced
by NCODE and 25% hypotheses from OTF, the
remainder been common to both systems. As ex-
pected, the system combination prefers hypothe-
ses coming from the best system. We can observe
nearly the same distribution for TEST.
5 Error Analysis
The high level of scores for automatic metrics
encouraged us to perform a detailed, small-scale
analysis of our system output, using the error types
proposed by Vilar et al. (2006). A single annota-
tor analyzed the output of our main submission, as
well as our OTF variant. Results are in Table 7.
Looking at the most important types of errors,
assuming the translation hypotheses were to be
used for rapid assimilation of the text content, we
find a moderate number of unknown terms and in-
correctly translated terms. The most frequent er-
ror types include missing fillers, incorrect disam-
biguation, form and order, which all have some
significant impact on automatic metrics. Compar-
ing more specifically the two systems used in this
small-scale study, we find that our combination
(which reused more than 70% of hypotheses from
NCODE) mostly improves over the OTF variant on
the choice of correct word form and word order.
We may attribute this in part to a more efficient
reordering strategy that better exploits POS tags.
6 Conclusion
In this paper, we have demonstrated a successful
approach that makes use of two flexible transla-
tion systems, an n-gram system and an on-the-fly
phrase-based model, in a new medical translation
task, through various approaches to perform do-
main adaptation. When combined with continu-
ous language models, which yield additional gains
of up to 2 BLEU points, moderate to high-quality
translations are obtained, as confirmed by a fine-
grained error analysis. The most challenging part
of the task was undoubtedly the lack on an internal
test to guide system development. Another inter-
esting negative result lies in the absence of success
for our configuration of the vector space model
of Chen et al. (2013) for adaptation. Lastly, a more
careful integration of medical terminology, as pro-
vided by the UMLS, proved necessary.
7 Acknowledgements
We would like to thank Guillaume Wisniewski and
the anonymous reviewers for their helpful com-
ments and suggestions.
251
References
Alexandre Allauzen, Nicolas P?echeux, Quoc Khanh
Do, Marco Dinarelli, Thomas Lavergne, Aur?elien
Max, Hai-son Le, and Franc?ois Yvon. 2013. LIMSI
@ WMT13. In Proceedings of the Workshkop on
Statistical Machine Translation, pages 62?69, Sofia,
Bulgaria.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3(6):1137?1155.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL, Ann Arbor, USA.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Ciprian Chelba and Alex Acero. 2004. Adaptation
of maximum entropy classifier: Little data can help
a lot. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Barcelona, Spain.
Stanley F. Chen and Joshua T. Goodman. 1996. An
empirical study of smoothing techniques for lan-
guage modeling. In Proceedings of the 34th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 310?318, Santa Cruz, NM.
Boxing Chen, Roland Kuhn, and George Foster. 2013.
Vector space model for adaptation in statistical ma-
chine translation. In Proceedings of ACL, Sofia,
Bulgaria.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427?436. Association for Computational Lin-
guistics.
Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A
Smith. 2011. Better Hypothesis Testing for Statisti-
cal Machine Translation : Controlling for Optimizer
Instability. In Better Hypothesis Testing for Statisti-
cal Machine Translation : Controlling for Optimizer
Instability, pages 176?181, Portland, Oregon.
Josep M. Crego and Jos?e B. Mari?no. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franc?ois Yvon, and Jos?e B. Mari?no.
2011. N-code: an open-source bilingual N-gram
SMT toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49?58.
Daniel D?echelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, H?el`ene May-
nard, and Franc?ois Yvon. 2008. LIMSI?s statisti-
cal translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, July. Associa-
tion for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?11, pages 1352?1362, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing, ICASSP?95,
pages 181?184, Detroit, MI.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513.
Association for Computational Linguistics, July.
Thomas Lavergne, Hai-Son Le, Alexandre Allauzen,
and Franc?ois Yvon. 2011. LIMSI?s experiments
in domain adaptation for IWSLT11. In Mei-Yuh
Hwang and Sebastian St?uker, editors, Proceedings
of the heigth International Workshop on Spoken
Language Translation (IWSLT), San Francisco, CA.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012a. Continuous space translation models with
neural networks. In Proceedings of the 2012 confer-
ence of the north american chapter of the associa-
tion for computational linguistics: Human language
technologies, pages 39?48, Montr?eal, Canada, June.
Association for Computational Linguistics.
Hai-Son Le, Thomas Lavergne, Alexandre Al-
lauzen, Marianna Apidianaki, Li Gong, Aur?elien
252
Max, Artem Sokolov, Guillaume Wisniewski, and
Franc?ois Yvon. 2012b. LIMSI @ WMT12. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 330?337, Montr?eal,
Canada.
Adam Lopez. 2008. Tera-Scale Translation Models
via Pattern Matching. In Proceedings of COLING,
Manchester, UK.
Jos?e B. Mari?no, Rafael E. Banchs, Josep M. Crego,
Adri`a de Gispert, Patrick Lambert, Jos?e A.R. Fonol-
losa, and Marta R. Costa-Juss`a. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
USA, July. Association for Computational Linguis-
tics.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, September.
Holger Schwenk, Daniel Dchelotte, and Jean-Luc Gau-
vain. 2006. Continuous space language models for
statistical machine translation. In Proceedings of the
COLING/ACL on Main conference poster sessions,
pages 723?730, Morristown, NJ, USA. Association
for Computational Linguistics.
L. Smith, T. Rindflesch, and W. J. Wilbur. 2004. Med-
post: a part of speech tagger for biomedical text.
Bioinformatics, 20(14):2320?2321.
A. Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), pages 901?904, Denver, Colorado,
September.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL, pages 101?104.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error Analysis of Statistical Ma-
chine Translation Output. In LREC, Genoa, Italy.
Richard Zens, Franz Joseph Och, and Herman Ney.
2002. Phrase-based statistical machine translation.
In M. Jarke, J. Koehler, and G. Lakemeyer, editors,
KI-2002: Advances in artificial intelligence, volume
2479 of LNAI, pages 18?32. Springer Verlag.
253
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 348?354,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
LIMSI Submission for WMT?14 QE Task
Guillaume Wisniewski and Nicolas P
?
echeux and Alexandre Allauzen and Franc?ois Yvon
Universit?e Paris Sud and LIMSI-CNRS
91 403 ORSAY CEDEX, France
{wisniews, pecheux, allauzen, yvon}@limsi.fr
Abstract
This paper describes LIMSI participation
to the WMT?14 Shared Task on Qual-
ity Estimation; we took part to the word-
level quality estimation task for English
to Spanish translations. Our system re-
lies on a random forest classifier, an en-
semble method that has been shown to
be very competitive for this kind of task,
when only a few dense and continuous fea-
tures are used. Notably, only 16 features
are used in our experiments. These fea-
tures describe, on the one hand, the qual-
ity of the association between the source
sentence and each target word and, on the
other hand, the fluency of the hypothe-
sis. Since the evaluation criterion is the
f
1
measure, a specific tuning strategy is
proposed to select the optimal values for
the hyper-parameters. Overall, our system
achieves a 0.67 f
1
score on a randomly ex-
tracted test set.
1 Introduction
This paper describes LIMSI submission to the
WMT?14 Shared Task on Quality Estimation. We
participated in the word-level quality estimation
task (Task 2) for the English to Spanish direction.
This task consists in predicting, for each word in
a translation hypothesis, whether this word should
be post-edited or should rather be kept unchanged.
Predicting translation quality at the word level
raises several interesting challenges. First, this is
a (relatively) new task and the best way to for-
mulate and evaluate it has still to be established.
Second, as most works on quality estimation have
only considered prediction at the sentence level, it
is not clear yet which features are really effective
to predict quality at the word and a set of base-
line features has still to be found. Finally, sev-
eral characteristic of the task (the limited number
of training examples, the unbalanced classes, etc.)
makes the use of ?traditional? machine learning al-
gorithms difficult. This papers describes how we
addressed this different issues for our participation
to the WMT?14 Shared Task.
The rest of this paper is organized as follows.
Section 2 gives an overview of the shared task data
that will justify some of the design decisions we
made. Section 3 describes the different features
we have considered and Section 4, the learning
methods used to estimate the classifiers parame-
ters. Finally the results of our models are pre-
sented and analyzed in Section 5.
2 World-Level Quality Estimation
WMT?14 shared task on quality estimation num-
ber 2 consists in predicting, for each word of a
translation hypothesis, whether this word should
be post-edited (denoted by the BAD label) or
should be kept unchanged (denoted by the OK la-
bel). The shared task organizers provide a bilin-
gual dataset from English to Spanish
1
made of
translations produced by three different MT sys-
tems and by one human translator; these transla-
tions have then been annotated with word-level la-
bels by professional translators. No additional in-
formation about the systems used, the derivation
of the translation (such as the lattices or the align-
ment between the source and the best translation
hypothesis) or the tokenization applied to identify
words is provided.
The distributions of the two labels for the dif-
ferent systems is displayed in Table 1. As it
could be expected, the class are, overall, unbal-
anced and the systems are of very different qual-
ity: the proportion of BAD and OK labels highly
depends on the system used to produce the transla-
tion hypotheses. However, as our preliminary ex-
periments have shown, the number of examples is
1
We did not consider the other language pairs.
348
too small to train a different confidence estimation
system for each system.
The distribution of the number of BAD labels
per sentence is very skewed: on average, one word
out of three (precisely 35.04%) in a sentence is la-
beled as BAD but the median of the distribution of
the ratio of word labeled BAD in a sentence is 20%
and its standard deviation is pretty high (34.75%).
Several sentences have all their words labeled as
either OK or BAD, which is quite surprising as the
sentences of the corpus for Task 2 have been se-
lected because there were ?near miss translations?
that is to say translations that should have con-
tained no more that 2 or 3 errors.
Another interesting finding is that the propor-
tion of word to post-edit is the same across the
different parts-of-speech (see Table 2).
2
Table 1: Number of examples and distribution of
labels for the different systems on the training set
System #sent. #words % OK % BAD
1 791 19,456 75.48 24.52
2 621 14,620 59.11 40.89
3 454 11,012 59.76 40.24
4 90 2,296 36.85 63.15
Total 1,956 47,384 64.90 35.10
Table 2: Distribution of labels according to the
POS on the training set
POS % in train % BAD
NOUN 23.81 35.02
ADP 15.06 35.48
DET 14.90 32.88
VERB 14.64 41.26
PUNCT 10.92 27.26
ADJ 6.61 35.68
CONJ 5.04 30.77
PRON 4.58 43.15
ADV 4.39 36.56
As the classes are unbalanced, prediction per-
formance will be evaluated in terms of precision,
recall and f
1
score computed on the BAD label.
More precisely, if the number of true positive (i.e.
2
We used FreeLing (http:nlp.lsi.upc.edu/
freeling/) to predict the POS tags of the translation
hypotheses and, for the sake of clarity, mapped the 71 tags
used by FreeLing to the 11 universal POS tags of Petrov et
al. (2012).
BAD word predicted as BAD), false positive (OK
word predicted as BAD) and false negative (BAD
word predicted as OK) are denoted tp
BAD
, fp
BAD
and fn
BAD
, respectively, the quality of a confidence
estimation system is evaluated by the three follow-
ing metrics:
p
BAD
=
tp
BAD
tp
BAD
+ fp
BAD
(1)
r
BAD
=
tp
BAD
tp
BAD
+ fn
BAD
(2)
f
1
=
2 ? p
BAD
? r
BAD
p
BAD
+ r
BAD
(3)
3 Features
In our experiments, we used 16 features to de-
scribe a given target word t
i
in a translation hy-
pothesis t = (t
j
)
m
j=1
. To avoid sparsity issues we
decided not to include any lexicalized information
such as the word or the previous word identities.
As the translation hypotheses were generated by
different MT systems, no white-box features (such
as word alignment or model scores) are consid-
ered. Our features can be organized in two broad
categories:
Association Features These features measure
the quality of the ?association? between the source
sentence and a target word: they characterize the
probability for a target word to appear in a transla-
tion of the source sentence. Two kinds of associa-
tion features can be distinguished.
The first one is derived from the lexicalized
probabilities p(t|s) that estimate the probability
that a source word s is translated by the target
word t
j
. These probabilities are aggregated using
an arithmetic mean:
p(t
j
|s) =
1
n
n
?
i=1
p(t
j
|s
i
) (4)
where s = (s
i
)
n
i=1
is the source sentence (with an
extra NULL token). We assume that p(t
j
|s
i
) = 0 if
the words t
j
and s
i
have never been aligned in the
train set and also consider the geometric mean of
the lexicalized probabilities, their maximum value
(i.e. max
s?s
p(t
j
|s)) as well as a binary feature
that fires when the target word t
j
is not in the lex-
icalized probabilities table.
The second kind of association features relies
on pseudo-references, that is to say, translations
of the source sentence produced by an indepen-
dent MT system. Many works have considered
349
pseudo-references to design new MT metrics (Al-
brecht and Hwa, 2007; Albrecht and Hwa, 2008)
or for confidence estimation (Soricut and Echi-
habi, 2010; Soricut and Narsale, 2012) but, to the
best of our knowledge, this is the first time that
they are used to predict confidence at the word
level.
Pseudo-references are used to define 3 binary
features which fire if the target word is in the
pseudo-reference, in a 2-gram shared between the
pseudo-reference and the translation hypothesis or
in a common 3-gram, respectively. The lattices
representing the search space considered to gen-
erate these pseudo-references also allow us to es-
timate the posterior probability of a target word
that quantifies the probability that it is part of the
system output (Gispert et al., 2013). Posteriors ag-
gregate two pieces of information for each word in
the final hypothesis: first, all the paths in the lat-
tice (i.e. the number of translation hypotheses in
the search space) where the word appears in are
considered; second, the decoder scores of these
paths are accumulated in order to derive a confi-
dence measure at the word level. In our experi-
ments, we considered pseudo-references and lat-
tices produced by the n-gram based system de-
veloped by our team for last year WMT evalu-
ation campaign (Allauzen et al., 2013), that has
achieved very good performance.
Fluency Features These features measure the
?fluency? of the target sentence and are based on
different language models: a ?traditional? 4-gram
language model estimated on WMT monolingual
and bilingual data (the language model used by
our system to generate the pseudo-references); a
continuous-space 10-gram language model esti-
mated with SOUL (Le et al., 2011) (also used by
our MT system) and a 4-gram language model
based on Part-of-Speech sequences. The latter
model was estimated on the Spanish side of the
bilingual data provided in the translation shared
task in 2013. These data were POS-tagged with
FreeLing (Padr?o and Stanilovsky, 2012).
All these language models have been used to de-
fine two different features :
? the probability of the word of interest p(t
j
|h)
where h = t
j?1
, ..., t
j?n+1
is the history
made of the n? 1 previous words or POS
? the ratio between the probability of
the sentence and the ?best? probabil-
ity that can be achieved if the target
word is replaced by any other word (i.e.
max
v?V
p(t
1
, ..., t
j?1
, v, t
j+1
, ..., t
m
) where
the max runs over all the words of the
vocabulary).
There is also a feature that describes the back-off
behavior of the conventional language model: its
value is the size of the largest n-gram of the trans-
lation hypothesis that can be estimated by the lan-
guage model without relying on back-off probabil-
ities.
Finally, there is a feature describing, for each
word that appears more than once in the train set,
the probability that this word is labeled BAD. This
probability is simply estimated by the ratio be-
tween the number of times this word is labeled
BAD and the number of occurrences of this word.
It must be noted that most of the features we
consider rely on models that are part of a ?clas-
sic? MT system. However their use for predicting
translation quality at the word-level is not straight-
forward, as they need to be applied to sentences
with a given unknown tokenization. Matching the
tokenization used to estimate the model to the one
used for collecting the annotations is a tedious and
error-prone process and some of the prediction er-
rors most probably result from mismatches in tok-
enization.
4 Learning Methods
4.1 Classifiers
Predicting whether a word in a translation hypoth-
esis should be post-edited or not can naturally be
framed as a binary classification task. Based on
our experiments in previous campaigns (Singh et
al., 2013; Zhuang et al., 2012), we considered ran-
dom forest in all our experiments.
3
Random forest (Breiman, 2001) is an ensem-
ble method that learns many classification trees
and predicts an aggregation of their result (for in-
stance by majority voting). In contrast with stan-
dard decision trees, in which each node is split
using the best split among all features, in a ran-
dom forest the split is chosen randomly. In spite
of this simple and counter-intuitive learning strat-
egy, random forests have proven to be very good
?out-of-the-box? learners. Random forests have
achieved very good performance in many similar
3
we have used the implementation provided by
scikit-learn (Pedregosa et al., 2011).
350
tasks (Chapelle and Chang, 2011), in which only
a few dense and continuous features are available,
possibly because of their ability to take into ac-
count complex interactions between features and
to automatically partition the continuous features
value into a discrete set of intervals that achieves
the best classification performance.
As a baseline, we consider logistic regres-
sion (Hastie et al., 2003), a simple linear model
where the parameters are estimated by maximiz-
ing the likelihood of the training set.
These two classifiers do not produce only a class
decision but yield an instance probability that rep-
resents the degree to which an instance is a mem-
ber of a class. As detailed in the next section,
thresholding this probability will allow us to di-
rectly optimize the f
1
score used to evaluate pre-
diction performance.
4.2 Optimizing the f
1
Score
As explained in Section 2, quality prediction will
be evaluated in terms of f
1
score. The learn-
ing methods we consider can not, as most learn-
ing method, directly optimize the f
1
measure dur-
ing training, since this metric does not decompose
over the examples. It is however possible to take
advantage of the fact that they actually estimate a
probability to find the largest f
1
score on the train-
ing set.
Indeed these probabilities are used with a
threshold (usually 0.5) to produce a discrete (bi-
nary) decision: if the probability is above the
threshold, the classifier produces a positive out-
put, and otherwise, a negative one. Each thresh-
old value produces a different trade-off between
true positives and false positives and consequently
between recall and precision: as the the threshold
becomes lower and lower, more and more exam-
ple are assigned to the positive class and recall in-
crease at the expense of precision.
Based on these observations, we propose the
following three-step method to optimize the f
1
score on the training set:
1. the classifier is first trained using the ?stan-
dard? learning procedure that optimizes either
the 0/1 loss (for random forest) or the likeli-
hood (for the logistic regression);
2. all the possible trade-offs between recall
and precision are enumerated by varying
the threshold; exploiting the monotonicity of
thresholded classifications,
4
this enumeration
can be efficiently done in O (n ? log n) and
results in at most n threshold values, where n
is the size of the training set (Fawcett, 2003);
3. all the f
1
scores achieved for the different
thresholds found in the previous step are eval-
uated; there are strong theoretical guaran-
tees that the optimal f
1
score that can be
achieved on the training set is one of these
values (Boyd and Vandenberghe, 2004).
Figure 1 shows how f
1
score varies with the deci-
sion threshold and allows to assess the difference
between the optimal value of the threshold and its
default value (0.5).
Figure 1: Evolution of the f
1
score with respect to
the threshold used to transform probabilities into
binary decisions
5 Experiments
The features and learning strategies described in
the two previous sections were evaluated on the
English to Spanish datasets. As no official devel-
opment set was provided by the shared task orga-
nizers, we randomly sampled 200 sentences from
the training set and use them as a test set through-
out the rest of this article. Preliminary experiments
show that the choice of this test has a very low im-
pact on the classification performance. The dif-
ferent hyper-parameters of the training algorithm
4
Any instance that is classified positive with respect to a
given threshold will be classified positive for all lower thresh-
olds as well.
351
Table 3: Prediction performance for the two learn-
ing strategies considered
Classifier thres. r
BAD
p
BAD
f
1
Random forest 0.43 0.64 0.69 0.67
Logistic regression 0.27 0.51 0.72 0.59
were chosen by maximizing classification perfor-
mance (as evaluated by the f
1
score) estimated on
150 sentences of the training set kept apart as a
validation set.
Results for the different learning algorithms
considered are presented in Table 3. Random for-
est clearly outperforms a simple logistic regres-
sion, which shows the importance of using non-
linear decision functions, a conclusion at pair with
our previous results (Zhuang et al., 2012; Singh et
al., 2013).
The overall performance, with a f
1
measure of
0.67, is pretty low and in our opinion, not good
enough to consider using such a quality estimation
system in a computer-assisted post-edition con-
text. However, as shown in Table 4, the prediction
performance highly depends on the POS category
of the words: it is quite good for ?plain? words
(like verb and nouns) but much worse for other
categories.
There are two possible explanations for this
observation: predicting the correctness of some
morpho-syntaxic categories may be intrinsically
harder (e.g. for punctuation the choice of which
can be highly controversial) or depend on infor-
mation that is not currently available to our sys-
tem. In particular, we do not consider any in-
formation about the structure of the sentence and
about the labels of the context, which may explain
why our system does not perform well in predict-
ing the labels of determiners and conjunctions. In
both cases, this result brings us to moderate our
previous conclusions: as a wrong punctuation sign
has not the same impact on translation quality as a
wrong verb, our system might, regardless of its f
1
score, be able to provide useful information about
the quality of a translation. This also suggests that
we should look for a more ?task-oriented? metric.
Finally, Figure 2 displays the importance of the
different features used in our system. Random
forests deliver a quantification of the importance
of a feature with respect to the predictability of the
target variable. This quantification is derived from
Table 4: Prediction performance for each POS tag
System f
1
VERB 0.73
PRON 0.72
ADJ 0.70
NOUN 0.69
ADV 0.69
overall 0.67
DET 0.62
ADP 0.61
CONJ 0.57
PUNCT 0.56
the position of a feature in a decision tree: fea-
tures used in the top nodes of the trees, which con-
tribute to the final prediction decision of a larger
fraction of the input samples, play a more impor-
tant role than features used near the leaves of the
tree. It appears that, as for our previous experi-
ments (Wisniewski et al., 2013), the most relevant
feature for predicting translation quality is the fea-
ture derived from the SOUL language model, even
if other fluency features seem to also play an im-
portant role. Surprisingly enough, features related
to the pseudo-reference do not seem to be useful.
Further experiments are needed to explain the rea-
sons of this observation.
6 Conclusion
In this paper we described the system submitted
for Task 2 of WMT?14 Shared Task on Quality
Estimation. Our system relies on a binary clas-
sifier and consider only a few dense and contin-
uous features. While the overall performance is
pretty low, a fine-grained analysis of the errors of
our system shows that it can predict the quality of
plain words pretty accurately which indicates that
a more ?task-oriented? evaluation may be needed.
Acknowledgments
This work was partly supported by ANR project
Transread (ANR-12-CORD-0015). Warm thanks
to Quoc Khanh Do for his help for training a SOUL
model for Spanish.
References
Joshua Albrecht and Rebecca Hwa. 2007. Regression
for sentence-level mt evaluation with pseudo refer-
352
0 0.02 0.04 0.06 0.08 0.1 0.12 0.14
notInIBM1Table
wordNotInSearchSpace
pseudoRefCommon3gram
pseudoRefCommon1gram
pseudoRefCommon2gram
maxMatchingNGramSize
geomIBM1
diffMaxLM
bestAl
wordPosterior
arithIBM1
posLM
tradiLM
maxLMScore
priorProba
soulLM
Feature Importance
F
e
a
t
u
r
e
Figure 2: Features considered by our system sorted by their relevance for predicting translation errors
ences. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
296?303, Prague, Czech Republic, June. ACL.
Joshua Albrecht and Rebecca Hwa. 2008. The role
of pseudo references in MT evaluation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 187?190, Columbus, Ohio, June.
ACL.
Alexandre Allauzen, Nicolas P?echeux, Quoc Khanh
Do, Marco Dinarelli, Thomas Lavergne, Aur?elien
Max, Hai-Son Le, and Franc?ois Yvon. 2013. LIMSI
@ WMT13. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 62?
69, Sofia, Bulgaria, August. ACL.
Stephen Boyd and Lieven Vandenberghe. 2004. Con-
vex Optimization. Cambridge University Press, New
York, NY, USA.
Leo Breiman. 2001. Random forests. Mach. Learn.,
45(1):5?32, October.
Olivier Chapelle and Yi Chang. 2011. Yahoo! learn-
ing to rank challenge overview. In Olivier Chapelle,
Yi Chang, and Tie-Yan Liu, editors, Yahoo! Learn-
ing to Rank Challenge, volume 14 of JMLR Pro-
ceedings, pages 1?24. JMLR.org.
Tom Fawcett. 2003. ROC Graphs: Notes and Practical
Considerations for Researchers. Technical Report
HPL-2003-4, HP Laboratories, Palo Alto.
Adri`a Gispert, Graeme Blackwood, Gonzalo Iglesias,
and William Byrne. 2013. N-gram posterior prob-
ability confidence measures for statistical machine
translation: an empirical study. Machine Transla-
tion, 27(2):85?114.
Trevor Hastie, Robert Tibshirani, and Jerome H. Fried-
man. 2003. The Elements of Statistical Learning.
Springer, July.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In
Acoustics, Speech and Signal Processing (ICASSP),
2011 IEEE International Conference on, pages
5524?5527. IEEE.
Llu??s Padr?o and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2012), Istanbul, Turkey, May. ELRA.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC?12), Istan-
bul, Turkey, may. European Language Resources
Association (ELRA).
Anil Kumar Singh, Guillaume Wisniewski, and
Franc?ois Yvon. 2013. LIMSI submission for
the WMT?13 quality estimation task: an experi-
ment with n-gram posteriors. In Proceedings of the
Eighth Workshop on Statistical Machine Transla-
tion, pages 398?404, Sofia, Bulgaria, August. ACL.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
353
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621, Uppsala, Sweden, July.
ACL.
Radu Soricut and Sushant Narsale. 2012. Combining
quality prediction and system selection for improved
automatic translation output. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 163?170, Montr?eal, Canada, June. ACL.
Guillaume Wisniewski, Anil Kumar Singh, and
Franc?ois Yvon. 2013. Quality estimation for ma-
chine translation: Some lessons learned. Machine
Translation, 27(3).
Yong Zhuang, Guillaume Wisniewski, and Franc?ois
Yvon. 2012. Non-linear models for confidence es-
timation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 157?162,
Montr?eal, Canada, June. ACL.
354
